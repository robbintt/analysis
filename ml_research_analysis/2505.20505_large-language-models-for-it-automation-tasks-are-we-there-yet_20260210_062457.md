---
ver: rpa2
title: 'Large Language Models for IT Automation Tasks: Are We There Yet?'
arxiv_id: '2505.20505'
source_url: https://arxiv.org/abs/2505.20505
tags:
- tasks
- code
- task
- ansible
- automation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ITAB, an execution-driven benchmark for evaluating
  large language models on real-world IT automation tasks using Ansible. The benchmark
  includes 126 diverse tasks across seven domains, with dynamic validation in containerized
  environments to test functional correctness rather than just syntax.
---

# Large Language Models for IT Automation Tasks: Are We There Yet?

## Quick Facts
- arXiv ID: 2505.20505
- Source URL: https://arxiv.org/abs/2505.20505
- Authors: Md Mahadi Hassan; John Salvador; Akond Rahman; Santu Karmaker
- Reference count: 40
- None of 14 open-source LLMs achieved pass@10 rate above 12% on real-world IT automation tasks

## Executive Summary
This paper introduces ITAB, an execution-driven benchmark for evaluating large language models on real-world IT automation tasks using Ansible. The benchmark includes 126 diverse tasks across seven domains, with dynamic validation in containerized environments to test functional correctness rather than just syntax. When evaluated across 14 open-source LLMs, none achieved a pass@10 rate above 12%, with the top model (Qwen2.5-Coder-7B) reaching only 12%. Error analysis of 1,411 failures revealed two primary failure modes: deficiencies in state reconciliation reasoning (44.87% of errors) and module-specific execution knowledge (24.37%). These findings demonstrate that current LLMs struggle with core IT automation requirements like tracking system state and applying precise module configurations, highlighting the need for architectural advances in state reasoning and domain-specific adaptation.

## Method Summary
The paper introduces ITAB (IT Automation Benchmark), an execution-driven evaluation framework that tests LLMs' ability to generate functional Ansible playbooks. The benchmark includes 126 curated tasks from Stack Overflow across seven IT automation domains. Evaluation uses a Docker-based test environment with four containerized nodes (Ubuntu, Alpine, CentOS, Red Hat) connected via SSH. Generated playbooks are executed dynamically, and success requires syntactic validity, successful execution, and achieving the desired system state. The study evaluates 14 open-source LLMs using temperature variations (0.2-0.8) and TELeR prompt levels 1-3, measuring pass@k (k ∈ {1, 3, 5, 10}) through unbiased estimators.

## Key Results
- Top model Qwen2.5-Coder-7B achieved only 12% pass@10, demonstrating significant LLM limitations
- 44.87% of errors stem from state reconciliation reasoning failures (variable handling, host targeting, path resolution, template logic)
- 24.37% of errors arise from module-specific knowledge gaps (attribute/parameter configuration, wrong module usage)
- Temperature effects show lower temperatures (0.2) maximize pass@1 reliability, while higher temperatures (0.6-0.8) boost pass@10 through increased output diversity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs fail at IT automation primarily because they cannot maintain coherent internal representations of system state across execution steps.
- **Mechanism:** IT automation tools like Ansible require state reconciliation: inferring desired states from scripts, comparing with current states, and applying only necessary changes. LLMs generate code token-by-token without maintaining a persistent state model, leading to errors in variable tracking, host targeting, path resolution, and template logic—tasks that require reasoning across execution scope.
- **Core assumption:** State reconciliation is fundamentally different from algorithmic code generation because it requires cross-scope reasoning about declarative system states rather than procedural logic.
- **Evidence anchors:**
  - [abstract] "44.87% of errors stem from state reconciliation reasoning failures"
  - [section 5] "failures in state reconciliation related reasoning are widespread...underscoring difficulties in tracking state across complex scopes (playbooks, roles, templates)"
  - [corpus] Related work on Ansible challenges (paper 98363) confirms practitioner difficulties with state management, but does not directly test LLM state reasoning mechanisms.
- **Break condition:** If models with explicit state-tracking architectures (e.g., memory-augmented or tool-calling agents) show significantly higher pass@k on state reconciliation tasks, this mechanism would be validated; otherwise, the bottleneck may lie elsewhere.

### Mechanism 2
- **Claim:** Pre-training on general code does not transfer to precise module configuration knowledge in IT automation.
- **Mechanism:** Models can identify appropriate Ansible modules ("what" to do) but frequently err in configuring module-specific attributes and parameters ("how" to do it). This knowledge gap arises because IT automation modules have idiosyncratic parameter schemas and execution semantics that are underrepresented in general code corpora.
- **Core assumption:** Module-specific configuration knowledge requires exposure to domain-specific documentation and usage patterns, not just syntactic fluency.
- **Evidence anchors:**
  - [abstract] "24.37% arise from module-specific knowledge gaps"
  - [section 5] "models may identify appropriate Ansible modules but frequently err in their precise implementation, with Attribute and Parameter Errors being common"
  - [corpus] Corpus does not contain direct studies of LLM module configuration knowledge transfer; this remains an inferential claim.
- **Break condition:** If fine-tuning on Ansible module documentation and examples significantly reduces attribute/parameter errors without improving state reconciliation, this would isolate the mechanism; if both improve together, the distinction is less clear.

### Mechanism 3
- **Claim:** Sampling temperature controls a precision-versus-exploration trade-off: lower temperatures maximize reliability (pass@1), while higher temperatures boost diversity and pass@10 for complex tasks.
- **Mechanism:** At lower temperatures, models sample high-probability tokens, favoring deterministic outputs that may be syntactically correct but miss rare correct solutions. At higher temperatures, increased stochasticity enables exploration of diverse solutions, occasionally discovering correct ones for complex or customized tasks, but at the cost of more frequent errors.
- **Core assumption:** Complex IT automation tasks require exploration beyond the model's default solution space.
- **Evidence anchors:**
  - [abstract] Not directly stated; inferred from methodology.
  - [section 4.2] "lower temperatures (e.g., 0.2) maximized pass@1 scores by favoring reliable, deterministic outputs. Conversely, higher temperatures (0.6–0.8) boosted pass@10 by increasing output diversity"
  - [section 6] "higher temperatures, by generating a more diverse set of outputs...enabled the model to occasionally discover correct, albeit rare, solutions"
  - [corpus] No direct corpus evidence for this mechanism in IT automation contexts.
- **Break condition:** If all models show monotonic improvement in pass@1 and pass@10 with lower temperatures, the exploration mechanism would be disconfirmed; if complex tasks consistently require higher temperatures for pass@10 gains, the mechanism holds.

## Foundational Learning

- **Concept: State Reconciliation (Infrastructure as Code)**
  - **Why needed here:** Understanding that Ansible playbooks are declarative (desired state) rather than imperative (step-by-step) is essential for diagnosing why syntactically correct scripts fail functionally.
  - **Quick check question:** Can you explain why a playbook that runs without syntax errors might still fail to achieve the intended system state?

- **Concept: Ansible Execution Model (Playbooks, Modules, Variables, Hosts)**
  - **Why needed here:** The error taxonomy (variable, host, path, template issues) maps directly to Ansible's execution primitives; familiarity with these terms is prerequisite for interpreting failure patterns.
  - **Quick check question:** What is the difference between a playbook's variable scope and a template's Jinja2 context, and how might an LLM confuse them?

- **Concept: LLM Sampling and Temperature**
  - **Why needed here:** The paper systematically varies temperature and analyzes its impact on pass@k; understanding stochastic sampling is necessary to interpret the trade-offs.
  - **Quick check question:** If you set temperature to 0.8, would you expect more or fewer distinct error types compared to temperature 0.2, and why?

## Architecture Onboarding

- **Component map:** ITAB Benchmark -> Evaluation Pipeline (Acquisition -> Execution -> Termination) -> Docker Test Environment -> pass@k Metrics
- **Critical path:**
  1. Task selection -> context analysis -> state definition -> parameter identification -> assertion development
  2. Prompt generation (TELeR levels 1-3, temperature 0.2-0.8) -> LLM inference -> YAML validation
  3. Playbook execution in Docker -> state assertion -> pass/fail classification -> error taxonomy labeling
- **Design tradeoffs:**
  - **Static vs. Dynamic Validation:** ITAB uses execution-based validation, which is resource-intensive but captures functional correctness; static analysis would miss state reconciliation failures
  - **Prompt Detail:** Higher TELeR levels provide more context but can lead to over-engineered solutions; the paper found inconsistent benefits across models
  - **Temperature Selection:** Lower temperatures prioritize reliability (pass@1), higher temperatures prioritize exploration (pass@10); practitioners must choose based on deployment constraints
- **Failure signatures:**
  - **State Reconciliation Failures (44.87%):** Errors in variable handling, host targeting, path resolution, template logic—look for undefined variables, wrong hosts, invalid paths, Jinja2 issues
  - **Module Knowledge Gaps (24.37%):** Attribute/parameter errors, wrong module usage—look for syntactically valid modules with incorrect configuration
  - **Syntax Errors (21.45% aggregated):** Invalid YAML, malformed playbooks—especially prevalent in reasoning-distilled models
- **First 3 experiments:**
  1. **Reproduce baseline pass@k for Qwen2.5-Coder-7B:** Run ITAB evaluation at T=0.2 and T=0.8, confirm reported pass@1 and pass@10 values, and manually inspect a sample of failures from each error category
  2. **Ablate prompt detail:** Compare pass@k across TELeR levels 1-3 for a single model, measuring both success rates and solution complexity (e.g., playbook length, redundant tasks)
  3. **Test error-aware prompting:** Implement the error-avoidance prompts (Tables 9-11) and quantify the marginal improvement over baseline prompts; document which error categories are reduced versus unaffected

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do proprietary LLMs (e.g., GPT-4, Claude) perform on IT automation tasks compared to the open-source models evaluated?
- **Basis in paper:** [explicit] The authors explicitly state in Section 9 (Limitations) that their "evaluation focuses exclusively on open-source models ranging from 3-14B parameters, leaving questions about how larger proprietary models or smaller specialized models might perform."
- **Why unresolved:** Budget constraints and the authors' focus on accessibility and cost-efficiency led them to exclude proprietary models from the study.
- **What evidence would resolve it:** Running the ITAB benchmark on proprietary models like GPT-4, Claude, or Gemini, using identical evaluation protocols and reporting pass@k metrics for direct comparison.

### Open Question 2
- **Question:** Can domain-specific fine-tuning or retrieval-augmented generation significantly reduce module-specific knowledge errors in IT automation?
- **Basis in paper:** [explicit] The authors identify that 24.37% of failures stem from "deficiencies in module-specific execution knowledge," and conclude that "overcoming key bottlenecks... demands more than prompt tuning—pointing to needs for architectural or domain-specific enhancements."
- **Why unresolved:** The paper only tests prompt-based interventions (TELeR levels, error-aware prompting), which yielded marginal improvements; no fine-tuning or RAG approaches were evaluated.
- **What evidence would resolve it:** Fine-tuning open-source models on Ansible documentation and module specifications, or implementing RAG with official Ansible docs, then re-evaluating on ITAB to measure reduction in module-related errors.

### Open Question 3
- **Question:** How do LLMs perform on multi-playbook orchestration scenarios that are common in enterprise IT automation?
- **Basis in paper:** [explicit] The authors note in Section 9 that their "evaluation primarily tests individual playbooks rather than complex multi-playbook orchestration scenarios that are common in enterprise environments."
- **Why unresolved:** ITAB tasks are self-contained single-playbook problems; enterprise automation typically involves interdependent playbooks, shared variables, and complex state management across execution stages.
- **What evidence would resolve it:** Extending ITAB with multi-playbook orchestration tasks requiring coordination across roles, variable precedence across files, and state reconciliation between dependent automation sequences.

### Open Question 4
- **Question:** What is the causal relationship between pretraining data composition (code/text ratio, dataset scale) and state reconciliation reasoning capabilities?
- **Basis in paper:** [inferred] The authors observe that Qwen2.5-Coder (70% code/20% text) outperformed CodeLlama (85% code), and general-purpose Llama-3.1-8B outperformed code-specialized models, suggesting "success in ITAB might be influenced not only by code volume but also by the interplay of code exposure, general language understanding, and dataset scale."
- **Why unresolved:** The study reports correlations between model characteristics and performance but does not isolate the causal contribution of specific training factors.
- **What evidence would resolve it:** Controlled experiments training models with systematically varied code/text ratios and scales on IT automation-specific corpora, then evaluating state reconciliation performance independently from syntactic correctness.

## Limitations

- The benchmark focuses exclusively on open-source models, leaving questions about proprietary model performance
- Evaluation tests individual playbooks rather than complex multi-playbook orchestration scenarios common in enterprise environments
- Missing task descriptions with implementation constraints and validation scripts create reproduction barriers

## Confidence

- State reconciliation failure mechanism: **Medium** - supported by error taxonomy but not causally validated
- Module knowledge transfer gap: **Low** - inferred from error patterns without direct knowledge testing
- Temperature-exploration trade-off: **Medium** - empirically observed but mechanism unclear
- Pass@k metric validity: **High** - execution-based validation is robust, though limited to valid syntax

## Next Checks

1. **State reasoning ablation:** Implement a memory-augmented LLM variant that maintains explicit state representations during playbook generation, then re-run ITAB to quantify reduction in state reconciliation failures.
2. **Module knowledge isolation:** Fine-tune a base model on Ansible module documentation and parameter schemas, then measure improvement specifically in attribute/parameter error rates while controlling for state reasoning ability.
3. **Temperature mechanism dissection:** For temperature 0.8 vs 0.2, analyze whether successful solutions at high temperature are qualitatively different (novel approaches) or merely more diverse attempts of similar strategies, using solution similarity clustering.