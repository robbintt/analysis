---
ver: rpa2
title: 'CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph
  Question Answering'
arxiv_id: '2504.09665'
source_url: https://arxiv.org/abs/2504.09665
tags:
- ambiguity
- entity
- clarification
- dataset
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses ambiguity resolution in knowledge graph question
  answering (KGQA), where user queries often contain entity and intent ambiguities
  that existing methods struggle to handle. The proposed CLEAR-KGQA framework employs
  a Bayesian inference mechanism to quantify query ambiguity and guide large language
  models (LLMs) in requesting clarification from users within a multi-turn dialogue
  framework.
---

# CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph Question Answering

## Quick Facts
- **arXiv ID:** 2504.09665
- **Source URL:** https://arxiv.org/abs/2504.09665
- **Reference count:** 31
- **Key outcome:** Achieves 83.00 F1 on WebQSP and 52.46 F1 on CWQ using clarification-enhanced ambiguity resolution for KGQA

## Executive Summary
CLEAR-KGQA addresses ambiguity resolution in knowledge graph question answering by introducing a Bayesian inference mechanism to quantify query ambiguity and guide LLMs in requesting clarification from users. The framework employs a multi-turn dialogue system where an LLM-based QA agent interacts with a user simulator to iteratively refine ambiguous queries. By detecting both entity and intent ambiguities through entropy scoring, the system can proactively request clarification, significantly improving performance on standard KGQA benchmarks compared to state-of-the-art baselines.

## Method Summary
The CLEAR-KGQA framework uses GPT-4o or fine-tuned Llama-3.1-8B agents with four tools: SearchNodes (entity linking), SearchGraphPattern (predicate discovery), ExecuteSPARQL (query execution), and AskForClarification (user interaction). A Bayesian entropy scoring plugin monitors tool outputs to detect ambiguity, injecting hints into observations when entity ambiguity scores exceed 0.6 or intent ambiguity scores exceed 0.8. The system employs a two-agent interaction framework where a dummy user agent simulates clarification responses using golden SPARQL, enabling training data creation from interaction histories. The approach operates in low-resource settings with 50 annotated samples per question category.

## Key Results
- Achieves 83.00 F1 on WebQSP (vs. 73.15 for GPT-4o baseline, +9.85)
- Achieves 52.46 F1 on CWQ (vs. 48.06 for GPT-4o baseline, +4.40)
- Reduces entity ambiguity errors by 42.8% and intent ambiguity errors by 39.2% compared to baselines
- Shows 11.8 point gain for GPT-4o and 15.32 point gain for fine-tuned models on WebQSP with plugin enabled

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bayesian entropy scoring enables principled detection of when clarification is needed.
- **Mechanism:** The system computes posterior probabilities P(ei|Q) for candidate entities using LLM perplexity over entity descriptions, then normalizes entropy H/log N to produce a 0-1 ambiguity score. When this score exceeds a threshold, a hint is injected into the observation to prompt clarification.
- **Core assumption:** LLM perplexity correlates with semantic relevance between questions and entity descriptions.
- **Evidence anchors:**
  - [abstract] "employs a Bayesian inference mechanism to quantify query ambiguity"
  - [Section III-E] Equations 5-8 show the full entropy calculation pipeline
  - [corpus] Related papers discuss KGQA complexity but lack specific Bayesian ambiguity quantification methods
- **Break condition:** If perplexity fails to capture semantic similarity (e.g., domain-specific terminology), ambiguity scores become unreliable.

### Mechanism 2
- **Claim:** Explicit clarification hints guide LLMs to use AskForClarification tool at appropriate moments.
- **Mechanism:** Rather than relying on LLM self-judgment (which the authors found uncontrolled), a plugin monitors tool outputs and injects "(Hint: The entity may be ambiguous.)" into observations when scores exceed thresholds, narrowing the action space.
- **Core assumption:** LLMs follow injected hints more reliably than autonomous judgment for tool selection.
- **Evidence anchors:**
  - [Section III-B] "relying solely on tool descriptions in prompts, LLM's tool invocation decisions remain uncontrollable"
  - [Section V-I] Ablation shows 9.85 point drop on WebQSP without plugin (73.15 vs 83.00)
  - [corpus] No direct corpus evidence for hint-based guidance vs. autonomous clarification
- **Break condition:** If hints become too frequent (low thresholds), user interaction burden increases without proportional accuracy gains.

### Mechanism 3
- **Claim:** Two-agent simulation enables training data creation from interaction histories.
- **Mechanism:** A dummy user agent (given golden SPARQL) simulates clarification responses, enabling the QA agent to practice disambiguation. Interaction histories are then used to generate unambiguous question variants.
- **Core assumption:** The user simulator accurately represents real user clarification behavior.
- **Evidence anchors:**
  - [Section III-D] Equations 3-4 define the two-agent formulation
  - [Section III-F] "use the interaction history... to construct a refined question"
  - [corpus] Corpus lacks similar two-agent training frameworks for KGQA
- **Break condition:** If simulated responses diverge from real user behavior, generated training data introduces distribution shift.

## Foundational Learning

- **Concept: Bayesian Inference for Uncertainty Quantification**
  - Why needed here: Core to the ambiguity scoring mechanism; requires understanding prior, likelihood, posterior, and entropy normalization
  - Quick check question: Can you explain why normalizing entropy by log N produces a comparable 0-1 score across different candidate set sizes?

- **Concept: Multi-turn Tool-Augmented LLM Agents**
  - Why needed here: The framework uses iterative tool calls (SearchNodes, SearchGraphPattern) with observation-action loops
  - Quick check question: How does the action space expand when adding AskForClarification to existing tools?

- **Concept: SPARQL Query Generation over Knowledge Graphs**
  - Why needed here: Final output is executable SPARQL; understanding graph patterns (eh, r, et) is essential
  - Quick check question: What's the difference between entity ambiguity (multiple nodes matching a name) and intent ambiguity (multiple predicates matching a relation)?

## Architecture Onboarding

- **Component map:**
  QA Agent (GPT-4o or fine-tuned Llama-3.1-8B) → calls Tools → receives Observations
  Tools: SearchNodes (entity linking), SearchGraphPattern (predicate discovery), ExecuteSPARQL (query execution), AskForClarification (user interaction)
  Clarification Plugin → monitors SearchNodes/SearchGraphPattern outputs → computes ambiguity scores → injects hints
  User Simulator Agent → responds to clarification requests using golden SPARQL context

- **Critical path:** Question input → SearchNodes → Entity ambiguity check (threshold 0.6) → [if ambiguous] AskForClarification → User response → SearchGraphPattern → Intent ambiguity check (threshold 0.8) → [if ambiguous] AskForClarification → User response → ExecuteSPARQL → Answer

- **Design tradeoffs:**
  - Lower thresholds = more clarifications = higher accuracy but more user friction (Figure 2 shows this curve)
  - Entity threshold (0.6) is stricter than intent threshold (0.8), reflecting different ambiguity distributions
  - GPT-4o benefits more from plugin than fine-tuned models (11.8 vs 15.32 point gain on WebQSP), suggesting instruction-following capability matters

- **Failure signatures:**
  - Predicate Recognition errors (25% for GPT-4o, 19% for fine-tuned): mapping natural language to KG predicates fails
  - Constraint Missing (29% for GPT-4o): temporal/ordinal constraints not translated to SPARQL
  - Reasoning Errors (39% for fine-tuned): predicate direction misinterpretation

- **First 3 experiments:**
  1. **Reproduce ambiguity score distribution:** Run the plugin on WebQSP test set without threshold filtering, plot score histograms to validate the polarized intent distribution claim (Figure 3)
  2. **Threshold sensitivity analysis:** Grid search entity threshold [0.5, 0.6, 0.7, 0.8, 0.9] with intent fixed at 0.8, measure F1 vs. average clarification calls per query
  3. **Ablation by ambiguity type:** Disable entity scoring only, then intent scoring only, to isolate which contributes more on datasets with different ambiguity profiles (WebQSP vs. CWQ)

## Open Questions the Paper Calls Out

- **Question:** How does CLEAR-KGQA perform when clarification requests are answered by real human users rather than an LLM-based simulator?
- **Question:** Do the ambiguity score thresholds (0.6 for entity, 0.8 for intent) generalize across different knowledge graphs and query distributions?
- **Question:** Can the predicate recognition and constraint handling errors identified in the error analysis be reduced through targeted extensions to the clarification framework?
- **Question:** What is the optimal trade-off between clarification frequency and answer accuracy in different deployment contexts?

## Limitations
- The system relies heavily on carefully crafted prompt templates that are not fully specified, creating uncertainty about exact replication
- The two-agent framework uses a user simulator that may not accurately reflect real human clarification patterns, potentially introducing distribution shift
- While demonstrated in low-resource settings, the methodology for ensuring representative category coverage with limited data remains unclear

## Confidence
- **High confidence:** Bayesian entropy scoring mechanism and its implementation details (equations provided, thresholds clearly specified at 0.6 for entities, 0.8 for intent)
- **Medium confidence:** Overall F1 improvements over baselines (83.00 on WebQSP, 52.46 on CWQ) though dependent on unspecified prompt templates and simulator behavior
- **Low confidence:** The effectiveness of the AskForClarification tool design in real-world scenarios beyond the simulator environment

## Next Checks
1. **Prompt template validation:** Obtain or reconstruct the exact prompt templates used and verify that the ambiguity scoring plugin consistently injects hints at appropriate thresholds across diverse query types
2. **Simulator behavior validation:** Compare clarification patterns from the simulator-generated dataset against actual human-human interactions on a held-out sample to quantify distribution shift
3. **Threshold optimization study:** Systematically vary entity (0.5-0.9) and intent (0.6-0.9) thresholds on a validation set to identify optimal trade-offs between accuracy gains and user interaction burden