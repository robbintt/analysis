---
ver: rpa2
title: 'Don''t Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting
  Strategy for Text Classification'
arxiv_id: '2502.07165'
source_url: https://arxiv.org/abs/2502.07165
tags:
- principles
- classification
- prompting
- principle
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent prompting strategy for text
  classification, where multiple LLM agents independently generate candidate principles
  from demonstrations, which are then consolidated into a final principle by a central
  agent. The consolidated principle is used to guide classification tasks.
---

# Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification

## Quick Facts
- arXiv ID: 2502.07165
- Source URL: https://arxiv.org/abs/2502.07165
- Reference count: 4
- Outperforms strong baselines with 1.55%–19.37% gains in macro-F1 score over zero-shot prompting

## Executive Summary
This paper introduces a multi-agent prompting strategy for text classification that generates task-specific principles from demonstrations rather than using raw examples. Multiple LLM agents independently extract candidate principles from sampled demonstrations, which are consolidated into a final principle by a central agent. The consolidated principle is then used to guide classification tasks, outperforming traditional in-context learning approaches while reducing inference costs through shorter input tokens.

## Method Summary
The method employs a multi-agent framework where six different LLM generators independently produce principle candidates from sampled demonstrations. These candidates are consolidated by a finalizer agent into a unified principle that captures task-specific decision criteria. The consolidated principle is appended to classification prompts, replacing instance-level context with task-level knowledge. This approach achieves competitive performance to few-shot in-context learning while requiring significantly lower inference costs due to shorter input token lengths.

## Key Results
- Achieves 1.55%–19.37% gains in macro-F1 score over zero-shot prompting across tested datasets
- Outperforms few-shot ICL while using shorter input tokens and lower inference costs
- Principles generated by the method are more effective than human-crafted ones on two private datasets
- Ablation studies confirm that label information and the multi-agent cooperative framework are critical to generating high-quality principles

## Why This Works (Mechanism)

### Mechanism 1
Abstracted principles derived from demonstrations guide classification more effectively than raw demonstrations. Multiple LLMs independently analyze demonstration samples to extract distinguishing features for each class, which are consolidated into a unified principle that captures task-specific decision criteria. This replaces instance-level context with task-level knowledge.

### Mechanism 2
Multi-agent collaboration produces higher-quality principles than single-agent generation. Multiple generator agents produce diverse principle candidates, capturing different aspects of the classification boundary. A finalizer agent consolidates these by integrating complementary insights and resolving conflicts, reducing single-agent blind spots and biases.

### Mechanism 3
Task-level principle generation reduces inference cost while maintaining or improving performance. Principles are generated once per task (not per instance), then appended to all classification prompts. This replaces the linear token growth of few-shot demonstrations with a fixed-size principle context, achieving shorter inputs at inference time.

## Foundational Learning

- **In-Context Learning (ICL)**: This method is an ICL enhancement strategy; understanding ICL's sensitivity to demonstration quality and ordering is prerequisite. Why needed here: This method is an ICL enhancement strategy; understanding ICL's sensitivity to demonstration quality and ordering is prerequisite. Quick check question: Why might adding more demonstrations degrade ICL performance in some cases?

- **Multi-Agent LLM Frameworks**: The architecture relies on orchestrating multiple agents with distinct roles (generator, finalizer, classifier). Why needed here: The architecture relies on orchestrating multiple agents with distinct roles (generator, finalizer, classifier). Quick check question: What is the difference between competitive (ranking) and collaborative (consolidation) multi-agent paradigms, and when might each fail?

- **Prompt Engineering for Classification**: Effective principle generation and consolidation prompts are central to the method's success. Why needed here: Effective principle generation and consolidation prompts are central to the method's success. Quick check question: How does explicitly instructing an LLM to "extract distinguishing features" differ from asking it to simply "classify"?

## Architecture Onboarding

- **Component map**: Generator Agents (6 LLMs) -> Finalizer Agent -> Classifier Agent
- **Critical path**: 1) Sample n demonstrations from training set, 2) Each generator agent independently produces principle candidates, 3) Finalizer agent consolidates candidates into unified principle, 4) Classifier agent receives: task description + finalized principle + input instance → prediction
- **Design tradeoffs**: Labeled vs. unlabeled demonstrations show inconsistent effects; consolidation outperformed ranking; stronger models for generation/finalization and weaker open-source models for classification to manage cost; more demos (16) did not consistently outperform fewer (4 or 8)
- **Failure signatures**: 1) Consolidated principles are generic (insufficient task-specific signals), 2) Ranking underperforms random (competitive mode failing), 3) Performance drops vs. vanilla zero-shot (principles may be misleading), 4) Multi-class tasks with many labels (principles become unwieldy)
- **First 3 experiments**: 1) Single-agent baseline: Implement principle generation with one LLM and compare macro-F1 against vanilla zero-shot, 2) Label ablation: Generate principles with labeled vs. unlabeled demonstrations (n=8) and measure impact, 3) Multi-agent consolidation validation: Run 3-agent generation with consolidation finalization; compare against single-agent to quantify collaboration benefit

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on proprietary models (Claude 3.5 Sonnet) for principle generation and consolidation raises reproducibility concerns
- Performance gains come from datasets with relatively small label spaces (up to 6 classes), scalability to multi-class problems unclear
- Consolidation process sometimes produces generic principles that could apply to any classification task

## Confidence
**High Confidence**: The mechanism that principles outperform raw demonstrations for classification guidance, The multi-agent framework consistently outperforms single-agent generation, Task-level principle generation reduces inference costs while maintaining performance

**Medium Confidence**: The consolidation approach is superior to ranking across all tested conditions, Label information improves principle quality (though effects are inconsistent), The specific agent capability allocation (stronger models for generation, weaker for classification) is optimal

**Low Confidence**: The method's effectiveness on datasets with large label spaces (>6 classes), The robustness of principles when demonstration samples contain noise or bias, The generalizability to domains requiring highly contextual, instance-specific judgments

## Next Checks
1. **Scalability Test**: Evaluate the method on multi-class datasets with 20+ labels to assess principle generation and consolidation effectiveness in high-dimensional label spaces.

2. **Robustness to Noisy Demonstrations**: Systematically inject controlled noise into demonstration samples and measure how principle quality and downstream classification performance degrade.

3. **Cross-Domain Transfer**: Train principles on one domain and test classification performance on a structurally similar but semantically different domain to assess true abstraction capability.