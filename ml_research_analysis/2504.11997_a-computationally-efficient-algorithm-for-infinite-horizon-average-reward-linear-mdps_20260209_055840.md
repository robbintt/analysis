---
ver: rpa2
title: A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear
  MDPs
arxiv_id: '2504.11997'
source_url: https://arxiv.org/abs/2504.11997
tags:
- value
- algorithm
- lemma
- clip
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reinforcement learning in infinite-horizon average-reward
  settings with linear MDPs, where the state space can be large or infinite. Previous
  approaches require computing the minimum of the value function over the entire state
  space, which is computationally prohibitive.
---

# A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs

## Quick Facts
- arXiv ID: 2504.11997
- Source URL: https://arxiv.org/abs/2504.11997
- Reference count: 40
- One-line result: γ-DC-LSCVI-UCB achieves O(sp(v*)√(d³T log(dT/δ) log T)) regret while being computationally efficient

## Executive Summary
This paper addresses the computational challenge of infinite-horizon average-reward reinforcement learning in linear MDPs, where previous approaches required computationally expensive global state-space minimization. The authors introduce a novel algorithm that achieves the same theoretical regret guarantees as prior work while only requiring computation over the set of states visited by the algorithm. The key innovation is an efficient clipping operation combined with deviation-controlled value iteration, which together maintain theoretical guarantees while dramatically reducing computational complexity. The algorithm achieves O(sp(v*)√(d³T log(dT/δ) log T)) regret where sp(v*) is the span of the optimal bias function.

## Method Summary
The proposed γ-DC-LSCVI-UCB algorithm uses a discount factor γ = 1 - 1/√T to reduce the average-reward problem to a discounted setting, where value iteration can be applied efficiently. The algorithm maintains a covariance matrix Λ_t for ridge regression, computing regression coefficients to estimate value functions. The core innovation is a deviation-controlled value iteration scheme that clips Q-functions using bounds derived from previous iterations to control value function drift. A key computational improvement is the clipping threshold m_t that only considers the minimum over visited states rather than the entire state space. The algorithm uses UCB bonuses scaled by β||φ(s,a)||_{Λ_t^{-1}} to ensure optimism, and maintains two previous Q-function sequences to enable the deviation control mechanism.

## Key Results
- Achieves O(sp(v*)√(d³T log(dT/δ) log T)) regret bound matching previous work
- Computational complexity independent of state space size (only depends on visited states)
- Introduces efficient clipping operation that enables tractable computation in infinite-horizon settings
- Deviation-controlled value iteration prevents regression coefficient amplification issues

## Why This Works (Mechanism)

### Mechanism 1: Efficient Clipping over Visited States
- Claim: Replacing global state-space minimization with visited-state minimization preserves theoretical guarantees while enabling computational tractability.
- Mechanism: The clipping threshold m_{t+1} = Ṽ^t_{t+1}(s_{t+1}) ∧ m_t uses only the next observed state, creating a telescoping error bound where the sum of clipping errors is O(1/(1-γ)).
- Core assumption: Visited states provide sufficient coverage to approximate global minimum for regret analysis.
- Evidence anchors: Abstract states the algorithm "only requires computing the minimum of value functions over the set of states visited by the algorithm."
- Break condition: If the agent visits only a narrow subset of states that doesn't include near-optimal clipping candidates, the telescoping bound may loosen significantly.

### Mechanism 2: Deviation-Controlled Value Iteration
- Claim: Explicitly clipping Q-functions to stay within bounds of previous iterations controls value function drift between episodes with different thresholds.
- Mechanism: Lines 6-8 of Algorithm 2 clip Q̃^t_u using bounds U^t_u = Q̃^{t-1}_u ∧ Q̃^{t-2}_u and L^t_u (shifted versions of previous Q-functions). Lemma 3 proves |Ṽ^{t+1}_u(s) - Ṽ^t_u(s)| ≤ m_{t-1} - m_{t+1}.
- Core assumption: Linear regression coefficient estimation can produce arbitrarily large errors when value functions differ slightly.
- Evidence anchors: Section 3.2 shows naive adaptation fails to control the difference due to regression coefficient amplification; Lemma 2 constructs explicit counterexample.
- Break condition: If clipping bounds L^t_u, U^t_u become too tight, optimism may be lost; if too loose, deviation control degrades.

### Mechanism 3: Discounted-Setting Approximation
- Claim: A carefully chosen discount factor γ = 1 - 1/√T makes the discounted optimal policy approximately optimal for the average-reward objective.
- Mechanism: Lemma 1 establishes |(1-γ)V*_γ(s) - J*| ≤ (1-γ)sp(v*), bounding approximation error. The regret analysis balances T(1-γ)sp(v*) against 1/(1-γ) terms.
- Core assumption: The optimal bias function span sp(v*) is bounded and known (or an upper bound is available).
- Evidence anchors: Section 2.2 shows Lemma 1 suggests the difference between optimal average reward J* and optimal discounted cumulative reward normalized by (1-γ) is small.
- Break condition: If sp(v*) is misspecified (actual span >> assumed span), regret scales with the true span, potentially invalidating bounds.

## Foundational Learning

- Concept: **Linear MDP structure**
  - Why needed here: The algorithm exploits [PV](s,a) = ⟨φ(s,a), w*(V)⟩ to estimate transitions via ridge regression rather than explicit model learning.
  - Quick check question: Can you explain why linear MDPs allow regression-based value estimation but tabular extended value iteration doesn't transfer directly?

- Concept: **Bias function and span constraints**
  - Why needed here: The regret bound scales with sp(v*); clipping constrains estimated value spans to H = 2sp(v*) for statistical efficiency.
  - Quick check question: What goes wrong if you clip to an arbitrary fixed span that doesn't relate to the optimal bias span?

- Concept: **Optimistic value iteration with UCB bonuses**
  - Why needed here: The bonus β||φ(s,a)||_{Λ^{-1}} ensures V^t_u(s) ≥ V*(s) (Lemma 5), enabling regret decomposition.
  - Quick check question: Why does the bonus scale with ||φ||_{Λ^{-1}} rather than a fixed exploration constant?

## Architecture Onboarding

- Component map:
  Data collector -> Regression module -> Value iteration loop -> Threshold tracker

- Critical path: The inner value iteration loop (lines 4-11 in Algorithm 2) runs O(T) iterations per timestep, making total complexity O(T³d²A). This is the computational bottleneck.

- Design tradeoffs:
  - Recomputing vs. caching: Unlike γ-LSCVI-UCB which reuses value functions when det(Λ_t) hasn't doubled, this algorithm recomputes every step to accommodate changing m_t. Cost: O(T³) vs. benefit: state-space-independent complexity.
  - Deviation control overhead: Maintaining two previous Q-function sequences (Q̃^{t-1}, Q̃^{t-2}) doubles memory but is necessary for linear MDPs (Lemma 2).

- Failure signatures:
  - Regret grows linearly rather than Õ(√T): Check if sp(v*) parameter is severely underestimated
  - Computational cost scales with state space: Verify clipping is using visited states only, not global minimum computation
  - Q-values diverge: Check deviation control bounds L^t_u, U^t_u are properly initialized

- First 3 experiments:
  1. Synthetic linear MDP with known sp(v*): Compare regret trajectory against theoretical Õ(√T) bound; verify computational cost is independent of state space size
  2. Ablation on deviation control: Run algorithm variant without L^t_u, U^t_u clipping bounds; confirm regret degradation or instability on pathological feature constructions from Lemma 2
  3. Sensitivity to span misspecification: Test with H = c·sp(v*) for c ∈ {0.5, 1, 2, 4}; observe regret scaling and optimism violations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational complexity of algorithms for infinite-horizon average-reward linear MDPs be improved to be strictly linear in the time horizon T?
- Basis in paper: [explicit] The authors state in the conclusion: "We leave further improving the computational complexity to be linear in T as future work."
- Why unresolved: The proposed algorithm has complexity of O(T³d²A) because it runs up to T steps of value iteration at every time step.
- What evidence would resolve it: An algorithm achieving Õ(√T) regret with total computational complexity of O(T·poly(d, A)).

### Open Question 2
- Question: Can the regret bound be tightened by a factor of √d through the use of variance-aware regression?
- Basis in paper: [explicit] The authors note: "An interesting future direction is to improve the regret bound by a factor of √d using variance-aware regression method."
- Why unresolved: Current regret bound is O(sp(v*)√(d³T)), while lower bound is Ω(d√(sp(v*)T)), leaving a √d gap in dimension dependence.
- What evidence would resolve it: An algorithm incorporating variance-aware weighting that achieves regret upper bound of O(sp(v*)d√T).

### Open Question 3
- Question: Can the efficient clipping and deviation-controlled value iteration techniques be extended to general function approximation settings?
- Basis in paper: [explicit] The conclusion lists this as a direction: "Another future direction is to extend the techniques to the general function approximation setting."
- Why unresolved: Current deviation control (Lemma 3) relies on specific linear properties and the form of the regression coefficient.
- What evidence would resolve it: A theoretical framework adapting the deviation-controlled value iteration scheme to general function classes with provable regret guarantees.

## Limitations

- The algorithm requires knowing or accurately estimating the optimal bias function span sp(v*), with regret scaling linearly with misspecification
- Computational complexity remains O(T³d²A) due to the inner value iteration loop, which may be prohibitive for very large time horizons
- The theoretical guarantees depend on the visited states providing sufficient coverage for the clipping operation to work effectively

## Confidence

- High confidence: The deviation-controlled value iteration mechanism and its theoretical justification (Lemma 2 and Lemma 3) are well-founded and rigorously proven
- Medium confidence: The clipping operation over visited states maintains theoretical guarantees, though this depends critically on visited states providing sufficient coverage
- Medium confidence: The reduction from average-reward to discounted settings via careful discount factor choice is sound, but relies on span parameter accuracy

## Next Checks

1. **Synthetic span sensitivity test**: Run the algorithm with deliberately incorrect sp(v*) estimates (e.g., 50%, 200% of true value) and measure how regret scales with the error ratio
2. **Coverage verification**: Design an MDP where the optimal policy visits a small subset of states, and test whether the visited-state clipping still provides the claimed theoretical guarantees
3. **Computational scaling experiment**: Measure actual runtime as T and d vary, comparing against the theoretical O(T³d²A) bound to identify practical bottlenecks