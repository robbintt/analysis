---
ver: rpa2
title: 'DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document
  Understanding'
arxiv_id: '2511.11552'
source_url: https://arxiv.org/abs/2511.11552
tags:
- page
- pages
- visual
- answer
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DocLens introduces a tool-augmented multi-agent framework that
  solves the evidence localization challenge in long visual document understanding.
  It employs a "Lens Module" that first navigates to relevant pages using OCR-augmented
  retrieval and then pinpoints specific visual elements through layout detection and
  cropping.
---

# DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding

## Quick Facts
- **arXiv ID:** 2511.11552
- **Source URL:** https://arxiv.org/abs/2511.11552
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing human experts for the first time.

## Executive Summary
DocLens is a multi-agent framework designed to address the evidence localization challenge in long visual document understanding. It introduces a "Lens Module" that first navigates to relevant pages using OCR-augmented retrieval and then pinpoints specific visual elements through layout detection and cropping. This fine-grained approach achieves near-perfect page recall (97.3%) and significantly improves comprehension of charts, tables, and figures. The "Reasoning Module" uses a sampling-adjudication mechanism to generate reliable answers. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing human experts for the first time, with particular strength in vision-centric and unanswerable queries.

## Method Summary
DocLens is a tool-augmented multi-agent framework that addresses long visual document understanding by first accurately localizing evidence pages and then isolating relevant visual elements. The Lens Module uses a Page Navigator (OCR tool + VLM) to retrieve relevant pages and an Element Localizer (layout tool + cropper) to extract charts and tables. The Reasoning Module employs a sampling-adjudication mechanism, generating multiple reasoning paths and using an adjudicator to select the best answer. Key hyperparameters include $T_e=8$ and $\tau=0.7$ for the Page Navigator, and $T_a=8$ and $\tau=0.7$ for the Answer Sampler.

## Key Results
- Achieves near-perfect evidence page recall (97.3%) using OCR-augmented navigation.
- Surpasses human experts on MMLongBench-Doc and FinRAGBench-V benchmarks for the first time.
- Excels on vision-centric queries and unanswerable questions (+13.8% on UNA subset).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** OCR-augmented navigation recovers evidence pages that visual-only retrieval misses.
- **Mechanism:** The Page Navigator inputs both page screenshots and OCR text into a Long-Context VLM. The text provides explicit lexical matching signals that complement visual attention, overcoming the "near-sightedness" of VLMs in long contexts.
- **Core assumption:** VLMs are currently more reliable at text-based retrieval than dense visual retrieval across many pages.
- **Evidence anchors:**
  - [abstract] "Lens Module uses a Page Navigator with OCR tools to achieve near-perfect evidence page recall (97.3%)..."
  - [section] Page 8, Table 4: OCR improves recall by 10.0% (Gemini-Pro) to 50.1% (Gemini-Flash) over visual-only baselines.
  - [corpus] Aligns with general findings in *MHier-RAG* and *URaG* that retrieval augmentation is critical for long-context VLMs.
- **Break condition:** If the document consists primarily of handwriting, obscure scripts, or diagrams with low OCR confidence, this lexical anchoring degrades, forcing reliance on weaker visual retrieval.

### Mechanism 2
- **Claim:** Spatial isolation of visual elements reduces reasoning noise and hallucination.
- **Mechanism:** The Element Localizer uses layout detection to crop specific charts or tables from the full page. This creates a "zoomed-in" view for the VLM, isolating the signal from the surrounding visual clutter (headers, footers, unrelated text).
- **Core assumption:** The VLM's reasoning accuracy is inversely proportional to the ratio of irrelevant visual pixels in the input.
- **Evidence anchors:**
  - [abstract] "...Element Localizer to identify fine-grained visual elements... DocLens... particularly excels on vision-centric... queries."
  - [section] Page 9, Figure 4: Shows performance gains are concentrated in "Visual-Only" and "Text&Visual" splits, not "Text-Only".
  - [corpus] *SlideAgent* (neighbor) also supports hierarchical decomposition of pages, validating the "divide-and-conquer" approach for visual docs.
- **Break condition:** If the layout detection tool fails to draw bounding boxes around non-standard or overlapping elements, the Localizer cannot crop the evidence, resulting in a fallback to the full-page view.

### Mechanism 3
- **Claim:** Sampling with Adjudication enforces faithfulness and mitigates hallucination.
- **Mechanism:** The Reasoning Module generates multiple distinct reasoning paths ($T_a$ samples). The Adjudicator acts as a critic, selecting the most consistent answer or identifying "unanswerable" states, rather than relying on a single greedy generation.
- **Core assumption:** Hallucinations are stochastic and unlikely to persist consistently across multiple sampling iterations compared to grounded truths.
- **Evidence anchors:**
  - [abstract] "...Reasoning Module employs a sampling-adjudication mechanism to generate reliable answers."
  - [section] Page 6, Table 1: Shows significant gains on the "Unanswerable" (UNA) subset (+13.8% for Gemini-Pro), indicating better uncertainty handling.
  - [corpus] *ALDEN* (neighbor) suggests active evidence gathering is key, but DocLens specifically adds the "Adjudicator" agent for consensus.
- **Break condition:** If the temperature $\tau$ is too low, samples become deterministic (no diversity); if too high, all samples may hallucinate, confusing the Adjudicator.

## Foundational Learning

- **Concept:** **Retrieval-Augmented Generation (RAG) for Vision**
  - **Why needed here:** DocLens is fundamentally a RAG architecture adapted for pixels. You must understand that the "Lens Module" is just a visual retriever.
  - **Quick check question:** Can you explain why the Page Navigator retrieves pages *before* the Element Localizer looks for charts, rather than scanning all charts in the document at once?

- **Concept:** **Layout Analysis and Bounding Boxes**
  - **Why needed here:** The system relies on external tools (like MinerU) to define "regions of interest." If you don't understand bounding boxes ($x_1, y_1, x_2, y_2$), you cannot debug the Element Localizer.
  - **Quick check question:** If the OCR tool returns a bounding box that is 10% too small, how would that visually crop the chart and potentially hide the legend?

- **Concept:** **Self-Consistency / Voting**
  - **Why needed here:** The Answer Sampler and Adjudicator rely on the principle that "majority vote" or "consistency check" beats single-pass inference.
  - **Quick check question:** Why does the Adjudicator prompt explicitly forbid frequency bias (ignoring how often an answer appears) and focus instead on "factual evidence"?

## Architecture Onboarding

- **Component map:** Input (PDF/Image + Query) -> Lens Module (Page Navigator -> Element Localizer) -> Reasoning Module (Answer Sampler -> Adjudicator) -> Output (Answer).
- **Critical path:** The OCR text generation is the most expensive pre-processing step. The Page Navigator's output determines the upper bound of accuracyâ€”if evidence is not in the top-k pages, the downstream reasoner will fail (hallucinate).
- **Design tradeoffs:**
  - **Recall vs. Precision in Page Nav:** The system optimizes for *recall* (near-perfect 97.3%), retrieving an average of 3.5 pages. This increases context window load but minimizes missed evidence.
  - **Cost vs. Reliability:** Sampling $T_a=8$ answers increases API costs significantly compared to baselines, but is required to hit the "Unanswerable" accuracy targets.
- **Failure signatures:**
  - **"Missing Evidence" Hallucination:** Usually a failure of the Page Navigator (evidence page was not retrieved).
  - **"Wrong Trend" Hallucination:** Usually a failure of the Element Localizer (it cropped the wrong chart or missed the legend).
  - **Over-refusal ("Not Answerable"):** Often occurs if the Adjudicator is too strict or if the Sampler failed to produce a coherent reasoning path.
- **First 3 experiments:**
  1. **Ablate the OCR:** Run the Page Navigator with *only* screenshots (no text) to verify the drop in Recall@10 reported in Table 4.
  2. **Visualize Crops:** Pass a dense financial report through the Element Localizer and manually inspect if bounding boxes cut off critical axis labels on charts.
  3. **Stress Test Unanswerable:** Feed the system questions designed to be unanswerable (e.g., "What is the revenue for year 2030?" in a 2020 report) to calibrate the Adjudicator's threshold.

## Open Questions the Paper Calls Out
None

## Limitations
- **Tool Dependency Risk:** Performance hinges on MinerU's OCR and layout detection accuracy, which may fail on handwriting or low-quality scans.
- **Context Window Pressure:** Retrieving 3.5 pages and generating 8 samples can exceed practical context limits, especially for high-resolution images.
- **Human Expert Baseline Calibration:** The claim of surpassing human experts lacks detailed methodology for the human evaluation setup.

## Confidence
- **High Confidence:** The evidence localization mechanism (OCR-augmented retrieval + spatial cropping) is well-supported by the 97.3% recall and the ablation study in Table 4.
- **Medium Confidence:** The sampling-adjudication mechanism's effectiveness is supported by UNA subset gains (+13.8%), but the exact adjudication criteria are complex.
- **Low Confidence:** The scalability claim for "long" documents (49+ pages) is demonstrated on the benchmarks but lacks analysis of performance degradation beyond tested ranges.

## Next Checks
1. **Tool Failure Simulation:** Intentionally degrade OCR output quality and measure the impact on Page Navigator recall and overall accuracy.
2. **Context Window Profiling:** Log exact token usage for API calls in the critical path to identify when the $K=50$ chunking strategy is triggered and measure performance drops.
3. **Human Expert Replication:** Replicate the human expert evaluation on a subset of the datasets with qualified annotators and report inter-annotator agreement.