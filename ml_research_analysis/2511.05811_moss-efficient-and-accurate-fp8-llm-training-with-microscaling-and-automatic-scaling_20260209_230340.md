---
ver: rpa2
title: 'MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic
  Scaling'
arxiv_id: '2511.05811'
source_url: https://arxiv.org/abs/2511.05811
tags:
- scaling
- training
- moss
- quantization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MOSS introduces an efficient FP8 training framework that overcomes\
  \ quantization and scaling bottlenecks in LLM training. The core method combines\
  \ a two-level microscaling strategy for activations\u2014using a high-precision\
  \ global FP32 scale with compact power-of-two local E8M0 scales\u2014to reduce dequantization\
  \ overhead in GEMM operations, and an automatic scaling approach for weights that\
  \ leverages Adam-like optimizers' bounded-update property to predict scaling factors\
  \ without runtime max-reduction."
---

# MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling

## Quick Facts
- arXiv ID: 2511.05811
- Source URL: https://arxiv.org/abs/2511.05811
- Reference count: 15
- Primary result: 34% throughput improvement over BF16 baselines while matching accuracy

## Executive Summary
MOSS introduces an efficient FP8 training framework for LLMs that overcomes quantization and scaling bottlenecks. The core innovation combines a two-level microscaling strategy for activations and an automatic scaling approach for weights. This framework achieves training accuracy matching BF16 baselines while delivering significant throughput improvements, demonstrating effectiveness on 7B parameter models under GPU resource constraints.

## Method Summary
MOSS addresses the computational overhead of dequantization in FP8 training through a two-level microscaling strategy for activations, using high-precision global FP32 scales with compact power-of-two local E8M0 scales. For weights, it employs an automatic scaling approach that leverages Adam-like optimizers' bounded-update property to predict scaling factors without runtime max-reduction. This eliminates expensive computations and frequent memory accesses, enabling efficient FP8 training while maintaining accuracy.

## Key Results
- Achieves 34% improvement in throughput compared to BF16 baselines
- Matches training accuracy of BF16 baselines on 7B parameter models
- Maintains competitive performance on downstream fine-tuning tasks
- Evaluated on OLMo-7B and LLaMA-2-7B models with 8 Hopper GPUs

## Why This Works (Mechanism)
The microscaling approach reduces quantization overhead by separating global and local scaling factors, allowing compact representation while preserving precision. The automatic weight scaling exploits the bounded nature of Adam optimizer updates to predict scaling factors statically, avoiding runtime max-reduction operations. Together, these innovations minimize memory bandwidth requirements and computational overhead during training.

## Foundational Learning

**Quantization and dequantization in neural networks**: Converting between low-precision (FP8) and high-precision (FP32) representations during computation introduces overhead. Needed to understand the computational bottleneck MOSS addresses. Quick check: Verify that GEMM operations dominate FP8 training time due to dequantization.

**Activation scaling strategies**: Different approaches to scaling activations affect both precision and computational efficiency. Needed to appreciate the innovation of two-level microscaling. Quick check: Confirm that using power-of-two local scales reduces quantization error compared to uniform scaling.

**Optimizer dynamics in large models**: Understanding how Adam-like optimizers behave in large models reveals why bounded updates enable static scaling factor prediction. Needed to grasp the automatic scaling mechanism. Quick check: Verify that weight updates in Adam remain within predictable bounds during training.

## Architecture Onboarding

**Component map**: Input tensors → Microscaling (activations) → GEMM operations → Automatic scaling (weights) → Output tensors

**Critical path**: The most time-consuming operations are dequantization in GEMM and max-reduction for weight scaling. MOSS optimizes both by eliminating runtime max-reduction and using compact local scales.

**Design tradeoffs**: MOSS trades some implementation complexity for significant throughput gains. The two-level scaling adds bookkeeping overhead but reduces quantization error and computational cost. The automatic scaling assumes bounded updates, which may not hold for all optimizer configurations.

**Failure signatures**: Training instability or accuracy degradation may indicate scaling factor misprediction or insufficient precision in local scales. Monitoring training loss and gradient norms can detect these issues early.

**First experiments**: 1) Compare throughput and accuracy against FP32 baseline on small model. 2) Vary local scale precision to find optimal balance between accuracy and speed. 3) Test scaling factor prediction accuracy across different optimizer configurations.

## Open Questions the Paper Calls Out

None specified in the provided information.

## Limitations
- Experimental validation limited to 7B parameter models; scaling assumptions for larger models unverified
- Limited analysis of numerical stability during extended training runs
- Focus on throughput improvements without comprehensive cost-benefit analysis including wall-clock time and energy efficiency

## Confidence
- Scaling to larger models: Medium - Claims of effective scaling lack experimental validation beyond 7B parameters
- Numerical stability: Low - Long training runs and fine-tuning stability not thoroughly evaluated
- Efficiency claims: Medium - Throughput improvements demonstrated but comprehensive resource utilization analysis missing

## Next Checks
1. Scale validation: Test MOSS on 30B+ parameter models to verify scaling assumptions and numerical stability over extended training periods
2. Convergence analysis: Compare convergence curves and final validation performance against BF16 baselines across multiple random seeds
3. Resource utilization audit: Measure actual memory bandwidth usage, power consumption, and wall-clock time per epoch to quantify the claimed efficiency gains comprehensively