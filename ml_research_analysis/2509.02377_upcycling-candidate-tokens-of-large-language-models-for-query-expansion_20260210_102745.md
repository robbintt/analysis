---
ver: rpa2
title: Upcycling Candidate Tokens of Large Language Models for Query Expansion
arxiv_id: '2509.02377'
source_url: https://arxiv.org/abs/2509.02377
tags:
- query
- ctqe
- retrieval
- tokens
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of query expansion in information
  retrieval, where the goal is to improve search performance by enriching user queries
  with related terms. While large language models (LLMs) have shown promise for query
  expansion, existing methods face a trade-off between generating diverse terms and
  computational cost.
---

# Upcycling Candidate Tokens of Large Language Models for Query Expansion

## Quick Facts
- arXiv ID: 2509.02377
- Source URL: https://arxiv.org/abs/2509.02377
- Reference count: 35
- Primary result: CTQE achieves 65.0 NDCG@10 on TREC19 with minimal token usage compared to more expensive multi-pass methods

## Executive Summary
This paper addresses the problem of query expansion in information retrieval, where the goal is to improve search performance by enriching user queries with related terms. While large language models (LLMs) have shown promise for query expansion, existing methods face a trade-off between generating diverse terms and computational cost. The paper proposes Candidate Token Query Expansion (CTQE), which leverages unselected candidate tokens from LLM decoding passes to achieve both relevance and diversity without additional inference cost.

CTQE demonstrates that tokens not selected by the LLM during generation—the top-k alternatives at each decoding step—contain valuable expansion signals. Experiments on 10 benchmarks using various LLMs and retrievers show that CTQE outperforms existing keyword-based query expansion methods and achieves competitive performance compared to more expensive approaches. Specifically, CTQE delivers strong retrieval performance with significantly lower computational cost, achieving up to 65.0 NDCG@10 on TREC19 with minimal token usage.

## Method Summary
CTQE extracts top-k=20 candidate tokens from LLM logprobs during single-pass keyword generation, filters them to first-position tokens only (removing duplicates and short tokens), and integrates them with generated keywords via interpolation. For lexical retrievers, CTQE builds separate indices for keywords and candidates, combining scores with α=0.9. For neural retrievers, it combines embeddings with weights α_q=0.5, α_W=0.1, α_C=0.1. The original query is repeated 5× before concatenation to prevent expansion terms from overwhelming it.

## Key Results
- CTQE achieves 65.0 NDCG@10 on TREC19 with minimal token usage compared to more expensive multi-pass methods
- CTQE outperforms existing keyword-based query expansion methods (Q2K, Exp4Fuse) across 10 benchmarks
- CTQE is particularly effective on low-resource datasets, outperforming pseudo-document generation approaches
- The method integrates seamlessly with both sparse and dense retrievers without additional inference cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Candidate tokens from LLM decoding provide semantically relevant expansion terms without additional inference cost.
- Mechanism: During autoregressive decoding, LLMs compute full probability distributions over the vocabulary at each step. The top-k alternatives at each position are conditioned on the query context and represent plausible continuations that reflect different semantic facets of the query intent.
- Core assumption: Unselected high-probability tokens capture query-relevant semantics rather than noise.
- Evidence anchors: [abstract] "These tokens, though not part of the final output, are conditioned on the full query and capture useful information." [section 2.2] "At each position j of a keyword wi, the LLM evaluates multiple alternatives before choosing the final token wi,j."

### Mechanism 2
- Claim: Restricting candidate tokens to first-position tokens preserves diversity while reducing conditional redundancy.
- Mechanism: Tokens at position j > 1 are conditioned on the previously selected token w_i,1, constraining their semantic space. First-position candidates across different generated keywords each represent independent semantic branches from the query, providing broader coverage.
- Core assumption: First-position tokens are sufficiently diverse and relevant; later positions add diminishing returns or noise.
- Evidence anchors: [section 2.2] "Since candidate tokens in C'i,j for j > 1 are strongly conditioned on the first token wi,1, they tend to offer limited semantic diversity." [figure 2 center] Ablation shows "Dedup + 1st pos" yields substantial gains over using all candidates or deduplication alone.

### Mechanism 3
- Claim: Interpolating keyword-based and candidate-based scores allows complementary signals to be combined without interference.
- Mechanism: For lexical retrievers, CTQE builds a keyword-augmented query (q + W) and separately indexes candidate tokens (C), then combines scores via weighted interpolation. This prevents candidate tokens from diluting the original query's precision while adding recall-oriented signals.
- Core assumption: Candidate tokens provide recall benefits that keyword expansion alone doesn't capture; the two signals are partially non-overlapping.
- Evidence anchors: [section 2.2] "S_CTQE(d) = α · S_expan(d) + (1 − α) · S_C(d), where α ∈ [0, 1] is a hyperparameter." [table 1] CTQE with PRF achieves 65.0 NDCG@10 on TREC19 vs. 61.0 for Q2K/PRF.

## Foundational Learning

- Concept: **Query Expansion (QE) in Information Retrieval**
  - Why needed here: CTQE is a QE method; understanding the problem it solves (vocabulary mismatch, semantic gap) is prerequisite.
  - Quick check question: Why does adding related terms to a query improve retrieval effectiveness?

- Concept: **LLM Autoregressive Decoding and Logprobs**
  - Why needed here: CTQE exploits the probability distribution over tokens at each decoding step; you need to understand what logprobs represent and how to access them.
  - Quick check question: At each generation step, what does the model compute before selecting the output token?

- Concept: **Sparse vs. Dense Retrieval Scoring**
  - Why needed here: CTQE applies different integration strategies for lexical (BM25), learned sparse (SPLADE), and dense (BGE) retrievers.
  - Quick check question: How does BM25 score a document given a query, and how does this differ from dense retrieval using embedding similarity?

## Architecture Onboarding

- Component map: LLM Generation Module -> Candidate Token Filter -> Query Composer -> Scoring Integration -> Retrieval Execution

- Critical path:
  1. Prompt LLM with query → extract generated keywords AND top-k candidate logprobs at each position
  2. Filter candidates → first-position only, deduplicated, length-filtered
  3. Build query representations per retriever type
  4. Compute interpolated scores → rank documents

- Design tradeoffs:
  - **k (top-k candidates)**: Higher k increases diversity but may introduce noise; paper uses k=20 (API limit)
  - **α (interpolation weight)**: Higher α prioritizes generated keywords over candidates; paper uses 0.9 for lexical
  - **Query repetition factor**: Original query repeated 5× to prevent expansion terms from overwhelming it; this is a heuristic that may need tuning per domain

- Failure signatures:
  - **Token budget exhausted with no gain**: Candidate tokens may be low-quality if LLM is weak or prompt is misaligned
  - **Performance degradation on high-resource datasets**: Over-expansion can hurt precision; check α and filtering
  - **Latency spikes despite single-pass design**: Subword index construction or embedding encoding for candidates may be unoptimized

- First 3 experiments:
  1. **Baseline comparison**: Run CTQE vs. Q2K (keyword-only) on a single BEIR dataset (e.g., SciFact) with BM25 to confirm candidate tokens add measurable gain
  2. **Ablation on filtering**: Compare "All candidates" vs. "Dedup only" vs. "Dedup + 1st pos" to validate the filtering pipeline contributes incrementally
  3. **Retriever compatibility**: Apply CTQE to both SPLADE and BGE on TREC DL19 to verify the neural integration formula works as intended

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive or learned weighting schemes for combining candidate tokens with generated keywords outperform the fixed interpolation weights (α=0.9, αq=0.5, αW=0.1, αC=0.1) used in CTQE?
- Basis in paper: [inferred] The paper manually sets hyperparameters for score interpolation across all experiments without exploring learned or query-adaptive weighting schemes.
- Why unresolved: The paper does not investigate whether the optimal weighting varies by query characteristics, dataset domain, or retriever type.
- What evidence would resolve it: Experiments comparing fixed weights against learned weighting networks or query-adaptive weighting schemes across the same benchmark suite.

### Open Question 2
- Question: Why does CTQE show particularly strong improvements on low-resource datasets compared to high-resource ones, and can this advantage be characterized theoretically?
- Basis in paper: [explicit] The paper states "CTQE is particularly effective on low-resource datasets, even outperforming the pseudo-document generation approaches" and "surprisingly, CTQE is particularly effective on low-resource datasets" without providing a thorough explanation.
- Why unresolved: The authors note this empirical finding but do not analyze the underlying mechanisms.
- What evidence would resolve it: A detailed analysis comparing the lexical overlap between candidate tokens and ground-truth relevant documents across low-resource vs. high-resource datasets.

### Open Question 3
- Question: Would access to more than top-20 candidate tokens (beyond the OpenAI API limit) yield diminishing returns or continued improvements in retrieval performance?
- Basis in paper: [explicit] The paper states "we set k = 20, corresponding to the top-20 candidate tokens considered at each decoding step, the maximum provided by the OpenAI API."
- Why unresolved: The k=20 constraint is an API limitation rather than an empirical choice.
- What evidence would resolve it: Experiments using open-source LLMs (e.g., LLaMA) with full access to vocabulary distributions, systematically varying k and measuring both NDCG@10 and noise ratio.

## Limitations

- Candidate token quality dependency: CTQE's performance critically depends on the quality of unselected candidate tokens, which may degrade to noise for low-resource languages or domains.
- Position restriction heuristic: The first-position filtering assumption lacks empirical validation on diverse query types and may not provide sufficient coverage for semantically homogeneous keywords.
- Hyperparameter sensitivity: The paper uses fixed values (k=20, α=0.9, 5× query repetition) without systematic sensitivity analysis, which may not generalize across different retrievers and domains.

## Confidence

- **High Confidence**: CTQE achieves competitive performance with lower computational cost than multi-pass query expansion methods. The ablation study on filtering strategies is well-supported by experimental results.
- **Medium Confidence**: CTQE provides additive benefits when combined with pseudo-relevance feedback. The interpolation framework is theoretically sound but may require domain-specific tuning.
- **Low Confidence**: The generalization of CTQE's performance across different LLM models and retriever architectures without hyperparameter adjustment.

## Next Checks

1. **Robustness to LLM variation**: Test CTQE with different LLM models (both stronger and weaker than gpt-4.1-mini) to verify performance consistency, specifically evaluating whether first-position filtering maintains effectiveness when candidate token quality varies.

2. **Latent failure mode analysis**: Systematically test CTQE on datasets where the LLM has known training coverage gaps (low-resource languages, niche domains) to measure whether candidate tokens degrade to noise and quantify the performance impact.

3. **Hyperparameter sensitivity mapping**: Conduct a grid search over k (top-k candidates), α (interpolation weight), and query repetition factor on a representative subset of datasets to identify whether performance is robust to these choices or requires per-dataset optimization.