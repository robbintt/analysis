---
ver: rpa2
title: You Need Better Attention Priors
arxiv_id: '2601.15380'
source_url: https://arxiv.org/abs/2601.15380
tags:
- attention
- prior
- priors
- standard
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GOAT is a new attention mechanism that generalizes standard dot-product
  attention by viewing it through the lens of Entropic Optimal Transport, revealing
  that standard attention corresponds to a transport problem regularized by an implicit
  uniform prior. We introduce a learnable, continuous prior that replaces this naive
  assumption while maintaining full compatibility with optimized kernels such as FlashAttention.
---

# You Need Better Attention Priors

## Quick Facts
- arXiv ID: 2601.15380
- Source URL: https://arxiv.org/abs/2601.15380
- Authors: Elon Litman; Gabe Guo
- Reference count: 35
- Key outcome: GOAT improves in-distribution perplexity by 1.55 points over ALiBi while maintaining robust extrapolation to 16× training length

## Executive Summary
GOAT introduces a learnable, continuous prior to attention mechanisms by reframing softmax attention as Entropic Optimal Transport (EOT) regularized by an implicit uniform prior. This allows the model to learn spatial structure directly within the attention computation, combining the flexibility of learned embeddings with the extrapolation benefits of fixed encodings. The method maintains full compatibility with FlashAttention while addressing the attention sink problem through a key-only bias.

## Method Summary
GOAT replaces standard attention by interpreting it through EOT theory, where the optimal attention distribution is the solution to minimizing transport cost plus entropy. The key innovation is replacing Shannon entropy with KL-divergence against a learnable prior π, resulting in an additive "log-prior" term in the attention logits. To maintain SDPA compatibility and translation equivariance, the log-prior is parameterized as a finite trigonometric polynomial (Fourier series) with relative and sink components. The method uses a scaling trick to ensure the prior enters the dot product "unscaled" while maintaining FlashAttention compatibility.

## Key Results
- Improves C4 language modeling in-distribution perplexity by 1.55 points over ALiBi
- Maintains near-perfect accuracy on long-context retrieval tasks at context lengths far exceeding training
- Achieves lower validation NLL and reduces peak CUDA memory allocation by 36% compared to RoPE on genome modeling
- Enables robust zero-shot resolution extrapolation on ImageNet-1k

## Why This Works (Mechanism)

### Mechanism 1: Attention as KL-Regularized Optimal Transport
Standard softmax attention is the unique solution to an Entropic Optimal Transport problem regularized by a uniform prior; replacing this with a learnable prior π generalizes the mechanism. By substituting Shannon entropy with KL-divergence against an arbitrary prior π, the optimal attention distribution shifts to p* = softmax(s/τ + log π), adding a "log-prior" term to the logits.

### Mechanism 2: Trigonometric Priors for SDPA Compatibility
To maintain compatibility with FlashAttention while enforcing translation equivariance and bounded extrapolation, the log-prior must be a finite trigonometric polynomial. The log-prior K_ij is split into relative (K_rel) and absolute/sink components, with K_rel parameterized as a Fourier series that can be computed as a dot product of positional query/key vectors using angle-difference identities.

### Mechanism 3: Sinks as Low-Signal Defaults
Attention sinks emerge as the optimal solution to the EOT objective when content signal is weak, and they can be disentangled from content representations via a key-only bias. Theorem 5.1 establishes that as content signal range ω_i → 0, the posterior collapses to the prior, preventing the model from "inventing" high-norm content vectors to act as sinks.

## Foundational Learning

- **Concept**: Entropic Optimal Transport (Sinkhorn Distances)
  - **Why needed**: The entire paper rests on re-framing attention not as a heuristic similarity match, but as a probabilistic transport plan regularized by entropy.
  - **Quick check**: Can you explain why replacing Shannon entropy H(p) with -KL(p || π) in the optimization objective results in an additive term in the logits?

- **Concept**: Spectral Bias / Fourier Features
  - **Why needed**: GOAT enforces a specific structural bias (translation equivariance) using trigonometric series.
  - **Quick check**: How does the dot product ⟨q_rel, k_rel⟩ recover a function of relative distance (i-j) using trigonometric identities?

- **Concept**: Attention Sinks & "Outlier" Tokens
  - **Why needed**: The paper offers a theoretical explanation for a phenomenon (massive attention to [BOS] tokens) often treated as a bug or artifact.
  - **Quick check**: In the GOAT framework, does a sink token need to have high semantic similarity to the query to attract attention?

## Architecture Onboarding

- **Component map**: Content Subspace (W_q^c, W_k^c) -> Positional Subspace (q_rel, k_rel, k_sink, q_sink) -> Fusion (Scaling Trick) -> Kernel (FlashAttention)
- **Critical path**: The Scaling Trick is the most critical implementation detail. You must construct composite vectors q', k' such that the dot product is ⟨q_c, k_c⟩/√d_c + K_ij.
- **Design tradeoffs**: Increasing d_p (positional rank) allows for more complex priors (more frequencies) but steals capacity from d_c (content).
- **Failure signatures**: Runaway Norms (if scaling trick is misconfigured), Extrapolation Collapse (if frequencies not properly constrained), Sink Blindness (if key-only bias not implemented correctly).
- **First 3 experiments**:
  1. Copy-Mixture Validation: Replicate the toy task to verify spectral weights α_r, β_r and sink bias u(j) are learning the intended structure.
  2. Ablation on Scaling: Train with correct composite scaling vs. standard uniform scaling to validate "unscaled prior" hypothesis.
  3. Extrapolation Stress Test: Train on L=1024, evaluate on L=4096 and L=8192 to ensure trigonometric boundedness prevents degradation.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does GOAT maintain efficiency and length-extrapolation capabilities when scaled to LLM sizes (7B+ parameters) and significantly longer training horizons?
- **Open Question 2**: Can the fixed geometric frequencies {ω_r} be replaced with learnable frequencies to improve expressivity without violating stability guarantees?
- **Open Question 3**: How can the translation-equivariant assumptions be adapted for non-Euclidean data structures like graph attention or irregular meshes?

## Limitations
- The EOT interpretation rests on modeling assumptions (queries as unit impulses, linear costs) that may not hold in all architectures
- The requirement for trigonometric priors is a strong architectural constraint that may limit expressivity for non-periodic positional relationships
- Performance improvements are demonstrated but the relative contribution of the learnable prior versus disentangled sinks versus improved scaling is not isolated

## Confidence
- **High Confidence**: The core EOT derivation showing standard attention as a uniform-prior transport problem is mathematically rigorous
- **Medium Confidence**: The practical implementation details (trigonometric parameterization, scaling trick) are logically derived but depend on specific architectural choices
- **Low Confidence**: The claim that this specific EOT-based prior is uniquely beneficial compared to other learned positional schemes is not definitively proven

## Next Checks
1. Ablation of Core Mechanisms: Train GOAT variants ablating key components (uniform prior, remove sink term, non-trigonometric positional parameterization) to isolate contributions
2. Frequency Sensitivity Analysis: Systematically vary number and distribution of frequencies {ω_r} to measure impact on in-distribution perplexity and extrapolation stability
3. Cross-Domain Stress Test: Evaluate GOAT on extreme extrapolation tasks (long genomic sequences, multi-resolution image classification) to validate robustness beyond reported benchmarks