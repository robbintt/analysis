---
ver: rpa2
title: 'Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for
  Large Language Model'
arxiv_id: '2507.06892'
source_url: https://arxiv.org/abs/2507.06892
tags:
- training
- off-policy
- policy
- data
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of on-policy reinforcement
  fine-tuning (RFT) methods like PPO and GRPO in improving reasoning abilities of
  large language models (LLMs), which require extensive computational cost due to
  the need for fresh data generation at each iteration. To overcome this limitation,
  the authors propose ReMix, a general approach that enables on-policy proximal policy
  gradient methods to leverage off-policy data generated during training.
---

# Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model

## Quick Facts
- arXiv ID: 2507.06892
- Source URL: https://arxiv.org/abs/2507.06892
- Reference count: 40
- Primary result: Achieves SOTA-level math reasoning performance with 30x-450x reduction in training data volume

## Executive Summary
This paper addresses the inefficiency of on-policy reinforcement fine-tuning (RFT) methods like PPO and GRPO for improving reasoning abilities of large language models. These methods require extensive computational cost due to fresh data generation at each iteration. The authors propose ReMix, a general approach that enables on-policy proximal policy gradient methods to leverage off-policy data generated during training, achieving state-of-the-art performance on five math reasoning benchmarks while dramatically reducing training costs.

## Method Summary
ReMix integrates three key components: mix-policy proximal policy gradient with increased update-to-data ratio for efficient training, KL-convex policy constraint to balance stability and flexibility, and policy reincarnation to seamlessly transition from efficient early-stage learning to stable asymptotic improvement. The method samples mini-batches from a mixture of on-policy and off-policy trajectories, uses importance sampling ratios to correct distribution shift, and implements a mid-training "reincarnation" step that switches from mix-policy to pure on-policy optimization. Experiments demonstrate ReMix achieves SOTA-level performance with over 30x to 450x reduction in training cost in terms of rollout data volume.

## Key Results
- Achieves SOTA-level performance on AIME'24, AMC'23, Minerva, OlympiadBench, and MATH500 benchmarks
- Reduces training cost by 30x to 450x in terms of rollout data volume compared to standard PPO/GRPO
- Reveals insights into how off-policy learning affects reasoning behaviors such as self-reflection and response length
- Demonstrates efficient sample utilization through increased update-to-data ratio while maintaining stability

## Why This Works (Mechanism)

### Mechanism 1: Mix-Policy Importance Sampling
Reusing historical rollouts via mix-policy importance sampling accelerates early-stage learning without immediate instability. The method samples mini-batches from a mixture of on-policy trajectories and off-policy trajectories from historical policies. Importance sampling ratios correct distribution shift with clipping applied around π_k/π_{k-i} ± ε rather than the standard [1-ε, 1+ε] range. This allows gradient updates using stale data while bounding variance from policy divergence. The distribution shift between consecutive policy versions remains bounded (importance ratio ≈ 1.0 ± small tolerance) so that clipped importance sampling provides valid gradient estimates.

### Mechanism 2: KL-Convex Policy Constraint
Dynamically interpolated KL constraints stabilize training by anchoring to both the original base model and the immediate predecessor policy. Instead of a single KL penalty D_KL(π_θ || π_base), ReMix uses a convex combination: λ·D_KL(π_θ || π_base) + (1-λ)·D_KL(π_θ || π_{k-1}). The base-model term preserves foundational capabilities and prevents catastrophic forgetting; the predecessor term allows rapid local adaptation. The coefficient λ decays over training steps. The base model π_base encodes generalizable reasoning capabilities, while π_{k-1} represents the current knowledge frontier worth preserving during rapid updates.

### Mechanism 3: Policy Reincarnation
A mid-training "reincarnation" step—switching from mix-policy to pure on-policy optimization—recovers asymptotic performance that off-policy bias would otherwise sacrifice. After T steps of Mix-PPG training, the method discards the historical policy buffer, resets the KL reference from π_base to the current π_T, and switches to standard PPO/GRPO. This removes accumulated off-policy bias while keeping the improved policy as the new baseline. Early off-policy updates provide sufficient exploration and skill acquisition, after which on-policy refinement can safely converge without destabilizing divergence.

## Foundational Learning

- **Concept: Importance Sampling in Policy Gradient**
  - Why needed: ReMix relies on correcting for distribution shift when using off-policy data. Without understanding how importance ratios r = π_current/π_old reweight gradients, the clipping strategy and its failure modes are opaque.
  - Quick check: Given a trajectory from policy π_old with reward R, what happens to the policy gradient estimate if π_current assigns near-zero probability to actions that π_old sampled frequently?

- **Concept: On-Policy vs Off-Policy Sample Efficiency Trade-off**
  - Why needed: The paper's core motivation is that on-policy methods (PPO, GRPO) discard data after each update, wasting compute. Understanding why off-policy methods can reuse data but suffer from bias is essential to grasp ReMix's hybrid design.
  - Quick check: Why does PPO require fresh rollouts every iteration, and what statistical assumption breaks when reusing stale trajectories?

- **Concept: KL Divergence as a Trust Region Constraint**
  - Why needed: ReMix's KL-convex constraint modifies the standard single-anchor KL penalty. Understanding KL's role in preventing large policy jumps clarifies why dual anchoring helps balance stability and adaptability.
  - Quick check: If you removed all KL constraints from PPO during fine-tuning on a small dataset, what failure mode would you expect?

## Architecture Onboarding

- **Component map:**
  Training Loop -> Stage 1 (Mix-PPG) -> Stage Transition (Policy Reincarnation) -> Stage 2 (On-Policy PPG)

- **Critical path:**
  1. Implement V-trace corrected advantage estimation (Eq. 8-10 in Appendix A) — this is non-optional for off-policy stability.
  2. Implement per-policy log_prob storage in rollout buffer; you must compute π_{k-i}(a|s) at training time, not re-generate.
  3. Implement KL-convex loss with decaying λ; incorrect λ scheduling is a common silent failure.
  4. Implement the reincarnation trigger at step T with reference model swap.

- **Design tradeoffs:**
  - UTD ratio (m): Higher m = more gradient steps per rollout → better sample efficiency, but amplifies off-policy bias. Paper uses m=2.
  - Off-policy proportion (p): Higher p = more data reuse → faster early gains, but more severe late-stage degradation. Paper uses p=0.4.
  - Reincarnation step (T): Earlier T → less off-policy accumulation but weaker starting point for on-policy phase. Paper uses T=50–100 for 1.5B, T=50 for 7B.
  - Historical window (N): Larger N = more past policies to sample from → more diversity, but older policies increase distribution shift. Paper uses N=2.

- **Failure signatures:**
  - Response length collapse: If average response length drops sharply and self-reflection tokens disappear, off-policy bias is too strong (p too high, N too large, or UTD too aggressive).
  - Importance ratio explosion: If mean importance ratio deviates significantly from 1.0 (e.g., > 1.001), the clipping is too loose or policies are diverging too fast.
  - Post-reincarnation performance drop: If accuracy drops after step T, the reincarnation reference π_T may be too weak; consider increasing T or reducing p.

- **First 3 experiments:**
  1. Baseline reproduction: Implement standard PPO on DeepScaleR-Preview-Dataset with the same 1.5B base model. Measure Pass@1 vs. rollout volume to establish a Pareto curve.
  2. Ablation on off-policy proportion (p): Train Mix-PPG (no reincarnation, no KL-convex) with p ∈ {0.1, 0.3, 0.5}. Plot Pass@1, response length, and importance ratio vs. steps.
  3. Full ReMix with reincarnation sweep: Implement full ReMix with T ∈ {25, 50, 100, 200} on a held-out subset. Verify that T ≈ 50–100 recovers asymptotic performance while T too early/late degrades it.

## Open Questions the Paper Calls Out

### Open Question 1: Adaptive Control of Off-Policy Data Proportion
Can an adaptive mechanism for controlling the proportion of off-policy data (p) improve the trade-off between training efficiency and final performance compared to the fixed proportions used in ReMix? The authors note in Limitations that "an adaptive control on the proportion of off-policy data should be possible and favorable." An experiment comparing ReMix with fixed p against a variant where p is adjusted dynamically (e.g., based on the importance sampling ratio or validation performance) would show superior sample efficiency or asymptotic performance.

### Open Question 2: Whipping Effect in Long-Horizon Reasoning
Does the "Whipping Effect" (the implicit preference for shorter responses) fundamentally limit the application of off-policy RFT in domains requiring long-horizon reasoning, such as coding or agentic tasks? The paper documents the phenomenon where off-policy discrepancy incentivizes shorter responses, leading to a "collapse mode of self-reflection behavior." A study applying ReMix to complex coding benchmarks (e.g., SWE-bench) and analyzing if the "Whipping Effect" degrades performance more severely than in mathematical reasoning would be valuable.

### Open Question 3: Scaling to Larger Models
How does ReMix scale to Large Language Models with parameter counts significantly larger than 7B? The authors note that "due to the computational resource constraint, we did not conduct experiments on models larger than 7B." Training results applying ReMix to a 70B+ parameter base model, demonstrating that the efficiency gains (30x-450x reduction in rollouts) hold without introducing optimization instabilities, would address this limitation.

### Open Question 4: Combining with Advanced RFT Techniques
Can ReMix be effectively combined with orthogonal advanced RFT techniques that utilize extrinsic off-policy guidance (e.g., LUFFY)? The authors mention that "our method is orthogonal to many of the advanced RFT methods" but do not explore the combination. A comparative study evaluating a model trained with ReMix while simultaneously incorporating distillation data from a stronger teacher model would show the interaction between these two distinct sources of off-policy data.

## Limitations
- The KL-convex constraint design assumes the base model retains robust reasoning capabilities, which may not hold if the base model has reasoning gaps
- Policy reincarnation timing appears empirically tuned rather than theoretically derived, making the method sensitive to hyperparameter selection
- V-trace approximation validity in the LLM reasoning context where action spaces are large and rewards are sparse has not been validated

## Confidence
- **High confidence**: The efficiency gains (30x-450x reduction in rollout data) are well-supported by controlled experiments
- **Medium confidence**: The behavioral observations (whipping effect, self-reflection changes) are documented but could be influenced by prompt engineering choices
- **Low confidence**: The claim that reincarnation "seamlessly" transitions between stages is primarily empirical; the mechanism for why exactly T=50-100 works optimally remains unclear

## Next Checks
1. **Sensitivity analysis on reincarnation timing**: Systematically vary T across {25, 50, 100, 200} on a held-out subset to quantify the trade-off between early efficiency gains and late-stage performance recovery
2. **Importance ratio monitoring**: Track the mean and variance of importance ratios π_current/π_old throughout training to empirically validate the bounded distribution shift assumption
3. **Cross-task generalization**: Test ReMix on non-math reasoning tasks (e.g., commonsense reasoning, code generation) to verify the approach generalizes beyond mathematical problem-solving