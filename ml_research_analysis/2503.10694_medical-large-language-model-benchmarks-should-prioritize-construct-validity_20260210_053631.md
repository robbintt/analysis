---
ver: rpa2
title: Medical Large Language Model Benchmarks Should Prioritize Construct Validity
arxiv_id: '2503.10694'
source_url: https://arxiv.org/abs/2503.10694
tags:
- validity
- medical
- benchmarks
- construct
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that medical LLM benchmarks need empirical validation\
  \ for construct validity\u2014ensuring they measure what they claim to measure in\
  \ real-world practice. It draws an analogy to psychological testing, showing how\
  \ validity theory can guide benchmark design."
---

# Medical Large Language Model Benchmarks Should Prioritize Construct Validity

## Quick Facts
- arXiv ID: 2503.10694
- Source URL: https://arxiv.org/abs/2503.10694
- Reference count: 30
- Primary result: Medical LLM benchmarks like MedQA show poor predictive validity when tested against real-world clinical data, indicating they may not measure actual clinical competence.

## Executive Summary
This paper argues that medical LLM benchmarks need empirical validation for construct validity—ensuring they measure what they claim to measure in real-world practice. Drawing an analogy to psychological testing, the authors propose using real-world EHR data to validate benchmarks, exemplified by experiments testing MedQA's predictive, content, and construct validity. Results show that model performance on MedQA correlates weakly with real-world clinical accuracy, and that MedQA's content domain is simpler than actual clinical practice. The paper advocates for a "benchmark-validation-first" approach, where benchmarks are empirically tested for validity before being used to evaluate LLMs.

## Method Summary
The authors validate MedQA benchmark by matching its clinical vignettes to real-world EHR cases using standardized medical codes (RxNorm for drugs, SNOMED for diagnoses). They extract UMLS concepts from both benchmark and real cases using cTAKES, then evaluate LLMs on both datasets. Validation metrics include criterion validity (conditional accuracy α = P(correct on real case | correct on benchmark)), content validity (concept coverage and task distribution comparison), and construct validity (correlation of model rankings between benchmark and real data).

## Key Results
- Benchmark performance poorly predicts real-world clinical accuracy (α values 0.29-0.56, barely above baseline)
- Real-world patient cases contain significantly more UMLS concepts than benchmark vignettes (~3x more)
- Model rankings differ substantially between benchmark and real-world data, indicating poor construct validity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medical LLM benchmarks function analogously to psychological tests, measuring latent constructs rather than directly observable capabilities.
- Mechanism: Both benchmark performance and psychological test scores serve as proxies for underlying constructs (e.g., "clinical reasoning," "intelligence") that lack operational definitions. The inference from score to construct requires validity evidence.
- Core assumption: The psychometric framework developed for human testing applies meaningfully to LLM evaluation.
- Evidence anchors:
  - [abstract] "construct validity refers to the ability of a test to measure an underlying 'construct', that is the actual conceptual target of evaluation"
  - [Section 2] "LLM benchmark datasets and psychological tests share a key conceptual similarity: both aim to measure a 'latent construct'—an abstract, unobservable trait that is not operationally defined"
  - [corpus] "Measuring what Matters: Construct Validity in Large Language Model Benchmarks" directly extends this psychometric framing
- Break condition: If LLMs process information fundamentally differently than humans, the analogy may not hold for certain constructs.

### Mechanism 2
- Claim: Criterion validity can be empirically tested by correlating benchmark performance with real-world clinical outcomes.
- Mechanism: Match benchmark items to similar real-world cases via standardized codes (RxNorm, SNOMED), then measure whether benchmark accuracy predicts real-world accuracy using conditional probability α = P(correct on real case | correct on benchmark).
- Core assumption: Code-based matching captures clinically meaningful similarity between vignettes and patient cases.
- Evidence anchors:
  - [Section 4.1] "α is the conditional accuracy of an LLM on real-world cases that resemble the clinical vignettes it correctly answered in the benchmark"
  - [Section 4.1] Results show α values (0.29–0.56) offer "little improvement over the overall accuracy," indicating poor predictive validity
  - [corpus] Corpus papers on validity frameworks exist but lack direct empirical criterion validation studies
- Break condition: If matching methodology fails to capture clinical similarity, observed gaps may reflect measurement error rather than true validity failures.

### Mechanism 3
- Claim: Content validity gaps arise because benchmarks sample from a narrower, cleaner content domain than real-world practice.
- Mechanism: Benchmarks like MedQA use streamlined vignettes; real clinical notes contain more UMLS concepts and different task distributions (more treatment decisions, fewer diagnosis questions).
- Core assumption: Concept count and task distribution are valid proxies for content domain complexity.
- Evidence anchors:
  - [Section 4.2] "real-world patient cases are associated with a significantly larger number of USMLE concepts compared to clinical vignettes in MedQA"
  - [Section 4.2] MedQA "disproportionately favors diagnostic questions over treatment-related ones"
  - [corpus] "Mind the Gap" paper on African disease burdens similarly finds content validity gaps in existing benchmarks for specific populations
- Break condition: More concepts ≠ harder task; noisy data may not require different clinical reasoning.

## Foundational Learning

- Concept: **Latent constructs**
  - Why needed here: Understanding that "clinical knowledge" or "reasoning" cannot be measured directly—only inferred from observable performance—is essential to grasping why validity matters.
  - Quick check question: Can you name a medical capability that could be measured directly versus one that is a latent construct?

- Concept: **Criterion vs. content vs. construct validity**
  - Why needed here: The paper uses this tripartite framework to structure validation experiments; each type answers a different question about benchmark quality.
  - Quick check question: If a benchmark predicts real-world performance but only tests diagnosis, which validity type(s) has it demonstrated?

- Concept: **UMLS/SNOMED/RxNorm ontologies**
  - Why needed here: These provide the standardized vocabulary for matching benchmark items to EHR data in validation experiments.
  - Quick check question: Why would code-based matching be necessary rather than text similarity?

## Architecture Onboarding

- Component map:
  Benchmark items -> Code extraction (RxNorm/SNOMED) -> EHR query engine -> cTAKES pipeline -> Validation scorer

- Critical path: Benchmark → Code mapping → EHR retrieval → Model inference on both → Compare rankings and conditional accuracy

- Design tradeoffs:
  - Matching on correct answer (diagnosis/drug) vs. matching on clinical presentation
  - Excluding Plan section vs. full note (avoids bias but loses context)
  - Static benchmark vs. continuously updated validation (relevance decay)

- Failure signatures:
  - α ≈ baseline accuracy → no predictive validity
  - Ranking inversions between benchmark and real data → poor construct validity
  - Large concept count disparity → content validity gap
  - High non-response rate on real notes (e.g., GPT-4) → distribution shift

- First 3 experiments:
  1. Replicate criterion validity test on your hospital's EHR with 50 matched cases per benchmark category
  2. Run content analysis comparing your patient population's top 20 diagnoses against benchmark coverage
  3. Test whether chain-of-thought reasoning correlates with expert clinician reasoning on a sample of 20 cases to probe whether correct answers reflect genuine understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can researchers and clinicians define a robust standard for "clinical similarity" to effectively match benchmark items with real-world patient cases?
- Basis in paper: [explicit] The authors state that matching patients based solely on the correct answer (e.g., SNOMED codes) may not represent the best measure of clinical similarity and suggest collaboration is needed to define a more robust standard.
- Why unresolved: Current validation methods rely on simple code matching, which fails to capture the complexity of patient presentations, yet no alternative framework for quantifying similarity has been established.
- What evidence would resolve it: A validated multi-dimensional metric that correlates benchmark performance with real-world outcomes better than simple diagnostic code matching.

### Open Question 2
- Question: Does high performance on medical LLM benchmarks reflect genuine clinical knowledge or merely spurious statistical pattern matching?
- Basis in paper: [explicit] The paper explicitly asks, "Is MedQA truly measuring clinical knowledge or spurious statistical pattern matching?" when discussing why high benchmark scores do not translate to real-world accuracy.
- Why unresolved: Models can achieve correct answers through reasoning paths that differ from clinical logic (e.g., distractor features in vignettes), and current benchmarks lack mechanisms to distinguish valid reasoning from shortcuts.
- What evidence would resolve it: Studies correlating chain-of-thought reasoning traces with expert clinician logic, demonstrating that correct answers are derived from medically sound premises rather than artifacts.

### Open Question 3
- Question: Can a decentralized ecosystem where hospitals validate benchmarks locally be implemented effectively without compromising standardization?
- Basis in paper: [inferred] The paper proposes a vision where hospitals validate benchmarks locally to maintain privacy, but it leaves the practical implementation, standardization of validation scores, and coordination across institutions as an unstated challenge.
- Why unresolved: While the vision solves data privacy issues, it introduces difficulties in ensuring that different hospital systems evaluate benchmarks using comparable criteria and computational environments.
- What evidence would resolve it: A prototype framework or pilot study demonstrating that diverse hospital systems can produce consistent validation scores for the same public benchmark using their private EHR data.

## Limitations

- The validation methodology relies on code-based matching which may not capture clinically meaningful similarity between benchmark items and real cases
- Study focuses exclusively on MedQA benchmark, limiting generalizability to other medical benchmarks
- External validity across different healthcare systems and patient populations remains untested

## Confidence

- **High confidence**: The theoretical framework connecting psychometric validity to LLM benchmarks is well-established and clearly articulated. The identification of validity gaps in current benchmarks is supported by multiple lines of evidence.
- **Medium confidence**: The empirical results showing weak predictive validity (α values of 0.29-0.56) are robust within the study's methodology, but the exact mechanisms linking code matching to clinical similarity remain uncertain.
- **Low confidence**: The generalizability of content validity findings to other benchmarks and healthcare settings has not been established, and the assumption that more UMLS concepts indicates harder tasks lacks direct validation.

## Next Checks

1. Replicate the criterion validity experiment across at least two additional hospitals with different patient demographics to test external validity
2. Conduct expert clinician review of matched cases to validate whether code-based matching captures clinically meaningful similarity
3. Compare concept count analysis with direct expert assessment of task complexity across a sample of benchmark and real-world cases