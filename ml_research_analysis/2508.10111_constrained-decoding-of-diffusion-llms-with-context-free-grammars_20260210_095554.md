---
ver: rpa2
title: Constrained Decoding of Diffusion LLMs with Context-Free Grammars
arxiv_id: '2508.10111'
source_url: https://arxiv.org/abs/2508.10111
tags:
- language
- json
- output
- constrained
- infilling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first constrained decoding method for
  diffusion language models (DLMs) that enforces context-free grammar (CFG) constraints.
  The key insight is reducing constrained infilling to deciding whether the intersection
  of a CFG and a regular language (describing all possible completions) is non-empty.
---

# Constrained Decoding of Diffusion LLMs with Context-Free Grammars

## Quick Facts
- **arXiv ID:** 2508.10111
- **Source URL:** https://arxiv.org/abs/2508.10111
- **Reference count:** 40
- **Primary result:** First constrained decoding method for diffusion LLMs enforcing CFG constraints, achieving up to 99.7% syntactic correctness and 7% functional correctness improvements.

## Executive Summary
This paper introduces the first constrained decoding method for diffusion language models (DLMs) that enforces context-free grammar (CFG) constraints. The key insight is reducing constrained infilling to deciding whether the intersection of a CFG and a regular language (describing all possible completions) is non-empty. An efficient algorithm is presented that leverages the structure of this intersection language, avoiding explicit construction and pruning non-generating symbols. Experiments on C++ code completion, JSON schema extraction, and SMILES molecule generation show significant improvements in syntactic correctness (up to 99.7%) and functional correctness (up to 7%) across multiple models while maintaining practical inference time overhead.

## Method Summary
The method reduces constrained decoding for DLMs to an additive infilling problem: checking whether a partial output with infilling regions can be completed to a valid word in the target CFG. This is solved by constructing a regular language of all possible completions (using NFAs for each fixed segment concatenated with wildcard states), then checking if the intersection with the CFG is non-empty. The intersection grammar is constructed implicitly using C2F+ε normal form (allowing A → B and A → ε) to avoid quadratic blowup, with an efficient bottom-up search marking generating nonterminals without materializing the full grammar. If the model fails after k rejections, valid completions are sampled from the intersection parse tree.

## Key Results
- Achieves 99.7% syntactic correctness on C++ HumanEval-X (vs 63.4% unconstrained)
- Improves functional correctness by up to 7% on JSON schema extraction task
- Maintains practical inference overhead (median 17% for 7B models, 125% for infilling tasks)
- Reduces residual syntax errors from 36.6% to 0.3% on C++ tasks

## Why This Works (Mechanism)

### Mechanism 1: Constrained Infilling Problem Reduction
The method reformulates constrained decoding as an "additive infilling problem" that subsumes PRE, FIM, MRI, and DLM generation paradigms. For partial output x = x1 ◇ x2 ◇ ... ◇ xn with infilling regions, it checks whether strings y1,...,yn-1 exist such that x1·y1·x2·...·yn-1·xn ∈ L. This reduction assumes additive token proposals without deletions.

### Mechanism 2: Regular Language Intersection for Feasibility Checking
The completion language Cx is shown to be regular by constructing an NFA that concatenates DFAs for fixed text segments with wildcard states accepting Σ* between them. Since CFL ∩ Regular = CFL, the intersection L∩ = L ∩ Cx is a CFL whose grammar G∩ can be constructed. The constrained infilling problem is solvable iff L∩ ≠ ∅.

### Mechanism 3: Efficient Implicit Search Over Intersection Grammar
Emptiness of the intersection language is decided without explicitly constructing the full intersection grammar, using bottom-up marking of generating nonterminals. The algorithm adapts standard emptiness checking for C2F+ε normal form, enumerating rules on-the-fly using the structure of intersection symbols (pÂq) without materializing all O(|V||Q|²) nonterminals.

## Foundational Learning

- **Context-Free Grammars (CFGs) and Context-Free Languages (CFLs)**: The entire method hinges on representing target constraints as CFGs and exploiting closure properties (CFL ∩ Regular = CFL) for intersection-based feasibility checking.
- **Regular Languages and Finite Automata (DFA/NFA)**: The completion language Cx must be shown regular and constructible as an NFA/DFA for intersection operations. Understanding NFA-to-DFA conversion and state complexity is critical.
- **Chomsky Normal Form (CNF) vs. C2F+ε Normal Form**: Standard intersection constructions require CNF, causing quadratic blowup. The paper introduces C2F+ε (allowing A → B and A → ε) to achieve only linear increase in productions.

## Architecture Onboarding

- **Component map**: Lexer/Tokenizer -> NFA Constructor -> NFA-to-DFA Converter -> CFG Normalizer -> Intersection Grammar Builder -> Implicit Emptiness Checker -> Completion Sampler -> Proposal Validator
- **Critical path**: Partial output → Lexer extracts lexeme sequences → Build NFA → Convert to minimal DFA → Intersect with normalized CFG → Implicit search for generating start symbol → Accept/reject proposal → Sample completion if needed
- **Design tradeoffs**: Overapproximation vs. precision (arbitrary infillings vs. accurate token counting), explicit vs. implicit grammar construction, rejection sampling vs. token masking
- **Failure signatures**: Timeout errors (exceeding 300s limit), incomplete completions (hitting token limit), residual syntax errors (5-40% with Con.− only), combinatorial lexing explosion
- **First 3 experiments**: 1) Single-region infilling validation on HumanEval C++ with syntactic correctness measurement, 2) Intersection grammar size profiling for 1-MRI to 3-MRI tasks, 3) Diffusion model token budget stress test on JSON schema task

## Open Questions the Paper Calls Out

### Open Question 1: Finite Token Budget Integration
How can the framework be modified to account for finite token budgets without explosion in intersection language size? The paper notes that accurately modeling remaining tokens would resolve residual syntax errors but drastically increases DFA size.

### Open Question 2: Incremental Parsing Optimization
Can incremental parsing techniques effectively reduce runtime overhead? The paper suggests reusing results from previous intersection computations since the CFG is fixed and DFA updates incrementally.

### Open Question 3: Context-Sensitive Language Extension
How can the framework be extended to support context-sensitive language features or semantic constraints? The paper identifies type checkers with typed holes as potential extensions beyond CFGs.

## Limitations
- Overapproximation and residual errors: Arbitrary-length infilling regions cause 5-40% residual syntax errors in constrained-only decoding
- Runtime scalability: Cubic complexity in worst case (O(|V||Q|²) nonterminals) with extreme outliers for small models (557% overhead)
- Lexical ambiguity handling: Complex union-NFA approach impact on validation speed for highly ambiguous partial outputs not fully characterized

## Confidence

**High Confidence Claims**:
- Reduction to intersection language emptiness checking is formally correct
- C2F+ε normal form optimization reduces grammar blowup mathematically
- Experimental results show significant syntactic correctness improvements

**Medium Confidence Claims**:
- Implicit search algorithm efficiently avoids explicit construction
- Runtime overhead remains practical for real-world use
- Fallback completion sampling improves functional correctness

**Low Confidence Claims**:
- Overapproximation error is negligible for practical applications
- Method scales to arbitrary CFGs beyond tested grammars
- Residual errors don't significantly impact practical applications

## Next Checks

**Validation Check 1: Worst-Case Complexity Profiling**
Implement both implicit and explicit intersection emptiness checking, measuring wall-clock time, memory usage, and search space size across grammars of increasing complexity and varying partial output patterns to quantify crossover points and identify worst-case behavior.

**Validation Check 2: Overapproximation Error Analysis**
Generate comprehensive test sets covering different infilling patterns, computing exact valid completions versus overapproximated sets to measure precision, recall, and error types, identifying grammars and patterns where overapproximation causes significant errors.

**Validation Check 3: Token-Level Constraint Integration**
Modify algorithm to compute intersection of CFG-valid tokens and model-predicted token probabilities at each position, comparing runtime and accuracy against pure CFG-constrained decoding and token masking approaches to evaluate whether token-level integration provides meaningful improvements.