---
ver: rpa2
title: 'MetricalARGS: A Taxonomy for Studying Metrical Poetry with LLMs'
arxiv_id: '2510.08188'
source_url: https://arxiv.org/abs/2510.08188
tags:
- poem
- tasks
- poetry
- generation
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces METRICALARGS, a taxonomy of tasks for studying
  metrical poetry with Large Language Models (LLMs) across four dimensions: Analysis,
  Retrieval, Generation, and Support. The taxonomy connects metrical poetry tasks
  to standard NLP tasks and provides guidance on datasets and evaluation metrics.'
---

# MetricalARGS: A Taxonomy for Studying Metrical Poetry with LLMs

## Quick Facts
- arXiv ID: 2510.08188
- Source URL: https://arxiv.org/abs/2510.08188
- Reference count: 22
- Key outcome: Taxonomy introducing METRICALARGS framework for studying metrical poetry with LLMs across Analysis, Retrieval, Generation, and Support dimensions

## Executive Summary
This paper introduces METRICALARGS, a comprehensive taxonomy designed to study metrical poetry tasks with Large Language Models (LLMs). The framework organizes tasks across four dimensions: Analysis (understanding poetic structures), Retrieval (finding relevant poems), Generation (creating new poems), and Support (auxiliary tasks). The authors connect these tasks to standard NLP operations and provide guidance on datasets and evaluation metrics. Using Telugu as a case study, they evaluate current LLMs' performance on these tasks. Their findings reveal significant challenges: while LLMs show moderate success in analysis tasks like summarization, they struggle considerably with retrieval and style transfer tasks, achieving 0% accuracy on both. The study also highlights critical issues with automated evaluation, showing that LLM-as-a-judge systems systematically overestimate performance by focusing on semantic content while missing structural and stylistic deficiencies in metrical poetry.

## Method Summary
The authors developed METRICALARGS as a structured taxonomy for studying metrical poetry with LLMs, organizing tasks into four dimensions: Analysis (understanding poetic structures), Retrieval (finding relevant poems), Generation (creating new poems), and Support (auxiliary tasks). They connected these tasks to standard NLP operations and provided guidance on datasets and evaluation metrics. The Telugu language was used as a case study to demonstrate the framework's application. Evaluation was conducted using GPT-5 and Gemini-2.5-Pro on 170 samples, with performance measured through accuracy and semantic similarity metrics. The study employed both automated evaluation (LLM-as-a-judge) and human evaluation to assess model outputs across the different task categories.

## Key Results
- GPT-5 achieved 0.70 accuracy for poem summarization while Gemini-2.5-Pro scored 0.85
- Both models failed completely on retrieval tasks (0% accuracy) and style transfer tasks (0% accuracy)
- LLM-as-a-judge systems consistently overestimated performance by focusing on semantic similarity while overlooking structural and stylistic deficiencies
- The study demonstrates both the potential and significant limitations of using LLMs for metrical poetry tasks

## Why This Works (Mechanism)
Assumption: The METRICALARGS framework provides a structured approach to decomposing complex metrical poetry tasks into manageable components that align with LLM capabilities. By categorizing tasks into Analysis, Retrieval, Generation, and Support dimensions, the framework may help identify where LLMs excel (semantic understanding) versus where they struggle (structural preservation and stylistic nuances). The connection to standard NLP operations provides a bridge between traditional language processing and the specialized requirements of metrical poetry.

## Foundational Learning
Unknown: The paper does not explicitly discuss foundational learning mechanisms that enable LLMs to process metrical poetry. It's unclear whether the models leverage pre-existing knowledge of poetic structures from training data or if they develop new understanding through fine-tuning on poetic corpora.

## Architecture Onboarding
Unknown: The paper does not detail specific architectural modifications or fine-tuning strategies for adapting LLMs to metrical poetry tasks. The evaluation uses off-the-shelf models (GPT-5 and Gemini-2.5-Pro) without apparent architectural onboarding for the specialized poetry domain.

## Open Questions the Paper Calls Out
- How can evaluation metrics be improved to better capture the nuanced quality of metrical poetry generation and transformation?
- What architectural modifications or training approaches might help LLMs better handle retrieval and style transfer tasks in metrical poetry?
- How can the gap between automated and human evaluation be bridged to provide more reliable assessment of LLM performance on creative language tasks?

## Limitations
- Complete failure of both GPT-5 and Gemini-2.5-Pro on retrieval and style transfer tasks (0% accuracy) suggests fundamental limitations in current LLMs or potential issues with task formulation
- Significant performance gap between automated and human evaluation raises questions about reliability of LLM-as-a-judge systems for creative and structured language tasks
- Reliance on Telugu as a single case study limits generalizability to other languages and poetic traditions
- Current evaluation metrics (accuracy, semantic similarity) may not adequately capture nuanced quality of metrical poetry generation and transformation
- The study does not explore architectural modifications or fine-tuning strategies that might improve LLM performance on metrical poetry tasks

## Confidence
- High Confidence: Current LLMs struggle significantly with retrieval and style transfer tasks in metrical poetry, evidenced by 0% accuracy rates across both models tested
- Medium Confidence: Automated evaluation systems overestimate LLM performance in creative language tasks, though this could be partially attributed to specific evaluation metrics chosen
- Low Confidence: Generalizability of findings across different languages and poetic traditions beyond Telugu, given the study's focus on a single case study language

## Next Checks
1. Conduct parallel evaluations using both automated metrics and human judgments across the same dataset to quantify systematic biases in LLM-as-a-judge systems for creative language tasks

2. Test METRICALARGS framework on at least three additional languages from different language families (e.g., English, Arabic, and Mandarin) to assess cross-linguistic applicability and identify language-specific challenges

3. Evaluate additional LLM architectures (including open-source models like LLaMA and Mistral) on the same METRICALARGS tasks to determine whether performance limitations are model-specific or represent fundamental challenges in LLM-metrical poetry interaction

4. Investigate architectural modifications or fine-tuning approaches that could improve LLM performance on retrieval and style transfer tasks in metrical poetry