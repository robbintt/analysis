---
ver: rpa2
title: 'NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment'
arxiv_id: '2509.18125'
source_url: https://arxiv.org/abs/2509.18125
tags:
- patient
- nurse
- scheduling
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NurseSchedRL is a reinforcement learning framework for nurse-patient
  assignment that uses Proximal Policy Optimization (PPO) with graph-attention encoding.
  It models nurses and patients as nodes in a bipartite graph, applying attention
  mechanisms to capture relationships like skill matching, fatigue, and geographic
  distance.
---

# NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment

## Quick Facts
- arXiv ID: 2509.18125
- Source URL: https://arxiv.org/abs/2509.18125
- Authors: Harsha Koduri
- Reference count: 12
- Primary result: RL framework using PPO with graph-attention encoding for adaptive nurse-patient assignment

## Executive Summary
NurseSchedRL is a reinforcement learning framework designed to optimize nurse-patient assignment in healthcare settings. The system models nurses and patients as nodes in a bipartite graph, leveraging attention mechanisms to capture complex relationships including skill matching, fatigue levels, and geographic proximity. Using Proximal Policy Optimization (PPO) as the learning algorithm, the framework learns adaptive policies that balance scheduling efficiency with skill alignment and fatigue management. The approach incorporates action masking to ensure only feasible assignments are considered during training.

## Method Summary
The framework represents the assignment problem as a bipartite graph where nurses and patients are nodes connected by edges representing potential assignments. Graph-attention networks encode the state by processing node features and relationships through multi-head attention mechanisms. PPO optimizes the policy by maximizing reward while constraining policy updates to prevent destructive changes. Action feasibility is enforced through masking, which eliminates invalid assignments from consideration. The system is trained entirely in simulation using synthetic healthcare data, learning to maximize a reward function that balances assignment quality metrics.

## Key Results
- Training curves show steady reward improvement over 5000 epochs, stabilizing near positive values
- Framework learns adaptive policies that improve scheduling efficiency, skill alignment, and fatigue management
- Demonstrates potential for RL to handle dynamic, constraint-heavy scheduling in healthcare environments

## Why This Works (Mechanism)
The graph-attention encoding captures complex nurse-patient relationships that traditional methods might miss, allowing the policy to learn nuanced matching criteria beyond simple skill compatibility. The attention mechanism weights different factors like fatigue and geographic distance dynamically based on context. PPO's clipped objective ensures stable learning while allowing the policy to explore different assignment strategies. Action masking enforces real-world constraints during training, ensuring the learned policy only considers feasible assignments.

## Foundational Learning
- Reinforcement Learning (PPO): Why needed - to learn adaptive policies for dynamic scheduling; Quick check - reward curve should show steady improvement
- Graph Neural Networks: Why needed - to encode relationships between nurses and patients; Quick check - attention weights should reflect intuitive relationships
- Attention Mechanisms: Why needed - to weigh different factors (skill, fatigue, distance) dynamically; Quick check - attention patterns should align with domain knowledge
- Action Masking: Why needed - to enforce real-world constraints during training; Quick check - masked actions should never appear in training data
- Proximal Policy Optimization: Why needed - to ensure stable policy updates in complex environments; Quick check - KL divergence should remain bounded

## Architecture Onboarding

Component Map: Graph-Attention Encoder -> PPO Policy Network -> Action Masking -> Reward Computation -> Environment

Critical Path: State observation -> Graph attention encoding -> Policy network output -> Action masking -> Environment step -> Reward calculation -> Policy update

Design Tradeoffs: Graph-attention vs. simpler encodings (complexity vs. expressiveness), PPO vs. other RL algorithms (stability vs. sample efficiency), synthetic vs. real data (control vs. realism)

Failure Signatures: Plateaued rewards (insufficient exploration), unstable learning (poor reward shaping), unrealistic assignments (improper masking), poor generalization (overfitting to synthetic data)

First 3 Experiments:
1. Verify attention mechanism correctly weights skill matching vs. fatigue in different scenarios
2. Test policy performance with varying nurse-patient ratios
3. Evaluate robustness to sudden changes in staffing or patient demand

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on real-world data remains unverified due to reliance on synthetic healthcare data
- Scalability to hospitals with complex, dynamic constraints and heterogeneous nurse-patient ratios is unclear
- Potential biases in attention mechanism from imbalanced skill distributions among nurses not addressed

## Confidence

High confidence in methodological soundness of PPO-based approach and graph-attention encoding for nurse-patient assignment.

Medium confidence in framework's ability to improve scheduling efficiency and skill alignment in controlled settings.

Low confidence in generalizability and robustness of learned policy to real-world hospital dynamics and data.

## Next Checks

1. Validate NurseSchedRL on real-world nurse-patient assignment datasets from multiple hospitals to assess generalizability across different healthcare systems.

2. Conduct ablation studies to evaluate the impact of individual components (e.g., attention mechanisms, masking) on policy performance and robustness.

3. Test the framework's adaptability to sudden changes in staffing levels or patient influx by simulating stress scenarios and measuring recovery time.