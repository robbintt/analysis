---
ver: rpa2
title: Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial
  Processes
arxiv_id: '2510.23817'
source_url: https://arxiv.org/abs/2510.23817
tags:
- xmeas
- fault
- industrial
- causal
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an integrated framework combining SHAP-based
  feature selection with causal analysis via Directed Acyclic Graphs (DAGs) for fault
  detection in industrial processes. The approach was evaluated on the Tennessee Eastman
  Process (TEP) dataset, reducing input dimensionality from 52 to 10 variables while
  maintaining classification accuracy of 0.872 and improving balanced accuracy from
  0.650 to 0.711 using MLP models.
---

# Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial Processes

## Quick Facts
- arXiv ID: 2510.23817
- Source URL: https://arxiv.org/abs/2510.23817
- Reference count: 8
- This study presents an integrated framework combining SHAP-based feature selection with causal analysis via Directed Acyclic Graphs (DAGs) for fault detection in industrial processes.

## Executive Summary
This study presents an integrated framework combining SHAP-based feature selection with causal analysis via Directed Acyclic Graphs (DAGs) for fault detection in industrial processes. The approach was evaluated on the Tennessee Eastman Process (TEP) dataset, reducing input dimensionality from 52 to 10 variables while maintaining classification accuracy of 0.872 and improving balanced accuracy from 0.650 to 0.711 using MLP models. The framework identified cooling-related variables and stripper parameters as critical causal nodes across five different causal discovery algorithms (PC, FCI, RFCI, LINGAM, and NOTEARS), with stripper temperature showing the strongest direct influence on fault conditions (weight 0.93). This dual approach of predictive interpretability and causal understanding enables targeted monitoring with reduced sensor requirements while providing actionable insights for process control.

## Method Summary
The framework integrates SHAP-based feature selection with causal discovery via DAGs for fault detection in industrial processes. The Tennessee Eastman Process dataset with 52 variables was preprocessed using SMOTE oversampling and random undersampling for class balance. Three machine learning models (MLP, XGBoost, KNN) were trained with stratified 5-fold cross-validation and hyperparameter optimization. SHAP values ranked variable importance, enabling dimensionality reduction to top-k subsets. Five causal discovery algorithms (PC, FCI, RFCI, LINGAM, NOTEARS) generated DAGs from the reduced feature set. The optimal configuration used 10 variables selected by SHAP, achieving 0.872 accuracy and 0.711 balanced accuracy with MLP, while identifying consistent causal structures across all five algorithms.

## Key Results
- Dimensionality reduction from 52 to 10 variables maintained classification accuracy (0.872) while improving balanced accuracy from 0.650 to 0.711
- Five causal discovery algorithms consistently identified cooling variables (XMV.10, XMV.11) and stripper parameters (XMEAS.17, XMEAS.18) as critical causal nodes
- Stripper temperature showed the strongest direct influence on faults (weight 0.93) across all causal discovery methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing input dimensionality from 52 to 10 variables maintains classification accuracy (0.872) while improving balanced accuracy (0.650→0.711) for MLP models on the TEP dataset.
- Mechanism: SHAP values quantify each variable's marginal contribution to predictions across all possible feature coalitions. By ranking variables by mean absolute SHAP importance and selecting top-k subsets, the framework eliminates noise variables that degrade minority class detection while preserving causally-relevant predictors. The reduced feature space mitigates the curse of dimensionality, allowing MLPs to learn cleaner decision boundaries.
- Core assumption: Variables with high predictive importance (SHAP) are also causally relevant to fault mechanisms, not merely correlated artifacts.
- Evidence anchors:
  - [abstract] "reducing input dimensionality from 52 to 10 variables while maintaining classification accuracy of 0.872 and improving balanced accuracy from 0.650 to 0.711"
  - [section 5.3/Table 9] MLP with 10 variables: Acc=0.872±0.006, BACC=0.711±0.027 vs. full 52-var BACC=0.650±0.019
  - [corpus] Neighbor paper on SHAP-based explanations for fault diagnosis confirms post-hoc interpretability preserves model flexibility while enabling feature attribution

### Mechanism 2
- Claim: Five causal discovery algorithms (PC, FCI, RFCI, LINGAM, NOTEARS) consistently identify cooling variables (XMV.10, XMV.11) and stripper parameters (XMEAS.17, XMEAS.18) as critical causal nodes, with stripper temperature showing the strongest direct influence on faults (weight 0.93 via LINGAM).
- Mechanism: Constraint-based algorithms (PC, FCI, RFCI) use conditional independence tests to prune edges from fully-connected graphs, identifying direct causal relationships while accounting for latent confounders (FCI/RFCI). Functional-based algorithms (LINGAM, NOTEARS) leverage non-Gaussianity or continuous optimization to estimate directed edge weights. Convergence across methodologically distinct approaches strengthens confidence in identified causal structures.
- Core assumption: The underlying process exhibits acyclic causal relationships measurable from observational data; algorithms correctly recover true causal structure rather than statistical artifacts.
- Evidence anchors:
  - [abstract] "identified cooling-related variables and stripper parameters as critical causal nodes across five different causal discovery algorithms... stripper temperature showing the strongest direct influence on fault conditions (weight 0.93)"
  - [section 5.4/Figures 6-10] DAG visualizations show XMV.10, XMV.11, XMEAS.17, XMEAS.18 as central nodes across all algorithms
  - [corpus] Causal Graph Spatial-Temporal Autoencoder paper confirms causal structure learning improves reliability and interpretability in process monitoring; Causal Digital Twins framework demonstrates causal models reduce false alarms in anomaly detection

### Mechanism 3
- Claim: Integrating SHAP-based feature selection with causal DAG analysis provides both accurate fault detection and actionable root-cause insights, enabling targeted monitoring with reduced sensor requirements.
- Mechanism: SHAP identifies which variables matter for prediction (interpretability layer), then causal discovery applied to the reduced subset reveals how those variables influence fault development (causality layer). Variables appearing significant in both analyses (e.g., XMV.10, XMV.11, XMEAS.17) represent high-confidence intervention targets. This dual-validation reduces false insights from either method alone.
- Core assumption: Convergence between SHAP importance and causal graph centrality indicates genuine fault-driving mechanisms rather than coincidental alignment.
- Evidence anchors:
  - [abstract] "This dual approach of predictive interpretability and causal understanding enables targeted monitoring with reduced sensor requirements while providing actionable insights for process control"
  - [section 1.1] "While SHAP quantifies which variables matter most for fault predictions (interpretability), DAGs reveal how and why those variables influence fault development (causality)"
  - [corpus] BatteryAgent paper similarly combines physics-informed interpretation with reasoning for root cause analysis; SHEP paper addresses interpretability barriers in fault diagnosis through post-hoc attribution

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations)
  - Why needed here: Core mechanism for feature selection; understanding how Shapley values distribute feature importance across coalitions is essential for interpreting Figure 4 and Table 9 results.
  - Quick check question: Given a model prediction of 0.8 and baseline of 0.5, if feature A has SHAP value +0.2, what does this mean for that specific prediction?

- Concept: Directed Acyclic Graphs (DAGs) for causal discovery
  - Why needed here: Framework outputs DAGs from five algorithms; understanding d-separation, edge direction semantics, and algorithm assumptions (constraint-based vs. functional) is required to interpret Figures 6-10 and assess result validity.
  - Quick check question: In a DAG with A→B→C, what conditional independence relationship must hold if the graph is faithful to the data?

- Concept: Class imbalance and balanced accuracy
  - Why needed here: TEP dataset has fault conditions vastly outnumbered by normal operations; the paper reports BACC improvements (0.650→0.711) as key success metric. Understanding why accuracy alone is misleading is critical.
  - Quick check question: A classifier achieves 95% accuracy on a dataset with 5% fault rate by always predicting "normal." What is its balanced accuracy?

## Architecture Onboarding

- Component map:
  - Data layer: TEP dataset (52 variables, 20 fault types), preprocessing with SMOTE oversampling + random undersampling for class balance
  - ML models: MLP (best performer), XGBoost, KNN with hyperparameter optimization via RandomizedSearchCV
  - Feature selection: SHAP analysis ranking variables by mean absolute importance, creating subsets of 7/10/12/15 variables
  - Causal discovery: Five algorithms (PC, FCI, RFCI, LINGAM, NOTEARS) applied to 10-variable subset
  - Evaluation: Stratified 5-fold cross-validation, metrics include Accuracy, BACC, Precision, Recall, F1, AUC-ROC

- Critical path:
  1. Train baseline models on full 52-variable dataset → establish performance reference
  2. Compute SHAP values on trained models → rank variable importance
  3. Select top-k variables (10 optimal per results) → reduce dimensionality
  4. Retrain models on reduced subset → evaluate performance retention
  5. Apply all five causal discovery algorithms to reduced subset → generate DAGs
  6. Identify consistent nodes/edges across algorithms → extract robust causal insights

- Design tradeoffs:
  - Dimensionality vs. information loss: 10 variables capture essential dynamics but may miss rare fault patterns requiring other sensors
  - Algorithm diversity vs. computational cost: Five causal algorithms provide convergence evidence but multiply computation time
  - Interpretability vs. accuracy: MLP selected over XGBoost (which had higher BACC on some subsets) due to overall performance balance
  - Offline analysis vs. real-time monitoring: Causal discovery computational complexity (explicitly noted as limitation) restricts framework to batch processing; real-time deployment requires pre-computed DAGs or approximate methods

- Failure signatures:
  - SHAP-selected features improve accuracy but causal graphs show no consistent patterns → features are predictive but non-causal (spurious correlation)
  - Different causal algorithms produce contradictory DAG structures → underlying assumptions violated (e.g., cycles, latent confounders, Gaussian noise for LINGAM)
  - Dimensionality reduction causes significant accuracy drop on specific fault types → those faults require excluded variables
  - BACC degrades while accuracy improves → model overfitting to majority class, imbalance handling insufficient

- First 3 experiments:
  1. Replicate baseline comparison: Train MLP, XGBoost, KNN on full 52-variable TEP data with 5-fold CV. Verify your results match Table 8 (MLP Acc=0.872, BACC=0.650). Deviation indicates implementation or data split issues.
  2. SHAP sensitivity analysis: Compute SHAP values for each model, create 7/10/12/15 variable subsets. Test if 10-variable subset consistently outperforms others across all three models (not just MLP). This validates whether 10 is universally optimal or model-specific.
  3. Single-algorithm causal validation: Apply only NOTEARS (continuous optimization, fastest) to the 10-variable subset. Compare resulting DAG to Figure 9. Edge weights within ±0.1 of reported values confirm correct implementation before running full 5-algorithm suite.

## Open Questions the Paper Calls Out

- Can scalable causal discovery algorithms or parallel computing approaches enable real-time fault detection while maintaining the interpretability benefits of the SHAP-DAG framework?
  - Basis in paper: [explicit] "the computational complexity of causal discovery limits real-time applicability, suggesting future work on scalable algorithms or parallel computing."
  - Why unresolved: Current causal discovery algorithms (PC, FCI, RFCI, LINGAM, NOTEARS) have computational costs that grow rapidly with dimensionality, making them impractical for real-time industrial monitoring where rapid response to faults is critical.
  - What evidence would resolve it: Demonstration of modified or new causal discovery algorithms achieving sub-second latency on industrial-scale datasets while producing DAGs comparable in quality to current batch-processing approaches.

- Does temporal causal analysis improve fault detection accuracy and root cause identification compared to the static DAG approach presented in this framework?
  - Basis in paper: [explicit] "Temporal causal analysis could further capture dynamic fault propagation, enhancing predictive capabilities."
  - Why unresolved: The current framework treats causal relationships as static, but industrial processes exhibit time-dependent dynamics where fault propagation pathways may change over time.
  - What evidence would resolve it: Comparative study showing temporal DAG methods achieving higher balanced accuracy or earlier fault detection than static methods on TEP or similar benchmarks.

- Does the SHAP-DAG framework maintain its predictive accuracy and interpretability when applied to real-world industrial processes beyond the simulated TEP benchmark?
  - Basis in paper: [inferred] from methodological limitations—the paper validates only on TEP simulation; industrial deployment would require testing on real manufacturing data with actual sensor noise, missing values, and operational complexities not present in controlled simulations.
  - Why unresolved: TEP is a well-controlled simulation with known fault types; real industrial processes may have unknown fault modes, different variable relationships, and practical constraints not captured in simulations.
  - What evidence would resolve it: Successful application of the framework to actual industrial datasets showing comparable dimensionality reduction while maintaining or improving balanced accuracy above baseline models.

## Limitations
- Computational complexity of causal discovery restricts framework to offline batch processing rather than real-time industrial monitoring
- SHAP-selected variables may be predictive but non-causal, potentially providing misleading intervention targets
- TEP simulation structure may not generalize to real industrial processes with cyclic control loops or unmeasured confounders

## Confidence
- **High confidence**: Dimensionality reduction maintains classification accuracy (0.872) while improving balanced accuracy (0.650→0.711) for MLP models on the TEP dataset, supported by specific numerical results in Table 9.
- **Medium confidence**: Five causal discovery algorithms consistently identify cooling variables and stripper parameters as critical nodes, though algorithmic assumptions (acyclicity, no unmeasured confounders) may not hold in all real industrial processes.
- **Low confidence**: The integration of SHAP importance with causal centrality reliably indicates genuine fault-driving mechanisms rather than coincidental alignment, as this convergence requires further validation across diverse process datasets.

## Next Checks
1. **Cross-process validation**: Apply the complete framework (SHAP + 5 DAG algorithms) to a second industrial benchmark dataset (e.g., SWaT water treatment system) to verify generalizability of the dual interpretability-causality approach beyond TEP.
2. **Cycle detection assessment**: Test the framework on a process dataset known to contain feedback loops to evaluate DAG limitations and identify failure modes when acyclicity assumptions are violated.
3. **Real-time approximation study**: Implement a simplified causal discovery method (e.g., single-pass constraint-based with reduced conditioning) and benchmark against the full 5-algorithm suite to quantify accuracy-speed tradeoffs for potential deployment scenarios.