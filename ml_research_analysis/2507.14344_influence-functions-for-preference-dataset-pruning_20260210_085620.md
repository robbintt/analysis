---
ver: rpa2
title: Influence Functions for Preference Dataset Pruning
arxiv_id: '2507.14344'
source_url: https://arxiv.org/abs/2507.14344
tags:
- influence
- training
- examples
- data
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies influence functions to detect and remove harmful
  examples from a human preference dataset used to train reward models. The method
  uses conjugate-gradient approximated inverse Hessians in the LoRA parameter space
  to estimate the influence of each training example on validation performance.
---

# Influence Functions for Preference Dataset Pruning

## Quick Facts
- **arXiv ID:** 2507.14344
- **Source URL:** https://arxiv.org/abs/2507.14344
- **Reference count:** 6
- **Primary result:** 1.5% improvement in pairwise accuracy after removing 10% of most harmful examples

## Executive Summary
This paper introduces influence functions as a method for detecting and removing harmful examples from human preference datasets used to train reward models. The approach leverages conjugate-gradient approximated inverse Hessians in LoRA parameter space to estimate each training example's influence on validation performance. By filtering out the most harmful 10% of examples, the method achieves a 1.5% improvement in pairwise accuracy compared to training on the full dataset. The study also reveals that gradient similarity (a first-order approximation) outperforms influence functions at identifying helpful examples but is less effective at detecting harmful ones, suggesting that local curvature is crucial for identifying harmful examples.

## Method Summary
The authors apply influence functions to detect harmful examples in preference datasets by computing how much each training example affects validation loss. They use conjugate-gradient methods to approximate the inverse Hessian in LoRA parameter space, making the computation more tractable. The approach involves calculating influence scores for each example, identifying the most harmful subset, and removing them from training. The method is compared against gradient similarity, which serves as a first-order approximation. Both methods are evaluated on their ability to identify helpful and harmful examples, with the filtered datasets used to train reward models and measure pairwise accuracy improvements.

## Key Results
- Removing the most harmful 10% of examples yields a 1.5% improvement in pairwise accuracy
- Influence functions are more effective than gradient similarity at detecting harmful examples
- Gradient similarity outperforms influence functions at identifying helpful examples
- Local curvature (second-order information) is important for detecting harmful examples

## Why This Works (Mechanism)
Influence functions work by estimating how the model parameters would change if a particular training example were removed. The key insight is that harmful examples create negative influence on validation performance, which can be quantified by computing the change in validation loss when removing that example. The method leverages second-order information through the Hessian approximation to capture local curvature, which is essential for distinguishing harmful examples that may appear similar to helpful ones based on first-order gradients alone.

## Foundational Learning

**Influence Functions:** Estimate model parameter changes when removing training examples; needed for identifying harmful examples; quick check: verify gradient changes match theoretical predictions.

**Conjugate-Gradient Approximation:** Iterative method for approximating inverse Hessians; needed to make influence function computation tractable; quick check: monitor convergence of gradient updates.

**LoRA Parameter Space:** Low-rank adaptation parameter space; needed to reduce computational complexity; quick check: verify parameter updates stay within low-rank constraints.

**Hessian Matrix:** Second-order derivative matrix capturing local curvature; needed for accurate influence estimation; quick check: ensure positive definiteness for stable optimization.

**Reward Model Training:** Supervised learning on preference data; needed to evaluate the impact of dataset pruning; quick check: monitor pairwise accuracy on validation set.

## Architecture Onboarding

**Component Map:** Data Loader -> Influence Function Calculator -> Filter Module -> Reward Model Trainer -> Evaluation Module

**Critical Path:** Data preparation → Influence score computation → Example filtering → Model training → Performance evaluation

**Design Tradeoffs:** Computational cost of influence functions vs. accuracy improvement; percentage of examples to filter; choice between first-order vs. second-order methods

**Failure Signatures:** No improvement in pairwise accuracy; computational resources exhausted; influence scores not distinguishing between helpful/harmful examples

**First Experiments:** 1) Run influence function computation on a small subset to verify implementation; 2) Compare influence scores with gradient similarity on a held-out validation set; 3) Train reward model with different pruning percentages to find optimal threshold

## Open Questions the Paper Calls Out
None

## Limitations
- Computational cost of influence function estimation remains prohibitive for large-scale applications
- 1.5% improvement may not justify computational overhead in all scenarios
- First-order approximations may be sufficient for identifying helpful examples

## Confidence

| Claim | Confidence |
|-------|------------|
| Influence functions can identify harmful examples better than gradient similarity | High |
| Local curvature matters for detecting harmful examples | Medium |
| Computational cost is a limitation | High |

## Next Checks

1. Test pruning approach across multiple reward model architectures and scales to verify generalization
2. Conduct ablation studies to determine optimal percentage of examples to prune
3. Evaluate whether combining influence function scores with gradient similarity scores provides better performance than either method alone