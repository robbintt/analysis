---
ver: rpa2
title: 'Dynamic-KGQA: A Scalable Framework for Generating Adaptive Question Answering
  Datasets'
arxiv_id: '2503.05049'
source_url: https://arxiv.org/abs/2503.05049
tags:
- arxiv
- question
- answer
- knowledge
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dynamic-KGQA is a scalable framework for generating adaptive QA
  datasets from knowledge graphs to mitigate data contamination risks in LLM evaluation.
  It extracts thematic subgraphs from seed texts, uses LLMs to generate QA pairs with
  controlled variability, and employs LLM-as-a-Judge for quality verification.
---

# Dynamic-KGQA: A Scalable Framework for Generating Adaptive Question Answering Datasets

## Quick Facts
- arXiv ID: 2503.05049
- Source URL: https://arxiv.org/abs/2503.05049
- Reference count: 40
- Generates adaptive QA datasets from knowledge graphs to mitigate LLM evaluation data contamination risks

## Executive Summary
Dynamic-KGQA is a scalable framework designed to generate adaptive QA datasets from knowledge graphs, addressing critical data contamination issues in LLM evaluation. The framework extracts thematic subgraphs from seed texts, uses LLMs to generate QA pairs with controlled variability, and employs LLM-as-a-Judge for quality verification. By producing compact subgraphs for each QA pair and supporting dynamic sampling, Dynamic-KGQA enables rigorous, contamination-resistant KGQA benchmarking while maintaining topic consistency across runs.

## Method Summary
Dynamic-KGQA operates through a multi-stage pipeline: (1) seed text extraction and thematic subgraph selection from knowledge graphs, (2) LLM-based QA pair generation with controlled variability parameters, (3) LLM-as-a-Judge quality verification, and (4) dynamic sampling for continuous dataset evolution. The framework generates compact subgraphs tailored to each QA pair, ensuring relevance and reducing computational overhead. It supports both dynamic generation for ongoing evaluation and static train/test/validation splits for traditional benchmarking comparisons.

## Key Results
- Generated 200K QA pairs from YAGO 4.5 with balanced topic distribution
- Statistical tests confirm topic consistency across dynamic runs (χ² p > 0.05, Cramer's V < 0.03)
- LLM baselines show lower scores on Dynamic-KGQA datasets compared to older datasets, suggesting reduced memorization effects

## Why This Works (Mechanism)
Dynamic-KGQA mitigates data contamination by generating fresh QA pairs from knowledge graphs rather than relying on static datasets that may overlap with LLM training data. The framework's dynamic generation capability ensures continuous evolution of evaluation data, while compact subgraph extraction focuses on relevant entities and relationships for each question. The LLM-as-a-Judge mechanism provides scalable quality control without requiring extensive human annotation.

## Foundational Learning

**Knowledge Graph Extraction**: Extracting thematic subgraphs from seed texts - needed for focused dataset generation, quick check: verify subgraph relevance to seed topics

**LLM-as-a-Judge**: Using LLMs to verify QA pair quality - needed for scalable quality control, quick check: validate judge accuracy against human annotations

**Dynamic Sampling**: Continuous dataset evolution with controlled variability - needed for contamination resistance, quick check: measure topic drift across sampling iterations

## Architecture Onboarding

**Component Map**: Seed Text -> Subgraph Extraction -> QA Generation -> Quality Verification -> Dynamic Sampling

**Critical Path**: Seed Text → Subgraph Extraction → QA Generation → Quality Verification

**Design Tradeoffs**: 
- Compact subgraphs reduce computational overhead but may miss contextual information
- LLM-as-a-Judge provides scalability but introduces potential bias
- Dynamic generation ensures freshness but requires continuous computational resources

**Failure Signatures**: 
- Topic drift indicates subgraph extraction issues
- Quality score degradation suggests LLM generation problems
- Memory constraints during subgraph extraction

**First 3 Experiments**:
1. Generate 1K QA pairs from a single seed text to validate subgraph extraction quality
2. Compare LLM-as-a-Judge scores against human annotations for 100 QA pairs
3. Measure topic consistency across three independent dynamic runs with different random seeds

## Open Questions the Paper Calls Out

None

## Limitations

- Dependence on LLM-generated QA pairs introduces potential variability in answer quality
- Single KG evaluation (YAGO 4.5) limits generalizability across domains
- Extent of contamination reduction unverified through direct training data overlap analysis

## Confidence

- Contamination mitigation benefits: Medium
- Topic consistency across dynamic runs: High
- Framework scalability claims: Medium

## Next Checks

1. Conduct cross-domain evaluation on multiple KGs (e.g., Wikidata, DBpedia) to assess framework generalizability and performance consistency
2. Implement direct training data overlap analysis to verify the extent of contamination reduction beyond comparative LLM scoring
3. Perform human evaluation of generated QA pairs to validate LLM-as-a-Judge accuracy and identify systematic generation biases