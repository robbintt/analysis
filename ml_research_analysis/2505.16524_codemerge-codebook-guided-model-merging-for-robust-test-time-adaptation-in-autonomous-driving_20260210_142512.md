---
ver: rpa2
title: 'CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in
  Autonomous Driving'
arxiv_id: '2505.16524'
source_url: https://arxiv.org/abs/2505.16524
tags:
- adaptation
- detection
- conference
- proc
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses robust 3D perception in autonomous driving
  under dynamic test-time conditions, where existing test-time adaptation (TTA) methods
  struggle due to unstable optimization and computational inefficiency. The proposed
  CodeMerge introduces a lightweight, scalable model merging framework that bypasses
  these limitations by operating in a compact latent space.
---

# CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving

## Quick Facts
- **arXiv ID:** 2505.16524
- **Source URL:** https://arxiv.org/abs/2505.16524
- **Reference count:** 40
- **Primary result:** CodeMerge achieves 14.9% NDS improvement on nuScenes-C and 7.6% mAP gain on nuScenes-to-KITTI, while being 3x more memory-efficient than existing TTA methods.

## Executive Summary
This paper addresses robust 3D perception in autonomous driving under dynamic test-time conditions, where existing test-time adaptation (TTA) methods struggle due to unstable optimization and computational inefficiency. The proposed CodeMerge introduces a lightweight, scalable model merging framework that bypasses these limitations by operating in a compact latent space. Instead of loading full models, CodeMerge represents each checkpoint with a low-dimensional fingerprint derived from the source model's penultimate features and constructs a key-value codebook. Merging coefficients are computed using ridge leverage scores on these fingerprints, enabling efficient model composition without compromising adaptation quality. CodeMerge improves end-to-end 3D detection NDS by 14.9% on nuScenes-C and LiDAR-based detection by over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as online mapping, motion prediction, and planning even without additional training. Code and pretrained models are publicly released.

## Method Summary
CodeMerge is a test-time adaptation framework that maintains a codebook of checkpoints and their corresponding fingerprints. During adaptation, it extracts intermediate features from the frozen source model, projects them to low-dimensional fingerprints, and computes ridge leverage scores to identify diverse checkpoints. The method selects top-K checkpoints and merges them using sign-consistent weighted averaging based on majority sign consensus. This merged model is used for both inference and generating pseudo-labels for the next adaptation step. The framework is compatible with both end-to-end systems like SparseDrive and modular detectors like SECOND, operating efficiently without loading full model weights.

## Key Results
- 14.9% NDS improvement on nuScenes-C corruption benchmark for end-to-end detection
- 7.6% mAP gain on nuScenes-to-KITTI cross-dataset transfer
- 3x memory efficiency compared to model optimization strategies (MOS)
- Benefits downstream tasks (mapping, motion prediction, planning) without additional training

## Why This Works (Mechanism)

### Mechanism 1: Latent Fingerprinting as a Proxy for Parameter Geometry
Low-dimensional feature projections from a frozen source model can reliably approximate high-dimensional parameter relationships between checkpoints. Instead of loading full model weights to measure similarity (expensive), CodeMerge extracts intermediate features from the current input using the original, frozen source model φ_Θ₀. These features are randomly projected into a compact "fingerprint" vector ẑ. The method assumes that if two fingerprints are correlated, their corresponding weight deltas are also geometrically correlated. The geometry of the penultimate activation space preserves the structural relationships of the weight space (Linear Mode Connectivity holds locally). Break condition: If the source model's feature extractor φ_Θ₀ collapses or fails to extract meaningful signals from the target domain (e.g., severe sensor failure yielding zero variance), the fingerprints become uninformative.

### Mechanism 2: Curvature-Aware Selection via Ridge Leverage Scores (RLS)
Ridge leverage scores act as a computationally efficient proxy for identifying diverse, "high-curvature" checkpoints that better approximate the loss landscape inverse Hessian. Rather than merging all recent models (EMA) or relying on expensive kernel similarity (MOS), CodeMerge scores checkpoints using RLS on the fingerprint matrix. Theoretically, RLS relates to the inverse of the curvature (Hessian) in the parameter space. A high score implies the checkpoint explores a novel direction in the loss landscape, reducing redundancy. The loss landscape of the 3D detection head can be locally approximated by a quadratic function where directions of high inverse curvature (captured by RLS) are optimal for merging. Break condition: If the buffer contains only highly correlated checkpoints (e.g., stationary input distribution), RLS may fail to identify diverse candidates, though the paper suggests this is robust via ablations.

### Mechanism 3: Sign Consistency for Destructive Interference Reduction
Aligning parameter signs to a majority consensus before averaging mitigates "cancellation" effects during weight merging. When merging Top-K checkpoints, parameters with conflicting signs (positive vs. negative) relative to the majority are masked (zeroed out) rather than averaged. This ensures that the merged model retains the functional logic of the consensus rather than canceling it out. Parameter signs in the detection head correspond to distinct functional decisions; conflicting signs indicate distinct modes that should not be linearly interpolated without alignment. Break condition: If the majority sign is systematically wrong (e.g., corrupted by bad pseudo-labels over a long horizon), the merge will reinforce errors.

## Foundational Learning

- **Concept: Linear Mode Connectivity (LMC)**
  - **Why needed here:** The entire merging strategy relies on the premise that fine-tuned models lie on a connected linear path in weight space where loss remains low.
  - **Quick check question:** Can you explain why averaging two models fine-tuned from different initializations might fail, whereas averaging models from the same initialization (LMC) succeeds?

- **Concept: Ridge Leverage Score**
  - **Why needed here:** This is the selection heuristic. It measures how unique a data point (fingerprint) is relative to the covariance of the dataset (previous fingerprints).
  - **Quick check question:** If a new fingerprint is perfectly orthogonal to all previous fingerprints, will its Ridge Leverage Score be high or low?

- **Concept: Pseudo-Labeling in TTA**
  - **Why needed here:** CodeMerge uses the merged model to generate "soft" targets for the next adaptation step.
  - **Quick check question:** How does CodeMerge prevent the "confirmation bias" where errors in the pseudo-labels degrade the model over time? (Hint: Look at the stability of merging vs. single model adaptation).

## Architecture Onboarding

- **Component map:** Input batch -> Frozen Encoder (ResNet50) -> Random Projector -> Codebook (circular buffer) -> RLS Scorer -> Merger (sign-consistent) -> Merged model for inference and pseudo-label generation

- **Critical path:** 1) Input batch arrives 2) Extract features via frozen source backbone 3) Project to fingerprint ẑ 4) Update Codebook (store current checkpoint and fingerprint) 5) Compute RLS for all keys in Codebook 6) Select Top-K; perform sign-consistent merge 7) Use merged model for inference AND pseudo-label generation for the next gradient step

- **Design tradeoffs:**
  - **Dimension d':** Paper uses 1024. Lower d' saves memory (Table 5 shows d'=256 is viable) but risks losing correlation with weight space
  - **Buffer Size K:** K=5 is optimal. K=3 lacks diversity; K=9 adds redundancy without performance gain
  - **Source vs. Current Features:** CodeMerge uses source features for keys to ensure a consistent reference frame, unlike MOS which uses current model features (expensive)

- **Failure signatures:**
  - **Correlation Collapse:** If Figure 3 correlation (r > 0.7) breaks (e.g., severe domain shift altering feature semantics), RLS scores become noise
  - **Sensor Crash:** Performance drops significantly (Table 1, Crash/Lost), though CodeMerge recovers better than baselines
  - **Latency Spikes:** If the matrix inversion in RLS scales poorly (though paper claims efficiency), real-time planning fails

- **First 3 experiments:**
  1. **Sanity Check Fingerprint Correlation:** Replicate Figure 3. Plot pairwise fingerprint differences vs. parameter differences on a held-out validation set. If r < 0.5, the projection dimension d' is too low or the backbone is inappropriate
  2. **Ablate Selection Strategy:** Compare "Leverage" vs. "Recent" (FIFO) vs. "Random" on a single corruption type (e.g., Motion Blur) to verify the specific contribution of RLS
  3. **Cross-Dataset Transfer:** Run the nuScenes → KITTI protocol (Table 3) to ensure the "Source Model" feature extractor generalizes enough to generate meaningful fingerprints for the new domain

## Open Questions the Paper Calls Out

- **Can CodeMerge effectively adapt end-to-end autonomous driving architectures other than SparseDrive, such as UniAD or VAD, which currently suffer from severe performance degradation on corruption benchmarks?**
  - The paper states experiments were primarily conducted on SparseDrive because "popular architectures, such as UniAD and VAD, experience over tenfold performance degradation on nuScenes-C, hindering effective adaptation training." The extreme baseline fragility of other popular architectures prevented their inclusion in the study, leaving their compatibility with the fingerprint-merging framework unknown.

- **Does the theoretical link between ridge leverage scores and the inverse Hessian curvature hold for detection heads with complex non-linear structures, such as deep MLPs or Transformers?**
  - The theoretical analysis relies on the assumption that "3D object detection models commonly use linear layers as final regression heads" to equate the fingerprint covariance with the parameter-space Hessian. The derivation simplifies the loss landscape to a ridge regression problem; it is unclear if the "curvature-aware" scoring remains valid or optimal when the regression head is highly non-linear.

- **How does the unbounded growth of the model codebook affect memory and latency in long-horizon, infinite streaming scenarios?**
  - Section 3.1 defines the codebook update as an append-only operation for "all past checkpoints," lacking a formal mechanism for pruning or forgetting older entries. While the method is memory-efficient compared to baselines, the "scalability" claims are evaluated on finite benchmarks; continuous appending could eventually saturate memory or degrade selection efficiency over months of driving.

## Limitations
- Reliance on source model feature extractor as stable reference frame introduces potential failure point under severe domain shifts
- RLS selection mechanism lacks theoretical guarantees about optimality in non-quadratic loss landscapes
- Correlation between fingerprints and parameter space could degrade under extreme conditions not tested in evaluation

## Confidence

- **High:** End-to-end detection performance improvements (14.9% NDS gain), computational efficiency claims, cross-task robustness without additional training
- **Medium:** RLS as an effective selection heuristic, sign-consistency merging mechanism, generalizability to cross-dataset scenarios
- **Low:** Theoretical justification for fingerprint-parameter correlation, robustness under sensor failure beyond tested scenarios, long-term stability of the adaptation loop

## Next Checks

1. **Stress Test Feature Correlation:** Systematically evaluate fingerprint-parameter correlation under controlled domain shifts (gradual weather degradation, sensor noise injection) to identify correlation collapse thresholds

2. **Ablate Selection Mechanism:** Compare RLS-based selection against random selection and recent-only selection under varying buffer sizes and corruption types to isolate RLS contribution

3. **Cross-Domain Generalization:** Validate on additional cross-dataset pairs (Waymo→nuScenes, nuScenes→Waymo) to test source model feature extractor generalization beyond the reported KITTI→nuScenes case