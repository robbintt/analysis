---
ver: rpa2
title: 'HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for
  Large-Scale Integer Linear Programs'
arxiv_id: '2509.15828'
source_url: https://arxiv.org/abs/2509.15828
tags:
- solving
- time
- neighborhood
- hyp-aso
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of solving large-scale Integer
  Linear Programs (ILPs) efficiently, which is critical for many combinatorial optimization
  problems. The proposed HyP-ASO framework combines a customized formula for variable
  selection probability calculation with a reinforcement learning policy for adaptive
  neighborhood size prediction.
---

# HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs

## Quick Facts
- arXiv ID: 2509.15828
- Source URL: https://arxiv.org/abs/2509.15828
- Reference count: 40
- This paper addresses the challenge of solving large-scale Integer Linear Programs (ILPs) efficiently, which is critical for many combinatorial optimization problems. The proposed HyP-ASO framework combines a customized formula for variable selection probability calculation with a reinforcement learning policy for adaptive neighborhood size prediction. This hybrid approach enables more effective neighborhood generation in Large Neighborhood Search. Extensive experiments on four benchmark ILP problems show that HyP-ASO significantly outperforms existing LNS-based methods in terms of both objective value and solving time. For instance, on medium-scale problems, it reduces solving time by over 90% compared to Gurobi. The method demonstrates strong scalability and generalization capabilities across different problem sizes and types.

## Executive Summary
This paper introduces HyP-ASO, a hybrid framework for solving large-scale Integer Linear Programs (ILPs) efficiently. The approach combines a customized formula for variable selection probability calculation with a reinforcement learning policy for adaptive neighborhood size prediction, enabling more effective Large Neighborhood Search. By leveraging historical high-quality solutions and dynamically adjusting search parameters, HyP-ASO significantly outperforms existing LNS-based methods in both solution quality and computational time, particularly for large-scale problems with millions of variables.

## Method Summary
HyP-ASO integrates two key mechanisms: (1) a solution pool-based variable selection formula that prioritizes variables based on their consistency across historical high-quality solutions, and (2) a reinforcement learning policy that predicts adaptive neighborhood sizes. The framework uses a Graph Neural Network with global pooling to encode the ILP structure, enabling generalization across problem scales. During search, the customized formula calculates selection probabilities while the RL policy determines how many variables to include in each neighborhood, creating a hybrid approach that balances exploration and exploitation.

## Key Results
- Reduces solving time by over 90% compared to Gurobi on medium-scale problems
- Outperforms existing LNS-based methods in both objective value and solving time across four benchmark ILP problems
- Demonstrates strong scalability, successfully generalizing from small training sets (10k vars) to hard test sets (1M vars)
- Ablation studies confirm the effectiveness of both the customized formula and RL-based adaptive sizing

## Why This Works (Mechanism)

### Mechanism 1: Solution Pool-Based Variable Selection
HyP-ASO improves neighborhood quality by prioritizing variables based on their consistency across historical high-quality solutions rather than random selection. The framework maintains a "solution pool" of feasible solutions found during the search. It calculates a "confidence score" for each variable based on its value in high-ranking solutions (Eq. 2). Variables in the current solution that deviate significantly from their historical confidence scores are assigned higher selection probabilities (Eq. 3). This targets the search toward "uncertain" or unstable parts of the solution space.

Core assumption: High-quality solutions share structural regularities, and variables that fluctuate heavily or deviate from high-quality historical norms are the bottlenecks preventing optimization.

Evidence anchors:
- [Section 4.1]: "The term $\sum xt,i,j/r_j$ aggregates the weighted contributions... higher-quality solutions contributing more."
- [Table 4]: Shows performance degradation when replacing the customized formula with uniform selection (HyP-ASO-UF).
- [corpus]: Related work "SPL-LNS" validates that sampling-enhanced strategies are effective for ILPs, though HyP-ASO specifically uses a ranking-based formula rather than generic sampling.

Break condition: If the initial solver fails to find a diverse set of feasible solutions quickly, the solution pool will lack the informational richness required to calculate meaningful confidence scores, causing the selection probability to degrade to random guessing.

### Mechanism 2: Adaptive Neighborhood Sizing via RL
A fixed neighborhood size limits LNS efficiency; predicting the size dynamically via Reinforcement Learning (RL) balances the trade-off between search space exploration and sub-problem complexity. The framework models the optimization process as a Markov Decision Process (MDP). A Graph Neural Network (GNN) encodes the current ILP state (variable/constraint features and current solution). An RL policy (PPO) outputs a continuous action $a_t \in (0,1)$ representing the ratio of variables to include in the neighborhood.

Core assumption: The "optimal" amount of problem to destroy changes during the solving process (e.g., larger neighborhoods early, smaller for refinement later) and can be inferred from the structural state of the ILP.

Evidence anchors:
- [Section 4.2]: "The neighborhood size plays a critical role: if it is too large, the solving time... if it is too small, the neighborhood search space may miss..."
- [Table 3]: Ablation study shows RL-based sizing (HyP-ASO) outperforms fixed ratios (30%, 50%, 70%) and statistical distributions (Gaussian, Uniform).
- [corpus]: "New Adaptive Mechanism for LNS using Dual Actor-Critic" supports the general efficacy of adaptive mechanisms in LNS, suggesting this is a robust design pattern.

Break condition: If the reward signal (improvement in objective value) is sparse or noisy, the RL policy may converge to a conservative fixed ratio, negating the benefits of adaptation.

### Mechanism 3: Lightweight Graph Pooling for Generalization
The architecture enables generalization to problems with millions of variables by using global pooling over graph embeddings rather than variable-level predictions. Instead of predicting actions for individual variables (which scales poorly), the GNN uses a global pooling layer to aggregate graph-level features. This fixed-size embedding feeds an MLP to predict the *scalar* neighborhood size ratio. This decouples model size from problem size.

Core assumption: The global "state" of the optimization problem contains sufficient information to determine the neighborhood ratio without needing to process every variable individually in the policy head.

Evidence anchors:
- [Appendix B.5]: "The global pooling layer is designed to facilitate generalization across ILPs of varying sizes."
- [Table 2]: Demonstrates successful generalization from small training sets (10k vars) to hard test sets (1M vars).
- [corpus]: "Feature Augmentation of GNNs for ILPs" suggests standard GNNs have expressiveness limits, implying HyP-ASO's success relies heavily on the specific pooling strategy and feature engineering.

Break condition: If critical optimization cues are local (specific to small clusters of constraints/variables), global pooling may obscure them, leading to poor sizing decisions on highly irregular problem structures.

## Foundational Learning

- **Concept: Large Neighborhood Search (LNS)**
  - **Why needed here:** This is the base heuristic engine of HyP-ASO. Understanding LNS is required to grasp *why* the paper focuses on "neighborhood generation" and "variable selection."
  - **Quick check question:** Can you explain why fixing a subset of variables (destroying part of the solution) helps accelerate solving compared to re-optimizing the whole problem from scratch?

- **Concept: Markov Decision Process (MDP) in Optimization**
  - **Why needed here:** The framework uses Reinforcement Learning, which requires formulating the ILP solving process as a state-transition game. You must understand State, Action, and Reward definitions in Section 4.2.
  - **Quick check question:** In HyP-ASO, the "Action" is not the final solution, but a specific parameter controlling the search. What is that parameter?

- **Concept: Bipartite Graph Representation of ILPs**
  - **Why needed here:** This is the input format for the Neural Network. The "Variable" and "Constraint" nodes form the structure upon which the GNN operates.
  - **Quick check question:** In the bipartite graph $G=(V, C, E)$, what do the edges $E$ represent regarding the linear constraints?

## Architecture Onboarding

- **Component map:** ILP Instance -> Bipartite Graph (Variables $V$, Constraints $C$, Edges $E$) -> Initializer (Standard Solver Gurobi) -> Solution Pool -> Probability Engine (Formula) -> Policy Engine (RL: GNN Encoder + Global Pooling -> MLP) -> Sampler -> Sub-ILP -> Solver -> Solution Pool (Loop)

- **Critical path:** The interaction between the **Probability Engine** and the **Policy Engine**. The formula decides *which* variables are promising, while the RL policy decides *how many* to select. If the formula identifies the wrong variables, the RL policy cannot compensate by adjusting the size.

- **Design tradeoffs:**
  - **Customized Formula vs. Learnable Selection:** The paper uses a hard-coded ranking formula for variable selection (Mechanism 1) rather than learning it. This reduces training complexity and data requirements but may lack flexibility on unseen problem structures.
  - **Global Pooling vs. Node-level Prediction:** Predicting a global ratio is highly scalable (1M+ vars) but loses granular control over specific variable selection compared to methods that output a score per variable.

- **Failure signatures:**
  - **Stagnation:** If the objective value plateaus early, the "Solution Pool" becomes repetitive, causing the Probability Engine to assign high confidence to potentially suboptimal values.
  - **Timeouts:** If the RL policy predicts an aggressive (large) neighborhood size for a complex constraint structure, the Sub-ILP solver will hit the iteration time limit without finding a solution.

- **First 3 experiments:**
  1. **Validate Selection Logic:** Run ablation comparing the Customized Formula (Eq. 3) against Uniform Sampling (HyP-ASO-UF) on a small dataset to verify the "Solution Pool" contribution.
  2. **Validate Sizing Logic:** Compare the RL-based sizing agent against fixed-ratio baselines (e.g., 50% fixed) to confirm the value of the Adaptive Size component.
  3. **Generalization Stress Test:** Train on the "Easy" dataset (10k vars) and immediately inference on "Hard" (1M vars) to verify that the Global Pooling architecture handles scale shifts without memory errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an automated mechanism for allocating solving time and managing the solution pool replace the current reliance on manually tuned hyperparameters?
- Basis in paper: [explicit] The authors list the manual tuning of solving time as a limitation and state in Future Work that they "aim to develop an automated mechanism for allocating solving time and dynamically managing the feasible solution pool."
- Why unresolved: The current framework requires manually setting distinct time limits ($t_0$, $t_p$, $t_{total}$) for small, medium, and hard datasets, which hinders the approach's adaptability and "fully automated" status.
- What evidence would resolve it: Demonstrations of HyP-ASO maintaining competitive solving times and objective values across varying problem scales without requiring manual time configuration adjustments.

### Open Question 2
- Question: Can a machine learning-based variable selection mechanism improve the reliability of neighborhood generation when high-quality feasible solutions are unavailable?
- Basis in paper: [explicit] The Limitations section notes the "customized scoring formula heavily depends on the availability of high-quality feasible solutions," and Future Work proposes developing "a more efficient variable selection mechanism by machine learning... increasing its generality and reliability."
- Why unresolved: The current confidence score calculation (Eq. 2) relies on a rich solution pool ($X_t$); if the initial solver fails to provide diverse solutions, the variable selection probabilities may be biased or uninformative.
- What evidence would resolve it: Comparative tests on challenging instances where initial solutions are sparse or low-quality, showing an ML-based selector outperforming the customized formula.

### Open Question 3
- Question: Does the HyP-ASO framework maintain its performance advantages when applied to general ILPs with complex constraint structures?
- Basis in paper: [explicit] The Future Work section states an intent to "investigate the applicability of HyP-ASO to general ILP problems, particularly those involving complex constraint structures."
- Why unresolved: The experimental validation primarily focuses on four specific NP-hard benchmarks (MIS, CA, MVC, SC) and two specific MIPLIB instances, which may not represent the structural diversity of all real-world ILPs.
- What evidence would resolve it: Evaluation on a diverse set of real-world benchmarks (e.g., full MIPLIB) showing that the bipartite graph representation and adaptive search generalize to varied constraint matrices.

## Limitations

- The customized variable selection formula relies heavily on the assumption that high-quality solutions share structural regularities, which may not hold for highly irregular or multi-modal ILP landscapes.
- The RL policy's performance depends on the quality of the initial solution pool; if the solver fails to find diverse high-quality solutions quickly, the framework's effectiveness may degrade significantly.
- The global pooling architecture, while enabling scalability, may miss important local optimization cues that are critical for certain problem structures.

## Confidence

- **High Confidence:** The framework's scalability claims and general architecture design are well-supported by ablation studies and generalization tests.
- **Medium Confidence:** The specific mechanisms (solution pool-based selection and adaptive sizing) are theoretically sound and validated empirically, but their performance may vary significantly across different ILP problem types.
- **Low Confidence:** The robustness of the approach to highly irregular problem structures and multi-modal landscapes has not been thoroughly tested.

## Next Checks

1. **Diversity Dependency Test:** Run experiments where the initial solver is deliberately constrained to find fewer or less diverse solutions, then measure the degradation in HyP-ASO's performance to quantify the framework's dependency on solution pool quality.

2. **Irregular Structure Test:** Apply HyP-ASO to ILP instances known for highly irregular constraint structures (e.g., random k-SAT problems at phase transition) to evaluate whether global pooling can still extract useful signals.

3. **Multi-modal Landscape Test:** Design experiments with ILPs that have multiple distinct high-quality solution clusters, and verify whether the solution pool-based selection can effectively navigate between these clusters rather than converging prematurely to one region.