---
ver: rpa2
title: Sensitivity of Small Language Models to Fine-tuning Data Contamination
arxiv_id: '2511.06763'
source_url: https://arxiv.org/abs/2511.06763
tags:
- contamination
- semantic
- data
- transformations
- reversal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically investigates the vulnerability of 23 Small
  Language Models (270M-4B parameters) to data contamination during instruction tuning.
  Four contamination types (character reversal, word reversal, irrelevant, counterfactual)
  were applied at 25%, 50%, 75%, and 100% levels to evaluate model robustness.
---

# Sensitivity of Small Language Models to Fine-tuning Data Contamination
## Quick Facts
- arXiv ID: 2511.06763
- Source URL: https://arxiv.org/abs/2511.06763
- Reference count: 40
- Key finding: Small Language Models show extreme sensitivity to syntactic contamination, with character reversal causing near-complete failure across all models tested

## Executive Summary
This study systematically investigates how Small Language Models (SLMs) respond to various forms of data contamination during instruction tuning. The researchers tested 23 models ranging from 270M to 4B parameters across four contamination types at multiple severity levels. Their findings reveal that SLMs are surprisingly vulnerable to syntactic manipulations, with even minimal character-level contamination causing catastrophic performance degradation. The study also uncovers a "capability curse" where larger, more capable models paradoxically become more susceptible to learning harmful patterns from counterfactual contamination, challenging assumptions about robustness scaling with model size.

## Method Summary
The researchers conducted a systematic evaluation of 23 Small Language Models (270M-4B parameters) by applying four distinct contamination types to their training data: character reversal, word reversal, irrelevant information injection, and counterfactual transformations. Each contamination type was applied at 25%, 50%, 75%, and 100% levels to assess model robustness. The evaluation framework measured grammatical correctness, semantic similarity, and response accuracy across different contamination severities. The study employed both instruction-tuned and non-aligned models to compare contamination sensitivity across different training approaches.

## Key Results
- Character reversal contamination caused near-complete model failure across all tested configurations, with accuracy dropping below 5% even at 25% contamination levels
- Larger models demonstrated better resilience to word reversal contamination, maintaining moderate performance while smaller models failed completely
- Models preserved high grammatical correctness (95-100%) and semantic similarity (75-85%) even under high levels of semantic contamination
- Counterfactual contamination revealed a "capability curse" where larger, more capable models became more susceptible to learning harmful patterns

## Why This Works (Mechanism)
The extreme sensitivity to syntactic contamination stems from how language models process sequential information. Character reversal fundamentally breaks the sequential dependencies that models learn during training, making it impossible for them to recover meaning even with minimal contamination. The differential impact across model sizes suggests that larger models develop more robust contextual representations that can partially compensate for word-level scrambling. The "capability curse" with counterfactual contamination likely occurs because larger models have greater capacity to overfit to misleading patterns, particularly when these patterns appear plausible within the training distribution.

## Foundational Learning
**Character-level processing**: Why needed - Understanding how models handle individual token sequences; Quick check - Verify model can process reversed character sequences without semantic collapse
**Semantic preservation**: Why needed - Critical for evaluating contamination impact on meaning; Quick check - Measure semantic similarity between clean and contaminated outputs
**Syntactic dependency learning**: Why needed - Core to understanding contamination vulnerabilities; Quick check - Test model performance on scrambled word order tasks
**Contextual representation robustness**: Why needed - Explains differential sensitivity across model sizes; Quick check - Compare representations across clean vs contaminated inputs
**Counterfactual reasoning capacity**: Why needed - Key to understanding "capability curse" phenomenon; Quick check - Evaluate model responses to plausible but false scenarios

## Architecture Onboarding
**Component map**: Training data pipeline -> Contamination injection module -> Model fine-tuning process -> Evaluation framework -> Performance metrics
**Critical path**: Clean training data → Fine-tuning → Contamination injection → Model evaluation → Performance assessment
**Design tradeoffs**: Synthetic vs real-world contamination, measurement granularity vs computational cost, model diversity vs controlled comparison
**Failure signatures**: Character reversal → complete task failure (<5% accuracy), Word reversal → size-dependent performance degradation, Semantic contamination → preserved grammar but altered meaning, Counterfactual → larger models more vulnerable
**First 3 experiments**: 1) Character reversal at 10% contamination increments, 2) Cross-contamination testing between model families, 3) Fine-tuning recovery assessment on previously contaminated models

## Open Questions the Paper Calls Out
None

## Limitations
- Contamination methodology relies on synthetic transformations that may not capture real-world data poisoning complexity
- Evaluation focuses primarily on grammatical correctness and semantic similarity, potentially overlooking other performance aspects
- Limited model diversity (23 models, 270M-4B parameters) may not represent broader SLM landscape

## Confidence
- High Confidence: Character reversal causes near-complete failure across all tested configurations
- Medium Confidence: Differential impact of word reversal across model sizes and preservation of grammatical correctness
- Medium Confidence: "Capability curse" finding regarding counterfactual contamination and larger models' increased susceptibility

## Next Checks
1. Conduct contamination testing using real-world data poisoning scenarios with subtle modifications
2. Extend evaluation to additional model families including decoder-only and encoder-decoder architectures
3. Implement cross-contamination experiments to assess transfer of learned vulnerabilities and recovery potential