---
ver: rpa2
title: 'CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal'
arxiv_id: '2512.19554'
source_url: https://arxiv.org/abs/2512.19554
tags:
- think
- arxiv
- reasoning
- answer
- care
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multimodal reasoning
  in large language models through reinforcement learning with verifiable rewards
  (RLVR). The core method, CARE (Contrastive Anchored REflection), introduces an anchored-contrastive
  objective that normalizes advantages within a subgroup of hard negatives, along
  with Reflection-Guided Resampling to convert representative failures into usable
  positives.
---

# CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal

## Quick Facts
- arXiv ID: 2512.19554
- Source URL: https://arxiv.org/abs/2512.19554
- Reference count: 40
- Primary result: CARE improves macro-averaged accuracy by 4.6 points over GRPO on six visual reasoning benchmarks

## Executive Summary
This paper addresses the challenge of improving multimodal reasoning in large language models through reinforcement learning with verifiable rewards (RLVR). The core method, CARE (Contrastive Anchored REflection), introduces an anchored-contrastive objective that normalizes advantages within a subgroup of hard negatives, along with Reflection-Guided Resampling to convert representative failures into usable positives. Empirical results show CARE improves macro-averaged accuracy by 4.6 points over GRPO on six visual reasoning benchmarks, achieving state-of-the-art performance on MathVista and MMMU-Pro with Qwen3-VL-8B.

## Method Summary
CARE introduces a novel anchored-contrastive objective that normalizes advantages within subgroups of hard negatives to improve stability in RLVR for multimodal reasoning. The method incorporates Reflection-Guided Resampling (RGR) to convert representative failures into usable positives by appending a repair cue. During training, multiple rollouts (G=8) are sampled per prompt, and the shortest successful rationale serves as an anchor. Hard negatives are selected based on cosine distance to the anchor, and advantages are normalized within subgroups. The approach also includes region-weighted token advantages, PPO clipping with KL regularization, and an all-negative rescue mechanism to handle cases where all rollouts fail.

## Key Results
- CARE achieves 4.6-point improvement in macro-averaged accuracy over GRPO across six visual reasoning benchmarks
- State-of-the-art performance on MathVista and MMMU-Pro with Qwen3-VL-8B model
- Demonstrates effectiveness on multiple datasets including ChartQA, Geometry3K, and ViRL39K
- Shows significant gains in exact-match accuracy for multimodal reasoning tasks

## Why This Works (Mechanism)
CARE addresses the challenge of RLVR instability in multimodal reasoning by introducing anchored-contrastive normalization. The method creates subgroups of hard negatives and normalizes advantages within each subgroup, preventing dominance by outliers. Reflection-Guided Resampling converts failures into positives by appending repair cues, expanding the effective training set. The anchored approach ensures that the shortest successful rationale guides the learning process, promoting efficiency. Region-weighted token advantages allow different treatment of rationale versus answer regions, while the all-negative rescue mechanism prevents gradient stalling when all rollouts fail.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Training framework where rewards are derived from exact-match evaluation of generated answers. Needed because multimodal reasoning requires verifiable feedback for effective learning. Quick check: Verify reward computation matches exact-match evaluation criteria.
- **Anchored-Contrastive Objective**: Normalization strategy that computes advantages within subgroups of hard negatives relative to an anchor. Needed to stabilize RL training by preventing outlier dominance. Quick check: Confirm subgroup formation and z-score normalization implementation.
- **Reflection-Guided Resampling (RGR)**: Technique that converts representative failures into positives by appending repair cues. Needed to expand effective training data and improve generalization. Quick check: Validate repair cue generation and positive conversion logic.
- **Region-Weighted Token Advantages**: Different treatment of rationale versus answer regions during training. Needed because reasoning quality and final answer correctness require different optimization strategies. Quick check: Verify region weight application matches specified values.
- **Hard Negative Mining**: Selection of challenging negative examples based on cosine distance to anchor. Needed to focus learning on difficult cases that provide maximum signal. Quick check: Confirm cosine distance computation and farthest-first deduplication.
- **All-Negative Rescue**: Mechanism that provides pseudo-rewards when all rollouts fail. Needed to prevent gradient stalling and ensure continuous learning. Quick check: Validate rescue trigger conditions and pseudo-reward application.

## Architecture Onboarding

**Component Map:**
Cold-start SFT -> Multi-rollout Sampling (G=8) -> Anchor Selection -> Hard Negative Mining -> Anchored-Contrastive Normalization -> Region-Weighted Advantages -> PPO Objective with KL Regularization -> All-Negative Rescue

**Critical Path:**
The critical path begins with cold-start SFT on Vision-R1-cold dataset to establish format adherence. Multiple rollouts are then sampled per prompt, with the shortest successful rationale serving as an anchor. Hard negatives are selected based on cosine distance, advantages are normalized within subgroups, and region-weighted token advantages are computed. The PPO objective with KL regularization drives parameter updates, with all-negative rescue ensuring continuous learning even when all rollouts fail.

**Design Tradeoffs:**
The anchored-contrastive approach trades computational complexity (multiple rollouts, subgroup processing) for improved stability and performance. RGR increases effective training data but adds complexity to failure analysis. Region-weighted advantages provide fine-grained control but require careful hyperparameter tuning. The all-negative rescue mechanism prevents training stalls but may introduce noise when genuine failures occur.

**Failure Signatures:**
- Gradient stalls or vanishing updates when all-negative groups dominate
- Rationale length inflation during training indicating improper anchor selection
- Performance degradation when subgroup normalization fails to stabilize advantages
- Inefficient learning when hard negative mining selects too few challenging examples

**First Experiments:**
1. Verify anchored-contrastive normalization by testing subgroup formation and z-score computation on synthetic data
2. Validate RGR functionality by checking positive conversion rates and repair cue generation
3. Test all-negative rescue mechanism by forcing scenarios with complete rollout failure

## Open Questions the Paper Calls Out
None

## Limitations
- Missing critical hyperparameters for RL training (learning rate, batch size, optimizer) prevent exact reproduction
- Format-compliance signal computation unspecified in reward formulation
- Cold-start SFT training details (epochs, learning rate, data processing) not provided
- Performance claims based on comparison with specific baselines without broader generalizability analysis

## Confidence

**High Confidence:** The algorithmic framework of CARE (anchored-contrastive objective with z-score normalization, RGR mechanism, and the integration of region-weighted token advantages) is clearly described and its core contribution is well-defined.

**Medium Confidence:** The empirical results demonstrating improved performance over GRPO baselines on the specified benchmarks are reported, but the lack of complete hyperparameter specifications introduces uncertainty in exact reproduction.

**Low Confidence:** Claims regarding the absolute "state-of-the-art" performance and the generalizability of the method's effectiveness to other models, datasets, or reward structures without further validation.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Conduct a systematic ablation study varying the critical hyperparameters (G, K, M, s, s_refl, γ, ε_low, ε_high, β, δ) to quantify their impact on final performance and identify the optimal configuration.

2. **Cross-Model Generalization**: Apply the CARE algorithm to a different multimodal model (e.g., LLaVA or Qwen2.5-VL) trained from scratch with the same cold-start SFT and RLVR pipeline to assess the method's broader applicability and robustness.

3. **Reward Function Ablation**: Isolate the contribution of the anchored-contrastive objective from the Reflection-Guided Resampling and other components by conducting ablations that remove RGR or use alternative reward formulations (e.g., only accuracy reward, or different normalization strategies) to understand the specific mechanisms driving the performance gains.