---
ver: rpa2
title: 'HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain
  RAG'
arxiv_id: '2512.22442'
source_url: https://arxiv.org/abs/2512.22442
tags:
- gemini
- filtering
- baseline
- query
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HiFi-RAG introduces a hierarchical filtering pipeline to improve
  retrieval-augmented generation in open-domain settings. The system uses Gemini 2.5
  Flash for query formulation, URL filtering, and hierarchical content parsing, followed
  by Gemini 2.5 Pro for two-pass answer generation.
---

# HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG

## Quick Facts
- arXiv ID: 2512.22442
- Source URL: https://arxiv.org/abs/2512.22442
- Reference count: 13
- Primary result: 19.6% ROUGE-L improvement over baseline on MMU-RAGent validation set

## Executive Summary
HiFi-RAG introduces a hierarchical filtering pipeline to improve retrieval-augmented generation in open-domain settings. The system uses Gemini 2.5 Flash for query formulation, URL filtering, and hierarchical content parsing, followed by Gemini 2.5 Pro for two-pass answer generation. On the MMU-RAGent validation set, HiFi-RAG improved ROUGE-L by 19.6% (to 0.274) and DeBERTaScore by 6.2% (to 0.677) over baseline. On Test2025, a dataset requiring post-January 2025 knowledge, it outperformed the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore. The hierarchical filtering approach removed 60.5% of irrelevant content and reduced URL counts by 33.5%, enhancing both precision and cost-efficiency.

## Method Summary
HiFi-RAG is a 5-stage pipeline: (1) Query reformulation with Gemini 2.5 Flash to convert verbose queries into concise search terms, (2) Retrieval plus URL filtering where Flash evaluates search API metadata to discard irrelevant sources (33.5% URL reduction), (3) Hierarchical content parsing that breaks HTML into sections by headers and applies LLM-based chunk filtering to remove 60.5% of irrelevant chunks using section titles and first 200 characters, (4) Two-pass generation with Gemini 2.5 Pro where Turn 1 drafts a comprehensive answer and Turn 2 refines it to match few-shot exemplars, and (5) Post-hoc citation verification with Flash. The system was evaluated on MMU-RAGent validation set (300 samples) and Test2025 dataset (100 samples for post-January 2025 knowledge) using ROUGE-L and DeBERTaScore metrics.

## Key Results
- Improved ROUGE-L by 19.6% (to 0.274) and DeBERTaScore by 6.2% (to 0.677) on MMU-RAGent validation set
- Outperformed parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore on Test2025 dataset
- Hierarchical filtering removed 60.5% of irrelevant content chunks while preserving query-relevant signals
- Pre-fetch URL filtering reduced URL count by 33.5% before expensive web scraping operations

## Why This Works (Mechanism)

### Mechanism 1: LLM-as-a-Reranker for Precision Context Selection
Using a lightweight LLM to evaluate document sections by title and snippet outperforms embedding-based similarity for filtering topically related but factually irrelevant content. Gemini 2.5 Flash ranks parsed HTML sections using only headers + first 200 characters, discarding 60.5% of chunks while preserving query-relevant signals. This gates expensive Pro inference with high-signal context.

### Mechanism 2: Two-Pass Generation Decouples Factuality from Style
Separating answer drafting from style refinement improves metric alignment with reference answers without degrading factual accuracy. Turn 1 generates a comprehensive answer from filtered context; Turn 2 revises to match few-shot exemplars. This prevents style-accuracy tradeoffs during single-pass generation.

### Mechanism 3: Pre-fetch URL Filtering Reduces Noise Before Scraping
Analyzing URL + title + preview content before scraping removes ~33.5% of irrelevant sources, improving latency and downstream context quality. Gemini Flash evaluates search API metadata to filter domains, outdated content, and constraint mismatches before expensive web scraping operations.

## Foundational Learning

- **Multi-stage cascaded inference**: HiFi-RAG uses Flash as a gatekeeper for Pro, trading off accuracy for cost at filtering stages while preserving quality at generation. Given a budget of $1 per 1000 queries, how would you allocate Flash vs. Pro calls across query reformulation, URL filtering, chunk filtering, drafting, and refinement?

- **Knowledge cutoff evaluation**: The Test2025 dataset isolates retrieval capability by requiring post-cutoff knowledge. How would you design a test set to distinguish whether a model's answer comes from retrieved context vs. parametric knowledge?

- **Hierarchical document parsing**: Parsing HTML into section-chunks under headers preserves structure that flat text chunking destroys. Given an HTML page with nested headers (`<h1>` → `<h2>` → `<h3>`), how would you assign a content chunk to its parent section for filtering?

## Architecture Onboarding

- Component map: User Query → [Flash: Query Reformulation] → Search API → [Flash: URL Filtering] → Scraper/Reddit API → [Hierarchical Parser] → [Flash: Chunk Filtering/Ranking] → [Pro: Turn 1 Draft] → [Pro: Turn 2 Refinement] → [Flash: Citation Verification] → Final Answer + Citations

- Critical path: Query → URL Filter → Chunk Filter → Draft → Refine. Any failure in filtering stages propagates garbage to Pro generation.

- Design tradeoffs:
  - Cost vs. Precision: LLM filtering is more accurate than embeddings but 4-6x more expensive than pure vector search. Paper claims acceptable cost due to Flash pricing.
  - Determinism vs. Adaptivity: Rejected agentic workflows (10x cost, timeout risk) in favor of deterministic pipeline. Tradeoff: less flexible, more predictable.
  - Style alignment vs. Reference drift: LLM-as-a-Judge refinement improved qualitative answers but degraded ROUGE/DeBERTaScore by altering phrasing away from ground truth.

- Failure signatures:
  - Over-filtering: If URL/chunk filters are too aggressive, context window may lack answer evidence (check: empty/short answers).
  - Query rephrasing ambiguity: Removing verbose context from queries can introduce ambiguity if filtering isn't applied (Table 3: ROUGE-L dropped 0.2966 → 0.2898 without URL filter).
  - Citation drift: Decoupled verification may miss claims if answer lacks explicit source anchors (verify: citation coverage on factual claims).

- First 3 experiments:
  1. Ablate chunk filtering: Replace LLM chunk ranking with embedding similarity. Measure ROUGE-L/DeBERTaScore delta on validation set.
  2. Stress-test URL filtering: Run pipeline on queries with ambiguous search results (e.g., "Apple" for fruit vs. company). Measure false negative rate in URL discard.
  3. Two-pass vs. single-pass: Run Turn 1 only (no style refinement) on Test2025. Measure metric gap to quantify refinement contribution vs. factual draft quality.

## Open Questions the Paper Calls Out

### Open Question 1
Can embedding-based retrieval be adapted to effectively distinguish between topically similar but factually irrelevant documents, or is LLM-based filtering strictly necessary for high-precision RAG? The paper found Voyage AI embeddings "struggled to distinguish between topically related but factually irrelevant noise" compared to LLM-based filtering, but did not explore whether different embedding strategies could close the performance gap.

### Open Question 2
Does query reformulation inherently introduce ambiguity that harms retrieval performance in open-domain settings when not paired with aggressive pre-filtering? The paper observed that "query rephrasing without URL filtering... resulted in lower scores," likely due to removal of context from verbose queries which introduced ambiguity.

### Open Question 3
How can automated refinement steps (like "LLM-as-a-Judge") be optimized to improve factual accuracy without degrading n-gram overlap metrics such as ROUGE-L? A "Checker" module improved qualitative results but "degraded automated metrics (ROUGE/DeBERTaScore), likely by altering the phrasing too aggressively away from reference text."

### Open Question 4
Does a deterministic, multi-stage pipeline consistently outperform agentic workflows in terms of accuracy-to-cost ratio, or is the observed inferiority of agents specific to the latency constraints of the MMU-RAGent benchmark? A full Gemini Agent with search tools was "10× more expensive and significantly slower... without improving ROUGE scores."

## Limitations
- Generalizability uncertainty: System optimized for ROUGE-L and DeBERTaScore on MMU-RAGent dataset, with no testing on alternative evaluation metrics like factuality scores or human judgment
- API dependency risk: Reliance on Gemini 2.5 Flash and Pro APIs with claimed "moderate cost increase" that could become prohibitive if pricing changes or scales to production
- Structural assumptions: Hierarchical filtering assumes section titles and opening snippets contain sufficient semantic signal, which may fail on poorly structured documents or queries requiring deep semantic understanding

## Confidence

- **High Confidence**: Architectural design of using Flash as a gatekeeper for Pro is well-justified by cost-quality tradeoff analysis. Two-pass generation approach has solid grounding in style-accuracy separation literature.
- **Medium Confidence**: Claimed metric improvements (19.6% ROUGE-L, 6.2% DeBERTaScore) are specific to the evaluation setup. Filtering effectiveness (60.5% chunk reduction, 33.5% URL reduction) appears reproducible given described methodology.
- **Low Confidence**: Generalization to other domains and evaluation metrics remains untested. Cost projections may not hold under different pricing models or scale requirements.

## Next Checks

1. **Ablate chunk filtering**: Replace LLM chunk ranking with embedding similarity on 50 held-out queries. Measure ROUGE-L/DeBERTaScore degradation and false positive rate in content filtering.

2. **Cross-metric evaluation**: Test HiFi-RAG on factuality benchmarks (e.g., TrueCase, WikiAsp) and human evaluation on 20 samples. Compare against ROUGE/DeBERTaScore performance to assess metric alignment gaps.

3. **Cost scaling analysis**: Model per-query costs at 10×, 100×, and 1000× query volumes using current API pricing. Identify the break-even point where cost increases outweigh metric improvements.