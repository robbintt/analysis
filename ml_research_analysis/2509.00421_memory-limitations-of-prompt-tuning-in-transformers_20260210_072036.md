---
ver: rpa2
title: Memory Limitations of Prompt Tuning in Transformers
arxiv_id: '2509.00421'
source_url: https://arxiv.org/abs/2509.00421
tags:
- prompt
- transformer
- tuning
- output
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We characterize the maximal memorization capability of a transformer\
  \ with respect to prompt tuning. That is we obtain an integer K\u2014depending on\
  \ the parameters of the transformer and on an upper bound R of the magnitude of\
  \ tokens\u2014such that for any inputs X1, ..., Xk \u2208 Rd\xD7m, the proportion\
  \ of outputs that are accessible through prompt tuning decreases exponentially with\
  \ k from the rank k \u2265 K."
---

# Memory Limitations of Prompt Tuning in Transformers

## Quick Facts
- **arXiv ID:** 2509.00421
- **Source URL:** https://arxiv.org/abs/2509.00421
- **Reference count:** 40
- **Primary result:** Characterizes maximal memorization capability of transformers with prompt tuning, showing memory capacity scales as $k \in O(m_p/m)$ and accessible outputs decrease exponentially when $k$ exceeds a threshold $K$

## Executive Summary
This paper establishes theoretical bounds on the memory limitations of transformers when using prompt tuning. The authors prove that the number of distinct input/output pairs a transformer can reliably memorize scales linearly with the pre-prompt length, specifically as $k \in O(m_p/m)$. They show that when the number of required outputs exceeds a critical threshold $K$, the proportion of accessible outputs decreases exponentially. The analysis reveals that single-layer transformers have particularly severe geometric constraints, with accessible output spaces effectively limited to hyperplanes, while the volumetric constraints apply to transformers of any depth.

## Method Summary
The paper uses mathematical proofs based on covering and packing numbers, Lipschitz analysis, and mean-field transformer frameworks to establish theoretical bounds. The key methodology involves computing the volume of reachable output spaces versus total output spaces, using Lipschitz continuity assumptions to bound the transformer's behavior on bounded inputs. The proofs proceed by establishing how many distinct output regions can be reached via prompt tuning and comparing this to the total number of required outputs. The mean-field analysis extends results to arbitrary sequence lengths, while single-layer analysis uses geometric arguments about the accessible output subspace.

## Key Results
- Memory capacity scales as $k \in O(m_p/m)$, meaning the number of memorizable pairs grows linearly with prompt length
- When $k > K$ (where $K$ depends on transformer parameters and input bounds), the proportion of accessible outputs decreases exponentially
- Single-layer transformers can only access outputs within an "almost hyperplane" subspace, severely limiting their expressivity for prompt tuning
- The results hold for approximate memorization up to error $\epsilon$, with tighter bounds for smaller $\epsilon$

## Why This Works (Mechanism)

### Mechanism 1: Volume-Based Accessibility Constraint
The proof utilizes covering and packing numbers to compare the volume of the total output space versus the reachable output space. Because transformers are Lipschitz continuous on bounded inputs, a finite set of pre-prompts can only reach a finite number of distinct output regions. As the number of required distinct outputs ($k$) increases, the ratio of reachable space to total space shrinks exponentially. The core assumption is that inputs are bounded by radius $r$ and the transformer has finite Lipschitz constant $L$.

### Mechanism 2: Linear Scaling of Information Capacity
The pre-prompt acts as a storage buffer, and the theoretical derivation shows that the maximal number of pairs $k$ scales as $O(m_p/m)$, where $m$ is the sequence length of each pair. This implies a hard cap on "information density" within the prompt tokens. The core assumption is that memorization is defined as the ability to approximate target outputs within error $\epsilon$.

### Mechanism 3: Geometric Limitation of Single-Layer Attention
In single-layer transformers, the output space accessible via prompt tuning is effectively restricted to a low-dimensional subspace (an "almost hyperplane"). The single-layer attention output for a query can be decomposed into a part dependent on the prompt and a part dependent on the input. For multi-head attention, the space of accessible outputs is constrained by the number of heads $h$ and dimension $d$.

## Foundational Learning

- **Concept: Lipschitz Continuity**
  - Why needed here: The entire theoretical bound rests on the transformer being Lipschitz continuous. If the function weren't Lipschitz, small changes in the prompt couldn't be bounded, and the covering number arguments would fail.
  - Quick check question: Does the model's output change by a bounded factor when the input is perturbed by $\delta$?

- **Concept: Covering and Packing Numbers**
  - Why needed here: These mathematical tools quantify the "size" or "complexity" of the input/output spaces. The paper uses them to count how many distinct "buckets" of outputs exist versus how many the prompt can reach.
  - Quick check question: How many balls of radius $\epsilon$ are needed to cover the entire space of possible outputs?

- **Concept: Mean-Field Limit**
  - Why needed here: To analyze contexts of theoretically infinite or arbitrary length, the paper shifts from discrete tokens to probability measures. This generalizes the findings beyond fixed sequence lengths.
  - Quick check question: Can we model the discrete sequence as a continuous probability distribution to simplify the limit analysis?

## Architecture Onboarding

- **Component map:** Input $X \in \mathbb{R}^{d \times m}$ + Pre-prompt $P \in \mathbb{R}^{d \times m_p}$ -> Transformer ($\tau$) with $l$ layers (Self-Attention + MLP) -> Output Approximation of target $Y$

- **Critical path:** The derivation of Theorem 4.7. You must understand how $C_{in}$ (reachable outputs) is derived from the prompt length and how $C_{out}$ (total possible outputs) grows with the number of pairs $k$. The exponential decay $\text{ratio} \approx C_{in}/C_{out}$ is the core theoretical contribution.

- **Design tradeoffs:**
  - Prompt Length vs. Fidelity: You cannot arbitrarily compress information. To double the number of memorized facts ($k$), you must roughly double the prompt tokens ($m_p$).
  - Depth vs. Expressivity: Single layers are geometrically brittle (restricted to hyperplanes). Depth is required to escape this specific dimensional trap, though it doesn't solve the volumetric scaling limit.

- **Failure signatures:**
  - The "Lost in the Middle" phenomenon: While often attributed to attention dilution, this paper suggests a fundamental mathematical bound: as context $k$ increases, the *proportion* of accessible answers drops exponentially.
  - Single-Layer Stagnation: If fine-tuning a single-layer adapter, you may find it mathematically impossible to map inputs to targets that lie outside the specific accessible subspace $E$.

- **First 3 experiments:**
  1. Verify Scaling Law: Synthetic test. Fix a transformer. Generate random I/O pairs. Increase $k$ while keeping $m_p$ fixed. Plot the accuracy decay. Does it follow the exponential curve predicted when $k > m_p/m$?
  2. Probe Single-Layer Geometry: Train a 1-layer transformer via prompt tuning to memorize 2 pairs $(X_1, Y_1), (X_2, Y_2)$. Project the optimal $Y_1, Y_2$ into the subspace $E$ defined in Theorem 5.7. Verify if points *outside* $E$ are unreachable.
  3. Lipschitz Bound Check: Measure the empirical Lipschitz constant of the target model. Compare the observed "memory capacity" with the theoretical bound $K$ derived in Section 4.3 to see if real models approach this theoretical limit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the linear scaling limits on memorization ($k \in O(m_p/m)$) translate directly to limits on generalization in in-context learning, or can transformers generalize to unseen inputs more efficiently than they memorize specific input/output pairs?
- Basis in paper: [inferred] The paper explicitly defines its scope as "memorization capability" (mapping specific $X_i$ to $Y_i$) and distinguishes this from prior work on "universal approximation" (generalization), leaving the connection between memory capacity and generalization performance unstated.
- Why unresolved: The proofs rely on packing numbers (counting distinct output vectors) to establish memory bounds, which quantifies the ability to store specific answers but does not theoretically address the ability to learn a mapping function for out-of-distribution queries.
- What evidence would resolve it: Theoretical analysis or empirical benchmarks showing that the error rate for generalization tasks decays at a different rate than the exponential decay of memorized output accessibility derived in Theorem 4.7.

### Open Question 2
- Question: How does the inclusion of Layer Normalization (LayerNorm) alter the linear scaling laws and exponential decay of accessible outputs established for prompt tuning?
- Basis in paper: [inferred] Definition 2.2 explicitly states that "layer normalization operation is omitted" for simplicity in the analysis, despite it being a standard component of the transformer architecture that affects the rank and Lipschitz properties of the network.
- Why unresolved: LayerNorm impacts the Lipschitz constant and mitigates rank collapse, which are central to the volume-based arguments used to derive the exponential decay of accessible outputs in Theorem 4.7.
- What evidence would resolve it: A derivation of Theorem 4.7 that incorporates the LayerNorm operator, showing whether the constant $K$ or the scaling relationship $O(m_p/m)$ changes when the normalization layer is introduced.

### Open Question 3
- Question: Can the result that the accessible space is "almost a hyperplane" (Theorem 5.7) be extended to single-layer transformers that do not satisfy the invertibility assumption ($\|W_1\|_2 \cdot \|W_2\|_2 < 1$)?
- Basis in paper: [explicit] Theorem 5.8 and the discussion in Section 5.2 explicitly rely on Assumption 5.5 ($\|W_1\|_2 \cdot \|W_2\|_2 < 1$) to ensure the MLP is invertible, which the authors describe as "mild" but necessary for the proof.
- Why unresolved: The theoretical guarantee that prompt tuning cannot access outputs outside a specific subspace (hyperplane) remains unproven for arbitrary weight matrices where the MLP might not be invertible.
- What evidence would resolve it: A generalized proof of Theorem 5.7 that removes Assumption 5.5, or a counter-example demonstrating that non-invertible MLPs allow prompt tuning to access a volume of the output space significantly larger than a hyperplane.

## Limitations

- The theoretical bounds rely heavily on Lipschitz continuity assumptions and specific architectural constraints that may be overly conservative for practical models
- The geometric limitations described for single-layer transformers may not fully capture the expressivity gains from architectural variations like rotary embeddings or alternative attention mechanisms
- The analysis assumes bounded inputs and well-behaved weight matrices, which may not hold in all practical scenarios

## Confidence

- **High Confidence**: The volume-based accessibility constraint (Mechanism 1) and its exponential decay property when $k$ exceeds the threshold. This follows from rigorous covering/packing number arguments that are mathematically sound.
- **Medium Confidence**: The linear scaling law $k \in O(m_p/m)$ (Mechanism 2). While the theoretical derivation is sound, empirical validation across diverse architectures would strengthen this claim.
- **Medium Confidence**: The single-layer geometric limitation (Mechanism 3). The mathematical proof is rigorous, but the practical relevance depends on how closely real-world models approximate the theoretical assumptions about weight bounds.

## Next Checks

1. **Empirical Scaling Verification**: Implement a synthetic experiment where you fix a transformer architecture, generate random input/output pairs, and systematically vary the number of pairs $k$ while keeping prompt length $m_p$ constant. Measure the accuracy decay curve and verify it matches the predicted exponential drop when $k$ exceeds the theoretical threshold $K$.

2. **Subspace Accessibility Test**: For a single-layer transformer, use prompt tuning to memorize two specific input/output pairs. Then attempt to train the model to output a third pair that lies outside the theoretically predicted accessible subspace $E$. If the model consistently fails to learn this out-of-subspace target, it validates the geometric limitation claim.

3. **Lipschitz Constant Validation**: Measure the empirical Lipschitz constant of a practical transformer model by computing the maximum ratio of output changes to input perturbations across the embedding space. Compare this measured value to the theoretical bounds used in the paper, and assess whether the predicted memory capacity $K$ aligns with observed memorization performance.