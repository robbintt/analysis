---
ver: rpa2
title: Self-Certifying Primal-Dual Optimization Proxies for Large-Scale Batch Economic
  Dispatch
arxiv_id: '2510.15850'
source_url: https://arxiv.org/abs/2510.15850
tags:
- solver
- proxies
- optimization
- power
- optimality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a hybrid solver framework for large-scale economic
  dispatch problems that combines machine learning proxies with classical optimization
  solvers to achieve both high speed and worst-case optimality guarantees. The key
  innovation is a self-certifying approach that leverages duality theory to efficiently
  bound the sub-optimality of ML predictions using the duality gap, falling back to
  exact optimization only when necessary.
---

# Self-Certifying Primal-Dual Optimization Proxies for Large-Scale Batch Economic Dispatch

## Quick Facts
- **arXiv ID**: 2510.15850
- **Source URL**: https://arxiv.org/abs/2510.15850
- **Reference count**: 32
- **Primary result**: Hybrid ML-optimization solver achieves 1000x speedup with <2% optimality gap on large-scale power systems

## Executive Summary
This paper introduces a hybrid solver framework that combines machine learning proxies with classical optimization to solve large-scale economic dispatch problems efficiently while guaranteeing worst-case optimality. The key innovation is a self-certifying approach that uses duality theory to bound the sub-optimality of ML predictions through the duality gap, falling back to exact optimization only when necessary. A novel training procedure jointly optimizes primal and dual proxies using a hinge-like loss function targeting a user-specified optimality tolerance. The framework enables trustworthy deployment of ML proxies in critical infrastructure by providing controllable worst-case performance guarantees.

## Method Summary
The hybrid solver framework combines ML prediction with classical optimization through a self-certifying mechanism. At its core, the approach uses duality theory to verify ML predictions: if the duality gap is sufficiently small, the prediction is certified as near-optimal; otherwise, the exact solver is invoked. The training procedure jointly optimizes primal and dual proxies using a hinge-like loss function that directly targets the desired optimality tolerance. This dual-proxy training ensures that predictions come with reliable quality guarantees. The framework is designed for batch economic dispatch problems in power systems, where it achieves dramatic speedups by avoiding exact optimization in most cases while maintaining theoretical optimality bounds.

## Key Results
- Achieves over 1000x speedup compared to parallelized simplex-based solvers on large-scale power systems
- Guarantees maximum optimality gaps below 2% while maintaining high accuracy
- Successfully scales to systems with up to 9,241 buses
- Maintains trustworthy worst-case performance guarantees through self-certification

## Why This Works (Mechanism)
The approach leverages duality theory from optimization: the duality gap provides a computable certificate of sub-optimality that can be efficiently estimated from the ML predictions themselves. By jointly training primal and dual proxies with a loss function that directly targets the optimality tolerance, the framework ensures that when the duality gap is small, the primal prediction is guaranteed to be near-optimal. The self-certifying mechanism only invokes expensive exact optimization when the gap exceeds the threshold, dramatically reducing computational cost while maintaining theoretical guarantees.

## Foundational Learning
- **Duality Theory in Optimization**: Why needed - Provides the mathematical foundation for certifying ML predictions; Quick check - Verify understanding of primal-dual relationships and weak/strong duality
- **Economic Dispatch Problem Structure**: Why needed - Problem-specific knowledge is crucial for effective proxy design; Quick check - Confirm understanding of power flow constraints and cost minimization objectives
- **ML for Optimization Proxies**: Why needed - Understanding how neural networks can approximate optimization solutions; Quick check - Review literature on learning optimization algorithms and end-to-end optimization
- **Batch Optimization**: Why needed - The framework targets solving many instances efficiently; Quick check - Understand the computational benefits of batch processing over sequential solving
- **Hinge-like Loss Functions**: Why needed - Novel training objective that directly targets optimality tolerance; Quick check - Verify understanding of margin-based losses and their properties
- **Verification vs. Prediction**: Why needed - The self-certifying approach fundamentally separates these concepts; Quick check - Understand when and how to trust ML predictions in safety-critical applications

## Architecture Onboarding

**Component Map**: Data -> ML Proxy (Primal & Dual) -> Duality Gap Verification -> Certified Prediction / Exact Solver

**Critical Path**: The most critical computational path is the duality gap verification, as it determines whether the ML prediction is accepted or exact optimization is invoked. This check must be extremely efficient to maintain the speedup benefits.

**Design Tradeoffs**: The framework trades off between prediction accuracy and verification cost. A tighter optimality tolerance requires more accurate proxies and potentially more frequent exact solver invocations. The hinge-like loss function represents a novel balance between training objectives and practical guarantees.

**Failure Signatures**: Primary failure modes include: (1) ML proxy consistently produces large duality gaps requiring frequent exact solving, (2) distribution shift causes proxy performance degradation, (3) numerical instability in duality gap computation, and (4) adversarial inputs that exploit the verification mechanism.

**3 First Experiments**:
1. Benchmark duality gap computation speed against exact solution time on representative instances
2. Test ML proxy accuracy on out-of-distribution samples to assess robustness
3. Profile the trade-off between optimality tolerance and speedup ratio across different problem sizes

## Open Questions the Paper Calls Out
None

## Limitations
- The methodology assumes duality gaps reliably bound sub-optimality, which may not generalize to all optimization problem classes
- The 1000x speedup comparison is against parallelized simplex solvers; performance relative to other modern solvers is unclear
- The hinge-like loss function's theoretical convergence properties are not fully established
- Claims of "trustworthy deployment" require validation beyond the experimental setup, particularly for adversarial inputs and distribution shifts

## Confidence

**High confidence**: The hybrid framework architecture and its basic components (ML proxy, duality gap verification, fallback mechanism)

**Medium confidence**: The experimental results and speed-up claims on the tested power system instances

**Medium confidence**: The self-certifying approach using duality gaps for bounding sub-optimality

**Low confidence**: The generalizability of the approach to other optimization problem classes beyond economic dispatch

## Next Checks
1. Benchmark against state-of-the-art commercial optimization solvers on the same problem instances to verify the claimed 1000x speedup is not artifactual
2. Test the framework's robustness under distribution shifts and adversarial perturbations to validate "trustworthy deployment" claims
3. Evaluate the approach on other optimization problem classes (e.g., unit commitment, stochastic programming) to assess generalizability beyond economic dispatch