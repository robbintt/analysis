---
ver: rpa2
title: Unbiased Stochastic Optimization for Gaussian Processes on Finite Dimensional
  RKHS
arxiv_id: '2508.20588'
source_url: https://arxiv.org/abs/2508.20588
tags:
- stochastic
- learning
- gaussian
- kernel
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two novel algorithms for stochastic hyperparameter
  learning in Gaussian Processes (GPs) when the kernel induces a finite-dimensional
  RKHS. The key challenge addressed is that standard methods (SVGP, biased SGD) either
  rely on approximations (inducing points) or biased gradients, without convergence
  guarantees to true stationary points.
---

# Unbiased Stochastic Optimization for Gaussian Processes on Finite Dimensional RKHS

## Quick Facts
- arXiv ID: 2508.20588
- Source URL: https://arxiv.org/abs/2508.20588
- Reference count: 16
- This paper proposes two novel algorithms for unbiased stochastic hyperparameter learning in Gaussian Processes when the kernel induces a finite-dimensional RKHS.

## Executive Summary
This paper addresses the challenge of stochastic hyperparameter learning in Gaussian Processes (GPs) by proposing two exact methods that avoid the biases of existing approaches. Standard methods like SVGP and biased SGD either rely on approximations (inducing points) or produce biased gradients, lacking convergence guarantees to true stationary points. The proposed algorithms reformulate the marginal likelihood optimization as either a minimax problem or a compositional stochastic gradient problem, enabling exact stochastic optimization with arbitrary batch sizes without requiring large batches or inducing points. Experiments demonstrate significantly lower negative log marginal likelihood than existing approaches, particularly with smaller batch sizes, making the methods valuable for memory-constrained environments.

## Method Summary
The paper introduces two novel algorithms for exact stochastic optimization of Gaussian Process hyperparameters when the kernel induces a finite-dimensional Reproducing Kernel Hilbert Space (RKHS). The first algorithm reformulates the marginal likelihood optimization as a nonconvex-concave minimax problem, where the log-determinant term is handled through an auxiliary variable and penalty constraint. This formulation allows for independent mini-batches in the minimization and maximization steps, producing unbiased gradient estimates. The second algorithm uses Stochastic Compositional Gradient Descent (SCGD), treating the log-determinant as a composition of expected-value functions and maintaining an exponentially-smoothed estimate of the kernel matrix across iterations. Both methods achieve exact stochastic optimization without requiring large batch sizes or inducing points.

## Key Results
- The proposed methods achieve significantly lower negative log marginal likelihood than SVGP and biased SGD, especially with smaller batch sizes
- Performance remains stable as batch size decreases, unlike existing stochastic GP methods which degrade
- On UCI regression datasets with neural network kernels, the methods consistently outperform competitors in optimizing marginal likelihood
- The advantage grows as batch size decreases, making the methods suitable for memory-constrained edge devices

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating GP hyperparameter learning as a nonconvex-concave minimax problem yields unbiased stochastic gradients even with small batch sizes.
- **Mechanism:** The marginal likelihood objective contains a log-determinant term, `log|F(θ)|`, which lacks a sum-decomposable gradient. The method avoids direct differentiation by introducing an auxiliary variable and a penalty constraint, transforming the problem into `min_ζ max_∥B∥≤1 Ψ(ζ,B)`. The function `Ψ` is a sum over individual data contributions, permitting independent mini-batches for the minimization and maximization steps, thereby producing unbiased gradient estimates.
- **Core assumption:** The kernel has a finite-dimensional feature map with moderate dimension `d`. Convergence guarantees rely on the concave inner maximization over a convex constraint set and properties from minimax optimization theory (Boţ & Böhm, 2020).
- **Evidence anchors:**
  - [abstract] The first algorithm leverages recent advances in minimax optimization theory for exact stochastic optimization.
  - [Section 3.1] Describes the minimax reformulation and the dual update rule, noting theoretical convergence in O(ε⁻⁸) iterations.
  - [corpus] No direct corpus evidence for this specific minimax formulation.
- **Break condition:** Performance degrades if the feature dimension `d` grows large, as per-iteration compute scales as O(d³) and storage as O(d²). Exactness is lost if the kernel lacks a finite feature representation.

### Mechanism 2
- **Claim:** Stochastic Compositional Gradient Descent (SCGD) enables exact stochastic optimization by treating the log-determinant as a composition of expected-value functions.
- **Mechanism:** The loss is expressed as a composition `l(θ) = v(u(θ))`, where the inner function `u(θ)` includes the matrix `F(θ)`. SCGD maintains an exponentially-smoothed estimate `F̃_t` of `F(θ_t)` across iterations. This estimate provides a stable, unbiased reference for computing gradients involving the log-determinant, decoupling gradient accuracy from the current batch size.
- **Core assumption:** Relies on standard SCGD assumptions (Wang et al., 2017): differentiability of the composed functions and appropriate step-size schedules (e.g., a_t = t^-3/4, b_t = t^-1/2).
- **Evidence anchors:**
  - [abstract] The second algorithm uses SCGD, treating the log-determinant as a composition of functions.
  - [Section 3.2] Provides explicit SCGD update rules for the parameters `θ` and the running estimate `F̃`.
  - [corpus] Foundational convergence theory is cited from Wang et al. (2017).
- **Break condition:** Fails if the running estimate `F̃_t` is not correctly scaled by `n/|S|` (reintroducing bias) or if the smoothing parameter `b_t` is poorly tuned, leading to stale or unstable estimates.

### Mechanism 3
- **Claim:** Performance remains stable as batch size decreases, unlike existing stochastic GP methods.
- **Mechanism:** Competing methods (SVGP, BSGD) suffer when memory restricts batch size or inducing points: BSGD's gradient bias grows with smaller batches, and SVGP's approximation quality depends on the number of inducing points. The proposed methods generate unbiased gradients for any non-zero batch size, decoupling optimization quality from batch size.
- **Core assumption:** The feature dimension `d` remains moderate and within memory limits. Dataset properties allow effective stochastic optimization with small batches.
- **Evidence anchors:**
  - [abstract] States the advantage is most pronounced for smaller batch sizes, making methods suitable for memory-constrained environments like edge devices.
  - [Section 5] Experimental results show consistent NLM values for proposed methods across batch sizes, while competitors degrade.
  - [corpus] No direct corpus evidence comparing this specific batch-size stability property.
- **Break condition:** The advantage diminishes if the required feature dimension `d` becomes so large that O(d²) memory exceeds the savings from O(b²) reduction, or if the dataset requires very large batches for stable convergence.

## Foundational Learning

- **Concept: Finite-dimensional Reproducing Kernel Hilbert Space (RKHS)**
  - **Why needed here:** The theoretical guarantees of exact, unbiased optimization depend entirely on the kernel having a finite-dimensional feature map `φ_α(x)`. Understanding this is critical for knowing when the method is applicable (e.g., linear kernels) versus when it requires approximations (e.g., Gaussian kernels with random features).
  - **Quick check question:** Given a kernel `k(x,x')`, how would you determine if it induces a finite-dimensional RKHS, and what is the dimension `d`?

- **Concept: Minimax Optimization (Nonconvex-Concave)**
  - **Why needed here:** The first algorithm reformulates the GP problem into a minimax form. Understanding the roles of the inner (concave) and outer (nonconvex) problems is necessary to implement the alternating proximal-gradient steps and tune the penalty parameter `µ`.
  - **Quick check question:** In the formulation `min_θ max_∥B∥≤1 Ψ(ζ,B)`, what constraint is placed on the maximization variable `B` and why is the inner problem concave?

- **Concept: Stochastic Compositional Gradient Descent (SCGD)**
  - **Why needed here:** The second algorithm applies SCGD to handle the non-decomposable log-determinant term. One must grasp the two-timescale update structure: the inner estimate `F̃` evolves more slowly than the outer parameter `θ`, and their interaction defines the method's dynamics.
  - **Quick check question:** In SCGD, what is the purpose of maintaining a running average `F̃_t` instead of computing `F(θ_t)` directly from the current mini-batch?

## Architecture Onboarding

- **Component map:**
  - **Feature Extractor (φ_α)** -> **Matrix Builder (F, A, F̃)** -> **Objective Evaluator** -> **Optimizer (Minimax or SCGD)** -> **Gradient Propagator**

- **Critical path:** The forward pass computes features `φ_α(x_i)` for a batch. These features are aggregated into a `d x d` matrix (e.g., `F_i`). This matrix is the input to the core log-determinant or minimax computation. The backward pass propagates gradients from this `d x d` matrix back to the feature extractor. The dominant computational cost is O(d³) per iteration from matrix operations on this `d x d` matrix.

- **Design tradeoffs:**
  - **Feature Dimension (d):** A larger `d` increases model capacity but directly increases compute O(d³) and memory O(d²). The method is designed for "moderate" `d`.
  - **Batch Size (b):** Unlike competitors, performance is robust to small `b`. Smaller `b` reduces per-iteration memory (O(bd) terms) but may increase noise.
  - **Minimax vs. SCGD:** Minimax is theoretically grounded but requires tuning penalty `µ`. SCGD is simpler to implement but requires tuning the smoothing parameter `b_t`. Experiments suggest SCGD with `b_t=0.9` works well, partly due to correct scaling.

- **Failure signatures:**
  - **Instability/NaNs:** May occur if `A` or `F` become ill-conditioned or singular. The method constrains `A ⪰ σ²I` to mitigate this.
  - **No convergence:** In Minimax, if the penalty `µ` is not increased over time (as theory suggests), the solution may not satisfy the constraint `A=F(θ)`. In SCGD, a poorly chosen `b_t` can lead to stale or unstable estimates.
  - **Worse-than-expected performance:** On infinite-dimensional kernels (e.g., RBF), performance depends on the quality of the finite-d approximation (e.g., Random Fourier Features).

- **First 3 experiments:**
  1. **Unit Test - Gradient Unbiasedness:** On a small dataset with a linear kernel, estimate the full-batch gradient of `log|F(θ)|`. Then, average many mini-batch gradients from both proposed algorithms. Verify that the averages converge to the full-batch gradient as the number of samples increases.
  2. **Ablation - Batch Size vs. Performance:** On a UCI dataset with a linear kernel, train models using BSGD, SVGP, and one proposed method across a range of batch sizes (e.g., 32, 64, 128, 256). Plot final negative log-marginal likelihood vs. batch size to confirm the proposed method's stability curve.
  3. **Hyperparameter Sensitivity:** For the SCGD method, run a sweep on the smoothing parameter `b_t` (e.g., [0.5, 0.7, 0.9, 0.99]) on a validation set. Analyze the trade-off between stability of `F̃` and responsiveness to changes in `θ`.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the disconnect between improved marginal likelihood optimization and test RMSE be resolved without resorting to cross-validation or early stopping?
- Basis in paper: [explicit] The authors note in the Conclusion that "the optimization advantage is not fully reflected in the test error" and that cross-validation is still needed.
- Why unresolved: Optimizing the marginal likelihood (a Bayesian model selection criterion) does not strictly minimize prediction error (frequentist metric), and the bias-variance trade-off in the stochastic setting for GPs is not fully understood.
- What evidence would resolve it: A theoretical analysis showing conditions where unbiased stochastic NLL minimization correlates with generalization error, or empirical results showing consistent RMSE improvements across diverse datasets without external tuning.

### Open Question 2
- Question: Can the computational and memory complexity with respect to the feature dimension $d$ be reduced to allow for high-dimensional feature spaces?
- Basis in paper: [explicit] The complexity analysis and Conclusion state that memory scales as $O(d^2)$ and computation as $O(d^3)$, restricting the method to "moderate" dimension representations.
- Why unresolved: The algorithms rely on explicit matrix operations (inverse/determinant) on the $d \times d$ feature covariance matrix, which fundamentally bottlenecks scaling.
- What evidence would resolve it: A modification of the Minimax or SCGD algorithms using iterative solvers (e.g., conjugate gradient) or sketching techniques that lower the dependency to linear or log-linear in $d$.

### Open Question 3
- Question: Is it possible to extend the exact convergence guarantees of these algorithms to infinite-dimensional RKHSs without relying on finite-dimensional approximations?
- Basis in paper: [explicit] The Abstract and Methodology section state the approach can be extended to infinite RKHSs only "at the cost of forgoing exactness" by using approximations like Random Fourier Features.
- Why unresolved: The unbiased gradient estimates depend on the finite trace properties of the explicit feature map; infinite dimensions generally induce bias in stochastic log-determinant estimators.
- What evidence would resolve it: Deriving an unbiased estimator for the log-determinant term (or the trace operator in the dual formulation) that holds for infinite-dimensional operators.

## Limitations
- The requirement for finite-dimensional feature maps limits applicability to kernels like linear or polynomial, with performance on infinite-dimensional kernels depending on approximation quality
- O(d³) computational complexity per iteration is prohibitive for very high-dimensional feature spaces
- The minimax approach requires careful tuning of the penalty parameter μ over time, which is not fully addressed in experimental validation

## Confidence
- **High Confidence:** The unbiased gradient property for small batch sizes is well-supported by mathematical reformulation and experimental results showing consistent NLM values across batch sizes
- **Medium Confidence:** The minimax reformulation has strong theoretical grounding but lacks direct experimental validation of its convergence properties compared to SCGD
- **Medium Confidence:** The SCGD approach is theoretically sound, but the optimal choice of smoothing parameter b_t is somewhat heuristic (0.9 works well) rather than derived from first principles

## Next Checks
1. **Scalability Analysis:** Systematically evaluate performance as feature dimension d increases (e.g., d=50, 100, 200) on both linear and RBF kernels with random features, measuring NLM, RMSE, and wall-clock time to identify the practical ceiling for d.
2. **Convergence Rate Comparison:** Implement the theoretical step-size schedules (a_t = t^(-3/4), b_t = t^(-1/2) for SCGD; increasing μ for minimax) and compare convergence rates against the heuristic parameters used in experiments (b_t=0.9 for SCGD).
3. **Alternative Kernel Approximation:** Replace the finite feature map with other approximation schemes (e.g., Orthogonal Random Features, Nyström method) to evaluate robustness to the choice of finite-dimensional representation for infinite-dimensional kernels.