---
ver: rpa2
title: 'Hierarchical Retrieval with Out-Of-Vocabulary Queries: A Case Study on SNOMED
  CT'
arxiv_id: '2511.16698'
source_url: https://arxiv.org/abs/2511.16698
tags:
- snomed
- ontology
- concepts
- retrieval
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of retrieving hierarchical concepts
  from SNOMED CT when queries have no exact matches in the ontology (OOV queries).
  The proposed method uses language model-based ontology embeddings trained in hyperbolic
  space to capture hierarchical relationships.
---

# Hierarchical Retrieval with Out-Of-Vocabulary Queries: A Case Study on SNOMED CT

## Quick Facts
- arXiv ID: 2511.16698
- Source URL: https://arxiv.org/abs/2511.16698
- Reference count: 13
- Key outcome: OnT model outperforms SBERT and lexical matching, achieving MRR scores of 0.68 for single-target and 0.67-0.68 for multi-target retrieval on 50 OOV queries.

## Executive Summary
This paper addresses the challenge of retrieving hierarchical concepts from SNOMED CT when queries have no exact matches in the ontology. The proposed method uses language model-based ontology embeddings trained in hyperbolic space to capture hierarchical relationships. Two approaches are compared: HiT (hierarchy-only) and OnT (hierarchy plus OWL relationships), both using depth-biased scoring functions. An evaluation dataset of 50 OOV queries was manually annotated against SNOMED CT concepts. The OnT model outperforms baselines including SBERT and lexical matching methods, achieving MRR scores of 0.68 for single-target retrieval and 0.67-0.68 for multi-target retrieval. The miniature OnT model trained on semantic branches performs best, demonstrating that embeddings trained on semantically coherent ontology fragments can improve retrieval accuracy.

## Method Summary
The method uses language model-based ontology embeddings trained in hyperbolic space (Poincaré ball) to capture hierarchical relationships. Two models are developed: HiT uses subsumption axioms only, while OnT extends this with OWL existential restrictions. Both use contrastive objectives with negative sampling. Retrieval employs either hyperbolic distance (d_κ) or a depth-biased subsumption score (s(C⊑D) = −[d_κ(xC, xD) + λ(||xD||κ − ||xC||κ)]). The OnT Mini variant is trained on four semantic branches (Body Structure, Clinical Finding, Event, Procedure) rather than the full ontology. Evaluation uses 50 manually annotated OOV queries against SNOMED CT concepts.

## Key Results
- OnT Mini achieves MRR of 0.68 for single-target retrieval and 0.67-0.68 for multi-target retrieval
- Depth-biased scoring provides up to 3-point MRR improvement over pure hyperbolic distance
- Training on semantic branches outperforms training on full SNOMED CT
- OnT outperforms SBERT and lexical matching baselines by significant margins
- H@5 scores consistently exceed 0.80 across all models and evaluation settings

## Why This Works (Mechanism)

### Mechanism 1: Hyperbolic Geometry Encodes Hierarchy
Embedding concepts in hyperbolic (Poincaré ball) space preserves hierarchical relationships more efficiently than Euclidean space for tree-like ontologies. General concepts are positioned near the origin while specific concepts move toward the circumference. Hyperbolic distance between points approximates hierarchical distance, enabling subsumption inference without explicit reasoning.

### Mechanism 2: Depth-Biased Subsumption Scoring
Combining hyperbolic distance with a depth-norm term enables asymmetric ranking that prefers valid parents over semantically similar siblings. The scoring function adds a centripetal bias: candidates deeper in the hierarchy are penalized as potential parents, favoring ancestors at appropriate depths.

### Mechanism 3: Semantic Fragment Training Improves Ranking
Training embeddings on semantically coherent ontology subsets yields better OOV retrieval accuracy. Full SNOMED CT contains ~350k concepts across heterogeneous domains. Training on focused branches reduces noise from unrelated regions, producing embeddings that better discriminate within relevant semantic neighborhoods.

## Foundational Learning

- **Hyperbolic vs. Euclidean Embedding Spaces**: Hyperbolic space expands exponentially toward the boundary, matching the exponential growth of tree depth—unlike Euclidean space, which distorts hierarchical distances. Quick check: Given a tree with branching factor 3 and depth 10, would Euclidean embeddings require higher dimensionality than hyperbolic to preserve all pairwise distances?

- **Subsumption and Description Logic (EL)**: OnT extends HiT by modeling OWL existential restrictions alongside simple subsumption. Quick check: If C ⊑ D and D ⊑ E, what does transitivity imply about C and E?

- **Contrastive Learning with Negative Sampling**: HiT and OnT use contrastive objectives; HiT's underperformance is attributed to random negative sampling potentially conflicting with hierarchical structure. Quick check: If "Pins and needles" is a positive pair with "Paresthesia," what makes a good negative example versus a harmful one?

## Architecture Onboarding

- **Component map**: Query string -> Language model encoder -> Hyperbolic embedding -> Scoring (d_κ or s(C⊑D)) -> Ranked concept list
- **Critical path**: 1) Preprocess SNOMED CT labels, 2) Select training fragment, 3) Train encoder with contrastive hyperbolic objectives, 4) Pre-compute and store all concept embeddings, 5) At inference: embed query, score, rank, return top-k
- **Design tradeoffs**: Full ontology vs. fragment (coverage vs. accuracy), d_κ vs. s(C⊑D) (simplicity vs. depth-bias tuning), HiT vs. OnT (simplicity vs. EL reasoning complexity)
- **Failure signatures**: Median rank >> 1 (embedding collapse), good H@5 but poor H@1 (specificity issues), SBERT outperforms OnT (training failure or fragment mismatch)
- **First 3 experiments**: 1) Reproduce single-target results on released 50-query dataset, 2) Ablate λ in s(C⊑D) with sweeps, 3) Test generalization by training on one branch and evaluating on another

## Open Questions the Paper Calls Out

- How does the performance of the OnT retrieval model change when evaluated on a larger dataset annotated by domain experts? The authors state the primary limitation is the "evaluation dataset size (50 samples)" and propose increasing dataset size with domain-expert annotators.

- What training strategies can effectively map large-scale ontologies to hyperbolic space without the performance degradation observed in full-model training? The paper notes miniature model outperforms full model and calls for investigating improvements to training strategies for effectively mapping ontologies at scale.

- To what extent does the hyperbolic retrieval approach generalize to ontologies outside of the biomedical domain? The authors claim the approach is "generalisable and can be extended to other ontologies," but all experiments are restricted to SNOMED CT.

## Limitations
- Evaluation dataset of 50 queries is relatively small for comprehensive validation
- Manual annotation introduces potential subjectivity in relevance judgments
- Approach depends on semantic branch selection, failing for queries spanning multiple domains
- Hyperbolic embedding training requires significant computational resources and careful hyperparameter management

## Confidence
- **High**: Performance improvements of OnT Mini over baselines (MRR 0.67-0.68), effectiveness of depth-biased scoring, benefits of semantic fragment training
- **Medium**: Generalization claims to other ontologies, clinical decision support applications, terminology navigation improvements
- **Low**: Scalability to full SNOMED CT (350k+ concepts), robustness across diverse query distributions, real-world deployment feasibility

## Next Checks
1. **Cross-Ontology Validation**: Evaluate OnT Mini on biomedical ontologies beyond SNOMED CT (e.g., MeSH, ICD-11) to test generalizability claims
2. **Query Distribution Analysis**: Systematically vary query semantic content to quantify performance degradation when queries target untrained ontology fragments
3. **Ablation Study on λ**: Conduct comprehensive sweeps of the depth-bias parameter λ across multiple ontologies to identify optimal ranges and assess sensitivity