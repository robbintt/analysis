---
ver: rpa2
title: 'OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction'
arxiv_id: '2512.13886'
source_url: https://arxiv.org/abs/2512.13886
tags:
- optima
- thanos
- anda
- sparsegpt
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OPTIMA introduces a one-shot post-training pruning method that
  achieves both high accuracy and computational efficiency for large language models.
  It reformulates layer-wise weight reconstruction as column-wise quadratic programs
  that share a common Hessian matrix, enabling per-column globally optimal updates.
---

# OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction

## Quick Facts
- **arXiv ID:** 2512.13886
- **Source URL:** https://arxiv.org/abs/2512.13886
- **Reference count:** 30
- **Primary result:** Achieves up to 3.97% absolute zero-shot accuracy gain while pruning 8B-parameter transformer in 40 hours on NVIDIA H100 using 60GB memory.

## Executive Summary
OPTIMA is a one-shot post-training pruning method for large language models that reformulates layer-wise weight reconstruction as column-wise quadratic programs (QPs) with a shared Hessian matrix. This structure enables per-column globally optimal updates, which are solved in parallel on accelerators using a first-order primal–dual solver. The method integrates with existing mask selectors and consistently improves zero-shot accuracy across multiple model families and sparsity regimes. On an NVIDIA H100, OPTIMA can prune an 8B-parameter transformer in 40 hours using 60GB of memory.

## Method Summary
OPTIMA addresses the weight reconstruction problem in post-training pruning by formulating it as a set of column-wise quadratic programs. Each QP corresponds to a single column of the weight matrix, and all QPs share a common Hessian matrix. This shared structure allows the QPs to be solved in parallel on accelerators using a first-order primal–dual method, which is efficient and scalable. By achieving globally optimal updates for each column, OPTIMA improves the quality of the pruned model compared to approximate solvers. The method is compatible with various mask selectors and can be applied to different transformer-based architectures.

## Key Results
- Up to 3.97% absolute zero-shot accuracy gain over baseline pruning methods.
- Prunes an 8B-parameter transformer in 40 hours on an NVIDIA H100 GPU using 60GB memory.
- Consistently improves accuracy across multiple model families and sparsity regimes.

## Why This Works (Mechanism)
OPTIMA achieves optimal one-shot pruning by reformulating layer-wise reconstruction as a set of column-wise quadratic programs (QPs) that share a common Hessian matrix. This shared structure enables globally optimal per-column updates, which are solved in parallel using a first-order primal–dual solver. The method leverages the mathematical properties of quadratic optimization to ensure that each weight column is updated to minimize reconstruction error under the given sparsity constraint, leading to better accuracy retention compared to approximate solvers.

## Foundational Learning
- **Quadratic Programming (QP):** A class of optimization problems where the objective function is quadratic and constraints are linear. *Why needed:* Core to OPTIMA’s column-wise reconstruction approach. *Quick check:* Verify objective function is convex (positive semi-definite Hessian).
- **Primal–Dual Optimization:** A method for solving optimization problems by considering both primal and dual formulations. *Why needed:* Enables efficient parallel solution of multiple QPs. *Quick check:* Monitor duality gap convergence.
- **Hessian Matrix:** Square matrix of second-order partial derivatives of a scalar-valued function. *Why needed:* Shared Hessian structure enables efficient parallel QP solving. *Quick check:* Confirm positive semi-definiteness for convexity.
- **Column-wise Reconstruction:** Reconstructing each column of a weight matrix independently. *Why needed:* Breaks large QP into smaller, parallelizable subproblems. *Quick check:* Verify reconstruction error per column.
- **Mask Selectors:** Algorithms that choose which weights to prune. *Why needed:* OPTIMA is agnostic to the mask selection method. *Quick check:* Test compatibility with multiple selectors.

## Architecture Onboarding

**Component Map:**
Mask Selector -> Column-wise QP Formulation -> Shared Hessian Matrix -> Parallel Primal–Dual Solver -> Reconstructed Weight Matrix

**Critical Path:**
Mask selection → QP formulation per column → Shared Hessian construction → Parallel solver execution → Weight matrix reconstruction

**Design Tradeoffs:**
- Global optimality vs. computational overhead (shared Hessian enables parallelism but requires extra memory).
- One-shot pruning vs. iterative refinement (trades retraining for upfront computation).
- Compatibility with mask selectors vs. integration complexity (agnostic to mask choice but needs QP solver).

**Failure Signatures:**
- Memory exhaustion due to large shared Hessian storage.
- Solver divergence if Hessian is not positive semi-definite.
- Accuracy loss if mask selector produces overly aggressive sparsity.

**First Experiments:**
1. Validate QP solution quality on a small toy model (e.g., 1M parameters) and compare to approximate solvers.
2. Benchmark memory usage and runtime scaling with increasing model size (e.g., 1B, 4B, 8B parameters).
3. Test accuracy retention across multiple sparsity levels (e.g., 50%, 70%, 90% sparsity) and mask selectors.

## Open Questions the Paper Calls Out
None.

## Limitations
- Memory usage (60GB for 8B parameters) may become prohibitive for larger models.
- Runtime (40 hours) could be sensitive to hardware specifics and sparsity patterns.
- Scalability to models beyond 8B parameters is not experimentally validated.
- Generalization to non-transformer architectures and diverse downstream tasks is not demonstrated.

## Confidence
- **High confidence:** Correctness of quadratic programming reformulation and shared Hessian insight.
- **Medium confidence:** Computational efficiency and memory scaling claims.
- **Low confidence:** Long-term generalizability and practical applicability to much larger models or diverse sparsity patterns without further empirical validation.

## Next Checks
1. Benchmark OPTIMA on transformer models > 8B parameters to confirm memory and runtime scaling claims.
2. Perform controlled ablation studies to isolate the impact of shared Hessian on solution quality versus solver speed.
3. Test integration with a broader range of mask selectors (including highly sparse and unstructured patterns) to verify robustness across sparsity regimes.