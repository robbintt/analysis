---
ver: rpa2
title: Non-Linear Counterfactual Aggregate Optimization
arxiv_id: '2509.03438'
source_url: https://arxiv.org/abs/2509.03438
tags:
- policy
- outcome
- learning
- policies
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for optimizing non-linear functions
  of aggregated outcomes in offline contextual bandits, addressing the limitations
  of traditional expected reward maximization. The core idea is to leverage concentration
  properties of sums of individual outcomes and use a Gaussian approximation via the
  Central Limit Theorem to derive a scalable descent algorithm that directly optimizes
  the stated non-linear objective.
---

# Non-Linear Counterfactual Aggregate Optimization

## Quick Facts
- **arXiv ID**: 2509.03438
- **Source URL**: https://arxiv.org/abs/2509.03438
- **Reference count**: 14
- **Primary result**: A method for optimizing non-linear functions of aggregated outcomes in offline contextual bandits using Gaussian approximation and a scalable descent algorithm.

## Executive Summary
This paper introduces a method for optimizing non-linear functions of aggregated outcomes in offline contextual bandits, addressing the limitations of traditional expected reward maximization. The core idea is to leverage concentration properties of sums of individual outcomes and use a Gaussian approximation via the Central Limit Theorem to derive a scalable descent algorithm that directly optimizes the stated non-linear objective. This allows targeting criteria such as maximizing the probability of exceeding a performance threshold, which can be more appropriate than maximizing expected payoff in certain applications like A/B testing. Experiments on synthetic data show that directly optimizing the probability of improvement yields robust policies with high average rewards and better median outcomes compared to standard IPS and Logarithmic Smoothing baselines, while IPS often produces unreliable, overconfident policies.

## Method Summary
The method addresses counterfactual policy optimization with non-linear objectives by approximating the distribution of aggregate outcomes using a Gaussian distribution derived from the Central Limit Theorem. The algorithm optimizes policies by directly maximizing the stated non-linear objective (such as probability of improvement) rather than maximizing expected reward and then transforming it. A key technical contribution is the gradient estimation procedure that works with the Gaussian approximation, enabling efficient optimization. The approach uses a logit transformation to bound importance weights, preventing extreme values that could destabilize the optimization process.

## Key Results
- Directly optimizing probability of improvement yields robust policies with high average rewards and better median outcomes
- Standard IPS methods often produce unreliable, overconfident policies that fail to deliver consistent performance
- Logarithmic Smoothing baseline performs better than IPS but still underperforms the proposed method in most metrics

## Why This Works (Mechanism)
The method works by exploiting the concentration properties of sums of individual outcomes, which allows approximating the distribution of aggregate outcomes with a Gaussian distribution. This approximation enables tractable optimization of non-linear objectives by providing a differentiable surrogate for the true objective. The logit transformation bounds importance weights, preventing extreme values that could destabilize the optimization process. By directly optimizing the stated non-linear objective rather than maximizing expected reward and then transforming it, the method avoids the compounding of approximation errors that can occur in two-stage approaches.

## Foundational Learning

**Counterfactual Policy Optimization**: A framework for learning policies from historical data without running experiments in the real world. Needed to understand the context and motivation for offline learning methods.

**Importance Sampling**: A technique for estimating expectations under a different distribution than the one that generated the data. Quick check: Can be used to estimate what would have happened under a different policy using observed data from the logging policy.

**Central Limit Theorem**: States that the distribution of sample means approaches a normal distribution as sample size increases, regardless of the population distribution. Needed to justify the Gaussian approximation of aggregate outcomes. Quick check: Applies when individual outcomes are independent and identically distributed.

**Function Transformation**: Converting one objective function to another through mathematical operations. Quick check: Maximizing a transformed version of expected reward is not equivalent to directly optimizing the transformed objective due to Jensen's inequality.

## Architecture Onboarding

**Component Map**: Data -> Reward Estimator -> Importance Weights -> Gaussian Approximation -> Gradient Estimation -> Policy Update

**Critical Path**: The algorithm requires accurate reward estimation, followed by importance weight calculation, Gaussian approximation of aggregate outcomes, gradient estimation for the non-linear objective, and iterative policy updates.

**Design Tradeoffs**: The Gaussian approximation trades accuracy for computational tractability, enabling scalable optimization of non-linear objectives. The logit transformation prevents extreme importance weights but may introduce bias in certain scenarios.

**Failure Signatures**: Convex utility functions or extremely high thresholds can cause the method to fail dramatically. The Gaussian approximation may break down for small sample sizes or heavy-tailed distributions.

**3 First Experiments**: 1) Validate Gaussian approximation accuracy on synthetic data with known reward distributions. 2) Compare performance on different non-linear objectives (probability of improvement, conditional value-at-risk). 3) Test sensitivity to reward estimator quality by varying the amount of training data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific theoretical conditions does the Gaussian approximation yield valid convergence guarantees for the proposed descent algorithm?
- Basis in paper: [explicit] The authors state they "plan to investigate conditions under which the algorithm possesses theoretical guarantees."
- Why unresolved: The paper currently relies on empirical validation on synthetic data and the intuitive justification of the Central Limit Theorem (CLT) without formal proof.
- What evidence would resolve it: A derivation of convergence rates or regret bounds that explicitly account for the approximation error introduced by the CLT and the properties of function $j$.

### Open Question 2
- Question: How can the optimization framework be extended to handle multidimensional outcomes, such as those involving budget constraints?
- Basis in paper: [explicit] The conclusion notes that the algorithm "can be extended to outcomes that are multidimensional, which allows to account to, for instance, budget constraints."
- Why unresolved: The current method is derived for a scalar sum of rewards ($H_\theta$), and the gradient estimation in Algorithm 1 is specific to the univariate Gaussian distribution.
- What evidence would resolve it: A modified gradient estimator and optimization procedure that jointly models the covariance of multiple aggregate outcomes.

### Open Question 3
- Question: How does the method perform in the identified failure modes involving convex criteria or extremely high thresholds?
- Basis in paper: [inferred] The authors admit it is "easy to design situation where this approach will dramatically fail (convex $j$ or extremely high threshold)."
- Why unresolved: The paper argues that risk aversion prevents large importance weights, but does not analyze the boundaries where this assumption breaks down.
- What evidence would resolve it: An empirical or theoretical analysis of the method's stability and variance control when optimizing convex utility functions or targeting improbable extremes of the reward distribution.

## Limitations

- The Gaussian approximation's accuracy for finite sample sizes and heavy-tailed reward distributions remains unclear
- Performance guarantees hinge on reward estimator quality, but sensitivity to estimation error is not thoroughly analyzed
- Experiments are limited to synthetic data, leaving real-world robustness and scalability uncertain

## Confidence

- **High**: The conceptual motivation for non-linear aggregate objectives and their relevance to practical applications
- **Medium**: The theoretical foundation of the Gaussian approximation and the convergence properties of the proposed algorithm
- **Low**: The empirical robustness and generalization of the method to real-world, high-dimensional data

## Next Checks

1. Conduct experiments on benchmark real-world offline contextual bandit datasets (e.g., from the Open Bandit Dataset or Yahoo! Learning to Rank) to assess scalability and robustness.
2. Perform sensitivity analyses on the impact of reward estimator quality and potential misspecification on the final policy's performance.
3. Quantify and compare the computational efficiency (wall-clock time, memory usage) of the proposed method against strong baselines across varying dataset sizes and dimensionalities.