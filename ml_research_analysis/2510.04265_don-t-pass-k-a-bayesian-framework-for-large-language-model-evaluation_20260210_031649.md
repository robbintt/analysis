---
ver: rpa2
title: 'Don''t Pass@k: A Bayesian Framework for Large Language Model Evaluation'
arxiv_id: '2510.04265'
source_url: https://arxiv.org/abs/2510.04265
tags:
- arxiv
- convergence
- pass
- urlhttps
- aime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Bayesian evaluation framework that replaces
  the widely used Pass@k metric for assessing large language model (LLM) reasoning
  performance. The authors address the instability and unreliability of Pass@k, particularly
  when the number of trials is limited and computational resources are constrained.
---

# Don't Pass@k: A Bayesian Framework for Large Language Model Evaluation

## Quick Facts
- arXiv ID: 2510.04265
- Source URL: https://arxiv.org/abs/2510.04265
- Reference count: 40
- Primary result: Bayesian evaluation achieves faster convergence and greater rank stability than Pass@k for LLM reasoning tasks

## Executive Summary
This paper introduces a Bayesian evaluation framework that addresses the instability and unreliability of the widely-used Pass@k metric for assessing large language model reasoning performance. The authors demonstrate that their method produces more stable model rankings with faster convergence across four math reasoning benchmarks, while providing principled uncertainty quantification through closed-form posterior expressions. The framework naturally extends to graded evaluations and offers transparent decision rules for determining statistically meaningful performance differences.

## Method Summary
The method models evaluation outcomes as categorical variables with a Dirichlet prior, yielding closed-form posterior means and credible intervals for any weighted rubric. For binary outcomes under uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy but provides additional uncertainty quantification. The framework computes exact posterior distributions without relying on CLT approximations, making it particularly suitable for scenarios with limited computational resources. Implementation requires counting per-question category outcomes, updating Dirichlet parameters, and computing posterior statistics via Algorithm 1.

## Key Results
- Bayesian approach achieves faster convergence to correct rankings with fewer samples than Pass@k and its variants
- Framework provides transparent decision rules for determining when observed performance differences are statistically meaningful
- In simulations with known ground-truth success rates, Bayesian procedure shows superior convergence rates compared to alternative methods
- Hundreds of trials may be required to distinguish models with very similar performance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework produces faster, more stable model rankings than Pass@k by using Bayesian posterior estimates with closed-form expressions.
- Mechanism: Per-question outcomes are modeled as a categorical distribution over C+1 categories. A Dirichlet prior is placed over category probabilities. Observed trial outcomes update this to a Dirichlet posterior via count aggregation. The posterior mean is a Bayesian optimal estimator for the weighted rubric score, minimizing quadratic loss. The update is analytic, so uncertainty is available immediately without bootstrapping.
- Core assumption: Per-question outcomes are multinomial-distributed with a latent probability vector; the Dirichlet is a conjugate prior.
- Evidence anchors:
  - [abstract]: "Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty."
  - [section 2.4]: µ(R) and σ²(R) are exact closed-form expressions computed via Algorithm 1.
  - [corpus]: Weak direct support for this exact setup; neighbor "Toward Ethical AI Through Bayesian Uncertainty in Neural Question Answering" discusses Bayesian uncertainty broadly.
- Break condition: If outcomes have strong sequential dependencies or a non-conjugate prior is required, closed-form updates fail.

### Mechanism 2
- Claim: Under a uniform prior, Bayesian ranking order-equivalence to average accuracy explains robustness while adding principled uncertainty.
- Mechanism: With uniform Dirichlet prior and binary outcomes, µ is a positive affine transformation of average accuracy. Rankings (order) are identical, but the Bayesian framework adds calibrated credible intervals without CLT approximations.
- Core assumption: Uniform prior; binary or categorical outcomes with a weight vector.
- Evidence anchors:
  - [abstract]: "Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass@1)."
  - [section 2.6]: Proves µ = A + (N/(1+C+N))·a, so µ > µ' iff a > a'.
  - [corpus]: Neighbor "Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred Datapoints" cautions against CLT, supporting Bayesian intervals.
- Break condition: Non-uniform priors break the affine relationship; rankings may differ.

### Mechanism 3
- Claim: Non-overlapping credible intervals provide a transparent decision rule for statistically significant differences.
- Mechanism: For large M, the posterior over the performance metric is approximately Gaussian N(µ, σ²). The difference between two models is also Gaussian with variance σ² + σ'². The z-score determines ranking confidence; declare significance only when intervals don't overlap.
- Core assumption: M is large enough for Gaussian approximation.
- Evidence anchors:
  - [abstract]: "yields stable rankings and a transparent decision rule for differences."
  - [section 2.5]: Derives Gaussian approximation and z-score-based confidence.
  - [corpus]: Neighbor "Don't Use the CLT in LLM Evals..." advocates exact/Bayesian intervals for small-n settings.
- Break condition: Very small M may violate Gaussian approximation; exact sampling may be needed.

## Foundational Learning

- Concept: Dirichlet distribution as a conjugate prior for multinomials.
  - Why needed here: Explains why closed-form posterior updates exist after counting categories.
  - Quick check question: Under uniform prior (α=1), if you observe 3 "correct" and 2 "wrong" in 5 trials, what are the posterior Dirichlet parameters?

- Concept: Bayesian credible intervals vs. frequentist confidence intervals.
  - Why needed here: Decision rules rely on credible intervals from the posterior.
  - Quick check question: Under Beta(2,5), what is the 95% equal-tailed credible interval?

- Concept: Kendall's τ rank correlation.
  - Why needed here: Convergence is measured via τ between rankings at trial n and gold standard.
  - Quick check question: For rankings [1,2,3] vs [1,3,2], is the pair (2,3) concordant or discordant?

## Architecture Onboarding

- Component map: Input matrix R -> Tallying per-question counts -> Dirichlet posterior update -> Compute µ and σ via Algorithm 1 -> Gaussian approximation -> z-score comparison -> Significance decision
- Critical path: Accurate categorization → count aggregation → exact posterior → interval-based decisions. Rubric errors propagate directly.
- Design tradeoffs:
  - Uniform vs. informative prior: Uniform is conservative; informative priors can accelerate convergence but risk bias.
  - Binary vs. multi-category rubrics: Binary is simplest; multi-category captures partial credit but needs careful weighting.
  - Decision threshold (z=1.645 vs. 1.96): Higher thresholds reduce false positives but increase sample requirements.
- Failure signatures:
  - Wild ranking fluctuations at small N: insufficient samples; intervals remain wide.
  - No convergence within trial budget: underlying differences too small; accept ties or increase N.
  - Prior dominates: D too large relative to N; posterior barely updates.
- First 3 experiments:
  1. Baseline replication on AIME'24 with uniform prior and binary rubric; verify order-equivalence to avg@N and faster τ convergence vs Pass@2/4/8.
  2. Prior sensitivity test: use R⁰ from an older model (D=4); compare convergence speed and check for over-weighting as in Fig. 3.
  3. Categorical rubric: implement 3-category schema (wrong, correct-unboxed, correct-boxed) with weights [0,1,2]; observe rank sensitivity vs binary as in Fig. 8.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can practical guidelines for selecting the prior strength ($D$) be formalized to ensure accelerated convergence without introducing bias from misaligned historical data?
- Basis in paper: [explicit] Section 2.8 states, "One of the goals of our future work will be to establish practical guidelines for D in different real-world use cases."
- Why unresolved: The authors demonstrate that informative priors (higher $D$) can accelerate convergence but risk "impeding accurate ranking" if the prior distribution differs from the updated model's true distribution (Fig. 3).
- What evidence would resolve it: A heuristic or automated method for tuning $D$ based on a measured distance metric between the prior model's performance and the current model's initial samples.

### Open Question 2
- Question: Can the framework be extended to jointly model the uncertainty of the rubric weights or labeling process in subjective tasks?
- Basis in paper: [inferred] Section D.2 notes a key limitation: "Bayes@N does not resolve disagreement or bias in the rubric or labeling process itself... Designing good rubrics and calibrating judges remain separate modeling decisions."
- Why unresolved: The current method assumes the rubric weights ($w$) and categories are deterministic, which may not hold for subjective evaluations (e.g., safety) where label definitions are contested.
- What evidence would resolve it: A hierarchical extension that treats rubric weights as random variables, successfully quantifying the compounding uncertainty from both finite samples and ambiguous rubrics.

### Open Question 3
- Question: What is the optimal sequential stopping rule for this Bayesian framework to minimize computational cost while guaranteeing a specific ranking stability?
- Basis in paper: [inferred] The introduction claims the framework "naturally supports sequential/online evaluation," yet the experiments rely on fixed trial counts ($N$) and post-hoc bootstrap analysis.
- Why unresolved: The paper provides the statistical tools (credible intervals) but does not formalize an algorithmic policy for adaptively halting inference once the intervals for top models are sufficiently separated.
- What evidence would resolve it: A derived stopping criterion (e.g., based on z-score thresholds) that reduces total inference calls compared to fixed-$N$ baselines while maintaining the same Kendall's $\tau$ correlation with the gold standard.

## Limitations
- The framework assumes stable per-question performance across trials, which may not hold for highly stochastic LLMs
- Implementation requires significant computational resources (80 trials per question across 30 questions)
- Exact rubric specifications and weight vectors are not fully detailed in the paper
- Prior specification can be challenging to calibrate appropriately in real-world settings

## Confidence

**High Confidence**: The theoretical foundations of the Dirichlet-multinomial framework and the closed-form posterior expressions are well-established in Bayesian statistics. The order-equivalence proof between uniform-prior Bayesian mean and average accuracy is mathematically rigorous and clearly demonstrated.

**Medium Confidence**: The empirical convergence results across the four benchmarks show consistent patterns, but the exact numerical outcomes depend on implementation details not fully specified (prompts, rubric weights). The Gaussian approximation for large M is reasonable but may break down in edge cases.

**Low Confidence**: The practical impact of prior specification (R⁰) is theoretically sound but may be difficult to calibrate appropriately in real-world settings without extensive domain knowledge.

## Next Checks

1. **Implementation Verification**: Replicate the binary rubric case (C=1) with uniform prior and verify that rankings exactly match average accuracy while obtaining closed-form uncertainty estimates. This serves as a baseline correctness check before extending to multi-category rubrics.

2. **Prior Sensitivity Analysis**: Systematically test different prior specifications (R⁰) on AIME'25 to quantify the trade-off between convergence acceleration and potential bias introduction. This validates the framework's robustness to prior misspecification.

3. **Edge Case Stress Testing**: Evaluate the framework's behavior on LLMs with extreme performance patterns (near-perfect or near-chance accuracy) to assess whether the Gaussian approximation breaks down and whether exact sampling methods become necessary.