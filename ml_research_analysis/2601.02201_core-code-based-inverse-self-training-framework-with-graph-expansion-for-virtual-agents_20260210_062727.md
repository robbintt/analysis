---
ver: rpa2
title: 'CORE: Code-based Inverse Self-Training Framework with Graph Expansion for
  Virtual Agents'
arxiv_id: '2601.02201'
source_url: https://arxiv.org/abs/2601.02201
tags:
- task
- trajectory
- graph
- label
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORE is a Code-based Inverse Self-Training Framework with Graph
  Expansion designed to bridge the gap between imitation and exploration for training
  multimodal virtual agents. It addresses the limitations of Behavior Cloning (low
  behavioral diversity) and Reinforcement Learning (reliance on manual reward design)
  by introducing Semantic Code Abstraction to automatically infer executable reward
  functions (Label Functions) from expert demonstrations, Strategy Graph Expansion
  to enhance in-domain behavioral diversity via multi-path exploration, and Trajectory-Guided
  Extrapolation to enrich out-of-domain diversity by leveraging both successful and
  failed trajectories.
---

# CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents

## Quick Facts
- arXiv ID: 2601.02201
- Source URL: https://arxiv.org/abs/2601.02201
- Reference count: 40
- Primary result: CORE significantly improves generalization and performance of multimodal virtual agents by bridging imitation and exploration through Semantic Code Abstraction, Strategy Graph Expansion, and Trajectory-Guided Extrapolation

## Executive Summary
CORE introduces a novel self-training framework that addresses the key limitations of existing approaches for training multimodal virtual agents. Traditional Behavior Cloning suffers from low behavioral diversity while Reinforcement Learning requires manual reward design. CORE overcomes these challenges by automatically inferring executable reward functions from expert demonstrations, enhancing in-domain diversity through multi-path exploration, and enriching out-of-domain diversity by leveraging both successful and failed trajectories. The framework demonstrates substantial performance improvements across multiple evaluation platforms.

## Method Summary
CORE operates through a three-component framework that integrates Semantic Code Abstraction, Strategy Graph Expansion, and Trajectory-Guided Extrapolation. The approach begins by extracting executable reward functions (Label Functions) from expert demonstrations, eliminating the need for manual reward design. It then constructs a strategy graph that captures the relationship between actions and states, enabling systematic exploration of alternative action sequences. Finally, the framework leverages both successful and failed trajectories to enrich behavioral diversity across in-domain and out-of-domain scenarios. This integrated approach enables virtual agents to learn from expert demonstrations while maintaining high behavioral diversity and generalization capability.

## Key Results
- CORE achieves steady improvements across training iterations on both VisualWebArena and AndroidWorld platforms
- Significant gains in generalization scores: from 6.93 to 11.31 on VisualWebArena and from 2.86 to 14.29 on AndroidWorld
- Outperforms Behavior Cloning, Reinforcement Learning, and prior self-training methods across evaluation metrics

## Why This Works (Mechanism)
CORE bridges the gap between imitation learning and exploration by automatically deriving reward functions from demonstrations rather than requiring manual specification. The strategy graph expansion enables systematic exploration of alternative paths while maintaining connection to expert behavior. By incorporating both successful and failed trajectories, the framework enriches behavioral diversity without sacrificing performance. This combination allows agents to explore novel strategies while remaining grounded in demonstrated successful behaviors.

## Foundational Learning
- **Semantic Code Abstraction**: Automatically infers executable reward functions from demonstrations; needed to eliminate manual reward design; quick check: verify reward functions align with expert behavior
- **Strategy Graph Construction**: Maps relationships between actions and states; needed for systematic exploration; quick check: validate graph captures key decision points
- **Trajectory Analysis**: Leverages both success and failure cases; needed for comprehensive behavioral diversity; quick check: confirm both trajectory types contribute to learning
- **Self-Training Dynamics**: Iteratively refines agent policies; needed for continuous improvement; quick check: monitor convergence across iterations
- **Multimodal Integration**: Handles multiple input modalities; needed for real-world applicability; quick check: test across different input types
- **Generalization Assessment**: Evaluates performance on unseen tasks; needed to measure true learning capability; quick check: validate performance transfer to new scenarios

## Architecture Onboarding

Component Map: Semantic Code Abstraction -> Strategy Graph Expansion -> Trajectory-Guided Extrapolation

Critical Path: Demonstrations → Code Abstraction → Graph Construction → Policy Refinement → Generalization

Design Tradeoffs: Automatic reward inference vs. manual specification; exploration diversity vs. stability; computational complexity vs. performance gains

Failure Signatures: Limited diversity in generated behaviors; poor generalization to novel scenarios; unstable learning across iterations

First Three Experiments:
1. Validate Semantic Code Abstraction by testing whether automatically inferred rewards match human-designed rewards
2. Test Strategy Graph Expansion by measuring behavioral diversity gains in controlled environments
3. Evaluate Trajectory-Guided Extrapolation by comparing performance with and without failure trajectory incorporation

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to only two specific platforms (VisualWebArena and AndroidWorld)
- Lack of detailed ablation studies showing individual component contributions
- Insufficient statistical analysis of performance variations across multiple runs

## Confidence
Performance Claims: Medium
Methodological Claims: Low-Medium
Comparative Claims: Medium

## Next Checks
1. Conduct extensive ablation studies to isolate and quantify individual contributions of each framework component
2. Test CORE's performance across a wider range of platforms and tasks beyond the current evaluation domains
3. Perform detailed statistical analysis including confidence intervals and significance testing across multiple runs and iterations