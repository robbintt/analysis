---
ver: rpa2
title: Consistency of Feature Attribution in Deep Learning Architectures for Multi-Omics
arxiv_id: '2507.22877'
source_url: https://arxiv.org/abs/2507.22877
tags:
- features
- data
- shap
- layer
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the consistency of feature attribution
  in deep learning models applied to multi-omics data using Shapley Additive Explanations
  (SHAP). The authors evaluate how SHAP rankings of biomolecules vary across different
  model architectures and random initializations.
---

# Consistency of Feature Attribution in Deep Learning Architectures for Multi-Omics

## Quick Facts
- arXiv ID: 2507.22877
- Source URL: https://arxiv.org/abs/2507.22877
- Reference count: 0
- Primary result: SHAP rankings are sensitive to model architecture and random initialization, making feature identification unreliable without multi-run validation.

## Executive Summary
This study investigates the consistency of Shapley Additive Explanations (SHAP) for identifying important biomolecules in multi-omics deep learning models. The authors find that SHAP feature rankings vary significantly across different network architectures and random weight initializations. While SHAP can effectively identify important features for classification and clustering tasks, the rankings lack reliability when used for feature identification in multi-view deep learning models. The study recommends obtaining SHAP scores from multiple training runs to assess robustness before using them for biomarker identification.

## Method Summary
The authors evaluate SHAP consistency using multi-view neural networks that process proteomics, lipidomics, and metabolomics data through separate subnetworks before fusion. They train models on H7N9 and H1N1 infection datasets, compute SHAP values using DeepExplainer, and analyze ranking consistency across 10 training runs with different random seeds. The study tests various architectural configurations including different layer sizes and combination schemes (averaging vs concatenation), and introduces noise features to assess robustness. Feature importance is validated through downstream classification using Random Forests on subsets of top-ranked features.

## Key Results
- SHAP rankings are highly sensitive to architectural choices, with Kendall's Tau correlations varying significantly across different layer sizes and fusion strategies
- Random weight initialization causes substantial variation in SHAP rankings, with some features showing rank changes of 50+ positions across training runs
- Adding noise features to one view can distort SHAP rankings of real features, but dynamic layer sizing only partially stabilizes rankings at the cost of predictive performance
- Multi-run aggregation of SHAP scores provides a simple method to identify consistently important biomolecules, though this increases computational cost

## Why This Works (Mechanism)

### Mechanism 1
SHAP values identify features that contribute to model predictions, but rankings vary across training runs and architectures. Shapley values approximate each feature's marginal contribution to the prediction (treated as the "payout" from game theory). DeepExplainer adapts this for neural networks by backpropagating activation differences from a reference input. The core assumption is that the model has learned a stable mapping from inputs to outputs that reflects meaningful structure rather than overfitting to noise. Break condition: If the model is underfit, overfit to noise, or the reference distribution is poorly chosen, SHAP attributions will reflect spurious patterns.

### Mechanism 2
Noise features in one view can distort SHAP rankings of real features, but dynamic layer sizing can partially stabilize rankings at the cost of predictive performance. Deep networks exploit any available features to reduce training error. Adding noise increases the search space; if the network has sufficient capacity, it may latch onto noise-feature combinations, diluting the attribution signal from real features. Core assumption: The network's capacity (layer sizes) is a controllable bottleneck that modulates how many spurious correlations it can learn. Break condition: If the signal-to-noise ratio is extremely low or the network is severely underparameterized, neither ranking stability nor performance will be acceptable.

### Mechanism 3
Aggregating SHAP scores across multiple training runs reveals robustness of feature importance, enabling identification of consistently high-ranked biomolecules. Random initialization and dropout induce variation in which features the model exploits. By inspecting rank distributions across runs, one can distinguish features that consistently rank high from those that are spuriously important in a single run. Core assumption: True biological drivers will show stable importance across different valid network configurations; spurious features will not. Break condition: If all training runs are unstable or the data itself lacks reproducible signal, aggregation will not help.

## Foundational Learning

- **Shapley values and SHAP approximations**: Why needed here: SHAP is the core attribution method being evaluated; understanding its game-theoretic basis clarifies why it can be sensitive to model idiosyncrasies. Quick check: Can you explain why Shapley values require a reference (background) distribution and how DeepExplainer approximates them efficiently?

- **Multi-view neural architectures**: Why needed here: The paper's architecture processes each omics type through separate subnetworks before fusion; knowing how fusion choices affect representations is essential. Quick check: What is the difference between averaging and concatenation as fusion strategies, and when might each be preferred?

- **Rank correlation metrics (Weighted Kendall's Tau)**: Why needed here: The paper quantifies ranking consistency using weighted Kendall's Tau, which emphasizes agreement at top ranks. Quick check: Why would one use a weighted rank correlation instead of a standard correlation when comparing feature importance lists?

## Architecture Onboarding

- **Component map**: Input views (omics datasets) -> Marginal models (per-view MLPs) -> Combination layer (mean/concatenation) -> Final prediction head -> Attribution module (DeepExplainer SHAP)

- **Critical path**: 1. Define view-specific architectures (layer sizes, compression ratio) 2. Train multi-view model with focal loss, monitor validation plateau 3. Compute SHAP values on training data; aggregate absolute values per feature 4. Rank features; compare across runs/architectures using weighted Kendall's Tau

- **Design tradeoffs**: Layer size vs. ranking stability (smaller layers constrain spurious learning but may hurt predictive capacity); Mean vs. concatenation fusion (mean handles missing views gracefully; concatenation preserves more information); Single-run vs. multi-run attribution (multi-run increases computational cost but provides robustness diagnostics)

- **Failure signatures**: SHAP rankings change drastically with different random seeds (high variance; consider multi-run aggregation); Adding noise features causes real-feature rankings to drop (capacity mismatch; consider rebalancing layer sizes); Subsetting to top features drastically reduces downstream classifier performance (SHAP not identifying discriminative features; reconsider model or attribution method)

- **First 3 experiments**: 1. Train same architecture 10 times with different seeds; compute SHAP rankings each time and measure pairwise weighted Kendall's Tau 2. Add controlled noise features to one view while keeping another fixed; observe ranking drift of original features under static vs. dynamic layer sizing 3. Select top p% of features by SHAP; train random forest on this subset and compare AUC to full-feature baseline

## Open Questions the Paper Calls Out

### Open Question 1
Do biomolecules with highly variable SHAP rankings across training runs lack biological plausibility compared to consistently ranked features? The authors state that "Future research towards the effectiveness of variable importance metrics could inspect the biological plausibility of features selected through visual inspection of the variation in scores across runs." This remains unresolved because the current study focused on computational metrics rather than validating selected biomolecules against known biological pathways or ground truth.

### Open Question 2
What specific architectural or algorithmic modifications can remedy the observed variation in SHAP values caused by random initialization and noise? The conclusion notes that "Further investigation into reliable methods for remedying the observed variation in SHAP values is warranted." Simple adjustments like dynamic layer sizing failed to simultaneously stabilize both attribution rankings and predictive performance.

### Open Question 3
Can layer sizing schemes be optimized to stabilize feature attribution rankings without sacrificing model generalization? The authors attempted to control rank degradation by adjusting layer sizes relative to input noise but found that this resulted in a trade-off where "stable variable attribution" led to "poor stability in actual predictive performance." The experiments showed inconsistent results, leaving the relationship between network capacity, compression, and attribution stability ambiguous.

## Limitations
- Findings are based on two specific multi-omics datasets with limited sample sizes (120 and 60 samples), raising concerns about generalizability
- The study does not exhaustively explore all possible architectural variations or alternative attribution methods
- SHAP values are computed post-training, providing no insight into training dynamics or convergence properties

## Confidence

- **High confidence**: Core finding that SHAP rankings are sensitive to architectural choices and random initialization (experimental evidence is direct and mechanism is well-established)
- **Medium confidence**: Multi-run aggregation can identify robust features (reasonable approach but lacks quantitative evidence on required runs or consistency thresholds)
- **Medium confidence**: Downstream validation results (shows feature subsets maintain reasonable performance but lacks comprehensive comparison to alternative methods)

## Next Checks

1. Replicate the SHAP consistency experiments across 3-5 additional multi-omics datasets with different biological contexts and sample sizes to test generalizability of the architectural sensitivity findings.

2. Systematically compare SHAP-based feature selection against established methods (LASSO, random forest importance, mutual information) in terms of both consistency metrics and downstream classification performance.

3. Investigate whether progressive training strategies (curriculum learning, pre-training on synthetic data) can improve SHAP consistency by promoting more stable feature representations across random initializations.