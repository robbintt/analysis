---
ver: rpa2
title: 'Winter Soldier: Backdooring Language Models at Pre-Training with Indirect
  Data Poisoning'
arxiv_id: '2506.14913'
source_url: https://arxiv.org/abs/2506.14913
tags:
- secret
- data
- training
- tokens
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an indirect data poisoning method for detecting
  if a language model has been trained on a specific dataset. The approach involves
  crafting poisoned training samples using gradient-based optimization so that the
  model learns a secret sequence (prompt and response) that never appears in the original
  training data.
---

# Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning

## Quick Facts
- arXiv ID: 2506.14913
- Source URL: https://arxiv.org/abs/2506.14913
- Reference count: 40
- Key result: Achieves p < 10^-55 detection confidence for identifying unauthorized training data usage with less than 0.005% poisoned tokens

## Executive Summary
This paper introduces an indirect data poisoning method for detecting if a language model has been trained on a specific dataset. The approach involves crafting poisoned training samples using gradient-based optimization so that the model learns a secret sequence (prompt and response) that never appears in the original training data. By analyzing the model's top-k predictions on the secret prompt, the method can detect unauthorized training with extremely high confidence using a theoretically certifiable binomial test. Experiments show that minimal poisoning (less than 0.005% of tokens) is sufficient to implant the secret without degrading model performance, making it more effective and practical than previous approaches requiring access to model logits.

## Method Summary
The method works by embedding a secret sequence into the model during pre-training through carefully crafted poisoned samples. These samples are optimized using gradient-based techniques to maximize the model's likelihood of generating the secret response when given the secret prompt. The poisoning process is designed to be imperceptible in terms of overall model performance while creating a detectable watermark. Detection relies on a binomial test that evaluates whether the model's top-k predictions on the secret prompt significantly deviate from random chance, providing theoretical guarantees about detection confidence.

## Key Results
- Achieves detection confidence of p < 10^-55 using a binomial test framework
- Requires less than 0.005% of poisoned tokens to successfully implant secret sequences
- Maintains model performance while embedding detectable watermarks
- More effective than previous methods requiring model logit access

## Why This Works (Mechanism)
The method exploits the fundamental property of neural network training where repeated exposure to specific patterns during gradient updates leads to permanent embedding of those patterns in the model's weights. By carefully crafting poisoned samples that maximize the probability of generating a specific secret sequence, the optimization process ensures this sequence becomes part of the model's learned distribution. The binomial test provides theoretical guarantees by leveraging the statistical improbability of random models consistently generating the secret sequence across multiple independent trials.

## Foundational Learning

**Gradient-based optimization** - Why needed: Enables precise control over what the model learns during training. Quick check: Verify poisoned samples increase log-likelihood of secret sequence.

**Binomial statistical testing** - Why needed: Provides theoretical guarantees for detection confidence. Quick check: Confirm p-values remain below threshold with multiple independent trials.

**Language model tokenization** - Why needed: Understanding how text is converted to numerical representations affects poisoning effectiveness. Quick check: Ensure poisoned tokens are distributed across multiple tokens of the secret sequence.

## Architecture Onboarding

**Component map:** Training data -> Gradient optimization -> Poisoned samples -> Model weights -> Secret sequence embedding

**Critical path:** Poisoned sample generation -> Model training -> Secret sequence implantation -> Detection via binomial test

**Design tradeoffs:** Minimal poisoning vs. detection confidence; performance preservation vs. watermark strength; computational cost vs. theoretical guarantees

**Failure signatures:** Poor secret sequence retention during fine-tuning; detection failure due to model architecture changes; performance degradation exceeding acceptable thresholds

**First experiments:** 1) Test secret sequence generation across different model scales, 2) Measure detection confidence with varying poisoned token percentages, 3) Evaluate robustness against standard fine-tuning procedures

## Open Questions the Paper Calls Out

None

## Limitations

- Effectiveness against model fine-tuning and distillation remains uncertain
- Performance degradation as a function of poisoned token percentage needs further exploration across diverse model architectures
- Resilience against differential privacy training and other defensive techniques is not extensively tested

## Confidence

High: Detection capability with theoretical guarantees
Medium: Practicality claims regarding minimal performance degradation and scalability
Low: Robustness against adversarial countermeasures and real-world deployment scenarios

## Next Checks

1. Test the method's effectiveness when models undergo standard fine-tuning procedures with varying learning rates and dataset sizes to assess secret sequence retention

2. Evaluate performance degradation as a function of poisoned token percentage across multiple model scales (from 350M to 7B parameters) to establish practical limits

3. Conduct experiments with differential privacy training and other defensive techniques to measure the method's resilience against common data protection mechanisms