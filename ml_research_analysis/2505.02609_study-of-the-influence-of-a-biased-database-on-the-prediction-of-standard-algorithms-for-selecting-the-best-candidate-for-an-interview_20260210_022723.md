---
ver: rpa2
title: Study of the influence of a biased database on the prediction of standard algorithms
  for selecting the best candidate for an interview
arxiv_id: '2505.02609'
source_url: https://arxiv.org/abs/2505.02609
tags:
- threshold
- biased
- figure
- each
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the impact of biased training data on the
  performance of standard machine learning algorithms in recruitment. Five algorithms
  (logistic regression, AIC logistic regression, L-nearest neighbors, multilayer perceptron,
  SVM) were trained on simulated recruitment data with varying levels of discrimination
  bias (binary and continuous thresholds) and self-censorship effects.
---

# Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview

## Quick Facts
- arXiv ID: 2505.02609
- Source URL: https://arxiv.org/abs/2505.02609
- Reference count: 7
- Five standard ML algorithms tested for recruitment bias mitigation

## Executive Summary
This study investigates how biased training data affects the performance of standard machine learning algorithms in recruitment contexts. The research demonstrates that algorithms trained on historically biased recruitment data significantly underperform compared to those trained on unbiased datasets, with accuracy dropping from approximately 97% to as low as 55% under severe bias conditions. The study examines various bias types including binary and continuous discrimination thresholds, as well as self-censorship effects where candidates avoid applying to positions they perceive as discriminatory.

The findings reveal that anonymizing candidate data only improves predictions when discrimination variables are weakly correlated with other variables, and that more sophisticated algorithms do not outperform simpler transparent methods in mitigating bias. Interestingly, the L-nearest neighbors method showed unique performance improvements when trained on biased data. These results highlight the critical importance of unbiased training data in recruitment AI systems and challenge assumptions about algorithmic sophistication as a solution to bias.

## Method Summary
The study employs a simulation-based approach to evaluate five standard machine learning algorithms - logistic regression, AIC logistic regression, L-nearest neighbors, multilayer perceptron, and SVM - in recruitment scenarios. The researchers created synthetic datasets with controlled levels of discrimination bias, varying parameters such as binary and continuous threshold discrimination, and self-censorship effects. Candidate quality was simplified to a binary classification (top vs. non-top candidates), and multiple experimental conditions were tested to assess algorithm performance under different bias scenarios. The simulation allowed precise manipulation of bias parameters while maintaining controlled experimental conditions.

## Key Results
- Algorithms trained on biased data showed accuracy reduction from ~97% to as low as 55% in severe bias conditions
- Anonymization of candidate data only improved predictions when discrimination variables were weakly correlated with other variables
- L-nearest neighbors method uniquely demonstrated improved performance when trained on biased datasets
- Complex algorithms did not outperform simpler transparent methods in bias mitigation

## Why This Works (Mechanism)
The degradation in algorithmic performance stems from the direct propagation of historical discrimination patterns encoded in training data. When algorithms learn from biased datasets, they internalize these biases as legitimate patterns, leading to systematic errors in candidate selection. The L-nearest neighbors method's unique improvement on biased data likely results from its local decision boundaries being less affected by global bias patterns. Anonymization fails to mitigate bias when protected attributes remain implicitly encoded through correlations with other features, demonstrating that simple data masking is insufficient without addressing underlying structural discrimination.

## Foundational Learning
- Bias propagation in ML systems: Historical discrimination encoded in training data directly impacts algorithmic predictions
- Why needed: Understanding how past biases affect current algorithmic decisions
- Quick check: Compare predictions on biased vs. unbiased training datasets

- Correlation between discrimination variables and other features: Anonymization effectiveness depends on variable correlations
- Why needed: Determines when de-biasing techniques will be effective
- Quick check: Calculate correlation coefficients between protected attributes and other features

- Algorithm sensitivity to training data quality: Different algorithms respond differently to biased training data
- Why needed: Identifies which methods are more robust to historical bias
- Quick check: Evaluate performance degradation across multiple algorithms

## Architecture Onboarding
Component map: Data generation -> Bias injection -> Algorithm training -> Performance evaluation -> Analysis

Critical path: Synthetic data creation → Bias parameter configuration → Algorithm training → Prediction accuracy measurement → Comparative analysis

Design tradeoffs: Simulation-based approach provides control but may lack ecological validity; binary candidate classification simplifies analysis but reduces real-world applicability

Failure signatures: Accuracy drops below 70% indicate significant bias impact; anonymization providing no benefit suggests high variable correlation

First experiments:
1. Test algorithm performance on unbiased baseline data
2. Introduce single bias parameter (binary threshold) and measure degradation
3. Compare anonymized vs. non-anonymized data under weak correlation conditions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit areas for future research include: the effectiveness of bias mitigation techniques on real-world recruitment datasets, the impact of intersectional discrimination (multiple protected attributes), and whether domain-specific knowledge can be incorporated to improve bias detection and correction.

## Limitations
- Simulation-based approach may not capture real-world recruitment complexity
- Binary candidate quality classification oversimplifies actual hiring decisions
- Focus on specific algorithms and bias types limits generalizability to other methods

## Confidence
- High confidence: Algorithms trained on biased data perform worse than those trained on unbiased data
- Medium confidence: Anonymization only helps when discrimination variables are weakly correlated with other variables
- Medium confidence: Complex algorithms don't outperform simpler methods in bias mitigation

## Next Checks
1. Validate findings using real-world recruitment datasets from multiple organizations to assess ecological validity
2. Test additional algorithms (e.g., random forests, gradient boosting) and bias types (intersectional discrimination, unconscious bias patterns)
3. Examine the impact of different data preprocessing techniques (feature engineering, resampling methods) on bias mitigation effectiveness