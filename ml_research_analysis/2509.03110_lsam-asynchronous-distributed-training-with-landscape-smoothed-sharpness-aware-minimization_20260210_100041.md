---
ver: rpa2
title: 'LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware
  Minimization'
arxiv_id: '2509.03110'
source_url: https://arxiv.org/abs/2509.03110
tags:
- lsam
- learning
- distributed
- sharpness-aware
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LSAM (Landscape-Smoothed Sharpness-Aware
  Minimization), a novel distributed optimizer that addresses the inefficiency of
  Sharpness-Aware Minimization (SAM) in large-batch training scenarios. LSAM combines
  SAM's adversarial robustness with asynchronous distributed sampling, generating
  a smoothed sharpness-aware loss landscape that eliminates synchronization bottlenecks
  and accelerates convergence.
---

# LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware Minimization

## Quick Facts
- arXiv ID: 2509.03110
- Source URL: https://arxiv.org/abs/2509.03110
- Reference count: 40
- This paper introduces LSAM, an asynchronous distributed optimizer that combines Sharpness-Aware Minimization with sampling parallelism to achieve faster convergence and better generalization in large-batch training scenarios.

## Executive Summary
LSAM addresses the inefficiency of Sharpness-Aware Minimization (SAM) in distributed settings by introducing sampling parallelism that bypasses batch-size sensitivity. The method generates a smoothed sharpness-aware loss landscape through kernel convolution, enabling both wide and deep minima seeking. By decoupling the fast sampling loop from the slow optimization loop, LSAM eliminates synchronization bottlenecks while maintaining SAM's generalization advantages, achieving superior performance across multiple architectures and datasets with faster convergence compared to data-parallel SAM and other distributed optimizers.

## Method Summary
LSAM reformulates SAM as a distribution-shaping mechanism rather than a pure optimizer, using sampling parallelism to overcome batch-size limitations. The method convolves SAM's sharpness-aware distribution with a Gaussian kernel to create a target distribution that balances flat and deep minima. Workers run asynchronous sampling loops independently, accumulating stochastic score estimates, while a global server aggregates gradients every nτ worker steps. The coupling term λ(xt - yt) in local updates prevents worker drift, and two-time-scale analysis proves convergence to stationary points for both constant and decaying perturbations.

## Key Results
- Consistently achieves lower final test errors compared to data-parallel SAM, LSGD, EASGD, and standard SGD across SVHN, CIFAR-10, and CIFAR-100
- Demonstrates faster convergence with 4-worker distributed training (τ=16) while maintaining SAM's generalization advantages
- Effectively scales in distributed settings without the batch-size degradation typical of data-parallel SAM
- Matches SGD convergence rates theoretically while providing superior empirical performance

## Why This Works (Mechanism)

### Mechanism 1: Sampling Parallelism Bypasses Batch Inflation
Sampling parallelism decouples SAM's effectiveness from batch size by having workers sample from conditional distributions independently rather than processing different data subsets. This increases sampling coverage without inflating batch size or diluting SAM's adversarial perturbation signal, enabling distributed scaling without SAM's usual large-batch degradation.

### Mechanism 2: Kernel Convolution Merges Flat-Minima Seeking with Deep-Minima Seeking
Convolving SAM's sharpness-aware distribution with a Gaussian kernel produces a target distribution that concentrates probability mass at minima that are both wide (generalization-friendly) and deep (low-loss). This balances SAM's tendency to suppress sharp minima regardless of depth with the need to find deep basins, achieving a smoothed landscape that is both selective and balanced.

### Mechanism 3: Two-Time-Scale Asynchronous Updates with Bounded Drift
Decoupling the sampling loop (fast, asynchronous) from the optimization loop (periodic sync) maintains convergence while eliminating per-iteration synchronization barriers. Workers run τ local sampling steps independently, with the coupling term λ(xt - yt) preventing drift from the global anchor. Convergence analysis proves worker parameters remain bounded with decaying expected squared norm.

## Foundational Learning

- **Concept: Sharpness-Aware Minimization (SAM) fundamentals**
  - Why needed: LSAM reformulates SAM from a pure optimizer into a distribution-shaping mechanism; understanding SAM's adversarial perturbation is essential
  - Quick check: Can you explain why SAM computes gradients at x + ρ·∇f(x)/||∇f(x)|| rather than at x itself, and what property of minima this promotes?

- **Concept: Langevin Dynamics / Stochastic Gradient Langevin Dynamics (SGLD)**
  - Why needed: LSAM's inner loop uses Langevin-style sampling to draw from q(x|y); understanding how injected noise + gradient descent produces samples from a target distribution is essential
  - Quick check: In SGLD, what does the injected Gaussian noise accomplish that pure gradient descent does not?

- **Concept: Two-time-scale stochastic approximation**
  - Why needed: LSAM's fast sampling loop + slow optimization loop is a two-time-scale system; convergence depends on relative step sizes and interaction between timescales
  - Quick check: Why must the "faster" process (sampling) use larger effective step sizes than the "slower" process (global optimization) for stable convergence?

## Architecture Onboarding

- **Component map:**
  Global Server -> GLOO (CPU) -> Local Servers -> NCCL (GPU) -> Workers/GPUs
  - Global server aggregates scores every nτ worker-iterations, applies optimizer step to y
  - Local servers manage thread-safe queues to prevent race conditions
  - Workers run Algorithm 2 inner loop: sampling + local score computation

- **Critical path:**
  1. Workers compute local score s(i) = ∇x log q(x|y) using current local x(i) and received global y
  2. Workers update x(i) ← Sampler(x(i), s(i), η) for τ steps
  3. After τ steps per worker (nτ total), local servers collect all x(i) - y differences
  4. Global server computes g'y = (1/nτ) Σ difference terms, applies momentum: g ← g'y + β(g'y - g'y_prev)
  5. Global server updates y ← Optimizer(y, g, η'), broadcasts new y

- **Design tradeoffs:**
  - Sync interval τ: Larger τ → less communication overhead but more worker drift; paper uses τ = 16
  - Perturbation radius ρ: Controls SAM's flatness-seeking strength; ρ = 0.1 works across experiments
  - Pulling coefficient λ: Anchoring strength preventing drift; λ = λ0/(ητ) with λ0 ∈ {0.1, 0.9}
  - Kernel choice: Gaussian kernel with unspecified bandwidth; other kernels possible but require verifying Assumption 2 conditions

- **Failure signatures:**
  - Divergent worker drift: If λ too small or τ too large, ||x - y|| grows unbounded; symptom: test error oscillates or degrades after initial improvement
  - Stale gradient corruption: If workers fall badly out of sync, aggregated gradient becomes noisy; symptom: training instability spikes at sync boundaries
  - Batch normalization desynchronization: Paper doesn't sync BN stats; if architecture heavily relies on BN, this may cause train-test discrepancy

- **First 3 experiments:**
  1. Single-worker baseline (n=1): Run LSAM with τ = 16 on CIFAR-10/ResNet-20. Verify convergence matches paper's curves. This isolates the algorithm from distributed complexity.
  2. Ablation on τ: With n=4 workers, sweep τ ∈ {4, 8, 16, 32}. Plot final test error vs. τ and wall-clock time vs. τ. Confirm τ = 16 is near optimal for your hardware.
  3. Compare async vs. sync SAM: Run data-parallel SAM (τ=1, per-iteration sync) vs. LSAM (τ=16 async) with identical total batch size. Measure wall-clock time to 90% of final accuracy and final test error. LSAM should win on time while maintaining comparable accuracy.

## Open Questions the Paper Calls Out

- Can the communication overhead of LSAM be reduced below the 2× cost of standard SAM while preserving its convergence and generalization benefits? (Remark 1 states the method still requires double communication cost)

- How does LSAM scale to distributed settings with significantly more workers (e.g., 16, 64, 128) and larger batch sizes beyond the 4-worker, batch-size-128 configuration tested? (All experiments use n=4 workers with per-worker batch size 128)

- Can the O(ρ²) neighborhood convergence bound for constant perturbations (Theorem 3) be tightened while maintaining the same assumptions? (Theorem 3 states convergence to a neighborhood of size O(ρ²))

## Limitations
- The implementation lacks fixed iteration counts for the inner sampling loop ("while not converged"), creating ambiguity that could affect convergence behavior
- Communication overhead remains double that of standard SAM despite addressing synchronization bottlenecks
- Theoretical convergence guarantees assume diminishing step sizes, but experiments use constant learning rates, limiting quantitative correspondence between theory and practice

## Confidence
- **High confidence:** The core claim that sampling parallelism eliminates SAM's batch-size bottleneck is well-supported by theoretical reasoning and the asynchronous convergence proof (Lemma 5)
- **Medium confidence:** The kernel convolution mechanism is mathematically sound but relies on assumptions about the smoothed landscape's benefits; evidence is primarily theoretical with limited ablation
- **Medium confidence:** Convergence guarantees are formally proven for decaying step sizes, but experiments use constant learning rates, creating a known practical divergence

## Next Checks
1. Implement Algorithm 1 (single-worker LSAM) and verify convergence curves on CIFAR-10/ResNet-20 for 150 epochs with ρ=0.1, η=0.2. Compare final test error and training loss trajectories against the paper's Figure 2.

2. Ablation study on communication interval τ: Run LSAM with n=4 workers and τ ∈ {4, 8, 16, 32} on CIFAR-10. Plot final test error vs. τ and wall-clock time vs. τ. Verify that τ=16 provides the claimed optimal tradeoff between async efficiency and worker drift.

3. Direct comparison of async vs. sync SAM with identical total batch size: Run data-parallel SAM (τ=1, per-iteration sync) vs. LSAM (τ=16 async) on CIFAR-10 with n×128 total batch size. Measure wall-clock time to reach 90% of final accuracy and final test error. LSAM should win significantly on time while maintaining comparable accuracy.