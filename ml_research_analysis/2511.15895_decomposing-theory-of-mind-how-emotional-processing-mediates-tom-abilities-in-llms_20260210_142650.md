---
ver: rpa2
title: 'Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities
  in LLMs'
arxiv_id: '2511.15895'
source_url: https://arxiv.org/abs/2511.15895
tags:
- cognitive
- emotional
- action
- emotion
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to decompose Theory of Mind (ToM)
  in LLMs by comparing activation patterns between steered and baseline models using
  cognitive action probes. The authors trained 45 binary linear probes on synthetic
  first-person narratives covering 45 cognitive actions, then applied Contrastive
  Activation Addition (CAA) steering to Gemma-3-4B and evaluated on 1,000 BigToM forward
  belief scenarios.
---

# Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs

## Quick Facts
- arXiv ID: 2511.15895
- Source URL: https://arxiv.org/abs/2511.15895
- Authors: Ivan Chulo; Ananya Joshi
- Reference count: 8
- Primary result: ToM improvement (32.5% to 46.7% accuracy) is mediated by increased emotional processing (emotion perception +2.23, emotion valuing +2.20) and suppressed analytical reasoning (questioning -0.78, convergent thinking -1.59)

## Executive Summary
This paper introduces a novel method to decompose Theory of Mind (ToM) capabilities in LLMs by comparing activation patterns between steered and baseline models. Using cognitive action probes trained on synthetic narratives and Contrastive Activation Addition (CAA) steering, the authors demonstrate that ToM improvements in Gemma-3-4B are driven by increased emotional processing and decreased analytical reasoning. The findings suggest that successful ToM in LLMs relies fundamentally on emotional understanding rather than analytical interrogation, providing mechanistic insight into how steering vectors improve social reasoning capabilities.

## Method Summary
The study employs 45 binary linear probes trained on 31,500 synthetic first-person narratives covering 45 cognitive actions. These probes are applied to Gemma-3-4B activations during BigToM forward belief scenario evaluation, comparing CAA-steered versus baseline conditions. CAA vectors are trained on contrastive pairs from BigToM, computing activation differences between correct and incorrect ToM completions. The approach identifies which cognitive processes are amplified or suppressed when steering improves ToM performance, revealing that emotional processing mediates the accuracy gains.

## Key Results
- ToM accuracy improved from 32.5% to 46.7% using CAA steering
- Emotional processes showed strong increases (emotion perception +2.23, emotion valuing +2.20)
- Analytical processes decreased (questioning -0.78, convergent thinking -1.59)
- Binary probes achieved 0.78 average AUC-ROC and 0.68 F1 across 45 actions
- CAA vectors were trained on layers 14-30 using PCA-centered activation differences

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Activation Addition (CAA) Captures Representational Differences
Steering vectors trained on correct vs. incorrect ToM completions shift model activations toward belief-attribution patterns. CAA computes the difference between activations paired with correct vs. incorrect belief attributions, then adds this vector during inference to bias representations toward accurate perspective-taking. Core assumption: The positive-negative completion contrast isolates ToM-relevant features rather than surface-level artifacts.

### Mechanism 2: Linear Probes Decode Cognitive Action Representations
Binary linear probes trained on synthetic narratives can detect when specific cognitive actions (e.g., "emotion perception") are active in model activations. Probes learn weights mapping activation patterns to cognitive action labels; when applied to steered vs. baseline activations, they reveal which processes are amplified or suppressed. Core assumption: First-person narrative training data elicits cognitive-action-specific activations that generalize to ToM tasks.

### Mechanism 3: Emotional Processing Mediates ToM Improvement (Correlational)
Steering-induced ToM improvements correlate with increased emotional process activation and decreased analytical process activation. Assumption—steering vectors shift the model's representational state toward emotional inference modes, which support perspective-taking more effectively than analytical interrogation. Core assumption: The correlation between emotional activation and ToM success reflects a functional relationship, not a confound.

## Foundational Learning

- **Contrastive Activation Addition (CAA)**
  - Why needed here: CAA is the intervention method; understanding how steering vectors are constructed and applied is essential to interpret results.
  - Quick check question: Given a contrastive pair (positive/negative completion), how would you compute a steering vector for layer L?

- **Linear Probing**
  - Why needed here: Probes decode which cognitive processes are active; interpreting their outputs requires understanding their training and limitations.
  - Quick check question: What does a probe AUC-ROC of 0.78 indicate about its discriminative ability, and what are failure modes?

- **Theory of Mind (ToM) Evaluation**
  - Why needed here: The BigToM benchmark and belief attribution tasks define the dependent variable; understanding their design prevents misinterpretation of accuracy gains.
  - Quick check question: In a false-belief scenario, what distinguishes a correct from an incorrect belief attribution?

## Architecture Onboarding

- **Component map**: Synthetic narratives -> Linear probes -> CAA steering vectors -> Gemma-3-4B activations -> BigToM evaluation -> Probe application -> Cognitive action analysis

- **Critical path**:
  1. Generate CAA vectors from contrastive ToM pairs
  2. Apply steering during BigToM evaluation
  3. Extract activations at question/answer timepoints
  4. Apply probes to decode cognitive action presence
  5. Compare steered vs. baseline activation patterns

- **Design tradeoffs**:
  - **Probe complexity vs. interpretability**: Linear probes are interpretable but may miss non-linear patterns; non-linear probes could improve accuracy but reduce transparency.
  - **Synthetic vs. real training data**: Synthetic narratives ensure controlled coverage but may not generalize to naturalistic ToM contexts.
  - **Layer selection**: Mid-layers (5-24) showed best probe performance; early/late layers may encode different information.

- **Failure signatures**:
  - **Probe overfitting**: High training AUC but low generalization to steered activations.
  - **Steering vector contamination**: If contrastive pairs contain artifacts (e.g., longer positive completions), steering may bias length rather than ToM.
  - **Category conflation**: Some cognitive actions (e.g., "understanding") showed lower probe performance (0.837 AUC), suggesting imprecise detection.

- **First 3 experiments**:
  1. **Reproduce baseline accuracy**: Run BigToM evaluation without steering to confirm 32.5% baseline.
  2. **Validate probe selectivity**: Apply probes to held-out narratives with known cognitive actions; verify AUC > 0.75.
  3. **Ablate steering layers**: Apply CAA only to specific layer ranges (e.g., 14-20 vs. 21-30) to identify critical steering locations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the findings that emotional processing mediates ToM performance generalize to larger model scales and different architectural families?
- **Basis in paper:** [explicit] The authors state in the Conclusion and Future Work that "Future work should validate these cognitive decomposition findings with multiple, bigger models and with additional data sources."
- **Why unresolved:** The entire study was restricted to a single model (Gemma-3-4B), leaving open the possibility that the observed reliance on emotional processing is an artifact of this specific parameter count or architecture.
- **What evidence would resolve it:** Replicating the CAA steering and probing methodology on larger models (e.g., Gemma-3-12B+) and different model families (e.g., Llama, Mistral) to confirm the mediation effect.

### Open Question 2
- **Question:** Does the observed suppression of analytical processes (e.g., questioning, convergent thinking) causally facilitate ToM reasoning, or is it an incidental byproduct of the steering intervention?
- **Basis in paper:** [inferred] The paper concludes that successful ToM "suppresses analytical interrogation" based on correlational probe data (activation differences), but it does not isolate whether this suppression is necessary for the performance boost.
- **Why unresolved:** While the data shows a correlation between improved accuracy and reduced analytical activation, the experimental design does not test whether preventing this suppression would negate the ToM improvements.
- **What evidence would resolve it:** A controlled ablation study where analytical process activations are explicitly maintained (clamped) during steering to observe if ToM performance improvements disappear.

### Open Question 3
- **Question:** Does the alignment between LLM ToM activation and human affective processing represent a genuine emulation of cognitive architecture or a functionally similar but mechanistically different convergence?
- **Basis in paper:** [explicit] The Discussion notes: "Whether this constitutes genuine emulation of cognitive architecture or emergent convergence on functionally equivalent representations remains an open question."
- **Why unresolved:** Current interpretability methods can identify *which* activations change (e.g., emotion perception increasing), but cannot determine if the underlying computational logic mirrors the neural mechanisms of human social cognition.
- **What evidence would resolve it:** Comparative analysis using representational similarity analysis (RSA) between the model's activation patterns and human neuroimaging data (fMRI/EEG) collected during matching theory of mind tasks.

### Open Question 4
- **Question:** To what extent do the linear probes trained on synthetic data capture semantic "cognitive actions" versus capturing the stylistic artifacts of the model that generated the training narratives?
- **Basis in paper:** [inferred] The Methodology section notes that all 31,500 training samples were synthetic first-person narratives generated by Gemma-3-4B, raising questions about circularity and the semantic validity of the probes.
- **Why unresolved:** If the model generating the training data associates specific cognitive actions with distinct but superficial linguistic markers (e.g., specific vocabulary or sentence structures), the probes may be detecting style rather than the intended internal process.
- **What evidence would resolve it:** Validating the probes on human-authored datasets containing the targeted cognitive actions or performing adversarial tests where stylistic markers are removed or shuffled.

## Limitations
- Mediation claim weakness: Correlation shown but not true mediation through statistical analysis
- CAA steering generalization: Effectiveness may be task-specific, trained on same benchmark
- Probe fidelity concerns: May capture surface-level lexical patterns rather than genuine cognitive representations

## Confidence
- **ToM accuracy improvement (32.5% → 46.7%)**: High confidence - Direct empirical measurement from BigToM benchmark
- **Emotional processing mediates ToM improvement**: Medium confidence - Correlation demonstrated but mediation lacks formal validation
- **CAA steering vectors isolate ToM-relevant features**: Medium confidence - Measurable effects shown but specific features unclear
- **Linear probes accurately decode cognitive actions**: Medium confidence - Performance metrics provided but semantic validity uncertain

## Next Checks
1. **Statistical mediation analysis**: Conduct formal mediation analysis (e.g., bootstrapping, Sobel test) to verify that emotional processing increases significantly account for the relationship between CAA steering and ToM improvement, controlling for potential confounds.

2. **Cross-task CAA generalization**: Apply CAA steering vectors trained on BigToM to independent ToM benchmarks (e.g., Social-Chem-101, Abductive NLI) to test whether emotional processing increases consistently mediate performance improvements across different ToM task types.

3. **Probe ablation study**: Systematically remove probe dimensions corresponding to emotional versus analytical processes to test whether emotional probes alone predict ToM accuracy improvements, establishing which probe categories are truly informative versus correlated artifacts.