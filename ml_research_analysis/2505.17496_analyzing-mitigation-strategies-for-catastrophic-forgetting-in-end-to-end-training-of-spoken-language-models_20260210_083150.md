---
ver: rpa2
title: Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training
  of Spoken Language Models
arxiv_id: '2505.17496'
source_url: https://arxiv.org/abs/2505.17496
tags:
- speech
- training
- language
- arxiv
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates catastrophic forgetting in end-to-end
  training of spoken language models (SLMs) and evaluates three mitigation strategies:
  model merging, discounting LoRA scaling factor, and experience replay. The study
  finds that experience replay is the most effective strategy, significantly reducing
  knowledge loss while maintaining performance across both text and speech modalities.'
---

# Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models

## Quick Facts
- arXiv ID: 2505.17496
- Source URL: https://arxiv.org/abs/2505.17496
- Reference count: 0
- Key outcome: Experience replay is the most effective strategy for mitigating catastrophic forgetting in SLM training, reducing knowledge loss while maintaining performance across text and speech modalities

## Executive Summary
This paper investigates catastrophic forgetting in end-to-end training of spoken language models (SLMs) when transitioning between automatic speech recognition (ASR) and text-to-speech (TTS) stages. The study evaluates three mitigation strategies: model merging, discounting LoRA scaling factor, and experience replay. Through comprehensive experiments, the authors demonstrate that experience replay significantly outperforms other approaches in preserving knowledge across training stages. When combined with other methods, experience replay achieves further performance gains. The research establishes experience replay as a robust solution for maintaining model capabilities across both text and speech modalities in SLMs.

## Method Summary
The authors evaluate three mitigation strategies for catastrophic forgetting in SLMs: model merging (combining weights from different stages), discounting LoRA scaling factor (reducing adaptation strength during training), and experience replay (mixing previous stage samples with current training data). They conduct experiments using Llama-3.2-11B as the backbone model with Qwen2.5-1.5B as the LLM component, training on 3000 hours of English speech data. The evaluation tracks performance across ASR, TTS, and Speech-to-Speech (S2S) tasks, measuring accuracy and word error rate. The experience replay strategy involves maintaining a buffer of samples from previous stages and interleaving them with current training data to prevent knowledge degradation.

## Key Results
- Experience replay reduces catastrophic forgetting most effectively, achieving 28.7% accuracy on LLaMA S2S compared to 16.7% with other methods
- Catastrophic forgetting is most severe between ASR and TTS stages, with significant performance degradation observed
- Combining experience replay with model merging shows mixed results, improving some tasks while degrading S2S performance
- All mitigation strategies help maintain text generation capabilities while preventing speech knowledge loss

## Why This Works (Mechanism)
Catastrophic forgetting occurs when neural networks overwrite previously learned knowledge during training on new tasks. In SLMs, this manifests as degradation of ASR capabilities when training transitions to TTS. The experience replay mechanism works by maintaining a buffer of samples from previous training stages and interleaving them with current training data, creating a balanced learning signal that preserves both old and new knowledge. Model merging attempts to preserve knowledge by combining weights from different stages, while discounting LoRA scaling reduces the adaptation strength to prevent overwriting. The effectiveness of experience replay suggests that balanced exposure to both task types during training is crucial for maintaining multi-modal capabilities.

## Foundational Learning
- Catastrophic forgetting: The phenomenon where neural networks lose previously acquired knowledge when learning new tasks, critical to understand for multi-stage training
  - Why needed: Explains the core problem being solved and why mitigation strategies are necessary
  - Quick check: Observe performance degradation when training on new tasks without mitigation

- Experience replay: A technique that maintains samples from previous training stages and interleaves them with current training data
  - Why needed: Provides the primary mechanism for preventing knowledge loss across training stages
  - Quick check: Compare performance with and without replay buffer during training transitions

- LoRA (Low-Rank Adaptation): A parameter-efficient fine-tuning method that adds low-rank matrices to existing model weights
  - Why needed: Used in the discounting strategy to control adaptation strength
  - Quick check: Verify that LoRA matrices can be scaled and merged with base weights

- Mixed-task training: The practice of training on multiple task types simultaneously or in alternating fashion
  - Why needed: Underlies the interleaving approach used in experience replay
  - Quick check: Ensure the model can handle alternating between different task formats

## Architecture Onboarding

**Component Map:**
ASR Model -> TTS Model -> Speech-to-Speech Model -> Experience Replay Buffer

**Critical Path:**
1. Pre-training on ASR data establishes speech-to-text capabilities
2. Fine-tuning on TTS data builds text-to-speech generation
3. Experience replay buffer maintains samples from both stages
4. Final fine-tuning on S2S data with interleaved replay samples

**Design Tradeoffs:**
- Model merging preserves exact weights but may not adapt well to new tasks
- Experience replay requires additional memory for buffer storage but provides superior performance
- Discounting LoRA reduces adaptation strength but may limit task-specific learning

**Failure Signatures:**
- Significant performance drop between ASR and TTS stages indicates catastrophic forgetting
- Poor S2S generation suggests inadequate integration of both modalities
- Inconsistent performance across tasks indicates imbalanced training

**First Experiments:**
1. Measure performance degradation when training transitions from ASR to TTS without mitigation
2. Compare single-stage versus interleaved training approaches
3. Evaluate buffer size impact on experience replay effectiveness

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do these mitigation strategies generalize to diverse SLM architectures, alternative backbone LLMs (e.g., smaller models), or training pipelines that do not rely on interleaved text-speech generation?
- Basis in paper: The conclusion explicitly states, "Future work will involve more comprehensive studies, including diverse training pipelines, models and various strategies."
- Why unresolved: The current study is restricted to a single backbone (Llama-3.2-11B) and a specific token-based architecture with interleaved generation, leaving the efficacy of replay or merging on encoder-based or smaller SLMs unknown.
- What evidence would resolve it: Evaluating the proposed experience replay and merging strategies on alternative architectures (e.g., SALMONN, Qwen-Audio) or smaller parameter models to see if the benefits of replay remain consistent.

### Open Question 2
- Question: What are the underlying mechanisms causing the most severe catastrophic forgetting to occur specifically between the ASR and TTS stages, rather than in later stages?
- Basis in paper: Section 4.1 notes, "There is a gap can be observed easily between ASR stage and TTS stage... the most serious forgetting appears in TTS stage," but the paper does not isolate whether the cause is the change in data distribution or the introduction of the word-by-word interleaved generation format.
- Why unresolved: The paper identifies the TTS stage as the critical failure point but does not ablate whether the forgetting is driven by the task switch, the data, or the specific interleaved training methodology required for TTS.
- What evidence would resolve it: An ablation study analyzing forgetting when training TTS without interleaving (if convergence allows) or when shuffling stage order, specifically monitoring gradient interference during the TTS stage.

### Open Question 3
- Question: Why does combining model merging with experience replay degrade performance in Speech-to-Speech (S2S) settings compared to experience replay alone?
- Basis in paper: Table 1 shows that while Experience Replay achieves 28.7% accuracy on LLaMA S2S, combining it with Linear Merge drops performance to 16.7%. The authors note mixed strategies help in some cases but do not explain this regression in S2S capabilities.
- Why unresolved: The paper establishes that merging helps retain text knowledge, but the interaction between merged weights and the replay buffer appears to introduce conflicts that specifically harm the model's ability to generate speech tokens.
- What evidence would resolve it: An analysis of token-level generation probabilities in the merged-vs-replay models to determine if merging re-introduces bias against speech tokens that the replay buffer had successfully reinforced.

## Limitations
- The study focuses on specific model scales (11B and 1.5B parameters) and may not generalize to larger or smaller architectures
- Limited evaluation to ASR, TTS, and S2S tasks without exploring other speech-related applications
- Does not investigate the computational overhead and memory requirements of experience replay buffers

## Confidence
High confidence in the core finding that catastrophic forgetting significantly impacts SLM performance across ASR and TTS stages, with experience replay emerging as the most effective mitigation strategy. The experimental methodology and results are robust, showing clear performance degradation without mitigation and consistent improvements with experience replay. However, Medium confidence exists regarding the generalizability of these findings to larger SLM architectures and more diverse speech tasks, as the study focuses on specific model scales and task combinations. The comparison between different mitigation strategies is well-structured, but the analysis of underlying mechanisms driving catastrophic forgetting remains limited. Additionally, while the combination of experience replay with other methods shows promise, the specific implementation details and optimal parameter settings warrant further investigation.

## Next Checks
1. Evaluate experience replay effectiveness on larger SLM architectures (1B+ parameters) to assess scalability.
2. Test the proposed mitigation strategies across a broader range of speech tasks beyond ASR and TTS to establish generalizability.
3. Conduct ablation studies to identify optimal replay buffer sizes and sampling strategies for experience replay in SLM training.