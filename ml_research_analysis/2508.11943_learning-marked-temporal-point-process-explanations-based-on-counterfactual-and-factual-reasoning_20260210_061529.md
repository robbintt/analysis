---
ver: rpa2
title: Learning Marked Temporal Point Process Explanations based on Counterfactual
  and Factual Reasoning
arxiv_id: '2508.11943'
source_url: https://arxiv.org/abs/2508.11943
tags:
- explanation
- mtpp
- counterfactual
- events
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of explaining predictions made
  by neural network-based Marked Temporal Point Process (MTPP) models, which are widely
  used in high-stakes applications. The key challenge is to identify the minimal and
  rational subset of historical events that explain why the model outputs a particular
  prediction, ensuring that the prediction accuracy based on this subset matches that
  of the full history while being better than the complement subset.
---

# Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning

## Quick Facts
- arXiv ID: 2508.11943
- Source URL: https://arxiv.org/abs/2508.11943
- Reference count: 40
- Primary result: Proposes CFF method combining counterfactual and factual reasoning for explaining MTPP predictions, achieving 6-10x speedup over baselines while maintaining explanation quality

## Executive Summary
This paper addresses the critical need for explainable predictions in neural network-based Marked Temporal Point Process (MTPP) models, which are widely deployed in high-stakes applications like social media analysis and healthcare monitoring. The key challenge is identifying minimal subsets of historical events that explain model predictions while maintaining prediction accuracy equivalent to using the full history. The authors define explanation as a combination of counterfactual (what if events were removed?) and factual (what if events were kept?) reasoning to avoid irrational explanations that arise from using either approach alone.

The proposed Counterfactual and Factual Explainer for MTPP (CFF) uses a learning-based approach with encoder-decoder transformer architecture, fully connected layers, and Gumbel-softmax sampling to differentially select events. Experiments on three real-world datasets demonstrate that CFF outperforms baseline methods in both explanation quality (measured by Optimal Transport Distance) and processing efficiency, being 6-10 times faster than greedy search approaches while consistently generating explanations with better predictive capability than random subsets of the same length.

## Method Summary
The CFF method addresses MTPP explanation by framing it as an optimization problem to find the minimal subset of historical events that preserves prediction accuracy. The method uses an encoder-decoder transformer architecture where the encoder processes the full event sequence, and the decoder, guided by counterfactual and factual reasoning constraints, selects which events to include in the explanation. The Gumbel-softmax trick enables differentiable sampling of events, allowing gradient-based optimization of the selection process. The optimization minimizes a surrogate hinge loss that balances two constraints: the explanation subset must achieve prediction accuracy comparable to the full history (factual constraint), and it must outperform the complement subset (counterfactual constraint). This dual-constraint approach prevents the selection of irrational subsets that might arise from using only one type of reasoning.

## Key Results
- CFF achieves 6-10x speedup compared to baseline greedy search methods while maintaining explanation quality
- Explanations generated by CFF consistently show better predictive capability than random subsets of equivalent length across all three datasets
- The method successfully identifies influential users in retweet activities, aligning with common sense understanding of social media dynamics
- CFF demonstrates stable performance across different hyperparameter settings, indicating robustness to parameter choices

## Why This Works (Mechanism)
The method works by leveraging the complementary strengths of counterfactual and factual reasoning. Counterfactual reasoning ensures that the selected subset is truly necessary for accurate prediction by requiring it to outperform the complement subset, while factual reasoning ensures sufficiency by requiring the subset to match the full history's predictive power. This dual-constraint approach prevents the selection of spurious correlations that might satisfy only one type of reasoning. The Gumbel-softmax trick enables differentiable sampling, allowing the model to learn which events are most informative through gradient-based optimization rather than heuristic selection rules.

## Foundational Learning

Temporal Point Processes
- Why needed: Core framework for modeling event sequences with timestamps and marks
- Quick check: Understand that events occur at specific times with associated categorical labels

Marked Temporal Point Processes
- Why needed: Extends basic point processes to handle events with additional attributes (marks)
- Quick check: Recognize that each event has both a timestamp and a categorical label

Transformer Architecture
- Why needed: Enables effective processing of sequential event data with attention mechanisms
- Quick check: Understand self-attention and positional encoding concepts

Counterfactual Reasoning
- Why needed: Evaluates what happens when certain events are removed from consideration
- Quick check: Can explain how predictions change when specific events are excluded

Factual Reasoning
- Why needed: Ensures selected events are sufficient to maintain prediction accuracy
- Quick check: Understands that the subset must perform as well as the full history

Optimal Transport Distance
- Why needed: Metric for measuring similarity between probability distributions of event sequences
- Quick check: Can explain how this metric compares the quality of different explanations

## Architecture Onboarding

Component Map: Input Sequence -> Encoder -> Decoder with Gumbel-softmax -> Event Selection -> Prediction Evaluation

Critical Path: Event sequence input flows through encoder to produce hidden representations, which the decoder uses with Gumbel-softmax sampling to select events, followed by prediction evaluation against counterfactual and factual constraints.

Design Tradeoffs: The method trades exact combinatorial optimization (exponential complexity) for differentiable approximation using Gumbel-softmax, achieving 6-10x speedup at the cost of some approximation error. The dual-constraint approach adds complexity but prevents irrational explanations that single-constraint methods might produce.

Failure Signatures: If counterfactual constraints are too weak, the method may select spurious correlations; if factual constraints are too strong, it may select overly large subsets. Poor hyperparameter tuning of the constraint balance can lead to suboptimal explanations that either miss important events or include unnecessary ones.

First Experiments:
1. Run CFF on a small synthetic dataset with known ground truth explanations to verify correct event selection
2. Compare explanation quality metrics between CFF and random selection on a validation split
3. Perform ablation study removing counterfactual or factual constraints to observe impact on explanation rationality

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies primarily on synthetic Optimal Transport Distance metrics rather than direct human judgment of explanation quality
- Performance gains come with approximation accuracy trade-offs due to Gumbel-softmax stochasticity
- Experimental validation covers only three real-world datasets, potentially limiting generalizability
- Assumes minimal subsets should maintain prediction accuracy equivalent to full history, which may not hold for all use cases

## Confidence
- High confidence: The core technical contribution of combining counterfactual and factual reasoning is well-defined and the optimization framework is sound
- Medium confidence: The experimental results showing performance improvements over baselines are reproducible, but the practical utility of explanations needs further validation
- Medium confidence: The efficiency gains are demonstrated, though real-world deployment constraints are not discussed

## Next Checks
1. Conduct a user study with domain experts to evaluate whether the generated explanations are interpretable and actionable in real-world scenarios
2. Test the method on additional diverse datasets with different event characteristics and sequence lengths to assess generalizability
3. Perform ablation studies to quantify the impact of the counterfactual-factual balance parameter on explanation quality and prediction accuracy