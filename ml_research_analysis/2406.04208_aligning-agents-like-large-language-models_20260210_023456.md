---
ver: rpa2
title: Aligning Agents like Large Language Models
arxiv_id: '2406.04208'
source_url: https://arxiv.org/abs/2406.04208
tags:
- agent
- arxiv
- agents
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes training agents in complex 3D environments using
  the same multi-stage approach as large language models (LLMs). The method combines
  large-scale unsupervised pre-training on diverse gameplay data, supervised fine-tuning
  on task-specific demonstrations, and reinforcement learning from human feedback
  (RLHF) using reward models trained from pairwise trajectory preferences.
---

# Aligning Agents like Large Language Models

## Quick Facts
- arXiv ID: 2406.04208
- Source URL: https://arxiv.org/abs/2406.04208
- Authors: Adam Jelley; Yuhan Cao; Dave Bignell; Amos Storkey; Sam Devlin; Tabish Rashid
- Reference count: 40
- Primary result: LLM-style multi-stage training pipeline successfully aligns visual agents in complex 3D environments, achieving >90% success rate on video game navigation tasks.

## Executive Summary
This paper proposes training agents in complex 3D environments using the same multi-stage approach as large language models (LLMs). The method combines large-scale unsupervised pre-training on diverse gameplay data, supervised fine-tuning on task-specific demonstrations, and reinforcement learning from human feedback (RLHF) using reward models trained from pairwise trajectory preferences. Applied to a video game navigation task from pixels, the approach successfully aligns the agent to reach a desired jump pad, achieving over 90% success rate. Key findings include: pre-training improves fine-tuning effectiveness and generalization; reward models using the agent's own representations outperform random encoders; and preference fine-tuning before online RL significantly accelerates alignment.

## Method Summary
The method employs a three-stage pipeline: (1) unsupervised pre-training on large-scale gameplay data (9,875 hours of human trajectories) using cross-entropy loss over discrete action components; (2) supervised fine-tuning on 300 curated successful trajectories; and (3) RLHF using a reward model trained from pairwise trajectory preferences. The agent architecture uses a ConvNeXt-style CNN encoder processing 128×128 RGB frames, followed by a GPT-2 style causal Transformer with 32-timestep context window. Actions are discretized into 16 components (12 buttons + 2 joystick axes in 11 buckets each). The reward model is initialized with pre-trained agent weights and trained using Bradley-Terry pairwise loss, with RL alignment performed via REINFORCE.

## Key Results
- Pre-training on diverse gameplay data significantly improves fine-tuning effectiveness and generalization compared to training from scratch
- Reward models initialized with pre-trained agent representations achieve >90% preference accuracy with only ~100 comparisons
- Preference fine-tuning before online RL accelerates alignment and improves sample efficiency
- The three-stage pipeline successfully aligns agents to specific behaviors under-represented in general data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large-scale unsupervised pre-training on diverse interaction data establishes a behavioral prior that improves robustness to distribution shift during fine-tuning.
- **Mechanism:** The paper posits that training on a large, noisy dataset (1.12 years of gameplay) allows the agent to learn common-sense recovery behaviors. When the fine-tuned agent encounters an out-of-distribution state (e.g., missing a target), the pre-trained "prior" enables it to recover (e.g., turning around) rather than executing a repetitive, failing action (e.g., running into a wall), which is the failure mode for agents trained from scratch on limited data.
- **Core assumption:** The pre-training data contains sufficient diversity to cover potential error states encountered during fine-tuning, allowing for behavioral recovery.
- **Evidence anchors:**
  - [section 3.3] States that the pre-trained agent can turn around to hit a jumppad after missing it, whereas the agent trained from scratch "runs into the wall continuously."
  - [figure 5 & 6] Visuals demonstrate the difference in failure modes between scratch-trained and pre-trained agents.
- **Break condition:** The mechanism likely fails if the pre-training data is too narrow or clean, preventing the model from learning recovery behaviors for novel error states.

### Mechanism 2
- **Claim:** Initializing the reward model (RM) with the pre-trained agent's weights enables highly feedback-efficient alignment.
- **Mechanism:** The pre-trained agent possesses established visual representations and temporal dynamics understanding. By replacing the action head with a scalar output (instead of training an RM from scratch), the model leverages these existing features to distinguish between preferred and non-preferred trajectories, achieving high accuracy with minimal human comparisons.
- **Core assumption:** The features learned for action prediction (imitation) are transferable and relevant for value prediction (preference ranking).
- **Evidence anchors:**
  - [abstract] Notes achieving "over 90% preference accuracy with only ~100 comparisons."
  - [section 3.5] Compares agent-initialized encoders against random encoders, finding the former significantly outperform the latter.
- **Break condition:** This efficiency gain may degrade if the preference task requires distinguishing subtle visual details not captured by the behavioral prior, or if the policy network is severely underparameterized.

### Mechanism 3
- **Claim:** A multi-stage pipeline (Pre-training → SFT → RLHF) is required to reliably achieve specific behaviors that are under-represented in the general data.
- **Mechanism:** Generic pre-training learns an average human distribution (multi-modal). SFT narrows this to task-relevant behaviors. RLHF (specifically REINFORCE with a learned reward model) is then used to "align" the agent toward a specific, subjective mode of that distribution (e.g., taking the *left* jumppad) which might otherwise be suppressed by the dominant mode (the *middle* jumppad).
- **Core assumption:** The reward model generalizes well enough to the online agent's behavior to prevent "reward hacking" (optimizing for high reward without fulfilling the intent).
- **Evidence anchors:**
  - [section 3.6] Demonstrates shifting the rollout distribution from a spread (Fig 4) to a targeted peak (Fig 20) using RLHF.
  - [figure 10] Visualizes the heatmap trajectory shift across the pipeline stages.
- **Break condition:** If the Supervised Fine-Tuning (SFT) dataset lacks diversity (e.g., only contains trajectories from easy spawn points), the subsequent RLHF stage may fail to generalize to harder initial states.

## Foundational Learning

- **Concept:** **Behavior Cloning (BC) / Imitation Learning**
  - **Why needed here:** The first two stages of the pipeline ("Unsupervised Pre-training" and "Supervised Fine-Tuning") are fundamentally Behavior Cloning tasks where the agent predicts the next action given a sequence of observations.
  - **Quick check question:** Can you explain why a policy trained via BC might fail if it encounters a state it never saw in the training data (covariate shift)?

- **Concept:** **Bradley-Terry Model**
  - **Why needed here:** This is the mathematical framework used to convert pairwise human preferences (Trajectory A > Trajectory B) into a scalar reward signal for training the reward model.
  - **Quick check question:** How does the Bradley-Terry model relate the difference in rewards between two trajectories to the probability of one being preferred over the other?

- **Concept:** **REINFORCE (Policy Gradient)**
  - **Why needed here:** The final alignment stage uses REINFORCE rather than more complex methods like PPO. It is used here because the reward is sparse (often evaluated only at the end of a trajectory), making simple trajectory-level reinforcement sufficient.
  - **Quick check question:** Why might REINFORCE be preferred over PPO when the reward signal is only available at the end of a trajectory rather than at every step?

## Architecture Onboarding

- **Component map:** ConvNeXt CNN (128×128×3 RGB → 1024 dim embeddings) → GPT-2 Transformer (8 layers, 1024 hidden dim, 8 heads, 4096 FFN, H=32 timesteps) → Policy Head (16 discrete action heads) + Reward Head (scalar value)

- **Critical path:**
  1. **Data Prep:** Convert continuous joystick actions into discrete buckets (11 buckets/x-y) to handle multi-modality.
  2. **Pre-training:** Train the Transformer+CNN to predict actions on the massive 9875-hour dataset.
  3. **SFT:** Fine-tune the policy on a curated set of 300 successful trajectories.
  4. **RM Training:** Generate trajectories, label preferences, and train the Reward Model (initializing weights from the SFT agent).
  5. **RL Alignment:** Run the agent online, score trajectories with the RM, and update the policy via REINFORCE.

- **Design tradeoffs:**
  - **Discretized Actions:** The paper discretizes continuous joystick inputs into 11 buckets. This sacrifices precision for the ability to model multi-modal distributions (e.g., going left vs. right at a fork) which a Gaussian mean would average out.
  - **Context Window (H=32):** A window of ~3 seconds is used. This trades off computational cost (quadratic attention) against the ability to solve partial observability (memory).

- **Failure signatures:**
  - **"Wall Hugging":** The agent executes "move forward" repeatedly when OOD. This indicates a lack of pre-training diversity or recovery data.
  - **Spawn Bias:** The agent learns to solve the task only from specific starting locations seen in the SFT data (Appendix H). This suggests SFT data diversity was insufficient.

- **First 3 experiments:**
  1. **Pre-training Ablation:** Train an equivalent model *from scratch* on the fine-tuning dataset only. Compare success rates and error recovery (Figure 5 vs Figure 4) to validate the pre-training benefit.
  2. **Reward Model Efficiency:** Train RMs with random encoders vs. agent encoders across varying numbers of comparisons (Figure 7) to verify representation transfer.
  3. **Alignment Direction:** Attempt to align the agent to a difficult target (e.g., Right jumppad) with and without "Preference Fine-Tuning" (ReST) to measure alignment efficiency gains.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation environment is synthetic and lacks the complexity of real-world robotics or open-world game scenarios
- The claim that pre-training enables robust recovery behaviors assumes the pre-training data contains sufficient diversity but lacks direct empirical validation
- Reward model efficiency gains may not generalize to more nuanced alignment objectives requiring subtle visual distinctions

## Confidence
- **High confidence**: The multi-stage pipeline successfully achieves the stated task (jumppad navigation >90% success)
- **Medium confidence**: Pre-training improves fine-tuning effectiveness and generalization based on qualitative error mode comparisons
- **Medium confidence**: Agent-initialized reward models outperform random encoders, though the evaluation is limited to this specific task
- **Low confidence**: The mechanism explaining why pre-training enables recovery behaviors (diversity coverage) lacks direct empirical validation

## Next Checks
1. **Pre-training Diversity Validation**: Systematically ablate pre-training data diversity (e.g., train on narrow vs. broad gameplay patterns) and measure recovery behavior rates during OOD state encounters to directly test the diversity hypothesis.

2. **Reward Model Transferability Test**: Apply the same reward model initialization approach to a qualitatively different preference task (e.g., stylistic preferences rather than goal-reaching) to test whether behavioral features generalize beyond task-specific objectives.

3. **Distribution Shift Stress Test**: Evaluate the pre-trained agent on states generated by randomizing initial conditions beyond what appears in the SFT dataset, measuring both success rates and error recovery patterns to quantify robustness claims.