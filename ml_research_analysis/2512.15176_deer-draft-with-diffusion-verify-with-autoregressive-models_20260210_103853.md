---
ver: rpa2
title: 'DEER: Draft with Diffusion, Verify with Autoregressive Models'
arxiv_id: '2512.15176'
source_url: https://arxiv.org/abs/2512.15176
tags:
- deer
- draft
- decoding
- tokens
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency bottleneck in large language
  model (LLM) inference, which becomes increasingly critical as models grow in size
  and context length requirements rise. The authors propose DEER, a speculative decoding
  framework that uses a diffusion language model (dLLM) as the sole draft generator,
  verified by an autoregressive (AR) model.
---

# DEER: Draft with Diffusion, Verify with Autoregressive Models

## Quick Facts
- arXiv ID: 2512.15176
- Source URL: https://arxiv.org/abs/2512.15176
- Authors: Zicong Cheng; Guo-Wei Yang; Jia Li; Zhijie Deng; Meng-Hao Guo; Shi-Min Hu
- Reference count: 40
- Primary result: Achieves 5.54x speedup on HumanEval with Qwen3-30B-A3B using diffusion drafter

## Executive Summary
This paper introduces DEER, a speculative decoding framework that uses diffusion language models (dLLMs) as draft generators verified by autoregressive (AR) models. The key innovation addresses the efficiency bottleneck in large language model inference by replacing traditional AR-based drafters with parallel block-generating dLLMs. DEER employs a two-stage training pipeline to align dLLMs with AR-style prefix-conditioned continuation, achieving draft acceptance lengths up to 32 tokens - significantly exceeding the 10 tokens of state-of-the-art methods. The framework is lossless and preserves the target AR model's output distribution while demonstrating scalability in batch inference and generalization to mathematical reasoning tasks.

## Method Summary
DEER uses a pretrained dLLM as the draft generator, which creates entire token blocks in parallel to avoid the left-to-right uncertainty accumulation of AR drafters. The framework employs a two-stage training pipeline: Stage I (AR-Style Continuation Distillation) trains the dLLM to denoise masked suffixes conditioned on prefix + SEP marker using teacher-generated answers, while Stage II (Prefix-Conditioned Accuracy Refinement) applies exponentially decaying weights to tokens near the prefix boundary to concentrate model capacity where verification first occurs. The verification process uses modified rejection sampling to ensure lossless output preservation. The draft model uses 0.5B parameters across all target models.

## Key Results
- Achieves draft acceptance lengths up to 32 tokens, far exceeding EAGLE-3's 10 tokens
- Delivers 5.54x speedup on HumanEval with Qwen3-30B-A3B compared to 2.41x for EAGLE-3
- Demonstrates scalability in batch inference settings and generalization to mathematical reasoning tasks
- Maintains lossless output preservation, ensuring exact target AR model distribution

## Why This Works (Mechanism)

### Mechanism 1: Parallel Block Generation Eliminates Left-to-Right Uncertainty Accumulation
DEER's dLLM generates entire blocks in one parallel denoising step, avoiding the progressive degradation in acceptance rates that plagues AR-based drafters. Unlike AR drafters where each token conditions on previously sampled (unverified) draft tokens causing errors to cascade, dLLMs generate independent proposals so errors at one position do not propagate to others.

### Mechanism 2: Two-Stage Diffusion-to-AR Alignment Enables Prefix-Conditioned Continuation
The two-stage training pipeline adapts pretrained dLLMs to generate coherent continuations from a prefix. Stage I teaches the dLLM to denoise masked suffixes conditioned on prefix + SEP marker using teacher-generated answers. Stage II applies exponentially decaying weights (wᵢ = α^(R-i)) to tokens near the prefix boundary, concentrating model capacity where the verifier first interacts with the draft.

### Mechanism 3: Lossless Verification via Modified Rejection Sampling
DEER preserves the exact output distribution of the target AR model through provably lossless draft-then-verify procedure. For each draft token, acceptance probability αᵢ = min(1, p_AR(ŷⱼ⁺ⁱ|x₁:ⱼ⁺ⁱ⁻¹) / q_θ(ŷⱼ⁺ⁱ|x₁:ⱼ)) is computed, and tokens are accepted with probability αᵢ or rejected and resampled from the residual distribution. Theorem E.2 proves this yields identical joint distributions to direct AR sampling.

## Foundational Learning

- **Speculative Decoding**: The draft-verify paradigm where a faster model proposes tokens and a slower model verifies them. Why needed: DEER is a speculative decoding variant; understanding this framework is prerequisite. Quick check: Can you explain why speculative decoding preserves the target distribution despite using a potentially imperfect drafter?

- **Discrete Diffusion Language Models**: Models that generate tokens through iterative denoising of noise-perturbed sequences in parallel. Why needed: The core innovation is using dLLMs as drafters; understanding their parallel token generation and denoising process is essential. Quick check: How does the denoising process in a discrete diffusion model differ from autoregressive next-token prediction?

- **Knowledge Distillation and Distribution Alignment**: The process of transferring behavior from a larger teacher model to a smaller student model while preserving key distributional properties. Why needed: The two-stage training pipeline distills the AR teacher's behavior into the dLLM drafter. Quick check: What could go wrong if the student (drafter) distribution has narrower support than the teacher (target) distribution?

## Architecture Onboarding

- **Component map**: dLLM Drafter -> Acceptance Module -> AR Target Model -> Final Output
- **Critical path**: Initialize dLLM from pretrained checkpoint → Run Stage I distillation on domain data → Run Stage II refinement → Deploy inference loop: draft k tokens → verify token-wise → accept/reject/resample
- **Design tradeoffs**: Block size (k) affects speedup vs verification overhead; larger blocks yield higher potential speedup but more verification overhead. Drafter size (0.5B used) balances speed vs quality. α weighting in Stage II requires careful tuning to avoid training instability.
- **Failure signatures**: Low acceptance length (τ < 3) indicates dLLM-AR misalignment; training divergence in Stage II suggests α too high; acceptance rate drops at batch > 1 indicates missing KV-cache support; speedup < 2x suggests block size too small or drafter too slow.
- **First 3 experiments**: 1) Baseline comparison vs EAGLE-3 on HumanEval with Qwen3-8B at temperature=0 measuring speedup and τ. 2) Stage II ablation - train draft model with/without refinement comparing acceptance lengths. 3) Block size sensitivity - sweep k ∈ {4, 8, 16, 32} on Qwen3-14B plotting τ vs k to find optimal operating point.

## Open Questions the Paper Calls Out

- **Maximum acceptance length limits**: The paper achieves 32 tokens but doesn't analyze whether this represents a fundamental limit imposed by dLLM architecture, alignment training, or verifier acceptance criteria. The "long-block resurgence effect" where probability mass increases again near maximum draft lengths lacks explanation.

- **KV-cache integration for batch inference**: Current DEER implementation lacks mature support for diffusion language models with KV caching in mainstream inference frameworks, limiting practical applicability in production serving scenarios despite demonstrated speedups.

- **Long-block resurgence mechanism**: The counterintuitive finding that acceptance probability increases near maximum draft lengths challenges standard assumptions about speculative decoding acceptance distributions and could reveal important properties of blockwise diffusion generation.

## Limitations

- **KV-cache and batch inference scalability**: DEER's dLLM does not currently support efficient KV caching, which is critical for batch inference performance and could significantly reduce real-world speedup gains compared to theoretical maximums.

- **Weakly trained draft model generalization**: While the paper demonstrates DEER works with "weakly trained" draft models, the definition is not precisely quantified and generalization claims are based on limited experimental validation.

- **Discrete diffusion model conversion**: The conversion process from AR models to discrete dLLMs is necessary but lacks detailed specification and validation that this conversion preserves model quality.

## Confidence

**High Confidence**:
- DEER achieves superior draft acceptance lengths (up to 32 tokens) compared to EAGLE-3 (10 tokens)
- The two-stage training pipeline effectively improves alignment
- Lossless verification is theoretically sound

**Medium Confidence**:
- The 5.54x speedup claim depends heavily on implementation details and hardware-specific optimizations
- Generalization to mathematical reasoning tasks with weakly trained draft models has limited experimental validation
- Scalability benefits in batch inference settings due to acknowledged KV-cache limitations

**Low Confidence**:
- Practical deployment performance in production serving environments
- Robustness of the two-stage training pipeline across different model architectures and domains
- Long-term stability of the exponentially weighted loss across different datasets

## Next Checks

**Validation Check 1**: Implement and benchmark DEER with proper KV caching support for the dLLM drafter in batch inference scenarios. Measure speedup degradation when serving 8-32 concurrent requests compared to single-request performance, and compare against EAGLE-3's batch capabilities.

**Validation Check 2**: Train DEER draft models on multiple domains (code, math, general chat) using the same weakly trained configuration. Evaluate acceptance lengths and speedups across all domains to quantify the claimed generalization capability and identify domain-specific failure modes.

**Validation Check 3**: Systematically vary the two-stage training hyperparameters (α in Stage II, training data volume, learning rates) across code and math domains. Document the impact on acceptance length, training stability, and final inference performance to establish robust training guidelines.