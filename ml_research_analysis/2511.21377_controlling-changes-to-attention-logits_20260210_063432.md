---
ver: rpa2
title: Controlling changes to attention logits
arxiv_id: '2511.21377'
source_url: https://arxiv.org/abs/2511.21377
tags:
- learning
- arxiv
- attention
- norm
- logits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a method to stabilize transformer training\
  \ by controlling changes in attention logits rather than their magnitude. The core\
  \ idea is to assign parameter-dependent learning rates to query and key weights\
  \ based on the norms of their corresponding matrices, inspired by Maximal Update\
  \ Parametrization (\xB5P) principles."
---

# Controlling changes to attention logits

## Quick Facts
- arXiv ID: 2511.21377
- Source URL: https://arxiv.org/abs/2511.21377
- Reference count: 9
- Primary result: QuacK controls attention logit changes via parameter-dependent learning rates, enabling stable training at learning rates up to 3e-2 while being ~10% faster than QK norm and compatible with MLA architectures.

## Executive Summary
This paper addresses transformer training instability caused by unbounded attention logit changes at high learning rates. Rather than normalizing logits directly (as in QK norm), the authors propose controlling the worst-case change in logits by assigning parameter-dependent learning rates to query and key weights. This approach is inspired by Maximal Update Parametrization principles and bounds logit changes independently of weight size. The method, called QuacK, demonstrates competitive performance with QK norm in standard Multi-Head Attention settings while being computationally cheaper and compatible with Multi-Latent Attention architectures where QK norm is inapplicable.

## Method Summary
The method adjusts query and key weight learning rates based on the norms of their respective weight matrices. For each attention head, the learning rate for query weights is scaled by the inverse norm of key weights (η^Q ∝ ||W_K||^-1), and vice versa for key weights (η^K ∝ ||W_Q||^-1). This cross-parameter scaling ensures that changes in logits remain bounded during training. The approach requires storing initial weight norms at initialization and recomputing current norms at each optimization step. A hyperparameter τ controls the initial relative learning rate for query/key weights and must be tuned (swept over {10^-2, 10^-1, 10^0, 10^1} in experiments).

## Key Results
- QuacK achieves stability at learning rates up to 3e-2, significantly higher than default training rates
- In MHA settings, QuacK performs competitively with QK norm while removing two RMS norm computations per attention block (~10% speedup)
- In MLA settings, QuacK outperforms QK clip and is the only viable option since QK norm cannot be applied to MLA architectures
- The method requires hyperparameter τ tuning but provides stable training across different learning rate regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bounding logit changes via cross-parameter learning rates stabilizes training
- Mechanism: Assign learning rates η_Q ∝ ||W_K||^-1 and η_K ∝ ||W_Q||^-1 so that first-order logit changes from both Q(ΔK)^T and (ΔQ)K^T remain bounded by a constant (Lemma 1). When Q is large, perturbations from K-updates would be amplified, so the learning rate for K must be reduced proportionally to ||W_Q||, and vice versa.
- Core assumption: Gradient norms ||G_Q/K|| are bounded by a constant D (satisfied by Adam/Muon), and weight norms are lower-bounded by c > 0.
- Evidence anchors:
  - [abstract] "These changes are controllable by assigning parameter-dependent learning rates to the query and key weights."
  - [Section 3, Lemma 1] "If there is a constant c such that 0 < c ≤ ||W_Q||, ||W_K||, and the learning rates satisfy η_Q ∝ ||W_K||^-1, and η_K ∝ ||W_Q||^-1, then the worst-case change in logits is bounded above independently of the weight size."
  - [corpus] No direct corpus evidence for this specific mechanism; corpus papers address MLA architecture and attention regularization but not cross-norm learning rates.
- Break condition: If gradient norms become unbounded (e.g., with SGD without gradient clipping), or if weights collapse toward zero breaking the lower-bound assumption.

### Mechanism 2
- Claim: Inter-parameter learning rates outperform fixed Q/K learning rate reduction
- Mechanism: Rather than uniformly reducing η for all Q/K weights by a fixed factor τ, QuacK dynamically adjusts each head's learning rates based on current weight norms. This preserves expressivity in heads with smaller weights while stabilizing heads with larger weights.
- Core assumption: Different attention heads develop different weight norm scales during training, requiring head-specific treatment.
- Evidence anchors:
  - [Section 4, Figure 2] "The ablation, which sets the learning rates for query and key weights to smaller fixed values, is stable, but underperforms QuacK in both the MHA and MLA settings."
  - [Section 1] "This preserves expressivity, while reining-in instability."
  - [corpus] Corpus papers discuss attention regularization methods (AttentionDrop) but do not compare fixed vs. adaptive learning rate approaches.
- Break condition: If all heads maintain similar norms throughout training, the benefit over fixed reduction diminishes.

### Mechanism 3
- Claim: QuacK enables higher base learning rates by preventing logit explosion without normalization overhead
- Mechanism: By controlling logit changes rather than magnitudes, QuacK avoids the two RMS norm computations per block required by QK norm. This provides ~10% faster training while still supporting learning rates up to 3e-2.
- Core assumption: The instability at high learning rates primarily manifests through unbounded logit changes rather than other failure modes.
- Evidence anchors:
  - [Section 4] "QuacK yields a speedup over QK norm by removing two RMS norm computations per attention block; in practice we observed ~10% faster training."
  - [Section 4, Figure 4] Shows QuacK controls both max logit and average absolute change in logit comparably to QK norm.
  - [corpus] Corpus paper "Muon: Training and Trade-offs with Latent Attention and MoE" discusses optimizer interactions with MLA but not this specific efficiency gain.
- Break condition: If other instability sources (e.g., residual stream explosions) dominate at high LR, QuacK alone may be insufficient.

## Foundational Learning

- Concept: **Maximal Update Parametrization (µP)**
  - Why needed here: QuacK is explicitly inspired by µP's desideratum of controlling activation changes during training, extending this from residual streams to attention logits.
  - Quick check question: Can you explain why µP scales learning rates inversely with width for readout layers?

- Concept: **Attention logit structure and entropy collapse**
  - Why needed here: Understanding why large logits cause training instability (attention entropy collapse) motivates the need for intervention.
  - Quick check question: What happens to attention distribution when logits become extremely large for a few positions?

- Concept: **Multi-Latent Attention (MLA) architecture**
  - Why needed here: MLA's compressed KV cache prevents full materialization of keys, making QK norm incompatible—the key motivation for QuacK's existence.
  - Quick check question: Why does MLA cache compressed latents rather than full key/value vectors?

## Architecture Onboarding

- Component map:
  - **MHA mode**: W_Q, W_K per head → learning rates adjusted via ||W_K||, ||W_Q|| respectively (Algorithm 1)
  - **MLA mode**: W_uq, W_uk, W_qr (per-head up projections), W_dq, W_dkv, W_kr (shared down projections) → each has specific norm-based learning rate factors (Algorithm 2)
  - **Hyperparameter τ**: Controls the initial relative learning rate for Q/K weights, swept over {10^-2, 10^-1, 10^0, 10^1}

- Critical path:
  1. At initialization: Compute and store initial norms for all Q/K weights
  2. Each optimization step: Recompute current norms, apply factors per Algorithm 1 (MHA) or Algorithm 2 (MLA)
  3. Use Frobenius norm (spectral shows minimal improvement per Figure 3)

- Design tradeoffs:
  - Frobenius vs. spectral norm: Spectral slightly better but not worth compute cost (Figure 3)
  - τ selection: Must be tuned; paper swept 4 orders of magnitude
  - vs. QK norm: Slightly worse peak performance in MHA, but faster and MLA-compatible

- Failure signatures:
  - Loss spikes at high LR with default settings → indicates need for intervention
  - Training instability with τ too high → reduce τ
  - Underfitting with τ too low → increase τ or verify norm computation

- First 3 experiments:
  1. Replicate Figure 1 ablation: Train with τ ∈ {10^-2, 10^-1, 10^0, 10^1} at η=3e-2 to verify stability gains over default
  2. Compare QuacK vs. QK norm on MHA at multiple learning rates (3e-4, 3e-3, 3e-2) to characterize performance gap
  3. Test MLA setting with QuacK vs. QK clip at η=3e-3 to verify MLA-specific benefits

## Open Questions the Paper Calls Out

- Question: Does QuacK maintain stability and performance advantages when scaling model size beyond 1B parameters and training duration beyond 5,000 steps?
  - Basis: [explicit] The authors explicitly state in Section 5 that results are "limited by short training durations (5k steps) and a single dataset and model architecture."
  - Why unresolved: Compute constraints prevented the authors from validating if the method prevents instability in larger models where logit blow-up is often more severe.
  - Evidence: Training runs on 7B+ parameter models over trillions of tokens showing stable loss curves comparable to QK norm.

- Question: Is QuacK effective with standard adaptive optimizers like Adam or AdamW, rather than the Muon optimizer used in the paper's experiments?
  - Basis: [inferred] All experimental results were generated using the Muon optimizer, though Lemma 1 claims applicability to Adam.
  - Why unresolved: Empirical validation was restricted to one optimizer, leaving the interaction with standard Adam weight decay or gradient moments unverified.
  - Evidence: Benchmark comparisons of QuacK against QK norm using AdamW on a 1B parameter model.

- Question: Does the method remain stable if query/key weight norms approach zero, violating the theoretical assumption of a positive lower bound?
  - Basis: [inferred] Lemma 1 requires a constant $c$ such that $c \le ||W_{Q/K}||$, but the paper does not verify if this holds during training with weight decay.
  - Why unresolved: If weights decay towards zero, the learning rate modulation could become unstable or undefined.
  - Evidence: Ablation studies tracking weight norms and stability under high weight decay coefficients.

## Limitations

- The method's stability guarantees depend on gradient norms remaining bounded and weights maintaining a positive lower bound, which may not hold with aggressive weight decay or non-adaptive optimizers.
- Performance gains come with the requirement to tune the hyperparameter τ, which may vary across different model architectures and scales.
- Experimental validation is limited to short training runs (5k steps) on a single dataset and model architecture, leaving long-training behavior and generalization to other domains unverified.

## Confidence

- **High Confidence**: The stability mechanism via cross-parameter learning rates (Mechanism 1) - supported by formal lemma and empirical validation across multiple settings
- **Medium Confidence**: The efficiency claims relative to QK norm - timing measurements are provided but could benefit from more rigorous benchmarking across hardware configurations
- **Medium Confidence**: The MLA-specific benefits - demonstrated on a single model family (Qwen3-based) and dataset (Cosmopedia-V2)

## Next Checks

1. **Scale Sensitivity Test**: Evaluate QuacK on models ranging from 100M to 10B parameters to verify the scaling properties and identify any size-dependent failure modes or hyperparameter adjustments needed.

2. **Optimizer Ablation**: Test QuacK with SGD+Momentum and AdamW to determine the robustness of stability improvements across optimizer families, particularly for the gradient norm boundedness assumption.

3. **Long-Training Stability**: Run extended training (100K+ steps) with QuacK at high learning rates to monitor for gradual weight norm decay or other long-tail failure modes not visible in short training runs.