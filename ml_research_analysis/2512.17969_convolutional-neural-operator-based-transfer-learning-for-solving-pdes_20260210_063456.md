---
ver: rpa2
title: Convolutional-neural-operator-based transfer learning for solving PDEs
arxiv_id: '2512.17969'
source_url: https://arxiv.org/abs/2512.17969
tags:
- uni00000013
- uni00000011
- uni00000014
- uni00000015
- uni00000017
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of applying convolutional neural
  operators (CNOs) for few-shot transfer learning in solving partial differential
  equations (PDEs). While CNOs have shown high accuracy in learning solution operators,
  their performance degrades when applied to new physical systems without adaptation.
---

# Convolutional-neural-operator-based transfer learning for solving PDEs

## Quick Facts
- arXiv ID: 2512.17969
- Source URL: https://arxiv.org/abs/2512.17969
- Reference count: 40
- Authors: Peng Fan; Guofei Pang
- Primary result: Neuron Linear Transformation (NLT) strategy outperforms fine-tuning and LoRA for few-shot transfer learning of convolutional neural operators on PDEs, demonstrating robustness to distribution shifts.

## Executive Summary
This paper addresses the challenge of applying convolutional neural operators (CNOs) for few-shot transfer learning in solving partial differential equations (PDEs). While CNOs have shown high accuracy in learning solution operators, their performance degrades when applied to new physical systems without adaptation. The authors propose a transfer learning framework where a CNO is pre-trained on a source dataset and then adapted to a small target dataset using three strategies: fine-tuning, low-rank adaptation (LoRA), and neuron linear transformation (NLT). The framework is evaluated on three PDEs: the Kuramoto-Sivashinsky equation, the Brusselator diffusion-reaction system, and the Navier-Stokes equations. The NLT strategy consistently outperforms the others, achieving the lowest test errors and demonstrating robustness to the magnitude of distribution shifts between source and target datasets.

## Method Summary
The method involves pre-training a convolutional neural operator (CNO) on a source dataset (512 samples) for a given PDE family, then adapting it to a target dataset (16 samples) with a distribution shift (e.g., different viscosity or frequency parameter). Three transfer strategies are compared: fine-tuning (last layers), LoRA (low-rank updates to convolutional kernels), and NLT (neuron linear transformation applying affine scaling to kernels). The CNO architecture is a U-Net style structure with lifting and projection layers. The framework is tested on three PDEs with controlled shifts in complexity parameters, using relative L1 error as the metric.

## Key Results
- NLT achieves the lowest test errors across all three PDEs (Kuramoto-Sivashinsky, Brusselator, Navier-Stokes) when transferring to target distributions with distribution shifts.
- NLT demonstrates robustness to the magnitude of distribution shift (measured by MMD), maintaining stable performance while LoRA and fine-tuning degrade significantly under high MMD conditions.
- The framework enables accurate surrogate models with minimal data dependency, requiring only 16 samples for adaptation compared to 512 for pre-training.

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning Bridges Distribution Gaps
If a Convolutional Neural Operator (CNO) is pre-trained on a source distribution, it can adapt to a target distribution with minimal data, provided the shift is addressed via parameter adjustment rather than random initialization. The pre-training phase captures the general spectral and structural features of the PDE family. The transfer phase (fine-tuning, LoRA, or NLT) acts as a corrective offset, realigning the learned manifold to the new physics without requiring the target dataset to relearn fundamental operators from scratch. The core assumption is that the source and target tasks share latent features (e.g., spatial derivatives, reaction terms) that are invariant or linearly transformable across the distribution shift.

### Mechanism 2: NLT Stabilizes Feature Space Under High Distribution Shift
Neuron Linear Transformation (NLT) appears to preserve the spatial topology of learned features better than LoRA or Fine-tuning when the distribution shift (quantified by MMD) is large. NLT applies an affine transformation ($W_t = f \times W_s + b$) to convolutional kernels. Unlike LoRA (low-rank additive update) or Fine-tuning (direct weight updates), this multiplicative-scaling approach may better preserve the relative magnitude and direction of general features while adjusting the domain bias, preventing the "forgetting" of fundamental physics. The core assumption is that the domain shift can be effectively modeled as a scaling and bias operation on the feature maps of the source model.

### Mechanism 3: Band-Limited Continuous-Discrete Equivalence
The CNO architecture enforces a spectral constraint (band-limiting) that ensures the discrete model respects continuous operator properties, reducing aliasing errors common in other neural operators. By restricting the function space to $B_w(D)$ (band-limited space) and using specific up/down-sampling schemes, the architecture guarantees that the learned solution operator is resolution-invariant and alias-free. This structural rigidity likely makes the model more amenable to linear parameter transformations (like NLT) than less constrained architectures. The core assumption is that the solution to the PDE can be accurately approximated within a band-limited function space (spectral compact support).

## Foundational Learning

### Concept: Neural Operators vs. PINNs
**Why needed here:** This paper focuses on learning the *solution operator* (mapping initial conditions to solutions) rather than fitting a specific solution instance (PINNs). Understanding this distinction is crucial for grasping the transfer learning objective.  
**Quick check question:** Are you trying to predict the time-evolution of a field $u(x,T)$ from an initial state $u_0(x)$, or solving for a static field given boundary conditions?

### Concept: Distribution Shift (MMD)
**Why needed here:** The paper quantifies the difficulty of the transfer task using Maximum Mean Discrepancy (MMD). Understanding that a higher MMD implies a harder transfer is crucial for interpreting the results.  
**Quick check question:** How does the model's performance degrade as the viscosity parameter $\nu$ or frequency parameter $K$ diverges from the training set?

### Concept: U-Net Architectures
**Why needed here:** The CNO is effectively a modified U-Net. Understanding encoder-decoder structures and skip connections is necessary to visualize where the transfer strategies (NLT/LoRA) are applied (convolutional layers).  
**Quick check question:** Do you understand how the "lifting" and "projection" layers map physical fields to latent channels and back?

## Architecture Onboarding

### Component map:
Input: Initial condition $u_0$ (matrix format) -> Lifting ($P$) -> ResNet/Inv Blocks (Conv + Activation + Sampling) -> Projection ($Q$) -> Output solution $u(x,T)$

### Critical path:
Pre-train CNO on source (512 samples) -> Freeze backbone -> Inject NLT parameters -> Train only $f, b$ on target (16 samples)

### Design tradeoffs:
- *NLT:* Best accuracy and stability; assumes linear transform is sufficient
- *LoRA:* Lower parameter count (potentially), but less stable under large distribution shifts (MMD)
- *Fine-tuning:* High capacity to learn, but prone to overfitting on 16 samples or destroying pre-trained features

### Failure signatures:
- **Supervised Baseline:** High error (>14%) due to insufficient data to converge
- **LoRA/Fine-tuning under high MMD:** Performance degrades rapidly as shift increases (e.g., Table 2, Navier-Stokes $\nu=1\times10^{-4}$)
- **No Transfer:** Orders of magnitude higher error (generalization gap)

### First 3 experiments:
1. **Reproduce Generalization Gap:** Train CNO on source ($K=6$) and test on target ($K=8$) without adaptation to confirm the error floor (Table 1)
2. **Ablation on Target Size:** Implement NLT transfer on Navier-Stokes while varying $n_t$ (16, 32, 64, 256) to reproduce Figure 1 curve
3. **Stress Test:** Compare LoRA vs. NLT on the maximum MMD scenario ($K=6 \to 12$) to verify NLT's stability advantage

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can the CNO-based transfer learning framework be extended to handle irregular or non-rectangular computational domains?  
**Basis in paper:** [explicit] The conclusion explicitly lists "loosening the requirement of a rectangular computational domain" as a primary direction for future work.  
**Why unresolved:** The current CNO architecture relies on a CNN-based structure that necessitates input/output functions be formatted as matrices (Page 4), inherently restricting the method to rectangular grids.  
**What evidence would resolve it:** Successful implementation and benchmarking of the framework on PDEs defined over complex geometries, such as fluid flow around airfoils, using techniques like domain decomposition or implicit neural representations.

### Open Question 2
**Question:** How can the framework be adapted to enable surrogate predictions for time-marching scenarios rather than mapping only to a fixed final time?  
**Basis in paper:** [explicit] The authors state in the conclusion that future work includes "allowing surrogate predictions for time marching."  
**Why unresolved:** The current problem formulation learns an operator mapping an initial condition $u_0(x)$ to a solution snapshot $u(x, T)$ at a single observation time $T$ (Page 3), lacking the autoregressive capability needed for full trajectory evolution.  
**What evidence would resolve it:** Modifying the architecture to predict $u(x, t + \Delta t)$ recursively, and demonstrating stability and error accumulation control over long time horizons for chaotic systems like the Kuramoto-Sivashinsky equation.

### Open Question 3
**Question:** Why does the Neuron Linear Transformation (NLT) strategy exhibit significantly higher robustness to the magnitude of distribution shift (MMD) compared to fine-tuning and LoRA?  
**Basis in paper:** [inferred] While the results (Table 2) show NLT errors remain stable as MMD increases (e.g., KS equation, Page 7) while LoRA errors double, the paper does not provide a theoretical explanation for this specific robustness.  
**Why unresolved:** The paper empirically observes NLT's "resilience" and "insensitivity" to distribution discrepancy but relies on heuristics rather than a theoretical derivation linking the affine transformation to the spectral properties of the PDE operators.  
**What evidence would resolve it:** A theoretical analysis or ablation study mapping the NLT transformation parameters to the frequency domain shifts between source and target datasets, explaining why this specific parameterization resists high-frequency distribution shifts better than low-rank updates.

## Limitations

- **Architecture Gaps:** The exact CNO backbone details (channel sizes, block depths, activation functions) are referenced but not reproduced in this paper, creating a dependency on [18] for faithful replication.
- **Single-Pass Evaluation:** Results are reported over 5 random runs but the variance is not explicitly shown in the tables, making it difficult to assess statistical significance of the observed differences between transfer strategies.
- **Dataset Specificity:** All experiments use controlled synthetic PDEs on a $128 \times 128$ grid with a single complexity parameter shift ($K$ or $\nu$). The framework's robustness to more complex or unstructured shifts (e.g., geometry changes, multi-parameter drift) is untested.

## Confidence

- **High:** The core claim that NLT outperforms LoRA and Fine-tuning for transfer learning in the tested PDE settings is well-supported by the presented results (Tables 1, 2, 3).
- **Medium:** The assertion that NLT's performance is "robust to the magnitude of distribution shift" is supported by the MMD analysis, but the range of shifts tested is narrow.
- **Low:** The general claim that this framework will "enable accurate surrogate models with minimal data dependency" across diverse PDE families is an extrapolation beyond the three tested equations.

## Next Checks

1. **Architecture Reconstruction:** Implement the CNO backbone from [18] and pre-train on the source dataset to reproduce the baseline generalization error, confirming the gap NLT aims to close.
2. **NLT Ablation Under Extreme Shift:** Test NLT on the maximum MMD scenario (e.g., $K=6 \to 12$ for Kuramoto-Sivashinsky) to empirically verify its claimed stability advantage over LoRA.
3. **Statistical Significance:** Re-run the NLT vs. LoRA comparison on the target dataset 10+ times and report the mean and standard deviation of errors to assess if the performance gap is statistically significant.