---
ver: rpa2
title: Local Entropy Search over Descent Sequences for Bayesian Optimization
arxiv_id: '2511.19241'
source_url: https://arxiv.org/abs/2511.19241
tags:
- function
- local
- objective
- evaluation
- descent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Local Entropy Search (LES) targets the solution reachable by an
  iterative optimizer starting from an initial design, rather than the global optimum.
  It propagates the posterior belief over the objective through the optimizer, yielding
  a distribution over descent sequences.
---

# Local Entropy Search over Descent Sequences for Bayesian Optimization

## Quick Facts
- **arXiv ID:** 2511.19241
- **Source URL:** https://arxiv.org/abs/2511.19241
- **Reference count:** 40
- **Primary result:** LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods on high-complexity synthetic objectives and benchmark problems.

## Executive Summary
Local Entropy Search (LES) is a Bayesian optimization method that targets the solution reachable by an iterative optimizer starting from an initial design, rather than the global optimum. It propagates the posterior belief over the objective through the optimizer, yielding a distribution over descent sequences. At each iteration, LES selects the next evaluation by maximizing mutual information with that distribution, using analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results show LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.

## Method Summary
LES uses a Gaussian Process surrogate model and targets the local optimum reachable from the current best guess via an iterative optimizer (e.g., Adam). It draws posterior samples, runs local optimizers on each sample to generate descent sequences, discretizes these into candidate points, and selects the next evaluation by maximizing mutual information between the candidate and the distribution of descent sequences. The acquisition function is computed as the difference between predictive entropy and expected conditional entropy after conditioning on the sampled sequences.

## Key Results
- LES consistently outperformed other methods on GP samples with high complexity in higher dimensions
- Achieved lower cumulative regret than global methods like MES in dimensions d=20, 30, 50
- Better final objective values with fewer evaluations compared to both local and global Bayesian optimization methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Focusing information gain on the trajectory of a local optimizer improves sample efficiency in high-complexity domains compared to global search.
- **Mechanism:** Propagates GP posterior through iterative optimizer to generate distribution over possible paths to local optimum, querying points that maximize mutual information with these paths.
- **Core assumption:** User's desired solution is the local optimum reachable from initial design, not necessarily global optimum.
- **Evidence anchors:** Abstract states LES targets solution reachable by iterative optimizer; section 1 contrasts with global search requiring uncertainty reduction across entire domain.
- **Break condition:** If objective is low-dimensional or multi-modal with global optimum far from starting basin, may converge to suboptimal local solution.

### Mechanism 2
- **Claim:** Selecting query points via entropy-based acquisition function efficiently reduces uncertainty regarding optimizer's path.
- **Mechanism:** Computes acquisition score as mutual information between candidate point and descent sequence, approximated by difference between predictive entropy and expected conditional entropy.
- **Core assumption:** GP posterior samples are sufficiently representative of true function landscape to generate meaningful descent sequences.
- **Evidence anchors:** Abstract mentions maximizing mutual information with distribution using analytic entropy calculations; section 5.1 formalizes acquisition function.
- **Break condition:** If Monte-Carlo samples are too few, approximation becomes noisy leading to sub-optimal querying.

### Mechanism 3
- **Claim:** Constraining search space to "reachable" neighborhood of current incumbent reduces cumulative regret in high-dimensional spaces.
- **Mechanism:** Starts descent sequence from current best guess rather than fixed initial point, dynamically restricting search region to avoid exploring irrelevant regions.
- **Core assumption:** High-complexity functions require localized refinement rather than broad exploration for efficient convergence.
- **Evidence anchors:** Section 5.2 states descent sequence always starts from current best guess; section 6.2 shows LES achieves lower cumulative regret than global methods in higher dimensions.
- **Break condition:** If optimizer gets stuck in local basin, "reachable" neighborhood excludes global optimum, resulting in high simple regret despite low cumulative regret.

## Foundational Learning

- **Concept: Gaussian Process (GP) Posterior Sampling**
  - **Why needed here:** LES relies on drawing smooth, differentiable sample functions from GP to run local optimizers on them.
  - **Quick check question:** Can you explain how a sample path drawn from a GP differs from the mean prediction of the GP?

- **Concept: Mutual Information (MI)**
  - **Why needed here:** Acquisition function is defined via MI, measuring reduction in uncertainty of descent sequence given observation at candidate point.
  - **Quick check question:** If entropy of descent sequence is H(Q) and conditional entropy after observing x is H(Q|x), what is the Mutual Information?

- **Concept: Iterative Local Optimization (e.g., Gradient Descent/Adam)**
  - **Why needed here:** LES wraps around these optimizers to generate descent sequences that represent paths to local optima.
  - **Quick check question:** How does step size in gradient descent affect stability and length of descent sequence?

## Architecture Onboarding

- **Component map:** Surrogate Model -> Path Generator -> Candidate Selector -> Acquisition Engine -> Outer Loop
- **Critical path:** Path Generator - computational cost scales linearly with number of samples and complexity of inner optimizer runs
- **Design tradeoffs:**
  - Accuracy vs. Speed: Increasing samples improves performance but significantly increases runtime (approx. 10x slower than TuRBO)
  - Local vs. Global: Choice of starting point determines basin of attraction; sacrifices global coverage for local precision
- **Failure signatures:**
  - Mode Collapse/Trapping: May show high variance or poor performance on highly multi-modal functions if commits to bad local basin early
  - Gradient Misestimation: If GP hyperparameters wrong, sample paths may be too smooth leading to false optima
- **First 3 experiments:**
  1. Sanity Check (1D/2D): Visualize GP samples and overlay generated descent sequences to verify acquisition function selects points along paths
  2. Hyperparameter Sensitivity: Vary L on medium-complexity GP sample task to verify lower L degrades performance but improves runtime
  3. Stress Test: Run LES on standard multi-modal benchmark vs global baseline to observe if gets stuck in local optima

## Open Questions the Paper Calls Out
None

## Limitations
- LES is inherently local and may get stuck in suboptimal basins on highly multi-modal functions
- Computational cost scales linearly with number of posterior samples and discretization points, limiting scalability
- Empirical validation primarily on synthetic benchmarks with limited real-world applications beyond Rover planning task

## Confidence

- **High confidence:** Theoretical framework (MI-based acquisition, GP posterior sampling, descent sequence propagation) is internally consistent and mathematically sound
- **Medium confidence:** Empirical results showing LES outperforming global methods on high-complexity GP samples and achieving lower cumulative regret in higher dimensions
- **Low confidence:** Claims about LES's behavior on real-world problems beyond Rover task and generalizability to extremely high-dimensional spaces (>50D)

## Next Checks
1. Ablation study: Compare LES variants with different numbers of Monte Carlo samples and discretization points to quantify accuracy-runtime tradeoff
2. Robustness test: Evaluate LES on diverse real-world black-box optimization problems to assess generalization beyond synthetic benchmarks
3. Scalability assessment: Benchmark LES on extremely high-dimensional problems (>50D) to determine if computational cost becomes prohibitive and whether performance degrades