---
ver: rpa2
title: 'HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented
  Pre-Training'
arxiv_id: '2507.10920'
source_url: https://arxiv.org/abs/2507.10920
tags:
- korean
- hanja
- language
- performance
- hanjabridge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HanjaBridge addresses semantic ambiguity in Korean LLMs by leveraging
  Hanja (Chinese characters) to disambiguate homophonous Sino-Korean words that share
  the same Hangul spelling. The method presents all possible Hanja candidates for
  each ambiguous word during training, enabling the model to learn contextual disambiguation
  while maintaining efficiency through token-level knowledge distillation.
---

# HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training

## Quick Facts
- arXiv ID: 2507.10920
- Source URL: https://arxiv.org/abs/2507.10920
- Authors: Seungho Choi
- Reference count: 9
- Key outcome: 21% relative improvement on KoBALT semantic disambiguation benchmark

## Executive Summary
HanjaBridge addresses semantic ambiguity in Korean LLMs by leveraging Hanja (Chinese characters) to disambiguate homophonous Sino-Korean words that share the same Hangul spelling. The method presents all possible Hanja candidates for each ambiguous word during training, enabling the model to learn contextual disambiguation while maintaining efficiency through token-level knowledge distillation. This approach prevents catastrophic forgetting of multilingual capabilities. Experiments show HanjaBridge achieves a 21% relative improvement on the KoBALT benchmark and preserves English performance, demonstrating effective cross-lingual transfer. The gains persist even without Hanja at inference time, ensuring practical deployment efficiency.

## Method Summary
HanjaBridge augments Korean LLM pre-training by inserting Hanja candidates for ambiguous Sino-Korean words during training. The method modifies the attention mechanism to restrict Hanja candidates from attending to each other while allowing cross-attention with their parent Korean token. Token-level knowledge distillation preserves multilingual capabilities by aligning student and frozen teacher representations. The approach uses a rolling queue of teacher hidden vectors for contrastive distillation, optimizing both language modeling and knowledge preservation objectives. Training is performed on Korean corpus with expanded vocabulary, while inference can use standard Korean tokens for efficiency.

## Key Results
- 21% relative improvement on KoBALT semantic disambiguation benchmark
- English performance preserved (0.6792 vs teacher 0.6976) while Full CPT degrades (0.6501)
- Optimal candidate count k=8; k=16 shows slight degradation
- Attention accuracy on correct Hanja candidates increases from 10.2% to 30.8% during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Presenting all possible Hanja candidates for ambiguous Korean words forces contextual disambiguation learning, improving semantic understanding.
- Mechanism: The model receives multiple Hanja forms in-line (e.g., "가격價格加擊") and must learn to weight the contextually appropriate character via attention. This transforms ambiguity from a liability into a training signal.
- Core assumption: The model's attention mechanism can learn to prioritize correct candidates when forced to choose among alternatives during pre-training.
- Evidence anchors:
  - [abstract]: "HanjaBridge presents the model with all possible Hanja candidates for a given homograph, encouraging the model to learn contextual disambiguation"
  - [Section 3.1]: "Because multiple Hanja candidates are presented simultaneously, the model cannot merely memorize a single correct answer; instead, it must leverage context to decide which Hanja yields the highest-probability distribution"
  - [corpus]: Related work on Korean-Hanja processing (HERITAGE platform) confirms Hanja disambiguation is a recognized challenge, though no direct validation of this specific multi-candidate approach exists externally.
- Break condition: If the Hanja dictionary has poor coverage of domain-specific or neologism terms, disambiguation signals degrade. Paper explicitly notes this limitation.

### Mechanism 2
- Claim: Restricted attention within Hanja expansion groups preserves semantic independence while enabling cross-lingual knowledge transfer.
- Mechanism: Korean tokens can attend to each Hanja candidate, but candidates cannot attend to each other. This prevents semantic blurring while allowing Chinese semantic knowledge to flow into Korean representations.
- Core assumption: Blocking inter-candidate attention prevents the model from conflating distinct Hanja meanings that share surface forms.
- Evidence anchors:
  - [Section 3.1]: "allowing candidates to attend to one another would blur their meanings. By restricting the attention pathway to 'Korean token ↔ each Hanja candidate,' we ensure that every candidate retains its independent meaning"
  - [Section 4.3, Table 3]: Attention accuracy on correct Hanja candidates increases from 10.2% to 30.8% (k=8) across training, quantitatively showing learned focus
  - [corpus]: No external validation of this specific attention masking pattern; mechanism remains paper-internal.
- Break condition: If the number of candidates (k) is too high (e.g., k=16), noise increases and performance drops slightly (Table 2 shows k=16 underperforms k=8).

### Mechanism 3
- Claim: Token-level contrastive knowledge distillation prevents catastrophic forgetting while enabling Korean specialization.
- Mechanism: A frozen teacher model's hidden representations are stored in a queue; the student aligns its token vectors to preserve the teacher's feature space structure while learning new Korean semantics. Loss is computed only on original Korean tokens, not Hanja expansions.
- Core assumption: Token-wise alignment provides denser supervision than sequence-level distillation, better preserving multilingual representations.
- Evidence anchors:
  - [abstract]: "This process is paired with token-level knowledge distillation to prevent catastrophic forgetting"
  - [Section 3.2]: "This token-wise distillation offers two advantages. First, every token supplies a learning signal, enabling dense supervision"
  - [Section 4.2, Table 2]: Full CPT drops English performance (0.6976→0.6501); HanjaBridge k=8 preserves it (0.6792), demonstrating forgetting mitigation
  - [corpus]: Knowledge distillation for low-resource language adaptation is supported by AfroXLMR-Comet work, though token-level contrastive variant is not independently validated.
- Break condition: If distillation weight λ is too low or teacher-student alignment fails, English performance degrades. Paper uses λ=0.1 but does not ablate this.

## Foundational Learning

- Concept: **Catastrophic forgetting in continual learning**
  - Why needed here: The paper's core premise is that naive continual pre-training on Korean destroys multilingual capabilities; understanding this stability-plasticity tension is essential to grasp why distillation is necessary.
  - Quick check question: Can you explain why updating all model parameters on Korean-only data would degrade English performance?

- Concept: **Word-sense disambiguation (WSD) and homophony**
  - Why needed here: Korean's Sino-Korean vocabulary creates systematic ambiguity (35% of Hanja entries are homophones); the method's success depends on understanding why surface form alone is insufficient.
  - Quick check question: Why can't a model infer that "의사" means "doctor" vs. "intention" from Hangul spelling alone?

- Concept: **Attention masking and causal language modeling**
  - Why needed here: The modified attention mask (Figure 3) is non-standard—understanding how transformers restrict information flow is prerequisite to implementing the Hanja expansion groups correctly.
  - Quick check question: In a standard causal mask, which tokens can position t attend to? How does HanjaBridge modify this?

## Architecture Onboarding

- Component map:
  - Tokenizer expansion -> Add Hanja characters to vocabulary (new embedding rows)
  - Hanja dictionary lookup -> Map Korean tokens to candidate Hanja lists (external resource)
  - Input augmentation layer -> Insert Hanja candidates in-line after Korean tokens
  - Modified attention mask -> Block inter-candidate attention within expansion groups
  - LM head -> Compute loss only on original Korean token positions (mt=1)
  - Teacher model -> Frozen copy of original pretrained model
  - Instance queue D -> Rolling buffer of teacher hidden vectors for contrastive distillation
  - Distillation loss -> Token-wise KL divergence between teacher and student similarity distributions

- Critical path:
  1. Hanja dictionary quality -> candidate coverage -> disambiguation signal strength
  2. Attention mask correctness -> candidate independence -> semantic clarity
  3. Distillation alignment -> forgetting prevention -> English performance preservation
  4. Candidate count k -> noise/signal balance -> optimal k=8 per experiments

- Design tradeoffs:
  - **k (candidate count)**: Higher k provides more disambiguation signal but increases noise. Paper finds k=8 optimal; k=16 degrades slightly.
  - **Hanja at inference**: Using Hanja at inference (+0.7% accuracy) vs. Korean-only (no token overhead). Paper recommends Korean-only for deployment.
  - **Layer unfreezing**: Student unfreezes only embeddings and select transformer blocks—trades full adaptation capacity for stability.
  - **Assumption**: Sequence length increases during training (65,536 tokens used) may require memory optimization.

- Failure signatures:
  - English benchmark scores drop significantly -> distillation loss too weak or teacher-student misalignment
  - Korean semantic tasks don't improve -> Hanja dictionary coverage insufficient or attention mask incorrectly implemented
  - Model attends uniformly to all Hanja candidates (no focus differentiation) -> training insufficient or learning rate issues
  - Token count explodes at inference -> Hanja augmentation incorrectly applied at inference time (should be training-only)

- First 3 experiments:
  1. **Validate attention focusing**: Replicate Table 3—measure attention accuracy on correct Hanja candidates across training steps. If accuracy doesn't increase, attention masking or loss computation is incorrect.
  2. **Ablate candidate count k**: Test k∈{0,2,4,8,16} on KoBALT/KoBEST. Confirm k=8 is optimal and k=0 (distillation-only) underperforms full method.
  3. **Verify forgetting prevention**: Compare English benchmark scores (BoolQ, COPA, Hellaswag, WiC) between Full CPT baseline and HanjaBridge. Confirm HanjaBridge preserves teacher-level English performance.

## Open Questions the Paper Calls Out
- Can HanjaBridge be effectively generalized to other low-resource languages with similar logographic-phonetic writing systems, such as Vietnamese (Sino-Vietnamese vocabulary) or Japanese (Kanji), and what adaptations would be required?
- How does the quality, coverage, and domain specificity of the Hanja-Hangul mapping dictionary impact model performance, particularly for neologisms and specialized technical terms?
- What is the optimal dynamic for selecting the number of Hanja candidates (k) per word based on ambiguity level or context, rather than using a fixed value?

## Limitations
- The method's effectiveness depends entirely on the quality and coverage of the Hanja-Hangul mapping dictionary, which is not fully specified in the paper.
- Attention focusing improves from 10.2% to 30.8% but remains far from perfect, suggesting incomplete disambiguation signals for many tokens.
- The optimal candidate count k=8 is found empirically, but the relationship between k, ambiguity level, and performance is not systematically explored.

## Confidence
- **High Confidence**: The core mechanism (Hanja candidates + attention masking + distillation) is technically sound and the experimental results (21% KoBALT improvement, English preservation) are reproducible with the given methodology.
- **Medium Confidence**: The claim that HanjaBridge prevents catastrophic forgetting is supported by the English benchmark results, but the distillation mechanism's effectiveness depends on correct implementation of the queue-based contrastive loss and teacher-student alignment.
- **Low Confidence**: The generalizability to domain-specific or neologism terms is uncertain because the Hanja dictionary coverage is not specified. The paper acknowledges this limitation but does not quantify its impact.

## Next Checks
1. **Hanja dictionary coverage validation**: Measure the percentage of ambiguous Korean words in a representative corpus that have Hanja candidates in the dictionary. Test the method's performance drop when the dictionary is artificially restricted to top-50% or top-25% most common words.
2. **Attention focusing quality**: Implement the attention mask and measure attention accuracy on correct Hanja candidates during training. If accuracy plateaus below 50%, investigate whether candidates are semantically too similar or the mask is incorrectly implemented.
3. **Forgetting prevention verification**: Train a Full CPT baseline (naive Korean-only fine-tuning) alongside HanjaBridge and monitor English benchmark scores (BoolQ, COPA, Hellaswag, WiC) throughout training. Confirm that HanjaBridge maintains teacher-level performance while Full CPT degrades.