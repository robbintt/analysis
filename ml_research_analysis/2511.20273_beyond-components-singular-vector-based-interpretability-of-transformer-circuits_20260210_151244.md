---
ver: rpa2
title: 'Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits'
arxiv_id: '2511.20273'
source_url: https://arxiv.org/abs/2511.20273
tags:
- head
- layer
- directions
- singular
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a fine-grained directional interpretability\
  \ framework for transformer circuits, decomposing attention and MLP components into\
  \ orthogonal singular directions to reveal superposed and independent subfunctions.\
  \ By applying singular value decomposition to augmented weight matrices and learning\
  \ sparse direction-level masks, the method identifies compact subspaces that capture\
  \ most of the model\u2019s behavior on tasks like Indirect Object Identification\
  \ (IOI), Gender Pronoun (GP), and Greater Than (GT)."
---

# Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits

## Quick Facts
- arXiv ID: 2511.20273
- Source URL: https://arxiv.org/abs/2511.20273
- Authors: Areeb Ahmad; Abhinav Joshi; Ashutosh Modi
- Reference count: 40
- One-line primary result: Less than 10% of singular directions suffice to reconstruct transformer task behavior with minimal KL divergence.

## Executive Summary
This paper introduces a fine-grained directional interpretability framework for transformer circuits by decomposing attention and MLP components into orthogonal singular directions. By applying singular value decomposition to augmented weight matrices and learning sparse direction-level masks, the method identifies compact subspaces that capture most of the model's behavior on tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT). Experiments on GPT-2 Small show that sparse subsets of singular directions faithfully reconstruct outputs while revealing interpretable subfunctions.

## Method Summary
The framework constructs augmented weight matrices for all transformer components (QK, OV, MLP projections) by folding biases into inputs, then applies SVD to obtain orthogonal singular directions. Learnable diagonal masks are optimized via KL divergence plus L1 regularization to identify sparse subsets of directions that preserve task-specific behavior. The method treats singular vectors as independent computational axes and uses scalar interventions on logit-aligned directions to validate causal interpretations.

## Key Results
- Less than 10% of singular directions suffice to faithfully reconstruct model outputs with minimal KL divergence on IOI, GP, and GT tasks
- Circuit components like "name movers" activate only sparse subsets of these directions
- Scalar interventions on logit-aligned OV directions successfully flip gender pronoun predictions with near-100% accuracy
- The framework reveals that transformer computations are distributed across low-rank subspaces rather than concentrated in single components

## Why This Works (Mechanism)

### Mechanism 1: Unified Linear Decomposition via Augmented SVD
Transformer attention and MLP computations can be expressed as superpositions of orthogonal subfunctions along singular directions. The method folds biases into weight matrices by augmenting inputs with a constant dimension, then applies SVD to all core transformations—QK, OV, and MLP projections—yielding a shared orthonormal basis where each rank-1 term represents an independent computational axis. This assumes singular vectors of augmented matrices approximately align with functionally meaningful subroutines rather than mere statistical variance.

### Mechanism 2: Sparse Direction-Level Masking via KL-Regularized Optimization
A small subset (~5-10%) of singular directions faithfully reconstructs task-specific model behavior. The method introduces learnable diagonal masks that scale singular values and optimizes KL divergence plus L1 regularization jointly across all components. Clean/corrupted input pairs encourage robustness to context variations. This assumes task behavior is concentrated in sparse, low-rank subspaces that survive L1 regularization without excessive KL penalty.

### Mechanism 3: Logit Receptors—Fixed Output-Aligned Directions Steered by Scalar Activations
Certain OV singular directions act as fixed "receptors" in logit space, each aligned with specific token sets; input-dependent scalar activations modulate their influence. For each OV direction, the method computes a token-preference vector that defines a fixed receptor, while attention-weighted context produces scalar activations that gate how strongly that receptor fires. Interventions swapping these scalars causally flip predictions.

## Foundational Learning

- **Singular Value Decomposition (SVD)**: The entire framework hinges on decomposing weight matrices into orthogonal rank-1 components; understanding how SVD provides orthonormal bases and how singular values indicate "importance" is prerequisite. Quick check: Given matrix M = UΣV^⊤, what does the i-th column of U represent, and how would setting σᵢ → 0 affect M?

- **Transformer Circuit Formulation (Elhage et al. 2021)**: The paper builds on the residual-stream view where attention heads and MLPs read from and write to a shared stream; understanding QK/OV circuits is essential to grasp what augmented matrices represent. Quick check: In an attention head, which matrix determines *where* to attend (QK) versus *what* to write back (OV)?

- **Sparsity-regularized optimization (L1 + KL)**: The mask learning objective balances behavioral fidelity (KL) against parsimony (L1); interpreting the trade-off and choosing λ correctly determines whether results are meaningful. Quick check: If λ is too large, what happens to the learned masks? If too small?

## Architecture Onboarding

- **Component map**: Input x → [Augment to [1,x]] → W_aug (QK/OV/MLP) → SVD → (U, Σ, V) → Mask M on Σ → Reconstruct fW_aug = U Σ M V^⊤ → Forward pass with masked weights → KL(original output, masked output)

- **Critical path**:
  1. Augment all weight matrices (QK, OV, MLP in/out) to fold biases
  2. Compute SVD once per component (offline, O(d³) per layer)
  3. Initialize masks M = I (or small random)
  4. For each batch: forward pass through masked model → compute KL + L1 → gradient update on M only
  5. Early-stop when validation KL stabilizes; analyze non-zero mask entries

- **Design tradeoffs**:
  - QK vs OV/MLP masking: QK uses only primary mask (no complementary subspace) to avoid spurious attention-kernel interference; OV/MLP retain both masked and complementary terms to preserve covariance structure
  - Sparsity vs fidelity: Higher λ → sparser masks but potentially worse KL; empirically λ ~ 1e-4 worked for IOI/GP/GT
  - Corruption strategy: Clean/corrupted pairs help isolate task-relevant directions; choice of corruption affects what is discovered

- **Failure signatures**:
  - Dense masks with high KL: Regularization too weak or task not well-localized in low-rank subspace
  - Near-zero masks everywhere: Regularization too strong or clean/corrupted pairs too similar
  - QK attention collapse: If complementary term accidentally included, attention scores become incoherent
  - Interventions don't flip predictions: Logit receptors may be weak or directions not truly causal

- **First 3 experiments**:
  1. Sanity check: On IOI task, verify that zero-masking all directions (M=0) produces random output; full masking (M=I) recovers original KL ≈ 0
  2. Sparsity sweep: Run mask optimization with λ ∈ {1e-5, 1e-4, 1e-3}; plot sparsity vs KL to find knee point
  3. Causal validation: For GP task, implement scalar swap intervention on top-3 gender-aligned OV directions; verify prediction flip rate > 90%

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the singular vector-based decomposition framework scale effectively to larger language models and more complex reasoning tasks? The authors state this remains an open question, as experiments were restricted to GPT-2 Small on standard benchmarks.

- **Open Question 2**: Do emergent behaviors arise from interactions between singular directions across different components that are invisible to single-matrix analysis? The paper notes that analyzing each augmented matrix independently may obscure emergent behaviors from interactions across components.

- **Open Question 3**: How can one distinguish true causal mediation from representational correlation within the identified singular directions? The authors admit that while masking encourages output faithfulness, disentangling true causal mediation from representational correlation remains an open challenge.

## Limitations
- The method depends on the assumption that task-relevant computation is aligned with SVD axes of augmented weight matrices
- The corruption strategy for clean/corrupted input pairs is underspecified
- The logit receptor interpretation lacks quantitative validation across multiple models and tasks
- Mask optimization is sensitive to λ and requires careful early stopping

## Confidence
- **High confidence**: The SVD decomposition and augmented matrix construction are mathematically sound and reproducible
- **Medium confidence**: The sparsity optimization finds compact subspaces for the three tested tasks; results are internally consistent but may not generalize
- **Low confidence**: The logit receptor causal interpretation and the claim that singular directions represent independent subfunctions rather than statistical artifacts

## Next Checks
1. **Ablation across models**: Apply the framework to GPT-2 Medium and GPT-Neo 125M on IOI; compare sparsity patterns and KL retention curves to assess whether the same singular directions are task-relevant or model-specific
2. **Intervention generalization**: For GP, swap scalar activations between non-gender-aligned OV directions (random pairs); verify that predictions remain stable unless swapping true gender receptors, confirming causality
3. **Alternative basis comparison**: Replace SVD with randomized SVD or ICA on augmented matrices; if the resulting masks show similar sparsity and KL retention, the findings are more likely to reflect true functional decomposition rather than SVD-specific artifacts