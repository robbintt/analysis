---
ver: rpa2
title: 'How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive
  Evaluation of LLMs'
arxiv_id: '2507.14307'
source_url: https://arxiv.org/abs/2507.14307
tags:
- llms
- aspect
- imperfective
- causal
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models (LLMs) process
  temporal meaning expressed by aspect in narratives, using experimental materials
  from human cognitive studies. The researchers developed an Expert-in-the-Loop probing
  pipeline that systematically evaluates LLM responses through multiple prompt variations
  and analyses.
---

# How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs

## Quick Facts
- arXiv ID: 2507.14307
- Source URL: https://arxiv.org/abs/2507.14307
- Reference count: 40
- Key outcome: LLMs over-rely on prototypical linguistic patterns for aspect processing, producing inconsistent judgments and struggling with causal reasoning derived from aspect.

## Executive Summary
This study investigates how large language models process temporal meaning expressed by grammatical aspect in narratives, using experimental materials from human cognitive studies. The researchers developed an Expert-in-the-Loop probing pipeline that systematically evaluates LLM responses through multiple prompt variations and analyses. They conducted three experiments examining semantic understanding (truth-value judgments), working memory activation (word completion tasks), and pragmatic inference (causal reasoning). The results show that LLMs over-rely on prototypical linguistic patterns, producing inconsistent aspectual judgments and struggling with causal reasoning derived from aspect, suggesting they process aspect fundamentally differently from humans and lack robust narrative understanding.

## Method Summary
The study used an Expert-in-the-Loop pipeline to evaluate LLM comprehension of grammatical aspect (perfective vs. imperfective) in narratives through three experiments. The pipeline generated 30 prompt variations per stimulus using FORMATSPREAD formatting perturbations and paraphrasing. They tested 7 models (Gemma2, Llama3.1, Qwen2, GPT-4o) on 16 narrative pairs adapted from human cognitive studies. Truth-value judgments measured semantic understanding, word completion tasks assessed working memory activation, and open-ended causal questioning examined pragmatic inference. Statistical analysis used multilevel models in R with human baselines from prior research.

## Key Results
- LLMs achieved 88% accuracy on perfective truth-value judgments but only 18% on non-prototypical imperfective cases, compared to 71% human baseline.
- Word-completion accuracy dropped ~33% when probes appeared near narrative effects versus near Cause 1, indicating failure to maintain causal focus.
- Larger models (70B) showed improved causal reasoning on open-ended questions but no clear performance advantage on structured aspectual tasks.

## Why This Works (Mechanism)

### Mechanism 1: Distributional Pattern Matching Over Semantic Understanding
- Claim: LLMs succeed at aspect processing primarily through statistical association with high-frequency prototypical patterns rather than conceptual grasp of temporal semantics.
- Mechanism: Perfective aspect + Accomplishment events co-occur frequently in training corpora; LLMs exploit this correlation, achieving 88% accuracy on perfective truth-value judgments. When this pattern breaks (imperfective + Accomplishment = non-prototypical), accuracy collapses to 18% vs. 71% human baseline.
- Core assumption: LLMs encode aspect as distributional co-occurrence statistics rather than grounded temporal concepts.
- Evidence anchors:
  - [abstract] "LLMs over-rely on prototypicality, produce inconsistent aspectual judgments"
  - [Section 5.1] Table 2 shows 88% accuracy for perfective-positive vs. 18% for imperfective-negative (semantically correct responses)
  - [corpus] Limited direct corroboration; corpus neighbor "LLMs Struggle with NLI for Perfect Aspect" (arxiv:2508.11927) confirms cross-linguistic aspect difficulties, but FMR=0.526 with no citations—evidence is suggestive but not strong.
- Break condition: If LLMs possessed genuine semantic understanding, performance should not degrade so dramatically on non-prototypical but grammatically valid constructions.

### Mechanism 2: Local Attention Without Distal Integration
- Claim: LLMs attend to pragmatic non-prototypicality in immediate context but fail to maintain causal relevance across narrative distance.
- Mechanism: Word-completion accuracy drops ~33% when probes are placed near the Effect vs. near Cause 1. LLMs show initial sensitivity to imperfective rarity but cannot sustain the "in-focus" status that humans maintain via episodic memory reactivation.
- Core assumption: Transformer attention mechanisms differ fundamentally from human working memory; they lack explicit mechanisms for maintaining discourse-level situation models over extended text.
- Evidence anchors:
  - [Section 5.2] "LLM completions have significantly higher match frequencies when the probe directly follows Cause 1 and are reduced after the effect"
  - [Section 6] "LLMs tend to not maintain causal pragmatic focus on imperfective events over the distance of the narrative"
  - [corpus] No direct corpus support for this specific mechanism; related work on temporal dynamics (arxiv:2501.05552, FMR=0.587) addresses meaning evolution but not narrative integration.
- Break condition: If LLMs constructed human-like situation models, word-completion rates should remain elevated for imperfective events near the Effect due to causal reactivation.

### Mechanism 3: Scaling Improves Open-Ended Inference But Not Core Semantics
- Claim: Larger models show improved causal reasoning on open-ended questions but this does not transfer to structured semantic tasks.
- Mechanism: Parameter scaling (7B → 72B) correlates with more human-like causal inference for imperfective causes, but larger models still fail truth-value judgments and word-completion tasks at comparable rates to smaller models.
- Core assumption: Causal inference and aspect semantics may be processed via separate mechanisms in LLMs; scaling benefits one pathway but not the other.
- Evidence anchors:
  - [Section 5.3] Figure 5 shows larger models trend toward human-like Cause 1 attribution for imperfective
  - [Section 6] "No clear relationship between model performance on structured aspectual tasks and their performance on open-ended causal inference tasks"
  - [corpus] Corpus evidence is weak; no scaling-specific aspect studies identified.
- Break condition: If scaling uniformly improved aspect comprehension, larger models should dominate across all three experiments, which they do not.

## Foundational Learning

- **Concept: Grammatical Aspect (Perfective vs. Imperfective)**
  - Why needed here: Central experimental manipulation; perfective marks completed events ("walked"), imperfective marks ongoing events ("was walking"). Critical for understanding why non-prototypical combinations challenge LLMs.
  - Quick check question: Given "She was walking to the store" (imperfective), can you infer she arrived at the store? (Answer: No—the event is presented as ongoing without entailment of completion.)

- **Concept: Lexical Aspect (Accomplishment Events)**
  - Why needed here: All experimental stimuli use Accomplishment events (events with natural endpoints: "wash dishes" → dishes washed). Understanding this is prerequisite to grasping why imperfective + Accomplishment is non-prototypical.
  - Quick check question: Does "run to the park" have a natural endpoint? Does "run in the park"? (Answer: Yes to the first; no to the second—this distinguishes Accomplishments from Activities.)

- **Concept: Situation Model / Mental Model in Reading**
  - Why needed here: Human readers construct dynamic mental representations of narratives; aspect signals whether events should remain "in focus" (imperfective) or recede (perfective). LLMs' failure to maintain this model is a core finding.
  - Quick check question: When reading "Rob was washing dishes" followed later by "Suddenly there was a loud noise," why might a reader connect these events? (Answer: The imperfective keeps the dish-washing event active in the situation model as an ongoing, potentially relevant cause.)

## Architecture Onboarding

- **Component map:**
  - Expert-in-the-Loop Pipeline -> Prompt Generator -> Prompt Perturbation Module -> Multi-Model Inference Engine -> Response Parser -> Statistical Analysis Module

- **Critical path:**
  1. Cognitive scientist uploads stimulus file (CSV with narrative variants)
  2. Define independent variables (aspect: perfective/imperfective; probe location: early/late)
  3. Pipeline generates 30 prompt variations per stimulus
  4. Run inference across all 7 models
  5. Parse responses and compute accuracy/frequency metrics
  6. Fit mixed-effects models: `accuracy ~ aspect * polarity + (1 | narrative)`
  7. Expert reviews intermediate results; iterate if needed

- **Design tradeoffs:**
  - Prompt perturbation adds robustness but increases compute cost 30×
  - Open-ended responses require GPT-4o-based auto-classification (κ=0.93 reliability) vs. human annotation (scalability vs. ground truth)
  - 4-bit quantization for models >9B parameters trades precision for feasibility
  - Single-stimulus presentation avoids order effects but limits efficiency

- **Failure signatures:**
  - LLM defaults to "True" for positive-polarity phrases regardless of aspect (prototypicality bias)
  - Word-completion responses ignore target word when probe appears late in narrative (distal integration failure)
  - Causal attribution nearly absent for perfective Cause 1 events, more so than humans (exaggerated completion inference)
  - High variance across prompt paraphrases indicates responses are prompt-artifacts, not stable representations

- **First 3 experiments:**
  1. **Truth-value judgment validation:** Present 5 stories with imperfective critical sentences; verify model accuracy <30% on negative-polarity final state judgments (semantic failure confirmation).
  2. **Probe location sensitivity test:** Run word-completion with near-Cause1 vs. near-Effect placement; confirm >25% drop in target-word completion rate for late probes.
  3. **Scaling interaction check:** Compare 8B vs. 70B parameter models on open-ended causal questions; verify larger models show 10-15% higher Cause 1 attribution for imperfective but not perfective conditions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the gap between LLMs and humans in processing non-prototypical imperfective aspect shrink with further model scaling, or does it represent a fundamental architectural limitation?
- Basis in paper: [explicit] "It remains uncertain whether further scaling will bridge the gap between LLMs and human-like aspectual processing."
- Why unresolved: The study found that while larger models showed some improvement in open-ended causal inference, no clear relationship emerged between model size and performance on structured aspectual tasks (truth-value judgments, word completion).
- What evidence would resolve it: Evaluating aspect comprehension across a wider range of model scales (including frontier models beyond those tested) and architectures to determine if performance follows a predictable scaling law or plateaus.

### Open Question 2
- Question: Do LLMs maintain separate processing mechanisms for aspectual semantics versus aspect-mediated causal reasoning, or do these deficits share a common source?
- Basis in paper: [explicit] "There is no clear relationship between model performance on structured aspectual tasks and their performance on open-ended causal inference tasks, indicating that aspect comprehension and causal reasoning may be processed separately in LLMs."
- Why unresolved: Models that excelled at causal inference (e.g., Llama-3.1-70B) did not perform well on semantic truth-value judgments, suggesting dissociation, but the sample of models is limited and the underlying mechanisms remain unclear.
- What evidence would resolve it: Probing internal representations (e.g., attention patterns, layer-wise activations) during both task types to assess whether the same or different neural substrates are involved.

### Open Question 3
- Question: To what extent do LLMs' explicit, declarative knowledge of grammatical concepts (e.g., defining "aspect") dissociate from their implicit, applied understanding of those same concepts in context?
- Basis in paper: [explicit] "Future research should explore how the tension between declarative and implicit knowledge extends to other linguistic and cognitive domains."
- Why unresolved: The authors note LLMs can articulate aspect definitions yet fail at implicit application; whether this pattern generalizes across linguistic phenomena is unknown.
- What evidence would resolve it: Systematic comparison of LLM performance on explicit metalinguistic tasks (definitions, rule explanations) versus implicit applications (judgments in context) across multiple grammatical domains beyond aspect.

## Limitations

- The experimental materials represent only 16 narrative pairs from one source, limiting generalizability across discourse types and aspectual phenomena.
- The word-completion task cannot definitively distinguish between genuine semantic understanding and shallow statistical patterns.
- The specific mechanism (distributional statistics vs. alternative explanations like prompt sensitivity) remains unconfirmed despite robust experimental evidence.

## Confidence

**High confidence**: LLMs show systematic failures on non-prototypical aspect constructions (perfective: 88% accuracy vs. imperfective: 18% accuracy on negative polarity), and word-completion rates drop significantly when probes appear near narrative effects.

**Medium confidence**: The interpretation that these failures indicate distributional pattern matching rather than semantic understanding is plausible but not definitively proven. Alternative explanations (prompt sensitivity, limited narrative integration capacity) could contribute to the observed patterns.

**Low confidence**: The scaling analysis suggesting larger models improve causal reasoning but not core semantics is tentative, based on only 7 models and without systematic scaling studies.

## Next Checks

1. **Cross-linguistic generalization**: Test the same experimental paradigm with languages that mark aspect differently (e.g., Russian, Turkish) to determine if LLM failures are universal or language-specific.

2. **Alternative interpretation testing**: Conduct controlled experiments varying prompt structure systematically (narrative context vs. isolated sentences, different syntactic positions) to distinguish between distributional pattern matching and prompt-induced artifacts as explanations for the accuracy differences.

3. **Discourse-level probing**: Implement follow-up experiments measuring situation model maintenance using longer narratives and multiple aspectual cues to test whether LLM failures reflect fundamental inability to construct integrated narrative representations or limitations in the current experimental design.