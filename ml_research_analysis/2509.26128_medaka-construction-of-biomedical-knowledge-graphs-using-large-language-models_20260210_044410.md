---
ver: rpa2
title: 'MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models'
arxiv_id: '2509.26128'
source_url: https://arxiv.org/abs/2509.26128
tags:
- drug
- biomedical
- knowledge
- information
- leaflets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MEDAKA, a biomedical knowledge graph constructed
  from drug leaflets using a web scraper and large language models. The proposed pipeline
  extracts structured triples from unstructured text, capturing clinically relevant
  attributes such as side effects, warnings, contraindications, ingredients, dosage
  guidelines, storage instructions, and physical characteristics.
---

# MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models

## Quick Facts
- **arXiv ID**: 2509.26128
- **Source URL**: https://arxiv.org/abs/2509.26128
- **Reference count**: 36
- **Primary result**: 41,142-node biomedical knowledge graph built from drug leaflets with 96.6% precision

## Executive Summary
MEDAKA is a biomedical knowledge graph constructed from drug leaflets using a web scraping pipeline and large language models to extract structured triples from unstructured text. The dataset captures clinically relevant attributes including side effects, warnings, contraindications, ingredients, dosage guidelines, storage instructions, and physical characteristics. With 41,142 nodes and 466,359 edges, MEDAKA demonstrates high precision (96.6-96.9%) and reasonable recall (87.2% on subset) while claiming broader coverage than existing biomedical KGs. The modular design enables potential adaptation to other domains for applications in patient safety monitoring and drug recommendation.

## Method Summary
The pipeline employs web scraping to collect drug leaflet data, followed by LLM-based extraction of structured triples from unstructured text. The process captures multiple clinically relevant attributes through prompt engineering and systematic extraction rules. Quality evaluation combines human annotation of 1,000 triples with LLM-as-a-judge assessment of 3,000 triples, achieving high precision scores. The modular architecture separates data collection, extraction, and validation components, theoretically enabling adaptation to other biomedical domains.

## Key Results
- Dataset comprises 41,142 nodes and 466,359 edges covering diverse drug information attributes
- Precision achieved 96.6% via human annotation and 96.9% via LLM-as-a-judge evaluation
- Recall measured at 87.2% on a subset of 140 triples
- Claims broader coverage than existing biomedical KGs like DrugBank and SIDER

## Why This Works (Mechanism)
The approach leverages LLMs' natural language understanding capabilities to parse complex medical text into structured triples, overcoming the challenge of extracting clinically relevant information from unstructured drug leaflets. The web scraping component provides comprehensive coverage of available drug information sources, while the modular design allows for systematic extraction of multiple attribute types. The combination of human and automated quality assessment provides robust validation of extracted knowledge.

## Foundational Learning
- **Knowledge Graph Construction**: Understanding how to represent biomedical relationships as structured triples is essential for creating searchable, interconnected drug information. Quick check: Verify entity and relation type coverage against established KGs.
- **LLM-based Information Extraction**: LLMs can parse unstructured medical text into structured formats when properly prompted and validated. Quick check: Test extraction accuracy on diverse medical document types.
- **Quality Assessment Methods**: Combining human annotation with LLM-as-a-judge provides comprehensive validation coverage. Quick check: Compare inter-annotator agreement and LLM consistency across evaluation samples.

## Architecture Onboarding

**Component Map**: Web Scraper -> LLM Extractor -> Quality Validator -> Knowledge Graph

**Critical Path**: Data collection → Triple extraction → Quality validation → Graph construction

**Design Tradeoffs**: The pipeline prioritizes precision over recall, accepting potential information loss for higher accuracy. The use of LLM-as-a-judge for large-scale validation trades computational cost for scalability, while human annotation provides gold-standard verification for smaller samples.

**Failure Signatures**: Extraction failures manifest as missing or incorrect attribute triples, particularly for complex dosage instructions or subtle contraindications. Quality validation failures appear as precision-recall imbalances, indicating either over-extraction or under-extraction problems.

**First 3 Experiments**:
1. Validate extraction accuracy on a benchmark set of drug leaflets with known gold-standard triples
2. Test coverage completeness by comparing extracted attributes against comprehensive drug databases
3. Evaluate end-to-end pipeline performance on a new domain (e.g., medical device documentation)

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Recall evaluation limited to subset of 140 triples, potentially underestimating information loss
- LLM-as-a-judge evaluation may introduce bias since same models used in construction
- Clinical application validation remains theoretical without deployed real-world implementations

## Confidence
- Dataset construction methodology: High
- Precision metrics: High
- Recall evaluation: Medium (subset-based)
- Coverage claims: Medium (comparative analysis limited)
- Clinical application potential: Low (theoretical at this stage)

## Next Checks
1. Conduct systematic comparison of MEDAKA's coverage against DrugBank, SIDER, and other established biomedical KGs using standardized entity and relation type metrics
2. Implement cross-validation where a separate LLM or human experts evaluate a random sample of triples not used in training the extraction pipeline
3. Deploy a pilot clinical application (e.g., automated drug safety alert system) to empirically validate the dataset's utility for patient safety monitoring