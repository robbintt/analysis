---
ver: rpa2
title: Two CFG Nahuatl for automatic corpora expansion
arxiv_id: '2512.14239'
source_url: https://arxiv.org/abs/2512.14239
tags:
- sentences
- nawatl
- language
- corpus
- gnaw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces two context-free grammars for expanding Nawatl
  corpora, a low-resource indigenous Mexican language. The grammars generate syntactically
  valid artificial sentences to enhance embedding learning and semantic similarity
  tasks.
---

# Two CFG Nahuatl for automatic corpora expansion

## Quick Facts
- arXiv ID: 2512.14239
- Source URL: https://arxiv.org/abs/2512.14239
- Reference count: 18
- Grammar-based corpus expansion increased semantic similarity task accuracy (Kendall's τ) from 0.493 to 0.540 (+9%) for Nawatl language processing.

## Executive Summary
This study introduces two context-free grammars for expanding Nawatl corpora, a low-resource indigenous Mexican language. The grammars generate syntactically valid artificial sentences to enhance embedding learning and semantic similarity tasks. The more advanced grammar, µGNAW⊕1, better captures Nawatl's grammatical structure, leading to improved performance. When used to expand the π-YALLI corpus and train FastText embeddings, µGNAW⊕1 increased semantic similarity task accuracy (Kendall's τ) from 0.493 to 0.540, outperforming some large language models. This demonstrates that targeted grammar-based corpus expansion can significantly benefit low-resource language processing.

## Method Summary
The method uses two non-recursive context-free grammars (µGNAW⊕0 and µGNAW⊕1) to generate Nawatl sentences. µGNAW⊕0 provides a baseline with Indo-European-like structures, while µGNAW⊕1 incorporates Nawatl-specific features like VSO word order and agglutinative morphology. The grammars use knowledge bases of 27 nouns, 27 verbs, and morphological markers. Generated sentences undergo semantic filtering (animate/inanimate noun-verb matching), post-processing (symbolic tagging, possessive morphology, paragraph segmentation, connector injection), and are merged with the authentic π-YALLI corpus. FastText Skip-Gram embeddings (300d, window=5, 20 iterations) are trained on the expanded corpus and evaluated on semantic similarity ranking using Kendall's τ correlation.

## Key Results
- µGNAW⊕1 grammar produced artificial sentences that better capture Nawatl's grammatical structure than µGNAW⊕0
- FastText embeddings trained on expanded corpus showed Kendall's τ increase from 0.493 to 0.540 (+9%)
- The expanded corpus approach outperformed pre-trained large language models (Llama3-8B, Mistral-7B) on semantic similarity tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Grammar-based corpus expansion can improve static embedding quality for low-resource languages.
- **Mechanism:** Context-free grammars (CFGs) generate large volumes of syntactically valid artificial sentences. These are merged with authentic corpora, providing additional training data for embedding algorithms that would otherwise suffer from data sparsity.
- **Core assumption:** Syntactically valid sentences, even if semantically weak, provide useful distributional signal for learning word representations.
- **Evidence anchors:**
  - [abstract] "The results show an improvement compared to the results obtained using only the original corpus without artificial expansion"
  - [section 6.3] "FastText algorithm... increasing the Kendall τMAX from 0.493 to 0.540 (+9% increase)"
  - [corpus] Limited corpus signals; related work shows similar CFG-based augmentation (arXiv:2510.04945) but no direct replications yet.
- **Break condition:** If generated sentences introduce systematic syntactic biases absent from real language, embeddings may learn spurious patterns.

### Mechanism 2
- **Claim:** Linguistically-informed grammar design produces better synthetic training data than generic templates.
- **Mechanism:** µGNAW⊕1 encodes Nawatl-specific features (VSO word order, agglutinative markers, verb-centric structures, possessive morphology). This yields artificial sentences closer to authentic Nawatl syntax than µGNAW⊕0, which uses Indo-European-derived phrase structures.
- **Core assumption:** Grammar rules that better approximate target language structure produce more useful synthetic training data.
- **Evidence anchors:**
  - [section 4.2] "µGNAW⊕1—better captures Nawatl's grammatical structure... the verb assumes a central and predominant role"
  - [section 6.3] "our hypothesis regarding the expansion of the Nawatl corpus proved more reliable with grammar µGNAW⊕1"
  - [corpus] No independent verification; related papers on Nawatl (arXiv:2601.02303) focus on dialect classification, not grammar-based expansion.
- **Break condition:** If target language exhibits significant dialectal variation (29 Nawatl varieties noted), a single grammar may overfit one dialect.

### Mechanism 3
- **Claim:** Semantic filtering and post-processing reduce combinatorial explosion while preserving utility.
- **Mechanism:** Animate/inanimate filters exclude implausible sentences (e.g., "fruit walks"). Symbolic tags defer marker selection to post-processing with authentic corpus distribution. Paragraph segmentation and rhetorical connector injection mimic document structure.
- **Core assumption:** Removing semantically anomalous sentences reduces noise without eliminating useful syntactic patterns.
- **Evidence anchors:**
  - [section 5.1] "filter based on the association between verbs and animate/inanimate nouns... allows a significant subset of sentences that lack easily intelligible semantics to be eliminated at low computational cost"
  - [section 5.2] Symbolic tagging limits combinatorial explosion from ~1.18×10¹² potential sentences to manageable corpus
  - [corpus] No corpus evidence on whether filtered vs. unfiltered synthetic data yields different embedding quality.
- **Break condition:** Aggressive filtering may exclude valid low-frequency constructions; weak filtering retains noise.

## Foundational Learning

- **Context-Free Grammars (CFGs):**
  - Why needed here: CFGs formally define sentence generation rules without recursion context, enabling systematic synthetic data production.
  - Quick check question: Can you explain why CFGs are "context-free" and how production rules expand non-terminals?

- **Static vs. Contextual Embeddings:**
  - Why needed here: The paper trains FastText (static) embeddings; understanding why contextual models (BERT) require 10-100M tokens clarifies the constraint.
  - Quick check question: What is the difference between Word2Vec/FastText embeddings and BERT contextual representations?

- **Kendall's τ Correlation:**
  - Why needed here: Semantic similarity evaluation uses Kendall's τ to compare model rankings against human rankings.
  - Quick check question: How does Kendall's τ measure ranking agreement, and what does τ = 0.540 indicate?

## Architecture Onboarding

- **Component map:**
  1. Grammar Definition (Prolog KB): Terminal nodes (nouns, verbs, markers) + production rules (VSO, VO, VS patterns)
  2. Sentence Generator: Applies grammar rules in generative mode
  3. Semantic Filter: Animate/inanimate noun-verb matching
  4. Post-processor: Symbolic tag replacement, possessive morphology, paragraph segmentation, connector injection
  5. Corpus Merger: Combines π-YALLI (authentic) + synthetic sentences with spelling unification
  6. Embedding Trainer: FastText Skip-Gram (300d, window=5, iterations=20)
  7. Evaluator: Semantic similarity ranking with Kendall's τ

- **Critical path:** Grammar design → Sentence generation → Filtering → Post-processing → Corpus merge → FastText training → Semantic similarity evaluation

- **Design tradeoffs:**
  - **Non-recursive grammar:** Limits sentence complexity but prevents infinite generation; authors deliberately avoid recursion
  - **3rd person singular only:** Reduces verb conjugation complexity but limits expressiveness
  - **Symbolic tags vs. direct generation:** Controls combinatorial explosion but adds post-processing complexity
  - **Static embeddings vs. LLMs:** Faster, requires less data, but may underperform large models on complex tasks

- **Failure signatures:**
  - Kendall's τ ≈ 0.24 (as with pre-trained FastText/Wikipedia): Corpus mismatch or insufficient domain coverage
  - High redundancy in generated sentences: Filter or symbolic tag distribution not applied correctly
  - Semantic anomalies (e.g., "corn eats rabbit"): Animate/inanimate filter not enforced

- **First 3 experiments:**
  1. **Baseline replication:** Train FastText on π-YALLI alone; measure Kendall's τ on semantic similarity task (expect ~0.49)
  2. **Grammar comparison:** Generate synthetic corpora with µGNAW⊕0 and µGNAW⊕1 separately; merge with π-YALLI; compare τ values to isolate grammar design effect
  3. **Filter ablation:** Generate µGNAW⊕1 corpus with and without animate/inanimate semantic filter; compare embedding quality to assess filter contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Probabilistic Context-Free Grammars (PCFGs) effectively parameterize node probabilities to improve the semantic validity of generated sentences?
- Basis in paper: [explicit] Section 7 proposes exploring PCFGs to modify parameters based on classification, whereas the current study uses static CFGs.
- Why unresolved: The current method relies on simple CFGs with post-hoc filtering; it is unknown if probabilistic generation can intrinsically produce higher-quality data.
- What evidence would resolve it: Experiments comparing semantic similarity scores of embeddings trained on PCFG-generated corpora versus the current µGNAW⊕1 corpus.

### Open Question 2
- Question: Does a supervised semantic classifier offer a significant advantage over the heuristic animate/inanimate filter for corpus quality?
- Basis in paper: [explicit] Section 5.1 states that a supervised filter was considered but deferred to future work due to computational demands.
- Why unresolved: The implemented filter relies on manual association of nouns and verbs; a learned filter might capture more complex semantic nuances.
- What evidence would resolve it: A comparative analysis of model performance on downstream tasks when trained on corpora filtered by supervised classifiers versus the current rule-based approach.

### Open Question 3
- Question: Does extending the grammar to include recursive rules, past/future tenses, and plurals yield better embedding representations?
- Basis in paper: [explicit] Section 4.1 limits the grammar to present tense and singular forms; Section 7 explicitly lists these as areas for future expansion.
- Why unresolved: The current results are based on a restricted subset of Nawatl grammar (non-recursive, present tense only).
- What evidence would resolve it: Performance metrics from embeddings trained on a temporally and morphologically richer artificial corpus.

## Limitations
- Limited corpus size: The π-YALLI corpus contains only ~6.63M tokens, which may not fully represent Nawatl's linguistic diversity across 29 dialects.
- Evaluation scope: Semantic similarity ranking is tested on only 30 reference sentences with 150 candidates, which may not capture the full spectrum of semantic relationships.
- Grammar coverage: Both CFGs are explicitly non-recursive and limited to 3rd person singular forms, which may not capture the full syntactic richness of Nawatl.

## Confidence
- **High confidence:** The basic mechanism of CFG-based corpus expansion improving static embeddings for low-resource languages is supported by both the results and related literature on data augmentation.
- **Medium confidence:** The superiority of µGNAW⊕1 over µGNAW⊕0 is demonstrated in the semantic similarity task, but the evaluation scope is limited and independent verification would strengthen this claim.
- **Low confidence:** The comparison with large language models (Llama3-8B, Mistral-7B) showing competitive performance is based on indirect evidence and may not hold across broader NLP tasks.

## Next Checks
1. **Replication on expanded evaluation set:** Replicate the semantic similarity experiments with a larger, more diverse set of human-annotated sentence pairs (target: 100+ reference sentences with 500+ candidates) to validate the robustness of the τ improvement.
2. **Dialectal generalization test:** Train embeddings using synthetic data generated from µGNAW⊕1 with terminal nodes sampled from different Nawatl dialect sources, then evaluate semantic similarity to assess whether the grammar generalizes across dialects or overfits to a specific variety.
3. **Downstream task validation:** Test the expanded corpora embeddings on additional low-resource NLP tasks beyond semantic similarity (e.g., part-of-speech tagging, named entity recognition, or dialect classification) to determine if the quality improvements transfer to practical applications.