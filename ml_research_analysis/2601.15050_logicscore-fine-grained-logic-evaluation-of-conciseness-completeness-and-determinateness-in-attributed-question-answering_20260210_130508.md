---
ver: rpa2
title: 'LogicScore: Fine-grained Logic Evaluation of Conciseness, Completeness, and
  Determinateness in Attributed Question Answering'
arxiv_id: '2601.15050'
source_url: https://arxiv.org/abs/2601.15050
tags:
- reasoning
- answer
- logical
- evaluation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LOGICSCORE, a fine-grained evaluation framework
  for assessing Attributed Question Answering (AQA) outputs based on three logic dimensions:
  Completeness, Conciseness, and Determinateness. By converting long-form answers
  into Horn Rules, LOGICSCORE shifts focus from isolated fact verification to global
  reasoning coherence.'
---

# LogicScore: Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering

## Quick Facts
- **arXiv ID**: 2601.15050
- **Source URL**: https://arxiv.org/abs/2601.15050
- **Reference count**: 40
- **Key outcome**: LOGICSCORE framework reveals gap between factual grounding and logical rigor in LLM AQA outputs

## Executive Summary
LOGICSCORE introduces a fine-grained evaluation framework for Attributed Question Answering (AQA) that assesses outputs across three logic dimensions: Completeness, Conciseness, and Determinateness. The framework converts long-form answers into Horn Rules to shift focus from isolated fact verification to global reasoning coherence. Extensive experiments across three multi-hop QA datasets with over 20 LLMs reveal that while leading models achieve high attribution precision (e.g., Gemini-3-Pro at 92.85%), they struggle significantly with logical quality, scoring only 35.11% in Conciseness. This exposes a critical gap between factual accuracy and reasoning integrity in current LLM systems.

## Method Summary
The LOGICSCORE framework transforms AQA outputs into Horn Rules, enabling evaluation of reasoning quality through three logic dimensions. Completeness measures whether all necessary facts are included to support the conclusion, Conciseness evaluates whether redundant or irrelevant information is present, and Determinateness assesses whether the answer contains sufficient information to uniquely determine the conclusion. By analyzing the logical structure rather than individual facts, LOGICSCORE provides a holistic assessment of reasoning coherence. The framework was tested across three multi-hop QA datasets using 20+ different LLMs, including GPT-5, Gemini-3-Pro, LLaMA3, and various SFT models.

## Key Results
- Gemini-3-Pro achieves 92.85% attribution precision but only 35.11% Conciseness score
- All tested LLMs show significant gaps between factual grounding and logical rigor
- LogicScore reveals systematic reasoning weaknesses across multiple model architectures
- The framework demonstrates robustness across diverse multi-hop QA scenarios

## Why This Works (Mechanism)
LOGICSCORE works by converting natural language AQA outputs into Horn Rules, which represent logical implications in a formal structure. This transformation allows the framework to analyze the global coherence of reasoning rather than checking individual facts in isolation. The Horn Rule representation captures the logical dependencies between premises and conclusions, making it possible to systematically evaluate whether the reasoning is complete (all necessary premises included), concise (no redundant information), and determinate (sufficient information to reach a unique conclusion). This formal approach provides objective, reproducible metrics for reasoning quality that complement traditional attribution-based evaluations.

## Foundational Learning
- **Horn Rule Logic**: Why needed: To formalize natural language reasoning into analyzable logical structures. Quick check: Can the answer be expressed as a set of logical implications without losing semantic meaning?
- **Multi-hop Reasoning**: Why needed: AQA tasks often require chaining multiple reasoning steps. Quick check: Does the evaluation framework capture intermediate reasoning steps and their dependencies?
- **Attribution Analysis**: Why needed: To verify factual grounding while assessing logical coherence. Quick check: Can the framework distinguish between correct facts and correct reasoning?
- **Long-form Answer Evaluation**: Why needed: AQA produces extended responses requiring holistic assessment. Quick check: Does the framework maintain consistency across varying answer lengths and complexities?
- **Logic Dimension Metrics**: Why needed: To quantify different aspects of reasoning quality. Quick check: Are the metrics sensitive enough to detect subtle reasoning flaws?
- **Cross-model Comparison**: Why needed: To benchmark reasoning capabilities across different LLM architectures. Quick check: Does the framework produce consistent evaluations across diverse model types?

## Architecture Onboarding
**Component Map**: Input AQA → Horn Rule Conversion → Logic Dimension Analysis → Completeness/Conciseness/Determinateness Scores
**Critical Path**: The Horn Rule conversion step is critical as it transforms natural language into analyzable logical structures. Any errors in this conversion directly impact all subsequent logic dimension evaluations.
**Design Tradeoffs**: Formal logic representation provides objective evaluation but may lose nuanced reasoning aspects; attribution precision focuses on factual accuracy while potentially missing logical gaps.
**Failure Signatures**: Models may achieve high attribution precision but fail on Conciseness, indicating strong factual grounding but poor reasoning economy; low Determinateness scores suggest answers lack sufficient information for unique conclusions.
**3 First Experiments**:
1. Test Horn Rule conversion accuracy on diverse AQA outputs from different domains
2. Validate logic dimension scores against human evaluations of reasoning quality
3. Compare LOGICSCORE metrics with traditional fact-based evaluation methods

## Open Questions the Paper Calls Out
None

## Limitations
- Horn Rule conversion methodology may not capture all reasoning complexity in multi-hop QA
- Logic dimension metrics may be sensitive to specific conversion approaches
- Framework focuses on three dimensions while potentially missing other reasoning aspects
- Study scope limited to three datasets and 20+ LLMs, potentially missing diverse AQA scenarios

## Confidence
- **High confidence**: Methodology for Horn Rule conversion and application to tested datasets
- **Medium confidence**: Generalizability of logic dimensions to other QA formats
- **Medium confidence**: Attribution precision metrics given LLM long-form generation limitations
- **Low confidence**: Framework's ability to capture all reasoning quality aspects in complex multi-hop scenarios

## Next Checks
1. Test framework applicability on additional multi-hop QA datasets with different complexity levels
2. Conduct ablation studies to isolate impact of each logic dimension on evaluation outcomes
3. Implement cross-validation with human evaluators to verify alignment with human judgment of reasoning quality