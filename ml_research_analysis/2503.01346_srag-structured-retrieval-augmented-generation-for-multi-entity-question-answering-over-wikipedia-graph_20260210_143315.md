---
ver: rpa2
title: 'SRAG: Structured Retrieval-Augmented Generation for Multi-Entity Question
  Answering over Wikipedia Graph'
arxiv_id: '2503.01346'
source_url: https://arxiv.org/abs/2503.01346
tags:
- entities
- table
- question
- llms
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of multi-entity question answering
  (MEQA) over Wikipedia, where existing retrieval-augmented generation (RAG) methods
  struggle to aggregate scattered information across multiple documents. The proposed
  Structured RAG (SRAG) framework addresses this by organizing extracted entities
  into relational tables with defined schemas, then applying table-based reasoning
  techniques.
---

# SRAG: Structured Retrieval-Augmented Generation for Multi-Entity Question Answering over Wikipedia Graph

## Quick Facts
- arXiv ID: 2503.01346
- Source URL: https://arxiv.org/abs/2503.01346
- Reference count: 11
- Achieves 88.9% accuracy on MEBench, outperforming state-of-the-art by 29.6%

## Executive Summary
SRAG tackles multi-entity question answering over Wikipedia by converting scattered information into structured relational tables. The system decouples retrieval from reasoning by first extracting entity properties into table schemas, then using SQL execution for final answers. This approach significantly outperforms long-context LLMs and RAG baselines, particularly for statistical queries requiring aggregation across multiple documents.

## Method Summary
SRAG operates in two stages: first, a GPT-4 model generates and validates SPARQL queries against Wikidata to retrieve relevant Wikipedia pages; second, a structured QA module creates table schemas from questions, uses Mistral-7B to extract entity information into tables, and generates SQL queries for final answers. The system achieves 88.9% overall accuracy on MEBench by focusing LLMs on structured data analysis rather than raw text aggregation.

## Key Results
- 88.9% overall accuracy on MEBench benchmark
- 29.6% improvement over state-of-the-art long-context LLMs
- 46% improvement for statistical query types requiring aggregation
- Effective across 8 query types including comparison, statistics, and relationship analysis

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Retrieval from Reasoning via Structured Tables
Organizing unstructured multi-document text into relational tables improves LLM performance by allowing deterministic SQL execution rather than implicit text aggregation. The system forces a schema onto scattered data, enabling clean structured analysis. Core assumption: LLMs perform better on structured data than noisy text contexts. Break condition: Fails with ambiguous or synonym-heavy properties causing SQL execution errors.

### Mechanism 2: Graph-Grounded Semantic Validation
LLM-generated queries are validated against Wikidata to reduce entity hallucination. An LLM generates rough SPARQL, then Wikipedia APIs verify and correct entity/property IDs before execution. Core assumption: The knowledge graph contains accurate mappings for requested entities. Break condition: Fails when LLM misinterprets relationship semantics, confusing relation types.

### Mechanism 3: Schema-Driven Extraction
Inferring table schema prior to extraction minimizes information redundancy and omission. The system analyzes questions to define specific table columns, then prompts LLM to fill this rigid structure. Core assumption: LLM can accurately infer necessary attributes from queries before seeing data. Break condition: Fails if schema generation oversimplifies query intent, missing critical columns.

## Foundational Learning

- **SPARQL (SPARQL Protocol and RDF Query Language)**
  - Why needed: SRAG relies on querying Wikipedia Graph to retrieve entity sets. Understanding graph queries vs. vector search is critical for debugging retrieval.
  - Quick check: How does SPARQL query ensure precise entity retrieval compared to vector similarity search?

- **Table-Based Reasoning / TableQA**
  - Why needed: SRAG converts problem from "Text-to-Text" to "Text-to-Table-to-SQL". Understanding LLM interface with tabular data is essential.
  - Quick check: Why might LLM struggle to answer "Average Age" from text but succeed when data is in SQL table?

- **Schema Induction**
  - Why needed: System dynamically creates database schema based on user's question. Understanding how to prompt LLM to design schema is prerequisite.
  - Quick check: If user asks "Who is older, Alice or Bob?", what minimal schema columns must LLM infer?

## Architecture Onboarding

- **Component map**: Query Analyzer (GPT-4) -> Semantic Validator (API-based) -> Retriever -> Extractor (Mistral-7B) -> Executor (GPT-4)
- **Critical path**: Schema Generation to Extraction is bottleneck. Wrong schema wastes extraction and causes SQL executor failure.
- **Design tradeoffs**: Cost vs. Precision (GPT-4 for planning, Mistral-7B for extraction). Rigidity vs. Flexibility (structured tables enable precise SQL but may lose nuance).
- **Failure signatures**: Relation Semantic Errors (LLM confuses relation types). Synonym Collision (SQL fails due to unnormalized strings).
- **First 3 experiments**: 
  1. Unit Test Retrieval: Input "US Presidents" query and verify SPARQL validation corrects "position held" relation.
  2. Schema Ablation: Manually provide "gold standard" schema for complex statistical query to isolate extraction vs. planning errors.
  3. Normalization Check: Run extractor on 10 distinct entities and check for non-normalized values.

## Open Questions the Paper Calls Out

### Open Question 1
How does SQL query-based table reasoning compare against simpler techniques like keyword extraction and summarization for multi-entity QA tasks? The authors assumed SQL's advantages were self-evident and did not conduct empirical comparisons, leaving unclear whether simpler methods might achieve competitive performance.

### Open Question 2
How can relation semantic parsing be improved to accurately identify relationships in SPARQL queries beyond recognizing entities? Current system correctly identifies entities but misclassifies relationships (e.g., confusing "instance of" with "position held"), propagating errors through retrieval and reasoning pipeline.

### Open Question 3
How can SRAG handle ambiguous real-world queries requiring advanced semantic inference to grasp implicit user intent? Current system assumes well-structured queries and lacks mechanisms for disambiguating underspecified or context-dependent questions common in real-world usage.

## Limitations
- System struggles with ambiguous queries requiring advanced semantic inference to grasp implicit user intent
- Relation semantic parsing errors cause retrieval failures when LLM confuses relationship types
- Synonym normalization issues in extraction phase cause SQL execution failures

## Confidence
- **High Confidence**: Overall framework design and 88.9% accuracy claims well-supported
- **Medium Confidence**: SPARQL validation and table schema generation implementation details partially described
- **Low Confidence**: Exact Mistral-7B extraction prompts and SQL execution environment details unknown

## Next Checks
1. SPARQL Validation Verification: Test "US Presidents" query to verify Wikidata API integration corrects relation semantic errors
2. Schema Generation Ablation: Manually provide "gold standard" schemas for complex statistical queries to isolate planning vs. extraction errors
3. Synonym Normalization Audit: Sample 50 entity extractions across 10 queries to quantify frequency of unnormalized multi-word synonyms in categorical columns