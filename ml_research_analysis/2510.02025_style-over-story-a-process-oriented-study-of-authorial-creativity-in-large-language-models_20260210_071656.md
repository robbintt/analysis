---
ver: rpa2
title: 'Style Over Story: A Process-Oriented Study of Authorial Creativity in Large
  Language Models'
arxiv_id: '2510.02025'
source_url: https://arxiv.org/abs/2510.02025
tags:
- narrative
- constraints
- setting
- each
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a constraint-selection methodology to measure
  the narrative preferences of large language models (LLMs), addressing the gap between
  output-focused evaluations and underlying authorial tendencies. By drawing on narratology,
  the authors created a library of 200 narrative constraints across four elements:
  Style, Character, Event, and Setting.'
---

# Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models

## Quick Facts
- arXiv ID: 2510.02025
- Source URL: https://arxiv.org/abs/2510.02025
- Reference count: 40
- Primary result: LLMs consistently prioritize Style over other narrative elements across models and instruction types

## Executive Summary
This study introduces a constraint-selection methodology to measure the narrative preferences of large language models (LLMs), addressing the gap between output-focused evaluations and underlying authorial tendencies. By drawing on narratology, the authors created a library of 200 narrative constraints across four elements: Style, Character, Event, and Setting. Six LLMs were prompted under three instruction types (Basic, Quality-focused, Creativity-focused) to select constraints for story planning, with results analyzed at element, category, and axis levels. The primary finding is that LLMs consistently prioritize Style over other narrative elements, with this preference remaining stable across models and instruction types.

## Method Summary
The authors developed a constraint-selection methodology based on narratology to assess LLM narrative preferences. They created a library of 200 narrative constraints across four elements: Style, Character, Event, and Setting. Six different LLMs were prompted using three instruction types: Basic, Quality-focused, and Creativity-focused. The models were asked to select constraints for story planning purposes. The selection patterns were then analyzed at multiple levels including element, category, and axis. This approach allowed the researchers to examine authorial preferences through the lens of constraint selection rather than focusing solely on generated outputs.

## Key Results
- LLMs consistently prioritize Style over other narrative elements across all tested models
- Creativity-focused instructions induced modest shifts within Event and Character elements but did not alter the overall Style preference
- Cross-model differences were most pronounced in content-related elements, while Style preferences showed broad convergence

## Why This Works (Mechanism)
None provided in the source material.

## Foundational Learning
- **Narratology**: The study of narrative structure and the ways that these affect our perception. Needed to create a theoretically grounded framework for analyzing narrative elements. Quick check: Understanding how narrative elements interact and influence storytelling.
- **Constraint-based evaluation**: Using predefined constraints to assess model behavior rather than solely analyzing generated outputs. Needed to measure authorial preferences systematically. Quick check: Can constraints effectively capture narrative decision-making processes?
- **Authorial creativity metrics**: Methods for quantifying creative decision-making in AI systems. Needed to move beyond traditional output evaluation. Quick check: How do we measure creative preferences in language models?

## Architecture Onboarding
- **Component map**: Constraint library (4 elements: Style, Character, Event, Setting) -> LLM selection process -> Analysis at element/category/axis levels
- **Critical path**: Instruction type selection -> Constraint presentation -> LLM constraint selection -> Preference analysis
- **Design tradeoffs**: Constraint-based approach provides systematic measurement but may not fully capture dynamic narrative generation processes
- **Failure signatures**: Inconsistent constraint selection patterns, model-specific anomalies in Style prioritization, instruction type effects not aligning with expectations
- **3 first experiments**:
  1. Test constraint selection consistency across multiple prompt sessions
  2. Compare constraint selection patterns with actual generated story outputs
  3. Evaluate different constraint set sizes to determine optimal evaluation framework

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The assumption that constraint preferences directly map to narrative priorities during actual story generation has medium confidence
- The modest variations induced by Creativity instructions within Event and Character elements require additional investigation
- Model-specific training data and architectural factors may play a larger role than the study fully explores

## Confidence
- **High confidence**: Primary finding of Style prioritization across models and instruction types
- **Medium confidence**: Assumption that constraint selection maps to narrative execution patterns
- **Medium confidence**: Generalizability of Style preference stability across instruction types

## Next Checks
1. Conduct a follow-up study measuring actual story generation outputs under identical constraint sets to verify whether constraint selection preferences translate to narrative execution patterns
2. Test additional LLMs beyond the six models studied, particularly those with different architectural approaches or training paradigms, to assess generalizability of Style prioritization
3. Implement longitudinal testing across multiple prompt sessions to determine whether the observed preferences remain stable over time and with repeated interactions, addressing potential adaptation effects or prompt-induced variations