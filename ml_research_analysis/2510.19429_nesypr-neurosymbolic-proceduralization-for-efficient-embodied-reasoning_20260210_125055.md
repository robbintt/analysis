---
ver: rpa2
title: 'NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning'
arxiv_id: '2510.19429'
source_url: https://arxiv.org/abs/2510.19429
tags:
- arxiv
- reasoning
- nesypr
- agent
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying language models
  (LMs) for embodied reasoning in dynamic, resource-constrained environments. The
  proposed method, NeSyPr, uses neurosymbolic proceduralization inspired by the ACT
  theory to compile multi-step symbolic reasoning into single-step LM inference, eliminating
  the need for online symbolic tools.
---

# NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning

## Quick Facts
- **arXiv ID:** 2510.19429
- **Source URL:** https://arxiv.org/abs/2510.19429
- **Reference count:** 40
- **Primary result:** Achieves 46.7% higher task success rate than 70B-scale reasoning models using a 70× smaller LM, with >90% latency reduction.

## Executive Summary
NeSyPr addresses the challenge of deploying language models for embodied reasoning in resource-constrained environments by compiling multi-step symbolic planning into single-step LM inference. The method learns to encode production rules into a vector-quantized procedural memory, eliminating the need for online symbolic tools during test time. Evaluated across PDDLGym, VirtualHome, and ALFWorld benchmarks, NeSyPr demonstrates strong structured reasoning capabilities while using significantly smaller models than traditional approaches.

## Method Summary
NeSyPr uses a two-phase neurosymbolic proceduralization approach. During training, symbolic planners generate action plans from declarative knowledge, and the LM learns to internalize production rules as composable procedures stored in a vector-quantized memory. The working memory is partitioned into d-dimensional chunks, each mapped to the nearest procedure-unit in a learned codebook via nearest-neighbor matching. At test time, the LM retrieves and composes these procedures through compositional and contrastive reasoning, using separate banks of successful and failed procedures to adapt its behavior without gradient updates. The backbone uses small LLaMA-3.2-1B or Qwen2.5-0.5B models with procedure-books sized at K=256 or K=224.

## Key Results
- 46.7% higher task success rate compared to 70B-scale reasoning models while using 70× smaller LM
- Reduces latency by over 90% compared to large-scale inference baselines
- Maintains zero forgetting rate and ~12% recovery rate through contrastive planning mechanism

## Why This Works (Mechanism)

### Mechanism 1: Symbolic-to-Procedural Knowledge Compilation
Multi-step symbolic planning is compiled into single-step LM inference by learning to internalize production rules during training. The LM stores these as composable procedures in a vector-quantized memory, enabling test-time reasoning without external symbolic solvers. This works when reasoning patterns are learnable and generalizable across tasks.

### Mechanism 2: Vector-Quantized Compositional Memory
Production rules are encoded as discrete, reusable procedure-units that compositionally combine to form task-specific procedures. The VQ approach forces the model to learn a finite vocabulary of reasoning primitives rather than memorizing entire plans, enabling better generalization when task-solving procedures share common substructures.

### Mechanism 3: Contrastive Reconstruction for Adaptive Test-Time Reasoning
Separate banks of successful (M+) and failed (M−) procedures enable adaptive reasoning without gradient updates. At inference, the current procedure is compared against these banks via cosine similarity, and contrastive decoding amplifies token probabilities aligned with successful procedures while suppressing those matching failures.

## Foundational Learning

- **ACT Theory's Declarative-to-Procedural Transition**: Understanding ACT's model of skill acquisition clarifies why the two-phase design (training compilation → test inference) should work. Declarative knowledge becomes procedural knowledge through practice.
  - *Quick check*: Can you explain the difference between "knowing the rules of chess" (declarative) and "recognizing winning patterns automatically" (procedural)?

- **Vector Quantization (VQ-VAE style)**: The procedure-book uses VQ to force discrete representations. Understanding VQ is essential because the partitioning and nearest-neighbor matching will seem arbitrary without it.
  - *Quick check*: Why would forcing discrete codes (rather than continuous vectors) help with compositional reuse?

- **Contrastive Decoding**: The test-time adaptation mechanism uses contrastive decoding between success- and failure-conditioned distributions. Understanding the intuition (amplify what works, suppress what fails) is essential for debugging contrastive planning failures.
  - *Quick check*: In the contrastive score equation, what happens if a token has high probability under both success and failure distributions?

## Architecture Onboarding

- **Component map:**
  Input (observation, goal, domain knowledge) → Embedding Layer → H₀ → Self-Attention → E_self → Cross-Attention with Working Memory M → E_work → Gated Memory Update → M updated → Vector Quantization: M → R (procedure-book lookup) → FFN with gated R integration → H_l → (if test time) Contrastive Reconstruction: R → R⁺, R⁻ (from M⁺/M⁻ banks) → Contrastive Decoding: p⁺ vs p⁻ → final distribution

- **Critical path:** The VQ lookup determines which procedure-units compose the runtime procedure R. If this fails (e.g., all chunks map to similar units), downstream contrastive planning has nothing meaningful to work with. The procedure-book C is the bottleneck.

- **Design tradeoffs:**
  - Procedure-book size (K): Larger K covers more task types but requires more training data. Paper uses K=256 for PDDLGym, K=224 for VirtualHome/ALFWorld.
  - Procedure-unit dimension (d): Smaller d enables finer granularity composition but risks losing semantic coherence. Paper uses d=32 (PDDLGym) and d=28 (VirtualHome/ALFWorld).
  - Reconstruction threshold (υ): Controls how strictly success/failure banks filter procedures. υ=0.95 is conservative; lowering accepts more procedures but risks overgeneralization.

- **Failure signatures:**
  - Procedure-book collapse: CSR plateaus despite more training—check if procedure-unit usage is concentrated in few units (heatmap shows uniform usage suggests healthy diversity).
  - Contrastive planning ineffective: Forgetting rate (FR) > 0 or low recovery rate (RR)—check if M⁺ and M⁻ are being populated.
  - Poor generalization to unseen tasks: Large train-to-test gap—procedure-book may be overfitting to training task patterns.

- **First 3 experiments:**
  1. Ablate VQ (remove procedure-book): Replace VQ with continuous memory and compare CSR on unseen tasks. Expected: significant drop (Table 6 shows 17.3% CSR drop without C).
  2. Vary reconstruction threshold (υ): Test υ ∈ {0.8, 0.9, 0.95, 0.99} on held-out tasks. Plot CSR vs. υ to find the over/under-generalization inflection point.
  3. Procedure-book interpretability check: For a fixed task type, visualize which procedure-units activate across different scene configurations. Verify that semantically similar tasks share procedure-unit patterns.

## Open Questions the Paper Calls Out

- **Integrating knowledge distillation from large teacher models**: The authors plan to explore combining knowledge distillation from larger LMs with neurosymbolic proceduralization to enhance generalization to complex real-world scenarios. This remains unresolved as current evaluation is restricted to simulated benchmarks.

- **Robustness to noisy or sub-optimal training plans**: The method relies on "optimal plans" from symbolic solvers, but it's unclear how VQ-VAE training behaves with noisy or sub-optimal trajectories. Real-world symbolic definitions can be incomplete or noisy.

- **Fixed procedure-book capacity in long-horizon continual learning**: The ablation study notes that small C limits task coverage, but it's unclear if the VQ codebook saturates or collapses as the diversity of required procedures grows indefinitely with thousands of tasks.

## Limitations

- Performance relies heavily on the quality and completeness of symbolic planners' declarative knowledge, which may be incomplete for complex real-world domains.
- The fixed capacity of the procedure-book may limit scalability to very large task spaces with diverse reasoning patterns.
- Evaluation is currently restricted to simulated benchmarks, with open questions about real-world deployment effectiveness.

## Confidence

- **Method reproducibility**: Medium - Key architectural details like memory dimensions and gating mechanisms are unspecified
- **Result validity**: High - Multiple benchmarks show consistent improvements over strong baselines
- **Mechanism understanding**: Medium - Core mechanisms are well-described but some implementation details are unclear
- **Generalizability claims**: Low - Current results are limited to specific benchmark domains

## Next Checks

1. Verify procedure-book update via EMA is occurring during training by monitoring codebook usage statistics
2. Test contrastive planning effectiveness by measuring forgetting rate (FR) and recovery rate (RR) on held-out tasks
3. Evaluate procedure-unit utilization patterns across task types to confirm compositional structure and diversity in the codebook