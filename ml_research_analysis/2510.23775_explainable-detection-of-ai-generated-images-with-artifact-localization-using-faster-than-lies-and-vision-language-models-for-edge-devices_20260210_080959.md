---
ver: rpa2
title: Explainable Detection of AI-Generated Images with Artifact Localization Using
  Faster-Than-Lies and Vision-Language Models for Edge Devices
arxiv_id: '2510.23775'
source_url: https://arxiv.org/abs/2510.23775
tags:
- images
- image
- artifacts
- detection
- artifact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting and explaining
  AI-generated images, particularly in low-resolution (32x32) formats, with adversarial
  robustness for edge device deployment. The proposed solution, "Faster-Than-Lies,"
  combines a lightweight convolutional classifier with a Vision-Language Model (Qwen2-VL-7B)
  to classify images as real or fake, localize artifacts, and generate human-understandable
  explanations.
---

# Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices

## Quick Facts
- arXiv ID: 2510.23775
- Source URL: https://arxiv.org/abs/2510.23775
- Reference count: 13
- Primary result: 96.5% accuracy on 32x32 AI-generated images with adversarial perturbations, plus artifact localization and VLM explanations

## Executive Summary
This paper addresses the challenge of detecting and explaining AI-generated images, particularly in low-resolution (32x32) formats, with adversarial robustness for edge device deployment. The proposed solution, "Faster-Than-Lies," combines a lightweight convolutional classifier with a Vision-Language Model (Qwen2-VL-7B) to classify images as real or fake, localize artifacts, and generate human-understandable explanations. The system achieves 96.5% accuracy on the extended CiFAKE dataset with adversarial perturbations and maintains an inference time of 175ms on 8-core CPUs. Artifact localization is enabled through autoencoder-based reconstruction error maps, which highlight regions of artificial generation.

## Method Summary
The method employs a two-stage pipeline: first, a lightweight CNN classifier ("Faster-Than-Lies") detects whether an image is real or AI-generated with 96.5% accuracy on perturbed 32x32 images. Second, for detected fake images, an autoencoder trained exclusively on real images generates reconstruction error maps that highlight synthetic artifact regions. These heatmaps, combined with CLIP-based artifact categorization, are passed to a VLM (Qwen2-VL-7B) to generate natural language explanations of detected anomalies. The approach integrates visual and linguistic reasoning for interpretable authenticity detection while maintaining edge-device compatibility through model compression and efficient architecture choices.

## Key Results
- Achieves 96.5% classification accuracy on 32x32 images from extended CiFAKE dataset with adversarial perturbations
- Maintains 175ms inference time on 8-core CPUs with model size of 98MB (29.8M parameters)
- Successfully categorizes 70 visual artifact types into eight semantic groups with VLM-generated explanations
- Demonstrates adversarial robustness through training-time perturbation augmentation across 11 perturbation types

## Why This Works (Mechanism)

### Mechanism 1
Autoencoder reconstruction error localizes AI-generated artifacts by exploiting distributional differences between real and synthetic image regions. An autoencoder trained exclusively on real images learns to reconstruct natural image statistics. When applied to AI-generated images, regions with synthetic artifacts (texture bleeding, inconsistent shadows, anatomical errors) yield higher pixel-wise reconstruction loss because the model never learned these unnatural patterns. These loss maps become localization heatmaps. The core assumption is that AI-generated artifacts produce systematically higher reconstruction errors than natural image regions. Evidence includes the abstract statement about "autoencoder-based reconstruction error maps" and references to similar reconstruction-error principles in DIRE and AEROBLADE. Break condition: if synthetic images match real-image distributions too closely, reconstruction error may not differentiate reliably.

### Mechanism 2
Adversarial perturbation augmentation during training induces robustness to evasion attacks by exposing the classifier to a diverse noise distribution. The training dataset is augmented with 11 perturbation types (Gaussian noise, salt-and-pepper, motion blur, pixelate, hue/saturation shifts, adversarial noise via MobileNetV2, quantization artifacts, mask-based corruption). The classifier learns invariant features that persist across these distortions, reducing susceptibility to adversarial examples designed to fool detection. The core assumption is that synthetic perturbations used in training sufficiently approximate real-world adversarial attacks. Evidence includes the abstract accuracy claim and the paper's description of augmentation strategy. Break condition: if adversarial attacks use perturbations not covered in the augmentation set, robustness may degrade.

### Mechanism 3
Two-stage VLM guidance (CLIP category filtering → Qwen2-VL explanation) produces interpretable artifact descriptions by narrowing search space before detailed analysis. CLIP ViT-B32 encodes the detected-fake image and scores similarity to 8 artifact category descriptions. The top 3 scoring categories are passed as structured context to Qwen2-VL-7B. The VLM then performs focused visual analysis within those categories, generating natural language explanations of specific artifacts. The core assumption is that CLIP provides sufficient coarse-grained categorization to guide VLM attention. Evidence includes the abstract statement about "categorize 70 visual artifact types into eight semantic groups" and the paper's description of the VLM prompting strategy. Break condition: if CLIP category scores are noisy or the VLM lacks grounding in low-resolution features, explanations may be hallucinated.

## Foundational Learning

- **Concept:** Autoencoder reconstruction as anomaly detection
  - Why needed here: The entire localization pipeline depends on understanding why training on "normal" (real) data makes reconstruction error a proxy for "abnormal" (synthetic) regions.
  - Quick check question: Given an autoencoder trained only on human faces, would you expect higher or lower reconstruction error on a face with anatomically impossible joint configurations? Why?

- **Concept:** Transfer learning and fine-tuning for VLMs
  - Why needed here: Qwen2-VL-7B was selected after comparing 7 VLMs; understanding how pre-trained vision-language representations adapt to low-resolution, domain-specific tasks is critical.
  - Quick check question: If a VLM was pre-trained on high-resolution natural images, what failure modes might you expect when applying it directly to 32×32 AI-generated image patches?

- **Concept:** Adversarial robustness via data augmentation
  - Why needed here: The 96.5% accuracy claim depends on perturbation-augmented training. Understanding the difference between random augmentations and adversarial training is essential for evaluating robustness claims.
  - Quick check question: Does training with Gaussian noise augmentation guarantee robustness to adversarial noise crafted by an attacker with knowledge of the model? Why or why not?

## Architecture Onboarding

- **Component map:** Input → FTL classifier → (if Fake) → Autoencoder reconstruction → reconstruction error map → CLIP ViT-B32 → top-3 artifact categories → Qwen2-VL-7B → natural language explanation → Output

- **Critical path:** Image → FTL classifier (if classified as Real, pipeline terminates) → If Fake → Autoencoder reconstruction → error heatmap → Image + heatmap → CLIP category scoring → top-3 categories → Image + heatmap + categories → Qwen2-VL → explanation text

- **Design tradeoffs:**
  - FTL vs. EfficientNet: FTL chosen for speed/accuracy balance (175ms vs. 136ms for EfficientNetB0, but higher accuracy). Assumption: 175ms is acceptable for edge deployment.
  - Resolution ceiling: System optimized for 32×32; upscaling introduced edge artifacts that complicated localization. Tradeoff: accept low-resolution constraints or risk upscaling artifacts.
  - VLM size vs. edge deployment: Qwen2-VL-7B (full precision: ~5.2s inference on L40s GPU) is too slow for edge; 4-bit quantized version (88% accuracy vs. 90%) may be required for actual edge deployment.

- **Failure signatures:**
  - High-confidence false positives on heavily perturbed real images: Augmentation may not cover all real-world degradation types.
  - Diffuse or uninformative heatmaps: If autoencoder learns to partially reconstruct synthetic patterns, localization precision degrades.
  - Hallucinated explanations: VLM may describe artifacts not present in the image, especially if CLIP category scores are ambiguous.
  - Latency spikes on CPU: FTL is 175ms, but localization adds ~1s, and VLM inference is not benchmarked on CPU — total pipeline latency unclear for edge deployment.

- **First 3 experiments:**
  1. **Baseline replication:** Train FTL on CiFAKE (no perturbations), measure accuracy drop when adversarial perturbations are applied at test time. This validates the augmentation claim.
  2. **Localization sanity check:** Generate heatmaps for a held-out set of real images with synthetic artifact patches overlaid. Measure precision/recall of heatmap hot spots vs. ground-truth artifact locations.
  3. **Explanation faithfulness probe:** For 50 manually annotated fake images with known artifacts, compare VLM explanations against human expert descriptions. Measure overlap in artifact type detection.

## Open Questions the Paper Calls Out

### Open Question 1
Can diffusion-based reconstruction methods significantly improve artifact localization precision compared to the current autoencoder-based reconstruction error maps? The conclusion states future work includes "enhancing localization precision through diffusion-based reconstruction" and the authors acknowledge that "Localisation has not fully been achieved." Comparative evaluation on the same dataset with quantitative localization metrics would resolve this.

### Open Question 2
How does classification accuracy and explanation quality scale when moving from 32x32 to higher resolutions (64x64, 128x128)? The potential improvements section states "expanding the solution to handle higher-resolution images could improve classification accuracy" but the entire experimental evaluation was constrained to 32x32 images. Multi-resolution experiments would establish the relationship between resolution and system performance.

### Open Question 3
Can the accuracy of VLM-generated artifact explanations be validated without ground-truth annotated datasets? The limitations section notes "The accuracy of the second solution's explanations cannot be tested on the current dataset provided to us" because the dataset provides real/fake labels but lacks human-verified artifact annotations. Creation of an artifact-annotated benchmark dataset or development of automated evaluation metrics would resolve this.

### Open Question 4
Does the system generalize to images generated by AI models not represented in the training distribution (e.g., DALL-E 3, Midjourney, Stable Diffusion XL)? The limitations note "Might be more AI image sources which we did not incorporate" and future work includes "evaluating cross-domain generalization." Zero-shot evaluation on held-out generators with accuracy and explanation quality metrics would establish generalization capability.

## Limitations
- FTL classifier architecture is not specified, making exact reproduction impossible
- Localization via autoencoder reconstruction error is acknowledged as "not fully achieved" with no quantitative validation metrics
- VLM explanation quality is completely unverified — no evidence that explanations correspond to actual detected artifacts

## Confidence

- **Classification Accuracy (96.5% on perturbed CiFAKE)**: Medium confidence — accuracy is reported but cannot be independently verified without exact FTL architecture and perturbation implementation details
- **Localization via Autoencoder Reconstruction Error**: Medium-Low confidence — mechanism is plausible but lacks quantitative validation and authors acknowledge it's "not fully achieved"
- **VLM-Generated Explanations**: Low confidence — there is no evaluation of explanation faithfulness, relevance, or accuracy; authors explicitly state explanations cannot be tested

## Next Checks

1. **Architecture Specification**: Obtain the exact FTL classifier architecture (layer configurations, attention mechanisms, initialization) to enable faithful reproduction and independent validation of the 96.5% accuracy claim.

2. **Localization Quantitative Evaluation**: Conduct a controlled experiment where synthetic artifacts are deliberately inserted into real images, then measure heatmap precision/recall against ground-truth artifact locations to establish whether reconstruction error provides reliable localization.

3. **Explanation Faithfulness Testing**: For a set of 50 manually annotated fake images with known artifact types, compare VLM explanations against human expert descriptions to measure explanation accuracy, relevance, and hallucination rates.