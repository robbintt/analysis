---
ver: rpa2
title: 'Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?'
arxiv_id: '2510.10457'
source_url: https://arxiv.org/abs/2510.10457
tags:
- ranking
- benchmark
- arxiv
- redundancy
- subset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficiently evaluating large
  language models (LLMs) by compressing benchmark datasets while preserving both accuracy
  reconstruction and ranking consistency. The authors observe that existing benchmarks
  contain substantial sample redundancy at both textual and ranking levels, which
  inflates evaluation costs without adding information.
---

# Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?

## Quick Facts
- arXiv ID: 2510.10457
- Source URL: https://arxiv.org/abs/2510.10457
- Reference count: 36
- Achieves up to 200× benchmark compression while preserving LLM ranking fidelity

## Executive Summary
This paper addresses the inefficiency of evaluating large language models (LLMs) on full benchmark datasets, which often contain substantial redundancy. The authors propose EssenceBench, a three-stage coarse-to-fine framework that compresses benchmarks by filtering redundant samples, using genetic algorithms to search for representative subsets, and refining selections through attribution-based grouping. Experiments on five standard benchmarks demonstrate that EssenceBench achieves up to 200× compression while maintaining ranking consistency within 5% error, significantly outperforming prior methods in both prediction accuracy and ranking preservation.

## Method Summary
The EssenceBench framework employs a three-stage approach to benchmark compression. First, it performs coarse filtering by removing samples with high text similarity (via bge-m3 embeddings) or high ranking correlation across LLMs, eliminating redundancy at both textual and ranking levels. Second, a genetic algorithm with tournament selection, crossover, mutation, and adjustment operators searches for compact subsets that optimize a fitness predictor (GAM). Third, an attribution-based refinement stage uses Explainable Boosting Machines to identify high-impact samples, partitions them into high/low/random groups, and iteratively refines each group through additional GA optimization to ensure diverse and representative coverage while maintaining ranking fidelity.

## Key Results
- Achieves up to 200× compression of benchmark datasets while preserving ranking fidelity
- On HellaSwag, 95% of LLM rankings remain within 5% error using only 200× fewer samples
- Outperforms prior compression methods in both RMSE prediction error and ranking preservation metrics across five benchmarks

## Why This Works (Mechanism)
The approach works by recognizing that benchmark datasets contain substantial redundancy at both the textual and ranking levels. By filtering out redundant samples and using evolutionary search with attribution-based refinement, the method identifies a minimal subset that preserves the relative performance relationships between LLMs. The coarse-to-fine strategy ensures that the compression process first removes obvious redundancies before fine-tuning the selection through genetic algorithms and attribution analysis, maintaining the essential information needed for accurate LLM comparison.

## Foundational Learning
- **Text similarity filtering**: Why needed - removes redundant prompts that provide no additional information; Quick check - compute cosine similarity between bge-m3 embeddings and filter above threshold
- **Ranking correlation analysis**: Why needed - identifies samples that don't differentiate LLM performance; Quick check - calculate Pearson correlation across model score vectors
- **Genetic algorithm optimization**: Why needed - efficiently searches for optimal sample subsets in discrete space; Quick check - monitor fitness improvement across generations
- **Attribution-based refinement**: Why needed - ensures diverse sample selection covering different performance aspects; Quick check - verify EBM attribution scores align with known benchmark characteristics

## Architecture Onboarding

**Component Map**
Score matrix -> Coarse filtering (text + ranking) -> GA search (GAM fitness) -> Attribution analysis (EBM) -> Group refinement (iterative GA) -> Compressed benchmark

**Critical Path**
The most critical sequence is coarse filtering followed by GA optimization. Without effective redundancy removal, the GA search becomes computationally expensive and less effective. The attribution refinement stage, while beneficial, is less critical than ensuring the initial subset captures essential ranking information.

**Design Tradeoffs**
- Computational cost vs. compression ratio: More aggressive filtering yields higher compression but risks losing important samples
- GA population size vs. convergence quality: Larger populations explore better but increase runtime
- Attribution granularity vs. sample diversity: Fine-grained attribution preserves nuance but may select similar samples

**Failure Signatures**
- Poor convergence: Fitness plateaus early, indicating insufficient population diversity or ineffective mutation
- Ranking instability: Large variations in model rankings between original and compressed benchmarks suggest loss of critical differentiating samples
- Overfitting: GA optimizes for training models but performs poorly on held-out models, indicating predictor bias

**3 First Experiments**
1. Apply coarse filtering only and measure ranking preservation to establish baseline redundancy levels
2. Run GA optimization with fixed population parameters and evaluate compression vs. ranking fidelity trade-off
3. Compare attribution-based grouping with random grouping to quantify refinement benefit

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on English-language benchmarks, limiting generalizability to other languages or specialized domains
- Assumes binary scoring matrices (pass/fail), which may not capture nuanced performance differences in more granular evaluation scenarios
- Coarse filtering thresholds and genetic algorithm hyperparameters are not fully specified, potentially affecting reproducibility
- Focuses on ranking preservation rather than absolute accuracy maintenance, which may be important for certain evaluation contexts

## Confidence

**High confidence:** The core methodology of combining redundancy detection with evolutionary search is technically sound and the empirical results showing 200× compression with preserved rankings are convincing.

**Medium confidence:** The attribution-based refinement stage shows promise, but the specific implementation details of the EBM model and its integration could benefit from more transparency.

**Medium confidence:** The claim that existing benchmarks contain substantial redundancy is well-supported, though the exact quantification may vary depending on the evaluation criteria used.

## Next Checks

1. Test the method on non-English benchmarks and domain-specific datasets to evaluate cross-lingual and cross-domain generalization.
2. Implement ablation studies varying the coarse filtering thresholds and GA hyperparameters to establish sensitivity and robustness.
3. Compare against alternative compression methods like clustering-based or importance-sampling approaches on the same benchmarks to establish relative performance.