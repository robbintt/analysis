---
ver: rpa2
title: 'Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker
  Model'
arxiv_id: '2504.13292'
source_url: https://arxiv.org/abs/2504.13292
tags:
- training
- groktransfer
- embedding
- target
- vinit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GrokTransfer accelerates grokking in neural networks by transferring
  learned embeddings from a weaker model to a stronger target model. The method first
  trains a smaller model until it achieves non-trivial test performance, then uses
  its learned embedding (via a low-rank transformation) to initialize the target model's
  embedding.
---

# Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker Model

## Quick Facts
- arXiv ID: 2504.13292
- Source URL: https://arxiv.org/abs/2504.13292
- Reference count: 40
- Primary result: GrokTransfer accelerates grokking by transferring embeddings from a weaker model, reducing training time by ~5x

## Executive Summary
GrokTransfer is a method to accelerate the grokking phenomenon (delayed generalization) in neural networks by transferring learned embeddings from a weaker, already-trained model to a stronger target model. The key insight is that embeddings learned by a smaller model that has partially solved the task can provide a favorable initialization for a larger model, allowing it to bypass the initial memorization phase and achieve continuous generalization. The method involves training a small model until it reaches non-trivial test performance, then using its embedding (via a low-rank transformation) to initialize the target model's embedding. This approach is validated across multiple algorithmic tasks including modular addition/multiplication and sparse parity, showing significant efficiency gains compared to standard training.

## Method Summary
GrokTransfer accelerates grokking by leveraging embeddings learned from a weaker model. The method works by first training a smaller model (weak model) on the target task until it achieves non-trivial test performance. The embedding weights from this weak model are then used to initialize a low-rank embedding structure in a larger target model. Specifically, the target model's embedding is defined as the product of the weak model's embedding matrix (A) and a randomly initialized projection matrix (B). Both A and B are then trained together. This approach restructures the optimization landscape, allowing the target model to generalize continuously rather than experiencing the typical memorization-generalization phase transition seen in grokking.

## Key Results
- GrokTransfer reduces wall-clock training time by approximately 5x compared to standard training
- The method effectively eliminates delayed generalization for both fully-connected networks and Transformers
- Cross-architecture transfer is successful (FNN embeddings transferred to Transformer target)
- Theoretical justification provided for synthetic XOR task with provable generalization acceleration

## Why This Works (Mechanism)

### Mechanism 1: Informative Embedding Restructuring
Transferring embeddings from a model that has successfully "grokked" (even partially) places the target model in a favorable region of the loss landscape, allowing it to bypass the initial memorization phase. The weak model learns a projection of the input data that separates the signal from noise, and when this projection is transferred, the target model starts with a representation where the task is inherently simpler.

### Mechanism 2: Low-Rank Initialization and Dynamics Reshaping
Structuring the target model's embedding as the product of the weak embedding (A) and a learnable matrix (B) constrains the optimization path to a subspace where generalization occurs faster. This reshapes the training dynamics from a "sudden phase transition" to "continuous progress."

### Mechanism 3: Capacity-Induced Feature Selection
A model with limited capacity (the weak model) is forced to prioritize "generalizing" features over "memorizing" features to minimize loss, making its embeddings ideal for transfer. The weak model cannot simply memorize due to size constraints; it must learn the underlying structure.

## Foundational Learning

- **Concept: Grokking (Delayed Generalization)**
  - Why needed here: This is the core phenomenon being manipulated. One must understand that standard training often involves a long plateau of overfitting before a sudden drop to perfect generalization.
  - Quick check question: If a model reaches 100% training accuracy but 0% test accuracy, is it grokking? (Answer: Not yet; grokking requires the eventual sudden transition to high test accuracy).

- **Concept: Embeddings as Feature Representations**
  - Why needed here: The paper argues the specific geometry of the embedding space dictates the learning speed. One-hot embeddings are isotropic/informative-less, while learned embeddings align with task vectors.
  - Quick check question: Why would a Fourier embedding allow a model to learn modular addition faster than a one-hot embedding? (Answer: Fourier embeddings map modular arithmetic operations into linear rotations, making the non-linear task linearly solvable).

- **Concept: Weak-to-Strong Generalization**
  - Why needed here: The method relies on a "dumber" model teaching a "smarter" model. Understanding that a smaller model can extract signal without having the capacity to fully utilize it is crucial.
  - Quick check question: Can a weak model that achieves only 30% accuracy still help a strong model reach 100% accuracy? (Answer: Yes, provided the weak model has learned a non-trivial representation of the data structure).

## Architecture Onboarding

- **Component map:** Weak Model (f_W) -> Transfer Matrix (A) -> Projection Matrix (B) -> Target Model (f_T)

- **Critical path:**
  1. Train f_W until test accuracy stabilizes (e.g., 30-70% range)
  2. Freeze/copy weights to matrix A
  3. Initialize f_T with E_T = A · B (where B is random)
  4. Train f_T with standard optimizers (AdamW), allowing gradients to flow to both A and B

- **Design tradeoffs:**
  - Weak Model Size: Too small = learns no signal; Too large = wastes compute and might memorize. Recommendation: Start with 2-3 layers or reduced width
  - Embedding Dimension (d_W): Must be sufficient to represent the task structure
  - Freezing vs. Training A: The paper trains both A and B. Freezing A might constrain the target too much

- **Failure signatures:**
  - Flat Test Accuracy: Weak model never achieved non-trivial performance → Transfer failed
  - Delayed Grokking Persists: The weak model was likely too large or trained too long on noise
  - Training Instability: Large learning rates in the target phase causing oscillations

- **First 3 experiments:**
  1. Baseline Modular Addition: Train a Transformer on modular addition (p=113) from scratch. Observe the grokking delay.
  2. Basic GrokTransfer (FNN → FNN): Train a tiny 2-layer FNN until 30% test accuracy. Transfer embedding to a larger FNN. Observe if the delay is eliminated.
  3. Cross-Architecture Transfer (FNN → Transformer): Train a small FNN (weak) and transfer embedding to a Transformer (target).

## Open Questions the Paper Calls Out

- Can GrokTransfer be theoretically justified for more complex problems beyond the synthetic XOR cluster data? The current theoretical result is limited to a "relatively simple XOR task" and calls for theoretical justification for complex problems as a future direction.

- Can embedding transfer be applied to improve training dynamics in broader contexts where grokking is not typically observed? The method was "only investigated on problems where grokking occurs" and asks if it can apply to weak-to-strong generalization in a "broader context."

- How does the choice of weak model architecture (FNN vs. Transformer) impact the quality of the transferred embedding? The paper demonstrates transferring embeddings from an FNN to a Transformer, but does not ablate the effect of the weak model's architecture type.

## Limitations

- Theoretical analysis is limited to synthetic XOR problems, while empirical validation spans multiple real tasks without rigorous theoretical justification for cross-architecture transfer.

- The method's effectiveness depends critically on the weak model reaching a "sweet spot" of performance (non-trivial but not fully memorized), but the criteria for this threshold are not precisely defined.

- The claim that capacity constraints force weak models to learn structural features is plausible but not rigorously proven across different task families.

## Confidence

- **High Confidence:** The core observation that embeddings drive grokking behavior is well-supported by mechanistic analysis and ablation studies. The low-rank initialization approach is theoretically grounded for the XOR case.

- **Medium Confidence:** Cross-architecture transfer (FNN → Transformer) works empirically but lacks theoretical justification. The claim that capacity constraints force weak models to learn structural features is plausible but not rigorously proven.

- **Low Confidence:** The paper's assertion that this method works "universally" across different grokking tasks may be overstated without systematic testing on diverse problem families.

## Next Checks

1. **Weak Model Performance Sensitivity:** Systematically vary the weak model's training duration and capacity to identify the precise performance threshold required for successful transfer.

2. **Embedding Geometry Analysis:** Visualize and compare the embedding spaces learned by weak vs. strong models across different tasks to verify structural similarity preservation.

3. **Architecture Transfer Boundaries:** Test transfer between more divergent architectures (e.g., RNN → Transformer) to establish the limits of cross-architecture applicability.