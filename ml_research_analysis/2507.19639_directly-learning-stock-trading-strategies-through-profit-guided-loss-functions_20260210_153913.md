---
ver: rpa2
title: Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions
arxiv_id: '2507.19639'
source_url: https://arxiv.org/abs/2507.19639
tags:
- loss
- stock
- profit
- trading
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes novel loss functions that enable artificial
  neural networks to directly learn stock trading strategies by optimizing for profit
  rather than prediction accuracy. The approach modifies the final output layer of
  any time-series forecasting model to output bounded trading decisions (buy/short/hold)
  for each stock, using custom loss functions that maximize returns.
---

# Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions

## Quick Facts
- arXiv ID: 2507.19639
- Source URL: https://arxiv.org/abs/2507.19639
- Authors: Devroop Kar; Zimeng Lyu; Sheeraja Rajakrishnan; Hao Zhang; Alex Ororbia; Travis Desell; Daniel Krutz
- Reference count: 40
- Key outcome: Novel loss functions enable neural networks to directly learn stock trading strategies by optimizing for profit rather than prediction accuracy, achieving 48-51% annual returns on S&P 500 stocks

## Executive Summary
This paper introduces profit-guided loss functions that enable artificial neural networks to directly learn stock trading strategies without requiring separate price prediction or policy optimization. The approach modifies the final output layer of any time-series forecasting model to output bounded trading decisions (buy/short/hold) for each stock, using custom loss functions that maximize returns. Experiments using transformer-based models on 50 S&P 500 stocks show significant profit generation compared to state-of-the-art reinforcement learning methods and buy-and-hold baselines, with the best configuration achieving 51.42% annual returns in 2021.

## Method Summary
The method replaces a model's final output layer with N+1 neurons (N stocks + 1 hold node) and applies tanh activation to bound outputs between -1 and 1. These bounded outputs encode both trading direction (via sign) and allocation magnitude (via absolute value). Custom profit-guided loss functions compute returns based on these outputs, normalizing by the maximum return across the portfolio for stability. The smooth sign function (tanh(10·x)) replaces the non-differentiable sign function to enable gradient flow. The approach trains directly for profit rather than price prediction, using any time-series forecasting architecture as the backbone.

## Key Results
- Crossformer with price-based loss achieved 51.42% annual returns in 2021, 51.04% in 2022, and 48.62% in 2023
- Outperformed RL methods (PPO, DDPG) by 10-50 percentage points across test years
- Smooth loss functions (using tanh approximation) were statistically significantly better than non-smooth variants for 4 of 6 models
- Price-based loss consistently outperformed return-based loss despite non-stationarity concerns

## Why This Works (Mechanism)

### Mechanism 1
Bounding final layer outputs with tanh and optimizing profit-based loss enables direct strategy learning without separate policy optimization. The output layer uses N+1 neurons with tanh activation to constrain outputs to [-1, 1], encoding both trading direction and allocation magnitude that the custom loss function optimizes directly for profit rather than prediction accuracy.

### Mechanism 2
Normalizing returns by the maximum return across the portfolio improves training stability compared to raw profit optimization. The denominator max(Ret) scales the loss to a bounded range, preventing gradient explosion from differing price magnitudes across stocks.

### Mechanism 3
Approximating the non-differentiable sign(x) function with smooth tanh(γx) resolves gradient discontinuity at zero. Replacing sign with tanh(10·x) creates a differentiable approximation that maintains near-binary behavior away from zero while enabling smooth gradients for backpropagation.

## Foundational Learning

- **Time-series forecasting architectures (Transformers, LSTM variants)**: Understanding attention mechanisms and temporal encoding is prerequisite to selecting and debugging backbone models. Quick check: How does multi-head attention in Crossformer capture inter-stock dependencies across time steps?
- **Gradient flow through bounded activations**: The tanh output layer and smooth sign approximation are critical for maintaining trainable gradients throughout the network. Quick check: What happens to the gradient when tanh saturates near ±1, and how does this affect learning?
- **Portfolio theory and position sizing**: The loss functions assume capital allocation matters—Vi terms represent fractions of available budget; understanding why allocation weights must sum to 1 is essential. Quick check: How does the hold node interact with capital allocation across risky assets?

## Architecture Onboarding

- **Component map**: Input features → Backbone temporal/cross-dimensional processing → Bounded output layer → Profit-guided loss computation → Backpropagation
- **Critical path**: Input: 8 features × 50 stocks × 96 time-step sequence → Backbone (Crossformer, DeformTime, etc.) → N+1 neurons with tanh → Smooth Loss III (price-based) → Annual return calculation
- **Design tradeoffs**: Price vs. return loss (price-based performed better despite non-stationarity); Hold node inclusion (no significant effect for top models); Smooth vs. non-smooth loss (stability gained at cost of theoretical optimality)
- **Failure signatures**: All outputs near 0 (network learned to minimize loss by avoiding trades); High variance across runs (consider increasing smoothing or using L2-normalized loss); Hold node dominates allocation (input features may lack predictive signal)
- **First 3 experiments**:
  1. Replicate single-stock output analysis on your chosen backbone to verify gradient flows correctly through the modified output layer
  2. Compare Loss III with and without hold node on a 1-year validation period to confirm whether hold node impacts your specific data
  3. Sweep γ ∈ {5, 10, 20} to find the optimal smoothing sharpness for your dataset and model combination

## Open Questions the Paper Calls Out

### Open Question 1
Why does using raw price (PRC) in the loss functions consistently yield higher profits than using returns (RET), despite returns being a better normalized indicator of market direction? No ablation or theoretical analysis confirms whether non-stationarity, scale effects, or gradient dynamics drive the PRC advantage.

### Open Question 2
Under what market conditions or model architectures does the hold node provide statistically significant risk mitigation? The hold node showed no significant effect for top models, but it is unclear whether this is universal, portfolio-specific, or dependent on volatility regimes.

### Open Question 3
Can an ensemble of TSF models with profit-guided loss functions outperform the best single model (e.g., Crossformer) in consistency across bear and bull markets? Performance varied by year and model, but no ensemble experiments were conducted.

### Open Question 4
How does performance degrade when the model is retrained weekly or monthly in a realistic production setting, instead of using a single annual forecast? No rolling-origin or online retraining experiments were conducted to evaluate temporal drift and retraining overhead.

## Limitations
- Results based exclusively on 50 S&P 500 stocks from private CRSP database, limiting generalizability
- Absence of transaction costs, slippage, and market impact in profit calculations
- Evaluation period (2021-2023) includes atypical market conditions that may not represent normal market behavior

## Confidence

**High Confidence Claims:**
- Mathematical formulation of profit-guided loss functions is internally consistent
- Experimental methodology (10 runs per configuration, validation-based model selection) follows standard practices
- Superiority over RL baselines is statistically supported through Mann-Whitney U-tests

**Medium Confidence Claims:**
- Generalization of results to other stock universes or market conditions
- Claim that price-based loss outperforms return-based loss across all contexts
- Assertion that hold nodes are generally unnecessary for well-performing models

**Low Confidence Claims:**
- Real-world profitability given absence of transaction costs and slippage modeling
- Robustness across different market regimes not represented in 2021-2023
- Performance relative to traditional quantitative strategies not tested

## Next Checks

1. **Transaction Cost Sensitivity Analysis**: Re-run the best-performing configurations (Crossformer with Loss III) with realistic bid-ask spreads and commission costs (0.1-0.5% per trade) to assess whether the 48-51% annual returns remain profitable after accounting for market frictions.

2. **Out-of-Sample Market Regime Test**: Apply the trained 2021-2023 models to historical data from 2000-2002 dot-com crash and 2008 financial crisis periods to evaluate robustness during extended drawdowns and high-volatility regimes.

3. **Multi-Asset Extension Validation**: Adapt the framework to include non-S&P 500 assets (ETFs, commodities, or international equities) to test whether the profit-guided loss function generalizes beyond large-cap US stocks and captures cross-asset relationships.