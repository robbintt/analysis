---
ver: rpa2
title: 'EQA-RM: A Generative Embodied Reward Model with Test-time Scaling'
arxiv_id: '2506.10389'
source_url: https://arxiv.org/abs/2506.10389
tags:
- reasoning
- reward
- eqa-rm
- score
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EQA-RM is a generative multimodal reward model tailored for Embodied
  Question Answering (EQA), addressing the need for nuanced evaluation of agents'
  spatial, temporal, and logical understanding. It uses Contrastive Group Relative
  Policy Optimization (C-GRPO) with rule-based contrastive rewards from data augmentations
  to learn fine-grained behavioral distinctions.
---

# EQA-RM: A Generative Embodied Reward Model with Test-time Scaling

## Quick Facts
- **arXiv ID**: 2506.10389
- **Source URL**: https://arxiv.org/abs/2506.10389
- **Reference count**: 26
- **Primary result**: Achieves 61.9% accuracy on EQARewardBench with 700 samples, outperforming GPT-4o, Claude-3.5-Haiku, and other baselines

## Executive Summary
EQA-RM is a generative multimodal reward model designed for Embodied Question Answering (EQA) evaluation. It uses a two-stage training approach: first Rejective Fine-Tuning (RFT) to establish output format competence, then Contrastive Group Relative Policy Optimization (C-GRPO) with rule-based contrastive rewards from data augmentations to learn fine-grained behavioral distinctions. The model generates structured critiques and scores, enabling test-time scaling for dynamic evaluation granularity. EQA-RM achieves 61.9% accuracy on EQARewardBench with only 700 training samples, demonstrating exceptional sample efficiency and outperforming both open-source and proprietary baselines.

## Method Summary
EQA-RM trains a generative multimodal reward model on EQA trajectories using Qwen2-VL-2B-Instruct as the base model. The two-stage training pipeline begins with RFT on filtered supervised data to teach structured output generation, followed by C-GRPO that optimizes for evaluative accuracy using contrastive rewards from three augmentations: frame shuffling, spatial masking, and reasoning step jumbling. The model generates textual critiques and scalar scores (0-10), with test-time scaling enabling multiple sampling and aggregation for improved accuracy without retraining.

## Key Results
- Achieves 61.9% accuracy on EQARewardBench with 700 training samples
- Outperforms strong baselines including Gemini-2.5-Flash, GPT-4o, and Claude-3.5-Haiku
- Test-time scaling improves accuracy from 42.47% (K=1) to 61.86% (K=32)
- Demonstrates 9.39% overall accuracy improvement over base model

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Perturbation Training
Training the model to preferentially score unperturbed EQA trajectories over synthetically degraded versions instills sensitivity to temporal order, spatial detail, and reasoning coherence. C-GRPO applies three targeted augmentations—frame shuffling (temporal), spatial region masking, and reasoning step jumbling—with contrastive rewards applied when original scores exceed perturbed scores beyond a threshold. This forces the model to internalize that temporal sequence, fine-grained visual detail, and logical flow are discriminative for accurate evaluation.

### Mechanism 2: Test-Time Scaling via Generative Sampling
The generative architecture allows sampling multiple diverse evaluative reasoning paths at inference and aggregating them to improve evaluation accuracy without retraining. Sampling K critiques with temperature=0.8 and top-p=0.9 produces independent critique-score pairs, with aggregation reducing variance and mitigating single-pass errors. Accuracy improves from 42.47% (K=1) to 61.86% (K=32), demonstrating the model's internal reasoning distribution contains recoverable signal.

### Mechanism 3: Two-Stage Cold-Start Training
Rejective Fine-Tuning (RFT) must precede reinforcement learning to establish output format competence before contrastive learning can improve evaluative acuity. Stage 1 (RFT) teaches structured critique+score output generation using filtered supervised data, while Stage 2 (C-GRPO) optimizes for evaluative accuracy. Ablation shows RFT alone reduces accuracy (-1.39%), and RL without RFT underperforms base model (-3.25% to -0.88%). Only the combined pipeline achieves gains (+9.39% overall accuracy).

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: C-GRPO builds on GRPO's core idea of computing advantages by normalizing rewards within groups of outputs from the same prompt
  - Quick check: Given 8 generated outputs with rewards [0.2, 0.3, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9], what is the advantage of the output with reward 0.9?

- **Concept: Contrastive Learning for Reward Models**
  - Why needed: C-GRPO uses contrastive signals (original vs. perturbed) rather than direct human preference labels
  - Quick check: If your temporal shuffling augmentation accidentally preserves frame order for short videos (N≤3 frames), what happens to the contrastive reward signal?

- **Concept: Generative vs. Classifier Reward Models**
  - Why needed: EQA-RM generates critiques and scores as text tokens, enabling test-time scaling
  - Quick check: Why can't a standard classifier-based reward model (outputting a single probability) benefit from test-time scaling via majority voting?

## Architecture Onboarding

- **Component map**: Input (question, predicted_answer, reasoning_trace, video_frames) → Qwen2-VL-2B-Instruct → Output Head (generates <critique>...</critique><score>N</score>) → Two-stage training (RFT → C-GRPO) → Inference (Sample K paths → aggregate)

- **Critical path**: 
  1. Data preparation: Generate diverse responses with Gemini-2.0-Flash, score with Gemini-2.5-Pro, verify with humans → ground truth scores
  2. RFT filtering: Generate NRFT candidates per instance, filter by score tolerance and difficulty, then SFT
  3. C-GRPO training: Compute original + 3 perturbed scores, calculate contrastive boosts, update policy via clipped objective with KL penalty

- **Design tradeoffs**: 
  - Model size: 2B model enables sample efficiency but may limit reasoning depth
  - Keyframe selection: N=5 frames per episode reduces compute but may miss critical visual evidence
  - Aggregation strategy: Majority voting is robust to outliers; averaging preserves score granularity

- **Failure signatures**: 
  - RFT collapse: Model generates well-formatted but superficially correct critiques
  - Contrastive reward gaming: Model learns to always give low scores to perturbed inputs
  - Test-time scaling failure: Accuracy plateaus or decreases with K

- **First 3 experiments**: 
  1. Validate RFT filtering thresholds by ablating τ and NRFT on held-out validation set
  2. Isolate contrastive components by training three separate models with only Rt, Rs, or Rr enabled
  3. Test-time scaling curve by plotting accuracy vs. K ∈ {1, 2, 4, 8, 16, 32} on both in-distribution and OOD splits

## Open Questions the Paper Calls Out

- **Open Question 1**: Can adaptive or learned augmentation strategies outperform the predefined perturbations (shuffled frames, masked regions, jumbled reasoning) used in C-GRPO for capturing broader EQA failure modes?
- **Open Question 2**: Can EQA-RM's structured critiques be leveraged as direct feedback signals to improve EQA policy models through distillation or critique-guided reinforcement learning?
- **Open Question 3**: How effectively does EQA-RM generalize to EQA environments, visual styles, or interaction paradigms substantially different from the OpenEQA/HM3D/ScanNet domains?
- **Open Question 4**: Can open-source models generate ground truth scores comparable to Gemini-2.5-Pro, reducing reliance on proprietary models in the training pipeline?

## Limitations

- **Generalization to Non-EQA Domains**: Contrastive training relies heavily on perturbations specific to video reasoning, with effectiveness for other multimodal evaluation tasks untested
- **Dependence on High-Quality Synthetic Data**: Two-stage training depends on generating diverse, high-quality candidate critiques using auxiliary LLMs, with robustness to noisier data uncertain
- **Resource Requirements for Test-Time Scaling**: Computational cost scales linearly with K, with K=32 representing significant inference overhead

## Confidence

- **High Confidence**: Two-stage training pipeline significantly outperforms individual components; test-time scaling demonstrably improves accuracy; strong baseline comparisons provide robust empirical support
- **Medium Confidence**: Specific augmentation strategies and their relative contributions are well-supported, but optimal perturbation parameters may be dataset-dependent; test-time scaling mechanism's limits at extreme K values need exploration
- **Low Confidence**: "Human-level performance" claim difficult to verify without direct human baseline comparisons; sample efficiency claim may not scale to more complex EQA scenarios

## Next Checks

1. **Domain Transfer Experiment**: Evaluate EQA-RM on a non-EQA multimodal task (e.g., visual dialogue or video summarization evaluation) using the same test-time scaling approach
2. **Synthetic Data Quality Analysis**: Systematically degrade the quality of auxiliary LLM-generated candidates and measure impact on RFT filtering effectiveness and final model accuracy
3. **Resource-Constrained Scaling**: Measure accuracy improvement curves for K=1, 2, 4, 8, 16, 32 while tracking inference time and compute cost to identify optimal K for practical deployment