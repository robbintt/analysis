---
ver: rpa2
title: Neurosymbolic Decision Trees
arxiv_id: '2503.08762'
source_url: https://arxiv.org/abs/2503.08762
tags:
- neural
- learning
- tests
- decision
- probabilistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces neurosymbolic decision trees (NDTs) and NeuID3,
  a novel structure learning algorithm that combines neural networks with symbolic
  logic. NDTs extend probabilistic decision trees by using neural networks to parameterize
  probabilities, enabling both symbolic and subsymbolic data (e.g., images) as features.
---

# Neurosymbolic Decision Trees

## Quick Facts
- arXiv ID: 2503.08762
- Source URL: https://arxiv.org/abs/2503.08762
- Authors: Matthias MÃ¶ller; Arvid Norlander; Pedro Zuidberg Dos Martires; Luc De Raedt
- Reference count: 40
- Primary result: NDTs with NeuID3 outperform neural networks and traditional decision trees on UCI datasets and Eleusis benchmark, especially with image features

## Executive Summary
This paper introduces Neurosymbolic Decision Trees (NDTs) and NeuID3, a novel structure learning algorithm that combines neural networks with symbolic logic. NDTs extend probabilistic decision trees by using neural networks to parameterize probabilities, enabling both symbolic and subsymbolic data (e.g., images) as features. NeuID3 learns the structure of NDTs from scratch, incorporating background knowledge through neurosymbolic rules. Experiments demonstrate that NDTs outperform neural networks, especially when Boolean features are replaced with images, and show benefits of including background knowledge.

## Method Summary
Neurosymbolic Decision Trees (NDTs) are introduced as an extension of probabilistic decision trees where neural networks parameterize the probabilities at each node. The NeuID3 algorithm learns the structure of NDTs from scratch by recursively selecting the best split using a greedy approach based on the log-likelihood gain. Background knowledge is incorporated through neurosymbolic rules, which are used to prune the search space and guide the structure learning process. The neural tests at each node are trained jointly with the structure learning, allowing for end-to-end optimization. The approach is evaluated on UCI datasets and the Eleusis card game benchmark, demonstrating superior performance compared to neural networks and traditional decision trees.

## Key Results
- NDTs outperform neural networks on UCI datasets, especially when Boolean features are replaced with images
- Incorporating background knowledge through neurosymbolic rules improves performance on the Eleusis benchmark
- Reusing trained neural tests for more complex rules enhances the learning efficiency and effectiveness of NDTs

## Why This Works (Mechanism)
NDTs work by combining the interpretability and structure learning capabilities of decision trees with the flexibility and feature representation power of neural networks. The neural parameterization allows NDTs to handle both symbolic and subsymbolic data, while the greedy structure learning algorithm efficiently explores the hypothesis space. The incorporation of background knowledge through neurosymbolic rules further constrains the search space, leading to more interpretable and accurate models.

## Foundational Learning
- Decision trees: Non-parametric supervised learning method for classification and regression, using a tree-like model of decisions
- Neural networks: A series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates
- Probabilistic models: Statistical models that incorporate probability distributions to represent uncertainty in the model parameters or predictions
- Neurosymbolic AI: Integration of neural and symbolic approaches to AI, combining the strengths of both paradigms
- Structure learning: The process of learning the structure of a probabilistic model from data, including the identification of relevant features and their relationships
- Background knowledge: Prior information or domain expertise that can be incorporated into the learning process to improve model performance and interpretability

## Architecture Onboarding
Component map: Input data -> Neural tests (parameterized by NNs) -> Decision nodes -> Leaf nodes with class probabilities

Critical path: Data preprocessing -> Structure learning with NeuID3 -> Neural test training -> Model evaluation

Design tradeoffs: Balancing interpretability (shallow trees) vs. accuracy (deeper trees), incorporating background knowledge vs. learning from scratch, handling symbolic vs. subsymbolic features

Failure signatures: Overfitting on small datasets, poor performance on complex symbolic rules, inability to handle large-scale data efficiently

First experiments:
1. Evaluate NDTs on a small UCI dataset with both symbolic and subsymbolic features
2. Compare NDTs with traditional decision trees and neural networks on a benchmark with known background knowledge
3. Test the scalability of NeuID3 on a larger dataset with increasing complexity of symbolic rules

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger datasets and more complex symbolic rules is uncertain
- Generalizability to real-world applications beyond tested benchmarks is not demonstrated
- Computational complexity of the algorithm is not detailed, limiting understanding of practical applicability

## Confidence
- High confidence in the theoretical foundation and novelty of NDTs and NeuID3
- Medium confidence in the experimental results, given the limited scope of datasets and benchmarks
- Low confidence in the scalability and generalizability of the approach without further validation

## Next Checks
1. Test NeuID3 on larger, more diverse datasets to evaluate scalability and robustness
2. Compare NDTs with other state-of-the-art neurosymbolic models on the same benchmarks
3. Conduct ablation studies to isolate the contributions of neural parameterization and background knowledge integration to overall performance