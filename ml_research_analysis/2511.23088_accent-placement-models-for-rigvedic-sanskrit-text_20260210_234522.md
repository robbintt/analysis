---
ver: rpa2
title: Accent Placement Models for Rigvedic Sanskrit Text
arxiv_id: '2511.23088'
source_url: https://arxiv.org/abs/2511.23088
tags:
- accent
- sanskrit
- vedic
- byt5
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of automatically restoring
  pitch accents in unaccented Rigvedic Sanskrit text, a task crucial for preserving
  the oral and melodic tradition of the Rigveda. A parallel corpus of 22,740 accented-unaccented
  verse pairs was constructed, and three approaches were compared: full fine-tuning
  of ByT5, a BiLSTM-CRF baseline, and LoRA-based parameter-efficient fine-tuning on
  ByT5.'
---

# Accent Placement Models for Rigvedic Sanskrit Text

## Quick Facts
- arXiv ID: 2511.23088
- Source URL: https://arxiv.org/abs/2511.23088
- Reference count: 6
- Primary result: Byte-level transformer models achieve best accent restoration accuracy for Rigvedic Sanskrit

## Executive Summary
This study addresses the challenge of automatically restoring pitch accents in unaccented Rigvedic Sanskrit text, a task crucial for preserving the oral and melodic tradition of the Rigveda. A parallel corpus of 22,740 accented-unaccented verse pairs was constructed, and three approaches were compared: full fine-tuning of ByT5, a BiLSTM-CRF baseline, and LoRA-based parameter-efficient fine-tuning on ByT5. Performance was measured using Word Error Rate (WER), Character Error Rate (CER), and a task-specific Diacritic Error Rate (DER). Full ByT5 fine-tuning achieved the best overall results with WER 0.1023, CER 0.0246, and DER 0.0685. LoRA offered a favorable efficiency-accuracy trade-off, while BiLSTM-CRF performed poorly on accent restoration despite reasonable orthographic accuracy.

## Method Summary
The study constructed a parallel corpus of 22,740 Rigvedic Sanskrit verse pairs (accented and unaccented) from 10 maṇḍalas and 1,028 sūktas. Three models were evaluated: full ByT5 fine-tuning (lr=3e-5, batch=32, 10 epochs), BiLSTM-CRF (256-d embeddings, 2-layer BiLSTM, CRF decoder, Adam lr=1e-3, 20 epochs), and LoRA on ByT5 (rank=8, α=16, 0.5% parameters updated). Models were evaluated using WER, CER, and DER metrics. The corpus was split 85/10/5 into training, development, and test sets, stratified by maṇḍala and sūkta length.

## Key Results
- Full ByT5 fine-tuning achieved WER 0.1023, CER 0.0246, and DER 0.0685
- LoRA reduced trainable parameters by ~200x but increased DER to 0.1598
- BiLSTM-CRF had CER 0.0448 but DER 0.3197, showing poor accent restoration despite reasonable orthographic accuracy

## Why This Works (Mechanism)

### Mechanism 1: Byte-Level Representation Bypasses Tokenization Fragility
- Operating directly on UTF-8 bytes allows the model to handle Unicode combining marks (accents) without subword tokenization artifacts
- ByT5 encodes each byte as a separate token, so accent marks (e.g., U+0951 svarita, U+0952 anudatta) are explicitly represented rather than merged into opaque subword units
- Core assumption: Accent placement is context-dependent and requires modeling long-range dependencies in metrical verse
- Evidence anchors: [abstract] "a byte-level Transformer that operates directly on Unicode combining marks"; [section 2] "Byte and character-level transformers avoid fragile tokenization in diacritic-rich scripts"

### Mechanism 2: Full Parameter Fine-Tuning Captures Phonological Context
- Updating all model parameters enables learning the implicit phonological and metrical rules governing accent placement
- Full fine-tuning allows attention heads to specialize for different linguistic signals—morphological boundaries, enclitic positions, and sandhi contexts—that influence accent location
- Core assumption: The pretrained ByT5 multilingual representations transfer to Vedic Sanskrit despite its low-resource status
- Evidence anchors: [section 6] "Full ByT5 fine-tuning outperforms alternatives by a wide margin"; [section 7] "ByT5's byte-level encoding captures compound co-occurrence patterns"

### Mechanism 3: LoRA Preserves Diacritic Learning but Underfits Orthography
- Low-rank adaptation (rank=8, updating only 0.5% of parameters) is sufficient for local accent decisions but insufficient for global sequence modeling
- LoRA modifies only self-attention projection matrices, which may capture local accent-to-character associations but lack capacity for longer phonological sequences
- Core assumption: Accent placement is a more localized decision than general orthographic reconstruction
- Evidence anchors: [section 4] "LoRA with rank 8 and α=16 was applied to the self-attention projection matrices. Only 0.5% of parameters were updated."; [section 6] "LoRA reduces trainable parameters by orders of magnitude but suffers in WER/CER"

## Foundational Learning

- **Concept: Unicode Combining Marks vs. Precomposed Characters**
  - Why needed here: Vedic accents (U+0951, U+0952) are combining marks appended to base characters, not separate glyphs. Incorrect handling causes normalization errors that corrupt training data
  - Quick check question: Can you explain why NFC vs. NFD normalization would change the number of "characters" in the string "अ॒ग्नि" (agní with udatta)?

- **Concept: Sequence Labeling (CRF) vs. Sequence-to-Sequence (Transformer)**
  - Why needed here: BiLSTM-CRF treats accent placement as per-position classification; ByT5 treats it as translation. The paper shows the latter generalizes better for context-sensitive accent rules
  - Quick check question: For input "deva," would a CRF model be able to consider the accent of the following word when assigning the current word's accent?

- **Concept: Diacritic-Isolated Evaluation (DER)**
  - Why needed here: Standard WER/CER conflate orthographic errors with accent errors. DER isolates accent mistakes, revealing that BiLSTM-CRF's "acceptable" CER (0.0448) masks catastrophic accent failure (DER 0.3197)
  - Quick check question: If a model outputs "agní" instead of "ágni," does CER capture this error? Does DER?

## Architecture Onboarding

- **Component map:**
  Input (unaccented Devanagari string) -> [Preprocessing: Unicode normalization, validation] -> [ByT5 Encoder-Decoder (full FT) or ByT5 + LoRA adapters or BiLSTM encoder -> CRF decoder] -> Output (accented Devanagari string) -> [Evaluation: WER / CER / DER computed against reference]

- **Critical path:**
  1. Ensure corpus pairs are truly parallel (same graphemes, accents only differ)
  2. Validate Unicode handling—lossy normalization will silently corrupt labels
  3. Monitor DER separately during training; CER/WER can hide accent-specific failure

- **Design tradeoffs:**
  - Full ByT5: Best accuracy (DER 0.0685), highest compute/memory cost
  - LoRA ByT5: ~200x fewer trainable parameters, 2x worse DER, 3.5x worse WER
  - BiLSTM-CRF: Most interpretable (crf layer weights inspectable), but DER >30% unusable for chanting/ASR applications

- **Failure signatures:**
  - Accent drift across word boundaries → indicates insufficient context window
  - udatta-svarita confusion on reduplicated verbs → model lacks tone hierarchy priors
  - Over-generation on enclitic particles (ha, ca, u) → model learned spurious co-occurrence patterns
  - BiLSTM-CRF: Low CER but high DER → learned orthography but not accent distribution

- **First 3 experiments:**
  1. **Baseline reproduction:** Train BiLSTM-CRF from scratch on provided splits; verify DER ~0.32 confirms paper's baseline
  2. **LoRA rank sweep:** Test rank ∈ {4, 8, 16, 32} to find efficiency-accuracy frontier not explored in paper
  3. **Ablation on training size:** Subsample training data (10k, 5k, 2k pairs) to measure data efficiency gap between full FT and LoRA

## Open Questions the Paper Calls Out

- **Question:** Can phonology-aware encoder architectures or explicit tone hierarchy modeling significantly reduce udātta–svarita confusion errors in Rigvedic accent restoration?
  - Basis in paper: [explicit] Error analysis notes that "15.1% [of errors] involved udātta–svarita swaps, common in reduplicated or rhythmic verb forms, indicating a need for explicit tone hierarchy modeling"
  - Why unresolved: The study evaluated only ByT5, BiLSTM-CRF, and LoRA; phonology-aware encoders and non-autoregressive transformers were explicitly listed as unexplored architectures
  - What evidence would resolve it: A controlled comparison adding phonology-informed features (e.g., Prātiśākhya rule encodings) or tone hierarchy layers to the current ByT5 framework, measuring DER reduction on the same test split

- **Question:** Can evaluation metrics be developed that capture alignment with traditional metrical and phonological rules described in Prātiśākhya and Śikṣā texts?
  - Basis in paper: [explicit] Limitations state: "We use WER, CER, and DER, which measure surface accuracy but do not capture alignment with deeper metrical or phonological rules described in traditional sources"
  - Why unresolved: Current metrics operate at string level without encoding domain knowledge; no phonologically-grounded metric exists for Vedic accent evaluation
  - What evidence would resolve it: A new metric validated against expert annotators' judgments of phonological correctness, correlated with rule compliance scores derived from Prātiśākhya specifications

- **Question:** Does scaling the training corpus beyond 22,740 verse pairs yield diminishing returns or continued generalization gains for transformer-based accent restoration?
  - Basis in paper: [explicit] Limitations note "The corpus is relatively small compared to modern NLP benchmarks, restricting model generalization and robustness"
  - Why unresolved: The study used a fixed corpus; learning curve dynamics for heritage-language diacritic restoration remain uncharacterized
  - What evidence would resolve it: Systematic experiments varying training set size (e.g., 5K, 10K, 20K, 50K verses) while tracking WER/CER/DER on held-out data to identify saturation points

## Limitations
- Corpus availability is uncertain—the primary source URL may not be publicly accessible
- DER metric lacks precise implementation details for normalization and comparison
- Limited evaluation to three model architectures without exploring phonology-aware or non-autoregressive alternatives
- Small corpus size (22,740 pairs) compared to modern NLP benchmarks restricts generalization

## Confidence
- **High confidence**: ByT5 fine-tuning performance claims (WER 0.1023, CER 0.0246, DER 0.0685) are supported by clear training procedures and evaluation methodology
- **Medium confidence**: LoRA efficiency claims (200x fewer parameters, DER 0.1598) are methodologically sound but lack ablation studies on LoRA rank parameters
- **Low confidence**: Claims about accent drift patterns (46.8% misplacement, 38.5% omission) are based on error analysis but lack statistical significance testing or comparison with human baseline performance

## Next Checks
1. **Corpus verification**: Obtain the parallel corpus from C-DAC or construct alternative parallel pairs from public Rigveda sources. Validate Unicode normalization by testing whether NFC/NFD forms of "अ॒ग्नि" (agní with udātta) have identical DER scores.

2. **DER metric replication**: Implement DER calculation following the paper's description and compare against WER/CER on a small test set. Verify that accent-only errors (e.g., "agní" vs "ágni") are correctly isolated.

3. **LoRA parameter sweep**: Train LoRA models with ranks {4, 8, 16, 32} to determine if higher ranks recover orthographic performance while maintaining efficiency gains, as suggested by the paper's observation that LoRA preserves local accent decisions but underfits orthography.