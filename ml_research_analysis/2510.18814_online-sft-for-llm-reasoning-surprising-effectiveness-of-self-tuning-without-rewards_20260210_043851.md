---
ver: rpa2
title: 'Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without
  Rewards'
arxiv_id: '2510.18814'
source_url: https://arxiv.org/abs/2510.18814
tags:
- osft
- reasoning
- grpo
- pass
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces online supervised finetuning (OSFT), a reward-free
  training paradigm that improves LLM reasoning by finetuning on the model's own self-generated
  data. The method is based on the observation that low-temperature sampling amplifies
  a model's existing preferences from pretraining.
---

# Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards

## Quick Facts
- **arXiv ID**: 2510.18814
- **Source URL**: https://arxiv.org/abs/2510.18814
- **Reference count**: 40
- **Primary result**: OSFT achieves GRPO-level performance on mathematical reasoning tasks using single rollouts and no reward modeling

## Executive Summary
This paper introduces Online Supervised Finetuning (OSFT), a reward-free training paradigm that improves large language model reasoning by finetuning on the model's own self-generated data. The method leverages the observation that low-temperature sampling amplifies a model's existing preferences from pretraining, allowing the model to reinforce its latent knowledge without external reward signals. OSFT uses decoupled sampling and training temperatures, with lower sampling temperatures to generate higher-quality reasoning chains for finetuning. Experiments demonstrate that OSFT achieves performance comparable to strong RLVR baselines like GRPO while being more efficient by using only one rollout per prompt.

## Method Summary
OSFT is based on the principle that low-temperature sampling amplifies a model's existing preferences from pretraining, allowing the model to reinforce its latent knowledge without external reward signals. The method uses decoupled sampling and training temperatures, where a lower sampling temperature (e.g., T=0.2) generates higher-quality reasoning chains that are then used for finetuning at a higher training temperature (e.g., T=1.0). This temperature decoupling allows OSFT to extract the "best" reasoning chains from the model's pretraining without introducing temperature-dependent artifacts. The training process involves sampling responses at low temperature, filtering for correctness, and finetuning the model on this curated dataset, all without requiring a reward model or multiple rollouts.

## Key Results
- OSFT achieves comparable performance to GRPO on mathematical reasoning benchmarks using only single rollouts
- The method demonstrates 3-5x efficiency gains by eliminating the need for reward model evaluations
- Temperature decoupling (low sampling T, high training T) is critical for optimal performance
- OSFT shows robustness across different model scales and reasoning task types

## Why This Works (Mechanism)
OSFT works by exploiting the temperature-dependent behavior of language models during sampling. At low temperatures, the model's sampling distribution becomes more peaked around high-probability tokens, which tend to correspond to the model's strongest learned patterns from pretraining. This effectively filters for the model's "best" reasoning chains without requiring external evaluation. By finetuning on these low-temperature samples at a higher training temperature, OSFT can reinforce these strong reasoning patterns while maintaining the model's ability to generate diverse responses during inference. The temperature decoupling is crucial because it allows the model to learn from its most confident outputs while preserving its generative flexibility.

## Foundational Learning
**Temperature Scaling in LLMs**
- *Why needed*: Understanding how temperature affects token sampling distributions and model behavior
- *Quick check*: Verify that lower temperatures produce more deterministic, high-confidence outputs while higher temperatures increase diversity

**Reinforcement Learning from Human Feedback (RLHF)**
- *Why needed*: Provides context for why reward-based finetuning is the current standard and what OSFT aims to replace
- *Quick check*: Confirm understanding of reward modeling, policy optimization, and preference learning fundamentals

**Self-Training and Pseudo-Labeling**
- *Why needed*: OSFT is fundamentally a form of self-training where the model generates its own training data
- *Quick check*: Ensure grasp of self-training dynamics, label noise, and the bias-variance tradeoff in self-generated data

## Architecture Onboarding

**Component Map**
Pretrained LLM -> Low-Temperature Sampling -> Response Filtering -> Supervised Finetuning -> Improved Reasoning Model

**Critical Path**
1. Model generates responses at low temperature (T=0.2)
2. Responses are filtered for correctness/quality
3. Filtered data is used for supervised finetuning at higher temperature (T=1.0)
4. Model's reasoning capabilities are enhanced through this self-reinforcement loop

**Design Tradeoffs**
- Single rollout vs. multiple rollouts: OSFT trades exhaustive exploration for computational efficiency
- No reward model: Eliminates reward modeling complexity but relies on temperature scaling to capture preferences
- Temperature decoupling: Balances between extracting high-quality samples and maintaining generative diversity

**Failure Signatures**
- If low-temperature sampling fails to produce meaningful diversity, OSFT may reinforce narrow patterns
- Temperature misalignment could lead to overfitting to temperature-specific artifacts
- Without proper filtering, OSFT might reinforce systematic biases present in the model's pretraining

**First Experiments**
1. Compare OSFT performance across different temperature pairs (T_sample, T_train)
2. Test OSFT on non-mathematical reasoning domains to assess generalization
3. Evaluate the impact of different filtering thresholds on final performance

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to mathematical reasoning tasks, unclear generalizability to other domains
- Theoretical justification for temperature-preference relationship remains speculative
- Assumes temperature scaling reliably captures meaningful preference ordering without reward signals

## Confidence
- **High**: Empirical observation that low-temperature sampling produces higher-quality mathematical reasoning outputs
- **Medium**: Claim of comparable performance to GRPO, though baselines are not state-of-the-art RLVR methods
- **Low**: Theoretical justification for why temperature-decoupled OSFT works across different model families

## Next Checks
1. Test OSFT on non-mathematical reasoning domains (commonsense reasoning, multi-hop inference, or code generation) to assess domain generalization
2. Compare OSFT against more recent and stronger RLVR baselines that incorporate advanced reward modeling techniques
3. Conduct ablation studies across different temperature ranges and model scales to determine the stability of the temperature-preference relationship