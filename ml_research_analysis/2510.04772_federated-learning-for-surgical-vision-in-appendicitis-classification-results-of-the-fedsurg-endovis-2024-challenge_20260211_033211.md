---
ver: rpa2
title: 'Federated Learning for Surgical Vision in Appendicitis Classification: Results
  of the FedSurg EndoVis 2024 Challenge'
arxiv_id: '2510.04772'
source_url: https://arxiv.org/abs/2510.04772
tags:
- challenge
- data
- center
- surgical
- team
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The FedSurg challenge benchmarks federated learning for surgical
  video classification using the Appendix300 dataset, addressing the need for privacy-preserving,
  multi-institutional AI development in surgery. Three teams developed diverse approaches:
  Team Santhi employed a frozen ViViT backbone with linear probing, Team Elbflorenz
  used a frozen EndoViT model with adaptive FedSAM, and Team Camma utilized a Siamese
  network with triplet loss and metric learning.'
---

# Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge

## Quick Facts
- arXiv ID: 2510.04772
- Source URL: https://arxiv.org/abs/2510.04772
- Reference count: 40
- Primary result: FedSurg challenge benchmarks federated learning for surgical video classification using the Appendix300 dataset

## Executive Summary
The FedSurg EndoVis 2024 challenge evaluated federated learning approaches for appendicitis classification from laparoscopic videos across multiple institutions. Three teams developed diverse strategies using frozen foundation models and various aggregation techniques to address privacy-preserving AI development in surgery. The challenge revealed significant challenges in generalizing from training centers to unseen centers, with all teams achieving F1-scores below 25% on the generalization task. However, all teams showed improved performance when adapting to specific centers, highlighting the tension between personalization and generalization in federated surgical learning.

## Method Summary
The challenge used a subset of the Appendix300 dataset with 223 laparoscopic videos from 4 German centers, with 153 videos from Centers 1-3 for training and 29 videos from Center 4 as a held-out test set. Each video provided 200 frames extracted around an annotated timestamp at 2 fps. Three tasks were evaluated: federated training across centers, generalization to an unseen center (Task 1), and center-specific adaptation (Task 2). Teams employed various approaches including frozen ViViT backbones with linear probing, frozen EndoViT models with adaptive FedSAM aggregation, and Siamese networks with triplet loss. Performance was measured using macro F1-score and Expected Cost for ordinal classification.

## Key Results
- Team Santhi achieved the highest F1-score of 23.03% and lowest Expected Cost of 12.41% on the unseen Center 4 generalization task
- All teams showed improved performance when fine-tuning on specific centers (Task 2), though ranking stability was low across bootstrapping iterations
- Spatiotemporal modeling and context-aware preprocessing emerged as promising strategies for improving generalization across centers
- Current methods struggle with class imbalance and domain shift between different surgical centers

## Why This Works (Mechanism)

### Mechanism 1: Spatiotemporal Feature Aggregation for Temporal Context
Video-level models with temporal modeling outperform frame-level aggregation for surgical classification under domain shift. The ViViT backbone processes 32 frames as a unified spatiotemporal token sequence, capturing temporal dependencies that single-frame models miss. This provides implicit context about inflammation progression dynamics within the surgical window. Core assumption: The 100-second window around the keyframe contains diagnostically relevant temporal patterns that generalize across centers. Break condition: If temporal window is misaligned with the diagnostic event, or if test center uses drastically different frame rates/resolutions that violate temporal coherence assumptions.

### Mechanism 2: Frozen Foundation Model with Linear Probing for FL Stability
Freezing pretrained backbones while training only classification heads improves generalization stability in federated settings with limited local data. Frozen encoders prevent catastrophic interference from heterogeneous local updates. Only the lightweight head parameters are aggregated, reducing the dimensionality of the consensus problem and maintaining transferable representations learned from large-scale pretraining. Core assumption: The pretrained backbone captures sufficiently general visual features that transfer to appendicitis grading without domain-specific fine-tuning. Break condition: If the domain gap between pretraining data and target is too large, frozen features become insufficient discriminators.

### Mechanism 3: Sharpness-Aware Minimization for Non-IID Robustness
FedSAM aggregation improves generalization by seeking flat minima in the loss landscape, reducing sensitivity to client-specific distribution shifts. Standard FedAvg converges to sharp minima that fit training center distributions but fail on new centers. FedSAM's perturbation-based optimization encourages flatter regions where small input shifts cause smaller performance degradation. Core assumption: The test center's distribution is a perturbation of the training distribution manifold. Break condition: If test center represents fundamentally different surgical protocols, equipment, or patient populations, flat minima provide no guarantee.

## Foundational Learning

- **Federated Averaging (FedAvg)**
  - Why needed here: The baseline aggregation method all teams adapted. Understanding how local gradients are weighted and averaged is essential for diagnosing why FedAvg struggles with non-IID surgical data.
  - Quick check question: Can you explain why FedAvg may cause client drift when centers have different class distributions?

- **Vision Transformers for Video (ViViT)**
  - Why needed here: The winning architecture. Requires understanding tokenization of video frames, positional encodings, and how temporal attention differs from 2D spatial attention.
  - Quick check question: How does ViViT's factorized encoder differ from uniformly sampling frames and processing them independently?

- **Ordinal Classification Metrics (Expected Cost)**
  - Why needed here: The challenge uses EC to penalize misclassifications by distance from true label. Standard accuracy would treat predicting class 0 instead of class 5 the same as predicting class 4.
  - Quick check question: For a 6-class ordinal problem, what is the maximum EC penalty and when is it incurred?

## Architecture Onboarding

- **Component map:**
[Local Center Data] → [Frame Sampler] → [Frozen Backbone (ViViT/EndoViT)]
                                                           ↓
[Local Head Training] ← [Global Aggregation Server] ← [Head Gradients]
         ↓
[Local Best Model Selection] → [Aggregate to Global] → [Redistribute]
                                                           ↓
[Test Time] → [Global Model for unseen center (Task 1)]
           → [Fine-tuned Local Model for known center (Task 2)]

- **Critical path:**
  1. Frame sampling strategy (32 frames from 200 available) — this was a key differentiator
  2. Backbone selection and freezing decision
  3. Aggregation strategy choice (FedAvg vs FedSAM vs FedMedian)
  4. Local epochs per round (Team Santhi: 20, Elbflorenz: 2, Camma: 5) — higher local epochs risk overfitting to local distribution

- **Design tradeoffs:**
  - Frozen vs fine-tuned backbone: Stability vs adaptability
  - Video-level vs frame-level prediction: Temporal context vs computational cost and robustness to bad frames
  - Higher local epochs vs more communication rounds: Faster local convergence vs better global consensus

- **Failure signatures:**
  - Model predicts single class for all inputs (Team Camma on Center 4 predicted class 0 for 27/29 samples)
  - Large gap between F1 and EC (indicates systematic off-by-one errors on ordinal scale)
  - Ranking instability under bootstrapping (Task 2 showed no team retained rank >50% of iterations)

- **First 3 experiments:**
  1. **Ablate frame sampling**: Compare uniform sampling vs center-biased sampling (Team Santhi's 60% bias toward center frames) on held-out validation split
  2. **Test backbone transfer**: Run linear probing with frozen ViViT vs frozen EndoViT vs frozen ResNet-50 to quantify domain-specific pretraining benefit
  3. **Aggregate strategy sweep**: Run FedAvg, FedSAM, and FedMedian with identical backbone and head to isolate aggregation effects on generalization (Task 1) vs adaptation (Task 2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can imbalance-aware aggregation strategies improve performance on rare inflammation stages in federated surgical video analysis?
- Basis in paper: [explicit] The authors state in Section 4.4 that "Imbalance-aware strategies, such as reweighting, resampling, or federated focal loss, are needed."
- Why unresolved: Rare inflammation stages were underrepresented, causing poor classification across all teams; FL amplifies this heterogeneity rather than resolving it.
- What evidence would resolve it: A controlled comparison demonstrating that federated focal loss or dynamic resampling achieves higher recall for minority classes compared to standard cross-entropy loss.

### Open Question 2
- Question: Does self-supervised pretraining on surgical video data yield better federated transferability than generic video datasets?
- Basis in paper: [explicit] Section 4.4 suggests that "self-supervised pretraining on large-scale surgical video datasets could yield more transferable representations."
- Why unresolved: Current top-performing models rely on generic pretraining (e.g., Kinetics-400), which may lack the specific features necessary to generalize to unseen surgical centers.
- What evidence would resolve it: Benchmarking models pretrained on domain-specific surgical data against those pretrained on general video datasets on the unseen center generalization task (Task 1).

### Open Question 3
- Question: How can hyperparameter tuning be effectively conducted in decentralized settings without access to the centralized data distribution?
- Basis in paper: [explicit] Section 4.3 identifies "difficulties in hyperparameter tuning in decentralized training" as a systemic limitation due to unknown data distributions.
- Why unresolved: Participants could not view the global data distribution, leading to suboptimal convergence and overfitting to local training data.
- What evidence would resolve it: Development of a decentralized optimization protocol (e.g., federated Bayesian optimization) that maintains stable performance rankings across heterogeneous centers.

## Limitations

- Small held-out test set (29 videos from Center 4) limits statistical power and makes performance differences vulnerable to outliers
- Low generalization F1-scores (<25%) indicate current methods struggle with domain shift between centers due to camera system and protocol differences
- Class imbalance across centers complicates evaluation, though per-class distributions are not reported

## Confidence

- **High confidence**: Federated learning framework implementation, metric definitions (F1, EC), general challenge structure and task definitions
- **Medium confidence**: Relative performance rankings (Task 1 vs Task 2), effectiveness of frozen backbone approaches, importance of spatiotemporal modeling
- **Low confidence**: Absolute performance numbers, specific architectural choices' impact, generalizability of findings to larger datasets or different surgical tasks

## Next Checks

1. **Statistical validation**: Perform permutation tests on F1-scores to determine if observed differences between teams are statistically significant given the small test set size
2. **Domain adaptation analysis**: Measure domain shift between centers using Fréchet Video Distance or similar metrics to quantify how camera/lighting differences affect generalization
3. **Class imbalance assessment**: Compute and report per-class frequency distributions across all centers to identify which inflammation stages are under-represented and design appropriate rebalancing strategies