---
ver: rpa2
title: 'LIMR: Less is More for RL Scaling'
arxiv_id: '2502.11886'
source_url: https://arxiv.org/abs/2502.11886
tags:
- training
- data
- learning
- limr
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether scaling up reinforcement learning
  (RL) training data improves reasoning capabilities in language models. The authors
  propose Learning Impact Measurement (LIM), a method to evaluate and prioritize training
  samples based on their alignment with model learning trajectories.
---

# LIMR: Less is More for RL Scaling

## Quick Facts
- arXiv ID: 2502.11886
- Source URL: https://arxiv.org/abs/2502.11886
- Reference count: 16
- Key outcome: Strategically selected RL training samples can outperform larger random datasets for reasoning tasks

## Executive Summary
This paper challenges the assumption that more reinforcement learning (RL) training data automatically leads to better reasoning performance in language models. The authors propose Learning Impact Measurement (LIM), a method to evaluate and prioritize training samples based on their alignment with model learning trajectories. Using LIM, they demonstrate that a carefully selected subset of 1,389 samples can outperform the full 8,523-sample dataset, achieving significant improvements on challenging reasoning benchmarks.

## Method Summary
The authors introduce LIMR (Learning Impact Measurement for Reinforcement Learning), which evaluates each training sample's contribution to model learning progress. The methodology involves measuring how much each sample impacts the model's trajectory during training, allowing for strategic selection of the most impactful samples. By focusing on quality over quantity, LIMR achieves superior performance with significantly fewer samples than traditional approaches.

## Key Results
- A strategically selected subset of 1,389 samples outperforms the full 8,523-sample dataset
- LIMR achieves 16.7% higher accuracy on AIME24 compared to baseline approaches
- LIMR outperforms LIMO and s1 by 13.0% and 22.2% on MATH500, respectively

## Why This Works (Mechanism)
The paper suggests that sample quality and strategic selection matter more than data scale for RL training in reasoning tasks. By focusing on samples that have the highest impact on the model's learning trajectory, LIMR ensures that each training example contributes maximally to improving reasoning capabilities. This approach avoids the diminishing returns and potential noise that can come from simply increasing dataset size.

## Foundational Learning
- **Reinforcement Learning Fundamentals**: Understanding reward maximization and policy optimization is crucial for grasping how LIMR improves sample selection. Quick check: Can you explain the difference between on-policy and off-policy RL?
- **Sample Efficiency in RL**: The concept that fewer, well-chosen samples can outperform larger datasets is central to LIMR's approach. Quick check: What metrics would you use to measure sample efficiency?
- **Learning Trajectory Analysis**: LIMR relies on tracking how samples affect the model's learning path. Quick check: How would you visualize a model's learning trajectory during training?

## Architecture Onboarding
**Component Map**: Data Sampler -> LIM Metric Calculator -> Sample Selector -> RL Trainer -> Performance Evaluator
**Critical Path**: The most time-consuming step is computing the LIM metric for all candidate samples, which requires multiple forward passes through the model.
**Design Tradeoffs**: LIMR trades computational overhead during sample selection for improved performance with fewer samples, versus the brute-force approach of training on all available data.
**Failure Signatures**: Poor sample selection may lead to suboptimal performance despite using fewer samples, or the LIM metric may become computationally prohibitive for very large candidate pools.
**First Experiments**:
1. Verify that LIM-selected samples consistently outperform random samples across multiple training runs
2. Test LIMR's effectiveness on different model sizes (7B, 14B, 33B) to confirm scalability
3. Compare LIMR against uncertainty sampling and diversity-based selection methods

## Open Questions the Paper Calls Out
The authors acknowledge that the LIM metric's robustness and sensitivity to hyperparameter choices need further validation. They also note that the approach's effectiveness across different reasoning domains and model architectures remains to be fully explored.

## Limitations
- The LIM methodology lacks extensive theoretical grounding and ablation studies on alternative selection criteria
- Comparison to established RLHF approaches is limited, focusing primarily on other data-efficient methods
- Experimental setup details are sparse, making independent replication challenging

## Confidence
- High confidence in the empirical observation that carefully selected samples can outperform larger random datasets for RL training
- Medium confidence in the specific LIM methodology's superiority, as the metric's robustness and sensitivity to hyperparameter choices are not thoroughly validated
- Medium confidence in the claimed performance gains, pending replication and independent verification of the benchmark results

## Next Checks
1. Conduct ablation studies varying the LIM metric parameters and compare against alternative sample selection strategies (e.g., uncertainty sampling, diversity-based selection) to establish robustness
2. Replicate the AIME24 and MATH500 experiments using independent implementations to verify the claimed 16.7%, 13.0%, and 22.2% performance improvements
3. Test LIMR's effectiveness on additional reasoning benchmarks beyond AIME24 and MATH500 to assess generalizability of the approach