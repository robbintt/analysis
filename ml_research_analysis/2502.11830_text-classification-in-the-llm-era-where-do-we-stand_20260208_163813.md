---
ver: rpa2
title: Text Classification in the LLM Era -- Where do we stand?
arxiv_id: '2502.11830'
source_url: https://arxiv.org/abs/2502.11830
tags:
- data
- classification
- text
- zero-shot
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares zero-shot, few-shot, synthetic data, and full-data
  approaches for text classification across 32 datasets in 8 languages. Zero-shot
  LLMs perform well for sentiment tasks but are outperformed by fine-tuned classifiers
  for other tasks, especially as label counts increase.
---

# Text Classification in the LLM Era -- Where do we stand?

## Quick Facts
- arXiv ID: 2502.11830
- Source URL: https://arxiv.org/abs/2502.11830
- Reference count: 24
- Primary result: Zero-shot LLMs excel at sentiment classification but are outperformed by fine-tuned approaches for complex tasks with many categories

## Executive Summary
This study comprehensively compares zero-shot, few-shot, synthetic data, and full-data approaches for text classification across 32 datasets in 8 languages. Zero-shot LLMs show strong performance for sentiment tasks but struggle with complex classification tasks involving many categories. Few-shot fine-tuning provides significant accuracy improvements for non-sentiment tasks when 10 labeled examples per category are available. Synthetic data generated from multiple LLMs can outperform zero-shot approaches and occasionally exceed GPT4 zero-shot performance, particularly for complex tasks with many categories.

## Method Summary
The paper evaluates four classification approaches across four tasks (sentiment, intent, scenario, taxi) in eight languages using 32 datasets. Zero-shot classification uses GPT-4, Qwen2.5-7B, Aya23-8B, and Aya-Expanse-8B with structured output enforcement via Instructor. Few-shot fine-tuning employs FastFit contrastive learning with 10 samples per label. Synthetic data is generated from three LLMs and used to train logistic regression classifiers on multilingual embeddings, mBERT fine-tuning, and Qwen2.5-7B-Instruct SFT. Full-data classifiers are trained on human-labeled data using the same model architectures.

## Key Results
- Zero-shot LLMs match or exceed fine-tuned classifiers for sentiment tasks (3 classes) but degrade significantly for multi-class tasks (60 classes)
- Few-shot fine-tuning with FastFit provides 5-10% accuracy gains over zero-shot GPT4 for non-sentiment tasks with 10 examples per category
- Synthetic data from multiple LLMs outperforms zero-shot classification with open LLMs and occasionally exceeds GPT4 zero-shot
- Language disparities show 11-29% accuracy gaps across all methods, with Arabic and Hindi showing systematic underperformance

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot LLMs excel at sentiment classification because sentiment tasks map to broadly learned linguistic patterns in pretraining data (ubiquitous subjective valence signals), while topic/intent classification with many categories requires sharper decision boundaries that emerge from task-specific gradient updates. The performance degrades as label count increases because pretraining corpora lack specific label alignment needed for high-cardinality topic taxonomies.

### Mechanism 2
Few-shot fine-tuning with FastFit provides 5-10% accuracy gains by transforming the encoder into a task-specific representation learner through contrastive learning. The approach pulls same-class embeddings together and pushes different-class embeddings apart, creating discriminative boundaries from minimal signal. FastFit uses token-level similarity scoring to handle many-class scenarios effectively.

### Mechanism 3
Synthetic data from multiple LLMs outperforms single-source generation because aggregating examples from GPT4, Qwen2.5-7B, and Aya-Expanse-8B introduces lexical and stylistic diversity that reduces model-specific generation biases. Training a lightweight classifier on this diverse synthetic corpus transfers the collective knowledge of multiple LLMs into a cheaper inference-time model.

## Foundational Learning

- **Concept: Zero-shot vs. Few-shot vs. Full-data Classification Paradigms**
  - Why needed here: The paper's core contribution compares these three regimes plus synthetic data across tasks and languages
  - Quick check question: Given a new classification task with 50 categories and 5 labeled examples per category, which approach does this paper suggest trying first?

- **Concept: Contrastive Learning for Text Classification**
  - Why needed here: FastFit's success over zero-shot relies on supervised contrastive learning
  - Quick check question: How does contrastive learning differ from standard cross-entropy fine-tuning in terms of what the loss optimizes?

- **Concept: Embedding-based Classification Pipelines**
  - Why needed here: The strongest simple baseline was logistic regression on embeddings
  - Quick check question: What are the computational tradeoffs between using a frozen embedding model + logistic regression versus fine-tuning BERT end-to-end?

## Architecture Onboarding

- **Component map:**
  - Zero-shot path: LLM (GPT4/Qwen/Aya) → Structured prompt with Instructor → Parsed label
  - Few-shot path: 10 examples/label → FastFit contrastive fine-tuning → Classifier
  - Synthetic data path: Multi-LLM generation (GPT4 + Qwen2.5 + Aya-Expanse) → Aggregate dataset → Train classifier (logistic regression on embeddings / mBERT fine-tune / Qwen instruction-tune)
  - Full-data path: Human-labeled data → Same classifier options as synthetic

- **Critical path:**
  1. Assess task type (sentiment vs. topic/intent) and label cardinality
  2. If sentiment/≤3 labels: try zero-shot GPT4 or Aya-Expanse first
  3. If >10 labels: zero-shot degrades; prioritize few-shot FastFit (if 10+ examples/label available) or synthetic data generation
  4. If high-quality labeled data exists: logistic regression on embeddings for simplicity, SFT for best accuracy with many labels

- **Design tradeoffs:**
  - Accuracy vs. cost: GPT4 zero-shot has ongoing API costs; synthetic data is one-time generation cost then cheap inference
  - Language coverage vs. performance: 11-29% accuracy gap between best/worst languages across all methods
  - Complexity vs. label count: Logistic regression strong for few labels; SFT necessary for 60-class INTENT task

- **Failure signatures:**
  - Zero-shot generates explanations instead of labels (10% error rate for open LLMs, mitigated by Instructor)
  - Few-shot shows >40% language gap for small datasets (TAXI 1500: French 69% vs. Arabic 30%)
  - Synthetic data generation 3x slower for non-Roman scripts with higher token costs

- **First 3 experiments:**
  1. Establish baseline: Run zero-shot GPT4 and Aya-Expanse on your target task; if accuracy >80% and label count ≤5, stop here for sentiment-like tasks
  2. Test few-shot ceiling: Sample 10 examples per label, train FastFit; compare to zero-shot to quantify the data-value gap
  3. Validate synthetic feasibility: Generate 50-100 synthetic examples per label from 2 LLMs (one open, one proprietary), train logistic regression on embeddings; if this matches or exceeds zero-shot, scale synthetic generation for full coverage

## Open Questions the Paper Calls Out

- Is the strong zero-shot performance on sentiment analysis tasks attributable to the subjective nature of the task or simply the low number of categories?
- Do the comparative advantages of few-shot fine-tuning and synthetic data transfer effectively to multi-label text classification?
- Do the performance trends and language disparities identified in this study persist when classifying long-form texts?
- Are the observed language disparities consistent across typologically diverse languages outside the Indo-European family?

## Limitations

- Study covers only 32 datasets across 8 languages, representing a small fraction of real-world classification scenarios
- Zero-shot prompt sensitivity not systematically explored, potentially affecting observed performance differences
- Synthetic data quality assessment limited to accuracy metrics without distributional validation
- Language coverage gaps (11-29% accuracy) not fully investigated regarding root causes

## Confidence

**High Confidence:**
- Zero-shot LLMs perform well for sentiment classification but degrade for complex tasks with many categories
- Few-shot fine-tuning with 10 examples per label provides measurable improvements over zero-shot for non-sentiment tasks
- Synthetic data from multiple LLMs can outperform single-source generation approaches

**Medium Confidence:**
- Synthetic data can occasionally exceed GPT4 zero-shot performance
- The 10-example threshold is optimal for few-shot fine-tuning across all non-sentiment tasks
- Logistic regression on embeddings represents the best simple baseline across languages

**Low Confidence:**
- Language disparities are primarily due to pretraining data imbalance rather than architectural limitations
- The synthetic data generation cost is justified by the performance gains across all task types
- FastFit's contrastive approach is universally superior to other few-shot methods

## Next Checks

1. **Prompt Sensitivity Analysis:** Systematically test 10+ different zero-shot prompt templates across sentiment and multi-class tasks to quantify the variance in LLM performance.

2. **Synthetic Data Distribution Validation:** Compare the distribution of synthetic examples to real data using KL divergence and coverage metrics for each category.

3. **Cross-Domain Generalization Test:** Apply the recommended classification approach to 5-10 domain-specific datasets outside the original 32 to test whether practical guidance holds beyond studied tasks.