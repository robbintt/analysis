---
ver: rpa2
title: A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large
  Language Models
arxiv_id: '2511.22109'
source_url: https://arxiv.org/abs/2511.22109
tags:
- persuasion
- features
- llms
- persuasive
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study develops a hybrid approach combining large language\
  \ models (LLMs) and psychological theory to predict persuasive success in online\
  \ discourse. The method uses LLM-generated ratings of eight theory-derived features\u2014\
  including epistemic emotion and willingness to share\u2014as inputs to a Random\
  \ Forest classifier trained on belief change data."
---

# A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large Language Models

## Quick Facts
- **arXiv ID:** 2511.22109
- **Source URL:** https://arxiv.org/abs/2511.22109
- **Reference count:** 40
- **Primary result:** Hybrid model combining LLM-generated theory-driven features with Random Forest classifier achieves 82% accuracy in detecting persuasive comments.

## Executive Summary
This study develops a hybrid approach combining large language models (LLMs) and psychological theory to predict persuasive success in online discourse. The method uses LLM-generated ratings of eight theory-derived features—including epistemic emotion and willingness to share—as inputs to a Random Forest classifier trained on belief change data. The model achieved 82% accuracy in classifying persuasive versus non-persuasive comments, outperforming both theory-driven and data-driven baselines. Epistemic emotion and willingness to share emerged as top predictors of belief change. The results demonstrate that integrating psychological insights with LLM capabilities enhances interpretability and predictive performance in persuasion detection, with broader applications in influence detection and misinformation mitigation.

## Method Summary
The approach uses LLMs (LLaMA3-70B, Gemma2-9B, or Mixtral-8x7B) to extract eight psychological features from comment text based on established persuasion theory. These features are combined with a Belief Update score predicted by an OLS regression trained on human-annotated experimental data. A Random Forest classifier (300 trees) is then trained on these features to classify comments as persuasive or non-persuasive. The method leverages the structured reasoning of LLMs for feature extraction while using classical ML for classification, balancing interpretability with predictive power.

## Key Results
- Hybrid model achieves 82% accuracy in classifying persuasive versus non-persuasive comments
- Epistemic emotion ("Interesting-If-True") and willingness to share ranked as top predictors
- Random Forest outperforms both theory-driven (OLS regression) and data-driven baselines
- Truthfulness consistently ranked among least important features, while epistemic engagement ranked highest

## Why This Works (Mechanism)

### Mechanism 1
LLMs guided by explicit psychological constructs produce higher-quality predictive signals than unguided black-box reasoning or surface-level text analysis. By constraining the LLM to rate specific theory-derived dimensions, the model transforms unstructured text into structured, interpretable feature vectors. This reduces the search space for the downstream classifier compared to raw term frequencies and provides more grounded inputs than generic LLM embeddings. Core assumption: LLMs can accurately approximate human ratings of abstract psychological constructs from text alone.

### Mechanism 2
Persuasion in online discourse is more strongly predicted by epistemic engagement (curiosity/interest) than by objective truthfulness or simple emotional valence. The Random Forest classifier identifies "Interesting-If-True" and "Shareable" as top predictors, suggesting successful persuasion relies on triggering cognitive engagement and social diffusion potential rather than just factual accuracy or positive tone. Core assumption: The Change My View dataset accurately reflects general persuasion dynamics, which may prioritize "viral" or "engaging" traits over strictly logical ones.

### Mechanism 3
Non-linear aggregation of theory-driven features via ensemble methods (Random Forest) outperforms linear theory-driven models (OLS Regression). Persuasion likely involves threshold effects and complex trade-offs between features. A Random Forest can capture these non-linear boundaries, whereas the OLS baseline assumes a linear additive relationship between features and belief update. Core assumption: The relationship between psychological features and persuasion is non-linear and cannot be captured by a simple weighted sum.

## Foundational Learning

- **Epistemic Emotions**: Feelings that arise from the processing of information and drive learning. Why needed: The study identifies "Interesting-If-True" as a top predictor, helping interpret why "interest" drives persuasion more than "truth." Quick check: How does "Interesting-If-True" differ from "Positive Emotion" in the context of argument processing?

- **Hybrid Modeling (Neuro-Symbolic)**: Combining the statistical power of LLMs (symbolic/feature extraction) with the structured decision-making of classical ML (Random Forest). Why needed: This paper is an instance of hybrid AI, balancing interpretability with predictive power. Quick check: Why use a Random Forest on LLM-generated features instead of just fine-tuning the LLM directly on the persuasion classification task?

- **Variable Importance (Permutation)**: Measures the drop in accuracy when a feature is shuffled, isolating its causal contribution to the model's performance. Why needed: The study relies on this metric to determine which features matter most. Quick check: If "Truthfulness" has low Variable Importance, does that mean truthfulness is irrelevant to persuasion, or just that it varies little in the dataset?

## Architecture Onboarding

- **Component map**: Raw text -> LLM Layer (8 feature ratings) -> Feature Vector (8+1 dimensions) -> Random Forest (300 trees) -> Binary classification output

- **Critical path**: Prompt Engineering (designing structured JSON output prompts) -> Rating Aggregation (converting word-based outputs to integers) -> Training (fitting Random Forest on feature vectors)

- **Design tradeoffs**: Interpretability vs. Accuracy (using LLMs as feature extractors sacrifices raw predictive power but gains interpretability), Cost (running 8 feature queries per comment via large LLMs is expensive compared to simple classifiers)

- **Failure signatures**: Rating Bias (LLaMA3 showed "generous" ratings vs. Gemma2's stricter approach), Domain Shift (struggles with organic social media where persuasive intent isn't explicitly signaled)

- **First 3 experiments**:
  1. Prompt Stability Test: Run same 100 comments through LLM extractor 5 times to check variance in "epistemic emotion" ratings
  2. Ablation Study: Retrain Random Forest removing top 2 features to quantify performance drop
  3. Cross-Domain Validation: Test trained model on different dataset (e.g., debates from different subreddit) to verify generalization

## Open Questions the Paper Calls Out

- **Generalizability**: Does the hybrid model maintain predictive accuracy when applied to online environments where persuasive intent is implicit rather than explicit? The authors note the Change My View subreddit limits generalizability and suggest testing on datasets with less explicit persuasive intent.

- **Human alignment**: How do LLM-generated feature ratings align with human annotations for the same psychological constructs? The paper explicitly notes the lack of validation against human-sourced ratings as a limitation.

- **Emotional arousal**: Does incorporating emotional arousal (in addition to valence) significantly improve classification of persuasive messages? The Discussion notes that current models capture valence but overlook arousal, which plays a crucial role in persuasion.

## Limitations

- Limited generalizability to organic online environments where persuasive intent is implicit rather than explicit
- No direct validation comparing LLM-generated feature ratings against human annotations
- High computational cost of running multiple LLM queries per comment for feature extraction
- Potential domain-specificity to the Change My View subreddit context

## Confidence

- **High confidence**: Experimental methodology is sound with appropriate baselines and validation procedures
- **Medium confidence**: Interpretation that epistemic emotions are strongest predictors of persuasion
- **Medium confidence**: Generalizability of results beyond Change My View subreddit

## Next Checks

1. Conduct human validation study comparing LLM-generated feature ratings against human annotators' assessments of the same 8 psychological constructs
2. Apply trained model to persuasive/non-persuasive comment pairs from different online forums to measure performance degradation
3. Systematically remove individual features from model to quantify independent contribution to accuracy, particularly testing whether "Interesting-If-True" alone drives most performance gain