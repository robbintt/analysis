---
ver: rpa2
title: 'Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document
  Retrieval'
arxiv_id: '2511.21121'
source_url: https://arxiv.org/abs/2511.21121
tags:
- page
- retrieval
- visionrag
- colpali
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VisionRAG addresses document retrieval limitations in OCR-based\
  \ and patch-based vision retrieval systems by introducing a three-pass pyramid indexing\
  \ framework that processes documents directly as images. The system extracts semantic\
  \ artifacts at multiple granularities\u2014page summaries, section headers, facts,\
  \ and visual hotspots\u2014and combines them through reciprocal rank fusion, enabling\
  \ efficient retrieval without dense patch embeddings or OCR dependencies."
---

# Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval

## Quick Facts
- arXiv ID: 2511.21121
- Source URL: https://arxiv.org/abs/2511.21121
- Reference count: 30
- Primary result: 0.8051 accuracy@10 on FinanceBench and 0.9629 recall@100 on TAT-DQA while storing only 17-27 vectors per page

## Executive Summary
VisionRAG introduces a three-pass pyramid indexing framework that processes document pages as images, extracting semantic artifacts at multiple granularities—page summaries, section headers, facts, and visual hotspots—and combining them through reciprocal rank fusion. This approach achieves state-of-the-art retrieval accuracy while using only 17-27 vectors per page compared to ~1,024 vectors for patch-based methods, eliminating OCR dependencies and dense patch embeddings. On financial document benchmarks, VisionRAG demonstrates superior performance with 0.8051 accuracy@10 on FinanceBench and 0.9629 recall@100 on TAT-DQA.

## Method Summary
VisionRAG processes document pages as images using a Vision-Language Model (VLM) to extract four artifact types: page summaries (global context), section headers (structure), facts (atomic claims), and visual hotspots (salient regions). These artifacts are embedded separately and indexed in four distinct collections. At query time, three query variants (original, keywords, synonyms) are generated and searched across all indices, with results fused via reciprocal rank fusion. The top-ranked pages are passed as base64 images to a VLM for final answer generation, achieving efficient retrieval without OCR preprocessing or dense patch embeddings.

## Key Results
- Achieves 0.8051 accuracy@10 on FinanceBench, outperforming traditional OCR-based and patch-based vision retrieval systems
- Demonstrates 0.9629 recall@100 on TAT-DQA while storing only 17-27 vectors per page versus ~1,024 for patch methods
- Ablation studies show facts provide largest individual gain (+6.3 points accuracy), with full pyramid outperforming any subset

## Why This Works (Mechanism)

### Mechanism 1: Multi-Granularity Pyramid Indexing
Decomposing page images into semantic artifacts at different granularities improves retrieval coverage over single-vector representations. A VLM extracts four artifact types per page—summaries, section headers, facts, and visual hotspots—each indexed separately. Different query types align with different artifact granularities (entity queries match facts; exploratory queries match summaries). Core assumption: VLMs can reliably extract accurate semantic artifacts without OCR preprocessing.

### Mechanism 2: Reciprocal Rank Fusion for Heterogeneous Signals
RRF robustly combines rankings from multiple indices without requiring score calibration. For each index and query variant pair, retrieve top-K candidates; aggregate via RRF formula (α=60, uniform weights). Pages ranking consistently well across indices receive higher fused scores; noise from any single index is naturally suppressed. Core assumption: Relevant pages receive moderate-to-strong ranks across multiple indices.

### Mechanism 3: Query Expansion via Keywords and Synonyms
Generating multiple query variants improves semantic coverage without dense patch matching. Three variants—original, keyword extraction, synonym paraphrase—are run against all indices; results fused via RRF. Keywords target salient lexical tokens; synonyms address vocabulary mismatch. Core assumption: Query variants capture complementary aspects of intent without excessive noise.

## Foundational Learning

- **Reciprocal Rank Fusion (RRF)**
  - Why needed: Core fusion mechanism combining 4 indices × 3 query variants = 12 ranked lists
  - Quick check: Given ranks r₁=3, r₂=15 for a page across two indices with α=60, what is its RRF contribution? (Answer: 1/63 + 1/75 ≈ 0.030)

- **Late Interaction Models (ColPali/ColBERT)**
  - Why needed: VisionRAG positions itself as an explicit-fusion alternative to MaxSim-based patch matching
  - Quick check: Why does ColPali require ~1,024 vectors per page? (Answer: 32×32 patch grid)

- **Vision-Language Models for Document Understanding**
  - Why needed: Extraction quality of summaries/facts/hotspots directly determines indexing quality
  - Quick check: What artifact types does VisionRAG extract from each page image? (Answer: summaries, section headers, facts, visual hotspots)

## Architecture Onboarding

- Component map: Page Image → VLM Extraction → [Summary | Sections | Facts | Hotspots] → Text Embeddings → 4 Separate Indices → Query → [Original | Keywords | Synonyms] → Embed → Search All Indices → RRF Fusion → Top-K Pages → VLM → Answer

- Critical path: VLM extraction latency (1-3s/page with GPT-4o) dominates indexing; query-time RRF fusion is ~2ms and not a bottleneck

- Design tradeoffs:
  - Higher embedding dimensions (3,072 vs 1,536): +0.04-0.06 recall but 2× storage
  - Local vs API embeddings: Local gives 10-14ms query latency; API adds 100-180ms network overhead
  - GPU (ColPali) vs CPU (VisionRAG): GPU faster for indexing (108h vs 402h for 1M pages) but requires 85.8GB storage vs 7.2GB

- Failure signatures:
  - Low Recall@K but high accuracy with oracle pages → retrieval bottleneck
  - High Recall@100 but low accuracy → generation/reasoning failure
  - Inconsistent results across VLM backends → extraction quality variance

- First 3 experiments:
  1. Single-index baseline: Run retrieval using only page summary index
  2. Index ablation: Add one index at a time to measure marginal contribution
  3. Query variant ablation: Compare original-only vs. original+keywords vs. full expansion

## Open Questions the Paper Calls Out

### Open Question 1
Can learned fusion weights and retrieval parameters via meta-optimization outperform uniform RRF weights across diverse document domains? Paper explicitly states this as a future direction but has not studied optimal weighting variation by document type or query class.

### Open Question 2
How does retrieval accuracy degrade when scaling from tens of thousands to millions or billions of pages without hierarchical or distributed retrieval extensions? Experiments are limited to FinanceBench (~1,870 pages) and TAT-DQA (~40,000 pages); billion-page scaling requires additional engineering.

### Open Question 3
Can external reasoning tools (calculators, table parsers, program synthesis) close the performance gap on multi-step numerical and aggregation queries? VisionRAG's modular design enables tool integration but no experiments quantify potential accuracy gains on arithmetic/aggregation questions.

## Limitations
- VLM extraction quality may degrade on dense tables, unusual layouts, or degraded document quality, impacting artifact quality and retrieval performance
- Query expansion may introduce noise for precise technical queries, potentially diluting intent
- Paper does not validate index independence or measure redundancy between summary, section, fact, and hotspot signals

## Confidence
- High confidence: Storage efficiency claims (17-27 vectors/page), retrieval accuracy on FinanceBench/TAT-DQA, and RRF implementation details
- Medium confidence: Multi-granularity pyramid indexing benefits (assumes VLM extraction reliability), query expansion effectiveness, and generalization beyond financial documents
- Low confidence: VLM extraction reliability across document types, optimal index weighting for RRF fusion, and scalability to documents with vastly different structures

## Next Checks
1. **VLM extraction validation**: Audit extracted artifacts on 50-100 pages from diverse document types to measure accuracy, hallucination rates, and consistency against OCR-based baselines
2. **Index independence measurement**: Calculate pairwise correlation coefficients between retrieval rankings from all four indices to identify redundancy and consolidation opportunities
3. **Query expansion ablation**: Systematically test query expansion variants on diverse query types to measure precision-recall tradeoffs and identify when expansion helps versus harms