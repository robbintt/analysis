---
ver: rpa2
title: 'Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised
  Multi-Dimensional Scaling'
arxiv_id: '2510.01025'
source_url: https://arxiv.org/abs/2510.01025
tags:
- linear
- circular
- semicircular
- cluster
- manifolds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Supervised Multi-Dimensional Scaling (SMDS) is a novel model-agnostic
  method for automatically discovering feature manifolds in language models. It extends
  classical MDS by incorporating supervision via labels to parametrize the underlying
  structure, enabling quantitative comparison of different geometric hypotheses.
---

# Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling

## Quick Facts
- arXiv ID: 2510.01025
- Source URL: https://arxiv.org/abs/2510.01025
- Authors: Federico Tiblias; Irina Bigoulaeva; Jingcheng Niu; Simone Balloccu; Iryna Gurevych
- Reference count: 40
- One-line primary result: SMDS automatically discovers interpretable geometric manifolds (circles, lines, clusters) in LLM activations that actively support reasoning.

## Executive Summary
Supervised Multi-Dimensional Scaling (SMDS) extends classical MDS by incorporating supervision via labels to parametrize the underlying structure, enabling quantitative comparison of different geometric hypotheses. Applied to temporal reasoning tasks, SMDS reveals that temporal entities form interpretable manifolds whose shapes match the semantics of the features. These structures are stable across model families and scales, dynamically adapt to task context, and actively support reasoning—perturbing aligned subspaces significantly degrades performance, unlike random subspaces.

## Method Summary
SMDS discovers feature manifolds by minimizing the discrepancy between distances in a projected subspace and "ideal" distances derived from supervised labels. The method extracts hidden states at specific token positions, computes ideal pairwise label distances based on geometric hypotheses, solves for a linear projection minimizing stress, and evaluates via cross-validation. Distance functions encode manifold hypotheses (linear, circular, cluster), and the method uses ridge regression to find the optimal projection. Stress-based scoring identifies the best-fitting manifold shape, with lower stress indicating better fit.

## Key Results
- Temporal entities (dates, times) form interpretable manifolds (circles, lines, clusters) whose shapes match feature semantics
- Manifold structures are stable across different model families and scales
- Dynamically adapt to task context - same base representation reshaped for different reasoning operations
- Actively support reasoning - perturbing manifold-aligned subspaces degrades performance more than random perturbations
- Stress-based scoring reliably identifies optimal manifold shapes, confirmed by cross-validation

## Why This Works (Mechanism)

### Mechanism 1: Supervised Geometric Projection
SMDS extends classical MDS by minimizing the discrepancy between distances in the projected subspace and "ideal" distances derived from supervised labels. This effectively solves a regression problem from activations to a parametrized manifold structure, assuming the feature manifold is linear or can be approximated by a low-dimensional linear subspace.

### Mechanism 2: Functional Role of Geometry in Reasoning
Feature manifolds appear to be active computational substrates; disrupting their geometry impairs reasoning more than disrupting random directions. The model encodes entities onto a manifold and transforms these positions to reason, suggesting geometric relationships between entity representations are essential for computation.

### Mechanism 3: Task-Dependent Manifold Reshaping
LLMs dynamically restructure feature manifolds based on the specific reasoning operation required by the prompt. The model maintains a base representation but transforms it into a task-specific geometry at later layers or token positions, aligning the representation space with the output format.

## Foundational Learning

- **Multi-Dimensional Scaling (MDS)**: Why needed: SMDS is the core contribution; understanding how MDS preserves pairwise distances is essential for grasping the stress metric. Quick check: If two points are close in the original high-dimensional space, where should they be in the SMDS projection?
- **Linear Representation Hypothesis**: Why needed: The paper extends this hypothesis by showing that related concepts form structures (manifolds), not just single directions. Quick check: Does the paper claim features are single directions, or structured collections of directions?
- **Stress (Goodness-of-fit)**: Why needed: This quantitative metric compares different geometric hypotheses and determines which "shape" the model actually uses. Quick check: If a "circular" hypothesis has lower stress than a "linear" hypothesis for days of the week, which geometry is the model likely using?

## Architecture Onboarding

- **Component map**: Input prompts -> Hidden state extraction (TE, LP, A positions) -> Distance function computation -> SMDS solver -> Stress evaluation
- **Critical path**: The definition of the distance function d(yᵢ, yⱼ) implicitly defines the hypothesis geometry. If this function doesn't match the semantic reality of the feature, the method fails to find a low-stress projection.
- **Design tradeoffs**: Linearity uses a linear projection W (interpretable but may miss non-linear manifolds); fixed m=3 dimensions standardizes comparison but may lose detail.
- **Failure signatures**: High stress indicates wrong hypothesis or layer; no transfer from TE to LP/A suggests information loss; control failure suggests overfitting.
- **First 3 experiments**:
  1. Apply SMDS to date task, verify circular structure with low stress at month token
  2. Compare stress values for linear vs. circular distance functions on date dataset
  3. Apply noise to recovered subspace W and confirm accuracy drops relative to random subspace intervention

## Open Questions the Paper Calls Out

- How do individual features combine to form complex multidimensional manifolds? The study focused on monodimensional features with only preliminary 2D extensions, leaving general feature interaction undefined.
- Do models maintain multiple valid geometric representations for a single concept rather than a single preferential manifold? Tightly clustered stress scores suggest models may dynamically select between equally valid geometries based on task context.
- How does the fuzziness or vagueness of temporal expressions impact the quality and structure of feature manifolds? The method relied on precise temporal pointers, excluding fuzzy expressions that cannot be mapped to exact numerical values.

## Limitations

- Linearity assumption may not capture genuinely non-linear manifolds
- Method assumes features are sufficiently separated to avoid interference
- Causal interpretation of geometry as computational substrate rests on suggestive but not definitive intervention evidence
- Generality across all LLM features not established beyond temporal reasoning domain

## Confidence

- **High Confidence**: SMDS discovery mechanism works for well-behaved features; stress scoring reliable; cross-validation confirms no overfitting; intervention effects are real
- **Medium Confidence**: Interpretation of geometry as computational substrate - effects exceed random noise but alternative explanations not fully ruled out
- **Low Confidence**: Generality across all LLM features - demonstrated success on temporal reasoning but not established for other feature types

## Next Checks

1. Implement and apply a non-linear variant of SMDS to quantify the cost of linearity and compare recovered structures
2. Apply SMDS to synthetic datasets with known random distributions to verify method correctly identifies absence of structure
3. Apply SMDS to at least two non-temporal feature types from the corpus to test generalization beyond temporal reasoning