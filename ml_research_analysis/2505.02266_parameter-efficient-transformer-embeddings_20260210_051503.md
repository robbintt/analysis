---
ver: rpa2
title: Parameter-Efficient Transformer Embeddings
arxiv_id: '2505.02266'
source_url: https://arxiv.org/abs/2505.02266
tags:
- embedding
- token
- fourier
- embeddings
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a parameter-efficient transformer embedding
  method that replaces traditional large embedding tables with a deterministic Fourier
  expansion of normalized token IDs, followed by a lightweight multilayer perceptron
  (MLP) to capture higher-order interactions. The approach leverages the statistical
  structure in Byte-Pair Encoding token IDs, mapping them to continuous values and
  expanding them using Fourier basis functions.
---

# Parameter-Efficient Transformer Embeddings

## Quick Facts
- arXiv ID: 2505.02266
- Source URL: https://arxiv.org/abs/2505.02266
- Authors: Henry Ndubuaku; Mouad Talhi
- Reference count: 21
- Primary result: Replaces learned embedding tables with deterministic Fourier expansion, reducing parameters from 8.9M to 1.1M while maintaining performance on NLI and STS tasks

## Executive Summary
This paper introduces PETE (Parameter-Efficient Transformer Embeddings), a method that replaces traditional learned embedding tables with a deterministic Fourier expansion of normalized token IDs. The approach leverages the frequency-based structure in BPE token IDs, mapping them to continuous values in [-1, 1] and expanding them using Fourier basis functions. A lightweight MLP with residual connection refines these deterministic features for task-specific adaptation. The method achieves competitive performance on natural language inference and sentence similarity tasks while significantly reducing model parameters and training time, demonstrating the potential for scalable, memory-efficient language models.

## Method Summary
PETE computes token embeddings by first normalizing BPE token IDs p to x ∈ [-1, 1] via x = 2p/(V-1) - 1. The normalized values are then expanded using a Fourier basis: T_i(p) = sin((⌊i/2⌋+1)πx) for even i, cos((⌊i/2⌋+1)πx) for odd i. A lightweight MLP with GeGLU activation and residual connection processes these fixed Fourier features: E(p) = MLP(T(p)) + T(p). The method trains using contrastive loss (CLIP/InfoNCE-style) on entailment pairs from SNLI/MNLI, with sentence representations obtained via average pooling. Experiments use BERT tokenizer (V=30,522), batch size 128, and mixed-precision training on a single RTX 4090.

## Key Results
- Parameter reduction: 8.9M → 1.1M for basic transformer configuration
- Competitive performance on SNLI/MNLI natural language inference tasks
- Strong zero-shot and fine-tuned performance on STS-B sentence similarity (Spearman correlation ≥76.0)
- Faster training convergence compared to standard learned embeddings
- No dropout required for stable training

## Why This Works (Mechanism)

### Mechanism 1
BPE tokenization assigns token IDs in frequency order (frequent tokens = lower IDs, rare tokens = higher IDs). Normalizing token ID p to x ∈ [-1, 1] preserves relative differences while enabling smooth transformations. This maps discrete token space to a continuous, scale-invariant domain. The core assumption is that frequency-based ordering contains structure correlating with semantic properties.

### Mechanism 2
A fixed Fourier basis expansion provides sufficient representational capacity to approximate smooth embedding functions on [-1, 1]. Lower-order terms capture global variations across ID space; higher-order terms encode finer distinctions. The orthogonal basis reduces initial feature correlation. The target embedding function must be sufficiently smooth to be well-approximated by truncated Fourier series.

### Mechanism 3
A lightweight MLP with residual connection learns to refine deterministic Fourier features, correcting for cases where base representations are too similar or semantically misaligned. The MLP learns nonlinear transformations that adjust the fixed basis representation. The residual connection frames learning as small corrections rather than full reconstruction.

## Foundational Learning

- **Fourier Basis Functions (Sinusoidal Positional Encodings)**: Extends the intuition behind sinusoidal positional encodings to token identity itself, treating token ID as a continuous signal. Quick check: Can you explain why sine/cosine functions at different frequencies form an orthogonal basis for representing bounded signals?

- **Residual Connections and Gradient Flow**: The design E(p) = MLP(T(p)) + T(p) relies on residual connections to stabilize training. Quick check: If the residual connection were removed (E(p) = MLP(T(p)) only), what would happen to gradient flow from downstream layers back to early training stages?

- **Contrastive Learning Objectives (InfoNCE/CLIP-style)**: Trains using contrastive loss on entailment pairs, not traditional language modeling. Quick check: How does the temperature parameter in contrastive loss affect the sharpness of the learned embedding distribution?

## Architecture Onboarding

- **Component map**: Tokenizer → Normalization → Fourier Expansion → MLP → Residual Addition → Transformer Attention
- **Critical path**: Token ID → Normalization → Fourier Expansion → MLP → Residual Addition → Transformer Attention. Errors in Fourier kernel or normalization propagate through all downstream processing.
- **Design tradeoffs**: MLP capacity vs. parameter savings (larger MLP reduces advantage); Fourier dimension vs. granularity (higher d provides finer resolution but increases compute); dropout omission (no dropout reported, but may not generalize).
- **Failure signatures**: Near-collision collapse (loss plateaus with high similarity between semantically distinct tokens); training instability with dropout (disrupts smooth progression); slow convergence at scale (adjacent tokens have nearly identical features).
- **First 3 experiments**: 1) Baseline parity check: Train 2-layer, 256d Fourier-embedding transformer on SNLI with identical hyperparameters to baseline; compare convergence and STS-B zero-shot Spearman correlation. 2) MLP ablation: Replace position-wise FFN after Fourier expansion with single linear projection; measure performance delta. 3) Tokenizer sensitivity test: Swap BERT tokenizer for non-frequency-ordered tokenizer; if performance drops significantly, confirms BPE-frequency-structure hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
Can PETE embeddings maintain competitive performance when scaled to large language models trained on web-scale corpora? Experiments were limited to small-scale models due to resource constraints, leaving large-scale validation incomplete.

### Open Question 2
How effectively do Fourier-based embeddings handle token-level tasks requiring fine-grained lexical distinctions (NER, POS-tagging)? Current evaluation focused only on sentence-level tasks; token-level tasks may require different embedding properties.

### Open Question 3
Can the MLP component adequately separate near-colliding Fourier representations for vocabularies exceeding 100k tokens? As the normalized token ID space becomes densely populated, initial Fourier representations of distinct tokens could become very close.

### Open Question 4
How well does the deterministic Fourier expansion capture phenomena like polysemy, homonymy, and morphological variation? Unlike learned embeddings which can assign context-independent representations, deterministic ID-based mappings may conflate semantically distinct tokens with similar IDs.

## Limitations

- Critical structural assumption: Method depends on BPE token IDs being assigned in frequency order; performance may degrade substantially with non-frequency-ordered tokenizers.
- Vocabulary scaling vulnerability: For extremely large vocabularies (>100k), adjacent tokens have nearly identical Fourier representations that the MLP may lack capacity to disambiguate.
- Dropout omission generalizability: Claim that no dropout is needed may not extend beyond tested configurations; no ablation studies provided.

## Confidence

**High confidence**: Parameter reduction claim (8.9M → 1.1M) is straightforward arithmetic and directly verifiable from architecture specifications.

**Medium confidence**: Performance parity claim is supported by experimental results but depends on specific task configurations and evaluation protocols; no standard deviations or statistical significance tests reported.

**Low confidence**: Assertion that BPE frequency ordering contains exploitable semantic structure is presented as an "empirical hypothesis" without direct validation; paper doesn't test alternative tokenizers.

## Next Checks

1. **Tokenizer sensitivity validation**: Systematically test the method with different tokenizers (BERT BPE, random ID assignment, Unigram) to verify that performance degradation occurs when frequency-based structure is removed.

2. **Vocabulary scaling stress test**: Evaluate the method on progressively larger vocabularies (V = 10k, 30k, 100k, 500k) to measure performance degradation as adjacent token similarity increases, and test whether increasing MLP capacity can compensate.

3. **Fourier embedding variance analysis**: For a fixed vocabulary, compute pairwise cosine similarity between all token embeddings and analyze the distribution to verify meaningful differentiation before MLP refinement.