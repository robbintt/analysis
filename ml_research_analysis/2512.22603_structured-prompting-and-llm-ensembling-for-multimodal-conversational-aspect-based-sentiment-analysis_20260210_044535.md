---
ver: rpa2
title: Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based
  Sentiment Analysis
arxiv_id: '2512.22603'
source_url: https://arxiv.org/abs/2512.22603
tags:
- sentiment
- multimodal
- extraction
- sextuple
- aspect-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles multimodal conversational aspect-based sentiment
  analysis, focusing on extracting sentiment sextuples and detecting sentiment flips
  in multi-party dialogues with multimodal content. The proposed approach uses structured
  prompting and LLM ensembling: a multi-sampling generation and refinement method
  for robust target-aspect extraction, a hybrid LLM optimization strategy to refine
  opinions, sentiments, and rationales, and an ensemble of three LLMs for sentiment
  flipping analysis.'
---

# Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis

## Quick Facts
- arXiv ID: 2512.22603
- Source URL: https://arxiv.org/abs/2512.22603
- Reference count: 40
- Third place in MCABSA Challenge with 47.38% average sextuple F1 and 74.12% sentiment flipping F1

## Executive Summary
This paper tackles multimodal conversational aspect-based sentiment analysis, focusing on extracting sentiment sextuples and detecting sentiment flips in multi-party dialogues with multimodal content. The proposed approach uses structured prompting and LLM ensembling: a multi-sampling generation and refinement method for robust target-aspect extraction, a hybrid LLM optimization strategy to refine opinions, sentiments, and rationales, and an ensemble of three LLMs for sentiment flipping analysis. The system achieved a 47.38% average score on sextuple extraction and a 74.12% exact match F1 on sentiment flipping detection, ranking third in the MCABSA Challenge. The results demonstrate the effectiveness of step-wise refinement and ensemble methods in handling complex multimodal sentiment tasks.

## Method Summary
The system processes multimodal dialogues through a three-stage pipeline. First, InternVL-3-14B and Qwen2-Audio convert visual and audio content into text captions. Second, a fine-tuned Qwen3-8B model with SWIFT+LoRA (rank=8, alpha=32, lr=1e-5) extracts target-aspect pairs using Multi-Sampling Generation and Refinement (MSGR) with high-temperature sampling (T=1.0) followed by GPT-4.1-mini adjudication. Third, a Hybrid LLM Optimization Strategy (HLOS) separates extraction (Target, Aspect, Holder, Opinion via fine-tuned Qwen) from reasoning refinement (Sentiment, Rationale via GPT-4.1-mini). For sentiment flipping detection, three models (Gemini-2.5-pro → GPT-4.1 → Qwen3-8B) are queried hierarchically with fallback fusion to ensure output completion.

## Key Results
- Achieved 47.38% average sextuple F1 score on target-aspect extraction and identification
- Reached 74.12% exact match F1 for sentiment flipping detection
- Ranked third in the MCABSA Challenge among participating teams
- HLOS refinement provided a 7.98% performance gain over baseline extraction-only approaches

## Why This Works (Mechanism)

### Mechanism 1
Multi-sampling with high-temperature generation followed by LLM-based adjudication likely improves the consistency of Target-Aspect (T-A) extraction compared to single-pass inference. The method uses a fine-tuned model (Qwen3-8B) with high temperature (T=1.0) to generate multiple candidate lists of T-A pairs. It first determines the consensus length of the list and then uses a more powerful LLM (GPT-4.1-mini) to select the best candidate for each specific index, reducing stochastic noise and hallucinations.

### Mechanism 2
A hybrid pipeline using specialized models for distinct sub-tasks (extraction vs. reasoning) appears to outperform monolithic generation for complex sextuples. The Hybrid LLM Optimization Strategy (HLOS) separates the task: a fine-tuned model handles pattern-heavy extraction (Target, Aspect, Holder, Opinion), while a general-purpose LLM (GPT-4.1-mini) refines semantic and logical components (Sentiment, Rationale). This leverages the fine-tuned model's pattern recognition and the LLM's reasoning capability.

### Mechanism 3
Hierarchical ensembling provides robustness against model silence (empty outputs) in sentiment flipping detection. For Subtask-II, the system queries three models. Instead of simple voting, it uses a priority fallback: Gemini (primary) -> GPT-4.1 (secondary) -> Qwen (tertiary). This specifically addresses the observation that top models occasionally fail to generate output, ensuring a result is always produced.

## Foundational Learning

**Aspect-Based Sentiment Analysis (ABSA)**
Why needed: This is the core task. Unlike simple sentiment analysis (positive/negative), this requires extracting structured tuples (Target + Aspect + Sentiment) from text.
Quick check: If a user says "The screen is great but the battery is weak," can you identify the two targets and their opposing sentiments?

**Multimodal Captioning**
Why needed: The system processes video/audio but feeds text to the main LLM. You must understand that the "vision" part is actually a text-description generation step first.
Quick check: How might converting a sarcastic tone in an audio file to text-only captions lose critical sentiment information?

**Error Propagation in Pipelines**
Why needed: The architecture is a pipeline (A -> B -> C). The paper explicitly struggles with errors in step A affecting step B.
Quick check: If the system identifies the wrong "Target" in step 1, is it possible for the "Rationale" in step 3 to be correct? (Hint: Likely no).

## Architecture Onboarding

**Component map:**
- Input: Multimodal Dialogue (Text + Audio + Video)
- Preprocessing: `InternVL-3` / `Qwen2-Audio` (convert media to text captions)
- Extraction Core: `SFT-Qwen3-8B` (Extracts Target, Aspect, Holder, Opinion)
- Refinement: `GPT-4.1-mini` (Refines Sentiment & Rationale via HLOS)
- Flipping Ensemble: `Gemini` + `GPT-4.1` + `Qwen3` (Hierarchical voting for Subtask-II)

**Critical path:** The **Target-Aspect (T-A) extraction** via MSGR. If this length/count is wrong, the subsequent Holder-Opinion and Rationale steps will operate on incorrect or misaligned data.

**Design tradeoffs:**
- Cost vs. Accuracy: Using GPT-4.1-mini for refinement improves accuracy but increases latency and API cost compared to a pure Qwen solution
- Stability vs. Diversity: The MSGR uses high temperature (T=1.0) to force diversity, requiring a subsequent "Refinement" step to filter noise, rather than using low temperature for direct deterministic answers

**Failure signatures:**
- Empty Flips: The ensemble returns nothing (check API failures or overly strict prompts)
- Misaligned Sextuples: The Rationale describes the wrong Target (indicates early-stage extraction error)
- Length Mismatch: MSGR fails to find a consensus length for T-A lists

**First 3 experiments:**
1. Validate MSGR: Run the T-A extractor with T=1.0 on a small validation set; verify if the "consensus length" logic matches ground truth counts
2. HLOS Ablation: Bypass the GPT-4.1 refinement step and check if "Opinion" and "Rationale" quality degrades significantly (validating the HLOS utility)
3. Ensemble Logic: Force all three Subtask-II models to run and compare results. Check if the hierarchical assumption (Gemini > GPT > Qwen) holds or if the smaller model ever corrects the larger one

## Open Questions the Paper Calls Out

**Open Question 1**
Can end-to-end models jointly optimizing all six sentiment elements outperform the hierarchical pipeline while eliminating error propagation? The current three-stage pipeline introduces cascading inaccuracies; early-stage errors in T-A extraction propagate to downstream elements despite modular refinements.

**Open Question 2**
Would incorporating graph-based speaker relationship representations improve trigger classification accuracy in sentiment flipping detection? Current Subtask-II approach uses independent model ensembling without explicit modeling of inter-speaker dynamics, potentially missing relational cues that signal sentiment shifts.

**Open Question 3**
Can the framework maintain performance when replacing proprietary refinement models (GPT-4.1-mini) with open-source alternatives? HLOS provides the largest single performance uplift (7.98% gain), but relies on proprietary APIs; cost-effective alternatives are untested.

## Limitations
- Heavy reliance on undisclosed prompt templates and unspecified MSGR hyperparameters limits reproducibility
- Complex multi-LLM pipeline creates significant cost and latency overhead without clear cost-efficiency analysis
- Error propagation vulnerability in the pipeline architecture remains unmitigated despite acknowledged risks
- Multimodal preprocessing to text captions may lose nuanced sentiment cues like sarcasm or tone

## Confidence

**High Confidence**: The hierarchical ensembling mechanism for handling empty outputs and the HLOS separation of extraction from reasoning tasks are well-supported by documented results.

**Medium Confidence**: The MSGR approach for T-A extraction shows promise but lacks detailed hyperparameter justification and quantitative ablation studies.

**Low Confidence**: Claims about cost-efficiency and real-world scalability are unsupported, given the complex multi-LLM architecture.

## Next Checks

1. **Ablation Study on MSGR**: Conduct controlled experiments varying the number of samples (m) and temperature (T) in MSGR to quantify the trade-off between recall and precision against a deterministic baseline.

2. **Error Propagation Analysis**: Log per-component accuracy across the pipeline (T-A, H-O, S-R) on validation data and calculate correlation between early-stage errors and downstream failures to quantify error accumulation.

3. **Cost-Performance Benchmarking**: Measure average inference cost and latency of the full pipeline on test data subset and compare metrics against a simpler baseline to assess practical viability.