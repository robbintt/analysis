---
ver: rpa2
title: 'On the usability of generative AI: Human generative AI'
arxiv_id: '2502.17714'
source_url: https://arxiv.org/abs/2502.17714
tags:
- user
- interac-on
- systems
- system
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies usability challenges in generative AI systems,
  particularly issues with user control, unpredictability, and difficulties in fine-tuning
  outputs through traditional prompt-based interactions. The authors propose a human-centered
  AI interface design that incorporates both voice and prompt-based interaction modes,
  with additional features like voice assistance, a control panel for output parameters
  (style, tone, length), and prompt-writing assistance using established frameworks.
---

# On the usability of generative AI: Human generative AI

## Quick Facts
- arXiv ID: 2502.17714
- Source URL: https://arxiv.org/abs/2502.17714
- Reference count: 0
- Human-centered AI interface design improves control and transparency through parameter controls, memory management, and prompt assistance

## Executive Summary
This paper identifies key usability challenges in generative AI systems, including user control difficulties, output unpredictability, and complex prompt-based interactions. The authors propose H-gAI, a human-centered interface design that combines voice and prompt-based interactions with features like a control panel for output parameters, voice assistance, and prompt-writing assistance using established frameworks. Usability testing with 5 participants showed that while users quickly adapted to basic tasks (completion times improved from 52 to 30.2 seconds), complex search-based tasks had an 80% failure rate, highlighting the need to balance additional functionality with intuitive design.

## Method Summary
The study employed a classical usability test with 5 participants using a Figma-based interactive prototype of the H-gAI interface. Participants performed 4 specific tasks: changing response style, adjusting voice playback speed, regenerating responses, and searching for delete history functionality. The testing used a within-subjects design with Think Aloud protocol, recording task completion scores (1-3 scale), completion times, and qualitative feedback across three phases (introduction, task execution, feedback).

## Key Results
- Users quickly adapted to the interface, with task completion times improving from 52 to 30.2 seconds across tasks
- Basic parameter adjustment tasks showed high success rates, while complex search-based tasks had 80% failure rate
- Participants appreciated the balance between familiar chat interface elements and new control features, though struggled to locate specific functions
- The control panel for output parameters was discovered but required explicit guidance for effective use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GUI-style parameter controls reduce cognitive burden of expressing intent through natural language prompts
- Mechanism: Pre-configurable output parameters (style, tone, length, language) → users specify intent via structured selectors → more predictable AI outputs → fewer iterative corrections
- Core assumption: Users identify desired output characteristics more easily from discrete options than articulate them as prose
- Evidence anchors: Control panel design with real dashboard for parameter control; COSTAR framework addresses intent misalignment through structured interactions
- Break condition: If parameter adjustments produce no perceptible output change or mental model misalignment

### Mechanism 2
- Claim: Explicit conversational memory increases trust by making retained context transparent and editable
- Mechanism: Visible memory state with toggle/edit capability → users verify AI knowledge → selective retention → reduced redundant context-setting → sustained trust
- Core assumption: Users prefer deliberate control over context persistence rather than opaque memory
- Evidence anchors: Memory toggle feature with default disabled state; user can check and modify stored elements
- Break condition: If memory management adds cognitive overhead exceeding utility or users forget their settings

### Mechanism 3
- Claim: Integrated prompt-writing assistance accelerates learning and improves output quality for non-experts
- Mechanism: Embedded frameworks (COSTAR, CARE, Chain of Thoughts, Few Shots) with step-by-step guidance → structured prompt construction → more complete intent articulation → higher-quality outputs and skill transfer
- Core assumption: Non-experts benefit more from guided frameworks than trial-and-error discovery
- Evidence anchors: Prompt assistance feature following user step-by-step; LACE paper shows controlled prompting improves workflow integration
- Break condition: If frameworks conflict with natural expression style or guidance becomes prescriptive

## Foundational Learning

- **Concept: Intent-based vs. Command-based Interaction**
  - Why needed here: Generative AI shifted interaction from command-based (specify steps) to intent-based (specify outcomes). Understanding this inversion is prerequisite to designing controls that restore user agency
  - Quick check question: Can you explain why adding GUI controls to an intent-based system is not a regression to command-based interaction?

- **Concept: Prompt as "Tip of the Iceberg"**
  - Why needed here: Written prompts underrepresent user intent; implicit knowledge and context are submerged. Effective interfaces must help externalize this implicit information
  - Quick check question: What interface elements in H-gAI specifically address the "submerged portion" of user intent?

- **Concept: Trust Calibration in AI Systems**
  - Why needed here: Trust determines adoption and misplaced trust (over-reliance) or lost trust (unexpected outputs) both harm usability. Transparency mechanisms calibrate trust appropriately
  - Quick check question: Why does H-gAI display bias/error warnings prominently at each new conversation rather than once during onboarding?

## Architecture Onboarding

- **Component map:**
  - Main conversation canvas (central, largest area)
  - Left sidebar (collapsible): chat history, project folders, prompt library, memory management, user guide, settings
  - Control panel (right-side overlay, activated on demand): basic output controls (language, style, tone, length), response patterns (roleplay roles), prompt assistance (frameworks)
  - Voice interaction layer: microphone input, speech-to-text, configurable voice output, automatic voice assistance trigger
  - Status/warning layer: generation progress indicator, completion status, per-conversation bias/error warning

- **Critical path:**
  1. On first registration → comprehensive user guide presented (must not be skipped)
  2. Start new conversation → bias warning displayed automatically
  3. Open control panel → adjust at least one parameter before prompting
  4. Use prompt assistance → select framework → complete guided prompt
  5. Review generated output → use regeneration/refinement actions

- **Design tradeoffs:**
  - Familiarity vs. feature discoverability: Maintaining similarity to existing chat interfaces aids recognition but caused users to overlook new features
  - Control density vs. simplicity: Control panel is optional and collapsible, but usability test showed users struggled to locate functions despite presence
  - Warning prominence vs. interaction friction: Prominent bias warnings aid trust calibration but could become ignored through habituation

- **Failure signatures:**
  - Users navigating to settings when controls exist on main screen (observed in usability test)
  - High failure rate on search-based tasks (80% failed Task 4: searching how to delete history)
  - Initial task completion times long (52s average), improving with repetition (30.2s by Task 4)—suggests discoverability issues

- **First 3 experiments:**
  1. **A/B test control panel visibility**: Compare default-expanded vs. default-collapsed control panel on first-time task completion rates and times
  2. **Search function stress test**: Systematically test search queries for all documented functions to identify gaps between user mental models and searchable terms
  3. **Warning habituation longitudinal test**: Run multi-session study to measure whether users stop reading bias warnings over time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does inclusion of parameter-rich control panel reduce cognitive load and improve user control compared to standard prompt-only interactions?
- Basis in paper: Authors propose control panel to address "difficulties in fine-tuning outputs," but results show users "struggled with locating specific functions"
- Why unresolved: Qualitative test with 5 participants insufficient to measure cognitive load or compare efficiency against baseline
- What evidence would resolve it: Comparative quantitative study measuring task completion times, NASA-TLX scores, and success rates between GUI-rich and standard chat interfaces

### Open Question 2
- Question: How does "Prompt Writing Assistance" feature impact output quality and user ability to learn effective prompting strategies?
- Basis in paper: Paper proposes AI-assisted composition using frameworks like COSTAR or CARE, but explicitly states this is proposal not yet evaluated
- Why unresolved: Study only assessed navigation of interface prototype, did not evaluate actual performance of prompting assistance or impact on content quality
- What evidence would resolve it: Experimental study comparing output quality and learning curves of users utilizing assisted prompting versus free-form prompting

### Open Question 3
- Question: How do task completion rates and navigation behaviors change when testing with statistically significant sample size and diverse demographic?
- Basis in paper: Authors state they "carried out classical usability test with 5 users" and noted Task 4 had highest failure rate at 80%
- Why unresolved: With only 5 participants, results regarding intuitiveness of "Projects" folder or "Search" function cannot be generalized
- What evidence would resolve it: Large-scale usability study (n > 20-30) involving users of varying ages and technical proficiencies

## Limitations

- Extremely small sample size (n=5) limits generalizability of findings and prevents statistical validation
- Limited task diversity means results may not reflect real-world usage patterns or edge cases
- Prototype nature means actual AI model integration performance and user experience remain unknown
- No longitudinal study to assess learning outcomes, habit formation, or long-term satisfaction

## Confidence

**High Confidence:**
- Interface successfully integrates familiar chat elements with new control features
- Users quickly adapted to basic task flows with significant time improvements
- Design principle of balancing familiarity with additional functionality addresses documented usability challenges

**Medium Confidence:**
- Mechanisms for improving user control through parameter controls and memory management are theoretically sound but require larger-scale validation
- Prompt assistance frameworks may accelerate learning though study didn't measure actual learning outcomes
- Prominence of bias warnings may improve trust calibration though habituation effects were not measured

**Low Confidence:**
- 80% failure rate on Task 4 represents critical usability flaw requiring immediate investigation
- Claims about improved output predictability through parameter controls cannot be validated without actual AI model outputs
- Long-term user satisfaction and learning outcomes remain unknown from brief usability test

## Next Checks

1. **Search Function Redesign Validation**: Conduct focused study testing multiple search interface designs and terminology indexing to resolve 80% failure rate on search-based tasks

2. **A/B Test Control Panel Visibility**: Compare user performance and satisfaction between default-expanded versus default-collapsed control panels to measure whether proactive exposure improves feature discovery

3. **Longitudinal Trust Calibration Study**: Run multi-session studies tracking user interactions with bias/error warnings over 4-6 weeks to measure habituation and test alternative designs to maintain attention