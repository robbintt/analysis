---
ver: rpa2
title: 'Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable
  Multimodal Emotion Understanding'
arxiv_id: '2512.02699'
source_url: https://arxiv.org/abs/2512.02699
tags:
- reasoning
- emotion
- migr
- audio
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding

## Quick Facts
- **arXiv ID:** 2512.02699
- **Source URL:** https://arxiv.org/abs/2512.02699
- **Reference count:** 40
- **Key outcome:** MIGR achieves 88.95% EPC (explanation-prediction consistency) vs 77.04% for ERV baseline

## Executive Summary
MIGR addresses reasoning drift in multimodal emotion recognition by prioritizing the emotion-dominant modality during generation. The model first estimates which modality (audio or visual) carries stronger emotional signals, then reorders reasoning segments to start with that modality. Combined with GRPO-style reward optimization that encourages modality-aligned reasoning, MIGR achieves state-of-the-art explanation-prediction consistency while maintaining competitive recognition accuracy.

## Method Summary
MIGR extends the HumanOmni architecture with modality importance estimation and reasoning reordering. During training, it compares predictions under audio-only, visual-only, and audio-visual inputs to identify the emotion-dominant modality. Reasoning text is decomposed into modality-specific segments and reordered so the dominant modality's evidence appears first. A two-stage training pipeline applies SFT with modality-aligned data augmentation followed by GRPO optimization with three rewards: modality order, grounding, and answer accuracy.

## Key Results
- 88.95% EPC (explanation-prediction consistency) vs 77.04% for ERV baseline
- 73.93% WAR recognition accuracy (slightly below non-reasoning baselines)
- 7.37% R×/A✓ consistency rate (vs 18.10% for ERV)
- 66.89% Surprise emotion accuracy (degradation from 78.84% for ERV)

## Why This Works (Mechanism)

### Mechanism 1: Modality Importance (MI) Estimation
Identifying which modality carries the strongest emotional signal before reasoning begins reduces early-stage reasoning drift. The model compares prediction accuracy under three conditions—audio-only, visual-only, and audio-visual input. If the model predicts correctly under audio-only but fails under visual-only, audio is designated as emotion-dominant. This pre-inference diagnosis guides subsequent reasoning order.

### Mechanism 2: Reasoning Sequence Reordering via MI
Structured reasoning that begins with emotion-dominant modality descriptions produces more coherent explanations. Training data reasoning is decomposed into `<aud_desc>` and `<vis_desc>` segments, then reordered so the dominant modality's segment appears first. During SFT, the model learns to initiate reasoning from the informative modality rather than defaulting to visual-first order.

### Mechanism 3: Modality-Aware Reward Optimization
Two complementary rewards—order reward and grounding reward—reinforce modality-aligned reasoning beyond what SFT alone achieves. The Modality-Aligned Order Reward (R_MAO) grants reward when reasoning starts with the correct modality token AND the target emotion appears within that segment. The Modality-Grounded Reasoning Reward (R_MGR) evaluates semantic alignment between modality-specific reasoning and target emotion using an external classifier.

## Foundational Learning

- **Reasoning Drift in Autoregressive Generation**
  - Why needed: The paper identifies that MLLM attention shifts from multimodal inputs to self-generated text during reasoning, causing subsequent reasoning to anchor on early (potentially incorrect) tokens.
  - Quick check: Can you explain why the first few generated tokens in chain-of-thought disproportionately affect final output consistency?

- **GRPO (Group Relative Policy Optimization)**
  - Why needed: The RL stage uses GRPO-style optimization rather than PPO; understanding baseline computation from grouped outputs is essential for implementing the reward pipeline.
  - Quick check: How does GRPO differ from PPO in terms of advantage estimation and value function requirements?

- **Explanation-Prediction Consistency (EPC) Metric**
  - Why needed: The primary evaluation metric measures whether the emotion expressed in reasoning matches the final predicted label; this differs from accuracy-focused metrics.
  - Quick check: Why might a model achieve high prediction accuracy but low EPC, and what does this indicate about reasoning reliability?

## Architecture Onboarding

- **Component map:**
  Qwen2.5-7B (LLM, HumanOmni init) -> SigLIP vision encoder (384×384, 8 frames) -> BERT text encoder -> Whisper-large-v3 audio encoder (16kHz → 128-channel mel-spectrogram) -> MI estimator (inference-time ablation) -> GRPO reward modules (emotion classifier, rule-based parser)

- **Critical path:**
  1. Preprocess video (8 frames @ 384×384) and audio (mel-spectrogram)
  2. Estimate MI via three forward passes (audio-only, visual-only, both)
  3. Construct training sample with reordered reasoning segments
  4. SFT on EMER + FAU-filtered augmented data (5 epochs, lr=2e-5)
  5. GRPO optimization with three rewards (1 epoch, lr=1e-6, G=4)

- **Design tradeoffs:**
  - Accuracy vs. Consistency: MIGR achieves lower WAR (73.93%) than non-reasoning HumanOmni (82.48%) but substantially higher EPC (88.95% vs. N/A). This is a deliberate tradeoff for trustworthy explanations.
  - Surprise category degradation: MIGR shows 12-point drop on Surprise vs. ERV; paper attributes this to Surprise-Fear ambiguity. Consider per-emotion analysis before deployment.
  - FAU-based filtering overhead: Additional data augmentation requires OpenFace AU extraction; ~437 filtered samples added to training.

- **Failure signatures:**
  - Missing dominant modality: When audio has no speech and visual cues are ambiguous, both `<aud_desc>` and `<vis_desc>` may lack emotional content, leading to neutral predictions.
  - Compound emotion confusion: Fear/Surprise confusion in 47 misclassified samples; 83% retain Surprise in Top-2 reasoning but final label flips.
  - Hallucination in baselines: R1-Omni and ERV generate non-existent subtitles; MIGR mitigates via modality grounding.

- **First 3 experiments:**
  1. MI validation: Run MI estimation on held-out samples; manually verify that "dominant" modality aligns with human intuition for 20-30 cases.
  2. Reward ablation: Train with R_MAO only, R_MGR only, and both; confirm both contribute (Table 5 shows 64.64 → 68.48 FCR when adding R_MGR).
  3. Attention analysis: Replicate Figure 5 attention visualization on your test set; verify MIGR shows modality-adaptive attention peaks vs. baseline's static bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance degradation on the Surprise emotion category (66.89% vs. 78.84% for ERV) be mitigated while preserving the consistency improvements?
- Basis in paper: [explicit] The authors acknowledge that MIGR exhibits "a pronounced degradation on the Surprise category compared with ERV" and state this aspect "has room for further improvement."
- Why unresolved: The analysis shows 83% of misclassified Surprise samples retain Surprise in Top-2 reasoning, suggesting the issue lies in final label assignment rather than evidence detection. The root cause of this decision boundary shift is not addressed.
- What evidence would resolve it: Experiments with modified reward functions that penalize Surprise-Fear confusion specifically, or architectural changes to the final classification layer that better separate these perceptually similar emotions.

### Open Question 2
- Question: Does the trade-off between reasoning consistency and recognition accuracy reflect an inherent limitation, or can both be simultaneously optimized?
- Basis in paper: [explicit] The authors observe that all reasoning-generating models including MIGR "exhibit slightly lower recognition accuracy than HumanOmni, suggesting that the explicit reasoning generation process can weaken the direct answer-prediction capability."
- Why unresolved: The paper does not investigate whether this trade-off is fundamental to the reasoning paradigm or an artifact of current training approaches. No ablation explores jointly optimizing for both objectives.
- What evidence would resolve it: Multi-objective optimization experiments varying the weight between consistency rewards and accuracy rewards, or architectures that decouple reasoning generation from final prediction.

### Open Question 3
- Question: How robust is the Modality Importance estimation mechanism when audio and visual modalities contain contradictory emotional signals of similar strength?
- Basis in paper: [inferred] The MI estimation compares predictions under unimodal inputs, but the handling of equally informative modalities only states "we include both orderings." The paper does not analyze cases where modalities conflict with comparable strength.
- Why unresolved: The validation focuses on benchmark performance, not on controlled analysis of MI estimation accuracy in adversarial or ambiguous multimodal scenarios.
- What evidence would resolve it: A synthetic dataset with known ground-truth modality importance labels, or analysis on samples where annotators strongly disagree about which modality is dominant.

## Limitations
- **Surprise emotion degradation:** 12-point accuracy drop compared to ERV baseline, attributed to Surprise-Fear ambiguity.
- **Recognition accuracy vs consistency trade-off:** WAR drops from 82.48% to 73.93% when adding reasoning generation.
- **FAU filtering complexity:** Requires OpenFace AU extraction and Emotion-AU mapping, adding preprocessing overhead.

## Confidence
- **High confidence:** Architecture components (Qwen2.5-7B, SigLIP, Whisper, BERT) and their initialization from HumanOmni are explicitly specified.
- **Medium confidence:** SFT training details (5 epochs, lr=2e-5, batch=32) are clear, but FAU filtering procedure and Emotion-AU mapping table are not fully specified.
- **Low confidence:** GRPO implementation details beyond learning rate (1e-6) and G=4, and the emotion classifier used for R_MGR rewards.

## Next Checks
1. Validate MI estimation accuracy by comparing estimated dominant modality against human annotations on 20-30 test samples.
2. Verify GRPO reward implementation by reproducing Table 5 ablation results showing R_MGR contribution.
3. Test attention distribution analysis on sample predictions to confirm modality-adaptive behavior vs. baseline static patterns.