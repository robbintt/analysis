---
ver: rpa2
title: Disentangling Fine-Tuning from Pre-Training in Visual Captioning with Hybrid
  Markov Logic
arxiv_id: '2503.13847'
source_url: https://arxiv.org/abs/2503.13847
tags:
- image
- training
- caption
- examples
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to disentangle what a visual
  captioning model learns during fine-tuning from what it has already learned during
  pre-training, using Hybrid Markov Logic Networks (HMLNs). The core idea is to model
  symbolic knowledge extracted from captions alongside visual features using a probabilistic
  framework, allowing for inference that quantifies the influence of training examples
  on generated captions.
---

# Disentangling Fine-Tuning from Pre-Training in Visual Captioning with Hybrid Markov Logic

## Quick Facts
- arXiv ID: 2503.13847
- Source URL: https://arxiv.org/abs/2503.13847
- Reference count: 40
- Authors: Monika Shah; Somdeb Sarkhel; Deepak Venugopal
- One-line result: Introduces HMLN-based framework to quantify fine-tuning vs pre-training influence in visual captioning models, showing BLIP2 relies more on pre-trained LLM knowledge.

## Executive Summary
This paper proposes a method to disentangle what a visual captioning model learns during fine-tuning from what it has already learned during pre-training. The approach uses Hybrid Markov Logic Networks (HMLNs) to model symbolic knowledge extracted from captions alongside visual features in a unified probabilistic framework. Two inference methods are developed: MAP estimation to quantify explainability of generated captions, and back-tracing to identify the most/least influential training examples. Experiments on MSCOCO show that VLLM-based models like BLIP2 have lower explainability scores, indicating stronger reliance on pre-trained knowledge.

## Method Summary
The method builds an HMLN distribution over training examples by relating symbolic predicates (extracted from captions) with visual features (extracted from images) using CLIP embeddings. The HMLN contains conjunctive features (C) that chain predicates sharing objects and XOR features (I) that encode mutual exclusivity. Discriminative weight learning is performed via contrastive divergence using Gibbs sampling. For inference, MAP estimation quantifies how well generated captions fit the training distribution (via MILP optimization), while back-tracing uses importance weighting to rank training examples by their influence on specific outputs.

## Key Results
- BLIP2 achieves lower MAP scores than non-VLLM models, indicating less explainability from fine-tuning examples
- AMT user study shows contrastive examples are significantly less interpretable for BLIP2 (paired t-test, p<0.05)
- Standard captioning metrics (BLEU4, METEOR, ROUGE, CiDER, SPICE) show no significant difference between VLLM and non-VLLM models
- Hellinger's distance reveals VLLM models have different output distributions compared to non-VLLMs

## Why This Works (Mechanism)

### Mechanism 1
A unified probabilistic distribution over symbolic concepts and real-valued visual features can represent training data relationships for captioning models. Hybrid Markov Logic Networks combine First-Order Logic formulas with continuous functions via a log-linear distribution. Conjunctive features (C) chain predicates sharing objects within captions; XOR features (I) encode mutual exclusivity between predicates. Real-valued terms connect visual and text modalities using CLIP embeddings—specifically, negative log-sigmoid of visual-predicate similarity for C features and Gaussian penalty for differences in I features. The distribution takes form P(X=x) = (1/Z)exp(Σᵢ θᵢsᵢ(x)) where sᵢ is the formula value.

### Mechanism 2
Maximum a Posteriori (MAP) estimation can quantify whether a generated caption is explainable by the training distribution. Given visual features X and generated caption predicates Y, compute the MAP assignment to remaining variables M̂ that maximizes the unnormalized log probability. Semantic equivalence constraints are added as virtual evidence between generated (Y) and reference (Y*) predicates using word embedding similarity scores. The optimization is approximated via Mixed Integer Linear Programming (MILP)—symbolic features become binary variables, continuous features become real-valued variables, with auxiliary variables and hard constraints encoding formulas. Larger MAP objective values indicate greater explainability.

### Mechanism 3
Training examples can be ranked by their influence on generated captions using importance sampling with clipped weights. Compute importance weights w(y) = P(y|X̂)/P(y|X) comparing probability under training vs. test image features. Use Gibbs sampling to draw samples from P(y|X), then estimate marginal densities P(X|Y) via Monte Carlo: P(X|Y) ≈ Σ max(w(yᵢ),1)·I_X[yᵢ] / Σ max(w(yᵢ),1). Unnormalized probabilities approximate true weights (asymptotically unbiased). Thinning (keeping every k-th sample) reduces dependency. Contrastive examples X⁺ = arg max P(X|Y) and X⁻ = arg min P(X|Y) show most/least influential training instances.

## Foundational Learning

- **Concept:** First-Order Logic Grounding
  - **Why needed here:** The HMLN framework requires instantiating logical formulas with concrete constants from training data. Variables in formulas are replaced with actual objects/predicates extracted from captions, creating ground atoms that become random variables.
  - **Quick check question:** Given formula "holding(X,Y) ∧ kite(Y)" and a caption "a man holding a red kite," what constants substitute for X and Y during grounding?

- **Concept:** Markov Chain Monte Carlo Convergence
  - **Why needed here:** Both parameter learning (contrastive divergence) and marginal inference rely on Gibbs sampling. Understanding burn-in (allowing chain to mix) and thinning (reducing sample dependency) is essential for unbiased estimates.
  - **Quick check question:** If you observe high variance in marginal density estimates across different Gibbs sampling runs, what might this indicate about the chain's mixing behavior?

- **Concept:** Importance Sampling Bias-Variance Tradeoff
  - **Why needed here:** The back-tracing mechanism uses importance weighting to estimate training example influence. Unnormalized weights introduce bias that only vanishes asymptotically; clipping prevents high-variance estimates from dominating.
  - **Quick check question:** Why does using unnormalized probability ratios (avoiding partition function computation) introduce bias, and under what conditions does this bias decrease?

## Architecture Onboarding

- **Component map:** Scene Graph Parser -> CLIP Encoder -> HMLN Structure Generator -> Parameter Learner -> MAP Inference Engine -> Marginal Inference Engine -> Contrastive Example Selector

- **Critical path:** Scene graph parsing quality determines predicate extraction accuracy → CLIP embedding alignment affects real-valued term computation → Gibbs sampler mixing governs estimate reliability → MILP solver efficiency limits scalability

- **Design tradeoffs:** Discriminative vs generative learning: conditioning on visual features improves efficiency but may miss joint distribution structure; Query-specific parameterization: scales to large datasets but may not capture global patterns; Unnormalized importance weights: tractable but asymptotically unbiased only; MILP approximation: tractable MAP inference but solution quality depends on solver parameters

- **Failure signatures:** Uniformly low MAP scores: check CLIP embedding alignment or scene graph parser output; High weight variance: increase burn-in or check formula connectivity; MILP timeouts: reduce context window for training example selection; Poor AMT interpretability ratings: verify semantic equivalence scoring captures true similarity

- **First 3 experiments:**
  1. Reproduce Figure 2 MAP scores on 100-image MSCOCO subset for BLIP2 and XLAN; verify BLIP2 shows lower MAP values (higher negative log)
  2. Manually validate scene graph parsing on 50 captions; check predicate extraction coverage for common objects/relationships
  3. Test importance weight stability across burn-in settings (100, 500, 1000) and thinning intervals (5, 10, 20); identify minimum stable configuration

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the disentanglement of fine-tuning versus pre-training knowledge differ across various Vision Large Language Model (VLLM) architectures beyond BLIP2? Basis: The authors state in the conclusion, "In future, we plan to generalize our work across different VLLMs." Unresolved because the study primarily contrasts BLIP2 against non-VLLM models, leaving the behavior of other prominent VLLMs (e.g., LLaVA, GPT-4V) unexplored.

- **Open Question 2:** Can the Hybrid Markov Logic Network (HMLN) framework be effectively adapted to explain knowledge acquisition in generative multimodal tasks other than captioning? Basis: The conclusion proposes generalizing the work "across different multimodal tasks such as Visual Question Answering." Unresolved because the current method relies on caption-specific templates (Conjunctive and XOR features); it is unknown if these symbolic structures can represent the reasoning required for tasks like VQA.

- **Open Question 3:** Does restricting the conjunctive feature chains to a maximum length of two predicates limit the expressiveness or accuracy of the explanations? Basis: Section IV.B notes that the size of the chain is limited to two "since beyond this, the chains seem to lack coherence." Unresolved because this heuristic avoids complexity but may fail to capture complex, multi-step semantic relationships present in dense images, potentially oversimplifying the "explanation."

## Limitations
- The scene graph parser configuration and predicate extraction format from reference [36] is not fully specified, which could affect grounding quality.
- Several algorithmic parameters (Gibbs sampling settings, weight initialization, convergence criteria) are mentioned but exact values or strategies are unspecified.
- The AMT user study provides qualitative support but sample size and exact prompt details are not reported, limiting reproducibility of interpretability findings.

## Confidence

- **High confidence**: The core HMLN framework combining symbolic and continuous features is technically sound and well-explained. The MAP inference mechanism for quantifying explainability is clearly defined.
- **Medium confidence**: The back-tracing approach using importance weighting is methodologically valid but relies on approximations (unnormalized weights, finite Gibbs samples) that introduce uncertainty in attribution quality.
- **Medium confidence**: The BLIP2 interpretability findings from AMT are plausible given the model's reliance on LLM knowledge, but require replication with larger samples and more rigorous controls.

## Next Checks

1. **Grounding quality validation**: Manually verify scene graph parser output on 50 captions to assess predicate extraction coverage and accuracy.
2. **Parameter sensitivity analysis**: Test MAP score stability across different Gibbs sampling burn-in periods (100, 500, 1000) and thinning intervals (5, 10, 20).
3. **Importance weight variance assessment**: Evaluate weight distribution variance across multiple runs to quantify uncertainty in contrastive example selection.