---
ver: rpa2
title: 'Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction'
arxiv_id: '2512.11399'
source_url: https://arxiv.org/abs/2512.11399
tags:
- video
- clips
- clip
- selection
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently summarizing long
  videos, particularly movies, while retaining critical visual information. The core
  idea is to select key video clips that contain important visual information not
  inferable from dialogue transcripts alone.
---

# Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction

## Quick Facts
- **arXiv ID**: 2512.11399
- **Source URL**: https://arxiv.org/abs/2512.11399
- **Reference count**: 40
- **Primary result**: 6% of video clips (derived from reference clips) is sufficient for complete multimodal summaries

## Executive Summary
This paper introduces a method for efficiently summarizing long videos by selecting visually salient clips that contain information not inferable from dialogue transcripts alone. The approach segments videos into 20-second clips, generates lightweight captions for each using a small VLM, and employs an LLM to select the top K most relevant clips. These selected clips are then recaptioned in detail and integrated into transcripts to create a multimodal screenplay for summarization. Experiments on the MovieSum dataset demonstrate that using only 6% of video content yields summaries nearly equivalent to those using the full screenplay, while significantly reducing computational cost.

## Method Summary
The method processes long videos through a pipeline: (1) Segment video into 20-second clips; (2) Generate lightweight captions for all clips using a small VLM (Qwen2.5-Omni-3B/7B) at 1 fps plus audio; (3) Use an LLM (Gemini 2.5 Flash) to select top K clips based on semantic content criteria; (4) Recaption selected clips with a stronger model (Gemini 2.5 Flash-Lite) for detailed descriptions; (5) Build a screenplay by inserting detailed recaptions into transcripts at proper timestamps with "Caption:" markers; (6) Generate a 1000-word multimodal summary using Gemini 2.5 Flash. The approach targets the MovieSum dataset (2200 movies; 200 validation, 200 test) with an average 2-hour runtime per movie.

## Key Results
- Using only 6% of video clips (derived from reference clips) achieves visual recall of 32.84% vs. 34.47% for full screenplay summaries
- The proposed clip selection method outperforms baselines like random and silent clip selection
- Higher K values improve recall but not summary quality, suggesting optimal K balances coverage and conciseness
- Lightweight captions enable cost-effective selection without processing raw video through large VLMs

## Why This Works (Mechanism)

### Mechanism 1
Lightweight captions provide sufficient signal for identifying visually salient clips, enabling cost-effective selection without processing raw video through a large VLM. A small VLM generates compact text descriptions of 20-second clips, and an LLM selects the top K clips by comparing caption semantic content against criteria of "important action or visual event." This decouples visual understanding (done cheaply once) from selection reasoning (done textually).

### Mechanism 2
A two-stage captioning process—lightweight for selection, detailed for summarization—creates a division of labor that optimizes both efficiency and summary quality. Stage 1 uses a small model to caption all clips (~354 per movie) at low cost. Stage 2 applies a stronger model only to the K selected clips. The detailed recaptions are inserted into transcripts to form a screenplay for summarization.

### Mechanism 3
Visual information is highly redundant in long-form video; only ~6% of clips contain non-inferable, summary-relevant content. The paper derives reference clips from human-annotated screenplays by identifying which summary facts require visual (not dialogue) support. These reference clips represent the minimal sufficient set.

## Foundational Learning

- **Concept**: Recall@K with Intersection-over-Reference (IoR)
  - **Why needed here**: This metric evaluates clip selection quality by measuring temporal overlap between predicted and reference clips. Unlike frame-level accuracy, it tolerates imprecise boundaries.
  - **Quick check question**: If your method selects a 20-second clip that overlaps a 15-second reference clip by 10 seconds, and τ=0.3, is it counted as retrieved?

- **Concept**: Multimodal fact decomposition
  - **Why needed here**: The paper evaluates summaries by decomposing them into atomic facts (simple clauses), then classifying each as Visual or Textual to measure modality-specific recall.
  - **Quick check question**: Given the sentence "John enters the dark room and whispers a warning," how many facts should be extracted, and which might be Visual vs. Textual?

- **Concept**: Two-shot prompting for selection
  - **Why needed here**: The paper improves clip selection by providing two annotated examples in the prompt, teaching the LLM what constitutes "important visual action."
  - **Quick check question**: What risk does two-shot prompting introduce if the examples are from genres dissimilar to the target video?

## Architecture Onboarding

- **Component map**: Video segmenter -> Lightweight captioner -> Selection LLM -> Recaptioning VLM -> Screenplay assembler -> Summarization LLM
- **Critical path**: Lightweight captioning is the throughput bottleneck (processes all ~354 clips). Selection LLM and recaptioning only process K clips. The summarization LLM processes the full screenplay (~25K words average).
- **Design tradeoffs**: Fixed 20-second clips vs. scene segmentation (fixed clips outperformed); K selection (higher K improves recall but not summary quality); Captioning model size (Qwen2.5-Omni-7B outperforms 3B but increases cost).
- **Failure signatures**: Low visual recall but high textual recall (lightweight captions missing action terms); High Recall@K but poor summary (recaptioning model not capturing detail or K too low); Selection selects silent clips preferentially (LLM biased toward captions without dialogue overlap).
- **First 3 experiments**: (1) Baseline sanity check: random clip selection vs. method on 10 movies (expect ~2x improvement in Recall@50); (2) Caption quality ablation: inspect 20 lightweight captions for action verbs, correlate missing verbs with false negatives; (3) K sensitivity test: run summarization with K=25, 50, 75 on 5 movies, verify visual recall increases but MFACTSUM saturates.

## Open Questions the Paper Calls Out
- Can an adaptive selection strategy dynamically optimize the number of clips (K) based on video duration and content density rather than a fixed count?
- Does the minimal-clip approach transfer effectively to other video domains or multimodal tasks beyond movie summarization?
- How does the relationship between retrieved clips and summary quality change in an unconstrained summarization setting without a fixed word count?

## Limitations
- Performance critically depends on transcript-to-video alignment quality, which relies on an external audio-based alignment tool without implementation details
- The reference clip extraction process assumes human screenplays perfectly capture all necessary visual information
- The study focuses exclusively on movies, limiting generalizability to other long-form video domains

## Confidence
- **High confidence**: The two-stage captioning approach and its cost-efficiency benefits are well-supported by ablation studies and performance comparisons
- **Medium confidence**: The claim that only 6% of clips contain non-inferable visual information depends on the accuracy of the reference extraction methodology
- **Medium confidence**: The superiority of fixed 20-second clips over scene segmentation is demonstrated on MovieSum but may not generalize to videos with different temporal structures

## Next Checks
1. **Transcript Alignment Validation**: Test the impact of different alignment methods (or alignment errors) on final summary quality by comparing results when using perfect vs. estimated timestamps for caption insertion
2. **Cross-Domain Generalization**: Apply the method to non-movie long videos (e.g., documentaries or instructional videos) to assess whether the 6% clip selection rate and selection effectiveness generalize beyond the movie domain
3. **Selection Interpretability Analysis**: For a sample of false negatives (missed reference clips), analyze whether the lightweight captions truly lack visual descriptors or if the selection LLM fails to recognize their importance, distinguishing caption quality from selection model issues