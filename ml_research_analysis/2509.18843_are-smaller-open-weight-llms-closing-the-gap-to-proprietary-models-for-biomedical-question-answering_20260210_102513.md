---
ver: rpa2
title: Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical
  Question Answering?
arxiv_id: '2509.18843'
source_url: https://arxiv.org/abs/2509.18843
tags:
- questions
- question
- batch
- open-weight
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated whether small open-weight language models
  (up to 14 billion parameters) could match the performance of proprietary models
  like GPT-4 and Claude in biomedical question answering. The researchers applied
  retrieval-augmented generation using the BioASQ dataset, retrieving relevant PubMed
  snippets based on embedding similarity, and enhancing performance with in-context
  learning and structured outputs.
---

# Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?

## Quick Facts
- **arXiv ID**: 2509.18843
- **Source URL**: https://arxiv.org/abs/2509.18843
- **Authors**: Damian Stachura; Joanna Konieczna; Artur Nowak
- **Reference count**: 35
- **Primary result**: Small open-weight LLMs (up to 14B parameters) matched or exceeded proprietary models in biomedical QA using retrieval-augmented generation and ensemble strategies.

## Executive Summary
This study investigates whether small open-weight language models can match proprietary models in biomedical question answering. Using the BioASQ dataset and retrieval-augmented generation with PubMed snippets, the researchers tested models including Phi-4, Gemma-3, Qwen2.5, and Mistral. Results show that open-weight models are highly competitive, often matching or surpassing proprietary models like GPT-4 and Claude, especially when using ensemble strategies. For yes/no questions, open models excelled in two of four batches, while factoid and list questions saw significant gains from model ensembling. The findings demonstrate that small open-weight LLMs can effectively replace larger closed-source models in biomedical QA tasks, particularly when leveraging ensemble strategies and RAG techniques.

## Method Summary
The researchers applied retrieval-augmented generation using the BioASQ dataset, retrieving relevant PubMed snippets based on embedding similarity with nomic-embed-text-v1. They enhanced performance with in-context learning and structured outputs, testing models individually and in ensemble configurations. The study used hand-crafted prompts with optional few-shot examples (3-shot for factoid/list, zero-shot for yes/no/summary), CFG-based constrained decoding, and frequency-based aggregation for ensembles. Models were evaluated on yes/no, factoid, list, and summary questions across four BioASQ batches, comparing open-weight models against proprietary baselines.

## Key Results
- Open-weight models matched or exceeded proprietary models in biomedical QA tasks
- Ensemble strategies significantly improved performance, especially for factoid and list questions
- Yes/no questions showed variable accuracy across batches, suggesting context quality as a bottleneck
- ROUGE F1 scores were lower than recall for summaries, indicating verbosity issues in generated responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding-based retrieval grounds model responses in domain-specific evidence, reducing hallucination for biomedical QA.
- Mechanism: Cosine similarity between question embeddings and snippet embeddings (via nomic-embed-text-v1) ranks PubMed content; top-10 snippets are injected into the prompt as context.
- Core assumption: The provided PubMed articles contain sufficient evidence to answer the question accurately.
- Evidence anchors:
  - [abstract] "retrieving the most relevant snippets based on embedding distance"
  - [section 2.1] "We utilized the sentence-transformers library with the nomic-embed-text-v1 model... computing embeddings for all snippets and the question"
  - [corpus] Weak corpus support—no direct retrieval-mechanism papers in neighbors; ECG-LLM paper mentions local deployment advantages but not retrieval specifics.
- Break condition: If relevant evidence is absent from the corpus, retrieval quality degrades and models may still hallucinate.

### Mechanism 2
- Claim: In-context learning with retrieved examples improves exact-answer extraction but harms summary and yes/no performance.
- Mechanism: Few-shot examples (3-shot for factoid/list) are retrieved from a Qdrant vector DB by similarity to the current question; zero-shot is used for yes/no and summary types based on empirical testing.
- Core assumption: Similar past questions provide transferable answer patterns without overfitting to example format.
- Evidence anchors:
  - [abstract] "enhancing performance with in-context learning"
  - [section 2.2] "3 examples were optimal for factoid and list questions, while a zero-shot approach was used for yes/no and summary type questions"
  - [corpus] No direct corpus support for this specific finding.
- Break condition: If few-shot examples are too similar or too different from the target question, performance may degrade; zero-shot may outperform when few-shot introduces noise.

### Mechanism 3
- Claim: Ensembling diverse model families improves exact-answer accuracy by aggregating complementary error patterns.
- Mechanism: For factoid/list questions, responses from multiple models are collected; frequency counting identifies the most common answers above a threshold. For yes/no, majority voting applies. Diversity across architectures (Phi, Qwen, Mistral, Gemma) is intentionally leveraged.
- Core assumption: Different model families make uncorrelated errors; combining them cancels individual weaknesses.
- Evidence anchors:
  - [abstract] "open models excelled in two of four batches... significant gains from model ensembling"
  - [section 2.6] "different LLMs, trained on varied data and architectures, inherently exhibit unique strengths that can be synergistic in an ensemble"
  - [corpus] BioHopR paper addresses multi-hop biomedical reasoning but not ensembling directly; weak corpus support.
- Break condition: If models share similar failure modes or the threshold is poorly calibrated, ensembling provides no benefit or amplifies errors.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: All mechanisms depend on grounding answers in retrieved PubMed snippets; without this, models rely on parametric knowledge which may be outdated or incorrect for biomedical facts.
  - Quick check question: Can you explain why cosine similarity between query and document embeddings might fail for negation queries (e.g., "Which drugs do NOT treat X")?

- Concept: **In-Context Learning (Few-Shot vs Zero-Shot)**
  - Why needed here: The paper shows heterogeneous effects—few-shot helps factoid/list but hurts yes/no/summary. Understanding when and why is critical for prompt design.
  - Quick check question: Given the paper's finding that zero-shot outperformed few-shot for yes/no questions, what hypothesis might explain this reversal?

- Concept: **Ensemble Aggregation Strategies**
  - Why needed here: The paper uses different aggregation for different question types (majority voting vs frequency thresholding). Understanding trade-offs is essential for extending this work.
  - Quick check question: Why might frequency-based aggregation work better for factoid questions than for summary questions?

## Architecture Onboarding

- Component map:
  Embedding Layer (nomic-embed-text-v1) -> Vector Store (Qdrant) -> Retriever (Top-k cosine similarity) -> Prompt Constructor (hand-crafted ± few-shot) -> LLM Pool (Open-weight + proprietary) -> Structured Output Layer (CFG-based) -> Ensemble Aggregator (frequency/majority voting)

- Critical path:
  1. Question received → embed via nomic model
  2. Retrieve top-10 snippets from Qdrant by cosine similarity
  3. Retrieve 3 similar historical questions (for factoid/list only)
  4. Construct prompt with snippets ± examples
  5. Generate with structured output constraint
  6. If ensemble: aggregate across model pool per question type

- Design tradeoffs:
  - **k=10 snippets**: Validated as optimal on historical data, but may miss relevant content for complex multi-hop questions
  - **3-shot for factoid/list**: Empirically optimal; more examples may cause context dilution
  - **4-bit quantization**: Used for batches 1-3 for efficiency; batch 4 tested unquantized with comparable results
  - **Cross-encoder reranking for summaries**: Used BiomedBERT for selection; added latency but improved quality

- Failure signatures:
  - Yes/no accuracy highly variable across batches (Table 3): suggests context quality is the bottleneck, not model capability
  - ROUGE F1 scores significantly lower than recall for summaries: indicates verbosity issues, models generating longer responses than ideal
  - DSPy prompts underperformed hand-crafted: suggests automated prompt optimization requires more tuning investment

- First 3 experiments:
  1. **Baseline single-model comparison**: Run Phi-4, Qwen2.5-14B, and GPT-4o individually on a held-out BioASQ batch with identical retrieval and prompts to establish component-level performance gaps.
  2. **Snippet ablation**: Vary k (5, 10, 15, 20) and measure impact on factoid accuracy and yes/no F1 to validate the k=10 assumption under different question complexity.
  3. **Ensemble diversity test**: Compare (a) same-family ensemble (all Gemma variants), (b) cross-family ensemble (Phi + Qwen + Mistral), and (c) hybrid (open + proprietary) to quantify diversity benefit and identify optimal ensemble composition.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is constrained to biomedical QA using BioASQ data, limiting generalizability to other domains
- Performance metrics lack human evaluation of clinical accuracy or answer correctness
- Retrieval mechanism depends entirely on PubMed snippet corpus quality and coverage
- Ensemble strategies are empirically effective but lack theoretical grounding for aggregation method selection

## Confidence
**High Confidence**: Open-weight models can match or exceed proprietary models in biomedical QA when using retrieval-augmented generation and ensemble strategies. This is supported by consistent performance parity across multiple model families and question types in the BioASQ benchmark.

**Medium Confidence**: Embedding-based retrieval effectively grounds model responses and reduces hallucination. While the mechanism is sound and the retrieval pipeline is well-specified, the corpus shows no direct support for this specific application, and the k=10 snippet cutoff is empirically chosen without theoretical justification.

**Low Confidence**: In-context learning with few-shot examples improves factoid/list performance but harms yes/no/summary outcomes. This finding is based on limited ablation testing within the study, with no external validation or theoretical explanation for the differential effects across question types.

## Next Checks
1. **Human Evaluation of Clinical Accuracy**: Recruit biomedical experts to rate a random sample of model-generated answers for factual correctness, completeness, and clinical safety, comparing open-weight ensembles to proprietary models on the same questions.

2. **Retrieval Quality and Coverage Analysis**: Systematically measure the overlap between retrieved snippets and ground-truth evidence for failed questions, and test performance when relevant evidence is intentionally removed from the corpus to quantify the impact of missing information.

3. **Ensemble Diversity and Error Correlation Study**: Analyze error patterns across model families on the same questions to measure inter-model correlation, and test whether ensemble performance degrades when combining models with similar failure modes or improves when aggregating highly diverse architectures.