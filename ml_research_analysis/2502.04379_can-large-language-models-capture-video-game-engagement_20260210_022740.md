---
ver: rpa2
title: Can Large Language Models Capture Video Game Engagement?
arxiv_id: '2502.04379'
source_url: https://arxiv.org/abs/2502.04379
tags:
- uni00000008
- uni00000010
- uni00000014
- uni00000015
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  accurately predict viewer engagement from gameplay videos. The authors conducted
  over 4,800 experiments using 80 minutes of annotated first-person shooter gameplay
  from the GameVibe corpus, testing multiple LLM architectures, model sizes, prompting
  strategies, and input modalities.
---

# Can Large Language Models Capture Video Game Engagement?

## Quick Facts
- **arXiv ID:** 2502.04379
- **Source URL:** https://arxiv.org/abs/2502.04379
- **Reference count:** 40
- **Primary result:** GPT-4o with multimodal few-shot prompting achieves up to 47% improvement over baseline in engagement prediction from gameplay videos, but generally falls short of human annotation performance.

## Executive Summary
This study investigates whether large language models can accurately predict viewer engagement from gameplay videos. Using 80 minutes of annotated first-person shooter gameplay from the GameVibe corpus, the authors conducted over 4,800 experiments testing multiple LLM architectures, model sizes, prompting strategies, and input modalities. The research demonstrates that while LLMs can outperform traditional machine learning baselines, they generally fall short of human-level annotation performance. The best results came from using GPT-4o with multimodal few-shot prompting, achieving up to 47% improvement over baseline in certain games, with an average 6% improvement across games.

## Method Summary
The study evaluates multimodal LLMs (GPT-4o, LLaVA-OV, Qwen2.5-VL) on binary classification of engagement change (increase vs. decrease) between consecutive video frames in 20 FPS games. Frames are extracted at 3-second intervals, cropped/resized to 512×512, with ground truth derived from continuous annotations resampled into 3-second windows. The best-performing setup uses Chain-of-Thought prompting with few-shot examples from the same game, providing one positive and one negative frame pair with explicit reasoning before each query. Performance is measured as relative accuracy gain (ΔA) over a Zero Rule Classifier baseline.

## Key Results
- GPT-4o with multimodal few-shot prompting achieved up to 47% improvement over baseline in certain games, with an average 6% improvement across games
- Text-based frame descriptions did not improve performance compared to direct multimodal prompting
- Model size significantly impacts performance, with larger models showing better results
- Games with clearer visuals and more dynamic gameplay were easier for LLMs to predict, while games with dark environments and repetitive sections were more challenging

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot examples with explicit Chain-of-Thought reasoning help LLMs align with human engagement annotation tasks compared to zero-shot or text-only inputs
- **Core assumption:** The model can transfer reasoning patterns from few-shot examples to unseen frames within the same domain
- **Evidence anchors:** GPT-4o significantly outperforms baselines using Few-Shot; multimodal few-shot prompting yields best results
- **Break condition:** If visual domain shifts drastically or reasoning chain is hallucinated rather than grounded in visual features

### Mechanism 2
- **Claim:** Performance depends on visual signal-to-noise ratio; clear, dynamic frames allow detection of engagement correlates, while dark/repetitive frames lead to prediction collapse
- **Core assumption:** Engagement correlates positively with visual dynamism and identifiable action elements
- **Evidence anchors:** Games with clearer visuals were easier to predict; dark games like HROT were more challenging
- **Break condition:** If engagement is driven by internal cognitive states rather than visual dynamism

### Mechanism 3
- **Claim:** Model performance scales with game popularity due to pre-training exposure to game content or discourse
- **Core assumption:** The "engagement" construct aligns with the model's internal representation of "exciting" content derived from training
- **Evidence anchors:** Model size significantly impacts performance; more popular games yield better predictions
- **Break condition:** If human annotation schema contradicts the model's learned definition

## Foundational Learning

- **Concept: Ordinal Preference Learning vs. Absolute Rating**
  - **Why needed here:** The paper treats engagement prediction as pairwise preference task rather than absolute regression, mitigating scale ambiguity and annotator bias
  - **Quick check question:** Why does asking an LLM "Which frame is more engaging?" yield different results than "Rate this frame 1-10"?

- **Concept: Temporal Alignment (Reaction Lag)**
  - **Why needed here:** The study uses -2 second temporal shift to align video frames with physiological/subjective reaction times
  - **Quick check question:** If an exciting event happens at t=0, but annotation peaks at t=2, how does training data window need to be adjusted?

- **Concept: Multimodal Fusion (Vision Encoder + LLM Backbone)**
  - **Why needed here:** Architectures rely on vision encoder to project pixel data into LLM's token space, explaining why text descriptions didn't help
  - **Quick check question:** How does a Vision Transformer map a pixel grid to a token that an LLM can reason about?

## Architecture Onboarding

- **Component map:** Input frames (512×512) + Text Prompt → Vision Encoder → Projector → LLM Backbone → Structured JSON output
- **Critical path:**
  1. Preprocessing: Resample annotations into 3-second windows, apply -2s temporal shift, discretize into binary labels
  2. Prompting: Construct Chain-of-Thought prompt with few-shot examples from held-out session of same game
  3. Inference & Parsing: Query model, parse decision, discard unparseable outputs
- **Design tradeoffs:** Direct multimodal input superior to text descriptions; GPT-4o offers highest accuracy but open-source 8B models offer cost efficiency
- **Failure signatures:** Visual confusion in dark games, output instability (refusals/formatting errors), prior overfitting to popular games
- **First 3 experiments:**
  1. Baseline Establishment: Run MLP and CNN baselines to establish ZeroR and ΔA baseline
  2. Modality Ablation: Compare GPT-4o performance on raw frames vs. advanced text descriptions
  3. Visual Difficulty Test: Evaluate best model specifically on 5 "hardest" games to quantify visual clarity failure mode

## Open Questions the Paper Calls Out

1. **Direct video input vs. static frames:** Does direct video input improve engagement prediction accuracy compared to static frame sampling? Current experiments used static frames to manage resource costs, potentially missing dynamic stimuli.

2. **RAG for dark/repetitive environments:** Can retrieval-augmented generation or memory mechanisms improve performance on games with dark or repetitive environments? The study relied on out-of-the-box knowledge priors without external context.

3. **Efficient fine-tuning for smaller models:** Does efficient fine-tuning (e.g., LoRA) allow smaller open-source models to match performance of large proprietary models? The study focused on zero-shot/few-shot capabilities, leaving the performance ceiling of adapted models unknown.

## Limitations
- Domain generalization: Performance on non-FPS games (strategy, RPG, narrative) remains untested
- Annotation subjectivity: Engagement is inherently subjective, and different annotators or cultural contexts might produce different ground truth patterns
- Temporal alignment assumption: The -2 second shift assumes consistent reaction lag across all viewers and game types

## Confidence

- **High confidence:** Comparative performance between LLM approaches and traditional ML baselines is well-supported by experimental results
- **Medium confidence:** Mechanism explaining why visual clarity affects performance is supported by qualitative observations but could benefit from more rigorous perceptual studies
- **Low confidence:** Hypothesis about model size improvements being due to pre-training exposure remains speculative without direct evidence

## Next Checks
1. **Cross-genre validation:** Test the best-performing model on non-FPS games from GameVibe corpus to assess domain generalization
2. **Annotation stability analysis:** Conduct inter-annotator reliability studies to quantify subjectivity of engagement annotations
3. **Feature importance ablation:** Systematically remove visual elements (contrast, color, motion) from game frames to isolate which visual features most strongly correlate with engagement prediction accuracy