---
ver: rpa2
title: Hierarchical Imitation Learning of Team Behavior from Heterogeneous Demonstrations
arxiv_id: '2502.17618'
source_url: https://arxiv.org/abs/2502.17618
tags:
- learning
- agent
- team
- behavior
- imitation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DTIL addresses the challenge of learning multimodal team behaviors
  from heterogeneous multi-agent demonstrations, particularly in partially observable
  environments. The core method employs a hierarchical policy structure where each
  agent has high-level subtask selection and low-level action policies, trained using
  a factored distribution-matching approach.
---

# Hierarchical Imitation Learning of Team Behavior from Heterogeneous Demonstrations

## Quick Facts
- arXiv ID: 2502.17618
- Source URL: https://arxiv.org/abs/2502.17618
- Authors: Sangwon Seo; Vaibhav Unhelkar
- Reference count: 40
- Primary result: DTIL learns multimodal team behaviors from heterogeneous multi-agent demonstrations, outperforming baselines in task performance and subtask modeling.

## Executive Summary
DTIL addresses the challenge of learning multimodal team behaviors from heterogeneous multi-agent demonstrations, particularly in partially observable environments. The core method employs a hierarchical policy structure where each agent has high-level subtask selection and low-level action policies, trained using a factored distribution-matching approach. This extends single-agent hierarchical imitation learning to multi-agent settings with theoretical guarantees. Experiments across six domains (including continuous and discrete spaces) demonstrate that DTIL outperforms baselines like MA-GAIL and MA-OptionGAIL in both task performance (achieving expert-level rewards) and accurately modeling multimodal behavior (with 75-78% subtask inference accuracy). The method also benefits from semi-supervision, showing improved performance with just 20% labeled subtasks.

## Method Summary
DTIL uses a hierarchical policy decomposition where each agent has a high-level subtask selection policy (ζ) and a low-level action policy (π). The method employs a factored distribution-matching approach to train these policies without adversarial learning, using IQ-Learn for occupancy measure matching. An EM-based loop handles semi-supervised learning by inferring subtask labels for unlabeled demonstrations. The approach extends single-agent hierarchical IL to multi-agent settings with theoretical guarantees for the partially observable case.

## Key Results
- Achieves expert-level rewards across six benchmark domains, outperforming MA-GAIL and MA-OptionGAIL
- Accurately models multimodal behavior with 75-78% subtask inference accuracy
- Benefits from semi-supervision, showing improved performance with just 20% labeled subtasks
- Handles both continuous and discrete action spaces effectively

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Policy Decomposition (AMM)
Explicitly separating high-level subtask selection from low-level action execution allows the model to capture multimodal team behaviors that appear conflicting in flat action spaces. The Agent Markov Model (AMM) treats an agent's policy as a tuple (π, ζ), where a high-level policy ζ selects a subtask x based on observation history, and a low-level policy π executes primitive actions a conditioned on that subtask. The set of possible subtasks X is finite, known a priori, and the expert's behavior is actually driven by these latent states.

### Mechanism 2: Factored Distribution Matching
Decomposing the objective function into separate occupancy measures for actions and subtasks stabilizes training compared to joint adversarial approaches. Instead of matching a joint occupancy measure ρ(o, a, x, x⁻), DTIL minimizes the divergence between two factored distributions: ρ(o, a, x) (action-subtask alignment) and ρ(o, x, x⁻) (subtask transition dynamics). The theoretical bijection (Theorem 5.1) holds, meaning matching these factored measures is sufficient to recover the true joint policy.

### Mechanism 3: EM-based Latent Inference (Semi-Supervision)
An Expectation-Maximization (EM) loop allows the system to ground unlabeled demonstrations by iteratively refining pseudo-labels for subtasks. The E-step infers the most likely subtask sequence χ̂ (labels) for unlabeled demonstrations using the current policy. The M-step updates the policy parameters (θ, φ) using these inferred labels via IQ-Learn. The initial policy or the provided labeled data (as little as 20%) is sufficient to bootstrap the inference process so errors do not accumulate.

## Foundational Learning

- **Concept: Decentralized Partially Observable MDP (Dec-POMDP)**
  - Why needed here: DTIL operates in settings where agents cannot see the full state s, only local observations o_i. Theorem 5.1 is explicitly proven for this Partially Observable setting, extending prior work that assumed full observability.
  - Quick check question: If an agent has full observability in this framework, does the "observation" o simply become the state s? (Yes, reducing the problem complexity).

- **Concept: Occupancy Measure Matching**
  - Why needed here: Unlike Behavioral Cloning (which matches actions step-by-step), DTIL matches the distribution of state-action pairs (occupancy measure). This is critical for long-horizon tasks where step-wise errors compound.
  - Quick check question: Why is minimizing D_f(ρ_π || ρ_E) often preferred over minimizing cross-entropy loss on actions? (It avoids compounding errors by considering the distribution over the whole trajectory).

- **Concept: The Option Framework (Hierarchical RL)**
  - Why needed here: The paper explicitly maps the Agent Markov Model (AMM) to the Option framework with a one-step option. Understanding that a "subtask" is essentially an "option" (a temporally extended action) is key to understanding the high-level policy ζ.
  - Quick check question: In DTIL, how long does an agent commit to a subtask? (The paper implies a one-step option structure where subtasks can switch dynamically at every timestep t based on ζ(x|o, x⁻)).

## Architecture Onboarding

- **Component map:** Trajectories τ = (o, a)₀:ₕ → High-Level Network (ζ_φ) → Low-Level Network (π_θ) → Q-Network (Shared) → Viterbi Solver

- **Critical path:**
  1. Data Prep: Gather unlabeled trajectories D
  2. E-Step: Run Viterbi on D using current (π, ζ) to generate pseudo-labels χ̂
  3. Rollout: Execute current policy in environment to collect new samples R (for online IL)
  4. M-Step: Update θ (action policy) and φ (subtask policy) using IQ-Learn on the augmented dataset D̃ ∪ R

- **Design tradeoffs:**
  - Supervision vs. Inference: Labeling 20% of data drastically improves subtask inference accuracy (Table 4). Fully unsupervised learning may result in "label switching" (latent subtask 1 maps to real subtask 2) without functional loss.
  - Tabular vs. Deep: The paper contrasts DTIL with BTIL (tabular). BTIL is more accurate in tiny domains; DTIL is required for continuous/high-dimensional spaces (Section 8.2.3).

- **Failure signatures:**
  - Unimodal Collapse: If the factored matching fails, the model reverts to a "mean" behavior (e.g., agents standing still or oscillating) rather than choosing a distinct subtask.
  - Inference Drift: In long horizons, the Viterbi algorithm (E-step) might hallucinate subtask switches that don't exist in the expert data to minimize local loss, leading to erratic policy updates.

- **First 3 experiments:**
  1. Sanity Check (Multi-Jobs-2): Run DTIL on the simplest domain with 100% supervision to verify the network capacity and IQ-Learn integration.
  2. Ablation (Supervision Ratio): Evaluate Subtask Inference Accuracy (Table 4) varying labeled data from 0% to 50% to determine the "labeling budget" required for your specific domain.
  3. Scalability (SMACv2): Validate the architecture on a complex continuous domain to ensure the Deep Learning components scale where BTIL (tabular baseline) fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the method be extended to automatically discover or learn the set of subtasks X rather than requiring them as finite prior knowledge?
- Basis in paper: Section 9 states that future methods should explore "more expressive hierarchical representations" because real-world subtasks may be difficult to define a priori.
- Why unresolved: The current mathematical formulation (Sec 3.2) and E-step rely on X being a finite, known set provided beforehand.
- Evidence would resolve it: An extension of DTIL that infers the cardinality and definition of X directly from data without pre-specification, maintaining theoretical convergence guarantees.

### Open Question 2
- Question: How robust is DTIL to the misspecification of the subtask set X (e.g., providing too few or irrelevant subtasks)?
- Basis in paper: Section 3.2 assumes the set of subtasks is "given as prior knowledge," yet Section 1 highlights that real-world demonstrations are heterogeneous and complex.
- Why unresolved: The experiments utilize domains where the subtask set is perfectly aligned with the expert's behavior; sensitivity to this input remains untested.
- Evidence would resolve it: An ablation study measuring performance degradation when the provided set X is incomplete, over-specified, or noisy compared to the ground truth.

### Open Question 3
- Question: Does DTIL effectively facilitate human-AI teaming in complex, real-world scenarios like AI-enabled team coaching?
- Basis in paper: Section 9 explicitly motivates future research into applications like "AI-enabled team coaching" and "end-user programming of multi-agent systems."
- Why unresolved: The evaluation relies on synthetic agents and RL benchmarks (SMACv2) rather than human-subject data or noisy real-world team interactions.
- Evidence would resolve it: Empirical validation through human-subject studies demonstrating that DTIL-based agents improve human team alignment or performance in collaborative tasks.

## Limitations

- Scalability to larger subtask spaces: The EM-based inference could become computationally prohibitive with hundreds of subtasks, and the bijection theorem may not scale gracefully.
- Dependence on initialization quality: The EM loop requires either labeled data or a reasonable initial policy; in fully unsupervised settings with sparse or noisy demonstrations, the Viterbi inference step may propagate errors.
- Generalization beyond hierarchical expert policies: If experts use reactive, non-hierarchical strategies, the high-level policy may fail to capture meaningful structure.

## Confidence

- High confidence: Task performance claims (expert-level rewards, outperformance of MA-GAIL/MA-OptionGAIL) - supported by six benchmark domains and direct comparisons.
- Medium confidence: Subtask inference accuracy (75-78%) - validated in controlled domains, but real-world applicability depends on subtask granularity and demonstration quality.
- Low confidence: Semi-supervision benefits - only tested with 20% labeled data; scaling to <10% or verifying robustness across diverse domains is unverified.

## Next Checks

1. Stress-test subtask scalability: Evaluate DTIL on domains with 50+ subtasks to assess EM computation time and inference accuracy degradation.
2. Test non-hierarchical experts: Run DTIL on environments where expert policies are purely reactive (no latent subtasks) to measure performance collapse.
3. Probe semi-supervision limits: Reduce labeled data to 5-10% and measure subtask inference accuracy and task performance to determine minimum supervision threshold.