---
ver: rpa2
title: 'Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods
  for Fairness Exploration'
arxiv_id: '2505.00802'
source_url: https://arxiv.org/abs/2505.00802
tags:
- race
- fairness
- status
- marital
- occupation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of local post-hoc explanation methods
  to detect and interpret algorithmic bias. The authors propose a pipeline that generates
  local explanations for demographic groups and aggregates them to derive fairness-related
  insights.
---

# Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration

## Quick Facts
- arXiv ID: 2505.00802
- Source URL: https://arxiv.org/abs/2505.00802
- Reference count: 40
- Key outcome: Local post-hoc XAI methods can detect and interpret algorithmic bias by generating and aggregating individual explanations across demographic groups.

## Executive Summary
This paper investigates whether local post-hoc explanation methods (LIME, SHAP, DiCE) can serve as bias detectors by correlating procedural fairness (feature attribution disparities) with distributive fairness (outcome disparities). The authors propose a pipeline that generates local explanations for demographic subgroups and aggregates them to derive fairness-related insights. Using datasets like Adult and COMPAS, they demonstrate that explanation methods can detect fairness violations consistent with distributive fairness metrics, reveal how removing protected attributes shifts importance to correlated proxies, and show that aggregation strategy affects fairness interpretation. The study validates AOPC evaluation as a reliability measure for explanation methods while highlighting critical design considerations for XAI in fairness contexts.

## Method Summary
The method uses local post-hoc XAI methods (LIME, SHAP, DiCE) to generate explanations for individual instances from a black-box model, then aggregates these explanations by demographic group to detect procedural fairness violations. The pipeline trains a Random Forest classifier on datasets like Adult and COMPAS, samples instances from specific subgroups (e.g., "Female/TP"), generates local explanations using multiple methods, and aggregates feature contributions using signed mean or counterfactual burden metrics. Distributive fairness is measured through metrics like Positive Rate and True Positive Rate differences, while procedural fairness is assessed through aggregated feature attributions and feature change percentages.

## Key Results
- Distributive fairness violations are detectable via procedural fairness signals when local explanations show opposing feature contribution signs across protected groups.
- Removing protected attributes shifts feature importance to correlated proxies rather than eliminating bias, maintaining the predictive pathway.
- Signed aggregation of local explanations is essential to detect directional bias, while absolute aggregation only reveals feature magnitude.
- AOPC evaluation demonstrates explanation methods are reliable but require careful configuration for fairness applications.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distributive fairness violations are detectable via procedural fairness signals when using local post-hoc explanations.
- **Mechanism:** Local explainers approximate model behavior for specific instances. Aggregated by demographic group, attributions reveal if a protected attribute contributes positively for one group and negatively for another, indicating disparate outcomes.
- **Core assumption:** Local surrogate models faithfully approximate the black-box model's decision boundaries for subpopulations.
- **Evidence anchors:** [abstract] explores relationship between procedural and distributive fairness; [section 4.2] shows sex contributions are consistently negative for females and positive for males; [corpus] weak direct support.
- **Break condition:** If the black-box model uses highly non-linear interactions that local linear surrogates cannot capture, aggregated attributions may fail to reflect true procedural mechanisms.

### Mechanism 2
- **Claim:** Removing protected attributes shifts feature importance to correlated proxies rather than eliminating bias.
- **Mechanism:** Models minimize loss by exploiting predictive signals. If a protected attribute is removed, the model compensates by increasing weight of correlated features, preserving the information pathway.
- **Core assumption:** The dataset contains statistically correlated features that carry sufficient information about the removed protected attribute.
- **Evidence anchors:** [abstract] states removing protected attribute shifts feature importance toward correlated proxies; [section 4.3] shows positive bias shifts from sex to marital status and relationship; [corpus] weak direct support.
- **Break condition:** If the dataset has no strongly correlated proxies, removing the protected attribute may successfully reduce procedural unfairness signals.

### Mechanism 3
- **Claim:** Signed aggregation of local explanations is required to detect the direction of bias.
- **Mechanism:** Summing explanation vectors with signs intact preserves "push" direction toward or away from favorable class. Taking absolute value before summing merges positive and negative pushes into single "importance" metric, hiding oppositional bias.
- **Core assumption:** Explanation method provides directional contributions relative to prediction probability.
- **Evidence anchors:** [section 3] explains preserving signs identifies contribution direction; [section 4.4] shows absolute mean fails to capture opposing directions; [corpus] "ExplainReduce" discusses summarizing local explanations.
- **Break condition:** If explanation method outputs strictly positive importance scores, this mechanism fails as system can only detect "importance" not directional favor.

## Foundational Learning

- **Concept: Local Post-hoc Explainability (LIME/SHAP)**
  - **Why needed here:** The pipeline relies on generating individual instance explanations before aggregation.
  - **Quick check question:** Can you explain why LIME might give different feature importance for two identical instances if neighborhood sampling changes?

- **Concept: Group Fairness Metrics (Distributive)**
  - **Why needed here:** The paper correlates XAI results with metrics like Demographic Parity and Equalized Odds.
  - **Quick check question:** If a model has perfect Demographic Parity but violates Equalized Odds, what kind of error disparity exists between the groups?

- **Concept: Proxy Variables & Indirect Discrimination**
  - **Why needed here:** RQ2 investigates "indirect discrimination."
  - **Quick check question:** If "Zip Code" is removed from a loan model to prevent location bias, but "Income" and "Occupation" remain, could location bias persist? Why?

## Architecture Onboarding

- **Component map:** Black-box Model -> Subgroup Sampler -> Explainer Engine -> Aggregator -> Disparity Analyzer
- **Critical path:**
  1. Run the black-box model to get predictions
  2. Identify specific subsets (P, TP, FP) per group
  3. Generate local explanations for these subsets
  4. Aggregate using signed mean or counterfactual burden
- **Design tradeoffs:**
  - LIME vs. SHAP: LIME is faster but noisier; SHAP is theoretically grounded but computationally heavier
  - DiCE vs. Feature Attribution: DiCE identifies "what needs to change" (recourse); SHAP/LIME identifies "what caused this" (attribution)
  - Aggregation: Absolute mean vs. Signed mean; use signed for bias direction, absolute for general feature relevance
- **Failure signatures:**
  - Proxy masking: You remove "Gender," accuracy stays same, fairness metrics improve slightly, but "Marital Status" contribution spikes
  - Opposing signs hidden: Aggregated absolute values look equal for both groups, but signed values show opposing directions
  - Explanation instability: LIME explanations fluctuate significantly for same instance due to random sampling
- **First 3 experiments:**
  1. Baseline Attribution Disparity: Train on Adult with "Sex" included, generate SHAP values for "Positive" class, compare mean signed contribution of "Sex" for Males vs. Females
  2. Proxy Detection (Ablation): Retrain excluding "Sex," re-run SHAP, verify if "Relationship" or "Marital Status" contribution increases for Female group
  3. Burden Check (DiCE): Use DiCE to generate counterfactuals for "False Negatives," calculate Euclidean distance (Burden) for Females vs. Males

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most appropriate method for aggregating local post-hoc explanations into global fairness insights without obscuring directional biases?
- Basis: The authors find different aggregation methods yield varying interpretations, with absolute methods potentially hiding oppositional biases.
- Why unresolved: The paper demonstrates choice of aggregation significantly impacts visibility of disparities but does not conclude which method provides ground truth for fairness assessment.

### Open Question 2
- Question: How can the fairness of the explanation methods themselves be evaluated to ensure they do not inherit biases from underlying data or models?
- Basis: The conclusion identifies evaluating the fairness of explanations themselves as necessary future work.
- Why unresolved: The current study assumes explanation method is a neutral observer, treating its output as a tool for detecting model bias rather than a potential source of bias.

### Open Question 3
- Question: How does removal of a protected attribute influence the trade-off between eliminating direct discrimination and exacerbating indirect discrimination via proxy features?
- Basis: The study finds removing protected attributes shifts feature importance to correlated proxies, often reinforcing advantages for non-protected groups.
- Why unresolved: While the paper observes shift in feature contributions, it leaves open question of managing resulting "induced discrimination" or identifying optimal feature inclusion point.

### Open Question 4
- Question: How do users perceive procedural unfairness when presented with local post-hoc explanations compared to standard fairness metrics?
- Basis: The authors state conducting a user study would be valuable for assessing procedural unfairness from users' perspective.
- Why unresolved: The paper relies on statistical correlation between distributive metrics and feature attribution but lacks empirical data on whether mathematical indicators align with human notions of "fair process."

## Limitations

- The study relies heavily on the assumption that local surrogate models accurately capture black-box decision boundaries, which may not hold for highly non-linear models.
- Empirical validation across diverse datasets is limited, with proxy detection mechanisms only partially supported by existing literature.
- The paper does not address potential explanations for bias that are orthogonal to feature attribution, such as model architecture choices or training data distribution shifts.

## Confidence

- **High Confidence:** The mechanism linking signed aggregation to bias direction detection is well-supported by direct evidence in results section.
- **Medium Confidence:** The proxy effect mechanism is logically sound and partially supported but lacks direct validation studies in corpus.
- **Low Confidence:** The paper does not provide sufficient empirical evidence to assess the claim about AOPC evaluation being comprehensive measure of explanation method trustworthiness.

## Next Checks

1. **Faithfulness Validation:** Compare LIME/SHAP feature attributions against ground-truth feature importance from white-box logistic regression model trained on same data to quantify approximation error.

2. **Proxy Robustness Test:** Systematically introduce synthetic proxy features correlated with protected attributes at varying correlation strengths and measure how feature attributions redistribute across spectrum.

3. **Cross-Architecture Generalization:** Repeat entire pipeline (explanation generation, aggregation, fairness analysis) using deep neural network instead of Random Forest to test whether observed patterns hold across model families.