---
ver: rpa2
title: 'Beyond Tokens in Language Models: Interpreting Activations through Text Genre
  Chunks'
arxiv_id: '2511.16540'
source_url: https://arxiv.org/abs/2511.16540
tags:
- text
- arxiv
- language
- dataset
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a method for interpreting large language
  model (LLM) activations by predicting text genres from model activations, moving
  beyond single-token analysis. The authors create two datasets: a synthetic dataset
  with 3,914 labeled text chunks across five categories (instructional, explanatory,
  speech, narrative, code), and the CORE dataset with pre-existing classifications.'
---

# Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks

## Quick Facts
- arXiv ID: 2511.16540
- Source URL: https://arxiv.org/abs/2511.16540
- Authors: Éloïse Benito-Rodriguez; Einar Urdshals; Jasmina Nasufi; Nicky Pochinkov
- Reference count: 40
- Key outcome: Method achieves up to 98% F1-score on synthetic dataset and 71% on CORE dataset for predicting text genres from LLM activations

## Executive Summary
This paper introduces a novel method for interpreting large language model (LLM) activations by predicting text genres from model activations, moving beyond single-token analysis. The authors create synthetic and real datasets with labeled text chunks across multiple genres, then extract activations from Mistral-7B at each layer. Using shallow learning classifiers, they demonstrate that genre information becomes increasingly separable in deeper layers, achieving high accuracy while outperforming control tasks with randomized parameters. This provides a proof of concept for genre inference from activations and lays groundwork for understanding LLM outputs at multi-token scales.

## Method Summary
The method involves creating labeled datasets of text chunks (paragraph-level segments split on double newlines) across genre categories, then extracting model activations from Mistral-7B's attention and MLP layers. Activations are mean-pooled across tokens within each chunk to create fixed-size representations, which are then fed to shallow scikit-learn classifiers (like LogisticRegression and SVC with StandardScaler). A control task using a model with randomized parameters validates that probe performance reflects genuine feature representations rather than spurious correlations. The approach is tested on both synthetic datasets (with clean, unambiguous labels achieving 98% F1) and the CORE dataset (real-world data achieving 71% F1).

## Key Results
- Genre prediction achieves F1-scores up to 98% on synthetic dataset and 71% on CORE dataset
- Performance improves in deeper transformer layers, showing layer-dependent semantic abstraction
- Control tasks with randomized parameters confirm probe validity, outperforming random baselines
- Dimensionality reduction (PHATE) shows clearer genre clusters in synthetic data versus overlapping categories in CORE

## Why This Works (Mechanism)

### Mechanism 1: Layer-dependent semantic abstraction for genre
- Claim: Text genre information becomes more linearly separable in deeper transformer layers
- Mechanism: Early layers encode local token-level features; later layers aggregate these into higher-level semantic abstractions (e.g., "instructional" vs "narrative" patterns) that persist across multi-token sequences
- Core assumption: Genre-relevant features are sufficiently distinct from other semantic features to form separable clusters in activation space
- Evidence anchors: [abstract] "genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers"; [section 4.2] "The deeper we go in the model, the higher the performances"
- Break condition: If genres share overlapping syntactic/semantic structures (e.g., "speech" vs "narrative"), probe accuracy degrades significantly—seen in CORE dataset's lower 71% score

### Mechanism 2: Control-task validation of probe legitimacy
- Claim: Shallow probes can distinguish genuine feature representations from spurious statistical artifacts
- Mechanism: Comparing probe performance on real model activations vs. activations from a model with randomized parameters isolates whether the probe is exploiting learned structure vs. incidental correlations
- Core assumption: Random-parameter models do not encode structured genre information; any signal above this baseline reflects genuine representation
- Evidence anchors: [section 3.2] "If the probes on the original Mistral-7B show better performance than the control task, this will mean that our probes do not rely on spurious correlation"; [section 5] "The performance of our probe method is superior to the control task, revealing the presence of the representation that we are searching for"
- Break condition: If probe architecture is overly expressive, it may overfit even to random activations—necessitating shallow/linear probes

### Mechanism 3: Mean-pooled chunk aggregation preserves genre signal
- Claim: Averaging activations across tokens in a text chunk retains genre-distinguishing information while reducing token-level noise
- Mechanism: Genre is a multi-token property; individual token activations vary, but the mean across a paragraph-sized chunk filters out idiosyncratic token features and retains the shared semantic "direction" associated with the genre
- Core assumption: Genre features are distributed relatively uniformly across chunk tokens, or at least not canceled out by averaging
- Evidence anchors: [section 3.2] "The means of activations aj_i serve as input for the probe models we train to predict the correct category"; [section 3] "chunks are analogous to paragraphs, and are simply split by double newlines"
- Break condition: If chunks are too short (few tokens) or mix genres, mean pooling dilutes signal—paper excludes mislabeled/mixed chunks but does not quantify boundary effects

## Foundational Learning

- Concept: **Residual stream and layer activations in transformers**
  - Why needed here: The method extracts activations from attention and MLP layers' outputs; understanding where genre information accumulates requires knowing the residual stream's role
  - Quick check question: Can you explain why later layers might encode more abstract features than earlier layers?

- Concept: **Probing classifiers and their validity pitfalls**
  - Why needed here: High probe accuracy does not guarantee the model "uses" those features; control tasks address this concern
  - Quick check question: What is the risk of using an overly complex probe, and how does a control task mitigate it?

- Concept: **Mean pooling for sequence-level representations**
  - Why needed here: The paper reduces variable-length chunk activations to fixed-size vectors via averaging
  - Quick check question: What types of features might be preserved or lost when averaging token embeddings across a paragraph?

## Architecture Onboarding

- Component map:
  - Input: Text chunks (paragraph-level, split on `\n\n`)
  - Model: Mistral-7B (or similar open-weights LLM)
  - Extraction: Hook into attention + MLP layer outputs and residual stream at each layer
  - Aggregation: Compute mean activation vector per chunk per layer
  - Probe: Scikit-learn classifiers (e.g., LogisticRegression, SVC) with StandardScaler
  - Control: Same pipeline on model with randomized parameters

- Critical path:
  1. Generate/obtain labeled chunk dataset (ensure clean genre boundaries)
  2. Extract activations from target layers (focus on mid-to-late layers per results)
  3. Apply control-task validation before interpreting probe success

- Design tradeoffs:
  - **Synthetic vs. real datasets**: Synthetic (clean labels, high F1) vs. CORE (noisier, lower F1 but more ecologically valid)
  - **Layer selection**: Earlier layers = faster extraction, less signal; later layers = more signal, potentially more redundancy
  - **Probe complexity**: Shallow probes generalize better but may underfit subtle distinctions

- Failure signatures:
  - Probe accuracy near random baseline → genre not represented or extraction/aggregation broken
  - Probe accuracy high on train, low on test → overfitting; reduce probe complexity or increase data
  - No layer-wise performance gradient → extraction error or model not processing input correctly

- First 3 experiments:
  1. Reproduce the synthetic-dataset result on Mistral-7B using the paper's GitHub code; confirm layer-wise F1 curve matches Figure 5
  2. Swap mean pooling for max pooling or attention-weighted pooling; compare F1 to assess aggregation sensitivity
  3. Test probe transfer: train on synthetic chunks, evaluate on CORE chunks to measure dataset-specificity of learned genre directions

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic dataset bias: 98% F1 score may overstate method's general applicability due to clean, unambiguous synthetic texts versus noisier real-world data
- Chunk size assumptions: Method assumes paragraph-sized chunks are optimal without systematic testing of boundary effects or hierarchical genre structures
- Probe expressiveness concerns: Paper doesn't explore whether shallow probes might still be too complex to serve as reliable control baselines

## Confidence

**High Confidence**: The core methodology of extracting activations, mean-pooling over chunks, and using shallow probes with control tasks is technically sound and well-supported by results. The layer-wise performance gradient (better performance in deeper layers) is consistently observed.

**Medium Confidence**: The interpretation that LLM activations encode high-level text structures in "identifiable ways" is supported but limited. The synthetic dataset results are robust, but CORE dataset results (71% F1) indicate significant limitations when applied to real-world data.

**Low Confidence**: Claims about the specific mechanism by which genres are represented (e.g., "genre-relevant features form separable clusters") lack direct evidence. The paper demonstrates that genre can be predicted from activations, but does not prove that the model explicitly represents genre as a distinct feature rather than as correlated patterns of other semantic features.

## Next Checks

1. **Cross-dataset Transferability Test**: Train probes on synthetic dataset, then evaluate on CORE dataset (and vice versa). Measure whether genre representations learned from clean synthetic data transfer to real-world texts, or if the method requires dataset-specific training.

2. **Chunk Boundary Sensitivity Analysis**: Systematically vary chunk sizes (sentence-level, paragraph-level, multi-paragraph) and chunk boundary definitions (split on newlines, fixed token counts, semantic boundaries). Compare F1 scores to determine optimal chunk configuration for genre extraction.

3. **Ablation on Probe Architecture**: Replace shallow probes with linear probes only, and test whether control task still shows significant performance gap. Also test whether random baselines can be beaten by chance with different pooling strategies (max pooling, attention-weighted pooling) to assess aggregation method sensitivity.