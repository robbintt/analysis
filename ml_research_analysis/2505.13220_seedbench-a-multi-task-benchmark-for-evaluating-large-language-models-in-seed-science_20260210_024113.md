---
ver: rpa2
title: 'SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in
  Seed Science'
arxiv_id: '2505.13220'
source_url: https://arxiv.org/abs/2505.13220
tags:
- breeding
- gene
- llms
- tasks
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SeedBench is the first multi-task benchmark for evaluating large
  language models in seed science, covering gene information retrieval, gene function
  and regulation, and variety breeding with 2,264 expert-validated questions across
  11 task types. The evaluation of 26 leading LLMs (proprietary, open-source, and
  domain-specific) reveals that while DeepSeek-V3-671B (63.3) and GPT-4 (62.06) achieve
  the highest overall scores, substantial gaps remain between LLM capabilities and
  real-world breeding challenges.
---

# SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science

## Quick Facts
- arXiv ID: 2505.13220
- Source URL: https://arxiv.org/abs/2505.13220
- Reference count: 15
- First multi-task benchmark for evaluating LLMs in seed science with 2,264 expert-validated questions across 11 task types

## Executive Summary
SeedBench introduces the first comprehensive multi-task benchmark for evaluating large language models in seed science, addressing critical gaps in assessing AI capabilities for agricultural breeding applications. The benchmark covers gene information retrieval, gene function and regulation, and variety breeding through 2,264 expert-validated questions. Evaluation of 26 leading LLMs reveals that while top models like DeepSeek-V3-671B and GPT-4 achieve the highest scores, substantial gaps remain between current AI capabilities and practical breeding requirements. Notably, domain-specific fine-tuned models underperform general models, and optimal performance is observed with 7B-14B parameter models, challenging conventional assumptions about model scaling in specialized domains.

## Method Summary
The SeedBench benchmark was constructed through systematic development of 2,264 expert-validated questions spanning 11 task types across three core seed science domains: gene information retrieval, gene function and regulation, and variety breeding. The evaluation protocol assessed 26 leading large language models including proprietary models (GPT-4, DeepSeek-V3-671B), open-source models, and domain-specific fine-tuned models. Performance metrics were calculated across all task types to generate comprehensive scores, with particular attention to identifying gaps between model capabilities and real-world breeding challenges. The benchmark specifically examined model performance across different parameter scales and fine-tuning approaches.

## Key Results
- DeepSeek-V3-671B achieved the highest overall score (63.3) followed by GPT-4 (62.06) among 26 evaluated models
- Domain-specific fine-tuned models significantly underperformed general-purpose models, contrary to expectations
- Optimal performance was observed with 7B-14B parameter models, challenging assumptions about model scaling
- Substantial gaps remain between current LLM capabilities and practical seed science breeding requirements

## Why This Works (Mechanism)
Assumption: The benchmark's effectiveness stems from its comprehensive coverage of interconnected seed science tasks that mirror real-world breeding workflows. The 11 task types are designed to progressively test model capabilities from basic information retrieval to complex functional analysis and breeding decision-making. Expert validation ensures questions reflect actual challenges faced by seed scientists, creating a realistic assessment framework. The multi-task structure reveals how well models can integrate knowledge across different aspects of seed science rather than excelling at isolated subtasks.

## Foundational Learning
- Seed Science Domain Knowledge: Understanding genetic variation, breeding methodologies, and plant biology fundamentals
- Multi-task Benchmarking: Framework for evaluating AI models across diverse, interconnected tasks
- Expert Validation Protocols: Methods for ensuring benchmark question quality and relevance
- Performance Gap Analysis: Techniques for identifying and quantifying limitations between AI capabilities and domain requirements

## Architecture Onboarding
Component Map: SeedBench -> Question Generation -> Expert Validation -> Model Evaluation -> Performance Analysis
Critical Path: Question development → Expert validation → Model testing → Gap analysis
Design Tradeoffs: Comprehensive coverage vs. benchmark complexity, general vs. domain-specific models, parameter scale optimization
Failure Signatures: Domain-specific models underperforming, multimodal integration limitations, explainability gaps
First Experiments: 1) Compare 7B vs 14B parameter performance across all task types, 2) Analyze failure modes of domain-specific fine-tuned models, 3) Test explainability capabilities on gene function questions

## Open Questions the Paper Calls Out
- Why do domain-specific fine-tuned models consistently underperform general-purpose models in seed science tasks?
- What specific knowledge gaps prevent current LLMs from meeting practical breeding requirements?
- How can the benchmark be expanded to evaluate multimodal capabilities for seed science applications?
- What are the optimal training strategies for developing effective domain-specific models in agricultural sciences?

## Limitations
- Current LLMs, including top performers, fall significantly short of real-world seed science requirements
- Domain-specific fine-tuned models underperform general models, suggesting current fine-tuning approaches may be ineffective
- Benchmark focuses on English-language seed science, limiting generalizability to other languages and regional contexts
- Limited evaluation of multimodal capabilities despite the visual nature of many seed science tasks

## Confidence
High Confidence: Benchmark construction methodology and expert validation process
Medium Confidence: Comparative performance rankings and model family comparisons
Medium Confidence: Observed optimal performance at 7B-14B parameter scale

## Next Checks
1. Conduct ablation studies on seed science fine-tuning datasets to identify specific knowledge gaps
2. Expand benchmark to include non-English seed science literature and regional agricultural practices
3. Implement human expert evaluation to validate correlation between automated metrics and practical breeding expertise
4. Test multimodal model performance on visual seed science tasks currently excluded from the benchmark