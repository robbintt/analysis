---
ver: rpa2
title: 'LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial
  Layout Planning'
arxiv_id: '2509.22720'
source_url: https://arxiv.org/abs/2509.22720
tags:
- scene
- spatial
- object
- diffusion
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LayoutAgent integrates vision-language reasoning with compositional
  diffusion to plan realistic multi-object scenes. Given object images, a VLM agent
  segments, estimates sizes, builds scene graphs, and refines prompts.
---

# LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning

## Quick Facts
- arXiv ID: 2509.22720
- Source URL: https://arxiv.org/abs/2509.22720
- Authors: Zezhong Fan; Xiaohan Li; Luyi Ma; Kai Zhao; Liang Peng; Topojoy Biswas; Evren Korpeoglu; Kaushiki Nag; Kannan Achan
- Reference count: 33
- Primary result: Integrates VLM-guided scene graphs with compositional diffusion to plan realistic multi-object layouts, outperforming baselines on coherence, realism, and aesthetics.

## Executive Summary
LayoutAgent is a vision-language agent that combines VLM reasoning with compositional diffusion to plan spatial layouts for multi-object scenes. Given a set of object images, the agent uses a VLM to segment objects, estimate their sizes, and construct a scene graph encoding spatial relationships. This scene graph is used to condition a compositional diffusion model, which generates spatially coherent bounding boxes for each object. A foreground-conditioned generator then renders the final layout. The method achieves state-of-the-art results on layout coherence, spatial realism, and aesthetic alignment, validated through CLIP/BLIP scores, conflict metrics, and VLM evaluations.

## Method Summary
LayoutAgent uses a two-stage approach: (1) VLM-guided scene understanding and (2) compositional diffusion-based layout generation. First, the VLM segments objects from input images, estimates their bounding box sizes, and builds a scene graph representing spatial relationships. This graph is converted into a textual prompt and refined for clarity. Second, a compositional diffusion model generates bounding boxes conditioned on the scene graph, ensuring spatial coherence. Finally, a foreground-conditioned generator renders the final layout. The method is trained end-to-end, with ablation studies confirming the benefits of scene graphs and diffusion models.

## Key Results
- Outperforms strong baselines on layout coherence, spatial realism, and aesthetic alignment (higher CLIP/BLIP scores, lower conflicts).
- Scene graphs and compositional diffusion significantly improve layout quality and robustness.
- Generalizes well to novel scenes and configurations with more objects than seen during training.

## Why This Works (Mechanism)
LayoutAgent leverages VLM reasoning to understand object semantics and spatial relationships, translating them into structured scene graphs. These graphs guide a compositional diffusion model to generate realistic, non-overlapping bounding boxes. The foreground-conditioned generator ensures the final layout respects object appearances and spatial arrangements. This integration of symbolic (scene graphs) and neural (diffusion) methods enables robust, scalable layout planning.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Needed for object segmentation, size estimation, and scene understanding. Quick check: Verify VLM accuracy on object detection and size estimation tasks.
- **Scene Graphs**: Represent spatial relationships between objects. Quick check: Ensure scene graph construction captures all relevant spatial relations (e.g., "left of," "above").
- **Compositional Diffusion Models**: Generate coherent layouts by conditioning on structured prompts. Quick check: Validate that diffusion models produce non-overlapping, spatially plausible bounding boxes.
- **Foreground-Conditioned Generation**: Ensures final layouts respect object appearances. Quick check: Confirm that generated layouts align with input object images.

## Architecture Onboarding

### Component Map
VLM (segmentation + size estimation) -> Scene Graph Construction -> Prompt Refinement -> Compositional Diffusion (bounding box generation) -> Foreground-Conditioned Generator (final layout)

### Critical Path
1. VLM processes input object images to extract semantic and spatial information.
2. Scene graph is constructed and converted to a textual prompt.
3. Compositional diffusion model generates bounding boxes conditioned on the prompt.
4. Foreground-conditioned generator renders the final layout.

### Design Tradeoffs
- **VLM vs. Rule-Based Segmentation**: VLMs offer semantic understanding but may be slower; rule-based methods are faster but less flexible.
- **Scene Graph vs. Flat Representation**: Scene graphs capture spatial relationships but add complexity; flat representations are simpler but less expressive.
- **Compositional Diffusion vs. Single-Step Generation**: Compositional diffusion ensures coherence but is computationally heavier; single-step methods are faster but less reliable.

### Failure Signatures
- **VLM Errors**: Inaccurate segmentation or size estimation leads to poor scene graphs and layouts.
- **Scene Graph Issues**: Missing or incorrect spatial relations result in unrealistic layouts.
- **Diffusion Model Failures**: Overlapping or incoherent bounding boxes indicate model instability or poor conditioning.

### First Experiments to Run
1. Test VLM segmentation and size estimation accuracy on a held-out dataset.
2. Validate scene graph construction by checking spatial relation accuracy.
3. Evaluate compositional diffusion model outputs for bounding box coherence and non-overlap.

## Open Questions the Paper Calls Out
None explicitly stated.

## Limitations
- VLM inaccuracies in segmentation or size estimation may propagate errors to the final layout.
- Scene graph construction assumes fixed spatial relationships, limiting generalization to novel arrangements.
- Evaluation relies on CLIP/BLIP scores, which may not fully capture human perceptual judgments.

## Confidence
- **High Confidence**: LayoutAgent's performance improvements over baselines are well-supported by quantitative results.
- **Medium Confidence**: Benefits of scene graphs and compositional diffusion are demonstrated, but impact in highly complex scenes is less certain.
- **Low Confidence**: VLM agent robustness in diverse or edge-case scenarios is not fully validated.

## Next Checks
1. Test LayoutAgent on scenes with significantly more objects (e.g., >10) or highly cluttered arrangements to assess scalability and robustness.
2. Conduct human perceptual studies to validate aesthetic and spatial realism claims beyond automated metrics.
3. Evaluate performance on rare or underrepresented object categories to ensure generalization beyond training data distribution.