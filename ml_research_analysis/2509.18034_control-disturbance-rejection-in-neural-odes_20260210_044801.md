---
ver: rpa2
title: Control Disturbance Rejection in Neural ODEs
arxiv_id: '2509.18034'
source_url: https://arxiv.org/abs/2509.18034
tags:
- control
- robust
- have
- algorithm
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an iterative training algorithm for Neural
  ODEs that provides models resilient to control (parameter) disturbances. The method
  builds on Tuning without Forgetting and introduces training points sequentially,
  updating parameters on new data within the space of parameters that do not decrease
  performance on previously learned training points.
---

# Control Disturbance Rejection in Neural ODEs

## Quick Facts
- arXiv ID: 2509.18034
- Source URL: https://arxiv.org/abs/2509.18034
- Authors: Erkan Bayram; Mohamed-Ali Belabbas; Tamer Başar
- Reference count: 14
- Primary result: Robust Neural ODE maintains classification accuracy around 0.96 up to disturbance magnitude 0.1, while standard Neural ODE degrades rapidly beyond 0.07

## Executive Summary
This paper proposes an iterative training algorithm for Neural ODEs that provides models resilient to control (parameter) disturbances. The method builds on Tuning without Forgetting and introduces training points sequentially, updating parameters on new data within the space of parameters that do not decrease performance on previously learned training points. Inspired by flat minima, the algorithm solves a minimax problem for a non-convex non-concave functional over an infinite-dimensional control space. A projected gradient descent algorithm is developed on the space of parameters that admits the structure of an infinite-dimensional Banach subspace. Through simulations on a classification task, the formulation enables the model to effectively learn new data points and gain robustness against control disturbance.

## Method Summary
The method trains Neural ODEs sequentially on data points by projecting gradient updates onto the kernel of previously learned trajectories. For each new point, it computes an adversarial disturbance that maximizes the cost function, then updates parameters in the null space of the linearized operators for all previous points. This ensures the model learns new data without catastrophic forgetting while simultaneously flattening the loss landscape around the parameters to enhance robustness to perturbations. The algorithm uses a minimax optimization where the inner maximization finds the worst-case disturbance and the outer minimization updates parameters using projected gradient descent.

## Key Results
- Robust Neural ODE maintains classification accuracy around 0.96 up to disturbance magnitude 0.1
- Standard Neural ODE degrades rapidly beyond disturbance magnitude 0.07
- Method successfully learns new data points sequentially without catastrophic forgetting
- Closed-form solution for worst-case disturbance is tractable due to finite-rank structure of linearized operators

## Why This Works (Mechanism)

### Mechanism 1
The infinite-dimensional minimax optimization is tractable because the inner maximization effectively resides in a finite-dimensional subspace. The linear operator mapping control variations to endpoint variations has finite rank, so there exists a compact finite-dimensional subspace that captures the exact image of the disturbance set. This allows solving the inner maximization using calculus of variations to derive a closed-form solution for the worst-case disturbance. The mechanism relies on first-order approximations valid only for small perturbations.

### Mechanism 2
The model learns new data points sequentially without catastrophic forgetting by projecting gradient updates onto the kernel of previously learned trajectories. To learn a new point while preserving outputs for previous points, the control update is restricted to the intersection of kernels of linearized operators for all previous points. This ensures updates don't alter endpoint mappings of previously memorized data to first order. The mechanism requires that the intersection of these kernels forms a non-trivial Banach submanifold.

### Mechanism 3
Robustness to parameter noise is achieved by evaluating the cost gradient at an adversarially perturbed control point. Instead of standard gradient descent, the algorithm calculates the worst-case disturbance for the current control and computes the gradient specifically at the perturbed point. This forces the optimization to flatten the loss landscape around nominal parameters, as the descent direction must work even at the worst nearby point. The regularization parameter must be large enough to ensure concavity of the inner maximization problem.

## Foundational Learning

- **Neural Ordinary Differential Equations (Neural ODEs)**: The substrate where the network is a dynamical system ẋ = f(x, u) with continuous time depth. Understanding how integration time affects state compared to ResNet layers is essential.

- **Calculus of Variations & Optimal Control**: Required to solve for worst-case disturbance by finding extremum of functional. Understanding first variation and Euler-Lagrange equations is necessary to follow proofs.

- **Continual Learning / Catastrophic Forgetting**: The algorithm processes data sequentially. Understanding why standard gradient descent overwrites previous knowledge clarifies why complex kernel projection is necessary.

## Architecture Onboarding

- **Component map**: Control u(t) -> Flow φ -> Linearized Operator L -> Adversarial Disturbance ε* -> Projection Matrix N(L)

- **Critical path**:
  1. Discretize control u
  2. Forward Pass: Integrate ODE to get trajectories
  3. Linearize: Compute L_i matrices for all memorized points
  4. Maximize (Inner): Compute ε* using closed-form solution
  5. Project (Outer): Project gradient at u+ε* onto null-space of stacked L
  6. Update: u ← u - α · proj_grad

- **Design tradeoffs**:
  - Memory vs. Plasticity: Kernel size shrinks as more points are memorized, eventually limiting learning
  - Robustness (ρ) vs. Performance: Large disturbance bound increases robustness but may degrade clean data accuracy
  - Discretization Resolution: Coarse time steps speed computation but may violate continuous-time assumptions

- **Failure signatures**:
  - Rigidity Lock: Gradients vanish when kernel intersection becomes empty or numerically collapsed
  - Divergent ε*: Norm explodes if regularization parameter is too small
  - Numerical Drift: First-order preservation drifts over iterations, requiring re-linearization

- **First 3 experiments**:
  1. Sanity Check - Overparameterization: Train on tiny dataset (2-3 points), verify ε* ≈ 0 and J=0
  2. Ablation - Projection vs. Standard: Compare Robust nODE against standard Neural ODE under weight noise
  3. Hyperparameter Sensitivity: Sweep regularization parameter and disturbance bound to identify stability thresholds

## Open Questions the Paper Calls Out

### Open Question 1
Can computational complexity of kernel projection be reduced to scale linearly with dataset size, given that constraint matrix grows linearly with training points? The paper demonstrates on 1000 points but lacks complexity analysis for large-scale applications.

### Open Question 2
Under what geometric conditions does the transversality assumption fail, and does failure lead to convergence to local minimum or divergent control? The assumption is difficult to verify empirically and consequences of violation are not analyzed.

### Open Question 3
Is there a constructive bound or adaptive schedule for regularization coefficient to ensure second-order sufficient condition holds during entire training? The paper uses λ₁=0.2 experimentally but provides no general method for determining this value.

### Open Question 4
How does sequential ordering of training points influence geometry of final flat minimum and resulting robustness? The algorithm's order-dependent solution path may create "dead-end" corridors limiting achievable robustness for subsequent points.

## Limitations
- The infinite-dimensional minimax formulation relies on strong linearity assumptions that may break for large disturbances
- Sequential training may eventually reach a rigidity point where no valid gradient updates exist
- Method's performance depends heavily on spectral properties of linearized operator, which are not characterized
- Computational complexity grows with number of memorized points due to increasing kernel projection dimensions

## Confidence
- **High confidence**: Classification results showing robustness degradation (0.96 accuracy up to ρ=0.1 vs rapid degradation at ρ=0.07 for standard ODE)
- **Medium confidence**: Theoretical mechanism of kernel projection preventing catastrophic forgetting, though practical failure modes exist
- **Medium confidence**: Closed-form solution for ε* is tractable due to finite-rank structure, but numerical stability depends on λ₁ selection

## Next Checks
1. **Rigidity threshold test**: Systematically increase training points until kernel projection yields zero-gradient updates, identifying practical memory limit
2. **Linearity validation**: Compare ε* computed via closed-form solution against numerical maximization for increasing ρ values to quantify linearization error
3. **Cross-task interference**: Train on two semantically different binary classification tasks sequentially and measure performance on both to test true continual learning capability