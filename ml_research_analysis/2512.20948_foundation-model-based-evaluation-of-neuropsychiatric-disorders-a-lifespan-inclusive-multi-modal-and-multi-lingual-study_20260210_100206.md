---
ver: rpa2
title: 'Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive,
  Multi-Modal, and Multi-Lingual Study'
arxiv_id: '2512.20948'
source_url: https://arxiv.org/abs/2512.20948
tags:
- speech
- detection
- multi-modal
- fusion
- depression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FEND, a foundation model-based evaluation\
  \ framework for detecting neuropsychiatric disorders such as Alzheimer\u2019s disease\
  \ (AD), depression, and autism spectrum disorder (ASD). The framework integrates\
  \ speech and text modalities using pre-trained foundation models (e.g., WavLM for\
  \ speech, E5 for text) and evaluates their performance across 13 multi-lingual datasets\
  \ spanning English, Chinese, Greek, French, and Dutch."
---

# Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study

## Quick Facts
- arXiv ID: 2512.20948
- Source URL: https://arxiv.org/abs/2512.20948
- Reference count: 40
- This paper introduces FEND, a foundation model-based evaluation framework for detecting neuropsychiatric disorders such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD).

## Executive Summary
This paper introduces FEND, a foundation model-based evaluation framework for detecting neuropsychiatric disorders such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD). The framework integrates speech and text modalities using pre-trained foundation models (e.g., WavLM for speech, E5 for text) and evaluates their performance across 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch. FEND employs both mono-modal and multi-modal fusion approaches, using classical fusion methods such as attention, tensor fusion, and memory fusion networks. Key findings include: multi-modal fusion improves AD and depression detection but underperforms in ASD due to dataset heterogeneity; modality imbalance is prevalent, with fusion failing to surpass the best mono-modal models; cross-corpus experiments show robust performance in consistent task and language settings but degradation in multi-lingual and task-heterogeneous scenarios. WavLM-Large excels in speech tasks, while E5-Large and multilingual models like mE5-L are effective for text. The study highlights the importance of language-specific models and tailored fusion strategies for different disorders. FEND provides extensive benchmarks and insights into performance-influencing factors, advancing automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment.

## Method Summary
The FEND framework uses frozen foundation models for feature extraction: WavLM-Large for speech representations and E5-Large (or mE5-Large for multilingual tasks) for text. Features are mapped to 128 dimensions and sequences are downsampled to 200 timesteps. Mono-modal classification uses a 3-layer MLP (256 hidden units, ReLU, 0.2 dropout), while multi-modal fusion employs 8 different methods including attention, tensor fusion, and memory fusion networks. The framework is evaluated on 13 datasets across three disorders (AD, depression, ASD) in five languages, using weighted accuracy, unweighted accuracy, and weighted F1-score as metrics. Training uses 80 epochs with learning rate 1e-3 and weight decay 1e-5.

## Key Results
- Multi-modal fusion achieves up to 93.1% F1-score for AD detection using attention-based methods, outperforming mono-modal approaches
- Modality imbalance is prevalent, with fusion failing to surpass the best mono-modal models on several datasets including EATD, MODMA, and ASDBank
- Cross-lingual experiments show robust performance in consistent task and language settings, but degradation in multi-lingual and task-heterogeneous scenarios
- WavLM-Large excels in speech tasks, while E5-Large and multilingual models like mE5-L are effective for text

## Why This Works (Mechanism)

### Mechanism 1
Multi-modal fusion of acoustic and linguistic representations improves detection performance for AD and depression when modalities provide complementary information. WavLM extracts speech representations capturing prosodic abnormalities (slowed rate, pitch patterns), while E5 captures semantic/syntactic patterns (lexical diversity, negative sentiment). Attention-based fusion dynamically weights these complementary cues. Core assumption: disorders manifest in both acoustic and linguistic dimensions, and fusion strategies can learn optimal combination weights. Evidence: multi-modal fusion excels in AD and depression detection, with attention-based methods achieving up to 93.1% F1-score; Attention fusion achieves 93.1% WF1 on D-VLOG, surpassing best mono-modal (E5-Large: 92.6%). Break condition: ASD detection fails to benefit from fusion due to speech modality dominance and dataset heterogeneity; modality imbalance causes fusion to underperform best mono-modal baseline.

### Mechanism 2
Large-scale pre-trained foundation models transfer domain-agnostic representations to neuropsychiatric detection without task-specific fine-tuning. Frozen WavLM (trained on 94k+ hours speech) and E5 (trained on large text corpora) provide high-quality embeddings that capture paralinguistic and semantic patterns relevant to disorders. Core assumption: pre-training objectives (masked prediction, contrastive learning) learn representations sufficiently general to capture disorder biomarkers. Evidence: frozen backbone approach with foundation models achieves strong performance; WavLM-Large achieves best speech performance (75.6% avg WF1 for AD), attributed to large-scale pre-training and advanced denoising mechanisms; self-supervised embeddings for depression detection show strong transfer from pre-training. Break condition: cross-lingual transfer fails when model language coverage doesn't match target (E5-Large drops to 61.2% on Greek ADReSS-M vs mE5-L's 72.3%); suggests language-specific pre-training is prerequisite for cross-lingual generalization.

### Mechanism 3
Modality imbalance mitigation techniques (OGM-GE, PMR) improve fusion performance by dynamically adjusting gradient contributions. When speech dominates (richer information), text modality is under-optimized. Gradient modulation rebalances learning, preventing the stronger modality from overwhelming optimization. Core assumption: imbalance stems from unequal information quality/contribution between modalities, not inherent modality uselessness. Evidence: modality imbalance is identified as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models; OGM-GE and PMR improve performance on ADReSS-M, EATD, MODMA, ASDBank, surpassing mono-modal baselines; limited corpus evidence on specific imbalance mitigation for neuropsychiatric tasks. Break condition: mitigation techniques provide partial improvement but don't fully resolve imbalance, especially in severe cross-lingual scenarios where text modality quality is fundamentally degraded.

## Foundational Learning

- **Concept: Self-supervised speech representations (WavLM, HuBERT)**
  - **Why needed here:** Understanding how masked prediction over audio creates embeddings that capture prosody, pitch, fluency patterns relevant to disorders.
  - **Quick check question:** Can you explain why denoising masked speech modeling (WavLM) might capture emotional/clinical cues better than purely ASR-focused pre-training?

- **Concept: Multi-modal fusion strategies (early vs. late, attention-based vs. tensor fusion)**
  - **Why needed here:** The paper benchmarks 8 fusion methods; understanding their trade-offs is essential for selecting appropriate strategies per disorder.
  - **Quick check question:** When would you choose sequence-level fusion (MulT, MFN) over utterance-level fusion (TFN, Attention) for neuropsychiatric detection?

- **Concept: Modality imbalance in multi-modal learning**
  - **Why needed here:** Critical failure mode identified; understanding gradient-based mitigation (OGM-GE) enables diagnosis and repair of underperforming fusion.
  - **Quick check question:** If fusion underperforms the best single modality, what diagnostic steps would you take to determine if modality imbalance is the cause?

## Architecture Onboarding

- **Component map:**
  Input: Raw speech audio → ASR (Whisper/SenseVoice) → Text transcripts
                                      ↓
  Feature Extraction (Frozen):
    Speech → WavLM-Large → 1024-dim sequence
    Text → E5-Large → 1024-dim sequence
                                      ↓
  Projection: 128-dim, downsampled to 200 timesteps
                                      ↓
  Fusion Module: MulT / Attention / TFN / LMF (selectable)
                                      ↓
  Classification Head: 3-layer MLP (256 hidden, ReLU, dropout 0.2)
                                      ↓
  Output: Binary/multi-class prediction

- **Critical path:** Feature extraction quality → Fusion method selection → Modality imbalance diagnosis. The frozen foundation models are the performance bottleneck; fusion gains are marginal if unimodal features are weak.

- **Design tradeoffs:**
  - Frozen vs. fine-tuned backbones: Frozen enables fair comparison but may leave performance on table; fine-tuning could improve but obscures feature quality assessment.
  - Attention fusion (robust across tasks) vs. tensor fusion (strong for AD specifically): No universal best; task-specific selection required.
  - Mono-modal simplicity vs. multi-modal complexity: Fusion adds 2-15% absolute improvement on favorable datasets, but can degrade performance on imbalanced/heterogeneous data.

- **Failure signatures:**
  - Cross-lingual degradation (e.g., English→Greek): Text modality quality collapses; mE5/mGTE required.
  - ASD fusion underperformance: Speech dominates (95.9% WF1 mono-modal), text adds noise; mono-modal preferable.
  - Small dataset overfitting: Multi-modal with many parameters overfits EATD/MODMA; mono-modal generalizes better.

- **First 3 experiments:**
  1. **Mono-modal baseline establishment:** Run WavLM-L + MLP and E5-L + MLP on all 13 datasets with frozen features. Document per-dataset WF1 to identify modality dominance patterns.
  2. **Fusion method ablation:** Compare Attention, MulT, TFN on 3 representative datasets (ADReSS for AD, D-VLOG for depression, ASDBank for ASD). Measure fusion gain over best mono-modal to diagnose imbalance.
  3. **Cross-lingual stress test:** Train on English ADReSS, test on Greek ADReSS-M with E5-L vs. mE5-L. Quantify language mismatch penalty and validate multilingual model necessity.

## Open Questions the Paper Calls Out

### Open Question 1
Can specialized multi-modal fusion methods be developed to handle the unique acoustic-linguistic patterns of neuropsychiatric disorders more effectively than general-purpose emotion recognition algorithms? Basis: Current multi-modal fusion methods often designed for three-modality fusion, which may not fully align with the characteristics of neuropsychiatric diseases. Unresolved because the benchmark shows standard fusion methods frequently fail to surpass mono-modal baselines in ASD detection due to dataset heterogeneity and modality imbalance. Evidence needed: A fusion architecture that consistently outperforms mono-modal baselines across ASD datasets and exceeds the performance of emotion-derived models.

### Open Question 2
How can cross-lingual generalization be improved in multi-modal frameworks to prevent performance degradation when the testing language differs from the training language? Basis: Domain adaptation methods are needed to address the specific cross-lingual challenges identified in datasets like ADReSS-M, noting that English-centric models degrade significantly on non-English data. Unresolved because the results show a drastic performance drop in cross-lingual tasks because text representations lose semantic quality when the model lacks pre-training in the target language. Evidence needed: A domain adaptation strategy that maintains high WF1 scores in cross-lingual test scenarios without requiring extensive target-language fine-tuning data.

### Open Question 3
Do advanced training paradigms, such as end-to-end fine-tuning or parameter-efficient fine-tuning (PEFT), offer a higher performance ceiling than the frozen feature extraction approach used in this study? Basis: Future work should involve investigating advanced training paradigms to explore the upper bounds of performance beyond the frozen-backbone methodology used to ensure fair comparison. Unresolved because the FEND framework freezes foundation model weights to isolate feature quality, potentially leaving performance gains on the table that could be realized by updating the acoustic and linguistic encoders. Evidence needed: Experiments demonstrating that fine-tuning the encoder weights yields statistically significant improvements in F1-scores over the frozen-MLP baseline without overfitting on limited medical datasets.

## Limitations

- Modality imbalance represents a fundamental constraint: multi-modal fusion fails to consistently outperform the best mono-modal baseline across datasets, particularly for depression and ASD detection.
- Cross-lingual performance drops significantly when model language coverage doesn't match target language, suggesting current multilingual models have insufficient cross-linguistic generalization for neuropsychiatric detection.
- ASD detection shows limited benefit from multi-modal approaches due to dataset heterogeneity and speech modality dominance, questioning the universal applicability of fusion strategies.

## Confidence

- High confidence: WavLM-Large provides superior speech representations for neuropsychiatric detection; Attention-based fusion achieves best overall performance on AD and depression tasks.
- Medium confidence: Cross-lingual transfer requires language-specific models (mE5-Large vs E5-Large); multi-modal fusion improves detection when modalities provide complementary information.
- Low confidence: Modality imbalance mitigation techniques (OGM-GE, PMR) provide consistent improvement across all datasets; tensor fusion's superiority for AD is generalizable beyond tested datasets.

## Next Checks

1. Implement gradient-based diagnostics to quantify modality imbalance severity on each dataset, then validate whether OGM-GE/PMR specifically address identified imbalance patterns rather than providing general regularization.
2. Conduct controlled ablation studies comparing frozen vs. fine-tuned foundation model backbones to determine if task-specific adaptation improves performance beyond feature quality assessment.
3. Test cross-lingual transfer systematically across all language pairs using consistent model architectures to establish whether observed Greek-English degradation generalizes to other language combinations.