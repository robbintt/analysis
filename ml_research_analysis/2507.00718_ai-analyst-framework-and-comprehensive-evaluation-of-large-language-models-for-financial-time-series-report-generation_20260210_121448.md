---
ver: rpa2
title: 'AI Analyst: Framework and Comprehensive Evaluation of Large Language Models
  for Financial Time Series Report Generation'
arxiv_id: '2507.00718'
source_url: https://arxiv.org/abs/2507.00718
tags:
- reports
- data
- time
- report
- index
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for generating financial reports
  from time series data using large language models. The framework combines prompt
  engineering, model selection, and automated evaluation, with a novel highlighting
  system that categorizes report segments as direct references, financial interpretations,
  or external knowledge.
---

# AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation

## Quick Facts
- arXiv ID: 2507.00718
- Source URL: https://arxiv.org/abs/2507.00718
- Reference count: 40
- Primary result: GPT-4o consistently produces highest quality financial reports from time series data across all metrics

## Executive Summary
This paper introduces a framework for generating financial reports from time series data using large language models, combining prompt engineering, model selection, and automated evaluation. The framework features a novel highlighting system that categorizes report segments as direct references to data, financial interpretations, or external knowledge, enabling assessment of factual grounding. Experiments with real and synthetic market indices demonstrate that GPT-4o consistently produces the highest quality reports, with automated G-Eval scores showing moderate alignment with human expert evaluations.

## Method Summary
The framework converts time series data into text-based or visual formats for LLM processing, applying technical indicators (SMA, EMA, RSI, MACD, Bollinger Bands, Fibonacci) automatically to market index data. Reports are generated using various LLMs (GPT-4o, GPT-4o-mini, Gemini, Llama3.2, Phi-3) and evaluated through automated G-Eval scoring on consistency, coherence, and fluency dimensions, supplemented by human expert annotation. A secondary LLM segments and classifies report content into direct references, financial interpretations, or external knowledge to assess factual grounding and reasoning capability.

## Key Results
- GPT-4o consistently produces highest quality reports with G-Eval consistency scores of 3.46-3.77, outperforming smaller models like Phi-3 (2.28-2.46)
- Automated G-Eval scores show moderate correlation with human evaluation (Spearman ρ 0.33-0.57, Kendall-Tau τ 0.18-0.44)
- Linguistic analysis reveals low diversity across reports (TTR(A) 0.02-0.03) with most maintaining neutral sentiment
- Temporal analysis shows decline in external knowledge references as models extrapolate beyond training cutoffs

## Why This Works (Mechanism)

### Mechanism 1: LLM-based Time Series Interpretation
- Claim: LLMs can generate coherent financial reports from numerical time series data through structured prompting.
- Mechanism: The framework converts time series data (close prices, technical indicators) into text-based or visual formats that LLMs process via in-context learning. The model applies learned financial reasoning patterns to describe trends, volatility, and patterns without fine-tuning.
- Core assumption: Pre-trained LLMs have sufficient financial knowledge embedded from training to interpret market data meaningfully.
- Evidence anchors:
  - [abstract] "Our experiments, utilizing both data from the real stock market indices and synthetic time series, demonstrate the capability of LLMs to produce coherent and informative financial reports."
  - [section 3.2] "Given financial time series that contain the close price, and/or technical indicators, the LLM is prompted to describe and summarize the patterns and trends observed in the data in an analyst report style."
  - [corpus] Related work (arXiv:2511.08616) confirms LLMs can produce interpretable stock forecasts from price data, supporting cross-domain reasoning capability.
- Break condition: Performance degrades significantly when: (a) data falls beyond training cutoff (consistency scores drop for GPT models post-cutoff), (b) models lack sufficient context length for longer TI reports (Llama3.2 truncation issues), or (c) smaller models lack financial reasoning capacity (Phi-3 repetitive prose, factual errors).

### Mechanism 2: Segment Source Classification for Factual Grounding
- Claim: Automated classification of report segments by information source enables evaluation of factual grounding and reasoning capability.
- Mechanism: A secondary LLM (GPT-4o) segments generated reports and classifies each segment into three categories: Direct Reference (DR) for data citations, Financial Interpretation (FI) for inference from data, and External Knowledge (EK) for outside information. This triage helps identify hallucination vs. legitimate reasoning.
- Core assumption: The classification LLM can accurately distinguish between data-derived statements and external knowledge, and categories are mutually exclusive.
- Evidence anchors:
  - [abstract] "We introduce an automated highlighting system to categorize information within the generated reports, differentiating between insights derived directly from time series data, stemming from financial reasoning, and those reliant on external knowledge."
  - [section 4.4] "The overall accuracy of 80% is reasonably high... most misclassifications occur when DR is predicted as FI."
  - [corpus] FAITH framework (arXiv:2508.05201) addresses tabular hallucination assessment in finance, providing complementary approach to factual grounding evaluation.
- Break condition: Classification fails when: (a) FI category boundaries blur with DR for trend summaries, (b) hedging language makes FI/EK distinction ambiguous, or (c) technical indicators are misclassified as FI instead of DR since TIs are part of input data.

### Mechanism 3: G-Eval as Proxy for Human Judgment
- Claim: LLM-based evaluation (G-Eval) correlates moderately with human expert assessment for financial report quality.
- Mechanism: GPT-4o evaluates generated reports on three dimensions (Consistency, Coherence, Fluency) using chain-of-thought prompts that define criteria and evaluation steps. Scores (1-5) are compared against human expert annotations.
- Core assumption: GPT-4o's evaluation criteria align with human expert judgment in the financial domain, and the evaluator model is not biased toward its own generations.
- Evidence anchors:
  - [section 4.1] "Spearman (ρ) 0.33-0.57, Kendall-Tau (τ) 0.18-0.44 capturing the alignment between G-Eval and Human Evaluation."
  - [section 6] "One of the main limitations is... potential bias in the automated evaluations towards models from GPT family as GPT-4o is used as the evaluator model."
  - [corpus] Limited direct corpus evidence for G-Eval in finance specifically; related work mentions evaluation challenges but not G-Eval validation.
- Break condition: Alignment breaks when: (a) G-Eval systematically favors GPT-family outputs (self-preference bias acknowledged), (b) human evaluators show high subjectivity variance for consistency/coherence, or (c) financial domain specificity requires expertise that general LLM evaluation lacks.

## Foundational Learning

- Concept: **Technical Indicators in Financial Analysis** (SMA, EMA, RSI, MACD, Bollinger Bands, Fibonacci Retracement)
  - Why needed here: TI reports require understanding how these indicators signal trends and shifts. The framework computes these automatically and expects models to reference them meaningfully.
  - Quick check question: Can you explain why RSI above 70 might indicate overbought conditions and how MACD crossovers signal momentum changes?

- Concept: **LLM-based Evaluation (G-Eval Framework)**
  - Why needed here: The framework relies on G-Eval for automated quality assessment. Understanding its chain-of-thought prompting approach and criteria definitions is essential for interpreting results.
  - Quick check question: What are the three evaluation dimensions used, and why might fluency scores show higher correlation than consistency scores?

- Concept: **Type-Token Ratio (TTR) for Lexical Diversity**
  - Why needed here: Linguistic analysis uses TTR(W) and TTR(A) to detect formulaic language. Low TTR(A) indicates template-like outputs across reports.
  - Quick check question: Why might TTR(A) be low while TTR(W) is high, and what does this suggest about model behavior?

## Architecture Onboarding

- Component map: Data Preparation -> Report Generation -> Evaluation -> Classification
- Critical path:
  1. Data ingestion (S&P 500, Nasdaq, DJIA, Nikkei 225) with 1-year overlapping windows
  2. Technical indicator computation (SMA, EMA, RSI, MACD, Volatility, Bollinger Bands, Fibonacci)
  3. Prompt construction (text-based or plot-based time series)
  4. Report generation via selected LLM
  5. G-Eval scoring (Consistency, Coherence, Fluency)
  6. Segment classification (DR/FI/EK categorization)

- Design tradeoffs:
  - Text-based vs. plot-based TI input: Text preserves numerical precision; plots leverage multimodal capabilities but may lose detail
  - GPT-4o vs. smaller models: Higher quality vs. cost/latency (GPT-4o scores 3.46-3.77 consistency vs. Phi-3 at 2.28-2.46)
  - Synthetic vs. real data testing: Synthetic enables out-of-distribution testing but may not capture real market dynamics
  - G-Eval vs. human evaluation: Scalability vs. ground truth reliability

- Failure signatures:
  - Context length overflow: Llama3.2 produces "word salad" when TI reports exceed 8092 tokens
  - Repetitive prose: Phi-3 generates low-diversity outputs (TTR(A)=0.02-0.03)
  - Temporal inconsistency: Reports describing events in impossible chronological order
  - Hallucinated technical indicators: References to 200-day MA when not in input
  - Post-cutoff degradation: GPT models show consistency decline for data beyond training period

- First 3 experiments:
  1. **Baseline quality comparison**: Generate short reports for S&P 500 (2020-2021) using all five models. Score with G-Eval. Validate top-2 and bottom-2 models with human evaluation (3 annotators, 4 reports each). Expected: GPT-4o > GPT-4o-mini ≈ Gemini > Llama3.2 > Phi-3.
  2. **Segment classification validation**: Sample 50 reports across models/types. Have one expert annotator label segments as DR/FI/EK. Calculate per-class precision/recall. Focus on DR→FI confusion patterns. Target: >75% accuracy.
  3. **Temporal degradation test**: Generate reports for 2019-2024 real data (overlapping windows). Track EK proportion and hedging word frequency relative to model cutoff dates. Hypothesis: EK declines, hedging increases post-cutoff.

## Open Questions the Paper Calls Out
None

## Limitations

- The framework's reliance on GPT-4o for both report generation and evaluation introduces potential bias, as acknowledged by the authors
- The 80% accuracy for segment classification represents a meaningful error rate, particularly in distinguishing DR from FI categories
- Consistent patterns of low linguistic diversity across all models suggest potential template-driven generation that may limit real-world applicability

## Confidence

- **High Confidence**: GPT-4o consistently produces highest quality reports across all metrics; automated G-Eval scores show moderate correlation with human evaluation; technical indicator computation and integration are reliable
- **Medium Confidence**: Segment classification accuracy of 80% is reasonably high but leaves room for error; linguistic diversity patterns are consistently observed but may not fully capture report quality; temporal performance degradation patterns are evident but mechanisms are not fully characterized
- **Low Confidence**: Generalization to different market conditions and asset classes beyond the tested indices; the framework's performance on extremely volatile or crisis periods; the extent to which low TTR scores reflect genuine limitations versus acceptable template usage in financial reporting

## Next Checks

1. **Cross-validation with independent evaluators**: Have three financial domain experts independently evaluate 50 randomly selected reports from different models and time periods, calculating inter-rater reliability (Cohen's kappa) to establish ground truth for evaluation metrics.

2. **Out-of-distribution stress testing**: Generate reports for synthetic time series with extreme volatility patterns, sharp regime shifts, and irregular intervals to assess model robustness beyond the relatively smooth real market data patterns tested.

3. **Temporal consistency audit**: For each model, generate reports for overlapping 3-month windows spanning 2019-2024, then systematically compare consistency scores and external knowledge references across periods relative to known model training cutoffs to quantify performance degradation patterns.