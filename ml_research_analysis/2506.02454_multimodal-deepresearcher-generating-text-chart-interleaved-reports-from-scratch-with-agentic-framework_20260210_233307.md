---
ver: rpa2
title: 'Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From
  Scratch with Agentic Framework'
arxiv_id: '2506.02454'
source_url: https://arxiv.org/abs/2506.02454
tags:
- visualization
- multimodal
- report
- chart
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task of generating multimodal reports
  that interleave text and charts from scratch. The authors propose Multimodal DeepResearcher,
  an agentic framework that combines formal visualization descriptions with in-context
  learning to generate diverse, high-quality charts integrated into coherent reports.
---

# Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework

## Quick Facts
- arXiv ID: 2506.02454
- Source URL: https://arxiv.org/abs/2506.02454
- Reference count: 40
- Introduces novel task of generating multimodal reports that interleave text and charts from scratch

## Executive Summary
This paper presents Multimodal DeepResearcher, an agentic framework for generating comprehensive multimodal reports that seamlessly integrate text and charts. The system addresses the challenge of creating diverse, high-quality visualizations embedded within coherent textual narratives. By combining formal visualization descriptions with in-context learning, the framework produces reports that effectively communicate complex information through both visual and textual means.

The approach operates through a four-stage pipeline: researching information, textualizing exemplar reports, planning content and style, and generating multimodal outputs. The system demonstrates significant improvements over existing baselines, achieving 82% overall win rate in human evaluations while maintaining strong performance across verifiability, visualization quality, and consistency metrics.

## Method Summary
Multimodal DeepResearcher employs an agentic framework that orchestrates the generation of multimodal reports through four distinct stages. The system first conducts comprehensive research on the given topic, gathering relevant information and data points. It then analyzes exemplar reports to understand effective textualization patterns and report structures. In the planning stage, the framework determines optimal content organization and stylistic choices for the specific topic. Finally, it generates the multimodal report by interleaving informative text with diverse, high-quality charts.

The framework leverages large language models (specifically Claude 3.7 Sonnet) to handle both the textual components and the formal descriptions required for chart generation. The in-context learning approach allows the system to adapt to different topics and report styles while maintaining coherence between textual explanations and visual representations.

## Key Results
- Achieves 82% overall win rate over DataNarrative baseline using Claude 3.7 Sonnet
- Demonstrates particular strengths in verifiability, visualization quality, and consistency metrics
- Evaluated on 100 diverse topics with both automatic and human assessment methods

## Why This Works (Mechanism)
The framework succeeds by treating report generation as an orchestrated multi-stage process rather than a single-shot generation task. By separating research, planning, and generation phases, the system can maintain coherence while producing diverse content. The use of formal visualization descriptions ensures that generated charts are not only diverse but also meaningful and appropriately contextualized within the report narrative.

## Foundational Learning
- **Agentic frameworks**: Why needed - enables autonomous decision-making across multiple stages; Quick check - can the system independently navigate from research to final output
- **In-context learning**: Why needed - allows adaptation to diverse topics without fine-tuning; Quick check - does the system maintain quality across different subject domains
- **Formal visualization descriptions**: Why needed - ensures generated charts are meaningful and appropriately contextualized; Quick check - can the system explain the rationale behind each visualization choice
- **Multimodal interleaving**: Why needed - creates more engaging and informative reports than text-only or chart-only approaches; Quick check - does the text-ch chart integration enhance comprehension
- **Pairwise human evaluation**: Why needed - provides reliable quality assessment beyond automated metrics; Quick check - do human raters consistently prefer generated reports over baselines

## Architecture Onboarding
**Component Map**: Research -> Textualization -> Planning -> Generation
**Critical Path**: Information gathering → Content planning → Multimodal synthesis
**Design Tradeoffs**: Uses LLM-based approach for flexibility vs. potential hallucination risks; prioritizes human evaluation over automated metrics for quality assessment
**Failure Signatures**: Inconsistent information between text and charts; poor visualization choices for the given context; lack of coherence in the narrative flow
**First Experiments**:
1. Test report generation on simple factual topics to verify basic functionality
2. Evaluate system's ability to maintain consistency across multiple sections
3. Assess visualization quality and appropriateness for different data types

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Human evaluation relied on only 3 annotators per report, potentially missing quality variations across 100 diverse topics
- Methodology assumes reliable information sources but doesn't thoroughly address misinformation risks or fact-checking limitations
- Focus on quality metrics without practical utility assessment for real-world decision-making scenarios

## Confidence
High confidence in core methodology and quantitative results (82% win rate)
Medium confidence in generalizability across domains beyond tested topics
Low confidence in real-world deployment readiness given lack of practical utility assessment

## Next Checks
1. Expand human evaluation to include domain experts across multiple fields with larger annotator pools to validate quality metrics
2. Conduct longitudinal studies to assess report stability and consistency across multiple generations of the same topic
3. Implement systematic error analysis to quantify and categorize types of hallucinations or factual inconsistencies in generated content