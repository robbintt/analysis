---
ver: rpa2
title: Enter the Void - Planning to Seek Entropy When Reward is Scarce
arxiv_id: '2505.16787'
source_url: https://arxiv.org/abs/2505.16787
tags:
- reward
- planning
- arxiv
- learning
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a planning mechanism for world-model-based
  RL agents that uses short-horizon latent predictions to anticipate and actively
  seek informative states, enhancing exploration. By leveraging model uncertainty
  via entropy of predicted latents and combining it with reward, a hierarchical planner
  dynamically decides when to commit to or replan trajectories, reducing dithering.
---

# Enter the Void - Planning to Seek Entropy When Reward is Scarce

## Quick Facts
- arXiv ID: 2505.16787
- Source URL: https://arxiv.org/abs/2505.16787
- Reference count: 40
- Improves sample efficiency of DreamerV3 by actively seeking high-uncertainty states via entropy-guided exploration.

## Executive Summary
This paper addresses exploration challenges in sparse-reward environments by introducing a planning mechanism that anticipates and actively seeks informative states. The method leverages model uncertainty through entropy of predicted latent states, combining it with reward signals to guide exploration. Applied to DreamerV3, the approach uses short-horizon latent predictions to dynamically balance between committing to trajectories and replanning, effectively reducing dithering behavior. Results show significant improvements in sample efficiency across MiniWorld mazes (50% faster completion, 60% fewer steps), Crafter (same reward in one-third the steps), and several DeepMind Control tasks.

## Method Summary
The method introduces an inference-time entropy-seeking planner for DreamerV3 that selects high-uncertainty, high-reward imagined trajectories. It generates N=256 candidate rollouts of horizon H=15-16 using a greedy actor plus world model, scoring trajectories via a combination of predicted reward and entropy of predicted latents. A hierarchical planner with a PPO meta-head dynamically decides when to commit to or replan trajectories, reducing dithering by applying p² squashing to replan probabilities. The meta-reward over horizon L combines base reward and entropy with L=32 for Crafter, 8 for MiniWorld, and 2 for DMC tasks. The approach is evaluated on MiniWorld mazes (with episodic visitation maps), Crafter, and 6 DMC-Vision tasks using single-environment setups to isolate reasoned exploration.

## Key Results
- MiniWorld mazes: 50% faster completion time, uses 60% of environment steps compared to baseline
- Crafter: Achieves same reward level in one-third of the steps required by baseline
- DeepMind Control: Generally improves sample efficiency across 6 tested tasks
- The method shows robust performance across different hyperparameter settings

## Why This Works (Mechanism)
The approach works by anticipating and actively seeking informative states through model uncertainty quantification. By leveraging entropy of predicted latent states as a proxy for epistemic uncertainty, the planner can identify regions of the state space that are both uncertain and potentially rewarding. The hierarchical meta-planner balances exploration and exploitation by dynamically deciding when to commit to trajectories versus replanning, which reduces inefficient dithering behavior. The combination of reward and entropy in trajectory scoring ensures the agent pursues both informative and valuable futures.

## Foundational Learning
- **RSSM (Recurrent State-Space Model)**: Why needed - learns latent dynamics for planning; Quick check - verify latent state dimensionality matches world model output
- **Entropy as Uncertainty**: Why needed - quantifies epistemic uncertainty to guide exploration; Quick check - compute entropy over discrete latent dimensions and verify values are meaningful
- **GAE (Generalized Advantage Estimation)**: Why needed - stable advantage estimation for meta-policy training; Quick check - monitor advantage estimates for stability during training
- **Meta-policy with PPO**: Why needed - learns when to replan vs. commit to trajectories; Quick check - verify replan rate stabilizes between 0.5-0.7, not near 1.0

## Architecture Onboarding
- **Component map**: RGB observations → Encoder → RSSM → World Model → Candidate Rollouts → Entropy + Reward Scoring → Best Trajectory → Meta-planner (PPO) → Replan Decision
- **Critical path**: Planning loop: generate candidates → score by Σ(reward + entropy) → select best → meta-planner outputs p → replan if u < p²
- **Design tradeoffs**: Short horizons (H=15-16) balance computational cost with meaningful exploration; discrete meta-actions simplify decision boundaries; p² squashing prevents excessive replanning
- **Failure signatures**: Excessive replanning causing dithering (replan rate near 1.0); planner chasing aleatoric noise in stochastic environments; low candidate diversity yielding marginal gains
- **3 first experiments**:
  1. Verify PPO meta-head outputs 5 discrete actions mapping to p ∈ {0,0.25,0.5,0.75,1}
  2. Test trajectory scoring with λ_r=λ_H=1.0 and verify sensitivity claims
  3. Implement entropy computation over RSSM prior and validate on a simple maze task

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The PPO meta-head architecture details (layer sizes, hidden dimensions) are not fully specified beyond parameter count
- MiniWorld reward function details including the blur parameter b and exact coefficients are incompletely described
- The relationship between "meta_action_quant: 5" and "num_meta_action_lwr: 2" in configuring the categorical output space is unclear
- Claims about meta-planner's ability to balance exploration and exploitation have medium confidence

## Confidence
- High confidence: Sample efficiency improvements across tasks; reduced dithering via meta-planner; robust performance across hyperparameters
- Medium confidence: Meta-planner's exploration-exploitation balance; effectiveness of entropy-based uncertainty quantification
- Low confidence: Exact PPO meta-head architecture; precise reward scaling coefficients; MiniWorld reward function implementation details

## Next Checks
1. Verify PPO meta-head implementation by reproducing the 5 discrete action outputs and ensuring p² squashing correctly reduces dithering rates
2. Test trajectory scoring sensitivity by sweeping λ_r and λ_H values to confirm claimed insensitivity and identify optimal scaling
3. Validate entropy computation implementation by comparing against baseline on high-variance stochastic environments to ensure aleatoric noise is not being misinterpreted as epistemic uncertainty