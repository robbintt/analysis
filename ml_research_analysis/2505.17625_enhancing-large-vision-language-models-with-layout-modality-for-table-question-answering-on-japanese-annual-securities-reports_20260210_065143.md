---
ver: rpa2
title: Enhancing Large Vision-Language Models with Layout Modality for Table Question
  Answering on Japanese Annual Securities Reports
arxiv_id: '2505.17625'
source_url: https://arxiv.org/abs/2505.17625
tags:
- table
- layout
- text
- information
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of table question answering
  (QA) in financial documents, where tables exist in various formats and require accurate
  structural understanding. The authors propose enhancing Large Vision-Language Models
  (LVLMs) by incorporating three modalities: text, layout (bounding-box coordinates),
  and image, instead of relying on image-only inputs.'
---

# Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports

## Quick Facts
- arXiv ID: 2505.17625
- Source URL: https://arxiv.org/abs/2505.17625
- Reference count: 16
- Key outcome: Text+Layout combination achieves 95.09% accuracy and 96.66% ANLS on Japanese securities table QA

## Executive Summary
This paper addresses table question answering in financial documents where tables have diverse formats requiring accurate structural understanding. The authors enhance Large Vision-Language Models by incorporating text, layout (bounding-box coordinates), and image modalities, moving beyond image-only inputs. They reformulate the NTCIR-18 U4 Table QA dataset into a simplified TableCellQA format focused on direct cell value extraction. Experimental results demonstrate that combining text and layout modalities significantly improves performance, with structured table text formats (HTML, JSON, Markdown) outperforming multimodal approaches when available.

## Method Summary
The authors fine-tune LLaVA-OneVision-qwen2-7b-ov to handle table QA using three modalities: image, text, and layout. Layout information is encoded through a 2-layer MLP that projects 4D bounding-box coordinates (xmin, ymin, xmax, ymax) to the LLM hidden dimension, then concatenated with text embeddings. The model is trained for 2 epochs with batch size 8, learning rate 1e-5, and warmup ratio 0.03. The task involves extracting direct cell values from tables in Japanese annual securities reports without arithmetic reasoning. The dataset is derived from NTCIR-18 U4 Table QA and consists of 10,278 training examples and 1,303 test examples.

## Key Results
- Text+Layout combination achieves highest accuracy at 95.09%
- Full combination (Text+Layout+Image) achieves highest ANLS at 96.66%
- Structured table formats (HTML/JSON/Markdown) outperform multimodal approaches when available
- Layout modality significantly improves table structure understanding compared to image-only baselines

## Why This Works (Mechanism)
The approach works because combining textual content with spatial layout information provides complementary signals for table understanding. While OCR extracts the semantic content of table cells, layout coordinates capture the structural relationships between cells, enabling the model to disambiguate between similarly named but spatially distinct table elements. This multimodal fusion addresses the limitations of image-only approaches that struggle with dense tables and OCR errors.

## Foundational Learning
- **LLaVA-OneVision**: Vision-language model architecture that processes both images and text through a unified transformer framework - needed to handle multimodal table inputs; quick check: verify input tensor shapes match model expectations
- **Bounding-box layout encoding**: Converting 4D spatial coordinates to embedding space via MLP - needed to provide structural context to the LLM; quick check: ensure MLP output dimension matches LLM hidden size
- **TableCellQA task formulation**: Simplified extraction task focusing on direct cell values without arithmetic reasoning - needed to isolate the impact of layout modality; quick check: verify cell ID mapping is correct
- **ANLS metric**: Approximate Normalized Levenshtein Similarity for evaluating answer similarity - needed for robust evaluation beyond exact matches; quick check: confirm metric implementation matches paper definition
- **OCR with layout extraction**: Extracting both text and bounding-box coordinates from rendered tables - needed to create the multimodal input; quick check: validate OCR quality on sample tables
- **HTML to PDF rendering pipeline**: Converting structured table data to images while preserving layout - needed to create consistent test conditions; quick check: verify rendered images match original table structure

## Architecture Onboarding
**Component map:** HTML table -> PDF render -> OCR text + bbox -> Layout MLP -> [Text+Layout] -> LLaVA-OneVision -> Answer prediction

**Critical path:** Table HTML → PDF rendering → OCR extraction (text + bbox) → Layout encoding → Multimodal fusion → LLM fine-tuning → QA evaluation

**Design tradeoffs:** The paper chooses to encode layout as simple MLP projections rather than more complex graph-based representations, trading potential expressiveness for computational efficiency and ease of integration with existing LLM architectures. The simplified TableCellQA task excludes arithmetic reasoning to focus on structural understanding, which may limit generalizability but provides cleaner ablation studies.

**Failure signatures:** Misalignment between layout embeddings and text tokens causes confusion between neighboring cells; OCR errors on dense tables degrade image-only baseline performance; dimension mismatches between layout embeddings and LLM hidden size cause training instability.

**First experiments:** 1) Fine-tune LLaVA-OneVision with specified hyperparameters and verify reported accuracy/ANLS; 2) Test layout encoding sensitivity by varying MLP architecture; 3) Evaluate approach on tables with mixed language content and complex layouts.

## Open Questions the Paper Calls Out
None

## Limitations
- Key architecture details like MLP specifications and OCR pipeline are underspecified, limiting reproducibility
- Study focuses on Japanese financial documents with simple extraction tasks, limiting generalizability
- Does not address computational costs or latency implications for deployment scenarios

## Confidence
- **High confidence**: Text+Layout combination outperforms single modalities; structured formats achieve superior performance when available
- **Medium confidence**: Layout modality significantly improves table structure understanding; simplified TableCellQA formulation is representative
- **Low confidence**: Specific MLP architecture details are sufficient for layout encoding; approach generalizes to tables with arithmetic reasoning

## Next Checks
1. Replicate the LLaVA-OneVision fine-tuning pipeline with specified hyperparameters and verify whether reported Accuracy (95.09%) and ANLS (96.66%) are achievable
2. Systematically vary the layout embedding MLP architecture to determine sensitivity and identify minimum sufficient configuration
3. Test the approach on tables with mixed language content and more complex layouts to evaluate robustness beyond Japanese securities reports