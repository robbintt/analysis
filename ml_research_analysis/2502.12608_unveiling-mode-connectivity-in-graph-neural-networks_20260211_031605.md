---
ver: rpa2
title: Unveiling Mode Connectivity in Graph Neural Networks
arxiv_id: '2502.12608'
source_url: https://arxiv.org/abs/2502.12608
tags:
- mode
- connectivity
- graph
- loss
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents the first systematic investigation of mode\
  \ connectivity in graph neural networks (GNNs), revealing that GNNs exhibit distinct\
  \ non-linear mode connectivity patterns unlike fully-connected networks or CNNs.\
  \ Through experiments on 12 diverse graphs, the authors demonstrate that graph structure\u2014\
  particularly properties like homophily and density\u2014dominates mode connectivity\
  \ behavior, with model architecture having minimal impact."
---

# Unveiling Mode Connectivity in Graph Neural Networks

## Quick Facts
- **arXiv ID**: 2502.12608
- **Source URL**: https://arxiv.org/abs/2502.12608
- **Reference count**: 40
- **Key outcome**: This work presents the first systematic investigation of mode connectivity in graph neural networks (GNNs), revealing that GNNs exhibit distinct non-linear mode connectivity patterns unlike fully-connected networks or CNNs. Through experiments on 12 diverse graphs, the authors demonstrate that graph structure—particularly properties like homophily and density—dominates mode connectivity behavior, with model architecture having minimal impact. They establish a theoretical framework providing a generalization bound based on loss barriers, validating the link between mode connectivity and generalization performance. The findings rationalize domain alignment strategies in graph learning and offer a novel geometric perspective for quantifying graph domain discrepancies using mode connectivity-based Wasserstein distances. These insights bridge theoretical understanding with practical implications for refining GNN training paradigms and knowledge transfer across graph domains.

## Executive Summary
This paper systematically investigates mode connectivity in Graph Neural Networks (GNNs) for the first time, revealing that GNNs exhibit distinct non-linear connectivity patterns compared to traditional neural networks. Through experiments on 12 real-world graphs and theoretical analysis, the authors demonstrate that graph structure properties—particularly density and homophily levels—dominate mode connectivity behavior, while model architecture has minimal impact. They establish a theoretical generalization bound linking loss barriers to generalization gap and propose a novel Wasserstein-based metric for quantifying domain discrepancies. The findings provide insights for improving GNN training and knowledge transfer across graph domains.

## Method Summary
The authors investigate mode connectivity by training two GNN models independently on the same graph with different initializations or data orders, then measuring the loss barrier along interpolation paths between the resulting parameter sets. They compare linear interpolation with non-linear quadratic Bézier curves to find low-loss paths connecting model minima. The study employs a synthetic Correlated Stochastic Block Model (CSBM) to systematically vary graph properties like density, homophily, and feature separability. Theoretical analysis provides a generalization bound based on loss barriers, while the Mode Connectivity Distance (d_MC) quantifies domain discrepancy between graphs using Wasserstein distances. Experiments cover node classification on 12 real-world graphs and synthetic datasets.

## Key Results
- GNNs exhibit distinct non-linear mode connectivity, diverging from patterns observed in fully-connected networks or CNNs
- Graph structure properties (density, homophily level) dominate mode connectivity behavior, with architecture having minimal impact
- Mode connectivity quality correlates with generalization gap and can quantify domain discrepancy via Wasserstein distance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNN minima are connected via non-linear (quadratic Bézier) paths rather than linear interpolation, with connectivity quality determined by graph structure.
- Mechanism: The non-i.i.d. nature of graph data creates loss landscapes where minima reside in connected low-loss manifolds, but these manifolds are curved rather than flat. Linear interpolation between independently trained models encounters loss barriers because the parameter space trajectory crosses elevated loss regions. Quadratic Bézier curves learn a control point that bends around these barriers.
- Core assumption: The loss landscape geometry reflects the underlying graph topology and message-passing dynamics rather than just the neural network architecture.
- Evidence anchors:
  - [abstract] "We uncover that GNNs exhibit distinct non-linear mode connectivity, diverging from patterns observed in fully-connected networks or CNNs."
  - [Section 3.1, Observation 1] "linear interpolation often results in noticeable loss barriers... most modes can still be connected by a simple polynomial curve, such as a quadratic Bézier curve"
  - [Figure 1 vs Figure 2] Visual comparison shows linear paths have high loss barriers while Bézier curves maintain low loss
- Break condition: If graph structure has no correlation with loss barrier magnitude, or if linear paths consistently outperform Bézier curves, the mechanism fails.

### Mechanism 2
- Claim: Graph properties—specifically density, homophily level (high or low), and feature separability—causally determine mode connectivity quality.
- Mechanism: Denser graphs enable stable feature propagation across connected nodes, reducing optimization variance. High homophily or high heterophily creates unambiguous class boundaries (same-class neighbors or distinct feature distributions), while intermediate homophily introduces label ambiguity that fragments the loss landscape. Feature separability ensures consistent local optima alignment across training runs.
- Core assumption: The CSBM synthetic model accurately captures how real-world graph properties influence GNN training dynamics.
- Evidence anchors:
  - [Section 3.2] "Models trained on denser graphs present lower loss barriers... models trained on graphs with either high homophily or high heterophily exhibit better mode connectivity compared to graphs with medium homophily level"
  - [Proposition 3.5] Mathematical bound links homophily h(G) and density (p_in + p_out) directly to loss barrier magnitude
  - [Figure 5] Empirical trends showing barrier vs. graph property relationships
- Break condition: If architecture changes (e.g., switching from GCN to GAT) significantly alter barrier patterns while graph structure stays constant, the graph-dominance claim weakens.

### Mechanism 3
- Claim: The loss barrier metric serves as a predictor of generalization gap and can quantify domain discrepancy via Wasserstein distance.
- Mechanism: The loss barrier captures sharpness between minima—higher barriers indicate isolated, steep minima associated with overfitting. The generalization bound (Theorem 4.1) formally links barrier magnitude to generalization gap. For domain transfer, the mode connectivity distance d_MC measures geometric similarity between loss landscapes of different graphs.
- Core assumption: Flatter minima (lower barriers) generalize better, and loss landscape geometry transfers across related domains.
- Evidence anchors:
  - [Theorem 4.1] "generalization gap satisfies Δ_gen ≤ O(8B(θ_a, θ_b) · n^(3/2)/(m(n-m)) · ...)"
  - [Figure 6] Training accuracy barrier shows stronger correlation with generalization gap than validation accuracy
  - [Section 4.2, Figure 7] d_MC correlates with domain adaptation performance gap
  - [corpus] Related work "Understanding Mode Connectivity via Parameter Space Symmetry" explores similar theoretical foundations but does not specifically address GNNs
- Break condition: If barrier-generalization correlation is weak or inconsistent across diverse graph types, the diagnostic utility fails.

## Foundational Learning

- Concept: **Mode Connectivity Basics**
  - Why needed here: The entire paper builds on understanding how independently trained neural network solutions relate in parameter space—whether they're isolated or connected by low-loss paths.
  - Quick check question: Can you explain why finding a low-loss curve between two trained models implies something about the loss landscape geometry?

- Concept: **Graph Homophily and Heterophily**
  - Why needed here: Homophily (nodes connect to similar nodes) vs. heterophily (nodes connect to dissimilar nodes) is central to understanding why mode connectivity varies across graphs.
  - Quick check question: Why might "medium" homophily levels cause worse mode connectivity than both high homophily and high heterophily?

- Concept: **Loss Landscape Geometry and Generalization**
  - Why needed here: The paper links flat minima (low barriers) to better generalization, requiring intuition about sharp vs. flat optima.
  - Quick check question: How does the "flatness" of a minimum relate to how well a model generalizes to unseen data?

## Architecture Onboarding

- Component map:
  - Backbone GNN: GCN (primary), with GraphSAGE and GAT as comparison points
  - Interpolation paths: Linear (Equation 7) and Quadratic Bézier (Equation 8) with learnable control point
  - Loss barrier metric (Definition 2.2): max_α[L(φ(α)) - L_lin(α)]
  - CSBM synthetic generator: Controlled experiments varying density (p_in + p_out), homophily ratio, and feature variance σ
  - Mode Connectivity Distance (Equation 13): Wasserstein-1 distance between loss distributions

- Critical path:
  1. Train two GNN models on same graph with different initializations/random seeds
  2. Compute linear interpolation loss barrier
  3. Fit Bézier curve to find non-linear low-loss path
  4. Correlate barrier values with graph properties and generalization gaps

- Design tradeoffs:
  - Linear vs. Bézier interpolation: Linear is computationally cheaper but fails on graphs with rugged loss landscapes (e.g., Squirrel dataset)
  - GCN vs. other GNNs: Paper shows architecture has minimal impact, so GCN suffices for mode connectivity analysis
  - Synthetic vs. real graphs: CSBM enables controlled property variation but may not capture all real-world complexity

- Failure signatures:
  - High loss barriers on dense, high-homophily graphs would contradict theoretical bounds
  - Strong architecture-dependent barrier variations would invalidate the "graph structure dominates" claim
  - Poor correlation between d_MC and domain transfer performance would undermine the Wasserstein metric

- First 3 experiments:
  1. Reproduce Figure 1 on 2-3 datasets (Cora for smooth, Squirrel for rugged): Train two GCNs, plot linear interpolation curves for train/test loss and accuracy
  2. Validate Bézier improvement: Implement learnable control point optimization and compare barrier reduction vs. linear path
  3. Test graph property correlation: Generate CSBM graphs with varying homophily (keep density fixed), measure barrier trend to verify U-shaped relationship

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does mode connectivity manifest in Graph Neural Networks (GNNs) when applied to link prediction and graph-level tasks?
- **Basis in paper:** [Explicit] The authors state in the conclusion: "Our research on mode connectivity has primarily focused on the node classification task. A promising future direction is to investigate the mode connectivity exhibited by GNNs on link prediction and graph-level tasks."
- **Why unresolved:** The current study restricted its empirical and theoretical analysis to node classification. The loss landscapes for link prediction (often using pairwise inner products) or graph-level tasks (involving pooling) may exhibit different geometric properties and connectivity barriers compared to the node-wise cross-entropy loss studied here.
- **What evidence would resolve it:** Replicating the interpolation experiments (linear and Bézier) on standard link prediction and graph classification benchmarks to observe if the non-linear connectivity patterns and correlation with graph density/homophily persist.

### Open Question 2
- **Question:** Can specific training strategies or architectural modifications mitigate the high loss barriers observed in graphs with intermediate homophily levels?
- **Basis in paper:** [Inferred] In Section 3.2, the authors identify "mid homophily pitfalls," noting that graphs with intermediate homophily exhibit significantly higher loss barriers (worse connectivity) compared to those with strong homophily or heterophily, but they do not propose a solution for this specific structural regime.
- **Why unresolved:** The paper characterizes the rugged loss landscape of mid-homophily graphs as a result of label ambiguity and optimization instability, but it leaves open the question of whether this disconnected landscape can be smoothed algorithmically to improve training.
- **What evidence would resolve it:** Experiments demonstrating that a specific regularization technique or loss landscape smoothing method successfully lowers loss barriers in synthetic or real-world graphs specifically selected for their intermediate homophily scores.

### Open Question 3
- **Question:** Can the Mode Connectivity Distance ($d_{MC}$) be effectively minimized as a training objective to improve transfer learning performance?
- **Basis in paper:** [Inferred] Section 4.2 introduces $d_{MC}$ as a metric to quantify domain discrepancy and validates its correlation with performance gaps. However, the paper only uses it to rationalize existing alignment strategies rather than using the metric itself to guide the alignment process.
- **Why unresolved:** While the paper theoretically links mode connectivity to generalization, using the connectivity curve as an explicit optimization target for domain adaptation involves complex computational overhead (curve fitting during training) that is not addressed by the current diagnostic framework.
- **What evidence would resolve it:** A domain adaptation algorithm that explicitly minimizes $d_{MC}$ between source and target domains, demonstrating improved accuracy over standard methods like DANN or UDAGCN on the benchmark tasks used in the paper.

## Limitations
- The architecture-independence claim rests on experiments with only GCN, GraphSAGE, and GAT on 12 graphs, which may not capture all GNN families
- The theoretical generalization bound depends on several simplifying assumptions about the loss landscape that may not hold for all graph types
- The mode connectivity distance as a domain discrepancy metric, while promising, requires validation across more diverse transfer learning scenarios

## Confidence
- **High Confidence**: Graph structure properties (density, homophily levels) determine mode connectivity patterns. This is well-supported by both empirical results and theoretical bounds.
- **Medium Confidence**: Mode connectivity serves as a reliable predictor of generalization gap and domain discrepancy. While evidence exists, the relationship may not be universally robust across all graph types and transfer scenarios.
- **Medium Confidence**: The practical utility of mode connectivity-based Wasserstein distances for domain alignment in graph learning. The concept is novel but requires more extensive validation.

## Next Checks
1. Test the architecture-independence claim by measuring mode connectivity barriers across a broader range of GNN variants (Gated GNNs, GIN, JK-Net) on identical graph datasets to verify that structural properties consistently dominate architectural differences.

2. Validate the generalization bound empirically by correlating loss barrier measurements with test performance across different graph properties and training regimes, checking whether the theoretical bound accurately predicts observed generalization gaps.

3. Evaluate the mode connectivity distance metric for domain adaptation by testing its predictive power on more diverse graph transfer learning tasks, including cross-domain node classification and graph-level prediction scenarios beyond the current setup.