---
ver: rpa2
title: How Memory in Optimization Algorithms Implicitly Modifies the Loss
arxiv_id: '2502.02132'
source_url: https://arxiv.org/abs/2502.02132
tags:
- learning
- where
- momentum
- memory
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a general framework for analyzing the implicit
  regularization introduced by memory in optimization algorithms. The authors propose
  a technique to approximate an optimization algorithm with memory by a memoryless
  algorithm plus a correction term.
---

# How Memory in Optimization Algorithms Implicitly Modifies the Loss

## Quick Facts
- arXiv ID: 2502.02132
- Source URL: https://arxiv.org/abs/2502.02132
- Authors: Matias D. Cattaneo; Boris Shigida
- Reference count: 40
- One-line primary result: AdamW's memory introduces implicit anti-regularization by penalizing the ℓ1-norm of the gradient, while Lion does not, explaining Lion's better generalization empirically.

## Executive Summary
This paper develops a general framework for analyzing the implicit regularization introduced by memory in optimization algorithms. The authors propose a technique to approximate an optimization algorithm with memory by a memoryless algorithm plus a correction term, which can be interpreted as a perturbation of the loss function. This allows insights into how memory implicitly regularizes the optimization dynamics. The key theoretical result provides an approximation bound between the original algorithm and the memoryless approximation, valid for sufficiently small learning rates. The authors apply their framework to analyze AdamW and Lion, finding that AdamW's memory introduces implicit anti-regularization by penalizing the ℓ1-norm of the gradient, while Lion does not have this effect.

## Method Summary
The paper introduces a framework to analyze how memory in optimization algorithms implicitly modifies the loss function. The method involves approximating an algorithm with exponentially decaying memory by a memoryless algorithm plus a correction term derived through Taylor expansion. This correction term acts as the gradient of an implicit modification to the loss landscape. The framework is then applied to specific algorithms like AdamW and Lion to identify their implicit regularization effects. The theoretical analysis is validated through empirical evaluations on vision (CIFAR-10 with ResNet-50) and language (WikiText-2 with Transformer-XL) tasks.

## Key Results
- AdamW's memory introduces implicit anti-regularization by penalizing the ℓ1-norm of the gradient, pushing toward sharper minima and worse generalization compared to Lion.
- The approximation bound between the original algorithm and the memoryless approximation is O(h²) for sufficiently small learning rates.
- Empirical evaluations confirm that AdamW's generalization can be improved by reducing its momentum parameter β₂, sometimes matching Lion's performance.

## Why This Works (Mechanism)

### Mechanism 1: Memory-to-Loss Conversion via Taylor Expansion
- **Claim:** An optimization algorithm with exponentially decaying memory can be approximated by a memoryless algorithm acting on a perturbed loss function.
- **Mechanism:** The framework expresses past iterates $\theta^{(n-k)}$ in terms of the current iterate $\theta^{(n)}$ using Taylor expansion. By substituting this history into the update rule, the dependence on past steps collapses into a single "correction term" $M^{(n)}(\theta)$ added to the current gradient step.
- **Core assumption:** Memory decays sufficiently fast (Assumption 3.1) and the learning rate $h$ is small enough that $O(h^3)$ terms are negligible.
- **Evidence anchors:** [abstract] "approximate an optimization algorithm with memory by a memoryless algorithm plus a correction term"; [section 3.1] Derivation showing the correction term formation.
- **Break condition:** If the learning rate $h$ is large, or memory does not decay exponentially, the approximation bound ($O(h^2)$) fails.

### Mechanism 2: AdamW's Implicit Anti-Regularization
- **Claim:** The memory structure in AdamW (specifically the $\beta_2$ parameter) implicitly penalizes the $\ell_1$-norm of the gradient, which theoretically pushes the dynamics toward sharper minima and worse generalization compared to Lion.
- **Mechanism:** By applying the general framework to AdamW, the correction term $M^{(n)}$ reveals an implicit term proportional to $\nabla \|\nabla L(\theta)\|_1$. Since $\beta_2$ is typically close to 1, this term dominates and "anti-regularizes" the loss.
- **Core assumption:** $\beta_2 > \beta_1$ (standard for AdamW) and full-batch or large-batch training where noise doesn't dominate the signal.
- **Evidence anchors:** [abstract] "AdamW's memory introduces implicit anti-regularization by penalizing the ℓ1-norm of the gradient"; [section 4] "if weight decay is sufficiently small, memory anti-regularizes (large-batch) AdamW."
- **Break condition:** If mini-batch noise is high (small batch sizes), the noise-induced regularization may overwhelm the memory-based anti-regularization.

### Mechanism 3: Mini-Batch Noise as Gradient Variance Regularization
- **Claim:** Mini-batch sampling adds an implicit regularization term proportional to the variance of the gradient across batches.
- **Mechanism:** When analyzing the expectation of the correction term $M^{(n)}$ over random permutations of mini-batches, a second term emerges: $E[(\nabla g - \nabla L)^T(g - L)]$. This acts as a penalty on gradient variance, effectively regularizing the loss.
- **Core assumption:** Mini-batches are sampled via random permutation without replacement.
- **Evidence anchors:** [section 5.2] "additional regularization term... proportional to E∥∇L(π(1))−∇L(θ)∥²"
- **Break condition:** In full-batch gradient descent, this variance term is zero.

## Foundational Learning

- **Concept: Modified Equations (Backward Error Analysis)**
  - **Why needed here:** This is the mathematical bridge connecting a discrete iterative algorithm (code) to a continuous differential equation or a modified loss (theory).
  - **Quick check question:** If an algorithm follows the trajectory $\theta_{n+1} = \theta_n - h \nabla L$, does it minimize $L(\theta)$ or $L(\theta) + \frac{h}{2}\|\nabla L(\theta)\|^2$?

- **Concept: Exponential Moving Average (EMA) as Momentum**
  - **Why needed here:** The paper explicitly models memory as "momentum variables" (EMAs of gradients). Understanding the decay rate $\beta$ determines how much "history" is retained.
  - **Quick check question:** How does increasing the momentum parameter $\beta$ (e.g., 0.9 to 0.99) change the effective window of history and the strength of the implicit regularization?

- **Concept: Flat vs. Sharp Minima**
  - **Why needed here:** The paper interprets the correction terms (like gradient norm penalization) in the context of generalization.
  - **Quick check question:** Does adding a term $\|\nabla L(\theta)\|^2$ to the loss make the optimizer prefer a wide valley or a narrow spike?

## Architecture Onboarding

- **Component map:** Iterates $\theta^{(n)}, \dots, \theta^{(0)}$ and Learning Rate $h$ -> Memory Module (EMA with $\beta$ parameters) -> Converter (Taylor Expansion logic) -> Output (modified update step minimizing $\tilde{L}(\theta) = L(\theta) + \text{correction}$)

- **Critical path:** The validity of the theoretical prediction relies on the **approximation bound** holding. You must verify that the trajectory of the memory-ful algorithm stays within $O(h^2)$ of the memoryless approximation on your specific loss landscape.

- **Design tradeoffs:**
  - **Stability vs. Generalization:** Increasing $\beta_2$ in AdamW stabilizes training (smoother gradients) but increases implicit anti-regularization (worse generalization).
  - **Compute vs. Theory:** Computing the exact correction term $M^{(n)}$ explicitly requires Hessian-vector products ($\nabla^2 L$), which adds compute overhead.

- **Failure signatures:**
  - **High Learning Rate:** The theory assumes small $h$. If the learning rate is high, the $O(h^3)$ error term explodes, and the memoryless approximation will diverge from the actual trajectory.
  - **Small Batches:** The specific prediction that "Lion > AdamW" relies on the memory effect dominating. If batch size is small, noise dominates, and the generalization gap may close or invert.

- **First 3 experiments:**
  1.  **Trajectory Validation:** Implement the "memoryless iteration" (Eq. 4) explicitly alongside standard AdamW. Plot the $\ell_\infty$ norm of the weight difference over epochs to confirm the $O(h^2)$ bound holds.
  2.  **Beta Sweep:** Train a CNN (e.g., ResNet-50) on CIFAR-10 using AdamW. Sweep $\beta_2$ (e.g., 0.9 to 0.999) while keeping $\beta_1$ fixed. Verify that test accuracy drops as $\beta_2 \to 1$.
  3.  **Lion Comparison:** Run the same setup with the Lion optimizer. Check if Lion maintains higher test accuracy than high-$\beta_2$ AdamW, and verify if Lion's correction term remains small/magnitude-insignificant compared to AdamW's.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does mini-batch noise interact with the implicit regularization induced by memory, specifically in regimes where Lion fails to outperform Adam?
  - **Basis:** [explicit] Section 6 states that characterizing the effect of mini-batch noise is "out of scope" and "a work in progress."
  - **Why unresolved:** The current theoretical framework primarily addresses full-batch settings, leaving the specific interaction with batch size-dependent performance gaps untheorized.
  - **What evidence would resolve it:** A theoretical extension incorporating stochastic noise terms into the modified loss, supported by empirical validation across varying batch sizes.

- **Open Question 2:** What implicit regularization or anti-regularization effects does the memoryless approximation framework identify for higher-order algorithms like Shampoo?
  - **Basis:** [explicit] Section 6 notes that while approximation results hold for algorithms like Shampoo, "interpreting the higher-order corrections is not trivial" and is left for future work.
  - **Why unresolved:** The complexity of the correction terms for Shampoo prevents immediate interpretation of the implicit bias compared to AdamW or Lion.
  - **What evidence would resolve it:** Derivation of the specific correction terms for Shampoo and experimental analysis correlating these terms with generalization performance.

- **Open Question 3:** How does the validity of the approximation bound degrade in "edge of stability" regimes where learning rates are effectively large?
  - **Basis:** [inferred] Section 6 admits the bounds require learning rates to be "sufficiently small," which contrasts with the "edge of stability" phenomenon common in deep learning training.
  - **Why unresolved:** The theoretical guarantees may not hold for the larger learning rates typically used in practice to achieve optimal convergence.
  - **What evidence would resolve it:** A theoretical analysis of error bounds under large learning rates or empirical measurements of the trajectory divergence between the algorithm and its memoryless approximation in unstable regimes.

## Limitations

- The framework's approximation bounds are only guaranteed for sufficiently small learning rates and exponentially decaying memory.
- The specific claim that AdamW's $\beta_2$ parameter induces implicit anti-regularization depends on the assumption of full-batch or large-batch training where gradient noise doesn't dominate.
- The interpretation of AdamW's anti-regularization as specifically pushing toward "sharp minima" relies on the assumption that gradient norm penalties behave this way in practice.

## Confidence

- **High Confidence**: The mathematical framework for converting memory to a correction term is well-established through Taylor expansion and modified equations theory. The general approximation bound (Theorem 3.2) is rigorous.
- **Medium Confidence**: The empirical validation on CIFAR-10 and WikiText-2 supports the theoretical predictions, but the results are limited to specific architectures and datasets.
- **Low Confidence**: The interpretation of AdamW's anti-regularization as specifically pushing toward "sharp minima" relies on assumptions about gradient norm penalties that may not always hold.

## Next Checks

1. **Learning Rate Sensitivity**: Validate the approximation bound across different learning rates by plotting the trajectory difference between original and memoryless algorithms as learning rate increases. Determine the threshold where the $O(h^2)$ bound breaks.

2. **Generalization Across Architectures**: Test the AdamW vs Lion comparison on additional architectures (Vision Transformer, BERT) and datasets (ImageNet, larger language models) to confirm the generalization trends are robust.

3. **Noise Regularization Isolation**: Conduct controlled experiments varying batch size while keeping total gradient noise constant to isolate the memory-based anti-regularization from noise-induced regularization, confirming the mechanism described in Section 5.2.