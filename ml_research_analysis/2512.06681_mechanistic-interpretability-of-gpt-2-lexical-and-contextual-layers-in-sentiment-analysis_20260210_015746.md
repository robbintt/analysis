---
ver: rpa2
title: 'Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment
  Analysis'
arxiv_id: '2512.06681'
source_url: https://arxiv.org/abs/2512.06681
tags:
- sentiment
- layers
- contextual
- processing
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a mechanistic interpretability study of GPT-2
  that causally examines how sentiment information is processed across its transformer
  layers. Using systematic activation patching across all 12 layers, we test the hypothesized
  two-stage sentiment architecture comprising early lexical detection and mid-layer
  contextual integration.
---

# Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis

## Quick Facts
- arXiv ID: 2512.06681
- Source URL: https://arxiv.org/abs/2512.06681
- Reference count: 4
- Primary result: Early layers (0-3) act as lexical sentiment detectors, but contextual integration occurs in late layers (8-11) rather than predicted middle layers, falsifying hierarchical processing hypotheses.

## Executive Summary
This paper presents a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, the study tests a hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. The experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position-specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses are falsified. Instead of mid-layer specialization, contextual phenomena such as negation, sarcasm, and domain shifts are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

## Method Summary
The study uses activation patching on GPT-2 (117M parameters) in inference mode to causally examine layer-wise contributions to sentiment processing. A linear probe achieves 95% validation accuracy on the final layer representation for binary sentiment classification. The methodology employs two datasets: a Lexical Detection Dataset (1,000 test cases, 6 contextual types) and a Contextual Integration Dataset (8,000 test pairs, 14 phenomena). For each layer independently, source activations are patched into target inputs, and the resulting change in sentiment probability is measured. Position specificity, variability, and total layer importance scores are computed to quantify each layer's role. The approach tests three hypotheses about middle-layer concentration, phenomenon specificity, and distributed processing, all of which are subsequently falsified by the experimental results.

## Key Results
- Early layers (0-3) encode stable, position-specific lexical sentiment signals independent of context (specificity score 0.147, variability 0.038)
- Contextual integration (negation, sarcasm, domain shifts) occurs primarily in late layers (8-11) through a unified mechanism, not in middle layers as predicted
- No middle-layer specialization observed; 8 of 15 phenomena peak at L11, with 87% sharing identical top-3 layers [L11, L10, L9]
- Total layer importance follows a monotonic gradient from early (15%) to late (46%) layers, forming a concentrated processing architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-2's early layers (0-3) detect lexical sentiment through position-specific, context-independent signals.
- Mechanism: Early layers encode stable polarity signals at exact token positions of sentiment words, with low variability (mean = 0.038) across different linguistic contexts. These layers localize sentiment information rather than processing sentences holistically.
- Core assumption: The model architecture requires stable lexical foundations before contextual modification can occur.
- Evidence anchors:
  - [abstract] "early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context"
  - [Section 5.1.4] Results support all four lexical hypotheses: sensitivity, early-layer dominance, position specificity, and context independence
  - [corpus] Related work on LayerNorm removal confirms mechanistic interpretability methods scale to GPT-2 XL
- Break condition: If early-layer patching produces inconsistent effects across word positions, or if context variability matches later layers.

### Mechanism 2
- Claim: Contextual integration (negation, sarcasm, domain shifts) occurs primarily in late layers (8-11) through a unified, non-modular mechanism—not in middle layers as predicted.
- Mechanism: Late layers function as a semantic integration hub where diverse phenomena (87% of tested types) converge on the same top-3 layers [L11, L10, L9]. The system routes semantically distinct modifications through shared computational pathways rather than specialized modules.
- Core assumption: Contextual reasoning requires high-level semantic representations that only emerge after substantial layer-wise transformation.
- Evidence anchors:
  - [abstract] "contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism"
  - [Section 5.2.2] 8 of 15 phenomena peak at L11; 87% share identical top-3 layers
  - [corpus] Limited direct corpus evidence for late-layer semantic integration; related work focuses on attention head behavior rather than layer-wise sentiment processing
- Break condition: If different contextual phenomena show distinct peak layers suggesting modularity, or if middle layers (4-7) show dominant effects.

### Mechanism 3
- Claim: Layer importance follows a monotonic gradient rather than uniform distribution, with late layers dominating (46%) followed by mid (39%) and early (15%) layers.
- Mechanism: Total layer importance scores increase 6.7-fold from L0 (828.7) to L11 (5,537.1), forming a concentrated gradient rather than distributed processing. Top-5 most important layers form a consecutive late-layer sequence [L11, L10, L9, L8, L7].
- Core assumption: Sentiment computation accumulates and refines representations progressively rather than processing in parallel across layers.
- Evidence anchors:
  - [Section 5.2.3] "monotonic decrease from late to early layers indicates a concentrated rather than distributed processing architecture"
  - [Section 5.2.4] Late layers handle 46% of total processing weight based on 8,000 test cases
  - [corpus] "Progressive Localisation in Localist LLMs" paper confirms gradual attention locality changes from distributed early layers to localized late layers
- Break condition: If importance scores plateau or show non-monotonic patterns with middle-layer peaks.

## Foundational Learning

- Concept: **Activation Patching**
  - Why needed here: The paper's causal claims depend on this intervention technique—selectively substituting activations between contrasting inputs to measure each layer's contribution. Correlational probing cannot establish causation.
  - Quick check question: Can you explain why high probing accuracy doesn't prove a model uses specific features?

- Concept: **Three-Stage Hierarchical Hypothesis (and its failure)**
  - Why needed here: The paper explicitly falsifies the predicted "early=surface, middle=syntax, late=semantics" pipeline. Understanding this prior framework is essential to recognize why the findings are surprising.
  - Quick check question: What layer range did researchers predict would handle contextual integration, and where did it actually occur?

- Concept: **Bimodal vs. Distributed Processing**
  - Why needed here: Results show contextual phenomena cluster in early OR late layers with no middle peaks—a bimodal pattern. This contradicts both the middle-layer concentration hypothesis and uniform distribution hypothesis.
  - Quick check question: If 57% of phenomena peak at L11 and 43% peak at L0-L2, what does this suggest about middle layers (L4-L7)?

## Architecture Onboarding

- Component map:
  Layers 0-3 (Lexical Detection) -> Layers 4-7 (Transitional) -> Layers 8-11 (Contextual Integration Hub)

- Critical path:
  1. Input → Tokenization → L0-L3 extract stable lexical sentiment at word positions
  2. L4-L7 provide gradient transition (no peak specialization)
  3. L8-L11 perform unified contextual integration (negation, sarcasm, intensification)
  4. Final representation → Sentiment classification (95% probe accuracy)

- Design tradeoffs:
  - **Unified vs. Modular**: GPT-2 sacrifices phenomenon-specific optimization for general-purpose late-layer integration—simpler but less interpretable
  - **Bimodal clustering**: Early lexical + late contextual creates a "hollow middle" architecture where L4-L7 lack distinctive functional roles
  - **Assumption**: The non-modular integration may limit fine-grained control over specific contextual phenomena

- Failure signatures:
  - If activation patching at L4-L7 produces strong sentiment shifts, the hypothesized architecture is incorrect
  - If different phenomena show distinct peak layers (e.g., negation at L6, sarcasm at L8), the unified mechanism claim fails
  - If importance scores are uniform across layers, the gradient concentration claim fails

- First 3 experiments:
  1. **Replicate lexical sensitivity**: Patch sentiment words vs. non-sentiment words at L0-L3; verify position specificity (target: mean specificity ~0.147, p < 0.001).
  2. **Test middle-layer falsification**: Run activation patching across all 12 layers for 15 contextual phenomena; confirm zero peaks at L4-L7.
  3. **Verify unified hub convergence**: For 8+ diverse phenomena, confirm top-3 layers match [L11, L10, L9] pattern within ±1 layer tolerance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the two-stage lexical-then-contextual architecture generalize to other transformer architectures (e.g., BERT, RoBERTa) and larger model scales?
- Basis in paper: [explicit] The conclusion explicitly states future work should "validate these patterns across diverse transformer architectures... to determine whether two-stage lexical-contextual processing represents a general architectural principle."
- Why unresolved: This study exclusively tested GPT-2 small (117M parameters); findings may be specific to this model's scale or decoder-only architecture.
- Evidence to resolve: Replicate the activation patching protocol across varying model sizes (e.g., GPT-4) and architectures (e.g., encoder-only BERT) to see if late-layer contextual integration persists.

### Open Question 2
- Question: Which specific attention heads and MLP blocks within the late layers constitute the "unified mechanism" for contextual integration?
- Basis in paper: [explicit] The conclusion calls for "fine-grained circuit-level analysis" to identify precise components, moving beyond layer-wise analysis.
- Why unresolved: The current methodology isolated effects at the layer level but did not pinpoint the specific computational sub-circuits (heads/neurons) responsible for handling negation, sarcasm, and intensification.
- Evidence to resolve: Apply head-level or neuron-level activation patching to map the exact internal pathways used for different contextual phenomena.

### Open Question 3
- Question: Does the rejection of middle-layer specialization for sentiment hold for other complex semantic tasks requiring contextual reasoning?
- Basis in paper: [inferred] The paper challenges "hierarchical models of transformer processing" based solely on sentiment data, leaving open the question of whether this falsification applies to other linguistic functions.
- Why unresolved: Sentiment involves specific pragmatic cues (e.g., sarcasm); it is unclear if the observed late-layer concentration is unique to sentiment or a general feature of semantic integration.
- Evidence to resolve: Apply the same causal intervention framework to other tasks like natural language inference or coreference resolution.

## Limitations
- The mechanistic interpretability findings depend critically on the activation-patching methodology, which assumes that replacing activations at a given layer captures that layer's isolated contribution without confounding effects from residual connections or layer-wise normalization.
- The unified late-layer integration hypothesis emerges from a relatively small set of 15 contextual phenomena—extending to more diverse linguistic phenomena could reveal additional modularity.
- The 95% probe accuracy, while high, doesn't guarantee the probe extracts the same features GPT-2 uses internally, as probing studies often show.

## Confidence
- **High Confidence**: Early-layer lexical detection claims (L0-L3 position-specific sentiment encoding with context independence). These are supported by clear statistical patterns (specificity 0.147, variability 0.038) across 1,000 test cases.
- **Medium Confidence**: Late-layer unified integration mechanism. While 87% of phenomena share top-3 layers, the non-modular hypothesis lacks comparative analysis against potential modular alternatives.
- **Low Confidence**: The complete falsification of middle-layer specialization. With only 39% total importance but no peaks, middle layers might still perform subtle processing not captured by the coarse-grained sentiment classification probe.

## Next Checks
1. **Probe Feature Analysis**: Conduct ablation studies removing LayerNorm and residual connections to verify activation-patching isolates layer contributions. Compare results with different probe architectures (MLP vs. logistic regression).
2. **Phenomenon Expansion**: Test the unified integration hypothesis with 10+ additional contextual phenomena including metaphor, ambiguity, and long-range dependencies. Verify whether these maintain the [L11, L10, L9] peak pattern.
3. **Cross-Model Replication**: Apply identical activation-patching methodology to GPT-2 Medium (355M) and GPT-2 Large (774M). Determine whether the bimodal architecture persists at scale or if middle layers develop specialized functions with increased capacity.