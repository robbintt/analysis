---
ver: rpa2
title: Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better
arxiv_id: '2506.09040'
source_url: https://arxiv.org/abs/2506.09040
tags:
- visual
- semantic
- asvr
- arxiv
- supervision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Autoregressive Semantic Visual Reconstruction (ASVR) addresses
  the limitation of typical large vision-language models (LVLMs), which apply autoregressive
  supervision only to textual outputs, overlooking fine-grained visual information.
  ASVR introduces a unified autoregressive framework that jointly trains visual and
  textual modalities by autoregressively reconstructing the semantic content of input
  images using a pretrained semantic visual tokenizer.
---

# Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better

## Quick Facts
- arXiv ID: 2506.09040
- Source URL: https://arxiv.org/abs/2506.09040
- Authors: Dianyi Wang; Wei Song; Yikun Wang; Siyuan Wang; Kaicheng Yu; Zhongyu Wei; Jiaqi Wang
- Reference count: 20
- Key outcome: Autoregressive Semantic Visual Reconstruction (ASVR) improves LLaVA-1.5 by 5% average across 14 multimodal benchmarks

## Executive Summary
Typical large vision-language models (LVLMs) apply autoregressive supervision only to textual outputs, overlooking fine-grained visual information. This paper introduces Autoregressive Semantic Visual Reconstruction (ASVR), a unified autoregressive framework that jointly trains visual and textual modalities by autoregressively reconstructing the semantic content of input images using a pretrained semantic visual tokenizer. ASVR enables models to effectively reconstruct discrete semantic tokens from continuous image features, consistently improving multimodal comprehension.

The approach demonstrates significant performance gains across varying data scales (556k-2M), visual input types, visual supervision strategies, and model architectures. Notably, ASVR outperforms denoising-based visual reconstruction methods and shows robust generalization across different LLM backbones and high-resolution scenarios.

## Method Summary
ASVR introduces a unified autoregressive framework that jointly trains visual and textual modalities by autoregressively reconstructing the semantic content of input images. The method uses a pretrained semantic visual tokenizer to convert images into discrete semantic tokens, which are then jointly reconstructed with textual outputs during autoregressive training. This approach addresses the limitation of typical LVLMs that apply autoregressive supervision only to textual outputs, overlooking fine-grained visual information. The framework enables effective reconstruction of discrete semantic tokens from continuous image features, improving multimodal comprehension across various conditions.

## Key Results
- ASVR improves LLaVA-1.5 by 5% average scores across 14 multimodal benchmarks
- Consistent improvements demonstrated across varying data scales (556k-2M)
- Outperforms denoising-based visual reconstruction methods
- Shows robust generalization across different LLM backbones and high-resolution scenarios

## Why This Works (Mechanism)
ASVR works by addressing a fundamental limitation in current LVLMs: the disconnect between visual and textual autoregressive training. By introducing a unified framework that jointly reconstructs both modalities, the model learns stronger cross-modal representations. The semantic visual tokenizer provides a bridge between continuous image features and discrete tokens that can be autoregressively predicted alongside text. This joint reconstruction forces the model to maintain visual-semantic consistency throughout the generation process, rather than treating vision and language as separate streams that only interact at the final output stage.

## Foundational Learning
- **Vision-Language Pre-training**: Needed to establish basic cross-modal understanding; quick check: model can align visual features with textual concepts
- **Semantic Tokenization**: Required to convert continuous visual information into discrete tokens for autoregressive modeling; quick check: tokenizer preserves semantic meaning across diverse image types
- **Autoregressive Training**: Essential for sequential generation and modeling dependencies; quick check: model can predict next token given previous context
- **Multimodal Fusion**: Critical for integrating visual and textual information; quick check: model can answer visual questions using both modalities
- **Cross-modal Attention**: Necessary for establishing relationships between vision and language; quick check: model attends to relevant visual regions when processing text

## Architecture Onboarding

Component map: Image -> Visual Tokenizer -> Semantic Tokens -> ASVR Model -> Joint Text+Vision Generation

Critical path: The core innovation lies in the joint autoregressive reconstruction loop where semantic tokens from the visual tokenizer are predicted alongside textual tokens, creating a unified training objective that strengthens cross-modal representations.

Design tradeoffs: The approach trades increased computational complexity during training for improved multimodal understanding. The reliance on a pretrained semantic visual tokenizer introduces potential bottlenecks but provides a stable bridge between continuous visual features and discrete autoregressive targets.

Failure signatures: Poor performance may manifest when the visual tokenizer fails to capture semantic content accurately, when the joint reconstruction objective overwhelms the model's capacity, or when the autoregressive visual reconstruction conflicts with the primary task objectives.

First experiments:
1. Validate that the semantic visual tokenizer can accurately reconstruct simple images when used in isolation
2. Test joint reconstruction performance on a small, controlled dataset before scaling up
3. Compare ASVR performance against baseline models on a single benchmark to establish initial improvements

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Generalizability beyond tested model architectures remains uncertain
- Long-term stability of improvements when scaling to larger datasets or different visual domains is unknown
- Reliance on pretrained semantic visual tokenizer may introduce bottlenecks if tokenizer coverage is limited or biased

## Confidence
- High confidence in technical feasibility of the autoregressive framework for joint visual-textual reconstruction
- Medium confidence in "consistent improvements" claim across varying conditions due to limited variance analysis
- Medium confidence in robustness generalization claims as evaluation covers multiple architectures but may not represent full real-world diversity

## Next Checks
1. Test ASVR on broader visual domains (medical imaging, satellite imagery) to assess semantic tokenizer limitations and generalization beyond natural images
2. Conduct ablation studies to quantify individual contributions of visual supervision strategy, model architecture, and data scale to observed improvements
3. Evaluate long-term performance and stability of ASVR-trained models when fine-tuned on downstream tasks after extended periods or with additional data