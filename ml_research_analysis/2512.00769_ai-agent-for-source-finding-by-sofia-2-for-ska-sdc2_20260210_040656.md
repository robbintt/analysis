---
ver: rpa2
title: AI Agent for Source Finding by SoFiA-2 for SKA-SDC2
arxiv_id: '2512.00769'
source_url: https://arxiv.org/abs/2512.00769
tags:
- agent
- training
- data
- parameters
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes an AI agent based on Soft Actor-Critic (SAC)
  to automatically optimize parameters for the SoFiA-2 source-finding algorithm. The
  agent interacts with simulated HI spectral cubes from the SKA-SDC2 dataset, using
  the SDC2 score as a reward signal to iteratively adjust key parameters (detection
  threshold, reliability threshold, scale kernel, and minimum SNR).
---

# AI Agent for Source Finding by SoFiA-2 for SKA-SDC2

## Quick Facts
- arXiv ID: 2512.00769
- Source URL: https://arxiv.org/abs/2512.00769
- Reference count: 6
- Proposes an AI agent based on Soft Actor-Critic (SAC) to automatically optimize parameters for the SoFiA-2 source-finding algorithm, achieving superior performance to benchmark parameters within 100 evaluation steps.

## Executive Summary
This work introduces a reinforcement learning approach to optimize the parameter configuration of the SoFiA-2 source-finding algorithm for the SKA-SDC2 dataset. By framing parameter tuning as a Markov Decision Process and employing the Soft Actor-Critic algorithm, the agent learns to maximize a reward function based on the SDC2 scoring metric. The method successfully identifies parameter configurations that outperform the benchmark set by Team SoFiA, demonstrating improved completeness and reliability across multiple sky patches while reducing computational time.

## Method Summary
The method employs a Soft Actor-Critic (SAC) reinforcement learning agent to optimize four continuous parameters of the SoFiA-2 source finder: detection threshold, reliability threshold, scale kernel, and minimum SNR. The agent interacts with simulated HI spectral cubes from the SKA-SDC2 dataset, using a custom Gymnasium environment that runs SoFiA-2 and evaluates output against ground truth. The state vector includes current parameter values and the score ratio, while actions represent updates to parameter values. Training proceeds for 10,000 steps across three representative training patches, with the agent learning to maximize a reward function that scales exponentially with the SDC2 score ratio and penalizes invalid outputs.

## Key Results
- The SAC agent outperforms the benchmark parameters within only 100 evaluation steps on test data
- Feature importance analysis confirms detection threshold as the most influential parameter
- Training on multiple representative patches improves generalization compared to single-patch training
- The method achieves reduced computational time compared to manual or grid-based parameter searches

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Regularized Policy Search for Continuous Parameters
The SAC algorithm enables efficient navigation of high-dimensional, continuous parameter spaces where grid search is computationally prohibitive. The agent optimizes a stochastic policy by maximizing an entropy-augmented objective, driving exploration of diverse parameter configurations while exploiting high-reward regions, preventing premature convergence to local optima.

### Mechanism 2: Reward Shaping via Ground-Truth Fidelity
The agent improves source detection quality by directly optimizing a composite reward function derived from the SDC2 scoring metric, which balances completeness and purity. The reward function applies exponential scaling to the score ratio and penalties for null results, creating dense feedback that guides the agent away from detection thresholds yielding too many false positives or missed sources.

### Mechanism 3: Generalization via Representative Spatial Sampling
Training the agent simultaneously on multiple representative sky patches (sparse and dense fields) enforces a robust policy that generalizes to unseen data regions. Aggregating scores from three distinct patches during training acts as data augmentation, preventing overfitting to the specific spatial covariance of a single field.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) in Scientific Software**
  - Why needed here: To frame the static software (SoFiA-2) as a dynamic environment where parameter states change and yield scalar rewards
  - Quick check question: Can you define the "State" and "Action" space for the SoFiA-2 agent? (Answer: State = current params + score ratio; Action = delta update to params)

- **Concept: Trade-offs in Source Finding (Completeness vs. Purity)**
  - Why needed here: The agent is essentially balancing the detection threshold against reliability filters
  - Quick check question: What happens to the score if `scfind.threshold` is set too low? (Answer: False detections increase, potentially lowering the reliability score/matched count ratio)

- **Concept: Off-Policy Learning & Replay Buffers**
  - Why needed here: SAC is off-policy, learning from past experiences stored in a replay buffer for sample efficiency
  - Quick check question: Why is sample efficiency crucial for this specific AI agent? (Answer: Because each "step" requires running the full SoFiA-2 pipeline on a data cube)

## Architecture Onboarding

- **Component map:** Agent (SAC) -> Environment (Gymnasium wrapper) -> SoFiA-2 executable -> Oracle (SDC2 Scorer) -> Data (SKA-SDC2 patches)
- **Critical path:** 1. Initialization: Random parameter sampling within defined ranges; 2. Execution: SoFiA-2 runs on training patches using sampled parameters; 3. Evaluation: SDC2 Scorer calculates score ratio; 4. Update: SAC computes reward, updates Critic and Actor networks; 5. Deployment: Run trained agent on test patch for 100 steps; select parameters with highest observed score
- **Design tradeoffs:** Small patches are fast but exhibit high variance; large patches offer stable gradients but slower iteration; excluding list-based parameters simplifies action space but limits optimization potential
- **Failure signatures:** Score collapse (reward stuck at -5.0 from null output); overfitting (high training scores but high variance in evaluation); non-convergence (score oscillates without trend)
- **First 3 experiments:** 1. Sanity Check: Run Team SoFiA benchmark parameters on training patches to establish baseline score; 2. Overfit Probe: Train on single small patch to observe instability and validate need for larger aggregate patches; 3. Feature Importance: Train Random Forest regressor on parameter-score logs to confirm detection threshold dominance

## Open Questions the Paper Calls Out

### Open Question 1
How can the AI agent framework be extended to optimize list-based parameters, such as kernel sizes in the S+C finder module?
- Basis: The authors state that tuning such parameters introduces additional challenges beyond the current scope
- Why unresolved: Current implementation is restricted to scalar continuous values, while lists represent a higher-dimensional and variable-length action space that standard continuous control algorithms don't natively support

### Open Question 2
What advanced strategies can mitigate the computational cost of sequential reinforcement learning training when increasing the number of parameters or their value ranges?
- Basis: Section 7 notes that expanding parameter search spaces significantly raises computational cost due to the sequential nature of reinforcement learning
- Why unresolved: Current training procedure requires sequential execution of SoFiA-2 over thousands of steps; scaling to higher dimensions without optimization is computationally prohibitive

### Open Question 3
Can the agent be adapted to perform effectively on real observational data where comprehensive ground truth catalogs are unavailable?
- Basis: The paper notes that the agent's dependence on ground truth limits its direct application to real observational data
- Why unresolved: Current reward mechanism relies on calculating the SDC2 score against a known truth catalog, which is absent in genuine observation scenarios

## Limitations
- The agent's effectiveness on real observational data with unknown systematics remains untested
- List-based parameters (kernels) are excluded from optimization, representing a significant limitation
- Claims about computational efficiency gains lack systematic benchmarking against alternative optimization methods

## Confidence
- **High Confidence:** SAC algorithm successfully navigates continuous parameter space and improves upon benchmark parameters; feature importance analysis is methodologically sound
- **Medium Confidence:** Generalization claims supported by cross-patch validation but rely on training patches adequately representing target data distribution; reward function design appropriate for SDC2 metric
- **Low Confidence:** Computational efficiency claims based on anecdotal comparisons rather than systematic benchmarking

## Next Checks
1. **Cross-Survey Validation:** Apply the trained agent to source-finding tasks on independent radio astronomy datasets (e.g., ASKAP or VLA surveys) to test generalization beyond SKA-SDC2 simulations
2. **Algorithm Comparison Benchmark:** Implement and compare the SAC approach against established parameter optimization techniques using identical computational budgets to quantify trade-offs
3. **Real Observational Test:** Run the optimized parameter sets on real HI spectral line observations with independently verified source catalogs to reveal whether the agent learns to exploit simulation artifacts or genuinely improves scientific extraction quality