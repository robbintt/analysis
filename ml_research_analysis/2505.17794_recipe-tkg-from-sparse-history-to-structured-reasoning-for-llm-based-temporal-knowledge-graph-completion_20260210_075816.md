---
ver: rpa2
title: 'RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal
  Knowledge Graph Completion'
arxiv_id: '2505.17794'
source_url: https://arxiv.org/abs/2505.17794
tags:
- sampling
- hits
- recipe-tkg
- temporal
- history
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RECIPE-TKG addresses the challenge of improving temporal knowledge
  graph completion when historical evidence is sparse or indirect. It introduces a
  three-stage framework: rule-based multi-hop history sampling to retrieve structurally
  diverse context, contrastive fine-tuning with lightweight adapters to shape relational
  semantics, and test-time semantic filtering to refine predictions based on embedding
  similarity.'
---

# RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2505.17794
- Source URL: https://arxiv.org/abs/2505.17794
- Reference count: 40
- Key outcome: Up to 30.6% relative improvement in Hits@10 over baseline LLM-based TKG completion methods, particularly in sparse history scenarios.

## Executive Summary
RECIPE-TKG introduces a novel three-stage framework for temporal knowledge graph completion that addresses the challenge of sparse historical evidence. The method combines rule-based multi-hop history sampling, contrastive fine-tuning with lightweight adapters, and test-time semantic filtering to improve LLM-based reasoning on temporal knowledge graphs. By explicitly retrieving structurally diverse historical context and refining predictions through semantic similarity filtering, RECIPE-TKG demonstrates significant performance gains across multiple benchmarks while maintaining computational efficiency through adapter-based fine-tuning.

## Method Summary
RECIPE-TKG addresses sparse history in temporal knowledge graph completion through a three-stage approach: (1) rule-based multi-hop history sampling retrieves structurally diverse temporal context using predefined mining rules; (2) contrastive fine-tuning with lightweight adapters shapes relational semantics while maintaining computational efficiency; and (3) test-time semantic filtering refines predictions by comparing embedding similarity. The framework integrates symbolic reasoning with neural methods, using structural patterns to guide history retrieval and semantic representations to filter predictions, ultimately improving both accuracy and semantic coherence in low-context settings.

## Key Results
- Achieves up to 30.6% relative improvement in Hits@10 compared to baseline LLM-based methods
- Outperforms previous approaches on ICEWS05-15, ICEWS18, and YAGO datasets across multiple metrics (Hits@1, Hits@3, Hits@10)
- Demonstrates particular strength in low-context scenarios where historical evidence is sparse or indirect

## Why This Works (Mechanism)
RECIPE-TKG's effectiveness stems from its multi-stage approach that addresses the fundamental challenge of limited historical context in temporal knowledge graphs. The rule-based multi-hop sampling strategy ensures retrieval of structurally diverse and relevant historical facts that might otherwise be missed by simple nearest-neighbor approaches. The contrastive fine-tuning stage shapes the model's understanding of temporal relationships through exposure to positive and negative examples, while the lightweight adapter architecture maintains efficiency. Finally, the semantic filtering stage leverages learned embeddings to identify and suppress predictions that lack coherence with established relation patterns, effectively reducing noise in the final output.

## Foundational Learning
- **Temporal Knowledge Graphs**: Knowledge representations that include time as a critical dimension, requiring reasoning over both structural and temporal patterns. *Why needed*: Traditional KGs lack temporal context, making them insufficient for reasoning about time-dependent relationships. *Quick check*: Can represent facts as (head, relation, tail, timestamp) tuples.
- **Multi-hop Reasoning**: The ability to traverse multiple edges in a graph to infer connections between distant nodes. *Why needed*: Direct connections may be sparse; indirect paths through intermediate entities can provide crucial context. *Quick check*: Can retrieve facts through paths of length 2 or more.
- **Contrastive Learning**: Training methodology that learns representations by comparing similar and dissimilar examples. *Why needed*: Helps the model distinguish between valid and invalid temporal relationships. *Quick check*: Model should correctly classify known positive and negative examples.
- **Lightweight Adapters**: Small neural modules inserted into pre-trained models for efficient fine-tuning. *Why needed*: Full fine-tuning is computationally expensive and may cause catastrophic forgetting. *Quick check*: Adapter parameters should be <<1% of total model parameters.
- **Semantic Filtering**: Post-processing step that uses learned embeddings to validate and refine predictions. *Why needed*: Raw model outputs may contain semantically incoherent predictions that need filtering. *Quick check*: Filtering should remove low-similarity predictions while preserving correct ones.

## Architecture Onboarding

**Component Map**: Rule Mining -> Multi-hop History Sampling -> Contrastive Fine-tuning -> Semantic Filtering -> Final Predictions

**Critical Path**: The core workflow follows: historical facts are sampled using rule-based multi-hop traversal, contrastive fine-tuning shapes the model's understanding of temporal relationships, and semantic filtering refines final predictions based on embedding similarity.

**Design Tradeoffs**: RECIPE-TKG trades computational overhead during the rule mining and history sampling phases for improved accuracy during inference. The use of lightweight adapters instead of full fine-tuning reduces computational cost but may limit the depth of adaptation possible. The semantic filtering approach adds a post-processing step that improves prediction quality but requires maintaining and computing relation embeddings.

**Failure Signatures**: Performance degradation may occur when historical facts are too sparse for meaningful multi-hop traversal, when the learned embeddings fail to capture subtle semantic distinctions between relations, or when the contrastive fine-tuning lacks sufficient positive examples. The system may also struggle with temporal granularity mismatches between training and inference data.

**First Experiments**:
1. Ablation study removing semantic filtering to quantify its contribution to overall performance
2. Stress test with artificially reduced historical context to measure performance degradation
3. Runtime analysis comparing adapter-based fine-tuning vs. full fine-tuning on computational resources

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can RECIPE-TKG be extended to handle partially observed or noisy histories where temporal information is incomplete or uncertain?
- Basis in paper: The limitations section states: "the framework assumes full observability of historical events, while in practice, such information may be incomplete or noisy. Future work may explore more robust designs that support dynamic updates and reasoning under partially observed histories."
- Why unresolved: The current framework requires clean, fully observed TKGs, limiting real-world applicability where data is inherently messy.
- What evidence would resolve it: Experiments on synthetic or real datasets with controlled missingness patterns showing maintained performance under partial observability.

### Open Question 2
- Question: How can the rule mining step be adapted for dynamically changing TKGs without requiring complete re-computation?
- Basis in paper: The limitations section notes: "The rule mining step requires offline learning before sampling, and must be repeated if the TKG changes."
- Why unresolved: Rule mining is computationally expensive; the current design assumes a static graph structure during deployment.
- What evidence would resolve it: An incremental rule update mechanism that achieves comparable accuracy to full re-training with reduced computational cost.

### Open Question 3
- Question: What dataset or structural characteristics explain why RECIPE-TKG underperforms TLogic on GDELT despite gains on ICEWS and YAGO?
- Basis in paper: Table 2 shows RECIPE-TKG does not outperform TLogic on GDELT (Hits@1: 0.095 vs 0.113), unlike other benchmarks. The paper does not analyze this discrepancy.
- Why unresolved: GDELT has finer temporal granularity (15 minutes vs. 1 day) and may contain different relational patterns that favor symbolic rule-based reasoning.
- What evidence would resolve it: Controlled experiments varying temporal granularity and relation distributions across datasets to isolate failure factors.

## Limitations
- Assumes full observability of historical events, limiting applicability to real-world scenarios with incomplete or noisy data
- Rule mining requires offline computation and must be repeated when the TKG changes, creating computational overhead
- Performance on GDELT dataset lags behind symbolic rule-based approaches, suggesting limitations with certain temporal granularities

## Confidence
**High Confidence**: Experimental results showing performance improvements over baseline methods, particularly the Hits@10 metrics and relative improvement percentages (up to 30.6%), are well-supported by presented data and methodology.

**Medium Confidence**: Claims about generating more semantically coherent predictions are supported by qualitative analysis but could benefit from more rigorous, quantitative semantic coherence measures.

**Medium Confidence**: Assertion that RECIPE-TKG particularly excels in low-context settings is supported by experimental results, but specific threshold characterization for performance degradation in extremely sparse scenarios is incomplete.

## Next Checks
1. Conduct ablation studies systematically removing each component (multi-hop sampling, contrastive fine-tuning, semantic filtering) to quantify individual contribution to overall performance gains.

2. Test RECIPE-TKG on additional temporal knowledge graph datasets with different characteristics (varying density, different temporal granularities, different domains) to assess generalizability beyond the four benchmark datasets.

3. Perform robustness analysis by introducing controlled amounts of noise in historical facts and measuring how performance degrades across different levels of sparsity and noise, particularly focusing on lower bounds of recoverable information.