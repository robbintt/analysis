---
ver: rpa2
title: 'BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains'
arxiv_id: '2510.25409'
source_url: https://arxiv.org/abs/2510.25409
tags:
- uni00000044
- uni00000051
- uni00000048
- uni00000003
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BhashaBench V1 is a comprehensive, domain-specific, bilingual benchmark
  designed to evaluate large language models on India-centric knowledge systems across
  Agriculture, Legal, Finance, and Ayurveda domains. It contains 74,166 question-answer
  pairs (52,494 in English, 21,672 in Hindi) curated from authentic government and
  professional examination papers, spanning 90+ subdomains and 500+ topics.
---

# BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains

## Quick Facts
- **arXiv ID:** 2510.25409
- **Source URL:** https://arxiv.org/abs/2510.25409
- **Reference count:** 40
- **Primary result:** BhashaBench V1 evaluates 29+ LLMs on India-centric knowledge across Agriculture, Legal, Finance, and Ayurveda domains, revealing significant performance gaps (59.74-76.49% accuracy) and language disparities.

## Executive Summary
BhashaBench V1 introduces a comprehensive bilingual benchmark designed to evaluate large language models on India-centric knowledge systems. The benchmark encompasses 74,166 question-answer pairs across four critical domains—Agriculture, Legal, Finance, and Ayurveda—with 52,494 questions in English and 21,672 in Hindi. These questions are curated from authentic government and professional examination papers, covering 90+ subdomains and 500+ topics. The benchmark addresses the gap in evaluating LLMs on culturally and regionally relevant knowledge, providing a rigorous test of models' capabilities in handling domain-specific terminology, mathematical reasoning, and traditional knowledge systems. Evaluation of 29+ LLMs reveals significant performance disparities across domains and languages, with models demonstrating stronger performance in Legal (76.49% accuracy) compared to Ayurveda (59.74%), and consistently performing better on English content compared to Hindi.

## Method Summary
BhashaBench V1 was constructed by systematically collecting and curating questions from authentic government and professional examination papers across four major domains relevant to India. The benchmark spans Agriculture (agricultural sciences and practices), Legal (constitutional and criminal law), Finance (banking, economics, and international finance), and Ayurveda (traditional medicine systems). Each domain was carefully mapped to its respective subdomains, ensuring comprehensive coverage of topics typically encountered in Indian professional examinations. The questions were manually verified for accuracy and relevance, with special attention paid to domain-specific terminology and cultural context. The benchmark's bilingual nature (English and Hindi) reflects India's linguistic diversity while maintaining consistency in question difficulty and domain coverage across both languages.

## Key Results
- Models achieved 76.49% accuracy in Legal domain but only 59.74% in Ayurveda, highlighting significant domain-specific performance gaps
- Cross-lingual evaluation showed consistent English superiority over Hindi, with performance disparities suggesting dataset quality or model capability differences
- Subdomain analysis revealed strong performance in Cyber Law and International Finance, while Panchakarma, Seed Science, and Human Rights remained notably weak

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its authentic question sources and comprehensive domain coverage. By utilizing actual government and professional examination questions, BhashaBench V1 ensures that evaluated models must demonstrate real-world competency in Indian professional contexts. The structured approach to domain and subdomain mapping creates a systematic evaluation framework that captures nuanced differences in model capabilities across specialized knowledge areas. The bilingual design, while introducing complexity, more accurately reflects India's linguistic landscape and tests models' ability to handle code-switching and cultural context across languages.

## Foundational Learning
- **Domain-specific terminology mastery** - Models must understand specialized vocabulary unique to Indian professional contexts; quick check: verify model handles legal/medical terms accurately
- **Cultural context comprehension** - Questions require understanding of Indian social, legal, and traditional systems; quick check: test model on culturally embedded scenarios
- **Mathematical reasoning in domain contexts** - Finance and Agriculture questions demand applied quantitative skills; quick check: evaluate numerical problem-solving accuracy
- **Multilingual proficiency** - Models must perform consistently across English and Hindi; quick check: compare parallel question performance across languages

## Architecture Onboarding

**Component Map:** Raw question corpus -> Domain classification -> Subdomain mapping -> Bilingual translation -> Quality verification -> Benchmark assembly

**Critical Path:** Question collection → Domain/Subdomain categorization → Translation (where applicable) → Expert verification → Benchmark integration → Model evaluation

**Design Tradeoffs:** Authenticity vs. generalization (using real exam questions may limit broader applicability), Bilingual coverage vs. depth (Hindi content is ~41% of dataset), Domain specificity vs. model flexibility (highly specialized questions may not transfer to general knowledge)

**Failure Signatures:** Domain-specific underperformance (e.g., Ayurveda at 59.74%), cross-lingual accuracy gaps, inability to handle traditional knowledge systems, weakness in subdomains requiring specialized cultural knowledge

**3 First Experiments:**
1. Evaluate a single top-performing model across all four domains to establish baseline performance distribution
2. Compare model performance on parallel English-Hindi question pairs to quantify cross-lingual capability gaps
3. Test model responses on historically difficult subdomains (Panchakarma, Human Rights) to identify specific failure patterns

## Open Questions the Paper Calls Out
- How do models handle questions requiring integration of multiple domain knowledge areas?
- What is the impact of temporal knowledge changes on model performance in dynamic domains like Finance and Cyber Law?
- Can models effectively transfer knowledge between closely related subdomains within the same domain?
- How does model performance vary with question complexity within each subdomain?
- What role does instruction-following capability play in domain-specific question answering?

## Limitations
- Construction from examination papers may limit generalizability to practical, real-world applications
- Bilingual scope excludes other major Indian languages, missing broader linguistic diversity
- Performance gaps between English and Hindi may reflect dataset quality differences rather than pure model capability
- Focus on professional examination content may not capture informal or conversational knowledge usage
- Static question set may not reflect evolving knowledge in rapidly changing domains

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Domain-specific performance variations are well-supported | High |
| Cross-lingual performance differences are demonstrated | Medium |
| Subdomain weakness identification is reliable | Medium |

## Next Checks
1. Conduct domain expert review of sample questions across all subdomains to validate question authenticity and difficulty calibration, particularly for low-performing areas like Human Rights and Panchakarma.
2. Perform cross-validation by translating a subset of questions bidirectionally between English and Hindi to assess whether performance gaps stem from language models or dataset quality differences.
3. Evaluate model performance on a temporal subset of questions (e.g., 2019-2020 vs 2021-2023) to assess whether current models can handle evolving knowledge in dynamic domains like Finance and Cyber Law.