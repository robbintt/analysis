---
ver: rpa2
title: 'PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup
  Route Prediction'
arxiv_id: '2505.03776'
source_url: https://arxiv.org/abs/2505.03776
tags:
- attention
- route
- prediction
- node
- proximity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PAPN, a supervised encoder-decoder architecture
  for route prediction in first-mile parcel pickup using proximity attention and pointer
  network decoder. The method combines local attention (based on reachability mask)
  and global context (via multi-head attention transformer encoder) to update node
  embeddings.
---

# PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction

## Quick Facts
- arXiv ID: 2505.03776
- Source URL: https://arxiv.org/abs/2505.03776
- Reference count: 15
- Outperforms state-of-the-art supervised methods on first-mile parcel pickup route prediction

## Executive Summary
This paper introduces PAPN, a supervised encoder-decoder architecture for route prediction in first-mile parcel pickup. The method combines local attention (based on reachability mask) and global context (via multi-head attention transformer encoder) to update node embeddings. Experiments on the LaDE dataset show PAPN outperforms all state-of-the-art supervised methods in most metrics, including KRC, LSD, and HR@3, and achieves competitive performance with the best reinforcement learning method DRL4Route.

## Method Summary
PAPN is an encoder-decoder architecture that predicts courier routes for parcel pickup. It uses proximity attention to capture local dependencies between currently-available pickup locations, then combines these local embeddings with global transformer embeddings. The pointer network decoder generates route sequences while respecting reachability constraints. The model is trained on the LaDE dataset with node features including task acceptance time, coordinates, distances, and time-to-pickup, along with edge features and per-timestep reachability masks.

## Key Results
- PAPN achieves higher KRC, LSD, and HR@3 than all state-of-the-art supervised methods
- Performance is competitive with reinforcement learning method DRL4Route
- Ablation study confirms the value of mixing local and global embeddings
- Top-k filtering on mask attention provides a tradeoff between constraint enforcement and solution space coverage

## Why This Works (Mechanism)

### Mechanism 1: Proximity Attention via Reachability Masks
Computing attention over reachability masks captures local dependencies between currently-available pickup locations, improving sequential prediction accuracy. The proximity attention layer projects node, edge, and mask embeddings into multi-head space, then computes attention scores via: α_ij^h = softmax(LeakyReLU(W_i X_i + W_j X_j + W_e E_ij + W_m M_i + W_m M_j)). This incorporates mask information directly into attention weights rather than treating availability as a post-hoc constraint.

### Mechanism 2: Local-Global Embedding Mixing
Combining proximity-attention local embeddings with transformer global embeddings yields more stable full-route predictions than either alone. Local embeddings from proximity attention are aggregated (sum/mean/max/min) to match global embedding dimensions, then mixed (sum or random selection) before decoder input.

### Mechanism 3: Pointer Network with Attention-Biased Selection
Using proximity attention scores to bias pointer network logits improves next-node prediction by skewing toward locations with high local inter-connectivity. The decoder computes pointer attention logits, then adds mask attention: P(π_t = i) = Softmax(u_ti + M_att) where M_att contains both reachability mask and attention scores from the encoder.

## Foundational Learning

- **Multi-head attention and Transformer encoder-decoder architectures**: The global context module uses standard transformer multi-head attention; understanding Q/K/V projections and residual connections is prerequisite. Quick check: Can you explain why multi-head attention concatenates outputs from H parallel attention functions rather than using a single larger projection?

- **Pointer Networks for variable-length sequence output**: The decoder must output sequences of varying lengths (different numbers of pickup locations per route); pointer networks enable pointing to input nodes rather than fixed vocabularies. Quick check: How does a pointer network differ from standard seq2seq with attention when the output vocabulary must match input elements?

- **Masking in autoregressive decoding**: Reachability masks constrain which nodes are valid at each timestep; understanding how masks are applied (element-wise multiplication with logits before softmax) is essential. Quick check: What happens if you apply a mask after softmax instead of before?

## Architecture Onboarding

- **Component map**: Input → Linear Embedding (nodes, edges, mask) → Proximity Attention Layer (local context + mask attention) → Transformer Encoder (global context) → Attention Mixing (aggregate local, combine with global) → Pointer Network Decoder (LSTM + glimpses + pointer attention + mask bias) → Route sequence output

- **Critical path**: Proximity attention outputs both updated node embeddings AND mask attention scores. Both paths must reach the decoder — node embeddings via mixing, mask attention via direct addition to logits.

- **Design tradeoffs**:
  - Top-k filtering on mask attention: Lower k increases precision but may over-constrain; higher k preserves options but dilutes signal.
  - Aggregation function for local embeddings: Sum preserves magnitude; mean normalizes; max/min capture extremes.
  - Learning rate: Deeper model requires lower LR (3e-5 vs 1e-4) to avoid overfitting.

- **Failure signatures**:
  - Rapid performance degradation after first few nodes → local-only embedding (missing global context).
  - Valid nodes never selected → top-k mask filtering too aggressive.
  - High variance across runs → learning rate too high for model depth.

- **First 3 experiments**:
  1. Reproduce ablation: Run OPAPN (local-only) vs PAPN-Mixed on same validation split to confirm mixing contribution (~0.15-0.2 KRC improvement expected).
  2. Aggregation function sweep: Test sum/mean/max/min for local embedding aggregation; paper suggests sum or mean as candidates.
  3. Learning rate validation: Compare 1e-4 vs 3e-5; expect lower LR to close gap with DRL4Route (~0.2 KRC improvement).

## Open Questions the Paper Calls Out

- **Can PAPN be extended to perform joint route and arrival time prediction while maintaining its performance advantages?**: The conclusion states this model can be integrated into a joint time-route prediction model and using the rich embeddings already available within the model for time prediction is promising. The current work focuses solely on route prediction; time prediction architecture and embedding integration remain unexplored.

- **What is the computational cost-to-performance tradeoff between PAPN and reinforcement learning methods like DRL4Route?**: The authors state the model poses the cost-return question when comparing both methods and note DRL4Route can raise training resources questions. No training time, inference latency, or resource consumption metrics are reported for either method.

- **How does PAPN performance degrade as the number of tasks per route increases beyond 25?**: Experiments only included instances with less than 25 tasks, and longer routes may stress the proximity attention mechanism differently. The reachability mask and attention computations scale with sequence length, but scalability limits are untested.

## Limitations
- Effectiveness depends critically on quality of reachability masks, which may not generalize well to different spatial-temporal constraints
- Aggregation strategy for mixing local and global embeddings is not fully specified
- Top-k filtering parameter for mask attention is unspecified, potentially affecting constraint enforcement
- Method requires careful hyperparameter tuning, particularly around learning rate selection

## Confidence
- **High confidence**: The core architecture combining proximity attention with transformer encoder is technically sound and well-supported by the ablation study.
- **Medium confidence**: Claims about superior performance versus all baselines are credible but depend on exact implementation details that are underspecified.
- **Medium confidence**: The mechanism explaining why mixing local and global embeddings improves stability is reasonable but not rigorously proven.

## Next Checks
1. **Ablation replication**: Reproduce the PAPN-Mixed vs OPAPN comparison on the same validation split to verify the ~0.15-0.2 KRC improvement claim.
2. **Aggregation function sweep**: Test all four aggregation methods (sum, mean, max, min) for local embedding aggregation to identify the optimal choice.
3. **Learning rate validation**: Compare training at 1e-4 vs 3e-5 to verify the performance gap and overfitting concerns.