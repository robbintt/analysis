---
ver: rpa2
title: 'AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise
  Orchestration for Effective and Efficient Collaboration'
arxiv_id: '2509.19236'
source_url: https://arxiv.org/abs/2509.19236
tags:
- agent
- expert
- team
- agentinit
- role
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AgentInit addresses the problem of effective multi-agent system
  initialization by introducing a framework that jointly optimizes agent team diversity
  and task relevance. It employs a two-module approach: standardized agent generation
  with iterative refinement and balanced team selection using Pareto optimization.'
---

# AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration

## Quick Facts
- arXiv ID: 2509.19236
- Source URL: https://arxiv.org/abs/2509.19236
- Authors: Chunhao Tian; Yutong Wang; Xuebo Liu; Zhexuan Wang; Liang Ding; Miao Zhang; Min Zhang
- Reference count: 40
- Primary result: Up to 1.2-1.6 point improvements in MAS performance with 50% token reduction

## Executive Summary
AgentInit introduces a framework for initializing multi-agent systems by jointly optimizing team diversity and task relevance. The method employs iterative agent generation with observer feedback followed by balanced team selection using Pareto optimization. Experiments demonstrate consistent performance improvements across nine benchmarks while significantly reducing token consumption compared to state-of-the-art initialization methods.

## Method Summary
AgentInit consists of two modules: (1) Standardized Agent Generation using Planner (task decomposition + agent construction), Formatter (NL-to-JSON), and Observer (feedback) agents iterated K=3 rounds, and (2) Balanced Team Selection that enumerates all teams within [Nmin=1, Nmax=5], computes relevance (cosine similarity) and diversity (Vendi Score via eigenvalue entropy), constructs Pareto frontier, and uses a Selector Agent to choose the final team. The framework operates on MiniLM embeddings and targets MAS frameworks like AutoGen and AgentPrune.

## Key Results
- 1.2-1.6 point improvement in overall performance across nine datasets
- 50% reduction in token consumption compared to baselines
- Pareto optimization outperforms optimizing relevance or diversity alone (91.3 vs 90.4-90.5)
- Redundancy elimination reduces max pairwise similarity from 58.73% to 54.96%

## Why This Works (Mechanism)

### Mechanism 1: Iterative Refinement with Observer Feedback
Multi-round agent generation with structured feedback improves candidate quality before selection. A Planner decomposes queries and generates agents; an Observer evaluates rationality and provides feedback ϕt; the process repeats for K rounds (default K=3). This mitigates single-pass generation biases and self-preference bias. Evidence shows K=1 yields 90.7 avg vs. K=3 at 91.3; K=5 shows diminishing returns at 91.2. Break condition: performance plateaus or degrades if feedback loops introduce noise.

### Mechanism 2: Pareto Optimization Balancing Relevance and Diversity
Joint optimization of task relevance and intra-team diversity via Pareto selection yields better-performing teams than optimizing either objective alone. All candidate team combinations within [Nmin, Nmax] are scored using Rel (cosine similarity) and Div (Vendi Score entropy). Pareto-optimal set T* contains non-dominated teams; a Selector LLM picks the final team. Evidence shows "Only Rel" = 90.4, "Only Div" = 90.5, both underperform AgentInit (91.3). Break condition: if candidate set lacks variance, Pareto frontier collapses to trivial solution.

### Mechanism 3: Redundancy Elimination Reduces Inference Overhead
Explicitly selecting against redundancy lowers token consumption without sacrificing performance. Maximum pairwise similarity Max(sij) within selected teams decreases after Balanced Team Selection; fewer redundant agents means less step repetition and task derailment. Evidence shows max pairwise similarity drops from 58.73% to 54.96% averaged across datasets. Prompt tokens reduced from ~1.9M (EvoAgent) to ~964K (AgentInit) for comparable tasks. Break condition: over-aggressive pruning could eliminate complementary perspectives.

## Foundational Learning

- **Pareto Dominance and Multi-Objective Optimization**
  - Why needed here: Team selection requires balancing competing objectives (relevance vs. diversity) without arbitrary weighting; Pareto theory formalizes trade-offs.
  - Quick check question: Given two teams A and B where A has higher relevance but lower diversity, does A dominate B?

- **Vendi Score (Diversity Metric via Eigenvalue Entropy)**
  - Why needed here: Quantifies diversity of a set using the Shannon entropy of similarity matrix eigenvalues; more principled than pairwise-average heuristics.
  - Quick check question: If all agents are semantically identical, what is the Vendi Score? (Answer: approaches 1, minimal diversity signal.)

- **Sentence Embeddings and Cosine Similarity**
  - Why needed here: Both Rel and Div objectives depend on embedding agents and queries into a shared semantic space for similarity computation.
  - Quick check question: Why might MiniLM underperform on domain-specific technical queries compared to a fine-tuned encoder?

## Architecture Onboarding

- **Component map:** Query → Planner decomposes → Agents generated (K rounds with Observer feedback) → Formatter (NL-to-JSON) → Final candidate set → Enumerate teams → Score Rel & Div → Construct Pareto frontier → Selector Agent picks final team → Agents instantiated in inference framework

- **Critical path:** 1) Query → Planner decomposes → Agents generated (K rounds with Observer feedback) 2) Final candidate set → Enumerate teams → Score Rel & Div → Pareto filter 3) Selector LLM chooses from T* → Agents instantiated in inference framework

- **Design tradeoffs:** K=3 balances quality vs. overhead; K=5 shows diminishing returns. [Nmin, Nmax]=[1,5] bounds reduce search space but may miss optimal configurations. MiniLM is fast but domain-agnostic; domain-specific encoders may improve Rel scores at cost of generality. Pareto-Best (default) vs. Random vs. Global Worst—ablation confirms strategy matters.

- **Failure signatures:** Collapsed Pareto frontier (candidate agents too similar; Div has no variance). Observer feedback loop instability (ϕt contradicts ϕt−1; generated agents oscillate). Token explosion at high Nmax (enumeration is O(C(n,r)) across all r ∈ [Nmin, Nmax]).

- **First 3 experiments:** 1) Baseline sanity check: Run AgentInit with K=1 vs. K=3 on held-out MMLU subset; confirm ~0.6-point delta. 2) Objective ablation: Disable Div (only Rel) or Rel (only Div); verify performance drops to ~90.4–90.5 vs. 91.3 full. 3) Scalability stress test: Set Nmax=10, measure GPU time and token usage; compare exhaustive enumeration vs. NSGA-II approximation on Pareto coverage (≥80% coverage).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the evaluation objectives of task relevance and intra-team diversity be refined to better capture subtle aspects of agent collaboration and performance?
- Basis in paper: [explicit] The Limitations section states that while current objectives provide an initial indication, their "ability to capture subtle aspects of collaboration and performance requires further refinement."
- Why unresolved: Current metrics rely on embedding similarities (Vendi Score, Cosine Similarity), which serve as proxies for interaction quality but may fail to detect nuanced collaborative behaviors like constructive conflict or semantic complementarity.
- What evidence would resolve it: Development and validation of new metrics that correlate strongly with human evaluations of team synergy or dynamic interaction graphs, outperforming the current static embedding-based scores.

### Open Question 2
- Question: How can AgentInit be adapted for resource-constrained environments to reduce token overhead and dependency on high-capability models?
- Basis in paper: [explicit] The Limitations section notes that the automated initialization process "results in significant token overhead and is heavily reliant on the capabilities of high-performing language models," posing challenges for resource-constrained settings.
- Why unresolved: The current framework utilizes a multi-round, multi-agent generation process (Planner, Observer, Formatter) and large backbone models (e.g., Deepseek-V3, Qwen2.5-72B), which are computationally expensive.
- What evidence would resolve it: Experiments demonstrating comparable initialization quality using smaller open-source models (e.g., 7B parameters) or quantization techniques, alongside analyses of prompt compression methods to reduce token consumption.

### Open Question 3
- Question: Do non-LLM-based generation mechanisms or external knowledge retrieval overcome the performance plateau observed in iterative agent generation?
- Basis in paper: [inferred] Section 5.1 notes that increasing iteration rounds ($K=3$ vs $K=5$) yields diminishing returns due to the "inherent limitations of generating agents exclusively through LLM-based interactions."
- Why unresolved: The method currently relies on the LLM's internal knowledge for role generation; once the model's internal diversity is exhausted, it cannot generate novel roles without external input.
- What evidence would resolve it: A comparative study where the Planner incorporates Retrieval-Augmented Generation (RAG) or external knowledge bases to generate agents, showing performance improvements beyond the $K=3$ iteration limit.

## Limitations
- Scalability challenges with exhaustive Pareto enumeration for larger candidate sets or wider team size bounds
- Underspecified Selector Agent decision criteria potentially introducing subjective bias
- Performance across diverse domains beyond tested academic and coding tasks remains unknown

## Confidence

- **High Confidence:** Core claim that balancing relevance and diversity via Pareto optimization improves MAS performance (validated across 9 datasets with consistent 1.2-1.6 point gains)
- **Medium Confidence:** Iterative refinement mechanism's effectiveness (K=3 shows consistent improvement, but K=5 results suggest diminishing returns may be dataset-dependent)
- **Medium Confidence:** Token reduction claims (though Table 7 shows 3.5% reduction in max pairwise similarity, the relationship between redundancy and token consumption during actual inference isn't fully isolated from other factors)

## Next Checks

1. **Scalability Validation:** Implement NSGA-II approximation for Nmax=10 and verify Pareto coverage ≥80% while maintaining <2-second selection time per team
2. **Domain Transfer Test:** Apply AgentInit to domain-specific tasks (legal, medical, creative writing) and compare performance degradation against general benchmarks
3. **Selection Criteria Transparency:** Document and validate the Selector Agent's decision logic through ablation studies showing how different selection criteria affect final team performance