---
ver: rpa2
title: 'RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction'
arxiv_id: '2511.21394'
source_url: https://arxiv.org/abs/2511.21394
tags:
- listwise
- user
- target
- meituan
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RIA addresses the challenge of optimizing listwise CTR prediction
  by unifying pointwise and listwise evaluation in a single end-to-end framework.
  It introduces four key components: User and Candidate Dual-Transformer (UCDT) for
  fine-grained user-item-context modeling, Context-aware User History and Target (CUHT)
  module for position-sensitive preference learning, Listwise Multi-HSTU (LMH) module
  to capture hierarchical item dependencies, and Embedding Cache (EC) module to bridge
  efficiency and effectiveness during inference.'
---

# RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction

## Quick Facts
- arXiv ID: 2511.21394
- Source URL: https://arxiv.org/abs/2511.21394
- Reference count: 11
- Primary result: +1.69% CTR and +4.54% CPM gains in online deployment

## Executive Summary
RIA addresses the challenge of optimizing listwise CTR prediction by unifying pointwise and listwise evaluation in a single end-to-end framework. It introduces four key components: User and Candidate Dual-Transformer (UCDT) for fine-grained user-item-context modeling, Context-aware User History and Target (CUHT) module for position-sensitive preference learning, Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies, and Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. Extensive experiments show RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan's advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.

## Method Summary
RIA is a unified framework that combines pointwise ranking and listwise reranking through shared representations. The UCDT module encodes candidate items and user context using HSTU transformers, with target attention producing item-level representations cached for reuse. The CUHT module applies position-wise self-attention over historical permutations and position-aware target attention to model position-sensitive preferences. The LMH module stacks HSTU layers to capture hierarchical list dependencies, with outputs fed to listwise prediction heads. The total loss combines pointwise and listwise binary cross-entropy objectives, enabling end-to-end optimization while maintaining inference efficiency through embedding caching.

## Key Results
- Achieves state-of-the-art AUC and LogLoss on both Avito and Meituan industrial datasets
- Monotonic AUC improvement with LMH depth scaling from 1 to 8 layers (0.6665→0.6730 on Meituan)
- Online A/B test deployment yields +1.69% CTR and +4.54% CPM improvements
- RIA_big (8 layers) increases latency from 26.1ms to 36.7ms while maintaining effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Shared Representations Across Ranking and Reranking
Sharing embedding layers between pointwise ranking and listwise evaluation enables knowledge transfer while reducing inference latency. The UCDT module pre-computes item-level representations and user-context embeddings during the ranking stage, which are reused by CUHT and LMH during reranking, eliminating redundant computation. This works because representations learned for pointwise prediction contain transferable signal for listwise evaluation, and the two tasks share underlying user-item interaction patterns.

### Mechanism 2: Position-Sensitive Target Attention (CUHT)
Explicitly modeling position-specific user preferences captures how historical behavior at each list position informs target predictions. PIAU applies self-attention within each historical permutation page, while PTAU computes target attention between the o-th position in the target list and all historical behaviors at position o. This isolates position-conditioned preference signals by leveraging the observation that users exhibit position-dependent browsing/click patterns that are learnable from permutation-level history.

### Mechanism 3: Hierarchical List Dependency Modeling (LMH Scaling)
Stacking HSTU layers in LMH captures progressively abstract list-level dependencies, with performance scaling monotonically with depth. Each layer refines the listwise representation based on the previous layer's output, enabling the model to learn hierarchical item interactions from local to global patterns. This hierarchical structure allows the model to capture complex list-level dependencies that simple attention mechanisms cannot represent.

## Foundational Learning

- **Target Attention (vs. Self-Attention)**: Why needed here: UCDT and PTAU both use target attention to query user history against specific candidate items. Quick check question: Given a candidate item embedding `q` and a sequence of user history embeddings `[h1, h2, ..., hT]`, what does target attention compute?

- **Listwise vs. Pointwise CTR Prediction**: Why needed here: RIA's core contribution is unifying these two paradigms. Quick check question: Why might an item's CTR differ when shown at position 1 vs. position 5 in a list, even for the same user?

- **HSTU (Hierarchical Sequential Transduction Units)**: Why needed here: HSTU is the base encoder for both UCDT and LMH. Quick check question: What inductive bias does a sequential transduction unit introduce compared to a standard Transformer layer?

## Architecture Onboarding

- **Component map**: Input candidate list X and user-context E_u → UCDT (HSTU encoding + target attention) → cached x''_i and H_k → CUHT (PIAU self-attention + PTAU target attention) → LMH (stacked HSTU layers) → listwise prediction

- **Critical path**: UCDT → (EC cache) → CUHT (PTAU) → LMH → listwise prediction. The cached x''_i and H_k representations must be available before CUHT/LMH execute.

- **Design tradeoffs**: RIA_small (1 HSTU layer) vs. RIA_big (8 layers): +0.42% CTR gain for +8.5ms latency. Pre-computation granularity: Caching at x''_i level balances flexibility vs. freshness. Position embedding scope: Per-position embeddings increase parameter count but enable position-specific preference learning.

- **Failure signatures**: AUC plateaus or degrades with more LMH layers → potential overfitting. Latency spike during reranking → EC cache misses. L1/L2 loss imbalance → one objective dominating gradients. Combinatorial sparsity warnings → user-item co-occurrence matrix too sparse.

- **First 3 experiments**: 1) Ablation of shared embeddings: Disable EC, compute UCDT representations on-the-fly during reranking. 2) LMH depth sweep: Replicate Figure 4 on your dataset (1, 2, 4, 8 layers). 3) Position sensitivity validation: Zero out PTAU, replace with learned position embeddings only.

## Open Questions the Paper Calls Out

### Open Question 1
Does the performance scaling law observed in the Listwise Multi-HSTU (LMH) module hold beyond 8 layers, or does the model eventually encounter diminishing returns or overfitting? The paper notes in Section 3.3 that "AUC increases monotonically with depth" between 1 and 8 layers and suggests a "scaling law in listwise modeling," but does not test deeper configurations.

### Open Question 2
Can the RIA_big architecture be adapted to meet stricter latency constraints without compromising the significant CTR gains observed in the online A/B tests? Table 3 shows RIA_big increases latency from 26.1ms to 36.7ms. While acceptable for the Meituan ad system, this ~40% latency increase may violate Service Level Objectives in higher-throughput or lower-latency recommendation environments.

### Open Question 3
Does the RIA framework generalize effectively to organic recommendation scenarios where user behavioral patterns differ from the commercial intent found in advertising datasets? The paper evaluates the model exclusively on advertising datasets (Avito and Meituan Ads), relying on CTR and CPM as primary metrics.

## Limitations

- The monotonic AUC improvement with LMH depth (1→8 layers) lacks evidence beyond one dataset and may not generalize
- Position-sensitive attention mechanism requires sufficient historical permutation data, which may not be available in sparse interaction scenarios
- Online A/B test results lack statistical significance testing and analysis of user segment heterogeneity

## Confidence

- **High confidence**: Empirical results showing RIA's superiority over baseline models on both public and industrial datasets
- **Medium confidence**: Mechanism claims about knowledge transfer through shared embeddings and position-sensitive learning, supported by architectural description but limited ablation studies
- **Medium confidence**: Online A/B test results showing +1.69% CTR and +4.54% CPM improvements, though no statistical significance testing is reported

## Next Checks

1. Conduct ablation studies isolating each mechanism: (a) disable EC to test knowledge transfer claim, (b) remove PTAU to validate position sensitivity contribution, (c) vary LMH depth to confirm scaling law

2. Test RIA's performance on datasets with different characteristics (e.g., higher sparsity, different position bias patterns) to assess generalizability

3. Measure statistical significance of online A/B test results and analyze user segment heterogeneity in CTR/CPM improvements