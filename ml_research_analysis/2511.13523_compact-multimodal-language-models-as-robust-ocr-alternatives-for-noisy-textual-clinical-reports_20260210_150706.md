---
ver: rpa2
title: Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual
  Clinical Reports
arxiv_id: '2511.13523'
source_url: https://arxiv.org/abs/2511.13523
tags:
- noise
- accuracy
- text
- document
- numeric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates compact multimodal language models as OCR
  alternatives for transcribing noisy clinical documents. Using 60 obstetric ultrasound
  reports photographed under real-world conditions, we compare eight systems across
  traditional OCR, neural pipelines, and MLLMs.
---

# Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports

## Quick Facts
- arXiv ID: 2511.13523
- Source URL: https://arxiv.org/abs/2511.13523
- Reference count: 8
- Primary result: Compact MLLMs achieve lowest error rates on noisy clinical documents while maintaining numeric accuracy

## Executive Summary
This study evaluates compact multimodal language models (MLLMs) as OCR alternatives for transcribing noisy clinical documents. Using 60 obstetric ultrasound reports photographed under real-world conditions, we compare eight systems across traditional OCR, neural pipelines, and MLLMs. Compact MLLMs (Qwen-2.5 VL, Phi-4 MM, InternVL-3.5-4B) achieve the lowest character error rates (0.031-0.040) and word error rates (0.075-0.096), outperforming classical and neural OCR pipelines. Critically, they maintain over 92% numeric accuracy with no significant correlation between numeric accuracy and overall error rates, unlike other systems. While requiring 11-67 seconds per image and 17-47 GB GPU memory, these MLLMs demonstrate robustness to common noise factors (blur, shadows, bleed-through) that degrade traditional OCR. The results position compact MLLMs as practical, privacy-preserving OCR solutions for on-premises healthcare digitization, particularly where numeric precision and noise resilience are critical.

## Method Summary
The study evaluates eight OCR systems on 60 obstetric ultrasound reports photographed under real-world conditions with common noise factors. Traditional OCR includes Tesseract, PaddleOCR, and docTR; neural pipelines include Surya and GOT-OCR 2.0; MLLMs include Qwen-2.5 VL-7B, Phi-4 MM, and InternVL-3.5-4B. All systems operate on raw RGB images without preprocessing. MLLMs use the prompt "You are performing OCR on this document. Transcribe all visible text verbatim as plain text." Metrics include character error rate (CER), word error rate (WER), numeric accuracy rate (Nacc), runtime, and GPU memory. Evaluation uses A100 80GB GPU with mixed precision (bfloat16) inference.

## Key Results
- Compact MLLMs achieve lowest error rates: CER 0.031-0.040, WER 0.075-0.096
- MLLMs maintain over 92% numeric accuracy with no correlation to overall error rates
- Traditional OCR systems show 2-3x higher error rates and significant numeric accuracy degradation
- MLLMs demonstrate robustness to noise factors (blur, shadows, bleed-through) that degrade classical OCR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-language integration enables noise-robust transcription by jointly processing visual and linguistic context
- Mechanism: MLLMs couple vision encoders with language decoders, allowing end-to-end text extraction that bypasses brittle page segmentation and character isolation stages used by traditional OCR. This joint processing provides contextual disambiguation when individual characters are degraded.
- Core assumption: The vision encoder captures sufficient signal from degraded regions for the language decoder to resolve ambiguities using learned linguistic priors.
- Evidence anchors:
  - [abstract] "MLLMs, which couple vision encoders with language decoders, have shown the emerging ability to transcribe text directly from images, potentially bypassing the need for brittle segmentation and pre-processing stages."
  - [Section 4.2] "compact MLLMs...demonstrate low and largely insignificant correlations [with noise indicators], indicating robustness to the common distortions present in handheld captures."
  - [corpus] Limited direct corpus support; related work on LLM OCR robustness (arXiv:2502.16781) examines noisy OCR data but focuses on downstream QA rather than transcription mechanisms.
- Break condition: If noise levels exceed the vision encoder's effective receptive field resolution, or if document layout deviates significantly from training distributions, contextual disambiguation may fail.

### Mechanism 2
- Claim: Numeric accuracy decouples from overall transcription error in MLLMs due to learned token-level attention patterns
- Mechanism: MLLMs appear to treat numeric sequences as distinct token classes with specialized attention, preserving digit-level accuracy even when surrounding text contains errors. Partial correlation analysis shows near-zero association between Nacc and CER/WER after controlling for numeric density.
- Core assumption: The model's pre-training includes sufficient numeric pattern exposure to develop digit-specific representations.
- Evidence anchors:
  - [abstract] "they maintain over 92% numeric accuracy with no significant correlation between numeric accuracy and overall error rates, unlike other systems."
  - [Section 4.3] "Qwen-2.5 VL demonstrates near-zero correlations [between Nacc and CER/WER], confirming its robustness in retaining numerical accuracy independently of overall transcription quality."
  - [corpus] No direct corpus evidence on numeric preservation mechanisms in MLLMs.
- Break condition: Unusual numeric formats (e.g., non-standard decimal separators, mixed scripts) may not benefit from learned digit attention if underrepresented in training.

### Mechanism 3
- Claim: Language model priors provide tolerance to regionally inflected medical terminology
- Mechanism: Pre-trained language components retain broad linguistic coverage, enabling recognition of phrase variants like "cardiac activity is appreciated" (Indian usage) versus "cardiac activity is present" (North American) without requiring domain-specific fine-tuning.
- Core assumption: The pre-training corpus included sufficient medical or formal register text to encode these linguistic patterns.
- Evidence anchors:
  - [Section 3.2] "These expressions are semantically equivalent but stylistically distinct, and may challenge models whose language priors are trained primarily on Western clinical corpora."
  - [Section 5] "their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization."
  - [corpus] Weak corpus support; no direct comparative studies on regional medical English in MLLMs identified.
- Break condition: Highly specialized terminology or novel abbreviations not present in pre-training data will still require domain adaptation.

## Foundational Learning

- Concept: **Vision-Language Model Architecture**
  - Why needed here: Understanding how vision encoders (typically ViT-based) connect to language decoders via projection layers explains why MLLMs can handle end-to-end document transcription.
  - Quick check question: Can you explain how a ViT encoder's patch embeddings are transformed into token representations for a language decoder?

- Concept: **OCR Error Metrics (CER, WER, Nacc)**
  - Why needed here: The paper evaluates systems using character-level and word-level edit distances plus numeric accuracy; understanding these metrics is essential for interpreting results and designing validation protocols.
  - Quick check question: Given reference "12.5 mm" and hypothesis "12.5cm," what are the CER, WER, and Nacc?

- Concept: **No-Reference Image Quality Assessment (NR-IQA)**
  - Why needed here: The paper examines whether NR-IQA metrics (BRISQUE, NIQE, DeQA-Doc) predict OCR degradation—relevant for building document triage pipelines.
  - Quick check question: Why would a no-reference metric be preferred over full-reference metrics for real-world clinical document capture?

## Architecture Onboarding

- Component map: Raw RGB image -> Vision encoder (ViT-based) -> Projection layer -> Language decoder (autoregressive) -> Plain text output
- Critical path:
  1. Image loading (PIL, preserve original resolution)
  2. Prompt construction: "You are performing OCR on this document. Transcribe all visible text verbatim as plain text."
  3. Inference with mixed precision (bfloat16 recommended)
  4. Output normalization (lowercase, whitespace collapse, punctuation handling)
  5. Metric computation (jiwer for CER/WER; custom regex for Nacc)

- Design tradeoffs:
  - **Qwen-2.5-VL-7B**: Best accuracy-speed balance (17GB VRAM, ~55s/image), recommended for most deployments
  - **InternVL-3.5-4B**: Lower VRAM (17GB), faster (~11s/image), slightly lower accuracy—good for resource-constrained environments
  - **Phi-4-MM (14B)**: High VRAM variance (up to 47GB), not recommended for constrained deployments
  - **docTR/PaddleOCR**: 10-20x faster but 2-3x higher error rates; suitable when speed outweighs accuracy

- Failure signatures:
  - **Background text transcription**: MLLMs may transcribe visible text from adjacent documents; requires masking or post-filtering
  - **Space handling inconsistencies**: GOT-OCR 2.0 shows low CER but inflated WER due to spurious space insertion
  - **Numeric density effects**: Traditional OCR shows Nacc degradation with increasing numeric content; MLLMs do not

- First 3 experiments:
  1. **Baseline replication**: Run Qwen-2.5-VL-7B on 10 sample documents from your domain; compute CER/WER against manual transcriptions to establish expected accuracy bounds.
  2. **Noise sensitivity profiling**: Correlate your documents' NR-IQA scores (BRISQUE, Laplacian variance) with per-document CER to determine if your capture conditions require MLLM-grade robustness.
  3. **Numeric validation**: Extract numeric spans from model outputs; verify Nacc >90% on documents with high numeric density before production deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can compact MLLMs reliably perform structured field extraction (e.g., key-value pairs, measurements) from noisy clinical documents, beyond plain transcription?
- Basis in paper: [explicit] Conclusion states "Future work includes structured extraction, layout improvements, and uncertainty-based review loops."
- Why unresolved: The study focused on page-level verbatim transcription; structured extraction and table parsing were not evaluated endpoints.
- What evidence would resolve it: Evaluation on clinical documents with gold-standard field annotations (e.g., extracting gestational age, fetal measurements) comparing MLLM outputs to reference structures.

### Open Question 2
- Question: How well do findings generalize to other clinical domains (e.g., radiology, pathology) and non-Indian medical English contexts?
- Basis in paper: [explicit] Limitations section: "Our corpus is single-domain (obstetric ultrasound) and region-specific (Indian medical English), which may limit direct portability."
- Why unresolved: Only obstetric ultrasound reports from one Indian hospital were evaluated; regional phrasings (e.g., "cardiac activity is appreciated") may not transfer.
- What evidence would resolve it: Cross-domain evaluation on diverse clinical document types across multiple geographic and linguistic settings.

### Open Question 3
- Question: What mechanisms can prevent MLLMs from transcribing irrelevant background text visible in document captures?
- Basis in paper: [inferred] Discussion notes "MLLMs occasionally transcribed excluded background text... suggesting potential for masks or filters to enhance deployment."
- Why unresolved: MLLMs' broader visual context capture leads them to transcribe text that human annotators excluded; no filtering strategy was tested.
- What evidence would resolve it: Comparative study of prompting strategies, visual masking, or post-processing filters on documents with visible background text.

### Open Question 4
- Question: Can uncertainty-based review loops effectively flag low-confidence transcriptions for manual verification in clinical workflows?
- Basis in paper: [explicit] Conclusion identifies "uncertainty-based review loops" as future work.
- Why unresolved: No uncertainty quantification or confidence scoring was implemented; it remains unknown whether model confidence correlates with error likelihood in this domain.
- What evidence would resolve it: Calibration analysis linking MLLM confidence scores to actual error rates, followed by workflow simulation measuring verification burden reduction.

## Limitations
- Single-domain evaluation (obstetric ultrasound) limits generalization to other clinical specialties
- Indian medical English phrasing may not transfer to other regional medical language variants
- High computational requirements (17-47GB GPU memory, 11-67s per image) create deployment barriers

## Confidence
- **High Confidence**: Comparative evaluation methodology is sound with clear metrics and systematic testing across eight systems
- **Medium Confidence**: Mechanism explanations are plausible based on architecture but lack direct empirical validation
- **Low Confidence**: Generalization claims beyond obstetric ultrasound reports and Indian medical English contexts are speculative

## Next Checks
1. **Domain Transfer Validation**: Test the same MLLM models on clinical documents from different specialties (radiology reports, pathology results, discharge summaries) to assess cross-domain robustness and identify any specialty-specific failure modes.

2. **Linguistic Variant Testing**: Evaluate model performance on medical documents using different regional English variants (North American, British, Australian) to quantify the observed linguistic tolerance and determine if fine-tuning is needed for specific healthcare systems.

3. **Operational Cost Analysis**: Conduct a comprehensive cost-benefit analysis comparing MLLM deployment (including GPU infrastructure, inference time, and maintenance) against traditional OCR solutions in realistic healthcare workflows, accounting for error correction overhead and system integration requirements.