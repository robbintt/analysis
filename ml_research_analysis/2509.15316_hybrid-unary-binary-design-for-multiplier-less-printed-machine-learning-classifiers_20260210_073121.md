---
ver: rpa2
title: Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers
arxiv_id: '2509.15316'
source_url: https://arxiv.org/abs/2509.15316
tags:
- unary
- printed
- hybrid
- power
- area
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Printed electronics enable flexible, low-cost ML circuits but face
  limitations in area and power due to large feature sizes and computational demands.
  This work introduces a hybrid unary-binary architecture for multiplier-less MLP
  classifiers that leverages PE's customization to remove costly encoders and simplify
  arithmetic operations.
---

# Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers

## Quick Facts
- **arXiv ID:** 2509.15316
- **Source URL:** https://arxiv.org/abs/2509.15316
- **Reference count:** 11
- **Primary result:** 46% area and 39% power reduction vs. state-of-the-art with <3% accuracy loss

## Executive Summary
This work introduces a hybrid unary-binary architecture for printed electronics ML classifiers that eliminates costly multipliers through arithmetic-level co-design. By leveraging the native thermometer code output of flash ADCs and combining unary arithmetic for input processing with binary logic in later layers, the approach achieves significant hardware efficiency gains. A novel layer-aware training method further optimizes weights to power-of-two values, enabling simple wiring instead of logic operations.

## Method Summary
The method combines unary arithmetic for input processing with binary logic in subsequent layers, leveraging flash ADC's native thermometer code output to avoid encoder hardware. The architecture uses hardwired weights to eliminate AND gates in unary multiplication through direct routing, and employs a layer-aware training approach that quantizes weights to powers-of-two via iterative retraining. This enables multiplier-less ML operations while maintaining accuracy within 3% of conventional implementations.

## Key Results
- 46% area reduction and 39% power reduction versus state-of-the-art binary MLP implementations
- Less than 3% accuracy loss across six benchmark datasets
- Self-powered operation capability below 30mW threshold
- Average 33.6% of weights successfully converted to power-of-two values

## Why This Works (Mechanism)

### Mechanism 1: ADC Encoder Removal
Flash ADCs internally produce thermometer codes before encoding to binary. By eliminating the encoder stage and feeding raw thermometer code directly to the first MLP layer, the system avoids both encoder hardware cost and separate input encoders that would otherwise be needed for unary processing.

### Mechanism 2: Unary Constant Multiplication Optimization
When weights are hardwired at fabrication time, unary multiplication can eliminate AND gates entirely. If a weight bit is 0, the corresponding connection can be omitted (always produces 0). If a weight bit is 1, the connection becomes a direct wire (input passes through unchanged).

### Mechanism 3: Layer-Aware Power-of-Two Weight Quantization
Converting weights to powers-of-two allows multiplication via simple bit-shifting (wiring in hardware). The training procedure quantizes weights, retrains for 2 epochs, and accepts if accuracy drop is below threshold T. This targets binary layers specifically since they dominate post-unary-stage hardware cost.

## Foundational Learning

- **Unary vs. binary arithmetic representations**: Unary streams encode values by the count/fraction of '1's rather than positional binary. Understanding this is prerequisite to grasping why AND gates can implement multiplication and why bit-width affects precision differently.
  - Quick check: Given an 8-bit unary stream with three '1's, what is its integer value? What is its real-valued (unipolar) value?

- **MLP forward pass as MAC operations**: The optimization targets multiply-accumulate operations specifically. Understanding that neuron computation = Σ(weight × input) + bias clarifies why simplifying multiplication cascades across all neurons in a layer.
  - Quick check: For a neuron with 4 inputs, how many multiplications and additions are required for one forward pass?

- **Printed electronics constraints**: PE's large feature sizes (μm vs. nm in silicon) make area optimization critical. The 30mW self-powered target is a hard physical constraint from printed battery limitations, not an arbitrary design choice.
  - Quick check: Why might a design achieving 50% area reduction but 35mW power be considered a failure for this application domain?

## Architecture Onboarding

- **Component map:** [Sensor] → [Pruned ADC (thermometer output, no encoder)] → [Layer 1: Unary parallel inputs × hardwired weights → binary accumulator + ReLU] → [Layer 2+: Binary fixed-point neurons with power-of-two weights where possible] → [Comparator tree] → [Output class]

- **Critical path:** Input bit-width (N) determines ADC comparator count and unary stream length → unary product routing → binary adder tree depth → final classifier output. The N-bit unary input width directly constrains both precision (max error 1/N per multiplication) and hardware cost.

- **Design tradeoffs:**
  - **Unary bit-width vs. accuracy:** Higher N reduces multiplication error (1/N bound) but increases ADC comparators and routing complexity. Paper uses 8-bit empirically.
  - **Power-of-two weight coverage vs. accuracy:** Aggressive quantization converts more weights (saving area/power) but risks accuracy threshold breach. Achieved 33.6% conversion on average.
  - **Fully-parallel vs. serial:** Paper commits to fully parallel (one inference/cycle) to avoid PE-unfriendly sequential control logic, accepting higher area for simplicity.

- **Failure signatures:**
  - Accuracy loss exceeds 3% on validation set → likely over-aggressive weight quantization or insufficient unary bit-width
  - Power exceeds 30mW → check if ADC pruning was applied; verify binary layer bit-widths haven't bloated
  - Area > 10 cm² for simple datasets → verify AND gate removal was actually synthesized; check for unpruned comparators
  - Training fails to converge after weight quantization → threshold T may be too strict or learning rate needs adjustment for m=2 epochs

- **First 3 experiments:**
  1. **Baseline sanity check:** Implement exact binary MLP from [7] on your target PE technology node. Measure area/power to confirm you're starting from the claimed baseline (~7.2 cm², ~24.1 mW average). This validates your synthesis flow.
  2. **Unary multiplication validation in isolation:** Create a single unary multiplier (input × constant weight) using the routing-only approach. Verify output bitstream matches expected value within 1/N error bound. Compare synthesized area against an AND-gate-based version.
  3. **End-to-end single-dataset pilot:** Implement the full hybrid pipeline on the simplest dataset (Balance Scale: 4 inputs, 21 MAC ops). This exposes integration issues between pruned ADC, unary first layer, and binary subsequent layers while keeping debug scope manageable. Target: verify <3% accuracy loss and measurable area/power reduction vs. baseline before scaling to larger datasets.

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- The approach trades ~3% accuracy for hardware efficiency, which may be prohibitive for safety-critical applications
- ASIC-style fixed weights at fabrication limit post-deployment adaptability
- The 33.6% average power-of-two conversion rate suggests practical limitations in the training method not fully explored

## Confidence

- **High confidence** in unary multiplication mechanism and ADC encoder removal benefits
- **Medium confidence** in power-of-two weight quantization gains due to limited sensitivity analysis
- **Low confidence** in absolute power/area claims without independent synthesis verification

## Next Checks
1. **Sensitivity analysis of threshold T:** Systematically vary T from 0.1% to 2% accuracy tolerance and measure resulting power-of-two conversion rates and final accuracy.
2. **Comparator pruning verification:** Implement the pruned ADC and measure actual silicon area/power savings vs. theoretical calculation.
3. **Generalization stress test:** Apply methodology to a dataset outside the six evaluated (e.g., EEG signal classification or financial time series).