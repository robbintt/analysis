---
ver: rpa2
title: 'Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study
  in Finance'
arxiv_id: '2511.02451'
source_url: https://arxiv.org/abs/2511.02451
tags:
- merging
- million
- knowledge
- financial
- other
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building specialized large
  language models for finance by proposing to merge existing continual pre-training
  (CPT) models specialized in finance, math, and Japanese. Instead of costly and unstable
  multi-skill training, the authors apply model merging techniques (Task Arithmetic,
  TIES, DARE-TIES) to combine CPT experts with a base model and with each other.
---

# Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance

## Quick Facts
- arXiv ID: 2511.02451
- Source URL: https://arxiv.org/abs/2511.02451
- Authors: Kentaro Ueda; François Portet; Hirohiko Suwa; Keiichi Yasumoto
- Reference count: 0
- Key outcome: Merging finance, math, and Japanese CPT models yields emergent reasoning capabilities while recovering general knowledge lost during domain specialization

## Executive Summary
This paper addresses the challenge of building specialized large language models for finance by proposing to merge existing continual pre-training (CPT) models specialized in finance, math, and Japanese. Instead of costly and unstable multi-skill training, the authors apply model merging techniques (Task Arithmetic, TIES, DARE-TIES) to combine CPT experts with a base model and with each other. They introduce a three-stage evaluation framework assessing knowledge recovery, complementarity, and emergence across 18 financial NLP tasks from 8 datasets. Results show that merging CPT models with the base recovers lost general knowledge, while merging different CPT models can yield emergent capabilities, such as improved reasoning.

## Method Summary
The authors propose a three-stage merging framework using Task Arithmetic (TA), TIES, and DARE-TIES to combine CPT models specialized in finance, math, and Japanese with a base Llama-3-8B model. Stage 1 merges each CPT with the base to recover general knowledge. Stage 2 merges the best two Stage-1 models to test complementarity. Stage 3 merges the Stage-2 result with the remaining CPT to test capacity limits. The framework evaluates performance using Macro-Outperform Gap (Macro-OG) and Oracle Retention across 18 financial NLP tasks. Hyperparameters are swept for each method, with TA requiring careful λ tuning and TIES offering more robust performance.

## Key Results
- Merging a CPT model with its base model consistently recovers general knowledge lost during domain specialization (positive Macro-Gain across all experiments)
- Merging complementary CPT models (Finance + Math) can yield emergent capabilities not present in either constituent model
- Task Arithmetic achieves the highest performance gains but is hyperparameter-sensitive, while TIES is more robust across settings
- Higher model similarity correlates with merging success, but emergent skills depend on more complex factors
- Three-way merging consistently underperforms, suggesting significant interference when integrating multiple specialized CPTs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Merging a CPT model with its base model recovers general-purpose knowledge lost during domain specialization.
- **Mechanism:** Continual pre-training shifts weights away from the base model to absorb domain knowledge, causing "catastrophic forgetting" of general capabilities. Task Arithmetic computes a task vector (τ = θ_CPT − θ_base) and scales it back toward the base, interpolating between specialized and general knowledge in weight space.
- **Core assumption:** The base model's weights encode recoverable general knowledge that remains geometrically accessible from the CPT model's position in parameter space.
- **Evidence anchors:**
  - [abstract] "merging an expert with its base model recovers general knowledge lost during CPT"
  - [Section 4.1.1] "all merged models exhibit positive Macro-Gain, indicating a consistent performance uplift over the constituent average"
  - [corpus] Related work on catastrophic forgetting (Thompson et al., 2019) cited throughout
- **Break condition:** If CPT causes radical weight reorganization (L2 distance too large), interpolation may fail to recover coherent knowledge. Paper notes "under extreme inter-model distances (L2 > 120 or cosine < 0.978), Macro-OG often drops sharply."

### Mechanism 2
- **Claim:** Merging complementary CPT models (e.g., Finance + Math) can yield emergent capabilities neither constituent possesses alone.
- **Mechanism:** Finance CPT provides document grounding and domain terminology; Math CPT provides symbolic reasoning procedures. Weight-space composition allows these capabilities to activate jointly during inference, enabling composite reasoning chains that require both skills.
- **Core assumption:** Knowledge from different domains is stored in partially non-overlapping parameter regions, allowing constructive rather than destructive composition.
- **Evidence anchors:**
  - [abstract] "merging experts improves performance and can yield emergent cross-domain skills"
  - [Section 4.3.1, Case ID193] Finance model failed at table parsing; Math model produced irrelevant reasoning; merged model "correctly retrieved both values and executed the equation with clear arithmetic reasoning"
  - [corpus] No direct corpus evidence for emergence mechanism; this remains an empirical observation requiring further theoretical grounding
- **Break condition:** When contextual understanding and arithmetic procedures are misaligned, "integration may induce interference and degrade coherent reasoning" [Section 4.3.2].

### Mechanism 3
- **Claim:** Sign-consistent sparse merging (TIES) reduces destructive interference by excluding conflicting weight updates.
- **Mechanism:** TIES retains only top-d% magnitude weights per task vector, zeros the rest, then computes majority-vote signs per parameter. Only models with matching signs contribute to the final weight—explicitly filtering sign conflicts that would cause cancellation or noise.
- **Core assumption:** High-magnitude weights carry the most task-relevant signal; sign agreement indicates compatible knowledge direction.
- **Evidence anchors:**
  - [Section 3.5] "By explicitly excluding sign conflicts, TIES-Merging promotes stable integration by combining only mutually consistent knowledge across CPTs"
  - [Section 4.4.1] "TI benefits from moderate sparsity and remains robust across settings"
  - [corpus] No corpus papers validate TIES specifically for CPT models; prior work focused on SFT
- **Break condition:** Excessive sparsity (low d) discards useful knowledge; insufficient sparsity allows interference. Paper sweeps d ∈ {0.1, ..., 0.9} with best results around 0.2–0.4 for dual-CPT merges.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The core problem CPT merging addresses; understanding that domain adaptation trades off general capability loss for specialization gain.
  - **Quick check question:** If a model trained on general text then continues training on finance corpora, what happens to its ability to answer general knowledge questions?

- **Concept: Task Vectors**
  - **Why needed here:** The fundamental unit of all three merge methods; task vectors encode "what changed" during training, enabling arithmetic operations on capabilities.
  - **Quick check question:** Given base model weights θ_base and fine-tuned weights θ_ft, what does the vector τ = θ_ft − θ_base represent?

- **Concept: Weight-Space Interference**
  - **Why needed here:** Explains why naive averaging fails and why sign-consistency matters; destructive interference occurs when updates point in opposite directions.
  - **Quick check question:** If two task vectors have opposite signs for the same parameter, what happens if you simply average them?

## Architecture Onboarding

- **Component map:** Base model (Llama-3-8B) → three independent CPT models (Finance, Math, Japanese) → three-stage merging framework
- **Critical path:** 1) Select CPT models sharing same base checkpoint 2) Stage 1: Merge each CPT with base, sweep hyperparameters, select best per domain 3) Stage 2: Merge top two Stage-1 models, verify Macro-OG > 0 4) Stage 3: Add third domain only if Stage 2 shows positive Oracle Retention
- **Design tradeoffs:**
  - Task Arithmetic: Highest peak performance, but requires careful λ tuning (0.6–0.8 worked best); fragile
  - TIES: More robust across hyperparameters, lower peak but safer default choice
  - DARE-TIES: Underperformed in this study; "rarely improves performance and often degrades under low d values"
  - Three-way merging: Showed consistent degradation—"integrating three specialized CPTs introduces significant interference"
- **Failure signatures:**
  - Negative Macro-OG: Merged model underperforms best constituent → interference dominates
  - Oracle Retention < 1.0: Failed to preserve peak constituent capabilities
  - Spurious reasoning chains: Merged model synthesizes incorrect logic (see Case ID194)
- **First 3 experiments:**
  1. **Reproduce Stage 1 with Task Arithmetic:** Merge Base + Finance CPT, sweep λ ∈ {0.5, 0.6, 0.7, 0.8}, measure Macro-Gain on held-out tasks. Verify positive gain confirms knowledge recovery.
  2. **Compare TIES vs. Task Arithmetic stability:** Run 5 random seeds for each method at best hyperparameter; compute variance. Expect TA variance > TI variance per Figure 7.
  3. **Test emergence hypothesis:** Identify 5 tasks where Finance and Math models both fail individually; check if merged FMTA/FMTI succeeds. Document as emergent or not per Section 4.3 methodology.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the efficacy of CPT merging generalize to larger model scales and diverse architectures beyond Llama-3-8B?
- **Basis in paper:** [explicit] The authors state in the Limitations that "it remains an open question whether this robustness extends to CPT models" regarding different model families, and suggest applying methods to "larger-scale models."
- **Why unresolved:** Computational constraints limited the study to a single 8B parameter architecture, leaving the behavior of merging in high-capacity models unknown.
- **What evidence would resolve it:** Replicating the three-stage merging framework on larger models (e.g., 70B+) and different architectures (e.g., Mistral) to verify if performance gains persist.

### Open Question 2
- **Question:** What specific factors beyond parameter-space similarity drive emergent capabilities in merged models?
- **Basis in paper:** [explicit] The Conclusion notes that while similarity correlates with merging success, "emergent skills depend on more complex factors" beyond simple distance metrics like L2 or cosine similarity.
- **Why unresolved:** The quantitative analysis showed no significant monotonic relationship between parameter distance and the "Outperform Gap" (emergence), making prediction difficult.
- **What evidence would resolve it:** Mechanistic interpretability studies identifying how distinct knowledge bases (e.g., math and finance) interact in the hidden states to form new reasoning pathways.

### Open Question 3
- **Question:** Does merging domain-specific CPT models degrade general-purpose language capabilities?
- **Basis in paper:** [explicit] The Limitations section notes the evaluation was "confined to finance-specific tasks, leaving the impact of merging on general-purpose language abilities unevaluated."
- **Why unresolved:** While the paper demonstrates domain knowledge recovery, it does not measure if the merging process introduces trade-offs in general reasoning or instruction following.
- **What evidence would resolve it:** Evaluating merged CPT models on standard general-purpose benchmarks (e.g., MMLU, HellaSwag) alongside domain-specific tasks.

### Open Question 4
- **Question:** Can CPT model merging enhance the performance of agentic approaches in financial domains?
- **Basis in paper:** [explicit] The Conclusion proposes it as a "promising direction for future work to investigate how CPT model merging can benefit agentic approaches."
- **Why unresolved:** The current study focused on static inference benchmarks, whereas agentic work requires multi-step planning and tool use.
- **What evidence would resolve it:** Integrating merged CPT models into agent frameworks to measure success rates in complex, multi-step financial research tasks.

## Limitations

- Three-way merging consistently underperforms due to significant interference when integrating multiple specialized CPTs
- The study is confined to finance-specific tasks, leaving the impact of merging on general-purpose language abilities unevaluated
- Computational constraints limited evaluation to Llama-3-8B, leaving generalization to larger models and different architectures unknown

## Confidence

**High confidence:** Knowledge recovery mechanism (Stage 1 results) - The paper demonstrates consistent Macro-Gain across all CPT models when merged with their base, supported by geometric interpolation theory and quantitative metrics.

**Medium confidence:** TIES robustness claim - While the paper shows TIES maintains stable performance across hyperparameter sweeps, the underlying assumption that sign-consistency filtering is universally beneficial lacks theoretical justification.

**Low confidence:** Emergence prediction framework - The paper identifies emergent capabilities post-hoc but cannot predict which model combinations will yield emergent skills versus spurious reasoning.

## Next Checks

1. **Longitudinal stability test:** Evaluate merged models after 1-2 weeks of continuous inference to measure performance drift. Track whether initial Macro-OG gains persist or decay due to weight-space instability.

2. **Predictive emergence framework:** Develop a pre-merge diagnostic using parameter similarity metrics (L2 distance, cosine similarity) and task vector alignment scores to predict whether a given model pair will yield emergent capabilities versus interference.

3. **Scaling experiment:** Test whether the three-way merging limitation persists with larger base models (e.g., Llama-3-70B) or whether increased capacity enables successful multi-skill integration. Vary base model size while holding CPT quality constant.