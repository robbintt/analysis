---
ver: rpa2
title: Machine-Assisted Grading of Nationwide School-Leaving Essay Exams with LLMs
  and Statistical NLP
arxiv_id: '2601.16314'
source_url: https://arxiv.org/abs/2601.16314
tags:
- instructions
- grading
- human
- scoring
- essay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models can score Estonian school-leaving essays\
  \ with accuracy comparable to human raters, often matching or falling within human\
  \ scoring variance. In a zero-shot setting, LLM-based grading achieved mean absolute\
  \ errors of 2.66\u20134.97 points on a 0\u201327 scale, with predictions frequently\
  \ within the range of human scores."
---

# Machine-Assisted Grading of Nationwide School-Leaving Essay Exams with LLMs and Statistical NLP

## Quick Facts
- **arXiv ID:** 2601.16314
- **Source URL:** https://arxiv.org/abs/2601.16314
- **Reference count:** 40
- **Primary result:** Large language models can score Estonian school-leaving essays with accuracy comparable to human raters, often matching or falling within human scoring variance.

## Executive Summary
This study investigates the feasibility of using large language models (LLMs) and statistical NLP techniques to automatically grade school-leaving essay exams in Estonian, a low-resource language. By employing a zero-shot prompting approach, the researchers demonstrate that LLMs can achieve grading accuracy comparable to human raters, with mean absolute errors of 2.66–4.97 points on a 0–27 scale. Statistical NLP models, leveraging linguistic and error features, perform similarly or better for specific accuracy-related categories. The findings suggest that automated essay scoring is technically viable in small-language contexts and can support scalable, consistent assessment when integrated into human-in-the-loop pipelines.

## Method Summary
The authors applied a zero-shot prompting approach, where LLMs were given essay prompts and scoring rubrics without prior fine-tuning on the dataset. Both zero-shot and few-shot (with 1-3 exemplars) settings were tested. Statistical NLP models were trained using linguistic and error-based features to complement LLM-based grading. The system was evaluated on a real-world dataset of Estonian school-leaving essays, comparing automated scores to those given by human raters. The methodology emphasizes minimal human input for scaling while maintaining oversight through a human-in-the-loop design.

## Key Results
- LLM-based grading achieved mean absolute errors of 2.66–4.97 points on a 0–27 scale.
- Predictions frequently fell within the range of human scores, indicating strong comparability.
- Statistical NLP models performed similarly or better on accuracy-related categories.

## Why This Works (Mechanism)
The zero-shot approach leverages the broad linguistic and reasoning capabilities of LLMs, allowing them to interpret essay prompts and rubrics without domain-specific training. By incorporating both LLM and statistical NLP methods, the system captures both holistic and feature-based aspects of essay quality. The human-in-the-loop design ensures that final grading decisions are overseen by humans, balancing automation with expert judgment.

## Foundational Learning

1. **Zero-shot learning**: Using pre-trained models without fine-tuning for a specific task.
   - Why needed: Enables quick deployment without labeled data.
   - Quick check: Try the model on a new prompt without any examples.

2. **Prompt engineering**: Crafting effective instructions for LLMs to follow.
   - Why needed: Directly impacts the quality and consistency of LLM outputs.
   - Quick check: Compare results using different prompt phrasings.

3. **Feature engineering for NLP**: Extracting linguistic and error-based metrics from text.
   - Why needed: Provides interpretable signals that complement LLM reasoning.
   - Quick check: Validate that engineered features correlate with human scores.

4. **Human-in-the-loop design**: Integrating human oversight into automated systems.
   - Why needed: Maintains quality control and addresses edge cases.
   - Quick check: Review a sample of automated scores with human graders.

5. **Cross-lingual transfer**: Applying models trained on one language to another.
   - Why needed: Expands applicability to low-resource languages.
   - Quick check: Test model performance on essays from different languages.

6. **Error analysis in NLP**: Systematically reviewing mismatches between automated and human scores.
   - Why needed: Identifies systematic biases or weaknesses in the model.
   - Quick check: Categorize and quantify error types.

## Architecture Onboarding

**Component Map:** Essays -> LLM/Statistical NLP models -> Score predictions -> Human-in-the-loop review -> Final scores

**Critical Path:** Raw essays are processed by both LLM and statistical NLP models, producing candidate scores. These are then reviewed by human graders, who make final determinations.

**Design Tradeoffs:** Zero-shot LLM use avoids costly fine-tuning but may yield higher variance; statistical NLP models offer interpretability but require feature engineering. Human oversight ensures fairness but limits full automation.

**Failure Signatures:** High disagreement between LLM and human scores; systematic bias against certain essay styles; low accuracy on rare or complex prompts.

**First 3 Experiments:**
1. Compare zero-shot LLM scores to few-shot and fine-tuned variants on a held-out set.
2. Conduct bias audits by analyzing scores across different demographics and essay topics.
3. Evaluate the impact of prompt phrasing on LLM consistency and accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Confidence is Medium for cross-language and cross-curriculum generalization, as testing was limited to Estonian essays.
- The zero-shot approach’s potential accuracy gains from fine-tuning are not explored.
- Potential biases in training data or prompt design are not directly measured.

## Confidence
- Accuracy and comparability claims: Medium
- Bias mitigation claims: Low
- Generalization claims: Medium

## Next Checks
1. Replicate the LLM scoring pipeline on essays from other languages and cultural contexts to test generalizability.
2. Conduct bias audits using diverse essay prompts and demographic variables to assess fairness.
3. Implement and evaluate a fine-tuning protocol on a subset of the dataset to quantify potential accuracy gains over zero-shot scoring.