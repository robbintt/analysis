---
ver: rpa2
title: 'MANBench: Is Your Multimodal Model Smarter than Human?'
arxiv_id: '2506.11080'
source_url: https://arxiv.org/abs/2506.11080
tags:
- image
- dataset
- manbench
- human
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MANBench is a bilingual benchmark (English and Chinese) for evaluating
  multimodal reasoning capabilities of humans and Multimodal Large Language Models
  (MLLMs). It contains 1,314 questions across nine tasks with 2,231 images, designed
  to emphasize intuitive reasoning, cross-modal integration, and real-world complexity.
---

# MANBench: Is Your Multimodal Model Smarter than Human?
## Quick Facts
- arXiv ID: 2506.11080
- Source URL: https://arxiv.org/abs/2506.11080
- Authors: Han Zhou; Qitong Xu; Yiheng Dong; Xin Yang
- Reference count: 31
- Primary result: MLLMs achieve less than 60% accuracy, falling short of average human performance across many domains

## Executive Summary
MANBench is a bilingual (English/Chinese) benchmark designed to evaluate multimodal reasoning capabilities of humans and Multimodal Large Language Models (MLLMs). It contains 1,314 questions across nine tasks with 2,231 images, emphasizing intuitive reasoning, cross-modal integration, and real-world complexity. The benchmark distinguishes between knowledge-based and non-knowledge-based questions, requiring reasoning rather than simple retrieval and mandating integration of textual and visual information. Through extensive human experiments with 575 participants and evaluation of 12 state-of-the-art MLLMs, the study found that while MLLMs excel in knowledge-based and text-image understanding tasks, they struggle with deeper cross-modal reasoning tasks like Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination.

## Method Summary
MANBench evaluates MLLMs using the VLMEvalKit framework with 1,314 multiple-choice questions across 9 tasks, split into Knowledge and Non-Knowledge categories. The benchmark includes 2,231 images from various sources and is available in both English and Chinese. Human baseline experiments involved 575 participants stratified by age and education. Evaluation uses temperature=0 and retry count=10. The benchmark separates knowledge retrieval from intuitive visual reasoning, requires mandatory cross-modal integration, and includes a difficulty gradient to distinguish between model scaling improvements and fundamental capability gaps.

## Key Results
- MLLMs achieve less than 60% accuracy overall, falling short of average human performance
- Models excel in knowledge-based tasks (75-85% accuracy) but struggle with non-knowledge reasoning tasks (20-30% accuracy)
- Both humans and MLLMs face significant challenges in highly complex tasks like Puzzles and Spatial Imagination
- GPT-4o shows visual dependency, with accuracy dropping significantly when images are masked in cross-modal tasks

## Why This Works (Mechanism)

### Mechanism 1: Decoupling of Retrieval from Reasoning
- **Claim**: Benchmark validity improves when prior knowledge is isolated from intuitive visual reasoning.
- **Mechanism**: MANBench separates tasks into "Knowledge" (e.g., Biology, Physics) and "Non-Knowledge" (e.g., Puzzles, Spatial Imagination). This forces the model to rely on visual processing and logical inference rather than retrieving memorized text correlations.
- **Core assumption**: Assumption: Model performance on knowledge-heavy datasets may mask deficiencies in core visual reasoning capabilities.
- **Evidence anchors**:
  - [abstract] "...separates questions requiring prior knowledge from those that do not..."
  - [section 4.4 Analysis] "children under the age of fourteen perform comparably to adults... indicating that prior knowledge is not crucial... emphasizing the dataset's focus on reasoning."
  - [corpus] "MME-Reasoning" (Neighbor) supports the need for explicit categorization of logical reasoning vs. knowledge retrieval.
- **Break condition**: If models begin to memorize the specific visual patterns of the "non-knowledge" puzzles (data contamination), the distinction fades.

### Mechanism 2: Mandatory Cross-Modal Integration
- **Claim**: Requiring joint text-image processing prevents "blind" solving where models answer using text priors alone.
- **Mechanism**: The benchmark design mandates that visual content is necessary to answer the question (Limitation 3). By analyzing performance drop when images are masked, one can verify if the model is actually "looking" at the image.
- **Core assumption**: Assumption: A robust MLLM must show significant performance degradation when visual input is removed for reasoning tasks.
- **Evidence anchors**:
  - [abstract] "...mandates the integration of textual and visual information..."
  - [section 4.4 Analysis / Figure 7] GPT-4o accuracy drops significantly in tasks like Image Consistency when images are removed (often refusing to answer), proving visual dependency.
  - [corpus] "Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark" (Neighbor) highlights similar constraints for multi-image contexts.
- **Break condition**: If the text prompt leaks the answer (language bias), the mechanism fails. The paper notes a specific case in "Transmorphic Understanding" where the model guessed "Joy" (positive bias) without images, slightly beating random chance.

### Mechanism 3: Difficulty Gradient for Ceiling Detection
- **Claim**: A wide difficulty spectrum is required to distinguish between model scaling improvements and fundamental capability gaps.
- **Mechanism**: MANBench includes tasks like "Puzzles" and "Spatial Imagination" which humans find difficult (avg <55% and ~39% respectively). This prevents the "ceiling effect" where models achieve 90%+ accuracy, obscuring differences between SOTA models.
- **Core assumption**: Assumption: Tasks that challenge human cognitive limits (spatial rotation, abstract pattern induction) are valid proxies for high-level machine intelligence.
- **Evidence anchors**:
  - [abstract] "...both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination."
  - [section 4.3.1] MLLMs achieve ~30% accuracy on Puzzles, comparable to random guessing, revealing a critical failure mode.
  - [section 2.2 Limitation 4] Mentions avoiding "Narrow Difficulty Spectrum."
- **Break condition**: If the difficulty stems from ambiguous phrasing rather than reasoning complexity (noise), the metric becomes a measure of robustness to noise rather than intelligence.

## Foundational Learning

- **Concept**: **Vision-Language Misalignment**
  - **Why needed here**: MANBench reveals that while models can align nouns to objects (Knowledge/Text-Image tasks), they fail to align "spatial relations" or "abstract emotional associations" (Transmorphic/Spatial tasks).
  - **Quick check question**: Can the model identify a "cat" (feature alignment) vs. can it determine if a "folded shape corresponds to an unfolded net" (relational alignment)?

- **Concept**: **Cross-Modal Reasoning Types**
  - **Why needed here**: The paper distinguishes between "Knowledge Retrieval," "Perception" (Text Locating), and "Cognitive Reasoning" (Puzzles). Differentiating these is crucial for architectural diagnosis.
  - **Quick check question**: Does the model solve the problem by recalling a fact (e.g., "Granite is a rock") or by processing visual geometry (e.g., "Which path is shortest?")?

- **Concept**: **Negative Constraints in Evaluation**
  - **Why needed here**: Section 2.2 explicitly designs the benchmark to avoid specific limitations (e.g., "Overreliance on Prior Knowledge"). Understanding what a benchmark *excludes* is as important as what it includes.
  - **Quick check question**: If a user scores highly on "Knowledge" but poorly on "Spatial Imagination," do they possess general multimodal intelligence or just a good database?

## Architecture Onboarding

- **Component map**: Vision Encoder (e.g., ViT) -> Projector -> LLM Backbone
- **Critical path**: The flow from **Image Processing** -> **Feature Projection** -> **LLM Reasoning**. The paper highlights that the bottleneck is currently in the **LLM Reasoning** phase (applying logic to visual tokens), specifically for spatial/abstract inferences, rather than the feature extraction phase.
- **Design tradeoffs**:
  - **Bilingual (En/Zh) vs. Mono-lingual**: Increases inclusivity but introduces translation variance (mitigated by manual review).
  - **Multiple Choice vs. Generation**: Multiple choice allows for automated, scalable evaluation (VLMEvalKit) but limits the expression of nuanced "open-ended" reasoning.
  - **Sample Size**: Trading depth (each participant doing 10% of the set) for breadth (575 participants) to manage human fatigue (approx. 20 mins per person).
- **Failure signatures**:
  - **The "Random Guesser"**: ~25% accuracy (or ~30% observed in Puzzles) indicates the visual tokens are effectively noise to the LLM; it cannot map the visual pattern to the logic.
  - **The "Bias Haver"**: As seen in the Transmorphic ablation, if the model answers "Joy" (positive bias) without images, it is hallucinating intent based on language priors.
  - **The "Refuser"**: Closed-source models (like GPT-4o in the ablation) refusing to answer when images are missing confirms strict visual dependency is a positive trait for these specific tasks.
- **First 3 experiments**:
  1. **Sanity Check (Knowledge vs. Reasoning)**: Run the target MLLM on the "Knowledge" subset vs. the "Puzzles" subset. If Knowledge is high (>75%) and Puzzles is low (~25%), the model is a "Retriever," not a "Reasoner."
  2. **Text-Only Ablation**: Run the benchmark with images masked. If performance stays above random, the text questions likely contain shortcuts or biases (failure of the benchmark design or model reliance on priors).
  3. **Human Baseline Calibration**: Before trusting the MLLM score, verify the human baseline on a specific subset (e.g., 5 participants) to ensure the "difficulty" isn't just ambiguous wording. Compare their time-to-solution against the paper's Figure 8.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training deficiencies prevent current MLLMs from achieving non-random performance on abstract puzzle and spatial imagination tasks?
- Basis in paper: [explicit] Section 4.3.1 notes that on the Puzzles task, MLLM accuracy is approximately 30%, which is "comparable to random guessing," highlighting a significant lack of capability in this domain compared to humans.
- Why unresolved: The paper identifies the low performance but does not investigate whether the failure stems from a lack of spatial inductive biases, tokenization issues, or the absence of abstract reasoning data in pre-training corpora.
- What evidence would resolve it: An ablation study showing significant performance improvements on MANBench's "Puzzles" task through specific architectural modifications (e.g., incorporating graph neural networks) or targeted fine-tuning on synthetic spatial reasoning data.

### Open Question 2
- Question: How can the performance gap between knowledge-based tasks and non-knowledge-based cross-modal reasoning tasks be effectively bridged in MLLMs?
- Basis in paper: [explicit] The Conclusion states that while MLLMs "excel in tasks such as text generation and knowledge retrieval, they encounter significant challenges in tasks requiring nuanced cross-modal reasoning."
- Why unresolved: The paper establishes a dichotomy where models utilize pre-trained knowledge effectively but fail when required to perform seamless integration of visual and textual logic without relying on prior facts, leaving the solution for this transfer as future work.
- What evidence would resolve it: The development of a training methodology that improves non-knowledge-based reasoning scores (e.g., Image Consistency, Transmorphic Understanding) to levels comparable with Knowledge scores, without merely memorizing new visual patterns.

### Open Question 3
- Question: To what extent do MLLMs rely on linguistic priors versus true visual integration when solving tasks like Transmorphic Understanding?
- Basis in paper: [inferred] Figure 7 shows that in the Transmorphic Understanding task, GPT-4o's accuracy without images slightly exceeded the random baseline. The analysis attributes this to the model's preference for positive emotions (like "joy") which appear more frequently as answers, suggesting a reliance on text-based priors over visual processing.
- Why unresolved: The paper observes the bias but does not quantify the degree to which this linguistic "cheating" inflates scores in multimodal settings compared to pure visual understanding.
- What evidence would resolve it: A counterfactual evaluation where the text prompts or answer distributions are manipulated to conflict with visual evidence, measuring the drop in accuracy to isolate visual reliance.

## Limitations
- The paper relies heavily on multiple-choice format, which may artificially constrain model reasoning and allow for pattern-matching shortcuts not present in open-ended tasks
- Human baseline establishment shows high variance (Best vs Average scores), raising questions about whether the benchmark difficulty is consistent across cognitive domains
- The specific mechanism by which bilingual design improves benchmark validity lacks direct validation of translation quality or cultural bias mitigation

## Confidence
- **High Confidence**: Claims about knowledge-based vs reasoning-based task separation (supported by human age-group analysis showing children perform similarly to adults on non-knowledge tasks)
- **Medium Confidence**: General claims about MLLMs underperforming humans across complex tasks (though specific accuracy numbers may vary with implementation details)
- **Low Confidence**: The specific mechanism by which bilingual design improves benchmark validity (no direct validation of translation quality or cultural bias mitigation)

## Next Checks
1. **Format Ablation Test**: Run a subset of questions in open-ended format rather than multiple-choice to determine if accuracy differences persist, validating that poor performance reflects reasoning limitations rather than multiple-choice constraints
2. **Cross-Lingual Consistency Check**: Evaluate the same model on both English and Chinese subsets using identical prompts to verify the bilingual design doesn't introduce systematic bias or performance artifacts
3. **Human-Expert Calibration**: Have domain experts (e.g., geometry teachers for Spatial Imagination tasks) review a random sample of questions to confirm difficulty stems from reasoning complexity rather than ambiguous wording or cultural specificity