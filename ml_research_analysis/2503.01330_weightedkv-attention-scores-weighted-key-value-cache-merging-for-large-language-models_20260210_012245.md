---
ver: rpa2
title: 'WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large Language
  Models'
arxiv_id: '2503.01330'
source_url: https://arxiv.org/abs/2503.01330
tags:
- cache
- values
- attention
- merging
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WeightedKV compresses KV cache by discarding unimportant token
  keys and merging their values into neighboring tokens via a convex combination weighted
  by average attention scores. This training-free approach preserves crucial information
  while reducing memory usage, as values contain more evenly distributed information
  compared to keys.
---

# WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large Language Models

## Quick Facts
- **arXiv ID**: 2503.01330
- **Source URL**: https://arxiv.org/abs/2503.01330
- **Reference count**: 21
- **Primary result**: WeightedKV achieves lower perplexity than baseline methods (StreamingLLM, H2O, TOVA, CaM) on four datasets using Llama-2-7B, particularly with smaller cache sizes

## Executive Summary
WeightedKV introduces a training-free method for compressing KV caches in large language models by discarding unimportant token keys and merging their values into neighboring tokens via convex combinations weighted by average attention scores. The approach leverages the observation that values contain more evenly distributed information compared to keys, allowing for effective compression without significant quality degradation. Evaluated across four language modeling datasets (PG19, OpenWebText2, ProofPile, ArXiv) using Llama-2-7B, WeightedKV demonstrates consistent perplexity improvements over baseline methods, particularly when cache sizes are constrained.

## Method Summary
WeightedKV operates by first computing average attention scores for each token in the KV cache, then identifying and discarding tokens with low attention scores while preserving their information through value merging. The merging process uses convex combinations where the discarded token's value is distributed to neighboring tokens proportionally to their attention weights. This approach maintains crucial contextual information while significantly reducing memory footprint, making it particularly valuable for long-text generation scenarios where KV cache size becomes a bottleneck. The method is training-free and can be applied to any transformer-based model without fine-tuning.

## Key Results
- WeightedKV achieves lower perplexity than baseline methods (StreamingLLM, H2O, TOVA, CaM) across four datasets
- Performance improvements are particularly pronounced with smaller cache sizes
- The method shows minimal perturbation to subsequent attention weights while maintaining context integrity
- Consistent improvements observed on PG19, OpenWebText2, ProofPile, and ArXiv datasets using Llama-2-7B

## Why This Works (Mechanism)
The effectiveness of WeightedKV stems from the asymmetric nature of information distribution between keys and values in transformer attention mechanisms. Keys serve as query-specific selectors that gate information flow, making them more context-dependent and less uniformly informative. Values, conversely, contain more general semantic information that can be partially preserved through weighted merging. By focusing compression on keys while preserving value information through convex combinations weighted by attention scores, WeightedKV maintains the essential context needed for coherent generation while reducing memory requirements. The attention score weighting ensures that merged values are combined in proportion to their relevance, minimizing information loss.

## Foundational Learning

**Transformer Attention Mechanism**
- *Why needed*: Understanding how keys, queries, and values interact is crucial for grasping why compressing keys while preserving values is effective
- *Quick check*: Verify that attention weights represent the importance of each value for computing the output representation

**KV Cache Compression**
- *Why needed*: The paper builds on existing compression techniques but introduces a novel approach to merging values
- *Quick check*: Confirm that KV caches store intermediate activations to avoid recomputation during autoregressive generation

**Convex Combinations**
- *Why needed*: The merging operation relies on weighted averaging to preserve information from discarded tokens
- *Quick check*: Ensure the weights sum to 1 and maintain the relative importance of merged values

## Architecture Onboarding

**Component Map**
- Attention layer -> Average attention score computation -> Token importance ranking -> Key discarding -> Value merging via convex combination -> Compressed KV cache

**Critical Path**
The critical computational path involves computing average attention scores across all tokens in the cache, ranking tokens by importance, discarding low-scoring keys, and performing weighted merging of corresponding values. The attention score computation must complete before any merging can occur, making it the primary bottleneck.

**Design Tradeoffs**
- Training-free deployment vs. task-specific optimization
- Memory reduction vs. potential information loss from discarded keys
- Computational overhead of attention score computation vs. savings from smaller cache

**Failure Signatures**
- Performance degradation when attention scores are uniformly distributed
- Quality loss in highly dynamic contexts where token importance shifts rapidly
- Compression artifacts when merging tokens with conflicting semantic content

**Three First Experiments**
1. Compare perplexity degradation when compressing different percentages of the cache
2. Measure attention weight perturbation before and after compression
3. Ablation study isolating the contribution of key-value asymmetry to performance

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Compression comes at the cost of potential information loss from discarded keys
- Evaluation focuses primarily on perplexity without examining factual consistency or semantic coherence
- Claims about values containing more evenly distributed information lack empirical validation through ablation studies
- Training-free nature may limit optimization for specific downstream tasks

## Confidence
- **High**: Core technical mechanism is clearly described and perplexity improvements over baselines are well-documented
- **Medium**: Claim about minimal perturbation to subsequent attention weights supported by limited evidence
- **Low**: Assertion about effectiveness with smaller cache sizes lacks statistical significance testing

## Next Checks
1. Conduct ablation studies to quantify the specific contribution of key-value asymmetry to WeightedKV's performance by comparing against methods that treat keys and values symmetrically
2. Evaluate generation quality beyond perplexity by measuring factual consistency, semantic coherence, and hallucination rates in long-form outputs
3. Perform statistical significance testing on the perplexity improvements across different cache sizes and datasets to confirm the robustness of claimed performance gains