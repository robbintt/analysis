---
ver: rpa2
title: How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining
arxiv_id: '2511.18903'
source_url: https://arxiv.org/abs/2511.18903
tags:
- data
- uni00000051
- uni00000048
- uni00000011
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a previously overlooked incompatibility between
  curriculum-based data ordering and standard learning rate decay schedules in large
  language model pretraining. While curriculum learning (training on data sorted by
  quality) improves performance with constant learning rates, its benefits diminish
  when combined with aggressive learning rate decay schedules.
---

# How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining

## Quick Facts
- arXiv ID: 2511.18903
- Source URL: https://arxiv.org/abs/2511.18903
- Authors: Kairong Luo; Zhenbo Sun; Haodong Wen; Xinyu Shi; Jiarui Cui; Chenyi Dang; Kaifeng Lyu; Wenguang Chen
- Reference count: 40
- Primary result: Curriculum learning's benefits diminish under standard LR decay; proposed CMA/CDMA strategies yield 1.64% avg. downstream accuracy improvement.

## Executive Summary
This paper reveals a critical incompatibility between curriculum-based data ordering and standard learning rate decay schedules in LLM pretraining. While curriculum learning (training on data sorted by quality) improves performance with constant learning rates, its advantages vanish when combined with aggressive LR decay. The authors show that LR acts as an implicit importance weight, and when high-quality data is processed during the low-LR phase, its influence is reduced. They propose two complementary strategies: moderate LR decay (final LR ≈ 1/3 of peak) and model averaging (curriculum model averaging or CMA), which computes weighted averages of recent checkpoints while maintaining constant LR. Combining these strategies achieves an average 1.64% improvement in downstream benchmarks without additional data refinement.

## Method Summary
The authors identify that curriculum learning's benefits are negated by standard LR decay schedules. They propose two complementary solutions: (1) using moderate LR decay where the final LR is approximately 1/3 of the peak LR, and (2) replacing LR decay with model averaging (specifically curriculum model averaging or CMA). CMA computes a weighted average of recent checkpoints while maintaining a constant learning rate. The approach is validated on 1.5B-parameter models trained on 30B tokens, achieving an average 1.64% improvement in downstream benchmark accuracy over standard random shuffling and decay schedules.

## Key Results
- Curriculum learning with constant LR outperforms random shuffling by ~1.7% on downstream benchmarks
- Standard LR decay (final LR ≈ 10⁻⁵) eliminates curriculum learning's advantages
- Moderate LR decay (final LR ≈ 10⁻³, ~1/3 of peak) combined with model averaging recovers and exceeds curriculum benefits
- CMA strategy alone (constant LR + averaging) achieves similar performance to moderate decay + averaging
- WMA underperforms EMA/SMA for curriculum-based training

## Why This Works (Mechanism)

### Mechanism 1
Aggressive LR decay reduces the effective influence of high-quality data samples when introduced late in training. The parameter update at step t is θ_{t+1} = θ_t - η_t g_t, where the learning rate η_t acts as an implicit importance weight. In standard curricula, high-quality data is presented at the end of training, but if the learning rate has decayed to a low value, the magnitude of the update from this valuable data is minimized, negating its benefit.

### Mechanism 2
Model averaging substitutes for LR decay to stabilize final model parameters while maintaining high update magnitude. Instead of reducing the learning rate to dampen noise in final training steps, the method maintains constant (or moderately decaying) LR and computes a weighted average (e.g., EMA, SMA) of final checkpoints. This averaging smooths parameter noise, yielding a stable final model without sacrificing the signal from high-quality data introduced late in training.

### Mechanism 3
A co-designed regime of ascending data order and moderate LR decay leverages both stable learning from low-quality data and high-impact learning from high-quality data. The model first trains on low-quality data to establish a broad knowledge base, then transitions to high-quality data when the LR is still substantial enough to permit meaningful updates. Model averaging further stabilizes the final model, exploiting the signal-to-noise trade-off.

## Foundational Learning

- **Learning Rate Schedules (Cosine, WSD)**: Understanding how WSD (Warmup-Stable-Decay) and cosine decay schedules reduce LR over time is essential to grasp why they penalize late-stage data. Quick check: Can you explain why an aggressive LR decay (e.g., to 10⁻⁵) might be problematic if the best training data is presented in the final 10% of steps?

- **Curriculum Learning (Ascending Quality)**: The paper's proposed method is built upon the curriculum learning paradigm, where data is presented in a specific order. Quick check: What is the rationale for presenting high-quality data late in training, and what does the paper show is a critical flaw in this rationale?

- **Model/Weight Averaging (EMA, SMA)**: The primary solution proposed (CMA) relies on replacing LR decay with model averaging. Quick check: How does computing a weighted average of the last few checkpoints at a constant learning rate solve a similar problem to decaying the learning rate?

## Architecture Onboarding

- **Component map**: Data Scheduler (sorts corpus by quality) -> Training Loop (modified LR scheduler with warmup-constant/moderate-decay) -> Checkpoint & Averaging Module (computes EMA/SMA of final checkpoints)

- **Critical path**: The joint configuration of data order and LR schedule. Sorting data and then applying standard cosine decay will likely fail to produce expected gains. Data must be sorted AND LR schedule adjusted (constant or moderate decay) AND model averaging applied.

- **Design tradeoffs**: 
  - Moderate Decay vs. Constant LR: Constant LR with averaging (CMA) is simpler but may not match final convergence of decay schedule on some benchmarks
  - Averaging Strategy: EMA with decay factor (e.g., α=0.2) gives more weight to last checkpoint, while SMA gives equal weight
  - Sorting Granularity: Global end-to-end sort is most effective but requires processing entire dataset at once

- **Failure signatures**:
  1. Marginal or no improvement: Likely caused by applying curriculum on top of standard, aggressive LR decay schedule
  2. Instability in final loss: May occur if LR is kept too high without model averaging or with insufficient averaging
  3. Degraded performance vs. baseline: Could happen if data quality metric is poorly chosen and not well-correlated with downstream task performance

- **First 3 experiments**:
  1. Baseline Reproduction: Train with random shuffling + WSD (final LR ≈ 10⁻⁵) vs. ascending curriculum + same WSD
  2. Ablation - LR Schedule: Train with ascending data order and standard cosine decay
  3. Pilot of Proposed Method: Train with ascending data order and constant LR, then apply EMA over last 6 checkpoints

## Open Questions the Paper Calls Out

### Open Question 1
Does the CMA strategy scale effectively to larger models (e.g., 7B+ parameters) and longer training runs (hundreds of billions of tokens)? The authors validated on "1.5B-parameter models trained over 30B tokens" and call for larger-scale validation.

### Open Question 2
Can the optimal combination of ending LR, model averaging strategy, and data ordering be theoretically characterized rather than determined empirically? The authors note this as "a promising direction for future work" and highlight the need for systematic recipes.

### Open Question 3
How robust is the CMA approach across diverse quality metrics and heterogeneous data mixtures beyond the DCLM and PreSelect scores tested? The ablation study showed PreSelect scores yielded slightly lower performance, suggesting method effectiveness may depend on metric alignment.

## Limitations
- Results validated primarily on 1.5B models and specific DCLM-Baseline data; generalization to larger models and different quality metrics is assumed but not confirmed
- The paper demonstrates improved downstream accuracy but doesn't analyze how the improved curriculum+LR schedule affects internal representations or knowledge organization
- Focus on ascending quality order; incompatibility may manifest differently with other curriculum strategies (e.g., mixed-quality batches)

## Confidence
- **High Confidence**: The core claim that aggressive LR decay diminishes benefits of curriculum learning is well-supported by empirical results and theoretical mechanism
- **Medium Confidence**: Proposed solutions (CMA and CDMA) are effective, but optimal parameter settings may be specific to experimental setup and require tuning for other contexts
- **Medium Confidence**: The paper argues for fundamental incompatibility between standard LR decay and curriculum learning, but this claim could be strengthened by testing alternative decay schedules or curriculum strategies

## Next Checks
1. **Scaling Validation**: Test CDMA approach on larger model sizes (e.g., 7B, 13B parameters) and different datasets to confirm benefits scale beyond 1.5B model
2. **Curriculum Strategy Comparison**: Compare CDMA against other curriculum learning strategies (e.g., mixed-quality batches) combined with standard LR decay to determine if incompatibility is general or specific to ascending quality order
3. **Representation Analysis**: Analyze internal representations (e.g., using probing classifiers) of models trained with and without CDMA to understand how improved curriculum+LR schedule affects learned knowledge and its organization