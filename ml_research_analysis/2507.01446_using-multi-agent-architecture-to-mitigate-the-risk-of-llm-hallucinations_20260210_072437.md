---
ver: rpa2
title: Using multi-agent architecture to mitigate the risk of LLM hallucinations
arxiv_id: '2507.01446'
source_url: https://arxiv.org/abs/2507.01446
tags:
- agent
- will
- message
- agents
- customer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a multi-agent system for processing SMS-based\
  \ customer requests in pharmacy settings, integrating Large Language Models (LLMs)\
  \ with fuzzy logic to reduce hallucination risks. The system uses an orchestration\
  \ agent to dynamically route messages to specialized worker agents\u2014one using\
  \ regex and fuzzy logic, another leveraging LLM tools for complex parsing."
---

# Using multi-agent architecture to mitigate the risk of LLM hallucinations
## Quick Facts
- arXiv ID: 2507.01446
- Source URL: https://arxiv.org/abs/2507.01446
- Reference count: 0
- Primary result: Multi-agent system with fuzzy logic and LLM cross-validation reduces hallucination risks in SMS-based pharmacy request processing

## Executive Summary
This paper presents a multi-agent system designed to process SMS-based customer requests in pharmacy settings while mitigating the risk of LLM hallucinations. The system combines an orchestration agent that dynamically routes messages to specialized worker agents, including one using regex and fuzzy logic for simple patterns and another leveraging LLM tools for complex parsing. Validation agents compare outputs across multiple LLM providers (Gemini, ChatGPT) to detect hallucinations, while fuzzy rules assess confidence and risk levels. Testing on 10 SMS samples over 50 repeated trials demonstrated accurate keyword extraction with only one false positive, successfully reducing hallucination risks through cross-validation and domain-specific confidence scoring.

## Method Summary
The system employs a multi-agent architecture where an orchestration agent receives SMS messages and routes them to specialized worker agents based on message complexity. One worker agent handles simple patterns using regex and fuzzy logic, while another agent leverages LLM tools for complex parsing tasks. Validation agents compare outputs from different LLM providers (Gemini and ChatGPT) to identify potential hallucinations. The system incorporates fuzzy logic rules to evaluate confidence scores and assess risk levels for each processed message. The architecture is designed to dynamically adapt to different message types and validate LLM outputs through cross-provider comparison, with particular focus on pharmacy domain applications.

## Key Results
- System achieved accurate keyword extraction from SMS messages with only one false positive across 50 repeated trials
- Cross-validation across multiple LLM providers (Gemini, ChatGPT) successfully detected and mitigated hallucination risks
- Fuzzy logic-based confidence scoring effectively assessed risk levels for processed messages in pharmacy domain

## Why This Works (Mechanism)
The multi-agent architecture works by distributing processing tasks across specialized agents, preventing any single point of failure and reducing the cognitive load on individual LLMs. The orchestration agent's dynamic routing ensures that simple messages are processed efficiently through regex and fuzzy logic, while complex messages receive LLM-powered analysis. Cross-validation between multiple LLM providers creates a consensus mechanism that identifies hallucinations when outputs diverge. The fuzzy logic component provides a quantitative framework for assessing confidence and risk, allowing the system to flag uncertain results for human review.

## Foundational Learning
- Multi-agent orchestration: Multiple specialized agents work together to solve complex problems by dividing tasks based on expertise and complexity
- Cross-LLM validation: Comparing outputs from different LLM providers creates redundancy that helps identify hallucinations when models disagree
- Fuzzy logic for confidence scoring: Mathematical framework for handling uncertainty and partial truths in decision-making processes
- Dynamic routing mechanisms: Intelligent distribution of tasks to appropriate agents based on message characteristics and complexity
- Validation agent patterns: Specialized components that verify and cross-check outputs from primary processing agents
- Domain-specific risk assessment: Application of confidence scoring tailored to specific industry contexts and requirements

## Architecture Onboarding
Component Map: SMS Input -> Orchestration Agent -> Worker Agents (Regex/Fuzzy + LLM Tool) -> Validation Agents (Gemini + ChatGPT) -> Fuzzy Logic Risk Assessment -> Output
Critical Path: SMS message enters system -> Orchestration agent routes to appropriate worker -> Worker processes message -> Validation agents cross-check outputs -> Fuzzy logic evaluates confidence -> Final validated output
Design Tradeoffs: Single powerful LLM vs. multiple specialized agents (complexity vs. hallucination mitigation); Simple regex rules vs. LLM parsing (speed vs. accuracy); Single validation vs. cross-provider comparison (simplicity vs. reliability)
Failure Signatures: All LLMs agreeing on incorrect output; Fuzzy logic confidence scores conflicting with validation results; Dynamic routing misclassifying message complexity; Validation agents failing to detect subtle hallucinations
First Experiments: 1) Test system with simple SMS patterns to verify regex/fuzzy agent performance; 2) Evaluate cross-validation effectiveness by introducing controlled hallucinations in one LLM; 3) Assess fuzzy logic confidence scoring accuracy across different risk scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size with only 10 SMS samples tested across 50 trials, potentially missing diverse real-world communication patterns
- Heavy reliance on comparing outputs across LLM providers without addressing scenarios where all models might hallucinate similarly
- Limited transparency in fuzzy logic rule derivation and lack of robustness testing across different contexts

## Confidence
- Hallucination mitigation effectiveness: Medium confidence - supported by experimental results but limited dataset
- Fuzzy logic-based risk assessment: Low confidence - rules lack detailed explanation and sensitivity analysis
- Keyword extraction accuracy: Medium confidence - results may be influenced by specific test sample characteristics

## Next Checks
1. Test system with a much larger and more diverse dataset of pharmacy SMS communications, including edge cases and ambiguous messages
2. Evaluate performance when different combinations of LLM providers are used, including scenarios where all models might hallucinate similarly
3. Conduct sensitivity analysis of fuzzy logic rules to assess robustness and reliability across different confidence and risk scoring scenarios