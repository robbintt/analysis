---
ver: rpa2
title: 'DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence'
arxiv_id: '2511.10404'
source_url: https://arxiv.org/abs/2511.10404
tags:
- entity
- delicate
- historical
- entities
- wikidata
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DELICATE addresses Entity Linking in historical Italian by combining\
  \ a BERT-based bi-encoder for candidate retrieval with a Gradient-Boosted Trees\
  \ (GBTs) classifier that incorporates Wikidata\u2019s entity type and temporal information.\
  \ This neuro-symbolic approach outperforms both neural baselines (BLINK IT A, mGENRE)\
  \ and LLM-based variants (BLINK IT A-LLM) on ENEIDE, achieving up to 72.67% micro-accuracy\
  \ and 69.07 F1 on end-to-end EL."
---

# DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence

## Quick Facts
- arXiv ID: 2511.10404
- Source URL: https://arxiv.org/abs/2511.10404
- Reference count: 33
- DELICATE achieves 72.67% micro-accuracy and 69.07 F1 on end-to-end Entity Linking for historical Italian texts

## Executive Summary
DELICATE is a neuro-symbolic approach for diachronic Entity Linking (EL) in historical Italian texts that combines a BERT-based bi-encoder for candidate retrieval with a Gradient-Boosted Trees (GBTs) classifier for re-ranking. The system incorporates both entity type and temporal information from Wikidata to address the challenges of historical texts where language, named entities, and entity references evolve over time. DELICATE outperforms both neural baselines (BLINK IT A, mGENRE) and LLM-based variants (BLINK IT A-LLM) on the ENEIDE dataset while maintaining computational efficiency through frozen retrieval weights.

## Method Summary
The DELICATE framework employs a two-stage process: first, a bi-encoder architecture retrieves candidate entities from a knowledge base using frozen BLINK IT A weights; second, a GBT classifier re-ranks these candidates by incorporating entity type and temporal evidence from Wikidata. The model is trained on ENEIDE, a dataset of 100 manually annotated historical Italian documents, and evaluated on both in-domain (ENEIDE) and out-of-domain (MHERCL-ITA) datasets. The approach addresses the unique challenges of historical EL by leveraging structured knowledge about entity types and temporal contexts to disambiguate mentions that may refer to different entities across time periods.

## Key Results
- Achieves 72.67% micro-accuracy and 69.07 F1 on end-to-end EL on ENEIDE dataset
- Outperforms BLINK IT A-LLM (67.30% accuracy) and matches mGENRE (73.30% accuracy) on ENEIDE
- Generalizes to out-of-domain MHERCL-ITA dataset (58.78% accuracy) without fine-tuning
- Provides interpretable predictions with better confidence calibration than purely neural systems

## Why This Works (Mechanism)
DELICATE leverages the complementary strengths of neural retrieval and symbolic re-ranking. The bi-encoder efficiently narrows down candidate entities while the GBT classifier applies structured reasoning about entity types and temporal contexts. By incorporating temporal evidence from Wikidata, the system can distinguish between entities that share names but exist in different historical periods. The supervised re-ranking approach allows the model to learn complex decision boundaries that combine lexical, semantic, and temporal features, resulting in more accurate and interpretable predictions compared to end-to-end neural models.

## Foundational Learning
- **Bi-encoder architecture**: Separates mention and entity encoding for efficient candidate retrieval; needed for scalability in large knowledge bases
- **Gradient-Boosted Trees**: Ensemble method that combines weak learners to create strong predictors; needed for interpretable re-ranking with structured features
- **Temporal entity linking**: Disambiguating entities based on historical time periods; needed because entity references change meaning across centuries
- **Entity type hierarchy**: Classification of entities into semantic categories; needed to capture type compatibility between mentions and candidates
- **Knowledge base population**: Extracting and structuring information from Wikidata; needed to provide temporal and type features for re-ranking
- **Cross-lingual transfer**: Adapting models from English to Italian; needed because most EL resources are developed for English first

## Architecture Onboarding
**Component Map**: Mention -> Bi-encoder (frozen) -> Candidate set -> GBT Re-ranker (temporal + type features) -> Final entity prediction

**Critical Path**: The most compute-intensive stage is the bi-encoder retrieval, but since weights are frozen, inference is fast. The GBT re-ranker is lightweight but crucial for accuracy gains.

**Design Tradeoffs**: Frozen bi-encoder weights sacrifice some domain adaptation for computational efficiency and faster inference. The symbolic re-ranking approach trades some end-to-end optimization for interpretability and confidence calibration.

**Failure Signatures**: Poor performance on entities with sparse temporal information in Wikidata, failure to capture subtle language changes in historical texts, and reduced accuracy for entities outside the type hierarchy coverage.

**First 3 Experiments**:
1. Ablation study removing temporal features to measure their contribution to accuracy
2. Testing on a temporally diverse corpus spanning multiple centuries to assess temporal generalization
3. Runtime comparison against end-to-end neural baselines on identical hardware

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Performance relies on two relatively small and specialized datasets (ENEIDE and MHERCL-ITA) that may not generalize to broader historical Italian domains
- Bi-encoder uses frozen BLINK IT A weights without fine-tuning on target corpora, potentially limiting adaptation to historical language variants
- Exclusivity to Italian historical texts leaves unclear whether the approach transfers effectively to other languages or time periods

## Confidence
*High confidence:* The core methodology combining bi-encoder retrieval with GBT-based re-ranking incorporating entity types and temporal features is sound and technically coherent.

*Medium confidence:* The claim of better interpretability and confidence calibration compared to neural systems is plausible but not empirically validated with user studies.

*Low confidence:* The generalization claim to MHERCL-ITA without fine-tuning needs stronger validation, as the datasets may share underlying characteristics despite being described as out-of-domain.

## Next Checks
1. Test DELICATE on a temporally diverse historical Italian corpus spanning multiple centuries to assess temporal generalization beyond the ENEIDE and MHERCL-ITA time ranges.

2. Conduct ablation studies isolating the contributions of entity type features versus temporal features to quantify their relative importance in the GBT re-ranker.

3. Compare runtime and memory requirements against end-to-end neural baselines (mGENRE, BLINK IT A-LLM) on identical hardware to empirically verify computational efficiency claims.