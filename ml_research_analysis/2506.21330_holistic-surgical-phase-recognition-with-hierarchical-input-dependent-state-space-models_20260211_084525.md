---
ver: rpa2
title: Holistic Surgical Phase Recognition with Hierarchical Input Dependent State
  Space Models
arxiv_id: '2506.21330'
source_url: https://arxiv.org/abs/2506.21330
tags:
- surgical
- phase
- state
- temporal
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hierarchical input-dependent state space
  model for surgical phase recognition in long surgical videos. The model leverages
  state space models' linear scaling property to process full-length videos efficiently
  while capturing both local and global temporal dynamics.
---

# Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models

## Quick Facts
- **arXiv ID**: 2506.21330
- **Source URL**: https://arxiv.org/abs/2506.21330
- **Reference count**: 36
- **Primary result**: Hierarchical Input-Dependent State Space Models achieve +2.8% accuracy on Cholec80, +4.3% on MICCAI2016, and +12.9% on Heichole datasets for surgical phase recognition

## Executive Summary
This paper introduces a hierarchical input-dependent state space model (HID-SSM) for surgical phase recognition in long surgical videos. The model leverages the linear scaling property of state space models to process full-length videos efficiently while capturing both local and global temporal dynamics. A temporally consistent visual feature extractor incorporates temporal information early in the pipeline. The hierarchical architecture consists of local-aggregation and global-relation state space model blocks that capture fine-grained surgical actions and overall workflow structure. The model is trained using hybrid discrete-continuous supervision, optimizing both phase labels and continuous phase progress. Experiments demonstrate significant performance improvements over state-of-the-art methods across three surgical video datasets.

## Method Summary
The method employs a two-stage pipeline: first, a temporally consistent feature extractor is pretrained using a Swin Transformer backbone with temporary SSM layers and temporal sparsification (every 5th frame plus sliding window). Second, the HID-SSM core processes the extracted features through a Phase Proposal Network that segments videos into pseudo-phases, followed by Local-Aggregation SSM blocks that process individual segments and Global-Relation SSM blocks that model overall workflow. The model uses hybrid supervision with weighted cross-entropy for discrete phase labels and mean squared error for continuous phase progress, trained with Adam optimizer and specific learning rates for different components.

## Key Results
- **Cholec80 dataset**: 94.5% accuracy (+2.8% over state-of-the-art)
- **MICCAI2016 dataset**: 90.4% accuracy (+4.3% over state-of-the-art)
- **Heichole dataset**: 78.5% micro-F1 score (+12.9% over state-of-the-art)
- **Performance on challenging phases**: Significantly improved recognition of short and variable phases like "Cleaning Coagulation" (65.87% Jaccard)

## Why This Works (Mechanism)

### Mechanism 1: Linear Complexity for Full-Video Context
The model processes entire surgical videos without the quadratic memory bottleneck of Transformers, enabling holistic decision making. State Space Models recast sequence modeling as a linear recurrence derived from discretized ordinary differential equations, achieving O(L) complexity versus O(L²) for attention-based methods. The matrix mixer view shows how global mixing is achieved without pairwise comparison, effectively compressing and retaining historical information over thousands of steps.

### Mechanism 2: Input-Dependent Selective Activation
The model dynamically filters information through an input-dependent timescale parameter Δ_t. Large Δ_t causes state decay (resetting state) to pay attention during critical phase transitions, while small Δ_t preserves state to ignore irrelevant frames. This mechanism exploits the temporal structure of surgical procedures, which consist of long periods of static workflow interrupted by rapid transitions.

### Mechanism 3: Hierarchical Local-Global Decomposition
The architecture splits processing into Local-Aggregation SSM (LA-SSM) and Global-Relation SSM (GR-SSM) blocks. LA-SSM uses block-diagonal matrices to process segmented pseudo-phases independently, isolating local dynamics. GR-SSM uses full matrices to integrate these segments into global workflow understanding. This separation stabilizes training and improves phase boundary accuracy by preventing competition between local and global representational capacity.

## Foundational Learning

- **State Space Models (SSMs) & Discretization**: Understanding how continuous ODEs transform into discrete recurrences is critical for grasping linear scaling advantages. Quick check: Can you explain why zero-order hold discretization allows processing continuous-time signals in discrete timesteps?

- **The Matrix Mixer View**: The paper reinterprets SSMs as "matrix mixers," bridging the understanding of how SSMs perform attention-like global mixing without explicit attention modules. Quick check: How does the matrix M structure differ between Global-Relation (GR) block and Local-Aggregation (LA) block?

- **Hybrid Discrete-Continuous Supervision**: The model optimizes both classification loss (Phase Label) and regression loss (Phase Progress ∈ 0..1). Quick check: Why might predicting continuous progress help the model learn better representations for discrete phase boundaries?

## Architecture Onboarding

- **Component map**: Input frames → Swin Transformer + Shallow SSM → Phase Proposal Network → LA-SSM (block-diagonal) → GR-SSM (full matrix) → MLP heads (discrete + continuous outputs)

- **Critical path**: The calculation of input-dependent timescale Δ_t. This is the "heart" of the selective activation mechanism. If Δ_t is not computed correctly or weights are frozen, the model reverts to a generic LTI system and loses adaptive capability.

- **Design tradeoffs**: 
  - Causal vs Contextual modes: Contextual yields +1.7% accuracy but restricts deployment to post-processing
  - Pre-training cost: Feature extractor requires specific pre-training stage with temporary SSM head, adding pipeline complexity

- **Failure signatures**: 
  - Non-determinism: ID-SSM atomic adds cause ~1% variance in results
  - PPN Instability: Incorrect pseudo-phase segmentation degrades LA-SSM feature aggregation

- **First 3 experiments**:
  1. **Matrix Visualization**: Run a short video and visualize matrix mixer M to verify it's not uniform (showing distinct attention to specific past frames)
  2. **Delta Dynamics Check**: Plot Δ_t over time during phase transitions to verify spikes/dips align with ground truth phase changes
  3. **Hybrid Loss Ablation**: Train with α=1.0 (discrete only) vs. α=0.7 to confirm contribution of continuous progress signal to Jaccard score

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but identifies key limitations including the inherent non-determinism of ID-SSMs causing ~1% performance fluctuations, the restricted experimental validation to laparoscopic cholecystectomy procedures, and the model's struggle with short, variable phases like "Cleaning Coagulation."

## Limitations
- Non-deterministic training behavior from ID-SSM atomic adds introduces ~1% variance in feature extraction, affecting reproducibility
- Experimental validation is restricted to three datasets all consisting exclusively of laparoscopic cholecystectomy, limiting generalizability to other surgical procedures
- The model struggles with short, variable phases, achieving only 65.87% Jaccard for "Cleaning Coagulation" compared to >90% for dominant phases

## Confidence
- **High confidence**: Linear complexity advantage over Transformers, hierarchical decomposition benefits, performance improvements on benchmark datasets
- **Medium confidence**: Input-dependent selective activation mechanism (supported by theory but limited empirical validation), feature extractor pretraining methodology (described but implementation details underspecified)

## Next Checks
1. **Reset mechanism verification**: Measure accuracy degradation when disabling input-dependent timescale calculation (fix Δ_t constant) to quantify selective activation contribution
2. **Hybrid supervision ablation**: Train with discrete-only (α=1.0) versus continuous-only supervision to determine optimal weighting
3. **Determinism stress test**: Run 10 identical training seeds with fixed random initialization to measure variance and establish confidence intervals for reported metrics