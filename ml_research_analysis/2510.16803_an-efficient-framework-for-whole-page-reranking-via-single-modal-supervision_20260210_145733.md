---
ver: rpa2
title: An Efficient Framework for Whole-Page Reranking via Single-Modal Supervision
arxiv_id: '2510.16803'
source_url: https://arxiv.org/abs/2510.16803
tags:
- ranking
- search
- data
- items
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the costly annotation bottleneck in whole-page\
  \ reranking by proposing SMAR, a framework that leverages strong single-modal rankers\
  \ to guide multimodal relevance alignment. SMAR uses two budget-aware strategies\u2014\
  Top-P sampling and binary search for iso-label anchors\u2014to select a small fraction\
  \ of samples for annotation, while the rest are supervised by upstream single-modal\
  \ scores."
---

# An Efficient Framework for Whole-Page Reranking via Single-Modal Supervision

## Quick Facts
- **arXiv ID**: 2510.16803
- **Source URL**: https://arxiv.org/abs/2510.16803
- **Reference count**: 40
- **Primary result**: Reduces annotation costs by 70–90% while improving whole-page ranking metrics through single-modal supervision transfer.

## Executive Summary
This paper addresses the costly annotation bottleneck in whole-page reranking by proposing SMAR, a framework that leverages strong single-modal rankers to guide multimodal relevance alignment. SMAR uses two budget-aware strategies—Top-P sampling and binary search for iso-label anchors—to select a small fraction of samples for annotation, while the rest are supervised by upstream single-modal scores. Experiments on Qilin and DuRank datasets show SMAR reduces annotation costs by 70–90% and improves NDCG, MRR, and MAP metrics. Online A/B testing confirms gains in CTR, user engagement, and expert judgments, validating SMAR's effectiveness for efficient and scalable SERP optimization.

## Method Summary
SMAR addresses whole-page reranking efficiency by using single-modal rankers to supervise multimodal reranker training with minimal human annotation. The framework selects data for annotation via Top-P (top fraction per modality) or iso-label anchors (binary search for cross-modal score matches), then trains a cross-attention reranker using combined human and upstream supervision. The reranker uses hybrid fusion of visual and text features with user embeddings to produce final scores, optimizing length-normalized ListMLE plus distillation from upstream rankers.

## Key Results
- Reduces annotation costs by 70–90% compared to full supervision
- SMAR with 10-30% annotation budget matches or exceeds full annotation performance on Qilin and DuRank
- Online A/B testing shows significant improvements in CTR, user engagement, and expert judgments
- Iso-label anchors enable effective cross-modal score alignment with limited labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Upstream single-modal ranker scores can substitute for human annotations when training cross-modal rerankers.
- Mechanism: The framework converts modality-specific scores into pairwise or listwise supervision signals. For any two items i, j from the same source, a pairwise preference y_ij = I[g_i^m > g_j^m] is derived, and the reranker is trained to preserve this ordering via margin-based loss. Listwise supervision uses KL divergence between normalized upstream score distributions and reranker output distributions.
- Core assumption: Single-modal rankers capture accurate intra-modality relevance that transfers to cross-modal contexts.
- Evidence anchors:
  - [abstract] "SMAR...leveraging strong Single-modal rankers to guide Modal-wise relevance Alignment"
  - [section 2.3] Explicit pairwise and listwise loss formulations using upstream scores g_m
  - [corpus] Related work on LLM-based supervision (arxiv 2510.01229) supports synthetic supervision viability, but no direct corpus evidence for single-modal-to-multimodal transfer specifically
- Break condition: If upstream rankers are poorly calibrated or biased, transferred supervision degrades rather than improves reranking.

### Mechanism 2
- Claim: Annotating only the top-ranked fraction per modality yields better cross-modal ranking than random annotation.
- Mechanism: Top-P strategy selects the top p·n_m items from each modality's ranked list for human annotation. The intuition is that users focus attention on top results; errors there are costlier. By concentrating annotation budget on high-exposure items, the model learns better cross-modal calibration where it matters most.
- Core assumption: Upstream rankers are sufficiently accurate that top-ranked items represent high-relevance samples worth annotating.
- Evidence anchors:
  - [abstract] "Top-P sampling...select a small fraction of samples for annotation"
  - [section 3.2.1, Table 1] SMAR 0%-10% (top 10%) outperforms random 30% and SMAR 30%-70% on MRR@10, MAP@10, NDCG
  - [corpus] No direct corpus evidence on top-P annotation efficiency in multimodal settings
- Break condition: If upstream rankers have systematic biases at the top (e.g., popularity bias), annotation concentrates on wrong samples.

### Mechanism 3
- Claim: Iso-label anchors enable cross-modal score alignment with limited labeled data.
- Mechanism: Binary search identifies items across modalities with matching upstream scores (or labels). These "tie" pairs serve as bridges for aligning score distributions. When no exact match exists, "virtual ties" insert items into the other modality's sequence. This creates cross-modal pairs for pairwise supervision.
- Core assumption: Comparable upstream scores across modalities indicate comparable relevance, enabling cross-modal calibration.
- Evidence anchors:
  - [abstract] "binary search for iso-label anchors—to select a small fraction of samples for annotation"
  - [section 2.4.2, Figure 5] Algorithm for finding ties and virtual ties across modality queues
  - [corpus] No corpus evidence for iso-label anchor strategies in related literature
- Break condition: If upstream score scales are incommensurable across modalities (Figure 2 shows different distributions), anchor pairs may not represent true equivalence.

## Foundational Learning

- Concept: **Learning to Rank objectives (pointwise, pairwise, listwise)**
  - Why needed here: SMAR uses all three—upstream supervision via pairwise/listwise, human labels via pointwise/listwise. Understanding when each applies is essential for implementing the loss composition.
  - Quick check question: Given a ranked list of 5 items with relevance labels [3, 2, 2, 1, 0], which loss type would penalize swapping positions 2 and 3?

- Concept: **Cross-attention mechanism**
  - Why needed here: The whole-page reranker uses cross-attention between user embeddings (keys/values) and item embeddings (queries) to capture personalized relevance, unlike self-attention rerankers that only model item-item interactions.
  - Quick check question: In cross-attention, if user features are unavailable at inference time, what architectural fallback is needed?

- Concept: **Calibration across heterogeneous score distributions**
  - Why needed here: Figure 2 shows video and text upstream scores follow different distributions. Direct comparison fails; iso-label anchors and listwise normalization are the proposed solutions.
  - Quick check question: If video scores range [0.7, 0.95] and text scores range [0.1, 0.6], would a simple min-max normalization enable fair cross-modal comparison? Why or why not?

## Architecture Onboarding

- Component map:
  - Upstream single-modal rankers (BERT-base-Chinese for text, Qwen-VL-2B for visual) → produce scores g_m^i for each candidate
  - Annotation strategies (Top-P selects top fraction per modality, Iso-label anchors finds cross-modal ties via binary search)
  - Whole-page reranker (PLM encoder + item feature extractor → cross-attention with user embeddings → MLP head for final scores)
  - Training loss (L_final = L_MLE(human labels) + α·L_MLE(video upstream) + β·L_MLE(text upstream))

- Critical path:
  1. Train upstream rankers on modality-specific data (prerequisite: have separate text/visual corpora)
  2. Apply Top-P or iso-label strategy to select subset for human annotation
  3. Construct candidate pages by merging upstream ranked lists
  4. Train reranker with combined loss: human supervision on labeled subset + upstream supervision on unlabeled remainder
  5. Deploy with cross-attention inference (requires user features at runtime)

- Design tradeoffs:
  - **Top-P vs. iso-label anchors**: Top-P is simpler but assumes top-ranked items are annotation-worthy; iso-label enables cross-modal calibration but requires discrete/bucketed scores and more complex implementation
  - **Pairwise vs. listwise upstream supervision**: Pairwise is robust to score scale differences; listwise preserves full ranking distribution but assumes calibrated scores
  - **Annotation budget allocation**: Paper shows 10-30% of data can match or exceed full annotation performance, but this depends on upstream ranker quality

- Failure signatures:
  - **Degraded performance vs. random baseline**: Likely upstream rankers are poorly trained or score scales are incommensurable
  - **High performance on top-K but poor tail ranking**: May indicate Top-P is too aggressive; try increasing annotation budget or using iso-label anchors
  - **Cross-modal pairs consistently mis-ranked**: Iso-label anchors may be finding false equivalences; verify upstream score calibration per modality

- First 3 experiments:
  1. **Ablate upstream supervision**: Train reranker with human labels only vs. human + upstream supervision on same annotation budget (10% Top-P). Expect: upstream supervision should improve NDCG by 2-5% as shown in Table 1.
  2. **Vary annotation budget**: Test Top-P at 5%, 10%, 20%, 30% on validation set. Expect: diminishing returns after 20-30%; top 10% may already match full annotation.
  3. **Validate iso-label anchor quality**: Manually inspect 50 anchor pairs to verify cross-modal relevance equivalence. If <70% agree with human judgment, score calibration is problematic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adaptive distillation strategies be developed to respond to dynamic user intent and data distribution shifts?
- Basis in paper: [explicit] The conclusion states future work includes "adaptive distillation that responds to intent and distribution shifts."
- Why unresolved: The current SMAR framework utilizes static budget-aware strategies (Top-P and Binary Search) which assume stable data characteristics.
- What evidence would resolve it: Experiments demonstrating a dynamic annotation selection mechanism that outperforms static strategies under simulated concept drift or temporal distribution shifts.

### Open Question 2
- Question: How can the binary search for iso-label anchors strategy be effectively generalized to datasets with continuous score distributions?
- Basis in paper: [inferred] Footnote 2 states the iso-label anchor strategy is "suitable for discrete bucketed features" and was not applied to the Qilin dataset because outputs were not bucketed.
- Why unresolved: The binary search relies on finding exact label matches ("ties"), which becomes computationally difficult or statistically improbable with continuous scoring systems without artificial bucketing.
- What evidence would resolve it: A modified matching algorithm (e.g., threshold-based or probabilistic anchoring) that successfully applies iso-label alignment to continuous score datasets like Qilin while maintaining cost efficiency.

### Open Question 3
- Question: To what extent does the performance of SMAR depend on the architecture of the backbone model compared to the supervision strategy?
- Basis in paper: [inferred] Table 2 shows that SMAR with 30% data surpasses full supervision for BERT, but fails to do so for Qwen3-Reranker, suggesting the framework's efficacy varies significantly across encoder-only and decoder-only architectures.
- Why unresolved: The paper does not analyze why the decoder-only model (Qwen3) appears to require more labeled data to match full supervision performance compared to the encoder-based model.
- What evidence would resolve it: Ablation studies comparing SMAR across a wider range of model architectures (e.g., encoder-decoder vs. decoder-only) to isolate the interaction between model capacity, architecture, and single-modal supervision.

## Limitations
- **Proprietary data dependency**: Core results rely on DuRank dataset, which is internal and unavailable for reproduction
- **Score distribution assumptions**: Iso-label anchor strategy assumes commensurable score scales across modalities, but Figure 2 shows fundamentally different distributions
- **Limited modality validation**: Framework only validated on text/video pairs from similar domains, limiting generalization claims

## Confidence
- **High confidence**: The fundamental insight that upstream single-modal rankers can provide supervision for multimodal reranking is well-supported by the loss formulations and empirical results.
- **Medium confidence**: The Top-P annotation strategy's superiority over random sampling is demonstrated, but the results depend heavily on upstream ranker quality assumptions that aren't fully validated.
- **Low confidence**: The iso-label anchor mechanism's effectiveness across diverse modality pairs remains theoretical, as the paper only validates on text/video pairs from similar domains.

## Next Checks
1. **Score Distribution Validation**: Plot histograms of upstream scores for each modality pair (text/video, text/image, etc.) to verify whether iso-label anchors can find meaningful cross-modal equivalences. If distributions don't overlap, the strategy fails.
2. **Ablation of Upstream Quality**: Train rerankers using deliberately degraded upstream rankers (e.g., random scores, biased training data) to determine the minimum quality threshold where supervision transfer breaks down.
3. **Cross-Modal Generalization**: Test SMAR's annotation efficiency on a third modality pair (e.g., image/text) using only Qilin's available data to assess whether the framework generalizes beyond the studied text/video combination.