---
ver: rpa2
title: Explainable AI in Spatial Analysis
arxiv_id: '2505.00591'
source_url: https://arxiv.org/abs/2505.00591
tags:
- spatial
- data
- feature
- values
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter presents a comprehensive framework for integrating
  eXplainable AI (XAI) into spatial analysis, addressing the black-box nature of machine
  learning models in geospatial applications. The core method, GeoShapley, extends
  Shapley value-based approaches to capture both intrinsic location effects and spatially
  varying interactions between features and geographic context.
---

# Explainable AI in Spatial Analysis

## Quick Facts
- arXiv ID: 2505.00591
- Source URL: https://arxiv.org/abs/2505.00591
- Reference count: 0
- Primary result: Framework integrates XAI into spatial analysis, separating location-invariant effects from spatially varying interactions

## Executive Summary
This paper introduces GeoShapley, a framework that extends Shapley value-based approaches to make machine learning models interpretable for spatial analysis. Applied to county-level voting prediction in the 2020 US Presidential election, GeoShapley successfully separates intrinsic location effects from feature-specific contributions, providing interpretable maps analogous to geographically weighted regression coefficients. The framework demonstrates how modern AI techniques can be made interpretable for spatial scientists while maintaining predictive performance.

## Method Summary
The approach trains an AutoML-optimized XGBoost model on county-level socio-demographic data with coordinates as location features. GeoShapley then decomposes predictions into intrinsic location effects, primary feature effects, and spatially varying interaction effects. These interaction values are smoothed using GWR to produce spatially varying coefficients for visualization. The method is validated through 5-fold cross-validation and bootstrap resampling for uncertainty estimation.

## Key Results
- XGBoost achieved out-of-sample R² of 0.933, matching MGWR performance
- GeoShapley successfully separated location-invariant primary effects from location-varying interaction effects
- Percentage of Black residents and educational attainment showed strongest contributions to voting patterns
- Spatially varying coefficients revealed non-linear relationships between socio-demographic variables and voting behavior

## Why This Works (Mechanism)

### Mechanism 1: Additive Spatial Decomposition via GeoShapley
If coordinates are treated as a joint feature within a Shapley framework, the model's prediction can be decomposed into intrinsic location effects and feature-specific contributions. GeoShapley extends Shapley interaction values to treat location as a unified player, allowing separation of predictions into base value, intrinsic location effect, primary feature effects, and spatially varying interaction effects.

### Mechanism 2: Mapping Interactions to Spatially Varying Coefficients
Spatially varying interaction effects can be combined with primary effects and smoothed spatially to approximate local coefficients from GWR. The framework estimates the partial derivative between combined GeoShapley effects and feature values using GWR smoothing, transforming abstract interaction contributions into interpretable spatially varying coefficients.

### Mechanism 3: Model-Agnostic Kernel Estimation
A kernel-based estimation method allows spatial explanations to be extracted from any machine learning model architecture without relying on model-specific internal parameters. This ensures explanation logic remains consistent even if the underlying black box model changes.

## Foundational Learning

- **Shapley Values (Game Theory)**: Mathematical foundation representing average marginal contribution of features across all possible coalitions. Quick check: If a feature has Shapley value of 0 for a specific prediction, does that mean no value globally or just no marginal impact on this prediction?

- **Spatial Heterogeneity vs. Spatial Autocorrelation**: Understanding that φ_GEO captures "intrinsic location effects" (spatial heterogeneity/context) distinct from simple spatial clustering. Quick check: Which GeoShapley term specifically captures that "location matters" independent of demographic variables?

- **Geographically Weighted Regression (GWR)**: Used as post-processing tool to convert GeoShapley values into coefficients. Quick check: Why use GWR as a "spatial smoother" rather than plotting raw Shapley values directly?

## Architecture Onboarding

- **Component map**: Input (tabular data + coordinates) -> Engine (AutoML trainer) -> Explainer (geoshapley package) -> Post-Processor (GWR wrapper) -> Output (maps of effects and coefficients)

- **Critical path**: Success relies heavily on model training. If model is underfit or ignores coordinates, explanations will correctly report zero spatial effects, creating false negatives about spatial processes.

- **Design tradeoffs**: Accuracy vs Interpretability (XGBoost with complex post-hoc explanation), True Shapley vs Readable Coefficients (GWR smoothing deviates from game-theoretic properties).

- **Failure signatures**: Sensitivity to correlation causing explanation variation between model types, nonsense explanations from poor model fit.

- **First 3 experiments**: 1) Baseline validation: Verify φ_GEO map matches Figure 3 patterns, 2) Ablation study: Remove coordinates and confirm spatial effects collapse to zero, 3) Algorithm comparison: Apply GeoShapley to Neural Network and compare coefficients against XGBoost results.

## Open Questions the Paper Calls Out

- How can more formal methods for visualizing spatial contributions in machine learning models be developed while maintaining adherence to game-theory properties?

- How can causal ML methods be effectively extended and applied to geospatial tasks to better understand spatial processes?

- How can we develop robust and transferable GeoAI models for tabular data tasks that account for unique characteristics of spatial tabular data?

- What methods beyond bootstrapping can be developed to better quantify uncertainties in XAI explanations for spatial models?

## Limitations

- Interpretability depends heavily on underlying ML model's ability to learn meaningful spatial patterns
- Explanations can be unstable when features are highly correlated, with different model types producing inconsistent attributions
- Computational cost scales poorly with dataset size due to kernel-based estimation

## Confidence

- High confidence: Mathematical framework of GeoShapley and decomposition of predictions into location-based components
- Medium confidence: Claim that GeoShapley captures spatial heterogeneity beyond simple spatial autocorrelation
- Medium confidence: Mapping of GeoShapley values to interpretable coefficients via GWR

## Next Checks

1. **Ablation Study**: Remove coordinate features from XGBoost model and re-run GeoShapley to confirm φ_GEO and φ_(GEO,j) collapse to zero

2. **Cross-Model Stability**: Apply GeoShapley to multiple ML architectures (Neural Network, Random Forest) on same dataset to assess consistency of spatial explanations

3. **Correlation Sensitivity**: Create correlated feature subsets and compare GeoShapley attributions across model types to quantify explanation instability