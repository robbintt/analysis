---
ver: rpa2
title: 'CHEER-Ekman: Fine-grained Embodied Emotion Classification'
arxiv_id: '2506.01047'
source_url: https://arxiv.org/abs/2506.01047
tags:
- emotion
- embodied
- body
- dataset
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of identifying fine-grained\
  \ embodied emotions in text by extending the CHEER dataset with Ekman\u2019s six\
  \ basic emotion categories, creating CHEER-Ekman. The authors use automatic best-worst\
  \ scaling with large language models (LLMs) to classify emotions, demonstrating\
  \ that this approach outperforms supervised methods."
---

# CHEER-Ekman: Fine-grained Embodied Emotion Classification

## Quick Facts
- **arXiv ID**: 2506.01047
- **Source URL**: https://arxiv.org/abs/2506.01047
- **Authors**: Phan Anh Duong; Cat Luong; Divyesh Bommana; Tianyu Jiang
- **Reference count**: 23
- **Primary result**: BWS with LLM outperforms supervised methods for embodied emotion classification, achieving F1 50.6 vs BERT's 49.6

## Executive Summary
This paper addresses the challenge of identifying fine-grained embodied emotions in text by extending the CHEER dataset with Ekman's six basic emotion categories, creating CHEER-Ekman. The authors use automatic best-worst scaling with large language models (LLMs) to classify emotions, demonstrating that this approach outperforms supervised methods. Key findings include: simplified prompts and chain-of-thought reasoning significantly improve emotion recognition accuracy; smaller models with CoT can match larger models; and BWS achieves superior performance over zero-shot and fine-tuned BERT, with the best F1-score of 50.6 surpassing supervised BERT at 49.6.

## Method Summary
The authors created CHEER-Ekman by automatically annotating the CHEER corpus with Ekman's six basic emotions using LLM-based best-worst scaling. The method involves generating emotion tuples, having LLMs identify the best and worst examples within each tuple, and then aggregating these judgments to assign emotion labels. They compared this approach against zero-shot prompting and fine-tuned BERT baselines, systematically testing different prompt formulations and model configurations to optimize performance.

## Key Results
- Best-Worst Scaling with LLM achieves F1-score of 50.6, outperforming fine-tuned BERT (49.6)
- Simplified prompts with chain-of-thought reasoning significantly improve accuracy
- Smaller models with CoT can match performance of larger models
- BWS method shows consistent superiority across all evaluation metrics compared to zero-shot and supervised approaches

## Why This Works (Mechanism)
The best-worst scaling approach works by leveraging relative judgments rather than absolute classifications. By having LLMs compare multiple instances and identify extremes within emotion categories, the method reduces ambiguity in emotion labeling and captures more nuanced distinctions between embodied emotional expressions.

## Foundational Learning
- **Best-Worst Scaling (BWS)**: A preference elicitation method that asks annotators to select both the best and worst options from a set - needed to reduce noise in emotion classification; quick check: verify tuple size affects accuracy
- **Embodied Emotion Detection**: Identifying emotions expressed through physical sensations in text - needed for this specialized classification task; quick check: ensure dataset contains sufficient embodied expressions
- **Chain-of-Thought Prompting**: Encouraging models to reason step-by-step before answering - needed to improve reasoning accuracy; quick check: compare CoT vs direct responses
- **Emotion Taxonomy Integration**: Mapping fine-grained emotions to basic categories - needed to balance granularity with data availability; quick check: verify coverage of target emotions
- **Automatic Annotation with LLMs**: Using models to label training data - needed for scalable dataset creation; quick check: assess annotation consistency

## Architecture Onboarding

**Component Map**: Text Input -> Preprocessing -> Tuple Generation -> LLM Judgment -> Aggregation -> Emotion Classification

**Critical Path**: Tuple generation and LLM judgment constitute the computational bottleneck, with accuracy scaling with tuple quantity but at increasing computational cost.

**Design Tradeoffs**: The system trades computational efficiency for accuracy by using multiple LLM inferences per classification, prioritizing precision over speed.

**Failure Signatures**: Performance degrades with metaphorical/idiomatic language, sparse emotion categories, and when tuple quantities are insufficient for reliable aggregation.

**First Experiments**:
1. Test different tuple sizes (3N, 6N, 12N, 24N, 36N, 48N, 72N) to identify optimal accuracy-cost tradeoff
2. Compare CoT vs direct prompting across different model sizes
3. Evaluate BWS performance on subsets of the dataset to assess scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating more granular emotion taxonomies (e.g., Cowen and Keltner's 27 categories) improve the coverage and accuracy of embodied emotion classification compared to Ekman's six basic emotions?
- Basis in paper: The authors state, "Future research may explore integrating alternative taxonomies into embodied emotion classification to enhance both granularity and coverage."
- Why unresolved: The current CHEER-Ekman dataset is limited to Ekman's six emotions, and the authors note that finer taxonomies often face sparsity issues in existing datasets.
- What evidence would resolve it: Annotating the CHEER corpus with a more granular taxonomy and benchmarking LLM performance to evaluate if finer distinctions can be reliably learned without data sparsity issues.

### Open Question 2
- Question: Does the performance improvement from simplified prompting strategies generalize to figurative or idiomatic language, or does it constitute overfitting to explicit phrasing?
- Basis in paper: The Limitations section notes, "Simplified prompts may inadvertently prioritize more explicit expressions of embodied emotion over subtler or more figurative language... concerns about potential overfitting to these simplified phrasings."
- Why unresolved: Error analysis revealed that 41% of zero-shot errors involved metaphorical or idiomatic expressions, suggesting a trade-off between prompt simplicity and nuance detection.
- What evidence would resolve it: Evaluating models prompted with simplified instructions on a specialized test set of metaphorical embodied expressions to verify if recall drops compared to technical prompts.

### Open Question 3
- Question: Can the computational overhead of the Best-Worst Scaling (BWS) method be reduced while maintaining its performance advantage over supervised baselines?
- Basis in paper: The authors identify scalability as a limitation, stating, "the scalability and computational overhead of this methodology present challenges: while higher tuple quantities lead to higher accuracy, they also impose significant computational costs."
- Why unresolved: While the authors tested up to 72N tuples, they relied on a smaller model due to resource constraints, and the cost-function curve of tuple count vs. accuracy gain remains inefficient.
- What evidence would resolve it: Implementing active learning strategies or early-stopping criteria for tuple generation to determine if classification accuracy can be preserved with significantly fewer LLM inferences.

## Limitations
- Reliance on automatic LLM annotation introduces potential labeling biases
- Moderate F1-score (50.6) indicates the task remains challenging despite advanced methods
- Limited generalizability across languages and cultural contexts
- Computational efficiency concerns for large-scale deployment

## Confidence
- **High Confidence**: Superiority of best-worst scaling over zero-shot and supervised BERT baselines
- **Medium Confidence**: Effectiveness of simplified prompts and chain-of-thought reasoning
- **Medium Confidence**: Claim that smaller models with CoT can match larger models

## Next Checks
1. Conduct human expert validation on a random sample of 500-1000 CHEER-Ekman annotations to assess inter-annotator agreement and LLM labeling accuracy
2. Test the best-worst scaling approach on emotion datasets in different languages to evaluate cross-linguistic generalizability
3. Perform ablation studies comparing computational costs (time, energy) of LLM-based BWS versus fine-tuned traditional models across different hardware configurations