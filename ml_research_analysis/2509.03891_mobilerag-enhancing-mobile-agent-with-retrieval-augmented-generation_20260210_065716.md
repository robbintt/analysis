---
ver: rpa2
title: 'MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation'
arxiv_id: '2509.03891'
source_url: https://arxiv.org/abs/2509.03891
tags:
- mobile
- agent
- mobilerag
- agents
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MobileRAG enhances mobile agents with retrieval-augmented generation,
  addressing limitations in reasoning, external interaction, and memory. It integrates
  InterRAG, LocalRAG, and MemRAG to improve query understanding, app retrieval, and
  task automation.
---

# MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2509.03891
- Source URL: https://arxiv.org/abs/2509.03891
- Reference count: 40
- MobileRAG achieves 10.3% improvement over state-of-the-art mobile agents with 100% app selection accuracy

## Executive Summary
MobileRAG addresses critical limitations in mobile agents' reasoning, external interaction, and memory through a retrieval-augmented generation framework. The system integrates three specialized RAG modules—InterRAG for web knowledge, LocalRAG for app retrieval, and MemRAG for memory storage—to enhance task automation and query understanding. Experiments on the MobileRAG-Eval benchmark demonstrate superior performance in handling complex, long-sequence tasks compared to existing approaches.

## Method Summary
MobileRAG is a mobile agent framework that combines retrieval-augmented generation with specialized knowledge modules. The system uses InterRAG to access real-time web information, LocalRAG to retrieve relevant apps from a structured knowledge base, and MemRAG to store and replay successful action sequences. A central multi-agent coordinator processes user queries by first checking memory, then supplementing with app knowledge and external information as needed, before generating executable action plans.

## Key Results
- 10.3% absolute improvement over state-of-the-art mobile agents on MobileRAG-Eval benchmark
- 100% accuracy in app selection tasks using LocalRAG
- Significant reduction in operational steps through effective memory reuse with MemRAG

## Why This Works (Mechanism)
MobileRAG's effectiveness stems from its hybrid architecture that combines local knowledge with external information retrieval. By first checking MemRAG for previously successful solutions, the system can quickly handle repeat tasks without LLM overhead. When facing new tasks, LocalRAG provides semantic search capabilities over app descriptions to ensure accurate app selection, while InterRAG supplements with real-time web knowledge for handling novel queries. This multi-stage retrieval approach ensures the LLM always has relevant context, reducing hallucinations and improving task completion rates.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** MobileRAG's core architecture relies on retrieving relevant context before generating responses, reducing hallucinations and improving domain-specific task performance
  - **Quick check question:** Can you explain the two-stage "retrieve-then-generate" pipeline and why it helps reduce LLM hallucinations for domain-specific knowledge?

- **Concept: Semantic Search & Embeddings**
  - **Why needed here:** LocalRAG and MemRAG effectiveness depends on finding similar items through vector embeddings rather than keyword matching
  - **Quick check question:** How does a bi-encoder embedding model differ from a cross-encoder reranker, and which one is used for initial retrieval in MobileRAG?

- **Concept: Mobile Agent Action Space**
  - **Why needed here:** MemRAG stores and replays successful action sequences, requiring understanding of atomic mobile agent actions
  - **Quick check question:** What are the typical atomic actions available to a GUI-based mobile agent, and what information is required to execute a TAP action?

## Architecture Onboarding

- **Component map:** User Query -> MemRAG Check -> (if new) InterRAG + LocalRAG -> Coordinator LLM -> Execution Engine -> (on success) MemRAG storage

- **Critical path:**
  1. User Query -> MemRAG Check (repeat task? replay steps & end)
  2. If new task: Query -> InterRAG (external knowledge) + LocalRAG (app selection)
  3. Coordinator LLM synthesizes retrieved info to plan atomic actions
  4. Execution Engine runs actions on device
  5. On Success -> MemRAG stores (query, actions) pair

- **Design tradeoffs:**
  - MemRAG Similarity Threshold: High (0.9) ensures accuracy but may miss reusable patterns; Low (0.7) increases reuse but risks incorrect sequences
  - InterRAG Latency: External API calls add significant latency vs local knowledge trade-off
  - Embedding Model Size: BGE-small balances semantic understanding with mobile deployment constraints

- **Failure signatures:**
  - App UI Drift: MemRAG replays sequences with moved/changed UI elements causing TAP failures
  - Noisy Retrieval: LocalRAG retrieves semantically similar but functionally incorrect apps
  - External Misinformation: InterRAG retrieves low-quality/outdated results leading to hallucination

- **First 3 experiments:**
  1. MemRAG Ablation: Execute 5 identical tasks twice, measure reduction in LLM calls and steps on second run
  2. LocalRAG App Selection Accuracy: Test 50 queries mapped to specific apps, measure top-3 retrieval accuracy vs baseline
  3. InterRAG Entity Resolution: Run 20 queries with novel entities, compare success rates with/without InterRAG enabled

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains demonstrated only on custom MobileRAG-Eval benchmark, limiting external validity
- Limited discussion of MemRAG database management and handling outdated action sequences from app updates
- Reliance on external Google Search API introduces latency and availability concerns for real-world deployment

## Confidence

**High Confidence:** Architectural design and three-RAG integration is technically sound and well-explained; 100% app selection accuracy is specific and verifiable

**Medium Confidence:** 10.3% performance improvement needs independent validation on external benchmarks; MemRAG efficiency gains require longer-term stability studies

**Low Confidence:** Paper doesn't adequately address failure modes with conflicting app suggestions or ambiguous queries; decision-making process for resolving InterRAG vs LocalRAG conflicts remains unclear

## Next Checks

1. **Cross-Dataset Validation:** Test MobileRAG on established mobile agent benchmarks like AndroidEnv or HumanEval to verify 10.3% improvement holds across different evaluation frameworks

2. **Longitudinal MemRAG Study:** Track system performance over 3-month period with frequent app updates to measure MemRAG handling of UI drift and sequence validity

3. **Failure Mode Analysis:** Systematically test ambiguous queries (e.g., "open browser" with multiple browsers) to evaluate conflict resolution between InterRAG and LocalRAG, documenting decision-making process