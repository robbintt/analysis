---
ver: rpa2
title: Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided
  Deep Learning
arxiv_id: '2509.12329'
source_url: https://arxiv.org/abs/2509.12329
tags:
- temperature
- surface
- data
- near-surface
- amplifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study proposes a data-driven, physics-guided deep learning
  approach called Amplifier Air-Transformer to generate hourly near-surface air temperature
  data at 2 km resolution over the contiguous United States. The method consists of
  two neural networks: the Amplifier model reconstructs cloud-obscured GOES-16 surface
  temperature data using a physics-guided convolutional neural network encoded with
  the annual temperature cycle and enhanced with ERA5 data, while the Air-Transformer
  model transforms the reconstructed surface temperature into air temperature using
  additional Earth surface and atmospheric properties.'
---

# Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning

## Quick Facts
- **arXiv ID**: 2509.12329
- **Source URL**: https://arxiv.org/abs/2509.12329
- **Reference count**: 40
- **Primary result**: Achieves hourly air temperature mapping accuracy of 1.93°C in station-based validation over CONUS (2018-2024)

## Executive Summary
This study introduces a physics-guided deep learning approach called Amplifier Air-Transformer to generate hourly near-surface air temperature data at 2 km resolution over the contiguous United States. The method employs two sequential neural networks: the Amplifier model reconstructs cloud-obscured GOES-16 surface temperature data using physics-derived annual temperature cycles and ERA5 reanalysis data, while the Air-Transformer model transforms the reconstructed surface temperature into air temperature using additional Earth surface and atmospheric properties. The approach achieves high accuracy (1.93°C RMSE) and provides uncertainty quantification through deep ensemble learning with 200 model snapshots.

## Method Summary
The approach consists of two sequential neural networks. First, the Amplifier model reconstructs GOES-16 surface temperature obscured by clouds using a physics-guided convolutional neural network encoded with the annual temperature cycle and enhanced with ERA5 data. Second, the Air-Transformer model transforms the reconstructed surface temperature into air temperature using 12 additional features including elevation, slope, and atmospheric variables. The models are trained separately by hour (Amplifier) and by month (Air-Transformer) on 77.7 billion surface temperature pixels and 155 million air temperature records from 2018-2024.

## Key Results
- Hourly air temperature mapping accuracy of 1.93°C in station-based validation
- Systematic performance bias of 0.5-1°C when temperatures exceed 35°C or fall below -10°C
- Uncertainty quantification via deep ensemble learning with 95% prediction interval coverage
- Performance improvement from 0.5-1°C to 0.3-0.5°C when including boundary layer height, total column water, and sensible heat flux

## Why This Works (Mechanism)

### Mechanism 1
Encoding physical temperature cycles into the neural network constrains predictions to physically plausible trajectories. The Amplifier model decomposes surface temperature into annual temperature cycle (ATC), linear amplification of ERA5 skin temperature, and residual spatiotemporal variations captured by convolutional layers. The ATC and ERA5 terms provide physics-derived bounds while the CNN learns only the residual.

### Mechanism 2
Separating surface temperature reconstruction from air temperature transformation into two sequential networks enables modular learning of distinct physical relationships. The first network learns to fill cloud-obscured GOES-16 surface temperature, while the second learns the latent mapping from reconstructed surface temperature to near-surface air temperature using 12 additional features capturing Earth surface properties.

### Mechanism 3
Deep ensemble learning with 200 model snapshots provides non-parametric uncertainty quantification that captures prediction variability without assuming a specific error distribution. During training, 200 snapshots are saved and predictions are aggregated with equal weights. The 2.5th and 97.5th percentiles define initial prediction intervals, which are then calibrated to ensure 95% coverage on training data.

## Foundational Learning

- **Annual/Sinusoidal Temperature Cycle Modeling**: The ATC component T_ATC(t) = T₀ + A·sin(2πt/N_DoY + φ) is a hard-coded physics prior. Quick check: Given a location with annual mean temperature 15°C and amplitude 20°C, what are the approximate maximum and minimum temperatures implied by the ATC?

- **Deep Ensemble vs. Monte Carlo Dropout**: The paper uses snapshot ensembles (saving checkpoints during training) rather than MC dropout. Quick check: What is the key difference between obtaining uncertainty from saved training snapshots versus generating predictions with dropout enabled at inference time?

- **Surface Temperature vs. Near-Surface Air Temperature Physics**: The paper's core challenge is transforming land surface temperature (LST, measured by satellite) to air temperature (T_air at 2m height). Quick check: On a clear summer afternoon, would you expect surface temperature to be higher, lower, or approximately equal to near-surface air temperature, and why?

## Architecture Onboarding

- **Component map**: GOES-16 LST (2km, hourly, cloud-gapped) → [AMPLIFIER MODEL] ← ERA5 skin temp (0.1°) + GOES-16 spectral reflectance → Reconstructed LST + uncertainty bounds → [AIR-TRANSFORMER MODEL] ← 12 features (lat/lon/hour, DEM/slope, 5 ERA5 vars) → Near-surface air temperature + propagated uncertainty

- **Critical path**: 1. Train Amplifier: 24 models/year (one per hour), 600 epochs, LR=0.1, save 200 snapshots from epoch 200 onward; 2. Compute reconstructed LST mean and 95% prediction intervals from ensemble; 3. Train Air-Transformer: 12 models/year (one per month), 500 epochs, LR=0.01, batch=65,536, 80/20 train/test split; 4. Propagate uncertainty bounds through Air-Transformer for final prediction intervals

- **Design tradeoffs**: Hourly/monthly model separation reduces data per model but captures diurnal/seasonal variations more precisely vs. a single unified model; deep ensemble (200 snapshots) provides uncertainty at low inference cost vs. Bayesian methods; excluding "aspect" feature after initial testing suggests feature selection is empirical.

- **Failure signatures**: RMSE >2.5°C during midday hours: check surface-air temperature relationship; systematic residuals at extreme temperatures (>35°C or <-10°C): model is conservative by design; prediction intervals not covering 95% of validation data: recalibrate λ factor.

- **First 3 experiments**: 1. Baseline replication on single region/month: Train both models on one CONUS region (e.g., East Coast) for January 2024; validate RMSE against reported ~1.9°C; 2. Ablation on climate reanalysis features: Remove boundary layer height, total column water, and sensible heat flux from Air-Transformer inputs; measure RMSE degradation; 3. Uncertainty calibration check: On 20% held-out stations, verify that 95% prediction intervals actually cover ~95% of observations.

## Open Questions the Paper Calls Out

- **Can integrating additional physical principles beyond the annual temperature cycle and ERA5 amplification improve model robustness under extreme temperature conditions and enable simulation-based estimation?**
- **Does fusing observations from multiple satellite platforms improve hourly air temperature mapping accuracy and spatial coverage compared to GOES-16 alone?**
- **Does uncertainty propagation through surface temperature alone adequately capture prediction uncertainty, given that atmospheric variables are provided at coarse resolution and without explicit uncertainty bounds?**

## Limitations
- Sequential architecture may propagate errors from surface to air temperature estimation
- ATC parameterization assumes sinusoidal temperature cycles that may not capture regional anomalies
- Performance degrades at extreme temperatures (>35°C or <-10°C) with 0.5-1°C systematic bias
- Uncertainty quantification relies on deep ensembles that may provide overconfident estimates

## Confidence

| Claim Cluster | Confidence |
|---------------|------------|
| Physics-guided ATC constraints | High |
| Sequential model architecture | Medium |
| Deep ensemble uncertainty | Medium |
| Overall performance claims | High |

## Next Checks
1. **Error propagation analysis**: Measure the correlation between LST reconstruction errors and subsequent air temperature prediction errors to quantify the impact of the sequential architecture.
2. **Out-of-sample uncertainty validation**: Test the 95% prediction intervals on a completely held-out year (e.g., 2024) to verify coverage statistics and interval reliability.
3. **Regional performance diagnostics**: Stratify validation RMSE by elevation, proximity to coastlines, and climate zones to identify geographic limitations not captured in aggregate metrics.