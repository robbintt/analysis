---
ver: rpa2
title: 'Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer
  Attention and Knowledge Distillation'
arxiv_id: '2511.14219'
source_url: https://arxiv.org/abs/2511.14219
tags:
- encoder
- attention
- noisy
- speech
- decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses hallucination in Whisper ASR models, particularly
  under noisy conditions, by proposing a two-stage architecture: Adaptive Layer Attention
  (ALA) to enhance encoder robustness through dynamic layer fusion, and Multi-Objective
  Knowledge Distillation (MOKD) to align student decoder behaviour with a clean teacher
  model. ALA groups encoder layers by similarity and applies learnable attention to
  fuse their representations, while MOKD uses cosine similarity and MSE losses on
  encoder, decoder, and cross-attention maps.'
---

# Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation

## Quick Facts
- **arXiv ID:** 2511.14219
- **Source URL:** https://arxiv.org/abs/2511.14219
- **Reference count:** 6
- **Primary result:** Proposed ALA + MOKD architecture reduces WER by up to 44% on noisy data while improving semantic accuracy across multiple languages

## Executive Summary
This paper addresses hallucination issues in Whisper ASR models, particularly under noisy conditions, through a two-stage architecture. The approach combines Adaptive Layer Attention (ALA) for enhanced encoder robustness via dynamic layer fusion with Multi-Objective Knowledge Distillation (MOKD) to align student decoder behavior with a clean teacher model. Experiments across Hindi, Arabic, French, and English demonstrate consistent improvements in Word Error Rate and SeMaScore over strong baselines, especially under low SNR conditions. The W-MOKD variant achieves notable gains while maintaining minimal inference overhead.

## Method Summary
The paper proposes a two-stage approach to mitigate hallucinations in Whisper ASR models. First, Adaptive Layer Attention (ALA) enhances encoder robustness by dynamically fusing representations from encoder layers grouped by similarity, using learnable attention mechanisms. Second, Multi-Objective Knowledge Distillation (MOKD) aligns the student decoder's behavior with a clean teacher model through cosine similarity and MSE losses on encoder, decoder, and cross-attention maps. The architecture operates in both encoder-only (E-MOKD) and full model (W-MOKD) configurations, with extensive evaluation showing consistent WER improvements across multiple languages and noise conditions.

## Key Results
- W-MOKD achieves up to 44% WER reduction on noisy data compared to baseline Whisper
- Consistent improvements across Hindi, Arabic, French, and English languages
- Enhanced semantic accuracy measured by SeMaScore while maintaining minimal inference overhead
- Performance gains are most pronounced under low SNR conditions (5dB, 10dB)

## Why This Works (Mechanism)
The approach works by addressing two key sources of hallucinations: noisy input representations and misaligned decoder predictions. ALA improves encoder robustness by dynamically weighting and fusing layer representations based on their similarity, allowing the model to focus on the most relevant features for each input segment. MOKD then ensures the student decoder learns to generate outputs that match the clean teacher's behavior, not just in terms of final predictions but also in the intermediate attention distributions. This dual approach tackles hallucinations at both the feature extraction and prediction stages.

## Foundational Learning

**Knowledge Distillation** - Transfer learning technique where a smaller student model learns from a larger teacher model's outputs
- *Why needed:* Enables leveraging clean, high-quality reference data to guide noisy model behavior
- *Quick check:* Compare student performance with and without distillation to verify knowledge transfer

**Attention Mechanisms** - Neural network components that weight input features based on their relevance
- *Why needed:* Allows dynamic focus on relevant encoder layers for different input segments
- *Quick check:* Visualize attention weights to ensure meaningful layer selection

**Semantic Accuracy Metrics** - Evaluation measures beyond WER that assess meaning preservation
- *Why needed:* WER alone doesn't capture semantic correctness of transcriptions
- *Quick check:* Compare WER and semantic scores to identify when improvements are substantive

**Layer Similarity Analysis** - Technique for grouping encoder layers based on representation similarity
- *Why needed:* Enables meaningful fusion of layers that capture similar features
- *Quick check:* Verify that grouped layers show high cosine similarity in their representations

## Architecture Onboarding

**Component Map:** Input Audio -> Encoder (with ALA) -> Decoder -> Output Text, with Teacher Model parallel for MOKD

**Critical Path:** Audio input flows through encoder layers → ALA fusion → decoder with cross-attention → final transcription output

**Design Tradeoffs:** ALA adds minimal parameters but requires layer similarity computation; MOKD needs clean reference data but provides semantic alignment

**Failure Signatures:** Increased WER under extreme noise conditions, attention maps showing uniform weights, semantic drift in low-resource languages

**3 First Experiments:**
1. Test ALA alone on clean data to verify it doesn't degrade baseline performance
2. Evaluate MOKD with different temperature parameters to optimize knowledge transfer
3. Measure inference latency overhead to confirm minimal computational impact

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on clean teacher model assumes availability of high-quality reference transcriptions
- Layer similarity-based grouping introduces heuristic that may limit discovery of optimal fusion strategies
- Evaluation focuses primarily on WER and SeMaScore, with limited analysis of computational overhead in production settings

## Confidence
- **High confidence** in empirical improvements across multiple languages and noise conditions
- **Medium confidence** in attribution of improvements specifically to ALA and MOKD components
- **Medium confidence** in generalizability given evaluation on four languages but limited domain diversity

## Next Checks
1. Conduct ablation studies with individual ALA and MOKD components disabled to quantify independent contributions
2. Test the approach on domain-shifted data (medical, technical, conversational) to assess robustness beyond evaluated languages
3. Implement and measure inference latency overhead in a real-time ASR pipeline to validate claimed minimal computational impact