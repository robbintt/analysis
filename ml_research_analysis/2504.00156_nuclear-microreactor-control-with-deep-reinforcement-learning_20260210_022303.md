---
ver: rpa2
title: Nuclear Microreactor Control with Deep Reinforcement Learning
arxiv_id: '2504.00156'
source_url: https://arxiv.org/abs/2504.00156
tags:
- control
- nuclear
- power
- reactor
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores deep reinforcement learning (RL) for drum-based
  control in nuclear microreactors, addressing the need for efficient load-following
  control in compact, autonomous systems. Using a point kinetics model with thermal
  and xenon feedback, the study compares single-agent RL, multi-agent RL (MARL), and
  PID controllers.
---

# Nuclear Microreactor Control with Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.00156
- Source URL: https://arxiv.org/abs/2504.00156
- Reference count: 40
- Primary result: Deep reinforcement learning controllers achieved similar or superior performance to PID controllers in nuclear microreactor drum-based control systems

## Executive Summary
This paper investigates the application of deep reinforcement learning (RL) for drum-based control in nuclear microreactors, addressing the critical need for efficient load-following control in compact, autonomous systems. The study compares single-agent RL, multi-agent RL (MARL), and traditional PID controllers using a point kinetics model with thermal and xenon feedback. RL controllers demonstrated performance on par with or exceeding PID controllers, with single-agent RL reducing tracking error in short transients and maintaining within 1% error in extended scenarios despite training only on short profiles. MARL enabled independent drum control while preserving symmetry, a capability PID and single-agent RL lacked.

The research highlights RL's potential for autonomous nuclear reactor control, with MARL offering unique advantages for multi-drum systems. Key findings include RL controllers showing greater robustness to Gaussian noise in power measurements compared to PID. The work establishes a foundation for future integration into high-fidelity simulations and experimental validation, though the authors acknowledge limitations in model complexity and the need for safety constraint implementation.

## Method Summary
The study employs a point kinetics model with thermal and xenon feedback to simulate nuclear microreactor dynamics. Three control approaches are compared: single-agent RL, multi-agent RL (MARL), and PID controllers. RL controllers are trained on short power profiles and tested on both short and extended transients. The MARL approach allows independent control of each drum while maintaining overall system symmetry. Performance metrics include tracking error, disturbance rejection, and noise robustness, with controllers evaluated under Gaussian noise conditions.

## Key Results
- RL controllers achieved similar or superior performance to PID in tracking power transients
- Single-agent RL reduced tracking error in short transients and maintained within 1% error in extended scenarios despite training only on short profiles
- MARL enabled independent drum control while preserving symmetry, a capability PID and single-agent RL lacked
- RL controllers demonstrated greater robustness to Gaussian noise in power measurements compared to PID

## Why This Works (Mechanism)
The paper leverages reinforcement learning's ability to learn complex control policies directly from interaction with the reactor model, without requiring explicit system identification. The point kinetics model with feedback captures essential reactor dynamics while remaining computationally tractable for RL training. MARL's architecture allows each drum controller to specialize while the overall system maintains operational symmetry through coordinated learning.

## Foundational Learning
- Point kinetics theory: Models reactor neutron population dynamics; needed for capturing fundamental reactor behavior in compact form
- Xenon poisoning dynamics: Accounts for xenon-135 buildup affecting reactivity; needed for realistic transient simulation
- Drum-based control systems: Mechanical control rods provide variable reactivity insertion; needed for understanding physical control mechanisms
- Multi-agent coordination: Multiple independent controllers working together; needed for MARL approach to maintain system symmetry
- Reinforcement learning fundamentals: Agent learns optimal policy through reward maximization; needed for training controllers without explicit models
- Thermal feedback mechanisms: Temperature changes affect reactivity; needed for capturing safety-relevant reactor behavior

## Architecture Onboarding

**Component Map:** Reactor model -> RL agents/MARL agents -> Drum actuators -> Power output -> Reward signal

**Critical Path:** Drum movement -> Reactivity change -> Power transient -> Feedback measurement -> Control action

**Design Tradeoffs:** Simplified point kinetics model enables faster training but may miss spatial effects; MARL adds coordination complexity but enables independent drum control

**Failure Signatures:** Oscillatory control responses, inability to track rapid transients, loss of drum symmetry, excessive noise sensitivity

**3 First Experiments:**
1. Test single-agent RL on a simple step-change power profile to verify basic learning capability
2. Compare MARL and single-agent RL on a symmetric load-following profile to observe symmetry preservation
3. Introduce measurement noise to both RL controllers to evaluate robustness differences

## Open Questions the Paper Calls Out
None

## Limitations
- Point kinetics model may not capture all relevant reactor dynamics, limiting generalizability to real-world scenarios
- RL controllers trained and tested only on specific power profiles, raising questions about performance under varied operational conditions
- Absence of safety constraints in the control objective could be problematic for actual reactor deployment

## Confidence

**High confidence:** Comparative performance results between RL and PID controllers within tested scenarios; advantages of MARL in maintaining drum symmetry

**Medium confidence:** Generalizability of results to different reactor configurations and operational conditions due to simplified modeling approach

**Low confidence:** Real-world deployment readiness given lack of experimental validation and unaddressed safety considerations

## Next Checks
1. Test RL controllers on a high-fidelity reactor simulator that includes spatial kinetics and more detailed thermal-hydraulic modeling
2. Evaluate controller performance under diverse operational scenarios including emergency conditions and multiple disturbance types
3. Implement safety constraints in the RL reward function and assess their impact on control performance and robustness