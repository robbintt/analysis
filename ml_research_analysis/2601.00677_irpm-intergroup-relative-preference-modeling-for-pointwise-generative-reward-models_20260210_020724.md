---
ver: rpa2
title: 'IRPM: Intergroup Relative Preference Modeling for Pointwise Generative Reward
  Models'
arxiv_id: '2601.00677'
source_url: https://arxiv.org/abs/2601.00677
tags:
- reward
- irpm
- preference
- response
- pointwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Intergroup Relative Preference Modeling (IRPM),
  a novel method for training pointwise Generative Reward Models (GRMs) from pairwise
  preference data. IRPM extends the Bradley-Terry paradigm by performing intergroup
  comparisons between groups of chosen and rejected responses, enabling pointwise
  rewards that are comparable across candidate sets.
---

# IRPM: Intergroup Relative Preference Modeling for Pointwise Generative Reward Models

## Quick Facts
- **arXiv ID:** 2601.00677
- **Source URL:** https://arxiv.org/abs/2601.00677
- **Reference count:** 40
- **Primary result:** IRPM achieves state-of-the-art pointwise GRM performance with 2.8-5.9% average improvements across RM-Bench, JudgeBench, RewardBench, and PPE while reducing computational complexity from O(n²) to O(n)

## Executive Summary
This paper introduces Intergroup Relative Preference Modeling (IRPM), a novel method for training pointwise Generative Reward Models (GRMs) from pairwise preference data. IRPM extends the Bradley-Terry paradigm by performing intergroup comparisons between groups of chosen and rejected responses, enabling pointwise rewards that are comparable across candidate sets. This approach reduces the computational complexity of reward evaluation from O(n²) to O(n) during reinforcement learning, addressing a key bottleneck in existing pairwise GRMs. Experiments demonstrate that IRPM achieves state-of-the-art performance among pointwise GRMs, with average improvements of 2.8-5.9% on benchmarks including RM-Bench, JudgeBench, RewardBench, and PPE. The method also matches or approaches the performance of leading pairwise GRMs while significantly reducing computational cost during training. Additional studies show that incorporating Chain-of-Thought reasoning and adaptive preference strength further improve performance. Post-training evaluations confirm that IRPM substantially reduces reward-model computation while maintaining competitive results.

## Method Summary
IRPM transforms pairwise preference data into pointwise rewards through intergroup comparisons. For each preference pair, it samples G rollouts for both chosen and rejected responses, then assigns rewards based on whether the chosen group's mean score exceeds the rejected group's mean by a margin δ. The key insight is that this intergroup comparison produces rewards on a comparable scale across different candidate sets, unlike traditional pairwise GRMs. The method supports three variants: IRPM-Mean (binary rewards bounded in [-1,+1]), IRPM-Preference (soft kernel rewards with score variance normalization), and IRPM-Interval (uncertainty-aware rewards using t-distribution confidence intervals). During training with GRPO, rewards are normalized within each group separately to maintain stability. The approach is implemented using the VERL framework with Qwen3 backbone models and includes format penalty for invalid outputs.

## Key Results
- IRPM achieves state-of-the-art performance among pointwise GRMs with average improvements of 2.8-5.9% across RM-Bench, JudgeBench, RewardBench, and PPE
- Computational complexity for reward evaluation reduced from O(n²) to O(n) during reinforcement learning
- IRPM matches or approaches the performance of leading pairwise GRMs while significantly reducing computational cost during training
- Incorporates Chain-of-Thought reasoning and adaptive preference strength to further improve performance
- Post-training evaluation shows substantial reduction in reward-model computation while maintaining competitive results

## Why This Works (Mechanism)
IRPM works by converting pairwise preference data into comparable pointwise rewards through intergroup statistical comparisons. Traditional pairwise GRMs evaluate all n candidate responses against each other (O(n²) complexity), but IRPM only needs to compare groups of responses (O(n) complexity). By sampling multiple rollouts per response and comparing group statistics rather than individual responses, IRPM produces rewards on a consistent scale across different candidate sets. The intergroup comparison framework preserves the relative preference information while enabling efficient pointwise evaluation. The method's variants (Mean, Preference, Interval) offer different trade-offs between reward stability and expressiveness, with IRPM-Mean providing bounded rewards that prevent training collapse.

## Foundational Learning

**Bradley-Terry Model**
- Why needed: Provides the theoretical foundation for pairwise preference modeling and relative comparisons
- Quick check: Can model pairwise comparison probabilities as logistic function of score differences

**Intergroup Statistical Comparison**
- Why needed: Enables conversion of pairwise preferences to comparable pointwise rewards
- Quick check: Group means can be compared using t-tests or simple threshold comparisons

**Reinforcement Learning from Human Feedback (RLHF)**
- Why needed: Framework for fine-tuning language models using reward signals
- Quick check: GRPO optimizes model policy to maximize expected reward while maintaining KL divergence

**Variance Normalization**
- Why needed: Prevents training instability when rewards have different scales across groups
- Quick check: Normalizing within groups separately maintains relative preferences while stabilizing optimization

## Architecture Onboarding

**Component Map**
Qwen3 Backbone -> IRPM Reward Module -> GRPO Optimizer -> Policy Updates -> Language Model

**Critical Path**
Prompt input → Response generation (G rollouts) → Group score computation → Intergroup comparison → Reward assignment → GRPO update

**Design Tradeoffs**
- Multiple rollouts (G=4) increase computational cost but improve reward stability and reduce tie rates
- IRPM-Mean provides bounded rewards (-1 to +1) preventing training collapse vs IRPM-Preference which suffers from unbounded score variance
- Within-group normalization maintains relative preferences while stabilizing training

**Failure Signatures**
- Training collapse with IRPM-Preference variant due to amplifying intragroup score variance
- High tie rate (8.2%) at evaluation without multi-sample inference scaling
- Poor performance if preference margin δ is set incorrectly

**First Experiments**
1. Implement basic IRPM-Mean reward computation and verify binary output in [-1,+1] range
2. Test group comparison with synthetic preference data to validate intergroup reward assignment
3. Run single training step with GRPO to confirm reward normalization prevents training divergence

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the discrimination capability of IRPM be improved to reduce tie rates without relying on multi-sample inference-time scaling?
- Basis in paper: [explicit] Appendix D.2 notes that resolving score ties "without additional inference cost is left for future work."
- Why unresolved: The current method reduces ties from 8.2% to 0.47% only by averaging multiple samples (voting), which increases compute.
- What evidence would resolve it: A deterministic architectural or training objective change that lowers the baseline tie rate.

**Open Question 2**
- Question: How can the training instability of the soft IRPM-Preference reward be stabilized?
- Basis in paper: [explicit] Section 4.3 observes that IRPM-Preference "results in training collapse" due to amplifying intragroup score variance.
- Why unresolved: The paper attributes this to a positive feedback loop in GRPO normalization but does not offer a remedy for the soft kernel.
- What evidence would resolve it: A stable training run using IRPM-Preference with a modified normalization or bounded loss function.

**Open Question 3**
- Question: Is the t-based confidence interval in IRPM-Interval robust for small group sizes given the likelihood of non-normal score distributions?
- Basis in paper: [inferred] Appendix E notes the normality assumption is an "approximation" and a "limitation" when G is small and scores are bounded.
- Why unresolved: The paper uses the interval as a heuristic without quantifying the error introduced by potential deviations from normality.
- What evidence would resolve it: A theoretical analysis or empirical comparison against non-parametric uncertainty estimators for intergroup thresholds.

## Limitations

- The method focuses exclusively on pairwise preference data conversion, leaving unclear how IRPM performs when trained directly on pointwise human ratings or combined preference-pointwise datasets
- IRPM's performance gains appear sensitive to the choice between IRPM-Mean and IRPM-Preference variants, with the latter suffering from score variance explosion during training
- The method requires multiple rollouts (G=4) per response during both training and evaluation, which may limit computational benefits in resource-constrained settings

## Confidence

**High confidence**: The computational complexity reduction claim (O(n²) → O(n)) is well-supported by the mathematical formulation and empirical timing measurements

**Medium confidence**: State-of-the-art performance claims relative to other pointwise GRMs are convincing, but direct comparison with pairwise GRMs shows mixed results (sometimes matching, sometimes slightly worse)

**Medium confidence**: The interpretability benefits through reduced tie rates (8.2% → 0.47% with voting@8) are demonstrated, but the practical impact on downstream alignment quality remains unclear

## Next Checks

1. Evaluate IRPM on datasets with explicit pointwise human ratings to assess whether the pairwise-to-pointwise conversion approach limits performance compared to direct pointwise training

2. Test IRPM with varying numbers of rollouts (G=1, 2, 8) to quantify the trade-off between computational efficiency and reward quality

3. Implement ablation studies removing Chain-of-Thought reasoning and adaptive preference strength to isolate their individual contributions to the reported performance gains