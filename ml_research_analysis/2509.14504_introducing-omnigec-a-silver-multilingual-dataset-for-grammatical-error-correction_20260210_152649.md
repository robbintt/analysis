---
ver: rpa2
title: 'Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction'
arxiv_id: '2509.14504'
source_url: https://arxiv.org/abs/2509.14504
tags:
- languages
- language
- data
- error
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniGEC, a multilingual silver-standard dataset
  for grammatical error correction (GEC) covering eleven languages. The dataset combines
  human Wikipedia edits with automatically corrected Reddit posts and Ukrainian social
  media texts using GPT-4o-mini.
---

# Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction

## Quick Facts
- arXiv ID: 2509.14504
- Source URL: https://arxiv.org/abs/2509.14504
- Reference count: 13
- This paper introduces OmniGEC, a multilingual silver-standard dataset for grammatical error correction (GEC) covering eleven languages. The dataset combines human Wikipedia edits with automatically corrected Reddit posts and Ukrainian social media texts using GPT-4o-mini. Automatic and manual evaluations show good quality, with 70%+ of corrections scoring "minimal" or "fluency" grades. Fine-tuning open-source models (Aya-Expanse 8B and Gemma-3 12B) on OmniGEC improves multilingual GEC performance and establishes new state-of-the-art results for paragraph-level GEC.

## Executive Summary
This paper introduces OmniGEC, a multilingual silver-standard dataset for grammatical error correction (GEC) covering eleven languages. The dataset combines human Wikipedia edits with automatically corrected Reddit posts and Ukrainian social media texts using GPT-4o-mini. Automatic and manual evaluations show good quality, with 70%+ of corrections scoring "minimal" or "fluency" grades. Fine-tuning open-source models (Aya-Expanse 8B and Gemma-3 12B) on OmniGEC improves multilingual GEC performance and establishes new state-of-the-art results for paragraph-level GEC.

## Method Summary
The method constructs OmniGEC through a three-stage pipeline: (1) Data collection from Wikipedia edits (filtered by newcomer task copyedit), Reddit posts (filtered by language ID and moderation), and Ukrainian social media (UberText 2.0); (2) GPT-4o-mini correction generation using language-specific few-shot prompts synthesized by o1-preview; (3) Aggregation of three diverse GPT-4o-mini outputs into single silver references. The dataset is then used to fine-tune Aya-Expanse-8B and Gemma-3-12B-IT using LoRA with specified hyperparameters (lr=3e-5, r=64, alpha=128) on combined gold and silver data.

## Key Results
- Fine-tuning on OmniGEC improves multilingual GEC performance across all 11 languages
- Low-resource languages (Estonian, Latvian) show massive GLEU improvements (+8.25 for Estonian) from synthetic data
- Gemma-3 12B benefits more than Aya-Expanse 8B from large-scale synthetic fine-tuning
- Human evaluation shows 70%+ of corrections score "minimal" or "fluency" grades

## Why This Works (Mechanism)

### Mechanism 1: Correction Aggregation to Mitigate LLM "Radiation"
GPT-4o-mini tends to produce partial corrections or "radiate" into multiple possibilities rather than a single complete edit. By prompting for three diverse corrections and then using a second prompt to aggregate them into one, the system creates a more "complete" silver standard. This completeness appears to outweigh the introduced risk of overcorrection.

### Mechanism 2: Scaling Laws for Low-Resource Adaptation
Low-resource languages (e.g., Estonian, Latvian) lack representation in the pre-training of models like Aya-Expanse. Fine-tuning on a large synthetic corpus provides the necessary signal for the model to generalize grammatical rules for these languages, resulting in massive GLEU score increases compared to high-resource languages that only see marginal gains.

### Mechanism 3: Parameter-Efficient Fine-Tuning (PEFT) Sensitivity
The smaller Aya-Expanse (8B) adapted quickly to the small gold dataset, but the larger Gemma-3 (12B) initially underperformed. Only when the training data was expanded with OmniGEC did the 12B model's higher capacity utilize the signal effectively, eventually surpassing the smaller model.

## Foundational Learning

- **Concept: Silver vs. Gold Standard Data**
  - Why needed here: The core contribution is a "silver" dataset (high quantity, auto-annotated) used to augment "gold" datasets (low quantity, human-annotated). Understanding this trade-off is essential for interpreting the performance gains.
  - Quick check question: Can you distinguish between the noise introduced by GPT-4o-mini annotations versus the errors in human Wikipedia edits?

- **Concept: ERRANT and GLEU Metrics**
  - Why needed here: The paper relies on GLEU (Generalized Language Understanding Evaluation) for paragraph-level scoring and ERRANT for error span classification. The inverse relationship observed (higher edit rate -> lower GLEU) is counter-intuitive but central to the evaluation.
  - Quick check question: Why might a system with higher precision but lower recall (and thus fewer edits) score better on GLEU in a multi-reference evaluation?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: The experiments use LoRA to fine-tune large models on limited GPU resources (single A100). Understanding LoRA is required to replicate the results or diagnose under-training.
  - Quick check question: If you observe the loss plateauing early on a 12B model with LoRA, would you increase the dataset size or the LoRA rank first?

## Architecture Onboarding

- **Component map:** Wikipedia API (filtered by newcomer task copyedit) -> Reddit API (filtered by langid and moderation) -> UberText 2.0 -> DeepL (prompt translation) -> o1-preview (few-shot prompt synthesis) -> GPT-4o-mini (3-step correction & aggregation) -> LoRA fine-tuning of Aya-Expanse-8B and Gemma-3-12B-IT

- **Critical path:**
  1. **Filtering:** Critical for Reddit/Wiki data to remove non-grammatical edits (e.g., "information updates" in Wiki) and toxic content.
  2. **Prompt Generation:** o1-preview generates language-specific few-shot prompts based on MultiGEC dev examples.
  3. **Correction:** GPT-4o-mini generates 3 candidates.
  4. **Aggregation:** GPT-4o-mini merges 3 candidates into final silver reference.
  5. **Fine-tuning:** LoRA fine-tuning on combined Gold + Silver data.

- **Design tradeoffs:**
  - **Human vs. Synthetic:** WikiEdits are human but noisy (avg grade 3.05); Reddit/Synthetic is cleaner (avg grade 3.6+) but prone to "overcorrection" and hallucination.
  - **Completeness vs. Fluency:** The aggregation strategy prioritizes "complete" corrections over "minimal" edits, potentially shifting the model bias toward fluency over strict grammatical correction.
  - **Cost vs. Coverage:** The pipeline uses expensive proprietary models (o1, GPT-4o-mini) to generate training data for open-source models.

- **Failure signatures:**
  - **Icelandic Token Limit:** Hard truncation at 1,600 tokens caused severe performance drops for Icelandic (avg essay length 1k-3k tokens). *Fix: Increase context window.*
  - **Wiki Noise:** 9.9% rejection rate in human eval due to domain-specific text or info-updates being treated as grammar edits. *Fix: Stricter diff filtering.*
  - **Overcorrection Bias:** Synthetic data may add unnecessary promotional text or "fluency" edits where "minimal" is preferred.

- **First 3 experiments:**
  1. **Baseline Validation:** Fine-tune base models solely on the MultiGEC gold set to establish the "small data" bias and verify model convergence.
  2. **Ablation by Source:** Train separate instances on WikiEdits-only vs. Reddit-Synthetic-only to isolate the noise vs. completeness trade-off for specific languages (e.g., Estonian vs. Ukrainian).
  3. **Context Window Test:** Increase `max_new_tokens` to >3000 specifically for Icelandic to verify if the poor performance is due to context truncation rather than model incapacity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the high quality observed in human evaluations of Ukrainian synthetic corrections generalize to the other ten languages in the OmniGEC dataset?
- Basis in paper: [inferred] The authors explicitly limit their human evaluation to Ukrainian due to time constraints, while noting the "need for a further multilingual assessment."
- Why unresolved: Only Ukrainian samples were graded by native speakers; the quality of GPT-4o-mini corrections for the remaining ten languages remains empirically unverified by humans.
- What evidence would resolve it: Human evaluation scores for a statistically significant sample of corrections across the ten non-Ukrainian languages.

### Open Question 2
- Question: To what extent does the larger UberText-GEC corpus improve model performance compared to the smaller Reddit-MultiGEC dataset?
- Basis in paper: [explicit] The authors identify "more thorough research on data quantity versus quality with UberText-GEC" as a key area for future ablation studies.
- Why unresolved: The UberText-GEC corpus was excluded from the training experiments due to time and cost limitations.
- What evidence would resolve it: Ablation studies comparing models fine-tuned on UberText-GEC versus Reddit-MultiGEC to measure the impact of data volume versus source quality.

### Open Question 3
- Question: Can Direct Preference Optimization (DPO) utilizing the collected human-in-the-loop scores improve performance beyond standard fine-tuning?
- Basis in paper: [explicit] The authors state they plan to research "preference optimization methods, like DPO," which are made possible by the dataset's human scores.
- Why unresolved: The current experiments utilize standard LoRA fine-tuning without implementing preference optimization techniques.
- What evidence would resolve it: Comparative results between baseline fine-tuned models and models trained with DPO using the human annotation grades.

## Limitations
- The quality of GPT-4o-mini corrections for non-Ukrainian languages remains unverified by human evaluation
- The aggregation strategy may introduce systematic overcorrection bias toward fluency edits
- Icelandic performance suffers from token limit truncation issues that may not reflect model capability

## Confidence
- **High Confidence:** The dataset construction pipeline (Wikipedia edits + Reddit + GPT-4o-mini corrections) is technically sound and reproducible. The performance improvements on MultiGEC-2025 are empirically demonstrated.
- **Medium Confidence:** The claim that large-scale synthetic data benefits low-resource languages more than high-resource languages is supported by the Estonian/Latvian examples, but the mechanism could involve either genuine learning or noise amplification.
- **Low Confidence:** The assertion that the aggregation strategy produces "more complete" corrections than single-output generation is based on qualitative assessment rather than quantitative comparison of correction completeness metrics.

## Next Checks
1. **Synthetic Error Analysis:** Conduct a systematic error analysis comparing GPT-4o-mini corrections to native speaker annotations for 100 samples from each low-resource language to quantify systematic error patterns in the synthetic data.
2. **Metric Correlation Study:** Analyze the relationship between edit rate, precision, recall, and GLEU across all 11 languages to determine whether higher GLEU correlates with better grammatical accuracy or simply fewer edits.
3. **Context Window Impact:** Repeat the Icelandic experiments with max_new_tokens increased to 3000 to definitively determine whether the poor performance is due to context truncation rather than model incapacity.