---
ver: rpa2
title: 'Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation
  with Multimodal Chain-of-Thought'
arxiv_id: '2509.18200'
source_url: https://arxiv.org/abs/2509.18200
tags:
- spatial
- reasoning
- user
- orientation
- landmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Conversational Orientation Reasoning (COR),
  a new benchmark for egocentric-to-allocentric orientation reasoning in Traditional
  Chinese conversational navigation. The proposed multimodal chain-of-thought (MCoT)
  framework integrates ASR-transcribed speech with landmark coordinates through a
  structured three-step reasoning process: extracting spatial relations, mapping coordinates
  to absolute directions, and inferring user orientation.'
---

# Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought

## Quick Facts
- **arXiv ID:** 2509.18200
- **Source URL:** https://arxiv.org/abs/2509.18200
- **Authors:** Yu Ti Huang
- **Reference count:** 35
- **One-line primary result:** MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts.

## Executive Summary
This paper introduces Conversational Orientation Reasoning (COR), a new benchmark for egocentric-to-allocentric orientation reasoning in Traditional Chinese conversational navigation. The proposed multimodal chain-of-thought (MCoT) framework integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: extracting spatial relations, mapping coordinates to absolute directions, and inferring user orientation. Using curriculum learning on Taiwan-LLM-13B-v2.0-Chat, MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. The model demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching, and maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity.

## Method Summary
The method uses Taiwan-LLM-13B-v2.0-Chat with LoRA fine-tuning, employing curriculum learning across four stages: relation extraction, coordinate-to-absolute direction mapping, orientation inference, and end-to-end multimodal reasoning. The model processes ASR transcripts and structured landmark coordinates through a three-step chain-of-thought framework, achieving high accuracy by explicitly structuring the spatial reasoning process. Training uses synthetic speech converted to ASR transcripts for evaluation, with a 10×10 grid coordinate system.

## Key Results
- MCoT achieves 100% orientation accuracy on clean transcripts versus 21.1% for baseline few-shot approaches
- With ASR transcripts, MCoT maintains 98.1% accuracy versus 16.2% for ASR-only baselines
- The model demonstrates cross-domain generalization with 97.8% accuracy on unseen navigation scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring reasoning into explicit intermediate steps reduces error propagation in spatial perspective-taking tasks.
- **Mechanism:** The Multimodal Chain-of-Thought (MCoT) framework forces the model to explicitly parse egocentric relations (e.g., "on my right") and calculate coordinate vectors before attempting the final allocentric inference. This acts as a "state checker," preventing the model from hallucinating orientations based on linguistic priors alone.
- **Core assumption:** The model possesses sufficient intrinsic capability to execute individual sub-tasks (vector math, relation extraction) if they are isolated, whereas it fails at the composite end-to-end mapping.
- **Evidence anchors:**
  - [abstract] "...integrates ASR-transcribed speech... through a structured three-step reasoning process: extracting spatial relations, mapping coordinates to absolute directions, and inferring user orientation."
  - [section 4.2 / Table 2] Comparison between B3 (Few-shot with CoT, 21.1% acc) and B5a (MCoT + curriculum, 100% acc) suggests that structure alone isn't enough without specific training, but the structure enables the learning.
  - [corpus] FantasyVLN (neighbor) supports that CoT reasoning is beneficial for Vision-Language Navigation tasks requiring long action sequences.
- **Break condition:** Fails if the intermediate steps (e.g., vector subtraction) exceed the model's intrinsic arithmetic or logical capabilities, or if the prompt structure is ignored.

### Mechanism 2
- **Claim:** Curriculum learning is likely required to stabilize the acquisition of multimodal grounding in resource-constrained models.
- **Mechanism:** By training on sub-tasks sequentially (S1: Relation Extraction $\rightarrow$ S2: Coordinate Mapping $\rightarrow$ S3: Orientation Reasoning), the model first learns to "see" the landmarks before learning to "map" them. This prevents the "unstable learning" observed in end-to-end approaches.
- **Core assumption:** The skill hierarchy is strict; one cannot reliably infer orientation if one cannot reliably extract the reference landmark from noisy text.
- **Evidence anchors:**
  - [section 3.3] "In preliminary experiments, directly training the model end-to-end led to unstable learning and poor generalization. We therefore adopt curriculum learning..."
  - [section 3.3] The training objective explicitly separates $f_\theta^{(0)}$ through $f_\theta^{(3)}$ into stage-wise fine-tuning.
  - [corpus] No direct corpus evidence provided for this specific curriculum mechanism in the neighbor list; relevant papers focus on inference rather than training dynamics.
- **Break condition:** If the distribution of the sub-tasks (e.g., synthetic clean text) diverges too significantly from the final evaluation distribution (noisy ASR text), the curriculum may overfit to clean patterns.

### Mechanism 3
- **Claim:** Structured spatial coordinates serve as a grounding signal to correct semantic noise from ASR transcripts.
- **Mechanism:** The model fuses noisy text ("Audio") with precise structured data ("Coordinates"). If the ASR misrecognizes a phonetically similar landmark name, the presence of the exact coordinate list in the prompt allows the model to disambiguate or "auto-complete" the intent using the spatial signal, provided the landmark reference isn't completely lost.
- **Core assumption:** The user's utterance contains enough residual information to link the egocentric description to the coordinate list, even if the name is garbled.
- **Evidence anchors:**
  - [section 4.3 / Table 3] A2 (ASR text only) achieves 16.2% accuracy, while A3 (ASR + Coordinates, no CoT) jumps to 26.4%, suggesting coordinates provide significant error correction.
  - [section 4.1] "The model also maintains high accuracy... and resilience to linguistic variation, domain shift, and referential ambiguity."
  - [corpus] WISE (neighbor) discusses weak-supervision for explanations, implying multimodal inputs aid in grounding, though not specific to coordinates.
- **Break condition:** Fails if the ASR error corrupts the *spatial relation* terms (e.g., transcribing "left" as "right") rather than just the landmark names, as coordinates do not correct semantic relation inversion.

## Foundational Learning

- **Concept: Egocentric vs. Allocentric Frames of Reference**
  - **Why needed here:** The core task is a coordinate transformation. Without understanding that "on my right" describes a relative vector dependent on an unknown heading, while "North" is a fixed world vector, the loss function and mapping rules (Table 1) will be unintelligible.
  - **Quick check question:** If a user faces West and says an object is "in front" of them, is the object to the West (absolute) or North (relative to the user's body if misinterpreted)? (Answer: West/Forward in their frame).

- **Concept: Curriculum Learning**
  - **Why needed here:** The paper relies on this to explain why their model converged when standard fine-tuning failed. Understanding this is necessary to replicate the training pipeline (Stages S1–S4).
  - **Quick check question:** Why would training on "Coordinate Mapping" (Stage S2) before "Orientation Reasoning" (Stage S3) prevent the model from learning unstable shortcuts?

- **Concept: Multimodal Chain-of-Thought (MCoT)**
  - **Why needed here:** The "Reasoning" in the paper title refers to this specific prompt structure. It differs from standard prompting by enforcing a verifiable logic trace rather than just an answer.
  - **Quick check question:** In the prompt template, does the model output just the final direction (N/S/E/W) or the vector calculation steps first?

## Architecture Onboarding

- **Component map:** Input Fusion (ASR Transcript + Grid Coordinates) -> Relation Extraction -> Coordinate Mapping -> Orientation Reasoning -> Structured Output
- **Critical path:** The flow moves from **Input Fusion** (Eq. 4) -> **Stage 3 Inference** (Eq. 3) is the critical capability). The **Curriculum Scheduler** is the critical novelty enabling the path.
- **Design tradeoffs:**
  - **Grid Discretization:** The paper uses a 10×10 Manhattan grid. *Tradeoff:* Simplifies vector calculation (no diagonals) but limits applicability to continuous real-world coordinates.
  - **Synthetic Speech:** Uses TTS -> ASR loop rather than human speech. *Tradeoff:* Ensures controlled noise levels but may underestimate prosodic or accent variations found in wild data.
  - **Model Size:** Uses a 13B model. *Tradeoff:* Fits resource-constrained settings (edge devices) but requires aggressive curriculum/LoRA to perform, unlike GPT-4 class models.
- **Failure signatures:**
  - **Direction Understanding Errors (9/13 residuals):** The model extracts relations and computes vectors correctly but applies the *wrong* mapping rule (e.g., assumes "left = West" implies facing North instead of South).
  - **ASR Misrecognition (3/13 residuals):** The relation term itself is corrupted (e.g., "front" heard as "left"), leading to a logically correct but factually wrong inference based on bad input.
  - **Format Errors:** High in baselines (B3: 39.2%), nonexistent in final model, suggesting the curriculum effectively constrains output syntax.
- **First 3 experiments:**
  1. **Verify Stage 1:** Isolate the Relation Extraction module. Feed clean text inputs and verify if the model correctly identifies $(q, \ell_r)$ before moving to Stage 2 training.
  2. **Noise Floor Test:** Run the final MCoT model on the ASR-only test set (Table 3, A2 vs A4b) to quantify exactly how much accuracy is recovered by the Coordinate modality.
  3. **Generalization Check:** Evaluate on the "Referential Ambiguity" subset (R3) to see if the model handles "that building" references by successfully cross-referencing the provided coordinate list.

## Open Questions the Paper Calls Out
- How does the MCoT framework perform in continuous, non-grid environments where diagonal orientations and non-axis-aligned landmarks exist?
- Does the framework maintain high accuracy when trained or evaluated on non-synthetic, real-time human speech containing natural disfluencies and environmental noise?
- Can visual observation replace structured coordinate inputs as the primary spatial modality for inferring orientation?

## Limitations
- The synthetic nature of evaluation data may not capture full complexity of real-world speech variations and environmental noise
- The 10×10 Manhattan grid discretization limits applicability to continuous real-world navigation scenarios
- The curriculum learning approach's effectiveness for models outside the 13B parameter range remains unclear

## Confidence

**High Confidence Claims:**
- The MCoT framework achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, outperforming all baselines
- Structured reasoning through explicit intermediate steps reduces error propagation compared to end-to-end approaches
- Curriculum learning is necessary to prevent unstable learning observed in direct fine-tuning approaches

**Medium Confidence Claims:**
- The model demonstrates robustness under noisy conversational conditions including ASR recognition errors and multilingual code-switching
- High accuracy is maintained in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity
- Coordinates serve as an effective grounding signal to correct semantic noise from ASR transcripts

**Low Confidence Claims:**
- The specific three-stage curriculum structure is optimal for this task
- The model's performance would scale proportionally to larger models or different architectures
- The error correction mechanism works equally well for all types of ASR errors

## Next Checks

1. **Real-World Speech Validation:** Evaluate the MCoT model on naturally recorded speech data from actual users in urban environments, rather than synthetic TTS-ASR loops. This would test whether the claimed robustness to code-switching and linguistic variation holds under genuine conversational conditions with diverse accents, background noise, and natural speech patterns.

2. **Continuous Coordinate Generalization:** Test the model's performance on continuous coordinate systems rather than the discretized 10×10 grid. This would validate whether the reasoning framework can handle real-world GPS coordinates and more complex spatial relationships beyond orthogonal movements.

3. **Cross-Architecture Transfer:** Implement the same curriculum learning approach on a significantly smaller model (e.g., 7B parameters) and a larger model (e.g., 30B parameters) to determine the scalability boundaries of the method. This would reveal whether the curriculum structure is truly model-agnostic or requires specific parameter ranges to function effectively.