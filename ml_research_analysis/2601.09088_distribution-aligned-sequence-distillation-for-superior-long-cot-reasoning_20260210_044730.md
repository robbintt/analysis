---
ver: rpa2
title: Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning
arxiv_id: '2601.09088'
source_url: https://arxiv.org/abs/2601.09088
tags:
- teacher
- student
- training
- distillation
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of improving long chain-of-thought
  (CoT) reasoning in smaller language models through sequence-level distillation from
  larger teacher models. The authors identify three key limitations in current SFT-based
  distillation: inadequate coverage of the teacher''s sequence-level distribution,
  misalignment between teacher and student output distributions, and exposure bias
  from teacher-forced training.'
---

# Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning

## Quick Facts
- arXiv ID: 2601.09088
- Source URL: https://arxiv.org/abs/2601.09088
- Authors: Shaotian Yan; Kaiyuan Liu; Chen Shen; Bing Wang; Sinan Fan; Jun Zhang; Yue Wu; Zheng Wang; Jieping Ye
- Reference count: 5
- Primary result: Achieves state-of-the-art 4B-scale reasoning performance with 88.5 AIME24 score using only 448K training samples

## Executive Summary
This paper addresses the challenge of improving long chain-of-thought reasoning in smaller language models through sequence-level distillation from larger teacher models. The authors identify three key limitations in current SFT-based distillation: inadequate coverage of the teacher's sequence-level distribution, misalignment between teacher and student output distributions, and exposure bias from teacher-forced training. To address these issues, they propose three methodological innovations: temperature-scheduled learning, divergence-aware sampling, and mixed-policy distillation. Using these techniques, they develop DASD-4B-Thinking, a 4B parameter model that achieves state-of-the-art performance among open-source models of comparable scale, scoring 88.5 on AIME24, 83.3 on AIME25, 69.3 on LiveCodeBench v5, and 68.4 on GPQA-Diamond.

## Method Summary
The approach employs a three-stage training pipeline: (1) low-temperature SFT on 105K samples at T=0.6 to establish stable learning foundations, (2) high-temperature SFT continuation on 330K samples at T=1.0 to capture broader teacher distribution modes, and (3) mixed-policy distillation using 12.7K samples combining student-generated prefixes with teacher-completed suffixes. Divergence-aware sampling identifies training examples where teacher confidence is high but student probability is low, prioritizing these for maximum gradient alignment. The method uses Qwen3-4B-Instruct-2507 as the student model and gpt-oss-120b as the teacher, with 64K context length and ZeRO-3 optimization.

## Key Results
- Achieves 88.5 AIME24, 83.3 AIME25, 69.3 LiveCodeBench v5, and 68.4 GPQA-Diamond scores
- Demonstrates state-of-the-art performance among 4B-scale open-source models
- Uses only 448K training samples, an order of magnitude fewer than comparable efforts
- Shows temperature-scheduled learning yields +2-4 absolute points over static-temperature baselines
- Divergence-aware sampling improves performance by +1-2 points over random sampling

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: Temperature-scheduled learning reconciles stable early-stage learning with broader mode coverage of the teacher's distribution.
**Mechanism**: Low-temperature sampling (T=0.6) produces concentrated, high-probability sequences that are easier for the student to learn, establishing a stable foundation. High-temperature sampling (T=1.0) then captures rarer modes and latent knowledge. Training sequentially allows the student to first internalize consistent patterns before expanding to diverse behaviors.
**Core assumption**: The student model's capacity bottleneck can be overcome through curriculum-style exposure, where easier patterns create scaffolding for harder ones.
**Evidence anchors**: [abstract]: "temperature-scheduled learning to broaden mode coverage"; [Section 3, Table 1]: "cold-starting with 50K samples at temperature 0.6 followed by continued training on another 50K samples at temperature 1.0 yields significant performance gains over all static-temperature baselines".

### Mechanism 2
**Claim**: Divergence-aware sampling (DAS) identifies training examples where the teacher has high confidence and the student has low probability, reducing misleading gradients.
**Mechanism**: By comparing sentence-level probabilities from teacher and student, the method categorizes responses into four types. "Teacher Sentences" (high teacher probability, low student probability) are prioritized because they provide correct gradient signals—SFT increases student probability toward the teacher's distribution rather than amplifying student overconfidence. This avoids the standard SFT failure mode where the student's erroneous high-probability predictions are reinforced.
**Core assumption**: Sentence-level probability discrepancies meaningfully reflect learning utility; token-level divergence would require shared tokenizers and is not assumed.
**Evidence anchors**: [abstract]: "divergence-aware sampling to better align the teacher's output distribution with the student's learning capacity"; [Section 4, Table 3]: DAS consistently outperforms random sampling across teacher-student pairs (e.g., 85.0 vs 83.1 on AIME24 with gpt-oss-120b teacher).

### Mechanism 3
**Claim**: Mixed-policy distillation reduces exposure bias by training on trajectories that combine student-generated prefixes with teacher-completed suffixes.
**Mechanism**: Standard SFT uses teacher-forced prefixes during training, but inference relies on student autoregressive generation—this mismatch causes error accumulation. Mixed-policy constructs hybrid sequences: the student generates prefixes, which are randomly truncated, and the teacher completes them. The student thus learns to recover from its own generation states while still receiving high-quality supervision.
**Core assumption**: A small subset of on-policy data (7.7K samples in the paper) is sufficient to correct distributional shift without destabilizing the already-trained policy.
**Evidence anchors**: [abstract]: "mixed-policy distillation to mitigate exposure bias"; [Section 5, Table 5]: Mixed-policy training improves AIME24/AIME25 despite using only 7.7K additional samples.

## Foundational Learning

- **Concept: Sequence-level vs Logit-level Knowledge Distillation**
  - Why needed here: The paper explicitly positions itself as improving sequence-level distillation (matching teacher's output distribution over complete responses) rather than logit-level distillation (matching token-level probabilities). Understanding this distinction clarifies why tokenizer differences between teacher and student are acceptable.
  - Quick check question: Given a teacher with vocabulary V_t and student with vocabulary V_s where V_t ≠ V_s, which distillation paradigm remains feasible?

- **Concept: Exposure Bias in Autoregressive Models**
  - Why needed here: The paper identifies exposure bias as a core limitation—models trained with teacher forcing see ground-truth prefixes but must generate from their own predictions at inference. Mixed-policy distillation directly targets this mismatch.
  - Quick check question: If a model achieves 95% next-token accuracy on teacher-forced sequences but only 60% on its own generated prefixes, what failure mode does this indicate?

- **Concept: Temperature Scaling in Softmax Distributions**
  - Why needed here: Temperature-scheduled learning relies on the fact that higher temperatures flatten output distributions (increasing diversity) while lower temperatures sharpen them (concentrating on high-probability modes). The paper shows low-T samples are easier to learn but provide worse coverage.
  - Quick check question: At temperature T→∞, what happens to the teacher's output distribution, and why would this harm student learning?

## Architecture Onboarding

- **Component map**: Question Collection → Response Sampling (T=0.6 + T=1.0) → Divergence-aware Filtering → Response Quality Filtering → Stage 1: Low-T SFT (105K samples) → Stage 2: High-T SFT (330K samples) → Stage 3: Mixed-Policy Distillation (12.7K samples) → DASD-4B-Thinking

- **Critical path**: Response sampling with DAS is the bottleneck—it requires forward passes through both teacher (to obtain probabilities during generation) and student (to compute divergence). The paper uses gpt-oss-120b as teacher and Qwen3-4B-Instruct-2507 as student.

- **Design tradeoffs**:
  - Temperature pair (0.6/1.0): Selected empirically; the paper notes optimal combinations depend on evaluation performance and response probability distributions.
  - DAS threshold: The paper does not specify explicit probability gap thresholds; sentence-level geometric mean probabilities are computed and ranked.
  - Mixed-policy truncation ratio: Random position "beyond half of total length" per response; higher truncation yields more student context but risks error propagation.

- **Failure signatures**:
  - Training loss fails to decrease on high-T data: Student lacks capacity; reduce dataset diversity or increase model size.
  - Mixed-policy training increases response length variability without accuracy gains: Teacher completions are not passing quality filters—tighten filtering or reduce truncation aggressiveness.
  - DAS-selected samples show no performance advantage over random sampling: Teacher and student distributions are already aligned; skip DAS overhead.

- **First 3 experiments**:
  1. **Baseline replication**: Sample 50K math responses at T=0.6 from teacher, SFT student for 6 epochs with learning rate 5e-5→1e-5 cosine decay. Record AIME24/AIME25. This establishes the naive SFT baseline.
  2. **Temperature-scheduled ablation**: Repeat experiment 1, then continue training on 50K T=1.0 samples. Compare against single-stage T=1.0-only training. Expected: curriculum approach yields +2–4 absolute points on challenging benchmarks.
  3. **DAS vs random sampling**: For the same 50K budget, compare random sampling against DAS (rank by teacher-student probability divergence, select top-K). Expected: DAS yields +1–2 points, confirming gradient alignment benefit.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can distribution-aware reweighting during SFT, using the teacher model's sequence-level output probabilities to weight training samples, improve distillation effectiveness and data efficiency over the current uniform-weighting approach?
- **Open Question 2**: What refinements to mixed-policy distillation can enhance training efficiency and stability while maintaining or improving the modest gains observed (+0.3% to +0.9% across benchmarks)?
- **Open Question 3**: Does divergence-aware sampling data curated for one student model architecture (dense 4B) transfer optimally to different architectures (e.g., MoE 30B-A3B), or does each student require DAS data re-computed with its own probability estimates?
- **Open Question 4**: What causes the observed negative correlation between "Boosted Sentences" (sentences where p_T ≈ p_S but p_D is significantly amplified) and test-set accuracy, and can this insight be leveraged to improve data filtering?

## Limitations

- **Unknown quantitative thresholds**: The paper does not specify exact probability thresholds for divergence-aware sampling, making precise reproduction difficult.
- **Proprietary teacher model**: The gpt-oss-120b teacher model is internal to Alibaba, requiring alternative teachers for reproduction that may yield different probability distributions.
- **Unspecified quality filters**: The "predefined quality filters" for teacher completions in mixed-policy distillation are not detailed, preventing exact replication of the filtering criteria.

## Confidence

**High Confidence**: The three-stage training pipeline demonstrably improves performance over single-stage baselines. Temperature-scheduled learning and exposure-bias mitigation yield consistent gains.

**Medium Confidence**: Divergence-aware sampling provides meaningful improvement, though the exact magnitude depends on unspecified probability thresholds. The mechanism is sound but implementation details matter significantly.

**Low Confidence**: The claim that only 448K samples achieve state-of-the-art results. While the numbers are presented, comparison with other 4B models' training data is limited, and the efficiency claim relies on assumptions about dataset quality and task difficulty.

## Next Checks

- **Check 1: DAS Sensitivity Analysis**: Vary the divergence threshold (p_teacher/p_student cutoffs at 1.5, 2.0, 3.0, 5.0) and measure performance impact on AIME benchmarks to quantify sensitivity to the unspecified threshold.

- **Check 2: Mixed-Policy Truncation Sensitivity**: Systematically vary the truncation ratio (20%, 40%, 60%, 80% of response length) during mixed-policy distillation and measure performance degradation to determine optimal truncation heuristics.

- **Check 3: Teacher-Student Distribution Alignment**: Compute and visualize sentence-level probability distributions of teacher and student models across the full dataset to validate whether the stated goal of "aligning teacher's output distribution with student's learning capacity" is actually achieved.