---
ver: rpa2
title: How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic
  Reasoning with Agentic Tool Use
arxiv_id: '2602.00528'
source_url: https://arxiv.org/abs/2602.00528
tags:
- reasoning
- llms
- poker
- toolpoker
- hold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well large language models (LLMs) play
  poker and reason strategically. It finds that LLMs struggle to match traditional
  algorithms, especially equilibrium-based solvers like CFR+, due to reliance on heuristics,
  factual misunderstandings, and gaps between reasoning and actions.
---

# How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use

## Quick Facts
- arXiv ID: 2602.00528
- Source URL: https://arxiv.org/abs/2602.00528
- Reference count: 40
- Primary result: ToolPoker achieves state-of-the-art gameplay performance by integrating external solvers for GTO actions while maintaining reasoning quality aligned with professional principles.

## Executive Summary
This paper investigates how well large language models can play poker with game-theoretic reasoning. It finds that LLMs struggle to match traditional algorithms like CFR+ due to reliance on heuristics and factual misunderstandings. The authors propose ToolPoker, a framework that uses external poker solvers to provide game-theoretic optimal actions and supporting quantities, enabling more precise reasoning. ToolPoker achieves state-of-the-art gameplay performance and produces reasoning traces closely aligned with professional game-theoretic principles.

## Method Summary
The paper evaluates LLM poker play across Kuhn Poker, Leduc Hold'em, and Limit Texas Hold'em. It uses a two-stage approach: behavior cloning on a small high-quality dataset (5k samples) with programmatic tool-tag augmentation, followed by PPO fine-tuning with a composite reward (answer + format + tool execution). The core innovation is a unified solver API that returns GTO actions and auxiliary quantities (equity, ranges) in a single call. Evaluation includes both gameplay metrics (chip earnings vs baselines) and reasoning quality (HR/FA/AC scores via GPT-4.1-mini judge).

## Key Results
- ToolPoker achieves state-of-the-art gameplay performance, winning 63.3 chips/100 games vs CFR+ in Limit Texas Hold'em
- ToolPoker produces reasoning traces with higher factual alignment (FA) and action consistency (AC) scores compared to pure LLM baselines
- Under intermittent tool access, HR and AC remain high while FA degrades first, suggesting partial internalization of strategic structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: External solver integration bypasses LLMs' fundamental inability to perform precise game-theoretic computations.
- Mechanism: The unified tool interface returns CFR-derived GTO actions and supporting quantities (equity, ranges) in a single API call. The LLM's role shifts from computing these values to synthesizing them into coherent reasoning traces, eliminating factual errors at the source.
- Core assumption: Solver outputs are correctly formatted and the LLM can reliably parse them.
- Evidence anchors:
  - [abstract] "ToolPoker...combines external solvers for GTO-consistent actions with more precise professional-style explanations"
  - [section 5.1] "we unify these functionalities into a single standardized interface that provides both the solver's actions and auxiliary statistics"
  - [corpus] Weak direct evidence—PokerBench and SpinGPT focus on training/evaluation, not tool integration specifically.
- Break condition: If tool outputs are noisy or the LLM fails to invoke tools correctly during rollout, reasoning quality degrades (observed in ToolPoker/BC ablation).

### Mechanism 2
- Claim: Programmatic data augmentation with standardized tool-invocation templates enables low-cost, high-quality behavior cloning.
- Mechanism: Rather than annotating large-scale reasoning traces from scratch, the authors automatically inject `<tool></tool>` and `<output></output>` tags into an existing 5k-sample expert dataset, teaching the model when and how to call solvers without expensive manual annotation.
- Core assumption: The underlying expert reasoning traces are sufficiently high-quality to serve as scaffolding.
- Evidence anchors:
  - [section 5.2] "we build an automated pipeline that programmatically augments the reasoning dataset...with standardized tool invocation templates"
  - [section 5.2] "ensuring high-quality and reducing annotation cost"
  - [corpus] No corpus papers explicitly address this data construction approach.
- Break condition: If the base reasoning traces contain systematic errors, tool augmentation propagates rather than corrects them.

### Mechanism 3
- Claim: The composite reward (answer + format + tool execution) jointly optimizes action correctness and reasoning structure.
- Mechanism: R_answer enforces alignment with CFR actions (+1/-1), R_format verifies correct tag ordering, and R_tool measures successful tool invocation rates. The weighted combination prevents the model from optimizing one component at the expense of others.
- Core assumption: The reward components are properly weighted (α_f, α_t) and CFR actions represent ground truth.
- Evidence anchors:
  - [section 5.2] "R(at_i, ât_i, ρt_i) = R_answer(at_i, ât_i) + α_f · R_format(ρt_i) + α_t · R_tool(ρt_i)"
  - [appendix G.8] Ablation shows R_answer is the main driver; removing it drops performance significantly
  - [corpus] ReTool uses similar composite rewards for math but in fully-observed domains; ToolPoker adapts this to imperfect-information settings.
- Break condition: If R_answer weight is too high, the model may ignore tool outputs; if too low, format/execution rewards dominate without improving gameplay.

## Foundational Learning

- **Nash Equilibrium and Counterfactual Regret Minimization (CFR)**
  - Why needed here: The paper assumes familiarity with CFR+ as the gold-standard solver and Nash equilibrium as the target strategy. Without this, the evaluation metrics (chip gain vs CFR) and the regret-inspired reward design are opaque.
  - Quick check question: Can you explain why CFR+ converges to Nash equilibrium in two-player zero-sum games?

- **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The problem formulation uses POMDP notation (S, A, T, R, Ω, O) to model poker's hidden information structure. Understanding observation functions vs. true states is essential.
  - Quick check question: What is the difference between the true state s_t and the observation o_t^i in poker?

- **Proximal Policy Optimization (PPO) with GAE**
  - Why needed here: Both BC-RIRL and ToolPoker use PPO as the RL backbone with KL regularization. Understanding clipping, advantage estimation, and the reference policy role is necessary to interpret training dynamics.
  - Quick check question: Why does PPO include a KL-divergence penalty against a reference policy?

## Architecture Onboarding

- **Component map:**
  Unified Solver API -> TIR Prompt Template -> BC Dataset Pipeline -> RL Training Loop -> Evaluation Suite

- **Critical path:**
  1. Verify solver API returns correct GTO actions for test hands
  2. Validate BC dataset quality—check tool tags match actual solver outputs
  3. Monitor R_tool during RL warmup (should reach ~1.0 quickly)
  4. Track R_answer convergence (primary gameplay driver)
  5. Evaluate against CFR+ (target: within 5 chips/100 games)

- **Design tradeoffs:**
  - **Single vs. multi-tool API**: Unified interface stabilizes training but reduces flexibility for future tool additions
  - **Small BC dataset (5k) vs. large action-only (400k)**: Quality reasoning traces are expensive; authors trade scale for depth
  - **Composite reward weighting**: Higher R_answer improves gameplay but may reduce reasoning interpretability if model shortcuts to action

- **Failure signatures:**
  - Low FA scores with high HR: Model mimics reasoning style without factual grounding (seen in ToolPoker/RL ablation)
  - High R_tool but low R_answer: Model calls tools correctly but ignores outputs in final answer
  - Chip loss vs CFR+ > 20/100 games: Likely state mis-specification in tool queries

- **First 3 experiments:**
  1. **Baseline sanity check**: Run vanilla Qwen2.5-7B against random agent to confirm expected chip losses (~-50 to -100)
  2. **BC-only ablation**: Train ToolPoker without RL stage; expect high HR but weaker gameplay and FA degradation
  3. **Reward component sweep**: Vary α_f and α_t weights (try 0.1, 0.5, 1.0); measure impact on R_answer convergence speed and final chip performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ToolPoker maintain strong performance when evaluated against real human poker players rather than algorithmic baselines?
- Basis in paper: [explicit] Appendix H.3 states: "While we have not yet evaluated ToolPoker on real human gameplay, extending our assessment to human or crowd-sourced datasets is an exciting direction for future work."
- Why unresolved: All evaluations in the paper use synthetic opponents (CFR, NFSP, DQN, DMC) or other LLMs, not humans who may exhibit more varied, unpredictable, or exploitatively adaptive strategies.
- What evidence would resolve it: Performance metrics (chip gains, reasoning quality scores) from ToolPoker competing against human players in controlled poker experiments.

### Open Question 2
- Question: How effectively can ToolPoker generalize to No-Limit Texas Hold'em, where bet sizing decisions vastly expand the strategy space?
- Basis in paper: [inferred] The paper evaluates only Limit Texas Hold'em, stating in Appendix B.3 that "No-Limit Hold'em... is the most popular poker format in practice" but noting its greater complexity. The unified tool interface and TIR framework may face scaling challenges.
- Why unresolved: No-limit introduces continuous action spaces (bet sizing), fundamentally changing solver requirements and potentially increasing tool-calling complexity.
- What evidence would resolve it: Ablation studies applying ToolPoker to No-Limit Hold'em against established baselines, measuring both gameplay and reasoning quality.

### Open Question 3
- Question: To what extent does ToolPoker internalize game-theoretic principles versus relying entirely on tool outputs?
- Basis in paper: [explicit] Appendix G.6 notes: "In realistic settings, external tools may be unavailable or only intermittently accessible... under intermittent tool access we find that HR and AC remain relatively high while FA degrades first."
- Why unresolved: The paper suggests ToolPoker "internalizes core strategic structures" but FA degradation under intermittent access indicates factual precision remains tool-dependent.
- What evidence would resolve it: Systematic evaluation of ToolPoker's reasoning quality and gameplay performance under varying tool availability conditions (no tools, partial access, delayed outputs).

## Limitations

- The unified solver API's robustness across poker variants is not extensively validated. While the paper claims consistent tool performance, the 5k BC dataset may not fully capture edge cases in Leduc Hold'em or Limit Texas Hold'em.
- The composite reward design relies heavily on CFR+ as ground truth, but no ablation tests removing this assumption. Alternative solvers or imperfect CFR outputs could degrade performance.
- ToolPoker's reasoning quality (FA/AC scores) remains lower than some baselines despite better gameplay, suggesting a potential misalignment between game-theoretic precision and human-interpretable reasoning.

## Confidence

- **High confidence**: The core finding that LLMs struggle with game-theoretic precision without external tools is well-supported by systematic comparisons across multiple poker variants.
- **Medium confidence**: The mechanism that tool integration improves gameplay is demonstrated, but the exact contribution of each reward component (answer vs format vs tool execution) is not fully isolated.
- **Medium confidence**: The claim that ToolPoker produces reasoning "closely aligned with professional game-theoretic principles" is supported by HR/FA/AC metrics, though these are LLM-as-a-judge evaluations rather than expert human validation.

## Next Checks

1. **Reward component ablation**: Systematically vary α_f and α_t weights in the composite reward to quantify each component's contribution to both gameplay performance and reasoning quality.
2. **Solver robustness test**: Evaluate ToolPoker with corrupted or noisy solver outputs to determine failure thresholds and assess the model's resilience to tool uncertainty.
3. **Cross-variant generalization**: Test the unified tool API's performance on unseen poker variants (e.g., No-Limit Hold'em) to validate the framework's generalizability beyond the three studied variants.