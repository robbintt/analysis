---
ver: rpa2
title: LM Agents for Coordinating Multi-User Information Gathering
arxiv_id: '2502.12328'
source_url: https://arxiv.org/abs/2502.12328
tags:
- information
- user
- about
- have
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PEOPLE JOIN, a benchmark for evaluating LM-mediated
  collaborative problem solving. Given a user request, PEOPLE JOIN agents must identify
  teammates who might be able to assist, converse with these teammates to gather information,
  and finally compile a useful answer or summary for the original user.
---

# LM Agents for Coordinating Multi-User Information Gathering

## Quick Facts
- arXiv ID: 2502.12328
- Source URL: https://arxiv.org/abs/2502.12328
- Reference count: 23
- Key outcome: LM-powered agents struggle with multi-user coordination, achieving Match scores from 24.4 to 54.8 across different models and tasks

## Executive Summary
This paper introduces PEOPLE JOIN, a benchmark for evaluating language model-mediated collaborative problem solving where agents must identify and coordinate with teammates to gather distributed information. The benchmark features two domains - PEOPLE JOIN-QA for tabular data questions and PEOPLE JOIN-DocCreation for document summarization tasks - adapted from existing NLP datasets but distributed across synthetic organizations of 2-20 users. Reference agent implementations using Phi-3-medium, GPT-4-turbo, and GPT-4o show that even strong language models struggle with multi-user coordination, with performance bottlenecks in people retrieval precision and information synthesis across organizational hierarchies.

## Method Summary
The PEOPLE JOIN benchmark uses a ReAct-style reactive agent architecture with BM25-based retrieval systems for both documents and people. Agents operate through a loop of observation, reflection (thought), and action, using tools to search documents, search for relevant people, resolve person identities, and send messages. The evaluation uses synthetic organizations derived from SPIDER (500 test tasks) and MULTINEWS (200 instances, 67 orgs), with LLM simulators representing teammates. Performance is measured using Match scores (0, 50, 100) for QA tasks, ROUGE-L and G-Eval for summarization, and efficiency metrics including message count, message size, and people contacted.

## Key Results
- GPT-4-turbo achieves highest Match score of 54.8% on PEOPLE JOIN-QA
- People-Precision (P-Prec) peaks at only 0.61, indicating ~40% of contacted users are irrelevant
- Performance drops significantly for redirection scenarios (38.0% Match) and DocCreation tasks
- Message counts range from 3.4 to 4.9 per task depending on model and task type

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit "reflection" steps (ReAct-style thinking) improve task accuracy by enabling dynamic re-planning
- **Mechanism:** The agent utilizes a `Reflection.thought()` action to assess current state and predict next action, allowing decomposition of complex queries before executing tool calls
- **Core assumption:** The underlying LLM possesses sufficient reasoning capability to generate useful reflections
- **Evidence anchors:** Reactive agent performs better than Reactive-NoRef across LLMs on Match; CooperBench suggests current agents lack social intelligence for team conflicts

### Mechanism 2
- **Claim:** Performance bottlenecked by precision of "People Retrieval" step
- **Mechanism:** Agent queries `search_relevant_people` function using BM25-based search with noisy expertise profiles
- **Core assumption:** Search index effectively retrieves relevant candidates despite imprecise descriptions
- **Evidence anchors:** Best performing agent has P-Prec of only 0.61; MAP suggests resolving conflicting directives remains challenging

### Mechanism 3
- **Claim:** Agents fail to correctly aggregate information when navigating organizational hierarchies or synthesizing fragmented data
- **Mechanism:** Task requires maintaining mental model of organization and updating plan when users redirect queries
- **Core assumption:** Agent can maintain state across multiple conversational turns without losing original intent
- **Evidence anchors:** Match score drops to 38.0% for redirection cases; Collaborative Memory highlights difficulty managing shared context

## Foundational Learning

- **Concept:** ReAct Pattern (Reasoning + Acting)
  - **Why needed here:** Reference agent built on this architecture; required to understand interleaving of thoughts with tool calls
  - **Quick check question:** Can you distinguish between an agent's "thought" (internal monologue) and its "action" (executable tool call) in the prompt structure?

- **Concept:** Retrieval Augmented Generation (RAG)
  - **Why needed here:** Agent relies on Document Search and People Search retrieval systems; must understand how BM25 scores affect candidate selection
  - **Quick check question:** If `search_relevant_people` returns low-confidence match, how should agent behave - proceed or ask for clarification?

- **Concept:** Multi-LLM Simulation (User Simulators)
  - **Why needed here:** Benchmark uses LLMs to simulate teammates; evaluation fidelity depends on accurate human behavior simulation
  - **Quick check question:** How does "User Simulator" prompt differ from "Agent" prompt? (One simulates persona with documents, other acts as helpful assistant)

## Architecture Onboarding

- **Component map:** User Simulators (LLM instances with documents/personas) -> Agent Core (LLM with Observe -> Reflect -> Act loop) -> Tools (EnterpriseSearch, Enterprise) -> Orchestrator (Python controller managing event loop)

- **Critical path:** Primary User sends message -> Agent calls search_documents -> search_people -> resolve_person -> send_message -> Target User Simulator generates response -> Agent receives response -> determines if more info needed -> compiles final answer

- **Design tradeoffs:**
  - Dyadic vs. Group Chat: Current implementation restricts to dyadic conversations, simplifying state management but limiting broadcast requests
  - Reflection vs. Cost: Reactive agent uses reflection tokens improving accuracy (+6.8 points for GPT-4-turbo) but increasing latency and token usage

- **Failure signatures:**
  - Premature Termination: Agent returns "couldn't find information" without exhausting contact list (30% of failures)
  - Query Drift: Agent asks overly-specific or poorly worded follow-ups causing "I don't know" responses (25% of failures)
  - Orchestration Loop: Agent fails to predict valid tool call, triggering "Failed to parse" recovery (10% of failures)

- **First 3 experiments:**
  1. Baseline Validation: Run Reactive agent on 50 random PeopleJoin-QA tasks to verify ~54.8 Match score and observe Reflection logs
  2. Failure Mode Analysis: Isolate 40 cases with poor query failures; experiment with adding Query Refinement step to translate SQL-like questions into natural language
  3. Retrieval Ablation: Replace BM25 search with embedding-based semantic search to test if P-Prec improves and reduces communication cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agents optimally select collaborators and contact order to balance accuracy with communication efficiency?
- Basis in paper: Abstract and conclusion identify determining "who to contact and in what order" as major open challenge
- Why unresolved: Current Reactive agents show suboptimal people-precision (0.61) and high message counts
- What evidence would resolve it: New agent strategies achieving higher Match scores with message counts approaching optimal baseline

### Open Question 2
- Question: Can agents effectively learn organization's structure from interaction history to improve future task performance?
- Basis in paper: Section 8 proposes AI agents that learn over time from interactions to refine communication strategies
- Why unresolved: Current implementations use static prompts without updating based on previous success/failure
- What evidence would resolve it: Demonstrated efficiency gains (fewer messages) in sequential tasks within same organization

### Open Question 3
- Question: How can agents be secured against privacy leaks when accessing distributed personal documents?
- Basis in paper: Section 8 notes "privacy risks emerge" and suggests future work on "privacy-centric evaluations"
- Why unresolved: Benchmark assumes full document access without modeling privacy constraints
- What evidence would resolve it: Benchmark extension measuring performance under data access or confidentiality constraints

### Open Question 4
- Question: How do social dynamics and alternative topologies (e.g., group chats) affect multi-user coordination?
- Basis in paper: Limitations section suggests exploring "group chats," "turn-around speed," and "social dynamics"
- Why unresolved: Current benchmark restricts to dyadic conversations and ignores social factors like user busyness
- What evidence would resolve it: Results from agents evaluated in environments simulating group channels or variable response latencies

## Limitations
- Benchmark relies on synthetic organizations with simulated users, raising ecological validity concerns about real human communication patterns
- LLM judges for QA tasks may share systematic weaknesses with agents being tested, introducing evaluation bias
- Limited set of 4 exemplars per domain may constrain agent generalization to novel organizational structures
- Current implementation ignores privacy constraints and potential information leakage risks
- Restricted to dyadic conversations, missing complexities of group coordination and social dynamics

## Confidence
- **High Confidence:** Architectural description of ReAct-style agent and performance relative to baseline models (54.8% Match for GPT-4-turbo) is well-documented and reproducible
- **Medium Confidence:** Failure mode analysis (30% early termination, 25% poor query formulation) appears empirically grounded but may not capture full real-world complexity
- **Medium Confidence:** Efficiency metrics provide useful comparative data but may not translate directly to real communication costs without human validation

## Next Checks
1. **Human Evaluation Validation:** Run subset of 20 tasks with actual human participants as organization members to compare simulator performance against real-world coordination patterns
2. **Retrieval System A/B Test:** Implement and test embedding-based semantic search for people retrieval to empirically validate whether improved P-Prec translates to better Match scores
3. **Exemplar Scaling Experiment:** Systematically vary number of exemplars (2, 4, 6, 8) in agent prompt to determine optimal training set size for maximizing Match score while minimizing context window usage