---
ver: rpa2
title: 'MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language
  Models'
arxiv_id: '2506.07400'
source_url: https://arxiv.org/abs/2506.07400
tags:
- clinical
- diagnostic
- medical
- glaucoma
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedChat is a multi-agent framework for automated glaucoma diagnosis
  from retinal fundus images that combines specialized vision models with role-specific
  LLM agents coordinated by a director agent. The system addresses the limitations
  of single-agent LLM approaches in medical imaging, such as hallucinations, limited
  interpretability, and insufficient domain-specific knowledge, by emulating multidisciplinary
  clinical workflows.
---

# MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models

## Quick Facts
- arXiv ID: 2506.07400
- Source URL: https://arxiv.org/abs/2506.07400
- Reference count: 40
- Primary result: Multi-agent framework combining vision models with role-specific LLM agents for glaucoma diagnosis from retinal fundus images

## Executive Summary
MedChat is a multi-agent framework that addresses limitations of single-agent LLM approaches in medical imaging by emulating multidisciplinary clinical workflows for glaucoma diagnosis from retinal fundus images. The system processes images through specialized vision models (SwinV2 classifier and SegFormer segmentor) and distributes findings to role-specific agents (ophthalmologist, optometrist, pharmacist, glaucoma specialist) that generate domain-specific sub-reports. These are synthesized by a director agent into a comprehensive diagnostic report. The framework demonstrates improved reliability and reduced hallucination risk compared to single-agent systems, with results showing clinically-grounded reports that integrate diverse medical perspectives and provide actionable recommendations based on image-derived features like cup-to-disc ratio and glaucoma probability.

## Method Summary
The MedChat framework combines computer-aided diagnosis models with vision transformers for glaucoma detection and cup-to-disc ratio calculation. It processes retinal fundus images through a SwinV2 classifier (producing glaucoma probability) and a SegFormer segmentor (generating optic disc/cup masks from which CDR is calculated). These quantitative outputs are verbalized into a structured core prompt that serves as evidence for all downstream reasoning. Role-specific agents (ophthalmologist, optometrist, pharmacist, glaucoma specialist) each receive this core prompt augmented with specialty-specific instructions and generate sub-reports from their clinical perspective. A director agent then synthesizes these sub-reports into a unified diagnostic report that improves interpretability and reduces hallucination risk. The framework operates as a RESTful API with a JavaScript/HTML frontend for image upload, chat interface, and PDF report generation.

## Key Results
- Multi-agent architecture reduces hallucination risk by anchoring LLM reasoning to quantitative vision model outputs
- Role-specific agents generate diverse diagnostic perspectives while maintaining clinical coherence
- Director agent synthesis produces unified reports that can correct minor inconsistencies across sub-reports
- Framework successfully integrates quantitative image features (CDR, glaucoma probability) into clinically actionable recommendations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Verbalizing vision model outputs into structured natural language reduces hallucination by anchoring LLM reasoning to quantitative evidence.
- Mechanism: The SwinV2 classifier produces a glaucoma probability that is discretized into four verbal grades (e.g., "likely glaucoma" for p ∈ [0.5, 0.9)). The SegFormer produces segmentation masks from which cup-to-disc ratio (CDR) is computed and verbalized (e.g., "approximately 0.62"). These become the factual core prompt shared across all agents.
- Core assumption: Discretization and verbalization preserve sufficient clinical signal for downstream reasoning while reducing LLM tendency to generate ungrounded claims.
- Evidence anchors:
  - [abstract] "combines computer-aided diagnosis models with vision transformers for glaucoma detection and cup-to-disc ratio calculation"
  - [section] Equation (1) defines Grade(p) mapping; Equation (2) defines CDR from segmentation masks; core prompt example provided in §III-B
  - [corpus] Citrus-V paper similarly emphasizes "unified medical image grounding for clinical reasoning," suggesting grounding is a recognized strategy, though corpus evidence for verbalization specifically is weak
- Break condition: If discretization thresholds (0.2, 0.5, 0.9) are mismatched to clinical decision boundaries, verbal grades may mislead agents; if CDR calculation from 2D masks fails to capture 3D anatomical truth, the anchor itself is unreliable.

### Mechanism 2
- Claim: Role-specific constraints on multiple LLM agents reduce overlap and encourage diverse diagnostic perspectives.
- Mechanism: Each agent receives the same core prompt augmented with role-specific instructions that constrain scope (e.g., "As a pharmacist... only include observations relevant to your specialty"). Agents operate independently, generating sub-reports from distinct clinical viewpoints.
- Core assumption: Role-specific prompting yields meaningfully divergent outputs rather than cosmetic variation; GPT-4.1 can maintain role discipline.
- Evidence anchors:
  - [abstract] "role-specific agents (ophthalmologist, optometrist, pharmacist, glaucoma specialist) to generate domain-specific sub-reports"
  - [section] §III-C describes prompt structure: "Avoid repeating what is not within your scope"; §VI notes limitation that "outputs often show a high degree of consensus, limiting the diversity of reasoning"
  - [corpus] Tree-of-Reasoning and MedCoAct papers also propose multi-agent decomposition for complex medical tasks, suggesting convergent evidence for the approach, but no direct comparison data
- Break condition: If role instructions are too weak, agents produce redundant outputs; if too rigid, they miss cross-domain insights. The paper explicitly notes consensus as a current limitation.

### Mechanism 3
- Claim: A director agent synthesizing multiple sub-reports improves coherence and can correct minor inaccuracies in individual outputs.
- Mechanism: The director agent receives concatenated sub-reports and is instructed to identify consensus, resolve contradictions, and produce a unified report without attributing sources. This creates a single authoritative document.
- Core assumption: Synthesis by a general-purpose LLM can resolve inconsistencies and synthesize insights without introducing new hallucinations.
- Evidence anchors:
  - [abstract] "director agent synthesizes these perspectives into a comprehensive diagnostic report that improves interpretability and reduces hallucination risk"
  - [section] §III-D states the director "can correct minor inaccuracies or inconsistencies" and produces "a unified and clinically appropriate plan"; §V shows an example synthesis
  - [corpus] Aligning Findings paper addresses "factual hallucinations" in MLLMs via reinforcement learning, suggesting synthesis without hallucination remains an open challenge; corpus does not validate director-level correction claims
- Break condition: If sub-reports contain conflicting facts (not just emphasis differences), the director may fabricate consensus or suppress valid disagreement. No validation protocol is described.

## Foundational Learning

- Concept: Vision-to-language grounding
  - Why needed here: You must understand how classification probabilities and segmentation outputs are converted into verbal prompts that LLMs can process. This is the evidentiary foundation for all downstream reasoning.
  - Quick check question: Given a glaucoma probability of 0.35, what verbal grade does the system produce, and why might threshold selection matter clinically?

- Concept: Multi-agent orchestration patterns
  - Why needed here: MedChat's architecture depends on coordinating independent agents with shared context but divergent roles. Understanding when to parallelize vs. sequence agents is critical for adaptation.
  - Quick check question: What happens if two role-specific agents produce contradictory recommendations—how does the system currently handle this, and what are the risks?

- Concept: Hallucination mitigation strategies
  - Why needed here: The paper claims reduced hallucination risk, but the mechanism relies entirely on input grounding and role constraints. You need to recognize where these safeguards can fail.
  - Quick check question: If the SegFormer missegments the optic cup, how does that error propagate through the rest of the pipeline?

## Architecture Onboarding

- Component map: Image → SwinV2 classifier & SegFormer segmentor → verbalized prompt → parallel role agents → director synthesis → final report
- Critical path: Image → classifier & segmentor → verbalized prompt → parallel role agents → director synthesis → final report. Any failure in vision components corrupts all downstream reasoning.
- Design tradeoffs:
  - Shared prompt ensures consistency but limits diversity (acknowledged in §VI)
  - Role-specific agents increase computational cost and latency but provide multi-perspective coverage
  - No fine-tuning means rapid deployment but reduced domain precision
  - No feedback loop means clinician corrections are not captured for system improvement
- Failure signatures:
  - High consensus across sub-reports despite role differences → prompts may be over-constraining or context too sparse
  - Director report introduces facts not in sub-reports → synthesis hallucination
  - Generic recommendations when clinical notes absent → insufficient context (noted limitation)
- First 3 experiments:
  1. Threshold sensitivity analysis: Vary the classifier discretization thresholds (currently 0.2/0.5/0.9) and measure impact on final report recommendations. This tests whether verbal grounding is robust or brittle.
  2. Role ablation study: Remove one agent role at a time and compare final reports to assess whether each role contributes unique, integrated content or redundant filler.
  3. Consistency stress test: Inject deliberate contradictions into sub-reports (e.g., pharmacist recommends medication A, glaucoma specialist contraindicates it) and evaluate whether the director resolves appropriately or produces incoherent output.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantitative impact of the multi-agent framework on diagnostic quality, coherence, and clinical accuracy compared to single-agent baselines?
- Basis in paper: [explicit] The authors state, "We leave formal evaluation of diagnostic quality, coherence, and clinical accuracy to future work."
- Why unresolved: The paper demonstrates the framework via a case study but provides no statistical analysis or benchmark comparisons against ground truth or other models.
- What evidence would resolve it: Results from standardized clinical evaluation metrics (e.g., expert clinician scoring or accuracy rates) comparing MedChat against single-agent LLM baselines.

### Open Question 2
- Question: Can dynamic, role-specific prompt construction successfully mitigate the "high degree of consensus" and limited reasoning diversity observed when agents share the same core prompt?
- Basis in paper: [explicit] The authors note that "outputs often show a high degree of consensus, limiting the diversity of reasoning" and suggest dynamic prompt construction as a future solution.
- Why unresolved: The current implementation provides the identical core prompt to all agents, which restricts the variance in their sub-reports.
- What evidence would resolve it: Ablation studies showing increased semantic diversity and distinct reasoning paths when agents receive curated subsets of clinical context versus a shared prompt.

### Open Question 3
- Question: Can reinforcement learning techniques, such as Group Relative Policy Optimization (GRPO), be effectively integrated to create a feedback loop for clinician oversight?
- Basis in paper: [explicit] The authors express interest in "applying reinforcement learning techniques such as Group Relative Policy Optimization" to capture expert corrections.
- Why unresolved: The current system lacks a feedback loop; expert corrections made during review are not captured or used to update the system.
- What evidence would resolve it: A working prototype where clinician feedback is converted into programmable reward functions, resulting in measurable improvement in subsequent report generation.

## Limitations
- No formal quantitative evaluation metrics provided; claims based on qualitative demonstration only
- Role-specific agents show high consensus and limited reasoning diversity despite distinct instructions
- System lacks feedback loop to capture and integrate clinician corrections for improvement

## Confidence
- **High confidence**: Architectural framework and vision model grounding mechanism (Equations 1-2 explicitly defined)
- **Medium confidence**: Hallucination reduction claims (mechanistically plausible but unvalidated)
- **Low confidence**: Clinical effectiveness and multi-agent benefit claims (no comparative evaluation data provided)

## Next Checks
1. Implement formal hallucination detection by comparing generated reports against ground truth findings to measure factual consistency rates
2. Conduct role contribution analysis by systematically removing individual agents and measuring degradation in report quality/completeness
3. Perform vision model sensitivity testing by introducing controlled errors in segmentation and classification to trace error propagation through the multi-agent pipeline