---
ver: rpa2
title: Evaluating the Systematic Reasoning Abilities of Large Language Models through
  Graph Coloring
arxiv_id: '2502.07087'
source_url: https://arxiv.org/abs/2502.07087
tags:
- math
- problems
- friends
- error
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Graph coloring tasks expose significant gaps in large language\
  \ models\u2019 (LLMs) systematic reasoning capabilities, with even advanced models\
  \ achieving less than 40% accuracy on moderately complex problems. Six models\u2014\
  four standard LLMs and two large reasoning models (LRMs)\u2014were evaluated on\
  \ procedurally generated graph coloring problems across multiple semantic frames."
---

# Evaluating the Systematic Reasoning Abilities of Large Language Models through Graph Coloring

## Quick Facts
- arXiv ID: 2502.07087
- Source URL: https://arxiv.org/abs/2502.07087
- Reference count: 40
- Standard LLMs achieved less than 40% accuracy on moderately complex graph coloring problems

## Executive Summary
This study evaluates the systematic reasoning capabilities of large language models (LLMs) through graph coloring tasks. Six models were tested across multiple semantic frames using procedurally generated problems. The results reveal significant limitations in both standard LLMs and large reasoning models (LRMs), with even advanced models struggling to achieve consistent accuracy on moderately complex problems. Standard LLMs consistently exceeded 60% error rates on difficult problem types, while LRMs showed improvement but still demonstrated dramatic error increases as problem complexity grew. No model achieved perfect accuracy even on simple 4-vertex, 2-color problems.

## Method Summary
The study employed procedurally generated graph coloring problems across multiple semantic frames to evaluate six models: four standard LLMs and two large reasoning models. Problems were designed with varying complexity levels, and models were tested on their ability to correctly assign colors to vertices while adhering to graph coloring constraints. The evaluation included both standard LLMs and LRMs to compare their systematic reasoning capabilities. Performance was measured across different problem types and semantic frames to identify model-specific biases and limitations.

## Key Results
- Standard LLMs achieved less than 40% accuracy on moderately complex graph coloring problems
- LRMs showed improvement but still had dramatic error increases (up to 15-16%) as problem complexity grew
- No model achieved perfect accuracy on simple 4-vertex, 2-color problems
- Frame choice significantly affected performance, revealing model-specific semantic biases

## Why This Works (Mechanism)
The graph coloring task serves as an effective benchmark for systematic reasoning because it requires models to maintain constraints across multiple variables while exploring possibility spaces. The procedural generation ensures consistent problem difficulty and prevents overfitting to specific patterns. The use of multiple semantic frames tests the models' ability to transfer reasoning skills across different contexts, exposing limitations in compositional reasoning.

## Foundational Learning
- Graph coloring theory: Understanding constraint satisfaction problems
  * Why needed: Forms the basis for evaluating systematic reasoning
  * Quick check: Can the model correctly solve simple 2-color problems
- Semantic frame theory: How language context affects reasoning
  * Why needed: Explains performance variations across different problem presentations
  * Quick check: Does model performance vary significantly between frames
- Procedural generation: Creating consistent, unbiased test problems
  * Why needed: Ensures reliable benchmarking across models
  * Quick check: Are generated problems uniformly difficult

## Architecture Onboarding
- Component map: Input Processing -> Graph Analysis -> Constraint Satisfaction -> Output Generation
- Critical path: Problem parsing -> Color assignment strategy -> Constraint validation -> Final solution
- Design tradeoffs: Between exploration of possibility space and computational efficiency
- Failure signatures: Consistent errors in constraint propagation and possibility space exploration
- First experiments:
  1. Test model on progressively more complex graph structures
  2. Compare performance across different semantic frame presentations
  3. Evaluate constraint satisfaction accuracy on simple vs. complex problems

## Open Questions the Paper Calls Out
The study does not explicitly call out specific open questions beyond those related to the observed limitations and the potential of procedural generation for benchmarking reasoning capabilities.

## Limitations
- Findings based on specific graph coloring problems may not generalize to all reasoning tasks
- Performance differences could be influenced by training data biases
- 40% accuracy ceiling may not represent performance on other systematic reasoning tasks
- Significant impact of frame choice suggests potential overfitting to specific problem presentations

## Confidence
- High confidence: General trend of LLMs struggling with complex graph coloring problems
- Medium confidence: Specific accuracy percentages and error rates reported for individual models
- Low confidence: Extent to which observed limitations generalize to other domains

## Next Checks
1. Replicate the study using a diverse set of graph coloring problems with varying complexities and semantic frames
2. Apply the same evaluation methodology to other systematic reasoning tasks, such as constraint satisfaction problems
3. Investigate the impact of advanced prompting techniques on LLM performance in graph coloring tasks