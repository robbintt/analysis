---
ver: rpa2
title: Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting
  Physical Activity in Older Adults Using Wearable Sensors
arxiv_id: '2509.18433'
source_url: https://arxiv.org/abs/2509.18433
tags:
- activity
- policy
- learning
- reward
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KANDI, a novel offline inverse reinforcement
  learning framework combining Kolmogorov-Arnold Networks (KAN) and Diffusion Policies
  for promoting physical activity in older adults using wearable sensor data from
  a clinical trial. KANDI addresses challenges in defining reward functions and capturing
  temporal variability in healthcare settings by inferring rewards from expert behavior
  using KAN's flexible function approximation, and generating high-fidelity actions
  using diffusion models within an Actor-Critic framework.
---

# Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors

## Quick Facts
- arXiv ID: 2509.18433
- Source URL: https://arxiv.org/abs/2509.18433
- Reference count: 40
- Primary result: Novel KANDI framework combines Kolmogorov-Arnold Networks with Diffusion Policies for offline IRL to promote physical activity in older adults using wearable sensor data from PEER study (N=134)

## Executive Summary
This paper introduces KANDI, an innovative offline inverse reinforcement learning framework that combines Kolmogorov-Arnold Networks (KAN) with Diffusion Policies to promote physical activity in older adults. The method addresses critical challenges in healthcare settings, including the difficulty of defining reward functions and capturing temporal variability in behavior patterns. By inferring rewards from expert behavior using KAN's flexible function approximation capabilities, and generating high-fidelity actions through diffusion models within an Actor-Critic framework, KANDI demonstrates effectiveness in increasing physical activity levels among older adults.

Evaluated on wearable sensor data from 134 participants in the PEER study, KANDI successfully learned optimal timing and policies for activity promotion, showing higher intervention probabilities during daytime hours. The framework also achieves state-of-the-art performance on D4RL benchmark tasks, with normalized scores ranging from 50.2 to 112.0 across multiple environments. This work represents a significant advancement in applying sophisticated offline IRL techniques to real-world healthcare applications, particularly for promoting healthy behaviors in vulnerable populations.

## Method Summary
KANDI integrates Kolmogorov-Arnold Networks (KAN) for flexible reward function approximation with Diffusion Policies within an Actor-Critic framework. The approach leverages offline inverse reinforcement learning to infer reward functions from expert behavior data collected via wearable sensors. KAN's ability to model complex, non-linear relationships enables accurate reward estimation, while diffusion models generate high-fidelity action sequences. The framework operates entirely offline, making it suitable for healthcare applications where online interaction may be impractical or risky. The method was specifically designed to address the challenges of temporal variability and reward function specification in promoting physical activity among older adults.

## Key Results
- Successfully learned optimal timing and policies to increase physical activity levels in older adults from PEER study (N=134)
- Demonstrated higher action probabilities during daytime hours, indicating effective temporal pattern recognition
- Achieved state-of-the-art performance on D4RL benchmark with normalized scores: 50.2 (halfcheetah-medium-v2), 111.8 (walker2d-medium-expert-v2), 112.0 (hopper-medium-expert-v2), 96.4 (kitchen-complete-v0), 74.3 (kitchen-mixed-v0), 66.1 (antmaze-large-play-v0)
- Effectively promoted standing behavior as a specific intervention outcome

## Why This Works (Mechanism)
KANDI works by addressing the fundamental challenges of reward function specification and temporal dynamics in healthcare settings. The Kolmogorov-Arnold Network component provides flexible, non-linear function approximation that can capture complex relationships between sensor data and desired behaviors without requiring explicit reward engineering. This is particularly valuable in healthcare where defining optimal reward functions is challenging. The diffusion policy component generates high-fidelity action sequences that respect the temporal structure of human behavior, allowing the system to learn when interventions are most likely to be effective. The offline nature of the framework enables learning from historical data without requiring risky real-time experimentation, which is crucial for applications involving vulnerable populations like older adults.

## Foundational Learning

**Kolmogorov-Arnold Networks (KAN)**
- Why needed: Provides flexible function approximation for modeling complex reward functions without explicit engineering
- Quick check: Verify KAN can capture non-linear relationships in sensor data that traditional MLPs might miss

**Diffusion Models**
- Why needed: Generates high-fidelity action sequences while respecting temporal structure of human behavior
- Quick check: Ensure diffusion sampling produces realistic, temporally coherent action sequences

**Offline Reinforcement Learning**
- Why needed: Enables learning from historical data without risky real-time experimentation, critical for healthcare applications
- Quick check: Confirm the method performs well without online interaction or exploration

**Actor-Critic Framework**
- Why needed: Balances policy optimization with value estimation for stable learning
- Quick check: Monitor actor-critic stability during training, especially with complex KAN reward functions

## Architecture Onboarding

**Component Map:** Wearable Sensors -> KAN Reward Network -> Diffusion Policy -> Action Generator -> Intervention Timing

**Critical Path:** The core inference pipeline processes raw sensor data through KAN to estimate rewards, which guide the diffusion policy in generating intervention actions. The temporal analysis module identifies optimal intervention timing based on learned patterns.

**Design Tradeoffs:** The framework prioritizes safety and data efficiency through offline learning, trading potential performance gains from online exploration for reduced risk. KAN's flexibility comes at computational cost compared to simpler reward models. The diffusion policy approach ensures high-quality action generation but requires careful tuning of sampling parameters.

**Failure Signatures:** Poor reward inference may manifest as interventions that don't align with expert behavior patterns. Diffusion sampling issues could produce unrealistic or temporally incoherent action sequences. Temporal analysis failures might result in interventions at suboptimal times regardless of action quality.

**First Experiments:**
1. Validate KAN reward approximation against ground truth rewards on synthetic tasks before applying to real sensor data
2. Test diffusion policy action quality on simpler benchmark tasks to establish baseline performance
3. Conduct ablation study comparing KAN-based rewards versus traditional reward engineering approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (N=134) from single clinical trial limits generalizability across diverse populations
- Evaluation focuses specifically on standing behavior promotion, leaving uncertainty about effectiveness for other physical activities
- Reliance on wearable sensor data may not capture all relevant behavioral nuances
- Computational complexity of KAN-diffusion integration raises questions about practical deployment requirements

## Confidence
- PEER study results: High confidence based on temporal analysis showing daytime intervention patterns
- D4RL benchmark claims: Medium confidence due to strong normalized scores (50.2-112.0) but lack of baseline implementation details
- Generalizability claims: Low confidence due to single-site study and limited population diversity

## Next Checks
1. Conduct multi-site clinical trials with larger, more diverse populations to validate generalizability across different demographic groups and healthcare environments
2. Perform ablation studies to isolate the contribution of KAN components versus diffusion policies in achieving the reported performance gains
3. Test KANDI's effectiveness across multiple physical activity types beyond standing behavior to assess broader applicability for comprehensive activity promotion