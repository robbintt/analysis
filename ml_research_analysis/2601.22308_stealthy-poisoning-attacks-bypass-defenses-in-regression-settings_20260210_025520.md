---
ver: rpa2
title: Stealthy Poisoning Attacks Bypass Defenses in Regression Settings
arxiv_id: '2601.22308'
source_url: https://arxiv.org/abs/2601.22308
tags:
- poisoning
- points
- attacks
- trim
- defenses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes stealthy poisoning attacks against regression
  models, addressing a gap in prior work that focused on high-detectability attacks.
  The authors formulate a novel multiobjective bilevel optimization problem balancing
  attack effectiveness (maximizing MSE) and stealth (minimizing detectability risk).
---

# Stealthy Poisoning Attacks Bypass Defenses in Regression Settings

## Quick Facts
- **arXiv ID**: 2601.22308
- **Source URL**: https://arxiv.org/abs/2601.22308
- **Reference count**: 40
- **Primary result**: Novel stealthy poisoning attacks bypass state-of-the-art defenses in regression; BayesClean defense outperforms alternatives at high poisoning ratios

## Executive Summary
This paper introduces stealthy poisoning attacks against regression models that balance attack effectiveness with detectability risk. Unlike prior work focusing on high-detectability attacks, the authors formulate a multiobjective bilevel optimization problem to create poisoning points that maximize MSE while minimizing detection risk. Experiments demonstrate that state-of-the-art defenses (TRIM, Huber, SEVER, Proda, DUTI) fail to mitigate stealthy attacks, with some defenses degrading performance. The paper proposes BayesClean, a novel defense leveraging Bayesian linear regression's predictive variance to reject poisoning points, which outperforms existing defenses when poisoning ratios exceed 20-30%.

## Method Summary
The attack uses multiobjective bilevel optimization to generate poisoning points that maximize validation MSE while minimizing detectability risk. The optimization balances two competing objectives via scalarization with parameter α, solved using Reverse-Mode Differentiation for hypergradient computation. Poisoning points are generated in mini-batches with projected gradient ascent. The detectability-risk function is based on loss differences between poisoned and clean models. BayesClean defense leverages Bayesian linear regression's predictive variance, classifying points into three sets based on their position relative to predictive mean ± kσ intervals, adapting rejection thresholds as poisoning increases.

## Key Results
- Stealthy poisoning attacks effectively bypass state-of-the-art defenses (TRIM, Huber, SEVER, Proda, DUTI)
- Defenses show negative test defense gain at high poisoning ratios under stealthy attacks
- BayesClean outperforms existing defenses at poisoning ratios >20-30%, even under stealthy attacks
- BayesClean requires no prior knowledge of poisoning ratios, unlike other defenses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiobjective bilevel optimization enables tunable stealth-effectiveness tradeoffs
- Mechanism: The attack formulation combines maximizing MSE and minimizing detectability risk into scalarized objective `A_d = αL(D_val, w*) - (1-α)R`. Bilevel structure accounts for model training process while optimizing poisoning points. Varying α ∈ [0,1] creates smooth transition between aggressive (α→1) and stealthy (α→0) attacks.
- Core assumption: Detectability-risk function accurately captures defender's detection behavior based on loss relative to clean model parameters
- Evidence anchors: [abstract] multiobjective bilevel optimization balancing effectiveness and stealth; [Section IV-A, Eq. 4] scalarization formula with α parameter
- Break condition: Defenses using detection strategies unrelated to loss-based outlier rejection

### Mechanism 2
- Claim: Normalization of competing objectives prevents dominance collapse
- Mechanism: Effectiveness (MSE) and detectability (loss-based risk) operate on different scales. Without normalization, one objective dominates optimization. Normalization uses reference values: `R_norm = R/R_ref` and `L_norm ≈ L(w*(i))/L(w*(i-1))` per-batch, enabling meaningful balance across α values.
- Core assumption: Reference values remain reasonable proxies for objective scales across different poisoning ratios
- Evidence anchors: [abstract] normalization method to balance competing objectives; [Section IV-A3] normalization approach with synthetic examples
- Break condition: Poisoning batches drastically shift objective landscape, causing normalization instability

### Mechanism 3
- Claim: Predictive variance under poisoning enables robust point rejection without prior knowledge of poisoning ratios
- Mechanism: BayesClean leverages Bayesian linear regression's predictive variance, which increases when training data deviates from underlying distribution. As poisoning points are added, noise precision estimate β⁻¹ increases, expanding predictive variance. Points classified into three sets based on position relative to predictive mean ± kσ intervals.
- Core assumption: Poisoning points systematically increase predictive variance in detectable way; clean points remain within tighter confidence bounds
- Evidence anchors: [abstract] BayesClean leverages predictive variance to reject poisoning points; [Section V-B] β⁻¹ updates in EM procedure; [Section V-D, Fig. 10] outperforms other defenses at >20-30% poisoning
- Break condition: Attackers craft poisoning points minimizing variance increase, staying close to predictive mean

## Foundational Learning

- Concept: **Bilevel optimization**
  - Why needed here: Attack formulation is inherently bilevel—outer problem optimizes poisoning points while inner problem represents model training. Understanding hypergradients is essential.
  - Quick check question: Can you explain why computing `∇_Xp A_d` requires differentiating through model's training process, and what implicit function theorem provides here?

- Concept: **Multiobjective scalarization**
  - Why needed here: Attack balances two objectives via weighted scalarization. Understanding tradeoff surfaces and how α controls operating point is critical.
  - Quick check question: If α=0.6 favors effectiveness over stealth, what happens to poisoning point distribution compared to α=0.2?

- Concept: **Bayesian linear regression and predictive uncertainty**
  - Why needed here: BayesClean relies on posterior distributions over weights and predictive variance decomposition. EM-based hyperparameter learning and how β changes under poisoning are central.
  - Quick check question: Why does predictive variance have two components (β⁻¹ and x*^T Σ_post x*), and which one responds to distribution shift from poisoning?

## Architecture Onboarding

- Component map:
  - Attack module: Multiobjective bilevel optimizer using Reverse-Mode Differentiation (RMD) for hypergradient computation; normalization layer for objective balancing; mini-batch poisoning point generator
  - Defense evaluation layer: Wrappers for TRIM, Huber, SEVER, Proda, DUTI; test defense gain computation
  - BayesClean defense: Bayesian LR with EM hyperparameter learning; predictive variance computation; three-set point classification (accept/flag/reject)
  - Experimental harness: Dataset loaders; LR and DNN model trainers; NMSE/defense gain metrics

- Critical path:
  1. Initialize poisoning points by cloning training samples
  2. For each batch: train model (inner problem), compute hypergradients via RMD, apply projected gradient ascent to poisoning points
  3. Compute normalization coefficients (R_ref at α=1, L_ref incrementally)
  4. For defense evaluation: apply each defense to poisoned training set, retrain, compute test NMSE and defense gain
  5. For BayesClean: learn λ, β via EM, compute posterior, classify points by predictive variance bounds

- Design tradeoffs:
  - α selection: Lower α = more stealthy but requires more poisoning points; higher α = more effective but detectable
  - Batch size vs. coordination: Larger batches enable coordinated optimization but increase computational cost
  - RMD truncation (T iterations): Fewer iterations = faster but less accurate hypergradients; more iterations = accurate but expensive
  - BayesClean thresholds (c₁, c₂): Tighter bounds = more false positives on clean data; looser bounds = miss poisoning points

- Failure signatures:
  - Attack failure: Test NMSE doesn't increase despite high poisoning ratio (check if normalization dominated by one objective)
  - Defense negative gain: Defense test NMSE > no-defense NMSE (likely at high poisoning ratios where clean/poisoned error distributions invert)
  - BayesClean over-rejection: Large I₃ set under low poisoning (check c₂ threshold tuning)
  - Gradient explosion in RMD: NaN values in hypergradients (reduce learning rate η or increase numerical stability)

- First 3 experiments:
  1. **Baseline attack effectiveness**: Run attack with α ∈ {1.0, 0.6, 0.3, 0.1} on Loan dataset with LR; plot test NMSE vs. poisoning ratio; verify that lower α produces smoother point distributions
  2. **Defense failure mode identification**: Apply TRIM, Huber, SEVER to poisoned models; identify poisoning ratio threshold where defense gain becomes negative; confirm this occurs at lower ratios for stealthier attacks
  3. **BayesClean threshold sensitivity**: Run BayesClean on Heart Disease dataset with varying c₂ ∈ {1.0, 1.5, 2.0} at 30% poisoning; measure false positive rate vs. true positive rate; compare against TRIM with known poisoning ratio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can uncertainty-based defense mechanisms be optimized specifically for Deep Neural Networks (DNNs) in regression tasks?
- Basis in paper: [explicit] Authors note defending DNNs is more challenging and explicitly call for further research on methods to defend DNNs in regression settings.
- Why unresolved: Proposed BayesClean uses Bayesian Linear Regression; results show it often fails or provides negative gain when applied to DNNs.
- What evidence would resolve it: Development of DNN-native defense using dropout or ensembles that maintains positive defense gain across all stealthy attack levels.

### Open Question 2
- Question: What is the precise mathematical threshold at which clean point errors exceed poisoning point errors in gradient-based defenses like TRIM?
- Basis in paper: [explicit] Authors state regarding TRIM's failure: "We will explore this hypothesis further in future work"
- Why unresolved: Paper relies on empirical observation without formal derivation of failure boundary, though hypothesizes model parameter shifting causes crossover.
- What evidence would resolve it: Theoretical proof defining poisoning ratio threshold or empirical curves showing intersection of clean and poisoned error rates.

### Open Question 3
- Question: Does the heuristic normalization of the effectiveness objective provide optimal trade-offs compared to a theoretically derived maximum?
- Basis in paper: [inferred] Authors use heuristic to normalize effectiveness objective because calculating true maximum is "difficult to obtain"
- Why unresolved: Unclear if heuristic introduces bias or sub-optimality in α-steered trade-offs between effectiveness and detectability.
- What evidence would resolve it: Comparison of attack optimization convergence and stealthiness using heuristic versus brute-force or analytical maximum estimation.

## Limitations

- Attack effectiveness against distribution-based defenses (not just loss-based outlier rejection) remains untested
- Normalization method's stability across drastically different poisoning ratios and datasets needs validation
- BayesClean defense's performance on DNNs is not demonstrated

## Confidence

- **High**: Bilevel optimization framework and hypergradient computation are mathematically sound and well-established
- **Medium**: Normalization technique's effectiveness in practice is supported by synthetic examples but needs more extensive validation
- **Medium**: Predictive variance-based detection in BayesClean is theoretically grounded but untested against sophisticated variance-aware attacks

## Next Checks

1. Test stealthy attack against distribution-based defenses (e.g., K-S test, isolation forest) that don't rely on loss thresholds
2. Evaluate normalization stability by running attack across 5x wider poisoning ratio ranges and comparing objective trajectories
3. Conduct adversarial training of poisoning points specifically to minimize predictive variance increase and measure BayesClean's true positive rate degradation