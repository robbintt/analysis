---
ver: rpa2
title: 'RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct
  Image Interpretation Model Shortcuts'
arxiv_id: '2509.08640'
source_url: https://arxiv.org/abs/2509.08640
tags:
- roentmod
- image
- scans
- pathology
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RoentMod is a counterfactual image editing framework that generates
  anatomically realistic chest X-rays with user-specified synthetic pathology while
  preserving original anatomical features. By combining RoentGen with a publicly available
  image-to-image modification model, RoentMod adds pathology to existing scans in
  under 2 seconds per image.
---

# RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts

## Quick Facts
- arXiv ID: 2509.08640
- Source URL: https://arxiv.org/abs/2509.08640
- Authors: Lauren H. Cooke; Matthias Jung; Jan M. Brendel; Nora M. Kerkovits; Borek Foldyna; Michael T. Lu; Vineet K. Raghu
- Reference count: 40
- Primary result: A counterfactual editing framework that improves diagnostic model specificity by 3-19% AUC by correcting shortcut learning

## Executive Summary
RoentMod is a synthetic image editing framework that adds user-specified pathologies to existing chest X-rays while preserving anatomical features. By combining RoentGen (a CXR generator) with a general image-to-image modification model, it creates counterfactual images in under 2 seconds per image. Reader studies showed 93% of generated images appeared realistic and 89-99% correctly incorporated specified findings. The framework revealed that state-of-the-art multi-task and foundation models frequently exploit off-target pathology as shortcuts, limiting specificity. Training with RoentMod-generated counterfactual images mitigated this vulnerability, improving model discrimination by 3-19% AUC in internal validation and 1-11% for 5 of 6 pathologies in external testing.

## Method Summary
RoentMod combines a pre-trained CXR generator (RoentGen) with a Stable Diffusion image-to-image pipeline without requiring additional training. The framework transplants RoentGen's weights into the img2img architecture, using a denoising loop to modify existing CXRs based on text prompts. Optimized parameters (strength=0.4, guidance_scale=4.0) balance pathology addition with anatomical preservation. The system generates counterfactual images where specific pathologies are added to "no finding" baseline images, enabling controlled experiments to identify shortcut learning in diagnostic models.

## Key Results
- 93% of generated images appeared realistic in reader studies
- 89-99% of images correctly incorporated specified findings
- State-of-the-art models showed shortcut learning, with 3-19% AUC improvement after training on counterfactual images
- Preserved anatomy comparable to real follow-up CXRs (pFID metrics)

## Why This Works (Mechanism)

### Mechanism 1: Architecture Transplantation for Zero-Shot Editing
RoentMod enables counterfactual editing of chest X-rays without requiring model retraining by combining distinct pre-trained components. The framework transplants the domain-specific knowledge (U-Net and text encoder weights) from RoentGen into the general Stable Diffusion Image-to-Image pipeline. The img2img pipeline provides the noise injection and denoising loop necessary to alter an existing image based on a text prompt, while the RoentGen weights ensure the alterations are anatomically plausible. The latent space of the general Stable Diffusion model is sufficiently aligned with the fine-tuned RoentGen weights to allow for seamless component swapping without fine-tuning.

### Mechanism 2: Shortcut Detection via Isolated Variable Manipulation
Adding a specific pathology to an image allows for the quantification of a model's reliance on spurious correlations (shortcuts). By generating a counterfactual image where only one variable (e.g., "edema") is added to a baseline "no finding" image, the framework creates a controlled experiment. If a multi-task model increases its prediction probability for an unrelated pathology (e.g., "hernia") when only edema was added, it evidences shortcut learning (relying on a generic "sick" signal rather than specific features). The synthetic addition of pathology is sufficiently localized and does not inadvertently alter image features associated with other pathologies.

### Mechanism 3: Specificity Enforcement via Counterfactual Augmentation
Training diagnostic models on counterfactual pairs improves discrimination by penalizing the mapping of generic "sickness" features to specific disease labels. The training paradigm presents the model with an image containing a synthetically added pathology (e.g., a lung mass) but explicitly labels co-occurring conditions as negative (or lower probability based on radiologist co-occurrence rates). This forces the model to decouple feature representations, learning that the features for "lung mass" do not justify a high prediction for "pleural effusion." The synthetic images are realistic enough that the feature representations learned from them transfer to real-world data distributions.

## Foundational Learning

- **Concept: Diffusion Models (Latent & Image-to-Image)**
  - **Why needed here:** RoentMod relies on Stable Diffusion's ability to iteratively denoise an image. Understanding the trade-off between *Guidance Scale* (adherence to text) and *Strength* (amount of noise added/alteration allowed) is critical for controlling the editing process.
  - **Quick check question:** If you set the `strength` parameter to 0 in an img2img pipeline, what is the output?

- **Concept: Shortcut Learning (Spurious Correlations)**
  - **Why needed here:** The paper aims to solve the issue where models predict outcomes based on easy-to-learn but irrelevant features (e.g., "sick patient" vibe) rather than the specific disease pathology.
  - **Quick check question:** Why might a model trained on data where 90% of pneumonia cases also have a chest tube falsely learn to detect the tube instead of the lung opacity?

- **Concept: Multi-Task Learning**
  - **Why needed here:** The target models being "stress-tested" predict multiple pathologies simultaneously. The paper exploits the tendency of these models to have "feature-sharing" where shortcuts in one task degrade specificity in others.
  - **Quick check question:** How does sharing a backbone encoder between different classification tasks make a model more vulnerable to a single spurious correlation?

## Architecture Onboarding

- **Component map:** Input CXR + Text Prompt -> VAE Encoder -> Noise Injection -> Denoising U-Net (RoentGen weights) -> VAE Decoder -> Output CXR
- **Critical path:** The parameter selection (Strength=0.4, Guidance=4.0). This balance is the primary bottleneck; too much strength destroys patient anatomy, too little ignores the pathology prompt.
- **Design tradeoffs:**
  - **Realism vs. Prompt Adherence:** Increasing guidance scale forces the model to draw the pathology but may degrade image quality.
  - **Identity vs. Editability:** Increasing strength allows for larger edits (e.g., adding a large mass) but risks losing the patient's original bone structure/identity.
  - **Co-occurrence Modeling:** The model struggles to add only one pathology without hallucinating physiologically linked conditions (e.g., adding edema often adds cardiomegaly).
- **Failure signatures:**
  - **Reverse Shortcut:** The model trained on counterfactuals may underpredict real pathologies that naturally co-occur, because the synthetic training data strictly isolated them.
  - **Hallucination:** The model may add tubes, lines, or text markers not requested in the prompt.
  - **Anatomy Drift:** High strength settings may alter the rotation of the chest or the shape of the ribcage.
- **First 3 experiments:**
  1. **Hyperparameter Sweep:** Run RoentMod on 5 "no finding" images across Strength [0.2-1.0] and Guidance [1.5-10]. Visually inspect to identify the "Goldilocks zone" where anatomy is preserved but pathologies appear.
  2. **Identity Preservation Check (pFID):** Calculate pairwise Fr√©chet Inception Distance (pFID) between original real images and their RoentMod counterparts vs. random pairs. Verify that "Same Person" pairs are statistically closer than "Different Person" pairs.
  3. **Stress Test Baseline:** Take a pre-trained model (e.g., TorchXRayVision), run inference on a batch of real "no finding" images, then run inference on their RoentMod "lung mass" counterparts. Plot the change in probability for Cardiomegaly. If it rises significantly, you have confirmed shortcut learning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the RoentMod framework effectively identify and mitigate shortcut learning in imaging modalities other than chest X-rays, such as CT or MRI?
- Basis in paper: The authors state that while they focused on CXR, they "expect that interpretation models for other imaging modalities will likely suffer from similar shortcuts; this will be tested in future work."
- Why unresolved: RoentMod relies on RoentGen, which is specifically fine-tuned for chest X-ray generation, and different modalities present unique constraints regarding resolution, 3D data, and texture.
- What evidence would resolve it: Successful application of the counterfactual editing and training pipeline on CT or MRI datasets, resulting in improved model generalization.

### Open Question 2
- Question: Would integrating structural causal models into the image generation process successfully reduce the unintended co-occurrence of pathologies?
- Basis in paper: The authors note that co-occurrence rates were higher than expected in synthetic scans and suggest that "structural causal models or region-specific editing could be used to improve RoentMod and further mitigate these correlations in future work."
- Why unresolved: The current diffusion-based generation relies on text-prompt conditioning which inherently captures training data correlations (e.g., cardiomegaly appearing with edema).
- What evidence would resolve it: A version of RoentMod utilizing causal modeling that generates synthetic scans with statistically independent pathology prevalence.

### Open Question 3
- Question: Can a multitask diagnostic model trained on a massive, multi-cohort dataset using the RoentMod training paradigm outperform current large foundation models?
- Basis in paper: The authors hypothesize that "a multitask diagnostic model trained on more examples across more cohorts with this RoentMod-supplemented training paradigm could potentially perform at or better than these larger models."
- Why unresolved: The current study was limited to the NIH-CXR 14 dataset for real training data, whereas foundation models are trained on much larger proprietary datasets.
- What evidence would resolve it: Discrimination metrics (AUC) from a RoentMod-trained model on a scale comparable to models like Ark+ or ElixrB.

### Open Question 4
- Question: Do subtle, human-invisible artifacts in counterfactual images inadvertently affect the quantification of shortcut learning?
- Basis in paper: The limitations section notes that "subtle changes may be present in counterfactually generated CXRs that are not visible to human readers, affecting our ability to accurately perform shortcut testing."
- Why unresolved: Radiologist validation confirms realism to the human eye, but deep learning models may detect pixel-level artifacts invisible to humans, potentially skewing stress-test results.
- What evidence would resolve it: Pixel-space analysis or frequency-domain comparisons between real and synthetic images to isolate non-semantic artifacts.

## Limitations

- Realism generalizability: Validation was conducted by a single radiologist without inter-rater reliability testing or testing on rare/complex findings
- Shortcuts quantification: Identifies shortcut learning qualitatively but lacks statistical measures of shortcut prevalence across entire dataset
- Counterfactual training risks: Acknowledges potential reverse-shortcut effects but does not empirically test whether models underpredict naturally co-occurring real pathologies

## Confidence

- **High Confidence**: Anatomical preservation metrics (pFID), basic functionality of RoentMod framework, general premise that synthetic editing can help identify shortcuts
- **Medium Confidence**: Reader study realism scores, AUC improvements in internal validation, co-occurrence modeling limitations
- **Low Confidence**: External validation results, real-world deployment considerations, long-term effectiveness of counterfactual training

## Next Checks

1. **Multi-reader Realism Assessment**: Re-evaluate 100 RoentMod-generated images with 3-5 independent radiologists using blinded randomized presentation to establish inter-rater reliability and identify failure modes.

2. **Distribution Shift Testing**: Test counterfactual-trained models on real-world datasets with natural pathology co-occurrence patterns to measure potential reverse-shortcut effects where models underpredict naturally co-occurring conditions.

3. **Prompt Robustness Analysis**: Systematically vary prompts for the same pathology (different wording, specificity levels) and measure consistency of generated outputs and their impact on shortcut detection reliability.