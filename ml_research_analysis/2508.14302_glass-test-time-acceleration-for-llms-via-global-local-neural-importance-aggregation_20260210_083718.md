---
ver: rpa2
title: 'GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance
  Aggregation'
arxiv_id: '2508.14302'
source_url: https://arxiv.org/abs/2508.14302
tags:
- arxiv
- griffin
- neurons
- methods
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces A/I-GLASS, a training-free method for dynamic
  FFN pruning in LLMs that aggregates global and local neural importance rankings
  using a Plackett-Luce maximum-likelihood framework. Unlike prior approaches that
  rely on static masks or costly predictors, GLASS fuses prompt-specific activation
  patterns with model-intrinsic statistics (activation magnitudes or impact scores)
  to identify critical neurons at inference time.
---

# GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation

## Quick Facts
- **arXiv ID:** 2508.14302
- **Source URL:** https://arxiv.org/abs/2508.14302
- **Reference count:** 33
- **Primary result:** Training-free dynamic FFN pruning using global-local neural importance aggregation achieves up to 45.10% lower perplexity and 25.26% lower KL divergence than prior training-free methods under 50% sparsity.

## Executive Summary
GLASS introduces a training-free method for dynamic FFN pruning in LLMs that aggregates global and local neural importance rankings using a Plackett-Luce maximum-likelihood framework. Unlike prior approaches relying on static masks or costly predictors, GLASS fuses prompt-specific activation patterns with model-intrinsic statistics to identify critical neurons at inference time. Evaluated across multiple models and benchmarks, GLASS demonstrates significant improvements in perplexity and KL divergence, particularly excelling in short-prompt, long-generation scenarios while requiring no offline training and adding no inference overhead.

## Method Summary
GLASS is a training-free method for dynamic FFN pruning that aggregates global and local neural importance rankings at inference time. The method uses a Plackett-Luce maximum-likelihood framework to fuse prompt-specific activation patterns with model-intrinsic statistics (activation magnitudes or impact scores) to identify critical neurons. Unlike prior approaches that rely on static masks or costly predictors, GLASS operates entirely at inference time without requiring any offline training, making it highly suitable for resource-constrained deployment scenarios.

## Key Results
- Achieves up to 45.10% lower perplexity compared to strongest training-free baseline at 50% sparsity
- Demonstrates 25.26% lower KL divergence under same sparsity conditions
- Particularly excels in short-prompt, long-generation scenarios across multiple model architectures

## Why This Works (Mechanism)
GLASS works by dynamically identifying and pruning less important neurons during inference through a sophisticated ranking aggregation mechanism. The method leverages the Plackett-Luce framework to combine global importance scores (derived from pre-computed model statistics) with local importance scores (computed from prompt-specific activations). This dual perspective allows the system to capture both general model behavior and context-specific neural activity patterns. By focusing computational resources on neurons with high aggregated importance, GLASS maintains model performance while achieving significant sparsity. The training-free nature eliminates the need for costly offline optimization while the dynamic aspect ensures adaptability to varying input contexts.

## Foundational Learning

**Plackett-Luce ranking aggregation**: A probabilistic framework for combining multiple ranked lists into a single consensus ranking. Needed to fuse global and local importance rankings in a statistically sound manner. Quick check: Verify that the combined ranking preserves relative ordering of top-k elements from both input rankings.

**Neural importance scoring**: Methods for quantifying individual neuron contribution to model outputs, typically based on activation magnitudes or gradient-based impact measures. Essential for identifying which neurons to preserve versus prune. Quick check: Confirm that importance scores correlate with downstream task performance when neurons are selectively removed.

**Dynamic sparsity vs static pruning**: The distinction between pruning decisions made at inference time based on input context versus fixed pruning masks determined during training. Critical for understanding GLASS's adaptive approach. Quick check: Compare performance variance across different input types between dynamic and static approaches.

## Architecture Onboarding

**Component map:** Input tokens → Prompt-specific activation computation → Local importance ranking → Global importance score retrieval → Plackett-Luce aggregation → Neuron selection mask → Pruned FFN computation → Output generation

**Critical path:** The most latency-sensitive components are the local importance ranking computation and the Plackett-Luce aggregation, as these must complete before FFN execution can begin. The global importance scores can be pre-computed and stored, minimizing runtime overhead.

**Design tradeoffs:** GLASS trades potential ranking computation overhead for the benefit of context-aware pruning, avoiding the accuracy degradation common in static approaches. The training-free design sacrifices the potential performance gains from fine-tuning but gains deployment flexibility and eliminates offline computational costs.

**Failure signatures:** Performance degradation typically manifests as increased perplexity on inputs where local importance rankings poorly align with global statistics, or when the Plackett-Luce aggregation fails to properly weight context-specific neural activity. Short prompts with atypical activation patterns may also trigger suboptimal pruning decisions.

**First experiments:** 1) Validate that individual local and global rankings correlate with task performance when used independently. 2) Test Plackett-Luce aggregation with synthetic rankings to verify consensus formation properties. 3) Measure end-to-end latency impact of the ranking and aggregation components on representative hardware.

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes neuron importance rankings remain sufficiently stationary across prompts for reliable Plackett-Luce aggregation
- Effectiveness on multilingual or domain-specific tasks remains untested
- May face scalability challenges for very large models (>100B parameters) due to computational overhead of maintaining and merging multiple importance rankings

## Confidence

- **High Confidence:** Experimental results demonstrating perplexity and KL divergence improvements over training-free baselines under controlled conditions (50% sparsity, English benchmarks)
- **Medium Confidence:** Claim of "no inference overhead" requires scrutiny, as Plackett-Luce aggregation and ranking computation may introduce latency in practice
- **Low Confidence:** Assertion that GLASS is "highly suitable for edge deployment" lacks validation in resource-constrained environments

## Next Checks

1. Test A/I-GLASS on non-English benchmarks and domain-specific tasks to assess robustness across diverse data distributions
2. Evaluate the method on models with >100B parameters to quantify computational overhead of global-local importance aggregation
3. Measure end-to-end latency, memory usage, and energy consumption on edge devices to validate practicality in resource-constrained settings