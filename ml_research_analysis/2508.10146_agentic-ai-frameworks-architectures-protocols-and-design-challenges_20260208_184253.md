---
ver: rpa2
title: 'Agentic AI Frameworks: Architectures, Protocols, and Design Challenges'
arxiv_id: '2508.10146'
source_url: https://arxiv.org/abs/2508.10146
tags:
- agent
- agents
- memory
- frameworks
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a systematic review and comparative analysis
  of leading Agentic AI frameworks (CrewAI, LangGraph, AutoGen, Semantic Kernel, Agno,
  Google ADK, MetaGPT), focusing on architectural principles, communication protocols,
  memory management, safety guardrails, and alignment with service-oriented computing.
  It identifies key limitations and proposes future research directions.
---

# Agentic AI Frameworks: Architectures, Protocols, and Design Challenges

## Quick Facts
- **arXiv ID:** 2508.10146
- **Source URL:** https://arxiv.org/abs/2508.10146
- **Reference count:** 29
- **Primary result:** Systematic review and comparative analysis of leading Agentic AI frameworks, identifying key limitations and proposing future research directions for interoperability and standardization.

## Executive Summary
This paper provides a comprehensive survey of modern Agentic AI frameworks including CrewAI, LangGraph, AutoGen, Semantic Kernel, Agno, Google ADK, and MetaGPT. It analyzes their architectural principles, communication protocols, memory management systems, and safety guardrails while mapping their alignment with service-oriented computing paradigms. The study identifies significant gaps in interoperability, standardization, and runtime agent discovery, proposing that universal protocols and benchmarks are needed for scalable multi-agent ecosystems.

## Method Summary
The study employs a qualitative feature extraction approach based on systematic documentation review of seven prominent Agentic AI frameworks and four communication protocols. The methodology constructs architectural taxonomies and comparison tables without empirical testing, focusing on identifying core components, capabilities, and limitations through feature analysis. No training algorithms or datasets are involved in this systematic review methodology.

## Key Results
- Modern agentic frameworks share core components (LLM, tools, memory, guardrails) but lack standardized interoperability protocols
- Memory management approaches vary significantly, with no consensus on optimal short-term vs. long-term storage strategies
- Service computing integration remains incomplete, with critical gaps in runtime discovery and dynamic composition capabilities
- Communication protocols (MCP, A2A, ANP, Agora) aim to enable agent interoperability but face semantic heterogeneity challenges

## Why This Works (Mechanism)

### Mechanism 1: Iterative Reasoning-Action Loops (The ReAct Pattern)
Modern agents achieve goal-directed autonomy by cyclically interleaving reasoning traces with external actions. The LLM generates a "Thought" (plan decomposition), executes an "Action" (tool call), and processes an "Observation" (tool output) to update internal state, persisting until a stopping criterion is met. This loop fails if the agent encounters uninterpretable tool errors or exceeds the model's effective context window, leading to hallucination or goal drift.

### Mechanism 2: State Preservation via Memory Systems
Agents exhibit context-aware behavior by retrieving relevant historical data to augment immediate prompt context. Interactions are encoded and stored in vector stores or databases, with retrieval mechanisms querying this store to inject pertinent past experiences into the LLM's context window. The mechanism degrades if retrieval latency introduces unacceptable lag or if "stale" memory pollutes the context with outdated preferences.

### Mechanism 3: Decoupled Interoperability via Standardized Protocols
Scalable multi-agent ecosystems require standardized communication protocols (MCP, A2A) to decouple agent logic from transport implementation. Agents interact through structured schemas rather than ad-hoc prompts, with protocols handling capability discovery and message routing. The mechanism fails if semantic heterogeneity persists, requiring complex translation layers that negate standardization benefits.

## Foundational Learning

- **Concept: ReAct / Chain-of-Thought Prompting**
  - **Why needed here:** This is the cognitive engine of modern agents. Without understanding that the model "thinks" step-by-step before acting, one cannot debug why an agent chose a specific (possibly erroneous) tool.
  - **Quick check question:** Can you distinguish between the *observation* the agent received and the *thought* it generated in response?

- **Concept: Service-Oriented Architecture (SOA) vs. Monoliths**
  - **Why needed here:** The paper frames Agentic AI as an evolution of service computing. Understanding concepts like "Discovery," "Publishing," and "Loose Coupling" is essential to evaluate the limitations of current frameworks.
  - **Quick check question:** If an agent changes its API endpoint, does the system crash (tight coupling) or does the registry update route the traffic (loose coupling)?

- **Concept: Context Window vs. Persistent Memory**
  - **Why needed here:** Engineers often confuse the model's finite context window (RAM) with persistent storage (Disk/Memory). The paper explicitly categorizes Short-Term vs. Long-Term memory, which dictates architectural choices.
  - **Quick check question:** If a conversation exceeds 100 turns, where does the data go, and how does the agent recall the first turn?

## Architecture Onboarding

- **Component map:** Core (The Brain): LLM (Reasoning Engine) → Periphery (The Body): Tools (Functions/APIs), Memory (Vector DB/State Store) → Governance (The Constraints): Guardrails (Validators/Retry Logic) → Interface (The Language): Protocols (MCP/A2A)

- **Critical path:**
  1. Define the Role: Establish the system prompt and persona
  2. Select Protocol/Transport: Determine how agents talk
  3. Implement Memory Strategy: Choose between stateless, short-term, or RAG-based long-term memory
  4. Add Guardrails: Wrap tools and outputs in validation logic

- **Design tradeoffs:**
  - AutoGen/CrewAI: High ease of use for multi-agent collaboration, but higher abstraction opacity
  - LangGraph: Steeper learning curve (graph state machines), but superior control over flow and determinism
  - Protocols: Implementing A2A/ANP adds overhead but is required for cross-framework interoperability; native integration is faster but creates silos

- **Failure signatures:**
  - Infinite Loops: Agent gets stuck in "Thought-Action" cycles without terminating
  - Context Overflow: Agent "forgets" initial instructions as conversation history grows
  - Tool Hallucination: Agent attempts to call a function with incorrect parameters

- **First 3 experiments:**
  1. Single Agent ReAct Loop: Implement a simple agent that uses a calculator tool and observe "Thought" vs. "Action" traces
  2. Memory Injection: Configure a CrewAI agent with memory=True, run a session, restart it, and query past data to verify persistence
  3. Protocol Ping: Set up two minimal agents where Agent A sends a "Task" object to Agent B via MCP/A2A and inspect the JSON payload structure

## Open Questions the Paper Calls Out

- **Open Question 1:** How can universal communication protocols be developed to resolve interoperability gaps between disparate Agentic AI frameworks?
  - **Basis in paper:** The conclusion identifies "developing universal agent communication protocols to enhance interoperability," while Section V notes frameworks operate in silos with incompatible abstractions.
  - **Why unresolved:** Current frameworks use distinct models for tasks and memory, preventing seamless code reuse or tool portability without significant translation layers.
  - **What evidence would resolve it:** A standardized protocol implementation allowing an agent from one framework to natively invoke a tool or agent from another without custom middleware.

- **Open Question 2:** What architectural models are required to enable dynamic runtime discovery and collaboration among agents in multi-agent systems?
  - **Basis in paper:** Section V highlights "No runtime discovery" as a critical limitation, noting agents cannot dynamically discover peers and interactions must be statically defined.
  - **Why unresolved:** Existing systems rely on pre-defined workflows, which limits scalability and prevents emergent cooperation as new agents cannot join the system ad-hoc.
  - **What evidence would resolve it:** An agent registry system where agents can publish capabilities and query peers in real-time, facilitating dynamic team formation during task execution.

- **Open Question 3:** What standardized benchmarks are necessary to ensure objective comparison and reproducibility across different Agentic AI frameworks?
  - **Basis in paper:** Section VI explicitly lists "establishing standardized benchmarks for objective comparison and reproducibility" as a key future direction.
  - **Why unresolved:** The paper notes a lack of systematic understanding regarding how frameworks differ in practical capabilities, making scientific comparison difficult.
  - **What evidence would resolve it:** A unified evaluation suite providing quantitative metrics for memory efficiency, coordination latency, and task success rates across frameworks.

## Limitations

- The systematic review relies entirely on documentation analysis rather than empirical testing of framework capabilities
- Critical details like specific version numbers for the frameworks reviewed are absent, impacting reproducibility
- Evaluation criteria for framework comparisons remain undefined, making objective assessment difficult
- Claims about interoperability gaps lack quantitative benchmarks comparing integration effort across frameworks

## Confidence

- **High Confidence:** The architectural taxonomy and identification of core components across frameworks are well-established and clearly documented
- **Medium Confidence:** The comparison of communication protocols and their stated purposes is accurate, though practical implementation challenges may vary
- **Low Confidence:** Specific gaps identified in service computing integration lack concrete metrics or case studies demonstrating severity in production environments

## Next Checks

1. **Version Drift Verification:** Install exact framework versions analyzed and reproduce basic agent examples to confirm API stability and feature support matches paper's claims
2. **Interoperability Benchmark:** Implement controlled experiment delegating identical tasks between frameworks using MCP and A2A protocols, measuring integration overhead and success rates to quantify interoperability gaps
3. **Memory System Stress Test:** Design multi-turn conversation exceeding typical context windows, implementing both short-term and long-term memory approaches to empirically validate the taxonomy and identify practical failure modes