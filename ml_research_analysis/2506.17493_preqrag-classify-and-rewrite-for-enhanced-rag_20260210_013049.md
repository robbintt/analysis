---
ver: rpa2
title: PreQRAG -- Classify and Rewrite for Enhanced RAG
arxiv_id: '2506.17493'
source_url: https://arxiv.org/abs/2506.17493
tags:
- question
- retrieval
- questions
- context
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PreQRAG, a retrieval-augmented generation
  system that improves performance by classifying and rewriting questions before retrieval.
  It distinguishes between single-document and multi-document questions, using question
  rewriting for single-document queries and decomposition for multi-document queries
  to optimize retrieval.
---

# PreQRAG -- Classify and Rewrite for Enhanced RAG

## Quick Facts
- arXiv ID: 2506.17493
- Source URL: https://arxiv.org/abs/2506.17493
- Reference count: 13
- Second place in SIGIR 2025 LiveRAG Challenge

## Executive Summary
PreQRAG introduces a novel retrieval-augmented generation system that enhances performance through strategic question preprocessing. The system classifies incoming questions as either single-document or multi-document queries, then applies targeted processing: question rewriting for single-document queries and decomposition for multi-document queries. This preprocessing approach optimizes the retrieval stage by ensuring queries are appropriately formatted for the complexity of the information needed. The system employs a hybrid retrieval approach combining dense and sparse models, followed by reranking and answer generation, achieving strong performance metrics in the SIGIR 2025 LiveRAG Challenge.

## Method Summary
PreQRAG implements a two-stage preprocessing pipeline for questions before retrieval. First, it classifies questions into single-document (requiring information from one source) or multi-document (requiring information from multiple sources) categories. For single-document questions, it applies question rewriting to optimize retrieval effectiveness. For multi-document questions, it employs decomposition techniques to break down complex queries into manageable sub-queries. The system then uses a hybrid retrieval approach combining both dense and sparse retrieval models to fetch relevant documents, followed by a reranking stage and final answer generation. This targeted preprocessing approach aims to improve retrieval quality and overall RAG performance by ensuring questions are optimally structured for the retrieval task.

## Key Results
- Achieved second place in the SIGIR 2025 LiveRAG Challenge
- Strong equivalence metric of 0.977
- High relevance score of 0.884
- Good faithfulness performance

## Why This Works (Mechanism)
The effectiveness of PreQRAG stems from its intelligent preprocessing of questions before retrieval. By distinguishing between single-document and multi-document queries, the system can apply the most appropriate transformation technique to each question type. Single-document questions benefit from rewriting to clarify intent and improve retrieval precision, while multi-document questions are decomposed to handle complexity and ensure comprehensive information gathering. This targeted approach prevents the common pitfall of treating all questions identically, which can lead to suboptimal retrieval results. The hybrid retrieval approach then leverages both semantic and keyword-based matching, capturing a broader range of relevant documents than either method alone.

## Foundational Learning

Dense Retrieval
- Why needed: Captures semantic similarity between queries and documents using vector representations
- Quick check: Verify that dense retrievers can handle synonymy and paraphrase matching effectively

Sparse Retrieval
- Why needed: Leverages exact keyword matching for precision in retrieving specific information
- Quick check: Confirm that sparse retrievers perform well on fact-based questions with specific terminology

Question Classification
- Why needed: Determines whether a question requires information from one or multiple documents
- Quick check: Test classification accuracy on diverse question types to ensure proper routing

Question Rewriting
- Why needed: Refines single-document queries to improve retrieval precision and relevance
- Quick check: Compare retrieval results before and after rewriting to measure improvement

Query Decomposition
- Why needed: Breaks down complex multi-document questions into simpler sub-queries
- Quick check: Validate that decomposed queries collectively capture all aspects of the original question

## Architecture Onboarding

Component Map: Question -> Classification -> (Rewrite/Decompose) -> Hybrid Retrieval -> Reranking -> Answer Generation

Critical Path: The most critical path is Question Classification -> (Rewrite/Decompose) -> Hybrid Retrieval, as errors in these stages directly impact the quality of retrieved documents and final answers.

Design Tradeoffs:
1. Classification accuracy vs. processing speed - more sophisticated classifiers may improve routing but increase latency
2. Hybrid retrieval balance - determining optimal weighting between dense and sparse retrieval components
3. Rewriting depth vs. query simplicity - extensive rewriting may improve precision but risk losing original intent

Failure Signatures:
1. Misclassification of question types leading to inappropriate processing
2. Over-rewriting causing loss of original query intent
3. Decomposition that fails to capture all aspects of multi-document questions

First 3 Experiments:
1. Compare retrieval performance with and without question classification to measure its impact
2. Test different hybrid retrieval weightings to find optimal balance between dense and sparse components
3. Evaluate the effect of varying rewriting depth on retrieval precision for single-document questions

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability beyond the LiveRAG Challenge dataset
- No extensive ablation studies on individual component contributions
- Unclear performance when individual components fail or underperform
- Results based on a specific challenge environment that may not represent real-world scenarios

## Confidence
- Performance improvement claims: Medium-High
- Architectural novelty claims: Medium-Low
- Generalizability to real-world applications: Low-Medium

## Next Checks
1. Conduct ablation studies removing each component (classification, rewriting, decomposition) to quantify their individual contributions to performance
2. Test PreQRAG on multiple RAG benchmark datasets beyond LiveRAG to assess generalizability across different domains and question types
3. Evaluate the system's performance with different retriever models (e.g., alternative dense and sparse retrievers) to determine robustness to retrieval component variations