---
ver: rpa2
title: Quantifying Memory Use in Reinforcement Learning with Temporal Range
arxiv_id: '2512.06204'
source_url: https://arxiv.org/abs/2512.06204
tags:
- temporal
- range
- memory
- copyk
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Temporal Range, a model-agnostic metric that
  quantifies how much a trained reinforcement learning policy relies on past observations.
  The metric computes a magnitude-weighted average look-back by aggregating Jacobian
  norms of the policy output with respect to historical inputs, providing a scalar
  measure of memory dependence for each sequence.
---

# Quantifying Memory Use in Reinforcement Learning with Temporal Range

## Quick Facts
- arXiv ID: 2512.06204
- Source URL: https://arxiv.org/abs/2512.06204
- Reference count: 14
- Primary result: Temporal Range is a model-agnostic metric that quantifies how much a trained RL policy relies on past observations through Jacobian-based first-order sensitivities.

## Executive Summary
This paper introduces Temporal Range, a model-agnostic metric that quantifies how much a trained reinforcement learning policy relies on past observations. The metric computes a magnitude-weighted average look-back by aggregating Jacobian norms of the policy output with respect to historical inputs, providing a scalar measure of memory dependence for each sequence. The authors validate Temporal Range across four architectures (LEM, GRU, LSTM, LinOSS) on POPGym diagnostic and control tasks, demonstrating its ability to align with ground-truth lag in Copy-k tasks and match minimum history windows required for near-optimal performance.

## Method Summary
Temporal Range computes first-order sensitivities of vector outputs to past inputs via Jacobian blocks, aggregates them into a temporal influence profile, and summarizes this as a magnitude-weighted average lag. For a policy unrolled over T steps, Jacobian blocks J_{s,t} = ∂y_s/∂x_t are computed for all future outputs s > t. Per-step weights w_t are calculated as the mean of matrix norms across future timesteps, then aggregated into a magnitude-weighted average lag \hat{ρ}_T = Σ w_t(T-t) / Σ w_t. When the original policy blocks gradients, a compact Long Expressive Memory (LEM) policy trained on the same task serves as a proxy for computing Temporal Range.

## Key Results
- Temporal Range remains near zero in fully observed control tasks and scales with ground-truth lag in Copy-k tasks
- The metric aligns with minimum history windows required for near-optimal performance as confirmed by window ablations
- GRU shows shorter effective memory on near-Markov tasks (ˆρT ≈2 on CartPole) while LEM and LSTM rely on longer history (ˆρT ≈10) despite being unnecessary for performance

## Why This Works (Mechanism)

### Mechanism 1: Jacobian-Based Temporal Sensitivity Aggregation
- Claim: First-order policy sensitivities to past observations quantify effective memory use
- Core assumption: Local linearization (Jacobian at evaluation point) faithfully summarizes policy's temporal dependence
- Evidence: Abstract defines computing Jacobian blocks and forming magnitude-weighted average lag; Section 3.1 provides formal definition
- Break condition: Strong higher-order temporal interactions may cause first-order Jacobians to underestimate true dependence

### Mechanism 2: Axiomatic Uniqueness for Linear Maps
- Claim: For vector-output linear maps, magnitude-weighted average lag is uniquely determined by natural axioms
- Core assumption: Axioms correctly capture what "range" should mean for temporal dependence
- Evidence: Abstract mentions linear setting axiomatization; Section 3.2 and Appendix A.3 provide formal propositions
- Break condition: Alternative axioms may suggest different functional forms

### Mechanism 3: LEM Proxy for Non-Differentiable Policies
- Claim: Compact LEM policy trained on same task provides reliable Jacobians when original policy blocks gradients
- Core assumption: Proxy learns similar temporal dependencies as original policy
- Evidence: Abstract mentions LEM proxy usage; Section 4 describes empirical validation showing proxy TR increases with k in Copy-k
- Break condition: Proxy failure to learn task or learn different strategy could decouple proxy TR from true memory use

## Foundational Learning

- Concept: Reverse-Mode Automatic Differentiation (Backpropagation)
  - Why needed here: Temporal Range computation requires Jacobians; reverse-mode autodiff efficiently computes these sensitivities in one backward pass
  - Quick check question: Given a policy unrolled for T steps, how many forward/backward passes are needed to compute all Jacobian blocks J_{s,t} for s > t?

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: POMDPs require memory; understanding Markov vs non-Markov observations clarifies why TR should grow under partial observability
  - Quick check question: If observations fully determine the state, what should Temporal Range approximately equal?

- Concept: Matrix Norms (Frobenius, Operator)
  - Why needed here: Aggregating Jacobian blocks requires converting matrices to scalars; choice of norm affects temporal influence profile
  - Quick check question: Why might Frobenius norm be preferred over spectral norm for averaging Jacobian magnitudes across multiple output dimensions?

## Architecture Onboarding

- Component map: Policy network (MLP/RNN/SSM) → unrolled over T steps → outputs y_1,...,y_T → Jacobian computation → w_t aggregation → \hat{ρ}_T calculation
- Critical path: 1) Ensure policy differentiable w.r.t. observation inputs 2) Implement reverse-mode autodiff for Jacobian blocks 3) Choose aggregation operator L 4) Average \hat{ρ}_T over multiple rollouts
- Design tradeoffs: Window length T vs compute; Mean vs max aggregation; Frobenius vs operator norm; Proxy vs direct computation
- Failure signatures: \hat{ρ}_T ≈ 0 on memory-requiring tasks; \hat{ρ}_T large on near-Markov tasks; High variance across rollouts; Proxy \hat{ρ}_T doesn't track ground truth
- First 3 experiments: 1) Copy-k validation with k∈{1,3,5,10} 2) Markov vs partially observed comparison 3) Window ablation correlation with performance recovery

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Temporal Range be effectively used as a regularization term during training to penalize unnecessary memory dependence?
- Basis in paper: Conclusion mentions future work on regularizers penalizing unnecessary range
- Why unresolved: Paper validates TR strictly as post-hoc diagnostic metric without exploring gradient dynamics when incorporated into loss
- What evidence would resolve it: Training curves showing \hat{ρ}_T penalty successfully minimizes effective context length while maintaining near-optimal return

### Open Question 2
- Question: Does divergence between policy-head and value-head Temporal Ranges predict training instability or sensitivity to GAE lambda?
- Basis in paper: Section 3.5 notes value heads often yield larger ranges and gap helps explain unstable advantage estimates
- Why unresolved: Empirical section focuses on action-vector outputs; theoretical link between range mismatch and advantage estimation variance remains qualitative
- What evidence would resolve it: Correlation analysis showing larger policy-value range gap corresponds to higher variance in advantage estimates

### Open Question 3
- Question: How does first-order sensitivity measured by Temporal Range align with causal dependence determined by intervention?
- Basis in paper: Conclusion lists causal perturbation checks as future work
- Why unresolved: Jacobian-based metrics can highlight active pathways that may not be strictly necessary; conversely might miss non-differentiable dependencies
- What evidence would resolve it: Comparison study where observations at timesteps with high influence weights w_t are perturbed to verify predicted changes

## Limitations
- Axiomatic uniqueness proof applies only to linear case; empirical validation for nonlinear policies limited to case studies
- Proxy LEM computation assumes proxy learns similar temporal strategy as original policy
- Choice of Frobenius norm and mean aggregation validated empirically but not theoretically justified as optimal

## Confidence
- High confidence: TR remains invariant under uniform rescaling, and single-step calibration (R1-n) works as claimed
- Medium confidence: TR correlates with ground-truth lag in Copy-k and aligns with window-ablation performance knees
- Medium confidence: TR discriminates memory usage across architectures and scales with partial observability

## Next Checks
1. Apply TR to broader set of memory-intensive RL benchmarks (e.g., Visual Doom, Atari with flickering sprites) to test robustness beyond POPGym
2. Systematically vary Jacobian aggregation operator and norm across architectures to identify combinations best tracking ground-truth lag
3. Train proxy LEM with different hyperparameters and compare proxy TR stability to ensure proxy choice doesn't introduce artifacts