---
ver: rpa2
title: 'Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis
  for SLMs'
arxiv_id: '2509.24107'
source_url: https://arxiv.org/abs/2509.24107
tags:
- zhang
- training
- tool
- search
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Fathom-DeepResearch introduces a novel agentic system for long-horizon
  information retrieval and synthesis using small language models (SLMs). The system
  consists of two specialized 4B models: Fathom-Search-4B, optimized for evidence-based
  web investigation, and Fathom-Synthesizer-4B, which converts search traces into
  structured DeepResearch reports.'
---

# Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs

## Quick Facts
- arXiv ID: 2509.24107
- Source URL: https://arxiv.org/abs/2509.24107
- Reference count: 13
- Fathom-DeepResearch achieves state-of-the-art open-weights performance on DeepSearch benchmarks using 4B models

## Executive Summary
Fathom-DeepResearch introduces an agentic system for long-horizon information retrieval and synthesis using small language models (SLMs). The system employs two specialized 4B models: Fathom-Search-4B for evidence-based web investigation and Fathom-Synthesizer-4B for converting search traces into structured DeepResearch reports. Through innovations including DUETQA dataset generation, RAPO RLVR training, and steerable reward functions, the system achieves superior performance on established benchmarks while maintaining generalizability to reasoning tasks.

## Method Summary
The system employs a two-stage architecture with Fathom-Search-4B optimized for evidence-based web investigation and Fathom-Synthesizer-4B for structured report generation. Training utilizes DUETQA, a 5K-sample dataset generated through multi-agent self-play enforcing search dependency, combined with RAPO (RL with Policy Option) for stabilized RLVR training. A steerable step-level reward function controls exploration depth and verification behavior. The system demonstrates state-of-the-art open-weights performance across multiple benchmarks including SimpleQA, FRAMES, WebWalker, Seal0, and MuSiQue, while generalizing to reasoning tasks like HLE, AIME-25, GPQA-Diamond, and MedQA.

## Key Results
- Achieves state-of-the-art open-weights performance on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue)
- Outperforms most closed-source proprietary systems on DeepResearch-Bench in comprehensive, citation-dense report generation
- Generalizes to reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA

## Why This Works (Mechanism)
The system's effectiveness stems from specialized model optimization for distinct retrieval and synthesis tasks, combined with reinforcement learning that addresses multi-turn tool use instability through RAPO. The steerable reward function enables precise control over exploration depth and verification behavior, while the DUETQA dataset ensures search dependency learning. The 4B parameter models achieve competitive performance through task-specific optimization rather than brute-force scaling.

## Foundational Learning
- **DUETQA Dataset Generation**: Multi-agent self-play creates synthetic training data enforcing search dependency - needed for realistic long-horizon training scenarios, quick check: verify synthetic traces match real user behavior patterns
- **RAPO Algorithm**: Stabilized RLVR for multi-turn tool use addresses training instability - needed for reliable learning in complex retrieval sequences, quick check: compare convergence stability against standard RLVR
- **Steerable Reward Functions**: Step-level reward control for exploration and verification - needed for adaptable behavior tuning, quick check: test reward sensitivity across different depth targets
- **Two-Model Architecture**: Specialized search vs. synthesis roles - needed for task decomposition, quick check: measure performance degradation when using single model for both tasks
- **4B Parameter Optimization**: Efficient model scaling for specific tasks - needed for practical deployment, quick check: benchmark against larger models on resource-constrained environments
- **Evidence-Based Retrieval**: Web investigation with citation tracking - needed for verifiable outputs, quick check: measure citation accuracy and completeness

## Architecture Onboarding

**Component Map:** Fathom-Search-4B -> Web Investigation -> Search Traces -> Fathom-Synthesizer-4B -> Structured Reports

**Critical Path:** Input query → Fathom-Search-4B (multi-turn web investigation) → Search traces with citations → Fathom-Synthesizer-4B → Structured DeepResearch report

**Design Tradeoffs:** Small 4B models chosen over larger alternatives for efficiency and specialization, accepting potential capability limitations for deployment practicality. Multi-agent synthetic data generation trades realism for scalability and control. Two-model architecture increases complexity but enables task-specific optimization.

**Failure Signatures:** Inconsistent citations indicate search model degradation; missing verification steps suggest reward function misalignment; incoherent report structure points to synthesis model failures; poor cross-domain generalization reveals training data limitations.

**First 3 Experiments:** 1) Run simple query through full pipeline and verify structured output format, 2) Test search model's citation accuracy on known fact-checking tasks, 3) Evaluate synthesis model's ability to organize multi-source information coherently

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance gains heavily dependent on specialized training pipeline and curated datasets, limiting generalizability to domains without similar resource investment
- RLVR approach introduces additional training complexity requiring substantial compute resources
- Long-term reliability in real-world deployment remains uncertain due to benchmark-focused evaluation

## Confidence

**High Confidence:** Core technical contributions (DUETQA dataset, RAPO algorithm, two-model architecture) are well-documented and reproducible. Benchmark performance on standardized evaluations appears reliable.

**Medium Confidence:** "State-of-the-art open-weights" claims require careful interpretation due to evaluation protocol differences and potential data contamination. Generalization to reasoning tasks needs broader validation.

**Low Confidence:** Real-world deployment reliability remains uncertain as evaluation focuses on controlled benchmark environments rather than dynamic, adversarial web conditions.

## Next Checks
1. **Cross-domain generalization test:** Evaluate system on three additional domains (legal research, technical documentation, scientific literature review) not represented in training data
2. **Robustness evaluation:** Test performance under adversarial conditions including unreliable sources, contradictory information, and dynamically changing web content
3. **Resource efficiency analysis:** Measure computational requirements, memory usage, and inference time across the full retrieval-synthesis pipeline for practical deployment guidance