---
ver: rpa2
title: 'ADAPT: Actively Discovering and Adapting to Preferences for any Task'
arxiv_id: '2504.04040'
source_url: https://arxiv.org/abs/2504.04040
tags:
- preferences
- contains
- user
- such
- eggs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Reflection-DPO trains a student LLM to follow user preferences\
  \ by imitating a privileged teacher while actively asking questions when information\
  \ is missing. It uses a reflection mechanism to determine when a question will help\
  \ predict the teacher\u2019s action, and employs Direct Preference Optimization\
  \ to fine-tune the student."
---

# ADAPT: Actively Discovering and Adapting to Preferences for any Task

## Quick Facts
- **arXiv ID**: 2504.04040
- **Source URL**: https://arxiv.org/abs/2504.04040
- **Reference count**: 40
- **Primary result**: Reflection-DPO achieves 44.1% preference satisfaction for seen personas and 42.9% for unseen personas, outperforming baseline by 6.1%

## Executive Summary
Reflection-DPO introduces a novel approach for training large language models to adapt to user preferences in long-horizon household tasks. The method trains a student LLM to imitate a privileged teacher LLM while actively asking questions when information is missing, using a reflection mechanism to determine when questioning will be beneficial. The approach employs Direct Preference Optimization to fine-tune the student model based on human preference data, enabling it to handle tasks where preferences are partially observable or hidden.

## Method Summary
Reflection-DPO combines active questioning with preference-based fine-tuning to create adaptable AI agents. The system trains a student LLM to follow user preferences by imitating a privileged teacher LLM that has access to complete information. A reflection mechanism determines when the student should ask questions to fill information gaps that would help predict the teacher's actions. After collecting interaction data through this process, Direct Preference Optimization (DPO) fine-tunes the student model using human preference data. The method is evaluated on ADAPT, a benchmark of long-horizon household tasks with hidden user preferences, demonstrating significant improvements in preference satisfaction compared to zero-shot approaches.

## Key Results
- Achieves 44.1% preference satisfaction rate for seen personas
- Achieves 42.9% preference satisfaction rate for unseen personas
- Outperforms zero-shot chain-of-thought baseline by 6.1%
- Reduces questions asked per interaction from 22 to approximately 10

## Why This Works (Mechanism)
The reflection mechanism acts as a gating function that determines when additional information will improve the student's ability to predict the teacher's actions. By only asking questions when the reflection score indicates potential benefit, the system avoids unnecessary queries while still gathering critical preference information. The Direct Preference Optimization fine-tuning then reinforces the behaviors that align with human preferences, creating a feedback loop that improves performance over time.

## Foundational Learning
- **Large Language Model Imitation**: Why needed - To leverage powerful pretrained models for task execution; Quick check - Can the student accurately reproduce teacher behaviors in simple scenarios?
- **Active Questioning**: Why needed - To gather missing preference information without overwhelming users; Quick check - Does the reflection mechanism correctly identify when questions are beneficial?
- **Direct Preference Optimization**: Why needed - To fine-tune models based on human preferences rather than just accuracy; Quick check - Does DPO improve preference satisfaction compared to standard fine-tuning?
- **Long-horizon Task Planning**: Why needed - Household tasks require multi-step reasoning and planning; Quick check - Can the system maintain coherent plans across extended interactions?
- **Hidden Preference Handling**: Why needed - Real-world preferences are often implicit or partially observable; Quick check - Does the system correctly infer preferences from limited observations?

## Architecture Onboarding

**Component Map**: User -> Student LLM -> Reflection Mechanism -> Question Generator -> Teacher LLM -> Preference Data -> DPO Fine-tuner -> Student LLM

**Critical Path**: User preference input → Student LLM action → Reflection check → Question (if needed) → Teacher LLM response → Preference data collection → DPO fine-tuning → Updated student model

**Design Tradeoffs**: The system balances between asking too many questions (which burdens users) and asking too few (which risks incorrect assumptions). The reflection mechanism's threshold setting is critical - too high and the system misses important information, too low and it becomes overly verbose. The choice of DPO over other fine-tuning methods prioritizes preference alignment over raw task completion accuracy.

**Failure Signatures**: The system may fail when reflection scores are consistently miscalibrated, leading to either excessive questioning or insufficient information gathering. It may also struggle with preferences that are highly contextual or change dynamically during interactions. Additionally, if the teacher LLM's preferences don't align well with human preferences, the fine-tuning process may reinforce suboptimal behaviors.

**First Experiments**: 1) Test reflection mechanism on simple preference inference tasks with known ground truth; 2) Evaluate question efficiency by comparing information gain per question asked; 3) Validate DPO fine-tuning by measuring preference satisfaction before and after training.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance still leaves significant room for improvement (44.1% satisfaction is far from perfect)
- The approach requires substantial human preference data for effective fine-tuning
- May struggle with preferences that are highly nuanced or culturally specific
- Computational overhead of running both student and teacher LLMs during training

## Confidence
- **Primary results**: High - Clear quantitative improvements shown over baseline
- **Method validity**: Medium - The approach is novel but relies on several assumptions about preference modeling
- **Generalizability**: Medium - Results are demonstrated on specific household tasks, may not transfer to all domains
- **Implementation feasibility**: High - The components are all well-established techniques combined in a novel way

## Next Checks
1. Validate reflection mechanism calibration across diverse preference types and task complexities
2. Test performance degradation when reducing the amount of preference training data
3. Evaluate real-world user studies to confirm preference satisfaction translates to actual user experience