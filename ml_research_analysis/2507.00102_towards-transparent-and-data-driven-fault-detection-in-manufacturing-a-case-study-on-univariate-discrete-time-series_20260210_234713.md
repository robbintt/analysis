---
ver: rpa2
title: 'Towards transparent and data-driven fault detection in manufacturing: A case
  study on univariate, discrete time series'
arxiv_id: '2507.00102'
source_url: https://arxiv.org/abs/2507.00102
tags:
- curve
- data
- quality
- force
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a transparent, data-driven fault detection
  system for manufacturing, specifically applied to the crimping process using univariate,
  discrete time series. The methodology combines a Random Forest classifier for multi-class
  fault classification, SHAP for post-hoc interpretability, and a domain-specific
  visualization technique that maps model explanations to operator-interpretable features.
---

# Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series

## Quick Facts
- arXiv ID: 2507.00102
- Source URL: https://arxiv.org/abs/2507.00102
- Authors: Bernd Hofmann; Patrick Bruendl; Huong Giang Nguyen; Joerg Franke
- Reference count: 40
- Primary result: 95.9% fault detection accuracy in crimping process using raw time series with interpretable XAI

## Executive Summary
This paper presents a human-centric, data-driven fault detection system for manufacturing that combines a Random Forest classifier with SHAP-based explainability and domain-specific visualization. Applied to the crimping process, the system uses raw crimp force curves (500 discrete time points) as input features rather than manual feature engineering. The approach achieves 95.9% accuracy while providing interpretable explanations that map statistical importance to physically meaningful process phases. Both quantitative perturbation analysis and qualitative expert assessments validate the system's relevance and interpretability.

## Method Summary
The methodology uses raw crimp force curves as input features, applying inversion, zero-setting, windowing to 500 points, and min-max scaling. A Random Forest classifier (150 estimators, max_depth=20) is trained on an 80/20 split. SHAP TreeExplainer generates importance values for each time point, which are then aggregated into four predefined physical phases (Centring, Rolling in, Compression, Springback) by calculating average importance per phase. A perturbation-based selectivity analysis validates feature importance by measuring accuracy degradation when specific phases are modified. The system includes a visualization interface that color-codes phases based on their importance for operator interpretation.

## Key Results
- Achieved 95.9% multi-class fault classification accuracy on crimp force curves
- SHAP-based explanations successfully mapped statistical importance to physically meaningful process phases
- Perturbation analysis confirmed feature relevance, with Compression phase removal causing accuracy to drop from 95.9% to 86.1%
- Expert assessments validated the interpretability and trustworthiness of the generated explanations

## Why This Works (Mechanism)

### Mechanism 1: Raw-Sensing Pattern Recognition
The system captures complex, non-linear dependencies by using complete raw time series as input features rather than manual summary statistics. The Random Forest treats each of the 500 discrete time steps as an independent feature, identifying boundary rules across the entire signal duration that capture subtle deviations in curve morphology missed by manual thresholds.

### Mechanism 2: Phase-Aggregated Attribution
Post-hoc explainability via SHAP is made interpretable for operators by aggregating point-wise importance scores into physically meaningful "phases" (Centring, Rolling in, Compression, Springback). SHAP assigns importance values to each of the 500 data points, which are then sliced into four predefined physical phases and averaged, mapping abstract statistical weights to domain concepts.

### Mechanism 3: Perturbation-Based Selectivity Validation
The relevance of identified explanation phases is verified quantitatively by observing model performance degradation when those phases are removed or corrupted. A "selectivity analysis" retrains the model on datasets where specific phases are replaced with zero, random, or removed values, confirming that accuracy drops significantly (e.g., from 95.9% to ~76-86%) when important features are perturbed.

## Foundational Learning

- **Random Forest Classifier**: An ensemble of decision trees voting on classification outcomes. Why needed: Serves as the core "black box" engine for multi-class fault detection. Quick check: If model outputs 95.9% accuracy, does this mean every single tree is 95.9% accurate? (No, accuracy is aggregated).

- **SHAP (SHapley Additive exPlanations)**: Assigns contribution values representing how much each feature pushes prediction away from base rate. Why needed: Translation layer making black box decisions interpretable. Quick check: If data point has SHAP value of 0, did it influence model's decision for that instance? (No).

- **Time Series Slicing (Windowing)**: Maps raw continuous data to discrete analysis points. Why needed: System relies on mapping raw points to physical stages for interpretation. Quick check: Why is raw curve of 3567 points reduced to 500 points before entering model? (To isolate "region of interest" where crimping occurs).

## Architecture Onboarding

- **Component map**: Input (Raw Crimp Force Curve) -> Preprocessing (Inversion → Zero-setting → Windowing → Min-Max Scaling) -> Model (Random Forest) -> Explainer (SHAP TreeExplainer) -> Aggregator (Slices 500 points into 4 phases) -> Interface (Line plot with color-coded "pipes")

- **Critical path**: The Slicing/Aggregation step. If physical phase boundaries are set incorrectly for a specific machine setup, the "transparent" explanation will highlight wrong part of curve, leading to operator distrust.

- **Design tradeoffs**: Raw vs. Feature-based (Raw-Sensing chosen for high information retention, XAI used to regain interpretability lost by using raw data); Global vs. Local Explanation (Local explanations aggregated for pseudo-global view via visualization).

- **Failure signatures**:
  - Symptom: High accuracy (>90%) but experts reject system. Cause: Visualization highlights "Springback" phase as important, but experts know it's physically irrelevant. Check for spurious correlations or misaligned phase boundaries.
  - Symptom: Accuracy drops significantly when "unimportant" phases are perturbed. Cause: Model using full curve context; "Selectivity" assumption that phases are independent might be flawed.

- **First 3 experiments**:
  1. Baseline Replication: Train RF on provided dataset (80/20 split) and verify 95.9% accuracy baseline using hyperparameters in Table 2.
  2. Perturbation Stress Test: Retrain model after replacing "Compression" phase (indices 150-345) with zeros. Verify accuracy drops below 90% as per Table 4.
  3. Expert Alignment Check: Show visualizations (e.g., Fig. 10) to domain expert without prediction label. Ask: "Based on highlighted zones, what do you think defect is?" to validate human-centric design.

## Open Questions the Paper Calls Out

### Open Question 1
Should time series segmentation be dynamically adapted for machine learning applications, or remain aligned with established domain-specific physical conventions? The current study predefined segmentation based on physical process phases to aid operator interpretability without testing if data-driven dynamic segmentation could improve performance or explanation fidelity.

### Open Question 2
Can domain-specific Large Language Models (LLMs) be effectively integrated to further enhance human-centric data-driven quality control systems? While the current system uses visual overlays for explanations, it does not implement natural language interfaces or generative AI to interpret model outputs further.

### Open Question 3
To what extent does the unequal distribution of data points across curve phases influence the results of the selectivity analysis? The perturbation analysis showed larger phases were most impactful, but it's unclear if this is due to physical relevance or simply higher information content from having more data points.

## Limitations

- Generalizability beyond crimping process uncertain due to reliance on predefined physical phase boundaries
- Perturbation analysis only examines phase-level feature importance without exploring feature interactions or temporal dependencies within phases
- Assumes alignment between statistical importance and domain knowledge that may not always hold across different manufacturing processes

## Confidence

- **High Confidence**: Random Forest model achieves reported 95.9% accuracy; preprocessing pipeline is well-specified and reproducible
- **Medium Confidence**: SHAP-based interpretability methodology is valid, though aggregation into physical phases assumes alignment between statistical importance and domain knowledge
- **Low Confidence**: Perturbation-based selectivity validation provides suggestive evidence but doesn't conclusively prove causal relationships between features and model decisions

## Next Checks

1. **Cross-Process Generalization Test**: Apply methodology to different manufacturing process (e.g., welding or drilling) to evaluate whether raw-sensing + SHAP + phase-aggregation approach maintains interpretability across domains.

2. **Phase Boundary Sensitivity Analysis**: Systematically vary phase boundary indices (x₁, x₂, x₃, x₄) by ±10% and measure changes in both model accuracy and explanation stability to assess robustness of interpretability claims.

3. **Feature Interaction Investigation**: Extend perturbation analysis to examine feature combinations and temporal sequences within phases, rather than treating phases as monolithic blocks, to validate whether model truly captures physical causation.