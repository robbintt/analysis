---
ver: rpa2
title: 'LABOR-LLM: Language-Based Occupational Representations with Large Language
  Models'
arxiv_id: '2406.17972'
source_url: https://arxiv.org/abs/2406.17972
tags:
- testse
- trainse
- occupation
- workers
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LABOR-LLM, a language-based approach for
  predicting workers' next occupations using large language models. The method converts
  tabular career history data into natural language resume-like text, fine-tunes a
  foundation LLM on these text representations, and uses the fine-tuned model to predict
  next occupations by extracting next-word probabilities.
---

# LABOR-LLM: Language-Based Occupational Representations with Large Language Models

## Quick Facts
- arXiv ID: 2406.17972
- Source URL: https://arxiv.org/abs/2406.17972
- Reference count: 40
- Key outcome: Fine-tuned LLM approach outperforms all prior models, achieving perplexity of 8.18 compared to 8.58 for CAREER on PSID dataset

## Executive Summary
This paper introduces LABOR-LLM, a language-based approach for predicting workers' next occupations using large language models. The method converts tabular career history data into natural language resume-like text, fine-tunes a foundation LLM on these text representations, and uses the fine-tuned model to predict next occupations by extracting next-word probabilities. The approach achieves state-of-the-art performance on three U.S. survey datasets, demonstrating that fine-tuning is crucial and that semantic encoding of job titles through natural language significantly improves prediction over numeric codes.

## Method Summary
The method converts career histories from PSID, NLSY79, and NLSY97 surveys into natural language resume templates, then fine-tunes Llama-2-7B or Llama-2-13B models on these text representations using next-token prediction. The fine-tuned models predict next occupations by computing probability distributions over valid job titles. The approach uses complete career histories (not just the most recent job) and can incorporate both representative survey data and non-representative resume datasets during fine-tuning. Inference uses 8-bit quantization and extracts probabilities via chain rule over token sequences.

## Key Results
- Fine-tuning is crucial: off-the-shelf Llama-2-7B perplexity is 364.655 vs. 8.184 for fine-tuned model on PSID81
- Semantic job titles outperform numeric codes: perplexity increases from 8.184 to 8.830 when using numeric titles
- Smaller models with more data can match larger models: FT-7B with 100% pooled data achieves perplexity 8.082 vs. 8.140 for FT-13B with PSID81-only training
- Model excels at auxiliary tasks: achieves AUC-ROC of 0.754 for unemployment prediction and 0.848 for occupation change detection

## Why This Works (Mechanism)

### Mechanism 1: Fine-tuning as Distribution Alignment
Fine-tuning adjusts a pre-trained LLM's parameters to align its next-token distribution with the empirical career transition distribution in survey data. Pre-trained models encode general linguistic patterns but lack calibration to specific career trajectory patterns. Fine-tuning shifts probability mass toward valid occupations and accurate conditional transition patterns without catastrophically overwriting semantic knowledge.

### Mechanism 2: Semantic Encoding of Job Titles
Natural language job titles enable the model to leverage semantic relationships encoded in pre-training, where similar occupations are embedded in nearby vector space regions. This allows generalization from observed transitions to similar unobserved transitions. Numeric codes sever this semantic link, forcing the model to learn transitions purely from observed code sequences without benefiting from linguistic prior knowledge.

### Mechanism 3: Data Scaling Compensates for Model Size
Transformer models exhibit data scaling laws where prediction error decreases with more training tokens. Non-representative data provides signal about general career transition patterns that complements domain-specific signal from representative survey data. A smaller model with more data can approximate the function class of a larger model with less data.

## Foundational Learning

- **Transformer Architecture & Self-Attention**: Llama-2 uses decoder-only transformers where self-attention allows the model to weigh different years in a career history differently when predicting the next occupation. Quick check: Explain how self-attention would help a model decide that a worker's recent job as "Data Analyst" is more predictive of their next job than a job from 15 years ago as "Retail Clerk."

- **Fine-Tuning vs. In-Context Learning**: The paper shows fine-tuning is essential; in-context learning helps off-the-shelf LLMs but does not match fine-tuned performance. Quick check: If you only had API access to GPT-4 and could not fine-tune, what strategy from Section G.1 might you use to improve prediction, and why would it still likely underperform?

- **Next-Token Prediction as Conditional Distribution Learning**: The model is trained to maximize log-likelihood of each token in the resume text, inducing a conditional probability distribution over occupations given history. Quick check: How would you extract P(occupation = "Teacher" | history) from a fine-tuned Llama-2 model given a prompt of a worker's career history?

## Architecture Onboarding

- **Component map**: Data Preprocessing -> Tokenizer -> Foundation Model -> Fine-Tuning -> Inference
- **Critical path**: 1) Prepare text templates (data-dependent, determines model input), 2) Fine-tune (computationally expensive, requires GPU monitoring), 3) Extract probabilities (requires model logits access)
- **Design tradeoffs**: Model size vs. data volume (larger models perform better with limited data, smaller models with more data can match performance), representative vs. non-representative data (adds performance but may introduce bias), prompt engineering vs. fine-tuning (cheap but underperforms)
- **Failure signatures**: No fine-tuning (perplexity >300, invalid job titles), numeric job titles (perplexity increases by ~0.6-0.8), insufficient history (performance degrades with <10 years)
- **First 3 experiments**: 1) Ablate fine-tuning (compare off-the-shelf vs. FT-7B perplexity and % valid job titles), 2) Compare representations (literal vs. numeric job titles perplexity difference), 3) Data scaling test (FT-7B on increasing data percentages vs. FT-13B)

## Open Questions the Paper Calls Out

- What is the optimal method for combining representative and non-representative data during fine-tuning to minimize bias while maximizing predictive accuracy? The authors state this provides "an interesting opportunity for future research" without testing specific weighting strategies.

- To what extent do the model's direct predictions over multi-year gaps remain internally consistent with sequential single-year predictions composed via Bayes' rule? The paper observes high correlation (0.93) but notes "We leave further exploration of this issue for future work."

- Can LABOR-LLM maintain robust predictive performance during periods of structural economic change where historical transition probabilities may no longer hold? The conclusion highlights that "performance can deteriorate when the underlying data-generating process changes."

## Limitations

- Results rely on three U.S. household surveys with total Nâ‰ˆ52,500 individuals, limiting generalizability to other labor markets
- Exact template formatting, preprocessing pipeline for occupation codes, and random seed settings are not fully specified, creating reproducibility concerns
- Model may inherit biases from training data (e.g., over-representation of certain demographic groups) without systematic evaluation across subpopulations

## Confidence

- **High confidence**: Fine-tuning is crucial for performance (perplexity drops from ~365 to ~8), and semantic job titles outperform numeric codes (robustly demonstrated across multiple model sizes)
- **Medium confidence**: Adding non-representative data improves performance and smaller models with more data can match larger models (lacks detailed ablation studies)
- **Low confidence**: Generalizability to other labor markets or prediction tasks is not established, and potential biases across subpopulations are not systematically evaluated

## Next Checks

1. **Reproducibility audit**: Reimplement text template generation pipeline exactly as described in Appendix B, fine-tune Llama-2-7B on PSID81 training data using same hyperparameters (batch size 32, learning rate 1e-5, 3-5 epochs), and verify perplexity of 8-9 on PSID81 test set.

2. **Semantic encoding test**: Conduct controlled experiment comparing literal job titles, numeric codes, and job titles with semantic perturbations (e.g., "Sr. Software Engineer" vs. "Software Engineer") to quantify semantic information contribution beyond raw token frequency.

3. **Subpopulation bias analysis**: Evaluate model performance separately across demographic subgroups (gender, race/ethnicity, education levels) on unemployment prediction task, reporting both accuracy metrics and calibration error to identify potential fairness concerns.