---
ver: rpa2
title: Foundation-Model-Boosted Multimodal Learning for fMRI-based Neuropathic Pain
  Drug Response Prediction
arxiv_id: '2503.00210'
source_url: https://arxiv.org/abs/2503.00210
tags:
- fmmt
- multimodal
- data
- learning
- fmri
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting drug response in
  neuropathic pain treatment using fMRI data, which is hindered by extreme data scarcity
  and the complexity of fMRI analysis. The proposed FMMTC framework tackles this by
  leveraging multimodal learning across time series and functional connectivity fMRI
  modalities, and by incorporating external knowledge from large pain-agnostic fMRI
  datasets via a foundation model (BrainLM).
---

# Foundation-Model-Boosted Multimodal Learning for fMRI-based Neuropathic Pain Drug Response Prediction

## Quick Facts
- arXiv ID: 2503.00210
- Source URL: https://arxiv.org/abs/2503.00210
- Reference count: 33
- Achieves up to 69.64% Matthews correlation coefficient (MCC) and 77.56% AUROC in drug-agnostic prediction, and 86.02% MCC in drug-specific prediction across cross-dataset evaluations

## Executive Summary
This paper addresses the challenge of predicting drug response in neuropathic pain treatment using fMRI data, which is hindered by extreme data scarcity and the complexity of fMRI analysis. The proposed FMMTC framework tackles this by leveraging multimodal learning across time series and functional connectivity fMRI modalities, and by incorporating external knowledge from large pain-agnostic fMRI datasets via a foundation model (BrainLM). The approach demonstrates superior performance over unimodal baselines, achieving up to 69.64% Matthews correlation coefficient (MCC) and 77.56% AUROC in drug-agnostic prediction, and 86.02% MCC in drug-specific prediction across cross-dataset evaluations. Ablation studies confirm the effectiveness of multimodal learning and external knowledge transfer, while interpretability analysis reveals the model's adaptive reliance on different modalities depending on the dataset, enhancing its generalizability and robustness in clinical applications.

## Method Summary
The FMMTC framework processes resting-state fMRI (rs-fMRI) data through two parallel encoders: a frozen foundation model (BrainLM) for time series (TS) features and a ResNet-18 for functional connectivity (FC) features. The TS encoder is pre-trained on large pain-agnostic datasets and remains frozen to prevent overfitting. FC matrices are computed from regional time series and processed through a CNN encoder. The 256-dimensional outputs from both encoders are concatenated into a 512-dimensional multimodal representation, which is then classified using a single-layer perceptron to predict drug response. The framework employs multimodal learning to integrate complementary information from both modalities while leveraging foundation model transfer to overcome extreme data scarcity.

## Key Results
- FMMTC achieves 69.64% MCC and 77.56% AUROC in drug-agnostic prediction across datasets
- FMMTC achieves 86.02% MCC in drug-specific prediction on in-house dataset
- Ablation studies show foundation model transfer improves MCC by at least 14.71% and multimodal learning outperforms all unimodal baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating time series (TS) and functional connectivity (FC) as distinct modalities and fusing them via concatenation improves prediction under data scarcity.
- **Mechanism:** TS encodes fine-grained temporal dynamics at the regional level; FC encodes global spatial relationships between brain regions. Concatenation preserves semantic integrity of each modality's features, allowing downstream layers to learn adaptive weighting. This complementary information capture compensates for limited sample size by maximizing signal extraction from two views of the same data.
- **Core assumption:** The transformer-derived TS features and CNN-derived FC features contain non-redundant predictive information that simple concatenation can preserve.
- **Evidence anchors:**
  - [abstract] "integrates complementary information from two rs-fMRI modalities: Time series and functional Connectivity"
  - [section] Table 1 shows concatenation achieves 69.64% MCC vs. 0% for cross-attention and 44.12% for MoE
  - [section] "TS focuses on fine-grained, regional features... while FC constructs a global brain network emphasizing spatial relationships"
  - [corpus] Weak/no direct corpus evidence on this specific TS+FC multimodal approach
- **Break condition:** If attention-based fusion mechanisms (cross-attention, MoE) outperform concatenation, the assumption that limited data necessitates simple fusion fails. Also breaks if ablation shows FC contributes negligible signal.

### Mechanism 2
- **Claim:** Freezing a foundation model (BrainLM) pre-trained on large pain-agnostic fMRI data and using it as a fixed encoder improves drug response prediction on small pain-specific datasets.
- **Mechanism:** BrainLM learns general fMRI representations from 50k+ UK Biobank scans (pain-agnostic). Freezing prevents overfitting/mode collapse when fine-tuning on ~60 samples. The pre-trained encoder provides robust, generalizable feature extraction that transfers to neuropathic pain domain despite domain shift.
- **Core assumption:** Representations learned from general population fMRI transfer to neuropathic pain populations without catastrophic forgetting or domain mismatch.
- **Evidence anchors:**
  - [abstract] "incorporating external knowledge from large pain-agnostic fMRI datasets via a foundation model (BrainLM)"
  - [section] BrainLM w/ pre-training achieves 63.36-63.72% MCC vs. 0% w/o pre-training (Exp. 6 vs. 7, Table 3)
  - [section] Ablation shows FM-powered transfer improves MCC by at least 14.71%
  - [corpus] "Towards a general-purpose foundation model for fMRI analysis" (NeuroSTORM) corroborates FM viability for fMRI
- **Break condition:** If frozen BrainLM underperforms a randomly initialized encoder on pain-specific data, the transfer assumption fails. Also breaks if fine-tuning BrainLM (unfreezing) yields superior results without collapse.

### Mechanism 3
- **Claim:** The model adaptively weights TS vs. FC modalities based on dataset characteristics, enabling cross-dataset generalization.
- **Mechanism:** Concatenation preserves modality-specific features without forced integration. The downstream predictor learns to emphasize whichever modality provides stronger signal for a given dataset. Integrated gradients reveal this dynamic behavior: OpenNeuro favors TS features; in-house dataset favors FC features.
- **Core assumption:** Different datasets have varying signal-to-noise ratios across modalities, and a unified model can learn dataset-appropriate weighting without explicit modality selection.
- **Evidence anchors:**
  - [abstract] "interpretability analysis reveals the model's adaptive reliance on different modalities depending on the dataset"
  - [section] Fig. 2(b) IG analysis shows TS features dominate OpenNeuro predictions; FC features dominate in-house predictions
  - [section] This aligns with unimodal baseline performance: BrainLM (TS) > ResNet (FC) on OpenNeuro; ResNet (FC) > BrainLM (TS) on in-house
  - [corpus] No corpus evidence on adaptive multimodal weighting in fMRI
- **Break condition:** If IG analysis shows consistent modality weighting across all datasets, the adaptive claim fails. Also breaks if forcing fixed modality weights (e.g., weighted sum) outperforms learned adaptation.

## Foundational Learning

- **Concept: Functional Connectivity (FC) from fMRI**
  - **Why needed here:** FC matrices (N×N correlation of ROI time series) are one of two input modalities. Understanding that FC represents synchronized brain activity between regions—not raw activation—is essential.
  - **Quick check question:** Given a 424-ROI parcellation with 200 timepoints, what is the shape of the FC matrix, and what does entry FC[i,j] represent?

- **Concept: Foundation Model Transfer Learning**
  - **Why needed here:** The entire TS encoder is a frozen pre-trained model. You must understand why freezing prevents overfitting and what "external knowledge" means in this context.
  - **Quick check question:** Why would fine-tuning BrainLM on 60 samples risk mode collapse, and what is the alternative strategy?

- **Concept: Integrated Gradients (IG) for Attribution**
  - **Why needed here:** IG is used to explain adaptive modality weighting. You need to understand IG as a gradient-based attribution method that satisfies sensitivity axioms.
  - **Quick check question:** If IG assigns higher magnitude to TS features than FC features for a given prediction, what does that imply about the model's reliance on each modality?

## Architecture Onboarding

- **Component map:**
  Raw rs-fMRI (4D) -> fMRIPrep preprocessing -> Preprocessed fMRI -> Resize to ICBM152 + A424 atlas parcellation -> Time Series XT: [N=424 ROIs, t=200 timepoints] -> [BrainLM Encoder (frozen)] -> TS Features RT [256-dim] and Correlation computation -> FC Matrix XC [424×424] -> [ResNet-18 + linear projection] -> FC Features RC [256-dim] -> RT + RC concatenation -> Multimodal Features [512-dim] -> [Single-layer perceptron] -> Drug Response ŷ (binary)

- **Critical path:** Raw fMRI -> parcellation (A424 atlas) -> TS + FC computation -> BrainLM (frozen) + ResNet-18 -> concatenation -> classifier. The A424 atlas choice is non-negotiable—it must match BrainLM's pre-training. The concatenation operation is the minimal fusion point; changing this to attention breaks the design.

- **Design tradeoffs:**
  - Frozen vs. fine-tuned BrainLM: Frozen chosen to prevent overfitting; sacrifices potential domain adaptation for stability
  - Concatenation vs. attention fusion: Concatenation chosen for simplicity and small-data robustness; sacrifices learned feature interactions
  - ResNet-18 vs. transformer for FC: ResNet chosen for parameter efficiency; sacrifices long-range dependency modeling in FC matrices
  - Binary classification formulation: Sacrifices ordinal response gradation for clinical interpretability

- **Failure signatures:**
  - Random-guess-level performance (MCC ≈ 0, AUROC ≈ 0.5): Check if BrainLM weights are actually frozen, or if FC encoder is not learning
  - Large performance gap between datasets (e.g., 69% vs. 48% MCC): Expected to some degree, but if gap > 30%, check data preprocessing consistency
  - Attention-based fusion returns 0% MCC (as in Table 1): Expected failure mode—these mechanisms require more data
  - BrainLM w/o pre-training returns 0% MCC (Exp. 6): Expected—untrained transformer overfits immediately

- **First 3 experiments:**
  1. **Unimodal baseline validation:** Train ResNet-18 on FC-only and BrainLM (frozen, no pre-training) on TS-only for drug-agnostic prediction. Verify that neither unimodal baseline consistently outperforms the other across both datasets.
  2. **Ablation: FM transfer impact:** Compare FMMTC with frozen pre-trained BrainLM vs. FMMTC with randomly initialized (or no pre-training) BrainLM on same data. Quantify MCC delta (target: >14% improvement per paper claims).
  3. **Fusion mechanism comparison:** Implement cross-attention, MoE, element-wise sum, and concatenation fusion. Verify that concatenation achieves highest MCC on held-out fold, confirming Table 1 finding. If attention outperforms, reconsider data size assumptions.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on frozen foundation models may limit adaptability to new neuropathic pain subpopulations
- Concatenation-based fusion may underutilize potential interactions between TS and FC features
- Interpretability analysis reveals adaptive weighting but doesn't explain underlying neurological mechanisms

## Confidence
- **High confidence:** Multimodal learning outperforms unimodal baselines (69.64% MCC vs. 0-44.12% MCC) and foundation model transfer improves performance (14.71%+ MCC gains)
- **Medium confidence:** Adaptive modality weighting claims based on IG analysis are correlational rather than causal explanations
- **Medium confidence:** Cross-dataset generalizability given performance still varies considerably (69.64% vs. 48.03% MCC)

## Next Checks
1. Test FMMTC on a third neuropathic pain dataset with different scanner parameters or demographic characteristics to quantify true cross-dataset generalization limits
2. Gradually unfreeze portions of BrainLM during fine-tuning to find the optimal balance between transfer learning and domain adaptation without catastrophic forgetting
3. Implement and compare more sophisticated fusion mechanisms (transformer-based, graph neural networks) as data availability increases to establish when concatenation ceases to be optimal