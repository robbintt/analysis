---
ver: rpa2
title: 'IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language
  Understanding'
arxiv_id: '2501.15747'
source_url: https://arxiv.org/abs/2501.15747
tags:
- languages
- indic
- language
- across
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndicMMLU-Pro, a comprehensive benchmark
  for evaluating Large Language Models (LLMs) on multi-task language understanding
  across nine major Indic languages. The benchmark adapts the MMLU-Pro framework to
  address the unique linguistic diversity and cultural complexity of the Indian subcontinent,
  covering languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi,
  Tamil, Telugu, and Urdu.
---

# IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding

## Quick Facts
- arXiv ID: 2501.15747
- Source URL: https://arxiv.org/abs/2501.15747
- Reference count: 27
- Major outcome: Introduces IndicMMLU-Pro benchmark for evaluating LLMs on multi-task language understanding across nine major Indic languages

## Executive Summary
This paper introduces IndicMMLU-Pro, a comprehensive benchmark for evaluating Large Language Models (LLMs) on multi-task language understanding across nine major Indic languages. The benchmark adapts the MMLU-Pro framework to address the unique linguistic diversity and cultural complexity of the Indian subcontinent, covering languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, and Urdu. The dataset was created using IndicTrans2 for translation from English MMLU-Pro, followed by rigorous quality assurance including back-translation and expert proofreading. Baseline results show GPT-4o significantly outperforms other models, achieving accuracy scores between 38.46% and 44.80% across languages, while specialized Indic language models demonstrate competitive performance.

## Method Summary
The benchmark development process involved translating the English MMLU-Pro dataset into nine Indic languages using IndicTrans2, a transformer-based machine translation model. The translation pipeline included multiple quality assurance steps: back-translation for consistency checks, expert proofreading to ensure cultural and linguistic appropriateness, and domain-specific validation. The benchmark covers 14 domains including STEM, humanities, and social sciences, with questions designed to test both expert-level knowledge and multi-step reasoning capabilities. The evaluation framework is implemented as an open-source pipeline on Hugging Face, enabling standardized testing across different models and languages.

## Key Results
- GPT-4o achieved the highest performance with accuracy scores ranging from 38.46% to 44.80% across all nine Indic languages
- Specialized Indic language models demonstrated competitive performance despite GPT-4o's overall superiority
- The benchmark successfully differentiated between model capabilities while highlighting the challenges of cross-linguistic evaluation

## Why This Works (Mechanism)
The benchmark leverages the established MMLU-Pro framework while adapting it to Indic languages through careful translation and cultural adaptation. The use of machine translation followed by human expert validation ensures both scalability and quality. The multi-domain approach tests comprehensive language understanding across diverse knowledge areas, while the standardized evaluation pipeline enables fair comparison between models with different architectural approaches and training data.

## Foundational Learning
- **MMLU-Pro framework**: Why needed - provides established multi-task evaluation methodology; Quick check - verify question complexity and reasoning requirements
- **Machine translation quality assurance**: Why needed - ensures linguistic accuracy across diverse languages; Quick check - measure BLEU scores and human evaluation agreement
- **Cultural adaptation in translation**: Why needed - prevents cultural bias and ensures question relevance; Quick check - expert review for cultural appropriateness
- **Multi-domain evaluation**: Why needed - tests comprehensive language understanding; Quick check - domain coverage analysis and difficulty calibration
- **Standardized evaluation pipeline**: Why needed - enables fair model comparison; Quick check - pipeline consistency across different model architectures
- **Cross-linguistic performance analysis**: Why needed - identifies language-specific model strengths/weaknesses; Quick check - correlation between language family and model performance

## Architecture Onboarding
- **Component map**: Data preparation (translation + quality assurance) -> Benchmark creation (domain organization) -> Model evaluation (standardized pipeline) -> Results analysis (performance comparison)
- **Critical path**: Translation quality assurance -> Domain-specific validation -> Evaluation pipeline execution -> Performance benchmarking
- **Design tradeoffs**: Automated translation vs. manual translation (speed vs. quality), broad domain coverage vs. depth in specific areas, standardized metrics vs. language-specific nuances
- **Failure signatures**: Inconsistent translations affecting question difficulty, cultural misalignment in question context, evaluation pipeline errors in scoring
- **First experiments**: 1) Run benchmark with subset of questions to validate translation quality, 2) Compare model performance on single vs. multiple domains, 3) Test evaluation pipeline with different model sizes and architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Potential for subtle semantic shifts during translation affecting question difficulty, though quality assurance steps were implemented
- Benchmark may not fully differentiate between model capabilities given narrow performance gaps between specialized Indic models and GPT-4o
- Lack of direct comparison with original English MMLU-Pro prevents assessment of translation effects on difficulty calibration

## Confidence
- Claims about GPT-4o outperforming other models: High confidence based on reported results
- Claims about translation quality and benchmark reliability: Medium confidence due to limited error rate metrics
- Claims about cultural appropriateness and domain coverage: Medium confidence based on expert review process

## Next Checks
1. Conduct detailed error analysis of translated questions to identify systematic translation artifacts or cultural misalignment
2. Run the benchmark with both translated and original English questions on the same models to quantify translation effects
3. Expand evaluation to include more diverse model architectures and sizes to better establish performance baselines and identify potential ceiling effects