---
ver: rpa2
title: Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries
arxiv_id: '2511.20854'
source_url: https://arxiv.org/abs/2511.20854
tags:
- recall
- video
- content
- retrieval
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOT2MEM, a large-scale dataset for modeling
  visual content memorability using tip-of-the-tongue (ToT) retrieval queries from
  Reddit. The dataset contains over 470,000 content-recall pairs across multiple domains,
  with a video-based subset of 82,500 videos paired with descriptive recall data.
---

# Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries

## Quick Facts
- **arXiv ID**: 2511.20854
- **Source URL**: https://arxiv.org/abs/2511.20854
- **Reference count**: 40
- **Primary result**: Introduces TOT2MEM dataset (470K content-recall pairs) and models that outperform GPT-4o on memorability description generation and ToT retrieval tasks

## Executive Summary
This paper introduces TOT2MEM, a large-scale dataset for visual content memorability modeling using tip-of-the-tongue (ToT) retrieval queries from Reddit. The dataset contains over 470,000 content-recall pairs across multiple domains, with a video-based subset of 82,500 videos paired with descriptive recall data. Using this unsupervised dataset, the authors propose two tasks: descriptive recall generation and multimodal ToT retrieval. They develop TOT2MEM-RECALL, a model fine-tuned on their dataset that outperforms state-of-the-art models like GPT-4o in generating memorability descriptions and generalizes to human-curated data. They also create TOT2MEM-RETRIEVAL, the first model for multimodal ToT retrieval, using contrastive training. Results show strong performance on both tasks, demonstrating that web-scale ToT queries provide rich signals for memorability modeling and can be effectively leveraged for real-world applications.

## Method Summary
The paper constructs TOT2MEM by collecting solved Reddit ToT posts (1.98M → 470K solved) through multi-stage validation including CSS flair detection, OP reply confirmation, moderator bot verification, and LLM-based answer extraction. For videos, they extract YouTube links, download content (<600s), process scenes (max 30) with AdaptiveDetector, OCR with PaddleOCR, and transcripts with Whisper. TOT2MEM-RECALL fine-tunes QwenVL 2.5 7B with LoRA (rank 64, frozen vision encoder) on 80K video-recall pairs for descriptive recall generation. TOT2MEM-RETRIEVAL uses VLM2Vec-V2 with InfoNCE contrastive loss and hard negative mining (top-50 SBERT-similar recalls) for multimodal ToT retrieval. Both models are trained on 4x H100-80 GPUs for 8 hours with DeepSpeed Zero and Flash Attention.

## Key Results
- TOT2MEM-RECALL achieves BLEU 0.242 vs. InternVL-2.5 8B's 0.211 and BERTScore 0.85 vs. baselines at 0.82 on descriptive recall generation
- TOT2MEM-RETRIEVAL achieves Recall@10 of 27.5% vs. VLM2Vec-V2's 25.4% and MRR@10 of 15.7% vs. 14.5% on ToT retrieval
- Strong zero-shot generalization to LAMBDA dataset (BERTScore 0.86 vs. baseline 0.84)
- First model capable of multimodal ToT retrieval using contrastive training

## Why This Works (Mechanism)

### Mechanism 1: Solved-Post Validation as Weak Supervision
Reddit's "solved" posts provide reliable recall-content pairs through multi-stage validation: CSS flair detection for solved status, OP reply confirmation, moderator bot verification, and LLM-based answer extraction. Posts where community successfully identified content from vague descriptions signal that recall descriptions contained sufficient memorable features. Core assumption: descriptive recall captures genuine memorability signals rather than just search keywords.

### Mechanism 2: Multimodal Fine-Tuning for Recall Generation
Fine-tuning VLMs on Reddit recall-content pairs teaches models to predict what humans remember about visual content. QwenVL 2.5 7B is fine-tuned with LoRA (rank 64, frozen vision encoder) on 80K video-recall pairs using scene images (max 30), OCR text, and Whisper-generated audio transcripts. Core assumption: recall patterns in Reddit ToT queries generalize to broader memorability prediction tasks.

### Mechanism 3: Contrastive Alignment for ToT Retrieval
Contrastive training aligns vague textual recall with video embeddings for retrieval. VLM2Vec-V2 is trained with InfoNCE loss using hard negatives (mined via SBERT semantic similarity, top-50 most similar recalls) and in-batch negatives. The model learns to embed queries and videos into shared space where semantically similar pairs are closer. Core assumption: recall descriptions share sufficient semantic structure with video content to enable meaningful alignment.

## Foundational Learning

- **Tip-of-the-Tongue (ToT) States in Cognitive Psychology**: Understanding that ToT queries represent genuine memory retrieval attempts with partial information is critical for interpreting the data as memorability signals rather than noisy search queries.
- **Contrastive Learning with InfoNCE Loss**: The retrieval model uses this training objective; understanding how positive/negative pairs shape the embedding space is critical for debugging retrieval failures.
- **LoRA (Low-Rank Adaptation) Fine-Tuning**: Both tasks use LoRA for parameter-efficient training; understanding rank selection and which modules to freeze is essential for reproduction and extension.

## Architecture Onboarding

- **Component map**: Reddit API → post filtering (1.98M → 470K solved) → YouTube link extraction → video download (600s max) → scene detection (AdaptiveDetector) → OCR (PaddleOCR) → ASR (Whisper) → LLM validation (DeepSeek-R1) → QwenVL 2.5 7B with LoRA (rank=64) → VLM2Vec-V2 with InfoNCE loss → shared embedding space

- **Critical path**: Data quality determines model quality—LLM validation step (>95% agreement with rule-based) is key filter; 30-scene limit constrains visual information; hard negative mining quality directly impacts retrieval discriminability

- **Design tradeoffs**: Scale vs. noise (470K pairs provide coverage but include episodic memory leakage); unsupervised vs. controlled data (Reddit lacks ground-truth memorability scores); video length truncation (600s max excludes long-form content)

- **Failure signatures**: Repetition loops in fine-tuned QwenVL (nucleus sampling doesn't fully resolve); link hallucination (some outputs generate non-existent YouTube links); semantic-visual mismatch (model describes visual features accurately while missing abstract semantic content)

- **First 3 experiments**:
  1. Ablation on proper noun masking: Mask proper nouns in OCR/ASR with empty strings, verify BLEU/ROUGE stability (~0.24 → 0.241)
  2. Hard negative quality analysis: Sample 100 hard negatives, manually assess whether they're challenging enough (should require fine-grained memorability discrimination)
  3. Zero-shot transfer to LAMBDA: Replicate on 2,205-sample LAMBDA dataset to validate generalization claim; expect BERTScore ~0.86 vs. baseline ~0.84

## Open Questions the Paper Calls Out

1. **Question**: To what extent does inherent noise in unsupervised ToT datasets (such as residual episodic context or OCR errors) induce specific types of hallucinations in Large Vision-Language Models?
   - **Basis in paper**: The authors state future work could study "how large vision–language models handle noisy or ambiguous inputs, including whether such noise can induce hallucinations."
   - **Why unresolved**: The paper demonstrates noise doesn't hinder aggregate performance on benchmarks but doesn't isolate or analyze generation of false information specifically caused by noisy training supervision.

2. **Question**: How can recall generation models be improved to balance high visual fidelity with semantic abstraction when visual content and underlying meaning are misaligned?
   - **Basis in paper**: The authors note the model produces descriptions highly aligned with visual content while ground truth recall is often abstract or semantic, indicating a "Gap Between Visual Features and Semantics."
   - **Why unresolved**: Current training paradigm encourages strong visual grounding, causing the model to fail on videos where "memorable" element is semantic summary rather than visual description.

3. **Question**: Does use of multimodal hard negatives (video-based) significantly improve performance in ToT retrieval compared to text-based hard negatives currently employed?
   - **Basis in paper**: The authors note they use text-based hard negatives because "using multimodal hard negatives is challenging" due to lack of precedent for embedding videos based on memorability.
   - **Why unresolved**: Text-based mining may fail to distinguish videos that are semantically distinct in text but visually similar, limiting model's ability to discriminate fine-grained visual memorability.

## Limitations

- Selection bias in solved posts may systematically exclude content lacking distinctive memorability features
- Zero-shot generalization claims may reflect domain-specific adaptation rather than general memorability learning
- Hard negative quality ambiguity affects reliability of retrieval performance claims

## Confidence

- **High Confidence**: Dataset construction methodology is clearly specified with detailed filtering stages and validation procedures; performance improvements over baselines are statistically measurable and reproducible
- **Medium Confidence**: Zero-shot transfer to LAMBDA shows promising results but domain shift between Reddit entertainment content and curated viral videos introduces uncertainty about true generalization
- **Low Confidence**: Claim that TOT2MEM provides "rich signals for memorability modeling" at web scale assumes Reddit ToT queries capture genuine memorability rather than just search intent

## Next Checks

1. **Cross-Dataset Generalization Study**: Evaluate TOT2MEM-RECALL on multiple controlled memorability datasets to determine if performance gains transfer across domains or reflect entertainment content specialization
2. **Hard Negative Quality Audit**: Manually sample and evaluate 100 hard negatives used in retrieval training to assess whether they provide meaningful discrimination challenges
3. **Unsolved Post Analysis**: Collect parallel dataset of unsolved Reddit ToT posts and evaluate whether same memorability patterns exist, comparing distribution of recall features between solved and unsolved posts