---
ver: rpa2
title: Representations of Text and Images Align From Layer One
arxiv_id: '2601.08017'
source_url: https://arxiv.org/abs/2601.08017
tags:
- images
- image
- layer
- text
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method to investigate when representations
  of concepts in text and images align in vision-language models. Instead of searching
  datasets, the authors synthesize images from scratch by optimizing them to match
  the representation of a textual concept at a specific layer.
---

# Representations of Text and Images Align From Layer One

## Quick Facts
- arXiv ID: 2601.08017
- Source URL: https://arxiv.org/abs/2601.08017
- Authors: Evžen Wybitul; Javier Rando; Florian Tramèr; Stanislav Fort
- Reference count: 27
- Primary result: Cross-modal alignment emerges in early layers of vision-language models, contrary to prior findings

## Executive Summary
This paper introduces a novel method to investigate when representations of concepts in text and images align in vision-language models. Rather than searching datasets, the authors synthesize images from scratch by optimizing them to match the representation of a textual concept at a specific layer. They then test whether an independent model recognizes the synthesized image as depicting the concept. Applied to Gemma 3 4B across seven layers and over 100 concepts, the method reveals that representations of text and images meaningfully align from layer 1 for many concepts including animals, seasons, activities, and emotions—contrary to prior findings that alignment only emerges in mid-to-late layers. Recognition rates exceed 50% for multiple categories in early layers, despite the model never being explicitly trained to align modalities. The approach is simple, fast, and dataset-free, providing direct, constructive evidence of cross-modal alignment on a concept-by-concept and layer-by-layer basis.

## Method Summary
The authors synthesize images to match textual representations at specific layers of a vision-language model, then test recognition with an independent model. The synthesis process involves: (1) initializing a random image, (2) optimizing it to match the target concept's representation at a chosen layer, (3) applying regularization to ensure transferability across models. The key innovation is bypassing dataset searches entirely by constructing images that directly realize the textual representation in visual form. This allows layer-by-layer analysis of when cross-modal alignment emerges. The authors apply this to Gemma 3 4B, examining 103 concepts across 7 layers (1, 4, 7, 10, 15, 20, 23), then test synthesized images with CLIP to measure recognition rates.

## Key Results
- Text and image representations align from layer 1 in Gemma 3, contradicting prior findings of mid-to-late layer alignment
- Recognition rates exceed 50% for animals, seasons, activities, and emotions in early layers
- A "middle-layer collapse" occurs at layers 15-20 where synthesis fails, but images from layers 20+ regain recognition capability
- InternVL 3 does not exhibit the middle-layer collapse phenomenon
- The method works dataset-free and concept-by-concept, enabling fine-grained analysis

## Why This Works (Mechanism)
The method works by directly constructing visual representations that match textual representations at specific network layers. By optimizing images to minimize the distance between their visual representation and the textual representation of a concept, the approach creates a concrete test of cross-modal alignment. The regularization ensures that synthesized images transfer to independent models, providing evidence that the alignment captures genuine semantic correspondence rather than model-specific artifacts. The layer-by-layer approach reveals when alignment emerges during the model's processing hierarchy, showing that meaningful cross-modal correspondence exists much earlier than previously thought.

## Foundational Learning
- Vision-language model architecture: Why needed - Understanding how text and image encoders interact is crucial for interpreting alignment patterns. Quick check - Review adapter-based vs early-fusion architectures.
- Representation similarity metrics: Why needed - The method relies on measuring distances between textual and visual representations. Quick check - Examine how cosine similarity is used to optimize synthesized images.
- Transferability in synthetic data: Why needed - Regularization ensures synthesized images work across models, validating alignment. Quick check - Review the regularization techniques that prevent adversarial artifacts.

## Architecture Onboarding
- Component map: Text encoder -> Adapter layers -> Image encoder -> Cross-modal attention -> Output layers
- Critical path: Textual concept → Layer-specific representation → Image optimization → CLIP recognition test
- Design tradeoffs: Dataset-free synthesis vs. potential for optimization artifacts; single-model focus vs. generalizability; simplicity vs. need for transferability regularization
- Failure signatures: Middle-layer collapse (layers 15-20 in Gemma); low recognition rates indicating lack of alignment; adversarial-looking images suggesting spurious optimization
- First experiments: 1) Apply synthesis to different vision-language models to test generalizability. 2) Modify regularization to improve human recognizability. 3) Compare adapter-based vs early-fusion models for alignment patterns.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What architectural or representational differences cause the "middle-layer collapse" in Gemma 3, and why does InternVL 3 not exhibit this phenomenon?
- Basis in paper: Section 3.2 identifies the collapse in layers 15–20 as meriting further investigation; Figure 4 shows InternVL does not suffer from this drop.
- Why unresolved: The authors ruled out semantic attention weighting as the cause (ablation in Section G), suggesting the issue is deeper in Gemma's specific encoding of multimodal information, but did not isolate the mechanism.
- What evidence would resolve it: A comparative mechanistic analysis of activation patterns in the middle layers of both models, or successful synthesis in Gemma using modified objective functions that bypass the collapse.

### Open Question 2
- Question: Can the Direct Ascent Synthesis regularization be refined to produce images that are reliably recognizable to humans, rather than just transferable to other models?
- Basis in paper: Section 2.3 and Section 5 note that while current techniques ensure model transfer, they do not reliably produce human-recognizable images, which is a "natural direction for future work."
- Why unresolved: Current augmentations (random shifts/noise) are designed to prevent adversarial artifacts for models, but human vision requires different structural priors which the current method does not enforce strictly enough.
- What evidence would resolve it: A modified synthesis pipeline incorporating human-centric priors (e.g., natural image statistics) that significantly increases human classification accuracy in a blinded study.

### Open Question 3
- Question: Do early-fusion models (e.g., Chameleon) show richer or distinct cross-modal alignment patterns compared to the adapter-based Gemma 3?
- Basis in paper: Section 5 speculates that the method could be "particularly powerful for early fusion models" where representations are more integrated.
- Why unresolved: The study primarily focused on Gemma 3 4B, an adapter-based model, leaving the behavior of unified architectures unexplored.
- What evidence would resolve it: Applying the layer-by-layer synthesis method to an early-fusion model and comparing the recognition rates and visual quality of synthesized images at early layers.

## Limitations
- The method relies on indirect inference through synthesized images rather than direct measurement of cross-modal alignment
- Results are based primarily on a single model family (Gemma 3 4B), limiting generalizability
- The optimization process may produce images that exploit spurious correlations rather than genuine semantic correspondence
- The study does not address whether early-layer alignment reflects genuine conceptual understanding or low-level visual-textual associations

## Confidence
- Claim: Representations align from layer 1: Medium
- Claim: Recognition rates >50% in early layers: High
- Claim: Dataset-free nature of method: High

## Next Checks
1. Apply the same synthesis-and-recognition pipeline to other vision-language models (e.g., CLIP, BLIP, LLaVA) to test cross-model consistency of early-layer alignment.
2. Compare results when using a different independent recognition model (e.g., supervised classifier trained on natural images) to rule out CLIP-specific biases.
3. Conduct ablation studies where the optimization objective is modified to explicitly penalize low-level visual features, testing whether early-layer alignment persists for high-level semantic concepts.