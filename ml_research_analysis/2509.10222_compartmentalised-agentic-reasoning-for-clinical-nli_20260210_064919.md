---
ver: rpa2
title: Compartmentalised Agentic Reasoning for Clinical NLI
arxiv_id: '2509.10222'
source_url: https://arxiv.org/abs/2509.10222
tags:
- reasoning
- solver
- premise
- statement
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Schema collapse limits large language model (LLM) reliability in
  clinical natural language inference (NLI). Models encode relevant facts but fail
  to apply reasoning-type-specific inferential schemas, leading to fluent yet invalid
  judgments.
---

# Compartmentalised Agentic Reasoning for Clinical NLI

## Quick Facts
- arXiv ID: 2509.10222
- Source URL: https://arxiv.org/abs/2509.10222
- Reference count: 22
- Primary result: CARENLI improves macro accuracy from ~23% to ~57% on clinical NLI benchmark by compartmentalising inference into reasoning-type-specific agents.

## Executive Summary
Clinical natural language inference (NLI) requires models to perform diverse reasoning tasks, but LLMs often suffer from "schema collapse," defaulting to generic heuristics that yield fluent but invalid judgments. CARENLI addresses this by decomposing inference into four specialized agents: a Router that classifies input into reasoning families, a Solver that applies family-specific decision procedures, a Verifier that audits outputs for compliance, and a Refiner that corrects errors. Evaluated on an expanded CTNLI benchmark with 200 instances across four reasoning families (Causal Attribution, Compositional Grounding, Epistemic Verification, Risk State Abstraction), CARENLI improves macro accuracy from about 23% to about 57%, an average gain of roughly 34 percentage points across four LLMs. Performance gains are largest for structurally demanding families (Risk State Abstraction +51.9%, Epistemic Verification +40%), and most gains stem from the schema-constrained solver.

## Method Summary
CARENLI implements an agentic pipeline where inputs are first routed to one of four reasoning families (Causal Attribution, Compositional Grounding, Epistemic Verification, Risk State Abstraction), then processed by a family-specific solver that enforces procedural constraints, followed by verification and optional refinement. The approach was evaluated on a 200-instance CTNLI benchmark using four backbone LLMs (GPT-5.1, GPT-4.1, GPT-4o-mini, DeepSeek R1). The method includes ablation studies with Oracle Router (ground-truth reasoning type) and component-level analysis of routing, solving, verifying, and refining stages.

## Key Results
- Macro accuracy improves from ~23% to ~57% across all LLMs (average +34 percentage points)
- Solver component drives majority of gains (60.5% Oracle accuracy vs 23.0% baseline)
- Refinement contributes marginal gains (+0.7% overall), with largest impact on Causal Attribution (+6.9%)
- Performance gains vary by reasoning family: Risk State Abstraction (+51.9%), Epistemic Verification (+40%), Causal Attribution (+33.6%), Compositional Grounding (+11.2%)

## Why This Works (Mechanism)

### Mechanism 1: Schema Disambiguation via Routing
- **Claim:** Explicitly classifying the reasoning type before inference reduces "schema collapse," where models default to surface-level heuristics.
- **Mechanism:** A Router agent maps input pairs to one of four reasoning families (Causal, Compositional, Epistemic, Risk), forcing the model to acknowledge the structural demands of the task rather than treating all inferences as generic entailment checks.
- **Core assumption:** The model possesses latent competence to distinguish reasoning types if prompted explicitly.
- **Evidence anchors:** Abstract states "routes each premise-statement pair to a reasoning family... mitigating this failure"; Section 4.3.1 shows "Incorrect reasoning-type routing induces downstream performance collapse... absolute gap of 49.7%."

### Mechanism 2: Constraint Enforcement via Specialized Solvers
- **Claim:** Performance gains are primarily driven by specialized solvers that enforce strict procedural constraints rather than generic reasoning.
- **Mechanism:** Instead of generic Chain-of-Thought, specific solvers apply formal rules—such as interventionist causality checks or evidential hierarchies—to prevent conflating association with causation or deferring to authority over objective evidence.
- **Core assumption:** The backbone LLM has sufficient parametric knowledge to execute the formal logic if the steps are explicitly defined.
- **Evidence anchors:** Abstract notes "applies a specialised solver with explicit verification... improves mean accuracy... +34 points"; Section 4.3.2 reports "Oracle Solver attains a macro accuracy of 60.5%... majority of the performance gain... is attributable to the solver."

### Mechanism 3: Critique-Guided Refinement
- **Claim:** An explicit verification step identifies factual grounding errors or schema violations, allowing a refiner to correct trajectories without human intervention.
- **Mechanism:** A Verifier agent audits the solver's trace against the premise and the reasoning schema. If violations are found, a Refiner applies "minimal, schema-preserving edits."
- **Core assumption:** The model can reliably identify its own errors when separated into a "critic" role.
- **Evidence anchors:** Abstract mentions "explicit verification and targeted refinement"; Section 4.3.4 reports "Refinement offers marginal gains... +0.7%... largest gains observed for Causal Attribution (+6.9%)."

## Foundational Learning

- **Concept: Schema Collapse**
  - **Why needed here:** The paper defines this as the core failure mode—models applying a generic inference heuristic to heterogeneous clinical tasks (e.g., assuming "association implies causation").
  - **Quick check question:** Can you distinguish why a temporal association (Drug A, then Recovery) does not entail causality without a control group?

- **Concept: Interventionist Causality**
  - **Why needed here:** The Causal Solver relies on the definition $CE(T, Y) = E[Y|do(T=1)] - E[Y|do(T=0)]$.
  - **Quick check question:** Does the premise provide an interventional contrast (control/baseline), or only observational data?

- **Concept: Evidential Hierarchies**
  - **Why needed here:** The Epistemic Solver requires prioritizing objective data (labs, imaging) over subjective claims (physician opinion).
  - **Quick check question:** In a conflict between a normal blood test and a physician's suspicion, which evidence dominates?

## Architecture Onboarding

- **Component map:** Router → Solver → Verifier → (if violation) Refiner
- **Critical path:** The **Router -> Solver** link. Section 4.3.1 shows misrouting causes a ~50% performance drop. The Solver is the primary performance driver (Section 4.3.2), while Refinement offers only marginal gains (Section 4.3.4).
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Running 4 agents sequentially increases latency and cost.
  - **Rigidity vs. Generalization:** Hard-coded schemas improve accuracy on the benchmark but may fail on out-of-distribution clinical text not fitting the 4 families.
- **Failure signatures:**
  - **Compositional Blindness:** Even with correct routing, models like GPT-5.1 scored near 0% on Compositional Grounding (Section 4.2), suggesting latent capacity limits.
  - **Misrouting Cascades:** Confusion between Causal and Compositional types (Section 4.3.1) leads to the application of wrong logic.
- **First 3 experiments:**
  1. **Oracle Router Ablation:** Bypass the Router and feed ground-truth reasoning types to the Solver to establish the upper performance bound (Section 3).
  2. **Router Confusion Matrix:** Evaluate the Router in isolation to identify which reasoning families are being conflated (e.g., Causal vs. Compositional).
  3. **Solver-Only vs. Full Pipeline:** Compare Solver performance against the full Solver+Verifier+Refiner pipeline to quantify the specific value of the correction loop (expected delta ~0.7%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can routing accuracy be improved, particularly for Compositional Grounding cases where classification accuracy ranges from 12.9–98.4% across models?
- Basis in paper: [explicit] The paper identifies the Router as "a significant bottleneck" and states "incorrect reasoning-type routing induces downstream performance collapse," with solver accuracy dropping from 70.8% on correctly routed items to 21.1% on misrouted items.
- Why unresolved: The paper diagnoses routing as the primary bottleneck but does not explore alternative routing architectures or strategies to improve classification, especially for the unstable Compositional Grounding category.
- What evidence would resolve it: Experiments with hierarchical routing, multi-label classification allowing reasoning-type mixing, or contrastive learning approaches evaluated on the same CTNLI benchmark.

### Open Question 2
- Question: What mechanisms beyond structured prompting are required to address persistent compositional reasoning failures, which remain at 25.6% accuracy despite scaffolding?
- Basis in paper: [explicit] The paper concludes "COMPOSITIONAL GROUNDING remains the weakest regime" and that "CARENLI primarily improves execution and constraint adherence, rather than introducing new compositional reasoning abilities where they are largely absent."
- Why unresolved: Structured inference amplifies latent competence but does not create reasoning capability where baseline performance is near-zero, suggesting a fundamental limitation of prompting-based approaches for compositional tasks.
- What evidence would resolve it: Comparative studies testing neuro-symbolic integration, tool-augmented verification (e.g., external drug interaction databases), or fine-tuning approaches specifically targeting multi-factor constraint reasoning.

### Open Question 3
- Question: How does CARENLI performance transfer to unconstrained clinical text beyond template-generated benchmark instances?
- Basis in paper: [explicit] The limitations section states "instances are generated from typed templates and therefore do not fully reflect the linguistic variability, discourse structure, and documentation artefacts found in real trial protocols, clinical notes, or EHR-derived text."
- Why unresolved: The controlled benchmark design enables targeted analysis but limits conclusions about real-world applicability where language variability and documentation artefacts are prevalent.
- What evidence would resolve it: Evaluation on EHR-derived clinical notes, unstructured trial protocols, or prospective clinical workflow data using the same reasoning family taxonomy.

### Open Question 4
- Question: Can verification and refinement stages be redesigned to provide meaningful gains beyond the observed +0.7% overall improvement?
- Basis in paper: [explicit] The paper reports "Verifier and refiner stages contribute very marginal, task-dependent gains (overall refinement +0.7%)" and notes refinement effectiveness is bounded by verifier signal quality and solver trajectory correctness.
- Why unresolved: Despite being a key architectural component, the verification-refinement loop provides limited benefit, and the paper does not explore why verifiers fail to catch errors or how refinement could be made more effective.
- What evidence would resolve it: Error analysis of verifier false negatives, experiments with more granular or hierarchical verification criteria, or iterative self-correction loops with explicit error taxonomy targeting.

## Limitations
- Relies on synthetically generated benchmark (200 instances) that may not capture real-world clinical text complexity
- Modest test set size (50 instances per reasoning family) limits statistical power for per-family comparisons
- Verification and refinement steps show only marginal gains (+0.7% macro accuracy), suggesting limited robustness
- Approach assumes all clinical reasoning can be categorized into four families, potentially limiting generalization

## Confidence
- **High confidence:** Schema collapse occurs in clinical NLI and routing problems to specialized solvers improves accuracy (49.7% performance gap when misrouted)
- **Medium confidence:** Compartmentalisation primarily drives gains through solver component rather than refinement (verification mechanism's limited impact suggests potential brittleness)
- **Low confidence:** Generalizability to real clinical text beyond synthetic benchmark (near-zero performance on Compositional Grounding across all models suggests fundamental capability limits)

## Next Checks
1. **External validation on real clinical text:** Apply CARENLI to naturally occurring clinical notes or trial reports from sources like MIMIC-IV or clinical trial databases, measuring performance on multi-hop inferences not present in the synthetic benchmark.

2. **Error analysis of refinement failures:** Systematically categorize the types of errors that persist through the verification step (e.g., factual gaps, schema violations, model hallucinations) to determine whether refinement is addressing the right failure modes.

3. **Latency and cost benchmarking:** Measure the wall-clock time and API costs for processing clinical documents of varying lengths through the full CARENLI pipeline, comparing against baseline approaches to establish practical deployment constraints.