---
ver: rpa2
title: 'ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking'
arxiv_id: '2510.13842'
source_url: https://arxiv.org/abs/2510.13842
tags:
- shot
- passages
- admit
- gid00001
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ADMIT addresses the challenge of knowledge poisoning in RAG-based
  fact-checking systems, where injected content can manipulate verification outcomes.
  The core method, ADMIT, employs a few-shot, semantically aligned adversarial passage
  generation strategy that iteratively refines content under proxy verification to
  flip fact-checking decisions while maintaining plausible justifications.
---

# ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking

## Quick Facts
- **arXiv ID:** 2510.13842
- **Source URL:** https://arxiv.org/abs/2510.13842
- **Reference count:** 40
- **One-line primary result:** ADMIT achieves 86% attack success rate at 0.93×10⁻⁶ poisoning rate, outperforming prior state-of-the-art by 11.2%.

## Executive Summary
ADMIT introduces a few-shot knowledge poisoning attack on Retrieval-Augmented Generation (RAG) based fact-checking systems. The method generates adversarial passages that can flip fact-checking verdicts while maintaining plausible justifications. By employing iterative refinement under proxy verification and semantic alignment, ADMIT demonstrates effectiveness across four benchmarks, 11 large language models, and four retrievers, achieving high attack success rates even at extremely low poisoning rates.

## Method Summary
ADMIT (ADversarial Multi-Injection Technique) targets RAG-based fact-checking systems by injecting adversarial passages into the knowledge base to flip verification verdicts. The attack employs a proxy construction phase to generate opposing evidence, followed by an iterative optimization loop where passages are refined based on proxy verifier feedback. The method uses multi-turn generation with memory resets and adversarial prefix augmentation to ensure retrieval of injected content. Experiments demonstrate effectiveness across FEVER, HealthVer, SciFact, and Climate-FEVER benchmarks using various retrievers and victim LLMs.

## Key Results
- Achieves 86% average attack success rate at an extremely low poisoning rate of 0.93×10⁻⁶
- Outperforms prior state-of-the-art by 11.2% in attack effectiveness
- Maintains effectiveness even when clean evidence is retrieved by the system
- Shows robust performance across four different benchmarks and multiple victim models

## Why This Works (Mechanism)
ADMIT exploits the semantic alignment between adversarial passages and retrieval systems to ensure injected content is retrieved during fact-checking. The iterative refinement process leverages proxy verification to optimize passages for flipping verdicts while maintaining plausible justifications. The method's effectiveness stems from its ability to generate semantically relevant content that bypasses traditional verification mechanisms, even when clean evidence exists.

## Foundational Learning
- **RAG-based fact-checking:** Why needed - Understanding the retrieval-augmented generation pipeline for verification; Quick check - Can identify how passages are retrieved and used in the verification process
- **Knowledge poisoning:** Why needed - Grasping the concept of injecting adversarial content to manipulate system behavior; Quick check - Can explain how injected passages affect verification outcomes
- **Proxy verification:** Why needed - Understanding the use of a separate model to optimize attack effectiveness; Quick check - Can describe how proxy feedback guides passage refinement
- **Semantic alignment:** Why needed - Recognizing how adversarial content matches retrieval system semantics; Quick check - Can identify how prefix augmentation ensures passage retrieval
- **Iterative optimization:** Why needed - Comprehending the multi-turn refinement process for adversarial content; Quick check - Can trace the steps from initial generation to final attack success

## Architecture Onboarding

**Component Map:** Target Claim -> Proxy Observation Space -> Optimization Loop -> Adversarial Passage -> Knowledge Base Injection

**Critical Path:** The optimization loop (Algorithm 2) is the core of ADMIT, where the generator refines passages based on proxy verifier feedback. This iterative process is essential for achieving high attack success rates.

**Design Tradeoffs:** The method balances attack effectiveness with plausibility by using proxy verification to ensure generated passages appear legitimate while still flipping verdicts. The choice of GPT-4o as both proxy and generator raises concerns about overfitting and generalizability.

**Failure Signatures:** Low retrieval recall indicates issues with adversarial prefix augmentation or semantic alignment. High proxy ASR but low victim ASR suggests overfitting to the proxy verifier. Non-linear ASR scaling with shot count may indicate the model is outputting "Not Enough Info" instead of flipping verdicts.

**3 First Experiments:**
1. Replicate the attack success rate on FEVER benchmark with GPT-4o as victim model
2. Test retrieval recall of injected passages using different retrievers (BM25 vs Contriever-ms)
3. Validate transferability by testing attacks optimized with GPT-4o on other victim models like LLaMA3

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4o as both proxy verifier and generator may limit generalizability and introduce bias
- Exclusion of "Not Enough Info" cases from success metrics may overestimate practical impact
- Experiments conducted only on English datasets, limiting cross-linguistic applicability
- Specific implementation details of LOKI search and lightweight classification are not fully provided

## Confidence

**High Confidence:** Task definition and evaluation metrics are clearly specified; general method approach is well-explained

**Medium Confidence:** Reported attack success rates and improvements are convincing but may be influenced by GPT-4o dependency

**Low Confidence:** Critical implementation details for proxy observation space construction are missing

## Next Checks

1. Validate transferability by testing ADMIT on a diverse set of victim models not used in the proxy optimization loop
2. Replicate the attack using an alternative proxy verifier (e.g., Llama 2) to ensure results are not GPT-4o-specific
3. Evaluate ADMIT's effectiveness on fact-checking datasets in languages other than English to assess cross-linguistic robustness