---
ver: rpa2
title: Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis
arxiv_id: '2509.08007'
source_url: https://arxiv.org/abs/2509.08007
tags:
- few-shot
- learning
- attention
- image
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of limited expert-annotated
  data in medical image analysis by proposing an expert-guided explainable few-shot
  learning framework. The core method integrates radiologist-provided regions of interest
  (ROIs) into model training using a novel explanation loss based on Dice similarity
  between Grad-CAM heatmaps and expert annotations.
---

# Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis

## Quick Facts
- arXiv ID: 2509.08007
- Source URL: https://arxiv.org/abs/2509.08007
- Authors: Ifrat Ikhtear Uddin; Longwei Wang; KC Santosh
- Reference count: 34
- Primary result: Expert-guided explainable few-shot learning framework improves medical image classification accuracy using radiologist-provided ROIs and explanation loss

## Executive Summary
This study addresses the challenge of limited expert-annotated data in medical image analysis by proposing an expert-guided explainable few-shot learning framework. The core method integrates radiologist-provided regions of interest (ROIs) into model training using a novel explanation loss based on Dice similarity between Grad-CAM heatmaps and expert annotations. This loss is jointly optimized with a prototypical network objective to encourage clinically meaningful feature learning. Experiments on BraTS (MRI) and VinDr-CXR (Chest X-ray) datasets demonstrate significant accuracy improvements: from 77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to non-guided models. Grad-CAM visualizations confirm that expert-guided training aligns attention with diagnostic regions, enhancing both predictive reliability and clinical trustworthiness in data-scarce healthcare settings.

## Method Summary
The proposed framework combines prototypical networks with an expert-guided explanation loss to improve few-shot medical image classification. The method leverages radiologist-provided regions of interest (ROIs) as prior knowledge, using a Dice similarity-based explanation loss that measures alignment between model-generated Grad-CAM heatmaps and expert annotations. This explanation loss is jointly optimized with the prototypical network objective, encouraging the model to learn features that are both discriminative and clinically interpretable. During training, the model learns to generate attention maps that match expert-identified diagnostic regions while simultaneously learning class prototypes in the feature space. The approach addresses the data scarcity challenge in medical imaging by incorporating expert knowledge directly into the learning process, making the model more reliable for clinical applications where interpretability is crucial for adoption.

## Key Results
- Accuracy improved from 77.09% to 83.61% on BraTS MRI dataset using expert-guided approach
- Classification performance increased from 54.33% to 73.29% on VinDr-CXR chest X-ray dataset
- Grad-CAM visualizations demonstrate superior alignment with expert-identified diagnostic regions compared to non-guided models

## Why This Works (Mechanism)
The framework succeeds by incorporating domain expertise directly into the learning process through a novel explanation loss. By using Dice similarity to measure agreement between model-generated attention maps and radiologist-provided ROIs, the method ensures that the model focuses on clinically relevant regions during feature extraction. This expert guidance addresses the fundamental challenge in few-shot learning where models may learn spurious correlations from limited examples. The joint optimization with prototypical networks creates a feedback loop where the model simultaneously learns to generate interpretable attention patterns and meaningful class representations. This dual objective prevents the model from overfitting to noise in small datasets while maintaining clinical relevance, making the learned representations both accurate and trustworthy for medical diagnosis.

## Foundational Learning
**Prototypical Networks**: Metric learning approach that computes class prototypes as mean feature vectors of support examples, then classifies query samples based on distance to these prototypes. Needed to handle few-shot scenarios by learning meaningful feature representations from limited examples. Quick check: Verify prototype computation matches class mean in embedding space.

**Grad-CAM (Gradient-weighted Class Activation Mapping)**: Visualization technique that generates heatmaps highlighting important regions for model predictions by computing gradients of class scores with respect to feature maps. Needed to provide interpretable explanations of model decisions. Quick check: Ensure gradients flow correctly through network to generate meaningful attention maps.

**Dice Similarity Coefficient**: Metric measuring overlap between two sets, calculated as 2|Aâˆ©B|/(|A|+|B|). Needed to quantify agreement between model attention and expert annotations. Quick check: Verify Dice computation handles edge cases like empty regions correctly.

**Explanation Loss**: Custom loss function that encourages model attention to align with expert-provided ROIs using Dice similarity between Grad-CAM heatmaps and ground truth regions. Needed to incorporate domain expertise into training process. Quick check: Confirm loss gradients properly guide attention toward diagnostic regions.

## Architecture Onboarding

**Component Map**: Input Images -> Backbone CNN -> Feature Extractor -> Prototypical Network Module -> Class Prediction + Grad-CAM Generation -> Explanation Loss (Dice Similarity with Expert ROIs) -> Joint Optimization

**Critical Path**: The critical computational path involves forward propagation through the backbone to extract features, prototype computation from support set, distance calculation for query classification, Grad-CAM generation for attention visualization, and explanation loss calculation comparing attention maps to expert ROIs.

**Design Tradeoffs**: The framework trades computational overhead from explanation loss calculation and Grad-CAM generation against improved accuracy and interpretability. Using expert ROIs provides strong guidance but requires additional annotation effort and assumes ROI quality. The joint optimization balances classification accuracy with explanation quality, potentially sacrificing some pure classification performance for clinical trustworthiness.

**Failure Signatures**: Model performance degrades when expert ROIs are inaccurate or incomplete, leading to misaligned attention maps and poor generalization. The explanation loss may dominate classification objective if Dice similarity weights are too high, causing the model to prioritize attention alignment over discriminative feature learning. Insufficient support examples in few-shot settings can result in unstable prototype computation and unreliable distance metrics.

**First Experiments**: 
1. Validate Grad-CAM generation by comparing attention maps on known diagnostic regions against expert annotations
2. Test explanation loss sensitivity by varying Dice similarity weight in joint objective
3. Perform ablation study removing expert guidance to quantify performance impact

## Open Questions the Paper Calls Out
None

## Limitations
- Method assumes expert-provided ROIs are consistently accurate and representative across all classes
- Scalability challenged by dependence on radiologist time and expertise for ROI annotation
- Performance improvements may not fully represent real-world clinical deployment scenarios due to limited sample sizes in few-shot settings

## Confidence

**High Confidence**: The experimental methodology and statistical significance of accuracy improvements are well-documented and reproducible.

**Medium Confidence**: The clinical relevance and practical applicability of the approach in diverse healthcare settings.

**Medium Confidence**: The scalability and robustness of the method across different medical imaging domains and annotation quality levels.

## Next Checks
1. Conduct ablation studies to quantify the impact of ROI annotation quality on model performance by systematically degrading annotation precision
2. Test the framework on additional medical imaging modalities (e.g., CT, ultrasound) with varying ROI annotation requirements to assess generalizability
3. Implement a cost-benefit analysis comparing the computational overhead and radiologist time investment against accuracy gains in clinical deployment scenarios