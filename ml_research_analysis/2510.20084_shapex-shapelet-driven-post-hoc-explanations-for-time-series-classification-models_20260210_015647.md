---
ver: rpa2
title: 'ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification
  Models'
arxiv_id: '2510.20084'
source_url: https://arxiv.org/abs/2510.20084
tags:
- perturbed
- auroc
- proportion
- time
- shapex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHAPEX introduces a novel post-hoc explanation framework for time
  series classification that addresses the limitation of existing methods by focusing
  on shapelet-driven segment-level attribution rather than individual timesteps. The
  core idea involves learning a compact set of shapelets using the Shapelet Describe-and-Detect
  (SDD) framework, which segments time series into meaningful units aligned with these
  shapelets, and then applying Shapley value analysis to assess their contribution
  to classification outcomes.
---

# ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models

## Quick Facts
- arXiv ID: 2510.20084
- Source URL: https://arxiv.org/abs/2510.20084
- Reference count: 40
- One-line primary result: Achieves 58.12% average AUPRC improvement over existing methods on synthetic datasets for time series classification explanations

## Executive Summary
SHAPEX introduces a novel post-hoc explanation framework for time series classification that addresses the limitation of existing methods by focusing on shapelet-driven segment-level attribution rather than individual timesteps. The core idea involves learning a compact set of shapelets using the Shapelet Describe-and-Detect (SDD) framework, which segments time series into meaningful units aligned with these shapelets, and then applying Shapley value analysis to assess their contribution to classification outcomes. Experiments demonstrate that SHAPEX consistently outperforms existing methods, achieving an average AUPRC improvement of 58.12% on synthetic datasets and showing superior robustness across 114 real-world datasets from the UCR archive. The framework also provides theoretical justification for interpreting its attributions as causal rather than merely correlational, enhancing trustworthiness in high-stakes applications like healthcare and finance.

## Method Summary
SHAPEX operates through a two-stage process to generate interpretable explanations for time series classification models. First, it learns a compact set of shapelets using the Shapelet Describe-and-Detect (SDD) framework, which segments time series into meaningful units aligned with these shapelets. The SDD framework identifies local regions in the time series that resemble learned shapelets, effectively creating segment-level representations. Second, SHAPEX applies Shapley value analysis to assess the contribution of these segments to the black-box model's classification outcomes. This approach addresses the limitation of existing methods that attribute importance to individual timesteps, which often fails to capture the temporal dependencies and patterns that actually drive classification decisions. By focusing on shapelet-driven segments, SHAPEX provides explanations that are more aligned with the underlying decision-making process of time series classifiers.

## Key Results
- Achieves 58.12% average AUPRC improvement over existing methods on synthetic datasets for time series classification explanations
- Demonstrates superior robustness across 114 real-world datasets from the UCR archive, consistently outperforming gradient-based and perturbation-based methods
- Provides theoretical justification for causal interpretation of explanations, showing that shapelet-driven segment attribution captures meaningful causal relationships rather than mere correlations

## Why This Works (Mechanism)
SHAPEX addresses a fundamental limitation in existing post-hoc explanation methods for time series: they typically attribute importance to individual timesteps, which fails to capture the temporal patterns and segments that actually drive classification decisions. By learning shapelets that represent meaningful temporal patterns and segmenting time series based on these shapelets, SHAPEX aligns the explanation process with how time series classifiers actually make decisions. The Shapley value computation then fairly distributes the contribution of each segment to the final classification outcome. This approach is particularly effective because shapelets capture the essential discriminating features in time series data, and segmenting based on these shapelets ensures that explanations focus on the most relevant temporal regions. The framework's ability to provide causal rather than merely correlational explanations further enhances its trustworthiness and applicability in critical domains.

## Foundational Learning
- Shapelets in time series classification: Discriminative subsequences that are highly representative of class membership, used as features for classification
  - Why needed: Understanding how shapelets capture temporal patterns that drive classification decisions
  - Quick check: Can you explain why a shapelet-based classifier might outperform traditional distance-based classifiers?

- Shapley values for feature attribution: A game-theoretic approach to fairly distribute the contribution of features to model predictions
  - Why needed: To compute segment-level contributions in a theoretically sound manner
  - Quick check: How does Shapley value differ from simple feature importance scores?

- Segment-level vs. timestep-level attribution: The distinction between attributing importance to entire temporal segments versus individual time points
  - Why needed: To understand the core innovation of SHAPEX over existing methods
  - Quick check: Why might segment-level attribution be more meaningful than timestep-level attribution for time series explanations?

- Post-hoc explainability for black-box models: Methods for interpreting predictions from models whose internal workings are opaque
  - Why needed: To understand the context and requirements for SHAPEX's application
  - Quick check: What are the key differences between ante-hoc and post-hoc explainability approaches?

- Causal vs. correlational explanations: The distinction between explanations that capture causal relationships versus mere statistical associations
  - Why needed: To understand the theoretical contribution regarding the causal interpretation of SHAPEX's explanations
  - Quick check: Why is causal interpretability particularly important in high-stakes domains like healthcare?

## Architecture Onboarding

**Component Map:** Raw Time Series -> SDD Shapelet Learning -> Shapelet-Based Segmentation -> Shapley Value Computation -> Segment-Level Attribution

**Critical Path:** The SDD framework (shapelet learning and segmentation) is the critical path, as the quality of learned shapelets directly determines the quality of the resulting explanations. Poor shapelet selection will propagate through to suboptimal segmentations and ultimately weak attributions.

**Design Tradeoffs:** SHAPEX sacrifices some computational efficiency (requiring a separate training phase for shapelet learning) in exchange for more meaningful and causally interpretable explanations. The two-stage design allows for careful optimization of the explanation module but introduces additional complexity compared to gradient-based methods that operate directly on pre-trained models.

**Failure Signatures:** Explanations may become unreliable if the SDD framework fails to learn discriminative shapelets, if the time series contains noise that obscures meaningful patterns, or if the segmentation process creates segments that are too coarse or too fine-grained relative to the decision boundaries of the black-box classifier.

**First Experiments:** 1) Apply SHAPEX to a simple synthetic dataset with known shapelet patterns to verify that the framework can correctly identify and attribute importance to the true discriminating features. 2) Compare SHAPEX's segment-level attributions against ground truth segment labels in a controlled setting. 3) Test SHAPEX on a standard UCR dataset using a well-known classifier to establish baseline performance before exploring more complex scenarios.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the Shapelet Describe-and-Detect (SDD) framework be generalized to effectively handle multivariate time series?
- Basis in paper: [explicit] The authors state in the conclusion that future work involves "generalized formulations for multivariate and irregular time series."
- Why unresolved: The current methodology explicitly simplifies the problem to the univariate setting ($D=1$), and the SDD framework segments time series based on shapelets that may not capture cross-channel dependencies present in multivariate data.
- What evidence would resolve it: A modification of the SDD framework that learns shapelets across multiple channels simultaneously and a demonstration of its performance on multivariate benchmarks (e.g., UEA Archive).

### Open Question 2
- Question: How can the selection of hyperparameters, specifically shapelet length and count, be automated to reduce manual tuning?
- Basis in paper: [explicit] The conclusion notes that "user-defined hyperparameters... limit the methodâ€™s plug-and-play usability on unseen domains."
- Why unresolved: The framework currently requires "dataset-specific tuning and manual calibration" to balance interpretability and performance, creating a barrier to entry for new datasets.
- What evidence would resolve it: The development of an adaptive mechanism (e.g., based on signal complexity or information criteria) that dynamically determines the optimal number and size of shapelets without user intervention.

### Open Question 3
- Question: Can the computational overhead of the two-stage training process be reduced through lightweight or joint-training strategies?
- Basis in paper: [explicit] The authors acknowledge that the two-stage design "sacrifices some efficiency" and suggest future work could explore "lightweight or joint-training strategies."
- Why unresolved: SHAPEX requires a separate training phase to learn shapelets and optimize the SDD module, which introduces additional computational cost compared to gradient-based methods that operate on pre-trained models.
- What evidence would resolve it: A single-stage training objective that jointly optimizes the black-box classifier and the explanation module (SDD) while maintaining the causal fidelity of the explanations.

## Limitations
- Computational complexity of shapelet learning and Shapley value computation may limit scalability to very long time series or real-time applications
- Dependence on quality of shapelets learned through SDD introduces vulnerability - suboptimal shapelet sets degrade explanation quality
- Current framework focuses on time series classification, with applicability to regression or anomaly detection tasks unexplored

## Confidence
- High: AUPRC improvement metrics on synthetic datasets (58.12% average gain) and UCR archive results
- Medium: Causal interpretation claims, requiring more extensive validation across diverse model architectures and data distributions
- High: Robustness claims across 114 datasets, given large-scale empirical validation

## Next Checks
1. Conduct scalability experiments measuring SHAPEX's runtime performance on time series of varying lengths (e.g., 1K, 10K, 100K timesteps) to quantify computational bottlenecks and identify practical limits for real-time deployment.

2. Perform ablation studies systematically removing or modifying components of the SDD framework to isolate the contribution of shapelet quality to overall explanation performance, establishing sensitivity to this critical component.

3. Extend validation to non-classification tasks including time series regression and anomaly detection, testing whether the shapelet-driven attribution approach generalizes beyond its current scope or requires significant modifications for these applications.