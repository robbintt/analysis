---
ver: rpa2
title: 'From Transformers to LLMs: A Systematic Survey of Efficiency Considerations
  in NLP'
arxiv_id: '2406.16893'
source_url: https://arxiv.org/abs/2406.16893
tags:
- language
- llms
- arxiv
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys efficiency considerations in Transformer-based
  Large Language Models (LLMs) within Natural Language Processing (NLP), analyzing
  312 articles published from 2011 to 2025. The study addresses the escalating computational
  demands, energy consumption, carbon footprint, and financial costs associated with
  LLMs.
---

# From Transformers to LLMs: A Systematic Survey of Efficiency Considerations in NLP

## Quick Facts
- arXiv ID: 2406.16893
- Source URL: https://arxiv.org/abs/2406.16893
- Reference count: 40
- Primary result: Survey of 312 articles (2011–2025) analyzing efficiency improvements in Transformer-based LLMs, with benchmark evaluation of 30+ models on 13 tasks

## Executive Summary
This paper systematically reviews efficiency considerations in Transformer-based Large Language Models (LLMs) within Natural Language Processing (NLP). It analyzes 312 articles published from 2011 to 2025, addressing the escalating computational demands, energy consumption, carbon footprint, and financial costs associated with LLMs. The study covers efficiency-improvement strategies across four key phases: data curation, model design, model downsizing, and dynamic inferencing, along with LLM adaptation techniques including pre-training, fine-tuning, prompt-engineering, and Retrieval-Augmented Generation (RAG). Through statistical analysis and performance evaluation on 13 benchmarks, the paper compares over 30 renowned NLP models, assessing their efficiency and efficacy.

## Method Summary
The study follows a systematic literature review methodology, retrieving 9,717 articles from digital libraries (Google Scholar, ACL Anthology, PMLR, ACM, IEEE Xplore, Semantic Scholar, ScienceDirect) using specified keywords. After PRISMA-based screening (Figure 1), 312 articles were selected for analysis. Benchmark evaluation was performed on 30+ NLP models across 13 tasks (Arena Elo, AAII, TB Hard, AA-LCR, HLE, MMLU-Pro, GPQA Diamond, LiveCodeBench, SciCode, IFBench, AIME 2025, MATH 500, HumanEval), recording performance scores and efficiency metrics. Statistical analysis included year-wise and category-wise distribution of articles, price-performance Pareto analysis (Figure 22), and computational resource estimation using scaling laws and carbon calculators.

## Key Results
- Mixture-of-Experts (MoE) architectures like Mixtral 8x7B activate only 2 of 8 experts per token, achieving faster inference than dense 70B models while maintaining strong performance.
- Post-training quantization (PTQ) methods like GPTQ enable up to 60% sparsity without retraining, significantly reducing memory footprint and inference latency.
- Early Exit (EE) techniques reduce average inference depth by 30-50% on simpler inputs while maintaining accuracy, with methods like CALM using confidence thresholds for dynamic depth control.
- Open-weight models (DeepSeek, Gemma, LLaMA, Mistral) demonstrate strong performance-to-cost ratios, shifting the field toward more efficient modeling practices.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixture-of-Experts (MoE) architectures reduce inference compute by activating only a subset of parameters per input while maintaining model capacity.
- Mechanism: MoE divides computation across specialized "expert" sub-networks. During inference, a routing mechanism directs each token to only the most relevant experts (typically 1-2 out of many), reducing active FLOPs while keeping total parameter count high. The paper notes Mixtral 8x7B activates only two of eight 7B experts per token, achieving faster inference than dense 70B models.
- Core assumption: Input tokens can be partitioned into semantically coherent groups that benefit from specialized processing.
- Evidence anchors:
  - [Section 4.1.2]: "Mixtral 8x7B with 46.7 billion total parameters, activates only two of its eight 7B experts per token. This results in faster inference and superior performance over LLaMA 2 having 70B parameters."
  - [Section 4.1.2]: "DeepSeekMoE includes fine-grained expert segmentation and shared expert isolation to enhance specialization and reduce redundancy."
  - [corpus]: Weak direct evidence; corpus neighbors focus on attention mechanisms and state space models, not MoE architectures.
- Break condition: If routing becomes unbalanced (few experts handle most inputs), latency increases and specialization degrades; MoE efficiency gains diminish for tasks requiring many experts per token.

### Mechanism 2
- Claim: Post-training quantization (PTQ) reduces memory footprint and inference latency with minimal performance loss by lowering numerical precision of weights and activations.
- Mechanism: PTQ maps high-precision (e.g., FP16) weights to lower bit-widths (e.g., INT8, INT4) using calibration data to determine optimal quantization ranges. Weight-only PTQ (e.g., GPTQ, AWQ) quantizes weights while keeping activations in higher precision; weight-activation PTQ (e.g., SmoothQuant, OmniQuant) quantizes both. The paper reports methods like GPTQ enabling up to 60% sparsity without retraining.
- Core assumption: The distribution of weights/activations during calibration approximates inference distributions; outliers can be handled via scaling, clipping, or mixed-precision.
- Evidence anchors:
  - [Section 4.1.3]: "PTQ can be classified on the basis of whether both weights and activations are quantized... Weight-only PTQ... AWQ introduced activation-aware quantization by keeping salient weights tied to high activation magnitudes in higher precision."
  - [Section 4.1.3]: "Quantization-Aware Training (QAT) enables the model to be more robust to loss of information during quantization to very low precision."
  - [corpus]: No direct corpus evidence on quantization; neighbors discuss attention efficiency broadly.
- Break condition: Aggressive quantization (e.g., to 2-4 bits) without careful outlier handling causes accuracy collapse, especially on tasks with large activation variance.

### Mechanism 3
- Claim: Early Exit (EE) reduces average inference depth by stopping computation at intermediate layers when confidence thresholds are met.
- Mechanism: EE attaches classifier heads at multiple hidden layers. During inference, if the entropy or softmax confidence of an intermediate prediction falls below a threshold, the model exits early, skipping remaining layers. The paper cites CALM exiting when confidence gaps exceed thresholds, trimming KV cache to save memory.
- Core assumption: Simpler inputs require fewer layers to reach confident predictions; deeper layers add diminishing value for easy cases.
- Evidence anchors:
  - [Section 4.1.4]: "DeeBERT uses entropy thresholds... PABEE exits when predictions stabilize across layers... CALM exits when the confidence gap between the top predictions exceeds a threshold."
  - [Section 4.1.4]: "BEExformer relies on fractional changes in entropy of the logits between the Transformer units for EE. This eliminates the need for absolute thresholds."
  - [corpus]: No direct corpus evidence on early exit; neighbors discuss attention mechanisms, not dynamic depth.
- Break condition: Poorly calibrated thresholds cause premature exits (accuracy drops) or under-utilization (no efficiency gain); exits must be trained to avoid "overthinking" on hard inputs.

## Foundational Learning

- Concept: Self-Attention Complexity (O(n²) in sequence length)
  - Why needed here: All efficiency mechanisms (MoE, quantization, EE) assume understanding that attention cost dominates and must be reduced; sparse attention, chunking, and token pruning directly address this.
  - Quick check question: Why does doubling sequence length quadruple attention computation in a standard transformer?

- Concept: Inference vs. Training Compute Tradeoffs
  - Why needed here: PTQ optimizes inference; QAT adds training cost for better inference; MoE reduces inference but not training; understanding this separation is critical for deployment decisions.
  - Quick check question: Which technique reduces inference FLOPs without changing training cost?

- Concept: Pareto Efficiency (Performance vs. Cost)
  - Why needed here: The paper frames efficiency as Pareto improvement—achieving benchmark performance at lower cost; interpreting Table 3 and Figure 22 requires this lens.
  - Quick check question: What does it mean when a model lies "on the Pareto front" in Figure 22?

## Architecture Onboarding

- Component map:
  - Data pipeline: De-duplication → Subset selection → Curriculum ordering
  - Model core: Embedding → Sparse attention (or chunked) → MoE feed-forward → Low-rank projection
  - Compression layer: Pruning mask → Quantization scales → Knowledge distillation head
  - Inference runtime: Early exit classifier → Token pruning gate → Speculative decoding verifier

- Critical path:
  1. Profile baseline model (FLOPs, latency, memory) on target workload.
  2. Apply PTQ (start with INT8 weight-only), measure accuracy drop.
  3. Add structured pruning (remove heads/layers), fine-tune with LoRA.
  4. If latency still high, add early exit at 50% depth.
  5. For long-context, enable sliding window or chunked attention.

- Design tradeoffs:
  - **Accuracy vs. Speed**: Quantization to INT4 saves 4× memory but risks 5-10% accuracy loss; requires calibration.
  - **Generality vs. Specialization**: Contextual pruning (Mini-GPTs) excels on narrow domains but degrades on diverse tasks.
  - **Training vs. Inference Cost**: QAT requires full retraining; PTQ is fast but less robust to distribution shift.

- Failure signatures:
  - **MoE routing collapse**: Few experts handle >80% of tokens; accuracy plateaus. Check expert utilization histogram.
  - **Quantization outlier damage**: Accuracy drops sharply on specific inputs with large activations. Inspect activation ranges during calibration.
  - **Early exit over-pruning**: Model exits too early on hard inputs. Tune entropy threshold using validation set.

- First 3 experiments:
  1. **Quantization sweep**: Apply GPTQ (INT8, INT4) to a 7B model; measure MMLU score drop and inference speedup on 1K prompts.
  2. **Early exit calibration**: Add exit heads at layers 16/32 of a 32-layer model; sweep confidence thresholds (0.7, 0.8, 0.9) and measure accuracy vs. average depth.
  3. **Pruning + LoRA recovery**: Prune 30% of attention heads; fine-tune with LoRA on task data; compare to unpruned baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the field move beyond operational metrics to a standardized, life-cycle assessment framework for LLMs that accounts for embodied emissions from hardware manufacturing?
- **Basis in paper:** [Explicit] The Conclusion states future research must "prioritize the efficiency of LLMs throughout their life cycle also accounting with the environmental footprint of the infrastructure involved," while Section 5.2 notes that hardware production costs are "often unaccounted for."
- **Why unresolved:** Current estimation tools (e.g., CarbonTracker) focus primarily on operational energy consumption, overlooking the significant carbon footprint of chip fabrication and infrastructure setup.
- **What evidence would resolve it:** A widely adopted benchmarking protocol that integrates embodied carbon coefficients for hardware with operational energy metrics into a single, standardized reporting metric.

### Open Question 2
- **Question:** Given the weak correlations between disparate efficiency metrics (e.g., parameter count, FLOPs, latency), how can a unified, multi-dimensional efficiency score be formulated for fair model comparison?
- **Basis in paper:** [Inferred] Section 5.2 states that relying on one or two metrics is "inadequate" due to "weak correlations among them," specifically noting how MoE models can have high parameter counts but low FLOPs, creating confusion in efficiency evaluation.
- **Why unresolved:** Models often excel in one metric (e.g., low latency) while performing poorly in another (e.g., high energy cost), making it difficult to rank models objectively without a composite score.
- **What evidence would resolve it:** An empirical study proposing and validating a composite index (e.g., a "Pareto Efficiency Score") that normalizes trade-offs between cost, latency, and accuracy across diverse model architectures.

### Open Question 3
- **Question:** To what extent can algorithmic efficiency innovations (like irregular pruning or sparse attention) be translated into real-world speedups on standard hardware without specialized accelerator support?
- **Basis in paper:** [Inferred] Section 4.1.3 notes that unstructured pruning methods produce irregular sparsity patterns that "require specialized hardware accelerators," and Section 6.2.2 highlights a "disparity" in hardware access, implying a gap between theoretical efficiency and practical deployment.
- **Why unresolved:** Many research papers report theoretical FLOPs reductions, but actual wall-clock speedups often fail to materialize on standard GPUs due to memory bandwidth bottlenecks and lack of sparse matrix support.
- **What evidence would resolve it:** Comparative benchmarks of SOTA pruning techniques (e.g., SparseGPT) measuring actual inference latency and throughput on common consumer hardware versus theoretical FLOP counts.

## Limitations

- Survey methodology relies on keyword-based retrieval with subjective title/abstract screening, introducing potential selection bias in the 312-article shortlist.
- Benchmark evaluation lacks detailed protocols, model versions, and API settings, making direct replication challenging.
- Efficiency metrics (latency, throughput, pricing) are derived from public APIs that may vary over time and across regions.
- Carbon footprint estimates use scaling laws and external calculators with assumptions about hardware and PUE, as direct measurements are unavailable.

## Confidence

- **High Confidence**: Identification of efficiency improvement categories and their representative techniques; adoption of open-weight models for performance-to-cost ratio.
- **Medium Confidence**: Reported efficiency gains from specific techniques (MoE, quantization, early exit) may vary based on implementation details, hardware, and workload characteristics.
- **Low Confidence**: Comparative ranking of models in Figure 22 depends on aggregated metrics across diverse benchmarks where individual task performance nuances could shift positioning.

## Next Checks

1. **Replicate Benchmark Evaluation Protocol**: Select 5 models from Table 3 (e.g., GPT-5, Llama 4, Gemma 3, Mistral 7B, DeepSeek) and evaluate them on 3 representative benchmarks (MMLU, HumanEval, Arena Elo) using standardized prompts, seeds, and API settings. Compare results with paper-reported scores to quantify evaluation variance.

2. **Validate MoE Routing Efficiency**: Implement Mixtral 8x7B or a similar MoE architecture and measure expert utilization distribution, inference latency, and accuracy on a classification task. Verify that the routing mechanism activates only the intended subset of experts per token and that performance matches the paper's claims.

3. **Test Quantization Calibration Sensitivity**: Apply GPTQ to a 7B model (e.g., LLaMA 2) at INT8 and INT4 precision. Measure accuracy drop on MMLU and inspect activation distributions during calibration. Confirm that outlier handling prevents accuracy collapse as described in the paper.