---
ver: rpa2
title: 'Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning'
arxiv_id: '2511.21490'
source_url: https://arxiv.org/abs/2511.21490
tags:
- weight
- learning
- task
- base
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Merge-and-Bound (M&B), a novel training approach
  for Class Incremental Learning (CIL) that directly manipulates model weights to
  enhance stability and plasticity. The method introduces two weight merging techniques:
  inter-task weight merging, which averages parameters across all previous tasks to
  form a base model, and intra-task weight merging, which averages multiple checkpoints
  within the current task to improve generalization.'
---

# Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning

## Quick Facts
- arXiv ID: 2511.21490
- Source URL: https://arxiv.org/abs/2511.21490
- Authors: Taehoon Kim; Donghwan Jang; Bohyung Han
- Reference count: 4
- Key result: Consistently improves state-of-the-art Class Incremental Learning performance, especially with limited memory budgets

## Executive Summary
This paper introduces Merge-and-Bound (M&B), a novel training approach for Class Incremental Learning that directly manipulates model weights to enhance stability and plasticity. The method introduces inter-task weight merging (averaging parameters across all previous tasks to form a base model), intra-task weight merging (averaging multiple checkpoints within the current task to improve generalization), and a bounded model update strategy (constraining weight updates to preserve knowledge from previous tasks). M&B is easily integrated into existing CIL methods without modifying architectures or loss functions and demonstrates consistent performance improvements across CIFAR-100 and ImageNet benchmarks.

## Method Summary
Merge-and-Bound (M&B) is a plug-in module for Class Incremental Learning that consists of three weight manipulation techniques. First, inter-task weight merging averages feature extractor weights from all previous tasks to create a base model initialization for new tasks. Second, intra-task weight merging maintains a running average of model parameters during training to improve generalization. Third, bounded updates constrain weight changes to stay within a radius B of the base model, preventing catastrophic forgetting. The method also requires a forward pass to update BatchNorm statistics after merging, rather than resetting them. M&B works with any existing CIL method and requires minimal additional memory and computation.

## Key Results
- M&B consistently improves state-of-the-art CIL methods on CIFAR-100 and ImageNet-100/1000
- The approach is particularly effective when memory budgets are limited
- Inter-task merging provides superior stability compared to recent-task biased approaches (15.38% vs 17.41-19.12% forgetting)
- Intra-task merging significantly improves new task accuracy (59.35% vs 51.60% without it)
- Bounded updates align optimization directions across tasks, reducing catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1: Inter-task Weight Merging
- **Claim**: Inter-task weight merging preserves accumulated knowledge across sequential tasks by maintaining a unified parameter representation.
- **Mechanism**: The algorithm computes a running average of feature extractor weights from all previous tasks (θ^(base)_(k+1) = (k-1)/k · θ^(base)_k + 1/k · θ_k) and concatenates classifier weights for expanding class sets. This creates a base model that serves as initialization for each new task, encoding all prior knowledge in parameter space rather than relying on stored exemplars.
- **Core assumption**: Models from different tasks occupy compatible regions in parameter space where linear interpolation preserves functional capabilities (assumes models lie in the same or connected loss basins).
- **Evidence anchors**: Table 5 shows simple moving average outperforms exponential moving average variants (15.38% forgetting vs 17.41-19.12%), suggesting equal weighting across all tasks is superior to recent-task bias.

### Mechanism 2: Intra-task Weight Merging
- **Claim**: Intra-task weight merging improves generalization on new tasks by averaging checkpoints along the training trajectory.
- **Mechanism**: During training on task k, the method maintains a running average of model parameters sampled every e_a epochs (Θ^(avg)_k ← (n·Θ^(avg)_k + Θ_k)/(n+1)). This captures multiple points within the current task's optimization landscape, effectively finding a flatter minimum that generalizes better to the new class distribution.
- **Core assumption**: Multiple checkpoints within a single task's training trajectory lie in the same loss basin and their average represents a better solution than the final checkpoint alone.
- **Evidence anchors**: Table 3 (w/o intra-task) shows removing this component drops average new accuracy from 59.35% to 51.60% while only slightly improving forgetting, demonstrating its role in plasticity.

### Mechanism 3: Bounded Model Updates
- **Claim**: Bounded updates constrain parameter drift to preserve previous knowledge while still allowing task-specific adaptation.
- **Mechanism**: Weight updates are clipped to stay within a bounded radius B of the base model: if ||ΔΘ|| > B, then ΔΘ ← B·ΔΘ/||ΔΘ||. This directly limits how far the model can move from the accumulated knowledge encoded in the base model, preventing catastrophic forgetting while permitting sufficient adaptation.
- **Core assumption**: The base model parameters represent a region in weight space where knowledge from all previous tasks can be approximately preserved, and useful solutions for new tasks exist within a bounded distance.
- **Evidence anchors**: Figure 3 demonstrates M&B creates positively correlated task updates (orange heatmap) versus baseline's uncorrelated/negative correlations (blue heatmap), showing aligned optimization directions.

## Foundational Learning

- **Concept**: Stability-Plasticity Dilemma in Continual Learning
  - **Why needed here**: M&B explicitly addresses this tradeoff: inter-task merging provides stability (preserving old knowledge), intra-task merging provides plasticity (adapting to new tasks), and bounded updates balance both.
  - **Quick check question**: Can you explain why averaging weights from previous tasks helps stability, and why this might conflict with learning new tasks effectively?

- **Concept**: Loss Basins and Mode Connectivity
  - **Why needed here**: The paper assumes models from different tasks can be meaningfully averaged, which requires understanding that neural network loss landscapes have connected regions (basins) where interpolation preserves functionality.
  - **Quick check question**: Why would averaging two models that both achieve low loss on different tasks potentially create a model that works poorly on both tasks, and what does this tell us about loss landscape geometry?

- **Concept**: Catastrophic Forgetting in Sequential Learning
  - **Why needed here**: The entire paper addresses this phenomenon where neural networks overwrite previous knowledge when learning new tasks; understanding the mechanism (gradient updates on shared parameters) is essential.
  - **Quick check question**: When training on task B after task A, why does standard SGD on task B's data cause performance degradation on task A, and how does constraining ||ΔΘ|| address this?

## Architecture Onboarding

- **Component map**:
  Base Model (M_base_k) -> Feature Extractor (θ_base_k) + Classifier (φ_base_k)
  Training Loop for Task k -> Initialize from M_base_k -> Every e_a epochs: Update intra-task average -> Every e_b epochs: Apply bounded update constraint -> Post-training: Replace Θ_k ← Θ_avg_k
  BatchNorm Handling -> Forward pass on running statistics

- **Critical path**:
  1. Start each task with base model (not random or previous task model)
  2. Train normally but intercept updates every e_b epochs to clip magnitude
  3. Maintain separate intra-task average updated every e_a epochs
  4. At task completion: (a) update base model with current parameters, (b) use intra-task average for inference
  5. Handle BatchNorm: one forward pass using current running statistics (no reset)

- **Design tradeoffs**:
  - **Averaging period (e_a)**: More frequent averaging (e_a=1) vs. less frequent (e_a=15) shows minimal difference (65.38% vs 65.51%), but implementation complexity differs
  - **Bound threshold (B)**: Larger values allow more adaptation but risk forgetting; paper shows 10-15 range works well
  - **Memory overhead**: Minimal (just store running averages), but need to persist base model across tasks
  - **Computational cost**: ~0.011s per bounded update check, ~0.003s per merge operation, one extra epoch for BN statistics

- **Failure signatures**:
  - **Negative task update correlations** (Figure 3a): New task gradients actively oppose previous task directions → need tighter bounds or different initialization
  - **Severe BatchNorm miscalibration** (Table 6 "R" column): 25.60% accuracy indicates wrong statistics handling → ensure forward pass uses current running stats, not reset
  - **Limited improvement with architecture expansion methods** (FOSTER results): If baseline already uses ensemble/expansion strategies, M&B adds less value

- **First 3 experiments**:
  1. **Component ablation on simple scenario**: Implement just inter-task merging on CIFAR-100 with 5 tasks, verify base model initialization improves over standard sequential training. Expected: reduced forgetting, slight accuracy gain.
  2. **Bounding threshold sweep**: Test B ∈ {5, 10, 15} on 10-task CIFAR-100, measure both forgetting and new task accuracy. Expected: tradeoff curve, optimal around 10.
  3. **BatchNorm handling validation**: Compare three approaches (reset + recompute, current running stats, no update) on single task. Expected: current running stats approach should match or exceed others, validate the paper's choice.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does Merge-and-Bound (M&B) maintain its efficacy when applied to modern architectures such as Vision Transformers (ViT), given that the method is validated primarily on ResNet backbones?
- **Basis in paper**: The experimental section exclusively utilizes CNN-based backbones (ResNet-32, ResNet-18). The theoretical motivation relies on the "loss basin" connectivity of SGD solutions, a property that behaves differently in Transformers compared to CNNs.
- **Why unresolved**: Weight averaging via linear interpolation (inter-task merging) is known to be effective for CNNs, but the mode connectivity for Transformers in continual learning scenarios is less established and may be disrupted by the bounded update constraint.
- **What evidence would resolve it**: Experimental results applying M&B to a ViT backbone on standard CIL benchmarks (e.g., ImageNet-100) comparing the performance gap between CNN and Transformer implementations.

### Open Question 2
- **Question**: How can the weight merging strategy be adapted to better synergize with architecture expansion methods like FOSTER, where the performance gains were observed to be modest?
- **Basis in paper**: The authors note that M&B's improvements on FOSTER are "modest" because FOSTER's inherent ensemble effects and capacity expansion "reduce the unique benefits of the proposed approach."
- **Why unresolved**: M&B assumes a static parameter space where weights can be meaningfully averaged. In expansion methods, the network geometry changes, making direct parameter averaging (Equation 1) misaligned with the dynamic model structure.
- **What evidence would resolve it**: A modified merging algorithm that accounts for dynamic network capacity, or an ablation study showing statistically significant improvements of M&B on expansion-based methods after addressing the structural misalignment.

### Open Question 3
- **Question**: Can the reliance on an extra forward pass for Batch Normalization (BN) statistics re-estimation be eliminated while maintaining performance?
- **Basis in paper**: The paper states that "an extra post-training data forwarding is required" for BN layers to estimate mean and variance after merging, and that failing to do so (or resetting statistics) causes performance degradation.
- **Why unresolved**: This requirement introduces an additional computational overhead per task, partially countering the method's goal of being a low-cost "plug-in" technique. A method to merge or update statistics analytically without a data pass is not provided.
- **What evidence would resolve it**: An analysis showing that a running estimate of BN statistics during training (without the extra pass) can achieve equivalent performance, or a theoretical justification for why the forward pass is strictly necessary for stability.

## Limitations
- Effectiveness depends on tasks occupying compatible regions in parameter space, which may not hold for highly dissimilar tasks
- No theoretical guarantees for the bounded update strategy; bound B choice remains somewhat heuristic
- Performance heavily depends on proper BatchNorm handling, with severe degradation if statistics are reset

## Confidence

**High confidence** in stability-plasticity mechanism: Multiple experiments (Figure 3, Table 3-5) consistently show improved forgetting and accuracy

**Medium confidence** in weight averaging assumptions: Works empirically but lacks theoretical grounding for why averaging preserves functionality across tasks

**Medium confidence** in BatchNorm handling: Single forward pass approach is justified but could benefit from more thorough ablation

## Next Checks

1. Test M&B on tasks with known parameter space incompatibility (e.g., drastically different architectures or domains) to validate the averaging assumptions

2. Conduct ablation studies on bound B across different task similarity scenarios to better understand when tighter vs. looser bounds are optimal

3. Compare the single forward pass BN update against alternative methods (reset+recompute, frozen statistics) across multiple datasets to verify robustness of this design choice