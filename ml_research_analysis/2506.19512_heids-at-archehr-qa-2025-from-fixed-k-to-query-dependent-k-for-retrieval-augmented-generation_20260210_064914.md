---
ver: rpa2
title: 'heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval
  Augmented Generation'
arxiv_id: '2506.19512'
source_url: https://arxiv.org/abs/2506.19512
tags:
- clinical
- attribution
- pipeline
- patient
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes heiDS's approach for the ArchEHR-QA 2025 shared
  task on generating answers with attributions from clinical notes. The team designed
  a retrieval-augmented generation pipeline that explores query-dependent-k retrieval
  strategies instead of fixed-k approaches.
---

# heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2506.19512
- Source URL: https://arxiv.org/abs/2506.19512
- Reference count: 39
- Primary result: Match baseline performance with query-dependent-k retrieval (strict F1=0.37) while enabling practical scaling to longer clinical notes

## Executive Summary
This paper describes heiDS's approach for the ArchEHR-QA 2025 shared task on generating answers with attributions from clinical notes. The team designed a retrieval-augmented generation pipeline that explores query-dependent-k retrieval strategies instead of fixed-k approaches. They experimented with methods like surprise, autocut, autocut*, and elbow for determining how many retrieved sentences to include based on similarity score distributions. The pipeline also compared post-retrieval and post-generation attribution approaches using LLaMA-3.3-70B and Mixtral-8x7B models. Results showed their best-performing configuration used the surprise RLT strategy with post-retrieval attribution, achieving a strict F1-score of 0.37 and overall relevance of 0.35.

## Method Summary
The pipeline retrieves clinical note sentences using semantic search with BAAI/bge-large-en-v1.5 embeddings indexed in FAISS. Instead of fixed-k retrieval, it employs query-dependent-k strategies including surprise (using generalized Pareto distributions from extreme value theory), autocut, autocut*, and elbow methods to dynamically determine the number of sentences to retrieve based on similarity score distributions. Queries are constructed by combining patient questions with clinician-rewritten versions. Retrieved sentences are included in prompts with explicit citation formatting instructions, and the system compares post-retrieval attribution (LLM generates with pre-retrieved evidence) versus post-generation attribution (citations added after generation). LLaMA-3.3-70B serves as the primary LLM with one-shot prompting and 200-token limits.

## Key Results
- Best configuration: surprise RLT strategy with post-retrieval attribution using LLaMA-3.3-70B achieved strict F1=0.37 and overall relevance=0.35
- Post-retrieval attribution outperformed post-generation (F1=0.37 vs 0.27) with same model and retrieval config
- High precision (0.62) but low recall (0.26) indicates selective retrieval that frequently misses relevant evidence sentences
- Patient+clinician query combination improved retrieval relevance over single-query approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-dependent-k retrieval via the "surprise" RLT strategy adaptively selects context size based on score distributions, reducing manual tuning overhead.
- Mechanism: The surprise method applies generalized Pareto distributions from extreme value theory to retrieval scores, establishing a dynamic threshold that truncates the ranked list when similarity scores drop below expected distributions. This yields variable k per query rather than a uniform cutoff.
- Core assumption: Similarity score distributions contain exploitable structure that correlates with relevance boundaries.
- Evidence anchors:
  - [abstract] "Instead of using a fixed top-k RLT retrieval strategy, we employ a query-dependent-k retrieval strategy, including the existing surprise and autocut methods..."
  - [section 2.3] "This method determines the number k of sentences to consider by first adjusting retrieval scores using generalized Pareto distributions from extreme value theory... It truncates a ranked list using a score threshold, allowing for a variable number of relevant sentences to be selected per query."
  - [corpus] Related work on dynamic context selection (arXiv:2512.14313) addresses similar concerns about fixed-k limitations, providing convergent evidence for this design direction.
- Break condition: Uniform or monotonically decreasing similarity scores without clear distributional breaks may yield suboptimal truncation points.

### Mechanism 2
- Claim: Post-retrieval attribution outperforms post-generation attribution by constraining the LLM to cite only pre-retrieved evidence during generation.
- Mechanism: Retrieved sentences are labeled with IDs and included in the prompt; the LLM is explicitly instructed to place citations (e.g., |id|) at sentence ends. Post-processing validates citation placement and filters spurious references.
- Core assumption: LLMs can reliably follow citation formatting instructions when evidence is provided upfront.
- Evidence anchors:
  - [section 3.2] "LLaMA-3.3-70B combined with the surprise retrieval strategy achieves a strict F1-score of 0.37 and overall relevance of 0.35, making it our top post-retrieval configuration."
  - [section 3.2] Post-generation attribution yielded strict F1=0.27 vs. 0.37 for post-retrieval with the same model.
  - [corpus] RAGentA (arXiv:2506.16988) similarly emphasizes attribution in RAG pipelines, suggesting this is an active design concern.
- Break condition: Retrieved context exceeds model's effective context window or contains high distractor ratio, degrading citation accuracy.

### Mechanism 3
- Claim: Combining patient and clinician questions as a unified query improves retrieval relevance over either alone.
- Mechanism: Patient questions capture layperson terminology; clinician questions provide professional framing. Concatenating both enriches the semantic search query, yielding better coverage of relevant clinical note passages.
- Core assumption: The embedding model can meaningfully represent hybrid layperson-clinician semantics in a single query vector.
- Evidence anchors:
  - [section 2.2] "...a query that is constructed using both patient and clinical questions instead of considering only one of them (see Appendix D)..."
  - [appendix D, table 3] Patient+Clinician query: F1=0.30, O=0.31 vs. Patient-only: F1=0.27, O=0.30.
  - [corpus] No direct corpus evidence found for query fusion in medical RAG; this mechanism relies primarily on the paper's internal ablation.
- Break condition: Patient and clinician questions contain contradictory information or divergent focus areas, confusing the retrieval embedding.

## Foundational Learning

- Concept: **Ranked List Truncation (RLT) strategies**
  - Why needed here: Understanding how surprise, autocut, autocut*, and elbow methods differ is essential for selecting and tuning retrieval cutoffs.
  - Quick check question: Given a monotonically decreasing similarity score list [0.85, 0.72, 0.68, 0.65, 0.62, ...], which RLT method would struggle most to identify a natural cutoff?

- Concept: **Attribution paradigms (post-retrieval vs. post-generation vs. model-driven)**
  - Why needed here: The paper explicitly compares post-retrieval and post-generation; grasping the tradeoffs informs pipeline selection.
  - Quick check question: Why might post-generation attribution yield lower factuality scores even when the same evidence is available?

- Concept: **Extreme value theory in retrieval scoring**
  - Why needed here: The surprise method leverages generalized Pareto distributions; understanding the statistical foundation helps diagnose failure cases.
  - Quick check question: What assumption does EVT make about the tail distribution of retrieval scores, and when might this assumption fail?

## Architecture Onboarding

- Component map: Embedding layer (BAAI/bge-large-en-v1.5) -> FAISS vector store -> Retrieval with RLT truncation (surprise/autocut/elbow/autocut*) -> LLM backbone (LLaMA-3.3-70B/Mixtrot-8x7B) -> Prompting with citation formatting -> Post-processing (citation validation)

- Critical path: Query construction (patient + clinician) -> embedding -> FAISS retrieval -> RLT truncation -> prompt assembly -> LLM generation -> citation post-processing

- Design tradeoffs:
  - **Fixed-k vs. query-dependent-k**: Fixed-k offers simplicity and predictability; query-dependent-k adapts to query difficulty but introduces truncation uncertainty.
  - **Post-retrieval vs. post-generation attribution**: Post-retrieval yields higher factuality (F1=0.37 vs. 0.27) but requires retrieved context upfront; post-generation decouples generation from retrieval at accuracy cost.
  - **LLaMA-3.3-70B vs. Mixtral-8x7B**: LLaMA consistently outperformed Mixtral across all configurations in this task.

- Failure signatures:
  - **Low recall with high precision**: Surprise strategy showed P=0.62, R=0.26—model is selective but misses relevant evidence.
  - **Over-attribution**: Post-generation with low thresholds (0.1–0.2) attributes nearly every answer sentence to all retrieved sentences.
  - **Under-attribution**: High thresholds (0.7–0.9) yield sparse citations, missing valid evidence links.
  - **Prompt sensitivity**: Small prompt changes affected output quality; one-shot with strict formatting outperformed zero-shot.

- First 3 experiments:
  1. **Baseline replication**: Run fixed-k=54 with all clinical note sentences, patient+clinician query, one-shot prompt, LLaMA-3.3-70B, 200-token limit. Verify strict F1 ≈ 0.34–0.36.
  2. **RLT strategy ablation**: Compare surprise, elbow, autocut*, and fixed-k=[10,15,20] on retrieval metrics (Table 1) before downstream generation. Identify which yields best strict+lenient F1 balance.
  3. **Attribution paradigm comparison**: For a single retrieval config (e.g., surprise), run both post-retrieval and post-generation attribution with LLaMA-3.3-70B. Confirm post-retrieval outperforms by ≥0.05 F1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does domain-specific text enrichment, such as expanding medical acronyms before indexing, significantly improve retrieval performance in this RAG pipeline?
- Basis in paper: [explicit] The authors state in the Limitations section that "Expanding medical acronyms to their complete form or enriching texts with domain-specific interpretations before indexing could improve retrieval performance," but this was not implemented.
- Why unresolved: The current implementation indexes raw clinical text, potentially causing the retrieval mechanism to miss semantic matches for abbreviated medical terms.
- What evidence would resolve it: A comparative evaluation on the development set measuring retrieval recall and answer F1-scores using a pre-processed index with expanded acronyms versus the raw text index.

### Open Question 2
- Question: How can the "surprise" Ranked List Truncation (RLT) strategy be optimized to improve recall without sacrificing the observed high precision?
- Basis in paper: [inferred] The authors note that their best pipeline achieved high precision (0.62) but low recall (0.26), indicating that the strategy "frequently overlooks relevant evidence sentences" despite selecting correct attributions.
- Why unresolved: The current truncation method appears overly selective, omitting essential context that could otherwise strengthen the generated answers.
- What evidence would resolve it: Experiments modifying the "surprise" threshold parameters or hybridizing the method with a minimum fixed-k retrieval floor to measure the trade-off curve between precision and recall.

### Open Question 3
- Question: How does the relative performance of query-dependent-k strategies compare to full-context baselines when applied to clinical notes with significantly larger context windows?
- Basis in paper: [explicit] The authors argue that while they matched baseline performance, their pipeline is necessary because "real-world applications can contain far more text" where including complete texts is "infeasible due to LLMs input length constraints."
- Why unresolved: The shared task dataset contained a maximum of only 54 sentences per case, which fits within the context window of modern LLMs, making it difficult to assess the true utility of the truncation methods in realistic, high-volume settings.
- What evidence would resolve it: Evaluation of the pipeline on a dataset specifically constructed to exceed standard LLM context windows (e.g., >100k tokens), comparing RAG performance against "lost-in-the-middle" phenomena observed in full-context usage.

## Limitations
- The surprise RLT implementation details are not fully specified, relying on external citations and GitHub references that may contain unstated assumptions
- Exact one-shot prompt used in best-performing configuration is truncated in Appendix C.2, making precise reproduction challenging
- Attribution post-processing steps beyond basic citation formatting are not described in detail

## Confidence
- **High confidence**: Query-dependent-k retrieval outperforms fixed-k in reducing manual tuning (supported by ablation results showing surprise RLT achieving F1=0.37)
- **Medium confidence**: Post-retrieval attribution significantly outperforms post-generation (robust result across metrics, but implementation details affect outcomes)
- **Medium confidence**: Patient+clinician query combination improves retrieval (supported by internal ablation, but lacks external validation)

## Next Checks
1. Implement the surprise RLT truncation algorithm and validate its behavior on a sample similarity score distribution to ensure proper identification of relevance boundaries
2. Run an ablation study comparing all four RLT strategies (surprise, autocut, autocut*, elbow) on the dev set to identify the optimal balance between precision and recall
3. Test the attribution system's sensitivity to prompt variations by systematically modifying the citation formatting instructions and measuring the impact on factuality scores