---
ver: rpa2
title: Infrequent Exploration in Linear Bandits
arxiv_id: '2510.26000'
source_url: https://arxiv.org/abs/2510.26000
tags:
- regret
- exploration
- time
- infex
- logt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INFEX, a framework for infrequent exploration
  in linear bandits that combines a base exploratory algorithm with greedy actions
  according to a predefined schedule. The method addresses the gap between fully adaptive
  exploration (which explores every step) and purely greedy approaches (which require
  strong assumptions).
---

# Infrequent Exploration in Linear Bandits

## Quick Facts
- arXiv ID: 2510.26000
- Source URL: https://arxiv.org/abs/2510.26000
- Authors: Harin Lee; Min-hwan Oh
- Reference count: 40
- Primary result: INFEX achieves instance-dependent regret matching LinUCB/LinTS with logarithmic exploration threshold

## Executive Summary
This paper introduces INFEX, a framework for infrequent exploration in linear bandits that combines a base exploratory algorithm with greedy actions according to a predefined schedule. The method addresses the gap between fully adaptive exploration (which explores every step) and purely greedy approaches (which require strong assumptions). The authors prove that INFEX achieves instance-dependent regret matching standard algorithms like LinUCB or LinTS, provided exploration frequency exceeds a logarithmic threshold. Specifically, the regret bound is O(1/∆(log T + d log log T + d log d)²), matching existing results up to constants independent of T. They also show this logarithmic threshold is necessary. Empirical results demonstrate that INFEX outperforms both purely greedy and fully exploratory baselines in regret and computational efficiency, with exploration schedules of 80-99% greedy actions showing favorable performance. The framework is general and modular, allowing integration of any linear bandit algorithm.

## Method Summary
INFEX operates by interleaving greedy actions with periodic exploration according to a predefined schedule. At each time step t, the algorithm either selects the greedy action that maximizes expected reward based on current estimates or triggers exploration using a base linear bandit algorithm. The key innovation is proving that infrequent exploration (logarithmic in T) suffices to achieve optimal instance-dependent regret bounds, bridging the gap between fully adaptive methods and purely greedy approaches. The framework is modular, allowing any linear bandit algorithm (such as LinUCB or LinTS) to serve as the exploration mechanism.

## Key Results
- INFEX achieves instance-dependent regret O(1/∆(log T + d log log T + d log d)²), matching standard linear bandit algorithms
- The logarithmic exploration threshold is proven necessary - sub-logarithmic exploration fails to achieve optimal regret
- Empirical results show INFEX outperforms purely greedy methods and matches fully adaptive approaches while being computationally more efficient
- Exploration schedules with 80-99% greedy actions perform favorably across tested scenarios

## Why This Works (Mechanism)
The framework exploits the observation that optimal instance-dependent regret bounds depend primarily on the number of exploration rounds rather than their frequency. By carefully scheduling exploration at logarithmic intervals, INFEX accumulates sufficient information about the optimal arm while minimizing costly exploration steps. The base algorithm's performance during exploration phases drives the overall regret, while greedy actions exploit current knowledge to reduce immediate regret.

## Foundational Learning
- Linear bandit theory: Understanding regret bounds and the trade-off between exploration and exploitation
  - Why needed: Provides the theoretical foundation for analyzing exploration frequency requirements
  - Quick check: Verify that the regret decomposition separates exploration and exploitation phases correctly

- Instance-dependent analysis: Focusing on problem-specific parameters like gap ∆ between optimal and suboptimal arms
  - Why needed: Enables tighter bounds than worst-case analysis and reveals the logarithmic threshold
  - Quick check: Confirm that the instance-dependent bound degrades gracefully as ∆ → 0

- Concentration inequalities for linear regression: Tools like self-normalized processes and elliptical potential bounds
  - Why needed: Provides the mathematical machinery to control estimation error during infrequent exploration
  - Quick check: Validate that confidence sets remain valid despite sparse exploration

## Architecture Onboarding
**Component Map:**
INFEX base scheduler -> Exploration trigger -> Base linear bandit algorithm -> Greedy action selector -> Environment

**Critical Path:**
Schedule generation → Exploration decision → Base algorithm execution (when exploring) → Greedy action computation (when exploiting) → Reward observation and update

**Design Tradeoffs:**
- Fixed schedule vs. adaptive exploration frequency
- Computational overhead of base algorithm vs. exploration frequency
- Memory requirements for storing exploration data vs. schedule granularity

**Failure Signatures:**
- Regret growing faster than O(log T) indicates exploration frequency below threshold
- Suboptimal performance compared to fully adaptive methods suggests poor base algorithm choice
- Computational overhead exceeding theoretical savings indicates implementation inefficiencies

**First Experiments:**
1. Compare regret curves for different exploration frequencies (logarithmic, polynomial, constant)
2. Test with different base algorithms (LinUCB, LinTS, Greedy) to verify modularity
3. Measure computational time vs. regret trade-off across problem scales

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes fixed, predetermined exploration schedules that cannot adapt to changing environments
- Analysis restricted to stochastic linear bandits with fixed feature spaces
- Computational efficiency gains lack theoretical worst-case guarantees
- Limited empirical validation to synthetic and single real-world dataset

## Confidence
- Regret bounds: High
- Computational efficiency claims: Medium
- Empirical performance superiority: Medium
- Necessity of logarithmic exploration: High

## Next Checks
1. Test INFEX on non-stationary linear bandit problems where reward distributions or feature spaces change over time.
2. Implement INFEX with various base algorithms (e.g., Thompson Sampling, UCB-V) to verify modularity claims hold across different exploration strategies.
3. Conduct ablation studies varying exploration schedules beyond the tested 80-99% greedy range to identify optimal trade-offs in different problem regimes.