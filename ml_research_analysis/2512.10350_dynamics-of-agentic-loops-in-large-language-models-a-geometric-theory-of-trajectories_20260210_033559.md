---
ver: rpa2
title: 'Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of
  Trajectories'
arxiv_id: '2512.10350'
source_url: https://arxiv.org/abs/2512.10350
tags:
- cluster
- similarity
- loop
- semantic
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a geometric framework for analyzing agentic\
  \ loops\u2014recursive LLM systems where outputs become inputs\u2014as discrete\
  \ dynamical systems in semantic embedding space. The authors define trajectories,\
  \ clusters, attractors, and dynamical regimes (contractive, oscillatory, exploratory)\
  \ using calibrated semantic similarity that aligns with human judgments while preserving\
  \ local stability."
---

# Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories

## Quick Facts
- arXiv ID: 2512.10350
- Source URL: https://arxiv.org/abs/2512.10350
- Authors: Nicolas Tacheny
- Reference count: 17
- Primary result: Geometric framework defines trajectories, clusters, attractors, and dynamical regimes (contractive, oscillatory, exploratory) in semantic embedding space; prompt design directly controls regime behavior.

## Executive Summary
This paper introduces a geometric framework for analyzing agentic loops—recursive LLM systems where outputs become inputs—as discrete dynamical systems in semantic embedding space. The authors define trajectories, clusters, attractors, and dynamical regimes (contractive, oscillatory, exploratory) using calibrated semantic similarity that aligns with human judgments while preserving local stability. Through controlled experiments on singular loops, they demonstrate that prompt design directly controls dynamical regime: a paraphrase prompt produces contractive dynamics with convergence to stable attractors and decreasing dispersion (local similarity >0.85), while a summarize-and-negate prompt yields exploratory dynamics with unbounded divergence and no cluster formation (local similarity <0.5). The framework enables stability analysis, trajectory prediction, and principled design of composite loops that balance convergence and exploration.

## Method Summary
The authors develop a geometric theory where agentic loops are modeled as discrete dynamical systems. They introduce an isotonic calibration of cosine similarity to eliminate embedding anisotropy bias while preserving local stability. Trajectories are sequences of embeddings through semantic space, and clusters (attractors) are identified using an incremental detection algorithm that checks similarity, dispersion, and patience constraints. Three dynamical regimes are defined: contractive (convergent with stable attractors), oscillatory (periodic with multiple attractors), and exploratory (divergent with no clusters). The framework is validated through controlled experiments comparing paraphrase versus summarize-and-negate prompts.

## Key Results
- Contractive loops (paraphrase prompts) show convergence to stable attractors with local similarity >0.85 and decreasing dispersion over time
- Exploratory loops (summarize-and-negate prompts) exhibit unbounded divergence with local similarity <0.5 and no cluster formation
- Isotonic calibration achieves Spearman correlation ≈ 0.86 with human judgments while preserving local stability ≥98%
- Prompt semantics directly control dynamical regime, independent of model architecture

## Why This Works (Mechanism)

### Mechanism 1: Isotonic Calibration
Raw cosine similarity clusters around high values due to anisotropic embedding distributions. A monotonic mapping learned via isotonic regression on human-annotated pairs recalibrates the similarity function (̃s = f_isotonic(⟨e₁, e₂⟩)), aligning measurements with semantic judgments while preserving order. Core assumption: The calibration transfer from STS benchmark data generalizes to agentic loop outputs.

### Mechanism 2: Cluster Detection Algorithm
An incremental cluster detection algorithm processes trajectories sequentially. A cluster C persists when: (1) consecutive similarity ̃s(eₜ₋₁, eₜ) ≥ λ, (2) dispersion from center of gravity stays below ρ, and (3) no more than κ consecutive violations occur. The attractor is the L₂-normalized mean of cluster embeddings. Core assumption: Semantic coherence in embedding space reflects meaningful stability in the generative process.

### Mechanism 3: Prompt-Controlled Dynamics
The transformation operator F = LLM ∘ P maps artifacts through prompt-structured instructions. Paraphrase-style prompts produce small, consistent semantic displacements (local contractivity), while negation-style prompts produce large, erratic displacements (divergence). The prompt encodes an effective "contraction rate." Core assumption: Observed regime differences are caused by prompt semantics, not sampling noise or model-specific behavior.

## Foundational Learning

- **Discrete dynamical systems (trajectories, attractors, fixed points)**: The entire framework models agentic loops as iterative mappings aₜ₊₁ = F(aₜ); understanding convergence/divergence requires basic dynamical systems intuition.
  - Quick check: Given a function f(x) = 0.9x, does the sequence x, f(x), f(f(x)), ... converge or diverge?

- **Embedding anisotropy and cosine similarity bias**: Raw cosine similarity is biased because embeddings cluster in narrow cones; calibration is required for meaningful geometric measurements.
  - Quick check: Why might two semantically unrelated sentences have cosine similarity 0.7 in a pretrained embedding space?

- **Isotonic regression and monotonic calibration**: The calibration procedure uses isotonic regression to learn a monotonic mapping that corrects similarity scores without changing their relative ordering.
  - Quick check: If you have pairs (raw_score, human_score), what constraint does isotonic regression impose on the learned mapping?

## Architecture Onboarding

- **Component map**: [Prompt Template P] → [LLM Generation] → [Artifact aₜ] → [Embedding ψ(aₜ)] → [Calibrated Similarity ̃s] → [Cluster Detection Algorithm (λ, ρ, κ)] → [Attractor Centers] → [Regime Classification]

- **Critical path**:
  1. Implement or obtain a calibrated similarity function (requires human-annotated STS pairs for isotonic regression)
  2. Run agentic loop for T iterations, storing all artifacts
  3. Embed all artifacts using the same encoder ψ
  4. Apply incremental cluster detection with chosen (λ, ρ, κ)
  5. Classify regime based on cluster persistence and dispersion trends

- **Design tradeoffs**:
  - λ (similarity threshold): Higher values detect only tight clusters; may miss broader attractors
  - ρ (dispersion threshold): Lower values detect micro-clusters; higher values merge phases
  - κ (patience): Higher values tolerate noise but may delay detecting regime transitions
  - Embedding model choice: Different models produce different geometries; calibration must be re-run

- **Failure signatures**:
  - No clusters detected even with relaxed parameters → likely exploratory regime or calibration failure
  - Too many short-lived clusters → λ or ρ too strict, or high trajectory stochasticity
  - Clusters spanning entire trajectory → ρ too loose, no regime differentiation
  - Low correlation between calibrated similarity and semantic judgment → calibration data mismatch

- **First 3 experiments**:
  1. Replicate contractive/exploratory dichotomy with different prompts (e.g., "simplify" vs. "contradict") on same initial text to validate prompt-control hypothesis.
  2. Vary temperature (0.3, 0.8, 1.2) within same prompt to test whether sampling stochasticity affects regime classification.
  3. Test multiple embedding models (e.g., different sentence encoders) on same trajectory to assess robustness of cluster detection to embedding geometry.

## Open Questions the Paper Calls Out

None

## Limitations

- Isotonic calibration relies on STS benchmark data that may not fully represent agentic loop output semantics, particularly in high-drift or adversarial regimes
- Cluster detection algorithm depends on three hyperparameters (λ, ρ, κ) without systematic sensitivity analysis
- All experiments use single LLM and embedding model pair, leaving architectural dependence and generalization untested

## Confidence

**High Confidence**: The geometric definitions (trajectories, clusters, attractors, dynamical regimes) are mathematically sound and internally consistent. The contractive vs. exploratory regime distinction is clearly demonstrated through controlled experiments with measurable differences in similarity metrics.

**Medium Confidence**: The isotonic calibration genuinely aligns with human semantic judgments while preserving local stability. While the paper reports Spearman correlation ≈ 0.86 and local stability ≥ 98%, the calibration's performance on agentic loop-specific semantics remains untested beyond the initial benchmarks.

**Low Confidence**: Prompt design independently controls dynamical regime independent of model architecture. This claim extrapolates from single-model experiments without validation across different LLMs or embedding spaces. The mechanism may be confounded by model-specific prompt sensitivity or temperature effects.

## Next Checks

1. **Cross-Model Validation**: Run identical prompt experiments (paraphrase vs. summarize-and-negate) on at least three different LLM architectures (e.g., GPT-4, Claude, LLaMA) using the same embedding model. Verify that dynamical regimes remain consistent and that prompt semantics dominate architectural effects.

2. **Calibration Transfer Testing**: Generate agentic loop trajectories specifically designed to stress-test semantic boundaries (e.g., "explain quantum computing in increasingly abstract terms"). Compare isotonic calibration performance against raw cosine similarity and against calibration functions trained on loop-specific data. Quantify degradation in human alignment.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary λ (0.6→0.95), ρ (0.1→0.4), and κ (1→5) across multiple trajectories. Map how regime classification changes with parameter settings and establish guidelines for robust cluster detection across different semantic domains.