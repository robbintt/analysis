---
ver: rpa2
title: 'Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs
  in Healthcare Chatbot Settings'
arxiv_id: '2509.24506'
source_url: https://arxiv.org/abs/2509.24506
tags:
- evaluation
- data
- arxiv
- queries
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Samiksha, a community-centered evaluation
  pipeline for large language models (LLMs) in healthcare chatbot settings. The authors
  co-designed the pipeline with civil society organizations (CSOs) and community members
  in India to ensure evaluations reflect real user needs and cultural contexts.
---

# Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings

## Quick Facts
- arXiv ID: 2509.24506
- Source URL: https://arxiv.org/abs/2509.24506
- Reference count: 40
- Primary result: Human evaluators detected more nuanced quality differences than LLM judges in multilingual healthcare chatbot evaluation

## Executive Summary
This paper introduces Samiksha, a community-centered evaluation pipeline for large language models in healthcare chatbot settings. The authors co-designed the pipeline with civil society organizations and community members in India to ensure evaluations reflect real user needs and cultural contexts. They conducted interviews with five CSOs to identify healthcare query themes, then engaged 15 data workers to create and localize 1,590 queries across three Indian languages. The resulting benchmark was used to evaluate three state-of-the-art multilingual LLMs using both human annotators and LLM-as-judge methods. Human evaluators were more sensitive to nuanced quality differences than LLM judges, who tended to produce compressed, near-ceiling scores.

## Method Summary
The Samiksha pipeline consists of three phases: (1) CSO interviews to identify 8 healthcare themes and create evaluation rubrics, (2) data workers creating 810 queries from scratch and localizing 780 chatbot log queries across Hindi, Kannada, and Malayalam, and (3) LLM response generation followed by dual evaluation from human annotators and LLM judges. The evaluation used four dimensions (clarity, helpfulness, accuracy, completeness) on a 3-point ordinal scale, plus pairwise comparisons. Responses were generated from three multilingual LLMs (Sarvam-M, Qwen3-235B-A22B, Llama-3.1-405B-Instruct) using a "health expert" system prompt.

## Key Results
- Human evaluators detected more nuanced quality differences than LLM judges, who produced compressed, near-ceiling scores
- LLM judges showed weak correlation with human annotators (Pearson ≈0.13) but strong correlation with each other (Pearson ≈0.40)
- Community-driven query creation captured culturally-embedded health concerns missed by synthetic or translated benchmarks
- CSO engagement shaped evaluation criteria to reflect end-user priorities rather than institutional assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Community-driven query creation surfaces culturally-embedded health concerns that synthetic or translated benchmarks miss
- Mechanism: Data workers draw on lived experience to produce queries reflecting social determinants, cultural beliefs, and stigma
- Core assumption: Native speakers with domain familiarity encode contextual nuance that non-local annotators cannot recover
- Evidence anchors: CSO interviews identified themes like "socio-economic conditions, cultural beliefs and stigma, and gender norms"; queries addressed post-Caesarean delivery preferences, myths about clothing and infertility, and papaya leaves for dengue
- Break condition: Data workers default to generic queries or example-anchored imitation, degrading cultural specificity

### Mechanism 2
- Claim: Human evaluators detect nuanced quality deficits that LLM-as-judge methods systematically compress
- Mechanism: Humans apply culturally-informed expectations; LLM judges exhibit length/quality conflation and lower sensitivity to completeness gaps
- Core assumption: Ordinal scales and rubrics are interpreted comparably enough that score distributions reveal judge sensitivity
- Evidence anchors: Human-LLM judge correlation of only ≈0.13; compressed LLM means vs dispersed human means in Figure 2a
- Break condition: If LLM judges are calibrated on human-labeled multilingual data, compression may reduce

### Mechanism 3
- Claim: CSO engagement shapes evaluation criteria to reflect end-user priorities
- Mechanism: CSO interviews surface domain-relevant topics, red/green flag behaviors, and pragmatic constraints
- Core assumption: CSOs accurately proxy community needs and can translate tacit knowledge into actionable criteria
- Evidence anchors: CSOs emphasized "socio-cultural sensitivity," non-authoritative responses, context elicitation, and language accessibility
- Break condition: If CSO perspectives diverge from actual user preferences or if CSO time constraints limit iterative feedback

## Foundational Learning

- Concept: LLM-as-judge evaluation paradigm
  - Why needed here: Central to comparing automated and human evaluators; understanding compression and alignment issues is prerequisite
  - Quick check question: Can you explain why pairwise comparisons may surface distinctions that absolute ratings miss, and what "ceiling effect" means in ordinal LLM judging?

- Concept: Community-centered / participatory AI design
  - Why needed here: Pipeline's legitimacy depends on co-design with CSOs and data workers
  - Quick check question: What is the difference between top-down crowdsourcing and bottom-up community engagement, and why might local annotators outperform non-local ones for cultural evaluation?

- Concept: Multilingual and cross-cultural NLP evaluation
  - Why needed here: Benchmark covers Hindi, Kannada, Malayalam; translation vs localization vs native creation have different validity properties
  - Quick check question: Why might translating an English benchmark into Indian languages lose linguistic and cultural context, and how does "localization" differ from direct translation?

## Architecture Onboarding

- Component map: CSO interviews → thematic analysis → 8 healthcare topics + example queries + evaluation rubrics → Data workers create/localize queries → LLM response generation → Human annotators + LLM judges rate responses
- Critical path: CSO interview saturation → Data worker training and query creation → Dual-track evaluation with rationale collection
- Design tradeoffs: Human vs automated evaluation (cost vs nuance), example-driven vs open-ended query creation (scaffolding vs bias), query creation vs localization (richness vs real-user phrasing)
- Failure signatures: Example-anchored imitation producing duplicate queries, LLM judge ceiling effects, context-stripped localization, CSO-representativeness gaps
- First 3 experiments: 1) Replicate human vs LLM judge correlation analysis on held-out queries, 2) Ablate example specificity and measure query diversity, 3) Introduce hybrid evaluation workflow: LLM triage + targeted human annotation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-as-judge methods be calibrated or improved to better align with human evaluators in multilingual, culturally-grounded evaluation settings?
- Basis in paper: Authors state LLM-judges cannot replace human evaluators and identifying effective community input incorporation is a key research challenge
- Why unresolved: LLM judges produced compressed, near-ceiling scores and showed lower sensitivity to nuanced factual inconsistencies
- What evidence would resolve it: Methods that significantly improve LLM-human correlation scores in multilingual benchmarks

### Open Question 2
- Question: How can community-centered evaluation pipelines scale while maintaining quality control and avoiding example-driven bias in query generation?
- Basis in paper: Authors note providing guidance at individual worker level will be challenging at scale and topic-level examples introduced bias
- Why unresolved: Trade-off between scaffolding and avoiding biased outputs remains unaddressed at scale
- What evidence would resolve it: Scaled implementation demonstrating maintained query diversity without excessive coordinator intervention

### Open Question 3
- Question: What is the optimal balance between LLM-based triage and targeted human annotation in hybrid evaluation workflows?
- Basis in paper: Authors propose hybrid workflow as potential solution but acknowledge purely human evaluation would be prohibitive
- Why unresolved: Need identified but specific hybrid approach not tested or validated
- What evidence would resolve it: Empirical comparison of different human-LLM allocation strategies

## Limitations
- CSO engagement process relies on assumptions about CSO representativeness that cannot be fully validated
- LLM-as-judge compression effect remains partially unexplained without systematic control for response length confounds
- No evidence establishing whether community-centered evaluation yields better health outcomes compared to traditional methods

## Confidence
- **High Confidence**: Human evaluators detect more nuanced quality differences than LLM judges
- **Medium Confidence**: Community-driven query creation captures culturally-embedded health concerns
- **Medium Confidence**: CSO engagement shapes evaluation criteria to reflect end-user priorities

## Next Checks
1. Conduct controlled experiment comparing health outcomes between chatbots evaluated using traditional benchmarks versus Samiksha pipeline
2. Systematically vary response length in LLM-as-judge evaluation to quantify extent of length/quality conflation effects
3. Implement representative user study to validate that CSO-derived evaluation criteria align with actual community preferences