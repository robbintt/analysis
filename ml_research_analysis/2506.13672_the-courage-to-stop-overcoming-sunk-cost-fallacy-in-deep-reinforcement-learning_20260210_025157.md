---
ver: rpa2
title: 'The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning'
arxiv_id: '2506.13672'
source_url: https://arxiv.org/abs/2506.13672
tags:
- learning
- least
- deep
- reinforcement
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of inefficiency in deep reinforcement
  learning caused by agents continuing to interact with the environment in low-quality,
  familiar trajectories, which wastes samples and contaminates replay buffers. The
  authors introduce LEAST, a method that enables early termination of episodes based
  on Q-value and gradient statistics, allowing agents to recognize when to stop unproductive
  interactions.
---

# The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.13672
- Source URL: https://arxiv.org/abs/2506.13672
- Reference count: 40
- Primary result: LEAST improves sample efficiency across TD3, SAC, REDQ, and DrQv2 on MuJoCo and DeepMind Control Suite benchmarks by 20-30% via early episode termination based on Q-value and gradient statistics.

## Executive Summary
This paper addresses the inefficiency of deep reinforcement learning agents continuing to interact with the environment in low-quality, familiar trajectories, which wastes samples and contaminates replay buffers. The authors introduce LEAST, a method that enables early termination of episodes based on Q-value and gradient statistics, allowing agents to recognize when to stop unproductive interactions. By comparing current Q-values and learning gradients to historical buffers, LEAST dynamically determines when to terminate episodes, improving sample efficiency. Experiments on MuJoCo and DeepMind Control Suite benchmarks show that LEAST significantly improves learning efficiency across multiple RL algorithms, with faster convergence and better final performance.

## Method Summary
LEAST is a plug-in module for off-policy actor-critic methods that implements adaptive early episode termination. It maintains two reflection buffers storing Q-values and gradient magnitudes from recent episodes. At each step, it computes an adaptive threshold using the median Q-value and modulates it by a gradient-based learning potential weight. When the current Q-value falls below this threshold, the episode terminates early. The method also includes entropy-based dynamic buffer resizing and adaptive exploration noise scheduling to handle policy instability and prevent agents from re-entering suboptimal trajectories.

## Key Results
- LEAST achieves 20-30% improvement in sample efficiency across TD3, SAC, REDQ, and DrQv2 on MuJoCo and DeepMind Control Suite benchmarks
- Early stopping reduces low-quality transitions in replay buffer from ~30% to ~10% of total samples
- The method shows consistent performance gains across different task difficulties and observation types (state-based and image-based)
- Ablation studies confirm both Q-value and gradient criteria are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Dual-Criteria Adaptive Stopping Threshold
LEAST improves sample efficiency by terminating episodes when the current trajectory is both low-quality (low Q-value) and well-learned (low gradient magnitude). The method maintains two reflection buffers (B_Q for Q-values, B_G for gradient magnitudes) across the K most recent episodes. At each intra-episode step i, it computes an adaptive threshold ε_i = Median(B_Q[:,i]), then modulates it by a learning-potential weight ω_i = Median(B_G[:,i]) / G_i. When the current Q̂_i falls below ω_i × ε_i (or ω_i^(-1) × ε_i for negative thresholds), the episode is terminated and reset.

### Mechanism 2: Replay Buffer Contamination Prevention
Early stopping reduces the proportion of uninformative transitions (low Q, low loss) in the replay buffer, improving the signal-to-noise ratio during training. By terminating low-quality trajectories before completion, LEAST prevents the accumulation of "dead" samples that provide neither reward signal nor learning gradient. This shifts the buffer distribution toward higher-quality experiences.

### Mechanism 3: Entropy-Guided Buffer Resizing and Noise Adaptation
Dynamic adjustment of reflection set size and exploration noise stabilizes learning when the policy is rapidly changing or stuck in suboptimal patterns. The method uses entropy-aware resizing: if H(B_Q) exceeds (1+γ)×H̄, the buffer B_Q,L expands/contracts to maintain stable central-tendency estimates. When early stopping frequency β exceeds threshold e, exploration noise σ increases per a sigmoid schedule, helping escape suboptimal attractors.

## Foundational Learning

- **Concept: Off-policy Actor-Critic Methods (TD3, SAC, REDQ)**
  - Why needed here: LEAST is designed as a plug-in module for off-policy algorithms with replay buffers and Q-function critics. Understanding how actor gradients flow from critic estimates is essential for interpreting why gradient magnitude indicates learning potential.
  - Quick check question: Can you explain why clipped double Q-learning in TD3 reduces overestimation bias, and how this affects LEAST's threshold reliability?

- **Concept: Temporal Difference Learning and Replay Buffers**
  - Why needed here: The paper's core claim is that replay buffer contamination harms TD optimization. Understanding TD error as a learning signal is necessary to interpret the gradient buffer B_G.
  - Quick check question: If a transition has low TD error (small gradient), what does this imply about how well the current Q-function fits that experience?

- **Concept: Median vs. Mean for Robust Estimation**
  - Why needed here: LEAST explicitly uses median over mean for threshold calculation to handle outliers from noisy early-training Q-values. This is a critical design choice for stability.
  - Quick check question: In a distribution with heavy tails (extreme Q-value outliers), why would median provide a more stable stopping threshold than arithmetic mean?

## Architecture Onboarding

- **Component map:** Environment step -> Q-value and gradient computation -> Reflection buffer update -> Threshold calculation -> Stopping decision -> Reset trigger
- **Critical path:** 1) Environment step → collect (s, a, r, s') and compute Q̂ = min(Q_θ1, Q_θ2) 2) Compute TD loss L_i → store in B_G; store Q̂ in B_Q 3) After t_start steps, compute ε_i and ω_i from reflection sets 4) If Q̂_i < threshold → trigger reset, increment stop counter 5) If stop frequency β high → increase σ for next episodes 6) Periodically: check entropy H(B_Q) → resize buffers if needed
- **Design tradeoffs:** Start time (t_start): Earlier start improves efficiency but risks premature stopping before B_Q stabilizes. Paper recommends 10-20% of total training steps. Initial set size (K): Larger K provides more stable thresholds but over-smooths recent policy behavior. Paper recommends K=250 episodes. Weight ω sensitivity: SAC is more sensitive to ω than TD3. Range [0.3, 0.6] is robust across algorithms.
- **Failure signatures:** Oscillating resets: Agent repeatedly stops at same state → noise schedule may be too weak; check σ is increasing properly. Never stopping: Threshold ε_i consistently below Q̂_i → B_Q may be corrupted by early bad data; verify t_start timing. Performance collapse after stopping enabled: Check if median calculation has numerical issues; verify B_Q dimensions match episode length L.
- **First 3 experiments:** 1) PointMaze validation: Implement LEAST on SAC with small/medium/large maze layouts. Verify that early stopping reduces "dead end" visits. This isolates the stopping mechanism in a domain where suboptimal trajectories are clearly identifiable. 2) Buffer distribution analysis: Run vanilla SAC vs. SAC+LEAST on Ant-v4 for 200K steps. Plot replay buffer in (Q, TD-loss) space to confirm reduction in low-Q/low-loss quadrant. This validates the buffer-contamination hypothesis. 3) Ablation of dual criteria: Compare three variants on Walker2d: (a) Q-only threshold, (b) Q+gradient threshold, (c) full LEAST with dynamic buffer. Plot learning curves to quantify contribution of each component. This establishes that gradient weighting provides measurable benefit over naive Q-stopping.

## Open Questions the Paper Calls Out

### Open Question 1
Can a more robust evaluator be developed to stabilize training and reduce the performance variance observed when using LEAST? The current dual-criteria approach introduces variance in some tasks, suggesting the stopping heuristic is sensitive or noisy. Demonstrated reduction in performance variance across seeds and tasks without sacrificing sample efficiency gains would resolve this.

### Open Question 2
Can the stopping mechanism be refined to filter out "blue region" data (high loss, low Q-value) that currently persists in the replay buffer? The current threshold effectively filters low Q/low loss data, but fails to identify low-quality transitions that still generate high learning signals despite being low value. A shift in replay buffer distribution showing a significant density decrease in the high-loss/low-Q quadrant would resolve this.

### Open Question 3
How can agents be prevented from immediately re-entering suboptimal trajectories after an early termination reset? While LEAST stops bad trajectories, it does not inherently change the policy immediately, leading to potential cyclical behavior. Trajectory visualizations or state-visitation counts showing diverse exploration paths immediately following reset events would resolve this.

## Limitations
- The paper assumes gradient magnitude inversely correlates with learning potential, but this relationship may not hold in highly stochastic or non-stationary environments
- While the paper claims early stopping reduces buffer pollution, it doesn't directly measure the downstream impact on TD error distribution or convergence stability
- Performance gains are demonstrated primarily on locomotion tasks; effectiveness on sparse-reward or partially observable environments remains untested

## Confidence

- **High confidence**: The core mechanism of dual-criteria stopping (Q-value + gradient) is well-specified and experimentally validated across multiple RL algorithms (TD3, SAC, REDQ, DrQv2)
- **Medium confidence**: The entropy-based buffer resizing and adaptive noise scheduling provide theoretical benefits, but the exact parameter sensitivity and edge cases are not fully explored
- **Low confidence**: The claim that gradient magnitude is a reliable proxy for learning potential lacks rigorous theoretical grounding—it's empirically motivated but not proven

## Next Checks

1. **Gradient signal validation**: Run ablation studies on Walker2d where (a) gradient magnitude is replaced with TD error, (b) replaced with actor gradient norm, and (c) use raw Q-value without gradient weighting. Compare sample efficiency to isolate the true contribution of gradient-based stopping.

2. **Replay buffer analysis**: After 200K training steps on HalfCheetah, extract and analyze the replay buffer: (a) plot TD error histogram for vanilla vs. LEAST, (b) measure gradient magnitude distribution, (c) compute effective sample size using influence functions to quantify how much each sample contributes to the final policy.

3. **Exploration trade-off study**: Implement a variant of LEAST that tracks "exploration value" (e.g., state visitation entropy) and only stops if the trajectory is both low-quality AND low-exploration. Compare against vanilla LEAST on sparse-reward tasks like SparseHalfCheetah to test if this prevents premature termination of potentially valuable exploration.