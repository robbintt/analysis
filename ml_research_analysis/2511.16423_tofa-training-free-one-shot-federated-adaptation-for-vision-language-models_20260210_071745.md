---
ver: rpa2
title: 'TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models'
arxiv_id: '2511.16423'
source_url: https://arxiv.org/abs/2511.16423
tags:
- learning
- federated
- prompt
- data
- one-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOFA, a training-free one-shot federated
  adaptation framework for vision-language models (VLMs) in federated learning settings.
  The key innovation is adapting VLMs to downstream tasks through a single round of
  communication between clients and server, without requiring additional training
  resources on either side.
---

# TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models

## Quick Facts
- **arXiv ID:** 2511.16423
- **Source URL:** https://arxiv.org/abs/2511.16423
- **Reference count:** 40
- **Primary result:** TOFA achieves 3-4% accuracy improvements over baseline methods while maintaining training-free operation in federated vision-language model adaptation.

## Executive Summary
This paper introduces TOFA, a training-free one-shot federated adaptation framework for vision-language models (VLMs) in federated learning settings. The key innovation is adapting VLMs to downstream tasks through a single round of communication between clients and server, without requiring additional training resources on either side. TOFA addresses three core challenges in federated VLM adaptation: insufficient exploitation of multimodal information, lack of specialized adaptation strategies for severe data heterogeneity, and additional training resource requirements for clients or servers.

## Method Summary
TOFA employs both visual and textual pipelines to extract task-relevant representations. For the visual pipeline, it uses a hierarchical Bayesian model to learn personalized, class-specific prototype distributions where global visual information serves as a prior, enabling personalized adaptation while maintaining robustness. For the textual pipeline, it evaluates and globally aligns locally generated text prompts using large language models, ensuring robustness across diverse environments. An adaptive weight calibration mechanism combines predictions from both modalities, balancing personalization and generalization to handle data heterogeneity. The method operates without any model training, making it particularly suitable for resource-constrained distributed environments.

## Key Results
- TOFA consistently outperforms existing one-shot baselines and even surpasses several training-based federated VLM adaptation methods
- The method achieves accuracy improvements of up to 3-4% over baseline methods on various vision datasets
- Extensive experiments across nine datasets demonstrate TOFA's effectiveness in handling severe data heterogeneity through label shift and feature shift scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Personalized visual classification boundaries can be derived by treating global feature statistics as a Bayesian prior for local inference.
- **Mechanism:** The visual pipeline utilizes a hierarchical Bayesian model. Clients compute local sufficient statistics (mean and covariance of visual embeddings) and transmit them to the server. The server aggregates these into a global posterior distribution. This global distribution serves as the "power prior" for the client, allowing them to compute a local posterior that blends local data reality with global structural knowledge without gradient descent.
- **Core assumption:** The visual features extracted by the VLM encoder follow a class-conditional Gaussian distribution, allowing for a closed-form solution using conjugate priors (Inverse-Wishart).
- **Evidence anchors:**
  - [Section 3.2] "We first derive the parameters of the global prompt distribution... using the Bayes' formula... hierarchically, the personalized representation extraction problem aims to deduce the local posterior distribution, with the global prompt distribution serving as the informative prior."
  - [Lemma 1] Explicitly defines the conjugate prior using Inverse-Wishart and Gaussian distributions.
- **Break condition:** If visual embeddings do not approximate a Gaussian distribution (e.g., multimodal clusters per class), the Gaussian Discriminant Analysis (GDA) classifier performance will degrade.

### Mechanism 2
- **Claim:** Robustness against data heterogeneity is achieved by globally aligning locally generated text prompts using an importance scoring metric.
- **Mechanism:** The textual pipeline generates descriptions using LLMs. It evaluates these descriptions against a baseline (e.g., "a photo of a {class}") using a KL-divergence-like score derived from local classification probabilities. The server aggregates these scores to select prompts that consistently perform well across diverse clients, filtering out prompts that overfit to specific local biases.
- **Core assumption:** Manually designed prompts (e.g., "a photo of a...") provide a robust, unbiased baseline for measuring the quality of LLM-generated prompts.
- **Evidence anchors:**
  - [Section 3.3] "Since manually designed inputs... are considered the most robust text inputs... the score function... assigns a higher importance score to text prompts with stronger robustness."
  - [Equation 6] Defines the significance scoring criterion $r(t_c^m)$.
- **Break condition:** If the manually defined baseline prompt is semantically misaligned with the downstream task, the relative scoring will be skewed, leading to suboptimal prompt selection.

### Mechanism 3
- **Claim:** An adaptive fusion of visual and textual predictions optimizes the trade-off between personalization and generalization.
- **Mechanism:** TOFA fuses the output probabilities from the visual (personalized) and textual (robust) pipelines using a sample-wise coefficient $\eta(z)$. This coefficient is determined by the relative prediction confidence (max softmax probability) of the two modalities. If the visual classifier is uncertain, the model leans on the robust textual classifier, and vice versa.
- **Core assumption:** The confidence score of a well-calibrated classifier is a valid surrogate for its accuracy (calibration assumption).
- **Evidence anchors:**
  - [Section 3.4] "The key in our prediction fusion lies in introducing a sample-wise mixing coefficient $\eta(z)$... minimizing the generalization error requires $\eta$ to be proportional to $\ell_2 - \ell_1$."
  - [Theorem 1] Provides the theoretical bound for the generalization error of the fused classifier.
- **Break condition:** If the models are poorly calibrated (e.g., overconfident on outliers), the weighting mechanism will favor the wrong modality for difficult samples.

## Foundational Learning

- **Concept: Gaussian Discriminant Analysis (GDA)**
  - **Why needed here:** The visual pipeline is not a neural network classifier but a probabilistic one. Understanding how means and covariances define decision boundaries is essential to grasp why TOFA transmits statistics rather than weights.
  - **Quick check question:** How does the assumption of a shared covariance matrix across classes affect the shape of the decision boundary?

- **Concept: Conjugate Priors (Bayesian Inference)**
  - **Why needed here:** The "training-free" nature relies on the fact that the posterior distribution has the same functional form as the prior. This allows the server to "add" client statistics to global statistics analytically.
  - **Quick check question:** Why is the Inverse-Wishart distribution used as the prior for the covariance matrix in a Gaussian likelihood model?

- **Concept: Modality Gap in VLMs**
  - **Why needed here:** TOFA fuses visual and textual predictions. Understanding that embeddings from different modalities lie in separate regions of the shared space explains why separate calibration and fusion are necessary.
  - **Quick check question:** Why can't we simply average the visual and text embeddings directly before classification?

## Architecture Onboarding

- **Component map:**
  - Client Side: Visual Module (computes sufficient statistics) -> Text Module (generates prompts and computes importance scores) -> Transmission to Server
  - Server Side: Aggregator (computes global posterior parameters and aggregates text importance scores) -> Transmission to Clients
  - Inference (Client-side): Fusion (combines local GDA classifier and weighted text classifier using confidence-based calibration)

- **Critical path:** The transmission of the sufficient statistics (S_k, ν_k, m_k) from client to server. If these statistics are corrupted or computed over insufficient data samples (N_c is too small), the covariance matrices may not be positive definite, breaking the hierarchical Bayesian update in Lemma 1.

- **Design tradeoffs:**
  - **Hyperparameter α:** Controls the influence of the global prior. Low α favors local personalization (risking overfitting); High α favors global generalization (risking underfitting on local outliers).
  - **Storage vs. Communication:** TOFA is communication-efficient (one-shot) but requires clients to store/generate LLM prompts and compute full covariance matrices, which may be memory-intensive for high-dimensional embeddings (e.g., ViT-L/14).

- **Failure signatures:**
  - **Singular Matrices:** If a client has only 1 or 2 samples for a class, the local covariance S_k cannot be estimated, leading to numerical instability.
  - **Semantic Drift:** If LLMs generate descriptions irrelevant to the visual domain (hallucination), the text pipeline will actively degrade performance if the alignment score r(t) fails to filter them out.

- **First 3 experiments:**
  1. **Sanity Check (Gaussian Assumption):** Visualize the t-SNE of visual embeddings for a single class across multiple clients to verify if they form elliptical (Gaussian) clusters or distinct multi-modal clusters.
  2. **Ablation on Fusion:** Run TOFA with η(z) fixed to 0 (Text-only) and 1 (Visual-only) to establish the performance bounds of the fusion mechanism.
  3. **Sensitivity to Data Scarcity:** Test the stability of the Bayesian update (Lemma 1) by systematically reducing the number of local shots (1-shot vs. 16-shot) to identify the breaking point where covariance estimation fails.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The method relies on the assumption that VLM visual features follow a Gaussian distribution per class, which may not hold for complex datasets with multi-modal class distributions.
- TOFA requires clients to have access to consistent LLM versions and synchronized parameters, which may not be feasible in all federated environments.
- The one-shot communication constraint, while enabling training-free operation, limits the model's ability to adapt to severe data drift or non-stationarity between communication rounds.

## Confidence
- **Medium:** The core assumption that VLM visual features follow a Gaussian distribution per class is reasonable but unverified across all datasets.
- **Low:** The robustness of the text pipeline depends critically on the choice of manually defined baseline prompts, which may be semantically misaligned with certain domains.
- **Medium:** The one-shot communication constraint is both a feature and a limitation, with effectiveness in dynamic, evolving environments remaining untested.

## Next Checks
1. **Diagnostic for Gaussian Assumption:** For each dataset, perform a normality test (e.g., Shapiro-Wilk) or visualize the visual embeddings per class to verify the elliptical cluster assumption underlying the Bayesian update.
2. **Prompt Robustness Analysis:** Systematically vary the baseline prompt ("a photo of a {class}", "a picture of a {class}", etc.) and measure the variance in selected LLM-generated prompts and downstream accuracy.
3. **Extreme Data Scarcity Test:** Evaluate TOFA's performance with 1-shot per class on highly heterogeneous datasets to identify the breaking point of the covariance estimation and Bayesian update.