---
ver: rpa2
title: Applications and Challenges of Fairness APIs in Machine Learning Software
arxiv_id: '2508.16377'
source_url: https://arxiv.org/abs/2508.16377
tags:
- fairness
- bias
- apis
- repositories
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application and challenges of fairness
  APIs in machine learning software. We analyze 204 GitHub repositories that utilize
  13 fairness APIs to understand their real-world use cases and challenges.
---

# Applications and Challenges of Fairness APIs in Machine Learning Software

## Quick Facts
- arXiv ID: 2508.16377
- Source URL: https://arxiv.org/abs/2508.16377
- Reference count: 40
- Primary result: Study of 204 GitHub repositories using 13 fairness APIs reveals learning-focused adoption and significant developer challenges in fairness implementation

## Executive Summary
This study investigates how fairness APIs are applied in real-world machine learning software and identifies key challenges developers face. Analyzing 204 GitHub repositories and 4,212 issue discussions, the research reveals that while fairness APIs are primarily used for generic learning (70.59%), they are increasingly being applied to solve specific real-world problems across 17 domains. The findings highlight that developers struggle most with understanding fairness concepts, selecting appropriate metrics, and troubleshooting issues. Group fairness metrics dominate usage, while individual and subgroup fairness remain largely unexplored. These insights provide critical guidance for improving fairness API design and adoption.

## Method Summary
The study employed GitHub Code Search to identify 204 Python repositories using 13 fairness API libraries, filtering through AST analysis to remove false positives. The researchers analyzed 4,212 issue discussions from 11 library repositories using LDA topic modeling (33 topics) to identify developer challenges across six SDLC stages. Open card sorting was used to classify 17 use cases into three activity types: Prediction, Analysis, and Operation. The analysis combined quantitative usage patterns with qualitative issue analysis to provide a comprehensive view of fairness API adoption and challenges.

## Key Results
- 70.59% of repositories use fairness APIs for generic learning and exploration, while 29.4% target real-world problem solving
- Bias detection primarily focuses on group fairness metrics, with subgroup and individual fairness metrics virtually absent
- In-processing approaches (especially fairness constraints) are the most common mitigation strategy at 44.8% usage
- Understanding fairness concepts and metric selection are the top challenges developers face

## Why This Works (Mechanism)

### Mechanism 1: The "Generic to Specific" Adoption Curve
- **Claim:** The observed high volume of generic learning repositories (70.59%) suggests that fairness APIs currently function more as educational scaffolding than as plug-and-play production components.
- **Mechanism:** Developers initially engage with fairness APIs through tutorials and toy examples to overcome the "Methodological/Opinion" barrier identified in issue discussions; only after this exploratory phase do they attempt to integrate these APIs into specific real-world domains like healthcare or legal systems.
- **Core assumption:** The prevalence of learning-focused repositories (RQ1) implies a necessary precursor step for developers to build intuition before tackling context-specific fairness.
- **Evidence anchors:**
  - [abstract] The paper identifies two primary purposes: learning and solving real-world problems.
  - [section 4.2] 70.59% of repositories focus on generic learning; issues related to "Opinion" and "Understanding Fairness Definition" are significant (RQ4).
  - [corpus] Related work on "Software Fairness Dilemma" suggests mitigation is complex and likely requires this learning phase to avoid zero-sum outcomes.
- **Break condition:** If documentation improved to the point where "Requirement Analysis" issues dropped significantly, we would expect to see the proportion of generic repositories decrease in favor of direct production implementation.

### Mechanism 2: Metric Selection via Granularity Avoidance
- **Claim:** The strong preference for Group and Utility fairness metrics over Individual or Subgroup metrics is likely driven by implementation complexity and the difficulty of defining context-specific fairness, rather than a lack of bias at finer granularities.
- **Mechanism:** Developers default to Group Fairness (Independence/Separation) and Utility metrics because they are easier to implement and interpret; Subgroup and Individual metrics remain "non-existent" in practice despite their theoretical importance for preventing "gerrymandering" or individual discrimination.
- **Core assumption:** The absence of Individual/Subgroup metric usage in the 204 repositories is a signal of high barriers to entry (conceptual or technical) rather than a lack of necessity.
- **Evidence anchors:**
  - [section 5.1.3] "Subgroup and Individual fairness metrics are non-existent" despite being available in the libraries.
  - [section 2.1] The paper defines five metric types, but Section 5.1 shows only Group and Utility are used in non-generic cases.
  - [corpus] The corpus notes "A Zoo of Fairness Metrics," reinforcing that the sheer variety may overwhelm developers, leading them to simpler options.
- **Break condition:** If tooling made Individual/Subgroup metrics as accessible as Group metrics, their adoption rate would likely increase to address "blind spots."

### Mechanism 3: In-Processing Dominance via Pipeline Integration
- **Claim:** The shift toward In-processing (specifically Fairness Constraints) as the most common mitigation approach indicates a preference for embedding fairness directly into the model training optimization loop.
- **Mechanism:** Developers find it more effective to constrain the model during training (In-processing) rather than altering the input data distribution (Pre-processing) or flipping labels post-hoc (Post-processing).
- **Core assumption:** The trend identified in RQ3 reflects a strategic architectural choice by practitioners to treat fairness as a first-class constraint during model design.
- **Evidence anchors:**
  - [section 5.2] In-processing accounts for 44.8% of mitigation, surpassing Pre-processing (37.9%); Fairness Constraints specifically account for 20.7%.
  - [section 5.2.3] The trend analysis (Figure 11) shows In-processing becoming the most popular approach over time.
- **Break condition:** If "Fairness Constraints" significantly degrade model performance (a concern raised in related literature), we might see a regression to Pre-processing techniques like Resampling.

## Foundational Learning

- **Concept: Group vs. Individual Fairness**
  - **Why needed here:** The study reveals that practitioners almost exclusively use Group fairness, potentially ignoring individual discrimination. Understanding the distinction is critical to diagnosing the "blind spots" in current architectures.
  - **Quick check question:** Does your current metric ensure fairness for an individual regardless of their group, or does it only ensure statistical parity between groups?

- **Concept: Mitigation Phases (Pre, In, Post)**
  - **Why needed here:** The paper maps mitigation usage to ML pipeline stages. Selecting the wrong phase (e.g., Post-processing when you control the training data) is a major architectural decision point.
  - **Quick check question:** If you cannot retrain the model but can modify the output labels, which mitigation phase is your only option?

- **Concept: Context-Based Fairness Definition**
  - **Why needed here:** "Understanding Fairness Definition" is a top challenge in issue discussions. Developers struggle to map abstract fairness definitions to specific domain constraints (e.g., healthcare vs. legal).
  - **Quick check question:** Can you articulate why "Demographic Parity" might be an inappropriate fairness metric for a loan approval system where creditworthiness is correlated with sensitive attributes?

## Architecture Onboarding

- **Component map:** Core Libraries (AIF360 -> Fairlearn) -> Pipeline Stages (Data Labeling -> Model Training -> Evaluation) -> Metric Layer (Utility + Group Fairness)
- **Critical path:**
  1. Requirement Analysis: Define what "fair" means for the specific context (RQ4 highlights this as a major bottleneck)
  2. Metric Selection: Select Group or Utility metrics based on available data; avoid Individual/Subgroup unless advanced tooling is available
  3. Mitigation Integration: Implement In-processing (Fairness Constraints) during the training phase
  4. Validation: Verify using Group metrics (e.g., Disparate Impact, Average Odds Difference)
- **Design tradeoffs:**
  - Accessibility vs. Granularity: Using Group/Utility metrics (Accessible) vs. Individual/Subgroup metrics (Granular but complex)
  - Pipeline Stage: In-processing (Integrated but model-invasive) vs. Pre-processing (Model-agnostic but may lose info)
- **Failure signatures:**
  - Stuck in "Requirement Analysis": High volume of "Opinion" questions in logs; inability to select a metric
  - Metric Mismatch: Using only Utility metrics (Accuracy) without Group metrics for a sensitive domain
  - Adoption Failure: Repository remains in "Generic Learning" phase (Tutorial) and never graduates to "Real-World" usage
- **First 3 experiments:**
  1. Metric Audit: Run the 5 metric classes (Group, Individual, etc.) on a benchmark dataset to observe the discrepancy between Group and Individual fairness scores
  2. In-Processing Integration: Implement a "Fairness Constraints" algorithm (e.g., `ExponentiatedGradient` from Fairlearn) in a standard classifier and measure the accuracy drop vs. fairness gain
  3. Issue Mining: Analyze 50 random GitHub issues from a fairness library to classify them by SDLC phase (Requirement, Deployment, Maintenance) to confirm the "Requirement Analysis" bottleneck

## Open Questions the Paper Calls Out

- **Open Question 1:** What are the specific barriers preventing the adoption of subgroup and individual fairness metrics in real-world ML software, and how can they be overcome?
  - Basis in paper: [explicit] The RQ2 Summary states: "Future studies should explore the challenges of developing and adopting these finer-level bias detection metrics."
  - Why unresolved: The study found that while group fairness is standard, finer-grained metrics (subgroup/individual) are virtually absent in practice, but the paper does not identify if this is due to complexity, lack of documentation, or performance costs.
  - What evidence would resolve it: Qualitative interviews with practitioners or user studies that measure the difficulty and perceived utility of implementing these specific metrics in non-generic applications.

- **Open Question 2:** Which bias mitigation approach is most effective for specific real-world scenarios, and what guidelines can direct developers to the correct choice?
  - Basis in paper: [explicit] The RQ3 Summary notes: "There is a need to study which mitigation approach is better in a given scenario, which may act as a guideline for the developers."
  - Why unresolved: The study observed that developers use pre-processing, in-processing, and post-processing methods for identical use cases (e.g., credit approval), but lacks data on which approach yields better fairness-accuracy trade-offs for those specific contexts.
  - What evidence would resolve it: A comparative empirical evaluation of mitigation algorithms across the diverse domains identified (e.g., legal, health, business) to measure their relative effectiveness.

- **Open Question 3:** What are the root causes of the persistent methodological and troubleshooting challenges in fairness APIs, and what automated solutions can address them?
  - Basis in paper: [explicit] Section 10 (Conclusion) states: "Our future work will... find the root causes and the solution of the challenges that the API developers are facing."
  - Why unresolved: The paper identifies *what* challenges exist (e.g., installation, fairness definition) but stops short of diagnosing the underlying structural or design flaws causing them or validating potential solutions.
  - What evidence would resolve it: A root-cause analysis of the 4,212 GitHub issues followed by the design and testing of automated tools (e.g., linters or IDE plugins) to prevent the most common errors.

## Limitations
- Study scope limited to Python repositories using 13 specific fairness libraries, potentially missing other implementations
- GitHub Code Search methodology may have selection bias as developers might not always include explicit import statements
- Topic modeling approach relies on manual interpretation for categorization, introducing potential subjectivity
- Analysis focuses on observable usage patterns and reported issues but cannot directly assess fairness effectiveness in production environments

## Confidence
- **High Confidence:** The quantitative findings on usage patterns (70.59% generic learning, 29.4% real-world applications) and the ranking of fairness metrics and mitigation approaches are based on concrete counts from 204 repositories
- **Medium Confidence:** The interpretation of why certain patterns exist (e.g., preference for Group fairness due to complexity) represents reasonable inference but requires additional validation
- **Medium Confidence:** The classification of 17 use cases and mapping of challenges to SDLC stages relies on open card sorting and LDA topic modeling, which are systematic but involve subjective interpretation

## Next Checks
1. **External Validation:** Replicate the study methodology on a different corpus (e.g., fairness implementations in non-Python languages or commercial software) to test the generalizability of findings
2. **Effectiveness Assessment:** Conduct a follow-up study measuring the actual fairness outcomes achieved by the identified mitigation approaches in production systems, beyond just usage patterns
3. **Tooling Impact Analysis:** Perform a controlled experiment comparing developer success rates when using advanced tooling for Individual/Subgroup metrics versus traditional Group metrics to validate the complexity hypothesis