---
ver: rpa2
title: A Human-In-The-Loop Approach for Improving Fairness in Predictive Business
  Process Monitoring
arxiv_id: '2508.17477'
source_url: https://arxiv.org/abs/2508.17477
tags:
- process
- attributes
- event
- sensitive
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a human-in-the-loop approach to improve fairness
  in predictive business process monitoring (PBPM). The method addresses the challenge
  of handling sensitive attributes that may be used both fairly and unfairly within
  the same process instance.
---

# A Human-In-The-Loop Approach for Improving Fairness in Predictive Business Process Monitoring

## Quick Facts
- arXiv ID: 2508.17477
- Source URL: https://arxiv.org/abs/2508.17477
- Reference count: 34
- Primary result: Human-in-the-loop approach reduces unfairness in PBPM while maintaining higher accuracy than models that ignore sensitive attributes

## Executive Summary
This paper addresses fairness challenges in predictive business process monitoring (PBPM) where sensitive attributes may be used both fairly and unfairly within the same process instance. The authors propose a human-in-the-loop approach that uses knowledge distillation to create an interpretable decision tree from a black-box PBPM model, which human experts then review to identify and remove unfairly biased decision nodes. After alterations, the model is fine-tuned using the fairer decision tree's predictions. Experiments on synthetic and real-life event logs demonstrate that the approach successfully reduces unfairness while maintaining higher accuracy than models that completely ignore sensitive attributes.

## Method Summary
The approach consists of four phases: (1) train a black-box next-activity prediction model on a biased event log, (2) distill the black-box model's knowledge into an interpretable decision tree, (3) have human experts review the decision tree to identify and alter unfairly biased nodes (either by discarding biased subtrees or retraining without sensitive attributes), and (4) fine-tune the original model using predictions from the altered decision tree. The method uses a 4-layer feedforward neural network (512→256→128→64, ReLU) with sliding window encoding, and measures fairness using demographic parity (ΔDP) while tracking accuracy.

## Key Results
- Successfully reduces unfairness (ΔDP) while maintaining higher accuracy than baseline models that ignore sensitive attributes
- Works effectively across various scenarios with different bias strengths and numbers of biased decisions
- Offers a promising tradeoff between fairness and accuracy compared to complete attribute removal approaches
- Demonstrates effectiveness on both synthetic (Cancer Screening) and real-life event logs (BPI Challenge 2012, Hospital Billing)

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation for Interpretability Bridge
Transferring knowledge from a black-box PBPM model to a decision tree creates an interpretable representation of learned decision rules without requiring direct access to model internals. The teacher model generates predictions on training data; the student model learns to imitate these predictions. Since decision trees are inherently interpretable through their split structure, this creates a faithful proxy for understanding which features drive predictions.

### Mechanism 2: Contextual Bias Discrimination via Human Review
Human experts can differentiate between fair and unfair uses of the same sensitive attribute within a single process instance by examining decision tree nodes. The decision tree's split nodes explicitly show which attributes are used for decisions. Experts review nodes that split on sensitive attributes and judge whether that use is justified or discriminatory, then apply one of two alteration strategies: discard (remove node, keep one subtree) or retrain (rebuild subtree without the sensitive attribute).

### Mechanism 3: Selective Fine-Tuning Preserves Performance
Fine-tuning the original model using predictions from the corrected decision tree (rather than retraining from scratch) retains useful learned patterns while propagating fairness corrections. After expert alteration produces fairer decision tree D*, its predictions serve as training targets for the original model M. The approach offers two options: use D* predictions for all prefixes, or only for prefixes where D and D* differ.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student Framework)**
  - Why needed here: The entire interpretability mechanism depends on understanding how a smaller model learns from a larger model's outputs rather than raw data.
  - Quick check question: If the teacher model achieves 89% accuracy and the student model achieves 84% on the same task, is this a successful distillation? (Answer: Yes, if the student is interpretable and sufficiently faithful to teacher behavior for the fairness review purpose.)

- **Demographic Parity (Fairness Metric)**
  - Why needed here: The paper measures unfairness using ΔDP, the absolute difference in positive prediction rates between demographic groups.
  - Quick check question: A model predicts positive outcomes for 70% of group A and 50% of group B. What is ΔDP? (Answer: |0.70 - 0.50| = 0.20; lower is fairer.)

- **Decision Tree Split Criteria and Interpretability**
  - Why needed here: Experts must read decision tree structure to identify which attributes drive splits at each node.
  - Quick check question: A decision tree node splits on "gender = female?" with left subtree leading to "pre-accept loan" and right subtree to "decline loan." What should an expert investigate? (Answer: Whether this split represents justified or discriminatory use of gender in loan decisions.)

## Architecture Onboarding

- **Component map:** [Biased Event Log] → [1. Training Phase] → Black-box Model M → [2. Distillation Phase] → Decision Tree D → [3. Alteration Phase] → Human Expert Review → Fairer Decision Tree D* → [4. Fine-Tuning Phase] → Updated Model M*

- **Critical path:** The distillation quality directly determines whether human experts see meaningful bias patterns. If D poorly approximates M, alterations may target spurious patterns while real biases remain hidden in M's opaque internals.

- **Design tradeoffs:**
  - Discard strategy: Simpler, guarantees bias removal, but may delete valid subtree decisions
  - Retrain strategy: Preserves subtree structure, but may introduce new biases via remaining sensitive attributes
  - Human review labor: More precise than automated fairness methods, but higher cost

- **Failure signatures:**
  - High ΔDP persists after fine-tuning → Increase fine-tuning epochs/learning rate
  - Accuracy drops sharply → Use selective fine-tuning (only on prefixes where D ≠ D*)
  - Decision tree too large to interpret → Reduce tree depth during distillation
  - Proxy bias detected → Iterate: review new nodes in re-distilled tree after fine-tuning

- **First 3 experiments:**
  1. Replicate on synthetic Cancer Screening log: Train M, distill to D, verify that D splits on gender nodes corresponding to known biased/unbiased decisions in the simulation design.
  2. Single-attribute bias test on Hospital Billing: Confirm M* reduces ΔDP for the biased attribute while maintaining accuracy above baseline F (Table 1, hb scenarios).
  3. Multi-attribute bias test: Run both age and gender biased scenario, verify M* reduces ΔDP for both attributes simultaneously (Figure 5b), checking for interaction effects.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the proposed knowledge distillation and fine-tuning approach maintain effectiveness when applied to outcome or remaining-time prediction tasks?
- **Open Question 2:** How effectively does the approach generalize to other black-box model architectures beyond the feedforward neural network used in the experiments?
- **Open Question 3:** How can the method be adapted to integrate event attributes for fairness evaluation alongside case attributes?
- **Open Question 4:** What specific tool support is required to efficiently find consensus among human experts when interpreting complex decision trees?

## Limitations

- The approach may struggle with highly complex event logs where distilled decision trees become too large for human interpretation.
- Proxy attributes can reintroduce bias even after removing direct sensitive attribute usage, though the paper claims this would be "obvious to the human expert in the next iteration."
- The effectiveness of human expert review depends heavily on consistent bias judgment and the ability to identify proxy attribute bias, which are acknowledged as limitations but not empirically validated.

## Confidence

- **High confidence:** The knowledge distillation mechanism (Mechanism 1) and its ability to create interpretable models is well-supported by both the paper's claims and related work (FairLoop).
- **Medium confidence:** The human review process for distinguishing fair from unfair bias (Mechanism 2) relies on expert judgment that, while theoretically sound, lacks direct empirical validation in the corpus.
- **Medium confidence:** The selective fine-tuning approach (Mechanism 3) shows promise in ablation studies but depends on successful execution of both distillation and human review phases.

## Next Checks

1. Replicate the single-attribute bias test on the Hospital Billing log to verify ΔDP reduction for the biased attribute while maintaining accuracy above the baseline F model (Table 1, hb scenarios).
2. Conduct the multi-attribute bias test by running both age and gender biased scenarios simultaneously, then verify M* reduces ΔDP for both attributes and check for interaction effects (Figure 5b).
3. Test the approach on a synthetic event log with known proxy attribute relationships to validate whether the human review process can identify and address indirect bias mechanisms.