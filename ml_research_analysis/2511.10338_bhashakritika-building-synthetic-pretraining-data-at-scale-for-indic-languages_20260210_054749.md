---
ver: rpa2
title: 'BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages'
arxiv_id: '2511.10338'
source_url: https://arxiv.org/abs/2511.10338
tags:
- data
- language
- generation
- synthetic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BhashaKritika, a 540B token synthetic multilingual
  pretraining corpus for 10 Indic languages, generated using a pipeline that combines
  document grounding, persona-based generation, math-focused data synthesis, and topic-aware
  retrieval augmentation. The corpus is filtered through a modular quality evaluation
  pipeline including language consistency checks, fluency scoring, heuristic content
  filtering, and bias detection.
---

# BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages

## Quick Facts
- **arXiv ID**: 2511.10338
- **Source URL**: https://arxiv.org/abs/2511.10338
- **Reference count**: 40
- **Primary result**: Introduces a 540B token synthetic multilingual pretraining corpus for 10 Indic languages that outperforms or matches real web data in low-resource settings while reducing bias.

## Executive Summary
This work introduces BhashaKritika, a 540B token synthetic multilingual pretraining corpus for 10 Indic languages. The corpus is generated using a pipeline that combines document grounding, persona-based generation, math-focused data synthesis, and topic-aware retrieval augmentation. A modular quality evaluation pipeline filters the synthetic data through language consistency checks, fluency scoring, heuristic content filtering, and bias detection. Experiments show that pretraining on this synthetic data yields performance comparable to or better than real web data, particularly in low-resource settings, while also demonstrating reduced bias relative to source documents.

## Method Summary
BhashaKritika employs five parallel generation strategies: document-grounded generation (conditioning on English FineWeb documents to generate knowledge-dense Indic text), persona-based generation (using PersonaHub personas with or without document grounding), math reasoning generation (converting Q-S pairs to textbook-style content), topic-aware retrieval-augmented generation (using RAG with FineWeb2 documents), and translation of Cosmopedia. The pipeline uses multiple LLMs per language (selected via human evaluation) and applies a 5-stage filtering process: language consistency, heuristic rules, KenLM perplexity scoring, quality classifier, and bias detection. This produces 540B tokens across Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu.

## Key Results
- Synthetic data matches or exceeds real web data performance on Indic benchmarks (IndicSentiment, MILU) when pretraining a 1B parameter model
- Document-grounded generation produces the largest volume (458.6B tokens) with highest knowledge density
- WEAT bias scores are reduced in synthetic data compared to source documents (Religion: 1.35→0.93, Race: 1.01→0.93)
- Modular quality filtering achieves 1.45–4.80% discard rates across generation methods while maintaining high quality

## Why This Works (Mechanism)

### Mechanism 1: Document Grounding for Factual Accuracy
Grounding synthetic generation in documents improves factual accuracy by providing structural templates and factual anchors. Source documents constrain generation toward coherent, verifiable content while enabling style transfer to textbook, blogpost, or wikihow formats in target languages.

### Mechanism 2: Multi-Model Ensemble Selection
Using different LLMs per language reduces model-specific biases and avoids collapse patterns. Human evaluation on small samples selects optimal models per language, ensuring each language gets appropriate treatment while maintaining diversity.

### Mechanism 3: Modular Quality Filtering
Sequential filtering with perplexity scoring, language detection, and heuristic rules removes low-quality synthetic data while preserving valid outputs. Each filter targets distinct failure modes, with 80th percentile perplexity thresholds balancing recall and precision.

## Foundational Learning

- **Concept: Perplexity as Fluency Proxy**
  - Why needed here: The quality pipeline uses KenLM 5-gram models to score synthetic text fluency. Understanding perplexity—lower scores indicate more probable (fluent) text under the language model—is essential for calibrating thresholds.
  - Quick check question: If a Tamil synthetic document has perplexity 40,000 but the validation set 80th percentile threshold is 35,100, should it be filtered?

- **Concept: WEAT Bias Measurement**
  - Why needed here: The paper quantifies social bias using Word Embedding Association Test, measuring associations between target word sets (e.g., Islam vs. Hindu) and attribute sets (e.g., negative vs. positive).
  - Quick check question: A WEAT effect size of 1.5 indicates what kind of bias strength? (Answer: Strong stereotypical association; scores >1.0 are considered high)

- **Concept: Synthetic Data Scaling Laws**
  - Why needed here: The paper references Muennighoff et al. (2023) on data-constrained scaling—model performance degrades after ~4 epochs on repeated data. Synthetic data provides fresh tokens to avoid this.
  - Quick check question: Why is synthetic data particularly valuable for low-resource Indic languages compared to simply reusing existing web data multiple times?

## Architecture Onboarding

- **Component map**: Source Documents (FineWeb/FineWeb2) -> Generation Layer (5 strategies) -> Model Pool (Krutrim-2, Gemma-3, LLaMA-3.3 70B, Sarvam-Translate) -> Quality Pipeline (5 filters) -> BhashaKritika Corpus

- **Critical path**: 
  1. Select source documents (English FineWeb or multilingual FineWeb2)
  2. Route to appropriate model based on target language (Table 1 mapping)
  3. Generate with style-specific prompts (Appendix C templates)
  4. Run through quality pipeline; compute discard rate
  5. Aggregate filtered tokens into BhashaKritika corpus

- **Design tradeoffs**:
  - Direct generation vs. translation: Direct generation preferred for quality, but not all models support all languages
  - English vs. Indic prompts: Table 3 shows English prompts with Indic documents yield lower discard rates
  - Persona grounding + documents: Table 4 shows adding documents to personas increases discard rate (3.50% vs. 1.48%)—random pairing introduces noise
  - Filter strictness: Tighter thresholds reduce bias but increase data loss

- **Failure signatures**:
  - Language mixing: Gujarati/Hindi show >10% language mismatch due to regional references
  - Perplexity spikes in Tamil/Bengali: English named entities inflate scores, triggering false positives
  - Repetition patterns: Qwen-3 exhibited high sentence repetitions—omitted from model pool
  - Bias persistence: Even after synthetic generation, WEAT scores show religion (1.30–1.73) and race (1.01–1.58) bias

- **First 3 experiments**:
  1. Generate 1B tokens for Hindi using document-grounded approach with Gemma-3. Run through quality pipeline, compute discard rates by filter, manually inspect 100 rejected samples
  2. For Tamil, generate identical documents using all candidate models. Run human evaluation (5 criteria from Appendix D) on 50 samples per model to replicate Table 20 selection logic
  3. Apply counter-stereotypical augmentation to small Hindi textbook subset (1000 documents). Recompute WEAT scores before/after to validate bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance advantage of synthetic data over web data persist when scaling model size beyond 1B parameters?
- Basis: Section 6 limits experiments to a 1B parameter LLaMA model
- Why unresolved: Synthetic data is knowledge-dense but may lack diversity of web-scale natural data, which could limit generalization in larger capacity models
- What evidence would resolve it: Comparative training runs for 7B+ models pretrained on BhashaKritika versus web-curated corpora

### Open Question 2
- Question: Can the targeted counter-stereotypical data augmentation strategy mitigate bias effectively across all 10 Indic languages without compromising factual accuracy?
- Basis: Section 4 details a small-scale intervention for religious bias in Hindi that reduced WEAT scores
- Why unresolved: Scaling data augmentation to diverse linguistic contexts risks introducing factual errors or degrading fluency
- What evidence would resolve it: WEAT scores and factual accuracy metrics for models trained on a fully mitigated, multi-lingual version of BhashaKritika

### Open Question 3
- Question: Can neural language models replace KenLM to more accurately filter fluency in synthetic Indic text prone to code-mixing?
- Basis: Section 6.3 notes that for Tamil and Bengali, English named entities cause "alarmingly high" discard rates in current KenLM filter
- Why unresolved: N-gram models penalize code-mixing heavily, potentially discarding valid high-quality synthetic text with English terminology
- What evidence would resolve it: Evaluation of neural-based filter distinguishing natural code-mixing from actual disfluency

## Limitations

- High discard rates for Tamil and Bengali due to English named entity contamination in perplexity filtering
- Residual bias persists across all evaluated dimensions (WEAT scores remain >0.9) despite reduction efforts
- Human evaluation on small samples may not generalize to full-scale generation quality

## Confidence

- **High Confidence**: Synthetic data matches or exceeds real web data in low-resource settings; document-grounded generation produces knowledge-dense outputs; modular quality filtering effectively removes low-quality data
- **Medium Confidence**: Multi-model ensemble selection improves diversity; perplexity thresholds generalize across languages (with Tamil/Bengali exceptions); bias reduction via synthetic generation is statistically significant but not complete
- **Low Confidence**: 80th percentile perplexity cutoff optimally balances recall and precision; human evaluation on small samples reliably predicts large-scale generation quality; counter-stereotypical augmentation fully mitigates residual bias

## Next Checks

1. Generate 100M additional tokens for Hindi using the same pipeline and compare human evaluation scores to original small-sample results. Track perplexity distribution and discard rates to detect degradation patterns.

2. Apply counter-stereotypical augmentation to the full textbook-style Hindi subset and recompute WEAT scores across all dimensions. Measure whether the reduction effect scales linearly or plateaus.

3. For Tamil and Bengali, develop named entity-aware perplexity scoring and compare discard rates before/after. Validate whether this reduces false positives without compromising fluency detection.