---
ver: rpa2
title: 'J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge'
arxiv_id: '2505.11875'
source_url: https://arxiv.org/abs/2505.11875
tags:
- answ
- attempt
- arxiv
- ckpt
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether simple test-time scaling (STTS)
  can enhance the capabilities of LLM-as-a-judge systems. While existing approaches
  lack consistent scaling behavior under STTS, the authors propose a two-stage training
  method that combines supervised fine-tuning on a reflection-enhanced dataset with
  reinforcement learning using verifiable rewards.
---

# J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2505.11875
- Source URL: https://arxiv.org/abs/2505.11875
- Reference count: 40
- Primary result: J1-7B surpasses previous SOTA LLM-as-a-judge by 4.8% and demonstrates 5.1% stronger scaling trend under simple test-time scaling

## Executive Summary
This paper investigates whether simple test-time scaling (STTS) can enhance the capabilities of LLM-as-a-judge systems. While existing approaches lack consistent scaling behavior under STTS, the authors propose a two-stage training method that combines supervised fine-tuning on a reflection-enhanced dataset with reinforcement learning using verifiable rewards. Their model, J1-7B, surpasses previous state-of-the-art LLM-as-a-judge by 4.8% and demonstrates a 5.1% stronger scaling trend under STTS. Key findings reveal that effective STTS capability emerges predominantly during the RL phase rather than from SFT alone, and that incorporating reasoning-intensive data during cold-start training further improves STTS effectiveness. The work highlights the potential of structured reflective reasoning strategies for developing more reliable and scalable LLM-as-a-judge systems.

## Method Summary
The authors propose a two-stage training methodology for developing LLM-as-a-judge systems capable of effective simple test-time scaling. The first stage employs supervised fine-tuning on a reflection-enhanced dataset where models generate both initial judgments and subsequent reflective critiques. The second stage applies reinforcement learning using verifiable rewards derived from correctness verification mechanisms. This approach is designed to cultivate both the ability to produce accurate judgments and the capacity to improve those judgments through self-reflection during test-time scaling. The training pipeline incorporates reasoning-intensive data during the initial cold-start phase to better prepare models for the iterative refinement process that characterizes STTS.

## Key Results
- J1-7B achieves 4.8% performance improvement over previous state-of-the-art LLM-as-a-judge systems
- Demonstrates 5.1% stronger scaling trend under simple test-time scaling compared to baselines
- Effective STTS capability emerges predominantly during reinforcement learning phase rather than supervised fine-tuning alone

## Why This Works (Mechanism)
The mechanism underlying J1's success appears to stem from the combination of reflective reasoning training and verifiable reward optimization. By training models to generate both initial judgments and subsequent reflections, the system develops metacognitive awareness that enables more effective test-time scaling. The RL phase with verifiable rewards reinforces the ability to recognize and correct errors through iterative refinement, creating a positive feedback loop that strengthens scaling behavior. The incorporation of reasoning-intensive data during cold-start training ensures the model has robust foundational reasoning capabilities before being exposed to the more complex task of self-guided improvement.

## Foundational Learning

**Simple Test-Time Scaling (STTS)**: The practice of applying the same model iteratively to improve outputs without architectural changes.
- *Why needed*: Enables performance gains without additional training or model size increases
- *Quick check*: Verify that repeated application of model produces consistent improvements

**Reflective Reasoning**: The capacity to critique and improve upon one's own outputs through metacognitive processes.
- *Why needed*: Enables models to identify weaknesses in their own judgments and refine them
- *Quick check*: Test whether model can generate meaningful critiques of its own previous outputs

**Verifiable Reward Signals**: Reward mechanisms based on objective correctness criteria rather than subjective human preferences.
- *Why needed*: Provides stable training signals that generalize better than preference-based rewards
- *Quick check*: Confirm rewards are based on measurable correctness rather than human preference labels

## Architecture Onboarding

**Component Map**: Input -> Encoder -> Judgment Head -> Reflection Head -> Reward Estimator -> Output

**Critical Path**: The core inference loop follows: input text → initial judgment generation → reflection generation → reward evaluation → (optional) iterative refinement based on reflection

**Design Tradeoffs**: The two-stage training approach trades increased training complexity for better STTS performance. Using verifiable rewards instead of human preferences provides more stable training signals but may miss nuanced quality aspects. The reflection mechanism adds computational overhead during inference but enables the key STTS capability.

**Failure Signatures**: 
- Ineffective scaling: Minimal improvement between initial and refined outputs
- Over-reflection: Excessive time spent on reflection without meaningful quality gains
- Reward misalignment: Model optimizes for verifiable metrics at expense of overall judgment quality

**First 3 Experiments**:
1. Compare J1's scaling behavior against baseline models across 2-5 scaling iterations
2. Ablation study removing either reflection component or verifiable reward mechanism
3. Test transfer of STTS capability to out-of-distribution evaluation tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on the J1-7B model size, limiting conclusions about scalability to other model sizes
- Reliance on synthetic data generation may not fully capture real-world evaluation complexity
- The underlying mechanisms driving STTS improvements are not fully understood

## Confidence

**High confidence**: Empirical results showing J1-7B's 4.8% performance improvement and 5.1% stronger scaling trend
**Medium confidence**: Assertion that RL phase is more critical than SFT for STTS capability development
**Medium confidence**: Effectiveness of reasoning-intensive data for cold-start training enhancement
**Low confidence**: Claims about general applicability across different model architectures or evaluation domains

## Next Checks

1. Cross-domain validation: Test J1 models on diverse evaluation tasks beyond the current scope, including multi-modal and domain-specific assessment scenarios, to verify robustness of STTS benefits.

2. Scaling boundary analysis: Evaluate models at multiple size scales (1B, 13B, 70B) to determine whether the observed scaling trends persist across different parameter regimes and identify potential saturation points.

3. Alternative training methodology comparison: Implement and compare alternative training approaches (e.g., direct preference optimization, outcome-supervised fine-tuning) against the proposed two-stage method to isolate the contribution of each component to STTS effectiveness.