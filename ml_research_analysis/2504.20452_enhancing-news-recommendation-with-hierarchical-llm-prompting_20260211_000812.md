---
ver: rpa2
title: Enhancing News Recommendation with Hierarchical LLM Prompting
arxiv_id: '2504.20452'
source_url: https://arxiv.org/abs/2504.20452
tags:
- news
- recommendation
- user
- title
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of capturing complex user preferences
  in news recommendation systems, which often rely on shallow representations like
  titles and abstracts. To overcome this, the authors propose PNR-LLM, a method that
  leverages Large Language Models (LLMs) to enrich news titles and abstracts with
  deeper semantic information and relevant entities.
---

# Enhancing News Recommendation with Hierarchical LLM Prompting

## Quick Facts
- arXiv ID: 2504.20452
- Source URL: https://arxiv.org/abs/2504.20452
- Reference count: 20
- Primary result: PNR-LLM achieves AUC of 68.88 on MIND-SMALL and 69.32 on MIND-LARGE, outperforming state-of-the-art baselines

## Executive Summary
This paper addresses the challenge of capturing complex user preferences in news recommendation systems by proposing PNR-LLM, a method that leverages Large Language Models (LLMs) to enrich news titles and abstracts with deeper semantic information and relevant entities. The approach uses a hierarchical prompting strategy to generate more engaging and contextually appropriate titles while maintaining their original intent. Extensive experiments on MIND datasets demonstrate that PNR-LLM significantly outperforms existing methods, achieving state-of-the-art performance with AUC scores of 68.88 and 69.32 on the MIND-SMALL and MIND-LARGE datasets respectively. The method is also model-agnostic, improving the performance of existing recommendation models when applied.

## Method Summary
The paper proposes a hierarchical LLM prompting approach to enhance news recommendation by enriching news titles and abstracts with deeper semantic information and relevant entities. The method consists of three steps: direct prompt for engaging title generation, entity extraction, and combining these into a refined title with entities. The news encoder uses GloVe word embeddings with CNN and attention mechanisms, combining them with entity and category embeddings. The user encoder applies attention over news embeddings from user click history. The model is trained using negative sampling and cross-entropy loss with 4 negative samples per positive, optimized using Adam with a learning rate of 1e-4.

## Key Results
- PNR-LLM achieves AUC of 68.88 on MIND-SMALL and 69.32 on MIND-LARGE, outperforming state-of-the-art baselines
- The method is model-agnostic, improving performance of existing recommendation models when applied
- Optimal generated title length is found to be 40 tokens

## Why This Works (Mechanism)
The hierarchical LLM prompting strategy effectively captures deeper semantic information by first generating engaging titles, then extracting relevant entities, and finally combining them into refined titles. This multi-step approach ensures that the generated content maintains the original intent while adding contextual richness. The attention mechanisms in both news and user encoders allow the model to focus on the most relevant information, while the combination of GloVe embeddings with entity and category information provides a comprehensive representation of news content.

## Foundational Learning
- **Hierarchical prompting**: Why needed - To systematically enrich news content with engaging titles and relevant entities; Quick check - Verify that each prompt step produces coherent and contextually appropriate output
- **Attention mechanisms**: Why needed - To focus on the most relevant parts of news content and user history; Quick check - Confirm attention weights highlight semantically important words and entities
- **Negative sampling**: Why needed - To effectively train click prediction models by contrasting clicked vs. unclicked news; Quick check - Ensure 4 negative samples per positive are properly sampled and balanced
- **Entity normalization**: Why needed - To standardize entity representations across different news articles; Quick check - Validate that entities like "U.S." and "United States" map to the same normalized form
- **TransE embeddings**: Why needed - To provide pre-trained entity representations based on knowledge graph relationships; Quick check - Verify entity embeddings capture meaningful semantic relationships
- **CNN for text encoding**: Why needed - To extract local features and patterns from news titles and abstracts; Quick check - Confirm CNN filters capture relevant n-gram patterns

## Architecture Onboarding

**Component map**: News -> LLM Enrichment -> GloVe+CNN+Attention -> Entity Attention -> User Encoder -> Click Prediction

**Critical path**: News text → Hierarchical LLM prompting → News encoder (GloVe+CNN+attention) → User encoder (attention over history) → Click probability prediction

**Design tradeoffs**: The paper chooses hierarchical prompting over single-step enrichment to better preserve original intent while adding semantic depth. Entity embeddings are initialized randomly rather than pre-trained to accommodate enriched entities. The model uses attention mechanisms instead of more complex architectures to maintain interpretability and efficiency.

**Failure signatures**: Generated titles that are too long (>40 tokens) or off-topic can degrade performance. Entity normalization inconsistencies can lead to duplicate representations of the same entities. If the LLM fails to capture relevant entities, the enriched representation loses semantic value.

**3 first experiments**:
1. Run hierarchical prompting on a 1,000-news subset to validate prompt effectiveness and cache results
2. Train the news encoder with enriched titles and compare against baseline using original titles only
3. Evaluate user encoder performance with attention over enriched news embeddings versus original embeddings

## Open Questions the Paper Calls Out
1. **Knowledge Graph Integration**: The paper suggests connecting generated entities with a knowledge graph to improve recommendation accuracy, but does not explore this integration.
2. **Factual Consistency**: While the method improves recommendation accuracy, it does not evaluate whether generated "intriguing" titles introduce semantic hallucinations or factual drift that could affect user trust.
3. **Breaking News Performance**: The method's efficacy on news articles published after the LLM's training cutoff date is not analyzed, raising questions about performance on time-sensitive content.
4. **Category-specific Optimization**: The optimal title length of 40 tokens is presented as a global hyperparameter without investigating whether different news categories (e.g., Politics vs. Sports) might benefit from different token lengths.

## Limitations
- Exact prompt templates for the hierarchical prompting are not fully specified in the paper
- CNN architecture details (filter sizes, number of filters, hidden dimensions) are missing
- Training hyperparameters including batch size, number of epochs, and dropout rates are not provided
- Integration method for enriched entity embeddings with original TransE embeddings is unclear

## Confidence
- **High confidence**: The core methodology of using hierarchical LLM prompting for news enrichment is well-described and the experimental setup (datasets, metrics, baseline comparisons) is clear
- **Medium confidence**: The overall model architecture and training procedure can be reasonably reconstructed, though some architectural details are missing
- **Low confidence**: Exact reproduction of results is challenging without the specific prompt templates and complete hyperparameter settings

## Next Checks
1. Implement the hierarchical prompting pipeline with a substitute LLM (e.g., LLaMA 3 or GPT-4o mini) on a 1,000-news subset to validate prompt effectiveness and cache results before full dataset processing
2. Conduct ablation studies comparing the full PNR-LLM approach against versions with: (a) only direct title enrichment, (b) only entity extraction, and (c) original titles without enrichment to quantify each component's contribution
3. Perform cross-dataset validation by testing the enriched representations on a different news recommendation dataset (e.g., Adressa) to assess generalization beyond MIND datasets