---
ver: rpa2
title: Associative Poisoning to Generative Machine Learning
arxiv_id: '2511.05177'
source_url: https://arxiv.org/abs/2511.05177
tags:
- poisoning
- features
- attack
- data
- associative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces associative poisoning, a novel data poisoning
  attack that manipulates statistical associations between feature pairs in generated
  data without requiring control over the training process. The attack modifies training
  data to induce or suppress feature correlations while preserving marginal distributions
  and output quality.
---

# Associative Poisoning to Generative Machine Learning

## Quick Facts
- arXiv ID: 2511.05177
- Source URL: https://arxiv.org/abs/2511.05177
- Reference count: 40
- Key outcome: Introduces associative poisoning that manipulates statistical associations between feature pairs in generated data without requiring control over training process, achieving significant shifts in mutual information (MI) and Matthews correlation coefficient (MCC) while preserving marginal distributions and output quality.

## Executive Summary
This paper introduces associative poisoning, a novel data poisoning attack that manipulates statistical associations between feature pairs in generated data without requiring control over the training process. The attack modifies training data to induce or suppress feature correlations while preserving marginal distributions and output quality. Theoretical analysis proves the attack's feasibility and stealth, and empirical evaluations on two state-of-the-art generative models (D-GAN and DDPM-IP) trained on CelebA and Recipe1M datasets demonstrate significant shifts in mutual information (MI) and Matthews correlation coefficient (MCC) between targeted features, with clean vs. poisoned models showing MI increases from 0.008 to 0.119 and MCC from 0.131 to 0.483 in one experiment. Marginal probabilities and Fréchet Inception Distance (FID) remain statistically similar, indicating successful stealth. An ablation study shows a linear relationship between poisoning extent and attack effectiveness. The attack represents a previously unexplored vulnerability in generative modeling, as existing defenses are ineffective against association-level manipulations.

## Method Summary
The attack works by rebalancing joint probability counts (P00, P01, P10, P11) for binary feature pairs in the training data. For positive associative poisoning, samples where F1=1,F2=0 are replaced with F1=1,F2=1 samples, and F1=0,F2=1 with F1=0,F2=0, increasing on-diagonal counts while maintaining marginal probabilities P(F1) and P(F2). The generative model learns this poisoned joint distribution and reproduces altered associations in generated samples. For continuous features, a swap-based approach sorts one feature and iteratively swaps adjacent inversions of the second feature to increase their Pearson correlation while preserving marginal distributions. The attack was evaluated on D-GAN and DDPM-IP models trained on CelebA and Recipe1M datasets, measuring fidelity through MI, MCC, and PCC changes, and stealth through marginal probability preservation and FID scores.

## Key Results
- MI increases from 0.008 to 0.119 and MCC from 0.131 to 0.483 between targeted feature pairs in poisoned vs. clean models
- Marginal distributions and FID scores remain statistically similar between poisoned and clean models, demonstrating stealth
- Linear relationship between poisoning extent and attack effectiveness identified in ablation study
- Theoretical proofs show MI is monotonically increasing with poisoning magnitude for independent features

## Why This Works (Mechanism)

### Mechanism 1: Joint Probability Manipulation Preserving Marginals
Modifying co-occurrence patterns between feature pairs in training data allows for inducing or suppressing associations in generated outputs without altering individual feature frequencies. The attack rebalances joint probability counts (P00, P01, P10, P11) for binary feature pairs. For positive associative poisoning, samples where F1=1,F2=0 are replaced with F1=1,F2=1 samples, and F1=0,F2=1 with F1=0,F2=0, increasing on-diagonal counts while maintaining marginal probabilities P(F1) and P(F2). The generative model learns this poisoned joint distribution and reproduces altered associations in generated samples.

### Mechanism 2: Theoretical Guarantees via Mutual Information and Matthews Correlation Coefficient
Theoretical proofs demonstrate that associative poisoning monotonically increases mutual information (MI) and linearly alters Matthews correlation coefficient (MCC) for originally independent binary features. For independent features F1 and F2, Theorem 1 proves MI'(ε) has a positive derivative for ε>0 and negative for ε<0, meaning MI increases with poisoning magnitude. MCC changes linearly with ε (MCC' = 1/√(uv-uv²-u²v+u²v²) > 0). Theorem 2 shows choosing ε = P01P10 - P00P11 drives both MI and MCC to zero (independence).

### Mechanism 3: Stealth Through Marginal Preservation and Output Quality Maintenance
By preserving marginal distributions and maintaining comparable FID scores, the attack evades detection methods relying on single-feature statistics or perceptual quality metrics. The paper formalizes a "Level-2" poisoning hierarchy: Level-0 attacks change dataset size (O(N) detection), Level-1 modify marginals (O(NM)), Level-2 (associative poisoning) only modify joint probabilities (O(NM²) detection complexity). Empirically, poisoned models show overlapping marginal distributions and convergent FID scores compared to clean models.

## Foundational Learning

- Concept: Mutual Information (MI)
  - Why needed here: MI quantifies statistical dependence between variables. The paper uses MI as a primary metric measuring attack success. Understanding MI is essential for interpreting Theorem 1 proofs and empirical results.
  - Quick check question: If two features X and Y are statistically independent, what is MI(X,Y)?

- Concept: Joint vs. Marginal Probability Distributions
  - Why needed here: The core attack exploits the distinction between joint and marginal distributions. Marginal probabilities P(F1), P(F2) are preserved while joint probabilities P(F1,F2) are manipulated. Critical for implementing poisoning and understanding stealth.
  - Quick check question: Given joint distribution P(X,Y), how do you compute the marginal P(X)?

- Concept: Matthews Correlation Coefficient (MCC)
  - Why needed here: MCC provides a correlation measure for binary features that is zero for independent variables. The paper proves MCC changes linearly with poisoning extent ε (Equation 5-6), making it a theoretically grounded metric alongside MI.
  - Quick check question: What range of values can MCC take, and what does MCC=0 indicate?

## Architecture Onboarding

- Component map:
  1. Training data poisoning module: Implements sample replacement for binary features and swap-based approach for continuous features
  2. Generative model training pipeline: Standard D-GAN or DDPM-IP training on poisoned data—no loss function or architecture modifications
  3. Evaluation framework: MI/MCC/PCC computation on generated samples, FID calculation for stealth verification, Mann-Whitney U tests for statistical significance

- Critical path:
  1. Identify target feature pairs and desired association direction (positive/negative)
  2. Compute original joint probability distribution P_ij
  3. Determine poisoning extent ε (maximum: ε = min{b,c} for positive, ε = max{-a,-d} for negative)
  4. Replace n = ε×N samples according to Definition 1 (binary) or Theorem 3 (continuous)
  5. Train generative model on poisoned dataset
  6. Generate samples, compute MI/MCC for fidelity; compute MP/FID for stealth

- Design tradeoffs:
  - Poisoning extent vs. stealth: Higher ε produces stronger association shifts but may approach detection thresholds. Ablation study (Figure 9) shows statistical significance appearing at ~10% control.
  - Feature pair selection: Targeting already-independent features (MI≈0) produces cleaner theoretical guarantees than modifying existing associations.
  - Replacement strategy: Hold-out set replacement limits poisoning to 20%; synthesis methods (e.g., DeepFake-based) could extend reach but introduce artifacts.

- Failure signatures:
  - Reduced association shift in generated outputs compared to training data indicates imperfect distribution learning by the generative model.
  - Counter-intuitive MI decrease (Figure 2, olive_oil/eggs) occurs when features are originally negatively associated; positive poisoning must first overcome the negative association.
  - For continuous features, no universal local swap rule guarantees MI increase (Proposition 3), making the continuous attack theoretically weaker.

- First 3 experiments:
  1. Reproduce binary two-feature attack on CelebA/D-GAN with Mouth_Slightly_Open/Wearing_Lipstick pair: Replace samples per Section 4.3.3, train 10 models with different seeds, measure MI/MCC at iteration 12,000. Expected: MI increases from ~0.008 to ~0.119 (Table 1).
  2. Verify marginal preservation: Compute marginal probabilities for both target features in generated samples across all 10 poisoned models. Run Mann-Whitney U test comparing to clean models. Expected: p > 0.05 (Table 2).
  3. Test ablation on poisoning extent: Train models with 5%, 10%, 15%, 20%, 25%, 30% adversarial control. Plot MI/MCC curves (expected: monotonic increase per Figure 9). Identify minimum threshold for statistical significance (expected: ~10%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is associative poisoning across text generation, audio synthesis, and non-image modalities such as tabular or graph data?
- Basis in paper: [explicit] "As our attack is agnostic to model architecture and data domain, it is essential to evaluate its effectiveness across a broader range of tasks, including text generation, audio synthesis, and non-image modalities such as tabular or graph data."
- Why unresolved: The empirical evaluation is limited to image generation (CelebA, Recipe1M) using D-GAN and DDPM-IP; no experiments were conducted on other modalities or architectures.
- What evidence would resolve it: Empirical results showing MI/MCC shifts and stealth metrics on LLMs, audio synthesizers, and tabular/graph generative models.

### Open Question 2
- Question: What practical, targeted defences can reliably detect or mitigate associative poisoning without requiring external trusted datasets?
- Basis in paper: [explicit] "The development of targeted defences against associative poisoning remains an open challenge."
- Why unresolved: The paper identifies limitations of existing defences (outlier detection, spectral signatures, activation clustering) and proposes only a high-level two-stage strategy that requires labels and may reduce dataset utility.
- What evidence would resolve it: A concrete defense method with quantified detection rates, false positive rates, and retained model utility across datasets.

### Open Question 3
- Question: Can associative poisoning be combined with other attack vectors (e.g., membership inference, model extraction) to create more powerful multi-faceted adversarial strategies?
- Basis in paper: [explicit] "it would be valuable to explore how associative poisoning might be combined with other attack vectors, such as membership inference or model extraction, to construct more powerful, multi-faceted adversarial strategies."
- Why unresolved: The paper evaluates associative poisoning in isolation; no experiments explore compositional or chained attacks.
- What evidence would resolve it: Empirical demonstration of combined attacks showing amplified impact on model privacy, integrity, or utility.

## Limitations

- The continuous feature extension relies on a weaker swap-based heuristic without guaranteed monotonic MI improvement
- The attack's effectiveness appears highly dependent on the specific model architecture and training dynamics, with Recipe1M showing convergence issues
- The stealth claims depend on defenders not monitoring joint distributions, which may be increasingly common as awareness grows

## Confidence

- Theoretical foundations for binary feature attacks are robust with rigorous proofs for MI and MCC behavior (High confidence)
- Continuous feature extension relies on weaker swap-based heuristic without guaranteed monotonic MI improvement (Medium confidence)
- Claim that generative models reliably capture and reproduce fine-grained statistical associations remains empirically supported but theoretically underspecified (Medium confidence)
- Attack effectiveness highly dependent on specific model architecture and training dynamics (Medium confidence)

## Next Checks

1. Reproduce the binary two-feature attack on CelebA with Mouth_Slightly_Open/Wearing_Lipstick, measuring MI shift from ~0.008 to ~0.119 across 10 independent runs.

2. Verify marginal probability preservation by running Mann-Whitney U tests comparing clean vs. poisoned models on both target features.

3. Conduct ablation study with poisoning extents 5%, 10%, 15%, 20%, 25%, 30% to identify the minimum threshold for statistically significant association shifts.