---
ver: rpa2
title: Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face
  Embedder
arxiv_id: '2507.10552'
source_url: https://arxiv.org/abs/2507.10552
tags:
- face
- self-supervised
- data
- chimpanzee
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first self-supervised chimpanzee face
  embedder trained entirely without identity labels, enabling scalable open-set re-identification
  from camera-trap footage. The approach leverages the DINOv2 framework to train Vision
  Transformers on automatically mined face crops from unlabelled videos, bypassing
  the need for manual ID annotation.
---

# Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder

## Quick Facts
- arXiv ID: 2507.10552
- Source URL: https://arxiv.org/abs/2507.10552
- Reference count: 0
- Primary result: Self-supervised chimpanzee face embedder trained without identity labels achieves state-of-the-art open-set re-identification on camera-trap benchmarks

## Executive Summary
This paper introduces the first self-supervised chimpanzee face embedder trained entirely without identity labels, enabling scalable open-set re-identification from camera-trap footage. The approach leverages the DINOv2 framework to train Vision Transformers on automatically mined face crops from unlabelled videos, bypassing the need for manual ID annotation. Experiments on challenging benchmarks (Bossou-9 and PetFaceC*) show that this self-supervised model outperforms fully supervised baselines in open-set re-identification and approaches their performance on in-captivity datasets.

## Method Summary
The method uses self-supervised learning with DINOv2 to train Vision Transformers on automatically mined face crops from unlabelled camera-trap videos. Instead of requiring identity-labeled data, the approach mines face crops from raw footage and learns embeddings through contrastive learning. The trained embedder is then used for re-identification by comparing new face crops against a gallery of known individuals. The pipeline processes video frames, detects and crops faces, and generates embeddings for matching without any manual annotation.

## Key Results
- Achieves 78.1% Re-ID accuracy on Bossou-9 benchmark using 22M parameters
- Scales to 81.6% Re-ID accuracy with 87M parameters model
- Demonstrates 39.3% Re-ID accuracy on PetFaceC* benchmark
- Runs at 100+ fps on single GPU, outperforming supervised baselines in open-set settings

## Why This Works (Mechanism)
The approach works by leveraging the rich temporal and visual information in unlabelled camera-trap footage. By mining face crops from continuous video streams, the model can learn discriminative features through self-supervised contrastive learning without requiring identity labels. The DINOv2 framework provides a strong foundation for learning robust visual representations that generalize across different lighting, poses, and environmental conditions typical in wild settings.

## Foundational Learning
- **Self-supervised learning**: Why needed - eliminates expensive manual annotation; Quick check - compare performance with supervised baseline
- **Vision Transformer architecture**: Why needed - captures global context in faces; Quick check - ablation study with CNN baseline
- **Contrastive learning**: Why needed - learns discriminative features without labels; Quick check - verify embedding space separability
- **Face mining from video**: Why needed - provides diverse training samples; Quick check - analyze mined crop quality distribution
- **DINOv2 framework**: Why needed - proven for universal visual features; Quick check - compare with other self-supervised methods
- **Open-set recognition**: Why needed - real-world scenarios have unseen individuals; Quick check - test on truly novel identities

## Architecture Onboarding
- **Component map**: Raw video -> Face detection -> Crop mining -> DINOv2 backbone -> Embedding layer -> Distance metric -> Re-ID decision
- **Critical path**: Video frame input → Face detection and cropping → DINOv2 feature extraction → Embedding generation → Nearest neighbor matching
- **Design tradeoffs**: Self-supervised approach trades labeled data requirements for potentially noisier training signals from mined crops, but gains scalability and generalization
- **Failure signatures**: Poor performance on occluded faces, extreme lighting conditions, or when mined crops are of low quality; degraded accuracy with very few training videos
- **First experiments**: 1) Validate face detection and cropping pipeline on sample videos, 2) Test DINOv2 feature quality on simple matching tasks, 3) Compare embedding distances between same vs different individuals

## Open Questions the Paper Calls Out
None

## Limitations
- Model robustness to extreme lighting and occlusion conditions not well-represented in current datasets
- Performance gains over supervised baselines need validation across broader range of wild primate species
- Reliance on quality of automatically mined face crops introduces new potential failure points

## Confidence
- **High**: Model achieves strong performance on tested benchmarks (Bossou-9 and PetFaceC*) with quantitative results
- **Medium**: Approach "eliminates the costly annotation bottleneck" - bypasses manual ID annotation but requires high-quality mined crops
- **Medium**: Scalability claim - demonstrated for tested model sizes but broader scaling not yet validated

## Next Checks
1. Evaluate model performance on additional wild primate species and habitats not represented in Bossou-9 or PetFaceC*
2. Test model robustness under extreme environmental conditions (very low light, heavy occlusion, rain, snow) and across wider range of camera trap hardware
3. Quantify impact of automatic face crop mining quality on downstream Re-ID accuracy, including failure modes and sensitivity to mining hyperparameters