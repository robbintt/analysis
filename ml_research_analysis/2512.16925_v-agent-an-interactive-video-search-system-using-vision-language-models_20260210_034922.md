---
ver: rpa2
title: 'V-Agent: An Interactive Video Search System Using Vision-Language Models'
arxiv_id: '2512.16925'
source_url: https://arxiv.org/abs/2512.16925
tags:
- retrieval
- video
- search
- arxiv
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces V-Agent, a multi-agent video search system\
  \ that overcomes limitations of text-based retrieval by using vision-language models\
  \ (VLMs) to interpret both visual and spoken content in videos. The system employs\
  \ three agents\u2014routing, search, and chat\u2014that collaborate to process user\
  \ queries, retrieve relevant videos, and enable interactive conversations."
---

# V-Agent: An Interactive Video Search System Using Vision-Language Models

## Quick Facts
- arXiv ID: 2512.16925
- Source URL: https://arxiv.org/abs/2512.16925
- Reference count: 40
- Achieves state-of-the-art zero-shot nDCG@10 of 0.680 on MultiVENT 2.0 benchmark

## Executive Summary
This paper introduces V-Agent, a multi-agent video search system that overcomes limitations of text-based retrieval by using vision-language models (VLMs) to interpret both visual and spoken content in videos. The system employs three agents—routing, search, and chat—that collaborate to process user queries, retrieve relevant videos, and enable interactive conversations. A key innovation is the fine-tuning of VLMs with a small video preference dataset and the addition of a retrieval vector from an image-text retrieval model, enhancing the model's vision-text alignment. The VLM-based retrieval model jointly embeds video frames and audio transcriptions for multimodal video understanding.

## Method Summary
The system fine-tunes Qwen2-VL-7B-Instruct on ShareGPTVideo's 17k video preference dataset using InfoNCE loss, then enhances it by adding a retrieval vector calculated as the difference between GME-Qwen2-VL-7B-Instruct and base Qwen2-VL weights. For each video, 48 frames are uniformly sampled and paired with Whisper-transcribed audio, both embedded independently by the VLM. The system employs a three-agent architecture where a routing agent directs queries, a search agent performs multimodal retrieval with re-ranking, and a chat agent synthesizes responses. Video embeddings are stored in pgvector (HNSW m=16) and fused using equal weighting (α=0.5) of visual and audio similarities.

## Key Results
- Achieves state-of-the-art zero-shot performance on MultiVENT 2.0 benchmark with nDCG@10 of 0.680 and Recall@10 of 0.676
- Outperforms existing models like MMMORRF in video retrieval tasks
- Ablation studies confirm effectiveness of re-ranking (+6% nDCG@10) and retrieval vector addition in improving retrieval quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting a pre-computed "retrieval vector" into a fine-tuned VLM may enhance vision-text alignment without requiring massive video-specific training data.
- **Mechanism:** The system calculates a vector τ by subtracting the weights of the base VLM (Qwen2-VL-Instruct) from a strong image-text retrieval model (GME). This vector τ is added to the video-fine-tuned model (M_F) to create the final retrieval model (M_R).
- **Core assumption:** The "retrieval capability" is a linear direction in weight space that can be detached from the image domain and applied to the video domain without disrupting the base model's reasoning faculties.
- **Evidence anchors:** [section] Section 3.1.2 defines τ = θ_GME - θ_Qwen and θ_MR = θ_MF + τ. [abstract] Mentions enhancing the model with a retrieval vector to overcome training data limitations.
- **Break condition:** Performance degrades if the architectures of the source retrieval model and target VLM diverge, or if the added vector overwrites specific video-temporal reasoning learned during fine-tuning.

### Mechanism 2
- **Claim:** Fusing independent embedding scores for visual frames and audio transcriptions likely improves retrieval robustness for complex queries.
- **Mechanism:** The system embeds video frames and ASR text separately. At query time, it computes distinct inner-product scores for frames (e_f) and audio (e_a) against the query (e_q). A weighted average (α=0.5) combines these scores to rank videos.
- **Core assumption:** Visual and audio modalities provide complementary signals that align linearly with the query intent, and simple linear fusion is sufficient to resolve modality conflicts.
- **Evidence anchors:** [section] Equation (6) defines the score fusion: score = α · ⟨e_f, e_q⟩ + (1-α) · ⟨e_a, e_q⟩. [abstract] States the model "independently embeds video frames and audio transcriptions."
- **Break condition:** Retrieval fails when visual and audio content are contradictory or when the fixed α weight mismatches the query modality.

### Mechanism 3
- **Claim:** LLM-based re-ranking serves as a semantic correction layer to improve precision beyond dense vector similarity.
- **Mechanism:** After the initial retrieval, a "rerank" tool (powered by an LLM) accepts the top-k candidate videos and the user query. It uses reasoning capabilities to re-order the list based on semantic relevance prompts, filtering out false positives from the vector search.
- **Core assumption:** The text descriptions/transcriptions provided to the reranker contain enough semantic truth to allow the LLM to judge relevance accurately.
- **Evidence anchors:** [section] Table 4 shows a 6 percentage point increase in nDCG@10 when re-ranking is enabled. [abstract] Mentions an "additional re-ranking module to further enhance video retrieval quality."
- **Break condition:** Effectiveness drops if the video descriptions or ASR transcriptions are hallucinated, sparse, or misleading.

## Foundational Learning

- **Concept:** Task Arithmetic / Chat Vectors
  - **Why needed here:** This technique is the core innovation for adapting the VLM. Without understanding weight editing, one cannot debug why the model gains retrieval skills but might lose conversational fluency.
  - **Quick check question:** If you add the "retrieval vector" from a French-language model to an English video model, what behavior would you hypothesize might emerge?

- **Concept:** Modality Fusion Strategies (Late Fusion)
  - **Why needed here:** The system relies on fusing audio and visual scores. Understanding Late Fusion is required to tune the α parameter or diagnose why one modality dominates results.
  - **Quick check question:** Why might averaging scores (Late Fusion) be safer than concatenating vectors (Early Fusion) when handling missing audio tracks?

- **Concept:** Zero-Shot Retrieval Benchmarks (MultiVENT 2.0)
  - **Why needed here:** The paper claims state-of-the-art "zero-shot" performance. Understanding what zero-shot means in this context is vital to distinguishing the model's generalization from mere memorization of training data.
  - **Quick check question:** Does a high zero-shot score on MultiVENT 2.0 imply the system will work on security camera footage, and why or why not?

## Architecture Onboarding

- **Component map:** OpenAI Whisper (ASR) + Frame Extractor → VLM (M_R) → pgvector (HNSW) → Routing Agent → Search Agent → Chat Agent
- **Critical path:** The Search Agent is the bottleneck. It must orchestrate the vector search (CPU/GPU) and the LLM re-ranking (API/Inference). Any latency here blocks the Chat Agent.
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The re-ranking step adds an LLM inference call (high latency) but provides a +6% nDCG gain.
  - **Modality Coverage:** Using only frames + ASR misses non-speech audio (e.g., explosions, music) which the embedding model might not distinguish without specific training.
- **Failure signatures:**
  - **"Silent Video" failure:** Retrieval returns poor results for videos with no speech if the visual content is ambiguous, as the audio score (1-α) becomes noise.
  - **"Vector Drift":** If the base VLM weights are updated but the retrieval vector τ is not recalculated, the addition operation may result in weight corruption.
- **First 3 experiments:**
  1. **Ablate the Retrieval Vector:** Run retrieval using only M_F (fine-tuned) vs. M_R (with vector) on a held-out set to quantify the exact contribution of the weight addition.
  2. **Alpha Sensitivity Analysis:** Sweep the fusion weight α (e.g., 0.0 to 1.0) on queries that are purely visual vs. purely topical to find optimal dynamic weighting rules.
  3. **Reranker Latency Budget:** Measure the end-to-end latency with and without the LLM reranker to determine if the 6% quality gain is worth the user-perceived delay.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can visual information be effectively integrated into the LLM-based re-ranking phase to improve candidate ranking? The authors note that visual information is currently not sufficiently incorporated during re-ranking and plan to explore integrating visual cues for better outcomes.

- **Open Question 2:** What is the optimal operational balance between retrieval effectiveness and system latency in a multi-agent video search architecture? The authors note that the agent-based flow introduces higher latency than simple retrieval and seek to balance effectiveness with system speed.

- **Open Question 3:** Does translating audio transcriptions into English prior to embedding result in information loss compared to native multilingual processing? The system translates non-English transcriptions to English using gpt-4o-mini before embedding, rather than utilizing a shared multilingual embedding space directly for the raw text.

## Limitations

- **Weight Arithmetic Generalization:** The retrieval vector transfer mechanism assumes linear separability of retrieval capabilities across models but has not been validated on architectures beyond the Qwen2-VL family or for non-English video domains.

- **Modality Fusion Reliability:** The fixed α=0.5 weighting assumes balanced visual and audio contributions, with no validation for scenarios with severe modality imbalance such as silent videos or speech-only content.

- **Reranker Dependence:** The LLM re-ranker's effectiveness relies on high-quality video descriptions and ASR transcripts, potentially degrading significantly with noisy or hallucinated transcriptions.

## Confidence

- **High Confidence:** Zero-shot benchmark results (MultiVENT 2.0 nDCG@10 = 0.680) - the evaluation methodology is standard and results are reproducible with the specified dataset.
- **Medium Confidence:** Weight arithmetic mechanism - while the math is clearly specified, the transferability of retrieval vectors across model families remains unproven.
- **Low Confidence:** Real-world deployment viability - the paper demonstrates academic benchmark performance but lacks validation on diverse, practical video corpora.

## Next Checks

1. **Weight Arithmetic Transferability Test:** Apply the retrieval vector τ from GME-Qwen2-VL to a different VLM architecture (e.g., LLaVA-NeXT) and measure retrieval performance degradation to validate the mechanism's generality.

2. **Modality Imbalance Evaluation:** Create test sets with extreme modality conditions (pure visual queries on speech-heavy videos, and vice versa) to measure the robustness of the α=0.5 fusion strategy.

3. **Reranker Hallucination Stress Test:** Evaluate retrieval performance when feeding the re-ranker with progressively more hallucinated or contradictory video descriptions to quantify the system's vulnerability to ASR errors.