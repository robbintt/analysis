---
ver: rpa2
title: A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks
arxiv_id: '2510.25366'
source_url: https://arxiv.org/abs/2510.25366
tags:
- adam
- loss
- gradient
- convex
- non-convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficient optimization in deep
  neural networks by leveraging convexity properties of the loss function. The authors
  propose a two-phase training algorithm that detects the transition from non-convex
  to convex regions of the loss function and switches between Adam and Conjugate Gradient
  (CG) optimization methods accordingly.
---

# A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks

## Quick Facts
- arXiv ID: 2510.25366
- Source URL: https://arxiv.org/abs/2510.25366
- Reference count: 4
- Primary result: Two-phase Adam→CG training consistently outperforms Adam-only across ViT architectures and datasets by exploiting convexity transitions in loss landscapes.

## Executive Summary
This paper addresses the problem of efficient optimization in deep neural networks by leveraging convexity properties of the loss function. The authors propose a two-phase training algorithm that detects the transition from non-convex to convex regions of the loss function and switches between Adam and Conjugate Gradient (CG) optimization methods accordingly. Experiments with various Vision Transformer architectures and VGG5 on MNIST, CIFAR-10, and CIFAR-100 datasets demonstrate consistent superiority of the two-phase approach, achieving better final loss and accuracy compared to pure Adam optimization.

## Method Summary
The proposed two-phase training algorithm monitors gradient norm behavior during Adam optimization to detect when the loss function transitions from non-convex to convex regions. Once the gradient norm drops below 90% of its peak value, training switches from Adam to Conjugate Gradient with golden section line search. This exploits the mathematical property that local minima are surrounded by convex neighborhoods where second-order methods like CG achieve superlinear convergence, while Adam efficiently navigates the initial non-convex phase.

## Key Results
- Two-phase Adam→CG consistently outperforms Adam-only optimization across all tested Vision Transformer variants
- Algorithm achieves better final loss and accuracy on MNIST, CIFAR-10, and CIFAR-100 datasets
- The convexity transition detection via gradient norm peaks is robust across different architectures and datasets
- Small batch sizes (<512) underperform while very large batches (>2048) slow convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local minima of smooth functions are surrounded by convex regions where second-order methods converge superlinearly.
- Mechanism: At any local minimum, the Hessian H(x) = ∇²L(x) is positive definite (all eigenvalues positive). This positive definiteness mathematically defines a convex neighborhood. Conjugate Gradient (CG) exploits the quadratic structure of convex functions to achieve superlinear convergence, unlike first-order methods which converge linearly.
- Core assumption: The training trajectory passes through such a convex region before reaching the minimum, rather than hitting multiple local minima or saddle points.
- Evidence anchors:
  - [abstract] "However, a local minimum implies that, in some environment around it, the function is convex. In this environment, second-order minimizing methods such as the Conjugate Gradient (CG) give a guaranteed superlinear convergence."
  - [section 1, page 2] "This axiomatically results from the definition of a local minimum... and the Hessian H(x) = ∇²L(x)... being positive definite"
  - [corpus] Weak direct support; related work on non-convex optimization (AYLA, SGD convergence studies) does not address convexity transitions.
- Break condition: If the loss landscape contains multiple alternating convex/non-convex regions (Figure 4 shows this is possible), or if saddle points with near-zero gradient norms appear, the simple two-phase model fails.

### Mechanism 2
- Claim: The gradient norm vs. loss relationship signals convexity transitions detectable during training.
- Mechanism: In non-convex regions, as loss decreases, gradient norm increases (the function steepens). In convex regions, as loss decreases, gradient norm decreases (level curves become sparser). This creates a characteristic "peak" in gradient norm that marks the swap point.
- Core assumption: The loss function follows a simple pattern: initial non-convex region → single transition point → convex region around minimum. No alternating patches.
- Evidence anchors:
  - [section 3, page 3] "The gradient norm... first increases (the non-convex region) and then decreases (the convex region)"
  - [section 5, page 6] Figure 5 shows empirical gradient norm vs. loss curve with clear peak at loss ≈ 0.04
  - [corpus] No direct corpus validation of this specific detection heuristic.
- Break condition: If gradient norm fluctuates without a clear peak (e.g., zigzag optimization paths), or shows multiple peaks, automatic detection fails. Paper notes smoothing may be required.

### Mechanism 3
- Claim: Adam efficiently navigates non-convex regions via adaptive step sizes; CG efficiently exploits convex regions via conjugate directions.
- Mechanism: Adam uses per-parameter adaptive learning rates from gradient moments, handling irregular landscapes and batch noise. CG builds conjugate search directions that remain orthogonal under the Hessian, enabling direct paths to quadratic minima. The switch at the convexity boundary pairs each optimizer with its favorable regime.
- Core assumption: Adam reaches the convex region (CG alone may not, per page 8: "CG alone did not perform well in the initial non-convex phase, which caused a considerable lag").
- Evidence anchors:
  - [section 4, page 4-5] Algorithm 1 formalizes the switch: monitor gradient norm, swap when gn ≤ gnmax × 0.9
  - [section 5, page 6] Figure 6 shows two-phase (blue→magenta) outperforms Adam-only (blue→green)
  - [corpus] No corpus comparison of hybrid Adam-CG approaches.
- Break condition: If Adam's batch noise prevents reaching the convex region, or if the convex region is too small for CG to amortize its per-step computational overhead (line search), the hybrid underperforms.

## Foundational Learning

- Concept: **Convexity and Hessian positive definiteness**
  - Why needed here: The entire method hinges on recognizing that local minima have convex neighborhoods defined by positive eigenvalues of ∇²L.
  - Quick check question: Given a point where ∇L = 0 and H has eigenvalues [-1, 2, 3], is this a local minimum? Is the neighborhood convex?

- Concept: **Conjugate Gradient method**
  - Why needed here: CG is the proposed second-phase optimizer. Understanding its conjugate direction construction and line search requirement is essential for implementation.
  - Quick check question: Why does CG require a line search within each iteration, and how does this differ from Adam's step?

- Concept: **Gradient norm as a landscape diagnostic**
  - Why needed here: The swap detection relies on interpreting gradient norm behavior relative to loss.
  - Quick check question: In a convex quadratic function f(x) = x², how does ||∇f|| behave as f decreases toward its minimum?

## Architecture Onboarding

- Component map: Adam optimizer → gradient norm monitoring → swap detection (gn < 0.9 × gnmax) → Conjugate Gradient with line search

- Critical path:
  1. Initialize Adam with batch size 512 (empirically validated for these datasets).
  2. Compute gradient norm after each epoch; update gnmax.
  3. When swap triggers, freeze Adam state and hand off parameters to CG.
  4. CG iterates with line search until convergence or epoch budget exhausted.

- Design tradeoffs:
  - Batch size: Small batches (<512) underperform; large (>2048) slow convergence. Batch size affects gradient norm noise.
  - gn_fact tolerance: 0.9 balances early swap (more CG epochs, risk of non-convex entry) vs. late swap (wasted Adam epochs in convex region).
  - CG line search tolerance: Tight tolerance improves convergence but increases compute per epoch.

- Failure signatures:
  - Gradient norm never peaks (monotonic increase or decrease): suggests hypothesis violated; fall back to Adam.
  - Multiple gradient norm peaks: indicates alternating convex/non-convex regions; two-phase unsuitable.
  - CG phase loss increases: line search failure or non-convex entry; verify swap timing.

- First 3 experiments:
  1. **Baseline replication**: Train ViT-small on CIFAR-10 with Adam only (700 epochs) vs. Adam→CG (300→490 epochs). Plot loss and gradient norm vs. loss to verify peak pattern.
  2. **Swap timing sensitivity**: Vary gn_fact ∈ {0.8, 0.85, 0.9, 0.95} and measure final loss. Early swaps risk non-convex CG; late swaps waste optimization budget.
  3. **Architecture generalization**: Apply to a different architecture (e.g., ResNet-18) on CIFAR-10. Check if gradient norm peak appears. If not, document failure mode.

## Open Questions the Paper Calls Out

- Does the non-convex-to-convex transition hypothesis hold for large-scale language models (LLMs) and text-based architectures?
  - Basis in paper: [explicit] "Nevertheless, the next goal of our work is to verify the hypothesis on a large text-based model."
  - Why unresolved: Experiments were limited to vision architectures (ViT variants, VGG5) on image classification tasks; no NLP or transformer decoder models were tested.
  - What evidence would resolve it: Apply the two-phase algorithm to training runs of LLMs (e.g., GPT-style models) and observe whether the gradient norm peak pattern persists and whether Adam+CG improves convergence.

- How does the two-phase algorithm scale to models with hundreds of millions or billions of parameters?
  - Basis in paper: [explicit] "Of course, it must be questioned how far this empirical finding can be generalized to arbitrary architectures, mainly to large models."
  - Why unresolved: Tested models were deliberately small to remain computationally feasible; CG's line search requires multiple forward passes, and its memory/compute overhead at scale is unknown.
  - What evidence would resolve it: Benchmark Adam+CG on large-scale models (e.g., ResNet-50, LLaMA-scale) with wall-clock time and memory comparisons.

- Is the gradient norm peak a universally reliable indicator of entry into a convex region, or can it produce false positives/negatives?
  - Basis in paper: [inferred] The paper acknowledges the hypothesis "will not apply to arbitrary tasks" and that "arbitrary patchwork" of convex/non-convex regions may exist.
  - Why unresolved: The swap point detection is heuristic; no theoretical guarantee links gradient norm monotonicity to local convexity in high-dimensional non-quadratic landscapes.
  - What evidence would resolve it: Systematic analysis across diverse loss surfaces, comparing detected swap points against explicit Hessian eigenvalue sign checks.

- How sensitive is the method's performance to the arbitrary threshold factor (gn_fact = 0.9) used for detecting the swap point?
  - Basis in paper: [inferred] The 0.9 threshold is stated without justification or sensitivity analysis; it may not generalize across datasets, architectures, or batch sizes.
  - Why unresolved: Different noise levels in gradient estimates (varying batch sizes, architectures) could shift the optimal threshold.
  - What evidence would resolve it: Ablation studies varying gn_fact across multiple model/dataset combinations to determine robustness and optimal ranges.

## Limitations

- The method critically depends on the existence of a single, well-defined convexity transition in the loss landscape, which may not hold for all architectures or tasks.
- Implementation details like Adam hyperparameters, CG line search tolerances, and ViT architectural specifics are unspecified, requiring assumptions that may affect reproducibility.
- The generality of the method across different architectures beyond Vision Transformers and datasets beyond CIFAR variants is not established.

## Confidence

- **High Confidence**: The mathematical foundation that local minima have convex neighborhoods (positive definite Hessian) is well-established in optimization theory.
- **Medium Confidence**: The empirical observation that real neural network loss functions exhibit this convexity structure and that the gradient norm peak reliably indicates the transition is supported by the presented experiments but needs broader validation.
- **Low Confidence**: The generality of the method across different architectures beyond Vision Transformers and datasets beyond CIFAR variants is not established.

## Next Checks

1. Test the algorithm on ResNet-18/CNN architectures on CIFAR-10 to verify gradient norm peak detection generalizes beyond Vision Transformers.
2. Vary gn_fact tolerance (0.8, 0.85, 0.9, 0.95) systematically to quantify sensitivity to swap timing and identify optimal tradeoff between Adam epochs and CG epochs.
3. Implement Adam-only with doubled epoch count (1400 epochs) as an alternative baseline to compare whether the two-phase approach's improvement is due to better optimization or simply more total epochs.