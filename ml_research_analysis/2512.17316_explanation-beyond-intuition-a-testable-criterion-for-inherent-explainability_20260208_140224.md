---
ver: rpa2
title: 'Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability'
arxiv_id: '2512.17316'
source_url: https://arxiv.org/abs/2512.17316
tags:
- explanation
- risk
- coverage
- evidence
- explainability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a formal criterion for inherent explainability
  in machine learning models, addressing the lack of rigorous standards in the field.
  The authors introduce a graph-based framework that decomposes models into explainable
  subgraphs using a hypothesis-evidence structure called annotations.
---

# Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability

## Quick Facts
- arXiv ID: 2512.17316
- Source URL: https://arxiv.org/abs/2512.17316
- Reference count: 40
- Authors propose formal criterion for inherent explainability using graph decomposition and annotation hierarchies

## Executive Summary
This paper addresses the lack of rigorous standards for inherent explainability in machine learning by proposing a formal, testable criterion based on graph decomposition and annotation hierarchies. The framework requires full structural coverage (explaining all model components) and compositional coverage (explaining how components combine), validated through verifiable hypothesis-evidence structures called annotations. Applied to the PREDICT cardiovascular disease risk model, the criterion demonstrates complete coverage and provides a scalable approach that can differentiate explainable models from merely explained ones, supporting regulatory compliance.

## Method Summary
The framework represents any ML model as a computational graph G = (V, E, W, Φ) where nodes are operations and edges represent data flow. Models are decomposed into explainable subgraphs with leaf annotations (A = (S, H_A, Ξ_A)) pairing subgraphs with human-comprehensible hypotheses and verifiable evidence. An annotation hierarchy tracks the composition path from leaves to global explanation, requiring both structural coverage (every node covered) and compositional coverage (every combination explained). The criterion is validated on a PREDICT Cox proportional hazards model with 12 predictors and 3 interaction terms.

## Key Results
- Framework successfully explains why simple linear regression is explainable while very large regression models and dense neural networks typically are not
- PREDICT cardiovascular disease risk model achieves full structural coverage (C^V_struct = 1.0) and full compositional coverage (C^V_comp = 1.0)
- Framework provides scalable, verifiable approach that can support regulatory compliance and differentiate explainable models from merely explained ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Any ML model can be uniformly represented as a computational graph for systematic decomposition
- Mechanism: Models formalized as DAGs G = (V, E, W, Φ) where nodes are operations, edges represent data flow, enabling uniform decomposition of regression, decision trees, and neural networks
- Core assumption: Meaningful subgraphs exist where behavior can be characterized independently
- Evidence anchors: Abstract states "uses graph theory for representing and decomposing models"; section 3.3 describes DAG representation
- Break condition: Dense networks with full connectivity lack natural structural boundaries

### Mechanism 2
- Claim: Explanations require two-level hypothesis-evidence structure (annotations) separating claims from verification
- Mechanism: Each annotation A = (S, H_A, Ξ_A) pairs subgraph with human-comprehensible hypothesis and verifiable evidence; allows same explanation to serve different audiences
- Core assumption: Intended audience accepts verification method as epistemically adequate
- Evidence anchors: Abstract mentions "verifiable hypothesis-evidence structure"; section 3.2 describes two-level structure
- Break condition: Evidence fails if verification method not accepted by target audience

### Mechanism 3
- Claim: Inherent explainability requires both full structural AND compositional coverage
- Mechanism: Structural coverage ensures every node covered by leaf annotation; compositional coverage ensures every combination explained through composition annotations
- Core assumption: Meaningful groupings exist for compositional explanations
- Evidence anchors: Abstract differentiates "structural coverage (explaining all components) and compositional coverage (explaining how components combine)"
- Break condition: Large models lack meaningful groupings for comprehensible compositional coverage

## Foundational Learning

- Concept: **Computational Graph Formalism**
  - Why needed here: Enables unified treatment of all model types; without understanding node/edge representation, decomposition strategy is opaque
  - Quick check question: Given a simple neural network layer (linear transform + ReLU), can you identify which parts become nodes vs. edges in the computational graph?

- Concept: **Hazard Ratios and Cox Proportional Hazards**
  - Why needed here: PREDICT case study uses Cox model where coefficients translate to hazard ratios; understanding HR = exp(β) is essential for interpreting annotations
  - Quick check question: If a coefficient β = 0.0756 for age, what is the hazard ratio and what does it mean clinically?

- Concept: **Epistemic Adequacy**
  - Why needed here: Framework makes verification standards context-dependent and audience-relative; understanding different community standards is crucial for application
  - Quick check question: Would a clinician and a statistician accept the same evidence for a model explanation? Why or why not?

## Architecture Onboarding

- Component map: Computational Graph -> Leaf Annotations -> Composition Annotations -> Annotation Hierarchy -> Coverage Metrics
- Critical path: 1) Convert model to computational graph 2) Identify meaningful subgraph boundaries 3) Create leaf annotations 4) Verify structural coverage 5) Build composition annotations 6) Verify compositional coverage 7) Confirm root annotation
- Design tradeoffs: Analytical vs. empirical verification (strength vs. tractability); decomposition granularity (clarity vs. complexity); audience scope (single vs. multiple explanations)
- Failure signatures: Structural coverage gaps (C_struct < 1.0); compositional vacuity (restates structure without insight); evidence-audience mismatch; intractable decomposition
- First 3 experiments:
  1. Validate on simple regression y = 2x₁ + 3x₂ + 1; create leaf annotations, verify C_struct = 1.0, create composition annotation for additive combination, verify C_comp = 1.0
  2. Test compositional failure on 100-feature regression with random feature names; demonstrate structural coverage succeeds but compositional coverage fails
  3. Reproduce PREDICT annotation for one predictor (e.g., diabetes with age interaction); construct complete annotation with hypothesis, analytical evidence, and composition with age cluster

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What defines the upper bound of tractability for creating well-formed global explanations in complex models?
- Basis in paper: [explicit] Authors state "it is unclear what the upper bound on what is tractable," noting boundary is ultimately empirical
- Why unresolved: Framework allows any valid explanation but doesn't theoretically predict when compositional coverage becomes impossible
- What evidence would resolve it: Empirical studies on diverse model architectures to determine where annotation hierarchy construction consistently fails

### Open Question 2
- Question: How can the utility and quality of a formally valid explanation be evaluated relative to specific audience and purpose?
- Basis in paper: [explicit] Framework "does not specifically address how to evaluate the quality of an explanation and if it is useful"
- Why unresolved: While criterion ensures explanation is accurate and verifiable, it doesn't measure if explanation successfully serves intended purpose
- What evidence would resolve it: User studies measuring decision-making efficacy and comprehension when provided with framework-satisfying explanations

### Open Question 3
- Question: Can meaningful subgraphs be identified within dense neural networks (e.g., transformers) to satisfy framework criteria?
- Basis in paper: [explicit] Lists "future research directions to explain neural networks" and highlights difficulty of finding "needles in a $10^{101}$-sized haystack" in dense models
- Why unresolved: Dense networks lack natural structural boundaries present in sparse networks or decision trees, making decomposition intractable
- What evidence would resolve it: Successful application to dense network by identifying hierarchy of functional circuits achieving full structural and compositional coverage

## Limitations
- Scalability to high-dimensional models with dense connectivity remains untested
- Verification standards are explicitly audience-relative, making application context-dependent
- Case study lacks complete statistical details (confidence intervals, significance thresholds)

## Confidence

- High confidence: Graph-based decomposition mechanism and annotation hierarchy structure are mathematically sound
- Medium confidence: PREDICT case study successfully demonstrates approach but uses inherently explainable model type
- Low confidence: Claims about framework scalability to complex models and general applicability to all ML model classes

## Next Checks
1. Test compositional coverage failure: Apply framework to 100-feature regression with random feature names; verify C_struct = 1.0 while compositional coverage fails due to lack of meaningful groupings
2. Evaluate dense network decomposition: Apply to fully connected neural network layer; document whether meaningful subgraph boundaries exist and quantify candidate decompositions (2^|E|)
3. Audience-dependent verification: Construct identical explanations using analytical vs. empirical evidence for PREDICT model; test whether clinicians and statisticians accept these as epistemically adequate verification methods