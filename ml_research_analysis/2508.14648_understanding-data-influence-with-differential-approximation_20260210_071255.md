---
ver: rpa2
title: Understanding Data Influence with Differential Approximation
arxiv_id: '2508.14648'
source_url: https://arxiv.org/abs/2508.14648
tags:
- influence
- data
- training
- diff-in
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diff-In, a novel influence estimation method
  that formulates sample influence as the cumulative sum of its changes across training
  iterations. Unlike previous methods that rely on convexity assumptions or fail to
  capture training dynamics, Diff-In uses second-order approximations to compute these
  changes efficiently through Hessian-gradient products approximated via finite differences.
---

# Understanding Data Influence with Differential Approximation

## Quick Facts
- arXiv ID: 2508.14648
- Source URL: https://arxiv.org/abs/2508.14648
- Reference count: 40
- Primary result: Diff-In achieves 91.4% precision on ImageNet data cleaning, outperforming prior methods by 13 percentage points

## Executive Summary
This paper introduces Diff-In, a novel influence estimation method that reformulates sample influence as the cumulative sum of parameter changes across training iterations. By approximating second-order information through Hessian-gradient products computed via finite differences, Diff-In achieves first-order computational complexity while maintaining second-order accuracy. The method demonstrates superior performance across three data-centric tasks—data cleaning (91.4% precision on ImageNet), data deletion (89.9% accuracy on LLMs), and coreset selection (1.3-2.1 percentage point improvements over CLIP-score)—while maintaining strong scalability through checkpoint-based time sampling.

## Method Summary
Diff-In computes influence by accumulating the differences in parameter influence estimates across training checkpoints rather than approximating the final influence directly. The method uses second-order Taylor expansion to approximate how each training step changes influence, computing Hessian-gradient products efficiently through finite differences of gradients. By sampling a small number of checkpoints (typically m=5), Diff-In captures training dynamics while maintaining O(p) complexity per sample, making it scalable to large models and datasets.

## Key Results
- Achieves 91.4% precision in identifying mislabeled data on ImageNet versus 78.4% for prior best methods
- Reaches 89.9% accuracy for data deletion on large language models versus 82.5% for state-of-the-art alternatives
- Consistently outperforms CLIP-score by 1.3-2.1 percentage points in coreset selection for vision-language pretraining across multiple downstream tasks

## Why This Works (Mechanism)

### Mechanism 1: Cumulative Difference Formulation
- **Claim**: Accumulating influence differences across training steps enables second-order approximation without requiring convex loss assumptions.
- **Mechanism**: Reformulates influence as Σₜ[Iᵗ⁺¹_θ(z) − Iᵗ_θ(z)] rather than directly approximating Iθ(z) = θ*₋z − θ*. Each difference term represents local changes that can be approximated with second-order Taylor expansion.
- **Core assumption**: Training trajectory is sufficiently smooth (ℓ-Lipschitz continuous gradients).
- **Evidence anchors**: [abstract] "cumulative sum of its changes/differences across successive training iterations"; [Section 4.1, Eq.(4)] formalizes cumulative sum; related work notes computational challenges with standard influence estimation.
- **Break condition**: Erratic training dynamics (large learning rates, sharp loss landscape changes) where local smoothness breaks down.

### Mechanism 2: Hessian-Gradient Products via Finite Differences
- **Claim**: Second-order information can be computed efficiently by approximating Hessian-gradient products using gradient differences.
- **Mechanism**: Uses H·G ≈ lim_{ε→0} [∇L(θ + εG) − ∇L(θ)] / ε, requiring only two gradient computations instead of explicit Hessian.
- **Core assumption**: Perturbation ε is small enough for valid approximation but large enough to avoid numerical instability.
- **Evidence anchors**: [abstract] "computing the product of the Hessian and gradient... using finite differences of first-order gradients"; [Section 4.1.1, Eq.(6)] provides finite difference formula; related work confirms curvature information improves accuracy.
- **Break condition**: Near-zero or highly noisy gradients causing numerical instability; very large parameter counts making even O(p) computation prohibitive.

### Mechanism 3: Checkpoint-Based Time Sampling
- **Claim**: Using a small number of strategically sampled checkpoints captures sufficient training dynamics while maintaining scalability.
- **Mechanism**: Instead of computing at all T steps (O(T²p) complexity), uniformly samples m checkpoints and uses Ďt(z) ≈ −t(ηt)²/N (Hᵗ_Bt Gᵗ_z + Hᵗ_z Gᵗ_Bt).
- **Core assumption**: Training influence accumulates smoothly enough that sparse temporal sampling captures the trajectory.
- **Evidence anchors**: [Section 4.2] describes checkpoint strategy; [Table 9] shows m=5 achieves near-optimal performance; related methods also use checkpoint strategies.
- **Break condition**: Critical influence changes between sampled checkpoints (early stopping points, phase transitions) causing sparse sampling to miss important dynamics.

## Foundational Learning

- **Concept: Leave-One-Out (LOO) Retraining and Influence Functions**
  - **Why needed here**: Diff-In approximates what would happen if you removed a training sample and retrained, but without actually doing the expensive retraining.
  - **Quick check question**: Can you explain why I(z,V) = L(V, θ*₋z) − L(V, θ*) measures "influence on validation loss"?

- **Concept: Taylor Expansion for Loss Landscape Approximation**
  - **Why needed here**: The method relies on second-order Taylor expansion at each step to approximate how parameters change when a sample's contribution shifts.
  - **Quick check question**: Why does a second-order (Hessian) term capture curvature information that first-order (gradient) terms miss?

- **Concept: Hessian-Vector Products Without Storing Hessians**
  - **Why needed here**: The finite-difference trick is central to making Diff-In computationally tractable.
  - **Quick check question**: Given a function f(θ), how would you compute Hf · v using only gradient calls?

## Architecture Onboarding

- **Component map**: Training data D -> Checkpoint sampler -> Influence difference calculator -> Accumulator -> Output (Iθ(z) and/or I(z,V))

- **Critical path**: The finite difference gradient computation (Eq. 6) is the computational bottleneck—requires one forward-backward pass for ∇L(θ) and one for ∇L(θ + εG). Ensure batch size and ε are tuned for numerical stability.

- **Design tradeoffs**:
  - **m (checkpoint count)**: Higher m → better accuracy but linear time increase. Paper recommends m=5 as default; m=1 (last checkpoint only) as efficient variant.
  - **Self-influence vs validation influence**: Self-influence I(z,z) is faster (no separate validation set) and works better for data cleaning; validation influence I(z,V) better for tasks where you care about downstream impact.
  - **Batch proxy size**: Paper uses random batch of 512-2048 as proxy for full dataset gradient; larger batches improve accuracy but increase memory/time.

- **Failure signatures**:
  - Low correlation with actual LOO retraining → checkpoint sampling may be too sparse or ε poorly chosen
  - Numerical overflow in finite difference → ε too small or gradient magnitudes too large (may need gradient clipping)
  - Method performs worse than baselines on specific task → verify you're using correct influence type (self-influence for cleaning, training influence for coreset selection)

- **First 3 experiments**:
  1. **Validation on small dataset**: Train ResNet-18 on CIFAR-10 subset, compute Diff-In for 30 samples, compare against brute-force LOO retraining using Pearson correlation (target: >0.7 as in Table 11).
  2. **Ablate checkpoint count**: Test m ∈ {1, 3, 5, 10} on data cleaning task to find optimal efficiency-accuracy tradeoff for your compute budget.
  3. **Perturbation sensitivity**: Vary ε in finite difference approximation to identify stable range; verify gradient magnitudes are reasonable (not exploding/vanishing) at sampled checkpoints.

## Open Questions the Paper Calls Out
None

## Limitations
- Method's effectiveness depends critically on smoothness of training dynamics - non-convex loss landscapes or erratic optimization may cause accumulated approximation errors
- Finite-difference Hessian approximation introduces numerical instability risks when gradients are near-zero or highly noisy
- While m=5 checkpoints provides strong empirical results, this may not generalize to all architectures or training regimes

## Confidence

**High confidence**: The cumulative difference formulation and finite-difference Hessian approximation (mechanisms 1 and 2) are mathematically sound and well-grounded in existing optimization theory

**Medium confidence**: The checkpoint sampling strategy's effectiveness across diverse training scenarios, particularly for non-vision tasks or very long training runs

**Medium confidence**: The claimed scalability benefits when applied to extremely large models (billion+ parameters) where even O(p) operations become computationally intensive

## Next Checks

1. Test Diff-In on training scenarios with known non-smooth dynamics (e.g., aggressive learning rate schedules, sharp loss landscapes) to quantify approximation error accumulation

2. Benchmark Diff-In on extremely large-scale models (LLMs with >10B parameters) to validate the claimed O(p) versus O(p²) complexity advantage

3. Evaluate the method's sensitivity to checkpoint sampling frequency by systematically varying m and measuring correlation with ground-truth LOO retraining across multiple tasks and datasets