---
ver: rpa2
title: 'ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures'
arxiv_id: '2512.19730'
source_url: https://arxiv.org/abs/2512.19730
tags:
- architectures
- detection
- features
- backdoor
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of poor generalization of neural
  backdoor detection methods across different model architectures. Existing learning-based
  approaches fail when detecting backdoors in architectures not seen during training.
---

# ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures

## Quick Facts
- **arXiv ID:** 2512.19730
- **Source URL:** https://arxiv.org/abs/2512.19730
- **Reference count:** 40
- **Primary result:** Achieves up to 42.5% improvement in detection performance (AUC) on unseen architectures compared to state-of-the-art methods

## Executive Summary
This paper addresses the critical challenge of neural backdoor detection generalization across diverse model architectures. Existing learning-based backdoor detection methods suffer from poor cross-architecture performance, often failing when detecting backdoors in architectures not seen during training. The authors propose ArcGen, a novel method that learns architecture-invariant features for backdoor detection through alignment layers and distribution-level alignment losses. The approach enables effective detection of backdoors in previously unseen architectures (e.g., CNNs detecting backdoors in ViTs) without requiring retraining, achieving significant performance improvements over existing methods.

## Method Summary
ArcGen employs a meta-learning framework that learns a shared feature space where architectural biases are minimized while preserving backdoor-relevant information. The method introduces learnable alignment layers that transform model outputs into architecture-invariant features, combined with an adversarial training process using an architecture discriminator to encourage distribution-level alignment. Additionally, sample-level alignment enforces that models with identical attack settings but different architectures have similar features. The framework jointly optimizes a learnable query set, alignment layers, detector, and architecture discriminator through a combination of target loss, distribution alignment loss (with gradient reversal), and sample-level alignment loss. Evaluation demonstrates significant improvements in cross-architecture detection performance across multiple datasets and architecture families.

## Key Results
- Achieves up to 42.5% improvement in detection AUC on unseen architectures compared to state-of-the-art methods
- Successfully detects backdoors in vision transformer models (ViT-B/16, ViT-L/32) without any transformer-specific training
- Shows consistent performance improvements across multiple datasets including GTSRB, CIFAR-10, and ImageNet-10

## Why This Works (Mechanism)

### Mechanism 1: Alignment Layer Transforms Architecture-Dependent Outputs
Learnable alignment layers process raw model outputs to project them into a shared feature space where architectural biases are reduced. This is theoretically supported by the universal approximation theorem, which suggests a sufficiently expressive network can minimize distribution divergence between architectures while preserving backdoor-relevant semantics. The alignment layers ensure that models with similar backdoor behaviors have aligned features regardless of their architecture.

### Mechanism 2: Adversarial Distribution-Level Alignment via Architecture Discriminator
An adversarial training process with an architecture discriminator encourages the feature extractor to produce features indistinguishable across architectures at the distribution level. A gradient reversal layer enables joint training where the discriminator tries to predict architecture from features while the feature extractor maximizes the discriminator's loss, forcing features to lose architecture-identifying information while maintaining backdoor discriminability.

### Mechanism 3: Sample-Level Alignment via Same-Attack Constraint
Models with identical attack settings (trigger pattern, target label) but different architectures are explicitly enforced to have similar features through sample-level alignment loss. This fine-grained alignment operates at the pairwise level, minimizing cosine distance between features of models sharing the same attack configuration. This constraint is applied periodically to avoid computational overhead and instability.

## Foundational Learning

- **Concept: Neural Backdoor Attacks (BadNets, Blended, WaNet, etc.)**
  - Why needed here: Understanding how backdoors are implanted (poisoning, trigger patterns, stealthy attacks) is essential to interpret why query-based feature extraction can reveal backdoor presence. The paper evaluates on five attack types with different trigger characteristics.
  - Quick check question: Can you explain why a patch-based trigger (BadNets) might produce different model output patterns than a warping-based trigger (WaNet), and why a detector would need to generalize across both?

- **Concept: Black-Box Query-Based Model Analysis**
  - Why needed here: ArcGen operates in a black-box setting, only accessing model outputs for input queries. This contrasts with white-box methods using gradients or internal activations. Understanding the black-box constraint clarifies why architecture-invariant features must be extracted solely from output behaviors.
  - Quick check question: If you only have access to a model's predicted class probabilities, what types of information could you still extract about the model's behavior?

- **Concept: Domain Generalization and Adversarial Feature Alignment**
  - Why needed here: The core challenge is a domain shift problem—features learned on some architectures don't generalize to unseen architectures. ArcGen adapts domain generalization techniques including adversarial training with an architecture discriminator inspired by DANN and architecture randomization augmentation.
  - Quick check question: In a domain adversarial neural network (DANN), what is the role of the gradient reversal layer, and what property of the features does it encourage?

## Architecture Onboarding

- **Component map:**
  Target Model --(query set q)--> Model Outputs p --> Alignment Layers θ_al --> Features e --> Detector θ_b (binary output) and Architecture Discriminator θ_ad (N-class)
  Joint Optimization with GRL: Target Loss L_t + λ_dla·L_dla + periodic L_sla

- **Critical path:**
  1. Train proxy backdoored models across multiple architectures with same attack settings for sample-level alignment
  2. Jointly optimize query set, alignment layers, detector, and discriminator using combined losses
  3. For any target model, feed optimized queries, process outputs through alignment layers, then apply detector

- **Design tradeoffs:**
  - More training architectures vs. generalization: Adding more architectures doesn't always improve results; trade-off between training cost and generalization quality
  - Alignment loss weights (λ_dla, τ_epoch): Higher distribution alignment weight may improve unseen-architecture performance but could harm seen-architecture performance
  - Proxy model generation: Using random triggers avoids prior attack knowledge but may not perfectly match real attack distributions
  - Alignment layer capacity: Two FC layers may be insufficient for very diverse architecture families

- **Failure signatures:**
  - Architecture overfitting: Features cluster by architecture rather than backdoor status (monitor via t-SNE)
  - Discriminator collapse: If discriminator loss doesn't decrease or fluctuates wildly, adversarial training is failing
  - Sample-level loss conflict: If L_sla pushes backdoored features toward benign clusters, detection performance degrades
  - Query set overfitting: If queries become too specialized to training architectures, they fail to elicit informative outputs from unseen architectures

- **First 3 experiments:**
  1. Reproduce cross-architecture generalization gap: Train MNTD on GTSRB using ResNet18 only, evaluate on ResNet18 vs. MobileNetV2 to confirm large AUC drop
  2. Ablate alignment layers: Train ArcGen without ALs to observe discriminator loss convergence failure and validate ALs' role in stabilizing adversarial training
  3. Test on transformer-based models without retraining: Using ImageNet-trained ArcGen (trained on CNNs), evaluate detection AUC on ViT-B/16, ViT-B/32, ViT-L/32 and compare to baselines

## Open Questions the Paper Calls Out

- **Question 1:** Can ArcGen be modified to generalize across tasks with varying numbers of output classes (e.g., CIFAR-10 to ImageNet) without requiring detector retraining?
  - Basis in paper: Authors identify generalizing across varying class configurations as an "open challenge" since current architecture relies on fixed-dimension inputs from model output probability vectors
  - Why unresolved: The alignment layer and detector architectures depend on fixed-dimension inputs derived from output probability vectors that change with the number of classes
  - What evidence would resolve it: A modified architecture successfully detecting backdoors on a dataset with N classes after being trained exclusively on datasets with M classes (where N ≠ M)

- **Question 2:** How does the specific combination and diversity of seen architectures influence the generalization gap, and can an optimal selection strategy be defined?
  - Basis in paper: Section VI-F and VII note that adding more training architectures doesn't always improve performance, and understanding "training architectures interaction effects" is highlighted as a research direction
  - Why unresolved: The paper observes performance fluctuations but doesn't provide a model to predict which architectural combinations yield the best generalization to specific unseen architectures
  - What evidence would resolve it: A study establishing correlation between specific architectural similarity metrics and detection transferability, resulting in a heuristic for selecting proxy training architectures

- **Question 3:** How robust is the alignment mechanism when the defender's surrogate data distribution differs significantly in domain from the target model's training data?
  - Basis in paper: The paper assumes defender knows target's training data distribution and tests this only by shifting between similar image subsets, leaving generalization across distinct domains unverified
  - Why unresolved: Alignment layers may overfit to output statistics of the surrogate domain, potentially failing when underlying input features and model behaviors are drastically different
  - What evidence would resolve it: Experiments evaluating detection AUC where proxy models are trained on a source domain (e.g., CIFAR-10) and target models on a distinct target domain (e.g., GTSRB or medical dataset)

## Limitations
- Assumes architectural differences in model outputs can be disentangled from backdoor information, which may not hold for highly complex architecture families
- Requires training proxy models on seen architectures, potentially computationally expensive for large-scale deployment
- Sample-level alignment assumes identical attack settings produce similar behaviors across architectures, which may fail for fundamentally different attack mechanisms

## Confidence

- **High Confidence:** Empirical results showing ArcGen's superior cross-architecture generalization (42.5% AUC improvement) are well-supported by extensive experiments across 16,896 models and multiple datasets
- **Medium Confidence:** Theoretical motivation via universal approximation theorem for alignment layers is plausible but not rigorously proven for this specific backdoor detection context
- **Medium Confidence:** Assumption that random trigger generation for proxy models adequately represents real attack distributions is reasonable but remains an approximation

## Next Checks

1. **Cross-Attack Generalization Test:** Evaluate ArcGen on a new attack type (e.g., Trojaning Attack or SIG) not used in any training set to verify it truly generalizes beyond the five attack types tested, assessing the robustness of the random trigger assumption

2. **Architecture Diversity Stress Test:** Test ArcGen on a more diverse set of architectures including vision transformers of varying depths (ViT-S, ViT-M) and hybrid CNN-Transformer models to assess the limits of architectural generalization and identify failure points where alignment fails

3. **Computational Cost Analysis:** Measure and report the training time and memory requirements for generating proxy models and training ArcGen compared to baseline methods, providing practical deployment guidance and identifying potential bottlenecks for large-scale applications