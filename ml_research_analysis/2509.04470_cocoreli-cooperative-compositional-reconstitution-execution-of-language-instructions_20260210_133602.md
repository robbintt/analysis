---
ver: rpa2
title: 'COCORELI: Cooperative, Compositional Reconstitution \& Execution of Language
  Instructions'
arxiv_id: '2509.04470'
source_url: https://arxiv.org/abs/2509.04470
tags:
- instructions
- task
- parts
- cocoreli
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COCORELI is a hybrid agentic framework that uses medium-sized LLMs,
  abstraction mechanisms, and a discourse module to follow complex instructions, minimize
  hallucination, and perform spatial reasoning in a 3D construction environment. It
  parses instructions through specialized agents, stores learned structures as relational
  graphs, and applies abstraction to generalize learned functions across tasks.
---

# COCORELI: Cooperative, Compositional Reconstitution & Execution of Language Instructions

## Quick Facts
- **arXiv ID:** 2509.04470
- **Source URL:** https://arxiv.org/abs/2509.04470
- **Reference count:** 22
- **Primary result:** Hybrid agentic framework achieves 100% accuracy on complex 3D construction tasks vs. 90% for larger LLM baselines.

## Executive Summary
COCORELI is a hybrid agentic framework that combines medium-sized LLMs with deterministic abstraction and discourse modules to follow complex instructions in a 3D construction environment. The system decomposes tasks into specialized agents (Parser, Locator, Builder, Executor) that work with an External Memory storing relational graphs of learned structures. By using iterative clarification loops to avoid hallucination and abstraction mechanisms to generalize learned functions, COCORELI achieves 100% accuracy on ENVIRONMENT tasks and outperforms single-LLM baselines by 10% overall, even when those baselines use larger models.

## Method Summary
The system uses a pipeline of specialized LLM agents (Llama 3.1 8b) to parse instructions, locate coordinates, retrieve/build object properties, and execute deterministic JSON programs. When agents detect missing information, the Discourse Module triggers clarification questions to users. Learned structures are stored as relational graphs in External Memory and abstracted into parameterized functions that can be re-instantiated elsewhere. The approach combines flexible language understanding with deterministic functions for storage, abstraction, and execution.

## Key Results
- Achieved 100% accuracy on object identification and coordinates in ENVIRONMENT tasks
- Outperformed single-LLM CoT and agentic LLM baselines by 10% overall
- Successfully generalized learned shapes to new contexts via abstraction
- On ToolBench API completion, achieved 100% precision and recall vs. 30.95% F1 for baselines

## Why This Works (Mechanism)

### Mechanism 1: Iterative Clarification via Discourse Module
The Discourse Module reduces hallucination by triggering clarification loops when information is missing. When a downstream agent cannot fulfill parameters due to missing input, it signals failure. The Discourse Module generates a specific question to the user, whose response is appended to instruction history, providing complete context for the next attempt. This forces grounding in user-provided data rather than LLM-generated content.

### Mechanism 2: Abstraction and Relational Graph Storage
Complex structures are stored as abstract relational graphs and parameterized functions, not raw action sequences. The Builder agent converts specific instances into graphs where nodes are parts and edges encode relative spatial positions. An Abstraction function parameterizes this graph, stripping concrete values. When recreating structures, an Apply function re-instantiates the abstract graph with new parameters, enabling generalization across tasks.

### Mechanism 3: Specialized Agent Decomposition with Medium-Sized LLMs
Decomposing complex tasks into specialized agents, each powered by a smaller LLM, outperforms single large LLMs by constraining each agent's problem space. A top-level parser breaks down instructions and routes sub-tasks to specialized agents (Locator for coordinates, Builder for properties). The Executor combines structured outputs into deterministic JSON programs. This division of labor reduces cognitive load and error probability.

## Foundational Learning

- **Concept: Graph-Based Spatial Representation**
  - Why needed here: Understanding how COCORELI stores spatial knowledge as relational graphs (nodes as parts, edges as relative positions) rather than absolute coordinates is crucial for grasping its generalization capability.
  - Quick check question: If the system learns a "two-stack" shape at location (1,1), and is asked to rebuild it at (5,5), what does the stored abstract graph specify, and what must the Apply function provide?

- **Concept: Hybrid Agentic Systems**
  - Why needed here: COCORELI combines LLM agents for flexible language understanding with deterministic functions for storage and execution. The performance gains come from this hybrid approach, not solely from the LLMs.
  - Quick check question: What are the two primary types of components in COCORELI's architecture, and what is the specific role of the "Executor" agent in bridging them?

- **Concept: Abstraction in In-Context Learning (ICL)**
  - Why needed here: The system learns abstract functions by identifying parameters, stripping them of specific values, storing the parameterized template, and re-filling it later. This goes beyond simple pattern copying.
  - Quick check question: In COCORELI, what two steps must the system perform to successfully "learn" and "re-use" a complex shape like the "C15 stack"?

## Architecture Onboarding

- **Component map:** User Input -> Instruction Parser -> (Locator, Builder in parallel) -> Discourse Module (if missing info) -> User -> Instruction Parser (loop) OR Executor -> JSON Output
- **Critical path:** User Input flows through Instruction Parser to Locator and Builder agents. If either detects missing information, the Discourse Module triggers a clarification loop with the user. Successful completion sends outputs to the Executor, which produces the final JSON.
- **Design tradeoffs:** Optimizes for reliability and hallucination reduction via clarification loops and structured output, but sacrifices dialogue flexibility. Can only handle single-turn clarification questions and cannot manage complex multi-turn discourse corrections.
- **Failure signatures:**
  1. Infinite Clarification Loop: System repeatedly asks for the same missing parameter after user provides it
  2. Graph Instantiation Error: Abstract graph is malformed or Apply function miscalculates coordinates
  3. Pipeline Stagnation: Complex instruction cannot be cleanly decomposed for parallel agents
- **First 3 experiments:**
  1. Run the Single-Part Placement Task (Task i): Provide fully specified instructions and verify correct JSON output without clarification
  2. Test the Clarification Loop (Task iv): Provide underspecified instructions and verify appropriate question generation and successful parameter filling
  3. Validate Abstraction and Reuse (Task v): Build a novel structure, label it, then recreate it elsewhere and verify correct graph retrieval and instantiation

## Open Questions the Paper Calls Out
1. **Multi-modal Integration:** Can the architecture be extended to integrate multi-modal inputs for spatial reasoning beyond text?
2. **Complex Discourse Management:** How can the discourse module handle complex conversational dynamics like user corrections or multi-part clarification questions?
3. **Autonomous Planning:** Can the system incorporate autonomous planning to decompose high-level, abstract instructions into executable functions?
4. **Unstructured API Documentation:** Can the abstraction workflow be automated to parse unstructured, real-world API documentation without manual formatting?

## Limitations
- Limited dialogue flexibility: can only handle single-turn clarification questions
- Requires domain-specific implementation of functions and memory structures
- Performance claims based primarily on custom dataset and subset of ToolBench workflows
- Abstraction algorithms may not scale well to highly irregular or physics-constrained structures

## Confidence
- **High:** Core hybrid agentic architecture is clearly described and logically sound
- **Medium:** Performance numbers on custom tasks are precise but hallucination detection methodology is not fully detailed
- **Medium:** ToolBench results are promising but based on undisclosed subset of 100 workflows

## Next Checks
1. Implement and test the Discourse Module clarification logic on underspecified instructions to ensure correct parameter identification and question generation
2. Validate Nearest Neighbor Scaling algorithm on complex, non-contiguous shapes to ensure no distortions break the bijection check
3. Benchmark COCORELI on a standard multi-task benchmark (e.g., BabyAI or HotPotQA) to assess generalization beyond custom environment