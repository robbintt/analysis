---
ver: rpa2
title: 'HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement
  Learning'
arxiv_id: '2505.15011'
source_url: https://arxiv.org/abs/2505.15011
tags:
- norms
- agent
- value
- social
- hava
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of aligning reinforcement learning
  agents with societal values, particularly when norms are both explicitly defined
  (legal/safety) and implicitly learned (social). The proposed method, HAVA (Hybrid
  Approach to Value-Alignment through Reward Weighing), integrates rule-based and
  data-driven norms into a single framework.
---

# HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.15011
- **Source URL:** https://arxiv.org/abs/2505.15011
- **Reference count:** 38
- **Primary result:** HAVA produces value-aligned policies in grid-world and traffic simulation that are statistically indistinguishable from human behavior while maintaining safety compliance

## Executive Summary
HAVA addresses reinforcement learning value alignment by integrating rule-based safety/legal norms with data-driven social norms through a reputation-weighted reward mechanism. The method separates norms into mandatory (rule-based) and tentative (data-driven) categories, enforcing safety constraints while allowing social flexibility. Experiments in both grid-world and continuous traffic simulation demonstrate that HAVA produces policies that balance safety compliance with social acceptability, outperforming ablation baselines that violate either safety or social norms.

## Method Summary
HAVA modifies reinforcement learning by augmenting the state space with a reputation score that tracks compliance with both rule-based (RB) and data-driven (DD) norms. The reputation value weights received rewards, creating pressure toward value-aligned behavior. RB norms are enforced through action filtering, ensuring safety/legal compliance, while DD norms influence behavior through reputation penalties. The method uses a forgiveness parameter (α) to control reputation recovery speed, creating meaningful differentiation between minor and severe violations. Training employs deep Q-learning with double, dueling, and noisy network extensions, using the reputation-weighted rewards for policy optimization.

## Key Results
- HAVA policies were statistically indistinguishable from human behaviors in traffic simulation (p-value = 0.42)
- RB-only ablation resulted in legally-compliant but socially-unacceptable aggressive driving
- DD-only ablation violated safety norms despite learning social behaviors
- Optimal performance achieved with α = 0.1, providing balanced norm compliance

## Why This Works (Mechanism)

### Mechanism 1: Reputation-Weighted Reward Shaping
Weighing task rewards by an agent's norm-compliance reputation motivates value-aligned behavior without manually engineering aligned reward functions. The reputation score w_t ∈ [0,1] multiplies positive rewards and amplifies penalties for negative rewards, creating pressure to maintain high compliance.

### Mechanism 2: Hierarchical Norm Separation with Mandatory Enforcement
Separating norms into mandatory (RB) and tentative (DD) groups with hard enforcement on the former prevents safety violations while allowing social flexibility. RB norms filter actions to prevent safety violations, while DD norms influence via reputation penalty.

### Mechanism 3: Forgiveness-Controlled Reputation Recovery
Non-linear reputation recovery (controlled by α) creates meaningful differentiation between minor and severe violations, enabling nuanced policy selection. Lower α values create extended penalty periods, affecting more timesteps and providing stronger alignment pressure.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)** - HAVA modifies a base MDP by augmenting state space with reputation and transforming the reward function. Understanding the original MDP structure is prerequisite to grasping how AV creates M'.
  - Quick check: Can you explain how adding w_t to the state space changes the agent's decision problem?

- **Concept: Normative Hierarchy in AI Systems** - The core insight is distinguishing mandatory vs. tentative norms. Without this conceptual foundation, the RB/DD separation appears arbitrary rather than principled.
  - Quick check: In a driving scenario, what distinguishes a "stop at red light" norm from a "merge politely" norm, and why might they require different enforcement mechanisms?

- **Concept: Deep Q-Learning with Enhancements** - The continuous experiment uses Double DQN with dueling and noisy networks. Understanding these components is necessary to replicate or extend the junction scenario.
  - Quick check: Why might noisy networks be particularly valuable when learning under norm constraints that reduce exploratory freedom?

## Architecture Onboarding

- **Component map:**
  Environment → State s_t → Agent Policy π(s_t ∪ w_t) → Action a_t
                                              ↓
                                    Alignment Value AV = <R_B, D_D>
                                              ↓
                     ┌────────────────────────┴────────────────────────┐
                     ↓                                                  ↓
              R_B(s_t) → A_RB (permitted actions)              D_D(s_t) → A_DD (preferred actions)
                     ↓                                                  ↓
              If a_t ∉ A_RB → replace with closest a_RB         Compute distance d_DD
                                                                              ↓
                              Compute δ_t = min(al_RB, al_DD) ←─────────────┘
                                              ↓
                              Update reputation w_t+1 via Equations 2-3
                                              ↓
                              Environment returns r_t → Compute R_AV via Equation 4

- **Critical path:** The reputation update (Equations 2-3) and reward weighing (Equation 4) are the only novel computational elements. If these are implemented incorrectly, the entire alignment pressure disappears.

- **Design tradeoffs:**
  - τ selection: Higher τ = more tolerance for violations, easier learning but weaker alignment
  - α selection: Lower α = stronger alignment pressure but potentially slower convergence
  - RB implementation: Must be complete for safety-critical norms
  - DD training: Requires representative human trajectory data

- **Failure signatures:**
  - Pure RB ablation: Agent converges quickly to legally-compliant but socially-unacceptable behavior
  - Pure DD ablation: Agent learns social behaviors but may violate safety norms
  - α too high: Policy indistinguishable from unaligned
  - α too low: May prevent convergence if recovery period exceeds episode length

- **First 3 experiments:**
  1. Discrete gridworld validation: Implement the 49-state lawn-tile example with three handcrafted policies
  2. RB-only traffic ablation: Train agent with only R_B norms (Krauss model + speed limit)
  3. Full HAVA with α sweep: Train with both R_B and D_D, testing α ∈ {10, 0.5, 0.1}

## Open Questions the Paper Calls Out

### Open Question 1
Can automatic tuning methods eliminate the need for manual selection of hyperparameters τ and α in HAVA?
- Basis: "Automatic tuning methods for finding the τ, α hyperparameter values could be another direction for future work."
- Why unresolved: Currently τ (tolerance threshold) and α (reputation recovery speed) require empirical tuning through experimentation
- What evidence would resolve it: Development and validation of an adaptive algorithm that automatically converges to appropriate τ and α values

### Open Question 2
How can HAVA distinguish antisocial but legal/safe behaviors from genuinely social legal/safe behaviors within the DD component?
- Basis: "One limitation of HAVA is its inability to distinguish antisocial but legal / safe behaviours from social legal / safe behaviours."
- Why unresolved: The current approach treats all behaviors compliant with RB as equally discoverable
- What evidence would resolve it: Integration of frequency-based methods or other filtering mechanisms

### Open Question 3
Does HAVA scale effectively to multi-agent environments with norm conflicts between agents?
- Basis: Experiments only test single-agent scenarios while real-world deployment would involve multiple agents
- Why unresolved: Reputation mechanism and norm compliance tracking have only been validated in single-agent settings
- What evidence would resolve it: Experiments in multi-agent SUMO traffic scenarios demonstrating maintained value alignment

## Limitations
- Critical architectural details unspecified, particularly neural network architectures and state representations
- Generalization claims lack supporting evidence beyond the two specific domains tested
- Statistical validation relies on single p-value without multiple validation methods

## Confidence

- **High confidence:** The reputation-weighting mechanism and hierarchical norm separation are clearly described and experimentally validated
- **Medium confidence:** Traffic simulation results showing human-indistinguishable behavior rely on assumptions about state representation and dataset quality
- **Low confidence:** Generalization claims to broader value alignment problems lack supporting evidence

## Next Checks

1. **Architecture Sensitivity Analysis:** Replicate the traffic experiment with varied neural network architectures to assess robustness to implementation choices

2. **Cross-Environment Generalization:** Test HAVA on a third, structurally different environment (e.g., multi-agent coordination task) to evaluate whether the reputation mechanism generalizes beyond single-agent navigation

3. **Statistical Validation Expansion:** Apply additional statistical tests beyond KS (e.g., Wasserstein distance, trajectory-level metrics) to strengthen claims about human behavioral similarity