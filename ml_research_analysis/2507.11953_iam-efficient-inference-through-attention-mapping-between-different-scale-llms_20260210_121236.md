---
ver: rpa2
title: 'IAM: Efficient Inference through Attention Mapping between Different-scale
  LLMs'
arxiv_id: '2507.11953'
source_url: https://arxiv.org/abs/2507.11953
tags:
- mapping
- attention
- cache
- layers
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of resource consumption in large
  language models (LLMs), particularly with long contexts. The authors identify high
  similarity in attention matrices across different-scale LLMs within the same series
  and introduce the IAM framework to exploit this for efficient inference.
---

# IAM: Efficient Inference through Attention Mapping between Different-scale LLMs

## Quick Facts
- **arXiv ID**: 2507.11953
- **Source URL**: https://arxiv.org/abs/2507.11953
- **Reference count**: 13
- **Key outcome**: IAM accelerates LLM prefill by 15% and reduces KV cache usage by 22.1% through attention matrix mapping from small to large models

## Executive Summary
IAM introduces a novel framework for efficient LLM inference by exploiting high similarity in attention matrices across different-scale models within the same series. The method maps attention matrices from a small language model (SLM) to a larger LLM, enabling accelerated attention computation and reduced KV cache usage. Through experiments on Qwen2 models, IAM demonstrates significant efficiency gains while maintaining performance, achieving dual benefits of computational speedup and memory reduction without requiring architectural modifications to the target LLM.

## Method Summary
IAM implements a cross-scale attention matrix substitution mechanism where an SLM runs in parallel with the target LLM during inference. The method calculates attention matrices in the SLM and uses cosine similarity to map them to corresponding layers in the LLM. During prefill, the framework establishes a mapping function f(i) that determines which SLM layers best approximate which LLM layers. For mapped layers, IAM bypasses expensive QK^T calculation and skips KV storage. The mapping focuses on deeper layers (typically the last 30%) while preserving early layers for feature extraction, with the established mapping remaining static through the decode phase.

## Key Results
- Achieves 15% prefill acceleration and 22.1% KV cache reduction on Qwen2-72B using Qwen2-0.5B as proxy
- Maintains performance with only 1.8 perplexity increase and minimal accuracy drop across benchmarks
- Demonstrates generalizability across different model series and orthogonality to existing KV cache optimization techniques

## Why This Works (Mechanism)

### Mechanism 1: Cross-Scale Attention Matrix Substitution
- **Claim:** Attention matrices from an SLM can functionally replace those in an LLM within the same model series
- **Evidence:** Table 1 shows cosine similarity is most effective for minimizing perplexity; abstract reports 15% speedup and 22.1% memory reduction
- **Break condition:** Different tokenizers or architecture series (e.g., LLaMA to Qwen) cause dimension mismatches

### Mechanism 2: Depth-Based Layer Selectivity
- **Claim:** Only specific depths are viable for optimization; early layers are critical while deeper layers have redundancy
- **Evidence:** Figure 2 shows mapping first sub-block causes catastrophic performance collapse while final block maintains ~98% accuracy
- **Break condition:** Mapping ratios exceeding ~50% result in appreciable performance degradation

### Mechanism 3: Static Mapping Consistency
- **Claim:** Attention pattern relationships established during prefill persist through decode
- **Evidence:** Figure 3 reports 91.12% average consistency rate from layer 16 onward
- **Break condition:** Generated content diverging wildly from prompt topic may reduce consistency

## Foundational Learning

- **Concept: Attention Matrix Structure ($A = \text{softmax}(QK^T)$)**
  - **Why needed:** IAM relies on flattening and comparing lower-triangular parts of these matrices
  - **Quick check:** Does dimension depend on sequence length or hidden dimension? (Answer: Sequence length NÃ—N)

- **Concept: Prefill vs. Decode Stages**
  - **Why needed:** Efficiency gains split between prefill acceleration and decode memory reduction
  - **Quick check:** Which stage hurts most from quadratic complexity? (Answer: Prefill, due to long prompts)

- **Concept: Cosine Similarity**
  - **Why needed:** Selection metric measuring alignment of attention vectors independent of magnitude
  - **Quick check:** Why might Euclidean distance fail where Cosine succeeds? (Answer: Distance sensitive to scale; Cosine focuses on pattern/angle)

## Architecture Onboarding

- **Component map:** Prompt -> SLM Forward (Prefill) -> Calculate Mapping f(i) -> LLM Forward (Prefill w/ Mapping) -> Store reduced KV Cache -> LLM Forward (Decode w/ Fixed Mapping) -> Output

- **Critical path:** SLM computes attention matrices during prefill, similarity calculator builds index map, mapping engine injects A'_{f(i)} into LLM forward pass, LLM uses reduced KV cache during decode

- **Design tradeoffs:** Higher mapping ratio increases throughput but risks perplexity; larger SLM provides better fidelity but higher overhead; latency vs memory optimization balance

- **Failure signatures:** Dimension mismatch errors with different tokenizers; performance collapse from early-layer mapping; hallucinations indicating static mapping failure

- **First 3 experiments:**
  1. Replicate Table 1 comparing cosine vs Pearson similarity on attention mapping for single layer block
  2. Implement binary search over layer blocks to find maximum safe mapping ratio before MMLU accuracy degrades
  3. Measure end-to-end TTFT and KV memory allocation on 8k context with and without IAM

## Open Questions the Paper Calls Out
None

## Limitations
- Method's generalizability to other architectures remains uncertain without cross-model series testing
- Static mapping consistency may break down in highly dynamic reasoning tasks with divergent generated content
- Value cache handling for mapped layers is not explicitly addressed, creating ambiguity about potential further optimizations

## Confidence

**High Confidence Claims:**
- High similarity exists in attention matrices across different-scale LLMs within same series
- 15% prefill acceleration and 22.1% KV cache reduction are directly measured and significant
- General framework architecture is clearly specified and experimentally validated

**Medium Confidence Claims:**
- IAM is orthogonal to existing KV cache optimization techniques (supported by ablation but not exhaustively tested)
- Early layers are critical for feature extraction while deeper layers have redundancy (supported by error analysis but needs more mechanistic studies)
- 91.12% consistency rate between prefill and decode is measured but may vary with different prompts

**Low Confidence Claims:**
- Generalizability to other model series is largely speculative
- Long-term stability of static mappings across extended generation sessions is not tested
- Potential for multiplicative benefits with other techniques is proposed but not empirically validated

## Next Checks
1. **Cross-Model Series Validation**: Test IAM with SLM from different architecture family (e.g., LLaMA-7B proxy for Qwen2-72B) to verify architectural similarity requirements
2. **Dynamic Context Evaluation**: Evaluate mapping consistency on reasoning tasks with divergent generated content to stress-test static mapping assumption
3. **Multi-Technique Combination**: Implement IAM with state-of-the-art KV cache compression to verify orthogonality and measure potential multiplicative performance gains