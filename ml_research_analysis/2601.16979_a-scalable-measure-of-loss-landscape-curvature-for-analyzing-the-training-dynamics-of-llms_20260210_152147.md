---
ver: rpa2
title: A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics
  of LLMs
arxiv_id: '2601.16979'
source_url: https://arxiv.org/abs/2601.16979
tags:
- sharpness
- learning
- critical
- training
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces critical sharpness, a computationally efficient
  proxy for Hessian sharpness that measures the maximum stable learning rate along
  the current update direction. Unlike Hessian sharpness, which requires expensive
  Hessian-vector products, critical sharpness can be estimated using only forward
  passes through a line search along the update direction, making it scalable to large
  language models.
---

# A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs

## Quick Facts
- arXiv ID: 2601.16979
- Source URL: https://arxiv.org/abs/2601.16979
- Authors: Dayal Singh Kalra; Jean-Christophe Gagnon-Audet; Andrey Gromov; Ishita Mediratta; Kelvin Niu; Alexander H Miller; Michael Shvartsman
- Reference count: 40
- One-line primary result: Critical sharpness is a computationally efficient proxy for Hessian sharpness that captures progressive sharpening and Edge of Stability dynamics at LLM scale.

## Executive Summary
This paper introduces critical sharpness, a scalable measure of loss landscape curvature that requires only forward passes through a line search along the update direction, making it feasible for large language models. Unlike Hessian sharpness which requires expensive Hessian-vector products, critical sharpness can be estimated using fewer than 10 forward passes. The authors demonstrate that critical sharpness reliably captures key Hessian sharpness phenomena including progressive sharpening (continuous increase in curvature throughout training) and the Edge of Stability (oscillations around a critical threshold).

The paper provides the first empirical evidence of progressive sharpening persisting at scale for models up to 7 billion parameters, extending observations previously limited to small-scale models. Additionally, the authors introduce relative critical sharpness to quantify curvature of one loss landscape relative to another, applying it to analyze catastrophic forgetting during fine-tuning by identifying optimal pre-training data fractions that balance specialization with base capability retention.

## Method Summary
Critical sharpness is computed via a two-phase line search along the update direction Δθ. First, an exponential search doubles or halves the learning rate η until finding a bracket [η_lower, η_upper] containing the critical learning rate η_c where loss begins to increase. Then, a binary search refines this bracket to relative error < ε = 1/16 in 4 steps. The critical sharpness is λ_c = 2/η_c. For relative critical sharpness, the same procedure evaluates a different loss function (e.g., pre-training loss) while stepping along an update direction derived from another loss (e.g., fine-tuning loss). This enables measuring how much pre-training data is needed to retain base capabilities during fine-tuning.

## Key Results
- Critical sharpness reliably captures progressive sharpening and Edge of Stability dynamics at 7B parameter scale, extending small-scale observations to production LLMs
- Progressive sharpening persists throughout pre-training and mid-training stages, showing continuous curvature increase
- Relative critical sharpness identifies an optimal pre-training data fraction of ~0.6-0.7 for balancing fine-tuning specialization with base capability retention
- The method requires only 5-6 forward passes versus expensive Hessian-vector products, making it scalable to large models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critical sharpness serves as a computationally efficient proxy for Hessian sharpness at LLM scale
- Mechanism: The critical learning rate η_c is found via line search along the update direction Δθ—the smallest LR causing loss to increase. Under quadratic approximation, critical sharpness λ_c = 2/η_c equals a weighted sum of Hessian eigenvalues, bounded by λ_max^H. When gradient aligns with top eigenvector, the measures coincide.
- Core assumption: The local loss landscape is approximately quadratic along the update direction; gradient has non-trivial alignment with top Hessian eigendirections.
- Evidence anchors:
  - [abstract] "requiring fewer than 10 forward passes... captures well-documented Hessian sharpness phenomena"
  - [Section 2.3, Result 2.1] Directional sharpness λ_dir = Σc_i²λ_i^H / Σc_i² ≤ λ_max^H
  - [corpus] Related work on curvature metrics exists (Rényi Sharpness, input-space sharpness), but direct validation of critical sharpness as Hessian proxy is limited to this paper's experiments.
- Break condition: Highly non-quadratic loss regions; very weak gradient-top-eigenvector alignment; extremely noisy gradient estimates.

### Mechanism 2
- Claim: Progressive sharpening and Edge of Stability dynamics persist at 7B parameter scale
- Mechanism: Sharpness increases continuously during training as LR schedule (warmup-stable-decay) modulates the stability threshold 2/η. During decay phase, threshold rises, causing observed sharpness to climb. Self-stabilization keeps sharpness oscillating near threshold rather than diverging.
- Core assumption: Edge of Stability dynamics generalize from small-scale (~10M params) studies to production-scale LLM training.
- Evidence anchors:
  - [abstract] "first empirical evidence of progressive sharpening persisting at scale for models up to 7 billion parameters"
  - [Figure 5] Critical sharpness curves for OLMo-2 7B show continuous increase through pre-training and mid-training
  - [corpus] Cohen et al. (2021) documented EoS at small scale; this paper extends empirically but mechanism remains correlational.
- Break condition: Fundamentally different LR schedules; alternative optimizers with different stability properties; extreme batch size variations.

### Mechanism 3
- Claim: Relative critical sharpness identifies optimal pre-training data fraction (~0.6-0.7) for balancing fine-tuning specialization with base capability retention
- Mechanism: Relative critical sharpness λ_c^(1→2) measures curvature of pre-training loss L_1 along fine-tuning update direction from L_2. Lower values indicate alignment—staying in "pre-training basin." At the intersection point of sharpness curves for different tasks, no single task constrains the maximum stable learning rate.
- Core assumption: "Basin retention" (η < 2/λ_c^(1→2)) preserves generic capabilities; leaving basin enables specialization but risks forgetting.
- Evidence anchors:
  - [Section 4] "sweet spot at a DCLM ratio of 0.6" where sharpness curves intersect
  - [Figure 6b,c] GSM8K improves outside basin; MMLU preserved inside basin; optimal balance near 0.6-0.7 ratio
  - [corpus] Catastrophic forgetting literature (rehearsal methods) supports mixing pre-training data, but sharpness-based guidance is novel and not independently validated.
- Break condition: When fine-tuning and pre-training objectives are fundamentally incompatible; when task distributions differ drastically.

## Foundational Learning

- Concept: **Hessian eigenvalues and curvature**
  - Why needed here: The entire framework builds on understanding how λ_max^H quantifies "sharpness" of loss minima and relates to training stability.
  - Quick check question: Can you explain why large Hessian eigenvalues correspond to "sharp" directions in parameter space?

- Concept: **Learning rate stability threshold**
  - Why needed here: Critical sharpness inverts the stability condition—if η > 2/λ, loss increases. Understanding this relationship is essential.
  - Quick check question: For a loss landscape with maximum curvature λ_max = 100, what learning rate would cause instability in gradient descent?

- Concept: **Line search methods**
  - Why needed here: The efficiency claim rests on exponential + binary search requiring only 5-6 forward passes.
  - Quick check question: Why does binary search converge in log(1/ε) iterations, and what tolerance does the paper use?

## Architecture Onboarding

- Component map:
  - Exponential search -> Binary search -> Critical sharpness computation -> Relative variant (optional)

- Critical path:
  1. Obtain update direction Δθ from current optimizer state
  2. Run exponential search (typically 1-2 iterations after warmup)
  3. Run binary search (4 iterations with ε = 1/16)
  4. Compute λ_c = 2/η_c
  5. For relative version, swap loss function for forward pass evaluation only

- Design tradeoffs:
  - **Tolerance vs. cost**: ε = 1/16 gives ~6% error in η_c estimate; tightening to ε = 1/64 requires 2 more forward passes
  - **Initial guess quality**: First measurement depends on η_0; subsequent measurements reuse previous η_c as warm start
  - **Batch size for estimation**: Paper uses 60× smaller batches than training for tractability; this affects EoS threshold but not progressive sharpening observation

- Failure signatures:
  - **Line search doesn't converge**: Check if loss is flat (gradient ≈ 0) or if η_0 is extremely far from η_c
  - **Sharpness spikes erratically**: May indicate training instability or loss landscape discontinuity
  - **Relative sharpness undefined**: L_1 may be increasing along Δθ even at infinitesimal step sizes

- First 3 experiments:
  1. **Validate against Hessian on small model**: Train MLP on CIFAR-10, compare λ_c vs λ_max^H throughout training to reproduce Figure 3 dynamics
  2. **Measure on your own checkpoints**: Apply to existing model checkpoints (any scale) to observe progressive sharpening pattern
  3. **Data mixing ablation**: For a fine-tuning setup, compute relative critical sharpness across pre-training data fractions to identify the intersection point before running full training

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Does the optimal pre-training data fraction (0.6–0.7) identified via relative critical sharpness generalize across different fine-tuning domains beyond math reasoning?
  - Basis in paper: [explicit] The authors state "We leave the validation of this prediction through downstream evaluation to future work" regarding their prediction of 0.6 DCLM ratio for the full Dolmino mix.
  - Why unresolved: The experiments validate the sweet spot primarily on math (GSM8K) vs. general reasoning (MMLU); other domains (code, instruction-following, long-context) remain untested.
  - What evidence would resolve it: Systematic evaluation across diverse fine-tuning domains with varying pre-training data fractions, comparing predicted vs. actual downstream performance.

- **Open Question 2**
  - Question: What are the practical consequences of the gap between critical sharpness and Hessian sharpness when the gradient is not aligned with the top eigenvector?
  - Basis in paper: [inferred] Result 2.1 establishes that critical sharpness equals Hessian sharpness only when the gradient aligns with the top eigenvector, and Figure 3 shows measurable deviations during training.
  - Why unresolved: The paper demonstrates the gap exists but does not analyze when it becomes large enough to affect diagnostic utility or training decisions.
  - What evidence would resolve it: Systematic study of gradient-top-eigenvector alignment throughout training and correlation with prediction errors in learning rate guidance.

- **Open Question 3**
  - Question: Can relative critical sharpness be extended to analyze training dynamics under distribution shifts, loss function changes (e.g., RL objectives), or multi-stage training beyond pre-training to fine-tuning?
  - Basis in paper: [explicit] The conclusion states relative critical sharpness "provides a general framework for analyzing changes in the loss landscape due to distribution shifts, changes in loss functions, or modifications to the training data mixture."
  - Why unresolved: The paper only applies relative critical sharpness to the pre-training to fine-tuning transition with next-token prediction losses.
  - What evidence would resolve it: Application of relative critical sharpness to RL fine-tuning (e.g., RLHF), domain adaptation scenarios, or multi-task curriculum learning.

## Limitations

- The critical sharpness measure's accuracy depends critically on the quadratic approximation of the loss landscape along the update direction, which has not been rigorously validated for deep networks beyond the MLP case study.
- The method requires careful initialization of the line search (η₀) and depends on stable gradient estimates from AdamW momentum accumulation, making it sensitive to hyperparameter choices.
- The optimal data mixing ratio (0.6-0.7) finding is based on limited experiments with specific model architectures and datasets, lacking theoretical guarantees for broader applicability.

## Confidence

- **High confidence**: The computational efficiency claims (O(5-6) forward passes) and scalability to 7B parameters are well-supported by the experimental results and algorithmic design.
- **Medium confidence**: The claim that critical sharpness captures Hessian sharpness phenomena (progressive sharpening and Edge of Stability) is empirically demonstrated but relies on correlation rather than direct equivalence proof. The quadratic approximation assumption needs further validation.
- **Medium confidence**: The optimal data mixing ratio (0.6-0.7) finding is based on limited experiments with specific model architectures and datasets. The sharpness-based interpretation of catastrophic forgetting provides useful intuition but lacks theoretical guarantees.

## Next Checks

1. **Direct Hessian comparison**: Compute and compare λ_c versus λ_max^H across training for multiple model architectures (MLP, CNN, transformer) to quantify the approximation error and identify conditions where the quadratic assumption breaks.

2. **Initialization sensitivity analysis**: Systematically vary η₀ and momentum accumulation periods to determine the robustness of critical sharpness estimates to hyperparameter choices, particularly for models with noisy gradients.

3. **Generalization to other optimizers**: Test critical sharpness with different optimizers (SGD with momentum, Adam, Lion) and learning rate schedules to verify whether the progressive sharpening and Edge of Stability phenomena persist across optimization algorithms.