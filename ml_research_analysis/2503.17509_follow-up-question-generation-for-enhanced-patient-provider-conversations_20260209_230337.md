---
ver: rpa2
title: Follow-up Question Generation For Enhanced Patient-Provider Conversations
arxiv_id: '2503.17509'
source_url: https://arxiv.org/abs/2503.17509
tags:
- questions
- patient
- question
- follow-up
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FollowupQ, a multi-agent framework for generating\
  \ follow-up questions in asynchronous patient-provider conversations. The core method\
  \ employs specialized agents to emulate clinical thought processes\u2014extracting\
  \ relevant EHR information, performing differential diagnosis, and clarifying patient-reported\
  \ symptoms."
---

# Follow-up Question Generation For Enhanced Patient-Provider Conversations

## Quick Facts
- arXiv ID: 2503.17509
- Source URL: https://arxiv.org/abs/2503.17509
- Reference count: 30
- Key outcome: FollowupQ achieves 17% improvement in RIM score over baseline LLMs on real patient data

## Executive Summary
This paper introduces FollowupQ, a multi-agent framework for generating follow-up questions in asynchronous patient-provider conversations. The system employs specialized agents to emulate clinical thought processes by extracting relevant EHR information, performing differential diagnosis, and clarifying patient-reported symptoms. Evaluated on a novel dataset containing 150 real and 250 synthetic patient messages with linked EHR data and over 2,300 provider-written follow-up questions, FollowupQ demonstrates significant improvements in follow-up question generation quality.

## Method Summary
FollowupQ is a multi-agent framework that generates follow-up questions for asynchronous patient-provider conversations. The system consists of specialized agents that emulate clinical thought processes, including extracting relevant EHR information, performing differential diagnosis, and clarifying patient-reported symptoms. The framework was evaluated on a novel dataset containing 150 real and 250 synthetic patient messages with linked EHR data and over 2,300 provider-written follow-up questions.

## Key Results
- 17% improvement in RIM score over baseline LLMs on real patient data
- 5% improvement in RIM score on synthetic data
- 34% reduction in provider follow-up communications by ensuring all necessary diagnostic questions are asked upfront

## Why This Works (Mechanism)
The FollowupQ framework leverages multi-agent collaboration to systematically address the clinical reasoning process. By separating concerns into specialized agents for EHR extraction, differential diagnosis, and symptom clarification, the system can more effectively identify gaps in patient information and generate targeted follow-up questions. This structured approach mirrors how clinicians naturally process patient information and formulate questions during care delivery.

## Foundational Learning
- Clinical Reasoning: The systematic process of gathering information, forming hypotheses, and testing them through targeted questions
  - Why needed: Enables structured approach to patient information gathering
  - Quick check: Can the system identify relevant symptoms and prioritize questions appropriately?
- EHR Integration: Connecting patient history, lab results, and previous encounters to current clinical context
  - Why needed: Provides comprehensive patient context for informed question generation
  - Quick check: Does the system accurately extract and synthesize relevant EHR data?
- Differential Diagnosis: The process of distinguishing between possible conditions based on presenting symptoms
  - Why needed: Guides question generation toward identifying the most likely conditions
  - Quick check: Are generated questions focused on differentiating between plausible diagnoses?

## Architecture Onboarding

Component Map:
Patient Message -> EHR Extraction Agent -> Differential Diagnosis Agent -> Symptom Clarification Agent -> Follow-up Question Generator

Critical Path:
1. Patient message analysis
2. EHR data retrieval and extraction
3. Clinical reasoning through differential diagnosis
4. Symptom clarification assessment
5. Follow-up question generation

Design Tradeoffs:
- Specialized agents vs. monolithic model: Specialized agents allow for more focused clinical reasoning but increase system complexity
- Real-time vs. batch processing: Real-time generation enables immediate patient care but requires efficient inference
- Question quality vs. generation speed: More thorough clinical reasoning produces better questions but takes longer

Failure Signatures:
- Incorrect EHR extraction leading to irrelevant questions
- Over-reliance on common diagnoses missing rare conditions
- Generating questions that are too generic or too specific
- Missing critical follow-up questions due to incomplete clinical reasoning

First 3 Experiments:
1. Compare RIM scores between FollowupQ and baseline LLMs on the real dataset
2. Evaluate reduction in provider follow-up communications in simulated clinical scenarios
3. Test the system's ability to generate questions across different medical specialties

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation based on a relatively small set of 150 real patient messages, raising questions about statistical significance
- Reliance on automated metrics (RIM scores) that may not fully capture clinical utility or appropriateness
- System performance heavily dependent on quality and completeness of EHR data across healthcare systems

## Confidence
- Confidence in core methodological approach: Medium
- Confidence in clinical impact claims: Low
- Confidence in evaluation metrics' clinical relevance: Medium

## Next Checks
1. Deploy FollowupQ in a live clinical setting with diverse patient populations to measure actual impact on provider workload and patient outcomes
2. Conduct a randomized controlled trial comparing FollowupQ-generated questions against provider-written questions for clinical accuracy and patient satisfaction
3. Perform extensive error analysis on generated questions to identify potential safety concerns or clinical reasoning gaps