---
ver: rpa2
title: Bidirectional predictive coding
arxiv_id: '2505.23415'
source_url: https://arxiv.org/abs/2505.23415
tags:
- learning
- inference
- generative
- discriminative
- discpc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bidirectional predictive coding (bPC) is a biologically plausible
  model of visual inference that integrates generative and discriminative processing,
  enabling the brain to both classify and reconstruct visual inputs. Unlike previous
  predictive coding models, which specialize in either generative or discriminative
  tasks, bPC achieves performance on par with state-of-the-art unidirectional models
  in both domains.
---

# Bidirectional predictive coding

## Quick Facts
- arXiv ID: 2505.23415
- Source URL: https://arxiv.org/abs/2505.23415
- Reference count: 40
- One-line primary result: Biologically plausible model integrating generative and discriminative processing, achieving state-of-the-art performance on visual tasks.

## Executive Summary
Bidirectional Predictive Coding (bPC) is a biologically plausible model of visual inference that integrates generative and discriminative processing, enabling the brain to both classify and reconstruct visual inputs. Unlike previous predictive coding models, which specialize in either generative or discriminative tasks, bPC achieves performance on par with state-of-the-art unidirectional models in both domains. It does so by developing an energy landscape optimized for both classification and generation, avoiding the overconfidence or bias present in traditional models.

## Method Summary
bPC is an energy-based model that minimizes a combined generative and discriminative energy function through iterative inference. The model uses separate error populations for top-down (generative) and bottom-up (discriminative) predictions, with shared latent representations. During training, activities are initialized via a fast feedforward sweep and then refined through gradient descent on the energy landscape. Weights are updated using local Hebbian-like rules. The model is evaluated on MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet using both MLP and VGG-style architectures.

## Key Results
- bPC achieves classification accuracy matching state-of-the-art discriminative models while simultaneously enabling high-quality generation
- The model successfully handles missing data by reconstructing occluded inputs through top-down priors
- Energy landscape visualization shows bPC creates sharp, class-specific minima compared to broad valleys in unidirectional models

## Why This Works (Mechanism)

### Mechanism 1
bPC creates a superior energy landscape for visual tasks by forcing a shared latent representation to satisfy both top-down (generative) and bottom-up (discriminative) constraints simultaneously. The bottom-up predictions restrict the space to valid class boundaries, while top-down predictions anchor these boundaries to actual data points, resulting in "sharp, class-specific minima" centered on valid inputs.

### Mechanism 2
bPC achieves computational efficiency comparable to feedforward networks by treating the bottom-up stream as an initialization for "amortized inference." Before iterative dynamics begin, network activities are initialized via a single fast pass through the bottom-up weights, projecting the input close to the final equilibrium state. The subsequent iterative refinement only needs to fine-tune this state rather than searching from scratch.

### Mechanism 3
bPC enables robust inference with missing sensory data because the generative pathway actively reconstructs missing pixels before classification is finalized. When input pixels are missing, the top-down prediction generates a "hallucination" of the missing input based on the inferred latent state, which is then used by the discriminative pathway to refine the classification.

## Foundational Learning

- **Predictive Coding Energy Minimization**: bPC relies on gradient descent on a global energy function rather than standard backpropagation. Inference is an iterative loop of reducing prediction errors.
  - *Quick check*: In bPC, does the network settle when the error is maximized or minimized?

- **Hebbian Plasticity & Local Learning Rules**: Biological plausibility rests on weight updates depending only on local pre- and post-synaptic activity, avoiding global credit assignment.
  - *Quick check*: Why is the weight update considered "local" compared to backpropagation?

- **Amortized Inference**: Explains why bPC isn't prohibitively slow. Distinguish between the fast initial pass (amortized) and the slow refinement pass (iterative).
  - *Quick check*: If you remove the bottom-up initialization and set all neurons to zero, what happens to the convergence speed?

## Architecture Onboarding

- **Component map**: Input $x_1$ → Hidden layers $x_2, x_3$ → Label $x_L$; Error neurons $\epsilon_{gen}, \epsilon_{disc}$ for each direction; Weights $W$ (top-down generative) and $V$ (bottom-up discriminative)

- **Critical path**:
  1. Clamp $x_1$ (input) and $x_L$ (label, if supervised). Set other $x$ via feedforward sweep through $V$
  2. Iterate activity updates using gradient descent on Energy $E$ for $T=8$ steps
  3. Update weights $W$ and $V$ using local learning rules based on settled errors

- **Design tradeoffs**:
  - Uses fewer value neurons (shared layers) but requires more error neurons than separate unidirectional models
  - Max-pooling in $V$ improves accuracy but introduces artifacts in $W$ reconstruction; stride-2 convolutions are potential alternative

- **Failure signatures**:
  - Noisy generations if $\alpha_{disc}$ is too high relative to $\alpha_{gen}$
  - Slow convergence if feedforward initialization is skipped
  - Collapse into purely discriminative mode if $\alpha_{gen}$ is too weak

- **First 3 experiments**:
  1. Train on XOR and plot energy landscape to verify sharp minima at correct coordinates
  2. Train on MNIST, mask 50% of pixels, and run inference to verify missing pixel reconstruction
  3. Vary $\alpha_{disc}$ vs $\alpha_{gen}$ on MNIST to observe transition from generative collapse to discriminative overconfidence

## Open Questions the Paper Calls Out

- Can the energy scaling factors ($\alpha_{gen}$ and $\alpha_{disc}$) be learned autonomously as adaptive precision parameters rather than manually tuned?
- Can bPC architectures be refined to eliminate generative artifacts without sacrificing discriminative accuracy?
- How does bPC compare to unidirectional and hybrid models in strictly time-constrained inference scenarios?

## Limitations

- Energy function scaling factors require manual tuning and may not generalize across datasets
- Deep architecture scalability and stability with shared latents are not fully characterized
- Biological plausibility claims are not supported by neural data or detailed brain circuit models

## Confidence

- **High Confidence**: Core integration claim supported by XOR landscape visualization and MNIST occlusion experiments
- **Medium Confidence**: CIFAR/Tiny-ImageNet performance claims, but comparison limited to other predictive coding models
- **Low Confidence**: Strong claims about modeling "flexible visual inference in the brain" lack neural data support

## Next Checks

1. Systematically vary $\alpha_{gen}$ and $\alpha_{disc}$ on MNIST and CIFAR to map performance landscape and identify failure modes
2. Train separate genPC and discPC models and compare their combined performance to bPC to test shared latent benefit
3. Implement bPC on deeper network (e.g., ResNet-18) to monitor training stability and test if 8-step inference is sufficient