---
ver: rpa2
title: Ontology-Aligned Embeddings for Data-Driven Labour Market Analytics
arxiv_id: '2509.04942'
source_url: https://arxiv.org/abs/2509.04942
tags:
- kldb
- data
- level
- titles
- isced
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a lightweight embedding-based approach to\
  \ align free-form German job titles with established occupational (KldB 2010) and\
  \ educational (ISCED 2011) taxonomies. By fine-tuning Sentence-BERT on contrastive\
  \ triplets derived from the Bundesagentur f\xFCr Arbeit\u2019s dataset, the method\
  \ learns semantic embeddings that capture job title semantics and enable fast, explainable\
  \ classification via approximate k-NN search."
---

# Ontology-Aligned Embeddings for Data-Driven Labour Market Analytics

## Quick Facts
- **arXiv ID**: 2509.04942
- **Source URL**: https://arxiv.org/abs/2509.04942
- **Reference count**: 40
- **Primary result**: Embedding-based method aligns German job titles with KldB 2010/ISCED 2011 taxonomies with 0.976 macro-accuracy at occupational area level

## Executive Summary
This paper presents a lightweight embedding-based approach to align free-form German job titles with established occupational (KldB 2010) and educational (ISCED 2011) taxonomies. By fine-tuning Sentence-BERT on contrastive triplets derived from Bundesagentur für Arbeit data, the method learns semantic embeddings that capture job title semantics and enable fast, explainable classification via approximate k-NN search. The approach achieves high accuracy (0.976 at occupational area level, 0.961 at requirement level) while supporting real-time applications in labor market analytics.

## Method Summary
The method fine-tunes German Sentence-BERT on 1.5M contrastive triplets constructed from Bundesagentur für Arbeit's dataset, where positives share KldB codes and negatives differ. Queries are structured with special separator tokens for job title, qualification, and skills. The model is trained with Multiple Negatives Ranking Loss plus Matryoshka Loss to enable flexible embedding dimensions. Inference uses HNSW approximate k-NN search with majority voting. ISCED 2011 levels are mapped via rule-based heuristics from KldB codes, qualifications, and keywords.

## Key Results
- Achieves 0.976 macro-accuracy at KldB occupational area level
- Maintains 0.961 accuracy at requirement-level (5th digit) classification
- Shows robust performance under textual perturbations (word order, spelling variants, abbreviations)
- Supports real-time inference with sub-second response times via HNSW indexing

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Triplet Learning on Taxonomic Structure
Fine-tuning Sentence-BERT on anchor-positive-negative triplets derived from KldB codes produces embeddings where semantically similar occupations cluster by their taxonomic membership. The training pushes semantically related job titles closer in embedding space while separating unrelated ones. The model learns to associate lexical variants with the same occupational category. Core assumption: KldB 2010 meaningfully groups occupations by shared tasks, skills, and complexity.

### Mechanism 2: k-NN Semantic Search as Classification
Framing classification as approximate nearest-neighbor search in embedding space enables sub-second inference with explainable results. After training, all canonical job titles with known KldB/ISCED codes are embedded and indexed. At inference, a query embedding is computed and HNSW graphs enable logarithmic-time retrieval of the k most similar indexed titles. Majority vote over k-nearest neighbors determines predicted codes.

### Mechanism 3: Structured Query Encoding with Special Tokens
Using special separator tokens to structure queries (job title, qualification, skills) encourages the model to learn distinguishable embedding subspaces for each information type. The query format `[JOB_TITLE_SEP] {title} [QUALIFICATION_SEP] {qual} [SKILL_SEP] {skills}` provides explicit structure. Matryoshka Loss further encourages the model to learn meaningful representations across different embedding dimensions (64–1024).

## Foundational Learning

- **Contrastive Learning and Triplet Loss**
  - Why needed here: The entire approach depends on understanding how anchor-positive-negative sampling shapes embedding geometry.
  - Quick check question: Given three items (A, P, N), what should happen to their distances during contrastive training?

- **Sentence-BERT and Siamese Networks**
  - Why needed here: The base model is a pre-trained Sentence-BERT; understanding how it produces fixed-size sentence embeddings is essential for debugging and extension.
  - Quick check question: How does a siamese BERT network produce comparable embeddings for different-length input sequences?

- **Approximate Nearest Neighbor Search (HNSW)**
  - Why needed here: Inference speed depends on HNSW indexing. Understanding its parameters affects latency-accuracy tradeoffs.
  - Quick check question: Why does HNSW achieve logarithmic search time at the cost of approximate (not exact) results?

## Architecture Onboarding

- **Component map**: BERUFENET.API → raw job titles, KldB codes, skills, qualifications → Triplet construction → Model fine-tuning → Index building → Inference service

- **Critical path**: Triplet quality → Embedding dimension choice → HNSW index parameters

- **Design tradeoffs**: Higher dimensions (1024) capture more nuance; lower dimensions (64) are faster and smaller. Matryoshka allows runtime choice. Higher k smooths predictions but may dilute precision for fine-grained codes.

- **Failure signatures**: High-confidence wrong predictions suggest triplet noise. Poor performance on rare job titles indicates insufficient positive samples. Gender/variant sensitivity suggests training data balance issues.

- **First 3 experiments**:
  1. Baseline retrieval sanity check: Embed 100 held-out job titles, retrieve k=5 neighbors, manually inspect semantic plausibility.
  2. Dimension sweep: Evaluate macro-F1 at dimensions 64, 128, 256, 512, 1024 to identify knee point.
  3. k-value tuning: Sweep k=1 to k=11 on validation set; plot accuracy vs. k to verify optimal value.

## Open Questions the Paper Calls Out

1. Can a two-step prediction process—first inferring ISCED 2011 levels to constrain the search space—improve the accuracy of KldB 2010 predictions for ambiguous job titles?

2. Does incorporating the KldB 2010 hierarchy directly into the training loss function significantly improve performance at the granular 5-digit "Type" level?

3. Can a learned mapping component replace the current rule-based heuristic for aligning KldB codes to ISCED levels without sacrificing alignment fidelity?

4. How robust is the model's classification accuracy when evaluated against "truly open-ended" free-form job titles found in administrative data, as opposed to the current dataset of indexed keywords?

## Limitations

- Training data access requires BERUFENET.API, which may have access restrictions
- Current evaluation relies on cleaner indexed keywords rather than naturally occurring noisy job titles
- Accuracy degrades significantly at granular classification levels (0.886 at Type level vs. 0.976 at Area level)
- No cross-lingual validation or performance data on non-German job titles

## Confidence

- **High confidence**: Core contrastive learning mechanism and k-NN classification approach are well-established with sufficient implementation detail
- **Medium confidence**: Ablation studies show robustness to perturbations, but lack of real-world noise testing creates uncertainty about production performance
- **Low confidence**: Incomplete ISCED mapping rules and unknown training hyperparameters require significant assumptions for reproduction

## Next Checks

1. Triplet construction sanity check: Build a small test set of 100 manually verified triplets using BERUFENET.API to verify contrastive loss behavior before full-scale training.

2. Production noise evaluation: Collect 50 naturally occurring job titles from job boards with spelling errors and abbreviations; compare model performance against clean test set.

3. Dimensionality vs. accuracy tradeoff: Systematically evaluate model at dimensions 64, 128, 256, 512, and 1024 using the same test set; plot macro-F1 vs. dimension to identify optimal production configuration.