---
ver: rpa2
title: 'An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and
  DeepSeek-V3'
arxiv_id: '2506.00312'
source_url: https://arxiv.org/abs/2506.00312
tags:
- reviews
- movie
- review
- llms
- imdb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models (GPT-4o, Gemini-2.0, and DeepSeek-V3) were
  used to generate movie reviews based on subtitles and screenplays, and the outputs
  were compared to IMDb reviews using sentiment, emotion, and semantic similarity
  analysis. The models could generate coherent reviews but differed in emotional tone:
  GPT-4o leaned positive, Gemini-2.0 amplified negative emotions, and DeepSeek-V3
  was more balanced.'
---

# An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3

## Quick Facts
- **arXiv ID:** 2506.00312
- **Source URL:** https://arxiv.org/abs/2506.00312
- **Reference count:** 40
- **Primary result:** LLM-generated movie reviews can mimic human reviews in style and structure, but exhibit model-specific emotional biases and weaker emotional depth.

## Executive Summary
This paper evaluates the ability of three large language models—GPT-4o, Gemini-2.0, and DeepSeek-V3—to generate movie reviews based on subtitles and screenplays, comparing their outputs to IMDb reviews using sentiment, emotion, and semantic similarity analysis. The models can produce coherent reviews, but differ in emotional tone: GPT-4o leans positive, Gemini-2.0 amplifies negative emotions, and DeepSeek-V3 is more balanced. Screenplay inputs produce more stable outputs than subtitles. A human survey found LLM-generated reviews often hard to distinguish from IMDb reviews, though emotional depth and stylistic coherence remain weaker.

## Method Summary
The study uses six Oscar-related movies and their corresponding subtitles and screenplays to generate 15 reviews per movie per LLM (5 personas x 3 sentiments) using GPT-4o, Gemini-2.0, and DeepSeek-V3 APIs. Reviews are compared to IMDb reviews using RoBERTa for sentiment polarity, DistilRoBERTa for emotion classification (joy, sadness, anger, fear, surprise, disgust, neutral), and TF-IDF cosine similarity. A human survey (n=50) assesses distinguishability between LLM and IMDb reviews.

## Key Results
- GPT-4o's reviews were closest to IMDb reviews in style and structure.
- Screenplay inputs produced more stable outputs than subtitles.
- LLM-generated reviews were often hard to distinguish from IMDb reviews in a human survey.
- GPT-4o showed positive emotional bias, Gemini-2.0 amplified negative emotions, and DeepSeek-V3 was more balanced.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Input context richness (screenplays vs. subtitles) conditions the emotional stability of generated reviews.
- **Mechanism:** Screenplays provide scene-level context and narrative progression, allowing models to ground emotional arcs. Subtitles provide fragmented dialogue without situational cues, leading models to hallucinate or amplify emotional intensity to fill gaps.
- **Core assumption:** The emotion classification model (DistilRoBERTa) accurately reflects the human-perceived emotional volatility of the text.
- **Evidence anchors:** [abstract] "screenplay inputs produced more stable outputs than subtitles"; [Page 12, Section 4.3] "generating movie reviews based on subtitles results in more extreme and fluctuating expressions of emotions compared to screenplays".

### Mechanism 2
- **Claim:** Model-specific alignment (RLHF/safety tuning) creates distinct "emotional priors" independent of the prompt.
- **Mechanism:** GPT-4o appears optimized for safe/appreciative tones (positive bias), while Gemini-2.0 allows or amplifies negative/critical signals (disgust/sadness bias). DeepSeek-V3, being a Mixture-of-Experts (MoE), routes tokens in a way that appears to balance these extremes.
- **Core assumption:** The observed biases are artifacts of the model's training/alignment rather than random variance in the small sample size (6 movies).
- **Evidence anchors:** [abstract] "GPT-4o leaned positive, Gemini-2.0 amplified negative emotions, and DeepSeek-V3 was more balanced"; [Page 7, Table 5] Shows GPT-4o Joy score at 0.376 vs Gemini-2.0 Disgust at 0.306.

### Mechanism 3
- **Claim:** Detailed persona prompts act as a variance-reduction mechanism, trading peak performance for output consistency.
- **Mechanism:** Detailed prompts (specifying age, personality, tone) constrain the token selection probability space, preventing the model from defaulting to generic "hallucinated" review templates.
- **Core assumption:** Cosine similarity to IMDb reviews is a valid proxy for "quality" or "human-likeness."
- **Evidence anchors:** [Page 13, Section 4.4] "reviews generated with detailed persona prompts display a more concentrated distribution and a smaller interquartile range"; [Page 22, Appendix A] Shows examples of detailed prompts used to enforce specific styles.

## Foundational Learning

- **Concept: N-gram Analysis (specifically Trigrams)**
  - **Why needed here:** Used to detect repetitive phrasing or "template-like" language that distinguishes LLM output from human writing (e.g., "tiger hidden dragon" vs. specific actor names).
  - **Quick check question:** Why would a trigram analysis flag an LLM-generated review as "artificial" even if the grammar is perfect?

- **Concept: Semantic vs. Lexical Similarity**
  - **Why needed here:** The paper uses TF-IDF and Cosine Similarity (lexical) rather than deep semantic embeddings. This measures structural/vocabulary overlap, not necessarily shared meaning.
  - **Quick check question:** If a generated review summarizes a movie accurately but uses completely different vocabulary, would Cosine Similarity (TF-IDF) return a high or low score?

- **Concept: Persona Prompting**
  - **Why needed here:** The core technique for steering model tone. Understanding how specific attributes (e.g., "right-wing extremist" vs. "professional critic") change the output distribution is essential.
  - **Quick check question:** What is the trade-off between a simple persona ("You are a critic") and a detailed persona ("You are a 60-year-old cynical critic...") in terms of output variance?

## Architecture Onboarding

- **Component map:** PDF Screenplays (OCR'd) & Subtitle files (cleaned) -> LLM APIs (GPT-4o, Gemini-2.0, DeepSeek-V3) -> RoBERTa (Sentiment), DistilRoBERTa (Emotion), TF-IDF (Similarity) -> Human survey (n=50)
- **Critical path:** Data Cleaning (OCR errors in screenplays and timestamps in subtitles must be rigorously removed) -> Prompt Engineering (Detailed prompts are the primary lever for controlling variance) -> Model Generation -> Evaluation (RoBERTa/DistilRoBERTa/TF-IDF)
- **Design tradeoffs:**
  - **Input Depth:** Subtitles are easier to process but yield volatile results. Screenplays are harder to clean (PDFs) but yield stable/nuanced results.
  - **Metrics:** The paper relies heavily on lexical overlap (Cosine/TF-IDF). This favors reviews that use common movie clichés over truly novel insights.
- **Failure signatures:**
  - **Content Moderation Interference:** Models refusing to generate reviews for "right-wing" or "troll" personas, or softening negative critiques (noted for GPT-4o).
  - **Template Collapse:** Gemini (detailed) producing phrases like "okay buckle buttercups" repetitively across different movies (Page 8, Table 4).
- **First 3 experiments:**
  1. **Input Sensitivity Test:** Run the same movie review generation using raw subtitles vs. cleaned screenplays for a single movie and compare the "Joy" and "Disgust" score distributions (expect higher variance in subtitles).
  2. **Persona Stability Test:** Generate 10 reviews for one movie using Gemini-2.0 with a "Simple" persona and 10 with a "Detailed" persona. Calculate the standard deviation of the Cosine Similarity scores (expect Detailed to have lower std dev).
  3. **Model Bias Probe:** Prompt all three models with a strictly "Neutral" prompt for a highly polarizing movie. Check if GPT-4o drifts positive and Gemini drifts negative using the RoBERTa polarity classifier.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the integration of multimodal inputs (video and audio) improve the ability of LLMs to capture emotional atmosphere and cinematographic style compared to text-only screenplay inputs?
  - **Basis in paper:** [explicit] The authors state that relying solely on textual inputs is a limitation and that future research should integrate video clips and audio to help LLMs better interpret elements like "anxiety induced by rapid camera cuts, emotionally charged musical cues, or symbolic colour schemes."
  - **Why unresolved:** The current study restricted its methodology to textual data (subtitles and screenplays), lacking the implementation of visual or auditory processing pipelines.
  - **What evidence would resolve it:** A comparative study evaluating reviews generated from multimodal inputs versus text-only inputs, specifically measuring accuracy in describing visual moods and non-dialogue audio cues.

- **Open Question 2:** To what extent do content moderation and safety mechanisms in models like GPT-4o and Gemini degrade the authenticity and critical depth of negative reviews?
  - **Basis in paper:** [explicit] The paper notes in the Discussion that "all three models enforced content moderation mechanisms" which often produced "less authentic negative expressions" or caused models to soften critiques, affecting realism.
  - **Why unresolved:** The study utilized standard API versions of the models without bypassing or controlling for internal safety filters, making it impossible to isolate the effect of the model's architecture from its safety alignment.
  - **What evidence would resolve it:** An ablation study comparing the sentiment intensity and lexical diversity of negative reviews generated by standard models versus "unfiltered" or locally deployed versions of similar architectures.

- **Open Question 3:** Does the quality of LLM-generated reviews decline significantly when applied to lesser-known, commercial, or non-English films outside the Oscar-winning dataset?
  - **Basis in paper:** [explicit] The authors acknowledge that the selected movies were Oscar winners with abundant data, and "had we used commercial movies, lesser-known titles, or non-English movies... the models’ performance might have significantly declined."
  - **Why unresolved:** The experimental scope was limited to six specific, high-profile movies, introducing a selection bias toward films with strong narrative structures and extensive existing reviews.
  - **What evidence would resolve it:** Replicating the evaluation framework on a dataset of low-box-office, independent, or culturally niche films to test for performance degradation.

## Limitations
- The analysis is based on only six critically acclaimed movies, which may not represent the full diversity of film genres or review styles.
- The paper does not specify the exact prompt templates used for the "standard" personas, only the detailed versions for Gemini (detailed).
- The use of TF-IDF cosine similarity as a proxy for review quality and human-likeness is a significant limitation, as it favors common movie clichés over novel insights.

## Confidence
- **High Confidence:** GPT-4o's reviews were most similar to IMDb reviews in style and structure; Screenplays as input produce more stable and coherent reviews than subtitles; LLM-generated reviews are often hard to distinguish from IMDb reviews in a human survey.
- **Medium Confidence:** GPT-4o has a positive emotional bias, Gemini-2.0 has a negative bias, and DeepSeek-V3 is more balanced; Detailed persona prompts reduce the variance in review outputs; The sentiment and emotion classification models (RoBERTa and DistilRoBERTa) accurately reflect the perceived emotional tone.
- **Low Confidence:** The observed model-specific emotional biases are solely due to training/alignment rather than random variance; The emotional depth and stylistic coherence of LLM-generated reviews are definitively weaker than human reviews.

## Next Checks
1. **Model Bias Probe:** Prompt all three models with a strictly "Neutral" sentiment for a highly polarizing movie (e.g., "Titanic"). Use the RoBERTa sentiment classifier to check if GPT-4o drifts positive and Gemini-2.0 drifts negative, validating the claimed emotional priors.
2. **Input Sensitivity Test:** For a single movie (e.g., "Shawshank Redemption"), generate reviews using both raw subtitles and cleaned screenplays. Compare the standard deviation of the "Joy" and "Disgust" scores from the DistilRoBERTa emotion classifier. This will confirm if screenplays yield more stable emotional outputs than subtitles.
3. **Persona Stability Test:** Generate 10 reviews for the same movie using Gemini-2.0 with a "Simple" persona (e.g., "You are a critic") and 10 with a "Detailed" persona (from Appendix A). Calculate the standard deviation of the TF-IDF cosine similarity scores against IMDb reviews. A lower standard deviation for the detailed persona would validate the claim that it reduces output variance.