---
ver: rpa2
title: 'ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented
  Generation'
arxiv_id: '2509.02330'
source_url: https://arxiv.org/abs/2509.02330
tags:
- gid00001
- code
- repair
- arxiv
- gid00041
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automated code repair using
  large language models (LLMs), where existing methods suffer from high training costs,
  expensive inference, or limited adaptability to out-of-distribution defects. The
  authors propose ReCode, a fine-grained retrieval-augmented generation framework
  that combines algorithm-aware retrieval with modular dual-view encoding to enhance
  code repair accuracy and efficiency.
---

# ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2509.02330
- Source URL: https://arxiv.org/abs/2509.02330
- Reference count: 40
- Primary result: ReCode achieves 2.7-5.3 percentage points higher test pass rates on RACodeBench compared to baselines while reducing inference cost by 2-4x

## Executive Summary
ReCode addresses the challenge of automated code repair using large language models by combining algorithm-aware retrieval with modular dual-view encoding. The framework narrows the retrieval space using algorithm type predictions and separately processes code and text to enable fine-grained semantic matching. Experimental results on RACodeBench and competitive programming datasets demonstrate that ReCode achieves higher repair accuracy with significantly reduced inference cost compared to existing methods.

## Method Summary
ReCode implements a retrieval-augmented generation framework that first predicts algorithm types from buggy code, then uses dual-view encoding to process text and code separately before fusing representations. The system retrieves relevant buggy-fixed exemplar pairs from algorithm-specific sub-knowledge bases and constructs prompts for LLM-based code repair. The approach uses OASIS-code-1.3B for code encoding, bge-m3 for text encoding, and combines these through feature fusion to enable fine-grained semantic matching during retrieval.

## Key Results
- ReCode improves test pass rates by 2.7-5.3 percentage points over baselines on RACodeBench
- Inference cost reduced by 2-4 times compared to existing methods
- Dual encoding improves test pass rates (Gemma-2-27B: 27.73% vs 26.25%; GPT-4o-mini: 38.49% vs 36.72%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Algorithm-aware narrowing of the retrieval space improves exemplar relevance
- Mechanism: LLM predicts algorithm type labels for buggy code, constraining retrieval to algorithm-specific sub-knowledge bases rather than full corpus
- Core assumption: LLM can accurately infer algorithm types from buggy code, and algorithmic similarity correlates with repair pattern similarity
- Evidence anchors: [abstract] algorithm-aware retrieval strategy; [section 3.3] multi-label classification approach; [corpus] Weak evidence from related work on RAG for code repair
- Break condition: If algorithm classification accuracy is low on new domains, retrieved examples may be misaligned

### Mechanism 2
- Claim: Dual-view encoding captures modality-specific semantics better than unified encoding
- Mechanism: Text and code encoded separately to capture distinct structural properties before fusion, avoiding modal interference
- Core assumption: Code and text have distinct structural properties better modeled independently
- Evidence anchors: [abstract] modular dual-encoder architecture; [table 1] Dual encoding improves test pass rates; [corpus] Related work supports separate encoding
- Break condition: If fusion step fails to combine complementary information effectively

### Mechanism 3
- Claim: Relevant in-context exemplars reduce inference cost while maintaining repair accuracy
- Mechanism: Retrieved buggy-fixed pairs provide explicit repair patterns, replacing sampling-heavy approaches with targeted few-shot inference
- Core assumption: Retrieved exemplars transfer repair logic to new problems without additional training
- Evidence anchors: [section 4.3] relevant examples lead to better performance under limited inference budgets; [figure 5] Relevant examples outperform random sampling; [corpus] Related work reports similar efficiency gains
- Break condition: If retrieved examples are syntactically similar but semantically mismatched, model may overfit to surface patterns

## Foundational Learning

- Concept: In-context learning
  - Why needed here: ReCode relies on providing exemplars at inference time rather than fine-tuning
  - Quick check question: Can you explain why in-context learning avoids catastrophic forgetting compared to fine-tuning?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Framework combines retrieval with generation; understanding baseline RAG clarifies what ReCode improves upon
  - Quick check question: What is the standard RAG pipeline, and where does ReCode modify it?

- Concept: Code embeddings and semantic matching
  - Why needed here: Dual-view encoding produces separate text and code representations; understanding how embeddings capture semantics is key to evaluating retrieval quality
  - Quick check question: Why might a unified text-code embedding fail to preserve code structural information?

## Architecture Onboarding

- Component map: Input parser -> Algorithm-aware classifier -> Dual-view encoder -> Hybrid retriever -> Prompt constructor -> Repair generator
- Critical path: Algorithm classification → dual-view encoding → retrieval → prompt construction → generation
- Design tradeoffs:
  - Multi-label vs. single-label algorithm classification: multi-label captures mixed paradigms but increases retrieval complexity
  - Unified vs. dual encoding: dual improves precision but adds encoder overhead
  - Knowledge base partitioning: algorithm-specific subsets improve relevance but require accurate tagging
- Failure signatures:
  - Low repair accuracy on out-of-distribution bugs: may indicate algorithm classification errors or insufficient knowledge base coverage
  - High inference cost despite retrieval: may reflect overly broad retrieval sets or weak filtering
  - Repetitive or irrelevant suggestions: retrieval may match surface syntax without semantic alignment
- First 3 experiments:
  1. Ablate algorithm-aware filtering and measure test pass rate drop on RACodeBench
  2. Replace dual encoding with unified encoding and compare retrieval precision metrics
  3. Vary number of retrieved exemplars (k=1,3,5) and plot test pass rate vs. inference cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of the preliminary algorithm type prediction impact the final code repair success rate?
- Basis in paper: Section 3.3 states system relies on LLM to "infer the algorithm type" to narrow search space
- Why unresolved: Paper evaluates end-to-end performance but not failure cases where wrong algorithm type is predicted
- What evidence would resolve it: Ablation study analyzing correlation between algorithm classification accuracy and final repair pass rates

### Open Question 2
- Question: Does the dual-view encoding strategy generalize effectively to industrial codebases and non-competitive programming domains?
- Basis in paper: Method evaluates exclusively on competitive programming datasets (Codeforces, AtCoder, etc.)
- Why unresolved: While claiming to address "real-world" scenarios, data source may not represent enterprise software complexity
- What evidence would resolve it: Experiments applying ReCode to Defects4J or Bugs.jar benchmarks

### Open Question 3
- Question: How does ReCode perform when encountering bugs derived from novel libraries or language features not present in the static knowledge base?
- Basis in paper: Knowledge base constructed from "historical archives," implying static cutoff point
- Why unresolved: Paper does not address "knowledge freshness" problem for libraries introduced after knowledge base creation
- What evidence would resolve it: Evaluation on temporal split of data with libraries introduced after knowledge base creation

## Limitations
- Algorithm classification accuracy is not reported, making it difficult to assess how often retrieval is misrouted
- Dual-view encoding fusion strategy is underspecified, raising questions about claimed modal separation benefits
- Knowledge base construction details (partitioning, indexing, sub-knowledge base sizes) are not fully specified

## Confidence
- High confidence: Dual-view encoding architecture and comparative performance gains are well-supported by ablation results
- Medium confidence: Algorithm-aware retrieval improves relevance based on indirect evidence (test pass rate improvements)
- Low confidence: Claim that efficiency gains are solely due to retrieval relevance is not fully isolated from other factors

## Next Checks
1. Measure algorithm classifier accuracy on held-out buggy code to quantify misclassification rates and impact on retrieval quality
2. Compare retrieval precision and recall between dual-view and unified encoding to directly validate semantic matching improvements
3. Ablate algorithm-aware filtering by retrieving from full corpus and measure drop in test pass rate to isolate contribution to performance gains