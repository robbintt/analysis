---
ver: rpa2
title: 'When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual
  Claim Verification'
arxiv_id: '2507.20700'
source_url: https://arxiv.org/abs/2507.20700
tags:
- across
- performance
- llms
- language
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks five state-of-the-art language models on
  the X-Fact dataset for multilingual claim verification across 25 languages with
  seven veracity categories. The experiments compare small language models (XLM-R,
  mT5) with large decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo) using fine-tuning
  and prompting approaches.
---

# When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification

## Quick Facts
- **arXiv ID**: 2507.20700
- **Source URL**: https://arxiv.org/abs/2507.20700
- **Reference count**: 11
- **Primary result**: XLM-R (270M) achieves 57.7% macro-F1, outperforming all tested LLMs (7-12B) on X-Fact multilingual claim verification

## Executive Summary
This study benchmarks five state-of-the-art language models on the X-Fact dataset for multilingual claim verification across 25 languages with seven veracity categories. The experiments compare small language models (XLM-R, mT5) with large decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo) using fine-tuning and prompting approaches. Surprisingly, XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B parameters), achieving 57.7% macro-F1 compared to the best LLM performance of 16.9%. This represents a 15.8% improvement over the previous state-of-the-art (41.9%). The analysis reveals that LLMs struggle to leverage evidence effectively and show pronounced biases toward frequent categories in imbalanced data.

## Method Summary
The study evaluates five language models on X-Fact, a dataset of 31,189 claims across 25 languages with seven veracity labels. Small language models (XLM-R, mT5) are fine-tuned end-to-end using AdamW with polynomial learning rate scheduling, while large language models (Llama 3.1 8B, Qwen 2.5 7B, Mistral Nemo 12B) are tested via 7-shot prompting and LoRA fine-tuning. The primary evaluation metric is macro-F1, with tests conducted on in-domain, out-of-domain, and zero-shot splits. Claims are paired with up to 5 evidence pieces (average 4.75) that are concatenated for model input.

## Key Results
- XLM-R achieves 57.7% macro-F1 on the test set, a 15.8% improvement over previous state-of-the-art
- Best LLM (Qwen 2.5 via LoRA) achieves only 16.9% macro-F1, with all LLMs underperforming SLMs
- LLMs show pronounced bias toward frequent categories (false, partly true) and struggle with minority classes
- Evidence concatenation format, while standard for SLMs, does not effectively support LLM reasoning

## Why This Works (Mechanism)
None

## Foundational Learning
**Claim Verification**: Process of assessing whether textual claims are true or false - needed to understand the task scope; quick check: verify models predict 7-class labels correctly
**Multilingual Processing**: Models must handle 25 languages simultaneously - needed for cross-lingual evaluation; quick check: test models on non-English languages
**Fine-Grained Classification**: Seven-way classification (vs binary) - needed to appreciate difficulty; quick check: examine confusion matrix for class distinctions
**Evidence Integration**: Combining claims with supporting evidence - needed to understand input format; quick check: test models with/without evidence
**Class Imbalance**: Severe skew toward "false" and "partly true" - needed to interpret performance gaps; quick check: compute class distribution statistics

## Architecture Onboarding

**Component Map**: Dataset (X-Fact) -> Data Preprocessing (claim+evidence concat) -> Model Training/Prompting -> Evaluation (macro-F1)

**Critical Path**: Data preparation → model fine-tuning/prompting → inference → metric computation

**Design Tradeoffs**: SLMs use full fine-tuning for task specialization vs LLMs use prompting to leverage general knowledge; SLMs concatenate evidence as text vs LLMs potentially use structured reasoning

**Failure Signatures**: Large macro-micro F1 gap (>20%) indicates label bias; LLMs perform worse with evidence than claim-only suggests ineffective evidence utilization

**First Experiments**:
1. Fine-tune XLM-R on X-Fact training split and evaluate on dev set
2. Run 7-shot prompting with Mistral Nemo on X-Fact test claims
3. Compare class-wise performance to identify which labels drive performance gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Findings specific to X-Fact dataset's fine-grained 7-way classification may not generalize to binary verification tasks
- Exact training durations and stopping criteria for fine-tuning experiments are unspecified
- 7-shot prompting examples are only partially detailed, complicating exact replication
- Results may not extend to larger LLMs (70B+ parameters) or different prompting strategies

## Confidence
**High Confidence**: XLM-R achieving 57.7% macro-F1 on X-Fact test; performance gap between SLMs and LLMs on this dataset; pronounced class imbalance effects
**Medium Confidence**: Generalization to other multilingual verification datasets; superiority of 270M over 7-12B parameters across all tasks; effectiveness of evidence concatenation
**Low Confidence**: Extrapolation to 70B+ LLMs; fundamental LLM ineffectiveness vs prompt engineering solutions; performance on unrepresented languages

## Next Checks
1. **Controlled parameter scaling test**: Reproduce experiments with intermediate-sized models (1-3B parameters) to determine if performance gap is linear or threshold-based
2. **Class-balanced subset evaluation**: Create and evaluate on balanced X-Fact subset to isolate fine-grained category handling vs class imbalance effects
3. **Evidence utilization analysis**: Systematically test LLM performance with claim-only, evidence-only, and hybrid inputs to quantify specific evidence integration difficulties