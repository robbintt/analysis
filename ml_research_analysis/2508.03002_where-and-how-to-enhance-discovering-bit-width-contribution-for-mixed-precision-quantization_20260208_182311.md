---
ver: rpa2
title: 'Where and How to Enhance: Discovering Bit-Width Contribution for Mixed Precision
  Quantization'
arxiv_id: '2508.03002'
source_url: https://arxiv.org/abs/2508.03002
tags:
- bit-width
- quantization
- search
- neural
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a critical issue in differentiable mixed precision
  quantization (DMPQ) where the magnitude of learnable bit-width parameters does not
  accurately reflect the actual contribution of bit-width operations to model performance.
  To address this, the authors propose a Shapley-based MPQ (SMPQ) method that directly
  measures the marginal contribution of bit-width candidates to the quantization task
  using Shapley values from cooperative game theory.
---

# Where and How to Enhance: Discovering Bit-Width Contribution for Mixed Precision Quantization

## Quick Facts
- **arXiv ID:** 2508.03002
- **Source URL:** https://arxiv.org/abs/2508.03002
- **Reference count:** 32
- **Primary result:** SMPQ outperforms gradient-based competitors across ResNet-18, MobileNetV2, ResNet-50, and Inception-V3 on ImageNet1K with higher accuracy and lower BOPs.

## Executive Summary
The paper addresses a fundamental issue in differentiable mixed precision quantization where learnable bit-width parameters (α) do not accurately reflect the true contribution of bit-width operations to model performance. To solve this, the authors propose Shapley-based MPQ (SMPQ) that uses Shapley values from cooperative game theory to directly measure the marginal contribution of each bit-width candidate. By leveraging Monte Carlo sampling for efficient computation and incorporating momentum for stability, SMPQ achieves superior accuracy-complexity trade-offs compared to existing gradient-based methods across multiple network architectures.

## Method Summary
SMPQ uses a bi-level optimization framework where network weights are trained via gradient descent on the training set, while bit-width contribution weights α are updated using Shapley values computed on the validation set. The Shapley value for each bit-width candidate is approximated using Monte Carlo sampling with truncated sampling to improve efficiency. Momentum is incorporated to stabilize the Shapley estimation process. During search, the supernet is quantized according to the current α values, and the contribution weights are updated proportionally to their Shapley values rather than gradient magnitudes. After search, the final bit-width policy is obtained by selecting the bit-width with maximum α per layer.

## Key Results
- SMPQ consistently outperforms gradient-based methods (EdMIPS, DNAS, FracBits-SAT, MetaMix) on ImageNet1K
- Achieves higher accuracy with lower bit operations (BOPs) across ResNet-18, MobileNetV2, ResNet-50, and Inception-V3
- Faster search times compared to competitors while maintaining superior accuracy
- Demonstrates the effectiveness of Shapley value-based bit-width selection in capturing true contribution

## Why This Works (Mechanism)

### Mechanism 1
The magnitude of learnable bit-width parameters α updated by gradient descent in DMPQ does not accurately reflect actual bit-width contribution to task performance. Shapley values measure the average marginal contribution of each bit-width operation across all possible coalitions, capturing cooperative interactions between bit-width operations that gradient descent ignores. The contribution weight α is updated proportionally to ψ(Jv(W*, α)) rather than gradient magnitude. Bit-width operations are not independent but cooperate, and their joint contributions differ from simple accumulation of individual contributions. [abstract] "the magnitude of quantization parameters does not necessarily reflect the actual contribution of the bit-width to the task performance"; [Section 3.2] Fig. 2 shows small α values can lead to high discretization accuracy; [Section 3.3] B3 accuracy exceeds sum of B1 and B2 individual changes, demonstrating joint contribution; [corpus] Weak direct corroboration; related MPQ papers do not address Shapley-based contribution.

### Mechanism 2
Monte Carlo sampling with truncation efficiently approximates Shapley values for large search spaces. Instead of enumerating 2^|O|×|E| subsets, Monte Carlo sampling draws random permutations. Truncated sampling clips the current sample if a bit-width causes significant performance drop, reducing unnecessary computation. The sampling distribution approximates the true marginal contribution expectation with sufficient samples M. [Section 4.2] "we adopt Monte-Carlo sampling method to get the approximation of the Shapley value, of which a truncated sample technique is also used"; [Section 5.4] Fig. 6 shows test error decreases with M; Fig. 7 shows medium truncation (threshold=0.5) achieves best trade-off; [corpus] No direct corpus evidence on MC sampling for Shapley in MPQ.

### Mechanism 3
Momentum stabilization reduces redundant fluctuations in Monte Carlo sampling-based Shapley estimation. Accumulated Shapley value qk is updated as exponential moving average: qk = β·qk-1 + λ·ψ/||ψ||₂, then αk = αk-1 + ξ·qk/||qk||₂. This balances historical and current contributions. Validation accuracy fluctuations from sampling are noise rather than signal, and temporal smoothing improves convergence. [Section 4.2] Eq. 8-9 define the momentum update; "we incorporate the momentum into the optimization for stabilization"; [Section 5.4] Table 5 shows SMPQ is relatively insensitive to β and ξ, with β=0.75, ξ=0.05 optimal; [corpus] No corpus corroboration; momentum for Shapley stabilization is novel to this work.

## Foundational Learning

- **Concept: Shapley Value (Cooperative Game Theory)**
  - **Why needed here:** Core mechanism for attributing contribution to players (bit-widths) in coalition games; requires understanding marginal contribution and efficiency axioms.
  - **Quick check question:** If players A and B together achieve value 10, A alone achieves 4, B alone achieves 3, what is B's Shapley value contribution when joining A? (Answer: 10 - 4 = 6)

- **Concept: Mixed Precision Quantization (MPQ)**
  - **Why needed here:** The target optimization domain; different layers use different bit-widths for weights/activations to balance accuracy-complexity trade-off.
  - **Quick check question:** Why would a convolutional layer processing edge features need higher precision than one processing semantic features? (Answer: Edge features often have higher spatial frequency information sensitive to quantization error)

- **Concept: Bi-level Optimization**
  - **Why needed here:** SMPQ uses nested optimization: inner loop trains network weights W on training data; outer loop selects bit-width α based on validation performance.
  - **Quick check question:** In Eq. 5, why must W* be optimized before α is updated? (Answer: α evaluation requires converged weights to measure true bit-width contribution on validation set)

## Architecture Onboarding

- **Component map:** Supernet G(V,E) -> Shapley Evaluator -> Momentum Buffer -> Contribution Weight α -> Network Weights W

- **Critical path:**
  1. Initialize supernet with pretrained full-precision weights
  2. For each epoch: train W on Dtrain (standard gradient descent)
  3. Sample M coalitions per edge, compute marginal contributions via forward passes on Dval
  4. Accumulate Shapley estimates with momentum (Eq. 9)
  5. Update α proportional to accumulated Shapley (Eq. 8)
  6. At search end, select argmax α per edge as final bit-width

- **Design tradeoffs:**
  - **Sample count M:** Higher M → better Shapley approximation but linear cost increase (Fig. 6); paper uses M=10
  - **Truncation threshold:** Too aggressive → miss informative samples; too conservative → waste computation (Fig. 7); paper uses 0.5
  - **Momentum β:** Higher β → more stable but slower adaptation; paper finds 0.75 robust (Table 5)
  - **Search space design:** Larger candidate sets (S3 vs S1) improve final accuracy but increase search cost

- **Failure signatures:**
  1. **α diverges or oscillates:** Likely momentum/sampling mismatch; check β and M calibration
  2. **Discretization accuracy << search-phase accuracy:** Shapley estimates not correlating with true contribution; verify Kendall's τ on held-out policies (Fig. 5 methodology)
  3. **Excessive GPU hours:** Truncation not firing; check threshold and validation batch size

- **First 3 experiments:**
  1. **Reproduce α pitfall on small subnet:** Train EdMIPS on ResNet-18 with search space S1, plot α magnitude vs discretization accuracy per edge; verify Fig. 2 pattern
  2. **SMPQ ablation on M:** Run SMPQ with M ∈ {1, 5, 10, 20} on MobileNetV2; plot test error vs GPU hours to find cost-accuracy frontier
  3. **Correlation validation:** After SMPQ search, sample 10 random discrete policies, compute Kendall's τ between α magnitude and test accuracy; confirm τ > 0.4 (Fig. 5 baseline)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the "alpha pitfall" phenomenon identified in convolutional networks also manifest in Transformer architectures, and can SMPQ handle the dynamic input dependencies of attention mechanisms?
- **Basis in paper:** [inferred] The experiments are restricted to CNNs (ResNet, MobileNet, Inception-V3), leaving the behavior of Shapley-based quantization on attention-based models unstated.
- **Why unresolved:** The contribution of bit-widths in attention layers (queries, keys, values) may differ fundamentally from convolutional weights, potentially altering the stability of the Shapley value estimation.
- **What evidence would resolve it:** Empirical evaluation of SMPQ on Vision Transformers (ViT) or BERT, comparing the correlation between α values and actual contribution against the CNN baselines.

### Open Question 2
- **Question:** How does the variance of the Monte Carlo sampling approximation scale with the complexity of the search space, and does it require more samples to converge in deeper networks?
- **Basis in paper:** [inferred] The method relies on Monte Carlo sampling to approximate Shapley values to avoid exponential complexity, but the paper only validates the approach on standard depth networks.
- **Why unresolved:** While the paper demonstrates efficiency on ResNet-50, the estimation noise might increase relative to the marginal contribution size in extremely deep or complex search spaces.
- **What evidence would resolve it:** A theoretical or empirical analysis of the approximation error relative to the number of parameters and the number of sampling iterations M.

### Open Question 3
- **Question:** Can the value function V(S) be reformulated to directly optimize for hardware metrics (latency, energy) rather than treating them as external constraints?
- **Basis in paper:** [inferred] The current optimization uses validation accuracy as the value function for Shapley calculation while treating Bit Operations (BOPs) as a constraint (Ω(Q) ≤ Ω₀).
- **Why unresolved:** BOPs are a proxy for efficiency; however, actual hardware latency and energy consumption depend on memory access patterns and data flow which are not captured by the current value function.
- **What evidence would resolve it:** A modified SMPQ framework where the Shapley value represents a weighted sum of accuracy and hardware efficiency, validated on physical hardware.

## Limitations
- The Monte Carlo approximation variance is not fully characterized, with M=10 appearing near the variance floor in experiments
- The optimal truncation threshold of 0.5 is architecture-specific with no theoretical basis for selection
- Claims about the "α pitfall" being universal are extrapolated from limited empirical validation

## Confidence
- **High confidence:** SMPQ outperforms gradient-based methods on reported ImageNet experiments (ResNet-18, MobileNetV2, ResNet-50, Inception-V3)
- **Medium confidence:** SMPQ's superiority is demonstrated on ImageNet1K only; generalization to other datasets or more complex architectures is untested
- **Low confidence:** Claims about the "α pitfall" being universal across all gradient-based MPQ methods are extrapolated from two examples without broader empirical validation

## Next Checks
1. **Ablation on sample count M:** Run SMPQ with M ∈ {1, 5, 10, 20} on MobileNetV2 and plot test error vs GPU hours to confirm the cost-accuracy frontier and verify M=10 is near-optimal
2. **Cross-dataset robustness:** Apply SMPQ to a non-ImageNet dataset (e.g., CIFAR-100) and compare against a gradient-based baseline (e.g., EdMIPS) to test generalizability
3. **Kendall's τ correlation validation:** After SMPQ search, sample 10 random discrete policies, compute Kendall's τ between α magnitude and test accuracy; confirm τ > 0.4 (Fig. 5 baseline) and test if the correlation persists under different truncation thresholds