---
ver: rpa2
title: 'Resting Neurons, Active Insights: Improving Input Sparsification for Large
  Language Models'
arxiv_id: '2512.12744'
source_url: https://arxiv.org/abs/2512.12744
tags:
- neurons
- spontaneous
- uni00000014
- arxiv
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap in large language models
  (LLMs) when using input sparsification techniques. While input sparsification improves
  efficiency by activating only a subset of input entries, it often leads to significant
  performance degradation compared to full models.
---

# Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models

## Quick Facts
- arXiv ID: 2512.12744
- Source URL: https://arxiv.org/abs/2512.12744
- Authors: Haotian Xu; Tian Gao; Tsui-Wei Weng; Tengfei Ma
- Reference count: 40
- Primary result: Adds trainable input-independent neurons to compensate for sparsification-induced performance loss

## Executive Summary
This paper addresses the performance gap in large language models (LLMs) when using input sparsification techniques. While input sparsification improves efficiency by activating only a subset of input entries, it often leads to significant performance degradation compared to full models. The authors propose spontaneous neurons, a novel mechanism inspired by biological neural systems' spontaneous baseline firing rates. These trainable, input-independent neurons act as compensatory units to stabilize activations in sparsified LLMs.

The method works by adding small sets of spontaneous neurons to each block in the transformer architecture. These neurons are learned via knowledge distillation, minimizing KL divergence between dense and sparse model outputs. At inference time, spontaneous neurons can be merged into bias terms, incurring no additional computational overhead. Experimental results demonstrate that spontaneous neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.

## Method Summary
The approach introduces trainable, input-independent "spontaneous neurons" that compensate for representational shifts caused by input sparsification in LLMs. The method involves adding a small set of learnable neurons to each transformer block, which are trained via knowledge distillation to minimize KL divergence between dense and sparse model outputs. These neurons capture prior expectations about typical activation patterns and can be merged into bias terms at inference, maintaining efficiency. The training is performed on calibration data with a frozen backbone, optimizing only the spontaneous neuron parameters.

## Key Results
- Achieves perplexity close to full models even at 50-60% sparsity levels on language modeling tasks
- Consistently improves performance over baseline methods on zero-shot reasoning tasks, particularly for complex reasoning and mathematical understanding
- Compatible with quantization techniques while maintaining high inference efficiency
- MLP down-projection injection achieves comparable performance to all-layer injection

## Why This Works (Mechanism)

### Mechanism 1: Bias Correction of Systematic Sparsification Error
- Claim: Input sparsification introduces a systematic residual error that can be partially corrected by learning an optimal constant bias vector.
- Mechanism: Sparsification S(·) zeroes low-magnitude activations, creating residual e(X) = WX - WS(X). The spontaneous neuron learns b* = E[e(X)], the expected residual, which is added back as a bias term. This satisfies L(b*) ≤ L(0) with strict improvement unless the residual mean vanishes.
- Core assumption: The residual error distribution has non-zero mean across typical inputs.
- Evidence anchors:
  - [Section 2.5]: Formal proof that optimal bias b* = E[e(X)] minimizes MSE L(b) = E[‖e(X) - b‖²]
  - [Appendix B.1]: Mathematical derivation showing strict improvement over zero bias
  - [corpus]: Related work on activation sparsity (TEAL, LaRoSA) focuses on computational savings without addressing representational shift; corpus lacks direct theoretical error-correction frameworks
- Break condition: If sparsification threshold is too low (minimal pruning) or residual mean approaches zero, bias correction yields negligible gains.

### Mechanism 2: Knowledge Distillation into Static Representations
- Claim: Spontaneous neurons encode prior expectations from the dense model via KL divergence minimization.
- Mechanism: The bias vector α is trained by minimizing KL(f(X), f(S(X); α⃗)) where f(·) is the LLM function. This distills dense model behavior into a compact, input-independent scaffold that complements dynamically pruned neurons.
- Core assumption: A small set of static activations can capture meaningful "prior expectations" that generalize across inputs.
- Evidence anchors:
  - [Section 2.4]: Training objective L = KL(f(X), f(S(X); α⃗))
  - [Section 3.4]: t-SNE visualization shows SPON hidden representations cluster closer to FULL than TEAL alone
  - [corpus]: DSMoE (arXiv:2502.12455) similarly addresses knowledge loss in sparsification through dynamic routing
- Break condition: If calibration data is unrepresentative of deployment distribution, distilled priors may not transfer.

### Mechanism 3: Layerwise Specialization of Bias Correction
- Claim: MLP down-projection layers, especially in upper layers, benefit most from spontaneous neuron injection.
- Mechanism: Upper layers capture local representations while lower layers specialize for final output generation. The down-projection in MLP is the most effective injection point, achieving performance comparable to all-layer injection.
- Core assumption: Different layers have varying sensitivity to representational perturbation from sparsification.
- Evidence anchors:
  - [Section 3.6]: MLP down-projection alone achieves perplexity 15.57 vs. 15.74 for attention-only on Llama3-1B
  - [Section 3.6]: Top layers show larger gains than bottom layers, consistent with prior layer-role findings
  - [corpus]: Weak corpus evidence on layerwise injection strategies; related work on sparse activation (R-Sparse) does not analyze injection locations
- Break condition: If model architecture differs significantly (e.g., non-Transformer), optimal injection locations may vary.

## Foundational Learning

- Concept: **Input Sparsification as Dynamic Neuron Pruning**
  - Why needed here: The paper reframes sparsification (thresholding activation entries) as equivalent to input-dependent neuron removal. Understanding this equivalence is essential for grasping why bias correction helps.
  - Quick check question: Given Y = Σᵢ xᵢwᵢ, what happens to the output when you threshold xᵢ < τ to zero?

- Concept: **KL Divergence for Distribution Alignment**
  - Why needed here: Spontaneous neurons are trained to match the sparse model's output distribution to the dense model's distribution. KL divergence quantifies this mismatch.
  - Quick check question: If two output distributions are identical, what is their KL divergence?

- Concept: **Bias Absorption for Zero Overhead**
  - Why needed here: The key practical benefit is that W·α can be precomputed as a bias term b, adding no matrix operations at inference.
  - Quick check question: Given a linear layer Y = WX + b, how many additional FLOPs are incurred by b compared to WX?

## Architecture Onboarding

- Component map:
  - **Sparsification function S(·)**: Magnitude-based thresholding (e.g., TEAL) applied to hidden states before linear layers
  - **Spontaneous activation α⃗**: Learnable vector per linear layer, trained via KL divergence loss
  - **Bias absorption**: Post-training, W·α⃗ is folded into existing bias b, preserving inference speed

- Critical path:
  1. Start with pretrained dense LLM (Llama3, Mistral, Qwen)
  2. Apply input sparsification S(·) at target sparsity level (e.g., 50%)
  3. Initialize α⃗ (small, e.g., one neuron per layer)
  4. Train α⃗ on calibration data (Wikitext) using KL loss against dense model outputs
  5. Absorb W·α⃗ into bias; deploy with sparsification only

- Design tradeoffs:
  - More spontaneous neurons → potentially better recovery but more parameters to train
  - Higher sparsity → greater efficiency but larger performance gap to close
  - Injection in all layers vs. MLP down-projection only: marginal performance difference, simpler to inject selectively

- Failure signatures:
  - Perplexity remains significantly above dense baseline → α⃗ may need more training epochs or wider calibration data
  - No speedup observed → sparsification may not be triggering correctly; verify threshold logic
  - Quantization instability → ensure bias absorption happens before quantization

- First 3 experiments:
  1. **Baseline replication**: Run TEAL at 50% sparsity on Llama3-8B with Wikitext; confirm perplexity matches ~8.34 from Table 1
  2. **Single-layer injection**: Add spontaneous neuron to MLP down-projection only; measure perplexity improvement
  3. **Ablation on sparsity levels**: Compare SPON vs. TEAL at 25%, 50%, 60% sparsity; plot performance gap closure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can spontaneous neurons be integrated effectively during the pre-training phase of LLMs rather than just as a post-hoc calibration step?
- Basis in paper: [explicit] The Conclusion states, "future work will explore spontaneous neurons in both pre-training and post-training regimes."
- Why unresolved: The current study focuses on training spontaneous neurons on calibration data (Wikitext) for already pre-trained models; the dynamics of training these units from scratch are untested.
- What evidence would resolve it: Experiments training models from scratch with spontaneous neurons enabled, comparing final dense performance and sparsification robustness against standard baselines.

### Open Question 2
- Question: Can specialized hardware kernels be designed to jointly optimize activation sparsity and weight quantization for real-world speedups?
- Basis in paper: [explicit] Section 3.7 notes that realizing the full efficiency potential "requires the development of specialized kernels that jointly support sparsity and quantization, which we leave as future work."
- Why unresolved: Current latency tests use generic implementations; fused kernel support is missing.
- What evidence would resolve it: Development and benchmarking of custom CUDA kernels that fuse sparse activation checks with quantized weight loading to demonstrate wall-clock speedups.

### Open Question 3
- Question: Why do top layers (closer to the embedding layer) exhibit larger gains from spontaneous neurons compared to bottom layers?
- Basis in paper: [inferred] Section 3.6 notes that top layers "exhibit larger gains" but relies on prior literature to hypothesize about local representations without providing a definitive causal mechanism.
- Why unresolved: The paper observes the trend empirically but does not isolate the specific representational mechanism causing this sensitivity to the bias correction.
- What evidence would resolve it: A layer-wise analysis of the residual error $e(X)$ distribution compared to the learned spontaneous activation magnitudes across different transformer blocks.

## Limitations

- Transferability across architectures: Method effectiveness for non-Transformer architectures or significantly different layer designs remains unverified
- Representative calibration data: KL divergence training assumes calibration dataset is representative of deployment scenarios
- Limited sparsification method exploration: Only TEAL method explored, with unknown sensitivity to different sparsification approaches

## Confidence

**High Confidence**: The core mechanism of bias correction for sparsification error (Mechanism 1) is mathematically sound and well-supported by the formal proof in Section 2.5. The experimental validation across multiple models and tasks provides strong empirical backing.

**Medium Confidence**: The knowledge distillation approach (Mechanism 2) shows consistent improvements in experiments, but the paper doesn't explore edge cases where the dense model's output distribution might not generalize well to sparse representations. The layerwise specialization claims (Mechanism 3) are supported by ablation studies but could benefit from broader architectural validation.

**Low Confidence**: The biological inspiration from spontaneous baseline firing rates, while intriguing, lacks direct experimental validation. The paper doesn't investigate whether the learned spontaneous neurons exhibit properties analogous to biological systems beyond their input-independence.

## Next Checks

1. **Cross-architecture transfer**: Apply spontaneous neurons to a non-Transformer architecture (e.g., Mamba or RWKV) to verify the method's generalizability beyond the current scope. Measure performance degradation and identify any architecture-specific adjustments needed.

2. **Distribution shift robustness**: Train spontaneous neurons on a domain-specific dataset (e.g., biomedical literature for MedMCQA) versus general Wikitext, then evaluate performance on both in-domain and out-of-domain tasks. This would quantify the method's robustness to training data distribution differences.

3. **Extreme sparsity regime**: Test spontaneous neurons at 70-80% sparsity levels to identify the practical limits of the approach. Compare performance degradation patterns with and without spontaneous neurons to determine whether the method scales effectively to more aggressive efficiency regimes.