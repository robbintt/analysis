---
ver: rpa2
title: Scaling Language-Centric Omnimodal Representation Learning
arxiv_id: '2510.11693'
source_url: https://arxiv.org/abs/2510.11693
tags:
- generative
- learning
- multimodal
- https
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of effective omnimodal representation
  learning by investigating the underlying reasons for the success of multimodal large
  language model (MLLM)-based embedding approaches. The authors propose that MLLMs
  implicitly acquire cross-modal alignment during generative pretraining, which enables
  lightweight contrastive learning (CL) to serve as an effective refinement stage.
---

# Scaling Language-Centric Omnimodal Representation Learning

## Quick Facts
- arXiv ID: 2510.11693
- Source URL: https://arxiv.org/abs/2510.11693
- Reference count: 40
- Primary result: State-of-the-art omnimodal representation learning using language-centric contrastive learning with significantly less training data than previous approaches

## Executive Summary
This work addresses the challenge of effective omnimodal representation learning by investigating the underlying reasons for the success of multimodal large language model (MLLM)-based embedding approaches. The authors propose that MLLMs implicitly acquire cross-modal alignment during generative pretraining, which enables lightweight contrastive learning (CL) to serve as an effective refinement stage. Based on this insight, they introduce a Language-Centric Omnimodal Embedding framework (LCO-EMB) that leverages language-centric data for efficient CL refinement while preserving the model's generative structure. Extensive experiments across diverse backbones and benchmarks demonstrate that LCO-EMB achieves state-of-the-art performance across modalities, even with significantly less training data than previous approaches. Furthermore, the authors identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scale positively with the MLLM's generative capabilities. This finding suggests that improving the MLLM's initial generative capability is an effective strategy for enhancing its potential in multimodal representations. The theoretical analysis, supported by a PAC-Bayesian generalization bound and empirical validation, confirms both the efficacy of LCO-EMB and the generality of GRSL, re-conceptualizing the role of contrastive learning in multimodal representation learning.

## Method Summary
LCO-EMB leverages language-centric contrastive learning to refine MLLM representations while preserving generative capabilities. The method isolates the LLM decoder, applies text-only contrastive learning via LoRA adapters (rank r=64, α=16), then merges and reintegrates into the full MLLM. This approach exploits implicit cross-modal alignment acquired during generative pretraining, allowing text-only training to improve representations across all modalities. The framework uses InfoNCE loss with language-centric data (all-NLI ~276k triplets, optional Scale-1M 1M pairs), evaluates on MIEB-Lite (51 tasks), and optionally includes ~94k synthetic multimodal pairs for downstream format calibration.

## Key Results
- Achieves state-of-the-art performance across diverse backbones (Qwen2.5-VL, LLaVA-Next, Qwen2.5-Omni) and benchmarks (MIEB-Lite, AudioCaps, MSR-VTT)
- Demonstrates significant data efficiency, using only 276k text pairs versus millions required by previous approaches
- Identifies Generation-Representation Scaling Law: representational performance after CL scales positively with base generative capability
- Shows LoRA (r=64, α=16) outperforms full fine-tuning by preserving cross-modal alignment while improving geometric properties

## Why This Works (Mechanism)

### Mechanism 1: Latent Cross-Modal Alignment
Text-only contrastive learning can improve representation quality across non-textual modalities (image, audio, video) without direct multimodal supervision because MLLMs acquire implicit cross-modal alignment during generative pretraining. The decoder must map multimodal inputs to a shared text-generation space, creating geometric binding between modalities. When CL improves the geometric properties (isotropy/uniformity) of the text embedding space, these improvements propagate to other modalities because their representations are already linearly bound to the text subspace within the decoder.

### Mechanism 2: Generation-Representation Scaling Law (GRSL)
The upper bound of an MLLM's representational performance after CL is determined by its generative capability before CL. A stronger generative prior captures higher mutual information between inputs and outputs. Theoretical analysis using PAC-Bayesian bounds suggests that lower generative loss (better prior) tightens the bound on the expected contrastive risk, making the optimization landscape for CL more favorable through a "warm start" effect.

### Mechanism 3: Preservation via Parameter-Efficient Fine-Tuning (PEFT)
LoRA is superior to full fine-tuning because it preserves the pre-trained latent alignment structure. CL introduces an objective mismatch with the autoregressive pretraining objective. Full fine-tuning perturbs weights too aggressively, disrupting the fragile cross-modal alignment. LoRA constrains the update rank, minimizing the KL divergence from the prior to the posterior, thereby retaining the "knowledge foundation" while projecting it into a similarity-matching space.

## Foundational Learning

**Concept: Anisotropy vs. Isotropy**
Why needed here: The paper argues that vanilla MLLMs suffer from "anisotropy" (embeddings collapsing to a narrow cone), and the primary function of CL here is to induce "isotropy" (uniform distribution) across modalities.
Quick check question: Can you explain why high anisotropy in an embedding space hurts retrieval performance? (Answer: It makes distinct inputs appear artificially similar)

**Concept: Contrastive Learning (InfoNCE)**
Why needed here: This is the refinement engine. Understanding that it pulls positive pairs together and pushes negative pairs apart is essential to grasp how it "fixes" the embedding geometry.
Quick check question: How does the "text-only" variant of CL differ from standard CLIP training? (Answer: CLIP aligns separate image and text encoders; this method refines a shared decoder using only text pairs)

**Concept: PAC-Bayesian Generalization Bounds**
Why needed here: This provides the theoretical justification for the GRSL. It links the complexity of the model shift (KL divergence) and the quality of the prior (generative loss) to the expected error.
Quick check question: In the context of this paper, what does a "lower generative loss" in the prior imply for the PAC-Bayes bound? (Answer: It tightens the upper bound on the expected contrastive risk)

## Architecture Onboarding

**Component map:** Modality Encoders -> Projector -> LLM Decoder (with LoRA adapters) -> Reintegrated MLLM

**Critical path:**
1. Freeze Modality Encoders and Projector
2. Inject LoRA into LLM Decoder
3. Train with Contrastive Learning on text pairs
4. Merge LoRA weights
5. Re-plug tuned decoder into original MLLM structure

**Design tradeoffs:**
- LoRA Rank/Alpha: Higher r allows more learning but risks "catastrophic forgetting" of generative alignment. Paper suggests r=64, α=16 for text-only
- Text-Only vs. Multimodal Data: Text-only (276k pairs) is highly efficient and generalizes well. Adding ~94k multimodal pairs calibrates for specific downstream formats

**Failure signatures:**
- Loss Spikes: Occurs if LoRA alpha is too high relative to rank (e.g., α=512), causing instability
- Capability Collapse: Occurs if continual SFT is done on narrow domains without mixing in general data to preserve alignment

**First 3 experiments:**
1. Ablation on LoRA vs. Full FT: Validate LoRA (r=64) outperforms Full Fine-Tuning on MIEB-Lite to confirm "preservation" hypothesis
2. Anisotropy Visualization: Train on text-only, then measure anisotropy on image embeddings to confirm cross-modal generalization without explicit image training
3. Scaling Law Verification: Take two backbones of different sizes, apply same CL recipe, verify delta in representation performance correlates with delta in base generative capability

## Open Questions the Paper Calls Out

**Open Question 1:** Does jointly optimizing generative and contrastive losses yield superior omnimodal representations compared to the sequential LCO-EMB pipeline? The authors state this requires high computational cost and is left for future work. Evidence needed: empirical comparisons between LCO-EMB and simultaneous generative-contrastive training on MIEB and SeaDoc benchmarks.

**Open Question 2:** How can the optimal relationship between LoRA rank (r) and alpha (α) be theoretically quantified to maximize latent alignment preservation? Appendix D notes optimal settings vary by task and model size. Evidence needed: theoretical framework modeling weight space perturbation relative to LoRA hyperparameters, validated by consistent performance across MIEB tasks.

**Open Question 3:** Does the "Generative Warm Start" hypothesis fail when the contrastive objective forces the representation space to diverge significantly from the generative prior? The PAC-Bayesian bound assumes contrastive fine-tuning acts as "lightweight" projection. Evidence needed: experiments measuring gradient alignment between generative and contrastive losses, correlating high conflict with failure to reach theoretical bounds.

## Limitations

- Cross-modal alignment fragility: Mechanism relies on implicit alignment being preserved through LoRA, but doesn't address scenarios where this alignment might be weak or absent in the base MLLM
- Data efficiency claims: Superiority of text-only training may partly reflect data curation rather than proposed mechanism itself
- Generalization scope: GRSL theoretically validated under specific conditions that may not hold for all architectures or objectives

## Confidence

**High confidence:** Core experimental results demonstrating LCO-EMB's state-of-the-art performance across MIEB-Lite, AudioCaps, and MSR-VTT benchmarks. Ablation studies clearly show LoRA's superiority over full fine-tuning.

**Medium confidence:** Mechanism explaining how text-only CL generalizes to non-text modalities through latent cross-modal alignment. While supported by anisotropy reduction measurements, direct evidence of geometric binding mechanism would strengthen this claim.

**Low confidence:** Theoretical derivation of Generation-Representation Scaling Law. PAC-Bayesian bound provides elegant framework, but empirical correlation between generative and representational capabilities doesn't establish causation or account for all confounding factors.

## Next Checks

1. **Cross-modal alignment stress test:** Systematically evaluate LCO-EMB on MLLMs with varying degrees of cross-modal alignment quality to identify minimum alignment threshold required for mechanism to function.

2. **Objective perturbation analysis:** Conduct controlled experiments varying KL divergence constraint in LoRA to quantify tradeoff between preserving generative priors and achieving optimal representational geometry.

3. **Scaling law validation:** Extend GRSL experiments to include MLLMs with fundamentally different architectural designs to test whether observed scaling relationship holds beyond unified generative models.