---
ver: rpa2
title: Rehearsal-free and Task-free Online Continual Learning With Contrastive Prompt
arxiv_id: '2510.00467'
source_url: https://arxiv.org/abs/2510.00467
tags:
- learning
- samples
- prompt
- continual
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses rehearsal-free and task-free online continual
  learning (F2OCL), a challenging scenario where catastrophic forgetting must be prevented
  without storing past samples or knowing task boundaries. The proposed method integrates
  prompt learning with a nearest-class-mean (NCM) classifier, where class-level prompts
  achieve parameter isolation and a contrastive loss ensures distinguishability between
  embeddings of different classes.
---

# Rehearsal-free and Task-free Online Continual Learning With Contrastive Prompt

## Quick Facts
- arXiv ID: 2510.00467
- Source URL: https://arxiv.org/abs/2510.00467
- Authors: Aopeng Wang; Ke Deng; Yongli Ren; Jun Luo
- Reference count: 40
- Primary result: Achieves 71.18% accuracy on CIFAR-100 and 46.75% on ImageNet-R in rehearsal-free task-free online continual learning

## Executive Summary
This paper introduces a rehearsal-free and task-free online continual learning (F2OCL) method that combines prompt learning with a nearest-class-mean (NCM) classifier. The approach maintains class-level prompts for parameter isolation and uses a dual contrastive loss to ensure embedding distinguishability without storing past samples or knowing task boundaries. Experiments demonstrate significant improvements over baselines on CIFAR-100 and ImageNet-R, achieving average accuracies of 71.18% and 46.75% respectively while preventing catastrophic forgetting.

## Method Summary
The method employs a frozen pre-trained ViT-B/16 encoder with class-level prompts for parameter isolation. Each class has dedicated prompt and key vectors stored in a prompt pool. The NCM classifier maintains running mean prototypes for each class. During training, a dual contrastive loss (Λ1 for prototype-based separation and Λ2 for intra-batch separation) updates prompts without storing samples. Inference uses nearest-key selection to retrieve the appropriate prompt, then classifies via nearest prototype. The approach processes data in one pass per batch, with optimal performance at 5 passes per batch.

## Key Results
- Achieves 71.18% average accuracy on CIFAR-100 and 46.75% on ImageNet-R in rehearsal-free setting
- Outperforms baselines including ViT-NCM, L2P, DualPrompt, LwF, and ER by significant margins
- Demonstrates superior forgetting resistance compared to linear classifier alternatives
- Shows effective embedding quality maintenance through contrastive prompt updates

## Why This Works (Mechanism)

### Mechanism 1: Class-Level Prompt Isolation
Assigning one dedicated prompt per class prevents interference across classes that would otherwise cause forgetting. Unlike prior methods (L2P, DualPrompt) that share prompts across tasks, this approach creates a unique (class, key, prompt) triplet for each class. When class y appears, only p_y is updated, leaving other prompts untouched—achieving parameter isolation without task boundaries. The core assumption is that classes have sufficiently distinct feature distributions that benefit from dedicated prompts; the number of classes does not grow unbounded in memory-constrained settings.

### Mechanism 2: Dual Contrastive Loss With Historical Prototypes
A two-part contrastive loss maintains embedding distinguishability both within the current batch and against accumulated class prototypes. Λ1 pulls a sample's augmented embedding toward its class prototype (historical mean) while pushing it away from other prototypes; Λ2 enforces intra-batch separation between classes. The weighting (α, β) balances historical vs. current batch information based on sample counts. The core assumption is that prototypes remain stable representations of class distributions even as prompts evolve; cosine similarity is an appropriate metric for this embedding space.

### Mechanism 3: NCM Classifier With Frozen Pre-trained Encoder
Nearest-Class-Mean classification over frozen ViT embeddings is more resistant to forgetting than linear classifiers. Prototypes are incrementally updated as running means, avoiding gradient-based weight updates that bias toward recent classes. The frozen ViT encoder prevents representation drift entirely. The core assumption is that pre-trained ViT features are sufficiently discriminative for the target domain without fine-tuning.

## Foundational Learning

- **Concept: Nearest Class Mean (NCM) Classification**
  - **Why needed here:** Core inference mechanism; must understand how prototypes aggregate class information and how distance-based prediction works.
  - **Quick check question:** Given prototypes μ₁, μ₂, μ₃ and a new sample embedding z, how would you compute the predicted class?

- **Concept: Contrastive Learning (InfoNCE-style objectives)**
  - **Why needed here:** The prompt update loss uses contrastive structure; understanding positive/negative sets and temperature scaling is essential.
  - **Quick check question:** In Eq. 4–7, what would happen if you set the temperature parameter τ → 0? What about τ → ∞?

- **Concept: Prompt Learning in Vision Transformers**
  - **Why needed here:** Must understand how learnable prompt tokens are prepended to ViT inputs and how they modulate representations without modifying backbone weights.
  - **Quick check question:** In a ViT-B/16 with prompt length L_p=20, where exactly are these 20 tokens inserted relative to the patch embeddings?

## Architecture Onboarding

- **Component map:** Frozen ViT-B/16 encoder -> Prompt pool (class, key, prompt triplets) -> NCM classifier (prototypes μ_c) -> Contrastive loss module (Λ1, Λ2)

- **Critical path:** Sample (x_i, y_i) -> Retrieve p_yi -> Compute augmented embedding z_xi = f(x_i, p_yi) -> Update prompt via contrastive loss -> Update prototype μ_yi via Eq. 8. Inference: Sample x -> Compute query q_x -> Find closest key k in pool -> Retrieve associated prompt p -> Compute z_x = f(x, p) -> Predict class with nearest prototype.

- **Design tradeoffs:** One prompt per class provides strong isolation but memory scales O(C × L_p × d) where C = number of classes. Frozen encoder provides maximum stability but zero adaptability to domain shifts; performance depends heavily on pre-training quality. Single-pass constraint preserves privacy but prompts may underfit (Table 2, Figure 4 suggest ~5 passes per batch is optimal).

- **Failure signatures:** Very low key selection accuracy (A_k) is expected behavior; Table 2 shows A_k ≈ 0.45–1.04% yet A_n remains high. Do not interpret as failure—prompts compensate. Sharp accuracy drop when switching from Sup-21K to DINO-1K indicates encoder quality is critical; verify pre-training alignment with target domain. Overfitting with >5 passes per batch: Figure 4 shows degradation at 10–15 passes; monitor prompt loss on held-out samples.

- **First 3 experiments:** 1) Reproduce CIFAR-100 baseline: Train with batch size 10, Sup-21K weights, report A_n and F_n; compare against ViT-NCM to isolate prompt contribution. 2) Ablate Λ1 (prototype term): Set α=0 and train; measure A_n drop to quantify the importance of historical prototype information. 3) Sweep passes per batch: Run 1, 3, 5, 10, 15 passes on CIFAR-100 subset; confirm optimal region around 5 passes and check for overfitting signs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the class-level prompt approach scale to datasets with thousands of classes, given the one-prompt-per-class design creates linear memory growth?
- Basis in paper: [inferred] The prompt pool stores a triplet (class, key, prompt) for each class, but experiments only tested 100 and 200 classes; no analysis of memory/computational scaling with larger class counts.
- Why unresolved: The paper does not discuss scalability limitations or alternative prompt sharing strategies for large-scale scenarios.
- What evidence would resolve it: Experiments on datasets with 1000+ classes reporting memory usage, inference time, and accuracy trade-offs.

### Open Question 2
- Question: Why does the method achieve high accuracy despite very low key selection accuracy (0.52–0.77), and can improving key selection further boost performance?
- Basis in paper: [explicit] Table 2 reports key selection accuracy Ak is very low, yet An is 40–70× higher; authors state "It is reasonable to attribute the significant improvement to prompts" but do not fully explain the mechanism.
- Why unresolved: The gap between key selection accuracy and classification performance is noted but not theoretically or empirically explained.
- What evidence would resolve it: Ablation studies analyzing embedding quality under oracle key selection vs. learned selection, and investigation of whether prompts compensate for poor key retrieval.

### Open Question 3
- Question: How dependent is the method on the choice and quality of pre-trained backbone, and can it work effectively with weaker or domain-specific pre-trained models?
- Basis in paper: [explicit] "When the pre-trained weights are changed from Sup-21K to DINO-1K, all methods present different levels of performance drop... the pre-trained weights are crucial to the performance."
- Why unresolved: The paper observes the sensitivity but does not propose mechanisms to reduce this dependency or test on non-ViT backbones.
- What evidence would resolve it: Experiments with diverse pre-trained models (e.g., ConvNeXt, Swin, domain-specific) and analysis of prompt learning's role in bridging backbone quality gaps.

### Open Question 4
- Question: What is the optimal strategy for determining the number of training passes per batch, and is this dataset-dependent or generalizable?
- Basis in paper: [explicit] "The best performances are achieved when the number of passes is 5. The prompts are under-fitting when the number of passes is 1. The prompts are over-fitting when the number of passes is 10 and even worse when 15."
- Why unresolved: The optimal value (5) is found empirically without theoretical justification or cross-dataset validation.
- What evidence would resolve it: Systematic study across multiple datasets with automated pass selection criteria based on convergence metrics or prompt embedding statistics.

## Limitations
- Memory footprint scales linearly with number of classes due to one-prompt-per-class design, potentially prohibitive in open-world settings
- Frozen ViT backbone provides stability but sacrifices adaptability to domain shifts; performance heavily depends on pre-training alignment
- Need for multiple passes per batch contradicts strict online setting and introduces critical hyperparameter
- Contrastive loss relies on cosine similarity assuming isotropic class distributions, may struggle with highly imbalanced or drifting data

## Confidence

- **High**: The superiority of NCM over linear classifiers in preventing forgetting (validated by ablation and comparison with baselines)
- **Medium**: The effectiveness of class-level prompt isolation (supported by ablation but no direct comparison to shared-prompt alternatives in F2OCL setting)
- **Medium**: The dual-contrastive loss formulation (conceptually sound but no ablation of Λ1 vs Λ2 components)

## Next Checks

1. **Memory Scaling Analysis**: Measure memory usage and inference latency as number of classes grows from 10 to 100; report per-class overhead and identify practical limits

2. **Encoder Adaptability Test**: Fine-tune the ViT backbone on small validation set and compare to frozen variant; quantify tradeoff between stability and adaptability

3. **Key Selection Ablation**: Implement oracle key selection (ground truth class) and measure gap to current method; assess whether key design is bottleneck