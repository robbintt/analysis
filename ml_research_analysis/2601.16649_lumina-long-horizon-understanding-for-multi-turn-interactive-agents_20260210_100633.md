---
ver: rpa2
title: 'LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents'
arxiv_id: '2601.16649'
source_url: https://arxiv.org/abs/2601.16649
tags:
- agent
- state
- task
- oracle
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) can effectively
  tackle long-horizon, multi-turn tasks by evaluating the relative importance of three
  key capabilities: planning, state tracking, and history pruning. To systematically
  measure these skills, the authors develop an oracle intervention framework that
  injects perfect information for each capability and assess the resulting performance
  changes.'
---

# LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents
## Quick Facts
- **arXiv ID**: 2601.16649
- **Source URL**: https://arxiv.org/abs/2601.16649
- **Reference count**: 15
- **Primary result**: Oracle interventions for planning, state tracking, and history pruning reveal capability-specific performance gains in procedural multi-turn game environments

## Executive Summary
This paper investigates how large language models (LLMs) can effectively tackle long-horizon, multi-turn tasks by evaluating the relative importance of three key capabilities: planning, state tracking, and history pruning. To systematically measure these skills, the authors develop an oracle intervention framework that injects perfect information for each capability and assess the resulting performance changes. They introduce procedurally generated game environments—ListWorld, TreeWorld, and GridWorld—that allow precise control over task complexity and enable accurate oracle annotations at every step. Experiments with Qwen3 models (4B-32B) show that while all interventions generally improve success rates, their effectiveness varies with model size and environment type. Larger models benefit more from state tracking, while smaller models gain more from history pruning. Across all settings, the gap between step accuracy and task success highlights the compounding impact of occasional errors in long-horizon reasoning. These findings guide future efforts to enhance LLM agents for complex, interactive, multi-turn scenarios.

## Method Summary
The authors systematically evaluate long-horizon multi-turn reasoning by creating controlled game environments where they can precisely measure the impact of individual capabilities. They introduce an oracle intervention framework that injects perfect information for planning, state tracking, and history pruning capabilities, allowing them to isolate each capability's contribution to overall performance. The procedural generation of ListWorld, TreeWorld, and GridWorld environments enables precise control over task complexity and accurate oracle annotations at every step. By testing Qwen3 models across different parameter sizes (4B-32B), they can analyze how capability importance varies with model scale. The methodology focuses on measuring both step-level accuracy and task completion rates to understand how occasional errors compound in long-horizon scenarios.

## Key Results
- Oracle interventions for all three capabilities (planning, state tracking, history pruning) improve performance, but effectiveness varies by model size and environment
- Larger models (32B) benefit more from state tracking interventions, while smaller models (4B) gain more from history pruning
- Step accuracy consistently exceeds task success rates across all settings, demonstrating the compounding effect of errors in long-horizon reasoning
- The procedural game environments successfully enable controlled experiments with precise capability isolation

## Why This Works (Mechanism)
The effectiveness of the three capabilities stems from their distinct roles in long-horizon reasoning. Planning provides the agent with a roadmap for achieving goals across multiple turns, enabling forward-looking decision making rather than reactive responses. State tracking maintains accurate awareness of the current environment state despite changes over time, preventing drift from reality that can derail complex tasks. History pruning manages the growing context by removing irrelevant information, reducing cognitive load and maintaining focus on task-relevant details. Together, these capabilities address the core challenges of maintaining coherence, managing complexity, and making consistent decisions across extended interaction sequences.

## Foundational Learning
- **Long-horizon reasoning**: The ability to maintain coherent decision-making across extended sequences of interactions. Why needed: Complex tasks require sustained planning and execution over many steps. Quick check: Can the agent complete tasks requiring 10+ sequential decisions?
- **State tracking**: Maintaining accurate awareness of the current environment state as it evolves over time. Why needed: Agents must base decisions on current reality rather than outdated information. Quick check: Does the agent correctly track changes after each action?
- **History pruning**: The skill of identifying and removing irrelevant context from growing conversation histories. Why needed: Long contexts degrade performance without effective information management. Quick check: Can the agent perform well despite increasing context length?
- **Oracle interventions**: A methodology for injecting perfect information to isolate capability contributions. Why needed: Enables systematic measurement of individual capability importance. Quick check: Does performance improve when perfect capability information is provided?
- **Procedural environment generation**: Creating controllable test scenarios with precise complexity parameters. Why needed: Allows systematic variation of task difficulty and complexity. Quick check: Can complexity be precisely controlled and measured?
- **Compounding error analysis**: Studying how individual mistakes accumulate across long interaction sequences. Why needed: Explains why step accuracy often exceeds task success rates. Quick check: Is there a consistent gap between step and task performance?

## Architecture Onboarding
- **Component map**: Game Environment -> Agent LLM -> Oracle Intervention Framework -> Performance Metrics
- **Critical path**: Procedural environment generation → agent interaction → oracle capability injection → performance measurement → analysis
- **Design tradeoffs**: Procedural generation enables precise control but may not capture real-world complexity; oracle interventions provide clean isolation but may not reflect practical deployment
- **Failure signatures**: Step accuracy exceeding task success indicates compounding errors; inconsistent gains across capabilities suggest capability-specific weaknesses; model-size-dependent benefits reveal architectural limitations
- **3 first experiments**: 1) Validate that all three oracle interventions individually improve performance baseline, 2) Test whether gains persist across different environment types, 3) Verify that model size correlates with capability importance patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Oracle interventions provide idealized upper bounds rather than realistic operational scenarios, limiting generalizability to deployed agents
- Procedural game environments may not capture real-world task complexity, uncertainty, and non-deterministic outcomes
- Analysis focuses on Qwen3 models (4B-32B) without establishing generalizability to other architectures or scale regimes
- The study does not establish causal mechanisms for size-dependent capability importance effects

## Confidence
- **High confidence**: Oracle intervention methodology validly isolates capability contributions; all three capabilities improve performance
- **Medium confidence**: Relative importance rankings across model sizes and environments are supported but may not generalize beyond tested architectures
- **Low confidence**: Specific numerical performance differences and their interpretation as capability requirements need further validation

## Next Checks
1. Validate oracle-based capability importance findings using non-oracle agents trained with targeted capability enhancements to confirm relative importance rankings in realistic settings
2. Test methodology across diverse model families and scales beyond Qwen3 (4B-32B) to establish robustness of capability importance patterns
3. Conduct ablation studies in more complex, real-world inspired environments with stochastic elements and partial observability to assess whether procedural games accurately predict practical capability requirements