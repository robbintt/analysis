---
ver: rpa2
title: 'From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines'
arxiv_id: '2512.11724'
source_url: https://arxiv.org/abs/2512.11724
tags:
- system
- latency
- user
- interactional
- modular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how modular Speech-to-Speech Retrieval-Augmented
  Generation (S2S-RAG) pipelines introduce conversational friction in real-world AI
  voice agents. The authors analyze a production system to show that performance trade-offs
  between ASR accuracy, LLM reasoning, and TTS synthesis directly degrade user experience,
  not just via latency but through misaligned timing, loss of paralinguistic cues,
  and inability to handle repair.
---

# From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines

## Quick Facts
- arXiv ID: 2512.11724
- Source URL: https://arxiv.org/abs/2512.11724
- Reference count: 1
- Primary result: Modular S2S-RAG pipelines introduce conversational friction through repair rigidity, expressive flattening, and temporal misalignment despite being ~15x cheaper than black-box alternatives.

## Executive Summary
This paper examines how modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines introduce conversational friction in real-world AI voice agents. The authors analyze a production system to show that performance trade-offs between ASR accuracy, LLM reasoning, and TTS synthesis directly degrade user experience, not just via latency but through misaligned timing, loss of paralinguistic cues, and inability to handle repair. Key results include ASR latency of 417ms with high WER (0.562) versus 2457ms with low WER (0.243); hybrid ASR-repair pipelines achieving 1040ms total latency; and end-to-end costs ~15x cheaper than black-box alternatives, but with reduced expressiveness and rigid turn-taking. The paper argues that building natural spoken AI is an infrastructure design challenge requiring coordinated orchestration of pipeline seams, not just isolated model optimization.

## Method Summary
The study analyzes a modular S2S-RAG pipeline consisting of VAD with probabilistic hysteresis, ASR (Typhoon or Google STT), optional textual repair layer, RAG retrieval, LLM reasoning, and TTS synthesis. The system implements half-duplex gating to manage turn-taking. Evaluation uses custom customer support dialogue datasets with code-switching between Thai and English technical terms. Metrics include latency at each pipeline stage, Normalized Word Error Rate (WER), Semantic Correction Score (LLM-as-a-Judge), and cost per turn. The hybrid repair layer combines fast ASR with lightweight LLM correction for code-switched terms.

## Key Results
- ASR latency of 417ms with high WER (0.562) versus 2457ms with low WER (0.243)
- Hybrid ASR-repair pipelines achieving 1040ms total latency
- End-to-end costs ~15x cheaper than black-box alternatives (Fluid pipeline $0.0010/turn vs. GPT-Realtime $0.0154/turn)
- Three identified friction mechanisms: Temporal Misalignment, Expressive Flattening, and Repair Rigidity

## Why This Works (Mechanism)

### Mechanism 1: Half-Duplex Gating Creates Repair Rigidity
- Claim: Modular pipelines that lock the audio channel during TTS output prevent real-time error correction, extending minor misrecognitions into multi-turn frustration loops.
- Mechanism: The system implements an exclusive lock on audio output during synthesis, making it "deaf" to barge-in attempts. When ASR errors occur (e.g., code-switching failures on "Azure"), users cannot interrupt and must wait for the full hallucinated response before correcting.
- Core assumption: Users expect collaborative turn-taking with backchanneling, not sequential monologues.
- Evidence anchors:
  - [abstract] "Repair Rigidity, where architectural gating prevents users from correcting errors in real-time"
  - [Section 5.1] Documents a 2-second repair becoming a 15-second loop when users cannot barge-in during mistaken responses
  - [corpus] FireRedChat paper explicitly addresses this via full-duplex architecture with controllable barge-in
- Break condition: Acoustic Echo Cancellation (AEC) with incremental processing would enable "Always-Listening" state, invalidating the half-duplex constraint.

### Mechanism 2: Text-Mediated Pipeline Strips Paralinguistic Cues
- Claim: Converting speech→text→speech loses prosodic information (sarcasm, hesitation, frustration), causing literal and socially inappropriate responses.
- Mechanism: ASR transcribes only lexical content; the LLM receives plain text without pitch, cadence, or emotional markers. The LLM interprets "Great, so you're telling me it failed again?" as positive sentiment rather than sarcasm.
- Core assumption: Emotional prosody carries pragmatic meaning that text alone cannot encode.
- Evidence anchors:
  - [abstract] "Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses"
  - [Section 5.2] Sarcastic user query met with cheerful response, breaking trust
  - [corpus] No direct corpus papers address paralinguistic loss; this mechanism is underexplored in related work
- Break condition: End-to-end audio-native models (e.g., GPT-Realtime) or audio-grounded emotion classifiers would preserve prosody through the pipeline.

### Mechanism 3: Hybrid Repair Layer Resolves ASR Speed-Accuracy Trade-off
- Claim: A lightweight LLM post-editing layer between fast ASR and reasoning LLM achieves better latency/accuracy profiles than waiting for accurate batch ASR.
- Mechanism: Fast ASR (417ms) produces phonetic transliterations for code-switched terms; a repair LLM (623ms) maps these to correct entities (Thai phonetic "AWS" → English acronym), yielding 1040ms total vs. 2457ms for accurate ASR alone.
- Core assumption: Phonetic errors are systematic and recoverable via context-aware correction; semantic drift is detectable.
- Evidence anchors:
  - [Section 4.1] Typhoon ASR: 417ms latency, 0.562 WER; Google STT: 2457ms, 0.243 WER
  - [Section 4.3] Repair layer achieves 0.85 Semantic Correction Score with 623ms fixed overhead
  - [corpus] KAME and AURA papers propose tandem/cascaded architectures but do not quantify repair-layer trade-offs
- Break condition: If code-switching errors become non-systematic or repair overhead exceeds ASR latency delta, the hybrid strategy loses advantage.

## Foundational Learning

- **Voice Activity Detection (VAD) with Probabilistic Hysteresis**
  - Why needed here: Raw ASR requires explicit floor-yield detection; without hysteresis, the system interrupts users during natural micro-pauses.
  - Quick check question: Can you explain why asymmetric thresholds (high confidence for turn-start, lower for turn-hold) prevent premature interruption?

- **Modular vs. End-to-End S2S Architectures**
  - Why needed here: The paper's central tension—control/determinism vs. fluidity/prosody—maps directly to this architectural choice.
  - Quick check question: What specific enterprise requirements (hallucination control, auditability, domain terminology) favor modular RAG pipelines over black-box end-to-end models?

- **Streaming vs. Batch Processing in Cascaded Pipelines**
  - Why needed here: The "Stream Aggregation Bottleneck" (Section 5.6) arises from token-streaming LLMs meeting phrase-boundary-requiring TTS.
  - Quick check question: For continuous-script languages like Thai, why does naive whitespace chunking fail for TTS input, and what must a Heuristic Aggregator do instead?

## Architecture Onboarding

- **Component map:**
  Hearing Layer: VAD with probabilistic hysteresis → ASR engine → message queue
  Orchestrator: Semaphore-based concurrency control, stateful context injection, RAG retrieval coordination
  Reasoning Core: LLM (configurable: lightweight vs. high-fidelity) + RAG vector store
  Speaking Layer: TTS synthesis with Half-Duplex Gate (locks input during output)

- **Critical path:**
  User speech → VAD floor-yield detection → ASR transcription → (optional Repair Layer) → RAG retrieval → LLM reasoning → TTS synthesis → Audio output. Total latency: 2–7 seconds depending on component choices.

- **Design tradeoffs:**
  - ASR: 417ms/0.562 WER (Typhoon) vs. 2457ms/0.243 WER (Google) vs. Hybrid 1040ms with repair
  - LLM: 1148ms lightweight vs. 5264ms high-fidelity (GPT-5)
  - Cost: Fluid pipeline $0.0010/turn vs. GPT-Realtime $0.0154/turn (~15x cheaper)
  - Control vs. Fluidity: Modular offers phrase-set injection and auditability; end-to-end offers superior prosody but no intervention points

- **Failure signatures:**
  - **Temporal Misalignment:** Silence >3 seconds triggers user "Are you still there?" queries, resetting VAD
  - **Expressive Flattening:** Literal responses to sarcastic or frustrated input
  - **Repair Rigidity:** User cannot barge-in during mistaken output; error propagates through full turn
  - **Stream Aggregation Bottleneck:** Choppy/staccato TTS from partial Thai word chunks

- **First 3 experiments:**
  1. **ASR Latency-Accuracy Benchmark:** Compare Typhoon, Google STT, and Hybrid (with repair layer) on code-switched technical queries; measure both WER and semantic correction scores.
  2. **Silence Threshold Testing:** Instrument user "are you there?" utterances against varying TTFB delays; identify the 3-second silence trigger point and test audio filler mitigation.
  3. **Repair Layer Ablation:** Disable the textual repair layer and measure downstream LLM failure rate on code-switched jargon; quantify the semantic drift cost vs. latency savings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fully incremental processing be implemented across ASR, LLM, and TTS components to enable mid-utterance interruption and re-planning?
- Basis in paper: [explicit] Section 7.2 identifies the need to shift to incremental components to eliminate "interactional deafness" caused by half-duplex gating.
- Why unresolved: Current modular components operate in rigid, sequential turns; the paper notes that coordinating these asynchronous operations for real-time interruption remains an under-analyzed challenge.
- What evidence would resolve it: A working prototype demonstrating a "barge-in" that triggers an immediate halt and semantic re-plan of the TTS output without completing the current turn.

### Open Question 2
- Question: How can modular S2S-RAG systems manage floor detection and turn-taking in multi-party conversations involving overlapping speech?
- Basis in paper: [explicit] Section 7.1 states that the current rigid Half-Duplex Gate is unworkable for group scenarios and requires solving speaker diarization and cross-talk.
- Why unresolved: The analysis is confined to dyadic (two-party) interaction; the authors highlight that group dynamics introduce exponential complexity in determining when the floor has been yielded.
- What evidence would resolve it: A system demonstration successfully differentiating multiple speakers and managing turn-yielding in a group setting without signal collision.

### Open Question 3
- Question: How can non-verbal cues like gaze and gestures be effectively integrated into the interactional floor management of voice agents?
- Basis in paper: [explicit] Section 7.4 proposes integrating multimodal signals (e.g., furrowed brow, breaking eye contact) to trigger clarification requests or signal turn completion.
- Why unresolved: The current system architecture relies solely on acoustic energy (VAD) and lacks input channels or processing logic for visual or gestural data.
- What evidence would resolve it: User studies showing reduced interactional friction or improved turn-taking accuracy when gaze detection is added as a parallel input stream to the VAD layer.

## Limitations
- The friction patterns identified may not generalize beyond the specific customer-support domain with Thai-English code-switching
- The quantitative impact of "expressive flattening" on user satisfaction remains largely qualitative
- The paper's analysis is confined to dyadic interaction, leaving multi-party conversation dynamics unexplored

## Confidence
- **High confidence**: Latency measurements and cost comparisons between modular and end-to-end approaches are concrete and reproducible
- **Medium confidence**: The mechanisms of repair rigidity and temporal misalignment are well-supported by examples
- **Low confidence**: The "expressive flattening" mechanism lacks direct empirical validation through user preference studies

## Next Checks
1. **Cross-Domain Friction Replication**: Test the modular pipeline with English-only technical support and casual conversation datasets to determine if repair rigidity and temporal misalignment persist across linguistic contexts.
2. **Expressive Flattening Quantification**: Conduct A/B testing where users interact with both modular and end-to-end systems on identical sarcastic/emotional prompts, measuring trust scores and task completion rates.
3. **Half-Duplex Alternative Evaluation**: Implement an Always-Listening architecture with incremental TTS synthesis and measure whether barge-in capability reduces repair loop duration by >50% compared to the current half-duplex design.