---
ver: rpa2
title: The Effect of Scripts and Formats on LLM Numeracy
arxiv_id: '2601.15251'
source_url: https://arxiv.org/abs/2601.15251
tags:
- answer
- arithmetic
- script
- scripts
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) achieve high accuracy on arithmetic
  when numbers are expressed in standard Hindu-Arabic numerals, but their performance
  drops substantially when numerals are rendered in non-standard scripts or formats.
  This work evaluates LLM numerical reasoning across 21 numeral scripts and 6 formatting
  variants, showing that model accuracy decreases by 66-87% when arithmetic expressions
  use non-Hindu-Arabic scripts.
---

# The Effect of Scripts and Formats on LLM Numeracy

## Quick Facts
- arXiv ID: 2601.15251
- Source URL: https://arxiv.org/abs/2601.15251
- Authors: Varshini Reddy; Craig W. Schmidt; Seth Ebner; Adam Wiemerslage; Yuval Pinter; Chris Tanner
- Reference count: 31
- Large language models show 66-87% accuracy drop when using non-standard numeral scripts

## Executive Summary
Large language models exhibit dramatically different arithmetic performance depending on whether numbers are expressed in standard Hindu-Arabic numerals versus alternative scripts or formats. The study systematically evaluates 21 numeral scripts and 6 formatting variants across multiple LLMs, revealing that models achieve high accuracy with familiar numerals but struggle significantly with unfamiliar representations. Through targeted prompting strategies including numeral translation, explicit mappings, and few-shot examples, the performance gap can be partially mitigated, with few-shot prompting showing the most consistent improvements across all tested models.

## Method Summary
The researchers conducted comprehensive experiments across 21 numeral scripts and 6 formatting variants, testing LLM performance on addition and multiplication tasks using integers up to 99. They evaluated multiple prompting strategies including direct prompts, translated operators, numeral mappings, and few-shot examples. The study measured accuracy differences between Hindu-Arabic numerals and alternative representations, analyzing performance across arithmetic operations and formatting conventions. Tokenization efficiency was examined as a potential explanatory factor for performance differences.

## Key Results
- LLMs achieve high accuracy on arithmetic with standard Hindu-Arabic numerals but performance drops 66-87% with non-standard scripts
- Few-shot prompting with formatted examples consistently improves performance across all tested models
- Multiplication is significantly more challenging than addition across all scripts and formatting variants

## Why This Works (Mechanism)
The performance differences stem from the interplay between tokenization efficiency, training data distribution, and model familiarity with numeral representations. Hindu-Arabic numerals benefit from frequent exposure in training data and efficient tokenization, while alternative scripts and formats introduce additional processing overhead. The tokenization process becomes less efficient with unfamiliar scripts, requiring more tokens to represent the same numerical information, which increases the cognitive load on the model during arithmetic reasoning.

## Foundational Learning

**Numeral Script Diversity** - Understanding the variety of writing systems used globally for representing numbers is essential because it reveals how different cultural contexts affect LLM performance. Quick check: Compare how the same number appears across different scripts (e.g., Arabic-Indic, Devanagari, Chinese).

**Tokenization Efficiency** - The relationship between character sequences and token boundaries directly impacts model performance on numerical tasks. Quick check: Measure token counts for the same numerical expression across different scripts.

**Arithmetic Operation Complexity** - Multiplication requires more computational steps than addition, making it inherently more challenging for models regardless of numeral representation. Quick check: Compare token-by-token processing requirements for addition versus multiplication.

## Architecture Onboarding

**Component Map**: Input Text -> Tokenizer -> Embedding Layer -> Transformer Blocks -> Output Layer -> Prediction

**Critical Path**: The tokenizer plays the most critical role as it determines how numerical information is encoded into tokens that the transformer can process. Poor tokenization of non-standard numerals creates cascading errors through the entire computation pipeline.

**Design Tradeoffs**: Models must balance between supporting diverse numeral representations and maintaining computational efficiency. Supporting many scripts increases model versatility but may dilute performance on any single representation due to limited training data per script.

**Failure Signatures**: Accuracy drops of 66-87% when using non-Hindu-Arabic scripts, with multiplication operations failing more frequently than addition. Tokenization inefficiency manifests as increased token counts and degraded arithmetic performance.

**First 3 Experiments**: 
1. Compare tokenization efficiency (tokens per character) across all 21 scripts
2. Test few-shot prompting effectiveness on a held-out script not used in initial experiments
3. Analyze attention patterns in transformer layers when processing different numeral scripts

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to addition and multiplication of small integers (â‰¤99), leaving unclear whether results extend to larger numbers, division, or multi-step reasoning
- 21 numeral scripts represent a broad but non-exhaustive sample; some scripts may exhibit different tokenization behaviors
- Analysis assumes consistent tokenizer behavior across models, but architectural differences in tokenization may influence script-specific performance independently

## Confidence

High: Core finding that standard Hindu-Arabic numerals yield superior performance compared to alternative scripts
Medium: Claim that multiplication is inherently more difficult than addition across all scripts
Low: Proposed mechanism linking tokenization efficiency directly to numerical reasoning performance

## Next Checks
1. Replicate experiments with division and subtraction problems to assess whether script effects persist across all arithmetic operations
2. Test models with synthetic numeral scripts designed to isolate tokenization effects from script familiarity
3. Conduct ablation studies varying training data exposure to different numeral formats to quantify the contribution of pretraining distribution to observed performance differences