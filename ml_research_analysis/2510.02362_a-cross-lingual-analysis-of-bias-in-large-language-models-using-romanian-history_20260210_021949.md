---
ver: rpa2
title: A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History
arxiv_id: '2510.02362'
source_url: https://arxiv.org/abs/2510.02362
tags:
- language
- romanian
- large
- across
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates bias in Large Language Models (LLMs) by
  examining their responses to controversial Romanian historical questions across
  four languages. A cross-lingual approach was employed, presenting 14 contentious
  historical statements to 13 different LLMs in Romanian, English, Hungarian, and
  Russian.
---

# A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History

## Quick Facts
- arXiv ID: 2510.02362
- Source URL: https://arxiv.org/abs/2510.02362
- Reference count: 4
- One-line primary result: LLM responses to historical questions vary significantly by language and format, with Russian and Hungarian showing culturally specific biases.

## Executive Summary
This study investigates bias in Large Language Models by examining their responses to controversial Romanian historical questions across four languages. The research employs a cross-lingual approach, presenting 14 contentious historical statements to 13 different LLMs in Romanian, English, Hungarian, and Russian. A three-stage questioning process captures both binary responses and nuanced essay answers, with an LLM-as-a-judge evaluating the essays. The findings reveal significant inconsistency in model responses, with binary stability averaging 75-81% across languages and numeric ratings frequently diverging from initial binary choices. Russian and Hungarian languages exhibited notable deviations from consensus on specific statements, reflecting culturally specific training data biases. Even with temperature adjustments, model consistency remained problematic, particularly for Romanian and Hungarian queries.

## Method Summary
The methodology involves presenting 14 contentious historical statements to 13 different LLMs across four languages: Romanian, English, Hungarian, and Russian. Each model receives the same statements in all four languages using a three-stage questioning process. First, models answer with a binary yes/no response. Second, they provide a numeric rating on a 1-10 scale. Third, they generate a detailed essay response. An LLM-as-a-judge evaluates the essay responses for quality and bias. The study tests consistency by adjusting temperature settings and comparing responses across languages to identify culturally specific biases.

## Key Results
- Binary response stability averaged 75-81% across languages, with Hungarian showing the lowest consistency at 75%.
- Numeric ratings frequently contradicted initial binary choices, with models changing their stance 23-28% of the time.
- Russian and Hungarian languages showed notable deviations from consensus on specific statements, reflecting culturally specific training data biases.

## Why This Works (Mechanism)
The study demonstrates that LLM responses are highly sensitive to linguistic context and prompt format, encoding dominant historiographical biases from their training data. By varying the language of questioning while maintaining identical historical content, the research reveals how cultural perspectives shape AI interpretations of contested history.

## Foundational Learning
- **Cross-lingual bias detection**: Why needed - to identify culturally specific training data influences; Quick check - compare responses across languages for same content
- **Prompt format sensitivity**: Why needed - to understand how question structure affects AI reasoning; Quick check - analyze response changes across binary, numeric, and essay formats
- **LLM-as-judge methodology**: Why needed - to evaluate AI-generated content objectively; Quick check - validate judge consistency across different model outputs

## Architecture Onboarding
**Component map**: Historical statements -> Multi-language prompt generation -> LLM inference -> Response collection -> LLM-as-judge evaluation -> Consistency analysis
**Critical path**: Statement selection → Prompt translation → Model inference → Response aggregation → Bias analysis
**Design tradeoffs**: Language diversity vs. cultural specificity; quantitative metrics vs. qualitative evaluation; temperature control vs. natural response generation
**Failure signatures**: Inconsistent binary responses (>25% change), contradictory numeric ratings, culturally divergent interpretations
**First experiments**: 1) Test single statement across all 13 models in one language, 2) Compare temperature settings (0.0, 0.7, 1.0) for consistency, 3) Validate LLM-as-judge against human evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond Romanian historical controversies remains uncertain
- LLM-as-a-judge reliability introduces potential bias without human validation
- Small set of 14 historical statements may not capture full bias spectrum
- Temperature adjustment implications for real-world deployment unclear

## Confidence
- **High**: Inconsistent model responses across languages and formats
- **Medium**: LLMs encode culturally specific biases from training data
- **Low**: LLMs are fundamentally unstable historical arbiters

## Next Checks
1. Replicate study with broader historical controversies from multiple cultural contexts
2. Conduct human evaluations of essay responses to validate LLM-as-a-judge results
3. Test consistency across wider range of temperature settings and prompt variations