---
ver: rpa2
title: 'Rethinking Caching for LLM Serving Systems: Beyond Traditional Heuristics'
arxiv_id: '2508.18736'
source_url: https://arxiv.org/abs/2508.18736
tags:
- siso
- cache
- caching
- semantic
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SISO, a semantic caching system designed
  to improve the efficiency of serving Large Language Models (LLMs) by caching semantically
  similar queries rather than exact matches. Traditional caching methods fail to recognize
  semantically similar queries, leading to inefficiencies in memory usage and cache
  performance.
---

# Rethinking Caching for LLM Serving Systems: Beyond Traditional Heuristics

## Quick Facts
- arXiv ID: 2508.18736
- Source URL: https://arxiv.org/abs/2508.18736
- Reference count: 40
- Primary result: SISO achieves up to 1.71× higher hit ratios and stronger SLO attainment compared to vLLM and GPTCache on single-turn LLM queries

## Executive Summary
This paper introduces SISO, a semantic caching system designed to improve LLM serving efficiency by caching semantically similar queries rather than exact matches. Traditional caching methods fail on paraphrased or semantically equivalent inputs, leading to inefficiencies. SISO addresses this by clustering queries into centroids, caching a single representative output per cluster, and using semantic locality for cache management. The system dynamically adjusts similarity thresholds to balance accuracy and latency under varying workloads, achieving significant performance improvements across diverse datasets while maintaining high output quality.

## Method Summary
SISO implements a two-phase approach: offline clustering and online serving. The offline phase embeds queries using paraphrase-albert-small-v2, clusters them via Community Detection (θ_C=0.86), and creates centroids with associated outputs and cluster sizes. The online phase uses HNSW search with dynamic threshold adjustment (θ_R∈[0.60,0.98]) based on an M/D/1 queue model to balance SLO attainment and output quality. Cache entries are managed using semantic locality rather than LRU, with re-clustering triggered when new queries reach ~10% of the initial dataset. The system was evaluated on five real-world datasets using LLaMa-3.1-8B and 70B models.

## Key Results
- Achieves 1.71× higher hit ratios compared to GPTCache
- Delivers 1.3× better SLO attainment (zero-load E2E latency)
- Maintains output quality with minimal F1 score degradation (99.5% on ShareGPT, 99.8% on Reddit)
- Requires 1.85-1.89× less memory than GPTCache for equivalent performance

## Why This Works (Mechanism)

### Mechanism 1: Centroid-based Memory Compression
Storing semantic centroids rather than individual query vectors maximizes cache coverage while minimizing memory footprint. By clustering semantically equivalent requests (e.g., "What is caching?" vs "Explain caching") and caching only the central vector and a single representative output, SISO deduplicates queries that would consume separate memory slots in exact-match caches. This assumes high cosine similarity correlates with output interchangeability.

### Mechanism 2: Semantic Locality-aware Eviction
Evicting cache entries based on long-term "semantic locality" (cluster stability) yields higher hit ratios than short-term heuristics like LRU. SISO tracks cluster size (number of queries represented by a centroid) and retains centroids with high cluster size due to their strong semantic locality. Replacements are triggered infrequently based on long-term trends rather than immediate memory pressure, assuming semantic locality remains relatively stable over time.

### Mechanism 3: Dynamic Thresholding via Queuing Theory
Adjusting the similarity threshold (θ_R) based on system load balances SLO attainment against output quality. SISO models the LLM server as an M/D/1 queue, lowering θ_R when estimated waiting time exceeds the SLO to make cached "similar" answers more accessible (trading accuracy for latency). When load is low, it raises θ_R to force exact computation (trading latency for accuracy).

## Foundational Learning

**Concept: Vector Embeddings & Cosine Similarity**
- Why needed: SISO relies entirely on representing text as vectors and calculating distances to find "semantically similar" queries
- Quick check: If two queries have cosine similarity of 0.99 but require different answers (e.g., "Disable the server" vs "Do not disable the server"), would SISO correctly handle this?

**Concept: LLM Serving SLOs (TTFT/TBT)**
- Why needed: The Dynamic Thresholding mechanism is explicitly driven by the need to meet Service Level Objectives (Time-To-First-Token and Time-Between-Tokens)
- Quick check: Why does lowering the similarity threshold (θ_R) improve TTFT during high load?

**Concept: Hierarchical Navigable Small World (HNSW) Graphs**
- Why needed: SISO uses HNSW for efficient vector search and optimizes it by placing high-locality centroids in upper layers
- Quick check: How does placing "popular" centroids higher in the HNSW graph reduce average search latency?

## Architecture Onboarding

**Component map:** SISO-Cluster (Embeddings -> Community Detection Clustering -> Centroid Repository) -> SISO-CacheManager (Merges new centroids, filters low-locality ones) -> SISO-Server (Receives Query -> Embeds -> Searches HNSW Cache -> Threshold Check) -> vLLM (LLM Inference Engine)

**Critical path:** The online query path. Latency added by the embedding step and HNSW search must be significantly lower than the LLM inference time to justify the cache.

**Design tradeoffs:** Clustering Frequency (re-clustering too often wastes compute; too rarely leads to stale centroids); Threshold Granularity (aggressive threshold lowering maintains SLOs but risks user churn due to low-quality answers)

**Failure signatures:** High Miss Rate on "Coding" Tasks (semantic caching assumes Input Sim ≈ Output Sim, which breaks for code where "print" vs "printf" matters); Cache Pollution (poor embedding model causes dissimilar queries to cluster together); SLO Violation on Burst Traffic (dynamic threshold logic lags behind sudden traffic spikes)

**First 3 experiments:** 1) Calibrate Embedding Model (run paraphrase-albert-small-v2 on domain data to verify gap between duplicate and non-duplicate cosine similarities); 2) Stress Test Dynamic Thresholding (apply Poisson-distributed load with high CV to verify M/D/1 model adjusts θ_R to maintain SLOs); 3) Eviction Policy Comparison (compare SISO's "Semantic Locality" eviction against LRU policy using fixed cache size on dataset with high query repetition)

## Open Questions the Paper Calls Out

**Open Question 1:** How can semantic caching systems be extended to effectively support context-dependent, multi-turn dialogues? The current system cannot support multi-turn queries because the assumption that "similar inputs yield similar outputs" fails when meaning depends on conversation history.

**Open Question 2:** Can semantic caching strategies be refined to maintain accuracy for tasks where minor input variations imply significantly different outputs, such as coding or debugging? SISO performs poorly on coding tasks because small changes in input fundamentally alter the required output.

**Open Question 3:** How does the reliance on offline-generated Threshold-to-Hit-Ratio (T2H) tables impact performance during sudden shifts in query semantics (concept drift)? The static mapping of thresholds to hit ratios may become invalid if the semantic distribution of incoming queries changes rapidly between re-clustering intervals.

## Limitations

- Embodies Dependence: Effectiveness critically depends on paraphrase-albert-small-v2's ability to distinguish semantically distinct queries, which may fail on domains requiring subtle semantic distinction
- Dynamic Thresholding Model Fragility: M/D/1 queue model assumes predictable traffic patterns and may be too slow to respond to bursty workloads
- Offline Clustering Overhead: Community Detection on large datasets could be computationally expensive and introduce maintenance overhead

## Confidence

**High Confidence:** Centroid-based memory compression delivers superior hit ratios (1.27-1.71× improvement); Semantic locality-based eviction outperforms LRU/LFU (96.1% centroid rank stability); SISO maintains output quality with minimal F1 degradation

**Medium Confidence:** Dynamic thresholding successfully balances SLO attainment and output quality across diverse workloads; Performance improvements scale across different LLM model sizes; HNSW optimization meaningfully reduces search latency

**Low Confidence:** Effectiveness on single-turn queries generalizes to multi-turn conversational workloads; System performs equivalently well across all five datasets without dataset-specific tuning; Memory savings claims hold under varying cache capacity constraints

## Next Checks

1. **Domain-Specific Embedding Validation:** Test paraphrase-albert-small-v2 on a dataset from your specific domain to measure the cosine similarity gap between true duplicates and semantically distinct queries

2. **Burst Traffic Simulation:** Apply a Poisson-distributed load with high Coefficient of Variation (CV > 5) to test the dynamic thresholding mechanism's responsiveness and measure whether the 10-second adjustment interval causes SLO violations

3. **Multi-Turn Query Evaluation:** Test SISO on a conversational dataset to verify whether single-turn query performance generalizes to multi-turn interactions where context history affects semantic similarity