---
ver: rpa2
title: Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained Scheduling
arxiv_id: '2501.12942'
source_url: https://arxiv.org/abs/2501.12942
tags:
- offline
- policy
- scheduling
- resource
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-user delay-constrained
  scheduling in real-time applications such as instant messaging, live streaming,
  and data center management. The key problem is to make real-time decisions that
  satisfy both delay and resource constraints without prior knowledge of system dynamics,
  which are often time-varying and difficult to estimate.
---

# Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained Scheduling

## Quick Facts
- **arXiv ID**: 2501.12942
- **Source URL**: https://arxiv.org/abs/2501.12942
- **Reference count**: 40
- **Primary result**: SOCD achieves higher throughput and more efficient resource utilization than existing methods across diverse environments, maintaining stable performance even in 100-user settings.

## Executive Summary
This paper addresses multi-user delay-constrained scheduling where real-time decisions must satisfy both delay and resource constraints without prior knowledge of time-varying system dynamics. Current learning-based methods require online interactions that degrade system performance during training. The authors propose SOCD (Scheduling By Offline Learning with Critic Guidance and Diffusion Generation), a novel offline reinforcement learning algorithm that uses a diffusion-based policy network complemented by a sampling-free critic network. SOCD integrates Lagrangian multiplier optimization into offline RL to train high-quality constraint-aware policies exclusively from pre-collected offline data, eliminating the need for online interactions while demonstrating superior performance across diverse environments including partially observable and large-scale settings.

## Method Summary
SOCD is a two-phase offline reinforcement learning approach for multi-user delay-constrained scheduling. First, a diffusion-based behavior cloning model is trained using score matching on offline data. Second, a sampling-free critic is iteratively trained using Monte Carlo returns, with the Lagrange multiplier updated based on offline estimates of resource consumption. At deployment, actions are sampled from the diffusion model and selected via Q-guidance. The method decomposes the multi-user MDP into per-user sub-MDPs for scalability, handling user indices through state/action encoding in a shared model.

## Key Results
- SOCD achieves higher throughput than BC and SOLAR baselines while satisfying resource constraints
- Maintains stable performance in 100-user environments where other methods degrade significantly
- Demonstrates superior resource utilization and consistent improvements across partially observable and large-scale settings
- Shows effectiveness in multi-hop networks and high user density scenarios

## Why This Works (Mechanism)

### Mechanism 1
Diffusion models capture multi-modal behavior distributions more faithfully than unimodal policies (e.g., Gaussian), preventing suboptimal convergence in offline RL. A score-based diffusion model learns the conditional score function ∇_a p_t(a|s) via denoising score matching. The reverse SDE recovers actions a_0 ~ μ_θ(·|s) from noise, preserving the complexity and diversity of the behavior policy in the offline dataset. Core assumption: The behavior policy in the offline dataset covers sufficiently diverse high-quality actions; the dataset is not exclusively poor or single-mode. Evidence anchors: [abstract] "SOCD innovatively employs a diffusion-based policy network... complemented by a sampling-free critic network for policy guidance." [Section 4.1.1] "It is critical for the BC model to express multi-modal action distributions, as previous studies have pointed out that uni-modal policies are prone to getting stuck in suboptimalities." Break condition: If the offline dataset has extremely narrow coverage (e.g., all actions from a single deterministic policy), the diffusion model will not extrapolate beyond the support, limiting improvement.

### Mechanism 2
A sampling-free critic avoids costly and error-prone action generation during TD learning, reducing extrapolation error and computational overhead. Instead of computing TD error with a' ~ π(·|s'), the critic uses the Monte Carlo return G_t = Σ γ^i r_i as the target. This avoids sampling actions not well-supported in the offline data. Double Q-learning (min over two Q-networks) mitigates overestimation. Core assumption: Trajectories in the offline dataset are sufficiently long and reward signals are reliable enough for Monte Carlo returns to provide stable supervision. Evidence anchors: [Section 4.1.2] "calculating the TD loss requires generating new (and potentially unseen) actions a', which is computationally inefficient... generating out-of-sample actions introduces potential extrapolation errors." [Section 4.1.2] "we propose a sampling-free approach for training the critic by directly using the discounted return as the target." Break condition: If trajectories are short or rewards are noisy, Monte Carlo returns become high-variance, destabilizing critic learning.

### Mechanism 3
Lagrangian dual optimization enforces average resource constraints in a fully offline manner by iteratively adjusting λ based on estimated policy resource consumption. The reward is re-parameterized as r_t = D(t) - λE(t), embedding the resource constraint into the objective. After each round of policy learning, λ is updated via gradient descent: λ ← λ - α(E₀ - Ê_π(λ)), where Ê_π(λ) is estimated from sampled offline trajectories under the current policy. This drives the policy toward the constraint boundary. Core assumption: The dual gap is small (strong duality holds approximately), and offline estimation of Ê_π(λ) is sufficiently accurate to guide λ updates. Evidence anchors: [Section 4.2] "the offline learning nature means that we cannot learn the value E_π*(λ) by interacting with the environment... we propose to estimate the resource consumption in an offline manner." [Section 3.1.1] "the optimal timely throughput T* is equal to the optimal value of the dual problem, i.e., T* = min_{λ≥0} g(λ)." Break condition: If the policy distribution diverges significantly from the behavior policy, the offline estimate Ê_π(λ) may be unreliable, causing λ to oscillate or converge to incorrect values.

## Foundational Learning
- **Concept: Offline Reinforcement Learning & Distributional Shift**
  - Why needed here: The entire SOCD framework depends on learning from fixed datasets without environment interaction; you must understand why policies can overestimate values on unseen actions and how to constrain them.
  - Quick check question: Can you explain why querying a Q-function on actions outside the offline dataset leads to overoptimistic value estimates?

- **Concept: Diffusion Models (Score-Based Generative Models)**
  - Why needed here: The behavior cloning component uses a reverse SDE to generate actions; you need to grasp how denoising score matching works and why it captures multi-modal distributions.
  - Quick check question: Given a noisy action a_t = α_t a_0 + σ_t ε, what does the score network s_θ(a_t, s, t) approximate, and how is it used in the reverse process?

- **Concept: Lagrangian Duality for Constrained Optimization**
  - Why needed here: Resource constraints are handled via the dual; you must understand how λ trades off throughput vs. resource consumption and how the dual gradient relates to constraint violation.
  - Quick check question: If the current policy consumes more resources than E₀, should λ increase or decrease? What does this imply for the reward r_t = D(t) - λE(t)?

## Architecture Onboarding
- **Component map**: Offline dataset -> Diffusion BC Model (μ_θ) -> Sampling-free Critic (Q_ϕ1, Q_ϕ2) -> Action Selector (DPM-solver, K samples) -> Lagrange Optimizer (λ updates) -> Constraint-aware Policy
- **Critical path**: 1) Collect offline dataset D (e.g., from RSD4 or other suboptimal policy). 2) Train diffusion BC model μ_θ once (100k steps, lr=1e-4). 3) Initialize λ; for each Lagrange iteration: retrain critics with updated rewards r = D - λE, estimate Ê_π(λ) via policy rollouts on offline data, update λ ← λ - α(E₀ - Ê_π(λ)). 4) At deployment: for each state s, sample K actions from diffusion model, select via Q-guidance.
- **Design tradeoffs**: Larger K (sampled actions) improves action quality but increases inference latency. Paper uses K=1024 with DPM-solver. Higher α (temperature coefficient) in importance sampling favors high-Q actions but may deviate from behavior support. Paper empirically finds argmax (α→∞) more stable. More Lagrange iterations improve constraint satisfaction but require retraining critics each round. User-level decomposition enables scalability (tested to 100 users) but assumes per-user independence in state/action encoding.
- **Failure signatures**: Policy collapses to mean action (mode collapse) → diffusion model undertrained or score network capacity insufficient. λ oscillates wildly → learning rate α too high or offline estimate Ê_π(λ) too noisy; reduce α or increase sampled trajectories. Resource constraint violated at deployment → offline estimate biased; verify dataset coverage near constraint boundary. Performance degrades with more users without decomposition → state/action dimensionality exceeds model capacity; implement user-level decomposition.
- **First 3 experiments**: 1) Reproduce the 4-user Poisson-1hop environment with a medium-quality offline dataset (e.g., 5000 trajectories from a suboptimal policy). Verify that SOCD throughput exceeds BC and SOLAR baselines while satisfying resource constraints. 2) Ablate the sampling-free critic: compare against a TD-based critic that samples actions from the diffusion model. Measure critic training time and final policy performance to validate the efficiency-accuracy tradeoff. 3) Test scalability by increasing users to 50+ with user-level decomposition. Monitor throughput stability, resource variance, and inference latency. Compare against non-decomposed baseline to confirm decomposition benefits.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions in the provided content.

## Limitations
- Theoretical justification is lacking for the empirical finding that argmax selection (α→∞) provides more stable performance than the theoretically derived importance sampling strategy.
- Performance in low-data regimes is untested; the algorithm's robustness against highly suboptimal or sparse offline datasets remains unknown.
- Convergence of the offline Lagrangian multiplier estimation to the optimal value that strictly satisfies long-term average constraints without online refinement is unproven.

## Confidence
- **High confidence**: The core mechanism of using diffusion models for behavior cloning in offline RL is well-established in the literature and the theoretical foundation is sound.
- **Medium confidence**: The sampling-free critic approach is novel and shows promise, but the reliance on Monte Carlo returns assumes sufficiently long trajectories and stable reward signals, which may not generalize to all scheduling scenarios.
- **Low confidence**: The Lagrangian multiplier updates in fully offline settings depend on accurate estimation of policy resource consumption, but this estimation can be unreliable when the learned policy differs substantially from the behavior policy.

## Next Checks
1. **Dataset Coverage Analysis**: Verify that the offline dataset contains sufficient diversity near the resource constraint boundary. Test whether SOCD performance degrades when trained on datasets with artificially reduced coverage.
2. **Trajectory Length Sensitivity**: Systematically vary trajectory lengths in the offline dataset and measure the impact on critic stability and final policy performance to validate the Monte Carlo return assumption.
3. **Real-World Deployment Test**: Implement SOCD on a small-scale real-time scheduling system (e.g., edge computing testbed) to validate whether simulation performance translates to practical deployment, particularly regarding latency constraints of the diffusion sampling process.