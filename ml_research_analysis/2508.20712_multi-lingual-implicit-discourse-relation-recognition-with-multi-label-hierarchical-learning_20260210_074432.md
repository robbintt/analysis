---
ver: rpa2
title: Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical
  Learning
arxiv_id: '2508.20712'
source_url: https://arxiv.org/abs/2508.20712
tags:
- discourse
- language
- discogem
- each
- multi-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first multi-lingual and multi-label classification
  model for implicit discourse relation recognition (IDRR). The proposed HArch model
  leverages hierarchical dependencies across all three sense levels in the PDTB 3.0
  framework to predict probability distributions, outperforming prior multi-label
  approaches.
---

# Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning

## Quick Facts
- arXiv ID: 2508.20712
- Source URL: https://arxiv.org/abs/2508.20712
- Reference count: 32
- Primary result: First multi-lingual and multi-label classification model for implicit discourse relation recognition that outperforms prior approaches and few-shot prompted LLMs across four languages

## Executive Summary
This work introduces the first multi-lingual and multi-label classification model for implicit discourse relation recognition (IDRR) using the PDTB 3.0 sense hierarchy. The proposed HArch model leverages hierarchical dependencies across three sense levels to predict probability distributions, achieving state-of-the-art results on the DiscoGeM 2.0 corpus across English, German, French, and Czech. Fine-tuned HArch models consistently outperform few-shot prompted LLMs (GPT-4o and Llama-4-Maverick) across all languages and settings. The approach also achieves SOTA results on the DiscoGeM 1.0 corpus, validating the effectiveness of the hierarchical approach.

## Method Summary
The HArch architecture concatenates ARG1 and ARG2 discourse arguments and feeds them through a pre-trained PLM encoder (RoBERTa-base for English, XLM-RoBERTa-base for multilingual). A shared linear layer and dropout transform the encoder representation, which feeds three classification heads simultaneously. Level-1 (4 senses) predictions are augmented through a two-step projection block and fused with the shared representation via a learnable weighted sum to inform Level-2 (17 senses). The same process repeats for Level-3 (28 senses) using both Level-1 and Level-2 outputs. Training uses summed Mean Absolute Error losses across all three heads with Adam optimizer, batch size 16, and learning rate 1e-5 for 10 epochs.

## Key Results
- HArch with RoBERTa achieves best results in English, while XLM-RoBERTa-HArch excels in the multi-lingual setting
- Fine-tuned HArch models consistently outperform GPT-4o and Llama-4-Maverick in few-shot prompting across all languages and settings
- RoBERTa-HArch achieves SOTA results on the DiscoGeM 1.0 corpus
- Multi-task joint training improves performance and reduces compute time compared to single-level models

## Why This Works (Mechanism)

### Mechanism 1
Cascading information from coarser-grained to finer-grained sense levels improves multi-label classification accuracy at deeper hierarchy levels. Level-1 predictions are projected through an augmentation block into the encoder's embedding space and combined with the shared representation via a learnable weighted sum. This augmented representation feeds Level-2; the same process repeats for Level-3 using both Level-1 and Level-2 outputs. This allows finer-grained heads to exploit coarse-grained predictions as structured priors. Evidence: RoBERTa-HArch improves Level-2 JS distance (0.478) vs. non-hierarchical baseline (0.498) and Level-3 (0.541 vs. 0.569). Break condition: If sense hierarchy were flattened, cascading would no longer provide structured signal.

### Mechanism 2
Joint multi-task learning across all sense levels yields both better performance and higher training efficiency than training separate single-level models. A shared encoder and linear+dropout representation feed three classification heads simultaneously. Gradients from all three losses update shared parameters, enabling positive transfer and regularization across levels. Evidence: RoBERTa-HArch outperforms RoBERTa-Individual at Level-2 (0.477 vs. 0.513) and Level-3 (0.548 vs. 0.589) with ~2.4× less compute time. Break condition: If one level's task were adversarial, joint training would degrade performance on other levels.

### Mechanism 3
Task-specific fine-tuned encoders outperform few-shot prompted LLMs on multi-label IDRR under the evaluation protocol used. Fine-tuning adapts encoder representations directly to the multi-label distribution matching objective and PDTB 3.0 label space. In contrast, few-shot prompting must infer both the task structure and the precise distributional output format from examples without weight updates. Evidence: XLM-RoBERTa-HArch achieves Level-2 JS distance 0.529 vs. GPT-4o 0.678 and Llama-4-Maverick 0.769. Break condition: If prompting were allowed chain-of-thought reasoning with extensive in-context examples calibrated to the distribution output format, the gap could narrow.

## Foundational Learning

- **Concept: Multi-label distributional annotation (DiscoGeM)**
  - Why needed: DiscoGeM represents each relation as a probability distribution over senses, reflecting annotator disagreement and genuine ambiguity. Models must predict distributions, not a single class.
  - Quick check: Can you explain why using argmax over a predicted distribution would lose signal for both training and evaluation?

- **Concept: PDTB 3.0 sense hierarchy (3 levels)**
  - Why needed: The architecture explicitly depends on parent–child relationships: Level-1 has 4 senses, Level-2 expands to 17, and Level-3 to 28. Understanding this structure is essential to interpret augmentation blocks and weighted sums.
  - Quick check: If Level-1 predicts "Contingency" with high probability, which Level-2 senses become more plausible a priori?

- **Concept: Jensen-Shannon (JS) distance for evaluation**
  - Why needed: The paper reports JS distance between predicted and reference distributions across levels. Lower JS indicates better distributional alignment. Understanding JS is critical to read results correctly.
  - Quick check: Why is JS distance preferred over simple accuracy for evaluating multi-label probability distributions?

## Architecture Onboarding

- **Component map:** Input ARG1+ARG2 string -> PLM encoder -> Shared linear+dropout -> Level-1 head -> Augmentation block -> Weighted sum -> Level-2 head -> Augmentation block -> Weighted sum -> Level-3 head -> Output distributions

- **Critical path:**
  1. Encode ARG1+ARG2 → shared representation
  2. Level-1 head → 4-dim distribution → augment → fuse into Level-2 input
  3. Level-2 head → 17-dim distribution → augment → fuse with both Level-1 and Level-2 augmentations into Level-3 input
  4. Level-3 head → 28-dim distribution
  5. Loss: Sum of MAE losses across all three heads; evaluate with JS distance

- **Design tradeoffs:**
  - RoBERTa-base wins for English; XLM-RoBERTa-base is best multilingually but slightly underperforms RoBERTa on English-only
  - ModernBERT underperformed expectations—likely due to pretraining data or optimization mismatch
  - Joint training reduces total time vs. three separate models while improving performance
  - Prompting LLMs is easier to deploy but substantially underperforms and costs more per inference at scale

- **Failure signatures:**
  - Level-1 performance similar to baseline, but Level-2/Level-3 not improving: check augmentation blocks and weighted sums
  - Multilingual model performs worse on specific language: may indicate data imbalance or interference
  - JS distance does not decrease with more epochs: possible overfitting or inappropriate learning rate

- **First 3 experiments:**
  1. Reproduce RoBERTa-HArch on DiscoGeM 2.0 English: train for 10 epochs, batch size 16, LR 1e-5, and verify Level-2/Level-3 JS distances match Table 2
  2. Ablate augmentation blocks: replace two-step projection with single linear projection and compare Level-2/Level-3 JS distances
  3. Swap XLM-RoBERTa for mBERT-base and compare multilingual performance to verify whether encoder capacity or pretraining quality drives advantage

## Open Questions the Paper Calls Out

### Open Question 1
How does translation influence discourse reasoning and label distribution across the four languages in the DiscoGeM 2.0 corpus? The authors plan to investigate how discourse sense predictions vary across languages and explore how translation influences discourse reasoning and label distribution. This is unresolved because the current work evaluates performance per language but does not analyze the specific cross-linguistic variations in parallel instances. A comparative error analysis of label distributions for parallel discourse relations would resolve this.

### Open Question 2
Can language-specific pre-trained encoders outperform the multi-lingual XLM-RoBERTa for German, French, and Czech within the HArch framework? The authors aim to compare the performance of using XLM-RoBERTa against encoders pre-trained specifically in these languages. This is unresolved because the study relied on XLM-RoBERTa to compensate for limited data, leaving the efficacy of monolingual encoders untested. Benchmarking HArch with monolingual encoders against the XLM-RoBERTa baseline would resolve this.

### Open Question 3
Would utilizing larger pre-trained encoders (e.g., RoBERTa-large or LLaMA-based backbones) yield significant performance gains in the HArch architecture? The authors note they did not experiment with larger models such as LLaMA 3 which might have led to improved performance due to computational constraints. It is undetermined if the model's performance is bottlenecked by the capacity of the base-sized encoders or the hierarchical learning architecture itself. An ablation study substituting the base encoders with large-scale variants would resolve this.

## Limitations
- Exact dropout rate and initialization scheme for learnable parameters are not specified
- No seed or randomization protocol is reported, making exact replication difficult
- Cross-lingual performance differences may be influenced by corpus size imbalances across languages

## Confidence
- **High confidence**: Hierarchical information cascading improves multi-label classification
- **High confidence**: Joint multi-task learning outperforms single-level training  
- **Medium confidence**: Fine-tuned models significantly outperform prompted LLMs

## Next Checks
1. Verify augmentation block gradient flow by examining α, β1, β2 weight evolution during training
2. Conduct ablation study with randomly shuffled sense hierarchy to test whether observed gains depend on meaningful PDTB 3.0 structure
3. Implement language-specific adapter layers in the multilingual model to test whether observed performance gaps are due to interference or insufficient model capacity