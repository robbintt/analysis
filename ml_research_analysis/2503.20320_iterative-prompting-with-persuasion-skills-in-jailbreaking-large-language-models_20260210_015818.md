---
ver: rpa2
title: Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models
arxiv_id: '2503.20320'
source_url: https://arxiv.org/abs/2503.20320
tags:
- prompts
- llms
- prompt
- attack
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study exploits large language models (LLMs) using an iterative
  prompting technique where each prompt is systematically modified and refined across
  multiple iterations to enhance its effectiveness in jailbreaking attacks. The approach
  analyzes response patterns of LLMs (GPT-3.5, GPT-4, LLaMa2, Vicuna, and ChatGLM)
  to adjust and optimize prompts for evading ethical and security constraints.
---

# Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models

## Quick Facts
- arXiv ID: 2503.20320
- Source URL: https://arxiv.org/abs/2503.20320
- Authors: Shih-Wen Ke; Guan-Yu Lai; Guo-Lin Fang; Hsi-Yuan Kao
- Reference count: 13
- Primary result: Iterative refinement with persuasion techniques achieves up to 90% attack success rate across victim LLMs

## Executive Summary
This paper presents an iterative prompting technique that systematically refines jailbreak prompts to enhance their effectiveness in bypassing LLM safety constraints. The approach combines weighted feedback from multiple victim models with persuasion techniques derived from human communication studies to generate adversarial prompts that evade ethical restrictions. Results demonstrate that attack success rates increase across refinement rounds, with the highest success against GPT-4 and ChatGLM (90%) and lowest against LLaMa2 (68%).

## Method Summary
The method uses a fine-tuned GPT-3.5-turbo-0125 attacker model that generates Persuasive Adversarial Prompts (PAPs) using five persuasion techniques (Logical Appeal, Authority Endorsement, Misrepresentation, Evidence-based Persuasion, and Bandwagon Appeal). After each attack round, responses from five victim LLMs are evaluated using keyword-based success detection, and a Weighted Attack Success Rate (WASR) score is calculated by aggregating binary success/failure signals weighted by each model's defense strength. This score is fed back to the attacker model to generate refined prompts for the next round, with the process continuing for up to four iterations before observed quality degradation.

## Key Results
- Attack success rates increase from 61.82% (round 1) to 83.70% (round 4) across all victim models
- Highest ASR of 90% achieved for GPT-4 and ChatGLM, lowest of 68% for LLaMa2
- Outperforms baseline methods (PAIR and PAP) in ASR while showing comparable performance with GCG and ArtPrompt
- Translation attacks (OURS-TL) show higher ASR than English, indicating cross-lingual vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1
Iterative refinement with weighted feedback systematically increases jailbreak success rates across victim models. The WASR scoring mechanism aggregates binary success/failure signals from five victim LLMs, weighted by each model's defense strength. This total score is fed back to the attacker model, which generates refined prompts in subsequent rounds. Higher WASR scores indicate prompts that successfully breach harder-to-attack models. The core assumption is that attacker models can meaningfully interpret numerical feedback scores to improve prompt quality without explicit gradient signals.

### Mechanism 2
Fine-tuning an attacker model on persuasion technique examples increases the effectiveness of generated adversarial prompts. The paper uses knowledge distillation: 520 harmful prompts are rephrased using 5 persuasion techniques (Logical Appeal, Authority Endorsement, Misrepresentation, Evidence-based Persuasion). This creates a training dataset where GPT-3.5 learns to generate PAPs that reframe harmful queries as academic, theoretical, or expert-justified requests. The core assumption is that persuasion techniques effective on humans transfer to LLMs because LLMs can be viewed as "human communicators."

### Mechanism 3
Keyword-based success detection provides a computationally efficient but noisy signal for iterative attack optimization. Success is determined by absence of refusal keywords ("I'm sorry", "can't provide"). If keywords are absent, the attack succeeds (score=1); if present, it fails (score=0). This binary signal is aggregated across all victim models and rounds. The core assumption is that absence of explicit refusal keywords indicates harmful content was generated—not just evasion or ambiguous responses.

## Foundational Learning

- **Concept: Prompt-Level vs. Token-Level Attacks**
  - Why needed here: The paper explicitly contrasts its approach with token-level methods (GCG) that use gradient-optimized gibberish suffixes. Understanding this distinction clarifies why persuasion-based attacks produce interpretable, human-readable outputs.
  - Quick check question: Can you explain why "jailbreak prompts that lack interpretability" (GCG) vs. "syntactically current sentences" (PAP) matter for real-world attack scenarios?

- **Concept: Knowledge Distillation for Adversarial Training**
  - Why needed here: The method uses a teacher model to generate PAP examples, then fine-tunes a student model (GPT-3.5) to reproduce this behavior. This pattern is common in adversarial ML but requires understanding of dataset construction and fine-tuning APIs.
  - Quick check question: If you had access only to black-box API access (no fine-tuning), how would you adapt this approach?

- **Concept: Weighted Scoring for Multi-Objective Optimization**
  - Why needed here: WASR weights victim models by defense strength (LLaMa2=0.22 highest, Vicuna=0.18 lowest). This encourages the attacker to prioritize harder targets rather than gaming easy ones.
  - Quick check question: Why might a simple average of success rates across models fail to incentivize attacking the most robust victims?

## Architecture Onboarding

- **Component map:** Harmful query → Attacker generates PAP with persuasion technique → Checking mechanism validates intent → PAP sent to all 5 victims → Keyword evaluator scores each response → WASR calculated → Score + previous prompts fed back to attacker → Next round

- **Critical path:** Attacker model generates PAP → All 5 victim models process prompt → Keyword evaluator determines success/failure for each → WASR calculator aggregates weighted scores → Feedback loop to attacker for refinement

- **Design tradeoffs:** Keyword-based detection is fast but may produce false positives; LLM-based evaluation would be more accurate but slower/costlier. Fixed weights from external benchmark may not reflect current model versions; dynamic weight estimation could improve but adds complexity. Manual intent checking is inefficient; paper calls for automation as future work.

- **Failure signatures:** ASR plateaus or declines after round 4 (observed in experiments). Prompts drift from original intent (e.g., "irrelevant responses"). Category-specific failures (hate crime ASR=0%, certain query types fail entirely). Translation attacks show higher ASR than English, suggesting cross-lingual vulnerabilities.

- **First 3 experiments:** 1) Replicate single-round baseline using 50-prompt subset from AdvBench Harmful Behaviors, measure ASR across 5 victims without iteration. 2) Ablate persuasion techniques by comparing ASR when using only one technique vs. all five. 3) Test intent drift threshold by running 5+ rounds and manually labeling prompt-intent alignment to validate the 4-round stopping criterion.

## Open Questions the Paper Calls Out

The paper does not explicitly list open questions, but several areas for future work are implied: automating the intent-checking mechanism to replace manual screening, developing dynamic weighting schemes that adapt to model-specific responses, and extending the approach to handle category-specific failures (particularly hate crime prompts).

## Limitations

- Keyword-based success detection remains unvalidated and may produce false positives/negatives
- Limited victim model diversity with only 5 models tested, all accessed via API except LLaMa2
- Category-specific failure modes where certain harmful content types show 0% ASR even after refinement
- Intent drift occurs without automated detection, making the 4-round stopping criterion somewhat arbitrary

## Confidence

- **High confidence:** Iterative refinement mechanism (ASR increases across rounds is well-documented)
- **Medium confidence:** Weighted scoring improves attack effectiveness (relies on external benchmark weights)
- **Low confidence:** Persuasion techniques transfer from humans to LLMs (theoretical justification only, no direct validation)

## Next Checks

1. Validate keyword detection accuracy by manually annotating 100 randomly selected successful attacks to verify whether absence of refusal keywords actually indicates harmful content generation vs. benign evasion. Calculate precision and recall.

2. Test dynamic weighting scheme by replacing fixed external weights with model-specific weights estimated from current-round success rates. Compare whether adaptive weighting maintains or improves ASR compared to the static WASR approach.

3. Implement automated intent checking by replacing manual prompt screening with an LLM-based intent verification system. Measure whether this prevents quality degradation beyond round 4 while maintaining ASR improvements.