---
ver: rpa2
title: 'Enhancing Job Matching: Occupation, Skill and Qualification Linking with the
  ESCO and EQF taxonomies'
arxiv_id: '2512.03195'
source_url: https://arxiv.org/abs/2512.03195
tags:
- entity
- esco
- linking
- evaluation
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study advances job entity extraction by comparing two language\
  \ model approaches\u2014sentence linking (SL) and entity linking (EL)\u2014to map\
  \ job vacancy texts to the ESCO and EQF taxonomies. Two novel datasets for occupations\
  \ and qualifications were introduced, along with a title similarity set to improve\
  \ classification."
---

# Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies

## Quick Facts
- **arXiv ID:** 2512.03195
- **Source URL:** https://arxiv.org/abs/2512.03195
- **Reference count:** 17
- **Key outcome:** Novel comparison of sentence linking vs. entity linking for ESCO/EQF job taxonomy mapping, with new datasets and state-of-the-art ER model.

## Executive Summary
This study advances job entity extraction by comparing two language model approaches—sentence linking (SL) and entity linking (EL)—to map job vacancy texts to the ESCO and EQF taxonomies. Two novel datasets for occupations and qualifications were introduced, along with a title similarity set to improve classification. For occupation extraction, SL performed best, especially when combining job titles with descriptions, while EL was superior for skill extraction. Qualification extraction showed no clear method advantage. An ER model trained on a job domain benchmark achieved a new state-of-the-art F1-score of 54.3. Generative LLMs did not improve results in this task. All models and datasets are publicly released to support further research.

## Method Summary
The paper compares two architectures for linking job vacancy text to ESCO and EQF taxonomies: Sentence Linking (SL) uses a bi-encoder to embed full job contexts and retrieve taxonomy matches, while Entity Linking (EL) first extracts specific skill spans via token classification before disambiguation. Two novel datasets were introduced: a title similarity set (210k pairs) for fine-tuning, and an extended Green Benchmark for qualification extraction. The study uses ESCO v1.1.1 (3,007 occupations, 13,890 skills) and EQF (814 entries). Evaluation metrics include Accuracy@1 for linking and strict F1-score for entity recognition. The best results came from SL with concatenated preferred labels and descriptions for occupations, and EL for skills, with a RoBERTa-based ER model achieving a state-of-the-art F1 of 54.3 on the Green Benchmark.

## Key Results
- SL architecture outperforms EL for occupation extraction when concatenating preferred labels with descriptions (Accuracy ~0.50).
- EL architecture is superior for skill extraction, isolating relevant spans from noisy context (Accuracy ~0.40).
- RoBERTa-based ER model achieves a new state-of-the-art F1-score of 54.3 on the Green Benchmark.
- Generative LLMs (Gemini, Llama) underperform supervised methods in this structured linking task.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Concatenating preferred labels with descriptions in a Sentence Linking (SL) architecture maximizes occupation retrieval accuracy.
- **Mechanism:** SL utilizes a bi-encoder (e.g., `all-mpnet-base-v2`) to embed full job contexts. By concatenating the taxonomy's "Preferred Label" and "Description," the resulting vector captures both semantic definition and title specificity, reducing ambiguity compared to using titles alone.
- **Core assumption:** Job titles are often ambiguous or missing; the surrounding text provides necessary disambiguation signals that pure title-matching misses.
- **Evidence anchors:**
  - [Page 4, Table 3] Shows "Single: Preferred Label and Description" (0.4981) outperforming "Preferred Label" alone (0.2416) for full-text occupation linking.
  - [Page 7] Concludes that "applying SL with embedding strategies that incorporate rich contextual information yields the best results for Occupation Extraction."
  - [corpus] Neighbors like "CareerBERT" support the efficacy of shared embedding spaces for job matching.
- **Break condition:** If job descriptions are extremely short or irrelevant (noise), concatenation introduces noise, degrading performance back to title-level baselines.

### Mechanism 2
- **Claim:** Entity Linking (EL) outperforms Sentence Linking for skill extraction because it isolates relevant spans from noisy context.
- **Mechanism:** The EL architecture decouples mention detection from disambiguation. By first extracting the exact skill span (e.g., "Java") via Entity Recognition (ER), the subsequent similarity search queries the knowledge base using only the skill text, filtering out irrelevant job requirements.
- **Core assumption:** The semantic match for a specific skill is contained within the skill phrase itself, while the surrounding sentence often contains unrelated requirements that dilute the signal in whole-sentence embeddings.
- **Evidence anchors:**
  - [Page 7, Table 6] Reports EL accuracy (0.3969) nearly doubling SL accuracy (0.2211) for skills.
  - [Page 5] Notes that for skills, "incorporating the broader context of the entity introduces noise."
  - [corpus] "Contrastive Bi-Encoder Models..." suggests context is vital, but this paper specifically qualifies that context helps *occupations* while *skills* require isolation.
- **Break condition:** If the ER module fails to detect the entity span (low recall), the EL pipeline fails completely, whereas SL might still retrieve a result based on residual semantics.

### Mechanism 3
- **Claim:** Supervised encoder models (RoBERTa) significantly outperform generative decoder LLMs (Gemini, Llama) for structured entity recognition in this domain.
- **Mechanism:** Supervised token classification (RoBERTa+CRF) forces the model to adhere to specific span boundaries (BIO tags) learned from labeled data. In contrast, generative LLMs prompted for extraction struggle with precise span alignment and hallucinate ESCO codes not present in the official taxonomy.
- **Core assumption:** Strict adherence to the ESCO hierarchy and precise text spans is required, which current zero-shot generative instruction following cannot guarantee.
- **Evidence anchors:**
  - [Page 6] Reports RoBERTa_base achieved a state-of-the-art F1 of 54.3 on the Green Benchmark.
  - [Page 8] States Gemini 1.5 Pro and Universal-NER "severely underperform supervised methods" (F1 0.22–0.33 vs 54.3).
  - [corpus] Corpus papers (e.g., "Leveraging LLMs") explore LLMs, but this paper provides specific evidence of their failure in *structured* linking tasks relative to encoders.
- **Break condition:** If a massive, high-quality instruction-tuning dataset specific to ESCO were available, generative models might close the gap, but this is not demonstrated here.

## Foundational Learning

- **Concept: Bi-Encoder (Sentence Transformer) Architecture**
  - **Why needed here:** This is the engine for Methodology 1 (SL) and the similarity step of Methodology 2 (EL). You must understand that it maps disparate text lengths (titles vs. descriptions) into a fixed vector space ($R^n$) for cosine similarity comparison.
  - **Quick check question:** How does the model handle the length mismatch between a 5-word job title and a 500-word job description during inference?

- **Concept: Token Classification vs. Text Generation**
  - **Why needed here:** The paper contrasts predicting a label for every token (ER module using RoBERTa) against generating text (LLMs). Understanding this distinction explains why the encoder won (structural precision) over the decoder (semantic fluency).
  - **Quick check question:** Why would a CRF layer on top of BERT improve the prediction of "B-Skill" followed by "I-Skill"?

- **Concept: Extreme Multi-Label Classification (XMLC)**
  - **Why needed here:** Mapping job descriptions to ESCO is not a multi-class problem (one label) but an XMLC problem (thousands of possible labels, e.g., 13,890 skills). This drives the need for efficient retrieval (vector search) rather than standard softmax classification.
  - **Quick check question:** Why is a standard softmax output layer infeasible when the label space is 13,000+ classes?

## Architecture Onboarding

- **Component map:**
  1. **Data Layer:** ESCO/EQF Vector DB (Pre-computed embeddings of taxonomy nodes).
  2. **Method 1 (SL) Pipeline:** Raw Text $\to$ Sentence Transformer $\to$ Cosine Search $\to$ Top-K Results.
  3. **Method 2 (EL) Pipeline:** Raw Text $\to$ **ER Module (RoBERTa+CRF)** $\to$ Extracted Spans $\to$ Sentence Transformer $\to$ Cosine Search.
  4. **Evaluation:** Accuracy@1 and Jaccard Similarity.

- **Critical path:** The **Entity Recognition (ER) Module**. The paper explicitly states "ER errors propagate to entity disambiguation." If the ER model misses a skill span (e.g., "Python"), the downstream similarity module cannot link it, resulting in a false negative.

- **Design tradeoffs:**
  - **Context vs. Noise:** Use SL for Occupations (needs context); use EL for Skills (context is noise).
  - **Speed vs. Structure:** SL is faster (one embedding pass); EL is slower (inference per token + embedding) but provides specific span locations.
  - **Generative vs. Discriminative:** Do not use LLMs for this specific task yet; they offer no performance gain and introduce latency/hallucination.

- **Failure signatures:**
  - **SL Failure:** Retrieves "Project Manager" when the description says "Manage projects" but the actual title is "Software Engineer" (noise from description).
  - **EL Failure:** Fails to extract "SQL" because it was listed in a non-standard format (e.g., "Strong SQL skills: required") that the ER model didn't recognize as a span.
  - **LLM Failure:** Hallucinates an ESCO code that looks valid but does not exist in the official v1.1.1 taxonomy.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run the `all-mpnet-base-v2` model on the Occupation dataset using the "Preferred Label + Description" concatenation strategy to verify the ~0.49 accuracy reported in Table 3.
  2. **ER Module Validation:** Train the `RoBERTa_base` model on the Green Benchmark to confirm the F1 > 54. This validates the gatekeeper for the EL pipeline.
  3. **Comparative Ablation:** On the Skills dataset, compare SL (full sentence embedding) vs. EL (extracted span embedding) to quantify the "noise reduction" effect claimed in Table 6.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specific prompt transformations or engineering strategies enable generative Large Language Models (LLMs) to outperform supervised linking methods for ESCO taxonomy mapping?
- **Basis in paper:** [explicit] The authors note that while current LLM experiments failed, "Possibly, there exists a prompt transformation that enhances information retrieval. This is a promising direction of future research."
- **Why unresolved:** Direct application of LLMs resulted in hallucinations and lower performance compared to supervised methods, leaving the potential of optimized prompting untapped.
- **What evidence would resolve it:** A comparative evaluation showing LLMs using optimized prompts achieving higher F1-scores than the current RoBERTa and Sentence Transformer baselines on the Green Benchmark.

### Open Question 2
- **Question:** Does incorporating the hierarchical structure of the ESCO taxonomy improve the accuracy of entity linking compared to treating entities as discrete nodes?
- **Basis in paper:** [explicit] Page 2 states that the study focuses "exclusively on discrete entities within ESCO and disregard[s] hierarchical relationships... We leave this aspect for future exploration."
- **Why unresolved:** Current models treat ESCO concepts as a flat list, ignoring the subclass relationships and broader/narrower connections inherent in the taxonomy.
- **What evidence would resolve it:** An architecture that leverages parent-child relationships during training or inference demonstrating improved Accuracy@1 over the non-hierarchical baselines.

### Open Question 3
- **Question:** How does the inclusion of diverse, non-UK job markets impact the clarity of results for qualification extraction methodologies?
- **Basis in paper:** [explicit] The authors found no clear advantage for qualification extraction and recommend: "For future research, it is recommended to expand the evaluation set to include qualifications from more diverse and general job markets."
- **Why unresolved:** The current qualification results may be skewed by the specific terminology of the UK job market and low inter-annotator agreement in the dataset.
- **What evidence would resolve it:** A new evaluation on a geographically diverse dataset where either Sentence Linking or Entity Linking demonstrates a statistically significant performance advantage.

## Limitations

- **Data Mapping Uncertainty:** The extension of the Green Benchmark to EQF qualifications uses an untested mapping heuristic between two distinct classification systems.
- **Task Generalization Risk:** Findings are based on Ethiopian job vacancy data and may not generalize to other labor markets with different linguistic patterns.
- **LLM Conclusion Limitation:** The claim that generative LLMs are unsuitable is based on zero-shot prompting without extensive instruction-tuning on ESCO-specific data.

## Confidence

- **High Confidence:** Comparative results of SL vs. EL for occupations and skills on the Ethiopian datasets.
- **Medium Confidence:** Conclusion that RoBERTa+CRF is a state-of-the-art ER model for this domain.
- **Medium Confidence:** Recommendation to use different architectures for different entity types (SL for occupations, EL for skills).
- **Low Confidence:** Broader applicability of these findings to other labor markets or the definitive unsuitability of generative LLMs without further experimentation.

## Next Checks

1. **Benchmark Generalization:** Evaluate the SL and EL pipelines on a job vacancy dataset from a different country or labor market to test if the context vs. isolation principle holds universally.

2. **ER Module Ablation:** Systematically test the impact of the Entity Recognition module's performance on the overall EL pipeline by training ER models with varying F1 scores and measuring the downstream effect on skill linking accuracy.

3. **LLM Fine-tuning Experiment:** Conduct a controlled experiment fine-tuning a generative LLM (e.g., Llama) on a subset of the Green Benchmark or a synthetic ESCO instruction dataset to test if the performance gap with encoders can be reduced.