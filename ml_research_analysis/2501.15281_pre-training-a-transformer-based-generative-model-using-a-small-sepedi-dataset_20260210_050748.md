---
ver: rpa2
title: Pre-training a Transformer-Based Generative Model Using a Small Sepedi Dataset
arxiv_id: '2501.15281'
source_url: https://arxiv.org/abs/2501.15281
tags:
- language
- text
- dataset
- sepedi
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses the challenge of developing language models
  for low-resourced languages, specifically focusing on Sepedi. The core method involves
  pre-training transformer-based generative models using two techniques: standard
  GPT-2 training and an occlusion-based approach, with a relatively small Sepedi dataset.'
---

# Pre-training a Transformer-Based Generative Model Using a Small Sepedi Dataset

## Quick Facts
- arXiv ID: 2501.15281
- Source URL: https://arxiv.org/abs/2501.15281
- Reference count: 40
- Primary result: Occlusion-based pre-training achieves slightly higher BLEU scores for Sepedi text generation despite worse validation loss metrics

## Executive Summary
This paper addresses the challenge of developing language models for low-resourced languages by pre-training transformer-based generative models for Sepedi, a Bantu language of South Africa. The authors compare standard GPT-2 training with an occlusion-based approach using a relatively small Sepedi dataset (63MB). After pre-training, both models are fine-tuned on Sepedi radio news data and evaluated on held-out test sets. The results show that while standard training achieves better validation loss and perplexity, the occlusion-based models produce slightly better text generation quality as measured by BLEU score. This establishes a new baseline for Sepedi language models and demonstrates the potential of occlusion-based training for robust text generation in low-resourced language settings.

## Method Summary
The authors pre-train two GPT-2 style generative models on the SepMono corpus (432,970 sentences, 11.36M tokens) using different approaches: standard next-token prediction and occlusion-based training with 30% token masking. Both models use a fine-tuned BPE tokenizer (vocabulary=50,225) trained on SepMono data. The standard model uses 8 layers and 8 attention heads, while the occlusion model uses 6 layers and 4 heads. Models are trained with AdamW optimizer (lr=2e-4, batch=512, dropout=0.3, weight decay=1e-2) for up to 100 epochs with early stopping. After pre-training, both models are fine-tuned on SepNews-1 radio news data (15,836 sentences) using gradual unfreezing, then evaluated on SepNews-2 (3,520 sentences) using validation loss, perplexity, and BLEU score.

## Key Results
- Non-occlusion models achieve better validation loss (2.69 vs 2.80) and perplexity (16.04 vs 31.89) during pre-training
- Occlusion-based models achieve slightly higher BLEU scores after fine-tuning (48.84% vs 44.98%)
- Gradual unfreezing during fine-tuning reduces validation perplexity by ~50% (31.89 → 16.48)
- Optimal occlusion probability determined to be 0.3 for this dataset size

## Why This Works (Mechanism)

### Mechanism 1: Occlusion-based pre-training
Occlusion-based pre-training may improve text generation quality (BLEU) even when degrading validation loss by forcing the model to learn more robust contextual representations. During training, tokens are randomly masked with probability 0.3, and the model must predict occluded tokens from surrounding context while also performing standard next-token prediction. This controlled noise may reduce overfitting to specific token sequences while encouraging broader semantic understanding. The trade-off between loss metrics and generation quality reflects different aspects of model capability, where occlusion forces learning recovery strategies that transfer to generation tasks.

### Mechanism 2: Gradual unfreezing
Gradual unfreezing during fine-tuning helps retain pre-trained knowledge while adapting to domain-specific data. Layers are frozen at fine-tuning start, with only top 2 layers unfreezing initially. Additional layers unfreeze progressively every 2 epochs from top to bottom. This prevents catastrophic forgetting by allowing later layers (task-specific) to adapt before earlier layers (general representations). The approach assumes earlier layers encode more generalizable linguistic patterns while later layers are more task-specific and should adapt first.

### Mechanism 3: BPE tokenization
BPE tokenization adapted to the target language improves handling of morphologically rich, low-resource languages. GPT2TokenizerFast is fine-tuned on SepMono corpus to learn language-specific subword patterns. BPE breaks words into frequent byte pairs, controlling vocabulary size while handling agglutinative morphology common in Sepedi. The approach assumes morphological structure of Sepedi can be captured through statistical subword patterns without explicit linguistic analysis.

## Foundational Learning

**Perplexity (PPL)**: Why needed here - Primary metric for language model quality; exponential of average negative log-likelihood. Lower is better. Quick check question: If a model achieves PPL=16, what does this mean about average branching factor per word? (Answer: ~16 equally likely choices per prediction)

**BLEU Score**: Why needed here - Measures n-gram overlap between generated and reference text; captures generation quality differently than loss metrics. Quick check question: Why might a model with worse perplexity achieve better BLEU? (Answer: Perplexity measures probability of exact sequences; BLEU tolerates synonyms/reordering via n-gram matching)

**Masked vs. Autoregressive Objectives**: Why needed here - Paper combines both; understanding the distinction explains why occlusion affects loss and generation differently. Quick check question: In GPT-2's standard training, can position i attend to position i+1? In occlusion training, what changes? (Answer: No—causal mask prevents this. Occlusion adds token masking but maintains causal structure)

## Architecture Onboarding

**Component map**: Input Text → BPE Tokenizer (vocab=50,225) → Token + Position Embeddings → N Decoder Blocks (6 for OCC, 8 for standard) → Masked Multi-Head Self-Attention (4-8 heads) → Feed-Forward Network → Layer Norm + Residual → Linear → Softmax → Next Token Prediction

**Critical path**: Tokenizer fine-tuning on SepMono → Pre-training on SepMono (80/10/10 split) → Fine-tuning on SepNews-1 → Evaluation on SepNews-2 (held-out time period)

**Design tradeoffs**:
| Choice | Benefit | Cost |
|--------|---------|------|
| Occlusion (p=0.3) | Higher BLEU (+3.86%) | Higher validation loss (+0.11) |
| Fewer layers/heads (OCC) | Regularization via noise | Reduced model capacity |
| Small dataset (63MB) | Feasible computation | Limited generalization |

**Failure signatures**:
- Validation loss plateaus early → model capacity insufficient or learning rate too low
- Test loss >> validation loss → distribution shift (temporal mismatch in SepNews-2)
- BLEU very low with coherent grammar → vocabulary/tokenization mismatch

**First 3 experiments**:
1. Replicate occlusion sweep with p∈{0.1, 0.3, 0.5} on validation loss; confirm 0.3 optimal for your data.
2. Ablate gradual unfreezing vs. full fine-tuning; measure catastrophic forgetting via pre-training task performance.
3. Test tokenizer: compare fine-tuned BPE vs. original GPT-2 tokenizer on OOV rate for Sepedi text.

## Open Questions the Paper Calls Out

**Open Question 1**: Does occlusion-based pre-training improve the generation of code-switched text compared to standard approaches? Basis: Authors explicitly state future work aims to analyze occlusion techniques' impact on generating code-switched text. Unresolved because current study focused exclusively on monolingual Sepedi data. Resolution would require evaluation on code-switched Sepedi-English corpora measuring semantic correctness and fluency.

**Open Question 2**: Does the higher BLEU score of occlusion models correlate with human-perceived quality better than the lower perplexity of non-occlusion models? Basis: Paper highlights trade-off where occlusion yields higher BLEU but worse perplexity, while human evaluation was omitted due to cost. Unresolved because it's unclear which metric better predicts actual text quality without human judgment. Resolution would require human evaluation ranking fluency and coherence of generated text samples.

**Open Question 3**: Can alternative fine-tuning strategies eliminate the performance gap in validation loss between occlusion and non-occlusion models? Basis: Authors note future work aims to experiment with other fine-tuning techniques. Unresolved because gradual unfreezing narrowed gap but occlusion models still suffered higher validation loss. Resolution would require experiments with discriminative fine-tuning or adapter-based approaches.

## Limitations
- Relatively small training dataset (63MB) constrains generalizability to other low-resourced languages
- Occlusion-based training shows mixed results with trade-off between optimization objectives and generation quality
- Specific implementation details of occlusion mechanism not fully specified, limiting reproducibility
- Evaluation relies heavily on BLEU score which may not capture all aspects of generation quality for morphologically rich languages

## Confidence

**High Confidence**: Pre-training generative models for low-resourced languages like Sepedi is feasible with limited data; standard GPT-2 training achieves better validation metrics than occlusion-based approaches; gradual unfreezing during fine-tuning is well-established in transfer learning literature.

**Medium Confidence**: Occlusion-based models achieve "slightly higher" BLEU scores indicating better text generation quality. While numbers show 3.86 percentage point improvement, paper lacks qualitative analysis or human evaluation to confirm improvement is meaningful; optimal occlusion probability of 0.3 based on limited experimentation.

**Low Confidence**: Generalizability of 0.3 occlusion probability across different low-resourced languages and corpus sizes. Paper doesn't test this hyperparameter extensively or provide theoretical justification for why this specific value works best.

## Next Checks

1. **Occlusion Implementation Verification**: Replicate exact occlusion mechanism by testing how occluded tokens are predicted (separate loss head vs joint prediction) and validate that 0.3 probability threshold holds across different dataset sizes and language families.

2. **Multi-metric Generation Evaluation**: Extend beyond BLEU to include ROUGE, BERTScore, and human evaluation of generated text quality, particularly focusing on morphological accuracy and semantic coherence in Sepedi.

3. **Cross-language Generalization Test**: Apply same occlusion-based pre-training approach to another morphologically rich low-resourced language (e.g., Xhosa or Zulu) with similar corpus size to verify if 0.3 occlusion probability remains optimal or if language-specific tuning is required.