---
ver: rpa2
title: 'Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal
  Predictive Distribution to Multivariate Settings'
arxiv_id: '2511.15146'
source_url: https://arxiv.org/abs/2511.15146
tags:
- distribution
- prediction
- conformal
- predictive
- transport
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of extending conformal prediction
  to multivariate outputs with vector-valued scores. The key innovation is a framework
  that leverages optimal transport (OT) to define a valid multivariate ranking, enabling
  finite-sample, distribution-free coverage guarantees.
---

# Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings

## Quick Facts
- **arXiv ID**: 2511.15146
- **Source URL**: https://arxiv.org/abs/2511.15146
- **Reference count**: 32
- **Key outcome**: Framework that uses optimal transport to construct valid multivariate prediction sets and CPDs with finite-sample calibration, avoiding data splitting and parametric assumptions.

## Executive Summary
This paper addresses the challenge of extending conformal prediction to multivariate outputs with vector-valued scores. The key innovation is leveraging optimal transport to define a valid multivariate ranking, enabling finite-sample, distribution-free coverage guarantees without scalarizing vector scores. By computing ranks via an augmented transport map that includes the candidate itself, the method preserves exchangeability and restores exact coverage. The computational bottleneck is overcome by proving the assignment is piecewise-constant across a polyhedral partition, reducing evaluation to fast cell lookups. The framework is extended to construct the first multivariate Conformal Predictive Distributions with finite-sample calibration, generalizing the Dempster-Hill procedure.

## Method Summary
The method "conformalizes" optimal transport quantile regions for multivariate scores. For each candidate prediction, it computes a rank via an augmented transport map on calibration scores plus the candidate. Coverage is ensured by exchangeability arguments. Computational tractability is achieved by proving the optimal assignment is piecewise-constant on a polyhedral partition of the score space, pre-computed by solving n+1 linear assignment problems. This partition enables fast inference through linear inequality checks. The framework extends to construct multivariate Conformal Predictive Distributions with exact finite-sample calibration by randomizing within Laguerre cells.

## Key Results
- First framework for multivariate conformal prediction sets that preserves exchangeability through augmented transport maps
- Proves optimal assignment is piecewise-constant on polyhedral partition, enabling tractable computation
- Constructs first multivariate Conformal Predictive Distributions with finite-sample calibration via semi-discrete OT with randomization
- Avoids data splitting, parametric assumptions, and curse of dimensionality of learning transport maps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including candidate in OT computation preserves exchangeability and restores finite-sample coverage
- Mechanism: Augmented transport map computed on calibration scores plus candidate ensures symmetric treatment
- Core assumption: Data points are exchangeable (permutation-invariant joint distribution)
- Evidence: [abstract] "crucial for preserving validity", [section 4.1.1] defines augmented distribution μ_Z, related work uses pre-fit maps with data splits
- Break condition: Non-exchangeable data (distribution shift, time series) may break coverage

### Mechanism 2
- Claim: Assignment is piecewise-constant on fixed polyhedral partition, reducing evaluation to cell lookups
- Mechanism: Pre-compute optimal costs C_k, define polyhedral cells R_j based on cost differences
- Core assumption: Target points are fixed discrete approximation of spherical uniform
- Evidence: [abstract] "piecewise-constant across polyhedral partition", [section 4.3, Proposition 4.4] shows polyhedral cell structure, neural OT approaches sacrifice finite-sample guarantees
- Break condition: Approximate solvers create approximate boundaries with uncharacterized coverage impact

### Mechanism 3
- Claim: Semi-discrete OT with randomization yields multivariate CPD with exact calibration
- Mechanism: Randomize within Laguerre cells to generalize 1D Dempster-Hill procedure
- Core assumption: Auxiliary randomness is independent of data
- Evidence: [abstract] "first multivariate CPDs with finite-sample calibration", [section 5.2, Theorem 5.1] proves PIT uniformity, related work focuses on sets not full distributions
- Break condition: Monotonicity proven only for residual scores in split-conformal setting

## Foundational Learning

- **Exchangeability**
  - Why needed: Coverage guarantees rest on exchangeability ensuring rank uniformity under permutation
  - Quick check: Why does splitting data into train/calibration/test preserve exchangeability but learning transport map on calibration before conformalization does not?

- **Optimal Transport and Pushforward**
  - Why needed: OT defines multivariate ranks via map pushing source to target uniform distribution
  - Quick check: Why does T_# P = U imply P(Z ∈ T^(-1)(B(0,r))) = U(B(0,r))—key to distribution-free quantile regions?

- **Probability Integral Transform (PIT)**
  - Why needed: CPD validity characterized as multivariate PIT requiring uniform transformed variable
  - Quick check: In 1D, why does empirical CDF F_n(Z_n+1) only achieve discrete uniformity, and how does randomization restore continuous uniformity?

## Architecture Onboarding

- **Component map**: Target grid construction -> Pre-compute polyhedral partition -> Query-time cell lookup -> Randomization (CPD)
- **Critical path**: Pre-computation of {C_k} requires O(n^4) with standard assignment solvers—dominates runtime for large calibration sets
- **Design tradeoffs**: Exact vs. approximate solvers (Sinkhorn reduces to O(n^3) but voids guarantees), conservative vs. randomized (non-randomized is conservative), residual vs. general scores (monotonicity proven only for residuals)
- **Failure signatures**: Coverage drops below 1-α (check exchangeability), unbounded prediction regions (verify r_α < 1), non-monotone predictive distribution (score may violate monotonicity conditions)
- **First 3 experiments**: 1) Sanity check on synthetic Gaussian verifying empirical coverage matches 1-α, 2) Ablation on calibration size measuring coverage and prediction set volume, 3) Comparison with scalarization baselines focusing on directional error discrimination

## Open Questions the Paper Calls Out

- **Open Question 1**: What are necessary and sufficient conditions on score function for cyclical monotonicity of predictive distribution map? (Explicit in sections 4.5 and 6, difficult characterization for general scores)
- **Open Question 2**: How do approximations in pre-computation (e.g., Sinkhorn algorithms) impact finite-sample calibration guarantees? (Explicit in section 6, consequences uncharacterized)
- **Open Question 3**: Can theoretical guarantees extend to full conformal prediction setting where model is retrained for every candidate? (Inferred from section 6, monotonicity results don't apply to full CP setting)

## Limitations
- **Computational Scalability**: O(n^4) pre-computation complexity creates fundamental bottleneck for n > 1000
- **Exchangeability Assumption**: Coverage guarantees critically depend on exchangeability with limited guidance on diagnosing violations
- **Monotonicity for General Scores**: Monotonicity proven only for residual scores in split-conformal setting; general scores remain open problem

## Confidence

- **High Confidence**: Theoretical validity of conservative prediction sets (Proposition 4.1, 4.2) and piecewise-constant structure (Proposition 4.4) with rigorous proofs
- **Medium Confidence**: Exact coverage guarantee for randomized CPD (Theorem 5.1) theoretically sound but practical implementation details underspecified
- **Low Confidence**: Claims about practical performance relative to baselines not empirically validated

## Next Checks

1. **Exchangeability Stress Test**: Systematically evaluate coverage under distribution shift (covariate shift, concept drift, temporal dependence) to quantify robustness
2. **Approximation Impact Study**: Compare coverage and runtime between exact assignment solvers and approximate methods (Sinkhorn, auction algorithm) across problem sizes
3. **Practical Performance Benchmark**: Implement full pipeline including grid construction heuristics and conduct experiments comparing OT-based sets against scalarization baselines on real-world multivariate regression tasks, measuring both coverage and sharpness