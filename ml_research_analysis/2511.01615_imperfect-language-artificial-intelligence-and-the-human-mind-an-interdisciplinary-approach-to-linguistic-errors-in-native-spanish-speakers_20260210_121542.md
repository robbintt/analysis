---
ver: rpa2
title: 'Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary
  Approach to Linguistic Errors in Native Spanish Speakers'
arxiv_id: '2511.01615'
source_url: https://arxiv.org/abs/2511.01615
tags:
- errors
- language
- llms
- linguistic
- spanish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This interdisciplinary study examined how native Spanish speakers\u2019\
  \ linguistic errors are interpreted by large language models (LLMs), integrating\
  \ theoretical linguistics, neurolinguistics, and NLP. A corpus of over 500 authentic\
  \ Spanish errors was compiled and tested against six leading LLMs (GPT-4, GPT-5,\
  \ Llama-3, Gemini, Grok, and DeepSeek) using a zero-shot error-correction prompt."
---

# Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers

## Quick Facts
- arXiv ID: 2511.01615
- Source URL: https://arxiv.org/abs/2511.01615
- Reference count: 0
- Primary result: LLMs achieved 100% accuracy on common Spanish errors like *haber* misuse and *dequeísmo*, but only 53–93% on subtle semantic and syntactic errors.

## Executive Summary
This interdisciplinary study examines how large language models interpret and correct linguistic errors made by native Spanish speakers. The research combines theoretical linguistics, neurolinguistics, and natural language processing to analyze over 500 authentic Spanish errors collected from social media. Six leading LLMs were tested using a zero-shot error correction prompt, revealing that while models handle common grammatical errors with perfect accuracy, they struggle significantly with context-dependent and subtle linguistic mistakes.

The findings highlight the limitations of current AI systems in processing complex language phenomena and underscore the need for linguistically informed NLP approaches specifically designed for Spanish. The study also establishes a framework for comparing human and machine error patterns, suggesting future research directions in dialectal analysis and multimodal error detection.

## Method Summary
The study compiled a corpus of 550+ authentic Spanish linguistic errors from Reddit, X, and Telegram, manually classified into four categories (spelling, syntactic, lexical, semantic) plus two Spanish-specific phenomena. A zero-shot error correction prompt was applied to six LLMs (GPT-4, GPT-5, Llama-3, Gemini, Grok, and DeepSeek) across 11 batches of 50 responses each. Performance was measured by accuracy rates per error category, with additional analysis of model rankings and failure patterns.

## Key Results
- LLMs achieved 100% accuracy on well-documented errors like *haber* misuse and *dequeísmo*
- Performance dropped to 53–93% on subtle spelling, syntactic, and semantic errors
- GPT-5 and DeepSeek performed best overall (87.45% and 86.15% accuracy, respectively)
- All models failed to detect determiner repetition and capitalization errors after punctuation
- Spanish underrepresentation in training data (0.77% vs 92.65% for English) contributed to performance gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve near-perfect accuracy on high-frequency, well-documented grammatical errors in Spanish.
- Mechanism: Statistical pattern internalization occurs when specific error types appear with sufficient frequency in training corpora, allowing models to recognize and correct them without explicit rules.
- Core assumption: Common errors like *haber* pluralization and *dequeísmo* appear frequently enough in web-scale training data to form stable statistical representations.
- Evidence anchors:
  - [abstract] "LLMs achieved 100% accuracy on well-documented errors like verb haber misuse and dequeísmo"
  - [section 4.3] "all evaluated models demonstrated 100% accuracy in detecting errors in the use of the verb 'haber' and in dequeísmo. This result suggests that the six models tested have internalized some of the most common errors in the Spanish language."
  - [corpus] Neighbor paper on low-resource languages confirms data scarcity directly limits model capability.
- Break condition: When errors are rare, dialect-specific, or require contextual interpretation beyond surface co-occurrence patterns.

### Mechanism 2
- Claim: LLM performance degrades significantly on subtle, context-dependent errors requiring pragmatic understanding.
- Mechanism: Errors demanding semantic disambiguation, stylistic appropriateness, or syntactic parallelism require processing that goes beyond next-token prediction—specifically, world knowledge and embodied cognition.
- Core assumption: The "bodilessness" of LLMs prevents deep semantic understanding that humans derive from lived experience.
- Evidence anchors:
  - [abstract] "performance dropped to 53–93% on subtler spelling, syntactic, and semantic errors"
  - [section 5.1] "The main divergence lies in the lack of embodied cognition in AI... LLMs are merely data-processing algorithms, they are devoid of this sensory and embodied foundation, preventing them from attaining a 'deep understanding of language'"
  - [corpus] "The Incomplete Bridge" paper documents systematic misalignment between AI research and psychological theory.
- Break condition: When context is explicitly provided through few-shot examples or chain-of-thought prompting (not tested in this study).

### Mechanism 3
- Claim: Spanish language underrepresentation in pretraining data creates systematic performance gaps.
- Mechanism: With only 0.77% of GPT-3's corpus in Spanish versus 92.65% in English, models lack sufficient exposure to Spanish morphosyntax, dialectal variation, and lexical nuance.
- Core assumption: Training data composition causally determines cross-linguistic capability disparities.
- Evidence anchors:
  - [section 2.3] "Spanish made up only 0.77% of GPT-3's training corpus, a stark contrast to English's 92.65%"
  - [section 2.3] Models "produce incorrect meanings for an important fraction of words and are not able to use most of the words correctly to write sentences with context"
  - [corpus] MultiNRC benchmark paper notes multilingual reasoning evaluation remains limited—confirming structural gaps persist.
- Break condition: When models receive targeted fine-tuning on linguistically-annotated Spanish corpora with explicit morphosyntactic features.

## Foundational Learning

- Concept: **Zero-shot prompting**
  - Why needed here: The study uses a zero-shot error correction prompt to evaluate LLMs without task-specific examples. Understanding this distinguishes baseline capability from what's achievable with few-shot or fine-tuned approaches.
  - Quick check question: Can you explain why zero-shot evaluation might underestimate model capability for complex error types?

- Concept: **Spanish morphosyntax** (*haber* impersonal, *dequeísmo*, *leísmo/laísmo*)
  - Why needed here: The paper's error taxonomy centers on Spanish-specific phenomena. You need this to interpret why 100% accuracy on these categories matters versus the 53% spelling floor.
  - Quick check question: Why would "*Habían muchas personas*" be incorrect in prescriptive Spanish, and what does this reveal about the model's internalized grammar?

- Concept: **Levelt's speech production model**
  - Why needed here: The neurolinguistic framework maps errors to production stages (conceptual → formulation → articulation). This provides the theoretical vocabulary for comparing human and AI error patterns.
  - Quick check question: At which stage of Levelt's model would a lexical substitution error ("dog" for "cat") originate, versus a phonological slip?

## Architecture Onboarding

- Component map:
  Corpus layer (550+ errors from social media) → Classification layer (4 error categories + 2 Spanish-specific phenomena) → Evaluation layer (zero-shot prompt + 6 LLMs) → Metrics layer (accuracy rates per category)

- Critical path:
  1. Social media data collection with error-focused filtering
  2. Manual classification using linguistic taxonomy
  3. Standardized prompt design (clarity + action + edge case handling)
  4. Batch inference across all six models
  5. Accuracy computation by error type and aggregate

- Design tradeoffs:
  - **Written-only corpus**: Excludes phonological errors (*lapsus linguae*)—limits neurolinguistic validity but improves reproducibility
  - **Zero-shot vs. few-shot**: May underestimate capability; paper acknowledges this limitation
  - **General-purpose models vs. fine-tuned GEC**: Study tests transfer capability rather than specialized systems; prior work shows fine-tuned smaller models can outperform general LLMs

- Failure signatures:
  - Models failing on determiner repetition: "Aquí abundan las faltas de ortografía y faltas de respeto" (all 6 missed the missing "las")
  - Capitalization after punctuation: "¡yo no veo alguno!"—none detected the lowercase "yo"
  - Subtle semantic substitutions: "alguno" vs. "ninguno"—only 1 of 6 models caught this

- First 3 experiments:
  1. **Replicate with few-shot prompting**: Add 3-5 annotated examples per error category to measure prompt-sensitivity lift (hypothesis: 5-15% improvement on semantic/lexical errors).
  2. **Dialectal stratification**: Partition corpus by dialect markers (seseo, ceceo, aspirated /s/) and evaluate per-dialect accuracy to identify systematic regional gaps.
  3. **Fine-tune LoRA adapter on synthetic error corpus**: Generate 5K synthetic Spanish error examples using the best-performing model (DeepSeek or GPT-5), then train a lightweight adapter—compare zero-shot vs. adapted performance on held-out errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs compensate for dialectal variations (e.g., *seseo* or [s] aspiration) in a manner comparable to human "perceptual confusion" mechanisms?
- Basis in paper: [explicit] The authors explicitly call for investigating dialectal variations and LLMs' capacity to compensate for the "perceptual confusion" these variations cause in humans (Section 6.5).
- Why unresolved: The study restricted its corpus to written social media text, excluding phonological speech errors and regional pronunciation variants.
- What evidence would resolve it: Experiments using transcriptions of spontaneous speech containing specific dialectal features to compare LLM compensation against human listener accuracy.

### Open Question 2
- Question: Do LLMs "reason" about linguistic errors using processes that mimic human neurolinguistic taxonomies (e.g., distinguishing phonological vs. lexical deficits)?
- Basis in paper: [explicit] The paper suggests applying evaluation metrics to compare the quality of LLM explanations with neurolinguistic taxonomies like Broca's and Wernicke's aphasia (Section 6.5).
- Why unresolved: Current evaluation focused solely on correction accuracy rather than analyzing the cognitive alignment of the model's explanatory output.
- What evidence would resolve it: A comparative study mapping LLM-generated error explanations against clinical neurolinguistic classifications for the same set of errors.

### Open Question 3
- Question: To what extent can LLMs detect errors that are strictly pragmatic or social violations rather than normative grammatical mistakes?
- Basis in paper: [explicit] The authors identify the need to design experiments testing LLMs' ability to detect errors that are mistakes only within a social or pragmatic context (Section 6.5).
- Why unresolved: The study's corpus focused on standard linguistic categories (spelling, syntax, semantics) without isolating errors of intentionality or social register.
- What evidence would resolve it: Testing models against a dataset of pragmatically incorrect utterances to see if they can identify the speaker's intentionality.

## Limitations

- Zero-shot evaluation framework may underestimate LLM capabilities for complex error types
- Corpus relies entirely on written text, excluding phonological errors significant in spoken Spanish
- Cross-model comparisons assume equivalent prompt sensitivity without empirical validation

## Confidence

- **High confidence**: 100% accuracy on *haber* misuse and *dequeísmo* errors across all models is well-supported by corpus evidence
- **Medium confidence**: Performance differences between models (87.45% for GPT-5 vs 86.15% for DeepSeek) are meaningful but may be prompt-sensitive
- **Low confidence**: Claims about LLMs' inability to achieve "deep understanding" due to lack of embodied cognition are speculative

## Next Checks

1. **Prompt Sensitivity Analysis**: Replicate error correction task using few-shot prompting with 3-5 annotated examples per error category to quantify performance lift from explicit demonstration versus zero-shot inference.

2. **Dialectal Performance Mapping**: Partition error corpus by identifiable dialectal features (seseo vs. ceceo, aspirated /s/, regional vocabulary) and measure per-dialect accuracy to identify systematic performance gaps across Spanish varieties.

3. **Fine-tuning Transfer Test**: Generate 5,000 synthetic Spanish error examples using the best-performing model, train a LoRA adapter on this synthetic corpus, then evaluate transfer to held-out authentic error set to measure benefit of specialized fine-tuning versus zero-shot generalization.