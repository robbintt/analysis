---
ver: rpa2
title: 'The View From Space: Navigating Instrumentation Differences with EOFMs'
arxiv_id: '2510.03316'
source_url: https://arxiv.org/abs/2510.03316
tags:
- data
- modality
- modalities
- each
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how sensor modality differences affect
  Earth Observation Foundation Models (EOFMs) by analyzing embeddings generated from
  different optical and SAR data streams. The authors compare Prithvi and DOFA models,
  representing typical and flexible EOFMs, respectively, using embeddings from Landsat-8,
  Landsat-9, HLS, Sentinel-2, and Sentinel-1 data over Indiana.
---

# The View From Space: Navigating Instrumentation Differences with EOFMs

## Quick Facts
- arXiv ID: 2510.03316
- Source URL: https://arxiv.org/abs/2510.03316
- Reference count: 6
- Primary result: Sensor modality creates distinct embedding clusters in EOFMs, limiting cross-sensor generalization.

## Executive Summary
This paper investigates how sensor modality differences affect Earth Observation Foundation Models (EOFMs) by analyzing embeddings generated from different optical and SAR data streams. The authors compare Prithvi and DOFA models, representing typical and flexible EOFMs, respectively, using embeddings from Landsat-8, Landsat-9, HLS, Sentinel-2, and Sentinel-1 data over Indiana. Through t-SNE visualizations, they show clear modality-based clustering in embedding spaces, indicating significant distribution shifts. Local neighborhood analysis reveals only 20-30% agreement in nearest neighbors across modalities, with modality-specific clustering remaining even after fine-tuning. The authors demonstrate that a Random Forest can predict input modality with 90% accuracy from embeddings. These findings highlight that sensor architecture profoundly impacts EOFM representations, necessitating careful consideration of pretraining data alignment and suggesting the need for more diverse, multimodal training datasets to build truly generalizable foundation models.

## Method Summary
The study analyzes embeddings from two EOFMs (Prithvi and DOFA) using 600 random 224×224 pixel patches (30m resolution) from Indiana, covering optical sensors (Landsat-8/9, HLS, Sentinel-2) and SAR (Sentinel-1). Using frozen pretrained encoders, the authors extract CLS tokens and patch embeddings, then apply t-SNE for global structure visualization and k-NN analysis for local neighborhood consistency. A Random Forest classifier (500 estimators) is trained to predict input modality from embeddings, achieving ~90% accuracy. The analysis reveals modality-based clustering in embedding spaces and only 20-30% agreement in nearest neighbors across different sensor types.

## Key Results
- t-SNE visualizations show clear modality-based clustering in embedding spaces
- Random Forest predicts input modality from embeddings with ~90% accuracy
- Local neighborhood analysis reveals only 20-30% agreement in nearest neighbors across modalities
- DOFA's flexible architecture shows only marginal improvement over Prithvi in modality prediction accuracy (88.7% vs 90.7%)

## Why This Works (Mechanism)

### Mechanism 1: Sensor-Specific Distribution Embedding
EOFM internal representations encode sensor hardware characteristics (modality) more strongly than semantic content, causing embeddings to cluster by satellite type rather than land cover. Optical and SAR sensors produce different statistical distributions (surface reflectance vs. backscatter) and processing stacks (atmospheric correction). The models encode these low-level statistical signatures as primary features in the latent space. Because variance between sensors (e.g., Landsat vs. Sentinel-2) exceeds variance within a single sensor's class labels, the embedding space segregates by modality first.

### Mechanism 2: Neighborhood Structure Sensitivity
Local semantic relationships (e.g., "crop similarity") are unstable across modalities, degrading performance for similarity search or few-shot tasks when query and database sensors differ. Embedding spaces organize "nearest neighbors" based on feature vectors. If a model weights spectral artifacts heavily, a "corn" pixel from Landsat will look closer to a "soybean" pixel from Landsat than a "corn" pixel from Sentinel-2. This breaks the assumption that semantic similarity is preserved across sensor boundaries.

### Mechanism 3: Dynamic Weight Modulation Limits
Flexible input handling (e.g., DOFA's dynamic embedding layer) reduces but does not eliminate sensor bias, likely because it relies on central wavelength metadata while ignoring spectral response function (SRF) differences. DOFA uses central wavelengths to generate input weights, treating bands as interchangeable based on spectral proximity. However, distinct sensor architectures (radiometric resolution, SRF shapes) create non-linear distribution shifts that simple wavelength alignment cannot correct.

## Foundational Learning

- **Distribution Shift / Domain Adaptation**
  - Why needed here: The paper is fundamentally about how data distributions change when switching sensors (Landsat to Sentinel) and how models fail to handle this. Understanding that $P_{train}(X) \neq P_{test}(X)$ is critical.
  - Quick check question: If I train a model on daylight images and test on night images, is this a covariate shift or a concept shift?

- **Embedding Space & Vector Similarity**
  - Why needed here: The analysis relies on visualizing (t-SNE) and querying (k-NN) the high-dimensional vectors produced by the models.
  - Quick check question: Why is cosine similarity often preferred over Euclidean distance for high-dimensional embedding comparisons?

- **Spectral Response Functions (SRF)**
  - Why needed here: The authors note that "matching bands by wavelength" fails. This is because two "Red" bands (e.g., 640nm) on different satellites have different widths and sensitivities (SRFs).
  - Quick check question: Why might two sensors measuring the exact same wavelength still output different Digital Numbers (DN) for the same target?

## Architecture Onboarding

- **Component map:** Multi-source Optical/SAR (HLS, Landsat, Sentinel-1/2) -> Encoder: Prithvi (ViT-based, HLS-specific) OR DOFA (Dynamic Linear Layer, wavelength-conditioned) -> Latent Space: 768-dim Embeddings -> Probes: t-SNE (Global Structure), k-NN/Random Forest (Local/Separability)

- **Critical path:** The evaluation pipeline depends on strict spatial-temporal alignment. You cannot compare embeddings if the ground truth (Crop Data Layer) or the image tiles (Landsat vs. Sentinel) are misaligned or captured in different seasons.

- **Design tradeoffs:**
  - Prithvi (Rigid): Optimized for specific HLS data → High performance on HLS, poor generalization to other optical sensors
  - DOFA (Flexible): Accepts any band configuration → Easier data ingestion, but "flexibility" is shallow (input layer only), leading to only marginal improvements in sensor invariance

- **Failure signatures:**
  - Modality Leakage: High accuracy (>85%) of a simple classifier (RF) predicting the sensor type from the embedding vector
  - Cluster Fragmentation: Semantic classes (e.g., "Corn") appearing as distinct sub-clusters based on sensor ID in t-SNE plots

- **First 3 experiments:**
  1. Leakage Test: Extract embeddings from your EOFM on a held-out validation set containing mixed sensors. Train a linear classifier to predict the sensor ID. If accuracy > 25% (random for 4 sensors), the model is not sensor-invariant.
  2. Cross-Modal Retrieval: Select a query patch from Sensor A (e.g., Sentinel-2). Retrieve top-5 nearest neighbors from a database of Sensor B (e.g., Landsat). Visually inspect if they match semantically or just radiometrically.
  3. Input Ablation: Feed noise statistics (mean/variance of bands) from Sensor A into the model alongside data from Sensor B to see if the model decision boundary shifts purely based on statistical properties of the input distribution.

## Open Questions the Paper Calls Out

- How does task-specific fine-tuning alter the modality-specific clustering observed in the embedding spaces of EOFMs? The authors state, "Many interesting questions can be further explored in the realm of understanding the impact fine-tuning has on the weights and representations... We propose these as future work."

- To what degree does the observed 20-30% local neighborhood agreement translate into performance drops for downstream tasks? The paper poses "downstream performance implications in specific cases" as an open area for future work, noting that in similarity search, "input modality becomes critically to downstream performance."

- Do the observed sensor-induced distribution shifts persist across different geographic regions and biomes? The methodology relies on a dataset composed of 600 samples solely from the US State of Indiana, leaving it unclear if clustering effects vary with different land cover types.

## Limitations

- Temporal alignment sensitivity: The analysis assumes 2-month mosaics are sufficient to smooth temporal effects, but seasonal changes could create artificial modality-based clustering even for identical sensors.
- CRF/SRF modeling gaps: The paper doesn't account for complete sensor spectral response function differences beyond central wavelength matching, which could explain persistent modality prediction accuracy.
- Generalizability constraint: Results are based on a single geographic region (Indiana) with specific crop types, limiting broader claims about EOFM behavior globally.

## Confidence

- **High Confidence:** Sensor modality prediction from embeddings (90% accuracy) - direct empirical result with clear methodology
- **Medium Confidence:** Semantic neighbor stability claims (20-30% agreement) - based on specific k-NN thresholds but sensitive to implementation details
- **Low Confidence:** DOFA flexibility claims - marginal improvement over Prithvi suggests the mechanism is incomplete, but the paper doesn't explore why wavelength mapping alone is insufficient

## Next Checks

1. Cross-Regional Validation: Repeat the analysis on a non-agricultural region (e.g., Mediterranean coast) to test if modality clustering persists across different land cover types.
2. Temporal Ablation Study: Compare embedding distributions for sensors with identical temporal coverage but different modalities to isolate sensor vs. seasonal effects.
3. SRF-Aware Input Transformation: Implement spectral response function normalization before feeding data into DOFA and measure changes in modality prediction accuracy and neighbor consistency.