---
ver: rpa2
title: 'LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment'
arxiv_id: '2601.19487'
source_url: https://arxiv.org/abs/2601.19487
tags:
- layer
- vector
- llm-v
- benign
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-VA resolves the jailbreak-overrefusal trade-off in safety-aligned
  LLMs by aligning the answer vector with the benign vector through closed-form weight
  updates. This approach makes the model's willingness to answer causally dependent
  on its safety assessment, rather than treating them as independent processes.
---

# LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment

## Quick Facts
- **arXiv ID**: 2601.19487
- **Source URL**: https://arxiv.org/abs/2601.19487
- **Reference count**: 40
- **Primary result**: Resolves jailbreak-overrefusal trade-off via vector alignment, achieving 11.45% higher F1 scores while preserving 95.92% utility across 12 LLMs.

## Executive Summary
LLM-VA introduces a novel approach to resolving the safety-utility trade-off in aligned LLMs by aligning the answer vector with the benign vector through closed-form weight updates. The method leverages the observation that LLMs encode answer decisions and safety assessments as nearly orthogonal directions in latent space, treating them as independent processes that cause both jailbreak and over-refusal failures. By making the model's willingness to answer causally dependent on its safety assessment, LLM-VA achieves significant improvements in F1 score (11.45% higher than best baseline) while maintaining 95.92% utility preservation across 12 diverse LLMs ranging from 3B to 14B parameters.

## Method Summary
LLM-VA works by first extracting two key vectors from each layer of the LLM using linear SVMs: the answer vector (v_a) that controls whether the model answers, and the benign vector (v_b) that encodes safety judgment. The method then applies closed-form weight updates to down-projection matrices to align v_a with v_b, making answering causally dependent on safety assessment. The alignment is performed iteratively across selected layers, with vector extraction and weight updates repeated until optimal validation performance is achieved. The approach requires no fine-tuning and automatically adapts to each model's safety bias without manual hyperparameter tuning.

## Key Results
- Achieves 11.45% higher F1 scores than the best baseline across 12 LLMs
- Preserves 95.92% utility while improving safety metrics
- Outperforms baselines (SEAL, PG-3, VectorSteer) on both S-Eval-Attack/Risk (jailbreak) and ORFuzzSet/NaturalQuestions (over-refusal) datasets
- Demonstrates automatic adaptation to different models' safety biases without manual tuning

## Why This Works (Mechanism)

### Mechanism 1
LLMs encode answer decisions and safety assessments as nearly orthogonal (~90°) directions in latent space, causing independent processing that produces both jailbreak and over-refusal failures. The answer vector (v_a) controls whether the model answers; the benign vector (v_b) encodes safety judgment. Near-orthogonality means a toxic input can have positive v_a projection (answered—jailbreak) while a benign input can have negative v_a projection (refused—over-refusal), because the two decisions are decoupled. This relies on the representation engineering hypothesis that linear directions in hidden states causally influence output behavior.

### Mechanism 2
Linear SVMs trained on layer outputs can extract v_a and v_b by finding maximum-margin hyperplanes that separate the relevant classes. For each layer, two SVMs are trained: one for benign vs. toxic inputs to extract v_b, and another for answered vs. refused samples to extract v_a. The unit-normalized SVM weight vector gives the control direction. This approach assumes decision boundaries are approximately linear and pass through origin in the hidden space, providing robustness through margin maximization.

### Mechanism 3
Closed-form pseudoinverse-based weight updates align v_a with v_b, making answering causally dependent on safety assessment without fine-tuning. The method modifies the down-projection matrix W so that x(W+∆)v_a ≈ (σ_a/σ_b)·xWv_b for all inputs, using the Moore-Penrose pseudoinverse to find the minimum-norm solution. The process iterates because modifying earlier layers shifts inputs to later layers, changing their effective v_a/v_b directions. This assumes modifying MLP/attention down-projection matrices suffices to control downstream behavior while preserving utility.

## Foundational Learning

- **Linear representation hypothesis in transformers**: Why needed - Entire method assumes concepts like "answer" and "safety" are linear directions in hidden states; without this, SVM extraction and alignment are meaningless. Quick check - If you project layer outputs onto a candidate v_a, do answered and refused samples separate with a boundary near zero?

- **Maximum-margin classification (SVMs)**: Why needed - Vector extraction uses linear SVMs; understanding that the weight vector defines the decision direction is essential. Quick check - For a linear SVM with weight w, what does sign(w·x) predict, and why does margin maximization help robustness?

- **Moore-Penrose pseudoinverse for minimum-norm solutions**: Why needed - Weight update requires solving underdetermined systems; pseudoinverse yields the smallest modification achieving alignment. Quick check - Given underdetermined Ax = b, what property does x = A⁺b satisfy among all solutions?

## Architecture Onboarding

- **Component map**: Data preparation -> Vector extraction -> Layer scoring -> Weight updater -> Iteration loop -> Validation
- **Critical path**: Extract vectors → Score and select layers → Compute and apply weight update → Re-extract → Repeat (typically 20-30 iterations) → Select best validation F1 model
- **Design tradeoffs**: L_select (layer count) - too few → limited alignment; too many → utility drops sharply when early layers are modified. Iteration count T - model-specific; some peak then degrade (over-modification), others keep improving. Training data quality directly affects vector accuracy.
- **Failure signatures**: Utility drops >10% - likely selecting too many layers, including early representation-critical layers. F1 improves then degrades with more iterations - over-modification, use early stopping on validation. High cross-model variance - different architectures need different T and L_select, no universal hyperparameters.
- **First 3 experiments**: 1) Verify orthogonality in your model by extracting v_a and v_b at all layers and computing angles to confirm ~90° pattern exists. 2) Perform layer selection ablation by varying L_select from 30 to 60 on one model, plotting F1 and utility to identify the knee point. 3) Run full iteration loop (T=30) on 3 diverse models, plotting validation F1 per iteration to observe convergence patterns and determine early-stopping strategy.

## Open Questions the Paper Calls Out

### Open Question 1
The effectiveness of LLM-VA on larger models (e.g., 70B+) remains to be validated. The study only validates models ranging from 3B to 14B parameters; larger models may possess different internal representations or require different hyperparameter settings.

### Open Question 2
Vector steering methods, including LLM-VA, are difficult to apply to LLMs with chain-of-thought reasoning due to computational expense and reasoning randomness. Current vector identification relies on direct output correlations, but CoT introduces intermediate reasoning steps that complicate the extraction of stable control vectors.

### Open Question 3
The binary benign/toxic classification assumption limits the method's ability to handle nuanced toxicity. Real-world toxicity is nuanced and multi-dimensional, but the current SVM-based method identifies a single "benign vector," making it unable to distinguish between different categories of harmful content.

### Open Question 4
Improving transferability remains future work after observing that performance on unseen datasets varies by task and model. The alignment appears sensitive to the specific distribution of the training data, limiting its application as a "plug-and-play" safety solution.

## Limitations
- Limited architectural diversity - evaluation covers only Transformer-based LLMs from 3B to 14B parameters
- Iteration sensitivity - some models degrade after improvement, requiring careful early stopping
- Binary toxicity assumption - cannot handle nuanced or multi-dimensional harmful content
- Unknown generalizability to chain-of-thought models and larger 70B+ parameter models

## Confidence
- **High Confidence**: The orthogonality observation (v_a ⊥ v_b at ~90°), SVM-based vector extraction methodology, and closed-form pseudoinverse weight update procedure
- **Medium Confidence**: The claim that LLMs encode answer and safety decisions as independent processes, and the overall method's effectiveness in resolving the jailbreak-overrefusal trade-off
- **Medium Confidence**: The 11.45% F1 improvement claim depends on specific evaluation setup and baselines used

## Next Validation Checks
1. **Orthogonality validation**: Extract v_a and v_b vectors across all layers in your target model and verify the ~90° orthogonality pattern exists before applying alignment - this is a necessary precondition for the method to apply.
2. **Layer selection sensitivity**: Perform ablation studies varying L_select from 30 to 60 layers, monitoring both F1 and utility preservation to identify the knee point where utility begins dropping sharply.
3. **Iteration stability analysis**: Run full iteration loops (T=30) on 3 diverse models from your domain, plotting validation F1 per iteration to determine whether your models exhibit convergence, peak-then-drop, or continuous improvement patterns for appropriate early stopping.