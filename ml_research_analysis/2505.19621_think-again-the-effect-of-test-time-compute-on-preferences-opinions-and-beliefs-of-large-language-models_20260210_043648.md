---
ver: rpa2
title: Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and
  Beliefs of Large Language Models
arxiv_id: '2505.19621'
source_url: https://arxiv.org/abs/2505.19621
tags:
- answer
- topics
- responses
- question
- pobs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces the Preference, Opinion, and Belief Survey
  (POBs) benchmark to evaluate subjective biases in large language models (LLMs) across
  societal, cultural, ethical, and personal topics. The benchmark uses Likert-scale
  questions and measures reliability, neutrality, and consistency.
---

# Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models

## Quick Facts
- arXiv ID: 2505.19621
- Source URL: https://arxiv.org/abs/2505.19621
- Reference count: 21
- Primary result: Reasoning and self-reflection techniques show limited improvement in LLM neutrality and consistency, with newer models exhibiting stronger biases

## Executive Summary
This paper introduces the Preference, Opinion, and Belief Survey (POBs) benchmark to evaluate subjective biases in large language models across societal, cultural, ethical, and personal topics. The benchmark uses Likert-scale questions and measures reliability, neutrality, and consistency. While reasoning and self-reflection techniques show limited improvement in model behavior, newer LLM versions tend to exhibit stronger biases and reduced consistency compared to older ones. The analysis reveals that most models lean toward progressive-collectivist viewpoints and often underestimate their own biases.

## Method Summary
The study evaluates 10 language models using three prompting approaches (Direct, Reasoning, Self-reflection) across 20 topics with 12-38 Likert-scale questions each. Models are prompted n=5 times per question using default API parameters. The POBs benchmark measures Reliability (R), Non-Neutrality Index (NNI), and Topical Consistency Index (TCI). Responses are parsed from XML tags, with refusals handled as imaginary values for reliability calculations. Ideological mapping is performed using correlation analysis on progressivism-conservatism and individualism-collectivism axes.

## Key Results
- Reasoning and self-reflection techniques offer only limited gains in neutrality and consistency for subjective domains
- Newer model versions within families show stronger biases (higher NNI) and reduced consistency (lower TCI) than older versions
- A strong negative correlation (r ∼ 0.9) exists between non-neutrality and topical consistency
- Most models lean toward progressive-collectivist viewpoints and tend to underestimate their own biases

## Why This Works (Mechanism)

### Mechanism 1: Test-Time Reasoning Effects on Subjective Domains
Reasoning mechanisms that improve performance on objective tasks show limited gains for neutrality and consistency in subjective domains. When models reason through Chain-of-Thought on subjective questions, the extended computation may heighten sensitivity to implicit biases or reveal internal conflicts, destabilizing responses rather than clarifying them.

### Mechanism 2: Self-Reflection for Opinion Revision
Self-reflection produces measurable but inconsistent opinion shifts and does not reliably improve neutrality or consistency. Models are prompted to review their initial reasoning and reconsider answers; some models shift opinions (0-8% substantial shifts), while others show near-zero change. Newer models within families show lower tendency to shift opinions.

### Mechanism 3: Neutrality-Consistency Trade-off
Models that express stronger opinions (higher non-neutrality) are less consistent across related questions; this trade-off is inherent and strong (r ∼ 0.9). Expressing strong stances requires the model to navigate nuanced, context-dependent reasoning, increasing internal tension and reducing topical consistency.

## Foundational Learning

- **Likert-scale polarity mapping**: The benchmark uses numerical polarity values (-1 to +1) to quantify stance strength; understanding this is essential for interpreting NNI and TCI metrics.
  - Quick check: Given a topic "Pro-Choice vs. Pro-Life" where Pro-Choice is assigned negative polarity, what does an average polarity of -0.7 indicate?

- **Test-time compute**: The paper evaluates reasoning and self-reflection as forms of increased test-time compute; distinguishing this from training-time compute clarifies why results differ across domains.
  - Quick check: How does increasing test-time compute differ from scaling model parameters?

- **Reference-free evaluation**: POBs is designed as a reference-free benchmark—it does not require human baselines to assess model stances. This enables direct auditing without demographic comparison.
  - Quick check: Why might a reference-free benchmark be preferred for auditing model ideologies?

## Architecture Onboarding

- **Component map**: POBs benchmark (20 topics, 12-38 questions each) -> Three prompting approaches (Direct, Reasoning, Self-reflection) -> Response parsing with XML tags -> Metrics computation (R, NNI, TCI) -> Ideological mapping and correlation analysis

- **Critical path**: 1. Run each question n=5 times per model per prompting technique 2. Parse responses and assign polarity values 3. Compute R, NNI, TCI per model/topic 4. Aggregate for ideological mapping and correlation analysis

- **Design tradeoffs**: Using default sampling parameters simulates real-world deployment but increases variance; "Refused" responses treated with imaginary polarity (0.5i) to distinguish from neutral but adds complexity; questions generated via Llama-3.3-70B may inherit generator biases

- **Failure signatures**: High invalid response rate (>7%) suggests prompt formatting issues; large divergence between Direct and Reasoning responses indicates reasoning instability; newer model versions showing higher NNI and lower TCI than predecessors signals regression

- **First 3 experiments**: 1. Replicate reliability analysis on subset of 3 models to validate metric computation 2. Run Declarative POBs on 2 models and compare self-reported vs. revealed stances 3. Apply hierarchical clustering on topical correlations to confirm topic groupings for one model family

## Open Questions the Paper Calls Out

- **Open Question 1**: Does training a Large Language Model (LLM) for neutrality on one controversial topic promote neutrality on related or opposing topics? The authors explicitly state this was not explored and note that if neutrality generalizes across controversies, it could reduce training costs and improve safety.

- **Open Question 2**: Do the opinions and stances declared by models in survey settings generalize to their actual behavior in recommendation or advisory scenarios? The authors note that stances don't necessarily imply consistent behavior when providing recommendations and plan to curate a benchmark to assess this.

- **Open Question 3**: To what extent do varying prompt structures influence model neutrality, refusal rates, and stance consistency? The paper states that reliance on specific prompting techniques may shape behavior in ways that don't generalize and suggests future studies should investigate varying prompt structures.

- **Open Question 4**: Do the subjective response distributions of LLMs conform to or significantly deviate from human societal norms? The authors state that determining this would require a human benchmark for comparison, which their reference-free methodology was designed to avoid.

## Limitations

- The impact of model training data shifts on POBs results remains unclear, as the study cannot definitively separate architectural improvements from training data composition effects
- Using Llama-3.3-70B as the question generator introduces potential circularity, as questions may inherit ideological patterns detected in evaluated models
- The study's focus on English-language models limits generalizability to multilingual contexts and cultural variations

## Confidence

- **High Confidence**: The core finding that reasoning and self-reflection show limited gains in subjective domains is well-supported by direct experimental evidence
- **Medium Confidence**: The ideological mapping onto progressivism-conservatism and individualism-collectivism axes relies on correlation analysis with interpretive decisions
- **Low Confidence**: Claims about specific mechanisms by which reasoning destabilizes subjective responses are largely theoretical without direct causal evidence

## Next Checks

1. Apply POBs to multilingual models and compare ideological patterns across languages to test generalizability of findings
2. Train model variants with systematically varied training data distributions and fine-tuning objectives, then evaluate their POBs performance to isolate effects of training content versus architecture
3. Create an alternative version of POBs using human-generated questions and compare results with the current benchmark to quantify potential generator bias effects