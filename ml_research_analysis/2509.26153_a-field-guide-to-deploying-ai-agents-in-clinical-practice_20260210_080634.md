---
ver: rpa2
title: A Field Guide to Deploying AI Agents in Clinical Practice
arxiv_id: '2509.26153'
source_url: https://arxiv.org/abs/2509.26153
tags:
- clinical
- data
- governance
- project
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A key outcome of this field guide is the demonstration that successful
  deployment of generative AI agents in clinical practice requires shifting focus
  from algorithmic development to the sociotechnical work of implementation. Through
  deployment of an immune-related adverse event detection agent at Mass General Brigham,
  the authors found that less than 20% of effort was spent on model development, while
  over 80% went to data integration, validation, economic value, drift management,
  and governance.
---

# A Field Guide to Deploying AI Agents in Clinical Practice

## Quick Facts
- arXiv ID: 2509.26153
- Source URL: https://arxiv.org/abs/2509.26153
- Reference count: 40
- Primary result: Successful clinical AI deployment requires shifting focus from algorithmic development to sociotechnical implementation, with >80% of effort going to data integration, validation, governance, and infrastructure rather than model development.

## Executive Summary
This field guide presents a comprehensive framework for deploying generative AI agents in clinical practice, based on real-world implementation of an immune-related adverse event detection system at Mass General Brigham. The authors demonstrate that successful deployment requires fundamentally different skills and infrastructure than traditional model development, with the majority of effort dedicated to sociotechnical implementation work. The guide provides concrete metrics, gate criteria, and implementation frameworks to help institutions bridge the gap between AI pilot projects and routine clinical use.

## Method Summary
The authors deployed a multi-agent LLM system to detect immune-related adverse events from clinical notes in patients receiving immune checkpoint inhibitors. The system used a 4-phase validation cycle: retrospective gold-standard validation achieving F1=0.91, 3-month silent mode prospective testing, limited field study rollout, and full deployment. Implementation occurred within a HIPAA-compliant Azure environment using Snowflake research enclave for data integration, with daily batch processing and human-in-the-loop verification. The project followed staged gate criteria including F1≥0.75, precision≥0.80, median time-to-alert<48 hours, and override rate<10%.

## Key Results
- Less than 20% of project effort was dedicated to prompt engineering and model development, while over 80% went to data integration, stakeholder alignment, regulatory navigation, and workflow integration
- The staged validation approach with explicit gate metrics reduced clinical risk by surfacing failure modes before widespread deployment
- Infrastructure investments that separate organizational foundations from project-specific work enabled reuse and accelerated deployment velocity

## Why This Works (Mechanism)

### Mechanism 1
Clinical AI deployment success depends primarily on sociotechnical implementation infrastructure, not algorithmic sophistication. The 80/20 distribution—where >80% of effort goes to data integration, validation, economics, drift management, and governance—creates a dependency: without organizational infrastructure investments, project-level work cannot proceed efficiently. Centralized data warehouses, security templates, and governance frameworks eliminate redundant setup for each project.

### Mechanism 2
Staged validation with explicit gate metrics reduces clinical risk by surfacing failure modes before widespread deployment. The four-phase progression (retrospective gold standard → silent mode → limited field study → full deployment) adds real-world complexity incrementally. Each phase maintains human oversight while exposing the system to progressively more realistic conditions, allowing detection of hallucinations, context loss, and workflow friction at controlled scale.

### Mechanism 3
Separating organizational infrastructure from project-specific work enables reuse and accelerates deployment velocity. Organizational investments (centralized Snowflake enclave, Azure landing zones, security templates, data concierge team) create shared foundations. Projects then build in isolated sandboxes with preconfigured compliance, reducing each project's setup from months to weeks. RACI matrices pre-allocate decision authority, eliminating ad-hoc approval bottlenecks.

## Foundational Learning

- **OMOP/i2b2 Common Data Models**: Understanding how structured and unstructured clinical data map to standardized schemas is prerequisite for data integration work. Quick check: How would a clinical progress note be represented differently in OMOP's NOTE table versus a raw EHR extract?
- **Human-in-the-Loop (HITL) System Design**: The entire deployment model assumes human verification of AI outputs before clinical action. Understanding override behavior, alert fatigue thresholds, and interface design is central to achieving the 2% override rate reported. Quick check: What factors would cause you to set a stricter override rate threshold (<5%) versus a more permissive one (<15%)?
- **LLM-Specific Failure Modes**: Unlike traditional ML drift, LLM drift manifests as behavioral changes—fabricating evidence, losing context in long inputs, or agreeing with incorrect user assertions. Monitoring must detect these qualitatively different failures. Quick check: How would you distinguish between a model hallucination (fabricated evidence) and a correct inference from implicit clinical context?

## Architecture Onboarding

- **Component map**: Epic EHR → daily ETL → Snowflake Research Enclave (bronze/silver/gold medallion) → project-specific sandbox → Azure Function Orchestrator → Container Apps (Extractor, Predictor) → Blob Storage (private endpoints) → Azure OpenAI API with HIPAA BAA
- **Critical path**: 
  1. Submit BAA/security tickets Day 1—they can take months. Define PHI handling (prompts vs. fine-tuning).
  2. Verify cohort completeness and note latency in sandbox before building inference pipeline.
  3. Build dual-annotated gold standard, achieve gate metrics (F1 ≥0.75, Precision ≥0.80).
  4. Run 3-month silent mode prospective logging without clinician alerts; validate internally.
  5. Limited user access with HCI studies before full deployment.
- **Design tradeoffs**:
  - Latency vs. complexity: Paper recommends batch processing (24-36hr) for most use cases; real-time adds ~10% value for 10x complexity.
  - Temperature setting: Lower temperature improves reproducibility but may reduce output richness for nuanced clinical reasoning.
  - Single-vendor vs. best-of-breed: Consider lock-in risks vs. integration ease; API version pinning becomes critical with vendor updates.
  - Cost allocation: Centralize fixed infrastructure (CapEx) across projects; allocate variable API costs (OpEx) to individual projects.
- **Failure signatures**:
  - Override rate >10%: Indicates poor model calibration, unclear output design, or workflow mismatch.
  - Hallucination in evidence attribution: Model cites clinical facts not present in source notes—requires manual review to detect.
  - Silent vendor API changes: Performance degrades without code changes; requires version pinning and weekly gold-set rescoring.
  - Security ticket delays blocking timeline: Should be submitted before project start, not after validation completes.
- **First 3 experiments**:
  1. Sandbox cohort validation: Query research enclave for target patient cohort (e.g., ICI recipients in prior 12 months); verify note types, completeness, and 24-36hr latency before building inference pipeline.
  2. Gate metric baseline: Run retrospective inference on held-out gold standard; confirm F1 ≥0.75 and Precision ≥0.80 before any prospective testing.
  3. Cost/latency instrumentation: Deploy logging for per-note inference cost (~$2/100 notes reported) and processing latency before scaling volume; establish baseline for drift detection.

## Open Questions the Paper Calls Out

### Open Question 1
What validated methods can reliably detect model and data drift in generative clinical AI agents, where drift may manifest as hallucinations or context loss rather than metric degradation? The authors state there are currently no widely accepted or validated approaches for detecting model or data drift in these systems, and that drift manifests as context loss or hallucination rather than simple metric degradation. Prospective studies comparing multiple drift detection approaches against clinician-identified failures over extended deployment periods would resolve this.

### Open Question 2
How effective is LLM-as-judge monitoring for detecting emergent failure modes in clinical agent outputs compared to human expert review? The authors are evaluating the efficacy of direct LLM-as-judge monitoring of the LLM's live output behavior, including automated tracking of failure modes such as factual consistency, tone, and hallucination rates. Head-to-head comparison of LLM-as-judge versus human clinician review on live production outputs, measuring sensitivity/specificity for detecting clinically significant errors, would resolve this.

### Open Question 3
What standardized frameworks can quantify the total economic value of clinical LLM agents, accounting for monitoring overhead, refactoring costs, and non-linear labor effects? The authors note that generative outputs complicate value quantification with subjective quality components and that reclaiming 10% of a clinician's time doesn't necessarily equate to 10% cost savings. Multi-site implementation studies tracking full lifecycle costs versus realized benefits using consistent accounting methodologies would resolve this.

### Open Question 4
What governance models best balance rapid iteration with safety assurance when LLM vendor updates can silently invalidate prior safety evidence? The authors state that silent vendor upgrades can invalidate prior safety evidence, requiring agile re-validation. Comparative analysis of governance approaches across institutions with different update cadences, measuring time-to-approval, safety incident rates, and innovation velocity, would resolve this.

## Limitations

- The 80/20 ratio of sociotechnical to algorithmic effort is based on a single institutional case study and may not generalize across healthcare systems with different infrastructure maturity
- The specific gate criteria (F1≥0.75, precision≥0.80, etc.) are task-dependent and may not apply to all clinical AI applications
- The paper's focus on LLM-based agents may not translate to other AI approaches or clinical domains with different data characteristics

## Confidence

- **High Confidence**: The staged validation approach with explicit gate metrics is well-supported by the case study and aligns with established clinical trial principles
- **Medium Confidence**: Infrastructure investment tiers are institutionally specific, though the principle of separating organizational infrastructure from project work is valid
- **Low Confidence**: The claim that model development accounts for "less than 20%" of effort may be task-dependent and could shift for applications requiring extensive prompt engineering

## Next Checks

1. **Infrastructure Readiness Assessment**: Evaluate whether your institution's existing data warehouse, security templates, and governance frameworks align with the Foundational ($0.6-1M) tier requirements before committing to AI projects

2. **Gold Standard Creation Pilot**: Conduct a small-scale test of dual-annotation with physician adjudication on 50-100 clinical notes to measure inter-annotator agreement and estimate the 6-month curation timeline

3. **Vendor Lock-in Analysis**: Assess the trade-offs between API version pinning (for stability) versus flexibility to adopt newer models, including cost projections for both approaches over a 2-year horizon