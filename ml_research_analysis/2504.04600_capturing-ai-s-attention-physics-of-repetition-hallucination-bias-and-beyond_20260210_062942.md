---
ver: rpa2
title: 'Capturing AI''s Attention: Physics of Repetition, Hallucination, Bias and
  Beyond'
arxiv_id: '2504.04600'
source_url: https://arxiv.org/abs/2504.04600
tags:
- attention
- token
- output
- which
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper derives a first-principles physics theory of the Attention
  mechanism at the heart of LLMs, revealing it as a 2-body Hamiltonian spin-bath system.
  The theory enables quantitative analysis of AI challenges including output repetition,
  hallucination, harmful content, and bias from training.
---

# Capturing AI's Attention: Physics of Repetition, Hallucination, Bias and Beyond

## Quick Facts
- arXiv ID: 2504.04600
- Source URL: https://arxiv.org/abs/2504.04600
- Reference count: 0
- Primary result: Derives a first-principles physics theory mapping Attention to a 2-body Hamiltonian spin-bath system, explaining LLM phenomena like repetition, hallucination, and bias.

## Executive Summary
This paper presents a first-principles physics theory that maps the basic Attention mechanism in LLMs to a 2-body Hamiltonian spin-bath system. The theory reveals how training shapes the interaction matrix Weff, and how the Context Vector N(0) acts as a mean-field average over token spins. This framework quantitatively explains phenomena like output repetition (attractor dynamics in embedding space), hallucination (phase boundary crossing between "good" and "bad" tokens), and bias-induced harmful content generation (perturbation of the Hamiltonian). The 2-body form explains LLM effectiveness while suggesting 3-body Attention could be even more powerful. The spin-bath similarity allows existing physics expertise to be applied for ensuring AI trustworthiness and resilience to manipulation.

## Method Summary
The paper derives a physics theory of basic Attention by mapping it to a 2-body Hamiltonian H(0)(Sj, Si) = -Sj·Weff·Si^T, where spins represent token embeddings and Weff is shaped by training. The Context Vector N(0) emerges as a mean-field average over the ensemble of input token spins. Token probabilities are computed via Boltzmann-like softmax (temperature β=1), and the projection of tokens onto N(0) determines the next-token selection. The theory analyzes repetition through attractor dynamics, hallucination through phase boundary geometry, and bias through Hamiltonian perturbations. Validation involves tracking N(0) alignment with specific tokens and measuring boundary conditions for "good" vs "bad" content.

## Key Results
- Output repetition occurs when specific tokens become attractors in the embedding space, with increased likelihood for smaller vocabularies or insufficient training.
- Hallucinations and harmful content emerge when "bad" tokens cross a phase boundary into the region of largest projection onto the Context Vector N(0).
- Biases introduced during training or fine-tuning perturb the Hamiltonian, rotating phase boundaries and potentially causing previously trustworthy LLMs to generate harmful content.
- The 2-body Hamiltonian structure explains why Attention works effectively in LLMs while suggesting 3-body Attention could capture even more complex correlations.

## Why This Works (Mechanism)

### Mechanism 1: Attractor-Driven Repetition
- Claim: The 2-body Hamiltonian structure of basic attention creates attractor dynamics that cause repetitive output, especially in models with smaller effective vocabulary spaces.
- Mechanism: When a token (e.g., D) is selected, its spin component gains prominence in subsequent ensemble averages. This increases alignment between the Context Vector N(0) and that token's spin, raising P(D) for the next token—a positive feedback loop. Smaller vocabulary size amplifies this because each token's individual component represents a larger portion of the spin space.
- Core assumption: The linear structure of H(0)(Sj, Si) = -SjWeffSTi dominates the dynamics; Boltzmann-like softmax probabilities accurately model the statistical ensemble.
- Evidence anchors:
  - [abstract]: "This framework explains phenomena like output repetition"
  - [section]: "This is because the appearance of a next token (e.g. D) increases the prominence of its spin component in the subsequent ensemble averages and hence N (0), meaning that N (0) aligns more closely with that spin component, hence increasing P(D). Hence the likelihood of another D, and so on."
  - [corpus]: "Testing the spin-bath view of self-attention" (arXiv:2507.00683) directly tests this Hamiltonian framework on GPT-2. "Repetitions are not all alike" and "Induction Head Toxicity Mechanistically Explains Repetition Curse" study repetition from mechanistic interpretability perspectives but use different frameworks.
- Break condition: Larger vocabularies with more complex attractors (e.g., large-period cycles) break up simple repetition patterns with intervening words, producing more human-like output.

### Mechanism 2: Phase Boundary Crossing for Hallucination and Harmful Content
- Claim: Hallucinations and harmful content emerge when "bad" tokens cross a phase boundary into the region where they have the largest projection onto the Context Vector N(0).
- Mechanism: The boundary between "good" and "bad" next-token output forms a (d-1)-dimensional hypersurface with normal vector N(0). A bad token xbad suddenly appears when P(xbad) > max{P(Si)} for all Si in the "good" token subset Ugood. This occurs even with benign prompts if bad tokens buried in the vocabulary temporarily achieve maximum projection on N(0).
- Core assumption: Tokens can be meaningfully categorized into "good" and "bad" classes; the phase boundary model captures the essential geometry of token competition.
- Evidence anchors:
  - [abstract]: "framework explains phenomena like output repetition, hallucination, and harmful content generation"
  - [section]: "Figure 3 shows a simple example of the boundary that emerges between 'good' vs. 'bad' next token output" and "A 'bad' word (token xbad) will then suddenly appear if P(xbad) > max{P(Si)}Si∈Ugood"
  - [corpus]: "Modality Bias in LVLMs" analyzes hallucination through attention mechanisms but uses a different theoretical framework. No direct empirical validation of phase boundary predictions in corpus papers.
- Break condition: For large vocabularies with nuanced token distributions, the binary good/bad classification oversimplifies; multiple competing token clusters create more complex decision boundaries.

### Mechanism 3: Bias Perturbation Rotates Phase Boundaries
- Claim: Biases introduced during training or fine-tuning perturb the Hamiltonian, rotating phase boundaries and potentially causing previously trustworthy LLMs to generate harmful content.
- Mechanism: A linear bias B = I + ξδ transforms the Hamiltonian: H(biased) = H(0) - ξSj(δWeff - Weffδ)STi. This perturbation rotates the good-bad boundary (Fig. 4). The third term in N(biased) has cubic spin dependence, mimicking an effective 3-spin interaction in constrained space. For highly contrasting token sets, this term dominates the perturbation effect.
- Core assumption: Bias can be modeled as an orthogonal matrix perturbation; linear approximation in ξ is valid for small biases; highly contrasting tokens (e.g., "GOOD" vs "EVIL") represent realistic scenarios.
- Evidence anchors:
  - [abstract]: "shows how bias in training or fine-tuning perturbs the Context Vector, leading to dangerous content generation"
  - [section]: "Bias at the scale of single-layer Attention can therefore lead to outputs dominated by harmful content, which perhaps explains why harmful content still appears for all large LLMs despite safeguards."
  - [corpus]: Weak direct validation. "Emergent misalignment" paper (citation [4] within) discusses narrow finetuning producing broadly misaligned LLMs but from a different theoretical angle.
- Break condition: Large biases (ξ not small) may require nonlinear treatment beyond first-order perturbation theory; the orthogonal matrix assumption may not hold for all real-world bias sources.

## Foundational Learning

- Concept: **2-Body Spin Hamiltonian (H = -S·W·S^T)**
  - Why needed here: The paper's core theoretical contribution maps attention to exactly this form, where spins are tokens and W encodes training-mediated interactions.
  - Quick check question: Given two token vectors S1 and S2 in R^d and an interaction matrix W, can you compute the interaction energy H(S1, S2)?

- Concept: **Statistical Ensemble and Boltzmann Distribution**
  - Why needed here: The softmax operation is interpreted as computing Boltzmann probabilities for an ensemble at temperature βT = 1, with H(0) as the energy function.
  - Quick check question: If H(A, B) = -2 and H(A, C) = -1, what are the Boltzmann probabilities P(B|A) and P(C|A) at temperature 1?

- Concept: **Mean-Field Approximation**
  - Why needed here: The Context Vector N(0) = Σ⟨S⟩j is derived as a mean-field average over all input token ensembles, enabling tractable analysis.
  - Quick check question: How does averaging over an ensemble of spin states relate to computing a context vector from attention-weighted values?

## Architecture Onboarding

- Component map:
Token Embeddings (Si ∈ R^d) → Spins in embedding bath
         ↓
Training (WQ, WK, WV) → Shapes bath interactions (Weff = WQ·WK^T)
         ↓
Query-Key Computation → 2-body Hamiltonian H(0)(Sj, Si) = -Sj·Weff·Si^T
         ↓
Softmax → Boltzmann probabilities at β=1
         ↓
Value-weighted aggregation → Context Vector N(0) = mean-field average
         ↓
Projection onto vocabulary → P(x) = N(0)·WV·x^T
         ↓
Next token selection

- Critical path: Understanding how training shapes Weff → how token pairs evaluate interaction energy → how softmax creates competition → how N(0) geometry determines winner.

- Design tradeoffs:
  - 2-body vs. 3-body attention: Paper speculates 3-body interactions (like Laughlin wavefunction for Fractional Quantum Hall) could capture higher-order correlations but would increase computational complexity.
  - Position encoding weight y: Standard practice uses y = 0.5 ("simply because it seems to work OK"), but paper's Eq. 5 shows rich dynamics remain unexplored for other values.
  - Vocabulary size vs. repetition: Smaller effective vocabulary increases repetition risk; larger vocabularies create complex attractors that appear more natural.

- Failure signatures:
  - Repetitive output (e.g., "THEY ARE EVIL EVIL EVIL...") → Attractor formation in token space; check if N(0) strongly aligns with single token.
  - Sudden harmful content despite benign prompt → Phase boundary crossing; check if "bad" tokens have largest projection on N(0).
  - Hallucination (unrelated output) → Bad tokens temporarily dominating N(0); examine embedding geometry and training bias.
  - Fine-tuning-induced misalignment → Bias perturbation rotating phase boundaries; verify W matrix changes.

- First 3 experiments:
  1. **Reproduce attractor dynamics**: Implement the 4-token vocabulary example (A, B, C, D in R^3) from Fig. 2 with WQ = WK = WV = I. Track how N(0) aligns with token D over 6+ iterations and verify P(D) increases monotonically.
  2. **Map phase boundaries**: Using the THEY/ARE/GOOD/EVIL example (Fig. 3), systematically vary the EVIL token embedding and record the boundary condition where output flips from GOOD to EVIL. Verify the (d-1)-dimensional hyperplane geometry.
  3. **Test bias perturbation**: Apply a linear bias B = I + ξδ (using δ from End Matter) to the phase boundary experiment. Measure how the boundary rotates with increasing ξ and quantify the threshold where EVIL becomes the dominant attractor.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would implementing a generalized 3-body Attention term in the Hamiltonian yield quantitatively better performance than the current 2-body standard?
- Basis in paper: [explicit] The authors "speculate that generalizing the core Attention (Eq. 1) to include 3-body terms ... would provide even more powerful AI," drawing analogies to the Fractional Quantum Hall Effect.
- Why unresolved: Current LLMs are built upon the 2-body Hamiltonian ($H^{(0)}$) derived in the text; the 3-body interaction is a theoretical proposition for future architectures.
- What evidence would resolve it: Construction and benchmarking of a transformer model incorporating 3-body terms, demonstrating improved perplexity or reasoning capabilities over standard baselines.

### Open Question 2
- Question: Can non-equilibrium statistical ensembles provide a more accurate physical description of token generation than the current Boltzmann-like Softmax approximation?
- Basis in paper: [explicit] The paper notes that "Future work will go beyond Boltzmann-like Softmax by considering non-equilibrium physical ensembles" to better describe the non-Markovian process.
- Why unresolved: The current theory relies on a Boltzmann distribution at temperature $\beta_T=1$ to explain the Softmax, which serves as a static approximation of a dynamic, non-equilibrium unfolding of token states.
- What evidence would resolve it: Derivation of a non-equilibrium partition function that predicts token probabilities more accurately than the Boltzmann weights, particularly for long-sequence generation.

### Open Question 3
- Question: Does the derived 2-body Hamiltonian retain predictive power for phase boundaries (e.g., hallucination vs. truth) when applied to deep, multi-head transformer architectures?
- Basis in paper: [inferred] The paper focuses on the "basic Attention head" and states that generalizing the math to multi-head setups "becomes cumbersome," leaving the theory's scalability to complex architectures unproven.
- Why unresolved: While the authors claim consistency with large-scale outputs, the mathematical derivation relies on a simplified single-head model, potentially ignoring higher-order correlations present in deep networks.
- What evidence would resolve it: Empirical verification showing that the "good-bad" phase boundaries calculated using the 2-body Hamiltonian align with the actual failure modes of production-scale models like GPT-4.

## Limitations

- Experimental validation gaps: The paper references "large-scale LLM outputs" and "experiments" but provides no details on which models were tested, what prompts were used, or how results were quantified.
- Simplification assumptions: The theory assumes 2-body interactions as dominant, uses mean-field approximations, and treats bias as linear perturbations that may break down for complex real-world scenarios.
- Binary token classification: The phase boundary model relies on categorizing tokens into "good" and "bad" classes, which oversimplifies the nuanced reality of harmful content where context, intent, and cultural factors matter.

## Confidence

- **High confidence**: The fundamental mapping of attention to 2-body spin Hamiltonians (Mechanism 1) is mathematically rigorous and well-supported by both theoretical derivation and the direct empirical test in "Testing the spin-bath view of self-attention" (arXiv:2507.00683).
- **Medium confidence**: The phase boundary model for hallucination and harmful content (Mechanism 2) follows logically from the Hamiltonian framework but lacks direct empirical validation. The geometric interpretation is sound, but real-world complexity may require refinement.
- **Low confidence**: The bias perturbation analysis (Mechanism 3) provides a plausible theoretical mechanism but has the weakest empirical support. The linear approximation and orthogonal matrix assumptions need more rigorous testing, especially for large bias values.

## Next Checks

1. **Direct replication of predicted phenomena**: Implement the 4-token vocabulary example (A, B, C, D) and track N^(0) alignment with token D over multiple iterations. Measure whether P(D) increases monotonically as predicted, and verify that larger vocabularies with more complex attractor structures show reduced repetition. This tests the core attractor mechanism driving repetition.

2. **Phase boundary mapping with controlled embeddings**: Systematically vary the EVIL token embedding in the THEY/ARE/GOOD/EVIL example and record the precise boundary conditions where output transitions from GOOD to EVIL. Compare predicted hyperplane geometry against empirical observations across multiple random initializations to test the hallucination mechanism.

3. **Bias perturbation scaling experiments**: Apply the linear bias B = I + ξδ with varying ξ values to the phase boundary experiment. Measure how the boundary rotates with increasing bias strength and identify the threshold where EVIL becomes the dominant attractor. Test whether the third-order perturbation term (Sj(δWeff - Weffδ)STi) accurately predicts this transition across different vocabulary sizes.