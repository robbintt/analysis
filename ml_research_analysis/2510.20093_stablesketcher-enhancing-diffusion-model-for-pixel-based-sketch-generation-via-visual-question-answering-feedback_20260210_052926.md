---
ver: rpa2
title: 'StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation
  via Visual Question Answering Feedback'
arxiv_id: '2510.20093'
source_url: https://arxiv.org/abs/2510.20093
tags:
- sketch
- diffusion
- sketches
- images
- stable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StableSketcher, a framework that enhances
  Stable Diffusion to generate human-drawn pixel-based sketches. The approach fine-tunes
  the VAE using a reconstruction loss combined with LPIPS for better perceptual quality
  and applies reinforcement learning with a VQA-based reward function to improve prompt
  fidelity.
---

# StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback

## Quick Facts
- arXiv ID: 2510.20093
- Source URL: https://arxiv.org/abs/2510.20093
- Reference count: 40
- Primary result: FID of 143.68 and TIFAScore of 0.68, outperforming Stable Diffusion baselines in sketch generation quality and prompt fidelity

## Executive Summary
This paper introduces StableSketcher, a framework that enhances Stable Diffusion to generate human-drawn pixel-based sketches. The approach fine-tunes the VAE using a reconstruction loss combined with LPIPS for better perceptual quality and applies reinforcement learning with a VQA-based reward function to improve prompt fidelity. A new dataset, SketchDUO, is introduced, providing sketch-caption-QA triplets with positive and negative examples to capture desired and undesired sketch styles. Experiments show StableSketcher achieves the best performance, with FID of 143.68 and TIFAScore of 0.68, outperforming Stable Diffusion baselines in both image quality and text-image alignment. Ablation studies confirm the effectiveness of the VQA-based reward and VAE loss design.

## Method Summary
StableSketcher enhances Stable Diffusion for sketch generation through three key components: (1) VAE fine-tuning with a combined MSE and LPIPS loss to improve perceptual reconstruction quality, (2) reinforcement learning with a VQA-based reward function for better prompt fidelity, and (3) training on a contrastive dataset (SketchDUO) containing positive (human-like) and negative (over-detailed) sketches. The framework uses DDPO to optimize the UNet denoising policy using a reward derived from a fine-tuned VQA model that evaluates both instance-level and sketch-style attributes through element-wise QA pairs.

## Key Results
- StableSketcher achieves FID of 143.68 and TIFAScore of 0.68 on sketch generation
- VAE fine-tuning with MSE+LPIPS loss significantly outperforms standard MSE+KL approaches, avoiding posterior collapse
- VQA-based RL reward (TIFAScore) demonstrates superior prompt fidelity compared to BERTScore-based rewards
- User studies show StableSketcher outperforms baselines in both quality and alignment metrics

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning the VAE with a combined MSE + LPIPS loss improves sketch reconstruction fidelity compared to the standard KL-regularized VAE. The standard Autoencoder KL in Stable Diffusion over-regularizes the latent space when the KL weight is large, forcing the posterior toward a standard normal and discarding fine-grained stroke information (posterior collapse). By replacing the KL term with LPIPS, which measures perceptual similarity across multi-layer CNN features, the encoder retains sketch-critical features like line sharpness and contour continuity while maintaining stable training via MSE. This works because sketches rely more on perceptual/contour fidelity than exact pixel-wise reconstruction or strictly Gaussian latents.

### Mechanism 2
A VQA-based reward function (TIFA-style) provides more fine-grained prompt fidelity signal than caption-based rewards like BERTScore. TIFAScore decomposes a prompt into element-level QA pairs (e.g., object count, background color) and computes accuracy via a VQA model. This verifies whether each prompt condition is satisfied, whereas BERTScore compares generated captions to prompts and often preserves only coarse semantics. The fine-tuned mPLUG-large VQA model on SketchDUO (accuracy improved from 61.3% to ~89%) serves as the reward critic for DDPO. This works because element-wise QA evaluation captures fine-grained alignment that coarse semantic similarity misses.

### Mechanism 3
Training on a contrastive dataset of positive (human-like) and negative (over-detailed/shaded) sketches improves style disentanglement. Positive examples define the target sketch style (black lines, white background, minimal abstraction), while negative examples capture common Stable Diffusion failures (excessive detail, shading, non-white backgrounds). The model learns to avoid undesired styles via contrastive signal embedded in captions and QA pairs. This works because the contrastive signal provides explicit style guidance that prevents the model from generating undesired sketch characteristics.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)**
  - Why needed here: StableSketcher modifies the VAE and UNet within an LDM; understanding latent-space diffusion is prerequisite
  - Quick check question: Can you explain why operating in latent space reduces computational cost compared to pixel-space diffusion?

- **Concept: Denoising Diffusion Policy Optimization (DDPO)**
  - Why needed here: The VQA reward is integrated via DDPO, which frames denoising as an MDP; engineers must grasp policy gradients in diffusion
  - Quick check question: How does DDPO differ from standard REINFORCE when applied to multi-step diffusion denoising?

- **Concept: VQA-based Evaluation (TIFAScore)**
  - Why needed here: The core reward mechanism depends on element-wise QA evaluation; misinterpreting TIFA vs BERTScore leads to wrong metric choice
  - Quick check question: What is the key limitation of BERTScore for fine-grained prompt fidelity that TIFAScore aims to address?

## Architecture Onboarding

- **Component map**: VAE (fine-tuned with MSE + LPIPS) -> UNet (fine-tuned on sketch-caption pairs) -> DDPO loop (generates candidates, evaluates with VQA, updates policy) -> SketchDUO dataset (images, captions, QA pairs)

- **Critical path**: 1. VAE fine-tuning (MSE+LPIPS) -> 2. UNet fine-tuning (sketch-caption pairs) -> 3. VQA model fine-tuning on SketchDUO QA -> 4. DDPO training with RVQA reward

- **Design tradeoffs**:
  - KL weight vs reconstruction: High KL stabilizes latents but collapses sketch details; removing KL in favor of LPIPS improves perceptual fidelity
  - TIFAScore vs BERTScore: TIFA is fine-grained but requires QA pair coverage and a reliable VQA model; BERTScore is coarse but simpler
  - Positive/negative ratio: 24K/11.8K balances style signal; over-representation of negatives may bias the model toward overly simplistic sketches

- **Failure signatures**:
  - Posterior collapse: Reconstructed/generated images become nearly white
  - Reward stagnation: RVQA plateaus during DDPO -> QA pairs may not cover failure modes or VQA model misclassifies
  - Over-regularized latents: Blur or loss of thin strokes -> LPIPS weight too high or insufficient training data

- **First 3 experiments**:
  1. Ablate VAE loss: Train VAE with (a) MSE+KL, (b) MSE+LPIPS, (c) MSE+LPIPS+KL (small weight) on SketchDUO positive set; measure reconstruction MSE/LPIPS and generation FID
  2. Reward comparison: Run DDPO with BERTScore vs TIFAScore reward on a held-out prompt set; track RVQA progression and final TIFAScore/CLIPScore
  3. Negative-data impact: Train with only positive examples vs positive+negative; evaluate style metrics (sketch simplicity, background whiteness) and user preference

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the StableSketcher framework maintain prompt fidelity and stylistic coherence when applied to complex multi-object scenes and long-tail object categories?
  - Basis: The current SketchDUO dataset constrains generalization to "long-tail objects" and "multi-object scenes" due to its focus on 30 common classes
  - Why unresolved: The current evaluation and dataset are restricted to single-instance sketches, leaving performance on compositional complexity untested
  - Evidence needed: Quantitative results and qualitative samples from evaluation on datasets containing multi-object sketches and rare object classes

- **Open Question 2**: Does the proposed VAE reconstruction loss (MSE + LPIPS) scale effectively to sketches with diverse stylistic factors, such as hatching, variable stroke density, or perspective views?
  - Basis: The paper identifies "stylistic diversity" as a limitation and plans to vary "style factors (e.g., stroke density and thickness, contour versus hatching, perspective)"
  - Why unresolved: The current definition of a sketch is restricted to "thick black lines... with no texture," and the model has not been validated on diverse artistic techniques
  - Evidence needed: Reconstruction and generation experiments using a dataset enriched with varied sketching styles

- **Open Question 3**: Does incorporating granular structural annotations—such as part labels, keypoints, and per-stroke metadata—yield superior semantic alignment compared to the current instance-level VQA-based reward?
  - Basis: The paper outlines plans to enrich the dataset with "part labels, keypoints, and per-stroke metadata" to address current data limitations
  - Why unresolved: The current framework relies on instance-level VQA feedback, which may lack the precision to correct specific structural errors in complex drawings
  - Evidence needed: A comparative study between the current VQA reward model and a structural-annotation reward model regarding semantic accuracy and object part correctness

## Limitations
- SketchDUO dataset size (1.8K training sketches) is modest for fine-tuning a large diffusion model, risking overfitting
- No direct comparison to other VQA-based RL rewards or structural annotation rewards
- Limited evaluation on complex multi-object scenes and long-tail object categories

## Confidence
- **High Confidence**: SketchDUO dataset construction, TIFAScore methodology, baseline FID/TIFAScore numbers
- **Medium Confidence**: VAE LPIPS fine-tuning improves perceptual quality (ablation shown, but limited scale), DDPO with VQA reward improves prompt fidelity (reward curves and metrics support, but user study n=24 is small)
- **Low Confidence**: Long-term stability of VAE fine-tuning without KL, robustness of VQA reward to novel sketch artifacts, scalability to larger or more diverse sketch domains

## Next Checks
1. **VAE Ablation Scale-Up**: Train VAE with MSE+LPIPS vs MSE+KL vs MSE only on 5K QuickDraw sketches; measure reconstruction LPIPS and generation FID after 30 epochs
2. **Reward Generalization Test**: Apply TIFAScore vs BERTScore DDPO rewards to prompts with novel elements (e.g., "a sketch of a dragon with three heads") not in SketchDUO; evaluate visual quality and element accuracy
3. **Negative Data Sensitivity**: Train StableSketcher with (a) only positive sketches, (b) positive+negative, (c) positive+curated negatives from baseline failures; compare style metrics and user preference