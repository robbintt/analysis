---
ver: rpa2
title: 'Position: Curvature Matrices Should Be Democratized via Linear Operators'
arxiv_id: '2501.19183'
source_url: https://arxiv.org/abs/2501.19183
tags:
- linear
- curvature
- matrices
- operators
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper advocates for using linear operators as a general abstraction
  to handle structured matrices, particularly curvature matrices like Hessians and
  Fisher information matrices, in machine learning. The authors developed curvlinops,
  a PyTorch library that provides curvature matrices through a unified linear operator
  interface, encapsulating complexity from automatic differentiation, batching, and
  non-determinism.
---

# Position: Curvature Matrices Should Be Democratized via Linear Operators

## Quick Facts
- arXiv ID: 2501.19183
- Source URL: https://arxiv.org/abs/2501.19183
- Authors: Felix Dangel; Runa Eschenhagen; Weronika Ormaniec; Andres Fernandez; Lukas Tatzel; Agustinus Kristiadi
- Reference count: 40
- Key outcome: Linear operators provide a unified interface for curvature matrices, enabling scalable and user-friendly second-order methods in deep learning

## Executive Summary
This paper argues that linear operators should be the standard abstraction for curvature matrices in machine learning. The authors present curvlinops, a PyTorch library that encapsulates the complexity of curvature matrix computation through a unified interface based on matrix-vector products. By representing Hessians, Fisher information matrices, and their approximations as linear operators, the library enables efficient second-order optimization, model merging, pruning, and spectral analysis without materializing full matrices. The approach simplifies downstream applications while maintaining scalability to large models.

## Method Summary
The method implements curvature matrices as linear operators that expose only matrix-vector products (matvecs) without materializing full matrices. The library provides operators for Hessian, Gauss-Newton, Fisher, and Kronecker-factored approximations (KFAC), automatically handling batching, non-determinism, and autodiff complexity. Users interact with curvature matrices through standard multiplication syntax while the library handles efficient computation via techniques like Pearlmutter's trick for HVPs and Kronecker product properties. The interface supports composition, inversion via iterative solvers, and export to SciPy for advanced linear algebra operations.

## Key Results
- KFAC approximation requires only 1.5-2.5× gradient computation time with minimal memory overhead
- Linear operators enable applications like second-order optimization, model merging, and pruning through a unified interface
- SciPy interoperability allows use of mature iterative solvers (CG, eigsh, svds) on GPU-executed matvecs

## Why This Works (Mechanism)

### Mechanism 1: Matrix-Free Abstraction via Linear Operators
Linear operators enable working with curvature matrices as if they were dense matrices, without materializing them in memory. A linear operator A exposes only the map v → A(v) (matrix-vector product). For structured matrices (diagonal, Kronecker-factored, sparse), this product can be computed efficiently without forming the full D×D matrix. The interface supports composition (A+B), transformation (A⁻¹ via iterative solvers), and standard multiplication syntax. Applications requiring frequent diagonal access, submatrix extraction, or entry-wise operations will face inefficiency from repeated matvec calls.

### Mechanism 2: Unified Interface Across Curvature Approximations
A single linear operator interface can represent diverse curvature matrices (Hessian, GGN, Fisher, KFAC) with varying accuracy-speed tradeoffs. All curvature matrices share the structure of second-order information about the loss landscape. The library implements each via its optimal matvec strategy: HVPs via nested autodiff for Hessians, VJP+JVP compositions for GGN, Kronecker factor operations for KFAC. Users swap curvature type via a constructor argument without changing downstream code. Approximations like KFAC assume layer-wise block structure; architectures violating this may yield incorrect curvature.

### Mechanism 3: SciPy Interoperability for Advanced Linear Algebra
Exporting PyTorch linear operators to SciPy unlocks mature iterative solvers (CG, LSMR, eigsh, svds) without reimplementation. The `.to_scipy()` method wraps the GPU-executed matvec in a `scipy.sparse.linalg.LinearOperator`. SciPy's Arnoldi-based eigensolvers (ARPACK backend) then compute truncated decompositions using only matvecs, with the heavy computation remaining on GPU. Very ill-conditioned matrices or those with slow spectral decay may cause ARPACK convergence issues; SciPy CPU-side control flow may become a bottleneck for very frequent matvecs.

## Foundational Learning

- **Hessian-vector products via Pearlmutter trick**: Understanding that ∇²L(θ)v = ∇(∇L(θ)ᵀv) enables O(1) memory HVPs vs. O(D²) for materialized Hessians. Quick check: Given a scalar loss L(θ), write pseudocode computing ∇²L(θ)v using only gradient calls.
- **Kronecker product properties (A⊗B)(v⊗w) = (Av)⊗(Bw)**: KFAC's efficiency hinges on never materializing A⊗B; instead, matvecs apply factors sequentially. Quick check: If A is 100×100 and B is 50×50, what's the memory ratio between storing A⊗B vs. storing A and B separately?
- **Conjugate gradients for linear system solving**: Computing A⁻¹v requires solving Az=v; CG does this iteratively using only matvecs with A. Quick check: Why does CG converge faster for matrices with clustered eigenvalues?

## Architecture Onboarding

- **Component map**:
  ```
  LinearOperator (base class)
  ├── HessianLinearOperator (nested autodiff HVPs)
  ├── GGNLinearOperator (VJP+JVP composition)
  ├── FisherLinearOperator / MC_FisherLinearOperator (pseudo-loss GGN trick)
  ├── KFACLinearOperator (Kronecker factors, per-layer)
  └── InverseLinearOperator subclasses
      ├── CGInverseLinearOperator (iterative solver)
      ├── NeumannInverseLinearOperator (series expansion)
      └── KFACInverseLinearOperator (analytic Kronecker inverse)
  
  Utilities:
  ├── hutchinson_diag(), hutchinson_trace() (RLA estimators)
  └── .to_scipy() (SciPy export wrapper)
  ```

- **Critical path**:
  1. Instantiate operator: `H = HessianLinearOperator(model, loss_func, params, data_loader)`
  2. Compute matvec: `Hv = H @ v` (v can be vector or tensor list matching params structure)
  3. (Optional) Transform: `H_inv = CGInverseLinearOperator(H)` then `H_inv @ g`
  4. (Optional) Export: `H_scipy = H.to_scipy()` then use `scipy.sparse.linalg.eigsh()`

- **Design tradeoffs**:
  - **Encapsulation vs. efficiency**: Isolated linear operators may duplicate forward-backward passes if gradient and curvature are computed separately
  - **Flexibility vs. misuse potential**: Single interface across curvature types risks users applying inappropriate curvature for their application
  - **Minimal interface vs. functionality**: Pure matvec interface cannot support direct submatrix access; applications needing this must estimate (e.g., Hutchinson diagonal) or materialize partially

- **Failure signatures**:
  - **Non-determinism error**: Sanity check fails if model has dropout/batchnorm in training mode. Fix: set `model.eval()` or use `check_deterministic=False` (with caution)
  - **Memory OOM on HVP**: Large batches + Hessian matvec exceed GPU memory. Fix: reduce batch size in data loader
  - **KFAC scaling bugs**: Incorrect curvature magnitude from mis-scaled Kronecker factors. Fix: run built-in equivalence tests against block-diagonal GGN

- **First 3 experiments**:
  1. **Sanity check on small MLP**: Create a 2-layer MLP on MNIST subset. Instantiate HessianLinearOperator and verify `H @ torch.ones(D)` runs. Compare diagonal estimate from `hutchinson_diag(H)` against directly computed diagonal (materialize small Hessian)
  2. **Curvature swap test**: On the same setup, create GGNLinearOperator, EmpiricalFisherLinearOperator, and KFACLinearOperator. Compare matvec outputs on the same vector—expect rank-order similarity but not equality. Time each matvec relative to a gradient computation
  3. **SciPy eigensolver integration**: Export HessianLinearOperator to SciPy via `.to_scipy()`. Use `eigsh(H, k=5)` to extract top 5 eigenvalues. Verify gradient has high overlap with top eigenspace per Gur-Ari et al. (2018) method

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does Fisher-weighted model merging using the full Fisher matrix (computed via linear operators) outperform the diagonal approximation in terms of downstream task performance?
- **Basis in paper**: In Section 3.1, Application 3, the authors present code for "Full Fisher merging via CG" and explicitly label it as an "(unexplored)" variant compared to the diagonal approach
- **Why unresolved**: The paper provides the implementation capability but does not offer empirical results comparing the accuracy or computational cost of the full Fisher approach against the diagonal approximation
- **What evidence would resolve it**: A comparative study benchmarking full Fisher merging against diagonal merging on standard multi-task model merging benchmarks

### Open Question 2
- **Question**: How can the linear operator abstraction be optimized to avoid redundant computation when used alongside standard gradient descent?
- **Basis in paper**: Section 5 ("Alternative Views") states that encapsulation may require "duplicated computation," specifically noting that computing a KFAC operator might necessitate an "additional redundant forward-backward pass"
- **Why unresolved**: While the abstraction hides complexity, it currently decouples the operator's internal graph from the main training loop, leading to potential inefficiency
- **What evidence would resolve it**: An extension of the library that shares intermediate activations (feature maps) between the training loop and the linear operator, demonstrating reduced FLOPs without breaking the interface

### Open Question 3
- **Question**: To what extent does the integration of forward-mode automatic differentiation improve the efficiency (speed and memory) of Hessian-vector and GGN-vector products in this framework?
- **Basis in paper**: Section A identifies the "lack of forward mode AD" and "compilation" as limitations, noting that JAX implementations using these features achieve HVPs in "2-3 gradients [memory]" versus the current 3-4.5
- **Why unresolved**: The library currently relies on reverse-mode AD for compatibility, which is theoretically less efficient for vector-Jacobian products involved in curvature computations
- **What evidence would resolve it**: Benchmarks comparing the current `curvlinops` implementation against a modified version utilizing PyTorch's functional API for forward-mode AD

## Limitations

- Performance claims are limited to two specific models (ResNet50 and nanoGPT) without broader architectural coverage
- The SciPy interoperability may not provide performance benefits for all use cases due to CPU-GPU data transfer overhead
- The abstraction may break for unconventional architectures where curvature matrices deviate from standard assumptions

## Confidence

- **High Confidence**: The linear operator abstraction itself is mathematically sound and the implementation correctly exposes matrix-vector products for various curvature matrices
- **Medium Confidence**: Performance benchmarks showing KFAC requiring only 1.5-2.5× gradient computation time are plausible but need broader validation across model architectures
- **Medium Confidence**: The claim that linear operators simplify downstream applications is supported by code examples but lacks systematic user studies

## Next Checks

1. **Architecture stress test**: Apply curvlinops to transformer variants with weight sharing or residual connections to verify KFAC assumptions don't break unexpectedly
2. **Memory scaling study**: Benchmark peak memory usage across model sizes (D=10⁴ to 10⁸ parameters) to validate the claimed sub-quadratic scaling
3. **Solver convergence analysis**: Measure SciPy iterative solver iteration counts and convergence rates for different curvature matrices across diverse deep learning tasks