---
ver: rpa2
title: 'Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare'
arxiv_id: '2510.01164'
source_url: https://arxiv.org/abs/2510.01164
tags:
- fairness
- social
- welfare
- task
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Social Welfare Function (SWF) Benchmark
  to evaluate large language models (LLMs) as societal decision-makers tasked with
  allocating scarce resources. The benchmark simulates a dynamic environment where
  an LLM sequentially assigns tasks to a diverse community, facing a persistent trade-off
  between collective efficiency (measured by ROI) and distributive fairness (measured
  by the Gini coefficient).
---

# Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare

## Quick Facts
- arXiv ID: 2510.01164
- Source URL: https://arxiv.org/abs/2510.01164
- Authors: Zhengliang Shi; Ruotian Ma; Jen-tse Huang; Xinbei Ma; Xingyu Chen; Mengru Wang; Qu Yang; Yue Wang; Fanghua Ye; Ziyang Chen; Shanyi Wang; Cixing Li; Wenxuan Wang; Zhaopeng Tu; Xiaolong Li; Zhaochun Ren; Linus
- Reference count: 29
- One-line primary result: 20 state-of-the-art LLMs ranked on a novel benchmark measuring their ability to balance efficiency and fairness in sequential resource allocation tasks

## Executive Summary
This paper introduces the Social Welfare Function (SWF) Benchmark to evaluate large language models as societal decision-makers tasked with allocating scarce resources. The benchmark simulates a dynamic environment where an LLM sequentially assigns tasks to a diverse community, facing a persistent trade-off between collective efficiency (measured by ROI) and distributive fairness (measured by the Gini coefficient). Experiments with 20 state-of-the-art LLMs reveal three key findings: (1) General conversational ability poorly predicts allocation skill, as top Arena models often rank low on SWF; (2) Most LLMs exhibit a strong utilitarian orientation, prioritizing efficiency at the cost of severe inequality; (3) Allocation strategies are highly susceptible to external influences, with output-length constraints and social-influence framing significantly shifting fairness-efficiency preferences.

## Method Summary
The study evaluates 20 LLM allocators on a benchmark where they sequentially assign 50 tasks across 63 task flows to 12 heterogeneous recipient agents (1.5B-72B parameters). The benchmark creates a persistent efficiency-fairness trade-off by clustering tasks via K-means based on agent performance signatures. Allocators receive sliding-window context of 3 recent turns and must decide task assignments via `<agent>` tags. Performance is measured by SWF Score = (1-Gini)×ROI, where Gini measures distributional fairness and ROI measures cumulative reward/cost ratio. Pre-computed reward/cost caches avoid repeated inference, and failed tasks are retried until success or maximum limit.

## Key Results
- General conversational ability poorly predicts allocation skill - top Arena models often rank low on SWF
- Most LLMs exhibit strong utilitarian orientation, prioritizing efficiency at cost of severe inequality
- Allocation strategies are highly susceptible to external influences - output-length constraints and social-influence framing significantly shift fairness-efficiency preferences
- Top-performing model: DeepSeek-V3 with SWF Score of 0.365
- Average fairness score across models: 0.45 (utilitarian tendency)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SWF benchmark creates persistent efficiency-fairness trade-offs by clustering tasks with stable agent performance hierarchies.
- Mechanism: Tasks are clustered via K-means based on orientation vectors (agent performance signatures), creating scenarios where assigning to high-ROI agents maximizes efficiency while distributing broadly improves fairness (Gini). The multiplicative SWF Score = (1-Gini)×ROI penalizes extreme single-objective optimization.
- Core assumption: Task clustering maintains consistent performance hierarchies across simulation rounds (validated via Spearman correlation of orientation vectors).
- Evidence anchors:
  - [abstract]: "benchmark is designed to create a persistent trade-off between maximizing collective efficiency (measured by Return on Investment) and ensuring distributive fairness (measured by the Gini coefficient)"
  - [section 2.3]: "This design creates a persistent dilemma for the LLM allocator: assigning tasks to a specific set of top agents maximizes efficiency, while distributing them more evenly promotes fairness"
  - [corpus]: "Online Social Welfare Function-based Resource Allocation" discusses similar centralized resource allocation with population-level welfare characterization (FMR=0.55), providing theoretical grounding but no direct experimental validation of this specific clustering approach.
- Break condition: If task orientation vectors have low Spearman correlation (<0.5), clustering fails to induce consistent hierarchies, and the trade-off becomes unstable.

### Mechanism 2
- Claim: Top Arena models underperform on SWF due to over-reliance on initial agent profiles rather than realized performance.
- Mechanism: High-performing conversational models exhibit stronger correlation with initial MMLU-based labels (e.g., GPT-5-High: 0.801*, Gemini2.5-Pro: 0.840*), misallocating tasks to agents with prestigious tags but moderate returns. Top SWF models (DeepSeek-V3: 0.557 label vs 0.756* ROI correlation) ground decisions in outcomes.
- Core assumption: Initial profile quality reflects general capability but not task-specific efficiency in the simulation environment.
- Evidence anchors:
  - [abstract]: "general conversational ability, as measured by popular leaderboards, is a poor predictor of its allocation skill"
  - [section 3.3, Table 6]: Correlation analysis shows top Arena models (Claude-4.1-Opus: 0.793*, GPT-5-High: 0.801*) have higher initial label correlation than top SWF models
  - [corpus]: Weak corpus support—no neighbor papers examine profile bias in LLM allocators specifically.
- Break condition: If agents' realized ROI strongly correlates with initial profiles (r>0.9), profile reliance would not cause misallocation.

### Mechanism 3
- Claim: LLM allocation strategies are susceptible to output-length constraints and social-influence framing.
- Mechanism: Shorter reasoning (via token limits) increases utilitarian orientation (fairness drops from 0.59→0.45 for DeepSeek-V3). Social influence (threats/temptation) shifts preferences toward fairness (+0.08 average) but reduces efficiency (−7.28 ROI average for temptation).
- Core assumption: Output length proxies for deliberation depth; social influence framing activates context-dependent value prioritization.
- Evidence anchors:
  - [abstract]: "Allocation strategies are highly susceptible to external influences, with output-length constraints and social-influence framing significantly shifting fairness-efficiency preferences"
  - [section 3.3, Figure 4; section 3.4, Table 3]: Quantitative evidence across 6 models for length constraints and 10 models for social influence
  - [corpus]: "Incentive-Aware Dynamic Resource Allocation" notes strategic agents respond to incentive structures (FMR=0.54), providing analogous but not identical evidence.
- Break condition: If models have fixed allocation preferences independent of prompting, interventions would show zero variance across conditions.

## Foundational Learning

- **Gini Coefficient as Fairness Metric**
  - Why needed here: Interpreting SWF scores requires understanding how Gini measures inequality (0=perfect equality, 1=maximum inequality).
  - Quick check question: If 3 agents receive [10, 10, 10] tasks vs [27, 2, 1] tasks, which has lower Gini and why?

- **Return on Investment (ROI) in Allocation Context**
  - Why needed here: Efficiency is measured as cumulative reward/cost ratio, not raw task completion.
  - Quick check question: If Agent A succeeds 8/10 tasks at cost 2 each, and Agent B succeeds 6/10 at cost 1 each, which has higher ROI?

- **Multiplicative Welfare Functions**
  - Why needed here: SWF Score = Fairness × Efficiency penalizes extreme optimization of either objective.
  - Quick check question: Why does SWF use multiplication rather than addition of normalized fairness and efficiency scores?

## Architecture Onboarding

- **Component map**: Allocator LLM (sovereign decision-maker) → Recipient agents (12 heterogeneous LLMs, 1.5B–72B parameters) → Environment (task sequence, reward/cost evaluation) → Feedback loop (ROI, Gini, history). Sliding window retains 3 most recent turns to manage context.

- **Critical path**: (1) Initialize recipient profiles from MMLU scores → (2) Load pre-cached task-reward-cost mappings → (3) Allocator selects recipient via `<agent>` tags → (4) Environment matches decision to cache → (5) Update ROI/Gini → (6) Append to history with sliding window → (7) Repeat for N=50 tasks per flow, 63 flows total.

- **Design tradeoffs**:
  - Pre-caching rewards avoids repeated recipient inference (efficiency gain) but assumes static agent capabilities (no learning)
  - Sliding window (3 turns) reduces context overflow but may lose long-horizon patterns
  - Anonymous agent IDs (AAA, LLL) prevent name bias but require profile tables for capability context

- **Failure signatures**:
  - Utilitarian collapse: ROI rises while Gini approaches 1.0 (efficiency-only optimization)
  - Profile over-reliance: Task counts correlate >0.8 with initial labels, <0.5 with realized ROI
  - Intervention insensitivity: Variance in fairness/efficiency <0.02 across social-influence conditions

- **First 3 experiments**:
  1. **Baseline SWF ranking**: Run all 20 allocator models on 3 randomly selected task flows with vanilla prompts; verify SWF Score = (1-Gini)×ROI matches Table 2 rankings within ±2 positions.
  2. **Length constraint ablation**: For top-3 and bottom-3 SWF models, compare vanilla vs. concise vs. extreme-short prompts on identical task flow; confirm fairness decreases as token count drops (Figure 4 pattern).
  3. **Social influence sensitivity test**: Apply all 4 persuasion strategies (temptation, threat, identification, internalization) to a single mid-ranked model (e.g., GPT-4.1); verify threats/temptation produce largest fairness shifts (Table 3 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications enable LLMs to perform explicit ethical reasoning rather than relying on implicit utilitarian biases?
- Basis in paper: [explicit] The conclusion states that "Promising directions also include exploring architectural changes that support explicit ethical reasoning," moving beyond simple prompt-based interventions.
- Why unresolved: The current study demonstrates that standard LLMs default to utilitarian orientations and are susceptible to external framing, but it does not propose or test structural model changes to resolve this instability.
- What evidence would resolve it: Evaluation of architectures with dedicated ethical reasoning modules or specialized attention heads on the SWF benchmark, showing robustness to framing and consistent alignment with target values.

### Open Question 2
- Question: How do negotiation and coalition dynamics alter an LLM's allocation strategies compared to the sovereign "dictator" model used in the benchmark?
- Basis in paper: [explicit] The authors identify a promising direction in "expanding the simulation to include more complex social dynamics like negotiation and coalitions" (Conclusion).
- Why unresolved: The current benchmark simulates a central allocator without feedback mechanisms or power dynamics, whereas real-world governance often involves bargaining and group formation.
- What evidence would resolve it: A modified SWF environment where recipient agents can reject allocations, bargain, or form coalitions, measuring shifts in the allocator’s efficiency-fairness trade-off.

### Open Question 3
- Question: How can models be aligned with diverse normative frameworks (e.g., Rawlsian or Egalitarian principles) rather than just maximizing the SWF score?
- Basis in paper: [explicit] The conclusion calls for "investigating how to align models with diverse normative frameworks, such as well-known Rawlsian or Egalitarian principles."
- Why unresolved: The paper focuses on the trade-off between efficiency and fairness but does not test if models can strictly adhere to specific philosophical definitions of justice.
- What evidence would resolve it: Experiments where models are evaluated against ground-truth allocations defined by Rawlsian (maximin) or Egalitarian rules, rather than a generic Gini/ROI product.

## Limitations

- Profile vs. Performance Bias: The claim that top Arena models underperform due to over-reliance on initial profiles rests on correlational evidence rather than controlled ablation.
- Intervention Stability: Reported shifts in fairness-efficiency preferences may reflect prompt sensitivity rather than stable governance tendencies, limited by single flows per condition.
- Task Clustering Validity: The stability of agent performance hierarchies across K-means clustered task flows is only asserted, not experimentally validated across multiple clustering runs.

## Confidence

- **High**: The benchmark design (SWF = (1-Gini)×ROI) creates a measurable efficiency-fairness trade-off; top conversational models ranking low on SWF is empirically observed.
- **Medium**: Most LLMs exhibit utilitarian orientation prioritizing efficiency over fairness; allocation strategies are susceptible to external influences (length constraints, social framing).
- **Low**: The specific mechanism of profile over-reliance causing top Arena models to underperform requires further controlled testing.

## Next Checks

1. **Profile Ablation Test**: Run top Arena and top SWF models on identical task flows with and without recipient profile information in prompts. Measure change in allocation correlation with realized ROI vs. initial labels.
2. **Clustering Stability Validation**: Generate 5 independent K-means clusterings of the task set. Verify Spearman correlation of orientation vectors across clusterings exceeds 0.7 to confirm consistent performance hierarchies.
3. **Intervention Replication**: For DeepSeek-V3 and Claude-4.1-Opus, repeat the length constraint and social influence experiments across 3 different task flows each. Confirm the variance in fairness-efficiency shifts remains significant (p<0.05) across flows.