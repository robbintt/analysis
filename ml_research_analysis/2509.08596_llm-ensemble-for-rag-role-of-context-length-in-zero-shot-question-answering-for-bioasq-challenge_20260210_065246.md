---
ver: rpa2
title: 'LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering
  for BioASQ Challenge'
arxiv_id: '2509.08596'
source_url: https://arxiv.org/abs/2509.08596
tags:
- bioasq
- question
- phase
- query
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot ensemble approach for biomedical
  question answering using large language models (LLMs) and retrieval-augmented generation
  (RAG). The authors developed a multi-stage information retrieval pipeline combining
  LLM-generated queries, BM25 search, and semantic reranking to retrieve relevant
  PubMed documents for answering BioASQ challenge questions.
---

# LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge

## Quick Facts
- arXiv ID: 2509.08596
- Source URL: https://arxiv.org/abs/2509.08596
- Reference count: 19
- Primary result: Zero-shot ensemble achieves 0.92 accuracy on BioASQ 13 Phase A+ Yes/No questions, ranking 1st on batch 4

## Executive Summary
This paper presents a zero-shot ensemble approach for biomedical question answering using large language models (LLMs) and retrieval-augmented generation (RAG). The authors developed a multi-stage information retrieval pipeline combining LLM-generated queries, BM25 search, and semantic reranking to retrieve relevant PubMed documents for answering BioASQ challenge questions. For QA, they used multiple LLMs (Gemini 2.0 Flash, Gemini 2.5 Flash, and Claude 3.7 Sonnet) with zero-shot prompting, followed by answer synthesis using Gemini 2.0 Flash to resolve contradictions and generate unified responses. Their system achieved state-of-the-art results on BioASQ 13 Phase A+ Yes/No questions (accuracy of 0.92, ranking 1st on batch 4) and strong performance on Phase B Yes/No questions (accuracy of 1.00 on batches 1-2, ranking 1st). The study found that longer contexts generally hurt answer quality, particularly for List and Factoid questions, highlighting the importance of focused retrieval in RAG systems.

## Method Summary
The system uses a zero-shot ensemble approach with multi-stage retrieval: LLM-generated Elasticsearch queries → BM25 retrieval (≤10,000 docs) → semantic reranking to top 300 documents → multiple LLM answer generation → synthesis with confidence scoring. For IR, Gemini 2.0 Flash generates structured queries, with Gemini 2.5 Pro refining if <5 results. Semantic reranking compresses context while maintaining precision. QA uses Gemini 2.0 Flash, Gemini 2.5 Flash Preview, and Claude 3.7 Sonnet with zero-shot prompting, followed by synthesis using Gemini 2.0 Flash to resolve contradictions and generate unified responses with confidence scores. If confidence <0.5, re-synthesis runs at temperature 0.0. Fallback search extends to PubMed Google search and health websites when insufficient evidence exists.

## Key Results
- Achieved 0.92 accuracy on BioASQ 13 Phase A+ Yes/No questions (1st place on batch 4)
- Achieved 1.00 accuracy on BioASQ Phase B Yes/No questions for batches 1-2 (1st place)
- Found that longer contexts generally hurt answer quality, particularly for List and Factoid questions
- Demonstrates ensemble approach can outperform individual LLMs in biomedical QA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated structured queries improve retrieval recall by bridging semantic gaps between natural language questions and lexical search.
- Mechanism: Gemini 2.0 Flash translates biomedical questions into Elasticsearch boolean queries with synonyms and related terms. If initial retrieval returns <5 documents, Gemini 2.5 Pro automatically relaxes the query by enabling approximate matching and removing rare terms.
- Core assumption: LLMs can accurately decompose biomedical questions into searchable keyword structures that preserve semantic intent.
- Evidence anchors:
  - [abstract] "LLMs can be used for information retrieval (IR)"
  - [section 3.1] "We use Gemini 2.0 Flash to generate a structured Elasticsearch query that captures the semantic intent of the question using synonyms, related terms, and full boolean query string syntax"
  - [corpus] Neighbor paper "Query2doc: Query expansion with large language models" [4] provides supporting evidence for query rewriting advantages, though in general domains.
- Break condition: If LLM generates malformed queries that fail validation, or if query relaxation over-generalizes and loses specificity, recall gains reverse into noise.

### Mechanism 2
- Claim: Semantic reranking after high-recall BM25 retrieval enables focused context windows that prevent answer quality degradation from information dilution.
- Mechanism: BM25 retrieves up to 10,000 candidates; Google semantic-ranker-default-004 re-scores by semantic similarity to the original question, selecting top 300 documents. This compression step maintains precision while fitting within transformer context limits.
- Core assumption: Semantic similarity to the question correlates with evidence relevance for answer generation.
- Evidence anchors:
  - [abstract] "longer contexts can dilute performance" and "risk information dilution and model disorientation"
  - [section 3.1] "we could not get adequate QA results on full article abstracts without this step"
  - [corpus] Neighbor paper "On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems" directly investigates context size effects in RAG systems (FMR=0.61, strong relevance).
- Break condition: If reranking model is misaligned with biomedical semantics, or if correct evidence documents rank below the 300-cutoff threshold, precision gains become precision losses.

### Mechanism 3
- Claim: Ensemble answer synthesis with confidence-based refinement produces more robust outputs than single-model generation, particularly for binary classification tasks.
- Mechanism: Multiple LLMs (Gemini 2.0 Flash, Gemini 2.5 Flash Preview, Claude 3.7 Sonnet) generate candidate answers. Gemini 2.0 Flash synthesizes these into a unified response with confidence scoring. If confidence <0.5, synthesis re-runs with temperature reduced from 0.1 to 0.0 for determinism.
- Core assumption: Answer disagreement across models signals uncertainty that can be resolved through explicit synthesis rather than simple voting.
- Evidence anchors:
  - [abstract] "ensembles can outperform individual LLMs" with "state-of-the-art accuracy on Yes/No questions"
  - [section 3.2] "This model is prompted to resolve any contradictions, select the most precise and specific answer components, and integrate complementary information"
  - [corpus] Weak direct corpus evidence on this specific synthesis mechanism; neighbor papers focus on retrieval-side ensembling rather than answer synthesis.
- Break condition: If all ensemble models share a systematic bias or hallucination, synthesis amplifies rather than corrects the error. Structured output tasks (Factoid/List) remain challenging regardless of ensemble size.

## Foundational Learning

- Concept: **BM25 lexical retrieval**
  - Why needed here: Foundation of the IR pipeline; understands term frequency and document length normalization for PubMed-scale retrieval before any neural components.
  - Quick check question: Can you explain why BM25 might return different results for "cancer treatment" vs "treatment of cancer"?

- Concept: **Semantic reranking with cross-encoders**
  - Why needed here: BM25 is lexical; reranking adds query-document interaction modeling to capture semantic relevance beyond keyword overlap.
  - Quick check question: What's the computational tradeoff between running a cross-encoder on 10,000 documents vs. 300 documents?

- Concept: **Zero-shot prompting for structured outputs**
  - Why needed here: The system relies on prompts (not fine-tuning) to generate both Elasticsearch queries and final answers, making prompt engineering critical.
  - Quick check question: Why might a zero-shot prompt produce inconsistent JSON formatting compared to a fine-tuned model?

## Architecture Onboarding

- Component map:
  ```
  Question → [Query Generator: Gemini 2.0 Flash] → [BM25 Elasticsearch: ≤10k docs]
           → [Query Refiner: Gemini 2.5 Pro] (if <5 results)
           → [Semantic Reranker] → Top 300 docs
           → [Answer Generators: Gemini/Claude variants] → Candidate answers
           → [Synthesizer: Gemini 2.0 Flash] → Final answer + confidence
           → [Fallback: Google Search on PubMed/external sources] (if insufficient evidence)
  ```

- Critical path: Query generation → BM25 retrieval → Reranking → Answer generation → Synthesis. The reranking threshold (300 docs) and confidence threshold (0.5) are empirically tuned chokepoints.

- Design tradeoffs:
  - High recall (10k docs) vs. precision (300 docs): Reranking step balances this but risks cutting off relevant evidence.
  - Zero-shot generalizability vs. structured output reliability: Ensemble excels at Yes/No but struggles with Factoid/List formatting.
  - Gemini 2.0 Flash (faster, better results) vs. Claude 3.7 Sonnet (better long-context extraction despite smaller nominal window).

- Failure signatures:
  - <5 documents retrieved: Triggers query refinement path (observed in ~5% of queries).
  - 3-7% of Phase A+ questions lack evidence in Elasticsearch: Triggers fallback to external sources.
  - Low confidence score (<0.5): Triggers re-synthesis at temperature 0.0.
  - Malformed structured outputs on Factoid/List questions: Indicates need to decouple answer generation from formatting.

- First 3 experiments:
  1. **Ablate the reranking step**: Compare QA accuracy using top 300 BM25 results vs. top 300 reranked results to isolate reranker contribution.
  2. **Vary the synthesis confidence threshold**: Test thresholds 0.3, 0.5, 0.7 to find the optimal precision-recall tradeoff for your domain.
  3. **Single-model vs. ensemble comparison**: Run identical questions through individual models and the full ensemble to quantify ensemble gain per question type (Yes/No vs. Factoid vs. List).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does decoupling answer generation from output formatting significantly improve accuracy for Factoid and List questions in zero-shot biomedical QA?
- Basis in paper: [explicit] The authors state in Section 4.2 that "answer generation and formatting should be decoupled into separate stages to improve the results on exact questions requiring more nuanced responses."
- Why unresolved: The current unified zero-shot framework struggled with structured output compliance for non-binary questions, resulting in lower rankings for Factoid (MRR 0.05–0.43) and List types compared to the high accuracy achieved for Yes/No questions.
- What evidence would resolve it: Ablation studies comparing single-stage generation against a modular pipeline (extraction followed by formatting) on BioASQ Factoid and List test sets.

### Open Question 2
- Question: Can uncertainty detection methods be used to dynamically trigger retrieval in biomedical RAG without compromising answer quality?
- Basis in paper: [explicit] Section 5.2 suggests "Uncertainty detection methods" as a future direction to "dynamically trigger retrieval... reducing unnecessary retrievals while maintaining or even improving answer quality."
- Why unresolved: The current system utilizes a fixed, multi-stage retrieval process (query generation, BM25, reranking) for every query, which ensures coverage but may lack efficiency.
- What evidence would resolve it: Performance metrics (accuracy/latency) of a selective-retrieval system compared to the current always-retrieve baseline, specifically analyzing cases where retrieval was skipped successfully.

### Open Question 3
- Question: What are the specific failure modes of "information dilution" in long contexts, and do they affect smaller-context models (e.g., Claude 3.7) differently than larger-context models (e.g., Gemini)?
- Basis in paper: [inferred] The paper notes that "longer contexts generally hurts the answer quality" and observes that Claude 3.7 Sonnet (technically smaller context) outperformed Gemini in extractive tasks, suggesting context window size does not correlate linearly with performance.
- Why unresolved: The study identifies the negative correlation between context length and performance but does not determine if this is due to attention diffusion, retrieval noise, or specific architectural limitations.
- What evidence would resolve it: Error analysis comparing model attention or confidence scores when processing full abstracts versus truncated snippets for the same set of questions.

## Limitations

- The system's reliance on prompt engineering without fine-tuning creates variability in structured output quality, particularly for Factoid and List question types where context length effects degrade performance.
- The query generation pipeline depends heavily on LLM judgment for Elasticsearch query construction, with no clear fallback when generated queries fail validation.
- The ensemble synthesis mechanism lacks transparency in how contradictions are resolved and how confidence scores are computed, making it difficult to diagnose systematic biases.

## Confidence

**High confidence** in claims about ensemble superiority for Yes/No questions, supported by concrete accuracy metrics and competition rankings (1st place on batch 4 of BioASQ 13 Phase A+).

**Medium confidence** in claims about context length effects and information dilution. While the abstract and section 3.1 explicitly state that "longer contexts can dilute performance" and describe context compression through reranking, the exact mechanisms and quantitative thresholds remain somewhat vague.

**Low confidence** in claims about the synthesis mechanism's reliability and the generalizability of the approach to Factoid/List questions. The paper mentions synthesis with confidence scoring but provides limited detail on implementation, and the performance gap between Yes/No and structured output tasks suggests fundamental limitations in the zero-shot approach.

## Next Checks

1. **Ablate the reranking step**: Run the complete pipeline with and without the Google semantic reranker (comparing top 300 BM25 results vs. top 300 reranked results) to quantify the reranker's contribution to answer quality and determine if the 300-document threshold is optimal.

2. **Analyze confidence threshold sensitivity**: Systematically test synthesis confidence thresholds at 0.3, 0.5, and 0.7 across all question types to identify the optimal precision-recall tradeoff and understand how confidence scoring impacts different question formats.

3. **Single-model vs. ensemble decomposition**: For each question type, run identical questions through individual LLMs (Gemini 2.0 Flash, Gemini 2.5 Flash, Claude 3.7 Sonnet) and compare performance against the full ensemble to quantify per-model contributions and identify whether ensemble gains are consistent across biomedical domains.