---
ver: rpa2
title: 'Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy
  in Llama-3.2'
arxiv_id: '2512.22671'
source_url: https://arxiv.org/abs/2512.22671
tags:
- pruning
- expansion
- ratio
- knowledge
- capabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates structured width pruning in GLU-MLP layers
  of Llama-3.2 models, revealing that pruning guided by the Maximum Absolute Weight
  (MAW) criterion does not induce uniform degradation but rather a selective capacity
  dichotomy. While factual knowledge tasks (MMLU, GSM8K) degrade predictably with
  decreasing expansion ratios, instruction-following improves substantially (+46%
  to +75% in IFEval for Llama-3.2-1B and 3B models), and multi-step reasoning remains
  robust.
---

# Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2

## Quick Facts
- **arXiv ID:** 2512.22671
- **Source URL:** https://arxiv.org/abs/2512.22671
- **Reference count:** 27
- **Key outcome:** MAW-guided width pruning in GLU-MLP layers selectively reduces factual knowledge while improving instruction-following and truthfulness discrimination.

## Executive Summary
This study reveals a surprising selective degradation pattern in Llama-3.2 models when applying structured width pruning with Maximum Absolute Weight (MAW) criterion. Contrary to the assumption that pruning causes uniform degradation, the research shows that factual knowledge tasks degrade predictably while instruction-following capabilities improve substantially. The effect is mediated by expansion ratio, with a sweet spot around 2.4× where instruction-following peaks. Most notably, an inverse correlation between factual knowledge and truthfulness discrimination emerges: as parametric knowledge degrades, the model's ability to distinguish true statements from misconceptions improves consistently.

## Method Summary
The method applies structured width pruning to GLU-MLP layers in Llama-3.2-1B and 3B Base models using the Maximum Absolute Weight (MAW) criterion. Pruning targets are the `gate_proj` and `up_proj` layers in the GLU architecture, with neurons removed in paired fashion to maintain architectural coherence. Importance scores are calculated as the sum of maximum absolute weights across both projections, and neurons with lowest scores are eliminated. The approach uses `optipfair` (v0.2.0) for pruning and evaluates performance across factual knowledge (MMLU, GSM8K), instruction-following (IFEval), and truthfulness metrics (TruthfulQA-MC1/2) at various expansion ratios from 1.0× to 4.0×.

## Key Results
- Instruction-following improves +46% to +75% in IFEval for Llama-3.2-1B and 3B models as expansion ratio decreases
- Factual knowledge (MMLU) degrades predictably with decreasing expansion ratios, dropping -53.8% in Llama-3B at 1.07×
- A robust inverse correlation (r = -0.864, p = 0.012 in Llama-3B) exists between factual knowledge capacity and truthfulness metrics
- Energy efficiency improves (up to 23% reduction in J/token), but single-request latency increases
- Instruction-tuned models do not benefit; performance converges to pruned base model

## Why This Works (Mechanism)

### Mechanism 1: Weight-Magnitude Neuron Functional Specialization
Neurons with higher absolute weight magnitudes preferentially support behavioral/algorithmic capabilities, while lower-magnitude neurons store parametric knowledge including common misconceptions. MAW retains high-magnitude neurons and preferentially eliminates low-magnitude ones, creating asymmetric preservation where instruction-following improves while factual knowledge degrades. The paper documents this empirical pattern but does not prove the underlying neural distribution.

### Mechanism 2: Paired-Projection Constraint in GLU Architectures
GLU gating requires identical neuron removal from both gate_proj and up_proj to maintain computational coherence. The gating operation uses element-wise multiplication, and dimension mismatch between projections breaks this operation. MAW accounts for this by summing importance scores across both projections before selection, amplifying selectivity by affecting both value and gate paths simultaneously.

### Mechanism 3: Knowledge-Truthfulness Inverse Correlation
Reducing parametric knowledge selectively removes misconception-prone information, improving truthfulness discrimination. As MMLU drops, TruthfulQA-MC2 improves (+16.7%), with correlation r = -0.864, p = 0.012. This suggests misconceptions are stored in low-weight neurons preferentially eliminated by MAW, though the storage mechanism remains hypothesized.

## Foundational Learning

- **GLU (Gated Linear Unit) Architecture**: The pruning target; understanding why gate_proj and up_proj must be pruned together requires understanding the gating mechanism. Quick check: In the equation `h = (xW_up ⊙ SiLU(xW_gate))W_down`, what happens if W_up has 5000 neurons and W_gate has 4000?
- **Expansion Ratio**: The paper's central independent variable; defined as `intermediate_dim / hidden_dim`. Controls MLP layer capacity. Quick check: If hidden_dim = 2048 and intermediate_dim = 4916, what is the expansion ratio?
- **Structured vs. Unstructured Pruning**: This is structured (neuron-level) pruning, not individual weight removal. Understanding the difference clarifies why the method is "surgical" and hardware-friendly. Quick check: Does removing 50% of individual weights (unstructured) change the tensor dimensions? Does removing 50% of neurons (structured)?

## Architecture Onboarding

- **Component map:** Input (d_model) → [gate_proj] → SiLU ─┐ [up_proj] ───────────┼── ⊙ (element-wise) → [down_proj] → Output (d_model) Both expand to d_ff (intermediate_dim)
- **Critical path:** 1. Calculate importance_scores for all neurons (sum of max-abs from both projections) 2. Rank neurons by score 3. Remove bottom-k% from gate_proj AND up_proj (must be identical indices) 4. Adjust down_proj input dimension to match new d_ff
- **Design tradeoffs:** Energy vs. Latency: -23% J/token but +18% latency in single-request; batch processing mitigates latency penalty Knowledge vs. Instruction-following: Cannot preserve both; MMLU degrades as IFEval improves Base vs. Instruct models: MAW removes instruction-tuning gains (converges to base performance)—not suitable for preserving fine-tuned behavior
- **Failure signatures:** Using VOW or PON instead of MAW: +9,207% LAMBADA perplexity at 10% pruning (catastrophic collapse) Non-paired pruning: Dimension mismatch, runtime error in gating operation Applying to instruction-tuned models: IFEval drops from 36.4% to 14.6% (-59.9%), converging to pruned base model
- **First 3 experiments:** 1. Validate MAW vs. alternatives: Prune Llama-3.2-1B at 10% using MAW, VOW, and PON. Measure WikiText and LAMBADA perplexity. Expect MAW: ~51% increase; VOW: ~337%; PON: ~527%. 2. Reproduce capability dichotomy: Prune to expansion ratios 4.0×, 2.4×, 1.6×. Evaluate MMLU and IFEval. Expect MMLU to drop monotonically; IFEval to peak around 2.4–2.8×. 3. Verify truthfulness correlation: Across all expansion ratios, plot MMLU vs. TruthfulQA-MC2. Calculate Pearson correlation. Expect r ≈ -0.6 to -0.9.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the capability dichotomy and the 2.4× expansion ratio equilibrium point persist in larger models (7B+) and non-Llama GLU architectures? The study is restricted to Llama-3.2-1B and 3B; the authors explicitly state they cannot claim the 2.4× ratio is universally optimal or that the patterns generalize. Replicating the MAW pruning experiments on Llama-70B and Mistral models would resolve this.

- **Open Question 2:** Why does the improvement in instruction-following fail to manifest in instruct-tuned models, where performance instead converges to the pruned base model? The paper hypothesizes that instruction tuning modifies low-magnitude neurons which MAW prioritizes for elimination, but admits this requires further validation. A comparative layer-wise analysis of weight distributions in base vs. instruct models would confirm if alignment knowledge is stored in neurons targeted by MAW.

- **Open Question 3:** Is the observed dichotomy a fundamental property of width pruning or a specific artifact of the Maximum Absolute Weight (MAW) selection criterion? The paper suggests investigating hybrid importance criteria to determine if the capability dichotomy is specific to MAW or a fundamental property of GLU pruning. Developing a stable, non-MAW pruning criterion that maintains model functionality and testing for the selective "robust instruction-following" effect would resolve this.

## Limitations

- The core mechanism linking weight magnitude to functional specialization remains hypothesized rather than proven
- Findings are limited to Llama-3.2 Base models; instruction-tuned versions show opposite behavior
- The inverse correlation between knowledge and truthfulness is documented but its causal basis is unclear
- Generalizability to other model families and pruning criteria beyond MAW is unknown

## Confidence

- **High confidence**: Empirical patterns of selective degradation and basic pruning methodology are reproducible
- **Medium confidence**: The mechanism linking weight magnitude to functional specialization is plausible but not definitively proven
- **Low confidence**: Generalizability to other model families, pruning criteria beyond MAW, or architectural variants beyond GLU-MLP

## Next Checks

1. **Cross-criterion validation**: Apply SliceGPT and AMP pruning criteria to Llama-3.2-3B and test whether they reproduce the knowledge-truthfulness dichotomy or produce uniform degradation as expected for functional suppression methods

2. **Architecture transfer test**: Apply MAW pruning to Llama-3.2 models with GeLU-MLP layers (rather than GLU) to determine if the paired-projection constraint and resulting dichotomy are GLU-specific

3. **Misconception localization experiment**: Using the pruned models, systematically probe neuron activation patterns on known LLM misconceptions to directly verify whether low-weight-magnitude neurons encode misconception-prone knowledge as hypothesized