---
ver: rpa2
title: Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity
arxiv_id: '2511.03606'
source_url: https://arxiv.org/abs/2511.03606
tags:
- inequalities
- concentration
- theorem
- inequality
- self-normalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes novel concentration inequalities for self-normalized
  vector-valued processes beyond the classical sub-Gaussian setting. The key innovation
  lies in constructing a nonnegative supermartingale that cleanly decouples the directional
  component of the process from the tail behavior of the noise, enabling the derivation
  of Bernstein- and Bennett-type bounds for light-tailed noises.
---

# Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity

## Quick Facts
- arXiv ID: 2511.03606
- Source URL: https://arxiv.org/abs/2511.03606
- Reference count: 40
- This paper establishes novel concentration inequalities for self-normalized vector-valued processes beyond the classical sub-Gaussian setting.

## Executive Summary
This paper establishes novel concentration inequalities for self-normalized vector-valued processes beyond the classical sub-Gaussian setting. The key innovation lies in constructing a nonnegative supermartingale that cleanly decouples the directional component of the process from the tail behavior of the noise, enabling the derivation of Bernstein- and Bennett-type bounds for light-tailed noises. The approach builds on tools from Pinelis (1994) to derive a supermartingale that adapts to various tail behaviors, leading to dimension-free concentration inequalities applicable in any separable Hilbert space.

## Method Summary
The paper constructs a nonnegative supermartingale $S_t = \cosh(\lambda \|(\rho I + V_t)^{-1/2} M_t\|) \exp(-\sum_{i=1}^t e_i(\lambda))$ where $e_i(\lambda) = \|G_i\|^2 E_{i-1}[\exp(\lambda|\epsilon_i|) - \lambda|\epsilon_i| - 1]$ and $G_t = (\rho I + V_t)^{-1/2} X_t$. This supermartingale is shown to yield time-uniform concentration bounds by applying Ville's inequality. The construction decouples the norm representation of the vector-valued process from the concentration of the one-dimensional noises, enabling Bernstein- and Bennett-type bounds that adapt to the variance of the noise rather than just its range.

## Key Results
- Constructs nonnegative supermartingales yielding time-uniform concentration bounds for vector-valued processes in separable Hilbert spaces
- Derives Bernstein- and Bennett-type inequalities that adapt to the variance of light-tailed noises rather than just their range
- Demonstrates empirical advantages in online linear regression and kernelized linear bandits where noise variance is small relative to its bound

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The construction decouples the geometric properties of the input vectors from the statistical properties of the noise, enabling dimension-free concentration.
- **Mechanism:** The paper defines $G_t = (\rho I + V_t)^{-1/2} X_t$ and notes that $\|G_t\| \le 1$. By bounding the norm of this direction vector independently of the Hilbert space dimension, the complexity of the input space is isolated from the noise term $\epsilon_t$. The supermartingale is constructed using $e_t(\lambda) = \|G_t\|^2 E_{t-1}[\dots]$, effectively treating the vector process via scalar norms.
- **Core assumption:** The input space is a separable Hilbert space (specifically a (2,1)-smooth Banach space).
- **Evidence anchors:**
  - [abstract]: "...supermartingale construction that cleanly decouples the norm representation of the vector-valued process from the concentration of the one-dimensional noises."
  - [section 4.1]: Explicitly shows $\|G_t\|^2 \le 1$ and decouples the term $\|G_t\| |\epsilon_t|$.
  - [corpus]: "A variational approach to dimension-free self-normalized concentration" discusses similar dimension-free goals.

### Mechanism 2
- **Claim:** Time-uniform validity is achieved by constructing a nonnegative supermartingale and applying Ville's inequality, rather than optimizing for a fixed sample size $n$.
- **Mechanism:** The process $S_t = \cosh(\dots) \exp(-\sum e_i(\lambda))$ is shown to be a nonnegative supermartingale. Ville's inequality (a time-uniform extension of Markov's inequality) guarantees that $S_t$ is unlikely to ever exceed a certain threshold, allowing the bound to hold simultaneously for all $t$ without union bounds or fixed horizon assumptions.
- **Core assumption:** The filtration $\mathcal{F}_t$ correctly models the information flow (Assumption 1), ensuring the supermartingale property holds.
- **Evidence anchors:**
  - [abstract]: "The main theoretical result establishes a nonnegative supermartingale that yields time-uniform concentration bounds..."
  - [section 2.2]: Defines Ville's inequality and its role in "anytime valid" inference.
  - [corpus]: Weak direct corpus links for the specific *supermartingale* construction, though "Uncertainty quantification for Markov chain induced martingales" relates generally.

### Mechanism 3
- **Claim:** Replacing sub-Gaussian assumptions with Bernstein/Bennett conditions allows the bounds to tighten based on the actual variance $\sigma^2$ rather than the worst-case range $B$.
- **Mechanism:** Theorems 2 and 3 utilize the Bernstein condition (Assumption 2) or bounded variance (Assumption 3). This allows the concentration width to scale with the standard deviation $\sigma$ in low-variance regimes, whereas sub-Gaussian bounds scale with the bound $B$. This "second-order" scaling theoretically reduces regret in bandit settings when noise is small.
- **Core assumption:** Noise $\epsilon_t$ must be light-tailed (sub-exponential) or bounded such that the moment generating function is controlled.
- **Evidence anchors:**
  - [abstract]: "...adapt to various light-tailed noise distributions... bounds adapt to variance..."
  - [section 5.2]: Corollary 3 shows regret scaling $O(\sigma \gamma_T \dots)$, explicitly highlighting the variance dependence.
  - [corpus]: "Generalized Kernelized Bandits" explores similar variance-adaptive/bandit connections.

## Foundational Learning

- **Concept: Nonnegative Supermartingales & Ville's Inequality**
  - **Why needed here:** This is the theoretical engine of the paper. Unlike standard concentration inequalities that hold for a fixed $n$, this paper relies on Ville's inequality to provide "anytime-valid" guarantees that hold uniformly over time.
  - **Quick check question:** If a process $M_t$ satisfies $E[M_t | \mathcal{F}_{t-1}] \le M_{t-1}$, does it guarantee that $M_t$ decreases, or just that it is not expected to increase?

- **Concept: Self-Normalization**
  - **Why needed here:** The object of study is $\|(\rho I + V_t)^{-1/2} M_t\|$. Understanding that $V_t$ (the Gram matrix) acts as a normalizer that scales the process $M_t$ is critical. The paper assumes $\|(\rho I + V_t)^{-1/2} X_t\| \le 1$ to bound this self-normalizing effect.
  - **Quick check question:** In linear regression, why do we divide by $X^T X$ (the Gram matrix) when estimating coefficients?

- **Concept: Sub-Gaussian vs. Sub-Exponential (Bernstein Condition)**
  - **Why needed here:** The paper's value proposition is moving "beyond sub-Gaussianity." You must understand that sub-Gaussian noise is strictly light-tailed, whereas the Bernstein condition allows for slightly heavier tails (sub-exponential) and, crucially, variance-dependent bounds.
  - **Quick check question:** If a random variable is bounded by $B$, is it necessarily sub-Gaussian? (Yes). Does using a Bernstein bound on it offer any advantage over a Hoeffding bound? (Yes, if variance is small).

## Architecture Onboarding

- **Component map:**
  - Inputs -> Variance Estimator -> Normalizer -> Supermartingale Core -> Output
  - Inputs: Sequential stream of vector-valued covariates $X_t$ and scalar responses $Y_t$
  - Variance Estimator: Required for empirical Bennett bounds (Theorem 5); estimates $\sigma^2$ online
  - Normalizer: Computes $V_t = \sum X_i X_i^T$ and its inverse square root
  - Supermartingale Core: Computes $S_t$ using $\lambda$ and noise terms to track concentration
  - Output: Confidence radius $J_t(\delta)$

- **Critical path:**
  1. **Assumption Check:** Verify if noise is bounded (use Thm 3/4/5) or just sub-exponential (use Thm 2)
  2. **Variance Estimation:** If using Theorem 5 (Empirical Bennett), implement a parallel variance estimator $\hat{\sigma}^2_{u,t,\delta}$
  3. **Bound Calculation:** Solve the inequality (or root-find for mixed bounds) to determine the current confidence set radius

- **Design tradeoffs:**
  - **Fixed vs. Anytime:** Use Theorems 2 or 3 for fixed-sample experiments (tighter, simpler). Use Theorems 4 or 5 for sequential monitoring/adaptive stopping (adds a $\log$ factor, computationally harder root-finding)
  - **Variance vs. Range:** Use these bounds if $\sigma \ll B$. If variance is close to the range, sub-Gaussian bounds might be tighter due to the extra logarithmic factors inherent in this method (Appendix C)

- **Failure signatures:**
  - **Logarithmic Looseness:** In one-dimensional reductions, the bounds here can be loose by a $\log$ factor compared to optimal scalar Bernstein bounds (see Appendix C discussion)
  - **Heavy Tails:** If noise violates the Bernstein condition (e.g., infinite variance), the mechanism fails completely
  - **Dimension dependence:** If implemented in a non-Hilbert space or the norm bound $\|G_t\| \le 1$ fails, dimension-free guarantees are lost

- **First 3 experiments:**
  1. **Variance-Adaptivity Check:** Run linear bandits with Beta-distributed noise (low variance) vs. Uniform noise (high variance). Compare cumulative regret against a baseline sub-Gaussian UCB algorithm
  2. **Empirical vs. Theoretical Variance:** Compare the performance of Theorem 5 (empirical variance) vs. Theorem 3 (oracle variance) to quantify the cost of unknown variance
  3. **Sequential Stopping:** Apply the Mixed Bennett bound (Thm 4) to an online regression task with adaptive stopping rules to verify time-uniform validity (coverage probability remains $\ge 1-\delta$ regardless of stopping time)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the supermartingale construction be refined to eliminate the extra logarithmic factor present in the current bounds?
- **Basis in paper:** [explicit] The authors identify finding a refined construction to circumvent the loose bound $V_{t-1} \preceq V_t$ as an "important and difficult open direction."
- **Why unresolved:** The current proof technique relies on $V_{t-1} \preceq V_t$, which inherently inflates the bounds by a logarithmic term compared to sub-Gaussian methods.
- **What evidence would resolve it:** A nonnegative supermartingale construction that avoids the $V_{t-1} \preceq V_t$ inequality, or a proof that the logarithmic factor is unavoidable for this class of processes.

### Open Question 2
- **Question:** Can the empirical Bennett-type bounds be extended to handle heteroscedastic noise where variances depend on covariates?
- **Basis in paper:** [explicit] The conclusion notes the empirical Bennett inequality assumes constant variance and expresses interest in extending it to "heteroscedastic noises."
- **Why unresolved:** Theorem 5 relies on variance confidence sequences (from Martinez-Taboada and Ramdas, 2025) that currently do not support covariate-dependent variance estimation.
- **What evidence would resolve it:** A covariate-dependent upper confidence sequence for the variance integrated with the proposed self-normalized concentration bounds.

### Open Question 3
- **Question:** Can these inequalities improve regret guarantees in reinforcement learning settings, such as linear Markov Decision Processes (MDPs)?
- **Basis in paper:** [explicit] The conclusion lists "reinforcement learning" and "safe Bayesian optimization" as natural extensions for future work.
- **Why unresolved:** The paper only validates the theoretical and empirical utility of the bounds in the context of online linear regression and linear bandits.
- **What evidence would resolve it:** Derivation of second-order regret bounds for linear MDPs using the proposed inequalities that outperform standard sub-Gaussian bounds.

## Limitations
- The supermartingale construction requires light-tailed noise assumptions (Bernstein condition), excluding heavy-tailed distributions
- The bounds can be loose by logarithmic factors compared to optimal scalar bounds in low-dimensional settings
- Empirical variance estimation (Theorem 5) adds computational complexity and may underperform when variance is large relative to the bound

## Confidence
- **Supermartingale construction (Mechanism 1&2):** High confidence based on explicit mathematical derivation and established Ville's inequality framework
- **Variance adaptation (Mechanism 3):** Medium confidence—theoretically sound, but empirical advantage depends sensitively on variance-to-bound ratio and noise distribution shape
- **Time-uniform validity:** High confidence based on Ville's inequality application

## Next Checks
1. **Heavy-Tail Robustness:** Test the bounds under Pareto-distributed noise with varying tail exponents to quantify the breakdown point when Bernstein conditions fail
2. **Dimension Sensitivity:** Implement the bounds in a non-Hilbert space (e.g., L¹ or L∞ norms) to verify the dimension-free claim breaks down when the norm bound ∥Gₜ∥ ≤ 1 no longer holds
3. **Sequential Monitoring:** Design a controlled experiment where the optimal stopping time is known a priori, then measure whether the time-uniform bounds maintain coverage probability ≥ 1-δ across all stopping times without prior knowledge of the horizon