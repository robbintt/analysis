---
ver: rpa2
title: Batch List-Decodable Linear Regression via Higher Moments
arxiv_id: '2503.09802'
source_url: https://arxiv.org/abs/2503.09802
tags:
- algorithm
- batches
- list
- batch
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the problem of list-decodable linear regression
  using batches, where an unknown fraction of batches are corrupted. The goal is to
  output a small list of regressors at least one of which is close to the true regressor.
---

# Batch List-Decodable Linear Regression via Higher Moments
## Quick Facts
- arXiv ID: 2503.09802
- Source URL: https://arxiv.org/abs/2503.09802
- Reference count: 40
- Key outcome: Achieves batch size $n = \Omega(\delta)(\alpha^{-\delta})$ and error $O(\alpha^{-\delta/2}/\sqrt{n})$ for list size $O(1/\alpha)$ using SoS-enhanced higher moments

## Executive Summary
This paper addresses list-decodable linear regression where an unknown fraction of data batches are corrupted. The authors develop a novel polynomial-time algorithm that substantially improves upon prior work by leveraging higher-moment information through the Sum-of-Squares (SoS) hierarchy. The key innovation is an iterative approach that combines SoS-based list-decodable mean estimation with a novel list pruning procedure. The algorithm succeeds with significantly smaller batch sizes than previous methods while maintaining small list sizes, under the assumption that covariates have bounded low-degree moments.

## Method Summary
The method employs a three-stage iterative process. First, it uses SoS-based list-decodable mean estimation on batch-averaged regressors, leveraging a novel SoS proof of the Marcinkiewicz-Zygmund inequality to certify higher-moment bounds. Second, it performs iterative residual fitting by transforming labels based on current candidates and solving for residuals. Third, it prunes the candidate list using consistency tests that solve linear inequality systems to maintain list size at $O(1/\alpha)$. The algorithm requires bounded degree-$2t$ moments for certifiable SoS bounds and achieves quasi-polynomial runtime due to high-degree SoS programs.

## Key Results
- Reduces batch size requirement from $\Omega(1/\alpha)$ to $\Omega(\alpha^{-\delta})$ for constant $\delta > 0$
- Achieves error rate $O(\alpha^{-\delta/2}/\sqrt{n})$ compared to prior $O(\sigma/\sqrt{n\alpha})$
- Maintains list size at $O(1/\alpha)$ through novel pruning procedure
- Provides first polynomial-time algorithm for this regime using higher moments

## Why This Works (Mechanism)

### Mechanism 1: SoS-Enhanced Moment Bounds
The algorithm reduces minimum batch size by utilizing higher-order moment information via a Sum-of-Squares (SoS) proof of the Marcinkiewicz-Zygmund inequality. This certifies bounds on $2k$-th central moments of batch-averaged regressors, enabling distinction between signal and noise with smaller batches. The core assumption is that clean covariates have degree-$\Theta(1/\delta)$ moments that are SoS-certifiably bounded.

### Mechanism 2: Iterative Residual Fitting
The system improves accuracy iteratively by reducing to regression with smaller target norm in each step. It estimates a coarse $\hat{\beta}$, transforms labels to $y' = y - \hat{\beta}^\top X$, and solves for the residual. This iterative refinement ensures stability when initial estimates are close to the true regressor.

### Mechanism 3: Cross-Candidate List Pruning
The algorithm prevents exponential list growth by pruning candidates that fail consistency tests against competitors. A candidate is kept only if it demonstrates smaller empirical $\ell_2$ error on distinct batch subsets compared to distant regressors. This uses a soft cluster approach to maintain list size at $O(1/\alpha)$.

## Foundational Learning

**Concept: List-Decodable Learning**
- Why needed: Standard regression assumes mostly correct data, but here inlier fraction $\alpha$ can be $< 0.5$, requiring reasoning about sets of plausible solutions
- Quick check: Can you explain why single-point estimators (like OLS) fail completely when $\alpha < 0.5$, regardless of batch size?

**Concept: Sum-of-Squares (SoS) Proofs**
- Why needed: The algorithm requires algorithmic "proof" of moment bounds; SoS provides increasingly tight but computationally expensive relaxations
- Quick check: What is the computational cost trade-off when increasing degree $k$ of SoS hierarchy?

**Concept: Hypercontractivity ($L_4 - L_2$ norm)**
- Why needed: Assumes covariates are hypercontractive to limit tail heaviness, ensuring outliers cannot dominate higher moments used in SoS proofs
- Quick check: Why do bounded 4th moments provide stronger robustness than bounded 2nd moments?

## Architecture Onboarding

**Component map:** Batch Sampler -> SoS Estimator -> Residual Transformer -> Pruning Module

**Critical path:** The Pruning Module is the novel bottleneck preventing exponential list growth. If the linear program in Eq 16-17 fails to solve efficiently, the algorithm becomes intractable.

**Design tradeoffs:** Batch Size $n$ vs. Computation Time
- Small batch size ($n \approx \alpha^{-\delta}$) requires higher degree $k$ in SoS proofs, leading to $d^{\Omega(k)}$ runtime
- Large batch size ($n \approx 1/\alpha$) allows low-degree (faster) SoS or non-SoS methods

**Failure signatures:**
- List Explosion: Output list size $\gg 1/\alpha$ indicates Pruning Module failed
- Empty List: Consistency test thresholds too aggressive for given noise level $\sigma$

**First 3 experiments:**
1. Batch Size Scaling: Vary $n$ (e.g., $n=10$ vs $n=100$) with fixed $\alpha=0.1$ to verify error bound $O(\alpha^{-\delta/2}/\sqrt{n})$
2. Corruption Threshold: Increase corrupted batch fraction past $1-\alpha$ to identify breaking point
3. Moment Verification: Test on heavy-tailed distributions to confirm SoS certification step is critical dependency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is quasi-polynomial runtime strictly necessary for batch size $n \sim \text{polylog}(1/\alpha)$, or does a fully polynomial-time algorithm exist?
- **Basis:** Paper notes achieving lower batch size regime requires super-constant degree $k$, resulting in quasi-polynomial runtime
- **Resolution:** Would require polynomial-time algorithm with $n \sim \text{polylog}(1/\alpha)$ or super-polynomial hardness proof

### Open Question 2
- **Question:** Can error guarantee be improved to optimal rate $\tilde{O}(\sigma/\sqrt{n})$ independent of $\alpha$?
- **Basis:** Current error rate $O(\alpha^{-\delta/2}/\sqrt{n})$ depends on $\alpha$ due to list-decoding density
- **Resolution:** Would require algorithm achieving $\tilde{O}(\sigma/\sqrt{n})$ with batch size $n \approx \alpha^{-\delta}$

### Open Question 3
- **Question:** Can SoS-certifiable moment requirements be relaxed to standard moment bounds while retaining improved batch sizes?
- **Basis:** Algorithm relies on Assumption 1.2.3 requiring degree-$2t$ moments to be SoS certifiably bounded
- **Resolution:** Would require algorithm operating under standard moment assumptions achieving same guarantees

### Open Question 4
- **Question:** Is batch size constraint $n \gg \log(1/\alpha)$ a fundamental barrier, or can efficient algorithms exist for $n$ between $\log(1/\alpha)$ and $\alpha^{-\delta}$?
- **Basis:** Appendix F provides evidence that $n \ll \log(1/\alpha)$ likely requires exponential time
- **Resolution:** Would require formal hardness proofs for $n = O(\log(1/\alpha))$ or improved algorithms bridging the gap

## Limitations

- Reliance on high-degree SoS proofs creates significant computational barriers, with no empirical validation for degrees beyond k=4
- Pruning mechanism's effectiveness depends on precise numerical tolerance in solving linear inequality systems, which may fail with realistic noise levels
- Complete algorithm performance at scale remains unverified due to lack of empirical results or implementation details for core SoS subroutine

## Confidence

- **High Confidence:** Theoretical framework connecting higher moments to improved batch size requirements through SoS Marcinkiewicz-Zygmund inequality
- **Medium Confidence:** Iterative refinement mechanism appears sound, but list pruning procedure's numerical stability uncertain
- **Low Confidence:** Complete algorithm's practical performance at scale unverified due to missing implementation details

## Next Checks

1. Implement SoS Marcinkiewicz-Zygmund inequality (Lemma 3.3) and verify certification of moment bounds on synthetic batch-averaged regressors under varying degrees k
2. Test iterative refinement process with k=2 (degree-4 SoS) on synthetic data to confirm list size remains O(1/α) after pruning, measuring separation constants
3. Benchmark runtime and solution quality for linear inequality pruning system (Eq 16-17) as noise σ approaches theoretical limits, identifying numerical failure thresholds