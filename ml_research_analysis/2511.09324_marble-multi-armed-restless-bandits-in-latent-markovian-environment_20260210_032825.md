---
ver: rpa2
title: 'MARBLE: Multi-Armed Restless Bandits in Latent Markovian Environment'
arxiv_id: '2511.09324'
source_url: https://arxiv.org/abs/2511.09324
tags:
- whittle
- state
- index
- restless
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MARBLE, a restless multi-armed bandit framework
  with a latent Markovian environment that induces nonstationary behavior through
  abrupt regime changes. The authors address the challenge of learning optimal policies
  when dynamics are driven by unobserved environmental states, relaxing the classical
  stationarity assumption.
---

# MARBLE: Multi-Armed Restless Bandits in Latent Markovian Environment

## Quick Facts
- **arXiv ID**: 2511.09324
- **Source URL**: https://arxiv.org/abs/2511.09324
- **Reference count**: 40
- **Primary result**: Synchronous Q-learning with Whittle Indices converges almost surely to optimal Whittle indices despite unobserved latent regime switches in a restless multi-armed bandit with latent Markovian environment.

## Executive Summary
This paper introduces MARBLE, a restless multi-armed bandit framework with a latent Markovian environment that induces nonstationary behavior through abrupt regime changes. The authors address the challenge of learning optimal policies when dynamics are driven by unobserved environmental states, relaxing the classical stationarity assumption. They propose the Markov-Averaged Indexability (MAI) criterion, which requires only that the average environment be indexable rather than every environment. Under this assumption, they prove that synchronous Q-learning with Whittle Indices (QWI) converges almost surely to the optimal Q-function and Whittle indices despite unobserved regime switches. The method is validated on a calibrated recommender system simulator (digital twin), where QWI consistently adapts to the shifting latent state and converges to an optimal policy, achieving near-identical average rewards compared to the oracle Whittle index policy over 500,000 iterations.

## Method Summary
The method employs synchronous Q-learning with Whittle Indices (QWI) operating on a two-timescale stochastic approximation framework. The fast timescale (step-size α_k) updates Q-values for all state-action pairs using samples from a calibrated simulator that returns environment-averaged transitions. The slow timescale (step-size β_k) updates the Whittle indices by driving the action gap to zero. The algorithm operates under the Markov-Averaged Indexability (MAI) criterion, which requires only that the environment-averaged problem be indexable. The simulator provides Monte Carlo samples from the averaged transition kernel, effectively marginalizing out the unobserved latent state. Step-sizes satisfy α_k = o(β_k) to ensure timescale separation, with α_k = 1/⌈k/10000⌉ and β_k = (1 + ⌈k·log(k)/10000⌉)^(-1) · I{k mod 10 = 0}.

## Key Results
- Synchronous QWI converges almost surely to optimal Whittle indices despite unobserved latent regime switches
- QWI achieves near-identical average rewards compared to the oracle Whittle index policy over 500,000 iterations
- The method consistently adapts to shifting latent states in the recommender system simulator
- Convergence holds under the Markov-Averaged Indexability (MAI) criterion, a relaxation of standard indexability

## Why This Works (Mechanism)

### Mechanism 1: Two-Timescale Stochastic Approximation Decomposition
Q-learning operates on a fast timescale (step-size α_k) while index updates operate on a slow timescale (step-size β_k), with β_k/α_k → 0. This separation allows Q-values to equilibrate for each fixed λ before λ adjusts, satisfying the tracking conditions of Borkar's two-timescale SA theorem. The fast ODE has a globally asymptotically stable equilibrium (a γ-contraction), and the slow ODE converges to the Whittle index via Lyapunov stability.

### Mechanism 2: Markov-Averaged Indexability (MAI) Relaxation
Instead of requiring each latent environment e ∈ E to be individually indexable, MAI requires that the passive region P̄ᵢ(λ) for the averaged MDP expands monotonically from ∅ to S as λ increases. The stationary distribution μ_E of the latent chain provides the averaging weights.

### Mechanism 3: Ergodic Averaging via Calibrated Simulator
A generative simulator Gᵢ(θ) with known parameters provides Monte Carlo samples from the averaged transition kernel. The update noise M_{k+1}^{i,z}(s,a) is a martingale difference sequence because E[r_k|F_k] = rᵢ_E(s,a) when marginalizing over the unobserved E_k via μ_E.

## Foundational Learning

- **Concept: Restless Multi-Armed Bandits and Whittle Relaxation**
  - Why needed here: Arms evolve when passive (unlike classic bandits), making optimal control PSPACE-hard. Whittle's Lagrangian relaxation converts the hard budget constraint to an expected constraint, enabling per-arm decomposition.
  - Quick check question: Explain why relaxing "exactly M arms active" to "M arms active in expectation" allows the global Q-function to decompose as a sum of per-arm Q-functions.

- **Concept: Two-Timescale Stochastic Approximation (Borkar's Theorem)**
  - Why needed here: The entire convergence proof rests on this framework. Understanding fast/slow ODE dynamics and the tracking condition is essential.
  - Quick check question: In coupled recursions x_{k+1} = x_k + a_k h(x_k, y_k) and y_{k+1} = y_k + b_k g(x_k, y_k), what role does b_k/a_k → 0 play in the asymptotic behavior?

- **Concept: Indexability and Whittle Index**
  - Why needed here: The Whittle index λ*ᵢ(z) is the subsidy making activation/passivity indifferent at state z. MAI generalizes this definition to latent environments.
  - Quick check question: For indexability, what monotonicity property must the passive region P(λ) satisfy, and why does this matter for defining a unique index?

## Architecture Onboarding

- **Component map:**
  Per-arm Q-tables → Calibrated simulator → Fast Q-update → Slow index update → Whittle index selection

- **Critical path:**
  1. Initialize Q-tables (zeros or small random) and indices (e.g., zeros)
  2. For each outer iteration k and arm i:
     - Sample (s, a, s') from simulator for all state-action pairs
     - Apply fast update (Eq. 6) with current λ_k^i
  3. Apply slow update (Eq. 7) at reference state z
  4. At deployment: observe arm states, activate M arms with largest indices (ε-greedy optional)

- **Design tradeoffs:**
  - Synchronous vs. asynchronous: Proof covers synchronous (full sweep updates); asynchronous is cited as future work
  - Simulator calibration: Known θ assumed; miscalibration voids guarantees
  - Tabular vs. NN: Proof is tabular; NN extensions ([17], [20]) exist but lack convergence guarantees under MAI

- **Failure signatures:**
  1. Indices oscillating or diverging → Verify step-size conditions (Σα_k = ∞, Σα_k² < ∞, α_k = o(β_k))
  2. Poor asymptotic reward relative to oracle → Check whether MAI actually holds for the averaged problem
  3. Slow/no convergence → Timescale separation may be insufficient; reduce α_k/β_k ratio

- **First 3 experiments:**
  1. Replicate homogeneous recommender setup (N=100, M=10, |S|=4, |E|=2) and verify index convergence to oracle values as in Fig 1(b)
  2. Ablation on MAI: construct a problem where each environment is indexable but the averaged problem is not; observe divergence
  3. Vary α_k/β_k ratio and measure convergence speed vs. stability; identify practical lower bound for timescale separation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does synchronous QWI maintain convergence guarantees when the calibrated simulator provides erroneous parameter estimates (θ) rather than ground-truth values?
- **Basis in paper:** [explicit] The conclusion states: "Future work aims to target synchronous QWI with an erroneously calibrated simulator and asynchronous QWI."
- **Why unresolved:** The theoretical convergence proof (Theorem IV.1) and the algorithm description explicitly assume access to calibrated simulators G_i(θ) with known parameters θ to sample environment-averaged transitions.
- **What evidence would resolve it:** A robustness analysis proving convergence bounds in terms of the simulation-to-reality gap (model bias), or empirical results showing stable index learning as simulator accuracy degrades.

### Open Question 2
- **Question:** Can the QWI algorithm be extended to the asynchronous (online) setting where learning relies on single-step real-time trajectories rather than synchronous full-sweep updates?
- **Basis in paper:** [explicit] The conclusion lists "asynchronous QWI" as a specific target for future work.
- **Why unresolved:** The current analysis relies on a synchronous update rule (Algorithm 1) using a generative model to update all state-action pairs simultaneously, which is often infeasible in live deployment.
- **What evidence would resolve it:** A convergence proof for the asynchronous Q-learning iteration under the MARBLE model, handling the non-stationary latent state without the ability to reset the simulator.

### Open Question 3
- **Question:** Are there structural conditions (e.g., specific transition families like restarting or recovering bandits) that guarantee the Markov-Averaged Indexability (MAI) criterion holds?
- **Basis in paper:** [inferred] The paper introduces the MAI criterion (Assumption III.4) as a relaxed requirement for convergence, but does not characterize the specific RMAB classes or kernel properties that satisfy it.
- **Why unresolved:** While the authors prove that QWI converges if MAI holds, they do not provide sufficient conditions to verify MAI a priori for a given problem instance, unlike standard indexability results for known bandit classes.
- **What evidence would resolve it:** A theoretical characterization proving that common RMAB families satisfy the MAI criterion, or a counter-example showing a standard indexable problem fails to be Markov-Averaged Indexable.

## Limitations
- Requires a calibrated simulator with known parameters, which may be impractical in real-world deployment
- The Markov-Averaged Indexability (MAI) criterion is not commonly verified in practice and lacks clear characterization
- Synchronous algorithm requires full-sweep updates, limiting scalability to large state spaces without function approximation
- Convergence proof is tabular; neural network extensions lack formal guarantees under MAI

## Confidence
- **High confidence**: Convergence proof under MAI (supported by Borkar's theorem and detailed ODE analysis)
- **Medium confidence**: Practical applicability given simulator requirement and MAI verification challenge
- **Low confidence**: Performance on problems where environment-averaged indexability fails

## Next Checks
1. Implement synthetic MARBLE environment with known parameters where MAI holds and verify QWI index convergence to oracle values over 500K iterations
2. Construct counterexample where averaged problem fails indexability (while individual environments are indexable) and demonstrate QWI divergence
3. Test sensitivity to step-size ratios α_k/β_k; measure convergence speed vs. stability across different timescale separations