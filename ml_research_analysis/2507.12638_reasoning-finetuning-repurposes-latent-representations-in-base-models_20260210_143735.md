---
ver: rpa2
title: Reasoning-Finetuning Repurposes Latent Representations in Base Models
arxiv_id: '2507.12638'
source_url: https://arxiv.org/abs/2507.12638
tags:
- steering
- backtracking
- reasoning
- base
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the mechanism behind backtracking behavior
  in reasoning-finetuned language models. The authors find that a direction in the
  residual stream of the base Llama-3.1-8B model can systematically induce backtracking
  when used to steer the DeepSeek-R1-Distill-Llama-8B reasoning model.
---

# Reasoning-Finetuning Repurposes Latent Representations in Base Models

## Quick Facts
- arXiv ID: 2507.12638
- Source URL: https://arxiv.org/abs/2507.12638
- Authors: Jake Ward; Chuqiao Lin; Constantin Venhoff; Neel Nanda
- Reference count: 11
- Key outcome: A direction in the residual stream of base Llama-3.1-8B can induce backtracking in DeepSeek-R1-Distill-Llama-8B, suggesting reasoning finetuning repurposes pre-existing base model representations rather than learning new capabilities from scratch.

## Executive Summary
This paper investigates the mechanism behind backtracking behavior in reasoning-finetuned language models. The authors find that a direction in the residual stream of the base Llama-3.1-8B model can systematically induce backtracking when used to steer the DeepSeek-R1-Distill-Llama-8B reasoning model. This direction does not induce backtracking in the base model itself, suggesting that reasoning finetuning repurposes pre-existing base model representations rather than learning new capabilities from scratch. The authors validate that this direction is not trivially explained by token-level attributes and hypothesize that multiple such directions may work together to mediate backtracking behavior.

## Method Summary
The authors generate 300 reasoning traces from DeepSeek-R1-Distill-Llama-8B across 10 prompt categories, then use GPT-4o to annotate backtracking sentences. They extract layer 10 residual stream activations at negative offsets (-13 to -8 tokens before backtracking events) and compute steering vectors using the Difference-of-Means method. These vectors are added to the residual stream during generation to induce backtracking behavior. The approach is validated by showing the steering vector is not simply boosting backtracking keyword logits directly.

## Key Results
- A direction in Llama-3.1-8B's residual stream induces backtracking in DeepSeek-R1-Distill-Llama-8B but not in the base model itself
- The optimal offset for steering is -13 to -8 tokens before backtracking events at layer 10
- Base-derived and reasoning-derived steering vectors have cosine similarity of ~0.74
- The identified direction is densely present in activations but requires moderate steering strength to be effective

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reasoning finetuning repurposes pre-existing latent representations in base models rather than creating new circuits from scratch.
- **Mechanism:** A direction exists in Llama-3.1-8B's residual stream that, when added to DeepSeek-R1-Distill-Llama-8B activations, systematically induces backtracking. This same direction does not trigger backtracking in the base model, indicating the representation was latently present but only functionally integrated into backtracking circuits during finetuning.
- **Core assumption:** The steering vector captures a causally relevant concept, not a statistical artifact.
- **Evidence anchors:**
  - [abstract] "This direction does not induce backtracking in the base model, suggesting that the reasoning finetuning process repurposes pre-existing representations to form new behavioral circuits."
  - [Section 3.2] Base-derived and reasoning-derived steering vectors have cosine similarity ~0.74, yet only the reasoning model uses this representation to initiate backtracking.
  - [corpus] Related work on self-reflection emergence (arXiv:2506.12217) similarly finds repurposing of latent capabilities via RLVR training.

### Mechanism 2
- **Claim:** Backtracking is triggered by representations formed upstream (8-13 tokens before the backtracking event), not at the moment of backtracking.
- **Mechanism:** Steering vectors computed with negative offset from backtracking events are causally relevant to the model's decision to backtrack. The optimal offset window (~-13 to -8 tokens at layer 10) captures "pre-backtracking" conceptual states rather than the backtracking tokens themselves.
- **Core assumption:** Earlier activations contain predictive signals about subsequent reasoning strategy shifts.
- **Evidence anchors:**
  - [Section 3.1] "The optimal offset for 10th-layer residual stream... is ~-13 to -8. This window usually covers the beginning of the sentence prior to backtracking."
  - [Figure 2] Steering effectiveness peaks at negative offsets, declining at zero offset.
  - [corpus] Related work (arXiv:2510.04128) finds internal states before "wait" tokens modulate reasoning patterns, supporting upstream causation.

### Mechanism 3
- **Claim:** Backtracking is mediated by multiple directions working in concert, not a single direction.
- **Mechanism:** The identified backtracking-inducing direction is densely present across activations and doesn't cleanly correlate with actual backtracking events. It requires moderate steering strength to be effective, suggesting threshold-based combination with other heuristic directions.
- **Core assumption:** Backtracking is a distributed circuit requiring multiple conditions to be satisfied.
- **Evidence anchors:**
  - [Section 4.2] "The base-derived steering direction is densely present in model activations... it does not cleanly correlate with backtracking when used as a probe."
  - [Section 4.2] "The actual backtracking mechanism may involve a linear combination of such heuristics."
  - [corpus] Corpus evidence is limited—no direct confirmation of multi-direction mechanisms in related work.

## Foundational Learning

- **Concept: Residual Stream Steering**
  - Why needed here: The entire method relies on adding steering vectors to residual stream activations to modify behavior.
  - Quick check question: Can you explain why steering at layer 10 rather than layer 0 or layer 31 might have different effects?

- **Concept: Difference-of-Means (DoM) Method**
  - Why needed here: The paper computes steering vectors by subtracting mean activations at non-backtracking positions from mean activations at pre-backtracking positions.
  - Quick check question: Given equation (3), why might DoM be sensitive to dataset composition and position selection?

- **Concept: Logit Lens**
  - Why needed here: Used to verify the steering vector isn't trivially boosting backtracking keyword logits directly.
  - Quick check question: If a steering vector had high "backtracking score" at layer 30 but low score at layer 10, what would that suggest about where the conceptual processing occurs?

## Architecture Onboarding

- **Component map:**
  - Base model: Llama-3.1-8B (32 layers, residual stream dimension ~4096)
  - Reasoning model: DeepSeek-R1-Distill-Llama-8B (same architecture, finetuned)
  - Target layer: Layer 10 residual stream (identified as most effective)
  - Steering pipeline: Generate reasoning traces → GPT-4o annotation → Extract activations at negative offset → Compute DoM vector → Add to residual stream during generation

- **Critical path:**
  1. Generate 300 reasoning traces across 10 categories
  2. Annotate backtracking sentences with GPT-4o
  3. Extract layer-10 activations at offset -8 to -13 from backtracking
  4. Compute steering vector: `v = MeanAct(backtracking_positions) - MeanAct(all_positions)`
  5. During inference: `activation[layer_10] += magnitude * v`

- **Design tradeoffs:**
  - Offset selection: Earlier offsets capture more causally relevant concepts but may miss precise triggers
  - Steering magnitude: Higher values increase backtracking but risk incoherent outputs
  - Layer selection: Earlier layers encode more abstract concepts; later layers encode token-level features

- **Failure signatures:**
  - Steering at wrong layer produces token-level artifacts rather than conceptual shifts
  - Zero offset steering has reduced effectiveness (capturing consequence, not cause)
  - Base model never backtracks regardless of steering (representation exists but circuit doesn't)

- **First 3 experiments:**
  1. Reproduce Figure 3: Steering the reasoning model with base-derived vectors at magnitudes [0, 4, 8, 12, 16, 20], measuring "Wait" token frequency
  2. Implement logit lens analysis: Project steering vector through unembedding matrix at layers [5, 10, 15, 20, 25, 30] to verify non-trivial encoding
  3. Test break condition: Compute steering vectors from a different base model family and measure cross-family transfer—if transfer works, the mechanism may be statistical rather than representation-specific

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What other directions besides the identified one contribute to the backtracking mechanism, and do they combine linearly or non-linearly?
- **Basis in paper:** [explicit] The authors state in Section 4.2: "Our observation that our identified direction is effective at moderate (but not small) steering strengths indicates that the actual backtracking mechanism may involve a linear combination of such heuristics. We leave further investigation of this hypothesis to future work."
- **Why unresolved:** The authors found the direction is densely present even without backtracking, suggesting additional components are required.
- **What evidence would resolve it:** Identifying multiple backtracking-related directions and testing whether their combined steering effects are additive or super-additive.

### Open Question 2
- **Question:** Does the repurposing of latent base model representations for backtracking generalize across different model families and scales?
- **Basis in paper:** [explicit] The conclusion states: "we examined a single reasoning model; further investigation to validate the robustness of our findings across different model families and scales is required in order to claim our findings are general."
- **Why unresolved:** Only Llama-3.1-8B and its distilled reasoning variant were studied.
- **What evidence would resolve it:** Replicating the steering vector derivation and effectiveness experiments on other base/finetuned pairs (e.g., Qwen, Mistral).

### Open Question 3
- **Question:** Why does adding Gaussian noise to activations produce coherent outputs with increased backtracking propensity?
- **Basis in paper:** [explicit] In Section 3.3, the authors note: "We observe anecdotally that this type of intervention results in coherent outputs with increased propensity for backtracking. We leave investigation of this phenomenon as a direction for future work."
- **Why unresolved:** This was an unexpected baseline finding that contradicts assumptions that arbitrary perturbations should degrade output quality.
- **What evidence would resolve it:** Systematic analysis of which activation dimensions noise affects, and whether noise projects onto backtracking-relevant subspaces.

## Limitations
- The generality of the steering direction across model families remains untested, limiting confidence in the broader validity of the repurposing hypothesis.
- The reliance on keyword-based backtracking detection may miss nuanced forms of backtracking or include false positives.
- The optimal offset window (-13 to -8 tokens) has not been validated across different reasoning domains, model scales, or base-reasoning model pairs.

## Confidence
- **High confidence:** The empirical observation that a direction in Llama-3.1-8B's residual stream induces backtracking in DeepSeek-R1-Distill-Llama-8B (Section 3). The experimental results are clear and reproducible.
- **Medium confidence:** The claim that reasoning finetuning repurposes pre-existing base model representations rather than learning new circuits from scratch. While the evidence is suggestive, alternative explanations cannot be fully ruled out without cross-family experiments.
- **Medium confidence:** The hypothesis that backtracking is mediated by multiple directions working in concert. The evidence is circumstantial, but direct confirmation is lacking.
- **Low confidence:** The specific causal mechanism by which the identified direction triggers backtracking behavior. The paper establishes correlation and manipulation but does not definitively prove the causal pathway.

## Next Checks
1. **Cross-family steering transfer test:** Compute steering vectors from a completely different base model family (e.g., GPT-Neo or Mistral) and test whether they can induce backtracking in DeepSeek-R1-Distill-Llama-8B. If successful, this would suggest the mechanism is based on shared statistical structure rather than specific representation repurposing.

2. **Multi-direction ablation study:** Systematically combine or remove multiple candidate steering directions to determine whether backtracking requires specific linear combinations. Use sparse autoencoder analysis to identify additional directions that correlate with backtracking events, then test whether removing or adding these directions modulates backtracking behavior.

3. **Offset window generalization:** Test the optimal offset window across diverse reasoning tasks beyond the 10 categories studied. Include mathematical reasoning, logical deduction, and multi-step planning tasks. Measure whether the optimal offset shifts systematically with task complexity or reasoning depth, which would inform the generality of the upstream causation mechanism.