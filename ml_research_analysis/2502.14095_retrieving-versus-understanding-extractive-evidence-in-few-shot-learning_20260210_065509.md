---
ver: rpa2
title: Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning
arxiv_id: '2502.14095'
source_url: https://arxiv.org/abs/2502.14095
tags:
- evidence
- label
- prediction
- given
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reliability of large language models
  (LMs) in identifying within-document evidence to support their predictions in a
  few-shot learning setting. The authors analyze the correlation between label prediction
  errors and evidence retrieval errors using five datasets with gold-standard human-annotated
  evidence, focusing on two proprietary models (GPT-4 and Gemini-1.5).
---

# Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning

## Quick Facts
- **arXiv ID**: 2502.14095
- **Source URL**: https://arxiv.org/abs/2502.14095
- **Reference count**: 7
- **Primary result**: Large language models reliably quote evidence from input documents in few-shot learning, with prediction errors more commonly stemming from evidence misinterpretation rather than missing key evidence.

## Executive Summary
This paper investigates the reliability of large language models in identifying and quoting evidence from within-document sources to support their predictions in few-shot learning scenarios. The authors analyze five datasets with gold-standard human-annotated evidence, focusing on two proprietary models (GPT-4 and Gemini-1.5). They find that LMs demonstrate strong reliability in quoting evidence from input documents, and critically, that prediction errors are more frequently associated with misinterpreting relevant evidence rather than failing to retrieve key evidence entirely. This suggests that verification applications can trust the evidence retrieval capability of these models, though the interpretation of that evidence remains a challenge.

## Method Summary
The authors conducted a comprehensive analysis of evidence retrieval reliability by comparing model predictions with gold-standard human annotations across five datasets. They examined two proprietary models (GPT-4 and Gemini-1.5) in a few-shot learning setting, focusing specifically on within-document evidence selection. The analysis measured the correlation between label prediction errors and evidence retrieval errors, distinguishing between cases where models failed to retrieve relevant evidence versus cases where they retrieved evidence but misinterpreted it. This approach allowed them to isolate whether failures were primarily due to retrieval issues or understanding issues.

## Key Results
- LMs demonstrate high reliability in quoting evidence from input documents when provided with relevant information
- Prediction errors are more commonly associated with misinterpreting relevant evidence rather than missing key evidence entirely
- Evidence retrieval errors are mostly not associated with evidence interpretation errors, indicating these are largely independent failure modes
- This independence is a positive signal for downstream verification applications that rely on model evidence extraction

## Why This Works (Mechanism)
The study reveals that large language models possess strong extractive capabilities when operating in few-shot settings with provided documents. The models appear to effectively scan and quote relevant passages from input documents, suggesting robust attention mechanisms and document processing capabilities. However, the interpretation of this evidence remains challenging, indicating that while the retrieval mechanism works well, the reasoning over retrieved evidence is where errors accumulate. This separation between retrieval and interpretation suggests that verification systems can focus on validating the reasoning step rather than worrying about evidence extraction quality.

## Foundational Learning

**Evidence retrieval in LMs** - Why needed: Understanding how models identify and extract relevant passages from documents is fundamental to assessing their reliability as information sources. Quick check: Can be validated by comparing model-extracted evidence against human annotations for relevance and completeness.

**Few-shot learning capabilities** - Why needed: The study's findings are specific to few-shot scenarios, which represent a middle ground between zero-shot and fully fine-tuned approaches. Quick check: Evaluate whether results generalize by testing on zero-shot and fine-tuned variants of the same tasks.

**Error correlation analysis** - Why needed: Distinguishing between retrieval errors and interpretation errors requires careful statistical analysis to avoid conflating distinct failure modes. Quick check: Use confusion matrices to explicitly separate error types and calculate their co-occurrence rates.

## Architecture Onboarding

**Component map**: Document input -> Evidence retrieval module -> Evidence interpretation module -> Label prediction -> Error analysis pipeline

**Critical path**: Document input flows through evidence retrieval to interpretation, with final predictions evaluated against gold annotations to identify error patterns.

**Design tradeoffs**: The study focuses on within-document retrieval rather than open-domain retrieval, sacrificing generality for controlled analysis but limiting applicability to broader information-seeking scenarios.

**Failure signatures**: Primary failures manifest as either missing relevant evidence (retrieval failure) or misinterpreting retrieved evidence (understanding failure), with the latter being more common.

**First experiments**:
1. Replicate the analysis using different prompt templates to assess robustness of evidence retrieval
2. Test model performance across varying document lengths to identify scaling effects
3. Compare within-document retrieval performance against cross-document retrieval scenarios

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Analysis is limited to two proprietary models (GPT-4 and Gemini-1.5), potentially limiting generalizability to other architectures or open-source alternatives
- The within-document retrieval setting may not reflect challenges in open-domain information-seeking scenarios
- Results may be sensitive to the specific annotation schemes and task framings used in the five analyzed datasets

## Confidence

**High**: Basic reliability of LMs to quote evidence from input documents
**Medium**: Prediction errors more commonly associated with evidence misinterpretation rather than missing evidence
**Low**: Evidence retrieval errors are mostly not associated with interpretation errors

## Next Checks

1. Replicate the analysis with additional open-source models (e.g., Llama, Mistral) to assess whether proprietary model advantages hold across architectures.

2. Extend the error analysis to cross-document retrieval scenarios and compare error patterns with the within-document setting studied here.

3. Conduct human evaluation studies to validate whether identified evidence interpretation errors align with human judgments of relevance and sufficiency.