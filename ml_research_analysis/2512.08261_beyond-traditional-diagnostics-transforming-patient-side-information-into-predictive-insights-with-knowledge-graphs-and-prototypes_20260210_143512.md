---
ver: rpa2
title: 'Beyond Traditional Diagnostics: Transforming Patient-Side Information into
  Predictive Insights with Knowledge Graphs and Prototypes'
arxiv_id: '2512.08261'
source_url: https://arxiv.org/abs/2512.08261
tags:
- uni00000013
- uni00000011
- disease
- patient
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KPI, a knowledge graph-enhanced and interpretable
  framework for patient-side disease prediction using only patient-provided information
  such as demographics and self-reported symptoms. To address class imbalance and
  lack of interpretability in existing methods, KPI constructs a unified disease knowledge
  graph, initializes clinically meaningful disease prototypes, and employs contrastive
  learning to improve prediction accuracy, especially for rare diseases.
---

# Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes

## Quick Facts
- **arXiv ID**: 2512.08261
- **Source URL**: https://arxiv.org/abs/2512.08261
- **Reference count**: 40
- **Primary result**: KPI achieves state-of-the-art disease prediction accuracy using only patient-provided information, with clinically meaningful explanations

## Executive Summary
This paper introduces KPI, a knowledge graph-enhanced framework for disease prediction using only patient-provided information such as demographics and self-reported symptoms. The method addresses class imbalance and interpretability challenges in traditional diagnostic models by constructing a unified disease knowledge graph, initializing clinically meaningful disease prototypes, and employing contrastive learning to improve prediction accuracy, especially for rare diseases. The framework also generates patient-specific, medically relevant explanations using large language models. Experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art baselines in predictive accuracy while providing clinically valid explanations aligned with patient narratives.

## Method Summary
KPI constructs a unified disease knowledge graph by prompting LLMs to extract entity-relation triplets from authoritative disease descriptions, then initializes disease prototypes via GAT-based graph encoding. The framework employs contrastive learning between patient embeddings and disease prototypes to enhance predictive accuracy, particularly for long-tailed diseases. It also uses semantic consistency regularization to align graph-based and text-based patient representations. During inference, KPI matches patient narratives to the most similar disease prototype using cosine similarity, and generates explanations by prompting LLMs with retrieved subgraphs and patient narratives.

## Key Results
- KPI achieves hit@1 accuracy of 0.4637 even with 0% training data for Lung cancer, demonstrating knowledge transfer
- Maintains strong performance across 6 disease categories with class imbalances up to 1:20.80 (Cold)
- Provides clinically valid explanations rated by experts for Medical Soundness, Patient Alignment, and Clarity

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph-Grounded Disease Prototypes
Structured medical knowledge anchors improve prediction accuracy, especially for rare diseases with limited training samples. KPI constructs a unified disease knowledge graph via LLM triplet extraction and initializes prototypes through GAT-based encoding, creating learnable class anchors that encode clinically meaningful semantic relationships rather than random initialization.

### Mechanism 2: Contrastive Prototype Alignment for Class Imbalance
Contrastive learning between patient embeddings and disease prototypes improves discrimination for underrepresented disease categories. KPI uses InfoNCE loss to pull text-based patient embeddings closer to their ground-truth disease prototype while pushing them away from all other prototypes, providing stable supervision signals even when training samples are sparse.

### Mechanism 3: Dual-Path Semantic Consistency Regularization
Aligning graph-based and text-based patient representations improves grounding and reduces reliance on spurious textual patterns. KPI computes two patient embeddings (from patient-specific subgraph via GAT and from narrative text via fine-tuned sentence transformer) and enforces alignment during training through consistency loss, encouraging the text encoder to implicitly capture relational structure.

## Foundational Learning

- **Graph Attention Networks (GAT)**: Why needed - KPI uses GAT to encode both the unified disease KG and patient-specific subgraphs. Understanding attention-weighted neighbor aggregation is essential for debugging prototype quality and subgraph relevance. Quick check - Given a node with neighbors {A, B, C} and attention weights {0.5, 0.3, 0.2}, what is the contribution of neighbor B to the node's updated representation?

- **Contrastive Learning (InfoNCE)**: Why needed - The core training objective is prototype-based contrastive loss. Understanding how negative sampling and temperature affect representation space is critical for tuning long-tail performance. Quick check - If τ → 0, what happens to the gradient signal for negative samples compared to τ → ∞?

- **Sentence Transformers and Mean Pooling**: Why needed - KPI uses frozen vs. fine-tuned sentence transformers for different components. Understanding pooling strategies and fine-tuning tradeoffs is necessary for architectural decisions. Quick check - Why might mean pooling preserve semantic similarity better than max pooling for clinical symptom phrases?

## Architecture Onboarding

- **Component map**: Disease descriptions → LLM triplet extraction → Semantic alignment/fusion → Unified KG G_u → Patient narratives → TF-IDF keywords → Prototype-augmented query → Subgraph retrieval → Dual encoding (GAT for h_gp, Sentence Transformer for h_p) → Prototype matching via cosine similarity

- **Critical path**: Narrative → Keyword extraction → Subgraph retrieval → h_p computation → Prototype matching. During training, the graph-based path (h_gp) is required; at inference, only h_p is used for efficiency.

- **Design tradeoffs**: Frozen vs. fine-tuned encoder (KG construction uses frozen to preserve biomedical semantics; patient encoding uses fine-tuned to capture patient-specific language), graph-based vs. text-only inference (training uses h_gp as semantic teacher; inference omits it for label-agnostic efficiency), 2-hop subgraph constraint (limits computational cost but may exclude relevant multi-hop medical relationships)

- **Failure signatures**: Low hit@1 on rare diseases despite contrastive training (check prototype separation), explanations mention irrelevant symptoms (check keyword extraction threshold), performance drops when removing KG (verify GAT convergence), inference much slower than baseline (check α_v distribution)

- **First 3 experiments**: 1) Ablation by component - Run KPI, KPI w/o S.C, KPI w/o P.K on all 6 disease categories to verify each component's contribution. 2) Long-tail robustness test - Subsample training data for one category from 100% → 10% while keeping others fixed, plot performance degradation curve. 3) Explanation quality audit - Sample 20 predictions per disease category; have clinicians rate Medical Soundness, Patient Alignment, and Clarity (0-5 scale) compared to LLM-only baselines.

## Open Questions the Paper Calls Out

- **Multimodal extension**: How can KPI be extended to integrate multimodal patient data, such as medical imaging or sensor readings, alongside text narratives? The current architecture relies exclusively on text-based sentence transformers and semantic consistency regularization, which are not designed to process visual or auditory signals.

- **Conditions for degradation**: Under what specific conditions does knowledge-guided prototype alignment degrade performance compared to pure data-driven baselines? Table VI shows that the variant without prototype knowledge outperformed the full KPI model on Coronary Heart Disease, but the paper does not explain why removing the medical knowledge component yielded better results for specific disease types.

- **Scalability**: Can KPI maintain computational efficiency and explanation quality when scaled to the full spectrum of clinical diseases? The current evaluation is limited to only 6 disease categories; graph attention mechanisms and LLM generation may face significant latency issues on a massive knowledge graph.

## Limitations

- **Knowledge Graph Construction Reliability**: The framework depends heavily on LLM-generated triplets from disease descriptions, with no specified source for the disease descriptions, leaving reproducibility uncertain.

- **Long-Tail Performance Bounds**: While KPI shows strong performance on 1:20.80 imbalance, it's unclear how it scales to more extreme imbalances (>100:1) common in rare disease prediction.

- **Explanation Validity**: LLM-generated explanations are evaluated qualitatively but not quantitatively against ground truth, with no validation that explanations align with actual diagnostic reasoning.

## Confidence

- **High Confidence**: Prototype-based contrastive learning improves performance over text-only baselines (directly supported by Table V comparisons across all 6 disease categories)
- **Medium Confidence**: KG grounding provides clinically meaningful prototypes (supported by ablation results but depends on LLM reliability for KG construction)
- **Low Confidence**: Explanations are medically sound and patient-aligned (only qualitative expert evaluation provided, no quantitative metrics or patient feedback validation)

## Next Checks

1. **Extreme Imbalance Test**: Train KPI with one disease category at 1:100 or 1:200 class ratio while keeping others at original ratios. Measure hit@1 degradation compared to baseline methods to identify performance limits.

2. **KG Quality Audit**: Sample 50 disease-symptom triplets from the constructed KG. Have medical experts rate accuracy (correct/incorrect/uncertain). Correlate accuracy rate with prototype performance across diseases to quantify KG noise impact.

3. **Explanation Benchmarking**: For 100 predictions, compare KPI explanations against GPT-4 and Gemini baselines using a standardized rubric (Medical Accuracy, Patient Relevance, Clarity). Compute inter-rater reliability and statistical significance to quantify KG grounding benefit.