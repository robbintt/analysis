---
ver: rpa2
title: 'Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised
  and Generative Approaches'
arxiv_id: '2508.08027'
source_url: https://arxiv.org/abs/2508.08027
tags:
- speech
- dysarthric
- decoding
- recognition
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks self-supervised ASR models (Wav2Vec, HuBERT,
  Whisper) with LLM-enhanced decoding (BART, GPT-2, Vicuna) for dysarthric speech
  recognition. It introduces LLM-based decoding as an integrated approach to improve
  transcription intelligibility, rather than using LLMs solely for post-correction.
---

# Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches

## Quick Facts
- arXiv ID: 2508.08027
- Source URL: https://arxiv.org/abs/2508.08027
- Reference count: 0
- Primary result: Whisper-Vicuna achieves WER of 0.21 on TORGO and 0.26 on UASpeech, outperforming CTC-based models and Whisper alone

## Executive Summary
This study benchmarks self-supervised ASR models (Wav2Vec, HuBERT, Whisper) combined with LLM-enhanced decoding (BART, GPT-2, Vicuna) for dysarthric speech recognition. The research introduces a novel approach where LLMs are integrated directly into the decoding process rather than used solely for post-correction. The primary finding shows that Whisper-Vicuna achieves the lowest WER (0.21 on TORGO, 0.26 on UASpeech), significantly outperforming CTC-based models (WER 0.50-0.54) and Whisper alone (WER 0.38-0.40). This demonstrates that LLM-enhanced decoding improves phoneme restoration and grammatical accuracy by leveraging linguistic constraints, particularly benefiting moderate-to-severe dysarthria cases. Cross-dataset evaluation reveals generalization challenges, with WER increasing notably when models are tested on unseen data, even for LLM-enhanced systems.

## Method Summary
The study evaluates three self-supervised ASR models (Wav2Vec 2.0, HuBERT, Whisper) paired with three LLM-based decoders (BART, GPT-2, Vicuna) using the TORGO and UASpeech dysarthric speech datasets. The ASR models use a bridge network (standard for CTC-based models, Q-Former for Whisper) to connect with LLM decoders that generate final transcriptions. Models are evaluated using Word Error Rate (WER) and Character Error Rate (CER) metrics. The methodology introduces LLM-based decoding as an integrated approach to improve transcription intelligibility, rather than using LLMs solely for post-correction. Cross-dataset evaluation is performed to assess generalization capabilities.

## Key Results
- Whisper-Vicuna achieves the lowest WER: 0.21 on TORGO and 0.26 on UASpeech
- CTC-based models show higher WER: 0.50-0.54 across both datasets
- Whisper alone performs worse than Whisper-Vicuna: WER 0.38-0.40
- Cross-dataset evaluation shows significant performance degradation for all models
- LLM-enhanced decoding improves phoneme restoration and grammatical accuracy

## Why This Works (Mechanism)
The integration of LLMs into the decoding process leverages their strong language modeling capabilities to correct phoneme-level errors and improve grammatical structure in dysarthric speech transcriptions. By incorporating linguistic constraints during the decoding phase rather than as post-processing, the system can better handle the acoustic distortions characteristic of dysarthric speech. The Vicuna model, with its larger parameter count and fine-tuning, provides superior language understanding compared to smaller models like GPT-2, enabling more effective error correction. This approach addresses the core challenge that dysarthric speech often contains phoneme substitutions, deletions, and insertions that standard ASR models struggle to correct.

## Foundational Learning

**Self-supervised learning for ASR**: These models (Wav2Vec, HuBERT, Whisper) are pre-trained on large unlabeled speech datasets to learn general speech representations, which are then fine-tuned on specific tasks. This is needed because dysarthric speech data is scarce and expensive to annotate.

**Bridge networks**: These modules connect the encoder representations from ASR models to the decoder inputs of LLMs. Different bridge architectures are required for CTC-based models versus encoder-decoder models like Whisper.

**LLM-enhanced decoding**: Integrating LLMs directly into the decoding process rather than using them for post-correction allows the model to leverage linguistic context during transcription generation, improving error correction for phoneme-level distortions.

**CTC vs encoder-decoder architectures**: CTC-based models predict character sequences directly while encoder-decoder models generate sequences autoregressively. This architectural difference requires different bridge mechanisms when connecting to LLMs.

**Cross-dataset generalization**: Testing models on datasets different from training data reveals how well the approach generalizes across different speakers, recording conditions, and dysarthria severities.

**Quick check**: Compare WER improvements when using LLM-enhanced decoding versus post-correction approaches to validate the integrated approach's effectiveness.

## Architecture Onboarding

**Component map**: Speech Input -> ASR Encoder -> Bridge Network -> LLM Decoder -> Text Output

**Critical path**: The bridge network is critical as it must effectively transform ASR encoder representations into a format suitable for LLM input while preserving acoustic information necessary for error correction.

**Design tradeoffs**: Larger LLMs (Vicuna) provide better performance but require more computational resources, creating a tradeoff between accuracy and real-time feasibility. The bridge architecture must balance between preserving ASR features and adapting to LLM requirements.

**Failure signatures**: Poor cross-dataset performance indicates overfitting to specific acoustic characteristics or speaker patterns. High WER with CTC-based models suggests limitations in handling phoneme-level distortions without strong language modeling support.

**First experiment**: Evaluate different bridge architectures (standard vs Q-Former) with the same ASR model to isolate the impact of bridge design on performance.

**Second experiment**: Compare LLM-enhanced decoding with post-correction approaches using the same ASR model to quantify the benefit of integrated decoding.

**Third experiment**: Test different decoding strategies (beam search parameters) with LLM-enhanced systems to optimize the tradeoff between accuracy and efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-enhanced decoding strategies mitigate the severe cross-dataset generalization failures observed when models trained on one dysarthric corpus are applied to another?
- Basis in paper: [explicit] The authors note in Section 4 that "Cross-dataset generalization is poor" and Table 3 shows WER increases significantly even for the best model (Whisper-Vicuna) when tested on unseen datasets.
- Why unresolved: The study demonstrates the problem exists but only suggests "adaptive learning strategies" as a future direction without testing them.
- What evidence would resolve it: Successful experiments showing stable WER performance when training on TORGO and testing on UASpeech (or vice versa) using improved adaptation techniques.

### Open Question 2
- Question: How can a unified ASR-LLM framework be designed to accommodate diverse encoder-decoder configurations, specifically enabling the evaluation of pairings like Whisper Encoder with BART?
- Basis in paper: [explicit] Section 4 states that "architectural constraints restrict broader encoder-decoder evaluations, such as Whisper Encoder + BART."
- Why unresolved: The current methodology relies on specific bridge mechanisms (Q-Former vs. standard Bridge Network) that may not be universally compatible with all model combinations.
- What evidence would resolve it: A modular architecture benchmark that successfully integrates Whisper's encoder with smaller LLMs (BART/GPT-2) without performance degradation or compatibility errors.

### Open Question 3
- Question: To what extent can data augmentation or synthetic speech generation improve the robustness of LLM-enhanced dysarthric ASR?
- Basis in paper: [explicit] Section 4 identifies limited data as a hindrance and explicitly states, "Data augmentation or synthetic speech generation could help."
- Why unresolved: The study relies on existing benchmark datasets (TORGO, UASpeech) and does not experiment with data expansion techniques.
- What evidence would resolve it: A comparative study showing improved WER and cross-dataset generalization after training LLM-decoders on augmented or synthetic dysarthric speech data.

## Limitations
- Evaluation limited to English datasets (TORGO, UASpeech), restricting cross-linguistic generalizability
- LLM integration requires substantial computational resources, potentially limiting real-time applications
- Decoding strategy optimization (e.g., beam search parameters) was not investigated, leaving performance improvement opportunities unexplored

## Confidence

**High confidence**: Whisper-Vicuna significantly outperforms CTC-based models and Whisper alone on TORGO and UASpeech datasets, with WER reductions of 40-50%.

**Medium confidence**: Cross-dataset generalization challenges are real but may be influenced by dataset-specific characteristics beyond dysarthria severity.

**Low confidence**: The claim that LLM-enhanced decoding specifically improves "phoneme restoration" is not explicitly demonstrated - while grammatical accuracy improves, the mechanism for phoneme-level corrections is not directly shown.

## Next Checks

1. Test the Whisper-Vicuna approach on additional dysarthric speech datasets in different languages to assess cross-linguistic generalization capabilities.

2. Conduct ablation studies with different LLM sizes and decoding configurations to optimize the tradeoff between performance and computational efficiency for practical deployment.

3. Implement user studies with dysarthric speakers to validate that LLM-enhanced transcriptions actually improve communication effectiveness, not just WER metrics.