---
ver: rpa2
title: 'Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs'
arxiv_id: '2510.07429'
source_url: https://arxiv.org/abs/2510.07429
tags:
- cost
- performance
- arxiv
- tasks
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently routing queries
  to the most suitable large language model (LLM) from a pool of candidates, balancing
  performance and cost. The core method, BARP (Bandit-feedback Routing with Preferences),
  formulates LLM routing as a multi-objective contextual bandit problem.
---

# Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs

## Quick Facts
- arXiv ID: 2510.07429
- Source URL: https://arxiv.org/abs/2510.07429
- Reference count: 19
- This paper proposes BARP, a bandit-feedback routing policy that adapts to user-defined accuracy-cost trade-offs at inference time, achieving strong generalization and performance across diverse LLM routing tasks.

## Executive Summary
This paper addresses the challenge of efficiently routing queries to the most suitable large language model (LLM) from a pool of candidates, balancing performance and cost. The core method, BARP (Bandit-feedback Routing with Preferences), formulates LLM routing as a multi-objective contextual bandit problem. BARP learns from bandit feedback—only observing the outcome of the chosen model—while conditioning on a user preference vector that specifies the trade-off between accuracy and cost. This allows the router to adapt to different user priorities without retraining. Comprehensive experiments show that BARP consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, while generalizing robustly to unseen tasks.

## Method Summary
BARP learns a routing policy as a contextual bandit, where the context is the concatenation of a prompt embedding and a preference vector, and the arms are the available LLMs. During training, the policy samples actions and receives bandit feedback (reward only for the chosen model), using entropy-regularized REINFORCE to optimize for the user-specified performance-cost trade-off. At inference, the router can dial the preference vector to adjust the balance between accuracy and cost without retraining. The policy is trained on a diverse set of tasks and LLMs, and is evaluated both in-distribution and on held-out tasks.

## Key Results
- BARP outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45% in average score.
- BARP generalizes well to unseen tasks, with only a 1.08% drop in average score when evaluated on held-out task families.
- The preference vector enables effective control over the performance-cost trade-off, with monotonic score and cost decreases as the cost weight increases.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training under bandit feedback (partial supervision) aligns with deployment conditions and improves generalization to unseen tasks.
- Mechanism: During training, the policy samples an action $a_t \sim \pi_\theta(\cdot|x_t, w_t)$ and receives reward only for that chosen LLM. This simulates the deployment constraint where only the selected model's outcome is observable. Policy gradients (REINFORCE) optimize the expected cumulative reward using this partial signal.
- Core assumption: The bandit-feedback training distribution sufficiently approximates the deployment distribution; the reward function $r_t = w_q^t q_t - w_c^t \tilde{c}_t$ captures the true utility trade-off.
- Evidence anchors:
  - [abstract] "trains under the same partial-feedback restriction as deployment"
  - [section 2.1] Defines context, arms, action, and reward with bandit feedback; Eq. (1) specifies reward formulation
  - [corpus] Related work on bandit-based routing (C2MAB-V, MAR) confirms bandit framing but lacks preference-tunable inference
- Break condition: If the cost cap $\tau$ is mis-specified (too low or high), normalized cost $\tilde{c}_t$ may dominate or underweight cost, skewing the learned trade-off.

### Mechanism 2
- Claim: Conditioning the policy on a preference vector $w = (w_q, w_c)$ enables inference-time control of the performance-cost trade-off without retraining.
- Mechanism: The preference encoder $\phi(w)$ maps the 2-D simplex vector to a higher-dimensional embedding, concatenated with prompt embedding $h(x)$ to form $z = [h(x); \phi(w)]$. The decision head $g_\theta(z)$ outputs logits over LLMs. Training randomly samples $w_t$ uniformly on the 1-simplex, teaching the policy to respond to varying preferences.
- Core assumption: The preference space (1-simplex) covers the operational trade-offs users need; the MLP $\phi$ can learn a useful mapping from sparse 2-D input.
- Evidence anchors:
  - [abstract] "operators can dial the performance–cost trade-off at test time without retraining"
  - [section 2.2] Describes architecture: frozen prompt encoder $h$, preference MLP $\phi$, decision head $g_\theta$, softmax output
  - [section 4.4.1] Figure 3 shows monotonic score decrease and cost decrease as $w_c$ varies from 0.2 to 0.8
  - [corpus] No directly comparable preference-tunable bandit router found; LLM Bandit (Li, 2025) introduces preferences but relies on full-label pretraining
- Break condition: If preference embeddings are not sufficiently disentangled from prompt embeddings, the policy may fail to generalize to unseen preference combinations at inference.

### Mechanism 3
- Claim: Entropy-regularized REINFORCE with a batch-mean baseline reduces variance and encourages exploration under sparse bandit feedback.
- Mechanism: The per-sample loss $L_t(\theta) = -(r_t - b_t) \log \pi_\theta(a_t|s_t) - \beta H(\pi_\theta(\cdot|s_t))$ combines policy gradient with entropy bonus. The baseline $b_t = \frac{1}{B}\sum_{i=1}^B r_t^{(i)}$ reduces variance; entropy coefficient $\beta = 0.05$ prevents premature convergence.
- Core assumption: The batch-mean baseline is a reasonable control variate; entropy regularization strength $\beta$ is correctly tuned.
- Evidence anchors:
  - [section 2.3] Eq. (5) defines the loss; Section 3.4 specifies $\beta = 0.05$
  - [section 4.6] Table 7 shows REINFORCE with MLP outperforms LinUCB, LinTS, $\epsilon$-greedy by 8-12% absolute score, suggesting non-linear policy is necessary
  - [corpus] Weak direct evidence; corpus includes bandit work in other domains (tutoring, navigation) but not entropy-regularized LLM routing
- Break condition: If $\beta$ is too high, the policy remains overly stochastic, hurting exploitation; if too low, it may collapse to a suboptimal deterministic policy early.

## Foundational Learning

- Concept: **Contextual Bandits**
  - Why needed here: The routing problem is framed as a contextual bandit (context = prompt + preference, arms = LLMs). Understanding the partial-feedback setting is essential to grasp why full-information methods (RouterDC, GraphRouter) are mismatched to deployment.
  - Quick check question: Can you explain why observing only the reward of the chosen arm makes off-policy evaluation harder than supervised learning?

- Concept: **Policy Gradient Methods (REINFORCE)**
  - Why needed here: BARP uses REINFORCE to optimize the expected reward under stochastic policy sampling. The entropy regularization and baseline are modifications you must understand to debug training instability.
  - Quick check question: What is the role of the baseline $b_t$ in reducing variance, and why is the batch-mean a practical choice?

- Concept: **Multi-Objective Optimization with Scalarization**
  - Why needed here: The reward $r_t = w_q^t q_t - w_c^t \tilde{c}_t$ scalarizes two objectives (score and cost) using user preferences. Understanding scalarization trade-offs helps diagnose when the router may behave unexpectedly.
  - Quick check question: If cost and score scales differ by orders of magnitude, what could go wrong even with the $\tau$ cap?

## Architecture Onboarding

- Component map:
  - Input: Prompt $x$ (tokenized, max 512 tokens) + preference vector $w \in \mathbb{R}^2$
  - Prompt encoder: Frozen `all-MiniLM-L6-v2` (384-dim output)
  - Preference encoder: Trainable 2-layer MLP with ReLU ($d_p$-dim output)
  - Fusion: Concatenation $z = [h(x); \phi(w)]$
  - Decision head: 2-layer MLP with ReLU → logits over $K$ LLMs
  - Output: Softmax over logits → action distribution $\pi_\theta(a|x, w)$

- Critical path:
  1. Encode prompt and preference → concatenate
  2. Pass through decision head → softmax
  3. Sample action during training (argmax at inference)
  4. Observe only chosen LLM's score and cost
  5. Compute reward and update policy

- Design tradeoffs:
  - **Encoder choice**: `MiniLM-L6-v2` (384-dim) outperforms BERT-base and matches E5-large (Table 5); compact encoder is sufficient, reducing latency
  - **Decision head**: MLP beats linear and bilinear (Table 6); non-linearity captures complex routing function
  - **Bandit algorithm**: REINFORCE with MLP beats classical linear bandits (LinUCB, LinTS) by >8% (Table 7), but classical methods have slightly lower cost

- Failure signatures:
  - **Score collapse on OOD tasks**: If policy overfits to training tasks, check entropy coefficient $\beta$ and training distribution coverage
  - **Cost insensitivity**: If varying $w_c$ does not change cost, verify cost normalization $\tilde{c}_t = \min(c_t/\tau, 1)$ and $\tau$ setting
  - **High variance gradients**: If training is unstable, increase batch size or adjust baseline $b_t$

- First 3 experiments:
  1. **Preference sweep validation**: Fix a held-out prompt set; vary $w_c \in \{0.2, 0.5, 0.8\}$ and plot both score and cost curves. Confirm monotonic trade-off as in Figure 3.
  2. **Encoder ablation**: Swap `MiniLM-L6-v2` for `BERT-base-uncased` and `E5-large-v2`. Compare routing accuracy and inference latency. Expect MiniLM to win on cost-effectiveness.
  3. **Learning algorithm comparison**: Implement LinUCB and $\epsilon$-greedy baselines on the same context representation. Compare average score and cost to REINFORCE (replicate Table 7) to validate non-linear policy benefit.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not test robustness to cross-dataset distribution shift, leaving uncertainty about generalization beyond task families.
- The effect of mis-specified cost cap $\tau$ on routing behavior is not analyzed, raising concerns about cost normalization sensitivity.
- The entropy regularization coefficient $\beta$ is fixed; its impact on convergence and exploitation-exploitation balance across tasks is not explored.

## Confidence
- **High confidence**: The core mechanism of bandit-feedback training (Mechanism 1) and preference conditioning (Mechanism 2) is well-supported by both theoretical framing and empirical results.
- **Medium confidence**: The variance reduction from entropy-regularized REINFORCE (Mechanism 3) is plausible but less directly validated.
- **Low confidence**: Claims about robustness to unseen tasks and preferences are inferred from generalization to held-out task families, but not explicitly tested under controlled distribution shift scenarios.

## Next Checks
1. **Distribution Shift Test**: Train BARP on a subset of tasks (e.g., MMLU, BigBench), then evaluate on a completely disjoint set (e.g., natural instructions, human-written prompts). Measure score and cost drops to quantify robustness.
2. **Preference Generalization Test**: Train on a restricted preference range (e.g., $w_c \in [0.3, 0.7]$), then test on extreme preferences ($w_c = 0.1, 0.9$). Assess whether the policy can interpolate or extrapolate preferences.
3. **Cost Cap Sensitivity**: Systematically vary $\tau$ (e.g., 10%, 50%, 200% of observed max cost) and measure routing accuracy and cost trade-offs. Identify regimes where cost normalization breaks down.