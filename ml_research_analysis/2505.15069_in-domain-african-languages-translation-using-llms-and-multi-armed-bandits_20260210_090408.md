---
ver: rpa2
title: In-Domain African Languages Translation Using LLMs and Multi-armed Bandits
arxiv_id: '2505.15069'
source_url: https://arxiv.org/abs/2505.15069
tags:
- translation
- machine
- data
- domain
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of domain adaptation in low-resource
  Neural Machine Translation (NMT) by proposing a bandit-based model selection approach.
  It uses contextual bandit algorithms (UCB, LinUCB, Neural Linear Bandit, and Thompson
  Sampling) to dynamically select the best NMT model for specific domains using limited
  in-domain data.
---

# In-Domain African Languages Translation Using LLMs and Multi-armed Bandits

## Quick Facts
- **arXiv ID**: 2505.15069
- **Source URL**: https://arxiv.org/abs/2505.15069
- **Reference count**: 18
- **Primary result**: Bandit-based NMT model selection achieves 2.68% average BLEU improvement over best individual model

## Executive Summary
This paper addresses the challenge of domain adaptation in low-resource Neural Machine Translation (NMT) by proposing a bandit-based model selection approach. It uses contextual bandit algorithms (UCB, LinUCB, Neural Linear Bandit, and Thompson Sampling) to dynamically select the best NMT model for specific domains using limited in-domain data. The method leverages feature embeddings from a language-agnostic encoder and applies reward signals based on BLEU and COMET metrics when reference translations are available, or CometKiwi for target-free scenarios. Experiments on three African languages (Yoruba, Swahili, Igbo) across News, Movies, and Religious domains show that bandit-based methods achieve competitive or superior performance compared to the best individual models, with UCB showing an average improvement of 2.68% in BLEU score.

## Method Summary
The approach treats NMT model selection as a multi-armed bandit problem where each translation model is an "arm." Source sentences are encoded using LaBSE to create context vectors, which contextual bandits (LinUCB, Neural Linear) use to make input-aware model selections. For reward computation, when parallel data exists, a weighted combination of BLEU and COMET metrics is used; otherwise, CometKiwi serves as a reference-free quality estimation metric. The bandit algorithms learn online which models perform best for specific input types and domains, balancing exploration of different models with exploitation of known good performers.

## Key Results
- UCB bandit achieves 2.68% average BLEU improvement over the best individual model (NLLB)
- Contextual bandits (LinUCB, Neural Linear) show competitive performance when LaBSE embeddings are informative
- Target-free bandit using CometKiwi demonstrates effectiveness without requiring reference translations
- Method works across three African languages (Yoruba, Swahili, Igbo) and three domains (News, Movies, Religious)

## Why This Works (Mechanism)

### Mechanism 1: Contextual Bandit Optimization for Model Selection
Framing NMT model selection as a multi-armed bandit problem enables statistically principled model choice with limited data. Each NMT model acts as an "arm," and the bandit algorithm balances exploration (trying different models) with exploitation (selecting models that previously performed well). UCB explicitly computes confidence bounds around estimated rewards, requiring fewer samples to identify optimal models than exhaustive evaluation.

### Mechanism 2: Language-Agnostic Sentence Embeddings as Bandit Context
LaBSE embeddings provide domain-relevant context signals that contextual bandits (LinUCB, Neural Linear) can use to make input-aware model selections. Each source sentence is encoded into a dense vector via LaBSE, which captures semantic content independent of language. Contextual bandits condition arm selection on these embeddings, learning which models perform better on specific input types.

### Mechanism 3: Reference-Free Quality Estimation for Target-Free Scenarios
CometKiwi enables bandit learning even when reference translations are unavailable, using quality estimation as a proxy reward. In low-resource settings, parallel data may not exist for validation. CometKiwi predicts translation quality directly from source-hypothesis pairs, providing a reward signal for bandit updates without gold references.

## Foundational Learning

- **Multi-Armed Bandits and the Exploration-Exploitation Tradeoff**
  - Why needed: The entire method rests on bandit algorithms
  - Quick check: Can you explain why UCB might select a model with lower observed average reward over a model with higher observed reward?

- **BLEU and COMET Metrics for Machine Translation**
  - Why needed: These metrics form the reward signal
  - Quick check: Why might a translation have high BLEU but low COMET, and which metric better captures semantic adequacy for domain adaptation?

- **Language-Agnostic Sentence Embeddings (LaBSE)**
  - Why needed: LaBSE embeddings are the sole input features for contextual bandits
  - Quick check: What types of sentence features would LaBSE need to capture for LinUCB to learn domain-specific model preferences?

## Architecture Onboarding

- **Component map**: Source sentence → LaBSE encoder → 768-dim embedding → Bandit algorithm → NMT model (arm) → Translation → Reward computation (BLEU+COMET or CometKiwi) → Bandit update

- **Critical path**: 1) Encode source sentence with LaBSE, 2) Bandit algorithm selects an NMT model (arm) based on context and history, 3) Selected model translates source → target, 4) Compute reward (BLEU+COMET or CometKiwi), 5) Update bandit parameters, 6) Repeat for next sentence

- **Design tradeoffs**:
  - UCB vs. Contextual (LinUCB, Neural Linear): UCB is simpler but ignores input features; LinUCB/Neural Linear may improve if context correlates with optimal model, but require more exploration data
  - λ parameter (BLEU vs. COMET weight): Paper uses λ=0.4 (favoring COMET), but optimal value may differ by domain/language
  - Exploration set size: Too small → bandit doesn't converge; too large → inefficient

- **Failure signatures**:
  - Bandit converges to single arm immediately: Exploration parameter (α in UCB) may be too low, or one model dominates across all domains
  - High variance in selected models without performance gain: Context features may not be informative; consider context-free bandits
  - Target-free bandit underperforms parallel-data bandit significantly: CometKiwi may be poorly calibrated for the language pair

- **First 3 experiments**:
  1. Baseline replication: Implement UCB with BLEU-only reward on one language-domain pair (e.g., English-Yoruba News) using 1000 samples. Verify ~2-3% BLEU improvement over random model selection.
  2. Ablation on reward composition: Compare λ∈{0.0, 0.4, 0.5, 1.0} to isolate BLEU vs. COMET contribution. Check if λ=0.4 generalizes across languages.
  3. Target-free validation: Run CometKiwi-based bandit on held-out data where references exist (but hide them from the bandit). Correlate CometKiwi reward trajectories with actual BLEU/COMET to assess quality estimation reliability.

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal reward weighting parameter (λ) between BLEU and COMET change when applied to a wider variety of low-resource languages or domains outside of the three African languages tested? The authors state they "find that λ = 0.4 achieves the best results in our case," implying this value was determined empirically for this specific dataset but may not be a universal constant.

### Open Question 2
How does the performance of contextual bandits (LinUCB, Neural Linear) degrade when the language-agnostic encoder (LaBSE) provides poor or noisy sentence representations for the target language? The effectiveness of context-based bandits is strictly coupled to the quality of the feature embeddings; if LaBSE fails to capture semantic similarity for a specific unseen language, the bandit loses its ability to condition selection on the input.

### Open Question 3
Can the bandit-based selection approach maintain its efficiency and low regret when the candidate model pool scales from 5 models to a significantly larger set? Multi-armed bandit algorithms generally suffer from increased regret as the number of arms grows; the efficiency demonstrated in a 5-arm setting may not scale linearly to scenarios with dozens of specialized models without algorithmic modifications.

### Open Question 4
Is the target-free scenario (using CometKiwi) robust against "reward hacking," where the bandit learns to select a model that maximizes the quality estimation score without improving actual translation accuracy? Quality Estimation (QE) metrics are proxy metrics and can sometimes diverge from human judgment or reference-based scores, potentially leading the bandit to exploit a model that "games" the QE metric.

## Limitations
- CometKiwi calibration is not empirically validated for the specific African languages tested
- Context feature relevance is assumed but not directly validated through ablation studies
- Model stability and convergence with only 1,000 samples per domain/language is not quantified

## Confidence

- **High confidence**: The general framework of using multi-armed bandits for model selection is sound and well-established. The 2.68% BLEU improvement claim with UCB is supported by the experimental setup and methodology description.

- **Medium confidence**: The contextual bandit improvements (LinUCB, Neural Linear) over context-free methods depend on LaBSE embeddings being domain-relevant, which is plausible but not directly validated. The CometKiwi-based target-free approach is methodologically sound but lacks empirical calibration validation.

- **Low confidence**: Without seeing the actual correlation between CometKiwi scores and ground truth quality for these specific languages, or ablation studies on context feature importance, key claims about these specific contributions remain uncertain.

## Next Checks

1. **CometKiwi calibration study**: For each language (Yoruba, Swahili, Igbo), compute CometKiwi scores on a held-out test set where references exist. Measure Pearson/Spearman correlation between CometKiwi scores and actual BLEU/COMET scores to validate that CometKiwi is a reliable proxy reward signal.

2. **Context feature ablation**: Run ablation experiments comparing contextual bandits (LinUCB, Neural Linear) against context-free bandits (UCB, Thompson Sampling) on the same datasets. If contextual methods do not show consistent improvement, this would indicate LaBSE embeddings are not capturing relevant domain features.

3. **Bandit stability analysis**: For each bandit algorithm and language-domain pair, compute the variance in arm selection over the 1,000-sample evaluation period. High variance or rapid convergence to suboptimal arms would indicate instability requiring hyperparameter tuning or more samples.