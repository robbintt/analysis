---
ver: rpa2
title: 'Robust equilibria in continuous games: From strategic to dynamic robustness'
arxiv_id: '2512.08138'
source_url: https://arxiv.org/abs/2512.08138
tags:
- equilibrium
- equilibria
- games
- which
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of Nash equilibria in continuous
  games, addressing both strategic and dynamic uncertainty. The authors introduce
  the concept of strategic robustness, defining equilibria that remain invariant under
  small perturbations to the game's payoff structure.
---

# Robust equilibria in continuous games: From strategic to dynamic robustness

## Quick Facts
- **arXiv ID:** 2512.08138
- **Source URL:** https://arxiv.org/abs/2512.08138
- **Reference count:** 40
- **Key outcome:** Characterizes robust Nash equilibria in continuous games via geometric gradient conditions and proves equivalence between strategic and dynamic robustness under FTRL dynamics.

## Executive Summary
This paper establishes a rigorous framework for analyzing robust Nash equilibria in continuous games under both strategic and dynamic uncertainty. The authors define strategic robustness as invariance under small perturbations to payoff structures and characterize such equilibria geometrically as extreme points where gradients lie in the interior of polar cones. They then show that strategic robustness implies dynamic robustness under FTRL dynamics, meaning equilibria remain stable under learning even with stochastic feedback. The work provides convergence guarantees for FTRL with entropic regularization on affinely constrained domains at geometric rates.

## Method Summary
The paper investigates robust equilibria in continuous games by defining strategic robustness through gradient-based perturbation metrics and geometric characterizations. The main approach uses FTRL (follow the regularized leader) dynamics with dual averaging to analyze convergence behavior under stochastic feedback. The method involves verifying equilibrium robustness through polar cone conditions, implementing FTRL with appropriate regularizers (steep or non-steep), and analyzing convergence rates based on the game's geometry and domain structure. The analysis distinguishes between local convergence results and specific conditions (like polyhedral domains) that enable stronger guarantees.

## Key Results
- Strategic robustness requires equilibria to lie at extreme points with gradients in the interior of polar cones
- Strategic robustness implies dynamic robustness under FTRL dynamics, but not vice versa
- Entropic regularization on affinely constrained domains achieves geometric convergence rates
- Standard uniform metrics fail to predict equilibrium stability; gradient-based metrics are necessary

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard uniform metrics for game similarity fail to predict equilibrium stability; gradient-based metrics succeed.
- **Mechanism:** Small uniform ($L^\infty$) perturbations to payoffs can induce high-frequency oscillations in the gradient field, arbitrarily destroying first-order stationarity conditions. By measuring game distance via the sup-norm of the difference between **gradient fields** ($\sup \|v(x) - \tilde{v}(x)\|_*$), the definition captures the structural stability required for equilibria to persist.
- **Core assumption:** The game is continuous with $C^1$ payoff functions.
- **Evidence anchors:**
  - [Page 7, Proposition 1]: "For any game $G$ and any equilibrium $x^* \in X$... there exists a perturbed game $\tilde{G}$, arbitrarily close to $G$ in the uniform metric... such that $x^*$ is not an equilibrium."
  - [Page 8, Definition 1]: Defines distance as $\sup \|v(x) - \tilde{v}(x)\|_*$.
  - [Corpus]: The paper "The impact of uncertainty on regularized learning in games" (Lotidis et al.) elaborates on how randomness affects these dynamics.
- **Break condition:** If the problem requires analyzing discrete finite games (where $L^\infty$ is valid) or non-differentiable payoff landscapes.

### Mechanism 2
- **Claim:** Strategic robustness is strictly determined by the location of the gradient vector relative to the normal cone of the action space.
- **Mechanism:** An equilibrium is robust if and only if the gradient vector $v(x^*)$ lies in the **interior** of the polar cone $PC(x^*)$. This geometric strictness ensures that even if the gradient is nudged by perturbations, the vector remains within the cone, preserving the variational inequality $\langle v(x^*), x - x^* \rangle \leq 0$.
- **Core assumption:** The action space $X$ is a compact convex subset.
- **Evidence anchors:**
  - [Abstract]: Mentions "geometric characterization... as extreme points of the action space with sharp gradient conditions."
  - [Page 9, Theorem 1]: Establishes equivalence between strategic robustness and $v(x^*) \in \text{int}(PC(x^*))$.
- **Break condition:** If the equilibrium lies in the relative interior of $X$ (where the polar cone has empty interior) or on a "flat" boundary portion.

### Mechanism 3
- **Claim:** "Follow the Regularized Leader" (FTRL) converts the geometric stability of robust equilibria into dynamic stability via dual averaging.
- **Mechanism:** FTRL maintains a dual variable $y_t$ that aggregates historical gradients. If an equilibrium is strategically robust, the expected gradient field points strictly "inward." This causes $y_t$ to drift deep into the polar cone (in the dual space), forcing the mirror map $Q(y_t)$ to select actions approaching $x^*$ with high probability, effectively filtering out stochastic noise.
- **Core assumption:** Feedback is stochastic (SFO) or payoff-based with vanishing perturbation radius (SPSA), and step-size $\gamma$ is small enough.
- **Evidence anchors:**
  - [Page 10, Theorem 2]: Proves local convergence to robust equilibria

## Foundational Learning

### Gradient-based perturbation metrics
- **Why needed:** Standard uniform metrics fail to capture the structural stability required for equilibria to persist under perturbations
- **Quick check:** Verify that small uniform perturbations can destroy first-order stationarity conditions while gradient-based perturbations preserve them

### Polar cone geometry
- **Why needed:** Determines whether gradient vectors remain within constraint boundaries after perturbations
- **Quick check:** Confirm that robust equilibria require gradients to lie in the interior of polar cones

### Dual averaging in FTRL
- **Why needed:** Aggregates historical gradient information to maintain stability under stochastic feedback
- **Quick check:** Verify that dual variables drift into polar cones for robust equilibria

## Architecture Onboarding

### Component map
FTRL algorithm -> Dual variable updates -> Mirror map projection -> Action selection -> Stochastic feedback -> Gradient estimation

### Critical path
Equilibrium verification -> FTRL implementation with appropriate regularizer -> Convergence monitoring under stochastic feedback

### Design tradeoffs
Steep vs non-steep regularizers: Steep regularizers (entropic) provide better stability and geometric rates but may have computational overhead; non-steep regularizers are simpler but can fail to converge to robust equilibria

### Failure signatures
- Divergence or oscillation indicates either non-robust equilibrium or inappropriate regularizer choice
- Subgeometric convergence suggests step-size issues or non-affinely constrained domains
- Sensitivity to initialization reveals basin of attraction limitations

### First experiments
1. Implement 1-player game example and verify strategic robustness classification via polar cone condition
2. Run FTRL with both entropic and quadratic regularizers on robust equilibrium, measuring convergence rates
3. Test FTRL behavior on non-robust equilibrium to verify divergence patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the geometric convergence rates to robust equilibria be established for general convex action spaces and non-decomposable regularizers?
- **Basis in paper:** [explicit] Theorem 3 establishes convergence rates explicitly only for "polyhedral domains" and "decomposable" regularizers (Page 10).
- **Why unresolved:** The proof relies on the specific variational geometry of polyhedral sets and the separability of decomposable regularizers to characterize active constraints and the rate function $\phi$.
- **What evidence would resolve it:** A convergence proof for non-polyhedral sets (e.g., ellipsoids) or non-decomposable regularizers, or a counter-example showing the rate strictly depends on polyhedral structure.

### Open Question 2
- **Question:** Under what specific conditions does the "eager" Mirror Descent (MD) algorithm converge to robust equilibria when using non-steep regularizers?
- **Basis in paper:** [explicit] Remark 7 contrasts FTRL with Mirror Descent, noting that for non-steep regularizers, MD fails to converge to a robust equilibrium in a toy example due to noise sensitivity (Page 11).
- **Why unresolved:** The paper focuses on FTRL, proving its dynamic robustness, while the analysis of MD is limited to showing that the "eager" nature of MD can destabilize convergence in specific instances.
- **What evidence would resolve it:** A theoretical characterization of MD stability for non-steep regularizers, or a proof that FTRL's dynamic robustness extends to MD only for steep regularizers.

### Open Question 3
- **Question:** Can global convergence guarantees be derived for specific classes of continuous games that admit a unique robust equilibrium?
- **Basis in paper:** [explicit] The paper limits results to local convergence because "continuous games may admit multiple Nash equilibria, global convergence guarantees are in general unattainable" (Page 10).
- **Why unresolved:** The convergence analysis is local, requiring initialization within a neighborhood of the equilibrium ($x_1 \in U$), as the gradient fields may point away from the equilibrium globally.
- **What evidence would resolve it:** A proof that for strictly monotone or potential games with a unique robust equilibrium, the basin of attraction is the entire strategy space.

## Limitations
- Results depend on compactness and differentiability assumptions that may not hold in all practical game settings
- Numerical verification is challenging due to sensitivity to step-size and perturbation bounds
- Geometric convergence rates require affinely constrained domains, limiting applicability

## Confidence
- **Theoretical claims:** High
- **Numerical reproduction:** Medium (due to unspecified constants and perturbation structures)
- **Practical applicability:** Medium (domain restrictions and assumption requirements)

## Next Checks
1. Implement the 1-player game example from Proposition 4 and verify strategic robustness classification by checking the gradient-polar cone condition
2. Run FTRL with both entropic and quadratic regularizers on a known robust equilibrium and measure convergence rates under different step-size schedules
3. Test FTRL behavior on a non-robust equilibrium to verify divergence or dependence on regularizer choice as predicted by Proposition 4