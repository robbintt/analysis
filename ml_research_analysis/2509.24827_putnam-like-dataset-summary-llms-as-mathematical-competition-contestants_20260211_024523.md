---
ver: rpa2
title: 'Putnam-like dataset summary: LLMs as mathematical competition contestants'
arxiv_id: '2509.24827'
source_url: https://arxiv.org/abs/2509.24827
tags:
- problems
- dataset
- mathematical
- solutions
- putnam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the performance of six large language models
  on the Putnam-like dataset, a benchmark consisting of 96 original mathematical problems
  inspired by the Putnam Competition and 576 LLM-generated solutions. The models evaluated
  include Google's Gemini family (gemini-2.5-pro-03-25 and gemini-2.5-flash-04-17),
  OpenAI's o3-mini-high and o4-mini-high, Anthropic's Claude Sonnet (sonnet-3.7),
  and DeepSeek's r1.
---

# Putnam-like dataset summary: LLMs as mathematical competition contestants

## Quick Facts
- arXiv ID: 2509.24827
- Source URL: https://arxiv.org/abs/2509.24827
- Authors: Bartosz Bieganowski; Daniel Strzelecki; Robert Skiba; Mateusz Topolewski
- Reference count: 16
- Primary result: Gemini 2.5 Pro achieved highest average score (8.68) on 96 original Putnam-style proof problems

## Executive Summary
This paper evaluates six large language models on mathematical proof generation using a benchmark of 96 original problems inspired by the Putnam Competition. The models produced solutions that were graded by human experts on a 0-10 scale for completeness and rigor. Gemini 2.5 Pro emerged as the top performer with an average score of 8.68, demonstrating the most mature mathematical reasoning capabilities among the tested models. The analysis reveals distinct behavioral patterns across models, with Gemini producing detailed, fully justified proofs while other models like DeepSeek r1 often provided only solution sketches without sufficient justification.

## Method Summary
The study evaluates six LLMs (Gemini 2.5 Pro/Flash, o3-mini-high/o4-mini-high, Claude Sonnet, DeepSeek r1) on 96 original Putnam-style problems across seven mathematical categories. Models were prompted with only problem statements (no explicit rigor requirements), and human experts graded all 576 solutions using step-based rubrics worth 10 points total. The dataset was designed to avoid contamination with existing benchmarks while maintaining comparable difficulty to actual Putnam competitions.

## Key Results
- Gemini 2.5 Pro achieved the highest average score of 8.68, significantly outperforming other models
- Score distributions were bimodal for top models (near-0 or near-10), indicating all-or-nothing proof synthesis
- Analysis and polynomial categories presented the greatest challenges, with median scores below 10
- Models showed stable behavioral patterns: Gemini produced detailed proofs, OpenAI models showed exploratory reasoning, and DeepSeek r1 frequently provided only sketches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models exhibit distinct "proof style" profiles that significantly impact grading outcomes under rigorous human evaluation
- Mechanism: Training data composition and response-length preferences create stable behavioral patterns—some models default to exhaustive justification (Gemini), others to exploratory sketches (OpenAI, DeepSeek). Under grading protocols requiring full rigor, completeness directly determines score distribution
- Core assumption: The prompting (minimal instructions, no explicit rigor requirements) reveals intrinsic model tendencies rather than prompt-driven behavior
- Evidence anchors:
  - [abstract]: "Gemini models produced detailed, fully justified proofs with few intermediate responses, while OpenAI's models showed exploratory reasoning... DeepSeek's r1 model frequently provides only a sketch of a solution"
  - [Section 2.4]: "r1 model frequently provides only a sketch of a solution, without providing any details or justifications"
  - [corpus]: Putnam-AXIOM (arXiv:2508.08292) confirms that final-answer benchmarks miss proof-quality differences

### Mechanism 2
- Claim: Mathematical category specialization emerges from differential exposure to proof techniques during training
- Mechanism: Categories like linear algebra (high scores: Gemini Pro 9.25 average) versus analysis (lower scores: Gemini Pro 8.53) reflect uneven representation of proof paradigms in training corpora. Analysis problems requiring epsilon-delta arguments exposed systematic weaknesses
- Core assumption: Training data contains more linear algebra and discrete math proofs than rigorous analysis proofs with explicit approximation bounds
- Evidence anchors:
  - [Section 2.3]: "the median is lower than 10 in three categories: abstract algebra, analysis, and polynomials"
  - [Section 2.3]: "some of the models did not get the maximum score because of the use of numerical computations and unjustified approximations"
  - [corpus]: FMC (arXiv:2507.11275) addresses formalization of competition problems

### Mechanism 3
- Claim: Bimodal score distributions (near-0 or near-10) indicate all-or-nothing proof synthesis rather than incremental reasoning
- Mechanism: For top models like Gemini 2.5 Pro, most solutions are either fully correct or fundamentally flawed—partial credit is rare. This suggests successful proof generation requires identifying the correct approach early; failure at this stage cascades into complete breakdown
- Core assumption: The scoring rubric's step-based partial credit structure accurately captures meaningful intermediate progress
- Evidence anchors:
  - [abstract]: "bimodal scoring distributions"
  - [Section 2.4]: "most solutions of this model [gemini-2.5-pro] were completely correct or completely incorrect; the number of partially correct solutions is very small"
  - [corpus]: IMProofBench (arXiv:2509.26076) notes existing benchmarks focus on final answers

## Foundational Learning

- Concept: **Proof rigor vs. intuition gradient**
  - Why needed here: The paper distinguishes models that "identify key ideas" from those producing "fully rigorous justifications"—evaluating LLMs requires calibrating where solutions fall on this spectrum
  - Quick check question: Can you explain why "the integrand is concentrated near u≈n" is insufficient without explicit bounds?

- Concept: **Dataset contamination vs. novelty**
  - Why needed here: The Putnam-like dataset was designed to avoid contamination affecting Putnam-AXIOM; understanding this tradeoff is essential for benchmark interpretation
  - Quick check question: Why might a model solve Putnam-2024 problems worse than Putnam-like problems despite the latter being "easier"?

- Concept: **Grading rubric construction**
  - Why needed here: Human experts used step-based rubrics; scores from 3–7 were more frequent than in actual Putnam competitions due to granular partial credit
  - Quick check question: How does a rubric that awards points for intermediate steps differ from binary correct/incorrect grading in what it measures?

## Architecture Onboarding

- Component map:
  - Problem set (96 problems, 7 categories) -> Model inference (6 LLMs) -> Human grading (step-based rubrics) -> Score aggregation

- Critical path:
  1. Problem selection → category labeling → difficulty assignment
  2. Model inference (minimal prompting, no rigor requirements)
  3. Human grading against rubric → score assignment
  4. Aggregation by model, category, difficulty level

- Design tradeoffs:
  - **Contamination avoidance** (original problems) vs. **comparability** (actual Putnam difficulty unknown)
  - **Minimal prompting** (reveals intrinsic behavior) vs. **instruction-following evaluation** (standardized output)
  - **Human grading** (accuracy) vs. **scalability** (automated grading correlation not yet validated)

- Failure signatures:
  - **Hallucinated citations**: Gemini invoking "Ryser's theorem" inapplicable to the problem
  - **Circular reasoning**: o3-mini-high committing petitio principii
  - **Sketch-only outputs**: DeepSeek r1 consistently providing outlines without justification
  - **Numerical approximation without bounds**: Analysis solutions losing points for unjustified approximations

- First 3 experiments:
  1. **Prompt sensitivity test**: Re-run with explicit "provide complete rigorous proof with all justifications" instruction; measure distribution shift toward higher completeness scores
  2. **Category stratified sampling**: Isolate analysis problems requiring epsilon-delta or approximation arguments; compare model performance with/without hints about proof technique
  3. **Partial credit calibration**: Have independent graders rescore a subset using competition-style holistic grading vs. step-based rubrics; quantify the rubric-induced distribution shift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does automated grading by LLMs (e.g., Gemini 2.5 Pro) correlate with human expert grading when evaluating rigorous mathematical proofs?
- Basis in paper: [explicit] The authors state, "The correlation of the human and automatic grades needs to be studied in future work," noting they relied exclusively on human grades for this analysis
- Why unresolved: While the dataset includes grades from both human experts and a model (gemini-2.5-pro_20250718), the paper treats human grades as the ground truth and does not statistically compare them against the model's automatic evaluations
- What evidence would resolve it: A statistical analysis (e.g., Cohen's Kappa or Pearson correlation) comparing the scores assigned by human experts versus the automated model grades across the 576 solutions

### Open Question 2
- Question: How does modifying the prompt to explicitly require rigorous justification affect the ability of "sketch-prone" models (like DeepSeek r1) to produce complete mathematical proofs?
- Basis in paper: [explicit] The conclusion identifies the need for "exploring the impact of advanced prompting techniques on the models’ ability to generate rigorous mathematical proof"
- Why unresolved: The models were prompted only with problem statements. The authors infer that the "sketch-like" behavior of some models may be due to this lack of explicit instructions, but this causal link is not tested
- What evidence would resolve it: A comparative experiment where specific models (e.g., DeepSeek r1, o3-mini-high) are re-evaluated on the same tasks using prompts that explicitly demand detailed, step-by-step justification

### Open Question 3
- Question: What specific failure modes cause LLMs to underperform significantly on problems involving mathematical analysis and polynomials compared to linear algebra?
- Basis in paper: [inferred] The paper notes lower median scores in analysis and polynomials and explicitly states: "The explanation of this fact requires a detailed study"
- Why unresolved: The analysis identifies the performance gap (linear algebra is easier, analysis/polynomials are harder) but only speculates on causes, such as unjustified numerical approximations in analysis problems, without a systematic error analysis
- What evidence would resolve it: A fine-grained error taxonomy of the failed solutions in the analysis and polynomial categories, quantifying errors like "unjustified approximation," "conceptual misunderstanding," and "algebraic error"

## Limitations

- The dataset uses original problems rather than actual Putnam competition questions, introducing uncertainty about direct comparability to human performance levels
- The automated grading approach (using Gemini models to evaluate other models) was not analyzed, leaving questions about inter-annotator reliability
- The study relies on a single grading protocol and does not explore how different prompting strategies might affect model performance

## Confidence

- **High Confidence**: The relative ranking of models (Gemini 2.5 Pro > other models) and the observation that Gemini models produce more detailed, justified proofs than other models
- **Medium Confidence**: The categorization of model "proof styles" and their stability across different problem types
- **Low Confidence**: The exact relationship between dataset difficulty and actual Putnam competition difficulty, as well as the generalizability of findings to non-competition mathematical reasoning tasks

## Next Checks

1. **Contamination Verification**: Cross-reference the Putnam-like dataset problems against Putnam-AXIOM to confirm the claimed zero-overlap and validate that the dataset successfully avoids contamination while maintaining comparable difficulty levels

2. **Grading Protocol Replication**: Have independent mathematical experts grade a subset of solutions using both the step-based rubric and traditional competition-style holistic grading to quantify how much the rubric structure influences score distributions and model rankings

3. **Prompt Sensitivity Analysis**: Re-run the evaluation with modified prompts that explicitly require "complete rigorous proofs with all justifications" and compare the resulting score distributions to assess whether observed style differences persist under different prompting strategies