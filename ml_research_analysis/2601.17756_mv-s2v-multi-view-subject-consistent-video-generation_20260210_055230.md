---
ver: rpa2
title: 'MV-S2V: Multi-View Subject-Consistent Video Generation'
arxiv_id: '2601.17756'
source_url: https://arxiv.org/abs/2601.17756
tags:
- video
- reference
- subject
- generation
- multi-view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of single-view subject-to-video
  generation by proposing a multi-view subject-to-video generation (MV-S2V) task that
  enforces 3D-level subject consistency. The key challenge is the lack of training
  data with multi-view subject displays and the need to distinguish between cross-subject
  and cross-view references in conditioning.
---

# MV-S2V: Multi-View Subject-Consistent Video Generation

## Quick Facts
- arXiv ID: 2601.17756
- Source URL: https://arxiv.org/abs/2601.17756
- Reference count: 13
- Synthesizes multi-view consistent videos with superior 3D subject consistency metrics

## Executive Summary
MV-S2V addresses the fundamental limitation of single-view subject-to-video generation by introducing a multi-view subject-to-video generation task that enforces 3D-level subject consistency. The method tackles the data scarcity problem by developing a synthetic data curation pipeline using existing image-to-video models to generate customized training videos with multi-view subject showcases, complemented by a small real-world captured dataset. The core innovation, Temporally Shifted RoPE (TS-RoPE), uses rotary position encoding to clearly separate different subjects and distinct views of the same subject in the conditioning process, enabling the model to distinguish between cross-subject and cross-view references effectively.

## Method Summary
The framework fine-tunes Phantom-Wan (based on Wan 2.1 T2V) with a novel TS-RoPE conditioning mechanism that temporally shifts video and reference tokens to separate cross-subject and cross-view information. The synthetic data pipeline generates 11,804 OC and 10,130 HOI samples using Uni3C for OC videos, Wan2.2 for HOI videos, and Grounded SAM for reference extraction. Training uses Rectified Flow loss with FusedAdam optimizer, batch size 64, and learning rate 1e-5 for 2000 iterations. Inference employs UniPC sampler with specific CFG parameters. The approach is evaluated on NAVI benchmark with comprehensive metrics including multi-view consistency (DINO/CLIP similarity, MEt3R) and 3D consistency (NN distance via π3 point clouds).

## Key Results
- Achieves superior multi-view and 3D subject consistency compared to state-of-the-art methods
- Successfully handles both Object-Centric (camera orbiting static objects) and Human-Object Interaction scenarios
- Demonstrates high-quality visual outputs with improved text-video alignment via ViCLIP metrics

## Why This Works (Mechanism)
The method works by explicitly encoding temporal and view information through TS-RoPE, which creates clear separation between different subjects and views in the rotary position encoding space. This separation allows the model to learn distinct representations for cross-subject and cross-view relationships, enabling consistent generation across multiple views while maintaining subject identity.

## Foundational Learning
- **Multi-view subject consistency**: Needed to generate videos that maintain consistent subject appearance across different camera views; quick check: verify DINO/CLIP similarity scores between generated and reference views.
- **Rotary Position Encoding (RoPE)**: Required for encoding positional information in transformer architectures; quick check: confirm that positional embeddings are correctly applied to both video and reference tokens.
- **Rectified Flow training**: Used for stable diffusion model training with better convergence properties; quick check: monitor training loss stability and sample quality progression.
- **Synthetic data curation**: Essential for creating training data with multi-view subject displays; quick check: validate that generated synthetic videos contain the required multi-view subject showcases.
- **Cross-subject vs cross-view separation**: Critical for distinguishing between different subjects and different views of the same subject; quick check: test that model maintains subject identity across views while differentiating between subjects.
- **3D consistency metrics**: Needed to quantitatively evaluate spatial consistency across views; quick check: compute π3 point cloud distances between generated and reference views.

## Architecture Onboarding
- **Component map**: Phantom-Wan base model -> TS-RoPE conditioning -> Rectified Flow training -> Multi-view reference generation -> NAVI evaluation
- **Critical path**: TS-RoPE implementation → synthetic data generation → model fine-tuning → multi-view consistency evaluation
- **Design tradeoffs**: Trade-off between synthetic data volume and real-world data quality; trade-off between TS-RoPE complexity and conditioning effectiveness
- **Failure signatures**: Object fragmentation or deformation indicates incorrect TS-RoPE implementation; "copy-paste" effects suggest insufficient decoupling between video frames and references
- **First experiments**: 1) Implement TS-RoPE with varying temporal shift δ values and validate cross-view consistency; 2) Generate synthetic multi-view videos using Uni3C/Wan2.2 and test reference extraction pipeline; 3) Fine-tune on synthetic data and evaluate basic multi-view consistency metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown temporal shift parameter δ for TS-RoPE is not specified, critical for correct implementation
- Real-world captured dataset (1,724 + 1,514 samples) is not publicly available, limiting exact reproduction
- Internal 3D asset collections for objects and humans are not specified or shared

## Confidence
- High confidence in theoretical framework and problem formulation
- Medium confidence in practical implementation details and training procedure
- Low confidence in exact result reproduction due to data and hyperparameter unavailability

## Next Checks
1. Implement and validate TS-RoPE with different δ values (starting with 64) to determine optimal separation between video and reference tokens
2. Recreate synthetic data pipeline using open alternatives and evaluate whether resulting training data achieves similar multi-view subject displays
3. Systematically test reference augmentation effects and temporal separation on final video quality and subject consistency metrics