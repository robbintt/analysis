---
ver: rpa2
title: A Human-In-The-Loop Approach for Improving Fairness in Predictive Business
  Process Monitoring
arxiv_id: '2508.17477'
source_url: https://arxiv.org/abs/2508.17477
tags:
- process
- attributes
- event
- sensitive
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a human-in-the-loop approach to improve fairness
  in predictive business process monitoring (PBPM). The method addresses the challenge
  of handling sensitive attributes that may be used both fairly and unfairly within
  the same process instance.
---

# A Human-In-The-Loop Approach for Improving Fairness in Predictive Business Process Monitoring

## Quick Facts
- arXiv ID: 2508.17477
- Source URL: https://arxiv.org/abs/2508.17477
- Reference count: 34
- Introduces a human-in-the-loop approach to improve fairness in predictive business process monitoring by identifying and removing unfairly biased decision nodes

## Executive Summary
This paper presents a novel human-in-the-loop approach to address fairness challenges in predictive business process monitoring (PBPM). The method tackles the complex problem of handling sensitive attributes that may be used both fairly and unfairly within the same process instance. By combining knowledge distillation with human expert review, the approach successfully reduces unfairness while maintaining higher accuracy than models that completely ignore sensitive attributes. The method demonstrates effectiveness across various scenarios with different bias strengths and numbers of biased decisions, offering a promising tradeoff between fairness and accuracy in PBPM applications.

## Method Summary
The approach employs knowledge distillation to transfer knowledge from a black-box PBPM model into an interpretable decision tree model. Human experts then review the decision tree to identify and remove unfairly biased decision nodes. After these alterations, the model is fine-tuned using the fairer decision tree's predictions. This iterative process allows for the identification and elimination of bias while preserving model accuracy. The method is evaluated on both synthetic data with controlled bias parameters and real-life event logs, demonstrating its effectiveness in reducing unfairness while maintaining predictive performance.

## Key Results
- Successfully reduces unfairness in PBPM while maintaining higher accuracy than models ignoring sensitive attributes
- Demonstrates effectiveness across various scenarios with different bias strengths and numbers of biased decisions
- Shows promising tradeoff between fairness and accuracy in PBPM applications

## Why This Works (Mechanism)
The approach works by leveraging the interpretability of decision trees to expose bias patterns that are hidden in black-box models. Human experts can identify specific decision nodes that unfairly discriminate based on sensitive attributes, which would be difficult or impossible to detect in complex neural networks or other opaque models. The knowledge distillation process preserves the predictive power of the original model while making its decision-making process transparent. By iteratively refining the model through human review and fine-tuning, the method gradually eliminates unfair bias while maintaining accuracy.

## Foundational Learning
- Knowledge Distillation: Why needed - To transfer knowledge from complex black-box models to interpretable decision trees; Quick check - Verify that the distilled model maintains similar predictive performance to the original
- Decision Tree Interpretability: Why needed - To make bias patterns visible to human reviewers; Quick check - Ensure decision trees are of manageable depth and complexity for human review
- Fairness Metrics: Why needed - To quantify and measure bias reduction; Quick check - Validate that chosen metrics align with domain-specific fairness requirements
- Human-in-the-Loop Review: Why needed - To leverage expert judgment in identifying unfair decisions; Quick check - Establish clear guidelines and training for human reviewers
- Fine-tuning Process: Why needed - To preserve model accuracy after removing biased decisions; Quick check - Monitor accuracy metrics throughout the refinement process

## Architecture Onboarding

Component Map: Black-box PBPM Model -> Knowledge Distillation -> Decision Tree -> Human Review -> Bias Removal -> Fine-tuning -> Fair PBPM Model

Critical Path: The most critical path is Black-box PBPM Model -> Knowledge Distillation -> Decision Tree -> Human Review -> Bias Removal -> Fine-tuning. Each step must succeed for the overall approach to work, with human review being the key differentiator from automated approaches.

Design Tradeoffs: The main tradeoff is between model interpretability and predictive power. While decision trees are interpretable, they may not capture all the nuances of complex business processes. The knowledge distillation step attempts to balance this by preserving predictive performance while enabling human review.

Failure Signatures: If human reviewers cannot identify bias patterns, the approach fails to improve fairness. If the fine-tuning process overfits to the reviewed data, accuracy may degrade. If knowledge distillation loses too much information, the final model may underperform the original.

Three First Experiments:
1. Validate knowledge distillation by comparing accuracy between original black-box model and distilled decision tree
2. Test human review process with synthetic bias patterns to ensure reviewers can correctly identify unfair decisions
3. Evaluate the impact of removing different numbers of biased decisions on overall model accuracy and fairness metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on synthetic data generation with controlled bias parameters, which may not capture real-world complexity
- Relatively small sample size of real-life event logs limits generalizability
- Human-in-the-loop component introduces potential subjectivity and scalability concerns
- Computational overhead from iterative refinement and human review may challenge real-time applications

## Confidence

High confidence in the core methodology and its ability to improve fairness while maintaining accuracy

Medium confidence in the scalability and generalizability of the approach across diverse real-world scenarios

Medium confidence in the long-term stability of fairness improvements, given potential changes in data distribution over time

## Next Checks

1. Conduct a longitudinal study to assess the stability of fairness improvements over time as data distributions and business processes evolve

2. Perform a comprehensive evaluation across a wider range of real-world event logs with varying complexity, sizes, and domain characteristics

3. Implement a user study with multiple human reviewers to assess the consistency, reliability, and potential biases in the human-in-the-loop fairness assessment process