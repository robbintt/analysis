---
ver: rpa2
title: 'AILS-NTUA at SemEval-2025 Task 3: Leveraging Large Language Models and Translation
  Strategies for Multilingual Hallucination Detection'
arxiv_id: '2503.02442'
source_url: https://arxiv.org/abs/2503.02442
tags:
- output
- hallucination
- hypothesis
- input
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AILS-NTUA addresses multilingual hallucination detection by leveraging
  LLM prompting and translation strategies without requiring model training. The method
  combines Llama 3.1 405B and Claude 3.5 Sonnet to detect hallucinated spans in text
  outputs across 14 languages.
---

# AILS-NTUA at SemEval-2025 Task 3: Leveraging Large Language Models and Translation Strategies for Multilingual Hallucination Detection

## Quick Facts
- arXiv ID: 2503.02442
- Source URL: https://arxiv.org/abs/2503.02442
- Reference count: 38
- Primary result: First place in low-resource Farsi and Czech, second in Italian, top-15% across most other languages

## Executive Summary
AILS-NTUA addresses multilingual hallucination detection in LLM outputs across 14 languages by leveraging large language models without requiring any training. The system combines Llama 3.1 405B and Claude 3.5 Sonnet to detect hallucinated spans through few-shot prompting and hypothesis exchange, with a key innovation being the translation of non-English inputs and outputs to English before detection. The approach achieves strong performance particularly in low-resource languages where translation proves especially beneficial, while also ranking highly across high-resource languages through ensemble aggregation of multiple detection strategies.

## Method Summary
The method employs a training-free ensemble of three detection components that process translated English versions of input-output pairs. For each sample, two hypotheses are first generated by the LLMs, then each detection component applies few-shot prompting with examples from four hallucination categories (input-output inconsistency, factual inconsistency, internal inconsistency, misspellings). The three components use different configurations: Claude with Llama hypothesis, Llama with Claude hypothesis, and Llama without hypothesis. Final probabilities are assigned to detected spans based on the ratio of components flagging them, enabling ensemble aggregation without requiring model retraining.

## Key Results
- Achieved first place ranking in low-resource languages Farsi and Czech
- Secured second place in Italian and top-15% performance across most other languages
- Translation into English proved especially beneficial for low-resource languages
- Few-shot prompting with hypothesis exchange drove significant gains in high-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating non-English input-output pairs to English improves hallucination detection, particularly for low-resource languages.
- Mechanism: Translation aligns multilingual inputs with the language where LLMs have strongest factual knowledge and reasoning capability, reducing the resource disparity that causes uneven reliability across languages.
- Core assumption: LLMs possess more reliable internal knowledge representations in English than in low-resource languages; translation preserves enough semantic information to enable cross-lingual detection.
- Evidence anchors:
  - [abstract] "Translation into English proves especially beneficial for low-resource languages."
  - [section 6] "The addition of translations and hypotheses has a more pronounced impact on low-resource languages compared to high-resource ones."
  - [corpus] Neighbor papers on Mu-SHROOM task (e.g., HalluSearch, keepitsimple) similarly leverage translation or cross-lingual strategies, suggesting convergent validity of the approach.

### Mechanism 2
- Claim: Cross-model hypothesis exchange—where each LLM receives the other's generated answer as a reference—improves span-level hallucination detection accuracy.
- Mechanism: Each model's independent generation serves as a comparative reference point; when the model output diverges from the hypothesis without factual basis, it signals hallucination. Cross-model use reduces single-model blind spots.
- Core assumption: Hallucinations are not systematically correlated across different LLM architectures; one model's correct answer can serve as a ground-truth proxy for detecting the other's errors.
- Evidence anchors:
  - [section 3] "The LLM is able to compare the hypothesis with the actual output provided to determine more effectively the presence of hallucinations and identify their respective spans."
  - [section C, table 9] Combinations like "C + C,LH" (Claude + Llama hypothesis) achieved IoU 0.53 vs. Claude alone at 0.465 for English.

### Mechanism 3
- Claim: Ensemble aggregation across three distinct prompting configurations—with and without hypothesis—yields higher IoU than any single configuration.
- Mechanism: Each configuration has different sensitivity profiles (with hypothesis: better at factual inconsistency; without hypothesis: broader coverage of potential hallucination spans). Probability-weighted span combination captures complementary signals.
- Core assumption: Hallucination types are heterogeneously detected across configurations; false positives/negatives are not perfectly correlated across the three component outputs.
- Evidence anchors:
  - [section 3] "For each produced span, the assigned probability is calculated as the ratio of the experiments that characterize it as hallucination over the total number of experiments (three)."
  - [section C, table 11] Combination IoU (0.53) exceeded individual component IoUs (0.47, 0.52) for English.

## Foundational Learning

- Concept: **Hallucination taxonomy (input-output inconsistency, factual inconsistency, internal inconsistency, misspellings)**
  - Why needed here: The few-shot prompt design explicitly provides one example per category; understanding these distinctions is required to customize prompts or interpret model reasoning.
  - Quick check question: Can you distinguish why "The 2024 Olympics were in Paris" is a factual inconsistency while "The 2024 Olympics were in Paris and also in London" is an internal inconsistency?

- Concept: **Intersection-over-Union (IoU) for span evaluation**
  - Why needed here: The task evaluation uses character-level IoU between predicted and gold hallucination spans; optimizing for this metric requires precise span boundary prediction, not just detection.
  - Quick check question: If the gold span is "Florida" (7 characters) and you predict "in Florida" (10 characters), what is the IoU?

- Concept: **Cross-lingual transfer via translation**
  - Why needed here: The architecture's core innovation is pivoting multilingual detection through English; you must understand when translation helps vs. introduces noise.
  - Quick check question: For a language where Google Translate has known systematic errors (e.g., morphological ambiguity), what alternative strategies could mitigate translation-induced hallucination false positives?

## Architecture Onboarding

- Component map: Translation layer -> Hypothesis generation (Llama 405B, Claude 3.5 Sonnet parallel) -> Detection components (3-way parallel) -> Ensemble aggregation -> Output spans with confidence scores
- Critical path: Input → Translation → Hypothesis generation (parallel) → Detection prompts (3-way parallel) → Span extraction → Probability aggregation → Output spans with confidence scores
- Design tradeoffs:
  - External translation vs. LLM self-translation: External (Google Translate) outperformed LLM-based translation (Table 3), likely due to dedicated translation models' superior quality, but adds API dependency and latency
  - With vs. without hypothesis: Hypothesis improves factual inconsistency detection; no-hypothesis component catches spans hypothesis-conditioned models miss. Ensemble captures both
  - Model choice (Llama 405B + Claude 3.5): Large models required for reliable few-shot reasoning; smaller models may not follow output format constraints consistently
- Failure signatures:
  - Output format violations (missing quotes around spans, extra text after span list): Indicates prompt-following failure; mitigate with stricter format instructions or post-processing regex
  - Hypothesis over-reliance: Model trusts provided hypothesis over internal knowledge, missing hallucinations where hypothesis is also wrong. Mitigate by including no-hypothesis component
  - Span boundary drift: Predicted spans don't exactly match original text (e.g., "the Florida" vs. "Florida"). Mitigate with explicit instructions and character-level validation
- First 3 experiments:
  1. **Baseline reproduction**: Implement single-model few-shot detection (Claude only, no translation, no hypothesis) on validation set to establish IoU baseline per language
  2. **Ablation on translation**: Compare three translation strategies (no translation, external translation, LLM self-translation) on 2 low-resource (Farsi, Czech) and 2 high-resource (Italian, English) languages to quantify translation contribution
  3. **Ensemble size test**: Compare 2-component vs. 3-component aggregation to determine if marginal improvement from third component justifies computational cost; track both IoU and correlation metrics

## Open Questions the Paper Calls Out

- Can the observed effectiveness of training-free prompting strategies be replicated using smaller, open-weights models (e.g., under 10B parameters), or is the reasoning capability of massive models (Llama 405B, Claude 3.5) a strict requirement for this approach?
- To what extent do translation errors from the external machine translation system (Google Translate) introduce noise or "secondary hallucinations" into the detection pipeline, particularly for low-resource languages?
- What is the underlying mechanism that makes a hypothesis generated by a *different* LLM more effective for detecting hallucinations than a self-generated hypothesis or no hypothesis?

## Limitations

- The paper doesn't fully specify how character-level offsets are mapped back from English detection results to the original language text, which could introduce systematic errors for languages with different word boundaries or character encodings
- The cross-model hypothesis exchange mechanism has a critical vulnerability: if both LLMs share systematic biases or hallucinate similar content, the hypothesis exchange could reinforce rather than correct errors
- The ensemble aggregation assumes independent detection errors across components, but this independence isn't empirically validated—correlated failures could make the ensemble worse than its best individual component

## Confidence

- **High confidence**: Translation strategy effectiveness for low-resource languages and overall task ranking performance
- **Medium confidence**: Cross-model hypothesis exchange mechanism
- **Low confidence**: Ensemble aggregation superiority

## Next Checks

1. **Span alignment validation**: Implement a controlled experiment comparing hallucination detection accuracy when using original-language few-shot prompts versus English-translated prompts, but with gold spans provided in the original language
2. **Cross-model correlation analysis**: Analyze the correlation between hallucination predictions from Llama and Claude across the development set to empirically validate the independence assumption underlying the hypothesis exchange mechanism
3. **Ensemble contribution isolation**: Conduct an ablation study comparing 2-component versus 3-component ensembles on a held-out validation set, measuring not just IoU but also correlation scores