---
ver: rpa2
title: 'Rethinking DPO: The Role of Rejected Responses in Preference Misalignment'
arxiv_id: '2506.12725'
source_url: https://arxiv.org/abs/2506.12725
tags:
- bdpo
- loss
- responses
- dpop
- chosen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a key limitation in Direct Preference Optimization
  (DPO) where the loss function disproportionately emphasizes reducing the probability
  of rejected responses, often at the expense of increasing the probability of chosen
  responses. To address this, the authors propose Bounded-DPO (BDPO), which replaces
  the rejected response distribution in the DPO loss with a mixture distribution incorporating
  the reference model, thereby bounding the influence of rejected responses while
  maintaining the original optimization structure.
---

# Rethinking DPO: The Role of Rejected Responses in Preference Misalignment

## Quick Facts
- **arXiv ID:** 2506.12725
- **Source URL:** https://arxiv.org/abs/2506.12725
- **Reference count:** 40
- **Key outcome:** Bounded-DPO (BDPO) outperforms DPO and DPO+NLL on IFEval and GSM8K benchmarks by bounding rejected response influence through a mixture distribution, achieving highest total scores while maintaining better reference model alignment.

## Executive Summary
This paper identifies a critical limitation in Direct Preference Optimization (DPO) where the loss function disproportionately emphasizes reducing the probability of rejected responses, often at the expense of increasing the probability of chosen responses. The authors propose Bounded-DPO (BDPO), which replaces the rejected response distribution in the DPO loss with a mixture distribution incorporating the reference model, thereby bounding the influence of rejected responses while maintaining the original optimization structure. Theoretically, BDPO guarantees a lower bound on the chosen response probability and provides an ideal optimal solution. Empirically, BDPO outperforms existing methods on instruction-following (IFEval) and mathematical reasoning (GSM8K) benchmarks using the QWEN 0.5B and 7B models, achieving the highest total scores while maintaining closer alignment with the reference model compared to DPO+NLL.

## Method Summary
BDPO modifies the standard DPO objective by replacing the rejected response distribution πθ(yl|x) with a mixture distribution πmix(yl|x) = λπθ(yl|x) + (1-λ)πref(yl|x) in the loss function. This mixture formulation ensures that the denominator in the gradient term is lower-bounded, preventing the 1/πθ(yl|x) divergence that causes unstable updates in standard DPO. The method trains first with SFT on chosen responses, then applies BDPO optimization using the modified loss. The approach uses a hyperparameter λ (default 0.5) to control the balance between preference optimization and reference model regularization.

## Key Results
- BDPO outperforms DPO and DPO+NLL on UltraFeedback → IFEval, achieving highest total scores across all model sizes
- BDPO maintains lower KL divergence from reference model compared to DPO+NLL while improving chosen response probability
- Theoretical guarantees show BDPO ensures a lower bound on chosen response probability and requires both maximizing chosen and minimizing rejected response probabilities at optimal solution

## Why This Works (Mechanism)

### Mechanism 1: Gradient Bounding
The mixture distribution in BDPO bounds the gradient magnitude when rejected response probability approaches zero. By replacing πθ(yl|x) with πmix(yl|x) = λπθ(yl|x) + (1-λ)πref(yl|x), the denominator in the gradient term is lower-bounded by (1-λ)πref(yl|x) > 0. This prevents the 1/πθ(yl|x) divergence that causes unstable updates in standard DPO.

### Mechanism 2: Chosen Probability Guarantee
BDPO guarantees a lower bound on chosen response probability throughout training. The mixture formulation creates a coupling where decreasing πθ(yl|x) cannot arbitrarily dominate the loss. Under monotonic loss decrease, πθ(yw|x) ≥ (1-λ)πref(yw|x) at all training steps.

### Mechanism 3: Optimal Solution Constraint
The optimal solution under BDPO requires both maximizing chosen probability AND minimizing rejected probability (unlike DPO which can minimize only rejected). BDPO's optimal solution is π*(yw|x) = 1 and π*(yl|x) = 0, which is strictly more constrained than DPO.

## Foundational Learning

- **Concept: DPO loss formulation and implicit reward function**
  - **Why needed here:** BDPO modifies the DPO objective; understanding the original rθ(y) = β log(πθ/πref) formulation is prerequisite to grasping how the mixture distribution changes optimization dynamics.
  - **Quick check question:** Can you explain why the DPO loss can be minimized by reducing only πθ(yl|x) without changing πθ(yw|x)?

- **Concept: KL divergence and reference model regularization**
  - **Why needed here:** The paper frames DPO as having dual objectives: optimizing preferences AND staying close to πref. BDPO's design trades off these objectives via λ.
  - **Quick check question:** What happens to forward vs. reverse KL when πθ becomes deterministic?

- **Concept: Gradient instability from log-space operations near zero**
  - **Why needed here:** The core failure mode in DPO is numerical/gradient instability when πθ(yl|x) → 0. Understanding log-space gradient behavior is essential.
  - **Quick check question:** Why does ∂log(π)/∂π become problematic as π → 0, and how does a mixture distribution help?

## Architecture Onboarding

- **Component map:**
  Standard DPO Pipeline: [Prompt x] → [πref] → reference logprobs → [πθ] → policy logprobs → [DPO Loss]
  BDPO Modification: [πθ(yl|x), πref(yl|x)] → [Mixture: πmix = λ·πθ + (1-λ)·πref] → [Modified Loss]

- **Critical path:**
  1. Implement mixture distribution computation (lines 3-5 in pseudocode)
  2. Modify loss to use πmix instead of πθ for rejected response only
  3. Keep πθ(yw) unchanged—mixture only applies to rejected term
  4. Hyperparameter tuning: λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} with λ=0.5 as recommended default

- **Design tradeoffs:**
  - λ → 1: Approaches DPO; stronger preference optimization but risk of rejected-only optimization
  - λ → 0: Focuses only on chosen response; more stable but weaker preference signal
  - Memory: No additional memory overhead (πref is already loaded for standard DPO)
  - Compute: Negligible overhead (one weighted average per rejected token)

- **Failure signatures:**
  - Chosen probability still decreasing → λ too high, check if πref(yw) is itself low
  - Model diverging from reference (high KL) → λ too low, or learning rate too high
  - No improvement over DPO → λ ≈ 1, effectively disabled the mechanism
  - Numerical issues with rejected responses → Check for πref(yl) ≈ 0 in dataset

- **First 3 experiments:**
  1. **Sanity check with toy data:** Replicate Figure 3 behavior with 4 prompts/4 responses. Verify BDPO suppresses OOD responses while DPO/DPOP don't. Track πθ(yw), πθ(yl), and OOD probabilities per step.
  2. **Learning dynamics on 1% UltraFeedback:** Train QWEN 0.5B on subsampled data for 100 epochs. Plot chosen/rejected log probabilities, KL divergence, and NLL loss. Compare against Figure 4 patterns.
  3. **Hyperparameter sweep for λ:** Grid search λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} on full UltraFeedback → IFEval evaluation. Replicate Figure 5 ablation to confirm λ=0.5 optimal for your setup.

## Open Questions the Paper Calls Out

- **Question:** How does the Bounded-DPO (BDPO) objective behave theoretically and empirically in multi-pair response settings where multiple preference pairs exist for a single prompt?
  - **Basis in paper:** The authors state, "Similar to prior work, our theoretical analysis focuses on the single-pair setting... While BDPO can be extended both theoretically and empirically to the multi-pair response setting, we leave this for future work."
  - **Why unresolved:** The theoretical guarantees provided (Theorem 1 and Theorem 2) explicitly assume a single response pair per prompt to prove optimal policy convergence and lower bounds.
  - **What evidence would resolve it:** A formal extension of the proofs in Appendix I to scenarios with multiple response pairs, supported by empirical results on datasets containing multiple ranked responses per prompt.

- **Question:** Can BDPO be effectively integrated into online or iterative alignment frameworks, such as iterative DPO or self-play methods?
  - **Basis in paper:** The limitations section notes, "although BDPO is applicable to variants such as online or iterative DPO, these directions are beyond the scope of this study."
  - **Why unresolved:** BDPO relies on a fixed reference model for its mixture distribution; the dynamics of this bound when the reference model is updated dynamically (as in online methods) are unexplored.
  - **What evidence would resolve it:** Empirical evaluations applying BDPO loss within an online reinforcement learning loop or iterative preference optimization pipeline to compare stability and performance against offline BDPO.

- **Question:** Does the superior performance of BDPO on Qwen models generalize to a broader range of model architectures and evaluation domains?
  - **Basis in paper:** The authors acknowledge, "Due to GPU constraints, we were unable to conduct experiments across a broader range of models... [and] we were not able to include a wider range of evaluation tasks."
  - **Why unresolved:** The empirical results are confined to Qwen2.5-0.5B/7B models and specific benchmarks (IFEval, GSM8K), leaving uncertainty regarding efficacy on other architectures (e.g., Llama, Mistral) or tasks like code generation.
  - **What evidence would resolve it:** Benchmarking BDPO against DPO and DPOP on diverse model families and a wider suite of standard evaluation tasks (e.g., MMLU, HumanEval).

## Limitations

- Theoretical guarantees assume single-pair preference settings and monotonic loss decrease, which may not hold in practice
- Empirical validation limited to Qwen2.5 models (0.5B and 7B) on two specific datasets without statistical significance testing
- Mechanism understanding gaps remain regarding why the mixture distribution prevents rejected-response-only optimization so effectively

## Confidence

**High confidence:** The theoretical analysis of gradient bounding (Mechanism 1) and the core mathematical formulation of BDPO are sound. The empirical improvements on IFEval and GSM8K are clearly demonstrated and reproducible with the provided methodology.

**Medium confidence:** The theoretical guarantees for chosen response probability lower bounds (Theorem 2) hold under stated assumptions but may not fully capture practical behavior. The empirical results are compelling but limited to specific model/dataset combinations without statistical significance testing.

**Low confidence:** Generalization to other model architectures, datasets with different preference distributions, and multi-pair settings remains untested. The long-term behavior of BDPO (beyond 3 epochs) and sensitivity to hyperparameter choices beyond λ are not thoroughly explored.

## Next Checks

1. **Multi-pair setting validation:** Test BDPO on datasets with multiple preference pairs per prompt (e.g., Arcee's Panda or UltraFeedback with multiple responses per instruction). Verify that the theoretical guarantees and empirical improvements extend beyond the single-pair assumption.

2. **Statistical significance testing:** Run BDPO and baseline methods (DPO, DPO+NLL) with 5-10 random seeds on UltraFeedback → IFEval. Calculate 95% confidence intervals and p-values to confirm that reported improvements are statistically significant rather than random variation.

3. **Cross-model architecture validation:** Implement BDPO for Llama 2 7B and Mistral 7B models on the same UltraFeedback → IFEval task. Compare improvement magnitude against Qwen2.5 results to assess whether BDPO's benefits are architecture-dependent or general across different model families.