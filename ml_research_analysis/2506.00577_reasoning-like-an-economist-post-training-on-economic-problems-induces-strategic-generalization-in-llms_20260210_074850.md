---
ver: rpa2
title: 'Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic
  Generalization in LLMs'
arxiv_id: '2506.00577'
source_url: https://arxiv.org/abs/2506.00577
tags:
- reasoning
- arxiv
- economic
- preprint
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether post-training techniques like Supervised
  Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) can
  effectively generalize to multi-agent scenarios. The authors curate Recon, a 7B-parameter
  open-source LLM trained on 2,100 high-quality economic reasoning problems across
  15 categories, including game theory and strategic reasoning.
---

# Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs

## Quick Facts
- arXiv ID: 2506.00577
- Source URL: https://arxiv.org/abs/2506.00577
- Reference count: 40
- A 7B LLM trained on economic reasoning problems generalizes to strategic multi-agent games

## Executive Summary
This paper demonstrates that post-training a 7B LLM on economic reasoning problems induces strategic generalization to multi-agent games. Through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning with Verifiable Rewards (RLVR), the model shows improved performance on economic benchmarks and increased Nash equilibrium convergence in game-theoretic scenarios. The results suggest that domain-aligned post-training can create economically rational behavior in interactive settings, providing a scalable approach to agent alignment.

## Method Summary
The authors train Recon, a 7B LLM, using a two-stage pipeline starting with SFT on 868 distilled CoT traces from QwQ-32B teacher model, followed by GRPO optimization on 1,800 curated economic problems. The training uses LoRA adapters with specific hyperparameters (rank=8, batch sizes of 8 and 32, learning rates of 2e-4 and 5e-6 respectively). A hierarchical reward function evaluates exact matches, partial matches, and format compliance. The model is evaluated on Recon-Eval benchmark, Complete-Information Games, and GTBench against GPT-4o-mini.

## Key Results
- 14.7% improvement in accuracy on Recon-Eval benchmark
- 9.5-point increase in Nash equilibrium convergence rates in multi-agent games
- Enhanced win rate on GTBench against GPT-4o-mini

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SFT distillation of structured reasoning traces creates a warm-start that stabilizes RL optimization
- **Mechanism:** SFT forces the model to internalize problem-solving syntax and domain logic before RL, preventing policy collapse during high-variance optimization
- **Core assumption:** The teacher model possesses superior reasoning logic that transfers via token prediction
- **Evidence anchors:** Section 3.3 confirms SFT provides strong initialization; Appendix A.5 shows RL fails without SFT warm-start
- **Break condition:** If teacher model produces flawed traces or student lacks capacity, SFT bakes in errors

### Mechanism 2
- **Claim:** GRPO optimization of verifiable rewards induces an "equilibrium prior"
- **Mechanism:** Outcome-based rewards teach the model to search for strategies surviving counter-factual scrutiny, mimicking Nash Equilibrium calculation
- **Core assumption:** Static economic problem search is functionally similar to multi-agent strategy search
- **Evidence anchors:** Section 6.1 explains outcome-aligned rewards create equilibrium prior; abstract states models display economically rational behavior despite no interaction supervision
- **Break condition:** If rewards are hackable or problems lack strategic depth, equilibrium prior won't form

### Mechanism 3
- **Claim:** Structural separation between reasoning and execution enables modular policy transfer
- **Mechanism:** Training template with <think> and \boxed{} creates distinct "policy-over-thoughts" module reused for game tree simulation
- **Core assumption:** Token structure acts as functional scaffold persisting during inference in new domains
- **Evidence anchors:** Section 6.1 links training template to inner-rollout/outer-commitment loop; related work supports language-augmented reasoning
- **Break condition:** If downstream environments don't allow thought generation, model cannot invoke planning module

## Foundational Learning

### Concept: Group Relative Policy Optimization (GRPO)
- **Why needed here:** This is the specific RL algorithm used (DeepSeek-Math variant)
- **Quick check question:** How does GRPO estimate the baseline for advantage calculation differently than PPO?

### Concept: Nash Equilibrium & Backward Induction
- **Why needed here:** These are core evaluation metrics and the theoretical mechanism behind strategic generalization
- **Quick check question:** In the "Draco" sequential game example, how does the model use backward induction to determine the optimal first move?

### Concept: Distillation of Reasoning Traces
- **Why needed here:** The SFT phase relies entirely on distillation from a strong teacher
- **Quick check question:** Why is filtering for correctness critical before using traces for SFT?

## Architecture Onboarding

### Component map:
DeepSeek-R1-Distill-Qwen-7B (Base) -> Recon Dataset (2,100 problems) + QwQ-32B (Teacher) -> Recon-CoT (868 traces) -> Unsloth + TRL (Training) -> Hierarchical Reward Function

### Critical path:
1. Data Curation: Sample high-error categories from STEER/EconLogicQA
2. Distillation: Generate CoT traces with QwQ-32B, filter correct ones
3. SFT: Train on 868 correct traces (essential warm-start)
4. RL (GRPO): Optimize verifiable rewards to induce strategic priors

### Design tradeoffs:
- Small Data vs. Diversity: 868 training samples risk overfitting but ensure quality
- SFT Stability vs. RL Generalization: SFT provides stability but RL drives equilibrium prior

### Failure signatures:
- RL Instability: If skipping SFT, GRPO rewards fail to converge
- Format Violation: Model generates <think> but fails \boxed{}, triggering -4 penalty
- Reward Hacking: Model finds shortcut to answer (mitigated by CoT distillation)

### First 3 experiments:
1. Ablate the Warm-start: Run GRPO directly on base model to verify convergence failure
2. Reward Sensitivity: Adjust reward hierarchy to test structural discipline degradation
3. Transfer Test: Evaluate on completely unseen game category to test equilibrium prior hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the methodology generalize to other structured domains like law, medicine, or civil design?
- **Basis in paper:** Section 6.2 states intent to generalize methodology to other domains
- **Why unresolved:** Current study restricted to economic problems
- **What evidence would resolve it:** Applying SFT+RLVR pipeline to legal/medical datasets and measuring reasoning accuracy and alignment

### Open Question 2
- **Question:** Does integrating explicit multi-agent negotiation workflows further enhance interactive reasoning?
- **Basis in paper:** Section 6.2 plans to investigate multi-agent workflow integration
- **Why unresolved:** Current results rely on offline post-training
- **What evidence would resolve it:** Comparative study evaluating Recon against version augmented with negotiation frameworks

### Open Question 3
- **Question:** Do improvements persist in imperfect or incomplete information settings?
- **Basis in paper:** Evaluation focuses on Complete-Information Games, may not represent real-world complexity
- **Why unresolved:** Demonstrates strategic generalization mostly in known payoff settings
- **What evidence would resolve it:** Evaluating on game-theoretic benchmarks for incomplete information (Kuhn Poker, sealed-bid auctions)

## Limitations

- Small training corpus (868 CoT traces, 1,800 problems) raises overfitting concerns
- GRPO implementation details (epsilon, beta hyperparameters) unspecified
- Transferability of equilibrium prior to domains beyond economics remains theoretical

## Confidence

- **High Confidence:** SFT training methodology and stabilizing effect on RL
- **Medium Confidence:** Accuracy improvements on economic benchmarks
- **Medium Confidence:** Multi-agent generalization claims
- **Low Confidence:** Scalability to larger models or different base architectures

## Next Checks

1. **Ablation on Multi-Agent Transfer:** Test Recon on completely novel game category not in training corpus
2. **GRPO Hyperparameter Sensitivity:** Systematically vary epsilon and beta values to establish robustness
3. **Teacher Distillation Quality:** Compare Recon performance when trained on CoT traces from different teacher models