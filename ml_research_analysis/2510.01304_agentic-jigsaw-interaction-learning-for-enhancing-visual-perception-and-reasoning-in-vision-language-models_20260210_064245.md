---
ver: rpa2
title: Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning
  in Vision-Language Models
arxiv_id: '2510.01304'
source_url: https://arxiv.org/abs/2510.01304
tags:
- jigsaw
- reasoning
- image
- visual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGILE addresses the limited visual perception and reasoning capabilities
  in large vision-language models, which struggle even on simple 2x2 jigsaw tasks.
  The proposed method formulates jigsaw solving as an interactive process where the
  model iteratively generates Python code to swap, observe, crop, and zoom into jigsaw
  pieces, receiving fine-grained visual feedback at each step.
---

# Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2510.01304
- **Source URL:** https://arxiv.org/abs/2510.01304
- **Reference count:** 27
- **Primary result:** Improves 2x2 jigsaw accuracy from 9.5% to 82.8% using interactive code generation for visual operations

## Executive Summary
AGILE addresses the limited visual perception and reasoning capabilities in large vision-language models, which struggle even on simple 2x2 jigsaw tasks. The proposed method formulates jigsaw solving as an interactive process where the model iteratively generates Python code to swap, observe, crop, and zoom into jigsaw pieces, receiving fine-grained visual feedback at each step. This interaction-driven learning is supported by scalable synthetic data generation, allowing training on large datasets without human supervision. Experimental results show AGILE improves 2x2 jigsaw accuracy from 9.5% to 82.8% and generalizes to 9 vision tasks with an average improvement of 3.1%. The approach offers an efficient solution to the scarcity of high-quality multimodal reinforcement learning data.

## Method Summary
AGILE implements an interactive learning framework where vision-language models solve jigsaw puzzles through iterative code generation. The model generates Python code commands to manipulate puzzle pieces (swap, observe, crop, zoom), receiving visual feedback after each operation. This process transforms jigsaw solving into a step-by-step reasoning task that requires both spatial understanding and logical planning. The method leverages synthetic data generation to create large-scale training datasets without human annotation, enabling scalable reinforcement learning for visual reasoning tasks. The interactive nature allows the model to build spatial understanding incrementally rather than requiring holistic scene comprehension from the start.

## Key Results
- 2x2 jigsaw puzzle accuracy improves from 9.5% to 82.8%
- Generalizes to 9 other vision tasks with average 3.1% improvement
- Demonstrates efficient solution to multimodal reinforcement learning data scarcity
- Shows scalability through synthetic data generation without human supervision

## Why This Works (Mechanism)
The mechanism works by breaking down complex visual reasoning into manageable interaction steps. Instead of requiring the model to solve the entire puzzle in one forward pass, AGILE enables iterative refinement through code-generated visual operations. Each interaction provides fine-grained feedback that helps the model build spatial relationships progressively. The synthetic data generation pipeline ensures diverse training scenarios while maintaining control over difficulty levels and task variations. This approach effectively bridges the gap between simple pattern recognition and complex reasoning by providing structured exploration paths through the solution space.

## Foundational Learning

**Python code generation for visual operations**: Why needed - Enables precise control over visual manipulations; Quick check - Verify generated code executes without runtime errors and produces expected visual transformations.

**Synthetic data generation**: Why needed - Scales training without human annotation; Quick check - Ensure generated puzzles maintain realistic spatial relationships and difficulty progression.

**Iterative visual feedback**: Why needed - Provides fine-grained learning signals; Quick check - Confirm feedback quality improves with each interaction step and guides correct reasoning paths.

## Architecture Onboarding

**Component map**: Vision-Language Model -> Code Generator -> Python Executor -> Visual Feedback -> Reasoning Update -> Vision-Language Model

**Critical path**: Input image → Code generation → Python execution → Visual feedback → Model update → Next code generation (repeats until puzzle solved)

**Design tradeoffs**: Interactive approach trades computational efficiency for improved reasoning capability; synthetic data provides scalability but may introduce distribution shift; fine-grained feedback enables better learning but requires more complex implementation.

**Failure signatures**: Incorrect code generation leading to invalid visual operations; accumulated errors from incorrect intermediate steps; distribution mismatch between synthetic training data and real-world puzzles.

**3 first experiments**:
1. Test basic code generation accuracy on simple image transformations (crop, rotate)
2. Validate synthetic data generation pipeline produces solvable puzzles with correct solutions
3. Evaluate single-step visual reasoning capability before full iterative interaction

## Open Questions the Paper Calls Out
None

## Limitations

The paper's baseline comparisons appear limited and may not account for model scale differences. The synthetic data generation approach could introduce distribution shift affecting real-world applicability. The claim of being an "efficient solution to scarcity of high-quality multimodal RL data" requires qualification since synthetic data quality varies significantly. Error accumulation through multiple interaction steps and lack of human-level performance comparison are unaddressed concerns.

## Confidence

- **High Confidence**: Technical methodology and synthetic data generation pipeline are well-documented and reproducible
- **Medium Confidence**: The 2x2 jigsaw accuracy improvement from 9.5% to 82.8% is specific and measurable, though baseline selection needs verification
- **Low Confidence**: Generalization claims to 9 other vision tasks lack sufficient baseline comparison details and may overstate practical significance

## Next Checks

1. Conduct ablation studies removing the interactive code generation component to isolate its contribution versus other training factors.

2. Test AGILE on more challenging jigsaw configurations (3x3, 4x4) and non-synthetic puzzles to assess scalability and real-world robustness.

3. Compare against recent vision-language models trained with alternative visual reasoning approaches on the same 9 vision tasks to establish relative performance gains.