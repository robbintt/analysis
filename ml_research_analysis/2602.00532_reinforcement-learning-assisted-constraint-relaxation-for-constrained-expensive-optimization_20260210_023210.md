---
ver: rpa2
title: Reinforcement Learning-assisted Constraint Relaxation for Constrained Expensive
  Optimization
arxiv_id: '2602.00532'
source_url: https://arxiv.org/abs/2602.00532
tags:
- optimization
- rleceo
- constraint
- learning
- evolutionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RLECEO, a novel Meta-Black-Box Optimization
  framework that leverages Deep Reinforcement Learning to learn adaptive constraint
  handling policies for constrained expensive optimization problems (CEOPs). The core
  method formulates the optimization process as a Markov Decision Process where a
  deep Q-network-based policy dynamically controls constraint relaxation levels (epsilon
  values) to balance objective optimization and constraint satisfaction.
---

# Reinforcement Learning-assisted Constraint Relaxation for Constrained Expensive Optimization

## Quick Facts
- **arXiv ID**: 2602.00532
- **Source URL**: https://arxiv.org/abs/2602.00532
- **Reference count**: 40
- **Primary result**: RLECEO learns adaptive ε-relaxation policies via RL, outperforming CEC/GECCO winners on constrained expensive optimization benchmarks

## Executive Summary
This paper introduces RLECEO, a novel Meta-Black-Box Optimization framework that leverages Deep Reinforcement Learning to automatically learn adaptive constraint handling policies for constrained expensive optimization problems (CEOPs). The core innovation lies in formulating the optimization process as a Markov Decision Process where a deep Q-network-based policy dynamically controls constraint relaxation levels (epsilon values) during search, balancing objective optimization and constraint satisfaction. The approach is trained on the CEC 2017 benchmark suite and demonstrates competitive or superior performance compared to strong baselines including recent CEC/GECCO competition winners, showing effectiveness under both leave-one-out cross-validation and train-test split validation.

## Method Summary
RLECEO uses Double DQN to learn an adaptive policy for controlling constraint relaxation levels in CEOPs. The framework operates as a meta-algorithm controlling L-SHADE's constraint handling through dynamic epsilon values. The agent observes a 10-dimensional state (combining fitness landscape and constraint violation features) and selects from 11 discrete actions determining the relaxation level. A composite reward function balances objective improvement and constraint satisfaction progress. Training occurs over 50 epochs using CEC 2017 benchmarks, with evaluation showing strong generalization across problem dimensions (10, 50, 100).

## Key Results
- RLECEO achieves competitive or superior performance compared to strong baselines including recent CEC/GECCO competition winners
- The learned policy demonstrates effective generalization across different problem dimensions through leave-one-out and train-test split validation
- Ablation studies confirm the importance of designed components, with the policy adapting its strategy based on problem characteristics

## Why This Works (Mechanism)
The approach works by transforming the constraint handling problem into a sequential decision-making task where the RL agent learns to dynamically adjust the constraint relaxation level based on the current search state. This adaptive control allows the algorithm to balance between aggressively exploring the objective space and conservatively respecting constraints, adapting its strategy to the specific characteristics of each problem. The double DQN architecture enables stable learning of this complex policy through experience replay and target network updates.

## Foundational Learning
- **Markov Decision Process formulation**: Needed to model the sequential nature of constraint handling decisions; quick check: verify state transitions follow Markov property
- **Double DQN architecture**: Required for stable Q-learning in continuous state spaces; quick check: monitor Q-value stability during training
- **Composite reward design**: Essential for balancing competing objectives; quick check: ensure reward components remain bounded and informative
- **Constraint relaxation parameterization**: Critical for meaningful policy actions; quick check: verify epsilon values produce different constraint satisfaction behaviors

## Architecture Onboarding
- **Component map**: State features (10D) → DQN agent → Action (epsilon value) → L-SHADE population update → Fitness evaluation → Reward calculation → Q-network update
- **Critical path**: State observation → Action selection → Population evolution → Reward computation → Network update
- **Design tradeoffs**: Discrete action space vs. continuous control, hand-crafted features vs. learned representations, composite reward vs. multi-objective optimization
- **Failure signatures**: Negative/unbounded rewards, poor generalization to higher dimensions, training instability/oscillation
- **First experiments**: 1) Verify state feature extraction with CEC 2017 problems, 2) Test DQN training with synthetic reward signals, 3) Validate L-SHADE integration with fixed epsilon values

## Open Questions the Paper Calls Out
- Can a more fine-grained state representation improve RLECEO's scalability and performance in high-dimensional search spaces (e.g., 100D and beyond)?
- How can the reward design be further optimized to better balance objective improvement and constraint satisfaction across diverse problem landscapes?
- Can RLECEO be effectively integrated with surrogate-assisted methods to handle the "expensive" nature of the optimization more directly?

## Limitations
- Key hyperparameters for L-SHADE integration remain unspecified, particularly comparison rule implementation and mutation strategies
- Experience replay buffer configuration and exploration schedule during training are not detailed
- State feature normalization procedure is unclear, potentially affecting training stability

## Confidence
- **Core methodology**: High - RL-based constraint handling is well-established approach
- **Performance claims**: Medium - strong baselines used but lacks independent replication
- **Generalizability**: Medium - shows good results on CEC 2017 but limited to specific benchmark suite
- **Reproducibility**: Low - critical implementation details missing

## Next Checks
1. Implement and test the exact state feature extraction with appropriate normalization bounds for CEC 2017 problems
2. Conduct sensitivity analysis on the L-SHADE comparison rule and ε-relaxation thresholds to identify critical hyperparameters
3. Evaluate the learned policy's transfer capability by training on 10D problems and testing on 50D/100D problems to quantify generalization limits