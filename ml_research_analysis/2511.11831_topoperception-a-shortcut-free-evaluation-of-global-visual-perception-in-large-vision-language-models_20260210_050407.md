---
ver: rpa2
title: 'TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in
  Large Vision-Language Models'
arxiv_id: '2511.11831'
source_url: https://arxiv.org/abs/2511.11831
tags:
- visual
- global
- image
- topoperception
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopoPerception introduces a benchmark for evaluating the global
  visual perception capabilities of large vision-language models (LVLMs) by leveraging
  topological properties of images, which are invariant to local features and therefore
  immune to local shortcuts. The benchmark employs a fixed multiple-choice question
  format with synthetic images that vary in perceptual granularity, allowing isolation
  of visual perception from reasoning and language generation abilities.
---

# TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models

## Quick Facts
- **arXiv ID:** 2511.11831
- **Source URL:** https://arxiv.org/abs/2511.11831
- **Authors:** Wenhao Zhou; Hao Zheng; Rong Zhao
- **Reference count:** 40
- **Primary result:** Current LVLMs perform at or near random chance on global visual perception tasks, with stronger reasoning models showing worse performance

## Executive Summary
TopoPerception introduces a benchmark for evaluating the global visual perception capabilities of large vision-language models (LVLMs) by leveraging topological properties of images, which are invariant to local features and therefore immune to local shortcuts. The benchmark employs a fixed multiple-choice question format with synthetic images that vary in perceptual granularity, allowing isolation of visual perception from reasoning and language generation abilities. Evaluation of state-of-the-art LVLMs across several model families reveals that even at the coarsest perceptual granularity, all models perform at or near random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerges where more powerful models with stronger reasoning capabilities exhibit lower accuracy, suggesting that scaling up models may exacerbate rather than alleviate this deficit. These findings highlight a critical bottleneck in current LVLMs and underscore the need for new training paradigms or architectures to improve global visual perception.

## Method Summary
TopoPerception uses synthetic binary images generated via uniform spanning trees on connected graphs, with 100 samples per topology category (B, C, D) per level. The images vary in resolution from 29×29 (Level 0, comparable to MNIST) to higher resolutions. Models are evaluated zero-shot using a fixed text-based multiple-choice format with open-ended text generation. The benchmark isolates global visual perception by using topological properties that depend on the entire image structure and are invariant to local features, preventing shortcut-based solutions.

## Key Results
- All evaluated LVLMs (GPT-4o, Claude-4, Gemini-2.5) perform at or near random chance (20%) on the benchmark
- Larger models with stronger reasoning capabilities consistently show lower accuracy than smaller models
- Performance remains near-random even at the coarsest perceptual granularity (29×29 resolution)
- Models exhibit strong category bias, often predicting the same answer regardless of input image

## Why This Works (Mechanism)

### Mechanism 1
Topological properties isolate global visual perception by being invariant to local features. Topology captures connectivity, holes, and interior/exterior relationships that depend on the entire image structure. Since no single local region determines these properties, models cannot solve tasks via local shortcuts—they must process the global configuration.

### Mechanism 2
Stronger reasoning capabilities correlate with lower accuracy on global perception tasks. Models with enhanced chain-of-thought reasoning may over-rely on language-based priors and verbal reasoning pathways. When visual representations are already lossy, reasoning processes amplify distortions rather than correct them—the model "thinks" in language and overrides weak visual signals.

### Mechanism 3
Visual encoders trained on semantic objectives discard non-linguistic global structure. Encoders like CLIP use contrastive learning to align visual features with language descriptions. Global topological properties have no natural linguistic correlate, so they receive no reinforcement signal and are treated as noise to be filtered during compression to tokens.

## Foundational Learning

- **Topological invariance**
  - Why needed here: The benchmark's validity depends on understanding that connectivity, holes, and containment relationships persist through continuous deformations and cannot be determined from local patches.
  - Quick check question: Given an image of a coffee mug and a donut, why would a topologist classify them as equivalent?

- **Inductive bias in neural architectures**
  - Why needed here: The paper argues that fixed resolutions, patching, and tokenization impose structural constraints that corrupt global representations before they reach the language model.
  - Quick check question: Why might resizing a 29×29 topological image to 224×224 (standard encoder input) distort the property being measured?

- **Shortcut learning**
  - Why needed here: The paper classifies shortcuts (statistical vs. semantic, Type 1 vs. Type 2) to explain why existing benchmarks overestimate perception.
  - Quick check question: In a VQA dataset where "How many wheels?" questions only appear with car images, what shortcut might a model learn?

## Architecture Onboarding

- **Component map:** Input image → Visual encoder (CLIP/ViT with fixed resolution, patch-based) → Projection layer (query network/MLP) → LLM backbone → Text output
- **Critical path:** Synthetic topological image (29×29 to variable resolution) → resizing/scaling to encoder input size → patch embedding (local windows) → token sequence → cross-modal projection → LLM hidden states → answer token
- **Design tradeoffs:**
  - Higher resolution input → more tokens → better detail but quadratic attention cost
  - Token reduction (e.g., pooling, pruning) → faster inference but risks discarding global structure
  - Stronger LLM reasoning → better language tasks but may interfere with perception (observed inverse correlation)
- **Failure signatures:**
  - Accuracy at 20% (random among 5 options) or 33.3% (biased toward 3 valid options) → model ignoring image entirely
  - Prediction distribution identical across image categories → model using fixed bias regardless of input
  - Larger model → lower accuracy within same family → reasoning-perception interference
- **First 3 experiments:**
  1. Control test: Present the same topological image with shuffled option labels. If predictions follow label bias rather than image content, confirm input-independence.
  2. Resolution sweep: Test model on images from Level 0 (29×29) through Level 5+ to identify the granularity threshold where accuracy degrades from biased-random to purely random.
  3. Ablation on reasoning: Compare accuracy when prompting with direct "Answer with only the letter" vs. chain-of-thought "Think step by step, then answer" to validate the reasoning-interference hypothesis within a single model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does enhanced reasoning capability in LVLMs negatively correlate with performance on global visual perception tasks?
- **Basis in paper:** The authors observe a consistent trend where "more powerful models with stronger reasoning capabilities exhibit lower accuracy" and suggest that language-based reasoning may "interfere with or override the models' already fragile visual signal."
- **Why unresolved:** The study identifies the correlation but only hypothesizes that reasoning models are misled by priors or lack precise internal representations; the mechanism remains unclear.
- **What evidence would resolve it:** Ablation studies isolating reasoning components (e.g., disabling Chain-of-Thought) or attention map analyses showing how language tokens suppress global visual tokens.

### Open Question 2
- **Question:** What specific architectural modifications can preserve global topological features during the visual encoding and alignment stages?
- **Basis in paper:** The conclusion states that "simply 'stitching' a fixed visual encoder to an LLM... may be insufficient" and explicitly calls for "new training paradigms or architectures" to address the visual perception bottleneck.
- **Why unresolved:** Current standard architectures (encoder + projector + LLM) treat global features as discardable noise; the benchmark shows current solutions fail.
- **What evidence would resolve it:** A model architecture designed for high-fidelity global retention (e.g., hierarchical encoders) achieving significantly above-random accuracy on TopoPerception.

### Open Question 3
- **Question:** Is the loss of global visual information primarily caused by the visual encoder's compression or the cross-modal projection into the LLM's embedding space?
- **Basis in paper:** The paper discusses information loss in both the "visual encoder" (due to fixed resolutions/patching) and the "projection module" (alignment discrepancy), but the evaluation is end-to-end.
- **Why unresolved:** The benchmark evaluates the system as a whole, leaving the exact locus of the "profound inability" to preserve global structure unidentified.
- **What evidence would resolve it:** Probing intermediate representations (e.g., CLS tokens or patch embeddings) to test if topological features are present before projection to the language model.

## Limitations
- The benchmark's design relies on models interpreting synthetic binary images of varying resolutions, but the interaction between resizing artifacts and topological perception is not fully characterized.
- While topological properties are formally invariant to continuous deformations, the practical implementation via discrete pixel grids may introduce discretization artifacts that could influence results.
- The inverse correlation between model reasoning strength and global perception accuracy is observed across models but lacks mechanistic explanation—whether this is causal or correlational remains unclear.

## Confidence
- **High:** Topology provides shortcut-free evaluation of global perception (mechanism 1)
- **High:** Current LVLMs show near-random performance on global perception tasks
- **Medium:** Reasoning interference hypothesis (mechanism 2) is plausible but not definitively proven
- **Medium:** Encoder compression hypothesis (mechanism 3) is consistent with observations but indirect
- **Low:** Claims about scaling exacerbating perception deficits are correlational without causal proof

## Next Checks
1. **Resolution robustness test:** Systematically evaluate model accuracy across the full range of image resolutions (29×29 to 224×224) to identify precise thresholds where topological properties become ambiguous due to discretization effects.
2. **Encoder ablation study:** Compare performance when using different visual encoders (CLIP, SigLIP, proprietary) with identical LLM backbones to isolate whether encoder architecture or training objectives drive the global perception bottleneck.
3. **Reasoning interference verification:** Conduct controlled experiments varying the presence/absence of chain-of-thought prompts within the same model family to directly measure the impact of reasoning pathways on global perception accuracy.