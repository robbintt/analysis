---
ver: rpa2
title: 'Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses,
  Challenges and Opportunities'
arxiv_id: '2505.23838'
source_url: https://arxiv.org/abs/2505.23838
tags:
- text-to-sql
- language
- arxiv
- data
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic survey of LLM-based text-to-SQL
  approaches, analyzing research trends, methodologies, datasets, and challenges.
  The survey reviewed 122 articles from April 2022 to October 2024, identifying key
  research paradigms including pre-processing (schema linking, cell value acquisition),
  in-context learning (prompt engineering, example selection, reasoning enhancement),
  fine-tuning (model architectures, training strategies), and post-processing (SQL
  correction, output consistency).
---

# Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities

## Quick Facts
- **arXiv ID:** 2505.23838
- **Source URL:** https://arxiv.org/abs/2505.23838
- **Reference count:** 40
- **Primary result:** Systematic survey of 122 LLM-based text-to-SQL articles from April 2022-October 2024 analyzing trends, methodologies, datasets, and challenges

## Executive Summary
This paper presents a comprehensive systematic literature review of LLM-based text-to-SQL approaches, examining research trends, methodologies, and challenges in the rapidly evolving field. The survey identifies four key research paradigms: pre-processing (schema linking, cell value acquisition), in-context learning (prompt engineering, example selection, reasoning enhancement), fine-tuning (model architectures, training strategies), and post-processing (SQL correction, output consistency). The authors analyze 122 articles from major digital libraries, revealing that schema linking and few-shot prompting are the most prevalent approaches, with EMNLP and ACL being the primary publication venues.

The study highlights critical challenges including structural complexity of SQL queries, schema comprehension difficulties, cross-domain generalization limitations, and model efficiency concerns. By providing a detailed taxonomy and comprehensive analysis of current capabilities and limitations, this survey offers valuable insights for researchers and practitioners working on natural language interfaces to databases, while identifying key future research directions that could advance the field beyond its current state.

## Method Summary
The authors conducted a systematic literature review (SLR) by querying five digital libraries (Web of Science, ScienceDirect, ACM DL, IEEEXplore, Google Scholar) using keywords "Text-to-SQL", "Text2SQL", "NL2SQL" for papers published between April 2022 and October 2024. After filtering results based on inclusion/exclusion criteria (English language, LLM-focused, >4 pages, excluding books/technical reports), they manually classified 122 relevant articles into four research paradigms: pre-processing, in-context learning, fine-tuning, and post-processing. The extracted data was analyzed to identify research trends, methodology distributions, and challenges, with results presented in tables and figures showing publication venues, approach frequencies, and dataset usage patterns.

## Key Results
- Surveyed 122 articles from April 2022 to October 2024, identifying four main research paradigms in LLM-based text-to-SQL
- Schema linking and few-shot prompting are the most prevalent approaches, with EMNLP and ACL being the primary publication venues
- Key challenges identified include structural complexity, schema comprehension, cross-domain generalization, and model efficiency
- Multi-step reasoning enhancement through decomposition shows promise for handling complex queries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Schema linking reduces hallucination by constraining the model's search space to relevant database entities before generation.
- **Mechanism:** The system extracts keywords from the Natural Language (NL) query and maps them to specific table and column names in the database schema. This creates a "filtered schema" that is injected into the prompt, preventing the LLM from inventing non-existent columns.
- **Core assumption:** The LLM can accurately identify relevant schema elements from the provided context better than it can recall or hallucinate them from its training data without guidance.
- **Evidence anchors:**
  - [abstract] Identifies schema linking as a key pre-processing paradigm.
  - [section 5.1] Describes schema linking as mapping words to schema elements to "ensure accurate SQL generation," citing DIN-SQL and C3.
  - [corpus] X-SQL reinforces the importance of "Expert Schema Linking" for understanding database schema information.
- **Break condition:** The database schema exceeds the LLM's context window, or the NL query uses ambiguous terms that map to multiple unrelated columns.

### Mechanism 2
- **Claim:** Reasoning enhancement via decomposition improves accuracy on complex queries by isolating specific cognitive steps.
- **Mechanism:** Instead of asking the LLM to generate SQL directly, the task is broken into sub-tasks (Task Decomposition) or the query is broken into sub-questions (Question Decomposition). This reduces the cognitive load per step, allowing the model to verify logic incrementally.
- **Core assumption:** The LLM possesses sufficient reasoning capability to execute individual steps correctly, and the aggregation logic of these steps is robust.
- **Evidence anchors:**
  - [section 5.2] States decomposition "enables LLMs to construct SQL progressively," citing DIN-SQL's division of the process into linking, classification, and generation.
  - [corpus] AmbiSQL supports this by treating ambiguity resolution as a specific detection task rather than a single generation step.
- **Break condition:** Simple queries where the overhead of decomposition increases latency without accuracy gains, or circular logic in the decomposition steps.

### Mechanism 3
- **Claim:** Execution-based feedback loops correct semantic errors that syntax checks miss.
- **Mechanism:** The LLM generates a candidate SQL query, executes it against the database, and receives an error message or unexpected result (e.g., empty set). This feedback is fed back into the LLM as context to refine the query (Self-Correction).
- **Core assumption:** The LLM can interpret execution errors (e.g., "column does not exist" vs. "syntax error") and map them back to the specific flaw in its previous generation.
- **Evidence anchors:**
  - [section 5.4] Highlights "Feedback-Guided SQL Correction" where models "iteratively refined based on execution feedback."
  - [corpus] SQLens validates that error detection and correction frameworks significantly help where models produce syntactically valid but semantically incorrect queries.
- **Break condition:** High execution cost on large databases (latency), or infinite correction loops if the model fails to understand the error.

## Foundational Learning

- **Concept: Schema Linking**
  - **Why needed here:** This is the primary pre-processing step. Without understanding which database columns correspond to natural language terms (e.g., "price" vs. `cost_usd`), the LLM cannot form a valid query.
  - **Quick check question:** Can you explain the difference between retrieving a "value" (Cell Value Acquisition) and identifying a "column name" (Schema Linking)?

- **Concept: In-Context Learning (ICL) vs. Fine-Tuning (FT)**
  - **Why needed here:** The paper distinguishes these as the two main implementation paradigms. You must decide if you are modifying the prompt (ICL) or the model weights (FT).
  - **Quick check question:** If you have a proprietary database and cannot send data to an external API, which paradigm is required? (Answer: FT with open-source models or on-premise ICL).

- **Concept: Execution Accuracy (EX) vs. Exact Match (EM)**
  - **Why needed here:** These are the primary evaluation metrics. EM is rigid (string match), while EX is functional (result match). You need EX to validate semantic correctness.
  - **Quick check question:** If a query produces the correct table result but uses a different `JOIN` order than the ground truth, is it EM or EX correct?

## Architecture Onboarding

- **Component map:** Input (Natural Language Query + Database Schema) -> Pre-Processor (Schema Linker & Question Rewriter) -> Prompt Constructor (few-shot examples + filtered schema) -> LLM (Proprietary or Open-Source) -> Post-Processor (SQL Corrector & Consistency Checker)

- **Critical path:** The Pre-processing (Schema Linking) stage. If the wrong columns are retrieved here, no amount of reasoning in the LLM will recover the correct answer.

- **Design tradeoffs:**
  - **Proprietary vs. Open-Source:** Proprietary (GPT-4) offers better reasoning but risks data privacy (discussed in Challenge 6). Open-Source requires resource-heavy Fine-Tuning (FFT/PEFT) to compete.
  - **Efficiency vs. Accuracy:** Multi-step decomposition improves accuracy but linearly increases latency (Challenge 4).

- **Failure signatures:**
  - **Hallucinated Columns:** SQL selects columns not in the schema (indicating Schema Linking failure).
  - **Empty Results:** Syntactically correct SQL that returns nothing (indicating logical error or over-constrained `WHERE` clause).
  - **Timeout:** Complex decomposition strategies running too long.

- **First 3 experiments:**
  1. **Zero-Shot Baseline:** Run GPT-4 or Llama-3 on your specific schema using a simple prompt to establish a performance floor.
  2. **Schema Linking Ablation:** Implement a basic BM25 retrieval for relevant columns and compare performance against feeding the whole schema.
  3. **Self-Correction Loop:** Implement a "Generate -> Execute -> Refine" loop and measure how many errors are caught by execution feedback.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can an optimal subset of database elements be selected during schema linking to balance computational efficiency with SQL accuracy, particularly for databases exceeding LLM context windows?
- **Basis in paper:** [explicit] Section 7.2 notes that while schema linking is prevalent, it is computationally complex and sometimes incorrect; the authors explicitly state, "how to select an optimal subset of database elements without compromising SQL accuracy and efficiency is worth investigating."
- **Why unresolved:** Current approaches either include the entire schema (incurring high latency) or risk excluding necessary information through filtering.
- **What evidence would resolve it:** A methodology that consistently reduces input token counts while maintaining or improving Execution Accuracy (EX) on large-schema benchmarks.

### Open Question 2
- **Question:** What novel fine-tuning (FT) architectures or training strategies can overcome the issues of diminished robustness and domain-specific constraints that currently limit FT adoption compared to In-Context Learning (ICL)?
- **Basis in paper:** [explicit] Section 7.2 states that FT-based approaches are less common than ICL because they often lead to "diminished robustness" on unseen data and suffer from high labeling costs.
- **Why unresolved:** Existing fine-tuned models often fail to generalize across domains as effectively as ICL methods, creating a trade-off between specialization and generalization.
- **What evidence would resolve it:** A fine-tuned model that demonstrates cross-domain generalization capabilities comparable to few-shot ICL baselines (e.g., GPT-4) on diverse datasets like BIRD or Spider.

### Open Question 3
- **Question:** How can LLM-based systems better handle context tracking and memory management to accurately resolve ambiguity in multi-turn, context-dependent text-to-SQL interactions?
- **Basis in paper:** [explicit] Section 7.2 identifies "Context-Dependent Text-to-SQL" as a vital future direction, noting that only three LLM-based studies currently exist in this area.
- **Why unresolved:** Models struggle to resolve pronouns, abbreviations, and vague references based on prior turns, and managing conversation history remains a significant challenge.
- **What evidence would resolve it:** Improved performance on multi-turn datasets (e.g., CoSQL, SParC) specifically on metrics requiring the resolution of ellipsis and co-reference.

## Limitations
- Paper selection restricted to articles published between April 2022 and October 2024, potentially missing earlier foundational work or late-breaking developments
- Exclusion criteria (minimum 4 pages, no books/technical reports) may have inadvertently omitted relevant contributions
- Focus on LLM-based approaches means valuable PLM-based or rule-based techniques that could complement LLM methods are not covered

## Confidence
- **High Confidence:** The taxonomy of research paradigms and their identified trends are well-supported by the corpus analysis and methodology section
- **Medium Confidence:** The challenges identified (structural complexity, schema comprehension, etc.) are reasonable extrapolations from the literature but may not capture all domain-specific issues
- **Medium Confidence:** The claimed prevalence of schema linking and few-shot prompting is based on paper counts, which may not accurately reflect actual implementation frequency in deployed systems

## Next Checks
1. **External Validation of Taxonomy:** Apply the survey's classification framework to a separate, recent batch of LLM-to-SQL papers (e.g., from 2025) to test whether the identified paradigms remain comprehensive and whether new paradigms have emerged.

2. **Implementation Feasibility Study:** Select 3-5 representative papers from each paradigm and attempt to reproduce their core contributions using publicly available models and datasets to validate claimed performance improvements and identify practical implementation challenges not captured in the survey.

3. **Deployment Impact Analysis:** Survey practitioners who have deployed LLM-based text-to-SQL systems to understand which of the identified challenges (e.g., efficiency, privacy) are most impactful in real-world settings versus theoretical concerns, and whether the survey's identified solutions adequately address these concerns.