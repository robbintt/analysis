---
ver: rpa2
title: Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of
  Hierarchical Reasoning Models
arxiv_id: '2601.10679'
source_url: https://arxiv.org/abs/2601.10679
tags:
- reasoning
- fixed
- arxiv
- latent
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the reasoning patterns of Hierarchical Reasoning
  Models (HRM) to understand their strengths and failure modes. The authors identify
  three surprising facts: (1) HRM can fail on extremely simple puzzles due to violation
  of the fixed point property - it sometimes continues updating its latent state even
  after finding the correct answer; (2) HRM exhibits "grokking" dynamics where the
  answer doesn''t improve uniformly but instead has a critical reasoning step that
  suddenly makes the answer correct; (3) Multiple fixed points exist in the latent
  space, and HRM can get trapped at incorrect ones.'
---

# Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models

## Quick Facts
- arXiv ID: 2601.10679
- Source URL: https://arxiv.org/abs/2601.10679
- Authors: Zirui Ren; Ziming Liu
- Reference count: 17
- Key result: Augmented HRM achieves 96.9% accuracy on Sudoku-Extreme vs 54.5% baseline

## Executive Summary
This paper reveals that Hierarchical Reasoning Models (HRM) operate through a "guessing" mechanism rather than traditional reasoning. Through mechanistic analysis, the authors identify three core failure modes: fixed point violations where models continue updating after finding correct answers, grokking dynamics with sudden solution discovery, and trapping at spurious fixed points in the latent space. Based on these insights, they develop Augmented HRM that combines data augmentation, input perturbation, and model bootstrapping to dramatically improve performance from 54.5% to 96.9% accuracy on extremely difficult Sudoku puzzles.

## Method Summary
The method involves analyzing HRM's reasoning patterns on Sudoku-Extreme puzzles through latent space trajectory visualization and failure mode classification. The authors propose three scaling strategies: data mixing (revealing random masked tokens during training), input perturbation (applying relabeling transformations at inference), and model bootstrapping (ensemble of 10 late-stage checkpoints). Augmented HRM combines all three techniques with deep supervision and one-step gradient training, achieving significant accuracy improvements through majority voting across perturbed inputs and model checkpoints.

## Key Results
- HRM fails on simple puzzles due to fixed point property violations, corrupting correct answers in early segments
- Non-trivial successes exhibit grokking dynamics with critical reasoning steps that suddenly produce correct answers
- Multiple fixed points exist in latent space, with HRM getting trapped at incorrect attractors
- Augmented HRM boosts Sudoku-Extreme accuracy from 54.5% to 96.9%, outperforming Tiny Recursive Model (87.4%)

## Why This Works (Mechanism)

### Mechanism 1: Fixed Point Seeking with Stability Violation
HRM iteratively updates latent states toward fixed points, but the assumed fixed point property (stability after finding correct answer) frequently breaks in practice. One-step gradient training disentangles segments, preventing explicit stability training. The model implicitly learns stability during training on hard puzzles, but lacks this training when exposed to simple puzzles, causing unnecessary updates post-solution.

### Mechanism 2: Multi-Attractor Landscape with Trapping Dynamics
The latent space contains multiple fixed points with competing basins of attraction. Initialization determines which attractor basin the trajectory enters - true attractors correspond to correct solutions while spurious attractors correspond to outputs with low conflict counts but wrong answers. If initialization falls in spurious basin, model may trap indefinitely or require many segments to escape.

### Mechanism 3: Guess Scaling via Intrinsic Diversity
HRM performance improves by scaling "guess attempts" through perturbations creating intrinsically diverse forward passes. Input perturbation through equivalent Sudoku transformations creates semantically identical but representationally distinct inputs. Model bootstrapping through different training checkpoints provides different fixed point landscapes. Both increase probability of hitting correct attractor.

## Foundational Learning

- **Fixed Point Dynamics in Recurrent Systems**
  - Why needed here: HRM's theoretical foundation assumes latent state reaches equilibrium, but this paper shows the assumption frequently fails
  - Quick check: Can you explain why one-step gradient training might prevent a recurrent model from learning stability?

- **Attractor Landscapes and Basin of Attraction**
  - Why needed here: Understanding why HRM sometimes "gets lost" requires visualizing latent space as having multiple attractors with competing basins
  - Quick check: If you initialize two trajectories near a boundary between attractors, what determines which attractor each converges to?

- **Grokking Dynamics vs. Gradual Refinement**
  - Why needed here: The paper challenges the intuition that recursive reasoning means incremental improvement; instead, loss plateaus then suddenly drops
  - Quick check: If a model's loss stays flat for 10 segments then drops to zero in segment 11, does this suggest gradual refinement or discrete state discovery?

## Architecture Onboarding

- **Component map**: Input network (f_I) -> Segment F (H-module + L-module) -> Output network (f_O) -> ACT mechanism
- **Critical path**: Input → embedding → segments (recursive, with hard reset of z_L per segment, z_H inherited across segments) → ACT decision → output extraction. Gradient flows one-step per segment due to detachment.
- **Design tradeoffs**: One-step gradient enables deep supervision without O(T²) cost but prevents explicit stability training; hard reset of z_L forces re-convergence each segment; deep supervision provides dense loss signal across reasoning depth
- **Failure signatures**: Fixed point violation (model continues updating after finding solution), spurious attractor trap (loss plateaus at non-zero value), non-trivial success (long plateau followed by sudden "grok" to correct answer)
- **First 3 experiments**:
  1. Simple puzzle probe: Test HRM on puzzles with 1-10 masked cells; measure stability rate and failure modes
  2. Latent trajectory visualization: Project z_i onto PCA plane; classify into four modes (trivial success/failure, non-trivial success/failure)
  3. Perturbation ablation: Test input relabeling alone, checkpoint ensemble alone, and combined; measure accuracy gain on Sudoku-Extreme

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the HRM segment dynamics explicitly implement a minimization algorithm on the conflict-count metric E?
  - Basis: Authors hypothesize dynamics minimize E but note it's "premature to say" and warrants future investigation
  - Why unresolved: Current evidence is correlational; spurious fixed points coincide with local minima of E, but no causal mechanism has been proven
  - What evidence would resolve it: Analysis showing segment updates consistently align with the gradient of E

- **Open Question 2**: Do the identified "guessing" dynamics and spurious attractors generalize to other recursive reasoning architectures?
  - Basis: Authors "conjecture that the qualitative taxonomy we provide will serve as a common vocabulary for the emerging class of recursive reasoners"
  - Why unresolved: Study is restricted to HRM; unknown if other latent reasoners (e.g., Coconut) rely on similar fixed-point guessing
  - What evidence would resolve it: Replication of PCA trajectory analysis on diverse recursive models like Universal Transformers

- **Open Question 3**: Can the latent space be structured to allow for stable, direct perturbation to escape spurious fixed points?
  - Basis: Authors note that perturbing latent state z directly "typically destructs the coherence... leading to unstable behavior"
  - Why unresolved: High dimensionality and lack of regularization make latent space fragile; arbitrary noise destroys semantic coherence
  - What evidence would resolve it: A regularization technique that allows noise injection in z to facilitate exploration without collapsing the reasoning state

## Limitations
- Analysis relies heavily on Sudoku-Extreme dataset which may not generalize to other reasoning tasks
- Fixed point violation hypothesis primarily supported by failure on simple puzzles but sample size not specified
- Grokking phenomenon described qualitatively but lacks systematic measurement across dataset
- Attractor identification methodology unclear despite visualization support

## Confidence
- Fixed point violation on simple puzzles: High confidence (directly tested with specific accuracy rates)
- Grokking dynamics in non-trivial successes: Medium confidence (described qualitatively)
- Multiple attractor landscape: Medium confidence (supported by visualizations but methodology unclear)
- Guess scaling effectiveness: High confidence (quantitative results show significant accuracy improvements)

## Next Checks
1. **Generalization Test**: Apply Augmented HRM to a different reasoning task (e.g., symbolic algebra or logical deduction) to verify mechanisms transfer beyond Sudoku
2. **Attractor Mapping**: Systematically map latent space by initializing trajectories from a grid of starting points to quantify attractors and their basins
3. **Perturbation Ablation**: Isolate individual contribution of each scaling strategy on puzzles of varying difficulty to determine most effective mechanism for different failure modes