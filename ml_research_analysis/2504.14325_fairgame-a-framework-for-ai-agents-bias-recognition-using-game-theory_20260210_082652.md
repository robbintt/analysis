---
ver: rpa2
title: 'FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory'
arxiv_id: '2504.14325'
source_url: https://arxiv.org/abs/2504.14325
tags:
- agents
- game
- games
- each
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FAIRGAME is a framework that enables reproducible testing of multi-agent
  LLM interactions using game-theoretic models. It supports configurable agent personalities,
  languages, and payoff matrices to detect biases and inconsistencies in LLM behavior.
---

# FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory

## Quick Facts
- arXiv ID: 2504.14325
- Source URL: https://arxiv.org/abs/2504.14325
- Reference count: 40
- FAIRGAME enables reproducible testing of multi-agent LLM interactions using game-theoretic models to detect biases and inconsistencies in LLM behavior

## Executive Summary
FAIRGAME is a framework that enables reproducible testing of multi-agent LLM interactions using game-theoretic models. It supports configurable agent personalities, languages, and payoff matrices to detect biases and inconsistencies in LLM behavior. Experiments with four LLMs (GPT-4o, Claude 3.5, Mistral Large, Llama 3.1) across five languages in Prisoner's Dilemma and Battle of the Sexes revealed that LLMs often deviate from game-theoretic predictions, exhibiting language-dependent cooperation levels and sensitivity to payoff structures. Internal variability, cross-language inconsistency, and sensitivity to payoffs were quantified using scoring metrics. Results indicate that LLMs may rely on prior knowledge beyond payoff matrices when selecting strategies, highlighting the need for controlled simulations to identify biases in strategic decision-making.

## Method Summary
FAIRGAME is a framework for testing multi-agent LLM interactions using game-theoretic models. It enables configurable agent personalities, languages, and payoff matrices to detect biases and inconsistencies in LLM behavior. The framework simulates agent interactions in games like Prisoner's Dilemma and Battle of the Sexes, measuring cooperation levels, consistency across languages, and sensitivity to payoff structures. Experiments tested four LLMs (GPT-4o, Claude 3.5, Mistral Large, Llama 3.1) across five languages, revealing deviations from game-theoretic predictions and highlighting the need for controlled simulations to identify biases in strategic decision-making.

## Key Results
- LLMs exhibit language-dependent cooperation levels and sensitivity to payoff structures in game-theoretic scenarios
- Internal variability, cross-language inconsistency, and sensitivity to payoffs were quantified using scoring metrics
- Results indicate LLMs may rely on prior knowledge beyond payoff matrices when selecting strategies

## Why This Works (Mechanism)
FAIRGAME leverages game-theoretic models to create controlled environments where LLM behavior can be systematically tested and measured. By using configurable parameters like agent personalities, languages, and payoff matrices, the framework isolates specific variables that might influence LLM decision-making. The scoring metrics provide quantitative measures of biases and inconsistencies, allowing researchers to identify patterns in LLM behavior that deviate from theoretical predictions. The controlled simulation environment eliminates many real-world complexities while still capturing strategic decision-making patterns.

## Foundational Learning
- Game-theoretic models (Prisoner's Dilemma, Battle of the Sexes): Provide structured scenarios for testing strategic decision-making - Quick check: Verify payoff matrices are correctly implemented
- Multi-agent LLM interaction simulation: Enables testing of collaborative and competitive behaviors - Quick check: Ensure agents can communicate effectively
- Language-dependent behavior analysis: Identifies cultural and linguistic influences on AI decision-making - Quick check: Test with multiple language pairs
- Scoring metrics for bias detection: Quantifies deviations from expected behavior - Quick check: Validate metric calculations
- Payoff matrix sensitivity: Measures how reward structures influence agent choices - Quick check: Test with varied payoff configurations
- Agent personality configuration: Tests how different behavioral prompts affect outcomes - Quick check: Verify personality prompt variations

## Architecture Onboarding

Component map: Configuration -> Game Setup -> Agent Interaction -> Result Collection -> Analysis

Critical path: User configures experiment parameters → FAIRGAME initializes game environment → Agents interact based on configured parameters → Results are collected and analyzed → Bias metrics are calculated

Design tradeoffs: FAIRGAME prioritizes reproducibility and controlled testing over real-world complexity. The framework sacrifices ecological validity for experimental precision, using simplified game-theoretic models rather than complex multi-agent scenarios. This tradeoff enables precise measurement of specific biases but may limit generalizability to real-world LLM applications.

Failure signatures: Inconsistent results across language pairs may indicate translation issues or cultural bias artifacts. High internal variability within a single LLM suggests instability in decision-making processes. Deviations from game-theoretic predictions could indicate reliance on prior knowledge or prompt engineering effects. Cross-language inconsistencies might reveal underlying model architecture biases.

First experiments:
1. Test Prisoner's Dilemma with default configurations across all four LLMs
2. Vary agent personalities while keeping language constant to measure personality effects
3. Compare cooperation levels between two languages for a single LLM

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies on simulated interactions rather than real-world multi-agent deployments
- Limited to two game-theoretic models, potentially limiting generalizability
- Language-dependent variations could be influenced by translation quality or cultural context
- Framework sensitivity to prompt engineering introduces potential measurement bias

## Confidence
High confidence in framework reproducibility and extensibility
Medium confidence in quantitative findings regarding LLM deviations from game-theoretic predictions
Low confidence in interpretation that LLMs rely on "prior knowledge beyond payoff matrices"

## Next Checks
1. Test FAIRGAME with additional game-theoretic models (e.g., Stag Hunt, Chicken Game) to assess whether observed LLM biases generalize across different strategic contexts and payoff structures

2. Conduct cross-lingual experiments using professional translations rather than automated translation to isolate language effects from translation artifacts and cultural context

3. Implement a sensitivity analysis varying prompt formulations, agent personalities, and temperature settings to quantify how experimental design choices influence measured LLM biases and strategic outcomes