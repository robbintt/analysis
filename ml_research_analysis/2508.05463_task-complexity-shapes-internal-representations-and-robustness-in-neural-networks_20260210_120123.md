---
ver: rpa2
title: Task complexity shapes internal representations and robustness in neural networks
arxiv_id: '2508.05463'
source_url: https://arxiv.org/abs/2508.05463
tags:
- accuracy
- neural
- network
- randomization
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how task complexity shapes internal representations
  and robustness in neural networks by introducing five data-agnostic probes: pruning,
  binarization, noise injection, sign flipping, and bipartite network randomization.
  The authors apply these methods to multilayer perceptrons (MLPs) trained on easy
  and hard classification tasks from MNIST and Fashion-MNIST datasets.'
---

# Task complexity shapes internal representations and robustness in neural networks

## Quick Facts
- **arXiv ID:** 2508.05463
- **Source URL:** https://arxiv.org/abs/2508.05463
- **Reference count:** 40
- **Primary result:** Task complexity determines whether neural networks rely on weight magnitudes (hard tasks) or just sign structure (easy tasks), revealed through five data-agnostic probes.

## Executive Summary
This study introduces five probes—pruning, binarization, noise injection, sign flipping, and bipartite network randomization—to assess how task complexity shapes internal representations in neural networks. Applied to MLPs trained on easy and hard MNIST/Fashion-MNIST tasks, the probes reveal that hard tasks depend critically on precise weight magnitudes while easy tasks rely primarily on sign structure. The authors demonstrate that moderate noise can enhance performance through stochastic resonance, and that preserving signed bipartite topology suffices for easy-task accuracy. These findings provide a model- and modality-agnostic measure of task complexity based on the performance gap between full-precision and binarized or shuffled networks.

## Method Summary
The authors train MLPs (784→64→1) on binary classification tasks using easy/hard pairs from MNIST and Fashion-MNIST datasets. Easy/hard classification is determined by Structural Similarity Index (SSIM) distance between digit classes. Models are trained with Adam optimizer and cosine-annealing learning rate for up to 10 epochs with early stopping. Five post-training probes are applied without fine-tuning: pruning (removing low-magnitude weights), binarization (converting weights to ±1), noise injection (adding uniform noise), sign flipping (reversing signs of small weights), and bipartite network randomization (preserving sign structure while randomizing connectivity). The same probes are applied to a fine-tuned DistilBERT model for Named Entity Recognition.

## Key Results
- Binarizing weights in hard-task models collapses accuracy to chance, while easy-task models remain robust (>90% accuracy)
- Moderate noise injection in binarized hard-task models produces stochastic resonance, improving accuracy by correcting sign errors in small-magnitude weights
- Preserving signed bipartite topology through network randomizations maintains high accuracy on easy tasks but not hard tasks
- Early layers in DistilBERT are less robust to binarization and randomization than later layers, with final layers showing remarkable resilience

## Why This Works (Mechanism)

### Mechanism 1: Task Complexity Determines Representation Fragility to Binarization
Hard-task models encode fine-grained distinctions through calibrated weight magnitudes, while easy-task models separate classes using only correct sign alignment. When weights are binarized to ±1, hard-task models collapse because they lose magnitude precision essential for difficult discriminations.

### Mechanism 2: Stochastic Resonance via Small-Weight Sign Correction
Moderate noise injection improves binarized model accuracy by flipping signs of incorrectly-signed small-magnitude weights. Small weights have uncertain signs during training (near-zero crossings), and noise comparable to σ(w) perturbs these weights, causing some to cross zero and correct sign errors.

### Mechanism 3: Signed Bipartite Topology Suffices for Easy-Task Performance
MLP layers function as signed bipartite graphs where aggregate sign patterns per node encode transformations. Randomizations preserving positive/negative degree distributions and edge signs maintain class-separating structure for easy tasks.

## Foundational Learning

- **Signed bipartite graph representation of MLP layers**: The paper's analytical framework treats each MLP layer as a bipartite graph with signed edges; without this, randomization strategies are unintelligible.
  - Quick check: Given a 64→64 MLP layer, how many nodes are in the left partition L and right partition R of the corresponding bipartite graph?

- **Stochastic resonance in nonlinear systems**: The noise injection results are interpreted through stochastic resonance; understanding why moderate noise helps but excessive noise harms is essential.
  - Quick check: In a bistable system, why does adding noise sometimes improve signal detection rather than degrade it?

- **Phase transitions in network percolation**: The sharp accuracy transition in pruned binarized hard-task models resembles phase transitions; the paper uses this language explicitly.
  - Quick check: What is the defining characteristic of a phase transition in a physical or network system?

## Architecture Onboarding

- **Component map**: Input layer → Bipartite graph L₀ → R₀ (signed, weighted edges) → Hidden layer(s) → Additional bipartite graphs R₀ → R₁, etc.
- **Critical path**: 1. Train model to convergence on target task 2. Apply probe (pruning/binarization/noise/flip/randomize) without retraining 3. Measure accuracy/F1 on test set 4. For randomizations: preserve P(k⁺), P(k⁻), and original signs to maintain performance
- **Design tradeoffs**: Binarization maximizes compression (1-bit weights) but only viable for easy tasks or robust late layers; pruning reduces FLOPs but hard-task models show sharp collapse points; randomization enables interpretability but requires storing sign patterns separately
- **Failure signatures**: Binarized model at chance accuracy indicates task too hard for magnitude-free representation; no stochastic resonance peak suggests weight distribution σ(w) is unusually narrow; early-layer binarization collapse in Transformers means residual connections don't compensate
- **First 3 experiments**: 1. Replicate binarization gap: Train MLPs on {0,7} (easy) vs {7,9} (hard) MNIST pairs; binarize and measure accuracy gap 2. Stochastic resonance sweep: Take failing binarized hard-task model; inject uniform noise U(-a, a) with a ∈ [0.001, 10.0]; plot accuracy vs. a/σ(w) 3. Layer-wise robustness on your model: Apply binarization to each layer of a fine-tuned transformer; identify which layers tolerate binarization

## Open Questions the Paper Calls Out

- **Representational similarity correlation**: How do probe-based robustness measures correlate with established representational similarity metrics like CKA or RSA? The current study focuses on functional performance rather than analyzing activation pattern geometry using similarity metrics.

- **Theoretical phase transition mechanism**: What drives the sharp performance phase transition observed in binarized models trained on hard tasks? The paper characterizes this phenomenologically but lacks formal theoretical explanation connecting task complexity to percolation-like behavior.

- **Convolutional layer applicability**: Do the observed robustness patterns hold for convolutional layers, or are they specific to fully connected architectures? Convolutional layers involve weight sharing and local connectivity that may alter probe effects compared to dense MLPs.

## Limitations
- Claims about task complexity shaping representations are well-supported for MLPs but extension to DistilBERT raises questions about applicability to self-attention models
- Mechanism explanations rely heavily on empirical observations without formal theoretical grounding
- Study uses fixed architecture dimensions (d=64) without exploring how probe effects scale with model capacity

## Confidence
- High confidence: Binarization collapse in hard-task MLPs (directly observable)
- Medium confidence: Stochastic resonance mechanism (plausible but novel, lacks corpus support)
- Medium confidence: Signed bipartite topology sufficiency for easy tasks (empirically supported but requires degree-distribution assumption)

## Next Checks
1. Test whether the binarization gap persists when training MLPs with identical architectures but different random seeds on the same task pairs to control for initialization effects.

2. Apply the bipartite randomization probes to a convolutional neural network (CNN) on CIFAR-10 to determine if signed bipartite topology preservation is architecture-dependent or general.

3. Measure the actual weight magnitude distributions in binarized vs. full-precision models to quantify whether hard-task models truly encode more information in weight magnitudes than easy-task models.