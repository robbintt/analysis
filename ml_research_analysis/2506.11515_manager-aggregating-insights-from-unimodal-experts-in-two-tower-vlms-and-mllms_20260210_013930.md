---
ver: rpa2
title: 'Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and
  MLLMs'
arxiv_id: '2506.11515'
source_url: https://arxiv.org/abs/2506.11515
tags:
- visual
- manager
- unimodal
- layer
- cross-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Manager, a lightweight and effective plugin
  that improves vision-language representation learning by adaptively aggregating
  insights from pre-trained unimodal experts at different layers. Under the Two-Tower
  VLM architecture, ManagerTower introduces managers in each cross-modal layer to
  aggregate multi-layer unimodal representations, significantly outperforming strong
  baselines on four downstream VL tasks.
---

# Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs

## Quick Facts
- arXiv ID: 2506.11515
- Source URL: https://arxiv.org/abs/2506.11515
- Reference count: 40
- Primary result: Manager plugin significantly improves vision-language representation learning by aggregating pre-trained unimodal expert insights

## Executive Summary
This paper introduces Manager, a lightweight plugin that enhances vision-language representation learning by adaptively aggregating insights from pre-trained unimodal experts at different layers. The approach is implemented in ManagerTower under the Two-Tower VLM architecture, where managers in each cross-modal layer aggregate multi-layer unimodal representations. The method demonstrates substantial performance improvements across four downstream VL tasks, outperforming strong baselines. The authors further extend their approach to Multimodal Large Language Models (MLLMs), showing that LLaVA-OV-Manager significantly boosts zero-shot performance across 20 downstream datasets with varying capabilities, images, and resolutions.

## Method Summary
Manager operates by introducing adaptive aggregation mechanisms at each cross-modal layer that leverage pre-trained unimodal experts. In the Two-Tower VLM architecture, ManagerTower integrates managers that dynamically combine multi-layer unimodal representations, allowing the model to benefit from depth-wise information aggregation. The approach is designed as a lightweight plugin that can be incorporated into existing architectures without requiring complete retraining. For MLLMs, the Manager extension is applied to LLaVA-OV, demonstrating that the aggregation strategy generalizes beyond traditional two-tower architectures. The orthogonal synergy between Manager and multi-grid algorithms is explored, showing complementary improvements in visual representation through depth and width dimensions.

## Key Results
- ManagerTower significantly outperforms strong baselines on four downstream VL tasks under Two-Tower VLM architecture
- LLaVA-OV-Manager demonstrates substantial zero-shot performance improvements across 20 downstream datasets with different capabilities, images, and resolutions
- The orthogonal synergy between Manager and multi-grid algorithms mitigates semantic ambiguity and further improves performance

## Why This Works (Mechanism)
Manager works by introducing adaptive aggregation at each cross-modal layer that leverages pre-trained unimodal experts. This mechanism allows the model to dynamically combine multi-layer unimodal representations, effectively capturing depth-wise information that would otherwise be lost in traditional cross-modal interactions. The lightweight plugin design enables efficient integration without significant computational overhead. The orthogonal synergy with multi-grid algorithms suggests that Manager improves visual representation through depth dimension while multi-grid addresses width dimension, creating complementary enhancements that jointly mitigate semantic ambiguity.

## Foundational Learning
- Two-Tower VLM Architecture: Understanding separate visual and language towers with cross-modal interactions
  - Why needed: Provides the architectural foundation for ManagerTower implementation
  - Quick check: Verify understanding of cross-modal layer positioning and unimodal tower separation

- Multi-Grid Algorithm: Grid-based training strategy that improves visual representation through width dimension
  - Why needed: Essential for understanding the orthogonal synergy claimed with Manager
  - Quick check: Confirm knowledge of grid-based training and its impact on visual features

- Unimodal Expert Pretraining: Leveraging separately pretrained visual and language models
  - Why needed: Core to Manager's aggregation mechanism and effectiveness
- Quick check: Understand how pre-trained experts contribute to cross-modal representation

- Zero-Shot Transfer: Evaluating model performance without task-specific fine-tuning
  - Why needed: Critical for assessing LLaVA-OV-Manager's generalization capabilities
  - Quick check: Verify understanding of zero-shot evaluation protocols and metrics

## Architecture Onboarding
Component map: Input -> Unimodal Experts -> Cross-Modal Layers with Managers -> Output
Critical path: Visual input and language input → separate unimodal processing → cross-modal layers with Manager aggregation → final cross-modal representation
Design tradeoffs: Manager provides depth-wise aggregation benefits but introduces additional parameters; orthogonal synergy with multi-grid suggests complementary improvements but requires careful integration
Failure signatures: Poor aggregation quality leading to semantic ambiguity; computational overhead from additional manager parameters; potential overfitting to specific unimodal expert characteristics
First experiments:
1. Ablation study removing Manager from cross-modal layers to quantify performance degradation
2. Comparative analysis of different aggregation strategies within Manager framework
3. Scalability test on larger datasets to evaluate generalization beyond MSCOCO and Flickr30k

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Limited scalability evaluation on larger, more diverse datasets beyond MSCOCO and Flickr30k
- Computational overhead quantification is insufficient, lacking explicit training time and inference speed analysis
- Extension to MLLMs focuses primarily on zero-shot performance without extensive fine-tuning comparisons
- Theoretical analysis of orthogonal synergy mechanisms between Manager and multi-grid algorithms is limited

## Confidence
- Manager effectiveness in Two-Tower VLMs: High confidence based on substantial improvements across multiple downstream tasks
- Extension to MLLMs: Medium confidence, supported by empirical evidence but limited fine-tuning comparisons
- Orthogonal synergy claims: Medium confidence, empirically supported but lacking detailed theoretical analysis
- Scalability claims: Low confidence due to evaluation on relatively small-scale datasets only

## Next Checks
1. Conduct experiments on larger-scale datasets (e.g., Conceptual Captions, Visual Genome) to assess scalability and generalization
2. Perform detailed computational analysis to quantify the overhead introduced by Manager in terms of training time and inference speed
3. Investigate the theoretical foundations of the orthogonal synergy between Manager and multi-grid algorithms through ablation studies and mathematical analysis