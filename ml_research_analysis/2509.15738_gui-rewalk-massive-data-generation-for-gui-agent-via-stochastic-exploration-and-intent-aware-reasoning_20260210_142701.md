---
ver: rpa2
title: 'GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration
  and Intent-Aware Reasoning'
arxiv_id: '2509.15738'
source_url: https://arxiv.org/abs/2509.15738
tags:
- task
- gui-rewalk
- urlhttps
- agents
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GUI-ReWalk is a reasoning-enhanced framework that generates realistic
  GUI interaction trajectories by combining stochastic exploration with goal-directed
  reasoning. It models human-like trial-and-error behaviors followed by intent-aware,
  purposeful actions, supporting multi-stride workflows across multiple applications.
---

# GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning

## Quick Facts
- arXiv ID: 2509.15738
- Source URL: https://arxiv.org/abs/2509.15738
- Reference count: 40
- Generates high-quality synthetic GUI interaction data that improves grounding and navigation benchmarks

## Executive Summary
GUI-ReWalk is a reasoning-enhanced framework for generating realistic GUI interaction trajectories by combining stochastic exploration with goal-directed reasoning. It models human-like trial-and-error behaviors followed by intent-aware, purposeful actions, supporting multi-stride workflows across multiple applications. Evaluated on grounding and navigation benchmarks, GUI-ReWalk achieves substantial improvements over comparable models, advancing robust automation in human-computer environments.

## Method Summary
GUI-ReWalk generates GUI trajectories through a three-phase process: (1) stochastic exploration via random walk with uniform action sampling, (2) goal inference using an LLM to derive intent from terminal states, and (3) task-guided completion under a goal-conditioned policy. The framework employs retrospective LLM-based annotation to create semantically rich supervision without manual labeling, and includes error-aware task recovery to reformulate stalled goals. Multi-stride trajectories are initiated across applications using cross-app task planning. The method trains Qwen2.5-VL-7B on synthetic data via supervised fine-tuning, achieving scalable and diverse dataset generation.

## Key Results
- Achieves 35.1 on Screenspot-Pro grounding benchmark
- Achieves 96.3 step success rate on AndroidControl navigation
- Generates over 50,000 navigation tasks with average 22.5 steps per trajectory

## Why This Works (Mechanism)

### Mechanism 1: Stochastic-to-Structured Trajectory Transition
GUI-ReWalk begins with random exploration as a Markov chain, then transitions to goal-directed MDP behavior where an LLM infers goals from terminal states. This exploration-then-exploitation pattern produces more diverse and human-like trajectories than purely task-driven approaches.

### Mechanism 2: Retrospective LLM-Based Annotation
The framework uses backward LLM labeling to generate semantically rich supervision without manual annotation. Given trajectories, the LLM infers step-level instructions and high-level task descriptions, creating training pairs with both local and global semantic grounding.

### Mechanism 3: Error-Aware Task Recovery with Goal Revision
When trajectories stall, the framework dynamically reformulates goals rather than abandoning execution. A recovery trigger detects stalled progress, and the LLM revises the goal to maintain data yield and capture recovery behaviors absent from clean demonstration data.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) vs. Markov Chains
  - Why needed here: The paper models random exploration as a Markov chain and goal-directed behavior as an MDP with goal-conditioned policy
  - Quick check question: Can you explain why adding a goal-conditioned policy π(a|s,g) transforms a Markov chain into something more structured?

- Concept: Sparse Reward Signals in RL
  - Why needed here: Section 3.3 defines reward as Rg = 1 only upon reaching goal state, zero otherwise
  - Quick check question: Why might sparse rewards make direct RL on GUI tasks impractical without additional structure?

- Concept: Trajectory Entropy as Diversity Metric
  - Why needed here: The paper claims GUI-ReWalk achieves "higher trajectory entropy" measuring behavioral diversity
  - Quick check question: Would you expect higher entropy trajectories to always improve downstream agent performance? When might they hurt?

## Architecture Onboarding

- Component map: Random Walk Module -> Goal Inference Module (Φ_LLM) -> Task-Guided Policy (π) -> Retrospective Annotator (B) -> Task Recovery Module (Ψ_LLM) -> Cross-App Initiator (Π_LLM)

- Critical path: Start from random app/state → execute random walk (5-10 steps, diminishing over strides) → LLM infers goal from final random-walk state → execute task-guided completion until goal reached or stalled → if stalled: trigger recovery, revise goal, continue → upon completion: retrospective annotation generates labels → cross-app initiator proposes next stride goal → repeat from step 1 in new app

- Design tradeoffs: Longer random walks increase diversity but reduce task coherence; paper gradually reduces random-walk length over strides. Multi-stride trajectories better reflect real workflows but cost ~$0.30-0.62 per trajectory. Filtering login-required trajectories protects privacy but limits workflow coverage.

- Failure signatures: Infinite loops (recovery repeatedly triggers without progress), semantic drift (cross-app tasks lose connection to original intent), system-side effects (random actions in Settings break subsequent trajectories), login walls (trajectories reach authentication pages and must be discarded).

- First 3 experiments: 1) Ablate random walk length (0 steps vs. 5 vs. 10) and measure trajectory diversity (entropy) and grounding accuracy on Screenspot-Pro. 2) Compare retrospective annotation quality against human labels on a held-out subset. 3) Disable task recovery and measure trajectory completion rate and average stride count.

## Open Questions the Paper Calls Out

### Open Question 1
How can GUI-ReWalk be adapted to handle authenticated sessions without compromising user privacy? The framework currently filters out trajectories encountering login pages, limiting coverage of workflows dependent on authenticated states. Integration with a secure credential manager or simulation environment would enable full traversal of gated application flows.

### Open Question 2
How can the framework mitigate system-level side effects caused by stochastic exploration? Random actions can trigger global changes (e.g., enabling airplane mode) that disrupt the environment. A sandboxed action space or dynamic masking of system-critical buttons during the random walk phase would prevent irreversible environment states.

### Open Question 3
Is a uniform random policy the most efficient strategy for the initial exploration phase? Uniform sampling may result in low-efficiency exploration compared to curiosity-driven or uncertainty-based strategies. A comparative analysis of trajectory diversity and model performance would reveal whether entropy-based exploration outperforms the current uniform approach.

### Open Question 4
Does Retrospective Annotation introduce semantic noise when interpreting failed or erratic trajectories? Attributing coherent intent to purely stochastic actions risks generating hallucinated supervision signals. Human evaluation measuring logical consistency of annotations in early vs. late phases would quantify annotation reliability.

## Limitations
- Trajectories encountering login pages are filtered out, limiting coverage of authenticated workflows
- System-level actions (airplane mode, network restrictions) can cause irreversible environment changes
- Dependency on LLM reasoning quality for both trajectory generation and retrospective annotation

## Confidence
- High confidence: Performance improvements on established benchmarks (Screenspot-Pro, AndroidControl) are well-documented and directly comparable to baselines
- Medium confidence: Claims about human-like trajectories are supported by design rationale and entropy metrics but lack direct validation against human demonstration data
- Low confidence: Scalability and cost-effectiveness claims (~$0.30-0.62 per trajectory) lack detailed computational cost breakdowns

## Next Checks
1. Sample 200 retrospective annotations and have human annotators rate their accuracy against ground-truth task descriptions to quantify LLM labeling reliability
2. Compare trajectory quality (diversity, coherence, completion rate) when removing the LLM goal inference step or the retrospective annotation step to isolate their contributions
3. Measure how often task recovery is triggered and whether revised goals maintain semantic continuity with original intent across 1000+ generated trajectories