---
ver: rpa2
title: 'Spread them Apart: Towards Robust Watermarking of Generated Content'
arxiv_id: '2502.07845'
source_url: https://arxiv.org/abs/2502.07845
tags:
- watermark
- image
- generated
- user
- watermarking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Spread them Apart, a watermarking framework
  for embedding digital watermarks into generated content without requiring retraining
  of the generative model. The method embeds watermarks during inference by optimizing
  latent representations to satisfy pixel difference constraints defined by the watermark
  and secret key.
---

# Spread them Apart: Towards Robust Watermarking of Generated Content

## Quick Facts
- arXiv ID: 2502.07845
- Source URL: https://arxiv.org/abs/2502.07845
- Reference count: 19
- Primary result: Watermarking framework embeds watermarks during inference without retraining, achieving robust attribution against synthetic attacks with SSIM 0.86, PSNR 29.4, and FID 13.2

## Executive Summary
This paper introduces "Spread them Apart," a watermarking framework that embeds digital watermarks into generated content during inference without requiring model retraining. The method optimizes latent representations to satisfy pixel difference constraints defined by the watermark and secret key, claiming robustness to bounded additive perturbations. The authors demonstrate that their approach achieves state-of-the-art performance in robustness to synthetic watermark removal attacks including brightness/contrast adjustment, gamma correction, and adversarial attacks, while maintaining comparable image quality metrics to existing methods.

## Method Summary
The framework embeds watermarks by optimizing latent representations during the inference process of generative models. Instead of modifying the model architecture or training process, the method works by adjusting the latent space to satisfy specific pixel difference constraints that encode the watermark information. This approach leverages the inherent flexibility of latent representations in modern generative models to embed imperceptible watermarks that can be later detected using the corresponding secret key.

## Key Results
- Achieves SSIM of 0.86 and PSNR of 29.4 while maintaining watermark robustness
- Demonstrates high true positive rates in both attribution and detection tasks under various attack conditions
- Matches state-of-the-art methods in robustness to synthetic watermark removal attacks including brightness/contrast adjustment, gamma correction, and adversarial attacks
- FID score of 13.2 indicates comparable image quality to baseline generative models

## Why This Works (Mechanism)
The method works by exploiting the optimization capabilities of latent representations in generative models. During inference, the framework modifies the latent space to create specific pixel difference patterns that encode the watermark. These patterns are designed to be imperceptible under normal viewing conditions but can be detected algorithmically using the secret key. The robustness claims stem from the theoretical proof that these embedded watermarks can withstand bounded additive perturbations, making them resilient to common image processing operations.

## Foundational Learning

**Latent Representations** - Why needed: Form the basis for watermark embedding without modifying model architecture. Quick check: Verify that latent space modifications preserve semantic content while enabling watermark encoding.

**Pixel Difference Constraints** - Why needed: Provide the mathematical framework for embedding detectable patterns. Quick check: Ensure constraints are satisfied while maintaining visual quality.

**Bounded Perturbation Theory** - Why needed: Underpins the robustness claims against image processing operations. Quick check: Validate theoretical bounds match empirical performance.

## Architecture Onboarding

**Component Map**: Input Image -> Latent Optimization -> Watermark Embedding -> Output Image

**Critical Path**: The core workflow involves taking the latent representation of generated content, optimizing it to satisfy watermark constraints, and producing the final watermarked output. The optimization process must balance watermark strength with visual quality preservation.

**Design Tradeoffs**: The framework trades some image quality for robustness to watermark removal attacks. The choice of pixel difference constraints and optimization objectives directly impacts both the imperceptibility of the watermark and its resilience to various attacks.

**Failure Signatures**: Watermark failure may manifest as reduced detection accuracy under strong attacks, visible artifacts in the watermarked image, or degradation in generative model output quality (measured by FID scores).

**3 First Experiments**:
1. Test watermark detection accuracy on clean images with varying watermark strengths
2. Evaluate image quality metrics (SSIM, PSNR) as a function of watermark robustness
3. Measure true positive rates under brightness/contrast adjustment attacks

## Open Questions the Paper Calls Out

The paper acknowledges that robustness claims are primarily validated against synthetic attacks, with limited evaluation against real-world watermark removal techniques or advanced adversarial strategies. The theoretical proof of robustness to bounded additive perturbations does not account for more sophisticated attacks that may exploit the watermark embedding mechanism itself. Additionally, the evaluation focuses on Stable Diffusion models, raising questions about generalizability to other generative architectures or modalities beyond images.

## Limitations

- Robustness claims primarily validated against synthetic attacks with limited real-world evaluation
- Theoretical proofs don't account for sophisticated attacks targeting the embedding mechanism
- Evaluation limited to Stable Diffusion models, raising generalizability concerns
- No assessment of performance under real-world conditions like compression or format conversion

## Confidence

**High**: Technical approach and mathematical formulation of watermark embedding mechanism
**Medium**: Robustness claims given limited attack surface evaluation
**Low**: Practical applicability across diverse real-world scenarios and generative model types

## Next Checks

1. Evaluate robustness against advanced adversarial watermark removal attacks that specifically target the embedding mechanism rather than generic image perturbations
2. Test generalizability across different generative model architectures (e.g., GANs, autoregressive models) and content modalities (text, video)
3. Assess performance under real-world conditions including compression, format conversion, and partial occlusion attacks that may be encountered in practical deployment scenarios