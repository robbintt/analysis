---
ver: rpa2
title: 'Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop
  Simulator, Causal Benchmark and Joint IL-RL Baseline'
arxiv_id: '2504.14709'
source_url: https://arxiv.org/abs/2504.14709
tags:
- learning
- imitation
- driving
- reinforcement
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the "copycat problem" in imitation learning-based
  planners, where policies overfit to initial conditions and fail to generalize to
  diverse scenarios. The authors propose a closed-loop simulator supporting both imitation
  and reinforcement learning, a causal benchmark derived from Waymo Open Dataset to
  rigorously assess this issue, and a joint IL-RL framework (MTR-SAC) to improve policy
  adaptability.
---

# Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline

## Quick Facts
- arXiv ID: 2504.14709
- Source URL: https://arxiv.org/abs/2504.14709
- Authors: Hui Zhou; Shaoshuai Shi; Hongsheng Li
- Reference count: 36
- Primary result: Joint IL-RL approach significantly outperforms pure IL and RL baselines on causal driving scenarios

## Executive Summary
This paper addresses the "copycat problem" in imitation learning-based planners, where policies overfit to initial conditions and fail to generalize to diverse scenarios. The authors propose a closed-loop simulator supporting both imitation and reinforcement learning, a causal benchmark derived from Waymo Open Dataset to rigorously assess this issue, and a joint IL-RL framework (MTR-SAC) to improve policy adaptability. Experiments show that the causal benchmark is more challenging than the original dataset, with completion rates dropping from 0.51 to 0.31. The integrated IL-RL approach significantly outperforms pure IL (0.58 vs. 0.31 completion on test4k) and RL-only methods, demonstrating better generalization and robustness to diverse driving goals.

## Method Summary
The authors introduce a closed-loop simulator that supports both imitation learning (IL) and reinforcement learning (RL) for autonomous driving planning. They create a causal benchmark by filtering the Waymo Open Dataset to include only scenarios requiring causal reasoning rather than simple reactive behaviors. The proposed MTR-SAC algorithm combines IL pretraining with RL fine-tuning using a multi-task reward structure. The causal benchmark is designed to expose the limitations of pure imitation learning by including diverse driving scenarios with varying goals and constraints that cannot be solved through simple copying of expert demonstrations.

## Key Results
- Causal benchmark completion rate drops from 0.51 to 0.31 compared to original dataset
- MTR-SAC achieves 0.58 completion rate on test4k, significantly outperforming pure IL (0.31)
- RL-only methods perform worse than both MTR-SAC and pure IL approaches
- The copycat problem is empirically demonstrated through significant performance degradation on causal scenarios

## Why This Works (Mechanism)
The joint IL-RL approach works by leveraging the generalization capabilities of reinforcement learning while maintaining the efficiency of imitation learning. IL provides a good initial policy that captures common driving patterns, while RL fine-tuning adapts the policy to handle diverse scenarios and causal reasoning tasks. The multi-task reward structure encourages the agent to learn multiple driving objectives simultaneously, improving robustness to different driving goals and constraints.

## Foundational Learning
- Causal reasoning in autonomous driving: Understanding how actions affect future states is crucial for handling complex driving scenarios that require planning rather than reactive behavior
- Imitation learning limitations: Pure IL can lead to overfitting to training distributions and failure to generalize to novel situations
- Reinforcement learning for generalization: RL can explore diverse states and actions, potentially discovering more robust policies
- Closed-loop simulation: Essential for training and evaluating planning policies in a realistic, interactive environment
- Multi-task learning: Combining multiple objectives can improve policy robustness and adaptability
- Dataset filtering for causal reasoning: Creating benchmarks that specifically test causal understanding rather than simple pattern matching

## Architecture Onboarding
- Component map: Waymo dataset -> Causal benchmark filter -> Closed-loop simulator -> MTR-SAC (IL pretraining -> RL fine-tuning) -> Policy evaluation
- Critical path: Data preprocessing → Causal filtering → IL pretraining → RL fine-tuning → Policy evaluation on causal benchmark
- Design tradeoffs: The authors balance between the efficiency of IL and the generalization capabilities of RL, choosing to combine both approaches rather than relying on a single method
- Failure signatures: Poor performance on causal scenarios indicates the copycat problem, while low completion rates on the causal benchmark suggest insufficient generalization
- First experiments:
  1. Evaluate pure IL policy on causal benchmark to establish baseline performance
  2. Train MTR-SAC from scratch (without IL pretraining) to assess the importance of IL initialization
  3. Compare MTR-SAC with pure RL methods to demonstrate the benefits of the joint approach

## Open Questions the Paper Calls Out
None provided in the input.

## Limitations
- Causal benchmark representativeness may be limited due to filtering process introducing biases
- Performance comparisons could be influenced by hyperparameter tuning differences not fully explored
- Simulator fidelity to real-world dynamics is not thoroughly validated
- Evaluation focuses primarily on completion rates rather than comprehensive safety metrics

## Confidence
- Causal benchmark validity: Medium
- MTR-SAC performance claims: Medium
- Simulator fidelity: Low

## Next Checks
1. Conduct a cross-validation study using scenarios from different geographic regions in the Waymo dataset to assess the causal benchmark's generalizability across diverse driving cultures and infrastructure.
2. Perform systematic ablation studies varying key hyperparameters (learning rates, reward weights, IL initialization strength) to establish the robustness of MTR-SAC's performance advantages.
3. Implement a transfer learning experiment where policies trained in the closed-loop simulator are tested on real-world data or a higher-fidelity simulation environment to quantify simulator-reality gaps.