---
ver: rpa2
title: Digitization of Document and Information Extraction using OCR
arxiv_id: '2506.11156'
source_url: https://arxiv.org/abs/2506.11156
tags:
- text
- extraction
- document
- documents
- scanned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a hybrid framework combining OCR techniques
  with Large Language Models (LLMs) to extract structured information from both scanned
  and digital documents. The system preprocesses scanned documents with techniques
  like binarization and skew correction, uses OCR tools (Tesseract, DocTR, Google
  Vision API) for text extraction, and employs LLMs for semantic interpretation and
  key-value pair extraction.
---

# Digitization of Document and Information Extraction using OCR
## Quick Facts
- arXiv ID: 2506.11156
- Source URL: https://arxiv.org/abs/2506.11156
- Reference count: 0
- Hybrid OCR + LLM framework achieves ~94% OCR accuracy and high-precision structured data extraction

## Executive Summary
This study presents a hybrid framework that combines Optical Character Recognition (OCR) techniques with Large Language Models (LLMs) to extract structured information from both scanned and digital documents. The system preprocesses scanned documents using techniques like binarization and skew correction, applies multiple OCR tools (Tesseract, DocTR, Google Vision API), and uses LLMs for semantic interpretation and key-value pair extraction. For digital documents, direct parsing via tools like pdfplumber and PyMuPDF is employed. The framework demonstrates high accuracy and precision in producing structured JSON outputs while maintaining flexibility across different document types.

## Method Summary
The framework preprocesses scanned documents with techniques including binarization, skew correction, and denoising to enhance OCR accuracy. It employs multiple OCR tools (Tesseract, DocTR, Google Vision API) for text extraction, with the Google Vision API achieving the highest accuracy at approximately 94%. For digital documents, direct parsing using tools like pdfplumber and PyMuPDF provides near-perfect accuracy. LLM-based extraction interprets semantic context and generates structured JSON outputs with high precision and minimal false positives, outperforming traditional rule-based methods. The system demonstrates scalability across various document types while maintaining flexibility in handling different layouts and formats.

## Key Results
- Google Vision API achieved highest OCR accuracy at ~94% across scanned documents
- Digital document parsing reached near-perfect accuracy (~100%) using direct extraction methods
- LLM-based extraction produced structured JSON outputs with high precision and minimal false positives

## Why This Works (Mechanism)
The framework's effectiveness stems from combining traditional OCR accuracy with LLM semantic understanding. The hybrid approach leverages OCR tools' strength in character recognition while using LLMs to interpret context and extract meaningful key-value pairs. This addresses the limitations of rule-based extraction methods by providing flexibility in handling diverse document layouts and semantic variations. The preprocessing steps enhance OCR performance on degraded scans, while the direct parsing approach for digital documents eliminates the accuracy loss associated with OCR conversion.

## Foundational Learning
1. **Document Preprocessing** - Why needed: Improves OCR accuracy on degraded scans by correcting skew, reducing noise, and enhancing contrast
   Quick check: Measure OCR accuracy improvement with/without preprocessing on low-quality documents

2. **Multi-tool OCR Strategy** - Why needed: Different OCR tools excel at different aspects (layout analysis, character recognition, language support)
   Quick check: Compare accuracy metrics across Tesseract, DocTR, and Google Vision API on identical document sets

3. **LLM-based Semantic Extraction** - Why needed: Transforms unstructured text into structured data by understanding context and relationships
   Quick check: Evaluate precision and recall of extracted key-value pairs against ground truth

4. **Direct Digital Parsing** - Why needed: Avoids OCR conversion errors by extracting text directly from digital document structures
   Quick check: Compare accuracy between direct parsing and OCR conversion on digital PDFs

## Architecture Onboarding
Component Map: Preprocessing -> OCR Extraction -> LLM Semantic Analysis -> Structured Output
Critical Path: Document Input -> Preprocessing (if scanned) -> OCR (scanned) OR Direct Parsing (digital) -> LLM Extraction -> JSON Output
Design Tradeoffs: OCR tools offer layout preservation but may introduce recognition errors; direct parsing is accurate but limited to digital formats; LLM extraction provides flexibility but requires prompt engineering
Failure Signatures: OCR errors manifest as character-level mistakes; LLM extraction failures appear as incorrect key-value associations or missed semantic relationships
First Experiments:
1. Benchmark OCR accuracy on a diverse document corpus with varying quality levels
2. Test LLM extraction precision on documents with complex layouts and specialized terminology
3. Compare processing times between OCR-based and direct parsing approaches for digital documents

## Open Questions the Paper Calls Out
None

## Limitations
- Performance metrics may not generalize across diverse document layouts, fonts, and quality conditions beyond the study's test set
- LLM extraction accuracy depends heavily on prompt quality and domain-specificity, which aren't fully detailed in the methodology
- Framework scalability claims require validation across substantially larger and more heterogeneous document collections

## Confidence
- OCR accuracy claims (High confidence): Google Vision API's superior performance aligns with established benchmarks
- LLM extraction effectiveness (Medium confidence): Results are promising but depend heavily on implementation details not fully specified
- Framework scalability (Low confidence): Theoretical framework well-designed, but real-world validation across diverse document types needed

## Next Checks
1. Cross-validation with multiple document repositories containing varied layouts, languages, and quality levels to test generalizability of the ~94% OCR accuracy claim
2. A/B testing against established commercial document processing systems using identical document sets to benchmark the LLM-based extraction performance
3. Stress testing the system with intentionally degraded document quality (noise, skew, low resolution) to identify performance degradation thresholds and failure modes