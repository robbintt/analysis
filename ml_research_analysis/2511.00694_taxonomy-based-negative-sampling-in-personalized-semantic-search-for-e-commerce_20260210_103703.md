---
ver: rpa2
title: Taxonomy-based Negative Sampling In Personalized Semantic Search for E-commerce
arxiv_id: '2511.00694'
source_url: https://arxiv.org/abs/2511.00694
tags:
- items
- search
- query
- sampling
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving semantic retrieval
  in e-commerce search by developing a model that can understand subtle differences
  between similar products while incorporating user personalization. The authors propose
  a taxonomy-based hard-negative sampling (TB-HNS) strategy that leverages hierarchical
  product categories to generate semantically challenging negatives during training,
  improving the model's ability to distinguish closely related items.
---

# Taxonomy-based Negative Sampling In Personalized Semantic Search for E-commerce

## Quick Facts
- arXiv ID: 2511.00694
- Source URL: https://arxiv.org/abs/2511.00694
- Reference count: 35
- Primary result: 77.89% Recall@24 with 2.70% conversion rate uplift in live A/B testing

## Executive Summary
This paper addresses the challenge of improving semantic retrieval in e-commerce search by developing a model that can understand subtle differences between similar products while incorporating user personalization. The authors propose a taxonomy-based hard-negative sampling strategy that leverages hierarchical product categories to generate semantically challenging negatives during training, improving the model's ability to distinguish closely related items. Their approach integrates personalization by modeling each customer's past purchase history and behavior, creating an enhanced semantic engine that falls back to non-personalized mode when needed. The system outperforms traditional BM25 and modern neural baselines on Recall@K metrics while demonstrating substantial business impact through improved conversion rates and average order value.

## Method Summary
The authors propose a taxonomy-based hard-negative sampling (TB-HNS) strategy that leverages hierarchical product categories to generate semantically challenging negatives during training. This approach improves the model's ability to distinguish closely related items by exposing it to more difficult negative examples drawn from the same product taxonomy. The personalization component models each customer's past purchase history and behavior, creating user-specific embeddings that enhance semantic matching. The system employs a two-stage retrieval architecture with dense retrieval followed by re-ranking, and includes a fallback mechanism to non-personalized search when personalization data is insufficient. The combined model achieves superior performance on Recall@K metrics while reducing training overhead through more efficient negative sampling.

## Key Results
- Combined model achieves 77.89% Recall@24 on e-commerce dataset
- Live A/B testing shows 2.70% conversion rate uplift and 2.04% add-to-cart rate improvement
- Taxonomy-based negatives reduce training overhead and accelerate convergence

## Why This Works (Mechanism)
The taxonomy-based hard-negative sampling strategy works by leveraging the hierarchical structure of product categories to generate semantically challenging negative examples during training. By sampling negatives from the same or closely related categories, the model learns to distinguish between products that share many semantic features but differ in subtle ways important for user preferences. This approach is more effective than random negative sampling because it exposes the model to the types of confusing cases it will encounter in real search scenarios. The personalization component enhances this by incorporating user-specific behavioral patterns, allowing the model to understand individual preferences within the semantic space. The two-stage retrieval architecture with fallback mechanisms ensures robustness by maintaining performance even when personalization data is limited.

## Foundational Learning

**Hierarchical product taxonomy**: Why needed - provides structured relationships between products for intelligent negative sampling. Quick check - verify taxonomy coverage and depth across product categories.

**Dense retrieval vs sparse retrieval**: Why needed - dense retrieval captures semantic similarity beyond keyword matching. Quick check - compare retrieval quality on semantically similar but lexically different queries.

**Hard-negative sampling**: Why needed - improves model discrimination by training on challenging negative examples. Quick check - measure training convergence speed and final model accuracy with different negative sampling strategies.

**User personalization in search**: Why needed - captures individual preferences to improve relevance. Quick check - evaluate personalization effectiveness across different user activity levels.

**Two-stage retrieval architecture**: Why needed - balances efficiency of initial retrieval with accuracy of re-ranking. Quick check - measure recall degradation when skipping re-ranking stage.

## Architecture Onboarding

Component map: User query -> Dense Retriever -> Candidate Pool -> Re-ranker -> Results. Taxonomy hierarchy -> Hard-negative sampler -> Training pipeline. User history -> Personalization encoder -> Combined embedding space.

Critical path: Query encoding → Dense retrieval → Hard-negative generation → Model training → Personalization integration → A/B testing deployment.

Design tradeoffs: The taxonomy-based sampling reduces training time but requires structured category data. Personalization improves relevance but needs sufficient historical data. The two-stage architecture balances speed and accuracy but adds complexity.

Failure signatures: Poor taxonomy quality leads to ineffective negatives. Insufficient user history causes personalization fallback. Cold-start items receive lower relevance scores initially.

First experiments: 1) Compare Recall@K with random vs taxonomy-based negatives. 2) Measure personalization impact on conversion rates across user segments. 3) Test cold-start performance for new products without purchase history.

## Open Questions the Paper Calls Out

None

## Limitations

- Reliance on single e-commerce dataset may limit generalizability to other domains
- Taxonomy-based sampling strategy may not translate well to domains lacking structured taxonomies
- Personalization effectiveness limited for truly cold-start users with minimal historical data

## Confidence

- **High Confidence**: The technical approach of combining dense retrieval with hard-negative sampling is sound and well-established in the literature. The reported improvements over BM25 and ANCE baselines are expected given the sophistication of the approach.
- **Medium Confidence**: The taxonomy-based hard-negative sampling methodology shows promise but requires validation on datasets with different taxonomic structures. The computational efficiency claims need independent verification across various hardware configurations.
- **Medium Confidence**: The A/B testing results showing business metric improvements are promising but platform-specific. The 0.6% average order value increase, while statistically significant, represents a modest business impact that may vary with market conditions.

## Next Checks

1. **Cross-Domain Validation**: Test the TB-HNS approach on non-e-commerce datasets (such as academic paper search or job recommendation) to assess generalizability of the taxonomy-based negative sampling strategy beyond product search scenarios.

2. **Cold-Start User Analysis**: Conduct a detailed ablation study measuring personalization effectiveness across different user activity levels, particularly focusing on users with minimal historical data to quantify the cold-start performance gap.

3. **Scalability Benchmark**: Evaluate training convergence rates and inference latency across different catalog sizes (10K, 100K, 1M items) to verify the claimed computational efficiency benefits hold at scale and identify potential bottlenecks in large-scale deployments.