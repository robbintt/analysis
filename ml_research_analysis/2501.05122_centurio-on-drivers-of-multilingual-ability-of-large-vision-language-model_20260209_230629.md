---
ver: rpa2
title: 'Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model'
arxiv_id: '2501.05122'
source_url: https://arxiv.org/abs/2501.05122
tags:
- l100
- languages
- llama
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the drivers of multilingual ability in
  Large Vision-Language Models (LVLMs), focusing on the optimal number of training
  languages, language distribution in pre-training and instruction-tuning data, and
  improving multilingual text-in-image understanding. The authors conduct a series
  of multi-stage experiments spanning 13 downstream tasks and 43 languages, revealing
  that (i) up to 100 training languages can be included simultaneously without degrading
  English performance, (ii) as little as 25-50% non-English data greatly improves
  multilingual performance while retaining strong English performance, and (iii) including
  non-English OCR data in pre-training and instruction-tuning is crucial for improving
  multilingual text-in-image understanding.
---

# Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model

## Quick Facts
- arXiv ID: 2501.05122
- Source URL: https://arxiv.org/abs/2501.05122
- Reference count: 40
- Authors trained Centurio, a 100-language LVLM, achieving state-of-the-art performance on 14 tasks and 56 languages

## Executive Summary
This paper investigates the drivers of multilingual ability in Large Vision-Language Models (LVLMs) through systematic experiments across 13 downstream tasks and 43 languages. The authors find that up to 100 training languages can be included simultaneously without degrading English performance, that as little as 25-50% non-English data greatly improves multilingual performance while retaining strong English performance, and that including non-English OCR data in pre-training and instruction-tuning is crucial for multilingual text-in-image understanding. They introduce a new benchmark, SMPQA, for evaluating multilingual text-in-image understanding and train Centurio, a 100-language LVLM, which matches popular multilingual open-weight LVLMs on English and high-resource languages while outperforming them on low(er)-resource languages.

## Method Summary
The authors conduct a series of multi-stage experiments using SigLIP SO400/384 image encoder with Phi-3.5 LLM backbone (3.8B parameters), employing LoRA fine-tuning (rank=256, α=512) for efficiency. They systematically vary the number of training languages (10→100), language distributions in pre-training and instruction-tuning data (0-100% English), and the presence of multilingual OCR data. The study uses machine-translated instruction data (via NLLB) to ensure comparability across 100 languages, synthetic OCR data generated by Synthdog, and evaluates on 13 downstream tasks including VQA, image captioning, and text-in-image understanding.

## Key Results
- Up to 100 training languages can be included simultaneously without degrading English performance
- As little as 25-50% non-English data greatly improves multilingual performance while retaining strong English performance
- Including non-English OCR data in pre-training and instruction-tuning is crucial for improving multilingual text-in-image understanding
- Centurio achieves state-of-the-art performance on 14 tasks and 56 languages, matching popular multilingual open-weight LVLMs on English and high-resource languages while outperforming them on low(er)-resource languages

## Why This Works (Mechanism)

### Mechanism 1: Language Exposure Over Proportion
The model leverages existing multilingual knowledge from the base LLM rather than learning each language from scratch. Cross-lingual transfer from the base LLM's multilingual representations, combined with instruction-tuning that activates these capabilities, means in-language training data presence matters more than proportion for multilingual LVLM performance. English performance can be retained with only 25-50% non-English data. This assumes the underlying LLM has sufficient multilingual pre-training to support cross-lingual transfer, and that LoRA fine-tuning preserves cross-lingual capabilities without catastrophic forgetting.

### Mechanism 2: Multilingual Pre-Training Distribution Effect
Pre-training on multilingual image captions aligns visual features with multilingual text embeddings, creating language-agnostic visual representations that support downstream tasks in any covered language. A more multilingual pre-training distribution (50% non-English) improves downstream multilingual performance, especially for low-resource languages, without harming English. This assumes machine-translated captions provide sufficient signal for visual-text alignment despite quality degradation in low-resource languages, and that the image encoder already provides sufficiently generic visual features.

### Mechanism 3: Multilingual OCR Data + Unfrozen Image Encoder
Multilingual OCR data teaches the image encoder to recognize script-specific visual patterns; unfreezing allows adaptation of visual features to non-Latin scripts that differ fundamentally from Latin-script pre-training data. Including non-English synthetic OCR data and unfreezing the image encoder is necessary for multilingual text-in-image understanding. This assumes synthetic OCR data (via Synthdog) transfers to real-world text-in-image tasks, and that the base image encoder has limited exposure to non-Latin scripts in its original pre-training.

## Foundational Learning

- **Concept: Two-Stage LVLM Training (Pre-training + Instruction Tuning)**
  - Why needed here: The paper's findings are stage-specific; optimal distributions differ between pre-training and instruction tuning.
  - Quick check question: Can you explain why pre-training focuses on image captions while instruction tuning uses diverse tasks?

- **Concept: Curse of Multilinguality**
  - Why needed here: The paper explicitly contrasts their findings with this well-known tradeoff; understanding it provides context for why their results are notable.
  - Quick check question: What is the traditional curse of multilinguality, and why might it not apply here?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: All experiments use LoRA for efficiency; understanding its constraints helps interpret why certain findings might be LoRA-specific.
  - Quick check question: What are the trade-offs of using LoRA vs. full fine-tuning for multilingual adaptation?

## Architecture Onboarding

- **Component map:** SigLIP SO400/384 image encoder -> 2-layer MLP projection -> Phi-3.5 LLM backbone -> LoRA fine-tuning (rank=256, α=512)

- **Critical path:**
  1. Choose LLM backbone with strong multilingual pre-training
  2. Translate English instruction-tuning data via NLLB to target languages
  3. Set English proportion (recommended: 50% for pre-training, 50% for instruction tuning)
  4. Generate synthetic OCR data via Synthdog for text-in-image tasks
  5. Train: pre-training first (unfreeze image encoder if OCR is needed), then instruction tuning with continued LoRA adapter

- **Design tradeoffs:**
  - Machine translation vs. native data: MT enables controlled experiments but may introduce noise; native data is higher quality but harder to scale to 100 languages
  - Image encoder freezing vs. unfreezing: Freezing is faster; unfreezing is necessary for multilingual OCR but increases compute and overfitting risk
  - Number of languages: More languages improve low-resource performance but require more complex data management; 100 languages worked without degradation, but this may be model-size dependent

- **Failure signatures:**
  - Near-zero performance on non-Latin scripts: Image encoder not trained on multilingual OCR data
  - Language fidelity failure (responding in wrong language): Insufficient in-language instruction tuning data
  - English performance drop: Non-English proportion too high (>75%) or LoRA rank too low
  - OCR failure on real images: Synthetic OCR data doesn't generalize; need real-world multilingual text data

- **First 3 experiments:**
  1. Baseline replication: Train with 50% English instruction tuning on 10 languages (T5 tier); verify no English degradation and language fidelity improvement
  2. Language scaling test: Incrementally add language tiers (T4→T3→T2→T1); monitor per-tier performance to confirm the "no curse" pattern
  3. OCR ablation: For a subset of languages, compare (a) no OCR data, (b) English-only OCR data, (c) multilingual OCR data with frozen image encoder, (d) multilingual OCR with unfrozen encoder; measure SMPQA Ground and Read metrics

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does scaling synthetic text-in-image training data by orders of magnitude specifically close the performance gap between Latin and non-Latin scripts in Large Vision-Language Models?
- **Basis in paper:** The authors observe a persistent performance gap for non-Latin scripts and state, "We hypothesize that orders of magnitude more text-in-image training data for other scripts are required for adequate performance."
- **Why unresolved:** The study was limited to relatively small amounts of synthetic OCR data (5k-10k samples per language), which was sufficient for Latin scripts but insufficient for others, leaving the upper bound of this approach untested.
- **What evidence would resolve it:** An ablation study training models on synthetic datasets scaled significantly larger (e.g., 100k-500k samples) specifically for non-Latin scripts, measuring if performance converges with Latin-script baselines.

### Open Question 2
- **Question:** To what extent does the "translationese" quality of machine-translated training data negatively impact model performance compared to native human-curated data, particularly for low-resource languages?
- **Basis in paper:** The authors list "Using Machine-Translated Training Data" as a limitation, noting, "Future work should consider evaluation setups to quantify the effect the MT data has on the final model."
- **Why unresolved:** The experimental setup relied entirely on machine translation (NLLB) to ensure comparability across 100 languages, making it impossible to isolate the effect of translation artifacts on the final model quality.
- **What evidence would resolve it:** A comparative study where identical training mixes are compared using machine translation versus high-quality human translations for specific low-resource languages (T1/T2).

### Open Question 3
- **Question:** What are the underlying mechanisms driving the observed "positive spillover," where increasing the number of training languages improves language fidelity for languages excluded from the training mix?
- **Basis in paper:** The authors note a surprising result: "Interestingly, the more multilingual the training, the larger the fidelity gains also for languages not included in training," but do not explain this phenomenon.
- **Why unresolved:** This suggests a form of cross-lingual transfer or general multilingual capability that contradicts the assumption that language skills are learned in isolation (the "curse of multilinguality").
- **What evidence would resolve it:** Mechanistic interpretability analysis (e.g., probing classifiers or attention head analysis) to determine if the model learns a "multilingual" state separate from specific language representations that facilitates zero-shot fidelity.

## Limitations

- The paper relies heavily on machine-translated instruction data and synthetic OCR data, with the quality gap between synthetic and native data remaining unmeasured, particularly for low-resource languages.
- The findings about optimal English proportions (25-50%) and language scaling may be specific to LoRA fine-tuning with fixed hyperparameters, and may not generalize to full fine-tuning or different model sizes.
- The evaluation uses academic benchmarks that may not reflect real-world multilingual vision-language use cases, and the new SMPQA benchmark covers only 20 languages and synthetic scenarios.

## Confidence

**High Confidence:** The core finding that English performance can be maintained with 25-50% non-English instruction-tuning data.

**Medium Confidence:** The claim about optimal pre-training distribution (50% English) improving multilingual performance.

**Medium Confidence:** The necessity of multilingual OCR data and unfrozen image encoder for text-in-image understanding.

**Low Confidence:** The scalability claim that 100 languages can be trained simultaneously without degradation.

## Next Checks

1. **Native Data Validation:** Re-run the instruction-tuning experiments (25-50% English) using native human-written data for 5-10 languages instead of machine translation to quantify the synthetic data quality gap and verify the mechanism holds with higher-quality inputs.

2. **Real OCR Data Test:** Evaluate Centurio on real-world multilingual text-in-image datasets (e.g., receipts, street signs, product packaging) to verify that synthetic OCR training generalizes beyond the controlled SMPQA benchmark scenarios.

3. **Architecture Scaling Study:** Repeat the 100-language training experiment with different model sizes (1B, 7B, 13B parameters) and LoRA configurations to determine whether the "no curse of multilinguality" finding is model-size dependent and identify potential upper bounds.