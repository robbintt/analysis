---
ver: rpa2
title: Surrogate Interpretable Graph for Random Decision Forests
arxiv_id: '2506.01988'
source_url: https://arxiv.org/abs/2506.01988
tags:
- feature
- features
- graph
- interaction
- indicates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpreting complex random
  forest models in high-stakes domains like healthcare, where increasing numbers of
  features and estimators can hinder understanding of global feature interactions.
  The authors propose a Surrogate Interpretable Graph (SIG) framework that extracts
  decision rules from random forests, clusters them, and constructs a weighted directed
  graph to visualize and analyze feature interactions.
---

# Surrogate Interpretable Graph for Random Decision Forests

## Quick Facts
- arXiv ID: 2506.01988
- Source URL: https://arxiv.org/abs/2506.01988
- Reference count: 40
- Primary result: SIG extracts decision rules from RFs, clusters them, and optimizes a weighted graph to visualize global feature interactions, scaling better than TreeSHAP in high-dimensional settings

## Executive Summary
This paper addresses the challenge of interpreting complex random forest models in high-stakes domains like healthcare, where increasing numbers of features and estimators can hinder understanding of global feature interactions. The authors propose a Surrogate Interpretable Graph (SIG) framework that extracts decision rules from random forests, clusters them, and constructs a weighted directed graph to visualize and analyze feature interactions. They optimize this graph using Mixed-Integer Linear Programming to highlight the most important relationships. Results on four datasets show that SIG provides clear, global explanations of feature interactions and scales better than TreeSHAP in high-dimensional settings, maintaining interpretability where TreeSHAP becomes computationally expensive.

## Method Summary
The SIG framework extracts decision rules from individual trees in a random forest via depth-first traversal, converting implicit tree structures into explicit IF-THEN rules. These rules are tokenized, TF-IDF encoded, and clustered using agglomerative clustering to reduce redundancy while preserving semantically similar decision logic. A weighted directed graph is constructed from clustered rules, where edges represent ordered feature pairs weighted by co-occurrence frequency. Mixed-Integer Linear Programming (MILP) is then applied to optimize graph sparsity while preserving path consistency and acyclicity, producing a minimal yet interpretable visualization of dominant feature interactions. The approach is validated on five tabular datasets with varying feature dimensions and sample sizes.

## Key Results
- SIG provides clear, global explanations of feature interactions through graph visualization and detailed interaction tables
- Scales better than TreeSHAP in high-dimensional settings, maintaining interpretability where TreeSHAP becomes computationally expensive
- Enhances model transparency and user confidence by offering an intuitive, graph-based visualization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decision rule extraction from random forests preserves the hierarchical feature interactions used by individual trees.
- **Mechanism:** Depth-first traversal with explicit stack visits each node in every tree of the forest. At each internal node, the algorithm records the feature and threshold condition, appending it to a partial rule path. Upon reaching a leaf, the complete rule (conjunction of conditions → class prediction) is stored. This converts the implicit tree structure into explicit IF-THEN rules.
- **Core assumption:** The decision paths within individual trees encode meaningful feature interactions that reflect the model's true decision logic.
- **Evidence anchors:**
  - [abstract]: "It uses graphs and mixed integer linear programming to analyze and visualize feature interactions."
  - [section 3.1]: "Extracting the decision rules from the individual trees allows us to capture the local decision-making."
  - [corpus]: Weak direct evidence for rule extraction specifically; related work "Unity Forests" notes RFs may not reliably capture interactions when covariates lack marginal effects.
- **Break condition:** If trees are very deep or numerous, extracted rules become prohibitively numerous and redundant, overwhelming downstream clustering.

### Mechanism 2
- **Claim:** TF-IDF encoding followed by agglomerative clustering reduces rule redundancy while preserving semantically similar decision logic.
- **Mechanism:** Each tokenized rule is treated as a "document" in a corpus. TF-IDF weights terms (feature-threshold tokens) by their frequency within a rule and inverse frequency across all rules. Rules with similar feature usage patterns have similar TF-IDF vectors. Agglomerative clustering (with cosine distance) merges similar rules into clusters, each represented by a centroid or prototypical rule. This compresses thousands of rules into a manageable set.
- **Core assumption:** Cosine similarity on TF-IDF vectors captures semantic similarity of decision logic.
- **Evidence anchors:**
  - [section 3.3]: "TF-IDF captures frequent patterns while downweighting rare ones."
  - [section 3.4]: "Clustering condenses these rules, preserving essential information while facilitating analysis."
  - [corpus]: No direct corpus evidence for this specific pipeline; "Forest-Guided Clustering" uses clustering differently (instance-based).
- **Break condition:** If rules have high variability in thresholds but share features, TF-IDF may underestimate their similarity, leading to excessive clusters.

### Mechanism 3
- **Claim:** Mixed-Integer Linear Programming (MILP) optimization on the rule-derived graph produces a sparse, interpretable visualization of dominant feature interactions.
- **Mechanism:** From clustered rules, ordered feature pairs (source → target) are extracted, and edges are weighted by co-occurrence frequency. MILP is formulated to minimize the number of selected edges subject to constraints: (1) path consistency from original rules, (2) acyclicity (DAG structure), and (3) connectivity constraints. Binary decision variables indicate edge inclusion. The solver identifies a minimal edge set that preserves the most salient hierarchical relationships.
- **Core assumption:** Edge frequency across rules correlates with decision importance.
- **Evidence anchors:**
  - [abstract]: "It uses graphs and mixed integer linear programming to analyze and visualize feature interactions."
  - [section 3.5]: "The goal is to preserve the connection with the most information."
  - [corpus]: "FIGNN" addresses interpretability in GNN surrogates via feature-specific masks but uses different optimization.
- **Break condition:** If the MILP solver is given an overly aggressive edge budget (k too small), the pruned graph may lose originating features (those with only outward arrows), resulting in "over-optimization" as noted in Appendix B.

## Foundational Learning

- **Concept: Random Forest Structure and Decision Paths**
  - **Why needed here:** The SIG method fundamentally operates on extracted tree paths. Without understanding how features split at nodes and form paths to leaves, you cannot interpret what the extracted rules represent.
  - **Quick check question:** Given a decision tree with depth 3, how many unique root-to-leaf paths exist in the worst case?

- **Concept: TF-IDF Vectorization for Non-Text Data**
  - **Why needed here:** The method repurposes a text retrieval technique for rule compression. Understanding how TF weights within-document frequency and IDF penalizes ubiquitous terms clarifies why certain features dominate clusters.
  - **Quick check question:** If a feature appears in every rule, will its TF-IDF score be high or low, and why?

- **Concept: Mixed-Integer Linear Programming Basics**
  - **Why needed here:** The graph sparsification step uses MILP with binary edge selection variables. Understanding constraint formulation (connectivity, acyclicity) and objective minimization is essential for debugging or extending the optimization.
  - **Quick check question:** In a MILP with binary variables x_ij for edge selection, what constraint ensures at least one edge crosses a cut separating node set S from its complement?

## Architecture Onboarding

- **Component map:** Rule Extractor -> Tokenizer/Encoder -> TF-IDF Vectorizer -> Agglomerative Clustering -> Graph Builder + MILP Optimizer
- **Critical path:** Rule extraction → TF-IDF → Clustering → Graph construction → MILP optimization. The clustering output directly determines which feature interactions enter the graph; errors here propagate to final interpretability.
- **Design tradeoffs:**
  - **Cluster count k:** Paper suggests k ≈ √(f + N) rounded to nearest perfect square. Lower k increases compression but may merge semantically distinct rules.
  - **MILP edge budget:** Controls sparsity. Too low → over-optimization (loss of originating features); too high → visual clutter.
  - **Distance metric:** Cosine similarity chosen over Euclidean for high-dimensional robustness (Section 3.4).
- **Failure signatures:**
  - **Over-optimization:** All remaining nodes have only inward arrows; no originating features identified (observed in Alzheimer dataset with k=15 edges, resolved at k=20).
  - **Excessive rules:** Rule count grows as O(T · L) where L ≈ 2^d. Deep forests with many trees produce millions of rules, degrading clustering performance.
  - **Threshold tokenization artifacts:** Close thresholds (e.g., 0.49 vs 0.50) treated as distinct tokens, fragmenting clusters.
- **First 3 experiments:**
  1. **Reproduce SIG on Heart dataset (14 features, 303 rows):** Train RF with ~15 estimators, extract rules, cluster with k=4, optimize graph with max_edges=15. Verify output matches Figure 4 structure and Table 8 interactions.
  2. **Ablate clustering step:** Compare SIG quality (edge semantic coherence, feature coverage) with and without clustering on Diabetes dataset. Quantify rule reduction ratio.
  3. **Stress test scalability:** On synthetic data, vary features (10→1000) and estimators (10→500). Measure wall-clock time for SIG vs TreeSHAP interaction computation. Plot crossover point where SIG becomes faster per Figure 7.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Surrogate Interpretable Graph (SIG) framework be effectively adapted for gradient boosting models like XGBoost while maintaining rule fidelity?
- **Basis in paper:** [explicit] The authors state, "The potential of boosting models such as XGBoost [54] for further investigation is promising."
- **Why unresolved:** The current methodology extracts rules from the parallel architecture of Random Forests; boosting relies on sequential error correction, which may alter how rules are aggregated and interpreted.
- **What evidence would resolve it:** Successful application of the SIG pipeline to XGBoost models, demonstrating that the extracted graph accurately reflects the sequential decision logic of boosting trees.

### Open Question 2
- **Question:** How does SIG visualization influence the speed and accuracy of domain expert decision-making compared to standard tools like TreeSHAP?
- **Basis in paper:** [explicit] The authors explicitly list "the lack of real-world domain expert user studies" as a limitation of the current work.
- **Why unresolved:** While the paper claims enhanced user confidence, it relies on logical argumentation rather than empirical human-subject testing to validate interpretability.
- **What evidence would resolve it:** Quantitative results from a controlled user study with clinicians or data scientists measuring task performance and subjective trust using SIG versus TreeSHAP.

### Open Question 3
- **Question:** How can local instance attributions (TreeSHAP) be mathematically integrated with SIG's global rule graph to provide cohort-level insights?
- **Basis in paper:** [explicit] The authors note, "In subsequent studies, the strengths of TreeSHAP's local attributions will be integrated with SIG's global rule graph."
- **Why unresolved:** The current framework provides a global view of feature interactions, but lacks the instance-specific granularity that methods like SHAP provide.
- **What evidence would resolve it:** A hybrid model or algorithm capable of mapping specific SHAP interaction values onto the nodes and edges of the global SIG structure.

## Limitations

- **Missing MILP details:** Exact MILP formulation for path consistency and DAG constraints is not provided, requiring assumptions during reproduction
- **Numerical threshold handling:** Treating threshold values as categorical tokens may fragment semantically similar rules (e.g., 0.49 vs 0.50)
- **No user studies:** Claims of enhanced interpretability lack empirical validation through domain expert testing

## Confidence

- **High confidence:** Rule extraction mechanism via depth-first traversal, TF-IDF encoding methodology, overall pipeline structure
- **Medium confidence:** MILP optimization effectiveness (limited evidence beyond qualitative visualization results), scalability claims vs TreeSHAP (only runtime comparison, no error analysis)
- **Low confidence:** Handling of numerical threshold proximity, robustness to different clustering hyperparameters, interpretability gains in real-world decision-making contexts

## Next Checks

1. Reproduce SIG on Heart dataset with n_estimators=15 and verify clustering produces expected feature interaction patterns matching published results
2. Compare MILP-optimized graph structure with and without clustering step on Diabetes dataset to quantify compression benefits and potential information loss
3. Benchmark SIG vs TreeSHAP on synthetic high-dimensional data (features 10→1000, estimators 10→500) to validate claimed scalability improvements and identify performance crossover points