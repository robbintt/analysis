---
ver: rpa2
title: 'From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability'
arxiv_id: '2511.12852'
source_url: https://arxiv.org/abs/2511.12852
tags:
- hidden
- input
- output
- network
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a control-theoretic framework for interpreting
  trained feedforward neural networks. The core idea is to treat a trained network
  as a nonlinear state-space system, where hidden activations form the state vector,
  and perform local linearization around a chosen operating point.
---

# From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability

## Quick Facts
- **arXiv ID**: 2511.12852
- **Source URL**: https://arxiv.org/abs/2511.12852
- **Reference count**: 20
- **Primary result**: Control-theoretic framework provides principled neuron importance scores through controllability, observability, and Hankel singular values computed from local linearizations of trained networks

## Executive Summary
This paper introduces a control-theoretic framework for interpreting trained feedforward neural networks by treating them as nonlinear state-space systems. The core insight is that local linearization around operating points enables classical control analysis tools - controllability and observability Gramians, and Hankel singular values - to quantify neuron importance. Experiments on small networks demonstrate that the framework identifies low-dimensional computational pathways, reveals how activation saturation reduces controllability, and provides principled rankings for neuron pruning and constraint selection. The method bridges black-box neural networks with white-box state-space analysis, offering a mathematically grounded approach to mechanistic interpretability.

## Method Summary
The framework treats hidden activations as a state vector and linearizes the network around an operating point to construct a local state-space model. Input-to-hidden and hidden-to-output Jacobians (B and C matrices) are computed through chain-rule composition of layer Jacobians. Controllability Gramian W_C = BB^T quantifies per-neuron sensitivity to input perturbations, while observability Gramian W_O = C^T C measures output influence. Hankel singular values derived from M = W_C W_O rank internal modes by joint input-output energy, with eigenvectors identifying pathway directions. Neuron importance is aggregated as Imp(j) = Σ σ_i^α v²_{i,j}, combining mode participation with singular value weighting.

## Key Results
- Framework successfully identifies dominant computational pathways in small networks (1-2-2-1 SwiGLU and 2-3-3-2 GELU)
- Controllability Gramians reveal which neurons are most sensitive to input perturbations at specific operating points
- Hankel singular values effectively rank internal modes, with dominant modes corresponding to high-importance neurons
- Activation saturation reduces controllability, shrinks dominant Hankel singular values, and shifts the dominant mode to different neurons

## Why This Works (Mechanism)

### Mechanism 1: Local Linearization Enables Classical Control Analysis
Treating hidden activations as a state vector and linearizing around operating points allows standard Gramian analysis to quantify neuron importance. For a network f_θ(x), Jacobians B = ∂h/∂x and C = ∂y/∂h define local input sensitivity and output influence. The core assumption is that the local linear approximation δh ≈ Bδx, δy ≈ Cδh remains informative near x* despite global nonlinearity.

### Mechanism 2: Controllability Gramian Quantifies Input-Excitability
Diagonal entries of W_C = BB^T measure per-neuron sensitivity to input perturbations. (W_C)_ii = ‖∂h_i/∂x‖² aggregates how strongly neuron i responds to all input directions. Large values indicate neurons easily "excited" by input changes; small values indicate locally dormant neurons. The core assumption is that input perturbations are the relevant notion of "excitation" for interpretability.

### Mechanism 3: Hankel Singular Values Identify Dominant Internal Pathways
Eigenvalues of M = W_C W_O rank internal modes by joint input-output energy, with eigenvectors identifying pathway directions. σ_i = √λ_i captures how strongly mode v_i participates in the input-output map. Large σ_i indicates modes both controllable (excited by input) and observable (influencing output). The neuron importance metric Imp(j) = Σ_i σ_i^α v²_{i,j} aggregates participation.

## Foundational Learning

- **State-space representations and linearization**: Why needed - framework treats neural networks as static state-space systems (δh = Bδx, δy = Cδh). Quick check - Given f(x) = σ(Wx + b) with σ = ReLU, what is the Jacobian at x* where all pre-activations are positive?
- **Controllability and observability Gramians**: Why needed - core insight maps classical control concepts to neuron importance. W_C and W_O quantify how states couple to inputs and outputs. Quick check - For a 1D system h_{k+1} = ah_k + bx_k, y_k = ch_h, what do the controllability and observability Gramians look like?
- **Hankel operators and balanced realizations**: Why needed - HSVs derive from M = W_C W_O and rank internal modes. Quick check - Why does balanced truncation preserve input-output behavior while reducing state dimension?

## Architecture Onboarding

- **Component map**: Forward pass → Jacobian module → Gramian module → Mode analysis module
- **Critical path**: Operating point selection → Forward pass → Activation derivatives D_ℓ → Chain-rule Jacobian composition → B and C matrices → Gramian construction → M eigendecomposition → HSVs and modes → Importance aggregation → Neuron ranking
- **Design tradeoffs**: Local vs. global (analysis is local to operating point), exact vs. approximate (full Gramians scale as O(n_h²)), activation choice (differentiable activations have well-defined derivatives)
- **Failure signatures**: Saturated activations (near-zero D_ℓ entries shrink Gramians), numerical instability (poorly conditioned eigenvalues), interpretation mismatch (high importance doesn't guarantee semantic interpretability)
- **First 3 experiments**: 1) Reproduce Example 1 (1-2-2-1 SwiGLU) with given weights at x* = 0.5; 2) Operating point sweep for Example 2 (2-3-3-2 GELU) at x* and x†; 3) Ablation test - zero out low-importance neurons and measure output change

## Open Questions the Paper Calls Out

### Open Question 1
Can low-rank factorizations, randomized sketching, or block-diagonal approximations of Gramians preserve interpretability insights for networks with millions of parameters? The paper only validates on networks with 4-6 hidden neurons; no scaling analysis or approximation error bounds are provided.

### Open Question 2
How should the framework be adapted to analyze transformer attention blocks, where the state includes queries, keys, values, and residual streams across token positions? Attention mechanisms have multi-head structure and cross-token interactions not captured by the current feedforward formulation.

### Open Question 3
Does pruning neurons with low importance scores Imp(j) preserve input-output behavior better than magnitude-based or gradient-based pruning baselines? The paper computes importance scores but conducts no ablation or pruning experiments to validate their predictive utility.

### Open Question 4
How does the local nature of the linear approximation affect the stability of neuron importance rankings across different operating points? The degree of operating-point sensitivity remains uncharacterized for comprehensive interpretability.

## Limitations
- Computational cost scales poorly with network width (O(n_h²) for Gramians), limiting practical application
- Local linearization assumption may break down for networks operating in highly nonlinear regimes
- Interpretability of Gramian-based importance scores remains theoretical - no guarantee these metrics align with human-understandable semantic features

## Confidence
- **High confidence**: Mathematical derivation of controllability/observability Gramians and Hankel singular values from linearized network model
- **Medium confidence**: Claim that diagonal entries of W_C measure "input-excitability" - while mathematically sound, practical interpretability remains unproven
- **Medium confidence**: Importance ranking through Imp(j) = Σ σ_i^α v²_{i,j} - aggregation method is principled but may not capture all relevant aspects

## Next Checks
1. **Robustness to operating point selection**: Systematically vary x* across input domain for Example 1 and plot how HSVs and Imp(j) rankings change
2. **Comparison to ablation studies**: For Example 2 at x†, zero out two highest-importance neurons and measure output degradation compared to random removal
3. **Scalability test with approximations**: Implement low-rank approximation of W_C and W_O, measure how well approximate HSVs and importance rankings match exact computation