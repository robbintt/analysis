---
ver: rpa2
title: Policy Gradient Guidance Enables Test Time Control
arxiv_id: '2510.02148'
source_url: https://arxiv.org/abs/2510.02148
tags:
- policy
- guidance
- diffusion
- gradient
- dropout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Policy Gradient Guidance (PGG) introduces a simple extension of
  classifier-free guidance from diffusion models to classical policy gradient methods
  like PPO. The method augments the policy gradient with an unconditional branch and
  interpolates conditional and unconditional branches, yielding a test-time control
  knob that modulates behavior without retraining.
---

# Policy Gradient Guidance Enables Test Time Control

## Quick Facts
- **arXiv ID:** 2510.02148
- **Source URL:** https://arxiv.org/abs/2510.02148
- **Reference count:** 40
- **Primary result:** Policy Gradient Guidance (PGG) extends classifier-free guidance to PPO, enabling test-time controllability without retraining by interpolating conditional and unconditional policy branches.

## Executive Summary
Policy Gradient Guidance (PGG) introduces a simple extension of classifier-free guidance from diffusion models to classical policy gradient methods like PPO. The method augments the policy gradient with an unconditional branch and interpolates conditional and unconditional branches, yielding a test-time control knob that modulates behavior without retraining. The key innovation is a theoretical derivation showing that the additional normalization term vanishes under advantage estimation, leading to a clean guided policy gradient update. Empirically, PGG is evaluated on discrete and continuous control benchmarks, demonstrating that training with modest guidance strength (γ=1.1) improves sample efficiency and provides stable test-time controllability.

## Method Summary
PGG builds on PPO by adding a learnable null embedding that serves as an unconditional policy branch π_θ(a|∅). During training, the guided policy is constructed as ̂π_θ(a|s) ∝ π_θ(a|∅)^(1−γ)·π_θ(a|s)^γ, interpolating between unconditional and conditional branches with guidance strength γ. The method is evaluated with two training variants: conditioning dropout (p_drop=0.1) and no dropout with training γ=1.1. At test time, γ can be adjusted without retraining to modulate behavior. The theoretical contribution shows that the normalization term in the guided policy gradient cancels under unbiased advantage estimation, yielding a clean update rule.

## Key Results
- Training with guidance (γ=1.1) and applying larger γ at test time consistently improves sample efficiency and performance in discrete control tasks (CartPole, Acrobot).
- For continuous control tasks (HalfCheetah, Hopper, Humanoid, Walker2d, Pusher, InvertedPendulum), training with modestly larger γ improves stability and sample efficiency, but too large γ values (>1.3) destabilize training in complex tasks.
- Conditioning dropout, while effective in discrete tasks, destabilizes continuous control; training with γ>1 provides a more stable and effective mechanism than dropout, especially in continuous domains.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Interpolating conditional and unconditional policies with guidance strength γ creates a test-time controllable policy without retraining.
- **Mechanism:** The guided policy π̂_θ(a|s) ∝ π_θ(a)^(1-γ) · π_θ(a|s)^γ blends an unconditional action prior with the state-conditioned policy. For γ=1, this recovers standard PPO; γ>1 amplifies conditional signal; γ<1 increases unconditional influence.
- **Core assumption:** The unconditional branch π_θ(a|∅) provides a meaningful "default action prior" learned across all states. Assumption: this prior captures general action tendencies that complement state-specific decisions.
- **Evidence anchors:**
  - [abstract] "interpolates conditional and unconditional branches, yielding a test-time control knob that modulates behavior without retraining"
  - [Section 4, Eq. 4] Defines guided policy as multiplicative interpolation in probability space
  - [Section 8.1] Discusses interpretation of π_θ(a|∅) as unconditional action prior
  - [corpus] Related work on null-text embedding optimization for diffusion alignment suggests similar unconditional branch manipulation is effective
- **Break condition:** If the unconditional branch is poorly trained (e.g., insufficient gradient signal), the interpolation provides no meaningful guidance and may degrade performance.

### Mechanism 2
- **Claim:** The normalization term ∇_θ log Z_θ(s) vanishes under advantage estimation, yielding a clean guided policy gradient.
- **Mechanism:** When plugging the guided policy gradient into the policy gradient theorem, a state-dependent normalization term appears: E_s~π̂[∇_θ log Z_θ(s) · E_a~π̂[A]]. Since E_a~π̂[A] = 0 by definition of the advantage function (A = Q - V), the entire term cancels.
- **Core assumption:** The advantage estimator is unbiased and converged sufficiently. Assumption: critic training provides accurate value estimates.
- **Evidence anchors:**
  - [abstract] "theoretical derivation showing that the additional normalization term vanishes under advantage estimation"
  - [Section 4] Full derivation from Eq. 6 to Eq. 7 showing cancellation
  - [Section 8.1] "our derivation relies on the cancellation of the Z(s) term, which in turn depends on the advantage estimator being unbiased"
  - [corpus] No direct corpus evidence on this specific mechanism; related diffusion guidance papers do not address policy gradient parameter derivatives
- **Break condition:** Biased or unconverged advantage estimates introduce the normalization term, potentially causing training instability.

### Mechanism 3
- **Claim:** Training with γ>1 (rather than conditioning dropout) provides stable gradient signal to both branches and improves continuous control performance.
- **Mechanism:** With γ>1, the coefficient (1-γ)<0, so the unconditional branch receives negative weight on high-advantage actions—learning "what not to do." The conditional branch is amplified. This ensures both branches receive gradient signal without stochastic rollout perturbation from dropout.
- **Core assumption:** Negative weighting of the unconditional branch provides useful gradient signal without destabilizing training. Assumption: modest γ values (1.1–1.2) balance amplification and stability.
- **Evidence anchors:**
  - [Section 7] "Intuitively, the unconditional branch is trained to model what not to do (negative weight on high-advantage actions)"
  - [Section 6, Figure 2] Dropout destabilizes continuous control while discrete tasks benefit
  - [Section 7, Figure 4] Training with γ=1.1 improves performance across continuous tasks until γ>1.3 causes sharp drops
  - [corpus] Related diffusion guidance work uses dropout, but corpus does not address γ>1 training as alternative
- **Break condition:** Excessive γ (>1.3 in complex tasks) destabilizes training; performance drops sharply in Humanoid, Walker2d.

## Foundational Learning

- **Concept: Policy Gradient Theorem and Advantage Estimation**
  - Why needed here: Understanding why ∇_θ J(θ) = E[A ∇_θ log π_θ(a|s)] is essential to grasp how the normalization cancellation works and why the guided gradient remains valid.
  - Quick check question: If advantage A is always zero for all state-action pairs, what happens to the policy gradient?

- **Concept: Classifier-Free Guidance in Diffusion Models**
  - Why needed here: PGG directly analogizes CFG's interpolation formula; understanding ∇_x log p(x|y) = (1-γ)∇_x log p(x) + γ∇_x log p(x|y) clarifies what PGG adapts and where the analogy breaks (parameter vs. input derivatives).
  - Quick check question: In diffusion CFG, why does the normalization term disappear when taking gradients with respect to x but not with respect to θ in policies?

- **Concept: On-Policy RL and Distribution Shift**
  - Why needed here: Conditioning dropout perturbs the policy distribution during rollouts, potentially causing harmful distribution shift in continuous control; understanding this explains why dropout works in discrete but fails in continuous domains.
  - Quick check question: If dropout causes the policy to sample from a different distribution during training vs. evaluation, what problem does this create for on-policy advantage estimation?

## Architecture Onboarding

- **Component map:**
  Base PPO (actor π_θ(a|s), critic V_ψ(s)) -> Add learnable null embedding for unconditional branch π_θ(a|∅) -> Construct guided policy ̂π_θ(a|s) ∝ π_θ(a|∅)^(1−γ)·π_θ(a|s)^γ -> Sample actions from ̂π_θ

- **Critical path:**
  1. Initialize null embedding (single parameter)
  2. During rollout: construct guided policy, sample actions from ̂π_θ
  3. Compute advantages via GAE (critic must be well-trained)
  4. Update with PPO clipped objective using ̂π_θ in log-prob calculations
  5. At test time: adjust γ without retraining to modulate behavior

- **Design tradeoffs:**
  - γ during training: Higher γ (1.1–1.2) improves performance but γ>1.3 risks instability in complex tasks
  - Dropout vs. γ>1 training: Dropout (p_drop=0.1) works for discrete tasks; γ>1 training is more stable for continuous control
  - Compute overhead: γ≠1 requires evaluating both conditional and unconditional branches (~2x compute vs. vanilla PPO)

- **Failure signatures:**
  - Performance degrades monotonically with increasing γ → unconditional branch poorly trained; ensure adequate training signal
  - Sharp performance drop at γ>1.3 in complex tasks → guidance strength exceeds stability threshold; reduce γ
  - Unstable training with dropout in continuous control → switch to γ>1 training without dropout
  - No benefit from guidance → critic unconverged; normalization term not canceling properly

- **First 3 experiments:**
  1. Validate implementation on discrete CartPole: Train with γ=1.1, test with γ∈{1, 1.5, 2, 5, 10}. Expect monotonic improvement with higher γ, confirming basic functionality.
  2. Test dropout vs. γ>1 on HalfCheetah: Train two models (dropout 10% with γ=1 vs. no dropout with γ=1.1), compare stability and asymptotic performance. Expect γ>1 to outperform dropout.
  3. Sweep γ at test time on a continuous task: Train once with γ=1.1, evaluate with γ∈{1.0, 1.05, ..., 1.5} to verify test-time controllability without retraining. Identify optimal γ and stability threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive or scheduled γ strategies mitigate the instability observed in high-dimensional continuous control tasks when using large guidance values?
- **Basis in paper:** [explicit] The conclusion states: "Future work includes... developing adaptive or scheduled γ strategies to mitigate instability in high-dimensional tasks."
- **Why unresolved:** The paper shows that γ > 1.3 destabilizes complex tasks like Humanoid and Walker2d, but only evaluates fixed γ values throughout training.
- **What evidence would resolve it:** Experiments comparing fixed vs. scheduled (e.g., curriculum-based or reward-dependent) γ strategies on high-dimensional MuJoCo tasks, measuring whether gradual γ increase prevents collapse.

### Open Question 2
- **Question:** What is the proper interpretation and optimal learning strategy for the unconditional policy πθ(a|∅)?
- **Basis in paper:** [explicit] Section 8.1 asks: "What does πθ(a|∅) mean?" and suggests it represents "an unconditional action prior," but the conclusion proposes "training on a better unconditional policy such as behavior cloning" as future work.
- **Why unresolved:** The paper trains πθ(a|∅) implicitly via the guided gradient, but never evaluates whether alternative training objectives improve guidance quality.
- **What evidence would resolve it:** Ablation studies comparing the current implicit training vs. explicit behavior cloning or unsupervised pretraining of the unconditional branch, measuring downstream task performance and test-time controllability.

### Open Question 3
- **Question:** How robust is the vanishing normalization term assumption when advantage estimates are biased or the critic is undertrained?
- **Basis in paper:** [inferred] Section 8.1 warns: "our derivation relies on the cancellation of the Z(s) term, which in turn depends on the advantage estimator being unbiased. A biased or non-converged advantage might introduce the normalization term and cause instability."
- **Why unresolved:** The paper assumes well-trained critics but provides no empirical sensitivity analysis of guidance behavior under critic bias or early-training regimes.
- **What evidence would resolve it:** Controlled experiments measuring policy degradation when applying PGG with deliberately biased advantage estimates (e.g., early stopping the critic, adding noise to advantages) across multiple environments.

### Open Question 4
- **Question:** Can PGG generalize to richer conditioning signals such as goals, language instructions, or human preferences?
- **Basis in paper:** [explicit] Section 8.1 notes: "guidance could be conditioned on goals or auxiliary signals" and the conclusion lists "extending PGG to richer conditioning signals (e.g., goals, preferences)" as future work.
- **Why unresolved:** The current formulation only conditions on environment states; it is unknown whether the theoretical derivation holds or remains stable when y represents sparse goals or preference feedback.
- **What evidence would resolve it:** Experiments applying PGG to goal-conditioned RL benchmarks (e.g., FetchReach) or preference-based RL settings, evaluating whether γ-controlled extrapolation improves zero-shot generalization to novel goals.

## Limitations
- PGG's performance gains depend critically on the quality of the unconditional branch π_θ(a|∅); undertrained unconditional branches provide no meaningful benefit.
- The theoretical derivation relies on unbiased advantage estimation; critic bias or convergence issues could reintroduce the normalization term, potentially destabilizing training.
- The exact stability threshold for γ values in continuous control tasks likely depends on task-specific dynamics and critic quality, which the paper doesn't fully characterize.

## Confidence
- **High confidence:** The discrete control results (CartPole, Acrobot) showing monotonic improvement with increased γ are robust and align with CFG intuition.
- **Medium confidence:** The continuous control results showing training instability at γ>1.3 in complex tasks (Humanoid, Walker2d) are well-documented, but the exact stability threshold likely depends on task-specific dynamics and critic quality.
- **Medium confidence:** The claim that γ>1 training is more stable than conditioning dropout for continuous control is supported by the empirical comparison, but the paper doesn't explore the full space of dropout probabilities or other regularization alternatives.

## Next Checks
1. **Unconditional branch sensitivity analysis:** Systematically vary the initialization scale and learning rate of the null embedding across tasks to determine when PGG fails due to undertrained unconditional branches.
2. **Advantage estimator bias test:** Intentionally introduce critic bias (e.g., by limiting critic training steps) and measure how the normalization term cancellation breaks down, correlating this with training instability.
3. **Task complexity gradient:** Evaluate PGG on a continuous control task with intermediate complexity (e.g., Ant-v4) to better characterize the transition from stable (γ<1.3) to unstable (γ>1.3) guidance regimes.