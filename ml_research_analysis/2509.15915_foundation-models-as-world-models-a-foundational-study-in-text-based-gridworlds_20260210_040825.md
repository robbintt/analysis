---
ver: rpa2
title: 'Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds'
arxiv_id: '2509.15915'
source_url: https://arxiv.org/abs/2509.15915
tags:
- reward
- learning
- agents
- grid
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the use of foundation models (FMs) as world
  models or decision-making agents within the reinforcement learning framework. Two
  main strategies are investigated: Foundation World Models (FWMs), where FMs simulate
  environment dynamics for training traditional RL agents, and Foundation Agents (FAs),
  where FMs directly select actions.'
---

# Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds

## Quick Facts
- arXiv ID: 2509.15915
- Source URL: https://arxiv.org/abs/2509.15915
- Reference count: 30
- Foundation models can serve as world models to improve RL sample efficiency and as direct agents, with varying success across task types

## Executive Summary
This paper investigates the use of foundation models (FMs) as world models and agents within reinforcement learning (RL) frameworks. The authors explore two main strategies: Foundation World Models (FWMs) that simulate environment dynamics for training traditional RL agents, and Foundation Agents (FAs) that directly select actions. Experiments conducted in text-based grid-world environments reveal that larger FMs demonstrate superior simulation accuracy and decision-making capabilities. While FAs excel in simple, deterministic tasks, they struggle with stochastic settings. Conversely, FWMs enable substantial sample efficiency gains when combined with RL agents, even when the simulated distributions are imperfect. The findings highlight the strong potential of FMs to enhance RL, particularly in complex, partially observable environments.

## Method Summary
The study employs foundation models as either world models or decision-making agents in text-based grid-world environments. Two strategies are investigated: Foundation World Models (FWMs) simulate environment dynamics to train traditional RL agents, while Foundation Agents (FAs) directly select actions using the FM. The FMs are fine-tuned on environment interactions to learn transition dynamics and action-value relationships. Experiments compare different FM sizes and assess performance in deterministic and stochastic settings, measuring simulation accuracy and sample efficiency gains when combined with traditional RL algorithms.

## Key Results
- Larger foundation models exhibit superior simulation accuracy and decision-making capabilities in grid-world environments
- Foundation Agents perform excellently in simple, deterministic tasks but struggle with stochastic settings
- Foundation World Models enable substantial sample efficiency gains when combined with RL agents, even with imperfect simulated distributions

## Why This Works (Mechanism)
The effectiveness of foundation models as world models stems from their ability to learn and generalize complex environment dynamics from limited interactions. Large language models possess strong in-context learning capabilities that allow them to capture transition probabilities and reward structures in grid-world environments. When used as world models, FMs can generate synthetic trajectories that approximate the true environment distribution, providing RL agents with additional training data without requiring real environment interactions. This reduces sample complexity by exposing the RL agent to a broader range of states and transitions than would be feasible through direct interaction alone.

## Foundational Learning
- **Reinforcement Learning**: Required to understand how agents learn optimal policies through reward maximization and the sample efficiency challenges that FWMs aim to address
- **World Models**: Essential for grasping how environment dynamics can be learned and simulated to support decision-making and training
- **Foundation Models**: Critical for understanding the capabilities of large language models in few-shot learning and their potential to generalize across tasks
- **Text-based GridWorlds**: Necessary context for the experimental domain where states, actions, and rewards are represented as text
- **Transfer Learning**: Relevant for understanding how pre-trained FMs can be adapted to new environments with minimal fine-tuning
- **Distribution Matching**: Important for evaluating how well simulated trajectories from FWMs approximate true environment distributions

## Architecture Onboarding

**Component Map:**
Foundation Model (FM) -> Environment Simulator (FWMs) OR Action Selector (FAs) -> Reward Function -> RL Agent (for FWMs) OR Direct Policy (for FAs)

**Critical Path:**
For FWMs: FM simulation of transitions → RL agent training on synthetic data → policy optimization
For FAs: FM action selection → environment interaction → reward observation → policy refinement

**Design Tradeoffs:**
- FWMs trade computational cost of FM inference for reduced environment interactions and improved sample efficiency
- FAs sacrifice the potential accuracy of specialized RL algorithms for the flexibility of a single model approach
- Larger FMs provide better performance but increase computational requirements for fine-tuning and inference
- Text-based representation enables broad applicability but may limit expressiveness compared to structured state representations

**Failure Signatures:**
- FWMs: Simulated distributions that diverge from true environment dynamics, leading to policies that fail when deployed in the real environment
- FAs: Inability to handle stochasticity and partial observability, resulting in brittle performance when environment assumptions are violated
- Both: Computational bottlenecks during inference or fine-tuning that limit practical deployment

**3 First Experiments:**
1. Compare FWMs of different sizes (small, medium, large) on sample efficiency in deterministic grid-world environments
2. Evaluate FA performance across varying levels of stochasticity in the environment
3. Measure the distributional similarity between FM-generated trajectories and real environment interactions using KL divergence

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is currently confined to simplified grid-world settings, which may not capture real-world complexities
- Performance gap between Foundation Agents and Foundation World Models in stochastic environments suggests limitations with uncertainty and partial observability
- Scalability of approaches to more complex environments and tasks remains unproven
- Resource requirements for fine-tuning and inference with large foundation models could present practical barriers

## Confidence
- **High Confidence**: Foundation models can serve as world models to improve sample efficiency in RL training
- **Medium Confidence**: Foundation models struggle with stochastic environments as direct agents
- **Medium Confidence**: Foundation models have potential to enhance RL in partially observable environments

## Next Checks
1. Evaluate FWMs and FAs in partially observable grid-world environments with longer time horizons and more complex state representations to test scalability
2. Conduct ablation studies comparing different foundation model sizes and fine-tuning strategies to quantify the impact of model capacity on performance
3. Implement and test the approaches in a non-grid-world environment (e.g., a continuous control task) to assess generalizability beyond discrete, structured settings