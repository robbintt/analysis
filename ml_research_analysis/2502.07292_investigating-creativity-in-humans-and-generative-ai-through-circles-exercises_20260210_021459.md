---
ver: rpa2
title: Investigating Creativity in Humans and Generative AI Through Circles Exercises
arxiv_id: '2502.07292'
source_url: https://arxiv.org/abs/2502.07292
tags:
- creativity
- genai
- human
- design
- categories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the phenomenon of "narrow creativity" in
  both humans and generative AI (GenAI) using the Circles Exercise, a creativity test
  where participants generate as many creative drawings as possible using a sheet
  of 30 blank circles. The study quantitatively analyzes human creativity by categorizing
  drawings into object types (e.g., animals, daily objects, vehicles), artistic expression
  techniques (simple sketches, detailed illustrations), and approaches to material
  utilization (direct use, personification, abstraction).
---

# Investigating Creativity in Humans and Generative AI Through Circles Exercises

## Quick Facts
- arXiv ID: 2502.07292
- Source URL: https://arxiv.org/abs/2502.07292
- Reference count: 40
- Key outcome: Both humans and GenAI exhibit narrow creativity in the Circles Exercise, concentrating 70% of outputs in a limited number of categories, with CoT prompting improving reasoning but not substantially broadening creative scope.

## Executive Summary
This study investigates creativity patterns in humans and generative AI using the Circles Exercise, a constrained ideation task where participants generate drawings using 28-30 blank circles. Through systematic quantitative analysis, the research reveals that both humans and AI exhibit "narrow creativity" by concentrating their outputs in familiar categories rather than exploring diverse design spaces. The findings show that advanced prompting strategies like Chain-of-Thought improve reasoning structure but fail to overcome the fundamental tendency toward exploitation over exploration.

## Method Summary
The study employs the Circles Exercise where participants/GenAI generate drawings using a sheet of 28-30 blank circles. Human data consists of 3367 drawings from 224 graduate design students across four semesters. Outputs are manually coded into 10 object categories, 5 material utilization approaches, and 3 artistic expression techniques. Three GenAI prompting strategies are tested: zero-shot, few-shot with human examples, and Chain-of-Thought prompting. Quantitative metrics include number of used categories, frequent categories, and percentage of drawings in frequent categories. The same coding framework and metrics are applied to both human and AI outputs for direct comparison.

## Key Results
- Both humans and GenAI concentrate 70% of their drawings in a limited number of frequent categories, demonstrating narrow creativity through exploitation rather than exploration
- Chain-of-Thought prompting improves reasoning structure but does not substantially broaden creative scope, with 68% of approaches still relying on dominant patterns
- Few-shot prompting with human examples transfers human narrow creativity patterns to GenAI outputs, matching the 70% concentration in frequent categories
- Humans tend to generate familiar, high-frequency ideas with strong inclination toward exploitation rather than exploration of the design space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Both humans and GenAI exhibit narrow creativity through concentrated exploitation of familiar design space regions
- Mechanism: Cognitive fixation causes idea generation to cluster around high-frequency categories. Once a conceptual anchor is activated (e.g., "soccer ball"), subsequent ideas probabilistically correlate within that semantic neighborhood rather than exploring distant regions. GenAI mirrors this through statistical sampling from training distribution modes.
- Core assumption: Idea generation follows structured cognitive patterns rather than random exploration.
- Evidence anchors:
  - [abstract] "We observe that both humans and GenAI focus on limited subsets of the design space."
  - [section 4.2.1] "70% of the drawn objects (70%) fall into frequent categories... indicates that the creativity of individuals is strongly inclined towards a limited set of frequent categories."
  - [corpus] Related work "Understanding Design Fixation in Generative AI" confirms GenAI similarly experiences design fixation, limiting ability to explore beyond familiar solutions.

### Mechanism 2
- Claim: Chain-of-Thought prompting improves reasoning structure but does not substantially expand creative breadth
- Mechanism: CoT decomposes tasks into intermediate reasoning steps, enabling systematic traversal within existing conceptual boundaries. However, without explicit mechanisms to identify and steer toward underexplored regions, CoT amplifies depth within familiar categories rather than breadth across the design space.
- Core assumption: Reasoning decomposition operates on available conceptual vocabulary without generating novel conceptual primitives.
- Evidence anchors:
  - [abstract] "advanced prompting strategies, such as Chain-of-Thought (CoT) prompting, mitigate narrow creativity issues but still fall short of substantially broadening the creative scope"
  - [section 4.3.2] "CoT does not effectively overcome the bias toward relying on dominant patterns of material utilization... 68% of the approaches employed by GenAI under CoT prompting narrow to the most frequently used methods."
  - [corpus] Limited direct corpus evidence on CoT specifically for creative tasks; this mechanism relies primarily on paper findings.

### Mechanism 3
- Claim: Few-shot prompting with human examples transfers human narrow creativity patterns to GenAI outputs
- Mechanism: Providing human-generated examples conditions the model's sampling distribution toward the statistical properties of those examples. If examples cluster in familiar categories, the model learns to mimic that concentration rather than diversify beyond it.
- Core assumption: GenAI's in-context learning prioritizes pattern matching to provided examples over independent exploration.
- Evidence anchors:
  - [section 4.3.1] "compared to zero-shot prompting, few-shot prompting produces GenAI results that are more quantitatively aligned with human performance... implies that providing human examples may lead to a similar representation of narrow creativity in GenAI"
  - [section 4.3.1] "few-shot=70%" for frequent category concentration matches human 70%.
  - [corpus] Related work "When Teams Embrace AI" examines human collaboration strategies in generative prompting for creative tasks, suggesting prompt design significantly influences output diversity.

## Foundational Learning

- Concept: **Divergent Thinking (Exploration vs. Exploitation)**
  - Why needed here: The paper's entire analysis framework rests on quantifying the balance between exploring new categories and exploiting familiar ones. Without this distinction, the metrics (# of used categories, % of frequent categories) lack interpretability.
  - Quick check question: Given a participant who draws 10 circles across 8 categories but places 6 drawings in one category, are they demonstrating exploration or exploitation?

- Concept: **Design Space Representation**
  - Why needed here: The paper operationalizes creativity as movement through a categorical design space. Understanding how to construct and partition this space is prerequisite to replicating or extending the analysis.
  - Quick check question: If you were analyzing creativity in a different domain (e.g., storytelling), what would be equivalent "categories" to the drawn object types here?

- Concept: **Prompt Engineering Strategies (Zero-shot, Few-shot, CoT)**
  - Why needed here: The paper compares these strategies systematically. Without understanding their mechanisms, you cannot interpret why CoT improved reasoning but not creative breadth, or why few-shot aligned with human narrow creativity.
  - Quick check question: Why might providing examples to a model reduce rather than increase output diversity?

## Architecture Onboarding

- Component map:
  Circles Exercise Instrument -> Categorical Coding Framework -> Quantitative Metrics Pipeline -> GenAI Prompting Module -> Comparative Analysis Layer

- Critical path:
  1. Define coding scheme from initial data subset
  2. Apply manual coding with inter-rater reliability checks
  3. Compute per-participant metrics
  4. Aggregate statistics (mean, std) across conditions
  5. Compare distribution patterns across human and GenAI conditions

- Design tradeoffs:
  - Manual vs. automated coding: Manual coding ensures conceptual validity but limits scalability. Automated classification would enable larger datasets but risks missing nuanced categories.
  - Number of categories: 10 object categories provide manageable granularity but may collapse meaningfully distinct ideas. More granular taxonomies increase annotation burden.
  - Time constraint (3 minutes): Creates ecological validity for human creativity under pressure but introduces time-as-confound when comparing to GenAI (which generates near-instantly).

- Failure signatures:
  - Coding inconsistency: Inter-rater disagreements >15% suggest ill-defined category boundaries
  - Floor effects: If # used categories approaches total available categories, metric loses discriminative power
  - Prompt leakage: GenAI outputs that reference the prompt text or meta-commentary indicate prompt design issues
  - Distribution collapse: All conditions showing identical category distributions suggests the task itself is overly constraining

- First 3 experiments:
  1. Replicate with different base shapes: Replace circles with squares, triangles, or irregular shapes to test whether narrow creativity is shape-specific or general.
  2. Strategic few-shot selection: Curate few-shot examples spanning diverse categories (one from each of the 10 types) and compare to random-example few-shot; hypothesis: strategic selection reduces narrow creativity transfer.
  3. Add exploration-encouraging CoT variant: Augment CoT prompt with explicit instruction to "identify categories you have not yet used and generate ideas for those"; compare breadth metrics to standard CoT.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the phenomenon of narrow creativity manifest similarly in other generative domains, such as writing, product design, or music?
- **Basis in paper:** [explicit] Section 5.3 states the authors aim to "explore narrow creativity in a broader range of generative creative tasks, such as visual arts, writing, and product design."
- **Why unresolved:** The current study isolated its analysis to the visual "Circles Exercise" and did not test other modalities.
- **What evidence would resolve it:** Replicating the quantitative analysis of exploration vs. exploitation in text-based or 3D modeling creative tasks.

### Open Question 2
- **Question:** Can interactive system features, such as real-time evaluation agents, effectively direct users toward unexplored areas of the design space?
- **Basis in paper:** [explicit] Section 5.4 proposes that "incorporating evaluation agents could provide real-time feedback, prompting users to move towards unexplored design space."
- **Why unresolved:** The study found that current advanced prompting strategies (like Chain-of-Thought) are insufficient to substantially broaden creative scope.
- **What evidence would resolve it:** A user study comparing design space coverage between users interacting with a baseline GenAI versus one equipped with an evaluation agent.

### Open Question 3
- **Question:** What specific interaction mechanisms allow humans to identify and address narrow creativity in AI outputs more effectively than prompting alone?
- **Basis in paper:** [inferred] The Conclusion states that sophisticated prompting is "insufficient on its own" and calls for "innovative human-GenAI interaction mechanisms" to foster groundbreaking ideas.
- **Why unresolved:** The paper demonstrates that both humans and GenAI default to frequent categories, but does not test a solution beyond different prompting styles.
- **What evidence would resolve it:** Prototype testing of a collaborative interface where human guidance corrects AI "narrowness" in real-time, showing increased category diversity.

## Limitations

- Manual coding approach limits scalability and introduces potential rater bias despite inter-rater reliability checks
- Focus on graduate design students and single creativity task (Circles Exercise) raises generalizability questions
- Single GenAI platform (OpenAI) without model version specification makes exact replication difficult
- Time-constrained human generation (3 minutes) vs. instantaneous GenAI output may confound speed with creativity patterns

## Confidence

- High confidence in core observation that both humans and GenAI exhibit narrow creativity through concentrated exploitation of familiar categories, supported by robust statistical patterns (70% concentration) and multiple validation checks
- Medium confidence in Chain-of-Thought prompting mechanism, as evidence shows it improves reasoning structure but limited direct corpus evidence exists for creative tasks specifically
- Medium confidence in few-shot prompting findings, as the mechanism is well-understood but few-shot example selection details were underspecified

## Next Checks

1. Conduct inter-rater reliability assessment on a subset of coded drawings to quantify coding consistency and establish acceptable thresholds
2. Test the Circles Exercise with different base shapes (squares, triangles) to determine whether narrow creativity patterns are shape-specific or general across constrained ideation tasks
3. Implement a strategic few-shot prompting variant where examples are explicitly selected to span all 10 object categories, then compare narrow creativity metrics to random-example few-shot to validate the transfer hypothesis