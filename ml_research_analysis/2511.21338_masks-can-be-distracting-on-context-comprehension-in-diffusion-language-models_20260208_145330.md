---
ver: rpa2
title: 'Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models'
arxiv_id: '2511.21338'
source_url: https://arxiv.org/abs/2511.21338
tags:
- masks
- nanswer
- noptions
- context
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates context comprehension in masked diffusion
  language models (MDLMs) and uncovers two key limitations. First, despite their global
  denoising objective and bidirectional attention, MDLMs exhibit a strong locality
  bias, performing best when relevant information is closest to the masked question,
  regardless of its absolute position.
---

# Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models

## Quick Facts
- **arXiv ID**: 2511.21338
- **Source URL**: https://arxiv.org/abs/2511.21338
- **Reference count**: 40
- **Key outcome**: MDLMs show strong locality bias and mask tokens act as distracting attention sinks; mask-agnostic fine-tuning improves robustness

## Executive Summary
This paper investigates context comprehension in masked diffusion language models (MDLMs) and uncovers two key limitations. First, despite their global denoising objective and bidirectional attention, MDLMs exhibit a strong locality bias, performing best when relevant information is closest to the masked question, regardless of its absolute position. Second, appending extra mask tokens for generation significantly degrades performance, with masks acting as distractors that impair long-context processing.

To address the distracting effect of masks, the authors introduce a mask-agnostic loss function that enforces prediction invariance to the number of appended masks. Fine-tuning with this objective substantially improves robustness, particularly for base models, and reduces locality bias. The study reveals that MDLMs, while promising, suffer from position-sensitive performance and sensitivity to mask tokens—limitations not widely recognized in prior work.

## Method Summary
The study evaluates MDLMs (LLaDA-8B, Dream-7B) on a custom few-shot learning suite with 16 tasks, measuring accuracy under varying conditions: position of relevant information relative to masked question, and number of appended mask tokens. The mask-agnostic (MA) loss combines cross-entropy with total variation distance between predictions under different mask counts. LoRA fine-tuning is applied on OpenOrca subset with specific hyperparameters (α=0.1, β=1.0/10.0 depending on model). Gradient attribution analysis quantifies attention allocation to mask versus context tokens.

## Key Results
- MDLMs show strong locality bias: performance peaks when relevant examples are nearest to the masked question, regardless of absolute position
- Appending extra masks causes significant accuracy degradation (~23-27% drop), acting as attention distractors
- Mask-agnostic fine-tuning substantially improves robustness to mask count, particularly for base models
- Gradient attribution reveals masks receive disproportionate attention (~0.22-0.28) compared to context tokens (~0.005-0.05)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MDLMs prioritize information closest to the masked token due to training dynamics.
- Mechanism: The masked diffusion loss is scaled by 1/p (masking probability), placing greater weight on low-mask scenarios where nearby context is typically sufficient. This incentivizes local context reliance during denoising.
- Core assumption: The weighting scheme creates a locality prior analogous to next-token prediction in ARLMs.
- Evidence anchors:
  - [section 4.2] "The masked diffusion loss is scaled by 1/p... training places greater weight on cases where only a few tokens are masked—scenarios where nearby context is usually sufficient for prediction."
  - [figure 2] Performance is highest when relevant information is near the masked question, regardless of absolute position.
  - [corpus] Limited direct evidence; concurrent work (Rulli et al. 2025) identifies attention pattern differences in MDLMs but does not directly address locality weighting.
- Break condition: If models are trained with uniform masking ratios or explicit long-range supervision, locality bias should diminish.

### Mechanism 2
- Claim: Extra mask tokens act as attention distractors, impairing context processing.
- Mechanism: Appended masks receive disproportionate gradient attribution (see Table 1), suggesting MDLMs allocate excessive attention to mask tokens at the expense of relevant context. This is mask-specific, not a repetition effect.
- Core assumption: Masks are not neutral placeholders but actively compete for attention during denoising.
- Evidence anchors:
  - [section 5.3] Replacing extra masks with repeated dots causes minimal degradation (~3–10%) versus masks (~23–27%).
  - [table 1] Gradient attribution to mask tokens is ~0.22–0.28 vs. ~0.005–0.05 for non-mask tokens.
  - [corpus] No directly comparable findings; most related work focuses on decoding speed rather than context attention.
- Break condition: If attention mechanisms are explicitly regularized to down-weight mask tokens, distractor effects should reduce.

### Mechanism 3
- Claim: Mask-agnostic fine-tuning enforces prediction invariance to mask count.
- Mechanism: The MA loss combines cross-entropy (L_CE) with total variation distance (L_TV) between predictions under different mask configurations. This explicitly penalizes distributional divergence caused by varying mask counts.
- Core assumption: The correct prediction should not depend on the number of appended masks.
- Evidence anchors:
  - [section 6.1] Formally defined as L_MA = αL_CE + βL_TV, where L_TV aligns predictions across mask counts.
  - [figure 9] MA-trained models maintain stable accuracy as mask count increases; CE-only does not.
  - [corpus] No corpus papers propose similar invariance objectives for MDLMs.
- Break condition: If mask count is an informative signal for output length (e.g., summarization), this invariance assumption may be inappropriate.

## Foundational Learning
- **Concept**: Masked Diffusion Language Models (MDLMs)
  - Why needed here: The paper's claims are specific to MDLMs' denoising objective and mask tokens; understanding the baseline paradigm is essential.
  - Quick check question: How does the denoising process differ from autoregressive next-token prediction?

- **Concept**: Locality Bias / Recency Bias in LMs
  - Why needed here: The paper diagnoses a locality bias in MDLMs and compares it to known AR biases; context is required to interpret findings.
  - Quick check question: What is the "lost-in-the-middle" phenomenon in autoregressive models?

- **Concept**: Total Variation (TV) Distance
  - Why needed here: The mask-agnostic loss uses TV distance to enforce invariance; understanding this metric clarifies the regularization mechanism.
  - Quick check question: What does TV distance measure between two probability distributions?

## Architecture Onboarding
- **Component map**: Input (prompt + noised answer + appended masks) -> Bidirectional transformer with masked diffusion objective (scaled by 1/p) -> Loss (MA loss = α·L_CE + β·L_TV) -> Inference (greedy or multi-step unmasking)

- **Critical path**: Configure mask count at inference (must be reported for reproducibility) -> Monitor performance vs. mask count to detect distractor effects -> If degradation observed, apply MA fine-tuning or use multi-step unmasking

- **Design tradeoffs**: Single-step decoding (fast) vs. multi-step unmasking (recovers accuracy but adds latency) -> MA fine-tuning improves robustness but may slightly reduce peak performance (LLaDA-Instruct) -> Fixed mask count (simple) vs. invariance training (requires extra pipeline)

- **Failure signatures**: Sharp accuracy drop as mask count increases (inverse scaling) -> Strong position dependence: performance collapses when relevant info is far from mask -> High gradient attribution to mask tokens vs. context tokens

- **First 3 experiments**:
  1. Sweep mask count (2, 50, 100, 200) on a few-shot task and plot accuracy to quantify distractor sensitivity.
  2. Vary position of relevant examples (left/middle/right) with fixed mask count to measure locality bias.
  3. Fine-tune with MA loss (α=0.1, β=1.0 for base models) on a small instruction dataset and re-evaluate mask robustness.

## Open Questions the Paper Calls Out
- **Question**: Do uniform diffusion models (which apply noise evenly rather than using explicit masks) exhibit the same context comprehension limitations as masked diffusion models?
  - Basis in paper: [explicit] The Future Work section suggests examining uniform diffusion models (e.g., Lou et al., 2023) to clarify "whether the issues we identify are intrinsic to the diffusion paradigm or specific to masked variants."
  - Why unresolved: The study focuses exclusively on Masked Diffusion Language Models (MDLMs) like LLaDA and Dream. It remains unknown if the "distracting" nature of mask tokens is a universal trait of diffusion models or an artifact of the masked approach.
  - What evidence would resolve it: Evaluating uniform diffusion models on the proposed few-shot learning suite to measure locality bias and sensitivity to decoding configurations without the presence of explicit mask tokens.

- **Question**: Does the $1/p$ weighting of the masked diffusion loss cause the observed locality bias?
  - Basis in paper: [inferred] Section 4.2 hypothesizes that because the loss is scaled by $1/p$ (where $p$ is masking probability), training places greater weight on cases with few masks, encouraging reliance on nearby context. The Future Work section explicitly calls for analyzing "weighting schemes and noise schedules" to explain this bias.
  - Why unresolved: The paper establishes the existence of the locality bias but does not isolate the specific component of the training dynamics responsible for it.
  - What evidence would resolve it: Ablation studies modifying the loss weighting scheme (e.g., removing the $1/p$ scaling) to observe if the model learns to utilize distant context more effectively.

- **Question**: How do positional biases in MDLMs shift as input sequences approach the model's context window limit?
  - Basis in paper: [explicit] The Limitations section notes that while ARLMs exhibit shifting biases near context limits, "Whether a similar effect occurs in MDLMs remains unclear; we plan to investigate how context window length influences locality biases in future work."
  - Why unresolved: The current study evaluates context comprehension well within the training limits (2048 or 4096 tokens). It is unknown if the locality bias worsens, improves, or changes form as the sequence length reaches the architectural maximum.
  - What evidence would resolve it: Extending the analysis to tasks specifically designed to saturate the context window (e.g., varying needle positions in sequences approaching 4096+ tokens).

## Limitations
- Findings are demonstrated primarily on few-shot classification tasks with single-token answers; generalizability to open-ended generation remains unclear
- Architectural specificity: Results focus on MDLMs with masked diffusion objectives; relative contributions of denoising training vs. bidirectional attention are not fully disentangled
- Evaluation protocol sensitivity: Mask formatting affects baseline accuracy, indicating sensitivity to tokenization and prompt engineering

## Confidence
- **High Confidence**: Empirical demonstration of position-dependent performance and degradation with extra mask tokens in MDLMs; mask-agnostic fine-tuning objective clearly defined and shows measurable improvements
- **Medium Confidence**: Mechanistic explanation attributing locality bias to 1/p scaling is plausible but relies on inductive reasoning rather than direct ablation studies; mask distractor claim supported but not definitively proven as sole mechanism
- **Low Confidence**: Claims about relative severity of MDLM vs. ARLM locality bias based on indirect comparisons rather than head-to-head controlled experiments; assertion that MA fine-tuning "substantially improves" base models is qualified by less pronounced gains on instruction-tuned models

## Next Checks
1. **Controlled Locality Bias A/B Test**: Replicate position-sensitivity experiments (Section 4.2) on both MDLMs and ARLMs using identical few-shot tasks and prompt formats to quantify relative severity of locality bias.

2. **Gradient Attribution Ablation**: Compare gradient attribution and performance when appending masks versus repeated non-mask tokens or random context tokens to strengthen claim that masks are uniquely distracting.

3. **Fine-Tuning Generalization Probe**: After MA fine-tuning, evaluate models on held-out tasks not seen during fine-tuning, including open-ended generation or multi-token answer tasks, to assess generalization and detect unintended side effects.