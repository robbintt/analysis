---
ver: rpa2
title: Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete
  Grids
arxiv_id: '2506.02050'
source_url: https://arxiv.org/abs/2506.02050
tags:
- state
- abstraction
- policy
- learning
- discrete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a decoupled hierarchical reinforcement learning
  framework with state abstraction (DcHRL-SA) to address the challenges of large-scale
  exploration spaces and partial observability in discrete grid environments. The
  method employs a dual-level architecture with a high-level RL-based actor and a
  low-level rule-based policy, combined with DeepMDP-based state abstraction to reduce
  state dimensionality.
---

# Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids

## Quick Facts
- **arXiv ID:** 2506.02050
- **Source URL:** https://arxiv.org/abs/2506.02050
- **Reference count:** 32
- **Primary result:** Decoupled HRL with state abstraction consistently outperforms PPO in discrete grid environments with sparse rewards and partial observability

## Executive Summary
This paper proposes a decoupled hierarchical reinforcement learning framework with state abstraction (DcHRL-SA) to address challenges of large-scale exploration spaces and partial observability in discrete grid environments. The method employs a dual-level architecture with a high-level RL-based actor and a low-level rule-based policy, combined with DeepMDP-based state abstraction to reduce state dimensionality. Experimental results in two customized discrete grid environments show that DcHRL-SA consistently outperforms PPO in terms of exploration efficiency, convergence speed, cumulative reward, and policy stability.

## Method Summary
The DcHRL-SA framework uses a dual-level architecture where a high-level PPO actor selects sub-goals from a predefined goal space, and a low-level rule-based policy executes these sub-goals as primitive action sequences. A DeepMDP-based state abstraction module maps discrete states to a lower-dimensional latent space, incorporating LSTM for POMDPs to encode historical observation-action sequences. An action mask mechanism filters invalid sub-goals based on environmental constraints. The framework is trained using joint optimization of the abstraction module (DeepMDP loss) and actor (PPO loss), with hyperparameters including lr=0.0001, γ=0.997, and batch sizes of 256-384.

## Key Results
- DcHRL-SA consistently outperforms PPO baseline in both DoorKey and Multi-Item Collection environments
- State abstraction reduces dimensions from 15×26 to 60 in DoorKey while maintaining performance
- Action masking improves exploration efficiency by preventing invalid sub-goal selection
- POMDP mode shows increased oscillation near convergence due to partial observability challenges

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling high-level RL actor from low-level rule-based policy improves exploration efficiency and convergence speed
- **Mechanism:** High-level actor selects sub-goals from predefined set while low-level rule-based policy executes them as primitive action sequences, creating temporal abstraction
- **Core assumption:** Low-level rule-based policy reliably executes sub-goals and sub-goals effectively cover necessary state transitions
- **Evidence anchors:** Significant improvement in final score and accelerated policy convergence compared to PPO baseline; hierarchical structures supported by corpus evidence
- **Break condition:** Performance gains disappear with poorly designed sub-goal space or unreliable low-level policy

### Mechanism 2
- **Claim:** DeepMDP-based state abstraction reduces dimensionality and improves training efficiency while preserving decision-relevant information
- **Mechanism:** Learned abstract mapping function clusters discrete states with similar dynamic behaviors into unified latent space; LSTM encodes observation-action sequences for POMDPs
- **Core assumption:** DeepMDP bisimulation metric effectively preserves critical information for downstream RL task
- **Evidence anchors:** State dimension reduction from 15×26 to 60 in DoorKey; abstraction utility supported by corpus evidence
- **Break condition:** Abstraction collapses into trivial representation with insufficient exploration

### Mechanism 3
- **Claim:** Action mask mechanism restricts high-level actor output to valid sub-goals, improving exploration efficiency
- **Mechanism:** Validity-checking function filters action space to valid subset based on environmental constraints
- **Core assumption:** Invalid actions can be reliably identified and removing them doesn't block optimal paths
- **Evidence anchors:** Improved exploration efficiency by avoiding ineffective exploration; limited corpus evidence for this specific mechanism
- **Break condition:** Mechanism fails if validity check is computationally expensive or overly restrictive

## Foundational Learning

- **Concept: Hierarchical Reinforcement Learning (HRL)**
  - **Why needed here:** To decompose complex, long-horizon tasks into simpler sub-problems, making exploration tractable
  - **Quick check question:** How does a high-level policy selecting sub-goals at lower temporal frequency aid an agent in a sparse-reward maze?

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - **Why needed here:** To understand why maintaining history of observations is crucial when current observation is insufficient
  - **Quick check question:** How does an agent's inability to see full map necessitate use of memory like LSTM?

- **Concept: State Abstraction / Representation Learning**
  - **Why needed here:** To grasp how mapping raw observations into lower-dimensional latent space makes learning more efficient
  - **Quick check question:** What is the core idea behind DeepMDP's bisimulation metric for creating useful abstract state?

## Architecture Onboarding

- **Component map:** Environment Observation -> State Abstraction (LSTM + Encoder) -> Latent State -> High-Level PPO Actor (with Action Masking) -> Valid Sub-Goal -> Low-Level Policy -> Environment Step

- **Critical path:** Joint optimization of abstraction (DeepMDP loss) and actor (PPO loss) through the sequential pipeline above

- **Design tradeoffs:**
  - Decoupled vs. Coupled HRL: Decoupling simplifies engineering but requires domain knowledge for low-level design
  - Rule-Based vs. Learned Low-Level: Guarantees valid sub-goal execution but lacks adaptability for novel situations
  - Abstraction Complexity: Larger latent dimension captures more information but risks overfitting and slower training

- **Failure signatures:**
  1. Abstraction Collapse: Loss is zero but RL performance is random, indicating trivial representation
  2. Premature Convergence: Actor converges to sub-optimal policy using few sub-goals, possibly due to aggressive masking
  3. POMDP Instability: Heavy oscillation in training curves, suggesting LSTM-based abstraction struggles to stabilize

- **First 3 experiments:**
  1. Baseline Reproduction: Run PPO vs. DcHRL vs. DcHRL-SA in MiniGrid-DoorKey-16x16, plot cumulative reward
  2. Ablation on State Abstraction: Train DcHRL-SA with abstraction module disabled to isolate its contribution
  3. Ablation on Action Masking: Train DcHRL-SA with action mask removed to measure impact on exploration efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can stability of DeepMDP abstraction function be improved in POMDPs to reduce oscillation during convergence?
- **Basis in paper:** Authors note that in POMDP mode, "incompleteness of information makes it difficult to converge to a stable abstraction function," causing increased oscillation in step curves near convergence
- **Why unresolved:** Current combination of LSTM-based history encoding and DeepMDP constraints struggles to maintain consistent latent representation under partial observability
- **What evidence would resolve it:** Successful application with smoother convergence curves or introduction of regularization term stabilizing abstraction mapping under partial observability

### Open Question 2
- **Question:** Why does DeepMDP mechanism yield limited improvements in final cumulative reward despite speeding up convergence?
- **Basis in paper:** Conclusion states that "introduction of DeepMDP abstraction mechanism results in limited improvement in final scores," distinguishing it from gains in convergence speed
- **Why unresolved:** Unclear if state dimensionality reduction discards critical information for optimal long-term decision-making or if abstraction learning objective conflicts with policy optimization
- **What evidence would resolve it:** Analysis of information retention within latent state space Z or modifications to loss function correlating with higher final rewards

### Open Question 3
- **Question:** Can decoupled framework maintain theoretical optimality and performance if discount factor γ is set to values strictly less than 1?
- **Basis in paper:** Theoretical proof preserving existence of optimal policy relies specifically on condition that discount factor γ = 1, whereas standard RL implementations use γ ≈ 0.997
- **Why unresolved:** Paper doesn't analyze how sensitive hierarchy's performance is to γ=1 assumption, which is often impractical for infinite horizon tasks requiring significant future discounting
- **What evidence would resolve it:** Sensitivity analysis showing policy performance degradation or stability as γ deviates from 1.0

## Limitations

- Low-level rule-based policy specifics (pathfinding algorithm and collision handling) are not fully specified, potentially affecting reproducibility
- DeepMDP abstraction network architecture details (hidden layers, LSTM dimensions, data augmentation) are referenced but not detailed, which could impact state compression effectiveness
- PPO actor/critic network layer sizes are unspecified, leaving training dynamics uncertain

## Confidence

- **High confidence:** The dual-level architecture's mechanism for improving exploration efficiency is well-supported by experimental results showing consistent performance gains over PPO
- **Medium confidence:** The effectiveness of DeepMDP-based state abstraction is supported by reduced state dimensions and comparable performance, but paper acknowledges risk of abstraction collapse with insufficient exploration
- **Medium confidence:** The action masking mechanism's contribution to exploration efficiency is described but lacks strong empirical validation, with no direct corpus evidence for this specific mechanism

## Next Checks

1. **Ablation on State Abstraction:** Train DcHRL-SA with abstraction module disabled or untrained to isolate its contribution to performance gains
2. **Ablation on Action Masking:** Train DcHRL-SA with action mask removed to measure its impact on exploration efficiency and policy convergence
3. **Low-Level Policy Verification:** Implement and test rule-based policy in isolation to verify its reliability in executing sub-goals, as its performance is critical to framework's success