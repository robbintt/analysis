---
ver: rpa2
title: Crosslingual Reasoning through Test-Time Scaling
arxiv_id: '2505.05408'
source_url: https://arxiv.org/abs/2505.05408
tags:
- reasoning
- language
- english
- languages
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the crosslingual reasoning capabilities
  of English-centric reasoning language models (RLMs) through test-time scaling. The
  authors find that increasing inference compute for RLMs improves multilingual mathematical
  reasoning performance across high-resource and low-resource languages, to the extent
  that smaller models can outperform models twice their size.
---

# Crosslingual Reasoning through Test-Time Scaling

## Quick Facts
- arXiv ID: 2505.05408
- Source URL: https://arxiv.org/abs/2505.05408
- Reference count: 40
- English-centric RLMs improve multilingual reasoning through test-time scaling, with smaller models outperforming larger ones

## Executive Summary
This paper investigates how English-centric reasoning language models perform on multilingual mathematical reasoning tasks when inference compute is scaled at test time. The authors find that increasing chain-of-thought length improves performance across high-resource and low-resource languages, but only for models with sufficient capacity (≥3B parameters). They observe a "quote-and-think" pattern where models quote non-English phrases and reason in English, and discover that forcing reasoning in high-resource languages yields better results than in low-resource ones. The study reveals that reasoning finetuning does not generalize well to out-of-domain tasks, particularly cultural commonsense knowledge.

## Method Summary
The study evaluates s1.1 models (1.5B, 3B, 7B, 14B, 32B parameters) finetuned on English reasoning samples from DeepSeek-R1, using budget forcing with truncation or extrapolation to scale chain-of-thought length. Evaluation is conducted on the MGSM benchmark across 11 languages using greedy decoding, comparing against Qwen2.5-Instruct baselines. Language forcing strategies include translated_wait tokens, prefix prompts, system prompts, and combined approaches. The study measures accuracy, language compliance, and inference FLOPs while analyzing performance across different language resource levels and domain generalization to tasks like Global-MMLU, FORK, and COPAL-ID.

## Key Results
- Test-time scaling improves multilingual mathematical reasoning for models ≥3B parameters, with smaller models outperforming larger ones through inference optimization
- RLMs exhibit "quote-and-think" behavior, quoting non-English phrases while reasoning in English
- Forcing reasoning in high-resource languages yields better performance and efficiency than in low-resource languages
- Benefits of reasoning finetuning do not generalize to out-of-domain tasks like cultural commonsense knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Crosslingual test-time scaling improves multilingual mathematical reasoning for models with sufficient capacity (≥3B parameters).
- Mechanism: Extending chain-of-thought generation length at inference time allows models to perform verification, backtracking, and self-correction on reasoning steps. The base multilingual capability (from pretraining) handles input parsing, while the reasoning capability (from English finetuning) handles solution derivation.
- Core assumption: Multilingual understanding and reasoning are partially separable—base models retain crosslingual comprehension even after English-only reasoning finetuning.
- Evidence anchors:
  - [abstract]

## Foundational Learning

### Concept 1: Chain-of-thought scaling
- Why needed: Enables iterative reasoning, verification, and self-correction during inference
- Quick check: Measure performance gains as maximum thinking tokens increase from 0.5k to 8k

### Concept 2: Language forcing strategies
- Why needed: Controls the language used for reasoning output to ensure crosslingual compliance
- Quick check: Compare accuracy and compliance across truncation, extrapolation, prefix, system prompt, and combined approaches

### Concept 3: Language resource dependency
- Why needed: Explains performance variance across high-resource vs low-resource languages
- Quick check: Correlate pretraining data language distribution with observed performance gains

### Concept 4: Capacity threshold effects
- Why needed: Identifies minimum model size required for test-time scaling benefits
- Quick check: Compare performance curves across 1.5B, 3B, 7B, 14B, and 32B parameter models

### Concept 5: Domain-specific generalization
- Why needed: Determines whether reasoning improvements transfer beyond mathematical tasks
- Quick check: Evaluate on STEM (MGSM, Global-MMLU) vs non-STEM (FORK, COPAL-ID) benchmarks

## Architecture Onboarding

### Component map
Qwen2.5-Instruct (base) -> English reasoning finetuning (s1) -> Test-time scaling (budget forcing) -> Language forcing (translated_wait/prefix/system) -> Output evaluation

### Critical path
Input → Multilingual parsing → English reasoning finetuning → Extended CoT generation → Answer extraction

### Design tradeoffs
- Truncation vs extrapolation: Simpler control vs natural termination handling
- Language forcing strength: Higher compliance vs potential accuracy degradation
- Model size: Larger capacity vs computational cost at inference

### Failure signatures
- <3B models: Minimal test-time scaling benefits regardless of strategy
- Weak forcing on LRLs: Low language compliance despite accuracy gains
- Non-STEM tasks: Overthinking without performance improvement

### First experiments
1. Run s1.1 models with budget forcing (0.5k-8k tokens) on MGSM across all 11 languages
2. Implement and compare four language forcing strategies on high-resource languages
3. Test cross-domain generalization on FORK and COPAL-ID benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multilingual reasoning training data be curated to overcome the Western-centric bias and poor translation quality that currently limit crosslingual generalization?
- Basis in paper: [explicit] The authors state that current translation technologies are "insufficient" due to Western-centric bias and poor low-resource language performance, calling for future work on multilingual augmentation techniques.
- Why unresolved: The paper demonstrates that English-centric reasoning generalizes well to high-resource languages but fails in low-resource and cultural contexts, and creating native reasoning data remains a bottleneck.
- What evidence would resolve it: A training dataset generated via novel multilingual augmentation that improves performance on the FORK or COPAL-ID benchmarks without requiring translation.

### Open Question 2
- Question: What is the relationship between pretraining data language distribution and crosslingual reasoning performance?
- Basis in paper: [implicit] The study observes strong performance disparities across high-resource vs low-resource languages without systematically characterizing the pretraining data composition for each target language.
- Why unresolved: The paper shows performance correlates with language resource availability but doesn't analyze whether this stems from reasoning capability vs language familiarity from pretraining.
- What evidence would resolve it: Detailed analysis correlating pretraining data statistics (token counts, quality metrics) for each target language with observed performance gains across different test-time scaling strategies.

## Limitations
- Crosslingual reasoning improvements are confined to mathematical reasoning tasks and do not generalize to cultural commonsense knowledge
- Performance exhibits strong resource disparity, with high-resource languages showing substantial gains while low-resource languages show minimal improvements
- A critical capacity threshold exists where models below 3B parameters show negligible improvement from test-time scaling

## Confidence

**High Confidence**: Test-time scaling improves multilingual mathematical reasoning for RLMs with ≥3B parameters, with smaller models outperforming larger ones through inference optimization.

**Medium Confidence**: English-centric RLMs can effectively reason about non-English inputs through test-time scaling, though the "quote-and-think" pattern suggests the mechanism may be more complex than pure crosslingual reasoning.

**Low Confidence**: The findings indicate robust crosslingual reasoning capabilities transferable to diverse domains, given the lack of generalization to out-of-domain tasks and strong dependence on language resource availability.

## Next Checks

1. Evaluate the same test-time scaling approach on non-mathematical reasoning tasks (e.g., commonsense reasoning, code generation, or logical inference) to determine whether benefits extend beyond the STEM domain.

2. Characterize pretraining data language distribution for each target language and correlate this with observed performance gains to disentangle effects of crosslingual reasoning capability versus language familiarity.

3. Implement and compare additional language forcing strategies (such as multilingual prefix tuning or adapter-based approaches) against evaluated methods to determine whether observed language compliance/accuracy trade-offs are optimal.