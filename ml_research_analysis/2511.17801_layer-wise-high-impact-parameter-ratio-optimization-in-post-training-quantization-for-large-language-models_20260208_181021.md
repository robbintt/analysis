---
ver: rpa2
title: Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization
  for Large Language Models
arxiv_id: '2511.17801'
source_url: https://arxiv.org/abs/2511.17801
tags:
- parameters
- quantization
- high-impact
- ratio
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of post-training quantization
  (PTQ) for large language models (LLMs), particularly the accuracy loss that occurs
  at extremely low bit-widths due to high-impact parameters. The authors propose a
  quadratic optimization framework that determines layer-specific ratios of high-impact
  parameters, rather than applying fixed ratios across all layers.
---

# Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models

## Quick Facts
- arXiv ID: 2511.17801
- Source URL: https://arxiv.org/abs/2511.17801
- Authors: Cuong Pham; Hoang Anh Dung; Cuong C. Nguyen; Trung Le; Gustavo Carneiro; Thanh-Toan Do
- Reference count: 6
- Primary result: Achieves 9.40 perplexity on WikiText-2 for LLaMA-2-7B with 2-bit quantization versus 37.37 for OmniQuant

## Executive Summary
This paper addresses the accuracy degradation problem in post-training quantization (PTQ) of large language models (LLMs) at extremely low bit-widths (2-3 bit). The authors propose a layer-wise optimization framework that determines optimal ratios of high-impact parameters to preserve using moderate precision, while quantizing remaining parameters to very low bit-widths. Their approach uses Fisher information to identify parameter sensitivity and quadratic optimization to allocate precision budgets across layers, outperforming state-of-the-art methods on both perplexity and downstream task accuracy metrics.

## Method Summary
The method introduces a hybrid quantization strategy that identifies high-impact parameters using Fisher information as a sensitivity metric. For each layer, it computes a matrix capturing quantization error interactions and solves a constrained quadratic optimization problem to determine the optimal ratio of high-impact parameters. High-impact parameters are quantized using advanced methods like AdaRound at moderate bit-widths (e.g., 4-bit), while remaining parameters use extremely low bit-widths (e.g., 2-bit) with efficient quantization. The approach is validated on LLaMA-2-7B and LLaMA-2-13B models using 128 calibration sequences from WikiText-2, showing significant improvements over baseline methods.

## Key Results
- Achieves 9.40 perplexity on WikiText-2 for LLaMA-2-7B with 2-bit weight quantization (versus 37.37 for OmniQuant)
- Reaches 7.77 perplexity on C4 dataset (versus 20.85 for OmniQuant)
- Improves zero-shot task accuracy to 49.04% average (versus 47.59% for OmniQuant) in W2A16g128 setting
- Outperforms OmniQuant across all tested bit-width configurations (W2, W3, W2A16g128)

## Why This Works (Mechanism)
The method works by recognizing that not all parameters contribute equally to model performance. By using Fisher information to identify high-impact parameters and allocating moderate precision only to these critical components, the approach preserves model accuracy while still achieving extreme compression. The layer-wise optimization ensures that precision allocation adapts to the varying importance of parameters across different layers, rather than applying uniform ratios that may be suboptimal.

## Foundational Learning
- **Fisher Information**: Measures parameter sensitivity to data; needed to rank parameter importance for hybrid quantization; quick check: verify channel-wise importance scores show meaningful variation
- **Quadratic Optimization**: Framework for finding optimal parameter ratio allocation; needed to solve the constrained optimization problem; quick check: validate solution satisfies constraint equations
- **Hybrid Quantization**: Combining different precision levels for different parameter subsets; needed to balance accuracy and compression; quick check: confirm high-impact channels receive higher precision
- **AdaRound**: Advanced quantization-aware training method; needed for fine-tuning high-impact parameters; quick check: verify AdaRound converges within specified iterations
- **Block Structure**: Layer grouping for computational efficiency; needed to limit matrix M computations; quick check: confirm block mapping matches LLaMA-2 architecture
- **Calibration Data**: Small dataset for sensitivity estimation; needed to compute Fisher information; quick check: verify 128 sequences provide stable importance rankings

## Architecture Onboarding
**Component Map**: Input Data -> Fisher Computation -> Matrix M Construction -> Optimization Solver -> Hybrid Quantization -> Evaluation

**Critical Path**: The most compute-intensive step is building matrix M, which captures layer-wise quantization error interactions. This requires computing reconstruction losses for all candidate ratios across layers within the same block, making it O(LÂ²) in the number of layers.

**Design Tradeoffs**: The method trades computational complexity in the optimization phase for better accuracy at inference time. While full pairwise computation across all layers would be prohibitively expensive, the block-wise approximation significantly reduces computation while maintaining accuracy.

**Failure Signatures**: 
- Perplexity scores significantly higher than reported (>10 on WikiText-2 suggests problems)
- Memory exhaustion during AdaRound indicates incorrect subset application
- Slow matrix M computation suggests missing block-wise optimization

**First Experiments**:
1. Verify Fisher information computation produces meaningful channel importance rankings by comparing top-k parameter preservation
2. Test matrix M construction with a small subset of layers to confirm block-wise approximation works
3. Validate the optimization solver finds feasible solutions that satisfy the ratio constraints

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Limited evaluation to only LLaMA-2 architecture without validation on other LLM architectures
- No hardware deployment metrics to verify claimed efficiency advantages of channel-wise quantization
- Sensitivity to calibration data characteristics not analyzed, leaving robustness questions unanswered

## Confidence
- High confidence: The mathematical framework and optimization problem formulation are clearly specified and reproducible
- Medium confidence: The overall methodology is sound, but critical implementation details are missing
- Low confidence: The exact implementation of Fisher computation and discrete optimization cannot be faithfully reproduced without additional information

## Next Checks
1. Verify the block structure mapping function block(l) for LLaMA-2 by checking layer connectivity patterns and confirming which layer pairs have non-zero off-diagonal elements in matrix M
2. Implement and benchmark multiple optimization solvers (greedy search, ILP, enumeration) for the constrained optimization problem to determine which yields the reported results
3. Validate the Fisher information computation by comparing per-channel importance rankings against baseline methods and ensuring the high-impact parameter identification is meaningful