---
ver: rpa2
title: Partially Rewriting a Transformer in Natural Language
arxiv_id: '2501.18838'
source_url: https://arxiv.org/abs/2501.18838
tags:
- language
- transcoder
- explanations
- latents
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of rewriting a neural network in
  a more interpretable form by replacing parts of the model with natural language
  explanations of its internal features. The authors approximate a feedforward network
  with a sparse transcoder, generate explanations for its latent features, and then
  use a language model to simulate the activations of these latents based on their
  explanations.
---

# Partially Rewriting a Transformer in Natural Language

## Quick Facts
- arXiv ID: 2501.18838
- Source URL: https://arxiv.org/abs/2501.18838
- Reference count: 13
- Primary result: Without calibration, replacing MLP latents with natural language explanations degrades performance to untrained model levels; quantile normalization helps but still underperforms zero ablation baseline.

## Executive Summary
This paper explores replacing parts of a neural network with natural language explanations of its internal features. The authors approximate a feedforward network with a sparse transcoder, generate explanations for its latent features, and use a language model to simulate the activations of these latents based on their explanations. They evaluate this approach by measuring the model's cross-entropy loss after partial replacement. The primary result shows that, without proper calibration, replacing even a small fraction of the latents with their natural language explanations degrades performance to the level of a model trained on only 10-15% of the data, performing no better than zeroing out the component entirely. However, with quantile normalization, the performance improves significantly, though it still does not surpass the zero ablation baseline. The paper highlights the need for more specific and detailed explanations to improve the effectiveness of this approach.

## Method Summary
The authors train a sparse transcoder to approximate the output of an MLP layer in Pythia 160M. The transcoder expands to a wider latent space (32,768 dimensions) with TopK sparsity (k=32) and includes a skip connection. Natural language explanations for each latent are generated using an automated pipeline. At inference, Llama 3 8B is prompted to predict whether each latent should activate given its explanation and context. These predictions are then quantile-normalized to match the empirical activation distribution before being used to replace the original MLP outputs.

## Key Results
- Without calibration, replacing MLP latents with natural language explanations degrades performance to untrained model levels (10-15% data training equivalent).
- Quantile normalization significantly improves performance but still underperforms the zero ablation baseline.
- Explanations with ~80% specificity result in too many false positives (~320 predicted active latents vs k=32), overwhelming the sparse prior.
- Top-scoring latents (by detection score) provide marginally better performance when replaced but do not close the gap to zero ablation.

## Why This Works (Mechanism)

### Mechanism 1: Sparse Transcoder Decomposition
- Claim: Replacing dense MLP layers with sparse transcoders yields interpretable features while approximately preserving function.
- Mechanism: A transcoder with TopK sparsity (k=32) projects MLP inputs to a wider latent space where individual dimensions fire rarely and (hypothesized) correspond to cleaner semantic features. A skip connection preserves information flow.
- Core assumption: Polysemantic neurons can be linearly decomposed into monosemantic latents.
- Evidence anchors:
  - [section 2] Transcoder trained to minimize MSE between its output and MLP output with TopK(k=32); skip connection added.
  - [section 3] Replacing one MLP with transcoder increases loss to ~25% training checkpoint—substantial but not catastrophic.
  - [corpus] Related work (Dunefsky et al., 2024, cited in paper) proposes sparse transcoders for interpretable circuits; limited external validation at scale.
- Break condition: If reconstruction error is too high or latents remain polysemantic, downstream simulation inherits compounded errors.

### Mechanism 2: LLM Simulation of Latent Activations
- Claim: An LLM can predict whether a latent activates given its natural language explanation and local context.
- Mechanism: For each latent, an explanation is generated (e.g., "male pronouns"). At inference, Llama 3 8B is prompted to score 0–9 how strongly the last token matches that explanation. The expected value becomes the raw predicted activation.
- Core assumption: Explanations capture sufficient specificity to distinguish active vs. inactive contexts.
- Evidence anchors:
  - [section 4.1] Observed specificity ~80% (vs. required ~99%+); predictor activates ~10× too many latents without normalization.
  - [section 4] Detection scores correlate with specificity/sensitivity; higher-scoring explanations perform better.
  - [corpus] Sparse autoencoder interpretability pipelines (Paulo et al., 2024, Templeton et al., 2024, cited) show mixed correlation between explanation scores and causal relevance; not yet validated as faithful replacements.
- Break condition: If explanations are underspecific (high false positives) or overspecific (low sensitivity), simulated activations diverge from ground truth regardless of prompt quality.

### Mechanism 3: Quantile Normalization for Calibration
- Claim: Mapping LLM predictions to the empirical activation distribution via quantile normalization partially recovers model performance.
- Mechanism: The simulator's raw predictions are miscalibrated (systematically over-predict high activations). Quantile normalization transforms predictions so their marginal distribution matches true activations computed over 10M tokens, pushing most predictions toward zero.
- Core assumption: The marginal distribution is the right target; calibration corrects systematic biases in the LLM's scoring.
- Evidence anchors:
  - [section 2.1, Figure 1] Without normalization, predictor over-predicts high activations by orders of magnitude; quantile normalization enforces sparsity prior.
  - [section 4, Figure 3] Normalized predictions significantly outperform unnormalized but still do not beat zero ablation baseline.
  - [corpus] No direct external corpus evidence on quantile normalization for SAE/transcoder simulation; this appears novel to the paper.
- Break condition: If sample size for estimating predicted-activation CDF is too small (e.g., 1K vs. 10K), mismatch between normalized and empirical distributions degrades performance (Figure A1, A2).

## Foundational Learning

- Concept: Sparse Autoencoders and Transcoders
  - Why needed here: These are the substrate being "rewritten." You must understand encoder/decoder structure, sparsity penalties (TopK), and reconstruction loss to reason about what's being approximated.
  - Quick check question: Given a 512-dim MLP hidden layer, what happens to representational capacity if you expand to 32,768 latents with TopK(k=32)?

- Concept: Calibration and Distribution Matching
  - Why needed here: The paper's main practical fix is quantile normalization. You need to grasp why marginal distribution alignment matters and how empirical CDFs are estimated.
  - Quick check question: If your predictor outputs scores in [0,1] but true activations are mostly 0 with rare spikes, what happens if you use raw scores without calibration?

- Concept: Specificity vs. Sensitivity Trade-offs
  - Why needed here: The core failure mode is low specificity (too many false positives). At 32,768 latents, 80% specificity still means ~6,500 spurious activations.
  - Quick check question: With k=32 active latents per token and 99% specificity, how many false positives do you expect on average?

## Architecture Onboarding

- Component map:
  Base model (Pythia 160M) -> Sparse transcoder (W1, TopK, W2, skip connection) -> Explanation generator -> LLM simulator (Llama 3 8B) -> Quantile normalizer -> TopK -> Decoded latents -> Model

- Critical path:
  1. Train transcoder to approximate MLP output (MSE loss, 8B tokens)
  2. Generate explanations for all 32,768 latents
  3. For each prompt, simulate each latent's activation (32,768 forward passes through Llama)
  4. Apply quantile normalization using precomputed CDFs
  5. Apply TopK, decode via W2 + W_skip, patch into model
  6. Measure cross-entropy loss delta

- Design tradeoffs:
  - Larger k improves transcoder approximation but increases simulation burden and false-positive surface area
  - More samples for predicted-activation CDF improve normalization fidelity but cost scales linearly
  - Selecting top-scoring latents for replacement helps marginally but does not close gap to zero ablation

- Failure signatures:
  - Loss equivalent to zero ablation: simulator is not meaningfully using explanations
  - Loss worse than zero ablation: likely normalization mismatch or sampling too few calibration points
  - High variance across prompts: explanations are context-sensitive in unpredictable ways

- First 3 experiments:
  1. Reproduce the transcoder training on a smaller model/layer; verify reconstruction error and TopK sparsity match paper.
  2. Implement quantile normalization with 10K vs. 1K sample sizes; confirm performance delta matches Figure A2.
  3. Replace only the top-100 highest-detection-score latents; plot loss vs. fraction replaced to validate whether high-score selection provides measurable gains over random selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can using contrast pairs of highly similar features generate natural language explanations with sufficient specificity and sensitivity to outperform the zero ablation baseline?
- Basis in paper: [explicit] The conclusion states, "new techniques are needed to make explanations more specific, for instance using contrast pairs of highly similar features to bring out additional details."
- Why unresolved: Current explanations result in low specificity (many false positives) or low sensitivity after quantile normalization, preventing the "rewritten" model from performing better than a zeroed-out component.
- What evidence would resolve it: An experiment where explanations are generated using contrastive methods, resulting in a cross-entropy loss lower than the zero ablation baseline when substituted into the model.

### Open Question 2
- Question: Do bias-corrected estimators for population quantiles improve the sample complexity and performance of the quantile normalization process?
- Basis in paper: [explicit] The authors note the bias in the empirical inverse CDF and state, "In a future draft of this paper we plan to experiment with bias-corrected estimators for the population quantiles."
- Why unresolved: The current quantile normalization requires large sample sizes (computationally expensive) to approximate the true activation distribution accurately; reducing sample complexity is necessary for scaling.
- What evidence would resolve it: Demonstrating that bias-corrected estimators achieve similar or better calibration performance than the current empirical method using significantly fewer prediction samples.

### Open Question 3
- Question: How does the inclusion of a linear skip connection in the transcoder architecture affect the trade-off between reconstruction fidelity and latent interpretability?
- Basis in paper: [explicit] Section 2 mentions the addition of a skip connection but notes, "We leave a deeper analysis of the skip connection for future work."
- Why unresolved: While the skip connection improves the transcoder's ability to approximate the original MLP, it is unknown if this addition degrades the monosemanticity or interpretability of the latent features.
- What evidence would resolve it: A comparative analysis of interpretability scores (e.g., detection scores) and reconstruction loss between transcoders trained with and without the skip connection.

## Limitations

- The core assumption that sparse transcoders decompose polysemantic neurons into monosemantic latents is not rigorously validated; latents may remain polysemantic or explanations may be post-hoc confabulations.
- The quantile normalization calibration is empirically effective but lacks theoretical grounding; it's unclear if marginal distribution alignment is optimal or if other targets would perform better.
- The LLM simulator's performance is heavily dependent on explanation quality; with ~80% specificity, the approach cannot reliably isolate active latents even with calibration.
- The evaluation is limited to a single model (Pythia 160M) and layer (layer 6 MLP); findings may not generalize to larger models, deeper layers, or different architectures.

## Confidence

- **High**: The experimental setup is clearly specified, and the core failure mode (uncalibrated predictions degrading to untrained model performance) is reproducible and well-documented.
- **Medium**: The effectiveness of quantile normalization as a calibration method is demonstrated but lacks theoretical justification and external validation. The specificity/sensitivity trade-off is well-observed but the path to improving explanations is unclear.
- **Low**: The claim that sparse transcoders decompose polysemantic neurons into monosemantic latents is supported by related work but not directly validated in this paper. The interpretability of explanations is asserted but not independently verified.

## Next Checks

1. **Transcoder Semantic Alignment**: Train the sparse transcoder and independently evaluate the monosemanticity of its latents using metrics from Dunefsky et al. (2024) or other SAE interpretability benchmarks. Compare the alignment between latents and their explanations beyond detection scores.
2. **Calibration Target Ablation**: Replace quantile normalization with alternative calibration methods (e.g., isotonic regression, conditional CDF alignment, or temperature scaling) and compare performance. This will test whether marginal distribution alignment is optimal.
3. **Explanation Quality Intervention**: Manually refine a subset of the lowest-scoring explanations to maximize specificity and sensitivity. Retrain the simulator with these improved explanations and measure the impact on loss and false-positive rates. This will validate whether the bottleneck is explanation quality rather than the simulation method itself.