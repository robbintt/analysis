---
ver: rpa2
title: 'Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language
  Models for Hidden Relation Extraction'
arxiv_id: '2506.18311'
source_url: https://arxiv.org/abs/2506.18311
tags:
- relation
- system
- entity
- entities
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction

## Quick Facts
- arXiv ID: 2506.18311
- Source URL: https://arxiv.org/abs/2506.18311
- Reference count: 14
- Key outcome: Zero-shot LLM prompting achieves 30% accuracy in extracting hidden cross-sentence biomedical relations from CORD-19 corpus

## Executive Summary
This paper proposes a zero-shot approach using large language models to extract hidden relations between biomedical entities that appear in separate sentences within COVID-19 research papers. The method identifies entity pairs lacking known relations, marks them with special tokens, and uses instruction-tuned LLMs to infer implicit connections. Manual validation shows Mixtral-8x7B-Instruct achieves 30% accuracy versus 12% for Flan-T5-11B, suggesting model scale and instruction quality significantly impact performance. The extracted relations aim to enhance the Covrelex-SE retrieval system by capturing cross-sentence contextual relationships missed by traditional parsers.

## Method Summary
The approach uses ScispaCy for entity extraction from CORD-19 documents, then filters entity pairs not appearing in the same sentence and lacking known relations in Covrelex-SE. These pairs are formatted with [E]…[/E] markers and submitted to zero-shot LLM prompts requesting JSON-formatted relation triples. Two models are evaluated: Flan-T5-11B and Mixtral-8x7B-Instruct. Manual validation by two annotators assesses generated relations for correctness, with Cohen's kappa measuring inter-annotator agreement. The system targets 50 entity pairs from 20 articles for initial evaluation.

## Key Results
- Mixtral-8x7B-Instruct achieves 30% accuracy in generating correct hidden relations versus 12% for Flan-T5-11B
- Cohen's kappa coefficient exceeds 0.6, indicating good inter-annotator agreement
- Most frequent error: generated relations do not contain the target entities
- Instruction quality and model scale significantly impact relation extraction performance

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot LLM prompting can extract cross-sentence relations that traditional parsers miss. Entity pairs in separate sentences are marked with [E] tokens and submitted to instruction-tuned LLMs with JSON prompts. The model infers implicit relationships from multi-sentence context windows. Performance depends on the LLM's domain knowledge encoding. Break condition: if entity pairs share no semantic connection within context, outputs become unreliable or hallucinated.

### Mechanism 2
Larger instruction-tuned models outperform smaller encoder-decoder models on biomedical relation extraction. Mixtral-8x7B-Instruct (56B parameters, decoder-only) achieves 30% accuracy versus Flan-T5 (11B, encoder-decoder) at 12%. The performance gap suggests model capacity and training data quality improve relation inference. Break condition: if tasks require precise entity boundary detection rather than semantic inference, encoder-decoder architectures may regain advantage.

### Mechanism 3
Manual validation with inter-annotator agreement provides a viable evaluation proxy without gold-standard labels. Two evaluators independently assess 50 LLM-generated relations; Cohen's kappa > 0.6 indicates acceptable agreement. This lends credibility to reported accuracy despite small sample size. Break condition: if evaluators interpret "hidden" differently, agreement may mask systematic bias rather than validate correctness.

## Foundational Learning

- **Relation Extraction (RE)**: Extracting structured (arg1, relation, arg2) triples from unstructured text. Quick check: Given "Aspirin reduces fever," can you identify the triple (Aspirin, reduces, fever)?

- **Zero-shot Prompting**: Extracting relations without task-specific training data, relying entirely on prompt design. Quick check: How would you frame a prompt to ask an LLM to extract relations without showing it examples?

- **Named Entity Recognition (NER) in Biomedical Domains**: ScispaCy extracts candidate entities before relation inference. Understanding NER limitations clarifies upstream error sources. Quick check: What challenges arise when detecting entity boundaries in phrases like "2019 new Coronavirus (n-Cov)"?

## Architecture Onboarding

- Component map: Entity Extraction -> Entity Pair Filtering -> Prompt Construction -> LLM Inference -> Manual Validation -> Database Insertion

- Critical path: Entity extraction → pair selection → prompt formatting → LLM inference → manual validation → database insertion. Errors in entity detection propagate through all downstream stages.

- Design tradeoffs:
  - **Precision vs Recall**: Filtering to cross-sentence pairs reduces noise but may miss intra-sentence hidden relations
  - **Model Size vs Inference Cost**: Mixtral outperforms Flan-T5 but requires significantly more compute
  - **Manual vs Automated Validation**: Human review ensures quality but doesn't scale; no automated filtering is proposed

- Failure signatures:
  - **Entity drift**: Generated relations reference entities not in the original pair
  - **Hallucinated relations**: LLM infers connections unsupported by context
  - **Format instability**: JSON parsing failures due to LLM output variability

- First 3 experiments:
  1. Replicate 50-pair evaluation on held-out CORD-19 subset to verify Mixtral's 30% accuracy
  2. Test intra-sentence vs cross-sentence pair accuracy to validate "hidden relation" assumption
  3. Introduce automated output filtering (JSON schema validation, entity matching) and measure impact

## Open Questions the Paper Calls Out

### Open Question 1
How can instability in LLM generated sequences be mitigated to ensure trustworthy performance? The conclusion states instability causes difficulty in achieving reliable performance and requires solving for practical deployment. Evidence needed: studies measuring output consistency across multiple runs or constrained decoding techniques yielding deterministic relation triples.

### Open Question 2
How can prompt engineering be optimized to ensure extracted relations strictly contain target entities? Section 4.3 identifies that generated relations frequently fail to contain target entities, listing this as an important future task. Evidence needed: modified prompting strategy or fine-tuning experiment achieving 100% containment of marked entities in output JSON objects.

### Open Question 3
Does inclusion of LLM-extracted hidden relations significantly improve downstream document retrieval performance compared to single-sentence extraction methods? While the abstract claims the goal is enhancing retrieval, experiments only evaluate relation extraction accuracy (30% success), not actual retrieval efficacy. Evidence needed: comparative evaluation of Covrelex-SE retrieval metrics using standard versus augmented databases.

## Limitations

- Manual validation approach with only 50 entity pairs limits generalizability of reported accuracies
- "Hidden relation" concept lacks objective benchmarks and clear definitions for annotators
- Entity extraction pipeline using ScispaCy introduces upstream uncertainty from biomedical NER limitations
- Performance gap between models may reflect architectural differences beyond instruction quality

## Confidence

- **High Confidence**: Zero-shot prompting mechanism with [E] token marking and JSON output requests; comparative performance between Mixtral and Flan-T5
- **Medium Confidence**: Claim that larger instruction-tuned models outperform smaller ones on biomedical RE; manual validation approach methodology
- **Low Confidence**: Broader applicability of "hidden relation extraction" as retrieval enhancement strategy without objective benchmarks or downstream retrieval evaluation

## Next Checks

1. Scale-up validation: Evaluate zero-shot prompting on 200+ entity pairs from diverse CORD-19 articles to assess whether Mixtral's 30% accuracy holds across broader biomedical topics

2. Downstream impact measurement: Measure actual retrieval performance improvements in Covrelex-SE by comparing search results with and without extracted hidden relations using standard IR metrics like NDCG or MRR

3. Automated validation framework: Develop pipeline combining JSON schema checking, entity matching verification, and contextual coherence scoring to reduce reliance on manual annotation and enable scalable evaluation