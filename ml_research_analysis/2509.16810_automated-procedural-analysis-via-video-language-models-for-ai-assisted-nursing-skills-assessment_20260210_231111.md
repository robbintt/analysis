---
ver: rpa2
title: Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing
  Skills Assessment
arxiv_id: '2509.16810'
source_url: https://arxiv.org/abs/2509.16810
tags:
- nursing
- temporal
- procedural
- training
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first video-language model (VLM)-based
  framework for automated nursing procedural assessment, addressing the challenge
  of subjective, time-intensive instructor evaluations in nursing education. The system
  follows a curriculum-inspired hierarchical progression from procedure recognition
  to fine-grained action segmentation and advanced error detection, enabling objective,
  interpretable feedback.
---

# Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment

## Quick Facts
- arXiv ID: 2509.16810
- Source URL: https://arxiv.org/abs/2509.16810
- Authors: Shen Chang; Dennis Liu; Renran Tian; Kristen L. Swartzell; Stacie L. Klingler; Amy M. Nagle; Nan Kong
- Reference count: 13
- Primary result: First VLM-based framework for automated nursing procedural assessment with 51.7% relative improvement in temporal segmentation and 55.4% relative improvement in missing action detection

## Executive Summary
This paper introduces the first video-language model (VLM)-based framework for automated nursing procedural assessment, addressing the challenge of subjective, time-intensive instructor evaluations in nursing education. The system follows a curriculum-inspired hierarchical progression from procedure recognition to fine-grained action segmentation and advanced error detection, enabling objective, interpretable feedback. Evaluated on nursing video datasets, the model achieves 31.4% accuracy in procedure identification, 51.7% relative improvement in temporal segmentation (F1: 0.352 at IoU ≥ 0.5), and 55.4% relative improvement in missing action detection (F1: 0.620). The approach supports scalable, standardized assessment across nursing skills, reducing instructor workload while maintaining evaluation quality.

## Method Summary
The framework employs a curriculum-inspired, multi-stage training approach using a Qwen2.5-VL-7B model with LoRA fine-tuning. It transforms video dynamics into static "Temporal Storyboards" with timestamp overlays, then progresses through four stages: procedure identification from untrimmed videos, dense captioning for fine-grained action segmentation, missing event prediction for error detection, and reordering event sequences for procedural reasoning. The system uses GPT-4o to generate ground-truth captions and synthetic error labels, with synthetically corrupted sequences (masking/shuffling) to train causal reasoning for error detection.

## Key Results
- 31.4% accuracy in procedure identification
- 51.7% relative improvement in temporal segmentation (F1: 0.352 at IoU ≥ 0.5)
- 55.4% relative improvement in missing action detection (F1: 0.620)
- 2.5× improvement over baseline models in detecting missing procedural steps
- Achieves precision-recall balance in error detection with F1 scores demonstrating practical utility

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A curriculum-inspired, multi-stage training progression (coarse-to-fine-to-reasoning) appears to improve temporal localization and error detection performance over single-stage baselines.
- **Mechanism:** The model first establishes global context (Procedure Identification) to ground its visual encoder. It then refines this boundary detection in Stage 2 (Dense Captioning), and finally applies logical constraints in Stages 3 & 4 (Reasoning). This prevents the model from attempting high-level causal reasoning before it has stable low-level visual features.
- **Core assumption:** Knowledge gained in earlier stages (recognition) positively transfers to later stages (reasoning) without catastrophic forgetting, which is managed by halving the learning rate at each stage.
- **Evidence anchors:** [abstract] "Mimicking human skill acquisition, the framework follows a curriculum-inspired progression... advancing from high-level action recognition... to procedural reasoning." [section: Experiments] "Stage 2’s primary role in enhancing temporal localization... best-performing configuration... achieves an F1 score of 0.352... surpassing the base model (0.232)."
- **Break condition:** If the domain shift between "correct" training videos and "error" testing videos is too large, the features learned in Stage 1 may not generalize to the corrupted sequences in Stage 3.

### Mechanism 2
- **Claim:** Transforming temporal video dynamics into static "Temporal Storyboards" with explicit timestamp overlays scaffolds the model's temporal reasoning capabilities.
- **Mechanism:** By sampling frames at 1 FPS and concatenating them into a single composite image with visible timestamps (SS:MS), the task shifts from implicit motion detection to explicit spatial-visual reasoning and OCR. This forces the model to attend to specific time indices when generating captions.
- **Core assumption:** The VLM possesses sufficient OCR capabilities to read the overlaid timestamps and correlate them with visual actions.
- **Evidence anchors:** [section: Methodology - Timestamp Overlay] "This approach transforms implicit temporal information into explicit visual cues... reframes aspects of the temporal localization challenge into a text-recognition task." [section: Experiments] "Timestamp overlays (-ts) yield mixed effects... base-s2-7b-ts improves slightly over base-s2-7b."
- **Break condition:** If actions occur faster than the 1 FPS sampling rate or require motion cues (e.g., assessing fluid dynamics or palpation force), the static storyboard will lose critical information.

### Mechanism 3
- **Claim:** Training on synthetically corrupted sequences (masking or shuffling steps) elicits causal reasoning capabilities for error detection.
- **Mechanism:** The model is forced to predict "missing" captions for masked frames or reorder shuffled sequences. This creates an internal representation of the "correct" procedure chain, allowing it to identify deviations (errors) rather than just memorizing visual patterns.
- **Core assumption:** The synthetic errors (random swaps/masks) accurately simulate the logical structure of real-world novice mistakes.
- **Evidence anchors:** [abstract] "...diagnosing errors by identifying missing or incorrect subactions... 55.4% relative improvement in missing action detection." [section: Methodology] "...missing event prediction... emulates human learners’ ability to reconstruct missing procedural steps through reasoning."
- **Break condition:** If the model hallucinates missing steps due to ambiguous visual context, precision drops (as seen in the high recall but lower precision for missing action detection).

## Foundational Learning

- **Concept: Temporal Intersection-over-Union (IoU)**
  - **Why needed here:** The primary evaluation metric (Table 1). You cannot interpret the "51.7% improvement" claim without understanding that IoU measures the overlap between the predicted time window and the ground truth.
  - **Quick check question:** If a model predicts an action lasts 10s-20s, but the ground truth is 12s-22s, what is the IoU? (Answer: Overlap is 8s (12-20), Union is 12s (10-22), IoU = 0.66).

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The implementation uses LoRA (rank 32) to fine-tune a 7B parameter model on limited GPU resources (A100). Understanding this is critical for reproducing the training setup.
  - **Quick check question:** Why does the paper target "all linear layers" with LoRA rather than just the attention weights? (To maximize domain adaptation capacity without full fine-tuning).

- **Concept: Hallucination in VLMs**
  - **Why needed here:** The paper notes the model sometimes produces "false positives" in error detection. This is a specific failure mode where the model invents actions or errors that aren't visually present.
  - **Quick check question:** In the "Missing Action Detection" task, does a high Recall (0.873) combined with lower Precision (0.480) suggest the model is missing errors or inventing them? (Inventing them).

## Architecture Onboarding

- **Component map:** Video frames (1 FPS) -> Temporal Storyboards with timestamp overlays -> Qwen2.5-VL-7B with LoRA -> GPT-4o generated captions and error labels -> Curriculum training (Stages 1-4)

- **Critical path:**
  1. **Data Curation:** Convert videos to "Temporal Storyboards" → Use GPT-4o to generate dense captions.
  2. **Synthesis:** Mask/Shuffle clips to create Stage 3 & 4 datasets.
  3. **Stage 1 Training:** Untrimmed videos, LR=1e-5 (Procedure recognition).
  4. **Stage 2 Training:** Trimmed clips, LR=5e-6 (Dense captioning).
  5. **Stage 3/4 Training:** Synthetically corrupted clips (Reasoning).

- **Design tradeoffs:**
  - **Static Storyboard vs. Native Video:** The 1 FPS storyboard simplifies temporal reasoning but loses high-frequency motion cues (e.g., tremors, rapid touch).
  - **Synthetic vs. Real Errors:** Training on synthetic data allows scalability but risks "sim-to-real" gap where the model fails on novel, real-world student errors not covered by random masking.
  - **7B vs. 3B Model:** The paper uses 7B for main results; the 3B model (Table 3) shows significantly worse temporal coverage, indicating size matters for this specific temporal reasoning task.

- **Failure signatures:**
  - **Drifting Boundaries:** The model correctly identifies the action but misaligns start/end times (low IoU despite correct classification).
  - **Over-sensitivity:** High recall but low precision in error detection (claiming a step is missing when it is just visually occluded or subtle).
  - **Loss of Early Knowledge:** If learning rates aren't decayed correctly between stages, the model may forget procedure names (Stage 1) while learning error detection (Stage 4).

- **First 3 experiments:**
  1. **Reproduce Stage 1 vs. Base:** Verify the 25.5% accuracy jump by training *only* Stage 1 on the untrimmed video set to validate the data pipeline.
  2. **Ablate Timestamp Overlays:** Run Stage 2 training with and without the "-ts" overlay to measure the specific delta in temporal localization (IoU) vs. caption quality.
  3. **Error Detection Generalization:** Train on the synthetic "Missing Event" dataset and test on a small held-out set of *real* student errors (if available) to probe the sim-to-real gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can temporal boundary detection be improved to support standalone assessment rather than a supplementary role?
- Basis in paper: [explicit] The conclusion states that "current temporal precision limitations suggest these systems may initially serve as supplementary assessment tools."
- Why unresolved: Low F1 scores at high IoU thresholds (e.g., F1@0.7 < 0.1) indicate the model struggles to pinpoint exact action start and end times.
- What evidence would resolve it: Architectural modifications yielding F1@0.7 > 0.5 on fine-grained segmentation of untrimmed real-world videos.

### Open Question 2
- Question: Does the model's performance on synthesized errors (masked/shuffled steps) transfer to natural, unstaged mistakes made by students?
- Basis in paper: [explicit] The abstract notes validation relied on "synthesized videos" to confirm potential for "real-world training variability."
- Why unresolved: Synthetic manipulations (e.g., blank frames) are visually distinct from the subtle, messy execution errors novices actually commit.
- What evidence would resolve it: High error detection rates (F1 > 0.6) on a held-out dataset of unscripted student performances.

### Open Question 3
- Question: Can the framework assess qualitative execution quality (e.g., aseptic technique) rather than just procedural sequence?
- Basis in paper: [inferred] The background highlights that AI struggles to evaluate "aseptic technique," while the current method focuses on step presence and ordering.
- Why unresolved: Outputs describe *what* action occurred and *when*, but lack metrics for *how well* the physical action was executed.
- What evidence would resolve it: Significant correlation between model-generated quality scores and expert human ratings on technical execution rubrics.

## Limitations

- **Data Quality Dependency:** Heavy reliance on GPT-4o for generating ground-truth captions and synthetic error labels introduces potential bias and may not accurately represent real-world nursing novice errors.
- **Motion Information Loss:** The 1 FPS sampling rate for temporal storyboards may miss critical high-frequency motion cues essential for nursing skills assessment, such as fluid dynamics, palpation force, or subtle hand tremors.
- **Generalization Across Clinical Domains:** While evaluated on nursing procedures, the framework's applicability to other clinical skills requiring different sensory modalities (e.g., auscultation, palpation) remains untested.

## Confidence

**High Confidence:** The multi-stage curriculum training approach (Stages 1-4) demonstrates clear performance improvements over single-stage baselines, particularly for temporal localization and error detection. The use of LoRA for efficient fine-tuning of large VLMs is a well-established technique with appropriate hyperparameter selection (rank 32, alpha 64).

**Medium Confidence:** The relative improvements in error detection (55.4% for missing actions) are significant but based on synthetic error data that may not reflect real-world nursing novice errors. The timestamp overlay mechanism shows mixed effects, with slight improvements in some configurations but no clear consistent benefit across all tasks.

**Low Confidence:** The claim that this is the "first" VLM-based framework for nursing procedural assessment lacks systematic literature review to confirm no prior work exists in this specific combination. The model's ability to detect subtle clinical errors (e.g., improper technique that doesn't omit steps but executes them incorrectly) is not demonstrated in the evaluation.

## Next Checks

1. **Real Error Generalization Test:** Train the model on the synthetic "Missing Event" dataset (Stage 3) and evaluate on a small held-out set of actual nursing student error videos (not synthetically generated) to quantify the sim-to-real gap in error detection performance.

2. **Motion Information Retention Analysis:** Systematically vary the frame sampling rate (e.g., 1 FPS, 5 FPS, 10 FPS) during temporal storyboard creation and measure the impact on temporal localization accuracy (IoU) and error detection performance to determine the minimum sampling rate that preserves critical motion information.

3. **Instructor Comparison Study:** Conduct a direct comparison between the model's error detection performance and experienced nursing instructors' assessments on the same video set, measuring inter-rater reliability and identifying systematic differences in error identification between human experts and the automated system.