---
ver: rpa2
title: A Self-Improving Architecture for Dynamic Safety in Large Language Models
arxiv_id: '2511.07645'
source_url: https://arxiv.org/abs/2511.07645
tags:
- safety
- architecture
- sisf
- static
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the Self-Improving Safety Framework (SISF),
  a novel software architecture enabling Large Language Models to autonomously adapt
  their safety protocols at runtime. SISF employs a dynamic feedback loop with an
  AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4
  Turbo) for generating new safety policies in response to failures.
---

# A Self-Improving Architecture for Dynamic Safety in Large Language Models

## Quick Facts
- arXiv ID: 2511.07645
- Source URL: https://arxiv.org/abs/2511.07645
- Reference count: 12
- Key outcome: Reduced ASR from 100% to 45.58% while maintaining 0.00% FPR on benign prompts

## Executive Summary
This paper introduces the Self-Improving Safety Framework (SISF), a novel software architecture enabling Large Language Models to autonomously adapt their safety protocols at runtime. SISF employs a dynamic feedback loop with an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) for generating new safety policies in response to failures. Tested on the AdvBench dataset using an unaligned Mistral-7B-v0.1 base model, SISF reduced the Attack Success Rate from 100% to 45.58% while maintaining a 0.00% False Positive Rate on benign prompts, demonstrating both effectiveness and practicality. The architecture shifts safety assurance from static, pre-deployment methods to an automated, runtime process, offering a viable path toward resilient, self-healing AI systems.

## Method Summary
The SISF architecture uses a container-based microservices approach with FastAPI REST endpoints. It employs Mistral-7B-v0.1 as the base model, GPT-4o as the Adjudicator, and GPT-4 Turbo as the Policy Synthesis Module. The system processes 520 adversarial prompts from AdvBench sequentially, starting with zero policies, and evaluates performance on 520 benign prompts from HH-RLHF. Policies are either HEURISTIC (regex-based) or EMBEDDING_SIMILARITY, applied in sequence: Block → Rewrite → Flag. The learning loop operates asynchronously to maintain low latency on the user path.

## Key Results
- Reduced Attack Success Rate from 100% to 45.58% on AdvBench dataset
- Achieved 0.00% False Positive Rate on benign prompts
- Synthesized 234 new policies (119 HEURISTIC, 115 EMBEDDING_SIMILARITY) achieving >1:1 blocking ratio
- Maintained p99 latency of 1.62 seconds for user-facing endpoint

## Why This Works (Mechanism)

### Mechanism 1: Asynchronous Learning Loop Separation
Separating the user-facing request path from the learning loop maintains low latency while enabling continuous adaptation. The synchronous path handles only local policy enforcement, while breach detection and policy synthesis happen asynchronously in the background. This architectural decoupling ensures user experience isn't degraded by expensive API calls to GPT-4o/GPT-4 Turbo. The tradeoff is an acceptable vulnerability window between first attack and policy deployment.

### Mechanism 2: Generalized Policy Synthesis from Specific Failures
An LLM-powered Policy Synthesis Module can generalize from individual breach instances to class-level defenses. The PSM analyzes failure context and generates either HEURISTIC or EMBEDDING_SIMILARITY policies, creating defenses against entire attack categories with >1:1 blocking ratio. The core assumption is that GPT-4 Turbo can reliably generalize without overfitting or creating overly broad filters.

### Mechanism 3: High-Fidelity Adjudication as Quality Gate
A highly accurate adjudicator (GPT-4o) prevents "policy poisoning" by ensuring only genuine breaches trigger policy synthesis. The Adjudicator evaluates (prompt, response) pairs with a "Zero-Tolerance" system prompt, achieving 0.00% FPR on benign data and 100% accuracy on adversarial validation. The assumption is that this performance generalizes to production workloads and diverse attack types.

## Foundational Learning

- **Concept: MAPE-K Feedback Loop (Monitor-Analyze-Plan-Execute-Knowledge)**
  - Why needed here: SISF is explicitly architected around this self-adaptive systems pattern—the Adjudicator monitors, PSM analyzes and plans, APS serves as Knowledge, and policy deployment executes.
  - Quick check question: Can you map each SISF component (Warden, Adjudicator, PSM, APS) to its corresponding MAPE-K phase?

- **Concept: Policy Enforcement Point (PEP) vs. Policy Decision Point (PDP)**
  - Why needed here: Understanding this XACML-derived separation clarifies why the Warden (PEP) applies policies while the Adjudicator (PDP) evaluates breaches—these are distinct responsibilities.
  - Quick check question: If the Adjudicator flags a breach, which component actually blocks the next similar request?

- **Concept: Attack Taxonomy (Syntactic vs. Semantic vs. Persona-based)**
  - Why needed here: The PSM synthesizes different policy types based on attack characteristics—HEURISTIC policies for syntactic patterns, EMBEDDING_SIMILARITY for semantic attacks.
  - Quick check question: Would a base64-encoded harmful request be better blocked by a HEURISTIC or EMBEDDING_SIMILARITY policy? What about a "Grandma exploit" semantic reframing?

## Architecture Onboarding

- **Component map:**
  - Warden LLM (Mistral-7B-v0.1) -> Adjudicator (GPT-4o) -> Policy Synthesis Module (GPT-4 Turbo) -> Adaptive Policy Store (in-memory Python singleton) -> Oversight Interface (Streamlit app)

- **Critical path:**
  1. User sends request to `/v1/chat` endpoint
  2. Warden queries APS for all active policies → applies in prioritized sequence (Block → Rewrite → Flag)
  3. If not blocked → forward to base Mistral-7B model → return response to user (sync path complete)
  4. Async: (prompt, response) enqueued to BackgroundQueue
  5. Adjudicator evaluates pair → if `is_breach=true`, triggers PSM
  6. PSM synthesizes new policy → `APS.add(policy, active=True)`
  7. Human operator reviews via Streamlit Oversight Interface → can toggle policies on/off

- **Design tradeoffs:**
  - Latency vs. Real-time Protection: 1.62s p99 sync latency vs. vulnerability window before policy deployment
  - Cost vs. Manual Red Teaming: $2.67 for 520-prompt evaluation vs. human red team costs
  - Autonomy vs. Control: Full auto-synthesis with HITL override capability through Oversight Interface
  - Generalization vs. Specificity: PSM may create overly narrow policies (inefficient) or overly broad ones (false positives)
  - Dependency Risk: Architecture relies on proprietary black-box models (GPT-4o, GPT-4 Turbo)

- **Failure signatures:**
  - Adjudicator drift: Spurious policies appearing in APS; check Oversight Interface for unexpected policy activations
  - Plateau effect: ASR stops improving despite continued policy synthesis; indicates "zero-day" attacks
  - Policy store growth: Unbounded policy accumulation could cause memory issues or slower enforcement
  - Async queue backlog: High attack velocity overwhelms background processing; vulnerability window expands
  - Upstream API changes: Sudden cost increases, latency spikes, or behavior changes in GPT-4o/Turbo

- **First 3 experiments:**
  1. Reproduce Adjudicator quality gate: Validate GPT-4o Adjudicator accuracy on a held-out adversarial subset before trusting its verdicts in production
  2. Run learning curve simulation: Process AdvBench sequentially starting with empty APS; verify ASR trajectory and policy type distribution matches reported ~50/50 HEURISTIC/EMBEDDING_SIMILARITY split
  3. Test FPR on benign data: Run mature SISF against HH-RLHF benign prompts; confirm 0.00% FPR or investigate any false positives to identify Adjudicator failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the SISF architecture outperform state-of-the-art static guardrails (e.g., Llama Guard) in final Attack Success Rate?
- Basis in paper: The authors state that a "direct comparison... was not performed" and that a "full comparative benchmark is the most critical next step for future work."
- Why unresolved: The study focused on validating the learning mechanism against a naive baseline rather than benchmarking against established defenses.
- What evidence would resolve it: A head-to-head evaluation of SISF versus Llama Guard on the AdvBench dataset.

### Open Question 2
- Question: Can the framework generalize its self-adaptive capabilities to other model families (e.g., Llama 3) and deceptive adversarial attacks?
- Basis in paper: The authors note results are limited to "a single base model... and one primary adversarial dataset" and suggest testing "other models" and "deceptive 'red-team attempts'".
- Why unresolved: The current validation is restricted to Mistral-7B-v0.1 and direct, non-deceptive attacks.
- What evidence would resolve it: Evaluation of the architecture on diverse models and complex, deceptive attack datasets like Anthropic/hh-rlhf.

### Open Question 3
- Question: Can the proprietary Adjudicator and Policy Synthesis modules be replaced by fine-tuned open-source models without performance loss?
- Basis in paper: The paper identifies a dependency on "proprietary, black-box models" and suggests future work explore "replacing these components with fine-tuned, open-source models."
- Why unresolved: Current implementation relies on GPT-4o and GPT-4 Turbo, which poses reproducibility and stability risks.
- What evidence would resolve it: An ablation study substituting API-based models with local, open-source alternatives while measuring policy synthesis quality.

## Limitations

- Reliance on proprietary black-box models (GPT-4o, GPT-4 Turbo) without ablation studies isolating SISF's contributions
- Perfect 0.00% FPR claim based on single benign dataset may not generalize to diverse production workloads
- Vulnerability window between attack and policy deployment not quantified for high-velocity attack scenarios

## Confidence

- **High Confidence**: The architectural separation of user path from learning loop is well-specified and the 1.62s p99 latency claim is directly supported by measurements. The MAPE-K pattern application to SISF is theoretically sound.
- **Medium Confidence**: The 45.58% ASR reduction and 0.00% FPR results are based on specific datasets (AdvBench, HH-RLHF) and may not generalize to all attack types or benign prompts. The 100% Adjudicator accuracy claim needs broader validation.
- **Low Confidence**: Claims about SISF's effectiveness against zero-day attacks or in production environments with high attack velocity are speculative, as the paper doesn't address these scenarios.

## Next Checks

1. **Adjudicator Robustness Test**: Validate GPT-4o Adjudicator accuracy on diverse adversarial subsets (including out-of-distribution attacks) before deployment, and monitor for performance drift over time.

2. **Learning Curve Validation**: Process AdvBench sequentially starting with empty APS to verify ASR trajectory matches reported results and examine policy type distribution patterns.

3. **Benign Data Generalization**: Test mature SISF against multiple benign datasets beyond HH-RLHF to confirm 0.00% FPR claim holds across different prompt distributions.