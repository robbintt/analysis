---
ver: rpa2
title: 'DualToken: Towards Unifying Visual Understanding and Generation with Dual
  Visual Vocabularies'
arxiv_id: '2503.14324'
source_url: https://arxiv.org/abs/2503.14324
tags:
- visual
- semantic
- arxiv
- understanding
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DualToken, a unified vision tokenizer for multimodal
  large language models (MLLMs) that addresses the conflict between visual understanding
  and generation objectives. Traditional vision tokenizers trained for reconstruction
  excel at generation but lack semantic capabilities, while contrastive learning-based
  encoders provide semantic understanding but struggle with generation.
---

# DualToken: Towards Unifying Visual Understanding and Generation with Dual Visual Vocabularies

## Quick Facts
- arXiv ID: 2503.14324
- Source URL: https://arxiv.org/abs/2503.14324
- Reference count: 40
- Key outcome: DualToken achieves state-of-the-art performance in both visual reconstruction and semantic understanding through hierarchical decoupling of objectives

## Executive Summary
DualToken introduces a unified vision tokenizer that addresses the fundamental conflict between visual understanding and generation objectives in multimodal large language models (MLLMs). Traditional vision tokenizers trained for reconstruction excel at generation but lack semantic capabilities, while contrastive learning-based encoders provide semantic understanding but struggle with generation. The key innovation is hierarchical decoupling of reconstruction and semantic objectives through separate codebooks: low-level perceptual features for reconstruction and high-level semantic features for understanding.

This approach transforms the inherent conflict into a synergistic relationship, enabling DualToken to achieve state-of-the-art performance in both reconstruction (rFID: 0.54, PSNR: 23.56, SSIM: 0.742 on ImageNet-1K validation) and semantic tasks (ImageNet zero-shot accuracy: 81.6%, T2I R@1: 21.05, I2T R@1: 21.55). The model surpasses dedicated models like VILA-U and SigLIP while demonstrating effectiveness in downstream MLLM understanding and generation tasks.

## Method Summary
DualToken proposes a hierarchical decoupling approach that separates visual features into two distinct codebooks based on their level of abstraction. The low-level codebook focuses on perceptual features optimized for reconstruction, while the high-level codebook captures semantic features for understanding tasks. This dual codebook architecture is trained jointly but with distinct objectives, allowing the model to leverage complementary strengths. The hierarchical structure ensures that reconstruction preserves fine-grained details while semantic understanding captures high-level concepts, creating a unified tokenizer that excels at both tasks simultaneously.

## Key Results
- State-of-the-art reconstruction performance: rFID: 0.54, PSNR: 23.56, SSIM: 0.742 on ImageNet-1K validation
- Superior semantic understanding: ImageNet zero-shot accuracy: 81.6%, T2I R@1: 21.05, I2T R@1: 21.55
- Outperforms dedicated models like VILA-U and SigLIP in both reconstruction and understanding tasks
- Effective in downstream MLLM understanding and generation tasks

## Why This Works (Mechanism)
The hierarchical decoupling mechanism works by separating visual features into two distinct levels of abstraction. Low-level features capture fine-grained perceptual details essential for accurate reconstruction, while high-level features encode semantic concepts necessary for understanding tasks. By training these codebooks with their respective objectives simultaneously, the model avoids the trade-off typically seen between reconstruction and understanding capabilities. This approach creates a synergistic relationship where each codebook specializes in its domain while contributing to a unified representation space that supports both generation and understanding tasks.

## Foundational Learning

**Vector Quantization** - A technique that maps continuous vectors to discrete codebook entries, enabling efficient compression and representation of visual features. Needed for converting continuous visual features into discrete tokens that can be processed by language models. Quick check: Verify that codebook size and update mechanism are appropriate for the task.

**Hierarchical Feature Decomposition** - The process of separating visual information into multiple levels of abstraction, from low-level perceptual details to high-level semantic concepts. Required to address the fundamental conflict between reconstruction and understanding objectives. Quick check: Ensure proper separation between perceptual and semantic feature spaces.

**Contrastive Learning** - A training approach that learns representations by contrasting positive and negative pairs, commonly used for semantic understanding. Essential for the high-level codebook to capture meaningful semantic relationships. Quick check: Validate that contrastive objectives align with semantic understanding goals.

**Perceptual Loss Functions** - Loss functions that measure similarity based on human visual perception rather than pixel-wise differences. Critical for the low-level codebook to preserve visual quality during reconstruction. Quick check: Confirm that perceptual metrics align with human judgment of image quality.

## Architecture Onboarding

Component map: Input image -> Feature extractor -> Low-level codebook (perceptual) + High-level codebook (semantic) -> Discrete tokens -> MLLM

Critical path: Image input → Multi-scale feature extraction → Hierarchical decomposition → Dual codebook quantization → Discrete token generation

Design tradeoffs: The dual codebook approach increases model complexity but provides superior performance across both tasks. The hierarchical structure adds computational overhead but enables specialization. The separation of objectives requires careful balancing during training to prevent one codebook from dominating the other.

Failure signatures: Poor reconstruction quality indicates issues with the low-level codebook's perceptual learning. Weak semantic understanding suggests problems with the high-level codebook's contrastive learning. Inconsistent token generation may indicate misalignment between the two codebooks' feature spaces.

First experiments:
1. Train each codebook independently to establish baseline performance for reconstruction and understanding
2. Evaluate reconstruction quality using PSNR, SSIM, and perceptual metrics
3. Test semantic understanding using zero-shot classification and retrieval benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to diverse datasets beyond ImageNet-1K remains unexplored
- Computational efficiency and memory usage during inference are not discussed
- Generalization to specialized domains like medical imaging or remote sensing is untested

## Confidence
- High: State-of-the-art performance claims supported by specific metrics (rFID, PSNR, SSIM, ImageNet accuracy, T2I/I2T retrieval)
- Medium: Synergy transformation claim has theoretical justification but limited empirical evidence
- Low: None identified

## Next Checks
1. Evaluate DualToken on diverse datasets (e.g., COCO, ADE20K, medical imaging datasets) to assess generalization capabilities across domains
2. Conduct ablation studies to quantify the individual contributions of the hierarchical decoupling mechanism versus other architectural components
3. Measure inference time and memory usage compared to baseline models to assess practical deployment considerations