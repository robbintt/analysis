---
ver: rpa2
title: 'Evaluating Machine Translation Models for English-Hindi Language Pairs: A
  Comparative Analysis'
arxiv_id: '2505.19604'
source_url: https://arxiv.org/abs/2505.19604
tags:
- translation
- machine
- evaluation
- metrics
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates machine translation models for English-Hindi\
  \ language pairs using an 18,000+ sentence parallel corpus and a domain-specific\
  \ FAQ dataset. Five metrics\u2014BLEU, WER, TER, BLEURT, and BERTScore\u2014are\
  \ used to assess translation quality."
---

# Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis

## Quick Facts
- arXiv ID: 2505.19604
- Source URL: https://arxiv.org/abs/2505.19604
- Reference count: 16
- Google Translate outperforms other models, especially in handling context and domain-specific translations.

## Executive Summary
This study evaluates machine translation models for English-Hindi language pairs using an 18,000+ sentence parallel corpus and a domain-specific FAQ dataset. Five metrics—BLEU, WER, TER, BLEURT, and BERTScore—are used to assess translation quality. Google Translate outperforms other models, especially in handling context and domain-specific translations. Performance degrades with increasing sentence length across all models. Issues such as gender marking, abbreviation handling, and idiomatic expressions are identified as challenges. The study highlights the need for improved models for low-resource languages and provides insights into practical deployment scenarios.

## Method Summary
The study evaluates four machine translation models (NLLB-200, Google Translate, OPUS-MT, IndicTrans2) on an 18,000+ sentence parallel English-Hindi corpus plus 400 domain-specific FAQ pairs. Models are evaluated using BLEU, WER, TER, BLEURT, and BERTScore metrics in both translation directions. No training is performed—models are used as-is. Evaluation includes unidirectional translation scoring and back-translation validation, with analysis of performance across sentence length ranges.

## Key Results
- Google Translate achieves highest BLEURT and BERTScore scores, indicating superior semantic quality
- All models show performance degradation as sentence length increases
- Lexical metrics (BLEU, WER, TER) fail to capture semantic equivalence, while semantic metrics better align with human judgment
- Open-source models struggle with gender marking, abbreviation handling, and idiomatic expressions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger training corpora with broader context exposure improves translation quality, particularly for domain-specific content.
- Mechanism: Google Translate leverages example-based machine translation (EBMT) that learns from millions of examples and uses broader context to resolve ambiguities (e.g., distinguishing "bank" as financial institution vs. river edge). This context extraction capability enables more accurate and coherent translations compared to models trained on smaller datasets.
- Core assumption: The quality improvement is causally attributed to dataset size and context modeling rather than architectural differences alone.
- Evidence anchors:
  - [abstract] "Google Translate outperforms other models, especially in handling context and domain-specific translations."
  - [section VI] "The findings underscore the robustness of Google Translate, which can be attributed to the extensive dataset accessible to the system."
  - [corpus] Neighbor paper on legal MT (arXiv:2512.18593) similarly emphasizes domain-specific training for specialized translation quality.
- Break condition: If a smaller model with comparable context mechanisms achieves similar performance, dataset size alone is not the causal factor.

### Mechanism 2
- Claim: Translation quality degrades as sentence length increases across all model architectures.
- Mechanism: Two factors contribute: (1) Limited availability of training corpora with extended sentences reduces model exposure to long-sequence patterns; (2) Encoding variable-length sentences into fixed-size vector representations causes information loss, as the neural network fails to encapsulate all details. Computational overhead scales linearly with sentence length during training.
- Core assumption: The degradation is primarily due to training data distribution and representation bottlenecks, not inference-time limitations.
- Evidence anchors:
  - [abstract] "Performance degrades with increasing sentence length across all models."
  - [section V.B] "All models show degrading translation accuracy as the word count in sentences being translated increases."
  - [corpus] No direct corpus evidence on length degradation mechanisms; neighbor papers focus on segmentation strategies but not length-specific failure analysis.
- Break condition: If models using variable-length representations (e.g., recent long-context transformers) show sustained performance at longer lengths, the fixed-vector bottleneck is the primary cause.

### Mechanism 3
- Claim: Lexical overlap metrics fail to capture semantic equivalence, leading to misleading quality assessments.
- Mechanism: BLEU and other lexical metrics count n-gram overlaps between hypothesis and reference translations. Semantically equivalent but lexically different translations receive low scores (e.g., "assaulted" vs. "attacked" scored 8.72 BLEU despite 85% human-rated similarity). Conversely, unrelated sentences with high word overlap can receive inflated scores (66.42 BLEU for unrelated sentences sharing common words).
- Core assumption: Human judgment accurately captures semantic similarity, and metrics should correlate with human assessment.
- Evidence anchors:
  - [abstract] "Five metrics—BLEU, WER, TER, BLEURT, and BERTScore—are used to assess translation quality."
  - [section II, Table I] Explicit demonstration of BLEU giving poor scores (8.72) to semantically similar sentences and high scores (66.42) to unrelated sentences with lexical overlap.
  - [corpus] Neighbor paper on TST evaluation metrics (arXiv:2502.04718) similarly questions metric reliability for semantic assessment.
- Break condition: If lexical metrics are combined with semantic metrics in ensemble evaluation, their individual failures may be mitigated.

## Foundational Learning

- Concept: **Transformer encoder-decoder architecture**
  - Why needed here: All evaluated models (NLLB-200, OPUS-MT, IndicTrans2) use transformer-based architectures. Understanding attention mechanisms, self-attention layers, and encoder-decoder flow is essential for interpreting model capabilities and limitations.
  - Quick check question: Can you explain how the encoder's self-attention mechanism differs from the encoder-decoder attention in the translation context?

- Concept: **N-gram precision and edit distance metrics**
  - Why needed here: BLEU, WER, and TER are foundational evaluation metrics. Understanding their calculation (n-gram matching, brevity penalty, edit operations) is necessary to interpret results and recognize their semantic blindness.
  - Quick check question: Why does BLEU include a brevity penalty, and what type of translation errors would WER catch that BLEU would miss?

- Concept: **Contextual embeddings (BERT-based representations)**
  - Why needed here: BLEURT and BERTScore use transformer-based embeddings to capture semantic similarity. Understanding how contextual embeddings differ from static word vectors explains why these metrics better correlate with human judgment.
  - Quick check question: How does BERTScore's use of "individual word embeddings and surrounding words" enable it to capture context that BLEU cannot?

## Architecture Onboarding

- Component map:
```
Evaluation Pipeline
├── Data Layer
│   ├── General Parallel Corpus (18,000+ sentence pairs)
│   └── FAQ Corpus (400 domain-specific Q&A pairs)
├── Translation Layer
│   ├── NLLB-200 (600M distilled, MoE layers)
│   ├── Google Translate API (EBMT-based)
│   ├── OPUS-MT (Marian-NMT, 6-layer transformer)
│   └── IndicTrans2 (India-optimized transformer)
├── Evaluation Layer
│   ├── Lexical Metrics: BLEU, WER, TER
│   └── ML-based Metrics: BLEURT, BERTScore, COMET
└── Analysis Layer
    ├── Unidirectional translation scoring
    ├── Back-translation validation
    └── Length-stratified performance analysis
```

- Critical path:
  1. **Corpus preparation** → Ensure parallel sentence alignment and domain coverage
  2. **Model selection** → Choose based on deployment constraints (open-source vs. API, latency vs. accuracy)
  3. **Multi-metric evaluation** → Use both lexical and semantic metrics to avoid single-metric blind spots
  4. **Length-stratified analysis** → Test performance across sentence length buckets to identify degradation thresholds
  5. **Error categorization** → Manually inspect failures in gender marking, abbreviations, and idioms

- Design tradeoffs:
  - **Google Translate (API)**: Highest quality, lowest deployment complexity, but requires network dependency and has cost/privacy considerations
  - **IndicTrans2 (open-source)**: Best open-source option for Indian languages, supports all 22 scheduled languages, but requires local infrastructure
  - **NLLB-200/OPUS-MT**: Lower resource requirements but significantly lower BLEURT scores (negative values indicate below-average quality)
  - **Lexical vs. semantic metrics**: Lexical metrics are faster and require no pre-training; semantic metrics (BLEURT, BERTScore) better correlate with human judgment but require pre-trained models

- Failure signatures:
  - **Gender marking errors**: Hindi's gendered nouns/adjectives/verbs receive incorrect gender assignments when English source lacks gender cues or training data has gender bias
  - **Abbreviation preservation**: Domain-specific abbreviations (e.g., PAN) remain untranslated, blocking comprehension for non-English readers
  - **Idiom literal translation**: Cultural proverbs translated word-for-word lose intended meaning
  - **Length-based degradation**: Quality drops measurably beyond ~20-30 words per sentence (exact threshold requires empirical testing)

- First 3 experiments:
  1. **Baseline benchmark**: Run all four models on a 500-sentence subset with all six metrics. Establish median scores and standard deviations per model per direction (En→Hi, Hi→En). Expected outcome: Google Translate > IndicTrans2 > NLLB-200 ≈ OPUS-MT.
  2. **Length stratification test**: Bin sentences by word count (1-10, 11-20, 21-30, 30+ words). Plot COMET/BLEURT scores vs. length for each model. Expected outcome: Linear degradation trend; identify practical length limits for deployment.
  3. **Domain adaptation probe**: Compare general corpus vs. FAQ corpus scores for each model. Calculate performance gap to quantify domain sensitivity. Expected outcome: Larger gap for open-source models; Google Translate shows smallest domain drop due to broader training coverage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can machine translation architectures be optimized to accurately resolve Hindi gender markers and expand domain-specific abbreviations (e.g., PAN) found in government documents?
- Basis in paper: [explicit] The authors explicitly identify gender marking errors and the failure to translate abbreviations as distinct "failing points" and "challenges" in Section V.C.
- Why unresolved: Current models often rely on biased training data for gender or lack the specific lexical context to expand acronyms, resulting in untranslated English abbreviations in the Hindi output.
- What evidence would resolve it: A comparative study showing improved BLEU/BLEURT scores on a test set rich in gendered nouns and acronyms after implementing gender-aware training objectives or context-aware abbreviation modules.

### Open Question 2
- Question: What specific architectural modifications are required to arrest the linear decline in translation quality as input sentence length increases?
- Basis in paper: [explicit] Section V.B states that "All models show degrading translation accuracy as the word count... increases," attributing this to the difficulty of encoding long sentences into fixed-size vectors.
- Why unresolved: While the paper identifies the "curse of sentence length," it does not evaluate potential solutions such as sparse attention mechanisms or hierarchical encoding to mitigate this specific failure mode.
- What evidence would resolve it: Evaluation results demonstrating that models using segmentation or long-context transformers maintain consistent metric scores (e.g., COMET) as word counts exceed the average training length.

### Open Question 3
- Question: To what extent would fine-tuning open-source models (like IndicTrans2) on the specific government FAQ domain close the performance gap with Google Translate?
- Basis in paper: [inferred] The study evaluates only "stock" versions of models and attributes Google's success to its massive dataset and context extraction capabilities, leaving the potential for domain-specific adaptation unexplored.
- Why unresolved: Without fine-tuning on the specialized FAQ corpus, it remains unclear if the open-source models' lower scores are due to architectural limitations or simply a lack of domain-specific exposure.
- What evidence would resolve it: A follow-up experiment benchmarking IndicTrans2 against Google Translate after the open-source model has been fine-tuned on the banking and tax regulation FAQ dataset.

## Limitations

- The evaluation corpus represents only two domains (general web text and FAQ-style content), limiting generalizability to other text types
- The study focuses exclusively on English-Hindi pairs, preventing conclusions about broader multilingual applicability
- The analysis identifies sentence length degradation but does not explore underlying mechanisms (training data distribution vs. architectural constraints)
- No statistical significance testing is performed to validate observed performance differences between models

## Confidence

**High Confidence Claims:**
- Google Translate outperforms open-source models on both lexical and semantic metrics
- All models show performance degradation with increasing sentence length
- Lexical metrics (BLEU, WER, TER) fail to capture semantic equivalence
- Domain-specific datasets reveal performance gaps not visible in general evaluation

**Medium Confidence Claims:**
- Dataset size and context modeling explain Google Translate's superiority
- The 18,000-sentence corpus provides adequate representation for general translation quality assessment
- Back-translation validation confirms translation accuracy

**Low Confidence Claims:**
- Specific threshold values for sentence length degradation (exact word count limits)
- Causal attribution of gender marking errors to training data bias vs. architectural limitations
- Comparative performance claims without statistical significance testing

## Next Checks

1. **Statistical Significance Testing**: Apply paired t-tests or Wilcoxon signed-rank tests to metric score differences between models. This will determine whether observed performance gaps (e.g., Google Translate vs. IndicTrans2 BLEURT differences) are statistically significant or could arise from random variation in the evaluation corpus.

2. **Cross-Domain Generalization**: Evaluate the same models on three additional domains: informal social media text, technical documentation, and medical/healthcare content. Compare performance drops across domains to identify whether FAQ-specific challenges generalize to other specialized text types.

3. **Error Analysis Replication**: Manually annotate 100 randomly selected translation errors from each model, categorizing them by type (gender marking, abbreviation handling, idiom translation, etc.). Calculate inter-annotator agreement to validate the error categorization scheme and ensure consistent identification of failure modes across annotators.