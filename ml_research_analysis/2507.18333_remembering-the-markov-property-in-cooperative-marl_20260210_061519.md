---
ver: rpa2
title: Remembering the Markov Property in Cooperative MARL
arxiv_id: '2507.18333'
source_url: https://arxiv.org/abs/2507.18333
tags:
- agents
- learning
- marl
- policies
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether deep MARL policies actually recover
  a Markovian signal, or if they learn brittle conventions instead. Through experiments
  in a custom "Prediction Game" and analysis of popular benchmarks (Hanabi, MaBrax,
  SMAX), the authors show that co-adapting agents often converge on non-robust conventions
  that bypass grounded observations and memory.
---

# Remembering the Markov Property in Cooperative MARL

## Quick Facts
- **arXiv ID**: 2507.18333
- **Source URL**: https://arxiv.org/abs/2507.18333
- **Authors**: Kale-ab Abebe Tessera; Leonard Hinckeldey; Riccardo Zamboni; David Abel; Amos Storkey
- **Reference count**: 32
- **Key outcome**: Deep MARL policies often learn brittle conventions rather than Markovian signals, with blind agents achieving surprisingly high performance in many environments.

## Executive Summary
This paper investigates whether deep MARL policies truly recover Markovian signals or instead learn brittle conventions that bypass grounded observations and memory. Through experiments in a custom "Prediction Game" and analysis of popular benchmarks (Hanabi, MaBrax, SMAX), the authors demonstrate that co-adapting agents often converge on non-robust conventions. When agents are trained together, they can achieve high performance through implicit conventions rather than observing and reasoning about the environment. However, when the environment design requires grounded, memory-based reasoning, the same models can learn appropriate policies.

## Method Summary
The authors introduce a custom "Prediction Game" environment where agents must predict each other's actions based on observation history. They systematically test blind agents (with observation channels removed), shuffled observations, and partner swapping to evaluate whether policies rely on grounded reasoning versus learned conventions. The study analyzes QMIX and ROMP architectures across multiple benchmark environments including Hanabi, MaBrax, and SMAX, comparing performance when agents are trained together versus with random partners.

## Key Results
- Blind agents (with observation channels removed) achieve surprisingly high performance in many environments, suggesting convention-based learning rather than grounded reasoning
- Policies that work well in training often fail when partnered with non-adaptive agents, indicating brittle co-adaptation
- When environments require grounded reasoning, models can learn memory-based policies, but this is not the default behavior
- Hanabi experiments show clear evidence of convention-based learning, while MaBrax and SMAX results are more nuanced

## Why This Works (Mechanism)
The paper argues that standard MARL training procedures incentivize agents to develop implicit conventions rather than grounded reasoning about observations and memory. When agents are trained together repeatedly, they can co-adapt to specific patterns and behaviors of their partners, creating brittle policies that work only in familiar contexts. This convention-based learning emerges because it's often easier for neural networks to learn specific partner behaviors than to maintain and reason about complex state representations.

## Foundational Learning
- **Dec-POMDP theory**: Understanding the theoretical framework for multi-agent partially observable decision processes, which requires agents to maintain individual beliefs about the environment state based on observations and history
- **Observation grounding**: The principle that agents should base decisions on actual environmental observations rather than learned patterns about partner behavior
- **Memory mechanisms in MARL**: How agents track and utilize historical information to make decisions in partially observable environments
- **Co-adaptation dynamics**: How agents' policies influence and adapt to each other during training, potentially leading to brittle conventions
- **Transfer learning in MARL**: Understanding how policies generalize when paired with new partners or in slightly different environments

## Architecture Onboarding

**Component map**: Environment -> Observation preprocessor -> Neural network (QMIX/ROMP) -> Action selector -> Reward signal -> Policy update

**Critical path**: Observation reception → Belief state update → Value estimation → Action selection → Environment interaction → Reward collection → Gradient update

**Design tradeoffs**: 
- QMIX provides factorized value decomposition but may limit coordination complexity
- ROMP offers recurrent processing for memory but increases computational cost
- Observation preprocessing affects whether agents can develop grounded versus convention-based policies

**Failure signatures**: 
- High performance with blind agents indicates convention-based learning
- Performance collapse when paired with new partners suggests brittle co-adaptation
- Inability to improve with observation augmentation indicates poor grounding

**First 3 experiments**:
1. Train blind agents in the Prediction Game to establish baseline convention-based performance
2. Compare QMIX vs ROMP architectures in Hanabi with observation shuffling
3. Test partner swapping in MaBrax to evaluate policy robustness

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the prevalence of non-Markovian behavior across different MARL domains and the generalizability of their findings beyond discrete action spaces and specific model architectures.

## Limitations
- Findings are primarily demonstrated in discrete action spaces and may not generalize to continuous control tasks
- The sample size of benchmark environments analyzed is relatively small
- Focus on specific model architectures (QMIX/ROMP) leaves open questions about alternative frameworks
- The "Prediction Game" environment, while insightful, may not capture all complexities of real-world MARL applications

## Confidence
- **High confidence**: Claims about blind agents succeeding in the Prediction Game and Hanabi; observations about convention-based learning
- **Medium confidence**: Generalization of findings to broader MARL; claims about benchmark inadequacy
- **Low confidence**: Specific recommendations for new benchmark design; universality of non-Markovian behavior

## Next Checks
1. Test blind agent performance across a wider range of MARL environments beyond the current sample
2. Evaluate the impact of different model architectures and learning algorithms on Markovian behavior emergence
3. Conduct ablation studies to quantify the contribution of individual observation features to policy performance