---
ver: rpa2
title: 'OATS: Online Data Augmentation for Time Series Foundation Models'
arxiv_id: '2601.19040'
source_url: https://arxiv.org/abs/2601.19040
tags:
- data
- training
- time
- series
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OATS is an online data augmentation method for time series foundation
  models (TSFMs) that dynamically generates high-quality synthetic data during training.
  It leverages data attribution scores to identify valuable training samples as guiding
  signals and uses a diffusion model to generate synthetic data conditioned on these
  signals.
---

# OATS: Online Data Augmentation for Time Series Foundation Models

## Quick Facts
- arXiv ID: 2601.19040
- Source URL: https://arxiv.org/abs/2601.19040
- Reference count: 40
- Authors: Junwei Deng; Chang Xu; Jiaqi W. Ma; Ming Jin; Chenghao Liu; Jiang Bian
- Primary result: Online data augmentation method that generates synthetic time series data during training using attribution-guided diffusion models

## Executive Summary
OATS introduces an online data augmentation framework specifically designed for time series foundation models. The method addresses the challenge of limited training data by generating synthetic samples during training rather than relying on pre-computed static augmentations. By leveraging data attribution scores to identify high-value training samples and conditioning a diffusion model on these signals, OATS creates synthetic data that targets the model's learning needs. The approach includes an explore-exploit mechanism to balance computational efficiency with augmentation effectiveness.

## Method Summary
OATS operates through a dynamic augmentation pipeline that identifies informative training samples using attribution scores, which serve as guidance signals for a conditional diffusion model. During training, the system evaluates which existing samples are most valuable for learning, then generates synthetic data conditioned on these high-value samples. An explore-exploit mechanism determines when to generate new synthetic data versus using existing samples, optimizing the trade-off between computational cost and learning benefit. The framework is designed to be memory-efficient while maintaining effectiveness across different TSFM architectures.

## Key Results
- Consistently outperforms regular training and static augmentation baselines across six diverse datasets
- Achieves better performance in normalized mean absolute percentage error (MAPE) and negative log-likelihood (NLL) metrics
- Demonstrates improved generalization and robustness compared to existing augmentation methods
- Shows effectiveness across two different TSFM architectures

## Why This Works (Mechanism)
OATS leverages the temporal dependencies inherent in time series data by using attribution scores to identify which training samples contain the most informative patterns. The diffusion model generates synthetic data conditioned on these high-value samples, ensuring that augmented data targets the model's learning needs rather than introducing arbitrary variations. The explore-exploit mechanism dynamically adjusts the augmentation strategy based on training progress, preventing overfitting to synthetic patterns while maintaining computational efficiency.

## Foundational Learning
- **Time Series Attribution Methods**: Needed to identify which samples contain the most informative patterns for model learning; Quick check: Validate attribution scores correlate with model performance improvements
- **Conditional Diffusion Models**: Required for generating synthetic time series data that preserves temporal dependencies while introducing meaningful variations; Quick check: Verify synthetic samples maintain statistical properties of real data
- **Foundation Model Training Dynamics**: Essential for understanding how synthetic data integrates into the learning process and affects generalization; Quick check: Monitor training curves for signs of overfitting or underfitting
- **Explore-Exploit Tradeoffs**: Critical for balancing computational cost against augmentation benefits during training; Quick check: Measure wall-clock time impact of augmentation strategy

## Architecture Onboarding

**Component Map**: Data Attribution Module -> Exploration/Exploitation Controller -> Conditional Diffusion Model -> TSFM Training Loop

**Critical Path**: During each training iteration, the system evaluates current sample importance, decides whether to generate synthetic data or use existing samples, generates conditioned synthetic data if needed, and feeds both real and synthetic data to the foundation model for training updates.

**Design Tradeoffs**: The framework prioritizes computational efficiency through the explore-exploit mechanism, which may limit the diversity of synthetic samples compared to exhaustive augmentation approaches. The attribution-guided conditioning improves relevance of synthetic data but requires additional computation for attribution scoring.

**Failure Signatures**: Poor attribution scoring may lead to generation of irrelevant synthetic data, degrading rather than improving model performance. Over-reliance on synthetic data through excessive exploitation can cause overfitting and reduced generalization to real-world patterns.

**Three First Experiments**:
1. Compare model performance with and without OATS augmentation on a single dataset to establish baseline effectiveness
2. Vary the exploration rate in the explore-exploit mechanism to identify optimal computational-efficiency trade-offs
3. Test attribution-guided conditioning against random conditioning to quantify the value of targeted augmentation

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness may degrade on datasets with low attribute diversity among samples
- Computational overhead, while addressed through memory-efficient design, remains incompletely characterized across hardware configurations
- Limited generalizability due to evaluation on only two TSFM architectures and six datasets

## Confidence
- Performance improvements over static augmentation: Medium
- Scalability to larger models and diverse temporal patterns: Low
- Robustness to noisy or corrupted training data: Low

## Next Checks
1. Evaluate OATS on time series datasets with lower attribute diversity to assess performance degradation and identify the minimum diversity threshold for effective augmentation
2. Conduct comprehensive computational profiling across different hardware configurations (GPU vs CPU) to quantify the actual training overhead introduced by the explore-exploit mechanism
3. Test OATS's robustness when applied to noisy or partially corrupted training data to determine its reliability in real-world scenarios with imperfect data quality