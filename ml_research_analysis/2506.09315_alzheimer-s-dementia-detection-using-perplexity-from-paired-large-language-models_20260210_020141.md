---
ver: rpa2
title: Alzheimer's Dementia Detection Using Perplexity from Paired Large Language
  Models
arxiv_id: '2506.09315'
source_url: https://arxiv.org/abs/2506.09315
tags:
- perplexity
- language
- alzheimer
- detection
- adress
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the paired perplexity approach to detect Alzheimer's
  dementia (AD) using a more advanced large language model (LLM), the instruction-following
  Mistral-7B. The method improves accuracy by an average of 3.33% over the best current
  paired perplexity method and by 6.35% over the top-ranked method from the ADReSS
  2020 challenge benchmark.
---

# Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models

## Quick Facts
- arXiv ID: 2506.09315
- Source URL: https://arxiv.org/abs/2506.09315
- Reference count: 0
- Best accuracy: 93.75% on ADReSS dataset

## Executive Summary
This paper introduces a paired perplexity approach using instruction-following Mistral-7B for Alzheimer's dementia detection, achieving state-of-the-art accuracy of 93.75% on the ADReSS benchmark. The method trains two specialized language models - one for AD speakers and one for healthy controls - then classifies test transcripts based on which model assigns lower perplexity. The approach improves accuracy by 3.33% over the best current paired perplexity method and 6.35% over the ADReSS 2020 challenge benchmark, while offering interpretable decision boundaries through synthetic text generation.

## Method Summary
The method fine-tunes two Mistral-7B-Instruct-v0.3 models using LoRA parameter-efficient adaptation, with one model trained on AD transcripts and another on healthy control transcripts. Both models are trained on instruction datasets that simulate interviewer-participant conversations about the Cookie Theft picture. Classification is performed by computing perplexity for each test transcript on both models, with the lower-perplexity model determining the speaker classification. The approach uses normalized logarithmic perplexity differences as the classification metric, with performance validated through accuracy, F1-score, and correlation with MMSE scores.

## Key Results
- Achieves 93.75% accuracy on ADReSS dataset
- Improves accuracy by 3.33% over best current paired perplexity method
- Improves accuracy by 6.35% over ADReSS 2020 challenge benchmark
- Correlation between perplexity and MMSE scores of r ≈ -0.70
- 30 out of 41 linguistic features differ significantly between AD-generated and HC-generated synthetic text

## Why This Works (Mechanism)

### Mechanism 1: Paired Perplexity Competition
The method improves classification accuracy by having two fine-tuned language models compete to predict the same transcript, with the lower-perplexity model indicating speaker class. This approach assumes AD speech contains learnable distributional patterns that differ systematically from healthy speech, and fine-tuning on small corpora can specialize models sufficiently to detect these differences.

### Mechanism 2: Instruction-Following Fine-Tuning with LoRA
Low-Rank Adaptation (LoRA) fine-tuning of instruction-following Mistral-7B enables effective AD language specialization without full model retraining. Each model is fine-tuned using LoRA (rank=16, α=32) on an instruction dataset where prompts ask for Cookie Theft picture descriptions and responses are transcripts from AD or HC speakers respectively.

### Mechanism 3: Linguistic Feature Internalization Enables Synthetic Generation
Fine-tuned models internalize AD-specific linguistic patterns (disfluencies, reduced semantic content), which can be probed via synthetic text generation for interpretability. By prompting fine-tuned models to generate Cookie Theft descriptions, researchers extract linguistic features that differ significantly between AD-generated and HC-generated text.

## Foundational Learning

- Concept: Perplexity as a probability distribution measure
  - Why needed here: The entire method relies on interpreting perplexity differences as classification signals
  - Quick check question: Given two models computing perplexity on the same sentence, what does it mean if one returns 50 and the other returns 200?

- Concept: Instruction-following vs. base LLMs
  - Why needed here: The paper specifically uses the instruction-following variant of Mistral-7B, which differs from earlier GPT-2 baselines
  - Quick check question: Why would an instruction-tuned model be preferred over a base autoregressive model for this classification task?

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: Fine-tuning 7B parameters directly would be computationally prohibitive; LoRA enables specialization with minimal trainable parameters
  - Quick check question: What components of the model does LoRA modify, and why does this preserve base model capabilities?

## Architecture Onboarding

- Component map: IPA symbol substitution -> transcript cleaning -> instruction dataset construction -> LoRA fine-tuning -> dual perplexity computation -> normalized log difference -> classification

- Critical path: 1. Balance dataset by gender, age, education, MMSE 2. Prevent data leakage between train/test splits 3. Fine-tune M_AD and M_C separately on class-specific subsets 4. Compute perplexity differences using normalized log formulation 5. Validate correlation with MMSE as sanity check

- Design tradeoffs: Perplexity formulation (plain difference, ratio, log difference, normalized log); classification threshold (fixed cutoffs vs. standard-deviation-adjusted); training epochs (6 for Mistral, limited to 5 for GPT-2)

- Failure signatures: Perplexity differences near zero across all samples; high variance across random seeds (SD > 3% accuracy); generated text lacks class-specific linguistic markers; MMSE correlation near zero

- First 3 experiments: 1. Replicate baseline on ADReSS with GPT-2 using paper's preprocessing 2. Fine-tune single Mistral-7B model on combined AD+HC data with classification head 3. Generate synthetic responses from fine-tuned models; manually inspect for AD markers

## Open Questions the Paper Calls Out

### Open Question 1
Can the synthetic text generated by the fine-tuned Mistral models be used effectively for data augmentation to improve AD detection performance on limited datasets? The study demonstrates the models learned linguistic features but does not test whether synthetic data improves classification accuracy.

### Open Question 2
How can the identified linguistic patterns be operationalized into specific interpretability tools for clinical decision support? The paper validates that the LLM learns distinct patterns but does not develop methods to present these insights to clinicians.

### Open Question 3
Does combining the proposed text-based perplexity method with acoustic features yield a superior multimodal detection system? The methodology intentionally isolates the text modality to benchmark the LLM perplexity approach, leaving multimodal integration unexplored.

## Limitations
- Small sample size (108 transcripts after preprocessing) raises concerns about model generalization
- Performance heavily dependent on quality and size of training data
- Interpretability advantage through synthetic generation not yet validated against clinical standards
- Results may reflect dataset-specific artifacts rather than universal AD markers

## Confidence
- Paired perplexity approach accuracy: Medium confidence (modest gains, small dataset concerns)
- Synthetic generation pattern learning: Medium confidence (feature differences significant but clinical relevance unclear)
- MMSE correlation validity: Medium confidence (meaningful connections but require further validation)

## Next Checks
1. Cross-dataset validation: Test paired perplexity approach on independent AD datasets to assess generalization beyond ADReSS
2. Feature importance analysis: Quantify which linguistic features from synthetic generation most strongly predict classification decisions
3. Threshold sensitivity analysis: Systematically evaluate classification performance across different decision boundaries and normalized formulations