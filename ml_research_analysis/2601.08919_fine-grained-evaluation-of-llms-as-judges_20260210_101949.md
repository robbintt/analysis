---
ver: rpa2
title: Fine Grained Evaluation of LLMs-as-Judges
arxiv_id: '2601.08919'
source_url: https://arxiv.org/abs/2601.08919
tags:
- document
- inex
- relevant
- llms
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well large language models (LLMs) can
  serve as relevance assessors in information retrieval, going beyond document-level
  judgments to examine whether LLMs can accurately highlight relevant passages within
  documents. Using the INEX Wikipedia test collections, the authors prompt LLMs to
  generate rationales by highlighting relevant content in documents, and compare these
  outputs to human annotations.
---

# Fine Grained Evaluation of LLMs-as-Judges

## Quick Facts
- arXiv ID: 2601.08919
- Source URL: https://arxiv.org/abs/2601.08919
- Reference count: 36
- Primary result: LLMs can identify relevant documents with reasonable accuracy but over-highlight content, inflating recall while lowering precision

## Executive Summary
This study investigates whether large language models can serve as fine-grained relevance assessors in information retrieval by highlighting relevant passages within documents rather than just providing binary judgments. Using the INEX Wikipedia test collections, the authors prompt LLMs to generate rationales by highlighting relevant content in documents and compare these outputs to human annotations. Experiments with Llama-3.1-8B and GPT-4.1-mini reveal that while LLMs can identify relevant documents with reasonable accuracy, they tend to over-highlight content, resulting in high recall but low precision. GPT-4.1-mini generally outperforms the smaller Llama model in precision, though both struggle particularly with documents containing sparse relevant passages. The study concludes that LLMs show promise for relevance assessment but are not yet reliable replacements for human assessors in fine-grained evaluation tasks.

## Method Summary
The study uses INEX 2009 and 2010 Wikipedia collections (2.66M articles) with judged queries (68 for 2009, 52 for 2010) and ground truth qrels containing highlighted passage offsets. The method employs 6-shot In-Context Learning with Llama-3.1-8B-Instruct and GPT-4.1-mini, using prompts that include task description, exemplars, and test instances. Three exemplar strategies based on document length are tested. LLM outputs are post-processed to extract explanation text, which is then mapped back to document byte offsets using pattern matching algorithms. Evaluation metrics include macro and micro precision, recall, and F1 scores based on character overlap between LLM-generated highlights and human ground truth.

## Key Results
- LLMs achieve reasonable document-level relevance accuracy but struggle with precise passage highlighting
- Both models exhibit significant over-highlighting behavior, inflating recall at the expense of precision
- GPT-4.1-mini consistently outperforms Llama-3.1-8B in precision metrics across experiments
- LLMs perform poorly when documents contain sparse relevant content ("needle in haystack" scenarios)
- Performance varies significantly based on exemplar selection strategy and document length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot In-Context Learning (ICL) enables passage-level segmentation
- Mechanism: By providing exemplars with highlighted "Explanations," the model conditions its generation on the structure and semantic specificity of examples, learning to identify relevant spans rather than binary labels
- Core assumption: The model can generalize "relevance" from few-shot examples without weight updates
- Evidence anchors:
  - Section 3.2: "Our prompt includes exemplars... a process referred to as In-Context Learning (ICL)..."
  - Abstract: "...prompt LLMs to not only judge whether documents are relevant... but to highlight relevant passages..."
  - Corpus: Neighbor papers suggest this mechanism is sensitive to ordering and selection of examples
- Break condition: Exemplars fail to cover domain or document length distribution of test set

### Mechanism 2
- Claim: Over-highlighting acts as a failure mode of calibration, inflating recall at expense of precision
- Mechanism: LLMs adopt an "inclusive" strategy, marking broad sections as relevant to ensure coverage rather than pinpointing exact boundaries
- Core assumption: The model prioritizes avoiding false negatives over false positives
- Evidence anchors:
  - Section 5: "...tendency of the model to over-highlight document content, thereby inflating recall scores."
  - Figure 3: Shows models frequently highlight >50% of document content
  - Abstract: "...often over-highlight content, inflating recall but lowering precision."
- Break condition: The mechanism breaks when documents are short or strictly focused

### Mechanism 3
- Claim: Attention dilution causes failure in sparse "Needle in Haystack" scenarios
- Mechanism: When relevant passages are small and scattered within large documents, the model's attention mechanism struggles to maintain strict exclusion criteria over long contexts
- Core assumption: Model performance depends on signal-to-noise ratio within input context window
- Evidence anchors:
  - Section 5: "...when only a small fraction of the document is relevant... the precision scores tend to be lower."
  - Section 5: "LLMs tend to struggle more when a document has discontiguous, relevant chunks."
- Break condition: Highly structured or short documents where relevant section is obvious and contiguous

## Foundational Learning

- Concept: **Precision vs. Recall in IR Evaluation**
  - Why needed here: The paper relies on this trade-off to diagnose LLM failure modes. Over-highlighting increases *Recall* (finding all relevant info) but decreases *Precision* (accuracy of highlighted text)
  - Quick check question: If a model highlights 90% of a document to capture a 10% relevant section, is it optimizing for recall or precision?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: This is the operational method for "teaching" the LLM the highlighting task without fine-tuning. Understanding how exemplars steer output is critical
  - Quick check question: How does varying the number or length of exemplars (k) affect the model's ability to generalize to new document lengths?

- Concept: **Offset Mapping (Text Alignment)**
  - Why needed here: LLMs generate text, but evaluation requires byte offsets. You must understand how to map generated text back to source document to compute metrics
  - Quick check question: Why can't we simply compare the raw text string of LLM output to ground truth string?

## Architecture Onboarding

- Component map: Input Loader -> Prompt Constructor -> LLM Engine -> Post-Processor -> Alignment Module -> Evaluator
- Critical path: The Alignment Module. The paper notes that LLMs might paraphrase or hallucinate, breaking exact substring match. The alignment step is the bottleneck for accurate metric calculation
- Design tradeoffs:
  - Model Size vs. Precision: Larger models yield higher precision but cost more and may still struggle with sparse relevance
  - Exemplar Length vs. Context: Using long exemplars helps with long documents but consumes context window budget
  - Temperature Settings: Setting T=0 is crucial to force extraction rather than creative paraphrasing
- Failure signatures:
  - Over-generation: Model highlights >50% of document (High Recall, Low Precision)
  - Hallucination: Llama-3.1-8B generating "Example 8" beyond requested output
  - Discontiguity Loss: Performance drops when relevant text is split into multiple disjoint passages
- First 3 experiments:
  1. Baseline Calibration: Run prompt with k=6 random exemplars on 100 documents to establish baseline Precision/Recall
  2. Exemplar Sensitivity Test: Swap "Exemplar Set 1" (short docs) for "Exemplar Set 2" (long docs) and measure delta in F1 score for long test documents
  3. Sparsity Stress Test: Filter for documents where ground truth is <10% of text length and measure drop in precision compared to dense documents

## Open Questions the Paper Calls Out

- **Question:** How does the selection and ordering of exemplars in few-shot prompts affect accuracy and precision of LLM rationale generation for passage highlighting?
  - Basis in paper: Section 3.2 states "the selection and ordering of exemplars... play a crucial role in determining the accuracy of the predictions... We plan to undertake a rigorous analysis of these factors in the near future."
  - Why unresolved: The paper used three fixed exemplar configurations without systematic optimization
  - What evidence would resolve it: A controlled study varying exemplar selection strategies (random vs. similarity-based vs. diversity-based) and orderings, measuring precision/recall across multiple LLMs and collections

- **Question:** Are the unexpectedly high false positive rates observed in LLM document relevance predictions causally linked to lexical cues (presence of query terms in documents)?
  - Basis in paper: Section 6 states: "Alaofi et al. reported that false positive predictions by LLMs are related to lexical cues... We also observe an unexpectedly large number of false positives... and need to look at whether these are also connected to lexical cues."
  - Why unresolved: The authors observed poor accuracy on non-relevant documents but did not analyze whether false positives correlated with query term presence
  - What evidence would resolve it: Correlation analysis between query term density in documents and false positive rates; ablation experiments masking query terms

- **Question:** Can reasoning-oriented models (e.g., GPT-5, DeepSeek-Reasoner) be effectively prompted to accurately highlight relevant passage spans within documents?
  - Basis in paper: Section 4.2 notes that preliminary experiments with reasoning models showed they "often fail to accurately highlight the exact spans of relevant tokens suited to our task"
  - Why unresolved: Reasoning models exhibit explicit reasoning traces but struggle with precise span identification, especially for longer documents
  - What evidence would resolve it: Comparative experiments with reasoning models using specialized prompting strategies designed to elicit precise span annotations

- **Question:** How do document length and the proportion of relevant content within documents interact to affect LLM precision in passage highlighting?
  - Basis in paper: Section 6 states: "The behaviour of LLMs across various criteria like document length and proportion of relevant content needs to be characterised more carefully."
  - Why unresolved: The analysis presented histograms showing trends but did not establish a predictive model or threshold effects
  - What evidence would resolve it: Regression analysis or segmented evaluation across controlled bins of document length and relevant content fraction

## Limitations

- The alignment mechanism between generated text and document offsets relies on pattern matching that may fail when LLMs paraphrase rather than extract verbatim text
- The study only tests two LLM models (Llama-3.1-8B and GPT-4.1-mini), limiting generalizability to other model families or sizes
- The exemplar selection strategy lacks precise specifications (exact length thresholds and sampling algorithms), making exact replication challenging

## Confidence

- **High Confidence:** The observation that LLMs over-highlight content, sacrificing precision for recall, is consistently demonstrated across multiple experiments and supported by quantitative metrics
- **Medium Confidence:** The claim that GPT-4.1-mini outperforms Llama-3.1-8B in precision is supported but could be influenced by prompt engineering choices rather than inherent model capability
- **Low Confidence:** The assertion that LLMs struggle specifically with "needle in haystack" scenarios needs more systematic analysis, as the sparsity claim is primarily anecdotal rather than quantified across different document types

## Next Checks

1. **Alignment Robustness Test:** Implement a controlled experiment comparing pattern-matching alignment accuracy against a ground truth where LLM outputs are known to be verbatim extracts versus paraphrased content

2. **Exemplar Sensitivity Analysis:** Systematically vary exemplar document lengths and count (k=3, k=6, k=9) to quantify the impact on precision-recall trade-offs for both short and long documents

3. **Cross-Domain Generalization:** Apply the same prompting strategy to a non-Wikipedia corpus (e.g., news articles or scientific papers) to test whether over-highlighting behavior persists across different text domains and structure