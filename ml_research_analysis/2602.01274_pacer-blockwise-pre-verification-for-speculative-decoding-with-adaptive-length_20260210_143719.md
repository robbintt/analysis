---
ver: rpa2
title: 'PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length'
arxiv_id: '2602.01274'
source_url: https://arxiv.org/abs/2602.01274
tags:
- draft
- decoding
- tokens
- speculative
- acceptance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of fixed window sizes in
  speculative decoding (SD) for large language model (LLM) inference. A key observation
  is that optimal draft lengths vary significantly across decoding steps, leading
  to wasted computation when using a fixed window size.
---

# PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length

## Quick Facts
- arXiv ID: 2602.01274
- Source URL: https://arxiv.org/abs/2602.01274
- Reference count: 40
- This paper proposes PACER, a method that dynamically controls draft length in speculative decoding using a lightweight pre-verification layer, achieving up to 2.66x speedup over autoregressive decoding and outperforming standard speculative decoding.

## Executive Summary
This paper addresses the inefficiency of fixed window sizes in speculative decoding (SD) for large language model (LLM) inference. The authors observe that optimal draft lengths vary significantly across decoding steps, leading to wasted computation when using a fixed window size. To overcome this, they propose PACER, a novel framework that dynamically controls draft length using a lightweight, trainable blockwise pre-verification layer. This layer pre-verifies draft tokens in blocks before they are sent to the target model, allowing the draft model to stop generating tokens early if pre-verification fails. PACER is implemented across multiple SD model pairs and evaluated on various benchmarks including code generation, mathematical reasoning, and text summarization.

## Method Summary
PACER introduces a blockwise pre-verification mechanism to dynamically control draft length in speculative decoding. The method generates draft tokens in fixed-size blocks (b), and after each block, a lightweight single-layer Transformer (MB) receives the draft hidden states plus positional embeddings and outputs estimated acceptance rates for each token in the block. If the block's mean acceptance estimate falls below a threshold t, drafting halts and the target model verifies all accumulated drafts in one forward pass. Otherwise, drafting continues to the next block. The threshold t grows by a factor ρ > 1 after each passed block to encourage aggressive early exploration while preventing unbounded drafting. The pre-verification layer is trained on SD-generated acceptance labels from the target model.

## Key Results
- PACER achieves up to 2.66x speedup over autoregressive decoding and consistently outperforms standard speculative decoding
- When integrated with Ouroboros, PACER achieves up to 3.09x speedup
- Optimal block size is found to be b=3-4 across tasks, with position embeddings improving acceptance prediction accuracy by 4.3%

## Why This Works (Mechanism)

### Mechanism 1
Blockwise pre-verification enables adaptive draft-length control with bounded overhead. Draft tokens are generated in fixed-size blocks (b). After each block, a lightweight single-layer Transformer (MB) receives the draft hidden states plus positional embeddings and outputs estimated acceptance rates. If the block's mean acceptance estimate falls below threshold t, drafting halts and the target model verifies all accumulated drafts in one forward pass. Otherwise, drafting continues to the next block. The pre-verification overhead is modest (~1.81ms vs 16.52ms draft, 67.31ms target for DeepSeek-Coder 1.3B/33B).

### Mechanism 2
Draft position information improves acceptance-rate prediction accuracy. Positional embeddings corresponding to absolute draft positions are added to hidden states before pre-verification. This captures the empirically observed decline in acceptance rates at later draft positions. Removing position embeddings drops HumanEval tokens/s from 41.80 to 39.99 (4.3% degradation).

### Mechanism 3
Threshold growth factor prevents unbounded drafting while encouraging aggressive early exploration. After each passed block, threshold t is multiplied by growth factor ρ > 1 (e.g., ρ = 1.05). This progressively lowers the stopping bar, making continuation more likely in later rounds. Without growth factor, acceptance length τ increases but decoding speed decreases due to over-drafting.

## Foundational Learning

- **Speculative Decoding (Draft-Verify Paradigm)**: Why needed here - PACER builds on SD's two-stage process; understanding the acceptance criterion is essential to grasp what the pre-verification layer approximates. Quick check question: Given draft distribution q and target distribution p, when is a draft token yᵢ accepted with probability 1?

- **Autoregressive Memory-Bound Latency**: Why needed here - The paper motivates SD by explaining that LLM inference is memory-bandwidth limited, not compute-limited; SD amortizes memory access by verifying multiple tokens in one target forward pass. Quick check question: Why does increasing batch size not linearly improve autoregressive decoding throughput?

- **Calibration and Overconfidence in LLMs**: Why needed here - The paper critiques entropy-based stopping heuristics because LLMs are miscalibrated (overconfident); PACER addresses this by learning from actual acceptance data. Quick check question: What does it mean for a model to be "calibrated" in terms of predicted probabilities?

## Architecture Onboarding

- **Component map**: Prompt -> Draft Model (MD) -> Pre-verification Layer (MB) -> Target Model (MT) -> Output
- **Critical path**: 1) Prompt → MD generates b tokens + hidden states; 2) MB computes acceptance estimates with position embeddings; 3) If mean(α̂) > t: append tokens, t ← t·ρ, repeat step 1; 4) If mean(α̂) ≤ t: send all accumulated drafts to MT for verification; 5) MT parallel forward → acceptance decisions → resample if rejected → update KV cache
- **Design tradeoffs**: Block size b (small b → more frequent overhead; large b → less granular control); Threshold t (higher → more conservative; lower → more aggressive); Growth factor ρ (encourages early stopping but caps maximum draft length)
- **Failure signatures**: Premature halting (short τ, high target count, modest speedup); Over-drafting (long τ but low tokens/s, high draft count); High pre-verification overhead (>5% of total runtime)
- **First 3 experiments**: 1) Baseline sweep: Run vanilla SD with fixed window sizes γ=1–10 on HumanEval; compare to PACER with default (b=4, t=0.7, ρ=1.05); 2) Ablation: block size - fix t and ρ, vary b∈{2,3,4,5,6}, plot tokens/s and τ vs b; 3) Cross-task generalization - train pre-verification layer on CodeAlpaca, evaluate on HumanEval, GSM8K, and CNN/DM

## Open Questions the Paper Calls Out
- How does the relative overhead of the pre-verification layer impact efficiency when applying PACER to smaller model pairs (e.g., sub-billion parameters) or resource-constrained edge devices?
- Can the pre-verification layer be generalized to transfer across different target models or domains without requiring retraining on the specific target's outputs?
- Can the blockwise pre-verification approach be extended to tree-based speculative decoding methods which utilize non-sequential draft structures?
- Can the hyperparameters (threshold t and growth factor ρ) be dynamically adapted during inference rather than requiring manual tuning per task?

## Limitations
- The pre-verification layer may become a bottleneck when both draft and target models are relatively small, making its relative overhead more pronounced
- The method lacks theoretical guarantees on speedup or acceptance rate compared to optimal fixed windows
- All experiments use DeepSeek-Coder as the target model, limiting claims of broad applicability across different LLMs

## Confidence
- **High Confidence**: The core mechanism of blockwise pre-verification for adaptive draft length is well-justified and empirically validated
- **Medium Confidence**: The design choices (block size b=3–4, threshold t=0.60–0.70, growth factor ρ=1.02–1.05) are supported by ablation studies but their optimality is task-dependent
- **Low Confidence**: Claims about cross-task generalization of the pre-verification layer are weakly supported with limited evaluation

## Next Checks
1. **Cross-Model Generalization**: Evaluate PACER with the same pre-verification layer across different target models (e.g., Llama-2 70B, Qwen2.5 32B) on a shared task (e.g., HumanEval). Measure speedup degradation to quantify model dependence.

2. **Out-of-Distribution Task Transfer**: Train the pre-verification layer on one task (e.g., code generation) and evaluate on a structurally different task (e.g., long-form summarization). Compare speedup and acceptance length to in-domain training to assess robustness.

3. **Pre-verification Overhead Scaling**: Measure pre-verification time as a fraction of total inference time across a range of target model sizes (e.g., 7B to 70B). Determine the threshold where pre-verification overhead negates speedup gains.