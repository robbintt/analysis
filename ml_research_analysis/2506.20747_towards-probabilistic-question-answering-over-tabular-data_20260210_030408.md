---
ver: rpa2
title: Towards Probabilistic Question Answering Over Tabular Data
arxiv_id: '2506.20747'
source_url: https://arxiv.org/abs/2506.20747
tags:
- probabilistic
- reasoning
- table
- question
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LUCARIO, a new benchmark and framework for
  probabilistic question answering over large tabular data. The method induces Bayesian
  Networks from tables, translates natural language queries into probabilistic queries,
  and uses large language models to generate final answers.
---

# Towards Probabilistic Question Answering Over Tabular Data

## Quick Facts
- arXiv ID: 2506.20747
- Source URL: https://arxiv.org/abs/2506.20747
- Reference count: 12
- Primary result: Auto-BN achieves 38.2% Acc0.02 and MAE of 0.103 on LUCARIO benchmark

## Executive Summary
This paper introduces LUCARIO, a new benchmark and framework for probabilistic question answering over large tabular data. The method induces Bayesian Networks from tables, translates natural language queries into probabilistic queries, and uses large language models to generate final answers. Auto-BN, the proposed framework, achieves 38.2% Acc0.02 and MAE of 0.103 on the benchmark, significantly outperforming baselines like LLM+Table (24.4% Acc0.02, MAE 0.171) and NL2SQL (13.2% Acc0.02, MAE 0.190). The results demonstrate the effectiveness of combining symbolic probabilistic reasoning with neural language models for handling uncertainty in tabular QA tasks.

## Method Summary
Auto-BN is a probabilistic QA framework that constructs Bayesian Networks from tabular data, then uses LLMs to translate natural language questions into structured probabilistic queries. The framework performs exact inference on the induced BNs and verbalizes the results. It extracts premises and insights from the BN structure, using Markov Blanket analysis to identify high-impact conditional patterns. The method achieves superior performance by combining symbolic probabilistic reasoning with neural language models for query understanding and answer generation.

## Key Results
- Auto-BN achieves 38.2% Acc0.02 and MAE of 0.103 on LUCARIO benchmark
- Outperforms LLM+Table (24.4% Acc0.02, MAE 0.171) and NL2SQL (13.2% Acc0.02, MAE 0.190)
- Insight extraction improves LLM+Premise performance from 10.2% to 23.9% Acc0.02

## Why This Works (Mechanism)

### Mechanism 1
Bayesian Networks automatically learned from tabular data capture conditional dependencies that enable exact probabilistic inference. Structure learning algorithms derive directed acyclic graphs from observed data distributions, encoding joint probability distributions factorizable as P(X₁,...,Xₙ) = ∏ᵢ P(Xᵢ|Parents(Xᵢ)). Variable elimination then computes exact conditional probabilities P(Query|Evidence). Core assumption: Statistical dependencies in the observed data reflect meaningful relationships for probabilistic queries (not necessarily causal).

### Mechanism 2
LLM-based query translation maps natural language to structured probabilistic queries more robustly than direct probability estimation. The LLM parses the question to identify target variables and evidence conditions, mapping paraphrased expressions (e.g., "slightly increase") to corresponding node-state pairs in the BN's discretized representation. Core assumption: The LLM can accurately ground linguistic variations to the BN's vocabulary without seeing explicit mappings during training.

### Mechanism 3
Insights derived from Markov Blankets concentrate premise information, reducing context length while preserving reasoning accuracy. Markov Blanket analysis identifies the minimal set of variables that render a target conditionally independent of all others. KL-divergence ranking selects high-impact conditional patterns, distilling ~79k premises into ~2.4k insights. Core assumption: High KL-divergence between conditional and marginal distributions indicates premises most relevant for inference.

## Foundational Learning

- **Bayesian Network Structure Learning**: Why needed here: Auto-BN must automatically construct BNs from tables without expert curation. Quick check question: Given a 3-variable dataset, can you explain why P(A|B,C) might require B→A←C vs. B→A→C structure?
- **Exact Probabilistic Inference (Variable Elimination)**: Why needed here: The framework computes ground-truth probabilities and Auto-BN answers via exact inference, not sampling. Quick check question: For a chain A→B→C, what is the computational order to compute P(C|A)?
- **Markov Blanket**: Why needed here: Insight extraction relies on identifying which variables are in each node's Markov blanket. Quick check question: What is the Markov blanket of a node in a Bayesian Network, and why does it render the node independent of all others?

## Architecture Onboarding

- **Component map**: Table → BN Constructor → Premise/Insight Generator → Query Translator (LLM) → Inference Engine → Answer Generator (LLM)
- **Critical path**: 1) Table → BN construction (5 min - 2 hours per table) 2) BN → Premise extraction → Insight selection 3) Question → LLM translation → Node-state mapping 4) Node-state pairs → Exact inference → Probability 5) Probability + Question → LLM → Natural language answer
- **Design tradeoffs**: Discretization granularity (more states = finer resolution but exponentially more premises), insight count vs. coverage (20 insights improve accuracy but may miss rare reasoning paths), LLM choice (GPT-4o outperforms smaller models by 5-6% on Acc0.02)
- **Failure signatures**: Error rate ~6.5% (GPT-4o) indicates translation failures, NL2SQL baseline produces 0.0/1.0 probabilities when WHERE clause matches zero records, Mixtral shows 52.8% error rate on NL2SQL due to invalid SQL generation
- **First 3 experiments**: 1) Run LLM+Table with 100 sampled rows on movies dataset (4,627 rows) to verify MAE ~0.17 2) Manually inspect 20 Auto-BN translations on "Natural" question type to count correct node-state mappings 3) Vary insight count (5, 10, 20, 50) on hockey dataset and plot Acc0.02/MAE to find saturation point

## Open Questions the Paper Calls Out

- **Can prompt adaptation and model-specific tuning bridge the performance gap between GPT-4o and open-source models?** The paper suggests this exploration is left to future work, noting that a consistent prompting format was used without optimization for smaller models like Llama and Mixtral.
- **How can external knowledge or semantic priors be integrated into Auto-BN structure learning?** The framework currently operates purely data-driven without incorporating external knowledge, leading to learned networks that may fail to capture true underlying causal mechanisms.
- **What mechanisms can effectively identify and mitigate failures in LLM-based query translation?** The paper reports that translation failures account for the observed error rate in Auto-BN, but doesn't explore methods to validate or self-correct this mapping step.

## Limitations
- BN structure learning algorithm is vaguely described as "modified" without details on scoring functions or hyperparameters
- LLM prompts for query translation and answer generation are completely omitted, preventing exact reproduction
- Discretization method for continuous variables is unspecified, though crucial for the framework's operation
- Purely data-driven approach may induce spurious dependencies without incorporating causal priors

## Confidence
- **High Confidence**: The general approach (BN induction + LLM translation + exact inference) is technically sound with substantial performance improvements documented
- **Medium Confidence**: Markov Blanket-based insight extraction shows promise but needs cross-dataset validation for general applicability
- **Low Confidence**: Claims about capturing uncertainty in a "principled way" are questionable given potential for spurious dependencies and lack of causal grounding

## Next Checks
1. **Translation Robustness Test**: Implement Auto-BN pipeline on held-out LUCARIO subset and systematically vary question phrasing to measure translation accuracy degradation with linguistic variation.
2. **Causal Structure Validation**: For tables with known causal relationships (e.g., purchase dataset), compare learned BN structure to ground-truth dependencies to quantify spurious dependency risk.
3. **Insight Coverage Analysis**: For each validation query, trace reasoning path through BN to identify needed premises and calculate percentage excluded by KL-divergence filtering.