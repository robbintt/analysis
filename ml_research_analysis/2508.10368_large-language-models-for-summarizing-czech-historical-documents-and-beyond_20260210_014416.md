---
ver: rpa2
title: Large Language Models for Summarizing Czech Historical Documents and Beyond
arxiv_id: '2508.10368'
source_url: https://arxiv.org/abs/2508.10368
tags:
- summarization
- dataset
- czech
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents large language models for summarizing Czech\
  \ historical documents. It introduces a novel dataset called Posel od \u02C7Cerchova\
  \ for summarization of historical Czech documents and achieves new state-of-the-art\
  \ results on the modern Czech summarization dataset SumeCzech using Mistral and\
  \ mT5 models."
---

# Large Language Models for Summarizing Czech Historical Documents and Beyond

## Quick Facts
- arXiv ID: 2508.10368
- Source URL: https://arxiv.org/abs/2508.10368
- Reference count: 3
- Primary result: New state-of-the-art Czech summarization with Mistral 7B and mT5 models

## Executive Summary
This paper introduces large language models for Czech text summarization, addressing both modern and historical documents. The authors present a novel dataset, Posel od Čerchova, for historical Czech summarization and achieve state-of-the-art results on the SumeCzech dataset using Mistral 7B and mT5 models. The work demonstrates that fine-tuning multilingual pre-trained models on Czech data yields significant performance improvements, with Mistral 7B outperforming mT5 on historical documents after domain-specific fine-tuning.

## Method Summary
The authors fine-tune two pre-trained models for Czech summarization: mT5-base (encoder-decoder) and Mistral 7B (decoder-only). mT5 is trained with AdamW optimizer (LR=0.001) on SumeCzech, while Mistral 7B uses QLoRA with 4-bit quantization for efficient fine-tuning. Both models are evaluated using ROUGE-raw metrics. Three variants are developed: M7B-SC (Mistral on SumeCzech), M7B-POC (Mistral further fine-tuned on historical Posel od Čerchova), and mT5-SC (mT5 on SumeCzech).

## Key Results
- M7B-SC establishes new state-of-the-art performance on SumeCzech dataset
- M7B-POC significantly outperforms mT5-SC on Posel od Čerchova historical dataset
- mT5-SC also achieves strong results on modern Czech summarization task

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning multilingual pre-trained models on Czech summarization data yields state-of-the-art performance. Pre-training on mC4 (mT5) or broad corpora (Mistral) provides multilingual representations; fine-tuning adapts these to Czech summarization by adjusting task-specific parameters while retaining transferred linguistic knowledge. Core assumption: The pre-trained multilingual representations transfer effectively to Czech, including morphological complexity.

### Mechanism 2
QLoRA enables efficient Mistral 7B fine-tuning with limited hardware while preserving summarization quality. 4-bit quantization compresses model weights; LoRA injects low-rank trainable matrices; only LoRA parameters update during training, reducing memory from ~14GB (FP16) to ~4GB while maintaining expressivity. Core assumption: Low-rank adaptations capture sufficient task-specific knowledge for Czech summarization without full fine-tuning.

### Mechanism 3
Domain-specific fine-tuning on historical Czech improves performance on historical documents versus modern-only training. Historical Czech contains outdated vocabulary, syntax shifts, and spelling variations; further fine-tuning on POC dataset exposes the model to these patterns, adapting its generation to historical contexts. Core assumption: 432 manually curated summaries provide sufficient signal for domain adaptation despite small scale.

## Foundational Learning

- Concept: Transformer encoder-decoder architectures (mT5) vs. decoder-only architectures (Mistral)
  - Why needed here: Understanding architectural differences informs model selection and expected behavior (mT5 bidirectional context vs. Mistral causal attention)
  - Quick check question: Can you explain why encoder-decoder models may handle summarization differently than decoder-only models with prefix attention?

- Concept: ROUGE metrics and ROUGE-raw variant
  - Why needed here: The paper uses ROUGE-raw-1/2/L; understanding n-gram overlap evaluation without stemming/lemmatization is critical for interpreting results
  - Quick check question: How does ROUGE-raw differ from standard ROUGE, and why might it be preferred for morphologically rich languages like Czech?

- Concept: Parameter-efficient fine-tuning (LoRA, QLoRA)
  - Why needed here: The paper uses QLoRA for Mistral 7B; understanding rank, alpha, and quantization is essential for reproduction and debugging
  - Quick check question: What happens to gradient flow if LoRA rank is set too low for a complex summarization task?

## Architecture Onboarding

- Component map: Czech text -> Tokenizer -> Backbone (Mistral 7B or mT5-base) -> QLoRA adapter (Mistral only) -> Generated summary -> ROUGE-raw evaluation

- Critical path: Load pre-trained model -> Apply QLoRA configuration (Mistral only) -> Load dataset -> Fine-tune with AdamW -> Evaluate with ROUGE-raw

- Design tradeoffs: Mistral 7B + QLoRA offers higher performance but requires more VRAM and longer inference; mT5-base is faster and lighter but scores lower on ROUGE; historical fine-tuning improves domain adaptation but needs manual curation

- Failure signatures: ROUGE-raw recall near zero indicates Czech output issues; training loss divergence suggests learning rate or quantization problems; extractive summaries mean undertraining; historical hallucinations suggest dataset is too small

- First 3 experiments: Reproduce M7B-SC baseline on SumeCzech test split; ablate QLoRA rank and measure VRAM vs. ROUGE trade-off; cross-domain test M7B-SC on POC-P to quantify domain gap

## Open Questions the Paper Calls Out

- Can hybrid modeling approaches combining extractive and abstractive methods improve summarization performance on historical Czech texts compared to the purely abstractive methods tested?
- How effectively do models fine-tuned on the 19th-century Posel od Čerchova corpus generalize to documents from different historical eras or distinct multilingual sources?
- Do the reported ROUGE-raw scores accurately reflect factual consistency and semantic preservation in historical summarization, or do the models suffer from hallucination?
- Does the use of LLM-generated (GPT-4/Claude 3 Opus) ground truth summaries introduce a style or length bias that limits the performance of smaller models like Mistral 7B?

## Limitations

- Small historical dataset (432 samples) raises concerns about overfitting and generalizability
- Several key hyperparameters remain unspecified, including exact training epochs, batch sizes, and LoRA configurations
- Evaluation relies exclusively on ROUGE-raw metrics, which don't measure semantic faithfulness or factual consistency

## Confidence

- **High Confidence**: mT5-base fine-tuned on SumeCzech achieves state-of-the-art results for modern Czech summarization
- **Medium Confidence**: Mistral 7B with QLoRA significantly outperforms mT5 on historical Posel od Čerchova dataset
- **Low Confidence**: QLoRA enables efficient Mistral 7B fine-tuning without performance degradation for Czech summarization

## Next Checks

1. Conduct hyperparameter sensitivity analysis varying LoRA rank, learning rates, and batch sizes to identify optimal configurations and assess result stability

2. Evaluate M7B-SC (fine-tuned only on SumeCzech) on POC-P test set to quantify the performance gap and validate the necessity of domain-specific historical fine-tuning

3. Augment Posel od Čerchova dataset with synthetic historical Czech text and monitor training/validation ROUGE scores to detect overfitting