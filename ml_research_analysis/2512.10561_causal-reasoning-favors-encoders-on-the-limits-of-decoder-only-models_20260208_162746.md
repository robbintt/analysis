---
ver: rpa2
title: 'Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models'
arxiv_id: '2512.10561'
source_url: https://arxiv.org/abs/2512.10561
tags:
- reasoning
- label
- period
- language
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares encoder and encoder-decoder architectures with
  decoder-only models for causal reasoning. The authors hypothesized that encoders'
  ability to project inputs into a latent space would make them better suited for
  multi-hop conjunctive reasoning compared to decoders.
---

# Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models

## Quick Facts
- **arXiv ID:** 2512.10561
- **Source URL:** https://arxiv.org/abs/2512.10561
- **Reference count:** 40
- **Primary result:** Encoder architectures with fine-tuning outperform decoder-only models on multi-hop conjunctive causal reasoning, especially under distributional shift.

## Executive Summary
This paper compares encoder, encoder-decoder, and decoder-only architectures for causal reasoning on synthetic first-order logic tasks. The authors hypothesize that encoders' ability to project inputs into a latent space makes them better suited for multi-hop conjunctive reasoning compared to decoders. Results show that fine-tuned encoder and encoder-decoder models generalize more robustly to distributional shifts than decoder-only models, while in-context learning alone is insufficient for reliable causal reasoning. Large-scale reasoning models (GPT-5) achieve near-perfect accuracy but at extreme computational cost.

## Method Summary
The study uses a synthetic SimpleLogic-based dataset with controllable reasoning depth (0-11) and two OOD test settings: natural language (NL) and non-natural language (NNL) vocabulary. Models from three architectural families are evaluated: encoder-only (BERT), encoder-decoder (BART, Flan-T5), and decoder-only (Qwen variants, GPT-4.1, Claude Opus 4.1). Fine-tuning uses 3 epochs, batch size 8, learning rate 5×10⁻⁵ on RTX 6000 GPU with supervision on both reasoning paths and final predictions. Zero-shot ICL is also tested with 0-5 shots.

## Key Results
- Encoder and encoder-decoder models with fine-tuning show significantly better OOD generalization than decoder-only models across all scales
- In-context learning alone is insufficient for reliable causal reasoning, with non-finetuned models showing near-random AUC scores (0.4-0.6)
- Decoder-only models match or surpass encoder performance only at very large scales (GPT-5), but with extreme computational cost
- Encoder models maintain more stable internal reasoning trajectories across increasing depth as measured by curvature similarity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoder architectures outperform decoder-only models on multi-hop conjunctive reasoning due to global projection capabilities.
- **Mechanism:** Encoders project the full input sequence into a latent space in a single pass, allowing every token to integrate information from the entire sequence via bidirectional attention. This enables logical programs to be encoded onto a single projection, which can be evaluated in one forward pass.
- **Core assumption:** Global contextualization is necessary for strict conjunctive control.
- **Evidence anchors:**
  - Abstract: "We hypothesize that, due to their ability to project the input into a latent space, encoder- and encoder–decoder architectures are better suited for said multi-hop conjunctive reasoning versus decoder-only models."
  - Section 3.2: Formal argument that encoders allow "instant global information sharing" while decoders must propagate step-by-step.
- **Break condition:** If logical rules are presented in sequential order matching the proof chain, decoder models may perform comparably.

### Mechanism 2
- **Claim:** Encoder models maintain more stable internal reasoning trajectories across increasing depth.
- **Mechanism:** Using curvature similarity as a mechanistic probe, the paper shows that encoder-only BERT maintains high and stable curvature similarity across depths 6-11, indicating consistent application of internal update rules. Decoder-only Qwen shows steep curvature drift, suggesting accumulation of local noise that disrupts higher-order invariants.
- **Core assumption:** Curvature similarity in representation space correlates with reliable logical composition.
- **Evidence anchors:**
  - Section 7: "BERT (encoder-only) exhibits the highest and most stable similarity across depths 6-11, suggesting that its representations evolve according to a coherent geometric transformation."
  - Section 7: "Decoder-only architectures, however, update representations autoregressively, accumulating local noise that disrupts higher-order invariants."
- **Break condition:** At very large scale (GPT-5), decoder-only models achieve near-perfect accuracy, suggesting scale may compensate for architectural limitations.

### Mechanism 3
- **Claim:** In-context learning alone is insufficient for reliable causal reasoning; fine-tuning is required.
- **Mechanism:** Non-finetuned models across all architectures exhibit near-random AUC scores (0.4-0.6). Fine-tuning with supervision on both reasoning paths and final predictions enables actual compositional reasoning rather than surface-level pattern matching.
- **Core assumption:** Supervised exposure to intermediate reasoning steps transfers to OOD generalization.
- **Evidence anchors:**
  - Abstract: "We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features."
  - Section 5.1: Non-finetuned models show "marginal changes (+0.5% average) in accuracy when increasing the number of shots from zero to five."
  - Section 5.2: Fine-tuned BERT-Base achieved AUC of 0.76 vs. 0.50 for Qwen3-1.7B in NL split.
- **Break condition:** GPT-5 with built-in chain-of-thought priors achieves near-perfect accuracy without task-specific fine-tuning.

## Foundational Learning

- **Concept: First-Order Logic (FOL) and Horn Clauses**
  - **Why needed here:** The paper evaluates reasoning on a subset of FOL using Horn-style rules. Understanding conjunctive premises and implication chains is essential to interpret the "depth" metric.
  - **Quick check question:** Given facts A and B, and rule "A ∧ B ⇒ C", can you derive C? What if only A is given?

- **Concept: Encoder vs. Decoder Architectures**
  - **Why needed here:** The core comparison hinges on how encoders (bidirectional attention, global projection) differ from decoders (causal masking, autoregressive generation). This affects information flow and reasoning capability.
  - **Quick check question:** In a decoder-only model with causal masking, can token position 5 attend to token position 10? What about in a BERT-style encoder?

- **Concept: Distributional Shift / Out-of-Distribution (OOD) Generalization**
  - **Why needed here:** The paper tests robustness via two OOD scenarios: (1) deeper reasoning chains than training, and (2) non-natural language vocabulary. Understanding why models fail on OOD data is central to the conclusions.
  - **Quick check question:** A model trained on depths 0-7 is tested on depth 11. If accuracy drops, is this a failure of memorization, generalization, or both?

## Architecture Onboarding

- **Component map:** Input facts/rules/query -> Encoder (bidirectional attention, global projection) -> [Decoder (autoregressive generation)] -> Output classification/prediction

- **Critical path:**
  1. Define reasoning task with controllable depth using SimpleLogic DSL with explicit tokens
  2. Generate training data with balanced depth distribution to avoid shallow-pattern overfitting
  3. Fine-tune with supervision on both reasoning path (proof chain) and final label
  4. Evaluate on OOD splits: deeper chains + non-natural language vocabulary
  5. Measure per-depth accuracy, AUC, and curvature similarity (if mechanistic interpretability is desired)

- **Design tradeoffs:**
  - Encoder-only: Highest efficiency, best OOD generalization, but limited to discriminative tasks
  - Decoder-only with ICL: Most flexible, no fine-tuning required, but brittle to distributional shift and inefficient for structured reasoning
  - Large-scale reasoning models: Near-perfect accuracy but extreme latency/cost
  - Fine-tuning overhead: 3 epochs on single RTX 6000 GPU required; manageable for encoder-decoder models

- **Failure signatures:**
  - Label collapse: Non-finetuned models outputting the same label repeatedly
  - Sharp accuracy drop at depth 3-4 in NNL split: Indicates reliance on lexical cues rather than logical structure
  - Near-random AUC despite reasonable accuracy: Suggests model is not learning discriminative features
  - Parsing errors from LLMs: Retries required; default to 0 for failed calls

- **First 3 experiments:**
  1. **Baseline comparison:** Evaluate zero-shot ICL on NL and NNL splits using GPT-4.1, Claude Opus 4.1, and Qwen2.5. Expect marginal improvement from 0→5 shots (+0.5%).
  2. **Fine-tuning ablation:** Fine-tune BERT-Base, BART-Base, Flan-T5-Base, and Qwen3-1.7B with identical hyperparameters. Compare per-depth accuracy and AUC.
  3. **Mechanistic probe:** Extract hidden states from fine-tuned models at each reasoning depth. Compute curvature similarity across depths 6-11.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can hybrid architectures be developed that successfully integrate the bidirectional context of encoders with the generative utility of decoder-only models for causal reasoning?
- **Basis in paper:** The authors conclude that empirical work should explore "architectures that merge the convenience of ICL with the capabilities of encoder-based architectures."
- **Why unresolved:** The study highlights a trade-off where decoders offer generative convenience but lack the robust projective reasoning of encoders; a unified model was not tested.
- **What evidence would resolve it:** A new model architecture combining bidirectional attention with causal generation that outperforms fine-tuned BERT/BART on the NNL dataset without requiring GPT-5 scale.

### Open Question 2
- **Question:** What are the formal theoretical bounds of In-Context Learning (ICL) regarding causal compositionality?
- **Basis in paper:** The conclusion states that "further mathematical development is required to formally show the bounds and limits to which this occurs" regarding ICL's inability to capture causal compositionality.
- **Why unresolved:** The paper provides empirical evidence of ICL failure but lacks a formal mathematical framework explaining why recursive token prediction fails on strict conjunctive control compared to latent space projection.
- **What evidence would resolve it:** A theoretical proof defining the maximum complexity of conjunctive rules manageable by ICL, supported by experiments validating this boundary across varying model sizes.

### Open Question 3
- **Question:** Does the near-perfect performance of large-scale reasoning models (like GPT-5) stem primarily from parameter scale or specific "chain-of-thought" training priors?
- **Basis in paper:** The authors note GPT-5 as a high-cost outlier, hypothesizing its success comes from "immense capacity and built-in chain-of-thought priors," but they do not isolate the causal factor.
- **Why unresolved:** It is unclear if simply scaling up standard decoders resolves the brittleness or if explicit reasoning-specific training data is required to bridge the performance gap with encoders.
- **What evidence would resolve it:** Ablation studies comparing standard large decoders against similarly sized models explicitly trained on chain-of-thought reasoning data to isolate the impact of training priors.

## Limitations
- The core hypothesis about encoders' global projection capability remains correlative rather than definitively proven
- Curvature similarity analysis relies on assumptions about geometric representation space that are not universally established for reasoning tasks
- The advantage of encoder architectures diminishes at very large scales, suggesting scale may compensate for architectural limitations

## Confidence
- **High Confidence:** Encoder and encoder-decoder models with fine-tuning show significantly better OOD generalization than decoder-only models across all tested scales
- **Medium Confidence:** The superiority of encoder architectures for multi-hop conjunctive reasoning is demonstrated but not definitively isolated from other architectural differences
- **Medium Confidence:** In-context learning alone is insufficient for reliable causal reasoning, though this may not hold for next-generation models with advanced chain-of-thought priors

## Next Checks
1. **Mechanism Isolation:** Design an ablation study where encoders are modified to use causal masking or decoders are given access to future tokens, keeping all other factors constant.
2. **Vocabulary Transfer Analysis:** Train encoder and decoder models on NNL data first, then evaluate on NL data (reverse OOD testing) to determine whether the advantage stems from handling OOD tokens specifically or from deeper architectural differences.
3. **Scale-Architecture Interaction:** Systematically test encoder and decoder models across more intermediate scales (e.g., 1B, 3B, 7B, 13B parameters) to precisely characterize when scale begins compensating for architectural limitations.