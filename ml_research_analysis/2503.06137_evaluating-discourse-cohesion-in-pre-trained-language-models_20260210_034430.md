---
ver: rpa2
title: Evaluating Discourse Cohesion in Pre-trained Language Models
arxiv_id: '2503.06137'
source_url: https://arxiv.org/abs/2503.06137
tags:
- cohesion
- language
- phenomena
- sentences
- discourse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a test suite to evaluate the cohesive ability
  of pre-trained language models (PLMs). It includes six cohesion phenomena: repetition,
  synonyms, ellipsis, substitution, reference, and conjunction, with examples between
  adjacent and non-adjacent sentences.'
---

# Evaluating Discourse Cohesion in Pre-trained Language Models

## Quick Facts
- **arXiv ID**: 2503.06137
- **Source URL**: https://arxiv.org/abs/2503.06137
- **Reference count**: 11
- **Primary result**: RoBERTa performs best overall in evaluating discourse cohesion phenomena in PLMs

## Executive Summary
This paper introduces a test suite to evaluate the cohesive ability of pre-trained language models (PLMs) across six discourse phenomena: repetition, synonyms, ellipsis, substitution, reference, and conjunction. The evaluation spans both adjacent and non-adjacent sentence pairs. Experimental results show that while RoBERTa demonstrates the strongest overall performance, all evaluated models (BERT, BART, RoBERTa) struggle significantly with conjunction, substitution, synonyms, and ellipsis phenomena. Notably, BART exhibits unique behavior where contextual information actually harms target word prediction performance. Attention analysis reveals that models allocate the most attention to repetition and synonyms while giving minimal attention to conjunction and discourse markers, which correlates with their poorer performance on these phenomena.

## Method Summary
The authors constructed a test suite evaluating six cohesion phenomena across adjacent and non-adjacent sentence pairs. They evaluated three PLMs (BERT, BART, RoBERTa) using a target word prediction task where models predict masked words with and without context. The evaluation measured how well context improves prediction accuracy across different discourse phenomena. Attention pattern analysis was conducted to examine how models distribute attention across different cohesion types. The study systematically compared model performance across phenomena and analyzed the relationship between attention allocation and task success rates.

## Key Results
- RoBERTa achieves the best overall performance across cohesion phenomena
- All models struggle significantly with conjunction, substitution, synonyms, and ellipsis
- Context improves target word prediction for most phenomena except BART
- Attention analysis shows repetition and synonyms receive the most attention while conjunction and discourse markers receive the least

## Why This Works (Mechanism)
The evaluation mechanism works by systematically testing PLMs' ability to handle specific discourse cohesion phenomena through controlled target word prediction tasks. By comparing performance with and without context, the study reveals how well models leverage discourse information for coherent text understanding. The attention analysis provides insight into how models process different cohesion types, showing that performance correlates with attention allocation patterns. This approach effectively isolates and measures specific aspects of discourse competence that are critical for natural language understanding.

## Foundational Learning
- **Discourse cohesion**: The linguistic elements that create semantic and pragmatic connections between sentences in text. Needed to understand how text maintains coherence. Quick check: Identify cohesive devices in a paragraph.
- **Ellipsis and substitution**: Grammatical mechanisms where elements are omitted or replaced with shorter forms while maintaining meaning. Critical for efficient communication. Quick check: Spot elliptical constructions in conversation.
- **Reference mechanisms**: How language points back to previously mentioned entities or concepts (anaphora, cataphora). Essential for tracking discourse participants. Quick check: Trace pronoun references in a text.
- **Discourse markers**: Words or phrases that signal relationships between discourse segments (however, therefore, meanwhile). Key for logical flow. Quick check: Identify discourse markers in argumentative text.
- **Attention mechanisms**: Neural network components that weight the importance of different input elements. Fundamental to transformer architecture. Quick check: Examine attention weight visualizations.

## Architecture Onboarding

**Component map**: Input text -> Tokenization -> Embedding layer -> Transformer layers (with attention heads) -> Output layer -> Prediction task

**Critical path**: Input text → Tokenization → Embedding → Transformer layers → Attention heads → Output layer → Task-specific prediction (e.g., masked word prediction)

**Design tradeoffs**: The study balances comprehensive evaluation of multiple cohesion phenomena against the complexity of isolating individual effects. Using masked word prediction as a proxy for discourse understanding simplifies the evaluation but may not capture all aspects of cohesive ability.

**Failure signatures**: Poor performance on conjunction and discourse markers indicates models struggle with logical relationships between text segments. BART's degraded performance with context suggests potential architectural limitations in handling long-range dependencies or discourse-level information.

**Three first experiments**:
1. Evaluate model performance on each cohesion phenomenon separately to identify specific weaknesses
2. Compare attention weight distributions across phenomena to correlate attention patterns with performance
3. Test whether fine-tuning on targeted cohesion examples improves performance on struggling phenomena

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Analysis focuses on adjacent and non-adjacent sentence pairs, potentially missing document-level discourse complexity
- Attention analysis based on aggregate patterns rather than fine-grained per-token attention weights
- Does not account for domain-specific training data effects or evaluate out-of-distribution text genres

## Confidence
- **High confidence**: BART's unique context-hurting behavior; RoBERTa's overall strong performance; attention patterns showing repetition/synonyms receive most attention
- **Medium confidence**: Relative difficulty ranking across cohesion phenomena; explanation of attention-performance relationship
- **Low confidence**: Absolute performance comparisons across models; generalizability to document-level discourse beyond adjacent sentences

## Next Checks
1. Conduct fine-grained attention analysis using per-token attention weights rather than aggregate patterns to verify the relationship between attention distribution and task performance
2. Evaluate model performance on discourse phenomena in out-of-domain text genres (legal, technical, creative writing) to assess robustness
3. Test whether fine-tuning on targeted discourse cohesion examples improves performance on the specific phenomena where models currently struggle (conjunction, substitution, synonyms, ellipsis)