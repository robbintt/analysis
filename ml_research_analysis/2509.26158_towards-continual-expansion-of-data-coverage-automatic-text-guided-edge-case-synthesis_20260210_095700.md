---
ver: rpa2
title: 'Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case
  Synthesis'
arxiv_id: '2509.26158'
source_url: https://arxiv.org/abs/2509.26158
tags:
- data
- naive
- dataset
- training
- manual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an automated pipeline for generating edge-case
  data to improve model robustness by leveraging a preference-tuned large language
  model (LLM) to rephrase image captions and a text-to-image (T2I) model to synthesize
  challenging visual scenarios. Evaluated on the FishEye8K object detection benchmark,
  the method achieves superior performance compared to naive augmentation and manually
  engineered prompts, both in terms of overall mAP and mAP w/o TP (a custom metric
  measuring improvements on previously undetected cases).
---

# Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis

## Quick Facts
- arXiv ID: 2509.26158
- Source URL: https://arxiv.org/abs/2509.26158
- Reference count: 40
- Primary result: Automatic edge-case synthesis achieves 0.384 mAP (vs 0.378 baseline) on FishEye8K by rephrasing captions to steer T2I toward difficult scenarios

## Executive Summary
This paper introduces an automated pipeline for generating edge-case data to improve model robustness by leveraging a preference-tuned large language model (LLM) to rephrase image captions and a text-to-image (T2I) model to synthesize challenging visual scenarios. Evaluated on the FishEye8K object detection benchmark, the method achieves superior performance compared to naive augmentation and manually engineered prompts, both in terms of overall mAP and mAP w/o TP (a custom metric measuring improvements on previously undetected cases). Specifically, mAP increases from 0.378 to 0.384, and mAP w/o TP from 0.362 to 0.366, with consistent gains maintained across model scales. The approach effectively expands training data coverage, reduces dataset bias, and automates the identification and synthesis of difficult cases, providing a scalable direction for building more reliable and continuously improving AI systems.

## Method Summary
The method constructs a self-improving loop where captions from real images are rephrased by a preference-tuned LLM to generate synthetic edge cases. The pipeline uses a base captioning model (InternVL3-38B) to extract captions from real images, which are then rephrased by Llama-3-8B-Instruct via DPO to maximize edge-ness - measured as the detection loss discrepancy between the discriminative model and pseudo-labels. These rephrased captions are rendered by a T2I model (Flux.1-dev) into synthetic images, which are pseudo-labeled and used to construct preference pairs for LLM fine-tuning. The augmented dataset is then merged with the original training data and used to retrain the object detector, with the option to iterate this process for progressive coverage expansion.

## Key Results
- Automatic prompt generation outperforms both naive augmentation (0.384 vs 0.378 mAP) and manually engineered prompts (0.384 vs 0.379 mAP)
- mAP w/o TP improves from 0.362 to 0.366, indicating effective correction of previously undetected cases
- Gains are consistent across model scales (YOLOv11-small, medium, large)
- Two iterations of augmentation yield further improvements (v2: 0.384 mAP vs v1: 0.381 mAP)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference-tuned LLM learns to generate captions that systematically produce high-loss images for the discriminative model
- Mechanism: DPO optimizes the LLM to preferentially output captions that, when rendered by T2I, yield synthetic images where the discriminative model's predictions diverge from pseudo-labels (high task loss = high "edge-ness")
- Core assumption: Task-specific loss on pseudo-labeled synthetic images is a reliable proxy for model blind spots
- Evidence anchors:
  - [abstract] "fine-tuned via preference learning, to rephrase image captions into diverse textual prompts that steer a Text-to-Image model toward generating difficult visual scenarios"
  - [section 2.2] "A higher value of s_i is interpreted as an indicator of edge-ness, as it signifies a greater discrepancy between the discriminative model's predictions and the pseudo annotations."
  - [section K, Table 6] Shows automatic v1 yields highest mean/median edge-ness (2.724/2.656) vs naive/manual

### Mechanism 2
- Claim: Caption-level semantic manipulation induces greater image diversity than latent-space perturbation
- Mechanism: The LLM rephrases captions by altering scene context, camera angles, lighting, and narrative style, which T2I models render as compositional variations beyond what naive seed variation achieves
- Core assumption: T2I models are sensitive to semantic changes in captions and can render them faithfully
- Evidence anchors:
  - [section 1] "simple semantic modifications to captions can induce substantial and meaningful diversity in the generated images"
  - [section 3.3, Figure 5] UMAP shows automatic generates broader embedding spread and shifts mode toward sparse regions of real data
  - [section I] Qualitative analysis shows LLM learns to enhance atmosphere, specify perspective, enrich urban context (e.g., "golden morning light" vs "daytime")

### Mechanism 3
- Claim: Iterative re-training creates progressive coverage expansion
- Mechanism: After augmentation, the updated discriminative model becomes the new edge-ness scorer, discovering harder blind spots in subsequent iterations
- Core assumption: Blind spots evolve and new weaknesses emerge as others are addressed
- Evidence anchors:
  - [section 2.3] "this entire pipeline can be executed iteratively... the system can progressively discover and synthesize increasingly complex edge-cases"
  - [section 3.2, Table 1] automatic v2 improves over automatic v1 (0.384 vs 0.381 mAP, 0.366 vs 0.363 mAP w/o TP)

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: Core algorithm for aligning LLM to edge-case generation without explicit reward modeling
  - Quick check question: Can you explain how DPO differs from RLHF in its treatment of the reference model?

- Concept: **Pseudo-labeling with Teacher Models**
  - Why needed here: Synthetic images lack ground-truth; pseudo-labeler provides supervision for both preference construction and final augmentation
  - Quick check question: What failure modes occur when pseudo-labeler and discriminative model share the same biases?

- Concept: **mAP and Custom Evaluation Metrics**
  - Why needed here: Standard mAP can mask improvements on rare cases; mAP w/o TP isolates blind-spot correction
  - Quick check question: Why does excluding baseline true positives make improvements on weak instances more detectable?

## Architecture Onboarding

- Component map: Image → Captioning (InternVL3-38B) → Base Caption → Rephrasing LLM (Llama-3-8B + DPO) → N Caption Variants → T2I (Flux.1-dev) → N Synthetic Images → Pseudo-labeler (Co-DETR) → Discriminative Model (YOLOv11) → Task Loss (Edge-ness Scoring) → Preference Pair Construction → DPO Fine-tuning → Augmented Dataset

- Critical path:
  1. Preference dataset construction (requires train-R split, caption generation, synthesis, pseudo-labeling, loss computation)
  2. DPO fine-tuning of LLM (requires preference pairs, LoRA setup)
  3. Inference-time augmentation (requires train-D captions, preference-tuned LLM, T2I generation, pseudo-labeling)

- Design tradeoffs:
  - N=5 variants balances diversity vs. compute cost (more variants = more synthesis/labeling overhead)
  - Caption-level manipulation trades latent-space control for interpretability and T2I compatibility
  - Pseudo-labeler quality caps synthetic data utility; noisy labels may mislead preference learning

- Failure signatures:
  - Mode collapse: If LLM learns repetitive rephrasings, check preference dataset diversity and DPO beta value
  - Low edge-ness: If synthetic images are too easy, verify pseudo-labeler-discriminative model gap isn't collapsed
  - Semantic drift: If captions diverge from plausible scenes, constrain prompt templates more strictly

- First 3 experiments:
  1. **Baseline comparison**: Reproduce naive vs. manual vs. automatic on FishEye8K subset to validate pipeline implementation
  2. **Ablation on N**: Test N=3,5,10 to measure diversity-compute tradeoff on held-out mAP w/o TP
  3. **Transferability check**: Train YOLOv11-medium on data curated by YOLOv11-small to confirm cross-scale validity (as in Appendix H)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would alternative uncertainty measures for edge-ness (e.g., ensemble disagreement, entropy-based scores, or epistemic uncertainty estimates) outperform the task-specific training loss used in this work?
- Basis in paper: [explicit] "For future work, we plan to... explore alternative uncertainty measures for edge-ness."
- Why unresolved: The paper only validates task loss as the edge-ness criterion. Different uncertainty quantification methods may capture distinct aspects of model blind spots.
- What evidence would resolve it: Systematic comparison of multiple edge-ness definitions on the same benchmark, measuring both final mAP gains and correlation with human-judged difficulty.

### Open Question 2
- Question: To what extent does the performance gain generalize beyond fisheye object detection to other domains (e.g., standard perspective cameras, medical imaging, satellite imagery) and tasks (e.g., instance segmentation, 3D detection)?
- Basis in paper: [explicit] "we plan to extend our study to a broader range of datasets"
- Why unresolved: FishEye8K has specific distortion characteristics and known scene biases; it remains unclear whether caption-based edge-case synthesis transfers to domains with different failure mode structures or where captions are less descriptive.
- What evidence would resolve it: Evaluation on at least 2–3 diverse benchmarks (e.g., COCO, Cityscapes, a medical imaging dataset) showing consistent mAP w/o TP improvements.

### Open Question 3
- Question: Can bias-aware fine-tuning of the T2I model and pseudo-labeler yield additional robustness gains compared to using off-the-shelf components?
- Basis in paper: [explicit] "considering dataset bias even during the fine-tuning of T2I models and pseudo-labelers could have improved performance."
- Why unresolved: The current pipeline uses a T2I model fine-tuned on train-D and a pseudo-labeler trained on train-D, both potentially inheriting dataset biases that may limit the diversity or correctness of synthesized edge cases.
- What evidence would resolve it: Ablation study comparing current setup against T2I/pseudo-labeler variants explicitly debiased (e.g., via re-sampling, adversarial debiasing, or balanced fine-tuning), with analysis of generated sample diversity and pseudo-label accuracy.

### Open Question 4
- Question: How many iterations of the self-improving loop can be applied before diminishing returns or negative transfer occur, and does the marginal benefit per iteration vary across model scales?
- Basis in paper: [inferred] The experiments only report up to two augmentation iterations (v1, v2); the iterative mechanism is a core design but its scalability is uncharacterized.
- Why unresolved: Without longer-horizon studies, it is unknown whether the loop stabilizes, plateaus, or degrades due to drift from real data distribution or accumulated pseudo-label noise.
- What evidence would resolve it: Run 5+ iterations with per-iteration metrics (mAP, mAP w/o TP, edge-ness distribution, pseudo-label quality) across small/medium/large model variants.

## Limitations

- Evaluation limited to single domain (fisheye object detection) without validation on other benchmarks
- Pseudo-labeler accuracy directly constrains synthetic data quality; systematic labeling errors propagate to preference learning
- N=5 caption variants represents computational compromise that may not capture full diversity space
- mAP w/o TP uses unusually strict IoU threshold (0.95) that may not reflect practical detection requirements

## Confidence

**High Confidence:** The core pipeline components (DPO preference tuning, iterative re-training, and synthetic data augmentation) are technically sound and the reported mAP improvements are statistically significant on the test set.

**Medium Confidence:** The superiority of automatic prompt generation over manual engineering is demonstrated, but evaluation only compares against a single baseline method.

**Low Confidence:** The claim that this approach provides a "scalable direction for building continuously improving AI systems" extrapolates beyond the single-iteration evaluation.

## Next Checks

1. **Cross-domain Transfer:** Apply the pipeline to a different object detection benchmark (e.g., COCO, BDD100K) to test generalization and assess whether the same preference-tuning approach works without domain-specific modifications.

2. **Pseudo-labeler Quality Analysis:** Conduct systematic evaluation of Co-DETR's accuracy on synthetic images across object categories and poses, correlating pseudo-label error rates with preference learning effectiveness.

3. **Latent-space vs. Caption Manipulation Comparison:** Implement direct comparison where both caption rephrasing and latent-space perturbations are applied to the same base captions, measuring diversity metrics and downstream task performance to quantify relative contributions.