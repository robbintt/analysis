---
ver: rpa2
title: 'Enhancing Sentiment Analysis through Multimodal Fusion: A BERT-DINOv2 Approach'
arxiv_id: '2503.07943'
source_url: https://arxiv.org/abs/2503.07943
tags:
- fusion
- sentiment
- multimodal
- bert
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a novel multimodal sentiment analysis architecture
  that integrates text and image data to enhance sentiment understanding. It employs
  BERT for text feature extraction and DINOv2 for image feature extraction, then fuses
  these features using three proposed methods: Basic Fusion Model, Self-Attention
  Fusion Model, and Dual-Attention Fusion Model.'
---

# Enhancing Sentiment Analysis through Multimodal Fusion: A BERT-DINOv2 Approach

## Quick Facts
- **arXiv ID:** 2503.07943
- **Source URL:** https://arxiv.org/abs/2503.07943
- **Reference count:** 28
- **Primary result:** Proposed multimodal sentiment analysis architecture achieves state-of-the-art performance on MVSA-single dataset with accuracy of 0.73 and F1 score of 0.71

## Executive Summary
This paper presents a novel multimodal sentiment analysis architecture that integrates text and image data to enhance sentiment understanding. The approach employs BERT for text feature extraction and DINOv2 for image feature extraction, then fuses these features using three proposed methods: Basic Fusion Model, Self-Attention Fusion Model, and Dual-Attention Fusion Model. The model is evaluated on three datasets (Memotion 7k, MVSA-single, and MVSA-multi) and achieves state-of-the-art performance on the MVSA-single dataset. The study demonstrates the effectiveness of combining BERT and DINOv2 through attention-based fusion techniques for multimodal sentiment analysis.

## Method Summary
The proposed architecture extracts text features using BERT and image features using DINOv2, then fuses these multimodal representations through three different approaches. The Basic Fusion Model performs simple concatenation of features, while the Self-Attention Fusion Model applies self-attention mechanisms to capture intra-modal relationships. The Dual-Attention Fusion Model further incorporates cross-modal attention to model interactions between text and image features. These fused representations are then passed through classification layers to predict sentiment. The model is trained end-to-end and evaluated across three benchmark datasets for multimodal sentiment analysis.

## Key Results
- Achieves state-of-the-art performance on MVSA-single dataset with accuracy of 0.73 and F1 score of 0.71
- Dual-Attention Fusion Model achieves macro F1 score of 0.3552 on Memotion 7k dataset, outperforming previous state-of-the-art models
- Demonstrates effectiveness of BERT-DINOv2 integration through attention-based fusion techniques across multiple datasets

## Why This Works (Mechanism)
The architecture leverages the complementary strengths of BERT for contextual text understanding and DINOv2 for rich visual feature extraction. BERT captures semantic and syntactic nuances in text through self-attention mechanisms, while DINOv2 provides robust visual representations trained on large-scale image data. The fusion mechanisms, particularly the attention-based approaches, enable the model to dynamically weigh and combine information from both modalities based on their relevance to the sentiment classification task. The Dual-Attention Fusion Model further enhances this by modeling cross-modal interactions, allowing the system to capture complex relationships between visual and textual elements that contribute to sentiment expression.

## Foundational Learning
- **BERT (Bidirectional Encoder Representations from Transformers):** Pre-trained language model that captures contextual word representations through bidirectional attention. Needed for robust text feature extraction in multimodal settings. Quick check: Verify BERT layer outputs capture semantic relationships in text data.
- **DINOv2 (Self-Distilled Vision Transformer):** Self-supervised vision model that provides rich image features without requiring labeled training data. Needed for extracting meaningful visual representations in sentiment analysis. Quick check: Confirm DINOv2 features encode relevant visual attributes for sentiment tasks.
- **Self-Attention Mechanisms:** Neural network component that weighs the importance of different elements in a sequence. Needed for capturing both intra-modal relationships within text or image features and inter-modal relationships between modalities. Quick check: Validate attention weights highlight relevant features for sentiment classification.
- **Multimodal Fusion Strategies:** Techniques for combining information from different modalities. Needed to integrate complementary information from text and images for enhanced sentiment understanding. Quick check: Compare performance of different fusion approaches on validation sets.
- **Attention-Based Fusion:** Methods that use attention mechanisms to dynamically combine multimodal features. Needed to adaptively weight contributions from text and image based on their relevance to the task. Quick check: Analyze attention weight distributions to ensure meaningful modality weighting.

## Architecture Onboarding

**Component Map:** Text Data -> BERT -> Text Features -> Fusion Module <- Image Features <- DINOv2 <- Image Data -> Classification Layer -> Sentiment Prediction

**Critical Path:** Image Data → DINOv2 → Image Features → Fusion Module → Classification Layer → Sentiment Prediction

**Design Tradeoffs:** The architecture balances between simple concatenation (Basic Fusion) and complex attention mechanisms (Self-Attention and Dual-Attention). While attention-based methods provide better performance by modeling complex relationships, they also increase computational complexity and may require more training data to prevent overfitting.

**Failure Signatures:** Performance degradation on datasets with complex multimodal interactions (evidenced by lower scores on Memotion 7k compared to MVSA-single). Potential failure modes include: (1) Misalignment between visual and textual sentiment cues, (2) Over-reliance on one modality when both are needed, (3) Inability to capture nuanced multimodal sarcasm or irony common in memes.

**3 First Experiments:**
1. Replace DINOv2 with a simpler image feature extractor (e.g., ResNet) to assess the contribution of advanced visual representations
2. Test the model on a controlled dataset where text and image sentiment are perfectly aligned vs. misaligned to evaluate fusion effectiveness
3. Perform cross-dataset evaluation by training on one dataset and testing on another to assess generalizability

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Significant performance gap across datasets (high on MVSA-single, lower on Memotion 7k) suggests potential generalizability issues
- Lack of detailed ablation studies makes it difficult to assess individual contributions of BERT, DINOv2, and fusion mechanisms
- Evaluation focuses primarily on accuracy and F1 scores without exploring precision-recall curves or confusion matrices that could reveal systematic biases

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| BERT and DINOv2 effectively extract features from text and images respectively | High |
| Proposed fusion methods improve multimodal sentiment analysis performance | Medium |
| Achieving "state-of-the-art" performance across all evaluated datasets | Low |

## Next Checks
1. Conduct comprehensive ablation studies to isolate the contributions of BERT feature extraction, DINOv2 feature extraction, and each fusion method to overall performance
2. Evaluate the model on additional multimodal sentiment analysis datasets beyond the three tested, particularly datasets with different characteristics
3. Perform error analysis using confusion matrices and precision-recall curves to identify specific sentiment categories or types of multimodal interactions where the model struggles