---
ver: rpa2
title: Offline Contextual Bandit with Counterfactual Sample Identification
arxiv_id: '2509.10520'
source_url: https://arxiv.org/abs/2509.10520
tags:
- policy
- action
- context
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses confounding in offline contextual bandit
  learning, where context features can bias the estimation of action rewards. The
  proposed Counterfactual Sample Identification (CSI) method reframes the problem:
  instead of directly predicting rewards, it learns to identify which action led to
  a successful outcome by comparing it to a counterfactual action sampled from the
  logging policy under the same context.'
---

# Offline Contextual Bandit with Counterfactual Sample Identification

## Quick Facts
- arXiv ID: 2509.10520
- Source URL: https://arxiv.org/abs/2509.10520
- Authors: Alexandre Gilotte; Otmane Sakhi; Imad Aouali; Benjamin Heymann
- Reference count: 27
- Key outcome: CSI improves CTR by 0.5-1% over DM baseline in production banner optimization

## Executive Summary
This paper addresses confounding in offline contextual bandit learning, where context features can bias the estimation of action rewards. The proposed Counterfactual Sample Identification (CSI) method reframes the problem: instead of directly predicting rewards, it learns to identify which action led to a successful outcome by comparing it to a counterfactual action sampled from the logging policy under the same context. This approach effectively models the multiplicative advantage of actions while reducing confounding effects from context. Experiments show CSI consistently outperforms direct reward modeling (DM) in synthetic environments, particularly as sample size increases. On a large-scale production system for banner design optimization, CSI improved click-through rates by 0.5-1% over the DM baseline, with similar performance even when using a reduced feature set. The method provides a practical alternative to IPS-based approaches while maintaining theoretical grounding.

## Method Summary
The Counterfactual Sample Identification (CSI) method addresses confounding in offline contextual bandit learning by reframing the reward prediction problem as an action identification task. Instead of directly predicting rewards for each action, CSI learns to identify which action led to a successful outcome by comparing it to a counterfactual action sampled from the logging policy under the same context. This approach effectively models the multiplicative advantage of actions while reducing confounding effects from context. The method uses a classification framework where the model predicts whether the logged action or the counterfactual action resulted in the observed outcome, enabling more robust estimation of action effectiveness.

## Key Results
- CSI consistently outperforms direct reward modeling (DM) in synthetic environments, particularly as sample size increases
- On a large-scale production banner optimization system, CSI improved click-through rates by 0.5-1% over the DM baseline
- CSI maintained similar performance even when using a reduced feature set compared to the full context

## Why This Works (Mechanism)
The CSI method works by reframing the reward prediction problem to focus on action identification rather than direct reward estimation. By comparing the logged action to a counterfactual action sampled from the logging policy under the same context, the model learns to identify which action led to success while implicitly controlling for context effects. This approach leverages the multiplicative advantage structure of contextual bandits, where the effectiveness of an action depends on both its inherent quality and how well it matches the context. By training on pairs of actions (logged vs. counterfactual) for the same context, the model learns to isolate the action's contribution to the outcome, reducing confounding from context features that might otherwise bias direct reward predictions.

## Foundational Learning
- **Contextual Bandits**: Framework where actions are chosen based on context features; needed to understand the problem setting
- **Confounding in Offline Learning**: When context features correlate with both actions and outcomes; quick check: verify correlation between context and action distributions
- **Counterfactual Reasoning**: Estimating what would have happened under different actions; quick check: validate counterfactual sampling quality
- **Multiplicative Advantage**: Action effectiveness depends on both action quality and context matching; quick check: test sensitivity to context changes
- **Off-Policy Evaluation**: Estimating policy performance from historical data; quick check: compare to known baselines
- **Classification vs. Regression**: Different approaches to learning from bandit feedback; quick check: evaluate both methods on same data

## Architecture Onboarding

**Component Map**
Logging Policy -> Data Collection -> Context-Action Pairs -> CSI Model -> Action Identification -> Reward Estimation

**Critical Path**
Context features → Counterfactual action sampling → Action pair generation → CSI classification → Reward inference

**Design Tradeoffs**
- Direct reward modeling vs. action identification approach
- Full context features vs. reduced feature sets
- Deterministic vs. stochastic logging policies

**Failure Signatures**
- Poor counterfactual sampling quality
- High correlation between context and actions
- Model overfitting to specific action patterns

**Three First Experiments**
1. Synthetic environment with controlled confounding to validate core mechanism
2. Ablation study on feature set size and composition
3. Comparison with IPS-based methods on same production dataset

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- Reliance on counterfactual sampling from logging policy may not capture full action distribution
- Method effectiveness may degrade with highly deterministic logging policies or extremely large action spaces
- Scalability to high-dimensional contexts not thoroughly addressed
- Limited analysis of behavior when logging policy changes over time

## Confidence
- High confidence in method's ability to reduce confounding effects in controlled synthetic environments
- Medium confidence in scalability and robustness to real-world production systems with complex contexts
- Low confidence in generalizability to scenarios with highly deterministic logging policies or extremely large action spaces

## Next Checks
1. Evaluate performance in scenarios with highly deterministic logging policies and large action spaces to assess robustness
2. Test scalability with high-dimensional contexts and compare to state-of-the-art methods in offline contextual bandit problems
3. Conduct analysis of method's behavior when logging policy changes over time, ensuring adaptability and stability in dynamic environments