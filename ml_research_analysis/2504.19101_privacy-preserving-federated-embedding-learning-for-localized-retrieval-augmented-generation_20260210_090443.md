---
ver: rpa2
title: Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented
  Generation
arxiv_id: '2504.19101'
source_url: https://arxiv.org/abs/2504.19101
tags:
- data
- uni00000013
- learning
- federated
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FedE4RAG, a privacy-preserving federated learning
  framework for localized Retrieval-Augmented Generation (RAG) systems. It addresses
  the challenge of training RAG retrievers on distributed private data without exposing
  sensitive information.
---

# Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID**: 2504.19101
- **Source URL**: https://arxiv.org/abs/2504.19101
- **Reference count**: 40
- **Primary result**: FedE4RAG achieves up to 90.22% MAP in retrieval tasks while preserving data privacy through homomorphic encryption.

## Executive Summary
FedE4RAG is a privacy-preserving federated learning framework designed to train RAG retriever models on distributed private data without exposing sensitive information. The system uses homomorphic encryption to protect model parameters during federated communication and knowledge distillation to improve local retriever generalization by transferring global knowledge. Evaluated on a financial domain dataset, FedE4RAG significantly outperformed baselines, achieving strong performance in both retrieval and generation tasks while maintaining data privacy.

## Method Summary
FedE4RAG implements privacy-preserving federated learning for localized RAG systems by combining homomorphic encryption, knowledge distillation, and contrastive fine-tuning. The framework distributes a pre-trained embedding model to clients, who train on local data while encrypting gradients for secure aggregation. Knowledge distillation aligns local and global model representations to address non-IID data challenges. The system was evaluated on FedE4FIN (43,658 synthetic query-chunk pairs) and RAG4FIN (100 queries, 24k pages) datasets using BAAI/bge-base-en-v1.5 architecture with InfoNCE and distillation losses, achieving strong retrieval and generation performance.

## Key Results
- FedE4RAG achieved 90.22% MAP in retrieval tasks on the financial domain dataset
- Significantly outperformed baselines SCenT and SVanE1.5base in both upstream retrieval and downstream generation tasks
- Demonstrated effective balance between data privacy and model performance in localized RAG deployments

## Why This Works (Mechanism)

### Mechanism 1: Privacy-Preserving Federated Aggregation
FedE4RAG enables collaborative training without exposing raw data through CKKS homomorphic encryption applied to local model gradients before transmission. The server aggregates encrypted gradients directly, ensuring only the final global model is decrypted. This prevents server or eavesdroppers from accessing individual client updates, assuming honest-but-curious participants and secure channels.

### Mechanism 2: Knowledge Distillation for Global-Local Knowledge Transfer
Knowledge distillation mitigates performance degradation from non-IID data by aligning local model similarity scores with the global model during training. This transfers "global knowledge" learned from all clients' data back to local models, correcting client drift caused by heterogeneous data distributions. The approach assumes the global model captures meaningful generalization across clients.

### Mechanism 3: Contrastive Fine-Tuning for RAG Retrieval
The RAG-specific fine-tuning module enhances embedding models' retrieval ability through synthetic query-chunk pairs and InfoNCE loss. The model learns to maximize similarity between queries and their corresponding golden contexts while minimizing similarity with negative samples. This directly optimizes the retriever for the downstream RAG task, assuming synthetic questions are sufficiently representative of real-world queries.

## Foundational Learning

- **Homomorphic Encryption**: Why needed? To securely aggregate model updates without the server seeing raw gradients that could leak private data. Quick check: Can you perform mathematical operations on encrypted data and decrypt the result to get the same answer as operations on raw data first?

- **Knowledge Distillation**: Why needed? To transfer knowledge from a generalized global model to local models without sharing original training data, helping local models generalize better. Quick check: How does training a student to mimic a teacher's output probabilities help it learn, even with different architectures?

- **Non-IID Data**: Why needed? In federated learning, data varies across clients (e.g., different banks), causing "client drift" that makes global aggregation difficult. FedE4RAG's distillation combats this. Quick check: If you average models trained on only dog photos and only cat photos, will the result classify both well? Why or why not?

## Architecture Onboarding

- **Component map**: Local Client (documents, vector DB, local retriever, LLM) -> Federated Server (coordinates training, aggregates encrypted gradients) -> FedE4RAG Modules (RAG-FT, KD-GLE, FED-HE on each client)

- **Critical path**: 1) Server distributes pre-trained BGE-base model to all clients. 2) Each client generates synthetic (query, chunk) pairs from local documents. 3) Clients train embedding model using InfoNCE + distillation loss against global model. 4) Clients encrypt gradients via CKKS and send to server. 5) Server aggregates encrypted gradients and updates global model. 6) Server distributes updated model to clients. 7) Clients use fine-tuned local models for private RAG inference.

- **Design tradeoffs**: Privacy vs. efficiency (HE adds computational overhead), global generalization vs. local personalization (distillation balances fit), synthetic data quality (depends on GPT-4o mini generation quality).

- **Failure signatures**: Degraded retrieval performance (check synthetic data or distillation weighting), non-convergence (verify data distributions or HE implementation), encryption/decryption errors (mismatched HE parameters), privacy breach concerns (membership inference attacks on final outputs).

- **First 3 experiments**: 1) Baseline ablation comparing FedE4RAG against local-only training and standard FedAvg to isolate distillation and data volume contributions. 2) Scaling test measuring training time and communication costs as clients increase from 5 to 50. 3) Data heterogeneity simulation measuring performance gap between FedE4RAG and centralized training under increasingly non-IID distributions.

## Open Questions the Paper Calls Out

1. **Active Adversary Resilience**: How resilient is FedE4RAG against active malicious adversaries or client collusion, given the "honest-but-curious" threat assumption? The paper excludes this scenario, so behavior under poisoning attacks remains untested.

2. **Differential Privacy Integration**: Can differential privacy be effectively integrated to enhance guarantees without degrading retrieval utility? The paper lists this as future work, with unknown trade-offs from DP noise on contrastive learning objectives.

3. **Scalability Optimization**: How can communication and computational efficiency be optimized for large-scale deployments beyond the five clients tested? The paper identifies this as a future direction, with current HE overhead as a bottleneck for hundreds of clients.

## Limitations

- Computational overhead from homomorphic encryption is "acceptable" per the paper but lacks concrete timing benchmarks for larger models or more clients
- Framework effectiveness depends heavily on synthetic data quality, which could degrade retrieval performance regardless of training approach
- Security guarantees assume "honest-but-curious" participants and exclude active malicious actors or collusion scenarios

## Confidence

- **High Confidence**: Core mechanism of homomorphic encryption for secure gradient aggregation and overall federated training framework
- **Medium Confidence**: Knowledge distillation effectiveness in mitigating non-IID data challenges in this specific RAG context
- **Low Confidence**: Claims about handling "real-world" privacy requirements without additional security measures for active adversaries

## Next Checks

1. **Ablation Study on Distillation Weight**: Systematically vary the distillation loss weight (Î») to identify optimal balance between local personalization and global generalization for different data heterogeneity levels

2. **Scaling Analysis with Client Count**: Measure training time per round and communication costs as clients increase from 5 to 50 to quantify practical limits of homomorphic encryption overhead

3. **Adversarial Robustness Test**: Evaluate vulnerability to membership inference attacks by training an attack model to predict whether specific documents were in client training sets, even with encrypted gradients