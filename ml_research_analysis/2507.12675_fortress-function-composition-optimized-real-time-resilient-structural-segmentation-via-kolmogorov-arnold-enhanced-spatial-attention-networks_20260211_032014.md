---
ver: rpa2
title: 'FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation
  via Kolmogorov-Arnold Enhanced Spatial Attention Networks'
arxiv_id: '2507.12675'
source_url: https://arxiv.org/abs/2507.12675
tags:
- fortress
- defect
- segmentation
- attention
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FORTRESS, a novel deep learning architecture
  for automated structural defect segmentation in civil infrastructure that addresses
  the challenge of achieving high accuracy while maintaining real-time computational
  efficiency. The proposed approach combines depthwise separable convolutions with
  adaptive Kolmogorov-Arnold Network (KAN) integration to create a parameter-efficient
  and computationally lightweight model.
---

# FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks

## Quick Facts
- **arXiv ID**: 2507.12675
- **Source URL**: https://arxiv.org/abs/2507.12675
- **Reference count**: 40
- **Key outcome**: Introduces FORTRESS, achieving F1=0.771 and mIoU=0.677 with 91% parameter reduction and 3× faster inference on structural defect segmentation

## Executive Summary
FORTRESS addresses the challenge of achieving high accuracy while maintaining real-time computational efficiency in automated structural defect segmentation for civil infrastructure. The proposed approach combines depthwise separable convolutions with adaptive Kolmogorov-Arnold Network (KAN) integration to create a parameter-efficient and computationally lightweight model. The architecture achieves state-of-the-art performance on benchmark infrastructure datasets while requiring only 2.9 million parameters and 1.17 GFLOPs.

## Method Summary
FORTRESS is a U-Net style deep learning architecture that leverages depthwise separable convolutions and adaptive TiKAN integration for efficient structural defect segmentation. The model uses a 5-level encoder-decoder structure with KANDoubleConv blocks that combine depthwise separable convolutions with optional TiKAN enhancement. TiKAN modules are activated conditionally based on channel dimension and spatial resolution thresholds. Multi-scale attention fusion combines spatial, channel, and KAN-enhanced features, with deep supervision at intermediate decoder levels. The model is trained using AdamW optimizer with cosine annealing schedule and multi-scale cross-entropy loss with class weighting.

## Key Results
- Achieves F1-score of 0.771 and mean IoU of 0.677 on benchmark infrastructure datasets
- Requires only 2.9 million parameters (91% reduction compared to conventional U-Net)
- Inference speed is 3× faster than conventional approaches with 1.17 GFLOPs
- Demonstrates superior performance and generalization across diverse infrastructure inspection scenarios

## Why This Works (Mechanism)

### Mechanism 1: Depthwise Separable Convolutions
- **Claim**: Reduces per-layer parameter count and computational cost by factorizing standard convolutions into depthwise and pointwise operations
- **Mechanism**: Standard 3x3 convolution with Cin input and Cout output channels has parameter cost Cin×Cout×9. Depthwise separable convolution replaces this with depthwise convolution (Cin×9) followed by pointwise 1x1 convolution (Cin×Cout), achieving ~9× reduction when Cout is large
- **Core assumption**: Factorized operations preserve sufficient representational capacity for structural defect patterns
- **Evidence anchors**: Abstract mentions 3.6× parameter reduction per layer; Section IV Eq. 12 derives reduction factor 9·Cout/(9+Cout)
- **Break condition**: If defect patterns require dense, non-factorizable spatial-channel correlations beyond what depthwise+pointwise can capture

### Mechanism 2: Adaptive TiKAN Integration
- **Claim**: Selectively applies function-composition transformations only when computationally beneficial
- **Mechanism**: TiKAN modules activated conditionally when Cin≥16 and H×W≤1024, applying spline-based learnable univariate functions to model complex patterns
- **Core assumption**: Lower-resolution features benefit more from function-composition transformations
- **Evidence anchors**: Abstract mentions adaptive TiKAN integration; Section IV Eq. 19 shows explicit activation criterion
- **Break condition**: If important defect patterns at higher resolutions require function composition, adaptive gate could omit useful transformations

### Mechanism 3: Multi-Scale Attention Fusion
- **Claim**: Improves defect boundary delineation and multi-scale pattern recognition
- **Mechanism**: Fuses spatial attention, channel attention, and TiKAN-enhanced features at each decoder level via learnable 1×1 convolution weight matrix
- **Core assumption**: Defects exhibit patterns at multiple scales better captured by combining attention types
- **Evidence anchors**: Abstract mentions multi-scale attention fusion; Section IV Eq. 24 shows fusion formulation
- **Break condition**: If attention modalities provide redundant or conflicting signals, fusion weights may not learn effective combinations

## Foundational Learning

- **Concept: Depthwise Separable Convolutions**
  - **Why needed here**: FORTRESS relies on this factorization for primary efficiency gains
  - **Quick check question**: Given input with 64 channels and output with 128 channels, what is the parameter reduction ratio of 3×3 depthwise separable convolution versus standard 3×3 convolution?

- **Concept: Kolmogorov-Arnold Networks (KANs)**
  - **Why needed here**: TiKAN is a core architectural novelty using learnable univariate functions instead of fixed activations
  - **Quick check question**: How does Kolmogorov-Arnold representation theorem justify using compositions of univariate functions for multivariate function approximation, and how does this differ from standard layer-wise linear transformations + ReLU?

- **Concept: Spatial and Channel Attention**
  - **Why needed here**: FORTRESS fuses both attention types with TiKAN features
  - **Quick check question**: For feature map of size 32×32×64, how does spatial attention compute 32×32 attention map, and how does channel attention compute 64-dimensional channel-wise weighting?

## Architecture Onboarding

- **Component map**: Input image (256×256×3) → encoder levels E1–E5 → decoder levels D1–D4 → final segmentation map
- **Critical path**: Input → encoder (H×W halved, channels doubled per level) → decoder (upsampling + skip connections + attention fusion) → deep supervision heads at decoder levels 2, 3, 4 → final output
- **Design tradeoffs**: Adaptive TiKAN thresholds balance accuracy vs efficiency; attention complexity adds parameters but improves performance; deep supervision weighting requires tuning
- **Failure signatures**: Over-smoothed boundaries if spatial attention kernels too small; underfitting on rare defects despite class weighting; TiKAN not activating if conditions not met; attention not improving if outputs are near-uniform
- **First 3 experiments**: 1) Baseline reproduction on CSDD with default hyperparameters, 2) Ablation of adaptive TiKAN comparing always-on/off/default variants, 3) Attention fusion simplification removing one modality at a time

## Open Questions the Paper Calls Out

### Open Question 1: Dynamic and Self-Adaptive KAN Integration
- Question: Can fixed heuristic thresholds for TiKAN activation (channels ≥ 16, resolution ≤ 1024) be replaced by learnable, self-adaptive mechanism?
- Basis: Section VIII explicitly lists "Dynamic and Self-Adaptive KAN Integration" as future direction
- Why unresolved: Current implementation uses hard-coded logic; requires differentiable gating mechanism or reinforcement learning policy
- Evidence needed: Modified FORTRESS with learned thresholds demonstrating equal/better accuracy with further reduced computational overhead

### Open Question 2: Extension to Multi-Modal and Temporal Data
- Question: Does FORTRESS generalize to thermal imagery, LiDAR, or temporal data streams?
- Basis: Section VIII identifies "Extension to Multi-Modal and Temporal Data" as promising direction
- Why unresolved: Current study restricted to static RGB images; unknown if modules robust to thermal noise or temporal inconsistencies
- Evidence needed: Successful training and evaluation on multi-sensor datasets and video datasets showing stable temporal segmentation

### Open Question 3: Explainability and Interpretable Defect Analysis
- Question: Can learned spline functions within TiKAN modules be translated into actionable physical insights regarding defect morphology?
- Basis: Section VIII highlights need for "Explainability and Interpretable Defect Analysis"
- Why unresolved: Mapping spline parameters to physical defect characteristics remains unsolved visualization problem
- Evidence needed: Visualization tool correlating spline activation patterns with distinct defect features, validated by domain experts

## Limitations
- Adaptive TiKAN activation thresholds are fixed hyperparameters without systematic ablation
- Dynamic Label Injection augmentation mechanism is described but implementation details are sparse
- Multi-attention fusion is novel but lacks external validation beyond paper's datasets
- Claims of generalization to all infrastructure inspection scenarios may be overstated

## Confidence
- **High Confidence**: Depthwise separable convolution efficiency claims (3.6× parameter reduction, 91% total reduction) are mathematically sound and well-supported
- **Medium Confidence**: Adaptive TiKAN gating and multi-scale attention fusion are core innovations with limited external validation
- **Medium Confidence**: State-of-the-art performance claims valid on tested datasets but may not generalize to all scenarios

## Next Checks
1. **TiKAN gating ablation**: Compare FORTRESS with three variants (TiKAN always off, always on, adaptive) to measure accuracy-efficiency tradeoff
2. **Attention modality contribution**: Remove each attention type individually and compare to full fusion to identify key contributors
3. **Cross-dataset generalization**: Test FORTRESS on additional infrastructure datasets (bridge, tunnel imagery) to validate claims of robustness to real-world scenarios