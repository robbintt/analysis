---
ver: rpa2
title: 'WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench'
arxiv_id: '2506.12841'
source_url: https://arxiv.org/abs/2506.12841
tags:
- game
- sheriff
- werewolf
- pool
- roles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces WereWolf-Plus, a benchmarking platform for
  evaluating LLM agents in the Werewolf game. The platform addresses limitations in
  existing benchmarks by supporting customizable roles (Seer, Witch, Hunter, Guard,
  Sheriff), enabling heterogeneous model assignments, and introducing comprehensive
  evaluation metrics.
---

# WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench

## Quick Facts
- arXiv ID: 2506.12841
- Source URL: https://arxiv.org/abs/2506.12841
- Reference count: 5
- Primary result: Experience-retrieval augmentation improves voting decisions and social influence in multi-agent Werewolf gameplay

## Executive Summary
WereWolf-Plus introduces a benchmarking platform for evaluating large language model agents in the social deduction game Werewolf. The platform addresses limitations in existing benchmarks by supporting customizable roles, enabling heterogeneous model assignments, and introducing comprehensive evaluation metrics. A key innovation is the experience-retrieval augmentation mechanism, which enhances agents' reasoning through contextual learning from past interactions. Experiments show that DeepSeek-V3 outperforms other models across most metrics, while the experience pool improves voting accuracy and social influence.

## Method Summary
The platform implements the Werewolf game with 8 or 12 players across multiple roles (Seer, Witch, Hunter, Guard, Sheriff, Werewolves, Villagers). LLM agents use API calls with temperature=1.0, max_tokens=2048, and top_p=1.0. The experience-retrieval augmentation mechanism computes embedding similarity between current game summaries and stored experiences (threshold 0.5), retrieving top-k experiences ranked by reward scores. Agents condition voting decisions on these retrieved precedents. The system supports heterogeneous model assignments and provides role-specific evaluation metrics including IRP, KRE, VSS, and character-oriented scores.

## Key Results
- DeepSeek-V3 outperforms other models (GPT-4o-mini, Doubao) across most evaluation metrics
- Experience pool improves Voting Success Score (VSS) from 0.37 to 0.42 for Doubao vs DeepSeek-V3 matchups
- Sheriff's effectiveness and overall agent cooperation benefit from experience-retrieval augmentation
- Heterogeneous model assignments reveal relative strategic strengths between different LLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Experience-retrieval augmentation improves LLM agent voting decisions by reusing successful historical reasoning patterns
- Mechanism: At each voting stage, the system computes similarity between the current game summary and stored experience summaries (filtered at 0.5 threshold), then retrieves top-k experiences ranked by reward scores. Agents condition their voting decisions on these retrieved precedents
- Core assumption: Past successful voting decisions in similar game states transfer to new contexts; the embedding similarity captures meaningful strategic equivalence
- Evidence anchors: [abstract] "A key innovation is the experience-retrieval augmentation mechanism, which enhances agents' reasoning through contextual learning from past interactions"; [Section 3.1] "we design an experience pool that retrieves historical interaction records during game play, enabling context-aware learning"; [Section 4.3, Table 4] VSS improved from 0.37 to 0.42 (Doubao vs DeepSeek-V3)

### Mechanism 2
- Claim: Role-specific evaluation metrics enable finer-grained assessment of LLM strategic capabilities beyond win/loss outcomes
- Mechanism: Separate metrics per role (e.g., Seer's werewolf detection rate, Witch's potion efficiency, Sheriff's vote influence) decompose performance into interpretable skill dimensions rather than aggregate success
- Core assumption: Strategic competence factorizes into role-specific skills that generalize across game instances
- Evidence anchors: [abstract] "introducing comprehensive evaluation metrics...experiments show that DeepSeek-V3 outperforms other models across most metrics"; [Section 3.2] Defines explicit formulas for each role's effectiveness score; [corpus] DSGBench [Tang et al., 2025] introduced coarser metrics (IRP, KSR, VSS) that WereWolf-Plus extends

### Mechanism 3
- Claim: Heterogeneous model assignment reveals relative strategic strengths by pitting different architectures against each other in controlled matchups
- Mechanism: The platform allows different LLMs to be assigned to specific roles or teams within the same game, enabling pairwise comparison under identical game conditions
- Core assumption: Performance differences under controlled assignment reflect underlying capability gaps rather than stochastic variance
- Evidence anchors: [abstract] "supporting customizable roles...enabling heterogeneous model assignments"; [Section 4.1] "We experimented with two models by making them play the good camp and the wolf camp respectively, which becomes a set of experiments"

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) for sequential decision-making
  - Why needed here: The experience pool mechanism embeds summaries, retrieves by similarity, and injects context—standard RAG applied to multi-turn game reasoning
  - Quick check question: Can you explain why embedding similarity might fail to capture strategic equivalence in social deduction games?

- Concept: Partial observability in multi-agent systems
  - Why needed here: Werewolf has hidden roles; agents must maintain belief states over others' identities and update based on observed actions and speech
  - Quick check question: How does information asymmetry between werewolves (who know each other) and villagers affect the design of fair evaluation metrics?

- Concept: Evaluation metric design for generative agents
  - Why needed here: The paper introduces role-specific formulas (e.g., Eq. 2-10); understanding trade-offs between interpretability, correlation with win rate, and robustness is essential
  - Quick check question: Why might a high Voting Success Score not correlate with overall game wins in team-based settings?

## Architecture Onboarding

- Component map:
  Game Engine -> Agent Interface -> Memory System (History + Experience pools) -> Retrieval Module -> Evaluation Layer

- Critical path:
  1. Initialize game with player count, role distribution, model assignments
  2. Night phase: werewolf kill → guard protect → seer check → witch save/poison
  3. Day phase: death announcement → sheriff election (if first day) → discussion → belief inference → vote → sheriff summary → revote
  4. At vote stage: generate summary → retrieve experiences → inject into prompt → LLM decision
  5. Post-game: update experience pool with summaries + reward (1000 - Tmax for wins, Tmax for losses)

- Design tradeoffs:
  - Only applying experience retrieval to voting (not skill usage or debate) limits coverage but simplifies evaluation
  - 10 games per configuration enables rapid iteration but reduces statistical power
  - Temperature=1.0 encourages diversity but may increase outcome variance

- Failure signatures:
  - Experience pool returns no matches (threshold too high or sparse history) → agent falls back to zero-shot reasoning
  - Guard violates consecutive-night rule (if not enforced) → game state corruption
  - LLM generates invalid actions (e.g., Witch using exhausted potion) → need validation layer

- First 3 experiments:
  1. Baseline comparison: Run homogeneous games (same model for all roles) to establish per-model win rates and role-specific metrics without experience augmentation
  2. Ablation on retrieval threshold: Vary similarity threshold (0.3, 0.5, 0.7) to measure sensitivity of VSS and Sheriff influence to retrieval precision vs. recall
  3. Cross-model transfer: Populate experience pool with DeepSeek-V3 games, then test if weaker models (GPT-4o-mini) improve when retrieving from stronger-agent experiences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does applying the experience-retrieval augmentation mechanism to non-voting phases (e.g., skill usage, debate) improve agent performance compared to its current implementation?
- Basis in paper: [explicit] The Conclusion states, "Currently, the experience pool mechanism is applied only during the voting phase; in future work, we will explore extending this mechanism to other decision-making stages such as skill usage and debate."
- Why unresolved: The current architecture limits context-aware learning to the voting stage, leaving the potential for reasoning enhancement in other critical game phases untested
- What evidence would resolve it: Ablation studies showing performance metrics (e.g., KRE, VSS) when the experience pool is active during skill usage or debate phases versus the baseline voting-only implementation

### Open Question 2
- Question: How does assigning distinct models to specific roles (special roles, villagers, werewolves) alter the dynamics of strategic interaction and team collaboration?
- Basis in paper: [explicit] The Conclusion proposes, "Assigning different models to special roles, villagers, and werewolves to better capture heterogeneous agent behaviors in strategic interactions."
- Why unresolved: Current experiments primarily focus on pitting one model's camp against another, rather than analyzing the complex dynamics of a single game populated by a mosaic of different models in various roles
- What evidence would resolve it: Experimental results from games with mixed-model configurations, analyzing team win rates and cross-model cooperation metrics

### Open Question 3
- Question: Does expanding the experience pool with high-quality external game records significantly enhance agents' reasoning ability and adaptability?
- Basis in paper: [explicit] The Conclusion lists, "Collecting or constructing additional high-quality game records to enrich the experience pool, and exploring diverse strategy injection methods."
- Why unresolved: The current reliance on self-generated game data may limit the diversity of strategies the agents encounter, potentially capping their reasoning adaptability
- What evidence would resolve it: Performance comparisons of agents using a baseline experience pool versus agents using a pool enriched with expert human game logs or diverse simulated strategies

## Limitations
- Only 10 games per configuration may not provide sufficient statistical power to capture variance in social deduction outcomes
- Experience-retrieval mechanism's effectiveness depends on embedding similarity capturing strategic equivalence, which may fail in novel game states
- Role-specific metrics may not fully account for interdependencies between roles, potentially leading to gaming of local metrics at team expense
- Exact prompt templates and alpha values for metric calculations are not provided, creating uncertainty in faithful reproduction

## Confidence

- High Confidence: The platform architecture (game engine, agent interface, memory system) is well-specified and reproducible
- Medium Confidence: The experience-retrieval mechanism improves voting performance based on reported metrics, though statistical significance is uncertain with n=10
- Medium Confidence: DeepSeek-V3 demonstrates superior performance across most metrics, though cross-camp comparisons are inherently noisy
- Low Confidence: The transferability of experience pool benefits across different LLM architectures (e.g., using DeepSeek-V3 experiences for Doubao) remains unproven

## Next Checks

1. **Statistical Power Analysis**: Re-run the baseline experiments with 30-50 games per configuration instead of 10 to establish confidence intervals for win rates and role-specific metrics, particularly for the VSS improvement claims

2. **Experience Pool Ablation**: Design an experiment where the experience pool is populated with games from one model architecture (e.g., DeepSeek-V3) but tested on a different architecture (e.g., GPT-4o-mini) to validate whether strategic reasoning transfers across models

3. **Threshold Sensitivity Testing**: Systematically vary the similarity threshold (0.3, 0.5, 0.7, 0.9) and measure the impact on VSS and Sheriff influence scores to understand the retrieval precision-recall tradeoff and identify optimal operating points