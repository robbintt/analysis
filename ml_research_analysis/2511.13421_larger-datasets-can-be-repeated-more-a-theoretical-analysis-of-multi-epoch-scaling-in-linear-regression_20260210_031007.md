---
ver: rpa2
title: 'Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch
  Scaling in Linear Regression'
arxiv_id: '2511.13421'
source_url: https://arxiv.org/abs/2511.13421
tags:
- data
- lemma
- proof
- have
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical analysis of how multi-epoch training
  reshapes data scaling laws in linear regression. The authors define the effective
  reuse rate E(K, N) as the multiplicative factor by which the dataset must grow under
  one-pass training to achieve the same test loss as K-epoch training on N samples.
---

# Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression

## Quick Facts
- arXiv ID: 2511.13421
- Source URL: https://arxiv.org/abs/2511.13421
- Reference count: 40
- Multi-epoch training provides linear gains for small K but saturates at Θ(log N) for large K

## Executive Summary
This paper presents a theoretical analysis of how multi-epoch training reshapes data scaling laws in linear regression. The authors define the effective reuse rate E(K, N) as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as K-epoch training on N samples. Through matrix concentration inequalities and bias-variance decomposition, they prove that small K yields linear gains (E(K,N)≈K) while large K plateaus at Θ(log N), showing larger datasets can be repeated more times before marginal benefits vanish.

## Method Summary
The authors analyze stochastic gradient descent (SGD) training dynamics in linear regression using matrix concentration inequalities and bias-variance decomposition. For strongly convex linear regression, they prove that when K is small (K=o(log N)), E(K,N)≈K, indicating each epoch yields linear gains. As K increases beyond log N, E(K,N) plateaus at Θ(log N). The theoretical predictions are validated through experiments with both synthetic data and large language models (LLMs), confirming the linear gain regime for small K and monotonic increase of effective reuse rate with dataset size.

## Key Results
- Multi-epoch training yields linear gains for small K: E(K,N)≈K when K≪log N
- For large K, effective reuse rate plateaus at Θ(log N), showing diminishing returns
- Larger datasets allow more effective epochs before saturation - E(K,N) increases monotonically with N
- These findings challenge assumptions that effective number of epochs is uniform across dataset sizes

## Why This Works (Mechanism)

### Mechanism 1: Linear Effective Reuse in Low-Epoch Regime
- **Claim:** Repeating data for a small number of epochs $K$ yields performance gains equivalent to scaling the dataset by a factor of $K$.
- **Mechanism:** In linear regression, the risk decomposes into bias and variance terms. When $K \ll \log N$, the optimal excess risk scales as $\Theta(\log(KN)/KN)$. Because this loss depends on the total iteration count $T=KN$, increasing $K$ is mathematically equivalent to increasing $N$ by the same factor.
- **Core assumption:** Strong convexity of the data covariance matrix and the use of an optimal constant learning rate $\eta$.
- **Evidence anchors:**
  - [abstract]: "When K is small, we prove that E(K, N) ≈ K, indicating that every new epoch yields a linear gain."
  - [section]: Theorem 4.2 states "when K ≪ log N, then E(K, N) = K(1 + o(1))."
  - [corpus]: Paper 733 ("Entropy-Guided Token Dropout") notes performance degradation under repeated exposure, supporting the fragility of this mechanism if $K$ grows too large.
- **Break condition:** If $K$ exceeds the $\log N$ threshold, the risk scaling shifts, and additional epochs provide diminishing returns rather than linear gains.

### Mechanism 2: Saturation via Variance-Limited Convergence
- **Claim:** As the number of epochs $K$ becomes large, the effective reuse rate $E(K, N)$ saturates at a value dependent on the dataset size $N$, rather than growing indefinitely with $K$.
- **Mechanism:** With high repetitions ($K \gg \log N$), the SGD noise (variance) averages out over the infinite passes, but the inherent sample variance of the finite dataset $N$ remains. The risk stops scaling with total steps $T$ and instead plateaus at $\Theta(1/N)$, effectively capping the utility of repetition at $\Theta(\log N)$ (strongly convex case).
- **Core assumption:** The model is trained for sufficient epochs to reach the "limited-reuse" regime ($K = \omega(\log N)$).
- **Evidence anchors:**
  - [abstract]: "As K increases, E(K, N) plateaus at a value of order Θ(log N)... implying larger datasets can be repeated more times before marginal benefit vanishes."
  - [section]: Theorem 4.1 shows that for large $K$, risk depends on $N$ but is independent of $K$.
  - [corpus]: Paper 100855 ("Multi-Epoch Differentially Private SGD") discusses bounds for repeated participation, implicitly acknowledging the bounded utility of infinite passes.
- **Break condition:** Attempting to push $K$ far beyond $\log N$ yields negligible loss reduction compared to simply acquiring fresh data.

### Mechanism 3: Dataset Size Scaling of Saturation Point
- **Claim:** Larger datasets allow for more effective epochs (higher saturation point) before returns diminish.
- **Mechanism:** The "effective reuse" capacity scales with $\log N$. A larger $N$ reduces the noise in the estimate of the population covariance matrix, allowing the optimizer to extract more signal per pass before the finite-sample noise floor is hit.
- **Core assumption:** The problem structure (strong convexity or power-law spectrum) implies a specific relationship between $N$ and the noise floor.
- **Evidence anchors:**
  - [abstract]: "...revealing that larger datasets can be repeated more times before the marginal benefit vanishes."
  - [section]: Theorem 5.2 extends this to Zipf-distributed data where saturation scales as a power of $N$.
  - [corpus]: Paper 95752 ("Datasets, Documents, and Repetitions") discusses practical constraints of limited data, aligning with the motivation that understanding saturation is critical for small $N$.
- **Break condition:** If the dataset is small ($N \approx \text{const}$), the saturation point is reached very quickly (e.g., $K=2$ or $3$), making multi-epoch training immediately inefficient.

## Foundational Learning

- **Concept: Bias-Variance Decomposition in SGD**
  - **Why needed here:** The theoretical proofs rely on separating the excess risk into bias (initialization error) and variance (label noise/sample noise) to analyze how they decay differently across epochs.
  - **Quick check question:** Does increasing the number of epochs $K$ reduce the bias term or the variance term in the excess risk? (Answer: It affects both, but the variance term saturates at $\sigma^2/N$).

- **Concept: Strong Convexity & Hessian Eigenvalues**
  - **Why needed here:** The scaling laws (e.g., $\log N$ saturation) are derived under the assumption of a strictly positive minimum eigenvalue ($\lambda_d \ge \mu$). Understanding the spectral decay is crucial for extending to Zipf/Power-law data.
  - **Quick check question:** Why does strong convexity lead to logarithmic scaling rather than polynomial scaling for the saturation point?

- **Concept: Optimal Learning Rate Scheduling**
  - **Why needed here:** The paper derives a specific optimal constant learning rate $\eta \propto \log(KN)/KN$ to achieve the theoretical bounds, contrasting with standard practice.
  - **Quick check question:** According to Lemma 4.4, how should the learning rate scale with the total number of training steps $T=KN$?

## Architecture Onboarding

- **Component map:** Dataset $D$ of size $N$ -> K-Epoch SGD with Random Shuffling -> Constant Learning Rate $\eta$ -> Effective Reuse Rate $E(K, N)$
- **Critical path:** Estimating the transition point $K^*$ where $E(K, N)$ deviates from $K$. This determines the maximum practical number of epochs before data acquisition becomes more valuable than repetition.
- **Design tradeoffs:**
  - **Constant vs. Decaying LR:** The theoretical analysis assumes a constant $\eta$ for tractability, whereas standard LLM training often uses cosine decay. Validating the theory may require isolating the LR variable.
  - **Compute vs. Data:** Multi-epoch training trades off data scarcity for compute inefficiency. Use $E(K, N)$ to quantify this exchange rate.
- **Failure signatures:**
  - **Premature Saturation:** If $E(K, N) < K$ immediately for $K=2$, the dataset may be too small or the learning rate suboptimal.
  - **Noise Dominance:** In the large $K$ regime, if loss fails to plateau at the predicted $\Theta(1/N)$, check label noise $\sigma^2$ assumptions.
- **First 3 experiments:**
  1. **Synthetic Validation:** Train a linear regression model on synthetic strongly-convex data. Plot $E(K, N)$ vs $K$ for varying $N$ to confirm the $\log N$ saturation scaling (replicating Figure 1a).
  2. **LR Sensitivity:** Implement the optimal LR $\eta = \frac{\log(KN)}{2\lambda_d KN}$ and compare the achieved loss against a grid-searched optimal LR to verify Lemma 4.4 is tight.
  3. **LLM Pilot:** Train a small LLM (e.g., 0.3B params) on a subset of tokens (e.g., 0.5B) for 4 epochs. Calculate $E(4, N)$ to check if it matches the "linear gain" regime ($E \approx 4$) as seen in Figure 2a.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effective reuse rate behavior ($E(K, N) \approx K$ followed by saturation) extend to neural networks with feature learning?
- Basis in paper: [explicit] The authors note the analysis is "limited to the linear model" and suggest extending the framework to "neural networks with feature learning."
- Why unresolved: The theoretical mechanisms for data reuse in non-linear feature learning dynamics are not yet characterized.
- What evidence would resolve it: Empirical scaling curves in deep networks showing saturation points scaling with $N$, or theoretical bounds derived for non-linear models.

### Open Question 2
- Question: Can strategies like curriculum learning or selective repetition of high-quality data improve the effective reuse rate compared to repeating the entire dataset uniformly?
- Basis in paper: [explicit] The conclusion proposes exploring "a more efficient and heuristic approach... such as data mixing, curriculum learning, or reusing only high-quality data."
- Why unresolved: The current theory assumes uniform multi-epoch training over the whole dataset.
- What evidence would resolve it: Empirical comparisons of $E(K, N)$ between uniform training and curriculum-based schedules on identical data budgets.

### Open Question 3
- Question: Can the scaling laws for the effective reuse rate be generalized to general non-strongly convex linear regression beyond the specific Zipf-distributed case?
- Basis in paper: [explicit] The authors state that their main results rely on strong convexity and ask to "generalize these proof ideas to general non-strongly convex linear regression."
- Why unresolved: The current proofs for the non-strongly convex case rely on the solvability of the Zipf-distribution assumptions.
- What evidence would resolve it: A theoretical derivation of the saturation point for linear regression with general polynomial spectral decay.

### Open Question 4
- Question: Is the technical assumption that $K = O(N^{0.1})$ (Assumption 4.3) strictly necessary, or do the results hold for arbitrarily large $K$?
- Basis in paper: [inferred] The authors state the bound is for "technical issues" but speculate the main results hold for arbitrarily large $K$.
- Why unresolved: The matrix concentration inequalities used in the proofs require this constraint to bound error terms.
- What evidence would resolve it: A proof of the scaling law that maintains tight error bounds for $K = \omega(N^{0.1})$.

## Limitations

- Strong convexity assumption: Theoretical results rely on strong convexity which may not hold for deep neural networks
- Constant learning rate requirement: Analysis requires optimal constant LR rather than standard cosine decay schedules
- No covariate shift modeling: Theory assumes i.i.d. data and doesn't account for distribution shift during multi-epoch training
- Validation vs test loss: Uses validation loss to compute effective reuse rates which may not perfectly reflect generalization

## Confidence

**High Confidence**: The theoretical framework for linear regression under strong convexity is mathematically rigorous and the proofs are sound. The core claim that E(K,N) ≈ K for small K and plateaus at Θ(log N) for large K is well-supported both theoretically and empirically in the synthetic experiments.

**Medium Confidence**: The extension to Zipf-distributed data and the power-law scaling of the saturation point is mathematically consistent but less extensively validated. The empirical validation on LLMs, while promising, involves smaller-scale experiments that may not fully capture the behavior of frontier models.

**Low Confidence**: The practical implications for large-scale LLM training remain somewhat speculative. The paper doesn't address how these findings interact with architectural choices, regularization techniques, or the complex optimization dynamics of very large models.

## Next Checks

1. **Non-convex extension**: Design experiments to test whether the E(K,N) scaling holds for non-convex objectives by training shallow neural networks with varying depth and width, comparing the effective reuse rates to the theoretical predictions.

2. **Schedule comparison**: Implement both constant and cosine decay learning rate schedules on the same synthetic regression task, measuring how much of the multi-epoch benefit is attributable to the schedule versus data reuse.

3. **Domain shift quantification**: Train an LLM on multi-epoch data while progressively introducing out-of-distribution examples in later epochs, measuring how the effective reuse rate changes as the training distribution shifts.