---
ver: rpa2
title: Signal or Noise? Evaluating Large Language Models in Resume Screening Across
  Contextual Variations and Human Expert Benchmarks
arxiv_id: '2507.08019'
source_url: https://arxiv.org/abs/2507.08019
tags:
- human
- llms
- recruitment
- expert
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether large language models (LLMs) exhibit
  consistent behavior or random variation in resume screening across different contextual
  inputs, comparing their performance to human experts. Using a controlled experimental
  design, three LLMs (Claude, GPT, and Gemini) and three human recruitment experts
  evaluated resumes under four context conditions: no company information, multinational
  corporation, startup, and reduced context.'
---

# Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks

## Quick Facts
- arXiv ID: 2507.08019
- Source URL: https://arxiv.org/abs/2507.08019
- Reference count: 0
- Three LLMs (Claude, GPT, Gemini) and three human experts evaluated resumes across four context conditions

## Executive Summary
This study investigates whether large language models exhibit consistent behavior or random variation in resume screening across different contextual inputs, comparing their performance to human experts. Using a controlled experimental design, the researchers found that LLM scores differed significantly from human experts (p < 0.01) across four of eight conditions. GPT showed strong context sensitivity, Gemini exhibited partial adaptation, and Claude demonstrated minimal responsiveness. The findings suggest that while LLMs can produce interpretable patterns with detailed prompts, they diverge substantially from human judgment, indicating that hybrid approaches combining LLM efficiency with human oversight may be optimal for recruitment automation.

## Method Summary
The study employed a controlled experimental design where three LLMs (Claude, GPT, and Gemini) and three human recruitment experts evaluated 30 anonymized Product Manager resumes across four context conditions: no company information, multinational corporation profile, startup profile, and reduced context with minimal job description. Participants scored each resume on a 0-100 scale and provided explicit weighting breakdowns for evaluation criteria. Statistical analysis included ANOVA for inter-evaluator differences, paired t-tests for contextual sensitivity, and Cohen's d effect sizes to measure alignment between LLM and human assessments.

## Key Results
- LLM scores differed significantly from human experts (p < 0.01) across four of eight experimental conditions
- GPT adapted strongly to company context (p < 0.001), while Claude showed minimal adaptation (p > 0.1)
- GPT scores inflated by 28.9% under reduced context conditions, whereas human experts maintained stable scores
- Meta-cognition analysis revealed distinct weighting patterns between LLMs and humans

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Detailed organizational context allows LLMs to simulate adaptability by mechanically redistributing evaluation weights, whereas minimal context leads to inconsistent "noise."
- **Mechanism:** The model likely attends to specific keywords in the company profile (e.g., "startup" vs. "MNC") and adjusts attention to corresponding resume sections.
- **Core assumption:** The observed score shifts reflect systematic adjustment of internal weighting schemas rather than random temperature-based variance.
- **Evidence anchors:**
  - [abstract] GPT adapts strongly to company context (p < 0.001), while Claude shows minimal adaptation (p > 0.1)
  - [section] Meta-cognition analysis shows GPT increased leadership/soft skills weighting from 15% to 25% when shifting to a multinational context
  - [corpus] "AI Hiring with LLMs: A Context-Aware... Framework" supports efficacy of context-aware agents in resume screening
- **Break condition:** If the prompt lacks explicit company descriptors, the mechanism fails, defaulting to generic scoring patterns

### Mechanism 2
- **Claim:** LLMs tend toward "optimistic" score inflation when context is reduced, acting as a noise generator that diverges from human expert risk aversion.
- **Mechanism:** Unlike humans who apply conservative priors when data is missing, LLMs appear to lack a "caution" parameter, potentially interpreting missing constraints as permission for higher scores.
- **Core assumption:** The score inflation is a byproduct of the model's training to generate helpful/compliant responses rather than a reasoning failure.
- **Evidence anchors:**
  - [abstract] Significant mean differences in LLM scoring across four of eight conditions
  - [section] "Reduced Context Analysis" shows GPT scores inflated from 64.3 to 82.9 (a 28.9% increase), whereas human experts maintained stable or conservative scores
  - [corpus] Corpus evidence for this specific "inflation under scarcity" mechanism is weak; related papers focus on demographic bias rather than uncertainty-based inflation
- **Break condition:** The mechanism triggers specifically under the "Reduced Context" condition; detailed prompts appear to dampen this inflation

### Mechanism 3
- **Claim:** LLMs exhibit "meta-cognition" via explicit categorical decomposition, which fundamentally differs from the holistic, experiential integration used by human experts.
- **Mechanism:** LLMs break the evaluation task into discrete percentages (e.g., "30% experience, 20% education"), while humans synthesize factors like "maturity" and "culture fit" qualitatively based on professional history.
- **Core assumption:** The explicit weighting reported by the LLM is a faithful representation of its internal processing logic and not a post-hoc rationalization.
- **Evidence anchors:**
  - [abstract] Meta-cognition analysis highlights distinct weighting patterns between LLMs and humans
  - [section] Expert 2 created distinct frameworks for different firms (e.g., 75% experience for startups), showing qualitative logic like "in a startup the person should be more mature," which LLMs did not replicate
  - [corpus] "FAIRE: Assessing Racial and Gender Bias" implies that decomposition mechanisms can mask or perpetuate systemic biases differently than human review
- **Break condition:** If a resume contains non-standard or highly creative formats that defy categorical parsing, the LLM's decomposition mechanism may fail or produce hallucinated categories

## Foundational Learning

- **Concept: Signal Detection Theory (SDT)**
  - **Why needed here:** The paper frames LLM reliability as a signal-to-noise problem. You must understand how to distinguish consistent evaluation patterns (signal) from random or context-induced variance (noise).
  - **Quick check question:** Can you distinguish between a score change caused by relevant context (signal) versus a score change caused by prompt phrasing ambiguity (noise)?

- **Concept: Context Sensitivity in Transformers**
  - **Why needed here:** The study proves that identical resumes receive different scores based on company context. Understanding how attention mechanisms weigh "Job Description" vs. "Company Profile" is essential for interpreting results.
  - **Quick check question:** How does removing the "Company Profile" section from a prompt alter the attention mechanism's focus on specific resume attributes?

- **Concept: Statistical Significance (p-value & ANOVA)**
  - **Why needed here:** The paper validates claims using ANOVA and t-tests (p < 0.01). You need to read these metrics to know if a difference is a real "signal" or just random chance.
  - **Quick check question:** If GPT adapts to context with p < 0.001 and Claude with p > 0.1, which model is statistically reliable for context-aware screening?

## Architecture Onboarding

- **Component map:** Resume Dataset + Context Variants -> LLM Evaluators (Claude, GPT, Gemini) vs. Human Expert Benchmarks -> ANOVA + Paired t-tests + Meta-cognition analysis
- **Critical path:**
  1. Standardize resume inputs to control for format variables
  2. Inject specific organizational context (e.g., MNC vs. Startup) into the system prompt
  3. Monitor for score inflation (the "Reduced Context" failure mode)
- **Design tradeoffs:**
  - **GPT:** High adaptability to context (Strong Signal) vs. High risk of score inflation when data is missing (Noise)
  - **Claude:** High stability/consistency vs. Low adaptability to organizational nuance
  - **Human-in-the-loop:** Essential for complex "holistic" assessment, but introduces subjectivity and fatigue
- **Failure signatures:**
  - **Score Inflation:** Average scores jump >20% when job descriptions are vague (GPT-specific vulnerability)
  - **Rigidity:** Scores fail to shift even when company context changes drastically (Claude-specific vulnerability)
  - **Zeroing:** Human experts assigning zeros across the board (indicative of fatigue or task rejection)
- **First 3 experiments:**
  1. **Baseline Consistency Check:** Run 10 identical resumes through GPT and Claude with "No Company" context. Verify if variance is <10%.
  2. **Stress Test (Reduced Context):** Strip the JD to 1 sentence. Measure if scores increase (GPT) or stay stable (Claude/Human).
  3. **Context Sensitivity Calibration:** Provide a "Startup" profile. Check if the model lowers the weight on "Formal Credentials" and increases weight on "Adaptability" (as human experts do).

## Open Questions the Paper Calls Out

- **Question:** Do systematic LLM-human expert evaluation differences create disparate impacts on protected demographic groups (e.g., race, gender, disability) in real-world hiring contexts?
  - **Basis in paper:** [explicit] The authors state: "Bias and fairness remain critical priorities for future research, as understanding whether systematic LLM-human differences create disparate impacts on protected groups is essential for responsible AI deployment."
  - **Why unresolved:** This study used anonymized resumes and did not examine demographic attributes; the experimental design intentionally removed identifying information, preventing any analysis of disparate impact.
  - **What evidence would resolve it:** A controlled field experiment or audit study presenting identical resumes with varied demographic markers (names, addresses, gendered language) to both LLMs and human experts, measuring differential scoring patterns across protected groups.

- **Question:** How do LLM-human evaluation alignment patterns differ across job roles with varying skill requirements (e.g., technical vs. creative vs. managerial positions)?
  - **Basis in paper:** [explicit] The authors note: "Focusing exclusively on Product Manager positions limits the external validity of the results, as patterns of LLM-human expert alignment may differ across roles with varying skill requirements and evaluation criteria."
  - **Why unresolved:** The study tested only one role type (Associate Product Manager); it is unknown whether the observed context sensitivity patterns and systematic scoring gaps generalize to positions with different evaluation criteria.
  - **What evidence would resolve it:** Replication of the experimental design across multiple job categories (e.g., software engineer, marketing manager, data analyst) with domain-specific human experts, comparing the magnitude and direction of LLM-human differences.

- **Question:** Can calibration mechanisms (e.g., fine-tuning, prompt engineering, hybrid systems) effectively align LLM outputs with human expert judgment in resume screening?
  - **Basis in paper:** [explicit] The authors state: "Research should also explore calibration mechanisms to better align LLM outputs with human expert judgment."
  - **Why unresolved:** This study evaluated LLMs in a zero-shot configuration without any calibration; whether systematic gaps can be reduced through intervention remains untested.
  - **What evidence would resolve it:** Comparative experiments testing calibration techniques (domain-specific fine-tuning, few-shot prompting with expert-labeled examples, output post-processing) against baseline LLM performance, measuring reduction in LLM-human scoring gaps.

## Limitations
- The study tested only one job function (Product Manager), limiting external validity across different skill requirements and evaluation criteria
- Exact LLM versions and API parameters (temperature, top_p) were not specified, affecting reproducibility
- The experimental design used anonymized resumes without demographic information, preventing analysis of disparate impact on protected groups

## Confidence

- **High Confidence:** The statistical finding that LLM scores differ significantly from human experts (p < 0.01) across multiple conditions is well-supported by the experimental design and analysis.
- **Medium Confidence:** The mechanism that LLMs mechanically redistribute evaluation weights based on keywords in company profiles is plausible given the evidence but requires further validation through attention mechanism analysis.
- **Medium Confidence:** The observation that reduced context leads to score inflation is supported by the data but lacks strong corpus evidence for the specific "inflation under scarcity" mechanism.
- **Low Confidence:** The claim that explicit categorical decomposition fundamentally differs from human holistic assessment relies heavily on self-reported expert frameworks that may not capture the full complexity of human judgment.

## Next Checks

1. **Mechanism Validation:** Conduct ablation studies where company profile keywords are systematically varied to determine which specific terms drive context sensitivity in each model.

2. **Generalization Test:** Apply the same experimental protocol to a different job function (e.g., Software Engineer) to verify whether the observed patterns hold across domains.

3. **Human Expert Calibration:** Implement a structured interview protocol with recruitment experts to map their holistic assessment criteria and compare against LLM decomposition schemas.