---
ver: rpa2
title: Self-Explaining Neural Networks for Business Process Monitoring
arxiv_id: '2503.18067'
source_url: https://arxiv.org/abs/2503.18067
tags:
- explanations
- sufficient
- event
- since
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of providing explanations for
  deep learning models used in predictive business process monitoring, specifically
  for Next Activity Prediction tasks. While existing post-hoc explanation methods
  often suffer from lack of faithfulness, high computational costs, and sensitivity
  to out-of-distribution samples, the authors propose a novel self-explaining neural
  network architecture that generates explanations as part of the model's output during
  training.
---

# Self-Explaining Neural Networks for Business Process Monitoring
## Quick Facts
- arXiv ID: 2503.18067
- Source URL: https://arxiv.org/abs/2503.18067
- Reference count: 9
- This work introduces a self-explaining neural network architecture for Next Activity Prediction in business process monitoring that generates explanations during training rather than using post-hoc methods

## Executive Summary
This paper addresses the challenge of providing faithful and efficient explanations for deep learning models in predictive business process monitoring. Traditional post-hoc explanation methods often suffer from lack of faithfulness, high computational costs, and sensitivity to out-of-distribution samples. The authors propose a novel self-explaining neural network architecture that generates explanations as part of the model's output during training, ensuring explanations are both sufficient and concise. Experiments on four real-world event log datasets demonstrate that this approach maintains prediction accuracy while producing more faithful explanations and being substantially more efficient than post-hoc methods like Anchors.

## Method Summary
The proposed method extends standard LSTM-based RNN architectures by adding an explanation component and adapting the optimization objective. The model is trained to jointly optimize prediction accuracy, explanation faithfulness, and explanation cardinality through a dual propagation process. This self-explaining approach generates explanations during training rather than using post-hoc methods, ensuring the explanations are faithful to the model's predictions. The architecture incorporates an explanation component that learns to provide sufficient and concise explanations while maintaining prediction performance, addressing the limitations of traditional post-hoc explanation techniques in business process monitoring scenarios.

## Key Results
- The self-explaining approach does not significantly degrade prediction performance compared to standard training
- Explanations produced are more faithful (higher proportion verified as sufficient) than post-hoc methods like Anchors
- The approach is substantially more efficient, reducing computation time by several orders of magnitude
- Particularly promising for high-stakes business environments where trustworthy and interpretable predictions are essential

## Why This Works (Mechanism)
The self-explaining neural network works by integrating explanation generation directly into the model's training process. By adapting the optimization objective to include explanation faithfulness and cardinality alongside prediction accuracy, the model learns to generate explanations that are inherently aligned with its decision-making process. The dual propagation mechanism ensures that both the prediction and explanation components are trained simultaneously, creating a symbiotic relationship where explanations emerge naturally from the model's learned representations rather than being retrofitted after the fact.

## Foundational Learning
- **LSTM-based RNNs**: Recurrent neural networks with long short-term memory cells are essential for capturing sequential patterns in event logs
  - *Why needed*: Business process monitoring requires understanding temporal dependencies in event sequences
  - *Quick check*: Can the model handle variable-length sequences typical in business processes?

- **Explanation faithfulness**: The degree to which explanations accurately represent the model's decision-making process
  - *Why needed*: Ensures explanations are trustworthy and can be relied upon for critical business decisions
  - *Quick check*: Do human evaluators consistently verify the sufficiency of generated explanations?

- **Post-hoc vs. self-explaining approaches**: Different strategies for generating model explanations
  - *Why needed*: Post-hoc methods often lack faithfulness and are computationally expensive
  - *Quick check*: How does computation time compare between self-explaining and post-hoc methods?

## Architecture Onboarding
- **Component map**: Input events -> LSTM layers -> Prediction component -> Output + Explanation component -> Faithful explanations
- **Critical path**: The dual propagation through both prediction and explanation components during training
- **Design tradeoffs**: Joint optimization of prediction accuracy and explanation quality vs. separate training phases
- **Failure signatures**: Degraded prediction performance when explanation component interferes with primary task
- **3 first experiments**:
  1. Baseline LSTM model without explanation component
  2. Model with explanation component but separate training objectives
  3. Full self-explaining model with joint optimization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on only four real-world datasets, potentially limiting generalizability
- Faithfulness assessment depends on human verification, introducing subjectivity and scalability concerns
- Comparison focuses on Anchors as the post-hoc method, not representing the full spectrum of explanation techniques
- Focus on Next Activity Prediction limits generalizability to other business process monitoring objectives

## Confidence
- **Prediction performance claims**: High - results show minimal degradation compared to standard models
- **Faithfulness and efficiency claims**: Medium - dataset limitations and specific comparison baseline affect confidence
- **Architectural contributions**: High - well-justified technical aspects and optimization framework

## Next Checks
1. Test the approach across a broader range of business process monitoring tasks beyond Next Activity Prediction
2. Conduct a comprehensive comparison with multiple post-hoc explanation methods to establish relative performance
3. Implement a more scalable and objective faithfulness evaluation metric to reduce human dependency