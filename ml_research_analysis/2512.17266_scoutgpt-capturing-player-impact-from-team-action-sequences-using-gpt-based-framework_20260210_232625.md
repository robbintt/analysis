---
ver: rpa2
title: 'ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based
  Framework'
arxiv_id: '2512.17266'
source_url: https://arxiv.org/abs/2512.17266
tags:
- player
- event
- value
- context
- tactical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ScoutGPT, a player-conditioned, value-aware\
  \ next-event prediction model based on a GPT-style autoregressive transformer, to\
  \ address the challenge of evaluating player performance in new tactical environments\
  \ for transfer decisions. The model jointly predicts the next on-ball action\u2019\
  s type, location, timing, and its estimated residual On-Ball Value (rOBV) while\
  \ conditioning on player identity and match context."
---

# ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework

## Quick Facts
- **arXiv ID:** 2512.17266
- **Source URL:** https://arxiv.org/abs/2512.17266
- **Reference count:** 16
- **Primary result:** Introduces GPT-based next-event prediction model with player conditioning and rOBV supervision for transfer scouting and tactical simulation

## Executive Summary
This paper presents ScoutGPT, a player-conditioned, value-aware next-event prediction model based on a GPT-style autoregressive transformer. The framework addresses the challenge of evaluating player performance in new tactical environments for transfer decisions by predicting not only the type and location of the next on-ball action but also its estimated residual On-Ball Value (rOBV). Evaluated on five Premier League seasons, the model demonstrates superior performance in next-event prediction accuracy, spatial precision, and value estimation compared to existing sequence-based baselines.

The key innovation lies in the unified tokenization of multimodal match events and the incorporation of player identity embeddings to capture individual behavioral styles. The framework enables counterfactual player substitution simulations, allowing practical assessment of how a player's behavioral and value profiles would change in a different tactical setup. Multiple case studies demonstrate the system's utility for scouting role-compatible alternatives and comparing positional adaptations.

## Method Summary
ScoutGPT is a GPT-style autoregressive transformer that treats football match play as a sequence of discrete tokens, including spatial coordinates, time, event type, player ID, and rOBV. The model uses a decoder-only transformer architecture with causal masking to predict the next event's attributes while conditioning on player identity and match context. Player embeddings are learned during training and can be substituted into new event sequences for counterfactual simulations. The framework is evaluated on five Premier League seasons using SPADL-formatted event data, with performance measured across next-event prediction accuracy, spatial precision, and value estimation tasks.

## Key Results
- Outperforms baseline models in next-event prediction accuracy (Tables 1-2)
- Achieves 4.30m spatial error in location prediction
- Successfully learns player embeddings that cluster by position role without using positional labels
- Enables counterfactual player substitution simulations for scouting analysis
- Demonstrates practical utility through multiple case studies including striker role comparisons

## Why This Works (Mechanism)

### Mechanism 1: Unified Tokenization of Multimodal Events
Treating heterogeneous match data (coordinates, time, action type) as a single sequence of discrete tokens allows the model to learn complex interactions between spatial and temporal dynamics. The architecture maps continuous attributes and categorical attributes into a shared vocabulary, enabling the Transformer to use self-attention to learn the joint probability distribution of these tokens autoregressively. This assumes football match evolution follows learnable syntactic rules similar to natural language.

### Mechanism 2: Conditioning via Player Embeddings
Player identity acts as a "style vector" that biases the prediction of event outcomes and locations. A learned embedding for player_id is injected into the sequence context, forcing the gradient descent to capture the latent "behavioral prior" of that player. This assumes a player's past behavior generalizes to new contexts and is not entirely defined by teammates.

### Mechanism 3: Residual On-Ball Value (rOBV) as a Target
Supervising the model with rOBV forces it to learn the "quality" of an action, not just the "type" of action. The model predicts rOBV as a token in the sequence, acting as a dense reward signal that correlates specific actions in specific contexts with future goal probability. This assumes the provided rOBV metric accurately captures the contribution of an action to the team's success.

## Foundational Learning

- **Concept: Decoder-Only Transformers (GPT)**
  - **Why needed here:** This is the engine of the model. You must understand causal masking (hiding future tokens) to understand why the model can predict the next event without "cheating."
  - **Quick check question:** If you removed the causal mask during training, could the model predict the first event of a match by looking at the last event?

- **Concept: Residual On-Ball Value (rOBV)**
  - **Why needed here:** This is the optimization target for "impact." You need to distinguish between predicting *what* happens (pass/shot) vs. *the value* of what happens (expected goal contribution).
  - **Quick check question:** Why is "residual" value used instead of raw action value? (Hint: Think about future sequence aggregation).

- **Concept: SPADL (Soccer Player Action Description Language)**
  - **Why needed here:** This is the input format. You need to know how raw event data is standardized into (x, y, time, type) tuples before it becomes tokens.
  - **Quick check question:** Does SPADL represent the game as a continuous video or a discrete list of actions?

## Architecture Onboarding

- **Component map:** Input (SPADL events) -> Tokenizer (discretizes into IDs) -> Backbone (NanoGPT decoder-only Transformer) -> Head (linear layer to vocab size) -> Output (softmax probabilities)

- **Critical path:** The Context Block (Section 2.1) is the most distinct feature. It prepends the list of 22 on-pitch players and match state to the event sequence. If this is corrupted, player conditioning is lost.

- **Design tradeoffs:**
  - Discretization vs. Regression: The paper chooses to discretize continuous values into tokens rather than using regression heads, unifying the architecture but potentially losing fine-grained spatial precision (4.30m error).
  - Episode length: Capped at 100 events to balance context retention against compute costs but truncates long possessions.

- **Failure signatures:**
  - Hallucinated Events: The model might predict physically impossible sequences if context attention is weak.
  - Identity Collapse: If player embeddings are under-trained, the model reverts to "average" player behavior.

- **First 3 experiments:**
  1. Next-Token Accuracy: Test if the model predicts generic event types correctly before testing player-specific styles.
  2. Spatial MAE: Validate the spatial discretization by measuring Mean Absolute Error of predicted (x,y) vs. ground truth.
  3. Substitution Sanity Check: Substitute a player with themselves. The predicted output distribution should closely match the ground truth distribution.

## Open Questions the Paper Calls Out
None

## Limitations
- Data sparsity for cold-start players with limited match history may render counterfactual simulations unreliable for scouting purposes.
- Temporal generalization beyond five seasons is untested, as player styles may evolve with tactical innovations or rule changes.
- Contextual bias in rOBV labels may perpetuate systematic biases when evaluating players in different contexts.

## Confidence
- **High Confidence:** Next-event prediction accuracy improvements, meaningful player embeddings clustering by position, basic counterfactual mechanism functionality.
- **Medium Confidence:** Practical utility of counterfactual simulations for scouting, ability to capture tactical adaptability, spatial precision sufficiency.
- **Low Confidence:** Framework effectiveness for players outside the dataset, stability of embeddings across tactical evolution, handling of complex tactical instructions.

## Next Checks
1. **Cold-Start Player Validation:** Test the model's performance on players with <50 career matches by comparing predicted behavioral profiles against actual performance when they join new teams.

2. **Cross-Competition Transferability:** Evaluate whether embeddings learned from Premier League data successfully predict player behavior in different leagues with different tactical norms.

3. **Temporal Stability Analysis:** Track how player embeddings evolve over consecutive seasons for the same individuals and correlate these changes with known tactical role changes or team system shifts.