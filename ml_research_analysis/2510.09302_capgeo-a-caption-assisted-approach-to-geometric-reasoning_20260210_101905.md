---
ver: rpa2
title: 'CapGeo: A Caption-Assisted Approach to Geometric Reasoning'
arxiv_id: '2510.09302'
source_url: https://arxiv.org/abs/2510.09302
tags:
- geometric
- reasoning
- arxiv
- captioning
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Geometric reasoning in multimodal large language models (MLLMs)
  is hindered by inadequate visual understanding of diagrams, not by reasoning capability.
  The proposed CapGeo framework converts geometric figures into structured captions
  that capture essential elements, relations, and numerical values, then feeds these
  captions to LLMs for reasoning.
---

# CapGeo: A Caption-Assisted Approach to Geometric Reasoning

## Quick Facts
- arXiv ID: 2510.09302
- Source URL: https://arxiv.org/abs/2510.09302
- Authors: Yuying Li; Siyi Qian; Hao Liang; Leqi Zheng; Ruichuan An; Yongzhen Guo; Wentao Zhang
- Reference count: 30
- Primary result: Caption-based geometric reasoning improves Qwen2.5-VL-72B from 8.6% to 59.0% accuracy on MathVerse Vision-Only tasks

## Executive Summary
Geometric reasoning in multimodal large language models is fundamentally limited by inadequate visual understanding of diagrams rather than reasoning capability itself. The CapGeo framework addresses this by converting geometric figures into structured captions that capture essential elements, relations, and numerical values, which are then fed to LLMs for reasoning. This approach dramatically improves performance across multiple MLLM architectures, demonstrating that the bottleneck in geometric problem-solving is visual comprehension rather than logical reasoning.

To support this research direction, the authors introduce CapGeo-Bench, a dataset of 4,641 high-quality geometry figure-caption pairs with a keypoint-based evaluation metric. This benchmark enables reliable assessment of geometric captioning ability and identifies captioning models that yield the best reasoning results. Together, CapGeo and CapGeo-Bench establish a new pathway for advancing multimodal geometric reasoning by bridging visual and textual modalities.

## Method Summary
The CapGeo framework operates by transforming geometric diagrams into structured textual descriptions that capture all relevant visual elements, their spatial relationships, and numerical measurements. These captions serve as input to standard LLMs, which then perform the reasoning tasks without directly processing the visual information. The approach leverages existing captioning models trained on geometric content and evaluates their effectiveness using the newly introduced CapGeo-Bench dataset, which provides both the geometric figures and their corresponding high-quality captions for training and evaluation purposes.

## Key Results
- Qwen2.5-VL-72B accuracy improves from 8.6% to 59.0% on MathVerse Vision-Only tasks using CapGeo
- Claude-Opus-4 performance increases from 44.8% to 73.0% with caption-assisted reasoning
- CapGeo-Bench dataset provides 4,641 high-quality geometry figure-caption pairs for evaluation
- Keypoint-based evaluation metric shows strong correlation with downstream reasoning performance

## Why This Works (Mechanism)
The framework works because geometric reasoning requires precise extraction of visual elements (points, lines, angles, measurements) and their relationships, which is a distinct capability from logical reasoning. By converting visual information into structured text, CapGeo leverages the strong language understanding capabilities of LLMs while bypassing their visual comprehension limitations. The caption generation process acts as a specialized visual encoder that transforms spatial relationships into explicit textual relationships that LLMs can process effectively.

## Foundational Learning
- **Geometric Element Recognition**: Understanding points, lines, angles, shapes, and measurements - needed because LLMs struggle to accurately identify and quantify visual geometric elements directly from diagrams
- **Spatial Relationship Encoding**: Converting spatial arrangements into explicit textual relationships - required because visual spatial reasoning is challenging for current multimodal models
- **Structured Caption Generation**: Creating consistent, comprehensive textual descriptions of geometric figures - essential for providing reliable input to reasoning models
- **Keypoint-Based Evaluation**: Assessing caption quality through spatial and measurement accuracy - necessary for validating that captions capture the information needed for reasoning
- **Multimodal-to-Text Conversion**: Transforming visual geometric information into textual format - fundamental to the approach of leveraging text-based reasoning capabilities
- **Geometric Reasoning Pipelines**: Understanding how visual comprehension and logical reasoning interact in geometric problem-solving - critical for identifying where current systems fail

## Architecture Onboarding
- **Component Map**: Geometric Diagram → Caption Generator → Structured Caption → LLM Reasoning Module → Answer Output
- **Critical Path**: The caption generation step is the critical path, as errors or omissions here directly propagate to reasoning failures regardless of LLM capability
- **Design Tradeoffs**: The framework trades direct visual processing for reliable textual encoding, sacrificing some spatial nuance for consistency and accuracy in element extraction
- **Failure Signatures**: Caption generation failures manifest as missing elements, incorrect measurements, or improperly encoded spatial relationships, leading to reasoning errors downstream
- **First Experiment 1**: Test caption generation accuracy on simple geometric figures with known correct element counts and measurements
- **First Experiment 2**: Evaluate reasoning performance on problems where visual elements are clearly present but relationships are complex
- **First Experiment 3**: Measure the impact of caption quality variations on reasoning accuracy across different geometric problem types

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The caption generation process may introduce systematic errors or biases that propagate to reasoning failures
- The 4,641-caption dataset may not fully capture the diversity of real-world geometric figures encountered in educational or scientific contexts
- The keypoint-based evaluation metric is indirect and may not capture nuanced visual understanding failures
- The approach may lose spatial relationships that are difficult to encode textually, particularly for complex proofs or three-dimensional reasoning

## Confidence
- **High Confidence**: Empirical results showing significant accuracy improvements across multiple MLLM architectures are well-supported by presented data
- **Medium Confidence**: Generalizability to geometric domains beyond MathVerse benchmark requires further validation
- **Low Confidence**: Long-term scalability for complex geometric problems requiring multi-step visual reasoning across multiple diagrams has not been thoroughly evaluated

## Next Checks
1. Test CapGeo's performance on geometric problems requiring three-dimensional reasoning and spatial transformations that may not be easily captured in textual captions
2. Evaluate the framework's robustness when caption generation fails or produces errors, measuring how such failures propagate to downstream reasoning performance
3. Assess whether the keypoint-based evaluation metric accurately predicts reasoning performance across diverse geometric problem types, particularly those requiring intuitive spatial understanding rather than explicit measurement extraction