---
ver: rpa2
title: 'Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent'
arxiv_id: '2509.17917'
source_url: https://arxiv.org/abs/2509.17917
tags:
- reward
- agent
- action
- uni0000015c
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Orcust, a reinforcement learning framework
  that improves GUI agent performance by integrating Principle-Constrained Reward
  Modeling (PCRM) with Online VM-Grounded Trajectory Construction (OVTC). PCRM combines
  environment-verifiable rules and LLM-derived critiques to provide stepwise, interpretable
  rewards, while OVTC uses lightweight virtual machines to autonomously generate high-fidelity
  interaction trajectories.
---

# Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent

## Quick Facts
- **arXiv ID**: 2509.17917
- **Source URL**: https://arxiv.org/abs/2509.17917
- **Reference count**: 40
- **One-line primary result**: State-of-the-art GUI agent achieving 22.2% improvement on ScreenSpot and 23.9% on ScreenSpot-Pro over base model

## Executive Summary
Orcust introduces a novel reinforcement learning framework for GUI agents that combines Principle-Constrained Reward Modeling (PCRM) with Online VM-Grounded Trajectory Construction (OVTC). The framework addresses the challenge of learning effective GUI interaction policies by providing stepwise, interpretable rewards through a combination of environment-verifiable rules and LLM-derived critiques. By leveraging lightweight virtual machines to autonomously generate high-fidelity interaction trajectories, Orcust achieves significant performance improvements across eight benchmark tasks, demonstrating robust reasoning, adaptability, and data efficiency in diverse GUI environments.

## Method Summary
Orcust's methodology integrates two key innovations: PCRM and OVTC. PCRM combines Environment-Verifiable Principles (EVP) - deterministic rule-based checks for action validity - with LLM-Derived Principles (LDP) - generative critiques evaluating reasoning quality. OVTC uses QEMU/KVM virtual machines to autonomously collect interaction trajectories with milestone-based rewards. The training process follows a two-phase approach: initial supervised fine-tuning (SFT) on curated trajectories, followed by reinforcement fine-tuning (RFT) using GRPO with the PCRM reward model. The framework demonstrates effectiveness across both low-level action execution and high-level multi-step tasks on mobile, desktop, and web platforms.

## Key Results
- Achieves 22.2% improvement on ScreenSpot benchmark over base model
- Demonstrates 23.9% improvement on ScreenSpot-Pro benchmark
- Outperforms prior models across eight diverse GUI benchmarks including ScreenSpot, ScreenSpot-Pro, AndroidControl-Low/High, GUI-Act-Web, OmniAct-Web/Desktop, and GUI-Odyssey

## Why This Works (Mechanism)
The framework's success stems from its stepwise feedback mechanism that provides interpretable rewards at each interaction step rather than sparse end-task rewards. PCRM's combination of EVP and LDP ensures both environment-grounded validity and high-level reasoning quality are captured in the reward signal. OVTC's autonomous trajectory generation using lightweight VMs enables scalable data collection without manual annotation overhead. The stepwise approach allows the model to learn from intermediate successes and failures, improving both action accuracy and reasoning capabilities.

## Foundational Learning
- **GUI Interaction Grounding**: Understanding visual elements and mapping them to actionable inputs - needed to bridge perception and action in GUI environments; quick check: validate cursor positioning and element selection accuracy
- **Reinforcement Learning with Stepwise Rewards**: Learning from intermediate feedback rather than end-task outcomes - needed for efficient learning in long-horizon tasks; quick check: monitor reward distribution across steps
- **Virtual Machine-Based Environment Simulation**: Using VMs to create controlled, reproducible GUI environments - needed for autonomous trajectory collection; quick check: verify VM snapshot consistency and action replay
- **LLM-Based Reward Modeling**: Leveraging language models for complex reasoning evaluation - needed for capturing high-level task understanding; quick check: validate critique quality and consistency across similar scenarios
- **Multi-Modal Embeddings**: Integrating visual, textual, and interaction data - needed for comprehensive GUI understanding; quick check: test cross-modal alignment in embedding space

## Architecture Onboarding

**Component Map**: VM Environment -> Action Executor -> Visual State Capturer -> PCRM Rewarder -> Model Update

**Critical Path**: User task template -> OVTC trajectory generation -> SFT pre-training -> PCRM reward computation -> GRPO-based RFT -> Evaluation

**Design Tradeoffs**: The framework trades computational overhead of VM orchestration and LLM critique generation for improved data quality and more nuanced reward signals. Lightweight VMs provide better fidelity than pure simulation while avoiding full system emulation costs.

**Failure Signatures**: 
- Inconsistent rewards from LDP critiques causing training instability
- EVP principle violations due to VM environment mismatch
- Suboptimal trajectories from poorly defined task templates
- Degradation in action precision due to reward sparsity

**3 First Experiments**:
1. Implement minimal PCRM with simplified EVP rules and test on synthetic trajectories
2. Create basic OVTC environment with one task template and validate milestone detection
3. Run SFT on small curated dataset and evaluate action accuracy improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Exact definitions and implementations of all principles in P remain underspecified, affecting reward design reproducibility
- VM harness and task template specifications for OVTC are not fully detailed
- Full dataset curation pipeline and criteria for initial SFT data are not provided
- Some evaluation metrics and task definitions across benchmarks lack complete description

## Confidence
- **High Confidence**: Claims about achieving state-of-the-art results on ScreenSpot and ScreenSpot-Pro benchmarks
- **Medium Confidence**: Claims regarding improved reasoning and adaptability across diverse GUI environments
- **Low Confidence**: Claims about data efficiency and robustness of the stepwise-feedback mechanism

## Next Checks
1. Replicate the PCRM reward design using publicly available LLMs to generate critiques and compare reward distributions
2. Implement a simplified OVTC environment using open-source virtualization tools to validate milestone detection
3. Conduct ablation studies removing LDP critiques or simplifying EVP rules to isolate their contributions to performance gains