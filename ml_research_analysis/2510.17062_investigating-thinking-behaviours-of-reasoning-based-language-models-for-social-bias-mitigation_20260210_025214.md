---
ver: rpa2
title: Investigating Thinking Behaviours of Reasoning-Based Language Models for Social
  Bias Mitigation
arxiv_id: '2510.17062'
source_url: https://arxiv.org/abs/2510.17062
tags:
- bias
- reasoning
- social
- contexts
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates why reasoning-based large language models
  exhibit social bias aggregation and proposes a lightweight prompt-based mitigation
  approach. The authors first demonstrate that while reasoning is necessary for model
  performance, it can also aggregate social bias through two specific failure patterns:
  stereotype repetition and irrelevant information injection.'
---

# Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation

## Quick Facts
- arXiv ID: 2510.17062
- Source URL: https://arxiv.org/abs/2510.17062
- Reference count: 15
- Primary result: A prompt-based mitigation approach that reduces social bias in reasoning models while maintaining or improving accuracy

## Executive Summary
This paper investigates why reasoning-based large language models exhibit social bias aggregation and proposes a lightweight prompt-based mitigation approach. The authors first demonstrate that while reasoning is necessary for model performance, it can also aggregate social bias through two specific failure patterns: stereotype repetition and irrelevant information injection. Through analysis of thinking-transition tokens and content-level patterns, they find that these failure modes consistently drive biased outputs. Based on these insights, they develop a targeted prompting method that guides models to review their own reasoning against these specific failure patterns. Experiments on three benchmarks (BBQ, StereoSet, and BOLD) show their approach effectively reduces bias while maintaining or improving accuracy across multiple reasoning-based models, with average bias score reductions of up to 4 percentage points and accuracy improvements of up to 3.6 percentage points.

## Method Summary
The authors propose a targeted prompt-based mitigation approach for reducing social bias in reasoning-based language models. The method works by first having the model generate its reasoning (Chain-of-Thought) for a given task, then guiding it to review its own reasoning against two identified failure patterns: stereotype repetition and irrelevant information injection. The approach involves prompting the model to critically examine its thinking process, identify any instances of these bias-inducing patterns, and revise its reasoning accordingly. This self-review mechanism is designed to be lightweight and adaptable across different reasoning models without requiring architectural modifications or additional training. The approach was evaluated across three benchmark datasets (BBQ, StereoSet, and BOLD) using multiple reasoning-based models, demonstrating effective bias reduction while maintaining or improving task accuracy.

## Key Results
- Bias score reductions of up to 4 percentage points across three benchmark datasets
- Accuracy improvements of up to 3.6 percentage points on reasoning tasks
- The mitigation approach maintains effectiveness across multiple reasoning-based models
- Self-review mechanism successfully addresses both stereotype repetition and irrelevant information injection patterns

## Why This Works (Mechanism)
The approach works by leveraging the models' own reasoning capabilities to identify and correct bias-inducing patterns in their thinking process. By prompting models to review their Chain-of-Thought against specific failure patterns, the method creates a self-correcting mechanism that reduces bias propagation while preserving the benefits of reasoning for task performance.

## Foundational Learning
- **Social bias in language models**: Understanding how LLMs encode and reproduce societal stereotypes - needed to identify the problem space; quick check: can you explain how biases manifest in model outputs?
- **Chain-of-Thought reasoning**: The process by which models generate intermediate reasoning steps - needed to understand how bias propagates through reasoning; quick check: can you distinguish between direct answers and reasoning-based responses?
- **Bias measurement metrics**: Standardized methods for quantifying social bias in model outputs - needed to evaluate mitigation effectiveness; quick check: can you name at least two bias measurement approaches?
- **Prompt engineering**: Techniques for guiding model behavior through carefully constructed instructions - needed to implement the mitigation strategy; quick check: can you describe how prompt structure affects model responses?
- **Self-consistency in reasoning**: Models' ability to review and correct their own outputs - needed to understand the self-review mechanism; quick check: can you explain how models can be prompted to evaluate their own reasoning?

## Architecture Onboarding
- **Component map**: Input prompt -> Chain-of-Thought generation -> Self-review prompt -> Revised reasoning -> Final output
- **Critical path**: The reasoning generation and self-review steps are critical for bias mitigation, as they directly address the identified failure patterns
- **Design tradeoffs**: Lightweight prompt-based approach vs. more computationally expensive fine-tuning methods; broader applicability vs. task-specific optimization
- **Failure signatures**: Stereotype repetition in reasoning chains and injection of irrelevant information that introduces bias
- **First experiments**: 1) Baseline bias measurement on BBQ dataset, 2) Self-review prompt testing on StereoSet, 3) Cross-model effectiveness validation on BOLD

## Open Questions the Paper Calls Out
None

## Limitations
- The identified failure patterns (stereotype repetition and irrelevant information injection) may not capture the full complexity of reasoning-related bias propagation
- Analysis relies on token-level heuristics that could miss nuanced reasoning patterns in complex or domain-specific contexts
- The mitigation approach requires manual identification of failure patterns for each new task, limiting scalability
- Evaluation focuses primarily on English language benchmarks, raising questions about generalizability to other languages and cultural contexts

## Confidence
- High confidence: The experimental methodology and results demonstrating bias reduction are robust and well-documented
- Medium confidence: The theoretical framework explaining why reasoning aggregates bias is plausible but could benefit from deeper causal analysis
- Medium confidence: The effectiveness of the prompt-based mitigation approach across different models and benchmarks

## Next Checks
1. Test the mitigation approach on multilingual datasets to assess cross-cultural bias reduction effectiveness
2. Conduct ablation studies to isolate the impact of each mitigation component (self-review, pattern identification, correction guidance)
3. Evaluate the approach on more complex reasoning tasks beyond social bias detection to assess generalizability to other reasoning-intensive domains