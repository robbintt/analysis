---
ver: rpa2
title: What's in the Box? Reasoning about Unseen Objects from Multimodal Cues
arxiv_id: '2506.14212'
source_url: https://arxiv.org/abs/2506.14212
tags:
- objects
- about
- boxes
- audio
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce a neurosymbolic model that integrates visual,
  auditory, and linguistic cues to infer hidden objects in boxes. Using neural networks
  to parse multimodal inputs into structured representations, the model applies Bayesian
  inference over hypotheses about object placements.
---

# What's in the Box? Reasoning about Unseen Objects from Multimodal Cues

## Quick Facts
- arXiv ID: 2506.14212
- Source URL: https://arxiv.org/abs/2506.14212
- Reference count: 3
- Key outcome: Neurosymbolic model achieves r=0.78 correlation with human judgments on multimodal object inference task

## Executive Summary
This paper introduces a neurosymbolic approach for inferring hidden objects in boxes from multimodal inputs (visual, auditory, linguistic). The model uses neural networks to parse open-ended multimodal inputs into structured symbolic representations, then applies Bayesian inference to integrate different sources of information. Tested in a novel "What's in the Box?" task with human observers watching videos of box shaking, the full model strongly correlates with human judgments (r = 0.78), substantially outperforming unimodal ablations and large multimodal neural baselines. Results demonstrate the importance of multimodal integration for reasoning about unseen objects under uncertainty.

## Method Summary
The model parses multimodal inputs through specialized neural networks: Llama 3.1 70B for object properties (size, weight, material, rigidity with uncertainty), Gemini 2.0 Flash for box dimensions, and CLAP for audio classification. It then generates all possible object-to-box placement hypotheses and computes posteriors using Bayesian inference with conditional independence assumptions. The system marginalizes these posteriors to produce per-object probability estimates, which are compared against human judgments. The approach handles uncertainty through distributional outputs from neural parsers and rejection sampling for visual likelihood estimation.

## Key Results
- Full model achieves r=0.78 correlation with human judgments
- Audio-only model: r=0.55; Vision-only model: r=0.52
- Gemini 2.0 Flash baseline: r=0.31
- Strong human agreement on inferred object placements

## Why This Works (Mechanism)

### Mechanism 1
Neural networks can parse open-ended multimodal inputs into structured symbolic representations that enable tractable probabilistic inference over hypotheses. The model uses modality-specific neural parsers (Llama 3.1 70B for language properties, Gemini 2.0 Flash for box dimensions, CLAP for audio classification) that output structured JSON representations with physical attributes and uncertainty estimates. These representations then feed a Bayesian inference engine.

### Mechanism 2
Treating audio and visual observations as conditionally independent given object placement hypotheses provides a tractable approximation for multimodal integration. The model computes P(H|O,A) ∝ P(O|H) × P(H|A), factorizing the joint likelihood. Visual likelihood uses rejection sampling over geometric constraints; audio likelihood uses CLAP's posterior distribution over object labels.

### Mechanism 3
LLM-encoded common sense knowledge can substitute for direct physical observation when generating priors over object properties. The LLM outputs not just point estimates but distributional parameters (mean dimensions with standard deviations), enabling the model to propagate uncertainty through rejection sampling.

## Foundational Learning

- **Bayesian hypothesis testing over combinatorial spaces**: Why needed - The model generates K!S(N,K) possible object placements and must compute posteriors efficiently. Quick check - Given 3 objects and 2 boxes, can you enumerate all placement hypotheses and explain how marginalization yields P(object₁ ∈ box₁)?

- **Conditional independence factorization**: Why needed - The tractability of the model depends on factoring P(O,A|H) into separate visual and audio terms. Quick check - If shaking a heavy box produces both a deep sound AND causes the experimenter's arms to strain visibly, are audio and visual cues still conditionally independent?

- **Rejection sampling for likelihood estimation**: Why needed - The visual module uses rejection sampling (1000 draws) to estimate whether objects fit in boxes given dimensional uncertainty. Quick check - If object dimensions have high variance (σ ≈ μ/2), how many samples are needed to distinguish "fits" from "doesn't fit" with 95% confidence?

## Architecture Onboarding

- **Component map**: Video frame → Vision Parser → box dimensions → Hypothesis Generator → hypotheses; Audio track → CLAP → P(H|A); LLM → object properties → rejection sampling → P(O|H). Bayesian Engine combines → final probabilities.

- **Critical path**: Multimodal inputs flow through specialized neural parsers to generate structured representations, which feed into hypothesis generation and Bayesian inference engine. The posterior computation integrates visual, audio, and linguistic evidence to produce per-object probability estimates.

- **Design tradeoffs**: Uniform priors over placements vs. learned priors (current choice limits generalization to scenarios with non-uniform object distributions); 1000 rejection samples vs. more (trades off accuracy vs. compute; authors note model sometimes shows more uncertainty than humans); CLAP (lightweight) vs. larger audio models (authors acknowledge CLAP misses nuanced sounds in multi-source scenarios).

- **Failure signatures**: Model uncertain while humans confident → likely missing visual cues (weight inference from motion, writing on boxes); Audio fails to distinguish objects → CLAP confusion when metallic + plastic sounds mix; LLM produces implausible dimensions → downstream rejection sampling gives near-zero likelihoods for all hypotheses.

- **First 3 experiments**: 1) Ablate individual neural parsers: Replace Llama with smaller model, or CLAP with larger audio model, to measure parser quality impact on final correlation with humans. 2) Test conditional independence violation: Construct stimuli where audio-visual cues are correlated (e.g., heavy objects always produce low sounds) vs. anti-correlated, measure if model over/underweights evidence. 3) Calibrate LLM uncertainty: Compare LLM-estimated object dimensions against ground truth for held-out objects; assess whether standard deviations reflect true estimation error.

## Open Questions the Paper Calls Out

1. How can the model be extended to dynamically adjust the weighting of different modalities based on their reliability and relevance to the task?

2. How can the model incorporate motion-based visual cues to infer properties like weight that humans use when observing box shaking?

3. How would the model perform in open-ended scenarios where there isn't a predefined list of possible objects?

4. Would relaxing the assumption of conditional independence between audio and visual signals improve the model's alignment with human judgments?

## Limitations

- Model's performance depends on three neural parsers whose outputs directly determine physical likelihoods, but individual parser quality is not independently validated against ground truth
- LLM uncertainty estimates for object dimensions are assumed to be well-calibrated but this assumption lacks empirical support
- Conditional independence approximation between audio and visual cues may break down when sound and appearance correlate, potentially causing systematic biases

## Confidence

- High confidence: The neurosymbolic architecture design and integration approach are well-specified and reproducible
- Medium confidence: The human correlation results (r=0.78) are robust but parser quality remains uncertain
- Medium confidence: The superiority over large multimodal models (Gemini 2.0: r=0.31) is demonstrated but baseline details are limited

## Next Checks

1. **Parser validation study**: Collect ground truth object dimensions and properties for a held-out set; compare LLM predictions and uncertainty estimates against actual measurements

2. **Conditional independence test**: Create controlled stimuli where audio and visual cues are systematically correlated or anti-correlated; measure if model posteriors show expected over/underweighting

3. **Ablation on neural parser quality**: Systematically degrade each parser (smaller models, noisier outputs) and measure degradation in human correlation; isolate which parser contributes most to uncertainty gaps