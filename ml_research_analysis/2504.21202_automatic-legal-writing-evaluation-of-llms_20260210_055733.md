---
ver: rpa2
title: Automatic Legal Writing Evaluation of LLMs
arxiv_id: '2504.21202'
source_url: https://arxiv.org/abs/2504.21202
tags:
- legal
- evaluation
- human
- score
- exam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating open-ended legal
  writing tasks, which traditionally require costly human expertise. To tackle this,
  the authors introduce oab-bench, a benchmark of 105 questions from recent Brazilian
  Bar Exams, complete with official grading guidelines.
---

# Automatic Legal Writing Evaluation of LLMs

## Quick Facts
- arXiv ID: 2504.21202
- Source URL: https://arxiv.org/abs/2504.21202
- Authors: Ramon Pires; Roseval Malaquias Junior; Rodrigo Nogueira
- Reference count: 37
- Primary result: Claude-3.5 Sonnet achieves 7.93/10 average on Brazilian Bar Exams, passing all exams

## Executive Summary
This paper introduces oab-bench, a benchmark of 105 Brazilian Bar Exam questions with official grading guidelines, to evaluate LLMs on open-ended legal writing tasks. The authors assess four LLMs on this benchmark, finding Claude-3.5 Sonnet performs best with an average score of 7.93/10, successfully passing all exams. They also explore using LLMs as automated judges, demonstrating that frontier models like OpenAI's o1 show strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators for subjective legal writing assessment.

## Method Summary
The authors developed oab-bench, a benchmark of 105 questions from recent Brazilian Bar Exams, complete with official grading guidelines. Four LLMs were evaluated on this benchmark using both human grading and automated LLM judging. For automated evaluation, frontier models like OpenAI's o1 were used to assess responses, with their scores compared against human evaluations to measure correlation. The study focused on measuring performance across different legal writing tasks and exploring the feasibility of automated judgment systems.

## Key Results
- Claude-3.5 Sonnet achieved the highest average score of 7.93/10 on the oab-bench benchmark
- The best-performing LLM passed all Brazilian Bar Exams in the benchmark
- Automated judges using frontier models showed strong correlation with human scores when evaluating approved exams

## Why This Works (Mechanism)
The approach works because it leverages official grading guidelines that provide structured criteria for legal writing evaluation. By using these standardized rubrics, both human and automated evaluators can apply consistent judgment criteria. The strong correlation between automated and human scores for approved exams suggests that well-structured grading guidelines can effectively guide automated evaluation systems in handling the subjective aspects of legal writing assessment.

## Foundational Learning
- Brazilian Bar Exam structure and grading criteria - why needed: To understand the evaluation context and standards
- Legal writing assessment principles - why needed: To appreciate the subjective nature of legal evaluation
- LLM capabilities in reasoning and writing - why needed: To contextualize the models' performance

## Architecture Onboarding

Component map: Question bank -> Grading guidelines -> LLM responses -> Automated judge -> Human evaluation

Critical path: Question generation → Grading guideline creation → LLM response generation → Automated evaluation → Human validation

Design tradeoffs: Standardized grading guidelines vs. flexibility for nuanced legal reasoning

Failure signatures: Automated judges may struggle with borderline cases and nuanced legal arguments

First experiments:
1. Test automated judge on failing exam responses
2. Cross-system evaluation with different legal jurisdictions
3. Detailed analysis of human-automated judge disagreements

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses exclusively on Brazilian Bar Exams, limiting generalizability
- Automated judge performance only validated for approved exams, not borderline or failing responses
- Grading guidelines may contain inherent biases that affect automated evaluation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| LLM performance on oab-bench | High |
| Automated judge correlation with human scores | Medium (limited to approved exams) |
| Generalizability to other legal writing contexts | Low |

## Next Checks
1. Test the automated judge system on a diverse set of failing or borderline exam responses to assess performance across the full spectrum of legal writing quality
2. Evaluate the benchmark and automated judging approach using legal writing samples from multiple jurisdictions to test cross-system applicability
3. Conduct a detailed error analysis comparing human and automated judge disagreements to identify systematic biases or blind spots in the automated evaluation