---
ver: rpa2
title: 'REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous
  Speech Translation'
arxiv_id: '2508.04946'
source_url: https://arxiv.org/abs/2508.04946
tags:
- reina
- translation
- policy
- speech
- streaming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REINA, a method for training simultaneous
  speech translation (SimulST) models by adapting non-streaming speech translation
  models. REINA uses a novel loss function derived from information theory that encourages
  waiting for more audio input only when it increases information gain.
---

# REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation

## Quick Facts
- **arXiv ID:** 2508.04946
- **Source URL:** https://arxiv.org/abs/2508.04946
- **Reference count:** 10
- **Primary result:** Achieves state-of-the-art streaming performance with up to 21% improvement in latency/quality tradeoff

## Executive Summary
REINA introduces a novel method for training simultaneous speech translation (SimulST) models by adapting non-streaming models using an information-theoretic loss function. The approach encourages the model to wait for more audio input only when it increases information gain, rather than using fixed thresholds or heuristic policies. Applied to open-source data across multiple language pairs, REINA demonstrates state-of-the-art streaming efficiency while maintaining translation quality. The paper also introduces NoSE, a new evaluation metric that normalizes streaming efficiency against non-streaming baseline performance.

## Method Summary
REINA employs a three-stage training process to convert a non-streaming speech-to-text translation model into a low-latency simultaneous model. First, a base model is trained using a Whisper Medium encoder and 16-layer Transformer decoder on multi-task learning objectives (ASR, NMT, and S2TT). Second, the model undergoes truncated finetuning on 80% randomly truncated audio clips to improve partial-audio estimation. Finally, a small policy network is trained on top of the decoder hidden states using the REINA loss, which maximizes covariance between policy output and batch-normalized differences in log-probabilities between partial and full audio contexts. The method uses a novel entropy-based regularization term to encourage adaptive waiting behavior only when information gain is present.

## Key Results
- Achieves state-of-the-art streaming performance on multiple language pairs
- Improves latency/quality tradeoff by up to 21% compared to prior approaches
- Introduces NoSE metric that provides more meaningful comparison of streaming efficiency
- Demonstrates effective adaptive READ/WRITE policy without requiring manual threshold tuning

## Why This Works (Mechanism)
REINA works by directly optimizing the policy network to maximize information gain from additional audio input. The core mechanism is the REINA loss function, which encourages the model to wait for more audio only when the additional context significantly improves translation quality. By using batch-normalized log-probability differences between partial and full audio contexts as the information gain signal, the policy learns to make optimal wait/emit decisions. The entropy regularization term ensures the policy doesn't wait unnecessarily, while monotonicity loss prevents erratic behavior in streaming scenarios.

## Foundational Learning

**Information Theory and Mutual Information:** Understanding how information gain is measured through differences in log-probabilities between partial and full audio contexts. *Why needed:* Forms the theoretical foundation for the REINA loss function. *Quick check:* Verify that batch normalization of log-probability differences produces meaningful information gain signals.

**Multi-task Learning with Shared Encoders:** Training a single encoder for multiple tasks (ASR, NMT, S2TT) with task-specific decoders. *Why needed:* Enables efficient pretraining of the base model before policy training. *Quick check:* Confirm that the encoder learns shared representations across all three tasks.

**Streaming Beam Search with Adaptive Policies:** Implementing beam search that can dynamically switch between reading audio and emitting translations based on policy output. *Why needed:* Required for inference with the trained policy network. *Quick check:* Ensure the patience factor correctly handles consecutive READ decisions without emission.

## Architecture Onboarding

**Component Map:** Whisper Medium Encoder -> 16-layer Transformer Decoder -> 2-layer Policy Network

**Critical Path:** Audio input → Encoder → Decoder hidden states → Policy Network → READ/WRITE decision → Beam Search

**Design Tradeoffs:** The architecture trades model complexity (larger decoder) for better streaming performance, using the policy network to compensate for the lack of native streaming architecture in the base model.

**Failure Signatures:** Policy values exploding to infinity indicates insufficient L2 regularization; poor low-latency performance suggests monotonicity loss is not properly enforced; inconsistent information gain estimation points to issues in batch normalization or truncated training.

**First Experiments:** 1) Train base model with multi-task learning and verify convergence across all tasks, 2) Implement truncated finetuning and measure improvement in partial-audio log-probability estimation, 3) Train policy network with REINA loss and verify stable policy values during training.

## Open Questions the Paper Calls Out

**SimulS2ST Extension:** The authors explicitly state that extending REINA to simultaneous speech-to-speech translation is the next step for enabling real-time cross-lingual interaction, though this has not been validated.

**Low-Resource Language Performance:** The current model relies heavily on large-scale synthetic data augmentation, raising questions about effectiveness for language pairs without such resources.

**Automated Threshold Selection:** The paper notes that determining optimal policy thresholds currently requires manual trial-and-error sweeps, suggesting the need for automated threshold selection methods.

## Limitations

- Relies on synthetic data generated by an unspecified in-house NMT model, creating reproducibility challenges
- Uses a different base architecture (Whisper Medium + Transformer decoder) compared to typical cascaded or streaming-native approaches
- Performance improvements depend on the quality and quantity of available training data

## Confidence

**High Confidence:** Core REINA methodology and three-stage training procedure are clearly specified and reproducible given appropriate datasets.

**Medium Confidence:** Reported performance improvements are credible but depend on implementation details of custom components and synthetic data quality.

**Low Confidence:** Direct comparison with other SimulST methods may be challenging due to architectural differences.

## Next Checks

1. **Policy Training Stability:** Monitor L2 regularization term during policy training to ensure stable policy values and prevent divergence.

2. **NoSE Metric Validation:** Reproduce NoSE calculation on a simple streaming baseline to confirm it correctly normalizes AL reduction against non-streaming performance.

3. **Truncated Training Impact:** Compare base model performance with and without Stage 2 truncated finetuning to quantify contribution to partial-audio estimation accuracy.