---
ver: rpa2
title: 'RAD: Retrieval High-quality Demonstrations to Enhance Decision-making'
arxiv_id: '2507.15356'
source_url: https://arxiv.org/abs/2507.15356
tags:
- trajectory
- offline
- states
- target
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAD addresses limited generalization in offline RL due to sparse
  data by dynamically retrieving high-return states from offline datasets and using
  a condition-guided diffusion model to plan toward them. This retrieval-based approach
  stitches trajectories flexibly and improves adaptability in out-of-distribution
  scenarios without relying on static augmentation.
---

# RAD: Retrieval High-quality Demonstrations to Enhance Decision-making

## Quick Facts
- arXiv ID: 2507.15356
- Source URL: https://arxiv.org/abs/2507.15356
- Reference count: 30
- Average normalized return of 81.2 on D4RL MuJoCo tasks

## Executive Summary
RAD addresses limited generalization in offline RL by dynamically retrieving high-return states from offline datasets and using a condition-guided diffusion model to plan toward them. This retrieval-based approach stitches trajectories flexibly and improves adaptability in out-of-distribution scenarios without relying on static augmentation. Evaluated on D4RL MuJoCo tasks, RAD achieves an average normalized return of 81.2, outperforming diffusion-based baselines like Diffuser (77.5) and Decision Transformer (78.9).

## Method Summary
RAD constructs a dense database from offline datasets containing state representations, trajectory IDs, timestep indices, and discounted returns. At inference, given current state s_t, it retrieves top-k states by cosine similarity, filters by return threshold, and selects the candidate with longest remaining trajectory. A 4-layer MLP estimates temporal distance to the target, which anchors the diffusion model's denoising process. The condition-guided diffusion model interpolates between current and target states while optimizing for trajectory return, producing coherent long-horizon trajectories.

## Key Results
- Achieves average normalized return of 81.2 on D4RL MuJoCo tasks
- Outperforms Diffuser baseline (77.5) and Decision Transformer (78.9)
- Notable gains in Hopper (82.5) and Walker2d (89.1) environments
- Performance drops in sparse datasets (HalfCheetah: 84.9 vs. ReDiffuser's 88.3)

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Guided Target Selection
Dynamically retrieving high-return states as planning targets enables trajectory stitching beyond static dataset coverage. Given current state s_t, compute cosine similarity against all states in a pre-built database. Filter top-k candidates by return threshold (|v_j - v*| ≤ η), then select the target with longest remaining trajectory to maximize planning horizon. Fails when dataset is sparse/noisy because retrieval cannot find semantically relevant high-return states.

### Mechanism 2: Step Estimation for Temporal Coherence
Predicting temporal distance to the target prevents incoherent trajectories (overshooting, stalling, degeneration). A 4-layer MLP f_ψ takes (s_t, s_g) and outputs predicted step count î, which serves as a spatiotemporal anchor in the conditioning input. Ablation shows performance drop when replaced with random step selection, especially in environments with variable trajectory lengths.

### Mechanism 3: Condition-Guided Diffusion with Return Guidance
Anchoring the denoising process on current and target states while guiding toward high returns produces coherent long-horizon trajectories. Initialize noisy trajectory τ^N with s_t at position 0 and s_g at position î. Reverse denoising is parameterized with return gradient ∇J_φ to steer generation toward high-return trajectories. If target is unreachable or return guidance conflicts with anchor constraints, generated trajectories may be infeasible.

## Foundational Learning

- **Offline RL Distribution Shift**: Understanding why offline RL fails with OOD states is essential. Quick check: Why can't standard RL algorithms simply train on offline data without modification?
- **Diffusion Models for Trajectory Generation**: RAD builds on Diffuser-style trajectory diffusion. Quick check: Explain the forward/reverse process in diffusion and how conditioning modifies generation.
- **Non-Parametric Retrieval in RL**: RAD's retrieval module differs from parametric policy learning. Quick check: What are the tradeoffs between retrieval-based vs. parametric policy approaches?

## Architecture Onboarding

- **Component map**: Database Builder -> Target Selection Module -> Step Estimation MLP -> Condition-Guided Diffuser
- **Critical path**: 1) Receive current state s_t 2) Query database → retrieve s_g and compute î 3) Initialize noisy trajectory with anchors at positions 0 and î 4) Run N-step denoising with return guidance 5) Execute first action â_t
- **Design tradeoffs**: Similarity threshold δ (0.9 optimal), Top-k selection (6 balances relevance/diversity), Diffusion horizon H (32 for MuJoCo)
- **Failure signatures**: Performance degradation on sparse/noisy datasets, high variance on some tasks, retrieval returns irrelevant targets when state similarity doesn't correlate with reachability
- **First 3 experiments**: 1) Ablate retrieval: Replace target selection with random sampling 2) Vary similarity threshold: Test δ ∈ {0.0, 0.5, 0.8, 0.9} 3) Compare horizon flexibility: Fix target timestep vs. random sampling during training

## Open Questions the Paper Calls Out

### Open Question 1
Can learned goal proposals or offline exploration priors extend RAD to sparse-reward environments where high-value target states are absent from the offline dataset? Authors state: "RAD also assumes that such target states exist in the dataset. In sparse environments, this may not hold. Future work could explore learned goal proposals or offline exploration priors."

### Open Question 2
How can RAD be made robust to noisy or stochastic environments where retrieval may return unreliable or unreachable states? Authors state: "Its success depends on retrieving reachable, high-value states; in noisy or stochastic settings, retrieval may fail. Improving robustness through learned embeddings or value-aware filtering is a key direction."

### Open Question 3
Does incorporating global planning or multi-step retrieval improve long-horizon reasoning compared to RAD's local per-decision retrieval? Authors state: "RAD performs retrieval locally per decision point; incorporating global planning or multi-step retrieval may further enhance long-horizon reasoning."

## Limitations
- Performance degrades on sparse/noisy datasets where retrieval cannot find relevant high-return states
- Method's sensitivity to dataset density is evident (HalfCheetah performance drops to 84.9)
- Computational overhead from retrieval database during inference is not addressed

## Confidence
- **High Confidence**: Retrieval-based target selection improves performance over static baselines
- **Medium Confidence**: Step estimation provides temporal coherence benefits
- **Low Confidence**: Return guidance mechanism details are insufficient to assess implementation fidelity

## Next Checks
1. **Dataset Density Test**: Evaluate RAD on increasingly sparse versions of D4RL datasets to quantify performance degradation and retrieval failure rates
2. **Architectural Fidelity Check**: Implement a minimal RAD variant without return guidance (only anchor-based diffusion) to isolate the contribution of return-based denoising
3. **Retrieval vs. Parametric Policy**: Compare RAD against a parametric policy trained on the same retrieval database to assess non-parametric retrieval advantages and computational tradeoffs