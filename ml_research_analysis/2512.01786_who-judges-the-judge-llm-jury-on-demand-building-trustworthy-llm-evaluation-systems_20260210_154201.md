---
ver: rpa2
title: 'Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation
  Systems'
arxiv_id: '2512.01786'
source_url: https://arxiv.org/abs/2512.01786
tags:
- jury
- judge
- score
- data
- judges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LLM Jury-on-Demand, a dynamic framework that
  improves LLM evaluation by learning to predict when individual judges will agree
  with human experts. Instead of relying on static averaging, the system extracts
  features from input and output texts, trains XGBoost models to estimate judge reliability
  per instance, and dynamically selects the most reliable judges for each case.
---

# Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems

## Quick Facts
- **arXiv ID**: 2512.01786
- **Source URL**: https://arxiv.org/abs/2512.01786
- **Reference count**: 40
- **Primary result**: Adaptive jury selection using XGBoost reliability predictors improves LLM evaluation Kendall's Tau correlation by 0.01-0.18 over static baselines

## Executive Summary
This paper introduces LLM Jury-on-Demand, a dynamic framework that improves LLM evaluation by learning to predict when individual judges will agree with human experts. Instead of relying on static averaging, the system extracts features from input and output texts, trains XGBoost models to estimate judge reliability per instance, and dynamically selects the most reliable judges for each case. Their reliability scores are then used as weights to aggregate final evaluations. Tested on summarization and RAG benchmarks, this approach achieved significantly higher Kendall's Tau correlation with human judgments than both single-judge and static-jury baselines, demonstrating that adaptive, context-aware jury selection leads to more trustworthy automated evaluation systems.

## Method Summary
The framework trains per-judge XGBoost classifiers to predict reliability based on text features, then dynamically selects the top K most reliable judges per instance and aggregates their scores using reliability as weights. The method extracts ~50 text features (size, complexity, embeddings) from input/output pairs, trains binary classifiers to predict if a judge's score will be within tolerance τ of human judgment, tunes jury size K and tolerance on validation data, and aggregates selected judges' scores via weighted averaging. This approach moves beyond static averaging by recognizing that judge reliability varies predictably based on textual features of the evaluation instance.

## Key Results
- Kendall's Tau correlation improvements of 0.01-0.18 absolute over all static jury baselines across summarization and RAG tasks
- Adaptive jury selection outperforms both single-judge evaluation and static jury approaches consistently
- Feature importance varies by task: text-size features dominate RAG evaluation while embedding features are more important for summarization
- Optimal jury size K exists (typically 5-7) balancing robustness against inclusion of unreliable judges

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Reliability Prediction
- Claim: Learning to predict per-instance judge reliability enables adaptive jury selection that outperforms static approaches.
- Mechanism: XGBoost models are trained on ~50 text-derived features (size, complexity, embeddings) to predict the probability that a given judge will agree with human experts for a specific instance. The output probability serves as a reliability score.
- Core assumption: A judge's reliability is not constant but varies predictably based on textual features of the input/output.
- Evidence anchors:
  - [abstract] "Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features."
  - [Section 3.2] "Key feature categories are: Text Size Features... Text Complexity Features... Embedding-Related Features..."
  - [Section 5.1] "Our Jury-on-Demand framework consistently outperforms all baselines across every task and metric."
- Break condition: If the relationship between text features and judge reliability is weak or highly unstable across domains, this mechanism's benefit will degrade.

### Mechanism 2: Dynamic Jury Selection and Weighting
- Claim: Selecting a subset of the most reliable judges per instance and weighting their scores by reliability improves final evaluation scores.
- Mechanism: For each data point, top K judges (e.g., K=5) with highest predicted reliability are selected. Their raw scores are aggregated via a weighted average, with weights proportional to their reliability scores.
- Core assumption: Higher predicted reliability correlates with better alignment with human judgment; aggregating reliable judges reduces noise and bias.
- Evidence anchors:
  - [abstract] "For each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights."
  - [Section 3.4] "The final evaluation score is calculated as a weighted average of the raw scores from those K jury members."
  - [Section 5.2] Analysis shows selection frequency aligns with performance: when Gemini 2.0 Flash performance drops at high character counts, its selection percentage drops correspondingly.
- Break condition: If all available judges are unreliable for a specific input type, or if the weighting scheme is too sensitive to noise in reliability predictions, this will not improve over baselines.

### Mechanism 3: Task-Specific Feature Importance
- Claim: The importance of different text features for predicting reliability varies by task and judge, justifying the use of a rich, multi-category feature set.
- Mechanism: XGBoost's internal feature importance reveals which features drive predictions. For RAG, text-size features are more important; for summarization, embedding features are more prominent.
- Core assumption: The features that signal reliability are not universal but depend on the evaluation task's nature.
- Evidence anchors:
  - [Section 5.2] "Text size-related features... are more prominent in RAG tasks. In contrast, embedding-based features... play a more significant role in summarization tasks."
  - [Appendix H.1] Ablation study shows removing embedding features hurts summarization more than RAG, and vice versa for text-size features.
- Break condition: If feature importance patterns shift drastically on new, unseen domains, the fixed, trained reliability models may not generalize well.

## Foundational Learning

### Concept: Kendall's Tau Correlation Coefficient
- Why needed here: This is the primary evaluation metric used to measure how well the jury's scores align with human expert rankings.
- Quick check question: Why is a rank correlation metric like Kendall's Tau preferred over exact score agreement measures (e.g., RMSE) for this task?

### Concept: Gradient-Boosted Decision Trees (XGBoost)
- Why needed here: This is the core machine learning model used to learn the mapping from text features to judge reliability.
- Quick check question: Why is XGBoost a suitable choice for this problem over, for example, a large neural network?

### Concept: Min-Max Normalization
- Why needed here: Human and model scores come from different scales (e.g., 1-5, 0-2). They must be normalized to a common [0,1] range before calculating tolerance for "good" labels.
- Quick check question: What would happen to the labeling process if scores were not normalized?

## Architecture Onboarding

### Component map:
Input Data -> Feature Extractor -> Reliability Predictors (N models) -> Jury Selector -> Score Aggregator

### Critical path:
The training and hyperparameter tuning of the **Reliability Predictors** is the most critical stage. Poorly tuned predictors (suboptimal K, τ, or XGBoost parameters) will propagate errors through selection and aggregation.

### Design tradeoffs:
- **Jury Size (K)**: Larger K provides robustness but risks including unreliable judges. The paper finds an optimal range exists.
- **Tolerance (τ)**: Higher tolerance for labeling makes training easier but may reduce signal precision.
- **Feature Set**: Richer features capture more signals but increase complexity and overfitting risk.

### Failure signatures:
- **Low Reliability Predictions**: If all judges have low predicted reliability for a data subset, the final score will be uncertain.
- **Poor Generalization**: Performance degrades on unseen domains if training data is not representative.
- **Calibration Issues**: Some judges have systematic score bias that reliability weighting does not correct.

### First 3 experiments:
1. **Baselines vs. Jury-on-Demand**: Compare the framework's Kendall's Tau against four static jury baselines (Average-All, Average-TopK, Weighted-Regression, Weighted-Tau) and all single judges on provided test sets.
2. **Ablation on Feature Sets**: Remove one feature category (e.g., embedding features) at a time and measure the drop in Kendall's Tau to confirm which features are most critical for a given task.
3. **Judge Selection Analysis**: For a specific task (e.g., RAG-Groundedness), bin test data by a key feature (e.g., character count) and observe how selection frequency of a known-weak judge changes across bins to validate adaptive selection.

## Open Questions the Paper Calls Out
None

## Limitations
- Feature importance patterns may not generalize to new domains, limiting cross-task applicability
- Computational overhead from training N separate XGBoost models per task-metric combination
- Systematic calibration issues for certain judges (LLAMA-3.2, Gemma, Phi-4) that reliability weighting alone cannot fully correct

## Confidence

**High confidence** in the core mechanism: The Kendall's Tau correlation improvements are substantial (0.01-0.18 absolute gains) with statistical significance confirmed by Wilcoxon tests and Cliff's delta. The adaptive jury selection logic is sound and the feature importance analysis provides internal validation.

**Medium confidence** in generalizability: While the framework performs well across six summarization and seven RAG benchmarks, the paper notes potential generalization challenges to completely unseen domains (Appendix J). The feature importance patterns may not transfer when evaluation tasks have fundamentally different characteristics.

**Low confidence** in complete reproducibility: Critical hyperparameters (final XGBoost settings, exact K and τ values per task-metric) are not fully specified. The embedding model for PCA features is also unspecified, though likely GloVe/BERT based on citations.

## Next Checks
1. **Cross-Domain Generalization Test**: Apply the trained reliability predictors to a completely different LLM evaluation task (e.g., code generation or mathematical reasoning) and measure Kendall's Tau degradation compared to domain-matched training.

2. **Judge Diversity Impact**: Systematically vary the pool of available judges (from 3 to 10) and measure how jury performance scales. Identify whether the framework maintains benefits with fewer judges or requires diversity in judge capabilities.

3. **Real-World Deployment Analysis**: Implement the framework in a live LLM evaluation pipeline and monitor whether predicted reliability scores correlate with actual human judgment consistency over time, particularly for edge cases where judges disagree.