---
ver: rpa2
title: A Single Character can Make or Break Your LLM Evals
arxiv_id: '2510.05152'
source_url: https://arxiv.org/abs/2510.05152
tags:
- delimiter
- b-instruct
- performance
- choice
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how the choice of a single character delimiter\
  \ used to separate demonstration examples in few-shot prompting significantly affects\
  \ the performance of large language models (LLMs) across multiple benchmarks (MMLU,\
  \ ARC, and Commonsense QA) and model families (Llama, Gemma, and Qwen). The authors\
  \ find that performance can vary by as much as \xB123% depending on the delimiter\
  \ used, and that model rankings can be manipulated to place any model in the lead\
  \ simply by changing the delimiter."
---

# A Single Character can Make or Break Your LLM Evals

## Quick Facts
- **arXiv ID**: 2510.05152
- **Source URL**: https://arxiv.org/abs/2510.05152
- **Reference count**: 40
- **Primary result**: Choice of delimiter between few-shot examples can cause ±23% performance variance and manipulate model rankings

## Executive Summary
This paper reveals that the choice of a single character delimiter used to separate demonstration examples in few-shot prompting can dramatically affect large language model performance across multiple benchmarks and model families. The authors find that performance can vary by as much as ±23% depending on the delimiter used, and that model rankings can be manipulated to place any model in the lead simply by changing the delimiter. They propose that specific delimiters function as attention steering mechanisms and demonstrate that explicitly specifying the chosen delimiter in the prompt consistently boosts robustness.

## Method Summary
The study evaluates how single-character delimiters affect LLM performance using the lm-evaluation-harness on three benchmarks: MMLU, ARC-Challenge, and CommonsenseQA. They test 30 non-alphanumeric ASCII delimiters across three model families (Llama-3.1, Gemma-2, Qwen2.5) and various scales. The method involves modifying task YAML files to change only the `fewshot_delimiter` field while keeping all other parameters constant. They also test robustness by explicitly specifying the delimiter in the prompt text and analyze attention patterns using the dictionary lookup task from Chen et al. (2024).

## Key Results
- Performance spread across delimiters reaches ±23% on MMLU for Qwen2.5-7B-instruct
- Model rankings are manipulable: any model can be made to rank #1 by choosing an appropriate delimiter
- Explicitly specifying the delimiter in the prompt provides robust performance gains (e.g., +27.9% on CommonsenseQA for Llama-3.1-8B)
- Good delimiters like "\n" and "!" steer attention toward key tokens, increasing attention scores by up to 25%
- This brittleness persists across topics, model families, and scales, and is not mitigated by supervised fine-tuning with random delimiters

## Why This Works (Mechanism)

### Mechanism 1: Attention Steering via Structural Cues
Specific delimiters function as attention steering mechanisms, directing model focus toward relevant task tokens. Good delimiters increase attention scores assigned to key tokens in demonstration examples by acting as distinct structural boundaries that help the self-attention mechanism discriminate between formatting noise and task-relevant signal. This mechanism likely fails if the delimiter token is semantically similar to content tokens, causing attention to be diffused rather than focused.

### Mechanism 2: Distributional Mismatch (Training Data Bias)
Performance variability stems from a mismatch between the prompt's delimiter distribution and the model's pre-training/instruction-tuning data distribution. Models optimized on specific formats (e.g., Markdown, code) have strong priors for high-frequency delimiters like "\n" (67.48% in Tulu SFT dataset). Using rare characters creates out-of-distribution inputs, forcing the model to rely on weaker heuristics rather than robust in-context learning circuits.

### Mechanism 3: Ambiguity Resolution via Explicit Instruction
Explicitly defining the delimiter in the prompt robustifies performance by forcing a specific interpretation of the separator character. Adding instructions like "separated by X" forces the model to treat the character as a structural operator rather than a content token, overriding default statistical pattern matching with rule-following mode and reducing variance from format guessing.

## Foundational Learning

- **Concept: In-Context Learning (ICL) & Demonstrations**
  - Why needed: The paper analyzes how the separator of ICL examples affects performance
  - Quick check: If I provide 3 examples of addition in the prompt, does the model update its weights to perform addition, or does it condition its prediction on the examples?

- **Concept: Self-Attention & Tokenization**
  - Why needed: The proposed mechanism involves "steering attention"
  - Quick check: Does a tokenizer necessarily treat a single character (like `#`) as a single token? (Hint: It depends on frequency/merges)

- **Concept: Evaluation Harnesses (LM-Eval-Harness)**
  - Why needed: The paper utilizes a specific evaluation pipeline to isolate the delimiter variable
  - Quick check: In an evaluation harness, what is the difference between the `fewshot_delimiter` and the `target_delimiter`?

## Architecture Onboarding

**Component map:**
- Prompt Constructor -> Tokenizer -> Model (Transformer) -> Filter/Parser -> Attention Probe (Captum)

**Critical path:**
1. Configuration: Define delimiter `X` in YAML config (`fewshot_delimiter`)
2. Formatting: Harness concatenates examples: `Q1 A1 [X] Q2 A2 [X]`
3. Inference: Model processes token stream
4. Extraction: Regex looks for answer after final `[X]`

**Design tradeoffs:**
- Standardization vs. Performance: Using `\n` is standard but might not yield SOTA for specific models
- Explicit Specification vs. Token Efficiency: Adding "separated by X" consumes context window tokens but stabilizes ranking

**Failure signatures:**
- Ranking Manipulation: Model drops from #1 to #3 solely because evaluator switched from `\n` to `&`
- Random Baseline: Accuracy drops to ~25% on MMLU when using ambiguous delimiters like `?`

**First 3 experiments:**
1. Baseline Sweep: Run MMLU on target model using 5 recommended delimiters (`\n`, `!`, `@`, etc.) to establish performance variance range
2. Instruction Ablation: Compare "Standard Prompt" vs. "Prompt with Explicit Separator Instruction" on worst-performing delimiter to test recovery
3. Attention Visualization: Run 4-shot task with good vs. bad delimiter and visualize attention map of final query token

## Open Questions the Paper Calls Out
None

## Limitations
- The exact relative contribution of attention mechanisms versus training data bias to performance variance remains unclear
- The effectiveness of explicit delimiter specification in compensating for underlying architectural limitations is uncertain
- Generalizability to non-chat-based models or specialized domains (coding, mathematical reasoning) is unknown

## Confidence

**High Confidence:**
- Performance variance due to delimiter choice is real and substantial (±23% spread)
- Model rankings are manipulable through delimiter selection
- Explicitly specifying the delimiter improves robustness across benchmarks
- Attention scores differ between good and bad delimiters in dictionary lookup tasks

**Medium Confidence:**
- The mechanism involves attention steering toward key tokens
- Distributional mismatch with training data contributes to brittleness
- Recommended delimiters (\n, !) are universally effective

**Low Confidence:**
- The exact relative contribution of attention mechanisms versus training data bias
- The effectiveness of explicit specification in compensating for architectural limitations
- Generalizability to non-chat-based models or specialized domains

## Next Checks

**Check 1:** Test delimiter sensitivity on OpenAI GPT models and Anthropic Claude models using same benchmarks to determine if brittleness is architecture-specific or general.

**Check 2:** Compare performance when explicitly specifying the delimiter versus using a control instruction to isolate whether robustness comes from explicit formatting or additional context.

**Check 3:** Extend attention analysis beyond dictionary lookup task to actual benchmark tasks (MMLU, ARC) by visualizing attention patterns for good versus bad delimiters on the same input examples.