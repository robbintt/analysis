---
ver: rpa2
title: 'SafeMT: Multi-turn Safety for Multimodal Language Models'
arxiv_id: '2510.12133'
source_url: https://arxiv.org/abs/2510.12133
tags:
- safety
- user
- dialogue
- assistant
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SafeMT is the first benchmark for evaluating the multi-turn safety
  of multimodal large language models (MLLMs) using dialogues and images. It includes
  2,000 harmful queries and 8,000 dialogues spanning 17 scenarios and four jailbreak
  methods.
---

# SafeMT: Multi-turn Safety for Multimodal Language Models

## Quick Facts
- arXiv ID: 2510.12133
- Source URL: https://arxiv.org/abs/2510.12133
- Authors: Han Zhu, Juntao Dai, Jiaming Ji, Haoran Li, Chengkun Cai, Pengcheng Wen, Chi-Min Chan, Boyuan Chen, Yaodong Yang, Sirui Han, Yike Guo
- Reference count: 38
- Primary result: First benchmark for multi-turn safety of MLLMs using dialogues and images, showing ASR increases with dialogue length and visual components raise risks

## Executive Summary
SafeMT introduces a benchmark for evaluating multi-turn safety in multimodal large language models (MLLMs) through dialogues and images. The benchmark includes 2,000 harmful queries and 8,000 dialogues spanning 17 scenarios and four jailbreak methods. Results demonstrate that attack success rates increase significantly with dialogue length, particularly in financial and political scenarios, and that visual components introduce additional safety vulnerabilities. The paper proposes a Safety Index that measures vulnerability by considering both attack success rate and the dialogue turn at which models are compromised, along with a dialogue safety moderator that outperforms existing guard models by detecting malicious intent in multi-turn conversations.

## Method Summary
SafeMT constructs a dataset of 2,000 harmful image-query pairs and 8,000 multi-turn dialogues (2/4/6/8 turns) across 17 safety scenarios using four jailbreak methods. A Safety Index (SI) is introduced to measure vulnerability, applying exponential decay weighting to earlier-turn attacks and including a stability term for consistency. A dialogue safety moderator is trained on Gemma-3-4B-it using 7,000 samples, fine-tuned via SFT with frozen vision encoders. The model predicts intent and generates context-aware guard prompts. Evaluation uses GPT-4o-mini as a safety classifier to label responses as safe/unsafe per scenario criteria.

## Key Results
- ASR increases significantly with dialogue length across all evaluated models
- Visual components raise safety risks beyond text-only interactions
- Safety Index captures both vulnerability timing and defense consistency better than raw ASR
- Dialogue safety moderator outperforms existing guard models in detecting malicious intent

## Why This Works (Mechanism)

### Mechanism 1
Multi-turn dialogues progressively erode safety mechanisms in MLLMs, increasing attack success rates. Models maintain insufficient context-aware safety state across dialogue turns, allowing harmful requests to be decomposed into innocuous-seeming segments or established cooperative behavior to be exploited.

### Mechanism 2
Visual components introduce additional safety vulnerabilities beyond text-only interactions. Images serve as attack vectors through attention redirection, embedding harmful context in cross-modal reasoning, and exploiting models' reduced vigilance when describing their own visual interpretations.

### Mechanism 3
The Safety Index (SI) captures both vulnerability timing and defense consistency better than raw ASR. SI applies exponential decay weighting (e^-k) to earlier-turn attacks, penalizing models compromised quickly, multiplied by a stability term measuring variance in safety across turns.

## Foundational Learning

- **Attack Success Rate (ASR)**: Foundation for Safety Index; measures proportion of harmful responses; must understand before interpreting SI improvements. Quick check: Given 100 queries where a model produces 23 harmful responses, what is the ASR?

- **Jailbreak Attack Taxonomy**: SafeMT uses four methods (Role Play, In-context Learning, Image Reference, Topic Change); understanding these informs defense design. Quick check: Which jailbreak method progressively introduces harm after establishing benign context?

- **Cross-Modal Safety Alignment**: Visual components create unique attack surfaces; standard text-based RLHF may not transfer. Quick check: Why might a model refuse a harmful text query but comply when the same intent is split across image description + follow-up question?

## Architecture Onboarding

- **Component map**: Harmful queries + images → Dialogue generation (GPT-4o/Claude/Gemini) → Refusal refinement → Moderator training (7K samples) → Evaluation (1K held-out) → ASR/SI calculation

- **Critical path**: Data collection → Dialogue generation with jailbreak methods → Manual refinement of refusal-containing dialogues → Moderator training with frozen vision encoder → GPT-4o-mini evaluation → SI computation

- **Design tradeoffs**: SI weights earlier turns heavier (e^-k decay) vs. uniform weighting—prioritizes rapid detection but may undervalue persistent threats; Moderator as plug-and-play module vs. integrated fine-tuning—flexibility but potential overfitting; GPT-4o-mini as evaluator vs. human annotation—scalability but 11% labeling error

- **Failure signatures**: Over-refusal causing excessive caution in Llama models; inconsistent protection across turns; financial domain gap with >65% ASR across all models

- **First 3 experiments**: 1) Baseline ASR measurement across turn lengths, 2) Ablation on modality comparing SafeMT vs text-only version, 3) Moderator integration test measuring ASR reduction vs baselines

## Open Questions the Paper Calls Out

- How can safety algorithms be designed to universally reduce excessive refusal behavior without compromising strict safety protocols? The current dialogue safety moderator causes overfitting in models like Llama-3.2, leading excessive benign query rejection.

- What specific safety vulnerabilities are introduced by multi-modal chain-of-thought (CoT) reasoning in MLLMs? The current benchmark evaluates standard dialogue turns but not how explicit reasoning steps combined with visual inputs might be exploited.

- How does inclusion of multiple images or adversarial visual inputs affect the Safety Index and robustness of MLLMs? The current scope is limited to single natural images, leaving unclear if linear risk increase holds with visual complexity or adversarial noise.

## Limitations

- Benchmark relies on GPT-4o-mini for safety classification with 89% agreement rate but no inter-annotator reliability data for gold standard
- Manual refinement process for refusal-containing dialogues is described but not operationalized
- Evaluation protocol treats all harmful queries as equally dangerous despite varying severity

## Confidence

- **High confidence**: ASR increases with dialogue turns (direct measurements across multiple models); visual components increase safety risks (cross-modal vs text-only comparisons); dialogue safety moderator improves protection (validated against baselines)
- **Medium confidence**: SI metric meaningfully captures timing and consistency (sound design but lacks direct validation against alternatives); jailbreak methods progressively reconstruct harmful intent (plausible but not exhaustively validated)
- **Low confidence**: 89% agreement rate between GPT-4o-mini and human annotators based on single annotator validation with no expertise or adjudication process information

## Next Checks

1. **Human validation study**: Have three independent annotators label 200 randomly sampled responses from SafeMT, calculate inter-annotator agreement, and compare to GPT-4o-mini outputs to quantify classification reliability

2. **Temporal threat analysis**: Analyze ASR distribution across turns to determine if early attacks are genuinely more frequent/successful than late attacks, validating the exponential weighting assumption in SI

3. **Cross-dataset generalization**: Evaluate SafeMT-trained safety moderators on an independent multi-turn safety benchmark (e.g., MTMCS-Bench) to test whether improvements transfer beyond the original dataset distribution