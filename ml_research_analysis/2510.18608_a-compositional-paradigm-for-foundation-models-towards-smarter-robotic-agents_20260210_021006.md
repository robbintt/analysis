---
ver: rpa2
title: 'A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents'
arxiv_id: '2510.18608'
source_url: https://arxiv.org/abs/2510.18608
tags:
- learning
- more
- robotic
- tasks
- robotics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a compositional paradigm for foundation models
  to create more adaptable, efficient, and intelligent AI systems. The authors address
  the limitations of current foundation models, which struggle with dynamic real-world
  scenarios without full retraining and show diminishing returns from simple scaling.
---

# A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents

## Quick Facts
- arXiv ID: 2510.18608
- Source URL: https://arxiv.org/abs/2510.18608
- Reference count: 16
- This paper proposes a compositional paradigm for foundation models to create more adaptable, efficient, and intelligent AI systems.

## Executive Summary
This paper addresses the limitations of current foundation models, which struggle with dynamic real-world scenarios without full retraining and show diminishing returns from simple scaling. The authors introduce Continual Learning and Compositionality principles to enhance foundation models, particularly in robotics. They propose a compositional paradigm that enables flexible and efficient adaptation across tasks by composing specialized, lightweight adapters with pre-trained models. The approach demonstrates significant improvements in both image classification and robotic manipulation tasks.

## Method Summary
The paper proposes a compositional paradigm that leverages Continual Learning and Compositionality to enhance foundation models. The method involves training separate task-specific adapters using Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA, then composing these adapters through hierarchical merging or attention-based routing. For image classification, the Hierarchical Adapter Merging (HAM) method groups similar adapters and merges them in stages to reduce interference. For robotics, the Weighted Selector Adapter (WSA) uses attention mechanisms to dynamically route and scale adapter outputs for manipulation tasks. The approach aims to enable adaptation to new tasks and environments without full model retraining.

## Key Results
- For image classification on CUB200, HAM achieved 55.17% accuracy compared to 47.56% for SD-LoRA and 36.02% for InfLoRA
- HAM trained faster (170.61 seconds) than competing methods (318.22 and 196.89 seconds respectively)
- For robotic manipulation, WSA achieved 0.91 success rate with 0.60 reward per step in just 14 hours, outperforming InstructRL (0% success) and OpenVLA (0% success)

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical merging of task-specific adapters reduces interference while preserving learned capabilities across a stream of tasks. Separate LoRA adapters are trained per experience, then combined in a two-stage process: first within similarity groups, then across groups, yielding a unified model capable of handling all seen tasks without full retraining.

### Mechanism 2
Lightweight adapters with attention-based selection can outperform large monolithic Vision-Language-Action models under constrained compute budgets. Small off-the-shelf pre-trained components are augmented with adapters and an attention mechanism that dynamically routes and scales features per context, enabling effective policy learning with fewer parameters and less training time.

### Mechanism 3
Compositional orchestration of specialized models enables dynamic adaptation to evolving scenarios with reduced retraining overhead. Multiple specialized models (or adapters) coexist and are orchestrated to solve complex tasks collectively; changing the orchestration logic adapts the system to new distributions or objectives without retraining the full base model.

## Foundational Learning

- **Concept: Continual Learning (CL)**
  - Why needed here: Essential for adapting FMs to dynamic environments without catastrophic forgetting and without full retraining
  - Quick check question: Can you explain the stability-plasticity tradeoff and name one regularization-based CL method?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT), specifically LoRA**
  - Why needed here: LoRA adapters are the core building blocks for the compositional paradigm proposed (HAM and WSA both rely on adapter-based modulation)
  - Quick check question: How does LoRA modify a pre-trained weight matrix, and what is the rank hyperparameter's effect on capacity vs. efficiency?

- **Concept: Attention Mechanisms**
  - Why needed here: WSA uses attention to dynamically route and scale adapter outputs based on context
  - Quick check question: Given query, key, and value projections, how is scaled dot-product attention computed, and what role does the softmax play?

## Architecture Onboarding

- **Component map**: Base FM (ViT for vision; small pre-trained policy or encoders for robotics) -> Adapter modules (LoRA-style low-rank updates per task or context) -> Hierarchical merger (HAM) or Attention router (WSA)

- **Critical path**: 
  1. Instantiate base FM and freeze base weights
  2. Train task-specific adapters on separate experiences/tasks
  3. For HAM: compute adapter similarity, group, merge hierarchically, evaluate unified model
  4. For WSA: attach attention router to modulate adapter contributions, train policy with RL or imitation under budget constraints

- **Design tradeoffs**: 
  - Adapter rank vs. expressivity (higher rank = more capacity, more interference risk)
  - Number of tasks before merging vs. memory/compute (more tasks = more adapters, longer merging)
  - Grouping granularity in HAM (coarse groups reduce interference but may lose specialization; fine groups preserve specialization but increase merge complexity)
  - Attention complexity in WSA vs. real-time control latency

- **Failure signatures**: 
  - Sudden accuracy drop after merging: likely cross-task interference; check group assignments and merge coefficients
  - Zero success rate in robotics despite training: check reward shaping, observation normalization, and whether adapters are being attended to (attention weights collapsing to uniform or zero)
  - Training time blowup: too many adapters or overly fine-grained groups; reduce task count or increase grouping threshold

- **First 3 experiments**:
  1. Replicate HAM on a subset of CUB200 (e.g., 10 tasks) with ViT-Base; compare hierarchical vs. flat LoRA merging and report accuracy and training time
  2. Ablation: vary adapter rank (e.g., r=4, 8, 16) and measure performance/interference tradeoff under fixed compute budget
  3. On a lightweight robotic simulation (e.g., Meta-World or ManiSkill3), train WSA with a small pre-trained encoder; compare success rate and reward per step against a strong baseline (e.g., a frozen FM with simple linear head) under identical training hours

## Open Questions the Paper Calls Out

- **Open Question 1**: How can robotic agents effectively abstract from learned low-level policies to reason over their own capabilities for high-level planning? The paper identifies that agents must "abstract from these policies and reason over their capabilities in order to come up with plans," but the current work focuses on acquiring skills via composition without implementing or evaluating the higher-level cognitive layer required for logical planning.

- **Open Question 2**: Can compositional approaches maintain safety alignment and prevent catastrophic forgetting while learning a precise world model? The conclusion identifies "learning a precise model of the world" as a key requirement to "adapt to sudden changes... without catastrophic forgetting and dangerous behaviors," but the paper presents preliminary results on task success without investigating safety implications or long-term memory stability.

- **Open Question 3**: Does the reduction in merging interference via hierarchical methods scale to open-ended, long-horizon robotic tasks? While HAM reduced interference in image classification, the authors describe their robotic manipulation experiments as "preliminary" and note that FMs generally struggle with distribution shifts, leaving uncertainty about whether the hierarchical merging strategy remains effective as the number of tasks grows significantly.

## Limitations

- Several key implementation details are unspecified, including exact LoRA configuration, similarity metrics for grouping, and specific robotics environment definitions
- The robotics experiments focus on simulated environments without validation on real-world hardware or noisy sensor inputs
- No rigorous comparison of computational efficiency (FLOPs, inference latency) between proposed methods and baselines
- Limited ablation studies or hyperparameter sensitivity analysis to understand robustness to design choices

## Confidence

- **High confidence** in the broad compositional paradigm and its motivation: clearly articulates limitations of current FMs and potential of CL and compositionality
- **Medium confidence** in the proposed hierarchical adapter merging (HAM) method: mechanism is well-described with compelling image classification results, but missing implementation details reduce reproducibility
- **Medium confidence** in the WSA method for robotics: strong reported results but less specified method with no comparison to other parameter-efficient RL approaches
- **Low confidence** in claims about out-of-distribution robustness and real-world applicability: not directly tested or validated

## Next Checks

1. **Ablation of adapter rank and merging granularity**: Systematically vary LoRA rank and clustering thresholds in HAM; measure both accuracy and training time to quantify the trade-off between capacity, interference, and efficiency

2. **Cross-task interference analysis**: For each task in the CUB200 split, measure per-task accuracy before and after each stage of hierarchical merging; identify which task combinations cause the largest degradation and why

3. **WSA robustness to adapter initialization**: Train WSA with adapters initialized from different pre-trained models (e.g., CLIP, ImageNet-1K features) and compare success rates and reward curves; test sensitivity to initialization and pre-training domain