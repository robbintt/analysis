---
ver: rpa2
title: 'CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative
  Policy Optimization'
arxiv_id: '2508.09074'
source_url: https://arxiv.org/abs/2508.09074
tags:
- evaluation
- dialogue
- role-playing
- character
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Comparative Policy Optimization (CPO), a reinforcement
  learning method for role-playing dialogue that addresses reward ambiguity by replacing
  sample-wise scoring with group-wise comparative scoring. The approach mimics human
  evaluation by comparing responses within groups to establish relative quality, reducing
  scoring instability and improving human agreement by approximately 20%.
---

# CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization

## Quick Facts
- arXiv ID: 2508.09074
- Source URL: https://arxiv.org/abs/2508.09074
- Reference count: 40
- Key outcome: CPO improves human agreement by ~20% and outperforms RLHF methods with 50-75% win rates in model comparisons

## Executive Summary
CPO addresses reward ambiguity in role-playing dialogue by replacing sample-wise scoring with group-wise comparative scoring. The approach mimics human evaluation by comparing responses within groups to establish relative quality, reducing scoring instability and improving human agreement. Experiments on CharacterEval, CharacterBench, and CharacterArena show CPO outperforms existing RLHF methods across multiple LLM backbones, with consistent performance gains in conversational ability, character consistency, and role-playing attractiveness.

## Method Summary
CPO modifies GRPO by having a reward model evaluate groups of N=16 responses jointly rather than individually. The judge LLM (Qwen2.5-72b-instruct) scores responses relative to each other using a comparative prompt, producing more discriminative rewards. These group-wise scores are then used in a GRPO-style optimization loop with a soft length penalty to update the policy model. The method is trained on synthetic dialogues generated via user simulation and evaluated using the CharacterArena framework that compares multi-turn trajectories.

## Key Results
- CPO improves human agreement by approximately 20% compared to sample-wise reward baselines
- Achieves 50-75% win rates in pairwise model comparisons on CharacterArena
- Demonstrates consistent performance gains when applied to other RLFT paradigms like RFT and DPO
- Shows significant improvements in conversation ability, character consistency, and role-playing attractiveness across Qwen and LLaMA backbones

## Why This Works (Mechanism)

### Mechanism 1
Shifting from sample-wise to group-wise reward scoring reduces evaluation ambiguity by mimicking human comparative judgment behavior. By evaluating candidate responses jointly, the reward model can establish relative quality baselines within context, leading to more discriminative and stable scores that mitigate prompt sensitivity and scoring collapse.

### Mechanism 2
The two-stage CharacterArena evaluation framework reduces context bias by generating multi-turn trajectories under controlled settings, then performing trajectory-level comparison. This holistic assessment captures temporal dynamics and persona consistency that single-turn evaluations miss, providing a fairer measure of role-playing ability.

### Mechanism 3
Group-wise rewarding generalizes to other RLFT paradigms by producing more stable advantage estimates. In GRPO, sample-wise scoring noise gets amplified during normalization, while comparative scoring yields more spread-out, meaningful rewards that lead to cleaner learning signals across preference learning methods.

## Foundational Learning

**Concept: Group Relative Policy Optimization (GRPO)**
- Why needed: CPO is a direct modification of GRPO's reward modeling step
- Quick check: In GRPO, how is the advantage estimate for a response calculated, and what is its purpose?

**Concept: Reward Modeling for Subjective Tasks**
- Why needed: Understanding why RMs struggle with open-ended criteria clarifies CPO's motivation
- Quick check: Why might a sample-wise LLM-as-a-Judge give two very different role-playing responses the same score (e.g., 0.7)?

**Concept: The Bradley-Terry Model and Preference Learning**
- Why needed: The comparative principle underlying CPO relates to how preference data is modeled
- Quick check: In a preference learning setup, does the Bradley-Terry model predict an absolute quality score or a probability of one item being preferred over another?

## Architecture Onboarding

**Component map:** Policy Model (π_θ) -> Reward Model (RM) -> CPO Optimizer -> CharacterArena Evaluator

**Critical path:**
1. Sample dialogue context from training data
2. Policy model generates group of G candidate responses
3. Reward Model evaluates all G responses together, producing relative scores
4. CPO optimizer computes advantages and updates policy model weights
5. Periodically evaluate on CharacterArena by simulating conversations and comparing trajectories

**Design tradeoffs:**
- **Group Size (G):** Larger groups provide more stable comparisons but increase RM computation cost linearly
- **Reward Model Choice:** More capable judges give better discriminations but are slower/costlier
- **Evaluation Criteria:** RM prompt's criteria directly shape what the model learns

**Failure signatures:**
1. **Reward Hacking (Length):** Model generates excessively long responses to impress RM, mitigated by soft overlength penalty
2. **Evaluation Collapse:** RM fails to discriminate, giving all responses similar scores and reverting to weak signal
3. **Persona Drift:** Model loses character consistency in long multi-turn interactions

**First 3 experiments:**
1. **Ablate Group Size:** Train CPO with G={2, 4, 8, 16, 32}, measure training stability and CharacterArena win rate
2. **Validate Generalization:** Apply group-wise rewarding to DPO and RFT on different backbone (e.g., LLaMA-3-8B)
3. **Human-RM Correlation Check:** Measure Pearson correlation between human rankings and RM's group-wise scores before full training

## Open Questions the Paper Calls Out

**Open Question 1:** Can CPO be extended to optimize trajectory-level rewards for multi-turn dialogue strategies rather than single-turn responses? The authors note current optimization primarily targets single-turn dialogue modeling and future research will expand to multi-turn strategies.

**Open Question 2:** Does comparative group-wise rewarding generalize effectively to other open-ended creative domains like long-form story continuation? The authors intend to conduct more comprehensive evaluations across broader open-ended tasks.

**Open Question 3:** How sensitive is CPO's performance to the specific capability and bias of the LLM judge used for reward modeling? The paper notes accuracy of all LLM judges declines as human agreement decreases, but doesn't fully characterize performance variance across different judge models.

## Limitations

- The 20% improvement in human agreement requires careful interpretation as relative improvement rather than absolute agreement levels
- The method's effectiveness depends heavily on the reward model's ability to produce meaningful relative scores within groups
- Long-term stability and prevention of reward collapse over extended training aren't thoroughly explored
- The trajectory-level evaluation framework's dependence on user simulator quality and character profile diversity isn't fully characterized

## Confidence

**High Confidence:** The core mechanism of shifting from sample-wise to group-wise comparative scoring is technically sound and well-explained. Experimental results showing CPO outperforms baseline methods are clear and reproducible.

**Medium Confidence:** Human agreement improvement claims and specific win rates are based on the CharacterArena framework, which while carefully designed, introduces evaluation complexity that may not fully capture real-world deployment scenarios.

**Low Confidence:** Claims about generalization to other RLFT paradigms beyond specific experiments shown, and assertions that this approach fundamentally solves reward ambiguity in all subjective tasks, extend beyond what's rigorously demonstrated.

## Next Checks

1. **RM Correlation Validation:** Conduct controlled experiments measuring human-RM correlation on response groups with known quality differences, systematically varying group size and RM prompt structure.

2. **Break Condition Testing:** Deliberately construct edge cases where the RM should struggle (groups of nearly identical responses or responses optimized to exploit prompt patterns) to measure whether the comparative approach maintains stability versus sample-wise scoring.

3. **Generalization Cross-Paradigm:** Test the group-wise rewarding approach on DPO and RFT with a different LLM backbone and dataset, comparing not just final performance but training stability metrics to verify claimed benefits aren't implementation-specific.