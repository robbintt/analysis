---
ver: rpa2
title: Adaptive Defense against Harmful Fine-Tuning for Large Language Models via
  Bayesian Data Scheduler
arxiv_id: '2510.27172'
source_url: https://arxiv.org/abs/2510.27172
tags:
- data
- harmful
- fine-tuning
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of harmful fine-tuning in large
  language models, where a small fraction of malicious data in user-provided datasets
  can compromise safety alignment. The proposed Bayesian Data Scheduler (BDS) formulates
  the defense as a Bayesian inference problem, learning posterior distributions of
  data safety attributes conditioned on both the fine-tuning and alignment datasets.
---

# Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler

## Quick Facts
- **arXiv ID**: 2510.27172
- **Source URL**: https://arxiv.org/abs/2510.27172
- **Reference count**: 40
- **Key outcome**: Bayesian Data Scheduler (BDS) achieves 74.4% improvement in harmful score at p=0.9 attack ratio and maintains low harmfulness scores (~1) across diverse attacks

## Executive Summary
This paper introduces Bayesian Data Scheduler (BDS), a novel defense mechanism against harmful fine-tuning attacks on large language models. The approach formulates defense as a Bayesian inference problem, learning posterior distributions of data safety attributes from both fine-tuning and alignment datasets. By weighting data points during fine-tuning based on inferred safety attributes, BDS effectively mitigates the influence of malicious data without requiring attack simulation. The method demonstrates state-of-the-art performance across five datasets, three model architectures, and various attack settings, achieving significant improvements in safety metrics while maintaining model utility.

## Method Summary
The Bayesian Data Scheduler treats harmful fine-tuning defense as a Bayesian inference problem where safety attributes of data points are learned from posterior distributions conditioned on both the fine-tuning dataset and the alignment dataset. The core innovation lies in using Bayesian inference to estimate the safety attributes of each data point, which then guides the fine-tuning process by assigning appropriate weights to data samples. Two variants are proposed: a Bayesian scalar scheduler that provides direct probability estimates, and an amortized Bayesian neural scheduler that learns to predict safety attributes efficiently for new data without retraining. This approach eliminates the need for attack simulation while maintaining strong defense capabilities across diverse attack scenarios.

## Key Results
- Achieves 74.4% improvement in harmful score at high attack ratio (p=0.9)
- Demonstrates average 50%+ improvement across all attack ratios
- Maintains low harmfulness scores (~1) across various advanced attack types

## Why This Works (Mechanism)
The method works by framing the defense problem as Bayesian inference, where the safety attribute of each data point is treated as a latent variable. By learning the posterior distribution of these safety attributes conditioned on both the fine-tuning and alignment datasets, BDS can effectively distinguish between benign and harmful data. The Bayesian approach naturally handles uncertainty in safety attribute estimation, allowing for principled weight assignment during fine-tuning. The amortized scheduler further enables efficient transfer to new data distributions without requiring complete retraining, making the defense practical for real-world deployment scenarios.

## Foundational Learning
- **Bayesian inference**: Needed to estimate posterior distributions of safety attributes; quick check: verify posterior convergence on held-out validation data
- **Amortized inference**: Required for efficient transfer to new data; quick check: measure inference time on new datasets
- **Safety attribute modeling**: Essential for distinguishing harmful from benign data; quick check: validate attribute predictions against known attack patterns
- **Weight-based fine-tuning**: Core mechanism for mitigating harmful influence; quick check: monitor weight distribution during training
- **Transfer learning**: Enables application to new domains; quick check: test performance on domain-shifted datasets
- **Adversarial robustness**: Critical for defense effectiveness; quick check: evaluate against adaptive attack strategies

## Architecture Onboarding
- **Component map**: Data → Bayesian Inference → Safety Attribute Weights → Fine-tuning Process → Protected Model
- **Critical path**: The inference of safety attributes from both fine-tuning and alignment data is the critical component that enables the entire defense mechanism
- **Design tradeoffs**: The method trades computational overhead during inference for stronger defense capabilities, avoiding the need for attack simulation
- **Failure signatures**: Performance degradation occurs when safety attribute distributions overlap significantly between benign and harmful data, or when attacks use novel patterns not represented in training
- **First experiments**: 1) Ablation study comparing Bayesian scalar vs amortized neural scheduler performance, 2) Transfer capability evaluation on out-of-domain datasets, 3) Robustness testing against adaptive attack strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes harmful data patterns remain consistent enough for Bayesian inference to generalize effectively
- Evaluation focuses predominantly on classification-based safety metrics, potentially missing subtle harm in multi-turn interactions
- Does not address computational overhead or memory requirements for large-scale deployments

## Confidence
- **High Confidence**: Mathematical formulation of Bayesian inference for safety attribute estimation is sound
- **Medium Confidence**: Empirical results demonstrate strong performance against tested attack types
- **Medium Confidence**: Amortized scheduler's transfer capabilities are promising but need broader testing

## Next Checks
1. Test BDS against adaptive adversaries who modify attack strategies between training and inference phases
2. Evaluate performance on long-form generation tasks and multi-turn conversational safety
3. Conduct ablation studies isolating contributions of Bayesian scalar versus amortized neural scheduler components