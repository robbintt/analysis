---
ver: rpa2
title: How to Improve the Robustness of Closed-Source Models on NLI
arxiv_id: '2505.20209'
source_url: https://arxiv.org/abs/2505.20209
tags:
- training
- data
- examples
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Closed-source LLMs suffer large OOD performance drops when fine-tuned.
  We propose selecting training data to improve robustness without changing model
  internals or increasing data size.
---

# How to Improve the Robustness of Closed-Source Models on NLI

## Quick Facts
- arXiv ID: 2505.20209
- Source URL: https://arxiv.org/abs/2505.20209
- Reference count: 40
- Primary result: Closed-source LLMs suffer large OOD performance drops when fine-tuned; data selection and synthetic data generation improve robustness without changing model internals

## Executive Summary
Fine-tuning closed-source LLMs improves in-distribution accuracy but causes significant out-of-distribution (OOD) performance drops. This paper proposes practical, budget-friendly data selection strategies to improve OOD robustness without altering model internals or increasing data size. By selecting challenging examples via uncertainty sampling or replacing portions of training data with carefully generated synthetic examples, we achieve measurable robustness gains. The approach is particularly effective for highly complex OOD datasets, improving performance by up to 1.5% with uncertainty sampling and 3.7% with synthetic data generation.

## Method Summary
The paper proposes three data-centric strategies for improving OOD robustness in closed-source LLMs under fixed training budgets. First, uncertainty sampling selects high-entropy examples from a candidate pool to upsample challenging cases. Second, synthetic data generation uses LLM prompting to create new examples, with complexity variations (short/simple vs. long/complex) tailored to dataset difficulty. Third, difficulty sampling ranks examples by baseline model errors. All methods maintain a fixed 10K training budget by replacing (not augmenting) selected examples. The optimal strategy depends on target OOD complexity: uncertainty sampling excels for challenging OOD data (baseline <70%), while synthetic data boosts easier OOD sets.

## Key Results
- Fine-tuning improves in-distribution accuracy but causes 5-11 percentage point drops on challenging OOD datasets
- Uncertainty sampling improves challenging OOD performance by up to 1.5%
- Synthetic data generation improves easier OOD sets by 3.7%, with complexity prompting extending gains to challenging OOD
- Large autoregressive models are significantly more robust than encoder models, even with less than 2% of their training data

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty Sampling for Challenging OOD Data
- Claim: Selecting training examples with high prediction entropy improves performance on challenging OOD datasets by up to 1.5%.
- Mechanism: High-entropy examples represent under-represented and challenging cases in the training distribution. By upsampling these instances, the model encounters more diverse decision boundaries during fine-tuning, reducing over-reliance on dataset-specific heuristics that don't generalize.
- Core assumption: Entropy correlates with genuine difficulty rather than label noise; the paper notes 24% annotation errors in uncertainty-sampled examples vs. 54% in misclassified sampling.
- Evidence anchors:
  - [abstract] "For highly complex OOD datasets, upsampling more challenging training examples can improve robustness by up to 1.5%."
  - [section 5.2] "Uncertainty Sampling performs best, improving performance on Challenge-OOD by an average of 1.45%... improvements are only observed on more challenging OOD datasets."
  - [corpus] Weak direct corpus support—neighbor papers focus on decomposition/explanation approaches rather than uncertainty-based sampling.
- Break condition: Does not improve Standard-OOD datasets; performance gains specific to challenging distributions where baseline accuracy <70%.

### Mechanism 2: Synthetic Data Generation for Distribution Coverage
- Claim: Replacing 5-15% of training data with LLM-generated examples improves Standard-OOD performance by 2.6-3.7%, with complexity prompting extending gains to challenging OOD.
- Mechanism: LLM-generated data introduces domain diversity absent from single-source training sets (e.g., SNLI). Short/simple synthetic examples help easier OOD; prompting for multi-sentence premises and contextually-dependent hypotheses increases difficulty, yielding broader gains.
- Core assumption: Generated labels are sufficiently accurate—the paper uses an "if in doubt, discard" consensus filter (8 predictions, retain only unanimous) to reduce noise.
- Evidence anchors:
  - [abstract] "For less complex OOD datasets, replacing a portion of the training set with LLM-generated examples can improve robustness by 3.7%."
  - [section 5.3] "Long & Complex Generation successfully improves performance on Challenge-OOD for GPT-4o-mini and Gemini-2.0-Flash, with improvements of up to 2.82%."
  - [section 5.4] "Short & Simple Generation produces less difficult examples... prompting strategies can successfully increase the complexity."
  - [corpus] Weak support—neighbor papers mention LLM-generated data but focus on theorem proving and decomposition rather than data augmentation for robustness.
- Break condition: Domain-specific synthetic data does not yield domain-specific improvements (surprisingly, MNLI-domain generation didn't outperform generic generation on MNLI test sets).

### Mechanism 3: Fine-tuning Induced Distributional Overfitting
- Claim: Fine-tuning improves in-distribution accuracy but causes large OOD performance drops (5-11 percentage points on Challenge-OOD) by encouraging heuristic learning.
- Mechanism: Fine-tuning optimizes for dataset-specific patterns that correlate with labels in the training distribution. These heuristics (e.g., lexical overlap, hypothesis-only cues) generalize poorly to OOD data with different statistical properties.
- Core assumption: The tradeoff is inherent to gradient-based optimization on limited, biased training data, not specific to particular architectures.
- Evidence anchors:
  - [abstract] "Fine-tuning improves in-distribution accuracy but harms robustness."
  - [section 5.1/Table 1] "After fine-tuning, Challenge-OOD performance drops by 5.13% for GPT-4o-mini and by 10.95% for Gemini-2.0-Flash."
  - [corpus] Partial support—neighbor papers discuss NLI robustness challenges but don't explicitly characterize the fine-tuning/OOD tradeoff in LLMs.
- Break condition: Assumption: the mechanism is not fully proven; the paper demonstrates the phenomenon but does not isolate whether it's due to heuristic learning vs. distribution shift sensitivity vs. other factors.

## Foundational Learning

- **Concept: Out-of-Distribution (OOD) Generalization**
  - Why needed here: The entire paper frames robustness as OOD performance; understanding the distinction between Challenge-OOD (baseline <70% accuracy) and Standard-OOD is critical for method selection.
  - Quick check question: Given a new OOD test set with 85% baseline accuracy, which strategy would you prioritize—uncertainty sampling or synthetic data generation?

- **Concept: Entropy-Based Uncertainty**
  - Why needed here: Uncertainty sampling requires interpreting model confidence; high entropy = uncertain predictions = candidate examples for upsampling.
  - Quick check question: Why might high-entropy examples include annotation errors, and how does this differ from misclassified sampling?

- **Concept: Fixed Training Budget Constraint**
  - Why needed here: Closed-source API fine-tuning charges per token; the paper explicitly maintains 10K instances by replacing (D_up) rather than augmenting, which is economically critical for production deployment.
  - Quick check question: If you have budget for 15K instances instead of 10K, would you still use replacement strategies or pure augmentation?

## Architecture Onboarding

- **Component map:**
  Training Data (D) -> Initial Subset (D_init, 10K) -> Baseline Model (M_base) via API fine-tuning -> Candidate Pool (D_potential) <- Remaining data -> Uncertainty Sampling OR Difficulty Score OR LLM Generation (Short/Long/Complex) -> D_up (5% per class, replaced D_down via random selection) -> Final Training Set (10K maintained) -> Fine-tuned Model -> Evaluate on ID + Challenge-OOD + Standard-OOD

- **Critical path:**
  1. Establish baseline M_base with 10K random samples from SNLI/MNLI
  2. For uncertainty sampling: extract soft predictions from M_base, compute entropy, select top-K high-entropy examples per class
  3. For synthetic generation: prompt same LLM with domain/complexity instructions, apply consensus filter (8-shot, unanimous agreement)
  4. Replace 5-15% of D_init (random D_down selection), fine-tune via API
  5. Evaluate across 12 OOD datasets; report Challenge-OOD and Standard-OOD aggregates separately

- **Design tradeoffs:**
  - Uncertainty sampling requires probability outputs (available for GPT-4o-mini, NOT for Gemini/Command R)
  - Synthetic data quality vs. quantity: more complex prompts increase difficulty but also annotation error rates (12% vs. 4%)
  - K=5% replacement is optimal; K=10-15% shows diminishing returns (over-representation of challenging examples)

- **Failure signatures:**
  - Uncertainty sampling fails to improve Standard-OOD (only helps Challenge-OOD)
  - Short/simple synthetic data fails on Challenge-OOD (insufficient complexity)
  - Domain-specific synthetic data does not yield domain-specific gains (unexpected)

- **First 3 experiments:**
  1. Replicate baseline tradeoff: fine-tune GPT-4o-mini on 10K SNLI, measure ID vs. Challenge-OOD vs. Standard-OOD drops to validate the phenomenon on your target model.
  2. Ablate uncertainty sampling with varying K (5%, 10%, 15%) to find optimal replacement ratio for your data distribution.
  3. Test Long & Complex Generation on your specific OOD test sets; if gains <1%, diagnose whether your OOD data is already sufficiently complex (in which case synthetic data may be redundant).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do these data selection methods generalize to tasks with less subjective labeling than NLI, such as factual verification or logic?
- **Basis in paper:** [explicit] The authors note in the Limitations section that findings might not generalize to other tasks and that math tasks "potentially not having the same challenges with annotation errors or subjective labelling."
- **Why unresolved:** The paper focuses exclusively on NLI, where annotation subjectivity is high, and a brief math experiment showed mixed results depending on the "K" parameter.
- **What evidence would resolve it:** Applying uncertainty and difficulty sampling to datasets with objective ground truth (e.g., formal logic, code generation) and comparing performance drops.

### Open Question 2
- **Question:** How can one select the optimal data-centric strategy *a priori* without access to the target OOD distribution?
- **Basis in paper:** [explicit] The Abstract states, "We find that the optimal strategy depends on the complexity of the OOD data."
- **Why unresolved:** The paper establishes that uncertainty sampling helps challenging OOD data while synthetic data helps easier OOD data, but requires the user to know the target difficulty profile beforehand.
- **What evidence would resolve it:** A method that dynamically balances the training set or a proxy metric that predicts which strategy suits an unseen target distribution.

### Open Question 3
- **Question:** Can the performance of "Misclassified Sampling" be recovered by filtering the high rate of annotation errors (54%) found in the selected examples?
- **Basis in paper:** [explicit] Section 5.4 states, "Misclassified Sampling examples have the highest difficulty score... but we find this also corresponds to the most annotation errors (54%)."
- **Why unresolved:** The high noise floor suggests the method identifies difficult examples but fails because the labels are wrong, rather than the method being theoretically unsound.
- **What evidence would resolve it:** Re-running Misclassified Sampling with a human or LLM-in-the-loop verification step to correct labels before fine-tuning.

## Limitations
- Limited model scope: Experiments confined to GPT-4o-mini, Gemini-2.0-Flash, and Command R+; cannot verify if robustness advantage holds at scale
- Data quality assumptions: Synthetic data generation relies on unanimous consensus filter; alternative quality controls not explored
- Distribution shift specificity: Uncertainty sampling only improves Challenge-OOD datasets; unclear what makes a dataset "challenging" beyond accuracy thresholds

## Confidence
- **Medium**: Limited model scope - experiments confined to three models, missing larger autoregressive models
- **Low-Medium**: Data quality assumptions - synthetic data quality controls not fully validated
- **Medium**: Distribution shift specificity - unclear characterization of what makes datasets challenging
- **High**: Economic constraints - fixed budget and replacement strategy explicitly stated

## Next Checks
1. **Cross-model robustness comparison**: Fine-tune GPT-4 and Claude on identical SNLI/MNLI subsets; measure Challenge-OOD drops to verify if autoregressive architectures consistently outperform encoder models.

2. **Uncertainty sampling ablation on domain-specific OOD**: Apply the method to a new OOD set with 85-90% baseline accuracy; compare gains against synthetic data generation to validate the challenging-vs-standard OOD heuristic.

3. **Synthetic data quality scaling**: Generate 5%, 10%, and 15% synthetic data with and without the unanimous consensus filter; measure annotation error rates and downstream OOD performance to quantify the cost-benefit tradeoff of stricter quality controls.