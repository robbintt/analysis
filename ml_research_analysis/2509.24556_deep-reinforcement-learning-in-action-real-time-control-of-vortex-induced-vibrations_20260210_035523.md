---
ver: rpa2
title: 'Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced
  Vibrations'
arxiv_id: '2509.24556'
source_url: https://arxiv.org/abs/2509.24556
tags:
- control
- cylinder
- flow
- learning
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This experimental study applies deep reinforcement learning (DRL)
  to suppress vortex-induced vibrations (VIV) of a circular cylinder in cross-flow
  at Re=3000 using rotary actuation. Unlike prior numerical studies limited to low
  Reynolds numbers, this work demonstrates real-time control in a challenging experimental
  setting.
---

# Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations

## Quick Facts
- arXiv ID: 2509.24556
- Source URL: https://arxiv.org/abs/2509.24556
- Reference count: 40
- Key outcome: Deep reinforcement learning achieves up to 95% suppression of vortex-induced vibrations in real-time experiments using only state feedback, with performance enhanced by incorporating past control actions to overcome actuator delays.

## Executive Summary
This experimental study demonstrates real-time control of vortex-induced vibrations (VIV) on a circular cylinder in cross-flow at Re=3000 using deep reinforcement learning (DRL) and rotary actuation. Unlike prior numerical studies limited to low Reynolds numbers, this work successfully implements active flow control in a challenging experimental setting. The DRL agent learns to suppress vibrations through low-frequency rotary actuation that leverages the lock-on phenomenon, achieving up to 80% reduction. By augmenting the state with past control actions, the agent overcomes actuator delays and discovers high-frequency strategies that exceed 95% attenuation. These results showcase DRL's adaptability for real-world flow control problems and its ability to discover effective strategies that may differ from conventional approaches.

## Method Summary
The experimental setup features an elastically-mounted circular cylinder (D=17.5mm, L=160mm) with natural frequency fn=1.96Hz in a water channel at Re=3000. A brushed DC motor with PWM control provides rotary actuation, while laser displacement sensors and accelerometers measure cylinder motion. The DRL agent uses Proximal Policy Optimization (PPO) with a 2-hidden-layer neural network (64 units each). State vectors include displacement Y/D and velocity Ẏ/fnD, optionally augmented with the two most recent control actions to address ~200ms actuator delay. The agent outputs PWM duty cycles in [-0.4, 0.4], with reward r = -|Y/D|. Training occurs over ~300 episodes (60 minutes total) at reduced velocities U≈5-6 where lock-in is prominent.

## Key Results
- DRL agent achieves up to 80% vibration suppression using low-frequency rotary control that leverages the lock-on phenomenon
- State augmentation with past control actions enables discovery of high-frequency strategies achieving >95% attenuation
- The agent learns a voltage-to-rotation mapping implicitly through trial-and-error, producing smooth sinusoidal motion from discrete PWM commands
- Performance degradation due to actuator delay is effectively mitigated by incorporating historical action information into the state vector

## Why This Works (Mechanism)

### Mechanism 1: Lock-on through low-frequency synchronization
Low-frequency rotary actuation (fr/fn ≈ 0.6–0.8) suppresses VIV by synchronizing vortex shedding with the forcing frequency, breaking the resonance condition. The DRL agent learns to rotate the cylinder sinusoidally at frequencies below the natural frequency, causing the wake to lock onto the actuation frequency rather than the structure's natural frequency. This maintains a 2S vortex mode but at the forced frequency, reducing energy transfer into the structure. The mechanism fails if actuation frequency approaches fn (fr/fn ≈ 1), which can increase vibration amplitude.

### Mechanism 2: State augmentation for delay compensation
The motor exhibits ~200 ms delay between voltage command and full rotational speed. Without memory, the agent associates observed states with pending (not yet effective) actions, converging on conservative low-frequency policies. By including the two most recent control actions in the state vector, the agent gains implicit memory of pending actuator dynamics, learning that high-frequency commands (fr/fn > 2.5) will take effect with predictable lag and achieve >95% suppression. This mechanism fails if delay varies significantly or exceeds the history window.

### Mechanism 3: End-to-end voltage-to-rotation learning
Rather than commanding rotational velocity (requiring a motor controller and transfer function model), the agent outputs PWM duty cycle directly. Through trial-and-error interaction, the actor network learns the nonlinear mapping from voltage to angular velocity, as evidenced by smooth sinusoidal rotation emerging from discrete voltage steps. This mechanism fails if motor characteristics drift due to temperature or wear, requiring retraining.

## Foundational Learning

- **Concept: Vortex-induced vibration (VIV) and lock-in**
  - Why needed here: The entire control objective is to suppress VIV, which occurs when vortex shedding frequency matches structural natural frequency. Understanding lock-in explains why vibration is large and how lock-on control breaks this resonance.
  - Quick check question: If a cylinder has natural frequency fn = 2 Hz and flow velocity produces Strouhal-based shedding at ~2 Hz, will VIV amplitude be large or small? (Answer: Yes, large amplitude due to resonance)

- **Concept: Proximal Policy Optimization (PPO) basics**
  - Why needed here: The paper uses PPO as its DRL algorithm. Understanding actor-critic structure and policy gradient concepts is necessary to modify the network or training procedure.
  - Quick check question: In PPO, does the actor network output a single action or a distribution over actions? (Answer: Distribution)

- **Concept: Actuation delay in control systems**
  - Why needed here: The key insight of this paper is that delay degrades performance and can be mitigated through state augmentation. This principle transfers to any real-world DRL deployment.
  - Quick check question: If your actuator has 100 ms delay and your control loop runs at 50 Hz (20 ms steps), how many past actions should you include in the state? (Answer: n = ceil(100/20) = 5)

## Architecture Onboarding

- **Component map:**
  - Sensors: Laser displacement sensor (Y) -> DAQ -> Agent
  - Sensors: Accelerometer (Ẏ) -> DAQ -> Agent
  - Sensors: Rotary encoder (Ω) -> DAQ -> Agent
  - DAQ: NI-6281 board, sampled at ~10 Hz -> Agent
  - Agent: PPO actor-critic (2 hidden layers × 64 units) -> Action
  - Action: PWM duty cycle ∈ [-0.4, 0.4] -> Motor controller
  - Actuator: Brushed DC motor via Phidget DCC1002 controller -> Cylinder rotation

- **Critical path:**
  1. Verify VIV occurs without control (run baseline at target Re, confirm A/D ≈ 0.6)
  2. Characterize actuator delay (step response test, measure time to full speed)
  3. Determine history window length from delay (n = ceil(delay / control_period))
  4. Train PPO with augmented state for ~300 episodes (~60 min experimental time)
  5. Deploy deterministic policy from actor network

- **Design tradeoffs:**
  - Control frequency vs. action granularity: Motor controller limits updates to 100 ms intervals; faster systems need different hardware
  - State augmentation length: Longer history improves delay compensation but increases network input dimension and may slow convergence
  - Reward shaping: Current reward only penalizes displacement; adding actuation effort penalty would trade suppression for energy cost

- **Failure signatures:**
  - Policy converges to zero actuation: Reward signal too weak or exploration insufficient
  - High-frequency oscillations with no suppression: Agent learned wrong phase relationship; check sensor calibration
  - Gradual performance degradation: Motor heating or mechanical wear changing dynamics

- **First 3 experiments:**
  1. **Baseline VIV characterization:** Run without control at multiple reduced velocities (U = 4–9), measure A/D curve to confirm lock-in region
  2. **Actuator delay measurement:** Apply step voltage, record rotary encoder response, compute lag (should be ~200 ms); verify this matches control period ratio
  3. **Ablation on state augmentation:** Train two agents (n=0 vs n=2) and compare dominant actuation frequency and final reward; expect n=2 to achieve higher frequency and reward

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the deep reinforcement learning framework be adapted to maximize energy harvesting from vortex-induced vibrations rather than vibration suppression?
- Basis in paper: The authors state in the Conclusion, "We plan to extend the experimental DRL framework to explore energy harvesting strategies in vortex-induced vibrations."
- Why unresolved: The current study focuses exclusively on minimizing vibration amplitude (suppression) via a specific reward function, whereas energy harvesting requires optimizing for power extraction which may necessitate sustaining specific oscillation amplitudes.
- What evidence would resolve it: Experimental results using a modified reward function (e.g., maximizing harvested power or efficiency) to train an agent on the same apparatus.

### Open Question 2
- Question: How does the performance and stability of the learned DRL control policy scale with Reynolds numbers significantly higher than the tested Re = 3000?
- Basis in paper: The Conclusion notes the authors aim to "perform experiments at higher Reynolds numbers to approach more realistic flow conditions."
- Why unresolved: The current study is limited to Re = 3000; it is unknown if the low-dimensional state feedback (displacement/velocity) remains sufficient for control in more turbulent regimes where flow structures are more complex.
- What evidence would resolve it: Deployment of the trained agent or re-training in experimental flows with Re > 10,000, analyzing suppression percentages and wake characteristics.

### Open Question 3
- Question: What causes the divergence in control mechanisms between experimental implementations (lock-on) and low-Reynolds numerical simulations (flow stabilization)?
- Basis in paper: Section 5.4 highlights that while the experiment utilized the "lock-on" phenomenon, prior numerical studies (Ren et al.) achieved suppression via stabilizing flow modes, a discrepancy attributed to Reynolds number, noise, or sampling rates.
- Why unresolved: It is unclear if the "lock-on" strategy is a physical necessity at high Re, a result of experimental constraints (delay/noise), or an artifact of the specific DRL exploration parameters.
- What evidence would resolve it: Ablation studies in simulation that introduce experimental artifacts (actuation lag, sensor noise) to see if the agent switches from flow stabilization to lock-on strategies.

## Limitations
- Study limited to single Reynolds number (Re = 3000), restricting generalizability across flow regimes
- Performance relies on successful lock-on exploitation, which may not generalize to all VIV scenarios
- Training conducted in open-loop without disturbance rejection validation

## Confidence

- **High confidence:** The basic DRL approach works for VIV suppression, achieving ~80% reduction through low-frequency control
- **Medium confidence:** State augmentation with past actions enables higher-frequency strategies and improved suppression (>95%)
- **Medium confidence:** The mechanisms linking low-frequency actuation to lock-on and past-action augmentation to delay compensation

## Next Checks

1. **Cross-Reynolds validation:** Test the trained policy at Re = 2000 and Re = 4000 to assess generalizability
2. **Disturbance rejection test:** Apply external perturbations during steady-state operation to evaluate robustness
3. **Actuator delay sweep:** Systematically vary control frequency and history window length to map the delay compensation boundary