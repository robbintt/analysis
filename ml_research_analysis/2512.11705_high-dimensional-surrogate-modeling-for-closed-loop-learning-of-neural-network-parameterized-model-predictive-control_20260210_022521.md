---
ver: rpa2
title: High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized
  Model Predictive Control
arxiv_id: '2512.11705'
source_url: https://arxiv.org/abs/2512.11705
tags:
- bayesian
- cost
- closed-loop
- neural
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of optimizing dense high-dimensional\
  \ neural network\u2013parameterized model predictive control (MPC) cost functions,\
  \ where conventional Gaussian process surrogates degrade in performance as dimensionality\
  \ grows. The core method idea is to use Bayesian neural networks (BNNs) and infinite-width\
  \ BNNs as surrogate models within Bayesian optimization, replacing the standard\
  \ Mat\xE9rn-kernel Gaussian process baseline."
---

# High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control

## Quick Facts
- arXiv ID: 2512.11705
- Source URL: https://arxiv.org/abs/2512.11705
- Reference count: 4
- One-line primary result: Infinite-width BNN surrogates enable data-efficient optimization of dense high-dimensional NN-MPC cost functions where GPs fail.

## Executive Summary
This work addresses the challenge of optimizing dense high-dimensional neural network–parameterized model predictive control (MPC) cost functions, where conventional Gaussian process surrogates degrade in performance as dimensionality grows. The core method idea is to use Bayesian neural networks (BNNs) and infinite-width BNNs as surrogate models within Bayesian optimization, replacing the standard Matérn-kernel Gaussian process baseline. Through simulation on a cart-pole system, the results show that BNN-based surrogates—especially infinite-width BNNs—achieve faster and more reliable convergence of the closed-loop cost than Matérn GPs as the number of parameters increases. Infinite-width BNNs maintain strong performance even beyond 1000 parameters, while Matérn GPs lose effectiveness and approach random-search behavior in high dimensions. These findings indicate that infinite-width BNN surrogates are promising for scalable, data-efficient learning in dense high-dimensional MPC parameter spaces.

## Method Summary
The method employs Bayesian optimization to tune a neural network–augmented MPC cost function parameterized by a high-dimensional vector θ. The closed-loop performance G(θ) is evaluated by simulating the MPC controller over a fixed horizon. Three surrogate models are compared: (1) Matérn-kernel Gaussian processes, (2) finite-width Bayesian neural networks (inferred via MCMC), and (3) infinite-width BNNs (equivalent to GPs with a specific compositional kernel). The infinite-width limit avoids MCMC sampling by yielding an analytic GP posterior, making it computationally feasible for high-dimensional problems. BO selects the next θ using logarithmic Expected Improvement, and results are averaged over 21 independent runs.

## Key Results
- I-BNN surrogates achieve faster and more reliable convergence than Matérn GPs as parameter dimensionality increases.
- Matérn GPs lose effectiveness and approach random-search behavior beyond a few hundred dimensions.
- I-BNNs maintain strong performance even beyond 1116 parameters, while finite-width BNNs become computationally prohibitive.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compositional kernel structure in I-BNNs captures high-dimensional cost function geometry better than stationary distance-based kernels.
- Mechanism: I-BNNs construct kernels by recursively propagating covariances through L layers using K_ℓ(θ, θ') = σ²_b + σ²_w · E[ϕ(z₁)ϕ(z₂)] (Eq. 13). Each layer composes a nonlinear transformation, creating a kernel that depends on the full input structure rather than just θ - θ' distance. Matérn kernels measure similarity via |θ - θ'|, which becomes uninformative when all points are roughly equidistant in dense high-dimensional spaces (distance concentration).
- Core assumption: The closed-loop cost G(θ) has compositional structure that layered transformations can exploit.
- Evidence anchors:
  - [abstract]: "Bayesian neural network surrogate models achieve faster and more reliable convergence... and enable successful optimization of parameterizations with hundreds of dimensions"
  - [section 2.2]: Eq. 11-13 define recursive covariance propagation; Eq. 10 establishes I-BNN as GP with architecture-determined kernel
  - [corpus]: Weak—neighbor papers address hierarchical BO and MPC tuning but not kernel structure comparisons
- Break condition: If G(θ) is approximately stationary or has latent low-dimensional structure, simpler kernels may suffice or outperform.

### Mechanism 2
- Claim: Computational bottlenecks shift based on surrogate choice: GP scales with data (O(n³)), finite BNN scales with parameters (O(nθ)), I-BNN inherits GP data-scaling with richer kernels.
- Mechanism: GP requires inverting K_y ∈ R^(n×n) at O(n³) cost (Eq. 4). Finite-width BNNs require MCMC sampling over all network weights W^(s) ~ q(W) to estimate posterior moments (Eq. 8)—cost grows with network size. I-BNNs avoid sampling entirely: the infinite-width limit yields an analytic GP posterior (Eq. 10), so inference cost remains O(n³) in data regardless of the underlying network depth.
- Core assumption: Closed-loop experiments are expensive enough that surrogate inference overhead is secondary, but not negligible.
- Evidence anchors:
  - [abstract]: "Finite-width Bayesian neural networks perform well up to a few hundred parameters but become computationally prohibitive at higher dimensions"
  - [section 4.2]: "MCMC-based finite-width BNNs are computationally prohibitive in this regime [1116 parameters]"
  - [corpus]: No direct corpus comparison of surrogate computational scaling
- Break condition: If experiments are extremely cheap or extremely expensive relative to inference, the optimal surrogate choice shifts.

### Mechanism 3
- Claim: BNN-based surrogates maintain informative uncertainty estimates in dense high dimensions, enabling acquisition functions to distinguish promising regions from explored ones.
- Mechanism: BO selects θ_{n+1} = argmax α(θ; D_n) (Eq. 14), where α depends on predictive mean and variance. Assumption: In high dimensions, Matérn kernel variance estimates become near-uniform (all inputs appear equally uncertain), reducing α to random search. BNN kernels maintain heteroscedastic uncertainty patterns due to compositional structure, preserving the exploration-exploitation signal.
- Core assumption: Acquisition function quality—not just mean prediction—drives BO convergence; kernel structure directly affects uncertainty calibration.
- Evidence anchors:
  - [abstract]: "Matérn-kernel Gaussian processes rapidly lose effectiveness" and "approach random-search behavior beyond a few hundred dimensions"
  - [section 4.2, Fig. 2]: At 546 parameters, "Matérn GP behaves only marginally better than random"; at 1116, "indistinguishable from random sampling"
  - [corpus]: No corpus papers directly analyze acquisition function degradation mechanisms in high-dimensional BO
- Break condition: If acquisition is purely exploitative (no exploration term), or if external mechanisms guide exploration, kernel-induced uncertainty matters less.

## Foundational Learning

- Concept: **Gaussian Process regression and kernel functions**
  - Why needed here: I-BNNs are mathematically equivalent to GPs with specific kernels (Eq. 10); understanding GP posterior computation (Eq. 4) is required to implement or modify the surrogate.
  - Quick check question: Given a GP with kernel k and training data {(θᵢ, yᵢ)}, write the posterior predictive mean and variance at a new point θ*.

- Concept: **Bayesian neural network posterior inference**
  - Why needed here: Finite-width BNNs require approximate inference (MCMC, VI) over weights; understanding why this becomes expensive at scale explains the I-BNN motivation.
  - Quick check question: Why does the BNN posterior p(W|D) (Eq. 7) not admit closed-form solution, and what are two common approximation strategies?

- Concept: **Bayesian optimization loop and acquisition functions**
  - Why needed here: The surrogate is only useful insofar as it enables better θ selection via acquisition function optimization (Eq. 14); understanding the explore-exploit tradeoff is essential.
  - Quick check question: What two quantities from the surrogate predictive distribution does Expected Improvement use, and how do they balance exploration vs exploitation?

## Architecture Onboarding

- Component map:
  - **MPC core** -> **NN cost augment** -> **Closed-loop evaluator** -> **Surrogate model** -> **Acquisition optimizer** -> **BO loop**

- Critical path:
  1. Implement baseline MPC with quadratic cost; verify closed-loop behavior
  2. Add NN cost term with small network (e.g., 5 neurons); confirm different θ yields different G(θ)
  3. Implement GP surrogate with Matérn kernel; verify BO improves over random in ~10-20 dim
  4. Scale NN to 20+ neurons (500+ dim); observe GP degradation
  5. Implement I-BNN kernel via recursive covariance (Eq. 11-13); compare convergence

- Design tradeoffs:
  - **Surrogate choice**: GP is simplest but fails >~200 dim; BNN works to ~500 dim but MCMC cost grows; I-BNN scales best but requires kernel implementation
  - **NN cost architecture**: Deeper/wider = more expressive but higher dim; 2 hidden layers used in paper
  - **Acquisition function**: Log-EI used in paper; UCB or others may change relative surrogate performance
  - **Initial design**: More random samples improve surrogate initialization but cost experiments

- Failure signatures:
  - **GP → random behavior**: Best-cost curve flattens to match random sampling baseline (Fig. 2, right panel)
  - **BNN timeout**: Per-iteration time exceeds practical limits as nθ grows; MCMC chains fail to converge
  - **I-BNN numerical issues**: Covariance matrices become ill-conditioned; add jitter or check kernel computation
  - **No improvement across iterations**: Check acquisition function optimization (may be stuck in local maxima); scale input bounds

- First 3 experiments:
  1. **Low-dim sanity check**: 5-neuron cost (~66 parameters); compare GP vs I-BNN vs random. Expect all BO methods to outperform random; differences small.
  2. **Mid-dim stress test**: 20-neuron cost (~546 parameters). Expect GP to degrade toward random, I-BNN to maintain advantage. Profile per-iteration time.
  3. **High-dim limit**: 30-neuron cost (~1116 parameters). Run only I-BNN and random (BNN computationally prohibitive). Verify I-BNN still shows learning signal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can infinite-width BNN surrogates be integrated with stability and safety constraints to guarantee safe exploration during controller learning?
- Basis in paper: [explicit] The conclusion states future work will address "the integration of infinite-width Bayesian neural network surrogates with stability and safety considerations."
- Why unresolved: The current work focuses purely on performance optimization without incorporating stability certificates or safety constraints into the acquisition function or surrogate model.
- What evidence would resolve it: Demonstration of I-BNN-based BO with formal stability/safety guarantees, e.g., via constrained acquisition functions or stability-informed priors, on a safety-critical control task.

### Open Question 2
- Question: Do I-BNN surrogates retain their performance advantage on more complex, nonlinear, or higher-dimensional dynamical systems beyond the cart-pole benchmark?
- Basis in paper: [explicit] The conclusion notes "This work focuses on a single dynamical system" and future work will "extend the evaluation to additional [...] dynamical systems."
- Why unresolved: The cart-pole system is relatively simple (4 states, 1 input); it remains unclear whether findings generalize to systems with more complex dynamics, higher state dimensions, or stronger nonlinearities.
- What evidence would resolve it: Comparative evaluation of surrogate models on diverse benchmarks (e.g., robotic manipulators, aerospace systems, process control) showing consistent I-BNN advantages.

### Open Question 3
- Question: Can the I-BNN-based learning approach transfer successfully to real-world hardware experiments with noise, delays, and model mismatch?
- Basis in paper: [explicit] Future work will extend to "experimental setups" beyond simulation.
- Why unresolved: All results are simulation-based; real-world factors such as sensor noise, actuator dynamics, communication delays, and plant-model mismatch may degrade surrogate model accuracy and optimization convergence.
- What evidence would resolve it: Successful closed-loop learning demonstrations on physical hardware showing comparable convergence behavior to simulation results.

## Limitations
- The work lacks empirical validation on real hardware or more complex dynamical systems, relying entirely on a linearized cart-pole simulation.
- The computational cost analysis is qualitative—quantitative wall-clock comparisons between finite-width BNN, I-BNN, and GP surrogates are absent.
- The study does not explore whether gains persist under different activation functions, deeper architectures, or alternative acquisition functions.

## Confidence
- **High confidence**: The core finding that I-BNN surrogates outperform Matérn GPs in dense high-dimensional spaces is well-supported by the ablation study across 66–1116 parameters, with clear statistical separation from random search.
- **Medium confidence**: The claim that finite-width BNNs become computationally prohibitive beyond ~500 dimensions is plausible but lacks explicit runtime data; MCMC efficiency depends heavily on implementation details.
- **Medium confidence**: The mechanism that I-BNNs maintain informative uncertainty estimates due to compositional kernel structure is theoretically grounded but not empirically verified—no direct comparison of predictive uncertainty calibration across surrogates is provided.

## Next Checks
1. **Uncertainty Calibration Test**: Compare predictive variance surfaces across GP, finite-width BNN, and I-BNN surrogates on held-out data to verify that I-BNNs maintain heteroscedastic uncertainty in high dimensions.
2. **Runtime Profiling**: Measure per-iteration wall-clock time for each surrogate (GP, finite-width BNN, I-BNN) across low, mid, and high dimensions to quantify the computational scaling advantage claimed for I-BNNs.
3. **Activation Function Ablation**: Repeat the 546-parameter experiment with tanh or sigmoid activations to determine whether the compositional kernel advantage is specific to ReLU or more general.