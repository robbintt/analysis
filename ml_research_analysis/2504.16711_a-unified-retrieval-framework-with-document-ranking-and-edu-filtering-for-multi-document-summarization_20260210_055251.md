---
ver: rpa2
title: A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document
  Summarization
arxiv_id: '2504.16711'
source_url: https://arxiv.org/abs/2504.16711
tags:
- retrieval
- document
- ranking
- summarization
- edus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified retrieval framework for multi-document
  summarization (MDS) that addresses context-length constraints by combining document
  ranking and EDU filtering. The method uses salient elementary discourse units (EDUs)
  as latent queries to guide document ranking, then filters out irrelevant EDUs instead
  of truncating tokens.
---

# A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization

## Quick Facts
- arXiv ID: 2504.16711
- Source URL: https://arxiv.org/abs/2504.16711
- Authors: Shiyin Tan; Jaeeon Park; Dongyuan Li; Renhe Jiang; Manabu Okumura
- Reference count: 40
- One-line primary result: ReREF improves ROUGE scores across four MDS datasets with diverse summarizers by up to +1.85 on Wikisum.

## Executive Summary
This paper introduces a unified retrieval framework for multi-document summarization (MDS) that addresses context-length constraints by combining document ranking and EDU filtering. The method uses salient elementary discourse units (EDUs) as latent queries to guide document ranking, then filters out irrelevant EDUs instead of truncating tokens. An Expectation-Maximization algorithm iteratively refines EDU salience scores and document relevance rankings. Evaluated across four MDS datasets with diverse summarizers (BART, PEGASUS, PRIMERA, LLaMA, StableLM) in both fully supervised and few-shot settings, the framework consistently improves ROUGE scores—achieving up to +1.85 on Wikisum and +1.11 on WCEP-10 in PRIMERA. Human evaluation confirms gains in informativeness and fluency. Ablation studies show both EDU filtering and document ranking are essential for performance, and the method outperforms BM25 and DYLE in query selection and ranking accuracy.

## Method Summary
The framework encodes documents with Longformer (chunk size 1024) and segments them into EDUs using the DMRST parser. EDU embeddings are computed via SpanExt attention pooling, and document embeddings via gated pooling of EDU embeddings. A filtering model uses cross-attention over document embeddings plus MLP to score EDU salience, while a ranking model scores documents via multi-query dot-product similarity. An EM loop unifies query selection (top-k salient EDUs) and parameter updates, with losses combined as L_total = L_rank + λ·L_filter (λ=1.0). Training data is created by scoring EDUs and documents against reference summaries using multi-qa-mpnet-base-cos-v1 cosine similarity. At inference, the model ranks documents, removes lowest-salience EDUs to fit context length, and passes processed input to the downstream summarizer.

## Key Results
- Achieves up to +1.85 ROUGE improvement on Wikisum and +1.11 on WCEP-10 in PRIMERA setting
- Outperforms BM25 and DYLE baselines in both document ranking and query selection metrics
- Ablation studies confirm both EDU filtering and document ranking are essential for performance gains
- Human evaluation shows improved informativeness and fluency compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Latent Query Selection for Document Ranking
The framework identifies the most salient EDUs from input documents and uses them as "latent queries" to calculate relevance scores for ranking documents. This process is integrated into an Expectation-Maximization (EM) algorithm, where the E-step selects these queries based on EDU salience scores. The core assumption is that the salience of an EDU, determined by its alignment with the reference summary during training, correlates with its ability to act as an effective query for retrieving relevant documents for summarization.

### Mechanism 2: Fine-Grained EDU Filtering for Context-Length Management
Instead of simply dropping the last tokens or sentences to fit a context window, the framework calculates a salience score for each EDU and removes the lowest-ranked EDUs. This preserves a higher density of relevant information and reduces irrelevant content that could distract the summarizer. The core assumption is that salient information is not evenly distributed, and even within a single sentence, some segments (EDUs) are more critical for the final summary than others.

### Mechanism 3: Unified Optimization via Expectation-Maximization (EM)
The EM algorithm treats latent queries as unobserved variables. The E-step selects the most salient EDUs as these queries, while the M-step updates the parameters of both the filtering and ranking models based on the losses calculated using these queries. This iterative process allows the document embeddings and EDU representations to be refined jointly, solving the interdependent tasks of identifying salient content, ranking documents, and filtering irrelevant units as a joint optimization problem.

## Foundational Learning

**Concept: Elementary Discourse Unit (EDU)**
- Why needed here: This is the fundamental unit of operation for the framework's filtering and query selection mechanisms. Understanding that an EDU is a minimal, coherent discourse segment, smaller than a sentence, is crucial.
- Quick check question: How does an EDU differ from a sentence or a token in the context of semantic coherence?

**Concept: Expectation-Maximization (EM) Algorithm**
- Why needed here: The entire framework is built around an EM process to unify ranking and filtering. One must grasp the E-step (inferring latent queries) and the M-step (optimizing model parameters) to understand the system's iterative refinement.
- Quick check question: In this framework, what is treated as the latent variable in the E-step, and what is being optimized in the M-step?

**Concept: Transformer Context-Length Limitation**
- Why needed here: The core problem this paper addresses is the fixed input length of transformer-based summarizers. The motivation for ranking and filtering is to fit the most critical information within this constraint.
- Quick check question: Why is simple truncation (dropping the last tokens) an inadequate solution for multi-document summarization?

## Architecture Onboarding

**Component map:** DMRST parser -> Longformer encoder -> EDU embeddings (SpanExt pooling) -> Filtering Model (cross-attention + MLP) and Ranking Model (gated pooling + dot-product) -> EM loop -> Downstream summarizer

**Critical path:**
1. **Preprocessing:** Input documents are segmented into EDUs using the DMRST parser
2. **Training Data Creation:** EDUs are scored and documents are ranked based on their similarity to a reference summary
3. **Iterative Training (EM):** The model is trained by alternating between the E-step (selecting salient EDUs as queries) and the M-step (updating the Filtering and Ranking models)
4. **Inference:** The trained model scores and ranks input documents and filters low-scoring EDUs before passing the processed context to the downstream summarizer

**Design tradeoffs:**
- **Granularity vs. Complexity:** Using EDUs provides fine-grained filtering but requires an external parser (DMRST), adding a dependency and potential error source from segmentation inaccuracies
- **Supervision Type:** The training data creation relies on the assumption that similarity to the reference summary is a perfect proxy for salience
- **Iterative vs. Single-Pass:** The EM algorithm is more complex to implement and train than a single-pass model but is key to the framework's unified performance

**Failure signatures:**
- **Over-filtering:** Excessive removal of EDUs results in a summary that is too sparse or misses key details
- **Under-ranking:** Important documents are still ranked low and truncated, suggesting the latent queries are not capturing the core topic
- **Segmentation Artifacts:** The generated summary is disjointed or nonsensical, possibly caused by poor EDU segmentation breaking semantic units

**First 3 experiments:**
1. **Ablation Study:** Remove the filtering component and use only ranking (or vice-versa) to confirm that both mechanisms contribute to the performance gain
2. **K-Parameter Sweep:** Vary the number of latent queries (k) to find the optimal point that balances query diversity and focus
3. **Granularity Analysis:** Compare the performance of the model when using sentence-level segmentation versus EDU-level segmentation to quantify the benefit of the finer granularity

## Open Questions the Paper Calls Out
- Can the ReREF framework be optimized to maintain low latency and computational efficiency when applied to extremely large-scale, real-time multi-document summarization scenarios?
- To what extent does the performance of the ReREF framework depend on the accuracy of the external EDU segmentation parser (DMRST), and is it robust to segmentation errors?
- Is the framework's performance constrained by the specific choice of the pre-trained model (multi-qa-mpnet-base-cos-v1) used to generate the ground-truth relevance scores for training?

## Limitations
- Performance depends on the accuracy of external EDU segmentation parser (DMRST), which could propagate errors through the pipeline
- Training data creation assumes reference summaries perfectly align with salience, but real-world summaries may contain noise or biases
- EM algorithm's convergence properties and sensitivity to initialization are not fully specified, affecting reproducibility

## Confidence

**High Confidence:**
- EDU filtering mechanism improves context-length management compared to naive truncation
- Unified EM framework outperforms baseline methods in both document ranking and query selection metrics

**Medium Confidence:**
- Scalability to very large document collections has not been established
- Effectiveness of latent queries compared to traditional query methods needs more direct comparison

**Low Confidence:**
- Performance on extremely short summaries (<50 words) has not been tested
- Generalization to non-English languages or specialized domains is untested

## Next Checks
1. **Convergence Analysis:** Run EM training with multiple random seeds and track stability of selected latent queries over iterations to assess sensitivity to initialization
2. **Granularity Impact Study:** Compare performance using sentence-level versus EDU-level segmentation across all four datasets to quantify granularity benefits
3. **Query Ablation Test:** Replace EM-selected latent queries with alternative sources (BM25, RAKE, random EDU) while keeping other components constant to isolate query selection contribution