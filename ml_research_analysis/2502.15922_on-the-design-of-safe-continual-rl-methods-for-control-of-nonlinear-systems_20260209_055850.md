---
ver: rpa2
title: On the Design of Safe Continual RL Methods for Control of Nonlinear Systems
arxiv_id: '2502.15922'
source_url: https://arxiv.org/abs/2502.15922
tags:
- safe
- task
- learning
- safety
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the intersection of safe and continual reinforcement
  learning for control of nonlinear systems subject to safety constraints. The authors
  study how typical safe RL algorithms (CPO) and continual RL algorithms (PPO+EWC)
  perform when environmental conditions change over time, such as when faults occur
  in robotic systems.
---

# On the Design of Safe Continual RL Methods for Control of Nonlinear Systems

## Quick Facts
- arXiv ID: 2502.15922
- Source URL: https://arxiv.org/abs/2502.15922
- Reference count: 24
- Primary result: Safe EWC achieves better balance of safety and continual learning than CPO or PPO+EWC alone

## Executive Summary
This paper addresses the challenge of safe continual reinforcement learning for control of nonlinear systems subject to safety constraints. The authors investigate how typical safe RL algorithms (CPO) and continual RL algorithms (PPO+EWC) perform when environmental conditions change over time, such as when faults occur in robotic systems. Through experiments on MuJoCo HalfCheetah and Ant environments with velocity constraints and joint loss non-stationarity, they find that CPO maintains safety but catastrophically forgets previous task knowledge, while PPO+EWC learns to remember tasks but ignores safety constraints. To address this gap, the authors propose a simple reward-shaping method (Safe EWC) that modifies the PPO+EWC reward to penalize safety violations. Their results show that Safe EWC achieves a better balance, maintaining safety comparable to CPO while significantly reducing catastrophic forgetting.

## Method Summary
The paper evaluates three algorithms on safe continual RL tasks: CPO (constrained policy optimization), PPO+EWC (policy optimization with elastic weight consolidation), and Safe EWC (reward shaping with EWC regularization). The experimental setup uses Safety Gymnasium's velocity-constrained MuJoCo HalfCheetah and Ant environments, modified to remove front/back legs to simulate joint failures. Agents are trained on a sequence of tasks, switching environments every 1M timesteps over 8M total timesteps. Metrics include Task Forget Percentage (percentage drop in reward when revisiting a task) and Total Cost (cumulative constraint violations). Safe EWC modifies the reward function by subtracting β times the cost, while maintaining EWC regularization with λ=10.

## Key Results
- Safe EWC achieves better balance of safety and continual learning than CPO or PPO+EWC alone
- CPO maintains safety but catastrophically forgets previous tasks (forgetting percentages up to 108%)
- PPO+EWC learns to remember tasks but ignores safety constraints (cost spikes to 25,000+)
- Safe EWC reduces forgetting to 19.6-62.3% while maintaining costs comparable to CPO

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Elastic Weight Consolidation (EWC) reduces catastrophic forgetting by penalizing changes to task-critical neural network parameters.
- **Mechanism:** EWC computes the Fisher information matrix F to identify which parameters θ were most important for solving previous tasks. The regularization term λ/2 Σᵢ Fᵢ(θᵢ − θ*_{A,i})² is added to the loss, creating a soft constraint that increases loss when important weights shift significantly. Separate Fisher matrices are stored per task, allowing the penalty to preserve knowledge across multiple task transitions.
- **Core assumption:** The Fisher information matrix accurately captures parameter importance, and the quadratic penalty sufficiently protects critical weights without preventing new learning.
- **Evidence anchors:**
  - [Section III.B]: "The key idea behind EWC is to determine which weights of the neural network were most important for solving the previous task. Then, the network is penalized for drastically changing those weights."
  - [Table I]: PPO+EWC achieved 26.1% forgetting on nominal HalfCheetah task vs. CPO's 46.6%, demonstrating the mechanism's effectiveness.
  - [Corpus]: Related work on safe continual RL methods (arXiv:2601.05152) surveys similar regularization approaches for non-stationary environments, suggesting broader applicability.
- **Break condition:** When λ is set too high, the agent cannot learn new tasks; when too low, forgetting approaches standard PPO levels. Complex multi-objective optimization may cause interference between EWC penalties for different tasks.

### Mechanism 2
- **Claim:** Reward shaping with cost penalties converts safety constraints into optimizable reward signals that EWC can preserve across task changes.
- **Mechanism:** The Safe EWC modification transforms the CMDP into a modified MDP by defining r_{Safe EWC}(s,a) = r(s,a) − βC(s,a). This embeds safety into the objective function, meaning the Fisher information matrix captures both task performance and safety behavior as "important" to preserve. When the agent revisits a task, EWC's memory of safe actions reduces safety violations.
- **Core assumption:** The cost weight β appropriately balances safety vs. reward; safety constraints are consistent across tasks (not time-varying); soft constraint violations are acceptable.
- **Evidence anchors:**
  - [Abstract]: "Safe EWC achieves a better balance, maintaining safety comparable to CPO while significantly reducing catastrophic forgetting."
  - [Table I]: Safe EWC achieved total cost of 680.1 on nominal HalfCheetah vs. CPO's 787.4, with lower forgetting (19.6% vs 46.6%).
  - [Corpus]: Weak corpus evidence directly comparing reward shaping vs. constrained optimization for continual learning—this appears to be an underexplored area per the paper's motivation.
- **Break condition:** If β is mis-specified, the agent may either ignore safety (β too low) or become overly conservative (β too high). The paper notes that CPO "automatically picks penalty coefficients" while Safe EWC requires manual tuning.

### Mechanism 3
- **Claim:** CPO's constrained optimization maintains safety but provides no mechanism for remembering previous task policies.
- **Mechanism:** CPO performs trust-region policy updates that maximize reward subject to cost constraints and KL-divergence limits: π_{k+1} = argmax E[A^{π_k}] subject to J_{C_i}(π_k) + (1/(1−γ))E[A^{π_k}_{C_i}] ≤ d_i and D̄_{KL}(π||π_k) ≤ δ. The constraint satisfaction is myopic—it ensures safety at each update but has no mechanism to protect weights important for previous tasks, leading to catastrophic forgetting when environment dynamics change.
- **Core assumption:** The trust-region constraint prevents catastrophic policy changes locally, but this does not generalize to protecting long-term memory across non-stationary environments.
- **Evidence anchors:**
  - [Section III.B]: "CPO handles constraints directly in the policy optimization process. It performs trust-region policy updates that ensure the policy at the next step is not outside a stable region."
  - [Table II]: CPO exhibited 15.1% to 50.8% forgetting across Ant tasks despite maintaining safety, demonstrating the mechanism's limitation.
  - [Corpus]: Related work on barrier states and control Lyapunov functions (arXiv:2504.15423, arXiv:2501.15373) addresses safety but does not explicitly tackle continual learning memory preservation.
- **Break condition:** When tasks require substantially different policies (e.g., crawling vs. walking after leg loss), CPO overwrites previous policy parameters to optimize current performance, causing irreversible forgetting.

## Foundational Learning

- **Concept:** Constrained Markov Decision Processes (CMDPs)
  - **Why needed here:** The paper extends standard RL to include cost functions C(s,a) with constraints d, requiring understanding of how to balance reward maximization with constraint satisfaction.
  - **Quick check question:** Can you explain why a velocity constraint creates a conflict between the reward objective (travel far) and safety (don't exceed threshold)?

- **Concept:** Fisher Information Matrix for Parameter Importance
  - **Why needed here:** EWC relies on approximating which neural network parameters are "important" for a task using Fisher information; understanding this is critical for implementing and debugging continual learning.
  - **Quick check question:** Why does EWC use a diagonal approximation of the Fisher matrix rather than the full matrix?

- **Concept:** Trust-Region Policy Optimization
  - **Why needed here:** CPO builds on trust-region methods; understanding KL-divergence constraints on policy updates is necessary to interpret why CPO maintains stable safety but forgets tasks.
  - **Quick check question:** How does constraining D_{KL}(π_{new} || π_{old}) prevent dangerous policy changes during training?

## Architecture Onboarding

- **Component map:**
  - PPO/CPO base algorithm -> EWC regularization module -> Safety module (CPO constraints OR Safe EWC reward shaping) -> Task detection system -> Memory storage for Fisher matrices

- **Critical path:**
  1. Train agent on Task 1 until convergence (or fixed budget)
  2. At task boundary, compute Fisher matrix F using final 20 episodes
  3. Store F and current parameters θ* for completed task
  4. Initialize new task learning with EWC penalty: L = L_{PPO} + λ/2 Σ Fᵢ(θᵢ − θ*ᵢ)²
  5. If using Safe EWC, additionally modify reward: r' = r − βC
  6. Repeat for each task in sequence

- **Design tradeoffs:**
  - **λ (EWC strength):** Higher values preserve past tasks better but slow new learning; paper found λ=10 stable for HalfCheetah
  - **β (cost weight):** Must be calibrated to reward scale; paper used β=5 based on max_reward/max_cost ratio
  - **Algorithm choice:** CPO = safer but forgets; PPO+EWC = remembers but unsafe; Safe EWC = middle ground with manual tuning overhead
  - **Task granularity:** Paper found simple parameter changes (mass, friction) don't require continual learning—only structural changes (joint loss) do

- **Failure signatures:**
  - **CPO forgetting:** Rapid reward drop when revisiting tasks (e.g., 108% forget on front HalfCheetah task)
  - **PPO+EWC safety violations:** Cost curves spiking to 25,000+ on HalfCheetah vs. ~700 for safe methods
  - **Safe EWC over-conservatism:** Lower final task reward than CPO (e.g., Ant nominal: 2880 vs 2901) if β too high
  - **Sample efficiency issues:** PPO-based methods slower to converge than CPO on Ant (4-5M timesteps vs. faster CPO learning)

- **First 3 experiments:**
  1. **Baseline replication:** Train CPO, PPO+EWC, and Safe EWC on the {nominal, back} HalfCheetah sequence (2M timesteps each) to verify you can reproduce the forgetting and cost differences reported in Table I.
  2. **Hyperparameter sensitivity:** Vary λ ∈ {1, 5, 10, 25, 100} and β ∈ {1, 5, 10} on a single task transition to characterize the Pareto frontier of forgetting vs. safety.
  3. **Domain transfer:** Apply Safe EWC to a new non-stationary environment (e.g., Safety Gymnasium's Hopper or Walker2D) to test whether the reward-shaping approach generalizes beyond locomotion tasks with joint loss.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Which types of non-stationarity actually require continual RL mechanisms versus those where a single robust policy suffices?
- **Basis in paper:** [explicit] "It is not clear what types of tasks or task sequences require mechanisms from continual RL."
- **Why unresolved:** The authors observed that parameter changes (mass, friction) allowed a single policy to solve all tasks, but structural changes (joint loss) required continual learning—no systematic characterization exists.
- **What evidence would resolve it:** A taxonomy mapping non-stationarity types (parameter drift, structural faults, environment shifts) to whether continual mechanisms provide measurable benefit over standard safe RL.

### Open Question 2
- **Question:** Can safe continual RL algorithms be developed that automatically tune the safety-reward tradeoff without manual hyperparameter selection?
- **Basis in paper:** [explicit] "Safe EWC is a first step at a safe continual RL algorithm, but more sophisticated algorithms that are less dependent on hyperparameters should be developed in future work."
- **Why unresolved:** Safe EWC requires manually tuned β (cost weight) and λ (EWC strength), whereas CPO automatically adjusts penalty coefficients.
- **What evidence would resolve it:** Development and evaluation of algorithms with adaptive penalty mechanisms that match CPO's automatic tuning while retaining continual learning properties.

### Open Question 3
- **Question:** Why does the catastrophic forgetting of safe RL algorithms vary significantly across different robotic systems?
- **Basis in paper:** [explicit] "This implies that the ability of an agent trained using a safe RL algorithm to avoid catastrophic forgetting can be task-dependent, warranting future studies."
- **Why unresolved:** CPO exhibited 46.6% forgetting on HalfCheetah nominal task but only 15.1% on Ant nominal task—the factors driving this difference remain unidentified.
- **What evidence would resolve it:** Controlled ablation studies across environments varying action space dimensionality, observation complexity, and constraint-reward coupling to identify which factors predict forgetting severity.

## Limitations
- Experimental scope limited to velocity-constrained MuJoCo locomotion tasks with simple structural changes
- Reward shaping requires careful manual hyperparameter tuning of β per environment
- Assumes perfect task boundary detection, which is unrealistic in practical applications
- Effectiveness of 20-episode Fisher computation may not scale to more complex systems

## Confidence

- **High confidence**: The fundamental observation that CPO maintains safety while PPO+EWC prevents forgetting is well-supported by the experimental results.
- **Medium confidence**: The claim that simple reward shaping can solve the safe continual RL problem is supported within the tested domain but may not generalize to environments with varying constraint types or reward structures.
- **Low confidence**: The paper's suggestion that this approach applies broadly to "control of nonlinear systems" is not validated beyond the MuJoCo locomotion domain.

## Next Checks

1. **Cross-domain validation**: Test Safe EWC on Safety Gym's Hopper or Walker2D environments with different constraint types (e.g., goal-reaching with obstacle avoidance) to verify generalizability beyond velocity constraints and joint loss scenarios.

2. **Adaptive β tuning**: Implement an automatic β selection mechanism that adjusts based on observed constraint violations during training, rather than requiring manual calibration per environment.

3. **Real-world fault scenarios**: Evaluate the approach on a robotic platform with realistic fault conditions (e.g., motor degradation, sensor noise) to test whether the learned safety behavior transfers from simulation to physical systems.