---
ver: rpa2
title: Long-Tailed Data Classification by Increasing and Decreasing Neurons During
  Training
arxiv_id: '2507.09940'
source_url: https://arxiv.org/abs/2507.09940
tags:
- neurons
- neuron
- learning
- classes
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve classification performance
  on long-tailed data by dynamically adjusting the number of neurons during training.
  Inspired by biological neural plasticity, the approach periodically adds neurons
  important for minority-class representation and prunes less influential ones, based
  on accumulated loss gradients.
---

# Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training

## Quick Facts
- arXiv ID: 2507.09940
- Source URL: https://arxiv.org/abs/2507.09940
- Reference count: 33
- Primary result: Improves minority-class accuracy in long-tailed classification by dynamically adding/removing neurons based on gradient magnitude

## Executive Summary
This paper proposes a method to improve classification performance on long-tailed data by dynamically adjusting the number of neurons during training. Inspired by biological neural plasticity, the approach periodically adds neurons important for minority-class representation and prunes less influential ones, based on accumulated loss gradients. To mitigate class imbalance effects, gradients are reweighted per class, and weight scaling is applied after each modification to stabilize training. Newly added neurons are initialized with randomized BatchNorm parameters to ensure gradient diversity. Experiments on Imbalanced CIFAR-10/100, ImageNet-LT, and iNaturalist-2018, using five representative architectures, show consistent improvements over fixed-size networks.

## Method Summary
The method dynamically restructures neural networks during training by periodically adding neurons important for minority-class learning and pruning less influential ones. At each modification epoch, neurons are ranked by accumulated gradient magnitude over the epoch, with class weights amplifying minority-class gradients. The top α% neurons are duplicated (weights copied) and bottom α% are removed, maintaining total neuron count. Weight scaling preserves output magnitudes, and new neurons receive randomized BatchNorm parameters to ensure gradient diversity. This process repeats every E_mod epochs throughout training.

## Key Results
- ResNet18 on Imbalanced CIFAR-100 (p=100) improved from 37.9% to 41.8% accuracy
- ViT on ImageNet-LT improved from 10.8% to 17.6% on the "Few" class
- Method effectively enhances minority-class learning without degrading majority-class performance
- Integrates well with existing imbalance-handling techniques

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient magnitude accumulated over an epoch provides a more reliable signal for neuron importance than single-batch gradients or weight magnitude, particularly for identifying neurons critical to minority-class learning.
- **Evidence anchors:**
  - [section 3.1.3] "This approach was empirically found to be more effective than pruning neurons based on $\ell_1$-norm"
  - [Table 4] Gradient-based selection (accumulated) achieves 75.4% vs 73.5% for L1-based and 73.9% for final-batch gradients on CIFAR-10 (p=100)
  - [corpus] Related work SInGE [31] confirms integrated gradients over time improve pruning selection vs short-term fluctuations

### Mechanism 2
- **Claim:** Per-class gradient reweighting amplifies minority-class influence on neuron selection decisions, creating a feedback loop where capacity preferentially expands for underrepresented classes.
- **Evidence anchors:**
  - [section 3.2.1] "This boosts the gradient contribution of minority classes"
  - [section 3.2.2] "By reweighting minority-class samples, their associated gradients become more prominent"
  - [Table 5] Ablation shows gradient reweighting (GR) adds +0.2% on CIFAR-10 and +0.8% on CIFAR-100 when combined with neuron selection

### Mechanism 3
- **Claim:** Weight scaling and BatchNorm parameter randomization prevent training collapse after neuron modifications by preserving activation statistics and ensuring gradient diversity.
- **Evidence anchors:**
  - [section 3.4.1] "This scaling avoids abrupt shifts in parameter distributions after neuron modification"
  - [section 3.5.1] "This strategy prevents all newly created neurons from starting with identical statistics"
  - [Table 5] Weight scaling (WS) alone adds +0.4% on CIFAR-100; combined with GDA, full model achieves best results

## Foundational Learning

- **Concept: Class imbalance in gradient-based optimization**
  - **Why needed here:** Understanding why majority classes dominate gradient updates explains why simple resampling is insufficient and motivates gradient reweighting.
  - **Quick check question:** Given a batch with 99 majority samples and 1 minority sample, what proportion of total gradient magnitude comes from the minority class before reweighting? After reweighting with $w_c = 100$?

- **Concept: Neuroplasticity and dynamic network capacity**
  - **Why needed here:** The method draws inspiration from biological neural pruning and neurogenesis; understanding the analogy clarifies why paired addition/removal preserves total neuron count.
  - **Quick check question:** Why does the paper add AND remove neurons simultaneously rather than only adding? What would happen if only addition occurred over 100 epochs?

- **Concept: Gradient accumulation across batches**
  - **Why needed here:** Core to neuron selection; must understand why single-batch gradients fail for minority classes and how epoch-level accumulation helps.
  - **Quick check question:** If minority class appears in only 10% of batches, how does accumulating gradients over an epoch change the relative importance of minority-activated neurons compared to using only the final batch?

## Architecture Onboarding

- **Component map:**
  Training Loop -> Gradient accumulation buffer -> Class-weighted gradient scaling -> Epoch boundary handler -> Neuron ranking and modification -> Weight scaling and BN randomization

- **Critical path:**
  1. **Gradient buffer initialization** → must match network architecture (CNN channels, MLP neurons, Transformer hidden dims)
  2. **Accumulation with class weights** → critical for minority-class signal
  3. **Neuron duplication (not random init)** → copies weights from selected neurons
  4. **Paired addition/removal** → maintains total neuron count
  5. **Apply to final network** → no architecture change at inference

- **Design tradeoffs:**
  - **α (modification ratio):** Paper finds α≈0.3 optimal; too small (≤0.1) has limited effect, too large (≥0.5) causes instability
  - **E_mod (modification epoch):** Every 30 epochs for CIFAR, 20 for ImageNet-LT; earlier = more adaptation but higher instability risk
  - **Architecture compatibility:** Works on CNN channels and Transformer hidden dims; explicitly avoids modifying attention heads

- **Failure signatures:**
  - Training loss oscillation after modification epochs → α too large, reduce to 0.2
  - No accuracy improvement over baseline → check gradient accumulation is epoch-level, not batch-level
  - Minority-class accuracy degrades → class weights not applied or minority samples absent from most batches
  - ViT/MambaOut shows degradation (Figure 1) → architecture-specific sensitivity; requires lower α or later E_mod

- **First 3 experiments:**
  1. **Sanity check:** Implement on ResNet18 + CIFAR-10 (p=100) with α=0.3, E_mod=30; should see ~72%→75% accuracy. Compare gradient-based vs L1-based neuron selection to verify Table 4 results.
  2. **Ablation path:** Run 4 configurations on same setup: (NAM only), (NAM+GR), (NAM+GR+WS), (full); verify progression matches Table 5 (74.3→74.5→74.9→75.4).
  3. **Architecture robustness:** Test on ViT-B with reduced α=0.15; if still degrades, try E_mod=50 to allow more pre-modification learning before structural changes.

## Open Questions the Paper Calls Out
- Can adaptive scheduling of neuron modification improve performance compared to the current fixed-interval approach?
- What is the precise trade-off between the accuracy gains and the computational overhead introduced by the proposed dynamic restructuring?
- Does the method maintain its effectiveness when transferred to domains with extreme imbalance, such as medical imaging or anomaly detection?

## Limitations
- Method's effectiveness depends critically on hyperparameters (α, E_mod, σ) that are underspecified in the paper
- Generalization to extremely long-tailed distributions (p>>100) is not demonstrated
- No analysis of computational overhead from dynamic neuron modifications

## Confidence
- **High confidence** in the core mechanism (gradient-based neuron selection with class weighting) due to strong ablation results
- **Medium confidence** in the specific implementation details (weight scaling factor derivation, BatchNorm randomization scheme) due to limited exposition
- **Low confidence** in architecture-specific robustness given reported degradation on ViT and MambaOut

## Next Checks
1. Verify gradient accumulation implementation: Run with and without epoch-level accumulation on CIFAR-10 (p=100); expect ~2% accuracy difference
2. Test class-weighting sensitivity: Remove reweighting (wc=1) while keeping neuron selection; accuracy should drop by 0.5-1%
3. Confirm weight scaling necessity: Remove scaling (s=1) after modifications; expect training instability or accuracy collapse on CIFAR-100