---
ver: rpa2
title: 'A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for
  Multimodal Question Answering'
arxiv_id: '2508.10337'
source_url: https://arxiv.org/abs/2508.10337
tags:
- learning
- answer
- task
- question
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed a retrieval-augmented vision-language system
  for the MM-RAG QA competition, using curriculum learning within reinforcement learning
  to improve accuracy and reduce hallucinations. Their approach combines supervised
  fine-tuning via knowledge distillation, staged reinforcement learning on increasingly
  difficult question types, and multi-source retrieval augmentation.
---

# A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering

## Quick Facts
- arXiv ID: 2508.10337
- Source URL: https://arxiv.org/abs/2508.10337
- Reference count: 24
- Primary result: 1st place in MM-RAG QA competition Task 1 (52.38% truthfulness score lead)

## Executive Summary
The authors developed a retrieval-augmented vision-language system that combines supervised fine-tuning, curriculum learning within reinforcement learning, and multi-source retrieval augmentation to address multimodal question answering. Their approach achieves state-of-the-art performance in the META CRAG-MM challenge by progressively exposing the model to increasingly difficult samples, preventing reward collapse while maintaining high accuracy and low hallucination rates.

## Method Summary
The system uses Llama-3.2-11B-Vision-Instruct as the base model, first applying supervised fine-tuning with knowledge distilled from GPT-4.1 to build reasoning capabilities. Reinforcement learning follows using a three-stage curriculum where samples are classified as easy or hard by GPT-4o-mini, starting with easy samples only, progressing to mixed difficulty, and ending with real distribution. RAG integration uses model-generated web search queries with multi-stage retrieval (BM25+embedding→rerank) for Tasks 2 and 3.

## Key Results
- 1st place in Task 1 with 52.38% truthfulness score lead over competitors
- 3rd place in Task 3 (multi-turn conversation)
- Curriculum learning reduced hallucination while maintaining high accuracy (FMR=0.571)
- Ablation studies confirm each component is essential for balancing accuracy, missing rate, and hallucination

## Why This Works (Mechanism)

### Mechanism 1
Curriculum learning stabilizes reinforcement learning by preventing reward collapse. Progressive difficulty exposure—training first on easy samples (GPT-4.1 correct answers), then mixed difficulty, then real distribution—allows the model to acquire reasoning skills before learning refusal behavior. This prevents the model from learning that refusing all questions maximizes reward.

### Mechanism 2
Two-stage training separates capability acquisition from calibration. Supervised fine-tuning builds CoT reasoning and tool-use via distillation; RL then calibrates confidence boundaries using the reward signal. Deferring hallucination control to RL prevents conflicting gradient signals.

### Mechanism 3
Model-generated queries reduce retrieval noise compared to raw question embedding. The model abstracts information needs from visual+text context before retrieval, filtering cross-modal semantic gaps. Multi-stage retrieval (BM25+embedding→rerank) further reduces irrelevant context.

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed here: GRPO requires understanding advantage computation, reference models, and policy gradients. Quick check: Can you explain why a reference model prevents policy degradation in PPO-like algorithms?
- **Curriculum Learning**: Why needed here: Determining sample difficulty and pacing requires understanding of loss landscape dynamics. Quick check: How would you detect if a curriculum is too aggressive (jumping to hard samples too early)?
- **Knowledge Distillation**: Why needed here: Generating SFT data from GPT-4.1 requires understanding teacher-student capacity gaps. Quick check: What failure mode occurs if the student model is too small to replicate teacher reasoning?

## Architecture Onboarding

- **Component map**: SFT Module (LoRA adapter) -> RL Module (VisualRFT with GRPO) -> Curriculum Controller (GPT-4o-mini classifier) -> RAG Module (Query generator → web search → multi-stage retriever)
- **Critical path**: 1. SFT training (builds reasoning/tool-use), 2. Curriculum RL Stage 1 (easy samples only, builds answer confidence), 3. Curriculum RL Stage 2 (1:1 easy:hard, builds refusal calibration), 4. Curriculum RL Stage 3 (real distribution, final tuning), 5. RAG integration (query generation + retrieval for Tasks 2/3)
- **Design tradeoffs**: RL improves hallucination control but may degrade query generation; image search abandoned due to noise; missing rate kept high (~0.60-0.71) to minimize hallucination penalty
- **Failure signatures**: Reward black hole (RL without curriculum → missing rate >90%, accuracy <6%), query drift (after RL, generated queries become generic), format collapse (reward hacking produces correct answer format without valid content)
- **First 3 experiments**: 1. Baseline SFT-only: Train on distilled data without RL, measure hallucination rate (expect ~0.50), 2. Ablation Stage 1 vs Stage 3: Compare accuracy/hallucination tradeoff across curriculum stages (Table 3 shows -0.246→0.151 truthfulness), 3. Query quality test: Measure retrieval precision@5 for model-generated queries vs raw questions on held-out Task 2 samples

## Open Questions the Paper Calls Out

### Open Question 1
Can image search be effectively integrated into multi-modal RAG systems despite the noise and object detection limitations observed with Llama Vision? The authors abandoned image search entirely rather than attempting to fix the underlying issues. Visual grounding from sub-regions remains theoretically valuable for multi-modal QA but was not achieved.

### Open Question 2
How can reinforcement learning be modified to jointly optimize both answer accuracy and query generation capability without the observed trade-off? The authors accept this trade-off as acceptable for their competition goals, but no mechanism is proposed to maintain both capabilities simultaneously.

### Open Question 3
What is the sensitivity of curriculum learning performance to the accuracy of difficulty classification, given that GPT-4o mini may mislabel samples? Misclassified samples could place difficult examples in early training stages or vice versa, potentially destabilizing learning.

## Limitations

- Key hyperparameters for SFT and RL stages are omitted, making exact replication difficult
- Effectiveness of easy/hard classification relies on GPT-4o mini's judgment without validation against human annotations
- Tradeoff between query generation quality and RL fine-tuning is acknowledged but not quantitatively characterized

## Confidence

- **High**: Curriculum learning prevents reward collapse (supported by ablation showing RL without curriculum fails catastrophically)
- **Medium**: Knowledge distillation from GPT-4.1 provides sufficient reasoning foundation (assumed from performance gains but not directly measured)
- **Medium**: Model-generated queries reduce retrieval noise (mechanism is plausible but effectiveness could be measurement artifact)

## Next Checks

1. Replicate the SFT-only baseline and measure hallucination rate to establish the foundation quality before RL
2. Test easy/hard classification accuracy by human evaluation on a subset of training samples
3. Measure retrieval precision@5 for model-generated queries versus raw question embeddings on held-out data