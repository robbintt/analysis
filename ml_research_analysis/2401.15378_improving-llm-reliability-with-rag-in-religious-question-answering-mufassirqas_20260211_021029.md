---
ver: rpa2
title: 'Improving LLM Reliability with RAG in Religious Question-Answering: MufassirQAS'
arxiv_id: '2401.15378'
source_url: https://arxiv.org/abs/2401.15378
tags:
- system
- turkish
- engineering
- mufassirqas
- quran
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces MufassirQAS, a RAG-based system that enhances
  LLM reliability for religious question-answering by integrating Islamic texts (Quran
  translations, Hadith collections, and Islamic catechism) into a vector database.
  By retrieving contextually relevant passages before generating responses, MufassirQAS
  mitigates hallucinations and provides transparent citations, including source page
  numbers and referenced articles.
---

# Improving LLM Reliability with RAG in Religious Question-Answering: MufassirQAS

## Quick Facts
- arXiv ID: 2401.15378
- Source URL: https://arxiv.org/abs/2401.15378
- Reference count: 40
- Primary result: RAG-based system demonstrates superior accuracy on sensitive religious questions by providing verifiable citations

## Executive Summary
MufassirQAS is a Retrieval-Augmented Generation (RAG) system designed to enhance Large Language Model (LLM) reliability for Islamic religious question-answering. By integrating authoritative Islamic texts into a vector database and retrieving contextually relevant passages before generation, the system mitigates hallucinations and provides transparent citations with source page numbers. When tested against ChatGPT on sensitive religious questions, MufassirQAS delivered more definitive, accurate answers with verifiable references, demonstrating the potential of RAG to improve reliability in specialized domain question-answering while maintaining ethical discourse.

## Method Summary
The system uses a RAG pipeline with ChatGPT-3.5 Turbo (16k context, temperature 0.5) and OpenAI embeddings. Three Turkish PDF sources (Quran translation, Hadith collections, Islamic Catechism) are extracted, cleaned, and chunked using Recursive Character Text Splitter (1500 tokens, 100-token overlap). Chunks are embedded and stored in FAISS vector database. For each query, top-5 relevant chunks are retrieved and combined with a carefully designed system prompt before LLM generation. The system is deployed via Flowise on HuggingFace Spaces, displaying answers with source citations and retrieved chunks.

## Key Results
- Superior accuracy on sensitive religious questions compared to ChatGPT
- Definitive answers with verifiable citations (including page numbers)
- Reduced hallucinations through grounding in authoritative source passages
- Maintains ethical and respectful discourse through prompt constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG constrains LLM outputs to authoritative source passages, reducing hallucination frequency in domain-specific question-answering.
- Mechanism: User queries are embedded and matched against a pre-indexed vector database of chunked Islamic texts. The top-k retrieved chunks (k=5 in this implementation) are injected into the prompt context, giving the LLM grounded evidence to synthesize rather than generate from parametric memory alone.
- Core assumption: The authoritative sources contain sufficient information to answer the query, and the embedding model captures semantic relevance accurately.
- Evidence anchors:
  - [abstract] "By retrieving contextually relevant passages before generating responses, MufassirQAS mitigates hallucinations and provides transparent citations."
  - [section 2.2] "RAG addresses these challenges by redirecting the LLM to retrieve relevant information from authoritative, predetermined knowledge sources."
  - [corpus] Related work "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots" similarly uses RAG for hallucination mitigation in specialized domains, suggesting cross-domain applicability.
- Break condition: Queries requiring synthesis across distant concepts may fail if retrieval returns insufficient or fragmented chunks; the paper acknowledges "strict adherence to retrieved data" can lead to "fragmented responses that lack broader contextual understanding."

### Mechanism 2
- Claim: Larger chunk sizes with overlap preserve semantic coherence for complex religious texts requiring contextual interpretation.
- Mechanism: Text is split using a Recursive Character Text Splitter (delimiters: "\n\n", "\n", "") with 1500-token chunks and 100-token overlap. Higher overlap maintains logical connections across chunk boundaries for sensitive content.
- Core assumption: Token-based chunking preserves thematic units; overlap captures cross-chunk dependencies.
- Evidence anchors:
  - [section 4.2] "Since the context of religion issues could be long we assign a higher chunk length... we assign a higher value for chunk overlap."
  - [section 4.2] Token statistics show mean tokens per section ranging 737-1022, informing the 1500-token choice.
  - [corpus] No direct corpus evidence on optimal chunking for religious texts; this remains an empirical design choice.
- Break condition: Setting chunk size too large can "hinder the system's ability to maintain coherence and connect relevant knowledge" (section 8).

### Mechanism 3
- Claim: System prompts with explicit constraints can enforce ethical discourse and source citation behavior.
- Mechanism: A carefully designed system prompt defines response format, tone, abstention conditions, and requirements to cite Quranic verses and hadiths explicitly.
- Core assumption: LLM instruction-following is sufficiently robust for the model to reliably adhere to constraints across diverse query types.
- Evidence anchors:
  - [abstract] "We carefully designed system prompts to prevent harmful, offensive, or disrespectful outputs."
  - [section 4.4] "The system prompt must clearly define constraints, including the desired response format, tone, and conditions under which the model should abstain from generating a reply."
  - [section 6] "MufassirQAS tends to provide more references to hadiths and verses, as instructed in the system prompt."
  - [corpus] No corpus evidence quantifying prompt engineering effectiveness in religious domains.
- Break condition: LLM security vulnerabilities "can sometimes result in unexpected outputs" (section 7); prompt-based guardrails are not guaranteed.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: This is the core architecture. You must understand that RAG separates knowledge storage (external vector database) from reasoning (LLM), enabling domain adaptation without fine-tuning.
  - Quick check question: Can you explain why RAG might outperform fine-tuning for domains where knowledge changes or requires precise attribution?

- Concept: Vector embeddings and similarity search
  - Why needed here: The system relies on embedding models to convert text into vectors and similarity metrics to retrieve relevant chunks. Understanding embedding quality directly impacts retrieval relevance.
  - Quick check question: What embedding dimension and similarity metric (cosine, Euclidean, dot product) does your chosen model use, and how does this affect retrieval ranking?

- Concept: Chunking strategies for long-form documents
  - Why needed here: Religious texts have long, interdependent passages. Chunk size and overlap directly affect whether retrieved context is semantically complete.
  - Quick check question: Given a 2000-word theological passage, how would you determine appropriate chunk size and overlap to preserve argumentative coherence?

## Architecture Onboarding

- Component map: PDF sources -> Text extraction -> Chunking (1500 tokens, 100 overlap) -> Embedding generation -> FAISS vector storage -> Query embedding -> Similarity search (top-5) -> Context assembly -> ChatGPT-3.5 Turbo generation -> Response display

- Critical path: Query ingestion → embedding generation → vector search → context assembly (query + retrieved chunks + system prompt) → LLM generation → response with source display. Retrieval quality determines answer groundedness.

- Design tradeoffs:
  - Larger chunks (1500 tokens) preserve context but increase token costs and may dilute relevance signals.
  - Top-5 retrieval provides comprehensive coverage for complex religious topics but risks context window saturation.
  - Using ChatGPT-3.5 API trades local control for implementation simplicity; smaller models would require more rigorous retrieval and prompt optimization.

- Failure signatures:
  - Fragmented answers with disconnected facts: indicates chunk boundaries severing logical dependencies.
  - Vague or hedged responses: may indicate retrieval returned insufficient or weakly relevant chunks.
  - Missing citations: system prompt constraints not being followed; may require prompt refinement or explicit citation formatting instructions.
  - Unexpected/harmful outputs: potential prompt injection or LLM security vulnerability (acknowledged in section 7).

- First 3 experiments:
  1. Retrieval quality baseline: Test a set of 20-30 diverse religious questions, manually evaluate whether retrieved chunks contain the information needed to answer. Log similarity scores and chunk sources.
  2. Chunk size ablation: Compare 500, 1000, 1500, and 2000 token chunks on answer coherence and citation accuracy. Identify the point where larger chunks no longer improve (or start degrading) response quality.
  3. Prompt constraint testing: Submit adversarial or edge-case queries (sensitive topics, ambiguous phrasing) to verify system prompt guardrails prevent disrespectful or unsupported responses. Document any failures for prompt iteration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the lower bounds of model size necessary to maintain high retrieval accuracy and contextual understanding in religious RAG applications?
- Basis in paper: [explicit] The conclusion states, "As a future work, we aim to explore the scalability of RAG in smaller models, testing the lower bounds of model size while maintaining high retrieval accuracy."
- Why unresolved: The current study relied on ChatGPT-3.5 Turbo; the authors note that the performance gap will be more pronounced in smaller models that lack extensive training, but the exact limits of efficiency are unknown.
- What evidence would resolve it: Benchmarking results comparing MufassirQAS's performance using various smaller open-source LLMs (e.g., Llama 7B, Mistral) against the current ChatGPT-3.5 baseline.

### Open Question 2
- Question: How can the RAG architecture be optimized to synthesize meaningful connections between multiple retrieved text chunks to prevent fragmented or context-poor responses?
- Basis in paper: [inferred] The conclusion identifies a limitation where "when dealing with multiple retrieved chunks, the system sometimes struggles to establish meaningful connections between different pieces of information."
- Why unresolved: The current system sometimes produces fragmented responses due to strict adherence to retrieved data without a mechanism to bridge disparate information chunks coherently.
- What evidence would resolve it: Development of a re-ranking or context-synthesis layer that improves the logical flow of answers derived from multiple distinct sources, verified by qualitative analysis.

### Open Question 3
- Question: To what extent can prompt engineering and dataset refinement mitigate inherent biases and security vulnerabilities in religious QA systems?
- Basis in paper: [explicit] The abstract and conclusion list "refining prompt engineering techniques to further minimize biases" and addressing "security vulnerabilities" as necessary future work to ensure reliability.
- Why unresolved: While system prompts currently filter harmful content, the authors acknowledge that potential biases in the dataset and unexpected outputs remain challenges that simple prompting has not fully solved.
- What evidence would resolve it: A comparative study measuring the frequency of biased or vulnerable outputs before and after implementing advanced prompt engineering strategies (e.g., chain-of-thought scrutiny).

## Limitations
- Limited evaluation scope with qualitative comparison against ChatGPT on "sensitive religious questions" without standardized metrics
- Turkish language focus and specific source texts limit generalizability to other religious traditions
- System prompt guardrail effectiveness not empirically tested against adversarial queries
- Potential for fragmented responses due to strict adherence to retrieved data without synthesis mechanisms

## Confidence
**High Confidence**: The core RAG mechanism for reducing hallucinations through retrieval-augmented generation is well-established in the literature and the implementation details (chunk size, overlap, retrieval count) are clearly specified. The claim that RAG can provide verifiable citations by grounding responses in source texts is also well-supported.

**Medium Confidence**: The effectiveness of the 1500-token chunk size with 100-token overlap for preserving semantic coherence in religious texts is plausible but not empirically validated against alternative chunking strategies. The claim that the system provides "definitive answers" on sensitive questions compared to ChatGPT's "ambiguous" responses is based on qualitative comparison without standardized evaluation metrics.

**Low Confidence**: The robustness of the system prompt in preventing harmful or disrespectful outputs is asserted but not tested with edge cases or adversarial queries. The long-term reliability of the system across diverse religious questions and user populations is not demonstrated.

## Next Checks
1. **Quantitative Evaluation**: Conduct a blinded evaluation with 50-100 diverse religious questions scored by domain experts on accuracy, citation completeness, and answer definiteness. Compare against baseline LLM performance using standardized metrics.

2. **Robustness Testing**: Systematically test the system prompt guardrails using adversarial religious queries, edge cases, and prompt injection attempts. Document any failures and iterate on prompt engineering to improve constraint adherence.

3. **Chunking Strategy Ablation**: Compare the current 1500-token chunking strategy against alternatives (500, 1000, 2000 tokens) on a subset of complex theological questions requiring cross-chunk synthesis. Measure answer coherence, citation accuracy, and token efficiency to identify optimal parameters.