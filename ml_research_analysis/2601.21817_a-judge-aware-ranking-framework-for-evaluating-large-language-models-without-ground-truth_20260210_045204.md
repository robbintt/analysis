---
ver: rpa2
title: A Judge-Aware Ranking Framework for Evaluating Large Language Models without
  Ground Truth
arxiv_id: '2601.21817'
source_url: https://arxiv.org/abs/2601.21817
tags:
- judge
- ranking
- comparisons
- judges
- unweighted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a statistical framework for ranking large
  language models (LLMs) using pairwise comparisons from multiple judge LLMs, addressing
  the issue of heterogeneous judge reliability. The authors extend the Bradley-Terry-Luce
  model by incorporating judge-specific discrimination parameters to jointly estimate
  model quality and judge reliability without requiring ground-truth labels.
---

# A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth

## Quick Facts
- **arXiv ID:** 2601.21817
- **Source URL:** https://arxiv.org/abs/2601.21817
- **Reference count:** 8
- **Primary result:** Introduces judge-aware statistical framework for LLM ranking without ground truth

## Executive Summary
This paper addresses the challenge of ranking large language models (LLMs) without ground truth labels by proposing a judge-aware statistical framework. The authors extend the Bradley-Terry-Luce model to account for heterogeneous judge reliability through judge-specific discrimination parameters. The framework jointly estimates model quality and judge reliability from pairwise comparisons, enabling principled uncertainty quantification through confidence intervals.

The method demonstrates improved agreement with human preferences and higher data efficiency compared to unweighted baselines across multiple benchmarks including MT-Bench, Chatbot Arena, and a newly collected dataset with 45 models. The judge-aware approach produces more accurate rankings and narrower confidence intervals while maintaining proper coverage, addressing critical limitations in current LLM evaluation practices.

## Method Summary
The framework extends the Bradley-Terry-Luce model by incorporating judge-specific discrimination parameters that capture heterogeneous judge reliability. This allows the model to jointly estimate both LLM quality parameters and judge reliability parameters from pairwise comparison data. The authors establish identifiability conditions and prove consistency and asymptotic normality of the maximum likelihood estimator. The framework enables principled uncertainty quantification through confidence intervals and demonstrates improved data efficiency compared to unweighted baselines, particularly on small sample sizes (30-50 comparisons).

## Key Results
- Correctly places GPT-4 first in Chatbot Arena rankings where unweighted methods fail
- Achieves 13.5% narrower confidence intervals than standard BTL models while maintaining proper coverage
- Demonstrates superior performance on small sample sizes (30-50 comparisons) compared to unweighted baselines

## Why This Works (Mechanism)
The framework works by recognizing that different judge LLMs have varying reliability levels when evaluating other models. By incorporating judge-specific discrimination parameters into the Bradley-Terry-Luce model, the framework can weight comparisons differently based on judge reliability. This joint estimation approach allows the model to simultaneously learn both the quality of candidate LLMs and the reliability of judge LLMs from the same pairwise comparison data, without requiring external ground truth labels. The theoretical guarantees ensure that this approach produces consistent and asymptotically normal estimates, enabling reliable uncertainty quantification.

## Foundational Learning

**Bradley-Terry-Luce Model:** A statistical framework for ranking items based on pairwise comparisons, where the probability of one item beating another depends on their quality parameters. Why needed: Forms the theoretical foundation for the ranking framework. Quick check: Understand the basic probability formula P(i beats j) = qi/(qi + qj).

**Maximum Likelihood Estimation:** Statistical method for estimating model parameters by finding values that maximize the likelihood of observed data. Why needed: Enables joint estimation of model quality and judge reliability parameters. Quick check: Verify that the likelihood function is properly defined and differentiable.

**Asymptotic Normality:** Property where estimators converge to a normal distribution as sample size increases, enabling confidence interval construction. Why needed: Provides theoretical basis for uncertainty quantification in rankings. Quick check: Confirm that the Fisher information matrix is positive definite.

**Identifiability:** Statistical property ensuring that model parameters can be uniquely determined from observed data. Why needed: Guarantees that the joint estimation problem has a unique solution. Quick check: Verify that the rank conditions specified in the paper are satisfied.

**Confidence Interval Construction:** Method for quantifying uncertainty in parameter estimates using asymptotic normality. Why needed: Enables principled uncertainty quantification in model rankings. Quick check: Validate that coverage probabilities match theoretical guarantees.

## Architecture Onboarding

**Component Map:** Pairwise Comparison Data -> Judge-Aware BTL Model -> Parameter Estimation -> Model Quality Scores + Judge Reliability Scores -> Confidence Intervals

**Critical Path:** Data collection → Joint parameter estimation → Ranking output → Uncertainty quantification

**Design Tradeoffs:** The framework trades computational complexity for improved accuracy and uncertainty quantification. Joint estimation requires more sophisticated optimization compared to simple averaging approaches but yields more reliable rankings.

**Failure Signatures:** If judge reliability parameters are poorly estimated, rankings may be biased toward certain judge preferences. If sample size is too small, confidence intervals may be unreliable despite theoretical guarantees.

**First 3 Experiments:**
1. Validate that judge reliability parameters correctly identify known biases in judge models
2. Test ranking accuracy on synthetic data with known ground truth
3. Compare confidence interval coverage rates against theoretical predictions

## Open Questions the Paper Calls Out

None specified in the provided content.

## Limitations

- **Data Efficiency Generalizability:** Performance with larger model pools or cross-dataset scenarios remains untested
- **Model Architecture Dependency:** Framework may not generalize well to non-LLM systems or different evaluation paradigms
- **Computational Complexity:** Joint estimation likely increases computational overhead compared to simpler methods

## Confidence

**High Confidence:** Statistical assumptions and theoretical guarantees (Bradley-Terry-Luce assumptions, identifiability, consistency)
**Medium Confidence:** Data efficiency claims (based on 45 models, needs larger scale validation)
**Medium Confidence:** Model architecture dependency (requires testing with diverse model types)
**Medium Confidence:** Computational complexity trade-offs (not thoroughly discussed for large-scale applications)

## Next Checks

1. **Cross-Dataset Validation:** Test the framework on additional benchmark datasets beyond MT-Bench and Chatbot Arena, including datasets with known ground truth or human preferences, to validate the claimed improvement in agreement with human preferences.

2. **Stress Testing with Model Diversity:** Evaluate performance when ranking models with varying capabilities and architectures (e.g., including smaller models, specialized models, or non-LLM systems) to assess generalizability beyond the tested 45-model pool.

3. **Judge Model Ablation Studies:** Conduct systematic experiments varying the selection, number, and types of judge models to quantify their impact on ranking accuracy and identify potential judge-specific biases that could affect the framework's reliability.