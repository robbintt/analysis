---
ver: rpa2
title: On the Hardness of Computing Counterfactual and Semifactual Explanations in
  XAI
arxiv_id: '2601.09455'
source_url: https://arxiv.org/abs/2601.09455
tags:
- explanations
- counterfactual
- complexity
- problem
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the computational complexity of generating counterfactual
  and semi-factual explanations for machine learning models. It provides a comprehensive
  overview of existing results and introduces novel inapproximability results for
  several explanation settings.
---

# On the Hardness of Computing Counterfactual and Semifactual Explanations in XAI

## Quick Facts
- **arXiv ID:** 2601.09455
- **Source URL:** https://arxiv.org/abs/2601.09455
- **Reference count:** 36
- **Primary result:** Computing counterfactual and semi-factual explanations is NP-hard for common ML models, with inapproximability results showing even approximate solutions are hard to find.

## Executive Summary
This paper establishes that computing optimal counterfactual and semi-factual explanations for machine learning models is fundamentally a computationally hard problem. Through novel reduction proofs from 3-SAT, the authors demonstrate that finding these explanations is NP-hard or NP-complete for ensembles, ReLU networks, and k-nearest neighbors. The paper introduces inapproximability results showing that even finding approximate solutions within any reasonable factor is intractable, which has significant implications for the XAI community that relies on heuristic methods without theoretical guarantees.

## Method Summary
The authors employ a proof-by-reduction methodology, constructing instances of the WACHTER-CFE problem (Definition 2) from 3-SAT instances. They create regressors h(·) where a counterfactual explanation with low cost exists if and only if the corresponding 3-SAT formula is satisfiable. This establishes NP-hardness, while inapproximability follows from showing that a polynomial-time approximation algorithm would solve 3-SAT, contradicting P≠NP. The analysis covers both counterfactual and semi-factual explanations across multiple model types, with semi-factuals reaching higher complexity classes (Σ₂^p-complete) due to their logical structure.

## Key Results
- Computing counterfactual explanations is NP-hard for ReLU networks, additive trees, and kNN models
- No polynomial-time 2^p(n)-approximation algorithm exists for these models, assuming P≠NP
- Semi-factual explanations can reach Σ₂^p-complete complexity, making them harder than standard counterfactuals
- For simple models like Decision Trees, explanation generation remains in PTIME

## Why This Works (Mechanism)

### Mechanism 1: Computational Hardness via 3-SAT Reduction
The paper establishes hardness by reducing 3-SAT (NP-complete) to WACHTER-CFE. A regressor h(·) is constructed where a counterfactual with low cost exists iff the corresponding 3-SAT formula is satisfiable. Core assumption: P≠NP. Break condition: If input features are continuous and h(·) is strictly convex, polynomial-time solutions may be possible.

### Mechanism 2: The Inapproximability Barrier
Even approximate solutions are hard due to large gaps in the objective function (output is either 0 or massive value M). An approximation algorithm would solve the underlying decision problem, violating complexity assumptions. Core assumption: cost function θ(·) is bounded by a polynomial. Break condition: If users accept qualitatively different (heuristic) solutions rather than approximate optimal ones.

### Mechanism 3: Semi-factual Complexity via Object Conflict
Semi-factuals require identifying a "Minimum Sufficient Reason" (MSR), which involves verifying that a subset of features is sufficient regardless of other feature values. This adds logical quantification (∀) compared to counterfactuals' existence (∃). Core assumption: Models encode complex logical dependencies. Break condition: For simple Decision Trees, complexity drops to PTIME.

## Foundational Learning

**Complexity Classes (P, NP, Σ₂^p)**
- Why needed here: To interpret "hardness" results and distinguish between problems solvable in polynomial time, verifiable in polynomial time, and requiring oracle calls.
- Quick check question: Why is a problem being Σ₂^p-complete considered "harder" than one that is NP-complete?

**Optimization vs. Decision Problems**
- Why needed here: The paper analyzes counterfactuals as optimization problems but proves hardness using decision versions via reduction.
- Quick check question: How does reducing 3-SAT to the decision version of a counterfactual problem prove that finding the optimal counterfactual is hard?

**Approximation Algorithms**
- Why needed here: To understand the significance of "inapproximability" results. An approximation algorithm finds a "good enough" solution; proving none exists implies structural difficulty.
- Quick check question: In the context of Theorem 1, what does an approximation factor of 2^p(n) mean for the "cost" of the explanation generated?

## Architecture Onboarding

**Component map:**
Input (x_orig, Model h(·), Cost function θ) -> Solver Engine (Gradient Descent, Evolutionary, MILP) -> Constraint Layer (plausibility π(·) or robustness ΔH) -> Output (δ_cf, x_cf)

**Critical path:** The interaction between the Cost Function and the Model Decision Boundary. If the boundary is complex (non-convex/discontinuous as in ReLU ensembles), the solver acts as a heuristic search rather than an exact optimizer.

**Design tradeoffs:**
- Optimality vs. Tractability: Exact optimality is infeasible for complex models; architects must choose heuristic solvers that offer speed but no guarantees.
- Model Complexity vs. Explainability: Ensembles and ReLU nets provide higher accuracy but push explanation generation into NP-hard territory, whereas Decision Trees allow PTIME explanation generation.

**Failure signatures:**
- Timeout/Non-termination: Solver fails to find valid δ_cf within resource limits on high-dimensional binary inputs
- Infeasible Solution: Generated x_cf violates plausibility constraints (π(x_cf)=0) because optimization ignored hard logical constraints
- Sparsity Loss: Heuristic returns a counterfactual changing 50 features when a 3-feature change exists (due to approximation hardness)

**First 3 experiments:**
1. **Sanity Check on Simple Models:** Verify PTIME claims by generating counterfactuals for a Decision Tree; measure generation time against input dimension d (should be linear/polynomial).
2. **Heuristic Gap Analysis:** Run a genetic algorithm on a ReLU network with discrete inputs. Compare the cost θ of the found solution against a brute-force lower bound (if d is small) to observe the approximation gap.
3. **Constraint Stress Test:** Implement Eq. 3 (Robustness) on a ReLU ensemble. Measure the increase in computation time and failure rate compared to the "classic" non-robust formulation (Eq. 1) to validate the practical impact of theoretical hardness.

## Open Questions the Paper Calls Out

**Open Question 1: Can semi-factual explanations be efficiently approximated?**
- Basis in paper: [explicit] The conclusion notes an "urgent need for (in-)approximability studies of semi-factuals" similar to the new theorems provided for counterfactuals.
- Why unresolved: The authors proved inapproximability for counterfactuals but state that the lack of a "contrasting property" in semi-factuals hinders the transferability of these proofs.
- What evidence would resolve it: Theorems establishing whether polynomial-time approximation schemes exist for semi-factual explanations in models like ReLU networks or kNN.

**Open Question 2: What is the computational complexity of causal counterfactual explanations?**
- Basis in paper: [explicit] The paper states that the "absence of any complexity results on causal counterfactual explanations is alarming."
- Why unresolved: The authors restricted their analysis to non-causal approaches because current literature on complexity theory has not yet integrated structural causal models.
- What evidence would resolve it: Complexity proofs (e.g., NP-hardness) specifically for counterfactuals generated using structural causal models rather than independent feature assumptions.

**Open Question 3: What are the optimality guarantees for heuristic explanation methods in continuous, non-convex settings?**
- Basis in paper: [explicit] The discussion highlights that "formal statements for general non-convex scenarios remain an open problem," particularly for continuous features.
- Why unresolved: The paper's hardness results focus on discrete/categorical inputs; theoretical guarantees for gradient-based heuristics on continuous data are currently missing.
- What evidence would resolve it: Formal bounds on the optimality gap for common gradient-based heuristics when applied to non-convex models with continuous inputs.

## Limitations
- The paper relies on theoretical complexity assumptions (P ≠ NP) without empirical validation of practical impact
- Focus on discrete input spaces may limit applicability to continuous domains common in many XAI applications
- Assumption of polynomially bounded cost functions θ(·) may not hold for all practical implementations

## Confidence

**High Confidence:** The core theoretical framework establishing NP-hardness for counterfactual explanations in common models (ReLU networks, additive trees, kNN) is well-supported by standard reduction techniques and consistent with established complexity theory.

**Medium Confidence:** The inapproximability results (Theorems 1-3) follow logically from the reduction framework, though the specific polynomial bounds and approximation factors would benefit from explicit verification.

**Low Confidence:** The practical implications of these theoretical results for real-world XAI systems remain uncertain, as the paper does not empirically evaluate heuristic approaches or measure the gap between optimal and approximate solutions in practical settings.

## Next Checks
1. **Empirical Runtime Validation:** Implement the 3-SAT reduction for a simple ReLU network on CNF instances with varying variable counts (n ≤ 20). Measure generation times for counterfactual explanations and compare against theoretical predictions to validate the practical significance of NP-hardness.
2. **Heuristic Performance Benchmarking:** Deploy a genetic algorithm (as referenced in the corpus) on a ReLU network with binary inputs. Generate 100 random instances and compute the distribution of approximation ratios (cost of heuristic solution / cost of optimal solution) to quantify the practical impact of inapproximability.
3. **Constraint Impact Analysis:** Implement the robust counterfactual formulation (Eq. 3) for a kNN model and measure the increase in computation time and solution feasibility rate compared to the classic formulation (Eq. 1) across different neighborhood sizes k.