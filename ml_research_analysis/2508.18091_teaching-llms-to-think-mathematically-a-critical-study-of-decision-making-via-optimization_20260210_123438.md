---
ver: rpa2
title: 'Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making
  via Optimization'
arxiv_id: '2508.18091'
source_url: https://arxiv.org/abs/2508.18091
tags:
- llms
- optimization
- mathematical
- problems
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the capabilities of large language models
  (LLMs) in formulating and solving decision-making problems using mathematical programming.
  A systematic review and meta-analysis of recent literature is conducted, followed
  by targeted experiments using a newly constructed dataset of computer networking
  optimization problems.
---

# Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization

## Quick Facts
- **arXiv ID:** 2508.18091
- **Source URL:** https://arxiv.org/abs/2508.18091
- **Reference count:** 40
- **Primary result:** LLMs show promise in parsing natural language optimization problems but face accuracy, scalability, and interpretability limitations

## Executive Summary
This paper systematically evaluates large language models' capabilities in translating natural language networking optimization problems into mathematical formulations and executable solver code. Through a comprehensive literature review and targeted experiments with two state-of-the-art models (DeepSeek-Math-7b-Instruct and GPT-4o), the study reveals that while LLMs demonstrate significant progress in understanding and representing symbolic formulations, they still struggle with accuracy, scalability, and consistent interpretability. The research identifies key gaps in current approaches and proposes future directions including structured datasets, domain-specific fine-tuning, and hybrid neuro-symbolic architectures.

## Method Summary
The study combines a systematic literature review of recent mathematical programming research with empirical experiments on a newly constructed dataset of 10 networking optimization problems. Three prompting strategies (Act-as-expert, chain-of-thought, and self-consistency) were evaluated across zero-shot, one-shot, and two-shot settings using two LLM models. Performance was measured using three metrics: optimality gap (relative difference between generated and ground truth solutions), token-level F1 score (formulation overlap), and compilation accuracy (executable code generation). The experiments aimed to assess both the models' ability to parse natural language descriptions and their capacity to generate valid mathematical formulations.

## Key Results
- LLMs demonstrate strong capabilities in parsing natural language descriptions and generating symbolic mathematical formulations
- Self-consistency prompting strategy shows promise by generating multiple formulations and selecting the best
- Performance varies significantly across problem types, with simpler linear problems yielding better results than complex non-linear formulations
- Current evaluation metrics (optimality gap, token-level F1) have limitations in pinpointing specific error sources

## Why This Works (Mechanism)
LLMs leverage their extensive training on mathematical and programming content to recognize patterns in problem descriptions and map them to appropriate mathematical structures. The chain-of-thought approach enables step-by-step logical deduction, while self-consistency provides redundancy through multiple generation attempts. The models' ability to handle symbolic notation and translate between natural language and formal mathematical expressions forms the core mechanism for mathematical reasoning.

## Foundational Learning
- **Mathematical Programming Concepts**: Understanding optimization objectives, constraints, and decision variables - needed to formulate problems correctly
- **Natural Language Processing**: Mapping linguistic descriptions to formal mathematical structures - quick check: can the model identify key optimization components from text?
- **Code Generation**: Converting mathematical formulations into executable solver code - quick check: does the output compile without syntax errors?
- **Evaluation Metrics**: Optimality gap, token-level F1, compilation accuracy - needed to quantify model performance
- **Prompt Engineering**: Zero-shot, one-shot, two-shot strategies - quick check: does performance improve with additional examples?
- **Domain Knowledge**: Computer networking terminology and concepts - needed for accurate problem interpretation

## Architecture Onboarding
**Component Map:** Problem Description -> Natural Language Processing -> Mathematical Formulation -> Code Generation -> Solver Execution
**Critical Path:** Natural language parsing → mathematical formulation → code generation → compilation → solution verification
**Design Tradeoffs:** Accuracy vs. interpretability, complexity vs. scalability, monolithic vs. modular approaches
**Failure Signatures:** Hallucinated constraints, syntax errors in generated code, inconsistent performance across problem types
**First Experiments:** 1) Test model performance on problems with increasing complexity, 2) Compare different prompt strategies on the same problem set, 3) Evaluate domain transfer by testing on non-networking optimization problems

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Does training LLMs on datasets explicitly structured to decompose reasoning improve generalization to complex, unseen optimization problems?
- **Basis in paper:** Section IX-A proposes a new dataset structure including a "Problem breakdown" component to teach intermediate reasoning steps
- **Why unresolved:** It is currently unknown if exposing the intermediate logic of modeling explicitly in training data is sufficient to overcome the "surface-level pattern matching" limitation identified in Section VII-B
- **What evidence would resolve it:** A comparative study showing that models fine-tuned on this four-part structured dataset outperform those trained on standard datasets on complex, non-linear problems

### Open Question 2
- **Question:** Can a component-level evaluation framework provide better diagnostic utility than aggregate metrics?
- **Basis in paper:** Section X-A argues that current metrics lack granularity and cannot pinpoint the source of a formulation error
- **Why unresolved:** While the authors identify this limitation, they only propose the framework as a future direction
- **What evidence would resolve it:** Developing a modular evaluation tool and demonstrating that its distinct scores correlate more strongly with specific intervention strategies than aggregate scores do

### Open Question 3
- **Question:** Does a modular, multi-agent architecture outperform monolithic models in generating feasible solutions for high-constraint network optimization problems?
- **Basis in paper:** Section IX-B proposes "Component-wise Specialization" as a strategy to manage complexity
- **Why unresolved:** It is unclear if the overhead of coordinating multiple agents introduces integration errors that negate the benefits of specialization
- **What evidence would resolve it:** Experimental results comparing a multi-agent system against a monolithic GPT-4o baseline on the paper's networking dataset

## Limitations
- Study focuses exclusively on 10 computer networking optimization problems, limiting generalizability to other domains
- Specific code generation format (Pyomo, Gurobi, or other) is not explicitly stated, creating uncertainty about formulation-to-code conversion
- Self-consistency strategy selection criteria is not clearly defined beyond objective value comparison
- Benchmarking includes only two LLM models, limiting assessment of whether limitations are model-specific or inherent to current architectures

## Confidence
- **High confidence:** Systematic literature review methodology and identification of research gaps
- **Medium confidence:** Experimental methodology due to unclear details about code generation and few-shot prompt content
- **Medium confidence:** Conclusions about scalability limitations, consistent with broader observations about LLM reasoning

## Next Checks
1. Reconstruct the few-shot prompts using ComplexOR dataset examples and verify whether performance improves systematically across all prompting strategies
2. Implement a standardized code generation pipeline (e.g., automatic conversion to Pyomo) to eliminate ambiguity in the compilation accuracy metric
3. Test the same methodology on a different optimization domain (e.g., supply chain or scheduling) to assess domain transfer capabilities and determine if networking-specific terminology affects performance