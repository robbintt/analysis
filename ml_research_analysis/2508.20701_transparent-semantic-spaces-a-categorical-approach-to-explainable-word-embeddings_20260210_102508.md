---
ver: rpa2
title: 'Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings'
arxiv_id: '2508.20701'
source_url: https://arxiv.org/abs/2508.20701
tags:
- category
- word
- embeddings
- have
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework based on category theory
  to enhance the explainability of artificial intelligence systems, particularly focusing
  on word embeddings. The key topics include the construction of categories LT and
  PT, providing schematic representations of the semantics of a text T, and reframing
  the selection of the element with maximum probability as a categorical notion.
---

# Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings

## Quick Facts
- arXiv ID: 2508.20701
- Source URL: https://arxiv.org/abs/2508.20701
- Reference count: 2
- Authors: Ares Fabregat-Hernández; Javier Palanca; Vicent Botti
- One-line primary result: This paper introduces a novel framework based on category theory to enhance the explainability of artificial intelligence systems, particularly focusing on word embeddings.

## Executive Summary
This paper presents a novel mathematical framework using category theory to construct transparent, explainable word embeddings. The framework reframes the selection of maximum probability elements as categorical notions and constructs a monoidal category to visualize various methods of extracting semantic information from text. By establishing a mathematically precise method for comparing word embeddings, the paper demonstrates the equivalence between neural network algorithms (GloVe, Word2Vec) and metric MDS, transitioning from black box to transparent approaches. The work also addresses bias computation and mitigation at the semantic space level.

## Method Summary
The method constructs a syntax category $\mathcal{C}_T$ where morphisms are conditional probabilities of extensions, then uses enriched Yoneda embedding to map words into a functor category for cross-length comparison. A monoidal category $\mathcal{P}_T$ is defined where objects are sets of expressions and morphisms are probabilistic matrices, modeling semantic transmission as a "game of telephone." The framework proves neural embedding algorithms (GloVe, Word2Vec) are equivalent to metric MDS through divergence minimization. Bias is computed through quotients in the co-occurrence matrix before embedding, with mitigation strategies at the semantic space level.

## Key Results
- Construction of categories $\mathcal{L}_T$ and $\mathcal{P}_T$ providing schematic representations of text semantics
- Demonstration that GloVe and Word2Vec algorithms are equivalent to metric MDS embeddings
- Definition of divergence as a decoration on word embeddings enabling mathematical comparison
- Mathematical framework for computing and mitigating biases before embedding

## Why This Works (Mechanism)

### Mechanism 1: Enriched Yoneda Embedding for Comparability
- **Claim:** If expressions in a text are treated as objects in an enriched category, the Yoneda embedding allows for the comparison of expressions that do not share a direct sub-expression relationship (e.g., words of the same length).
- **Mechanism:** The paper constructs a syntax category $\mathcal{C}_T$ where morphisms are conditional probabilities of extensions. Because standard morphisms in $\mathcal{C}_T$ only exist between expressions of different lengths, the authors utilize the enriched Yoneda lemma to map words into a functor category. This replaces a word with its set of conditional probabilities relative to all other expressions, allowing for the definition of a similarity metric $p(v\|w)$ between words in the same graded piece.
- **Core assumption:** Semantic similarity can be fully captured by the statistical distribution of conditional probabilities derived strictly from the text corpus $T$.
- **Evidence anchors:**
  - [section 2.3] "By example 2.3, we see that we can now compare two words by comparing their embeddings in the functor category."
  - [abstract] "construction of categories LT and PT... reframing the selection of the element with maximum probability as a categorical notion."
  - [corpus] Weak direct support; neighbor papers focus on "semantic spaces" generally but lack the specific enriched category theory application.
- **Break Condition:** If language context requires external knowledge not present in $T$ (out-of-vocabulary issues), the internal probabilistic comparison fails to capture semantic equivalence.

### Mechanism 2: Monoidal Categories as Probabilistic State Machines
- **Claim:** The transmission of semantic meaning through a text can be modeled as a "game of telephone" within a monoidal category $\mathcal{P}_T$, where endomorphisms represent noise or semantic shifts.
- **Mechanism:** The paper defines $\mathcal{P}_T$ where objects are sets of expressions and morphisms are probabilistic matrices. By defining a tensor product $\otimes$, the framework models the concatenation of expressions. Endomorphisms on the word set $L^1_T$ act as transitions that distort or modify meaning probabilities, visualizing how information changes as it passes through different "cones" of context.
- **Core assumption:** The "semantic telephone" analogy holds; meaning distortion is a compositional process that can be modeled by matrix multiplication and tensor products.
- **Evidence anchors:**
  - [section 3] "Example 3.8 (Semantic Telephone)... encodes the noise a channel can have when transmitting information."
  - [section 3] Definition 3.5 establishes the monoidal structure $(\mathcal{P}_T, \otimes, L^0_T)$.
  - [corpus] "The Process of Categorical Clipping..." hints at categorical segmentation in neural layers, indirectly supporting the structural approach.
- **Break Condition:** If the tensor product of two expressions $X \otimes Y$ frequently results in the empty set (no occurrence in $T$), the monoidal structure loses descriptive power for rare combinations.

### Mechanism 3: Embedding Equivalence via Divergence Minimization
- **Claim:** Neural word embedding algorithms (GloVe, Word2Vec) are theoretically equivalent to Metric Multi-Dimensional Scaling (MDS) because they minimize the same divergence (loss function) between probabilistic similarity and geometric distance.
- **Mechanism:** The paper frames embeddings as mappings from a semantic space $\mathcal{P}_T$ to a configuration space $Conf$. It proves that minimizing the specific loss functions (divergence) of GloVe and Word2Vec results in vector configurations that satisfy the distance constraints of an MDS embedding.
- **Core assumption:** The divergence (loss) is the sole determinant of the embedding structure; optimization trajectories do not introduce distinct inductive biases that separate the algorithms.
- **Evidence anchors:**
  - [section 4] "Theorem 4.12. The GloVe and Word2Vec neural network embeddings are equivalent to metric MDS embeddings on vector spaces."
  - [abstract] "demonstrating the equivalence between the GloVe and Word2Vec algorithms and the metric MDS algorithm."
  - [corpus] "Prediction is not Explanation..." suggests skepticism about embedding explanations, highlighting the need for this rigorous equivalence proof.
- **Break Condition:** If regularization terms (weights $W_{ij}$ in GloVe) fundamentally alter the optimization landscape such that the global minimum is unreachable, the theoretical equivalence may not hold in practice.

## Foundational Learning

- **Concept:** **Enriched Category Theory (specifically $[0,1]$-enriched)**
  - **Why needed here:** The paper relies on categorifying probability distributions. Unlike standard categories where morphisms exist or don't (true/false), here hom-sets are values in $[0,1]$ (probabilities). Understanding this is required to grasp why composition is multiplication and identities are 1.
  - **Quick check question:** If $f: A \to B$ has probability $0.8$ and $g: B \to C$ has probability $0.5$, what is the upper bound of the composite $g \circ f$ in this framework?

- **Concept:** **The Yoneda Lemma**
  - **Why needed here:** The paper uses the Yoneda embedding to "solve" the issue of comparing words that aren't direct extensions of each other. You must understand that looking at an object's relationships to *all* other objects is often more informative than looking at the object itself.
  - **Quick check question:** How does mapping a word $w$ to the functor $\mathcal{C}_T(w, -)$ change the nature of the comparison between two words $v$ and $w$?

- **Concept:** **Metric Multidimensional Scaling (MDS)**
  - **Why needed here:** The paper claims to reduce neural "black boxes" to MDS. You need to know that MDS attempts to place points in a low-dimensional space such that the Euclidean distances between them approximate a given dissimilarity matrix.
  - **Quick check question:** If the paper equates Word2Vec to MDS, what does the Euclidean distance between two word vectors represent in the MDS context?

## Architecture Onboarding

- **Component map:** Text Corpus $T$ -> Alexandrov Space (poset $X_T$) -> Syntax Category $\mathcal{C}_T$ -> Semantic Category $\mathcal{L}_T$ -> Monoidal Category $\mathcal{P}_T$ -> Vector Configuration $(X, C_X)$

- **Critical path:** The definition of the **Semantic Space** (Def 3.9) as an endomorphism in $\mathcal{P}_T$. This is the pivot point where the abstract text statistics become a manipulable matrix that can be embedded into vector space or analyzed for bias.

- **Design tradeoffs:**
  - **Rigidity vs. Flexibility:** $\mathcal{L}_T$ is mathematically rigid (cannot handle smoothing or different counting methods easily). $\mathcal{P}_T$ is introduced to add flexibility (handling different probabilistic matrices) but increases abstraction complexity.
  - **Transparency vs. Dimension:** The framework removes the need to choose a dimension $d$ a priori (dimension-agnostic), but trades this for the complexity of handling infinite-dimensional probability matrices until the embedding step.

- **Failure signatures:**
  - **Sparsity:** If $T$ is too small, the infimum in Equation (9) will be taken over empty or tiny sets, making similarity calculations $p(v\|w)$ unstable.
  - **Non-commutativity Handling:** If the tensor product $X \otimes Y$ yields $L^0_T$ (empty/unit) too often, the category loses structure.

- **First 3 experiments:**
  1. **Verify Colimit Max-Prob:** Implement the colimit calculation for a diagram $F: \mathbf{2} \to \mathcal{L}_T$ on a toy corpus to verify it returns the expression with maximum conditional probability (Theorem 2.7).
  2. **Bias Detection:** Calculate the bias quotient $b_i(k,j)$ (Eq 4.44) for a known biased pair (e.g., "doctor", "man", "woman") directly from the co-occurrence matrix $P$ without training a neural network.
  3. **Loss Equivalence Check:** Run a standard GloVe implementation and an MDS algorithm on the same similarity matrix. Verify if the "stress" (MDS loss) and the GloVe weighted least squares loss converge to similar relative minima (Theorem 4.12).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be generalized to the over-category $L/T$ to analyze biases inherent in training corpora comprising multiple texts?
- **Basis in paper:** [explicit] Section 5.2 states that generalizing to the over-category "provides a rich mathematical framework for investigating biases" and is a direction for future exploration.
- **Why unresolved:** The current framework is defined for a single text $T$, limiting its ability to model interactions between multiple distinct texts or corpora.
- **Evidence would resolve it:** A formal construction of the over-category $L/T$ with definitions for limits and colimits in this multi-text context.

### Open Question 2
- **Question:** Can the concept of divergence in word embeddings be rigorously formalized as a combination of utility functions within a game-theoretic context?
- **Basis in paper:** [explicit] Section 5.2 suggests exploring "the connection between word embeddings, semantic spaces, and games through the lens of divergence as a combination of utility functions."
- **Why unresolved:** The link is currently an analogy; the paper does not define the specific game or utility functions that correspond to embedding divergence.
- **Evidence would resolve it:** A theorem establishing an isomorphism or functor between the category of embeddings and a category of games.

### Open Question 3
- **Question:** How must the definition of semantic spaces and bias be modified to remain valid for sub-word tokenization models?
- **Basis in paper:** [inferred] Section 5.1 notes that for sub-word tokens, "bias... loses the meaning it usually has and becomes a mere statistical preference," indicating the current definition is insufficient for modern tokenizers.
- **Why unresolved:** The current framework defines objects as expressions with semantic meaning, which breaks down when expressions are split into non-semantic sub-word units.
- **Evidence would resolve it:** A modified definition of semantic spaces that accounts for the statistical nature of sub-word units while retaining the ability to detect bias.

## Limitations
- The equivalence between neural embedding algorithms and MDS is proven under idealized conditions but may not hold with practical regularization terms and optimization constraints
- The dimension-agnostic semantic space construction, while theoretically elegant, introduces computational complexity that may limit practical applicability
- The bias mitigation framework assumes linear separability of bias from semantic content, which may not hold for more subtle forms of bias

## Confidence
- **Category Theory Framework Construction**: High confidence - The mathematical definitions and constructions are rigorous and internally consistent
- **Neural Embedding Algorithm Equivalence**: Medium confidence - Theoretical equivalence is proven, but practical implementations may diverge due to optimization specifics
- **Bias Detection and Mitigation**: Low-Medium confidence - The framework provides mathematical tools but practical effectiveness depends on corpus quality and implementation details

## Next Checks
1. **Verify Colimit Max-Prob:** Implement the colimit calculation for a diagram F: 2 → L_T on a toy corpus to verify it returns the expression with maximum conditional probability (Theorem 2.7).

2. **Bias Detection:** Calculate the bias quotient b_i(k,j) (Eq 4.44) for a known biased pair (e.g., "doctor", "man", "woman") directly from the co-occurrence matrix P without training a neural network.

3. **Loss Equivalence Check:** Run a standard GloVe implementation and an MDS algorithm on the same similarity matrix. Verify if the "stress" (MDS loss) and the GloVe weighted least squares loss converge to similar relative minima (Theorem 4.12).