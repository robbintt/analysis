---
ver: rpa2
title: Medical Large Language Model Benchmarks Should Prioritize Construct Validity
arxiv_id: '2503.10694'
source_url: https://arxiv.org/abs/2503.10694
tags:
- validity
- medical
- benchmarks
- construct
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that medical LLM benchmarks need empirical validation\
  \ for construct validity\u2014ensuring they measure what they claim to measure in\
  \ real-world practice. It draws an analogy to psychological testing, showing how\
  \ validity theory can guide benchmark design."
---

# Medical Large Language Model Benchmarks Should Prioritize Construct Validity

## Quick Facts
- arXiv ID: 2503.10694
- Source URL: https://arxiv.org/abs/2503.10694
- Reference count: 30
- Primary result: MedQA benchmark performance correlates weakly with real-world clinical accuracy

## Executive Summary
This paper argues that medical LLM benchmarks need empirical validation for construct validity—ensuring they measure what they claim to measure in real-world practice. Drawing an analogy to psychological testing, the authors propose using real-world EHR data to validate benchmarks. They demonstrate this approach with MedQA, showing that model performance correlates weakly with real-world clinical accuracy and that the benchmark's content domain is simpler than actual clinical practice. The paper advocates for a "benchmark-validation-first" approach where benchmarks are empirically tested before being used to evaluate LLMs.

## Method Summary
The method involves mapping benchmark answers to standardized medical codes (RxNorm/SNOMED), retrieving matched EHR cases containing these codes in assessment/plan sections, and evaluating LLMs on both benchmark and real-world cases. The study computes conditional accuracy α (probability of correct real-world answer given correct benchmark answer) and compares model rankings between MedQA and EHR performance. They also analyze content validity by comparing UMLS concept distributions between benchmark vignettes and real clinical notes, and test construct validity by examining ranking correlations.

## Key Results
- Model performance on MedQA correlates weakly with real-world clinical accuracy (low α values)
- Real-world patient cases contain significantly more UMLS concepts than MedQA clinical vignettes
- MedQA rankings differ substantially from real-world performance rankings
- The benchmark disproportionately favors diagnostic questions over treatment-related ones

## Why This Works (Mechanism)

### Mechanism 1: Latent Construct Analogy
Medical LLM benchmarks can be validated using psychological testing frameworks because both measure latent constructs that lack operational definitions. LLM "capabilities" are unobservable traits inferred from test performance, just as depression or intelligence are inferred from questionnaire responses. Validity theory provides systematic methods to assess whether test scores support inferences about the underlying construct.

### Mechanism 2: EHR-Based Criterion Validation
Benchmark questions can be matched to real-world clinical cases via standardized medical codes to test whether benchmark performance predicts real-world accuracy. This involves mapping diagnoses to SNOMED codes and drugs to RxNorm codes, querying EHR for clinical notes containing these codes, and computing conditional accuracy α.

### Mechanism 3: Content Domain Complexity Gap
Medical exam benchmarks represent a simplified content domain compared to real clinical practice. Clinical vignettes are intentionally streamlined to include only relevant information, while real EHR notes contain substantially more UMLS concepts (including irrelevant details), explaining why benchmark performance doesn't transfer to messy real data.

## Foundational Learning

- Concept: Construct Validity (Psychometrics)
  - Why needed here: The paper's framework depends on understanding how psychological tests establish validity through multiple evidence types converging on whether a test measures its intended construct.
  - Quick check question: A depression test correlates with psychiatrist diagnoses but only