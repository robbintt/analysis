---
ver: rpa2
title: Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints
arxiv_id: '2509.01899'
source_url: https://arxiv.org/abs/2509.01899
tags:
- chief
- entity
- medical
- complaint
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a weakly supervised method for extracting and
  linking medical entities in chief complaints, which are free-text patient descriptions
  of medical issues. The approach uses a split-and-match algorithm to generate weak
  annotations from 1.2 million de-identified chief complaint records, then trains
  a BERT-based model with label smoothing to identify entity mentions and link them
  to a pre-defined ontology.
---

# Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints

## Quick Facts
- arXiv ID: 2509.01899
- Source URL: https://arxiv.org/abs/2509.01899
- Authors: Zhimeng Luo; Zhendong Wang; Rui Meng; Diyang Xue; Adam Frisch; Daqing He
- Reference count: 13
- Key outcome: Weakly supervised method achieves F1 of 67.51% for entity extraction from chief complaints without human annotation

## Executive Summary
This paper presents a weakly supervised approach for extracting and linking medical entities from chief complaints, which are free-text patient descriptions of medical issues. The method uses a split-and-match algorithm to generate weak annotations from 1.2 million de-identified chief complaint records, then trains a BERT-based model with label smoothing to identify entity mentions and link them to a pre-defined ontology. The approach demonstrates superior performance compared to previous methods while avoiding the need for expensive human annotation.

The key innovation lies in leveraging large amounts of unlabeled data through weak supervision, addressing the challenge of processing noisy, unstandardized medical text where traditional supervised learning approaches are impractical due to annotation costs and privacy concerns. The entity extraction model achieves precision of 83.41%, recall of 56.70%, and F1 score of 67.51% in partial match evaluation, outperforming both matching-based methods and other neural models.

## Method Summary
The proposed method consists of two main components: entity extraction and entity linking. For entity extraction, the approach employs a split-and-match algorithm that generates weak annotations from 1.2 million de-identified chief complaint records by splitting text into smaller segments and matching them against a medical ontology. These automatically generated labels are then used to train a BERT-based model with label smoothing to mitigate the noise inherent in weak supervision. The entity linking component takes the extracted mentions and maps them to the most appropriate concept in the Unified Medical Language System (UMLS) ontology using a neural network that considers both the mention text and surrounding context. The two components are trained separately but can be combined for end-to-end entity recognition and linking.

## Key Results
- Entity extraction achieves precision of 83.41%, recall of 56.70%, and F1 score of 67.51% in partial match evaluation
- The model outperforms both matching-based methods and other neural models for entity extraction
- Entity linking demonstrates strong performance when combined with the extraction model
- The approach requires no human annotation while achieving superior results compared to previous methods

## Why This Works (Mechanism)
The method succeeds by leveraging the abundance of unlabeled clinical text data through weak supervision, which is particularly valuable in medical domains where annotation is expensive and privacy concerns limit data sharing. The split-and-match algorithm creates high-quality silver labels by breaking down longer clinical narratives into smaller, more manageable segments that can be matched against medical ontologies. Label smoothing in the BERT model helps the system handle the inherent noise in these automatically generated labels, preventing overfitting to potentially incorrect annotations. The entity linking component benefits from the contextual information captured by the extraction model, allowing for more accurate mapping to standardized medical concepts.

## Foundational Learning

**Weak Supervision**: A machine learning paradigm that uses noisy, imprecise sources to provide training data; needed because manual annotation of medical text is expensive and privacy-restricted; quick check: verify the quality and coverage of automatically generated labels.

**Label Smoothing**: A regularization technique that prevents the model from becoming overconfident in its predictions by distributing some probability mass across all classes; needed to handle noise in weak annotations; quick check: compare performance with and without label smoothing.

**Medical Ontologies**: Structured vocabularies like UMLS that provide standardized medical concepts and relationships; needed to map diverse clinical text to consistent medical terminology; quick check: evaluate linking accuracy across different ontology branches.

**BERT-based Models**: Transformer architectures pre-trained on large corpora that capture contextual information in text; needed for understanding the complex language patterns in clinical narratives; quick check: assess performance on domain-specific vs. general medical text.

## Architecture Onboarding

**Component Map**: Split-and-match algorithm -> Weak annotation generation -> BERT model with label smoothing -> Entity extraction -> Entity linking component -> UMLS ontology mapping

**Critical Path**: Chief complaint text → Split-and-match processing → Weak annotation generation → BERT model training → Entity extraction → Context-aware entity linking → UMLS concept mapping

**Design Tradeoffs**: The approach trades potential noise in weak annotations for massive scalability and privacy preservation, sacrificing some precision for broad coverage and avoiding manual annotation costs.

**Failure Signatures**: Poor performance on rare medical conditions not well-represented in the training data, difficulty with highly colloquial or non-standard medical descriptions, and potential cascading errors where extraction errors propagate to linking failures.

**First Experiments**: 1) Evaluate entity extraction performance on a held-out test set of manually annotated chief complaints; 2) Compare weak supervision approach against fully supervised baseline with limited annotations; 3) Test the model's ability to generalize to different types of clinical text beyond chief complaints.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation based on a relatively small validation set of only 350 manually annotated chief complaints, raising questions about generalizability
- Focus on chief complaints limits applicability to broader clinical documentation types
- Limited comparison with modern large language models and emerging neural architectures
- Lack of detailed analysis on noise characteristics in weak annotations and their impact on model performance

## Confidence
- High confidence in the methodological framework and overall approach
- Medium confidence in the reported performance metrics due to limited validation data
- Low confidence in generalizability beyond chief complaints and to different clinical contexts

## Next Checks
1. Evaluate the model on a larger and more diverse test set including different types of clinical notes beyond chief complaints (discharge summaries, progress notes, etc.)
2. Conduct an error analysis to characterize the types of false positives and false negatives, particularly focusing on how noise in the weak annotations affects model performance
3. Compare the proposed approach with fine-tuned large language models (e.g., clinical BERT variants, GPT models) on the same task to establish relative performance in the current landscape of NLP approaches