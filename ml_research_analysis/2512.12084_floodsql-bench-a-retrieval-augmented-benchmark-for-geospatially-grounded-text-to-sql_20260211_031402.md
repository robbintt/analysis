---
ver: rpa2
title: 'FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded
  Text-to-SQL'
arxiv_id: '2512.12084'
source_url: https://arxiv.org/abs/2512.12084
tags:
- table
- spatial
- joins
- unique
- census
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FloodSQL-Bench is a new benchmark designed for Text-to-SQL tasks
  in geospatial flood management domains. It integrates heterogeneous datasets through
  key-based, spatial, and hybrid joins to capture realistic flood-related information
  needs across social, infrastructural, and hazard data layers.
---

# FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL

## Quick Facts
- arXiv ID: 2512.12084
- Source URL: https://arxiv.org/abs/2512.12084
- Reference count: 19
- Primary result: Introduces a retrieval-augmented benchmark for Text-to-SQL in flood management domains, highlighting LLM challenges on complex geospatial queries

## Executive Summary
FloodSQL-Bench is a new benchmark designed for Text-to-SQL tasks in geospatial flood management domains. It integrates heterogeneous datasets through key-based, spatial, and hybrid joins to capture realistic flood-related information needs across social, infrastructural, and hazard data layers. The benchmark evaluates recent large language models under retrieval-augmented generation settings, measuring performance across six difficulty tiers from single-table lookups to triple-table spatialâ€“spatial joins. Results show that while LLMs can handle simple queries reasonably well, they exhibit substantial degradation on complex multi-table and geospatial queries, highlighting the need for structured, metadata-driven, and domain-aware methods. FLOODSQL-Bench provides a practical testbed for advancing Text-to-SQL research in high-stakes geospatial applications.

## Method Summary
The benchmark constructs a geospatially-grounded dataset by integrating flood-related social, infrastructural, and hazard datasets via key-based, spatial, and hybrid joins. Query complexity is stratified into six tiers, ranging from simple single-table lookups to triple-table spatial joins. Evaluation leverages retrieval-augmented generation pipelines, where LLMs must translate natural language questions into executable SQL queries that span heterogeneous data sources. Performance is assessed using standard SQL accuracy metrics, with a focus on both semantic correctness and adherence to geospatial constraints.

## Key Results
- LLMs achieve reasonable accuracy on simple single-table queries but struggle significantly with complex multi-table and spatial joins.
- Retrieval-augmented generation settings introduce variability, with performance tightly coupled to retrieval accuracy and prompt design.
- FLOODSQL-Bench exposes critical gaps in LLM ability to reason about geospatial relationships and metadata-driven joins in domain-specific contexts.

## Why This Works (Mechanism)
FloodSQL-Bench leverages realistic, heterogeneous geospatial datasets to simulate the complexity of real-world flood management queries. By integrating data through key-based, spatial, and hybrid joins, the benchmark forces models to reason across multiple data layers and handle both semantic and spatial constraints. The retrieval-augmented generation setup mirrors practical deployment scenarios, where relevant data must first be identified before query generation. This structure reveals the limitations of current LLMs in handling multi-hop reasoning and geospatial logic, especially as query complexity increases.

## Foundational Learning
- **Key-based joins**: Needed for linking records across tables via shared identifiers; quick check: verify foreign key relationships are correctly resolved.
- **Spatial joins**: Required for querying geospatially related features (e.g., flood zones and infrastructure); quick check: ensure spatial predicates (ST_Within, ST_Intersects) are correctly applied.
- **Hybrid joins**: Combine key and spatial logic for richer query semantics; quick check: validate that both attribute and geometry conditions are met.
- **Metadata-driven methods**: Enable schema inference and query construction from heterogeneous sources; quick check: confirm metadata extraction aligns with table relationships.
- **Retrieval-augmented generation**: Integrates external data retrieval into LLM workflows; quick check: assess retrieval relevance and coverage for each query.
- **Query difficulty stratification**: Organizes tasks by complexity to track model progress; quick check: ensure tier assignments match intended reasoning complexity.

## Architecture Onboarding
- **Component map**: Datasets -> Join Layer -> Query Generator -> LLM -> SQL Output -> Evaluation Metric
- **Critical path**: User query -> Retrieval engine -> Schema & data selection -> SQL generation -> Execution -> Accuracy assessment
- **Design tradeoffs**: Balances realism (heterogeneous, complex joins) with tractability (stratified difficulty, controlled retrieval); prioritizes domain relevance over generalizabilty.
- **Failure signatures**: Misalignment in join logic, incorrect geospatial predicates, or retrieval errors cascade into SQL inaccuracies; most failures occur at multi-table and spatial reasoning stages.
- **First experiments**:
  1. Execute single-table lookup queries to establish baseline LLM performance.
  2. Test two-table key-based joins to assess cross-table reasoning.
  3. Run spatial join queries to evaluate geospatial understanding.

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize beyond flood management or similar geospatial domains.
- Performance is highly sensitive to retrieval accuracy and prompt design, which are not standardized.
- The fixed schema and data format limit applicability to dynamic, real-world environments.

## Confidence
- **General domain transferability**: Medium
- **Benchmark ability to measure complex geospatial query progress**: High
- **Stability of retrieval-dependent results across LLM configurations**: Low

## Next Checks
1. Evaluate FLOODSQL-Bench with non-flood geospatial datasets to assess domain transfer.
2. Systematically vary retrieval strategies and prompt templates to measure their impact on LLM performance.
3. Conduct ablation studies on the role of schema simplification and metadata-driven methods in handling complex multi-table joins.