---
ver: rpa2
title: 'Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech
  in Low-Resource Indian Languages'
arxiv_id: '2505.20693'
source_url: https://arxiv.org/abs/2505.20693
tags:
- speech
- languages
- indian
- in-f5
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Phir Hera Fairy demonstrates that large-scale English TTS models
  can be effectively adapted to Indian languages through fine-tuning, achieving human-level
  speech synthesis and unlocking emergent abilities like polyglot fluency, voice cloning,
  and code-mixing. Direct fine-tuning on Indian languages without English data yields
  the best results, with MUSHRA scores reaching 73.4 overall and near-human naturalness
  for seen speakers.
---

# Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages

## Quick Facts
- **arXiv ID**: 2505.20693
- **Source URL**: https://arxiv.org/abs/2505.20693
- **Reference count**: 0
- **Primary result**: Large-scale English TTS models can be effectively adapted to Indian languages through fine-tuning, achieving human-level speech synthesis and emergent abilities like polyglot fluency, voice cloning, and code-mixing.

## Executive Summary
Phir Hera Fairy demonstrates that large-scale English TTS models can be effectively adapted to Indian languages through fine-tuning, achieving human-level speech synthesis and unlocking emergent abilities like polyglot fluency, voice cloning, and code-mixing. Direct fine-tuning on Indian languages without English data yields the best results, with MUSHRA scores reaching 73.4 overall and near-human naturalness for seen speakers. The approach also enables zero-resource TTS for languages like Bhojpuri and Tulu through transfer learning and synthetic data generation, achieving MUSHRA scores of 82.0 and 93.6, respectively. This method provides a scalable, compute-efficient solution for building inclusive, high-quality TTS systems for low-resource languages.

## Method Summary
The method adapts an English F5-TTS model (pretrained on ~100K hours) to 11 Indian languages by expanding the character vocabulary with 685 Indian script tokens and initializing new embeddings from the English checkpoint's latent space. The model is fine-tuned directly on 1,417 hours of Indian speech data using the EN→IN strategy (no English data), achieving optimal performance. Zero-resource TTS is enabled through cross-script transfer and synthetic data generation with human-in-the-loop validation. Evaluation uses MUSHRA variants for naturalness, speaker similarity, and intelligibility, along with objective metrics like WER and S-SIM.

## Key Results
- Direct fine-tuning on Indian languages (EN→IN) achieves highest MUSHRA score of 73.4, surpassing mixed-language approaches
- Near-human naturalness (MUSHRA 76.9) and speaker similarity (76.4) for seen speakers
- Zero-resource TTS for Bhojpuri and Tulu achieves MUSHRA scores of 82.0 and 93.6 respectively
- Polyglot synthesis and code-mixing capabilities emerge from fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Direct fine-tuning on target languages without English data yields better adaptation than mixed-language fine-tuning.
- **Mechanism**: Large-scale English pretraining provides strong acoustic and prosodic priors that transfer to Indian languages. Fine-tuning only on Indian data allows the model to fully optimize for the target domain rather than dividing capacity across languages.
- **Core assumption**: English checkpoint has learned language-agnostic speech synthesis capabilities that transfer across language families.
- **Evidence anchors**: Abstract states EN→IN strategy achieves highest MUSHRA score of 73.4; Section 4.1 confirms this counterintuitive finding.

### Mechanism 2
- **Claim**: Character-level modeling with vocabulary extension enables adaptation without phoneme-based G2P systems.
- **Mechanism**: Indian languages exhibit high phonetic orthography where written characters map closely to pronunciation. By extending vocabulary with 685 unique character tokens and initializing embeddings from English checkpoint, the model learns script-to-speech mappings directly.
- **Core assumption**: Orthography-to-phonology mapping is sufficiently regular in Indian scripts for character-level input.
- **Evidence anchors**: Section 2.1 explains Indian languages' high phonetic orthography makes character-based modeling natural; embedding initialization described in same section.

### Mechanism 3
- **Claim**: Zero-resource TTS is achievable via cross-script transfer and synthetic data generation with human-in-the-loop validation.
- **Mechanism**: IN-F5 generalizes to unseen languages by leveraging shared script representations. Synthetic speech is generated, validated by native speakers, then used for self-training fine-tuning.
- **Core assumption**: Script familiarity implies phonological similarity sufficient for intelligible synthesis; synthetic data quality adequate for self-training.
- **Evidence anchors**: Section 2.4 explains script overlap enables transfer; Section 4.4 shows Bhojpuri achieves MUSHRA 82.0 surpassing human recordings.

## Foundational Learning

- **Concept: Flow Matching in TTS (F5-TTS architecture)**
  - **Why needed here**: Base model uses flow matching rather than diffusion; understanding this informs fine-tuning stability and convergence expectations.
  - **Quick check question**: Can you explain why flow matching might converge faster than diffusion-based approaches for speech synthesis?

- **Concept: MUSHRA Evaluation Methodology**
  - **Why needed here**: All quality claims rest on MUSHRA variants; understanding metric's subjectivity and confidence intervals critical for interpreting results.
  - **Quick check question**: What does a MUSHRA score of 73.4 with 95% CI of ±2.0 actually tell you about perceptual quality?

- **Concept: Cross-Lingual Transfer in Speech Models**
  - **Why needed here**: Polyglot and zero-resource capabilities depend on transfer across language families; this isn't automatic.
  - **Quick check question**: Why would a model trained on Hindi be able to synthesize Tamil speech with a Hindi speaker's voice?

## Architecture Onboarding

- **Component map**: English F5-TTS (100K hours) → Vocabulary extension (685 tokens) → Direct fine-tuning on IN11 (1,417 hours) → IN-F5 polyglot model
- **Critical path**: Vocabulary extension → embedding initialization from English checkpoint → direct fine-tuning on IN11 only (EN→IN strategy) → 150K steps with AdamW (lr=5e-5) → evaluation
- **Design tradeoffs**: EN→IN vs EN→EN+IN: Target-only gives higher quality (73.4 vs 66.2 MUSHRA) but loses English capability; 10h/language is near-optimal (1.3% avg performance drop vs 100h)
- **Failure signatures**: Training from scratch (Φ→IN) collapses to MUSHRA 43.2—English pretraining essential; 1h/language: WER jumps to 59.4% (vs 31.3% at 10h)
- **First 3 experiments**: 1) Reproduce EN→IN vs EN→EN+IN: Fine-tune on single language with both strategies; verify MUSHRA gap. 2) Data scaling ablation: Train with 1h/10h/100h for one language; plot WER and S-SIM curves. 3) Zero-resource test: Attempt TTS for unseen script (e.g., Urdu Nastaliq) to probe limits of script-based transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does excluding English data during fine-tuning (EN→IN) yield higher naturalness than mixing English and Indian data (EN→EN+IN), despite concerns about catastrophic forgetting?
- Basis in paper: Authors note in Section 4.1 that superior performance of direct fine-tuning over mixed fine-tuning is a "surprising trend" and "counter-intuitive," leaving mechanism unexplained.
- Why unresolved: Paper empirically demonstrates English data harms Indian language adaptation but doesn't investigate whether this is due to data imbalance, gradient interference, or capacity limitations.
- What evidence would resolve it: Ablation studies analyzing gradient conflicts or representation degradation during mixed-language training.

### Open Question 2
- Question: Can the proposed zero-resource TTS framework succeed for languages that do not share a script with any language in the pre-training data?
- Basis in paper: Section 2.4 restricts zero-resource experiments to Tulu and Bhojpuri specifically because they share scripts with Kannada and Hindi, implying method may depend on this overlap.
- Why unresolved: Unclear if "zero-resource" capability relies on transfer learning from acoustically similar languages or simply on model's prior knowledge of grapheme-to-phoneme mapping for specific script.
- What evidence would resolve it: Applying human-in-the-loop synthetic pipeline to low-resource language with unique script not present in F5-TTS vocabulary.

### Open Question 3
- Question: How robust are the subjective quality scores for zero-resource languages given reliance on single expert evaluator?
- Basis in paper: Authors explicitly state in Section 4.4: "We humbly request readers to interpret MUSHRA scores with caution, as each language is evaluated by a single language expert..."
- Why unresolved: Sample size of one rater prevents calculation of inter-rater agreement or statistical significance, making it difficult to distinguish genuine quality from individual preference.
- What evidence would resolve it: Re-evaluating Bhojpuri and Tulu synthesis samples with larger, statistically significant pool of native speakers (e.g., n > 10).

## Limitations

- Data access and reproducibility: IN11 dataset composed of multiple sources with unclear licensing terms and potential access restrictions, limiting reproducibility outside well-resourced institutions
- MUSHRA evaluation subjectivity: Quality claims rest on inherently subjective MUSHRA variants vulnerable to cultural/linguistic bias; no-mentioned-reference protocol for S-SIM is methodologically problematic
- Zero-resource generalization limits: Method demonstrated only for languages with script overlap; assumption that script familiarity implies phonological similarity not universally valid

## Confidence

**High confidence** (empirical evidence strong, methodology sound):
- Direct fine-tuning on Indian languages only outperforms mixed-language fine-tuning (MUSHRA 73.4 vs 66.2)
- Data scaling: 10h/language achieves near-optimal quality (only 1.3% degradation vs 100h)
- IN-F5 matches or exceeds human quality for seen speakers in naturalness and speaker similarity

**Medium confidence** (results demonstrate effect but mechanism unclear):
- Polyglot synthesis capability (cross-language voice cloning)
- Code-mixing synthesis quality (55.5 MUSHRA-I for same-language pairs)
- Zero-resource success for Tulu and Bhojpuri

**Low confidence** (novel claims with limited validation):
- Generalizability to languages without script overlap
- Claims about "emergent" abilities without ablation studies
- Real-world deployment readiness (no latency, compute, or deployment metrics)

## Next Checks

1. **Script overlap ablation**: Test zero-resource TTS on languages with different scripts from any training language (e.g., Urdu in Nastaliq script, or languages using scripts not in the 11). This validates whether script similarity is truly necessary for cross-lingual transfer.

2. **Mixed-language fine-tuning ablation**: Systematically vary proportion of English data in fine-tuning (0%, 10%, 25%, 50%, 100%) for one language. This tests counterintuitive finding that English data degrades performance and identifies any sweet spots for bilingual capability.

3. **Orthography regularity validation**: For each of the 11 languages, measure correlation between written character sequences and actual phoneme sequences using aligned text-speech data. This empirically validates high phonetic orthography assumption that enables character-level modeling without G2P systems.