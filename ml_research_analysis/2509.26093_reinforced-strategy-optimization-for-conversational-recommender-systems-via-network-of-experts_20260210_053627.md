---
ver: rpa2
title: Reinforced Strategy Optimization for Conversational Recommender Systems via
  Network-of-Experts
arxiv_id: '2509.26093'
source_url: https://arxiv.org/abs/2509.26093
tags:
- strategy
- user
- conversational
- macro-level
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing conversational
  strategies in conversational recommender systems (CRSs) to improve user satisfaction
  and recommendation success. Existing LLM-based CRS methods rely on predefined prompts
  and lack explicit strategy optimization, leading to suboptimal interactions.
---

# Reinforced Strategy Optimization for Conversational Recommender Systems via Network-of-Experts

## Quick Facts
- arXiv ID: 2509.26093
- Source URL: https://arxiv.org/abs/2509.26093
- Reference count: 40
- Primary result: RSO achieves state-of-the-art performance on CRS tasks, improving conversation success rate and user satisfaction metrics over strong baselines.

## Executive Summary
This paper addresses the challenge of optimizing conversational strategies in conversational recommender systems (CRSs) to improve user satisfaction and recommendation success. Existing LLM-based CRS methods rely on predefined prompts and lack explicit strategy optimization, leading to suboptimal interactions. To tackle this, the authors propose a Reinforced Strategy Optimization (RSO) method that decomposes strategy generation into macro-level planning and micro-level adaptation through a network-of-experts architecture. The macro-level Planner selects conversational strategies (e.g., recommend, explain, encourage), while micro-level experts (Preference Reasoner, Fact Retriever, and Actor) adapt these strategies to user context and factual grounding. Strategy learning is formulated as a reinforcement learning problem guided by an LLM-based reward model, enabling automatic strategy exploration. Extensive experiments on INSPIRED and REDIAL datasets show that RSO significantly improves interaction performance, achieving state-of-the-art results in metrics such as watching intention (WI), persuasiveness (PRS), credibility (Cred), and conversation success rate (Conv-SR). For example, on INSPIRED, RSO achieves a Conv-SR of 0.98, outperforming strong baselines like PCCRS. The method demonstrates the effectiveness of explicit hierarchical strategy optimization in CRS.

## Method Summary
The method introduces a Network-of-Experts architecture for conversational recommender systems, consisting of a macro-level Planner and micro-level experts (Preference Reasoner, Fact Retriever, Actor). The Planner, a lightweight RoBERTa model, selects from 13 conversational strategies (e.g., recommend, explain, encourage). The micro-level experts perform preference reasoning, factual grounding, and response generation using LLM prompting without fine-tuning. Strategy learning is formulated as a reinforcement learning problem where the Planner is trained via entropy-regularized policy gradients using an LLM-based reward model. The approach uses two-stage training: supervised fine-tuning (SFT) warm-up followed by RL optimization. Experiments on INSPIRED and REDIAL movie recommendation datasets demonstrate significant improvements in conversation success rate, watching intention, persuasiveness, and credibility metrics compared to state-of-the-art baselines.

## Key Results
- RSO achieves Conv-SR of 0.98 on INSPIRED dataset, outperforming PCCRS and other strong baselines
- Significant improvements in credibility (Cred) from 2.73→3.83 with Fact Retriever on INSPIRED
- Ablation studies show Preference Reasoner and Fact Retriever each contribute meaningfully to performance gains
- RL phase enables discovery of context-appropriate strategies beyond training distribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical decomposition makes strategy optimization tractable by separating when-to-act decisions from how-to-act execution.
- **Mechanism:** The policy π_θ(a_t|s_t) factors into π_θ1(h_t|s_t) for macro-level strategy selection and π_θ2(a_t|h_t, s_t) for micro-level response generation conditioned on that strategy. This reduces a large entangled decision space into two smaller, more learnable sub-problems.
- **Core assumption:** Optimal conversational behavior can be meaningfully decomposed into a discrete strategy choice followed by strategy-conditioned response generation, and these can be optimized separately.
- **Evidence anchors:**
  - [abstract] "decomposes the process of generating strategy-driven response decisions into the macro-level strategy planning and micro-level strategy adaptation"
  - [Section III.A] "This hierarchical decomposition disentangles the optimization of different sub-tasks involved in CRS response generation, enabling more tractable learning at each level."
  - [corpus] Weak direct evidence—neighbor papers emphasize contextual adaptation but don't specifically validate hierarchical decomposition as a mechanism.
- **Break condition:** If macro-strategies are highly interdependent with response content (strategy meaning shifts drastically by context), decomposition may lose critical joint information.

### Mechanism 2
- **Claim:** Explicit preference reasoning and fact grounding provide stable contextual signals that enable a lightweight Planner to focus purely on strategy selection.
- **Mechanism:** The Preference Reasoner extracts structured preference representations C¹_t from dialogue history via LLM prompting. The Fact Retriever grounds responses by retrieving entity knowledge from an external KG (Wikipedia subset). These feed into the Actor, freeing the Planner to focus on strategy selection without handling complex context processing.
- **Core assumption:** Providing structured preference and factual context to downstream components allows a simpler model to effectively learn strategy selection.
- **Evidence anchors:**
  - [Section III.B] Describes how Preference Reasoner and Fact Retriever provide structured inputs to the Actor and Planner
  - [Section IV.C] Ablation studies show performance drops when these components are removed
  - [corpus] Unknown whether the components truly provide "stable" signals or just add complexity
- **Break condition:** If preference reasoning or fact retrieval become unreliable or introduce noise, the lightweight Planner may fail to compensate.

## Foundational Learning
The paper's approach builds on foundational work in reinforcement learning for dialogue systems, hierarchical decision-making, and knowledge-grounded conversation. It extends prior RL methods by incorporating explicit strategy optimization rather than treating dialogue as a monolithic response generation task. The use of entropy regularization for exploration aligns with established RL best practices. The decomposition approach draws from hierarchical reinforcement learning literature, though applied to conversational strategy rather than traditional control tasks.

## Architecture Onboarding
The Network-of-Experts architecture separates concerns across three main components: (1) a macro-level Planner that selects from 13 discrete conversational strategies using a lightweight RoBERTa model, (2) micro-level experts that handle specific subtasks without fine-tuning—Preference Reasoner for extracting user preferences, Fact Retriever for knowledge grounding, and Actor for response generation—and (3) an LLM-based reward model that provides feedback for reinforcement learning. This modular design allows each component to specialize in its task while maintaining end-to-end optimization through the RL framework.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the provided text
- Unknown whether the authors discuss limitations of their approach or directions for future work
- The evaluation focuses on movie recommendation domains; generalization to other domains remains an open question
- Long-term user engagement and system adaptation over multiple conversations is not addressed

## Limitations
- The approach requires extensive training data with strategy annotations, limiting applicability to domains where such data is unavailable
- The 13 predefined strategies may not cover all conversational scenarios, potentially constraining system flexibility
- The Fact Retriever depends on a knowledge graph covering only Wikipedia entities, which may miss domain-specific or emerging information
- The LLM-based reward model introduces computational overhead and potential bias from the underlying LLM
- The system's performance in multi-turn conversations beyond the scope of the evaluated datasets is unknown
- The method's scalability to more complex recommendation domains (e.g., diverse product categories) has not been demonstrated

## Confidence
High confidence in the reported results given the clear methodology, ablation studies, and comparison with strong baselines. The hierarchical decomposition approach is well-justified and the experimental validation on two datasets is thorough. However, some mechanisms rely on assumptions about strategy decomposition that would benefit from additional empirical validation.

## Next Checks
- Verify the specific conversational strategies used and their coverage across different recommendation scenarios
- Examine the knowledge graph construction and its coverage limitations
- Investigate the computational requirements of the LLM-based reward model
- Assess the method's performance on longer conversation sequences
- Check for any additional ablation studies on strategy selection versus response generation
- Verify the specific implementation details of the entropy regularization in the RL phase