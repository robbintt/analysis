---
ver: rpa2
title: Making Foundation Models Probabilistic via Singular Value Ensembles
arxiv_id: '2601.22068'
source_url: https://arxiv.org/abs/2601.22068
tags:
- singular
- ensemble
- learning
- foundation
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Singular Value Ensemble (SVE), a parameter-efficient
  method for uncertainty quantification in foundation models. SVE builds on the observation
  that pretrained model weight matrices contain semantically meaningful singular vectors,
  which form a knowledge basis.
---

# Making Foundation Models Probabilistic via Singular Value Ensembles

## Quick Facts
- arXiv ID: 2601.22068
- Source URL: https://arxiv.org/abs/2601.22068
- Authors: Mehmet Ozgur Turkoglu; Dominik J. Mühlematter; Alexander Becker; Konrad Schindler; Helge Aasen
- Reference count: 30
- Primary result: SVE achieves calibration comparable to deep ensembles with 10-700× fewer parameters

## Executive Summary
This paper introduces Singular Value Ensemble (SVE), a parameter-efficient method for uncertainty quantification in foundation models. SVE builds on the observation that pretrained model weight matrices contain semantically meaningful singular vectors, which form a knowledge basis. Instead of training independent models like traditional deep ensembles, SVE shares these singular vectors across ensemble members while learning only per-member singular values to modulate their contribution. This design achieves diversity through stochastic initialization and mini-batch sampling during joint training.

SVE demonstrates strong performance across NLP and vision tasks, achieving calibration comparable to or better than deep ensembles while using 10-700× fewer parameters. On vision benchmarks, SVE achieves the highest accuracy and competitive calibration on Oxford Pets, Flowers102, and CIFAR-100. For NLP, SVE achieves dramatically lower Expected Calibration Error than all baselines on ARC-Easy while maintaining competitive accuracy. The method also excels at out-of-distribution detection and dataset shift robustness. Computational analysis shows SVE reduces parameter and memory overhead by 99% compared to deep ensembles while maintaining comparable inference speed.

## Method Summary
SVE applies Singular Value Decomposition to pretrained weight matrices, freezing the singular vectors as a shared knowledge basis while training only per-member singular values. For each linear layer W, SVD yields W=UΣV^T, where U and V are frozen and M copies of Σ are trained with multiplicative initialization noise. Members process data jointly with shared cross-entropy loss, and predictions are averaged for inference. The method targets linear layers in vision transformers (QKV projections, MLP) and NLP models (attention heads, feed-forward layers), requiring only M×d parameters where d is the number of singular values.

## Key Results
- SVE achieves ECE <2.0% on Flowers102, CIFAR-100, and Oxford Pets, matching or outperforming deep ensembles
- Parameter overhead is 0.1-10% of backbone size versus 100% for deep ensembles (10-700× reduction)
- On ARC-Easy, SVE achieves ECE of 2.4% versus 12.2% for deep ensembles and 24.3% for single models
- OOD detection performance: AUROC 81.6% on CIFAR-100→CIFAR-10, comparable to deep ensembles (79.2%)
- Memory overhead is 1-3% versus 100% for deep ensembles, enabling deployment on resource-constrained devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Singular vectors of pretrained weight matrices encode semantically meaningful "knowledge directions" that can be preserved across ensemble members while modulating only their contribution strengths.
- Mechanism: SVD decomposes weight matrices W = UΣV^T. The method freezes U and V (singular vectors) as a shared knowledge basis and trains only per-member singular values Σ^(m), which rescale how strongly each direction contributes to outputs.
- Core assumption: Singular vectors from pretrained models constitute meaningful orthogonal knowledge subspaces, not merely mathematical artifacts.
- Evidence anchors:
  - [abstract]: "the singular vectors of the weight matrices constitute meaningful subspaces of the model's knowledge"
  - [section 1]: "the knowledge encoded in foundation models is organized along linear subspaces: the singular vectors of weight matrices correspond to semantically meaningful directions"
  - [corpus]: Weak direct support—neighbor papers focus on ensemble-based uncertainty broadly, not SVD-specific approaches
- Break condition: With randomly initialized weights (no pretrained knowledge basis), SVE performs worst among methods because there is no meaningful structure to preserve (see Figure 2).

### Mechanism 2
- Claim: Small symmetry-breaking perturbations to singular values, combined with stochastic mini-batch training, induce functional diversity among ensemble members without learning new directional parameters.
- Mechanism: Initialize each member's singular values as Σ^(m) = Σ ⊙ (1 + ε^(m)) with ε^(m) ~ N(0, σ²_init). Different perturbations plus different batch sampling cause members to converge to distinct rescalings of shared knowledge directions.
- Core assumption: The diversity space spanned by singular value combinations is sufficiently expressive to capture meaningful epistemic uncertainty.
- Evidence anchors:
  - [abstract]: "Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations"
  - [section 4]: Multiplicative initialization preserves relative ordering and guarantees positivity; σ_init = 0.01 in most experiments
  - [corpus]: No direct corpus evidence for this specific initialization scheme
- Break condition: If singular values collapse to similar values across members or diverge excessively, ensemble diversity degrades.

### Mechanism 3
- Claim: Disagreement among SVE members captures epistemic uncertainty, with members agreeing near training data and diverging in data-sparse regions.
- Mechanism: Prediction variance across M members serves as uncertainty proxy. Epistemic uncertainty is estimated by averaging predictions: p(y|x) ≈ (1/M) Σ p_θ^m(y|x).
- Core assumption: Per-member singular value diversity correlates with genuine epistemic uncertainty rather than random noise.
- Evidence anchors:
  - [section 1]: "near observed training samples, ensemble members agree; far from the data, they diverge. This disagreement serves as a proxy for epistemic uncertainty"
  - [Table 4]: OOD detection achieves AUROC 81.6% on CIFAR-100→CIFAR-10, comparable to deep ensembles (79.2%)
  - [corpus]: "Prediction uncertainty-aware planning using deep ensembles" supports ensemble-based uncertainty principles
- Break condition: If members make correlated errors or diversity doesn't reflect data sparsity, calibration and OOD detection will fail.

## Foundational Learning

- **Singular Value Decomposition (SVD)**
  - Why needed here: The entire method is built on decomposing W = UΣV^T. Without this, you cannot understand how frozen singular vectors become "knowledge directions" and trainable singular values become scaling knobs.
  - Quick check question: For a 768×3072 weight matrix, how many singular values would each ensemble member learn? (Answer: min(768, 3072) = 768)

- **Epistemic vs Aleatoric Uncertainty**
  - Why needed here: SVE targets epistemic uncertainty (reducible model ignorance), not aleatoric uncertainty (irreducible data noise). Confusing these leads to misinterpreting what ensembles measure.
  - Quick check question: Would adding more training data reduce epistemic uncertainty, aleatoric uncertainty, or both? (Answer: Only epistemic)

- **Expected Calibration Error (ECE)**
  - Why needed here: The paper's primary evaluation metric. You must understand that ECE measures the gap between predicted confidence and empirical accuracy.
  - Quick check question: If a model predicts 80% confidence on 100 samples and gets 80 correct, what is its calibration? (Answer: Perfect calibration for that bin)

## Architecture Onboarding

- **Component map:**
  - Pretrained backbone (frozen) -> SVD decomposition (one-time init) -> Per-member singular values -> Per-member classification heads -> Weight reconstruction at forward pass

- **Critical path:**
  1. Load pretrained model, identify target layers (Q/K/V/O projections, MLP)
  2. Compute SVD once per layer; freeze U, V
  3. Initialize M copies of Σ with multiplicative noise (σ_init ≈ 0.01)
  4. Train jointly with shared cross-entropy: L = (1/M) Σ L_CE(f^(m)(x), y)
  5. Inference: Run M forward passes, aggregate via averaging

- **Design tradeoffs:**
  - More members (M): Better calibration, linear FLOPs increase
  - Larger σ_init: More initial diversity, risk of destabilizing pretrained structure
  - Layer selection: More layers = more expressivity, more parameters
  - Sequential processing: Reduces memory but increases latency

- **Failure signatures:**
  - ECE worse than single model: Members converging to similar Σ values; check gradient flow
  - Accuracy collapse: σ_init too large (>0.05) or learning rate too high
  - No member diversity: Verify independent batch sampling; check initialization
  - Poor OOD detection: Ensemble variance not reflecting epistemic uncertainty

- **First 3 experiments:**
  1. Replicate Flowers102 with DINOv2-S/14 (M=4): Validate pipeline; target Acc ~95%, ECE ~1.0%
  2. Ablate ensemble size (M=1,2,4,8,16) on CIFAR-100: Understand scaling behavior
  3. Compare pretrained vs randomly initialized backbone: Confirm knowledge basis hypothesis; expect SVE to fail without pretrained vectors

## Open Questions the Paper Calls Out
- Can a hybrid approach combining SV-Ensemble with LoRA-Ensemble simultaneously leverage the parameter efficiency and calibration of the former and the expressivity of the latter?
- Can an SV-Ensemble be distilled into a single deterministic network that retains calibrated uncertainty estimates while eliminating the multi-pass inference cost?
- How can Singular Value Ensembles be adapted to operate natively within 4-bit or 8-bit quantized networks without requiring costly de-quantization?

## Limitations
- The core hypothesis about singular vectors encoding semantically meaningful knowledge lacks direct empirical validation beyond functional success
- Integration with 4/8-bit quantized networks is not straightforward because de-quantization is required
- Inference still requires multiple forward passes, limiting real-time applicability despite low parameter overhead

## Confidence
- **High confidence**: SVE's parameter efficiency and computational benefits; comparative calibration performance on standard benchmarks; failure modes with random initialization
- **Medium confidence**: The knowledge basis hypothesis for singular vectors; scalability to very large models (beyond 22M parameters); generalizability across diverse domains
- **Low confidence**: The specific mechanism by which singular value perturbations create meaningful epistemic uncertainty; whether SVE captures all forms of epistemic uncertainty or only those accessible through singular vector modulation

## Next Checks
1. **Knowledge Transfer Test**: Train SVE on a source domain (e.g., Flowers102), then evaluate calibration on a target domain (e.g., Oxford Pets) without fine-tuning. Compare against deep ensembles to test whether the singular vector knowledge basis generalizes.
2. **Layer Importance Analysis**: Systematically ablate SVD application from different layer types (QKV vs MLP) to determine which layers contribute most to calibration improvement. This would validate the claim about "knowledge directions" being most important in specific network components.
3. **Diversity Quantification**: Measure actual functional diversity among ensemble members using techniques like centered kernel alignment (CKA) on hidden representations or by analyzing agreement patterns across the input space. This would directly test whether singular value diversity correlates with epistemic uncertainty capture.