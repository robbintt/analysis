---
ver: rpa2
title: Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters
arxiv_id: '2508.07952'
source_url: https://arxiv.org/abs/2508.07952
tags:
- k-means
- clustering
- data
- feature
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHARK is a parameter-free feature-weighted k-means algorithm that
  uses Shapley values to quantify feature relevance. It decomposes the k-means objective
  into per-feature Shapley values, enabling efficient computation and principled weighting.
---

# Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters

## Quick Facts
- arXiv ID: 2508.07952
- Source URL: https://arxiv.org/abs/2508.07952
- Reference count: 8
- Primary result: SHARK is a parameter-free feature-weighted k-means algorithm that uses Shapley values to quantify feature relevance.

## Executive Summary
SHARK is a novel feature-weighted k-means algorithm that automatically determines feature importance using Shapley values without requiring additional hyperparameters. The method decomposes the k-means objective into per-feature Shapley values, enabling efficient computation and principled weighting. SHARK minimizes the harmonic mean of these values, down-weighting irrelevant features while preserving informative ones. Experiments on synthetic and real-world datasets show SHARK consistently matches or outperforms existing methods, achieving superior robustness and accuracy—particularly in high-dimensional and noisy settings.

## Method Summary
SHARK implements a parameter-free feature-weighted k-means clustering algorithm that leverages Shapley values to automatically determine feature relevance. The method initializes equal weights for all features, then iteratively updates both cluster assignments and feature weights. Feature weights are computed by inverting per-feature Shapley values (which measure variance contribution to the k-means objective) and normalizing. This approach eliminates the need for additional hyperparameters beyond the standard k-means parameter k, making it both theoretically principled and practically accessible.

## Key Results
- SHARK consistently matches or outperforms existing feature-weighted k-means methods across all tested datasets
- The method demonstrates superior robustness to noise and irrelevant features, particularly in high-dimensional settings
- SHARK achieves better accuracy than standard k-means on both synthetic and real-world datasets without requiring additional hyperparameter tuning

## Why This Works (Mechanism)
SHARK works by decomposing the k-means objective into per-feature Shapley values that quantify each feature's contribution to cluster formation. By minimizing the harmonic mean of these values, the algorithm naturally down-weights features that contribute little to the clustering objective (such as noise features) while preserving the influence of informative features. The analytic computation of Shapley values from variance decomposition makes this approach computationally efficient compared to methods requiring model retraining.

## Foundational Learning
- **Shapley values in clustering**: Measure feature importance by quantifying contribution to objective function - needed for principled feature weighting, check by verifying values align with intuitive feature relevance
- **Harmonic mean optimization**: Favors balanced feature contributions while penalizing dominance by single features - needed to avoid over-reliance on few features, check by examining weight distribution
- **Variance decomposition**: Breaks down total variance into per-feature components - needed for efficient Shapley value computation, check by validating variance sums to total

## Architecture Onboarding
**Component map**: Data -> Normalization -> Weight Initialization -> Iterative Update (Assignment -> Centroid Update -> Weight Update) -> Convergence

**Critical path**: The weight update step ω_v = φ_v^(-1) / Σ_u φ_u^(-1) is critical as it directly determines feature importance and drives convergence behavior

**Design tradeoffs**: 
- Advantage: Parameter-free operation beyond k-means parameter k
- Limitation: Assumes feature independence for Shapley attribution validity
- Risk: Potential weight instability with correlated features

**Failure signatures**: Degenerate weights (φ_v → 0 causing ω_v → ∞), sensitivity to initialization, poor performance on highly correlated features

**First experiments**: 1) Validate weight update computation on simple 2D dataset, 2) Test convergence behavior with varying k, 3) Compare ARI on synthetic data with known ground truth

## Open Questions the Paper Calls Out
- Can SHARK be modified to detect when feature weighting is unnecessary to avoid performance degradation in high-dimensional, noise-free settings?
- How does SHARK's performance relative to competitors change when controlling for the data normalization strategy?
- How does SHARK handle datasets with highly correlated features?

## Limitations
- Validation on large-scale, high-dimensional real-world data remains unexplored
- Computational complexity and runtime comparisons with existing methods are not discussed
- Sensitivity to the number of clusters k is not explicitly addressed

## Confidence
- **High confidence**: Theoretical formulation using Shapley values is sound and novel
- **Medium confidence**: Claims of superior robustness supported by synthetic experiments but need real-world validation
- **Low confidence**: Lack of computational complexity analysis limits practical applicability assessment

## Next Checks
1. Test SHARK on large-scale, high-dimensional datasets (e.g., ImageNet subsets or genomics data) to evaluate scalability and robustness
2. Compare SHARK's runtime and memory usage with competing methods (e.g., FWSA, LW-k-means) on datasets of varying sizes
3. Investigate SHARK's sensitivity to the choice of k and its performance in scenarios where the true number of clusters is unknown