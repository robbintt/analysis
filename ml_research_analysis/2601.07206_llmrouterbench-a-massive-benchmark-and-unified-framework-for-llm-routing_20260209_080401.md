---
ver: rpa2
title: 'LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing'
arxiv_id: '2601.07206'
source_url: https://arxiv.org/abs/2601.07206
tags:
- routing
- arxiv
- cost
- performance
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMRouterBench provides a large-scale benchmark and unified framework
  for evaluating large language model (LLM) routing. It covers 400K+ instances across
  21 datasets and 33 models, and supports both performance-oriented and performance-cost
  tradeoff settings.
---

# LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing

## Quick Facts
- arXiv ID: 2601.07206
- Source URL: https://arxiv.org/abs/2601.07206
- Authors: Hao Li; Yiqun Zhang; Zhaoyan Guo; Chenxu Wang; Shengji Tang; Qiaosheng Zhang; Yang Chen; Biqing Qi; Peng Ye; Lei Bai; Zhen Wang; Shuyue Hu
- Reference count: 40
- Most recent routing methods fail to outperform simple single-model baselines despite architectural diversity.

## Executive Summary
LLMRouterBench provides a comprehensive benchmark and unified framework for evaluating large language model (LLM) routing across 400K+ instances spanning 21 datasets and 33 models. The benchmark reveals that while strong model complementarity exists as the foundation for routing, most recent routing methods deliver similar performance and fail to significantly outperform a simple single-model baseline. A substantial gap remains between current routers and an Oracle baseline, primarily driven by model-recall failures on hard queries where only 1-3 models answer correctly. The framework supports both performance-oriented and performance-cost tradeoff evaluations, enabling tri-objective optimization across performance, cost, and latency.

## Method Summary
LLMRouterBench provides a unified evaluation framework for LLM routing with standardized components: a Collector that gathers model outputs across datasets, an Evaluator that computes task-specific scores, and an Adaptor that formats data for different routing algorithms. The benchmark covers 21 diverse datasets across mathematics, code, logic, knowledge, affective, instruction following, and tool use domains. It evaluates 10 routing baselines using 70/30 train/test splits with 5 random seeds, comparing against Random Router, Best Single, and Oracle baselines. The framework supports two model pools: lightweight ~7B models for performance-oriented routing and flagship models including GPT-5 and Claude-v4 for performance-cost tradeoff evaluation.

## Key Results
- Most routing methods (GraphRouter, EmbedLLM, Avengers, etc.) deliver similar performance despite architectural differences, with Avengers and Avengers-Pro achieving near-zero ParetoDist in cost-performance tradeoff.
- A significant performance gap exists between current routers and Oracle, primarily due to model-recall failures on hard queries where only 1-3 models answer correctly (~12% of test set).
- Embedding quality has minimal impact on routing performance; replacing gte-qwen2-7B-instruct with much weaker embedders (nli-bert-base, all-MiniLM-L6-v2) yields negligible differences.
- Carefully curated small model pools outperform large random ensembles, with diminishing returns from adding more models.

## Why This Works (Mechanism)

### Mechanism 1: Model Complementarity Creates Routing Opportunity
- Different models specialize in different capability regions (e.g., Intern-S1-mini excels at mathematics, Qwen-Coder at code, NVIDIA-Nemo at logical reasoning). When routers correctly map query characteristics to model specializations, they capture this complementarity. This assumes query features reliably predict which model will succeed and this prediction is learnable from training data. Break condition: If models converge to similar capability profiles or query features are insufficient to discriminate optimal model choice, routing gains collapse to near-zero.

### Mechanism 2: Model-Recall Failure Constrains Performance Ceiling
- On queries where only 1-3 models answer correctly (~12% of test set), routers achieve 23-25% accuracy vs. near-ceiling on easier queries. This suggests routers learn coarse domain boundaries but fail at fine-grained specialist identification. Break condition: If model-recall failures cannot be reduced through uncertainty estimation or difficulty-aware routing, the practical ceiling for routing remains far below Oracle.

### Mechanism 3: Embedding Representations Are Not the Primary Bottleneck
- Routing performance is surprisingly insensitive to embedding model quality. Replacing gte-qwen2-7B-instruct with substantially weaker embedders yields negligible performance difference. This suggests current routers underutilize available semantic signal. Break condition: If routing mechanisms improve to fully exploit semantic representations, embedding quality may become a bottleneck again—but current evidence suggests this is not the limiting factor.

## Foundational Learning

- **Concept: Pareto Optimality in Multi-Objective Optimization**
  - Why needed here: The performance-cost tradeoff evaluation uses Pareto frontiers to identify configurations that are simultaneously better on at least one objective without sacrificing another. Avengers-Pro's near-zero ParetoDist indicates it operates on this frontier.
  - Quick check question: Given two configurations A (90% accuracy, $10 cost) and B (85% accuracy, $5 cost), does either Pareto-dominate the other?

- **Concept: Oracle Upper Bound and Hindsight Analysis**
  - Why needed here: The Oracle baseline (selecting the best model per-query with perfect foresight) establishes the theoretical maximum routing performance. Gap@O quantifies how far current methods fall short.
  - Quick check question: Why is Oracle performance always ≥ Best Single performance, and when would they be equal?

- **Concept: Diminishing Returns from Ensemble Expansion**
  - Why needed here: The paper shows adding models yields sublinear gains; curation matters more than scale. This has direct implications for deployment cost vs. routing complexity tradeoffs.
  - Quick check question: If Oracle accuracy plateaus at ~93% with 20 models but reaches ~85% with just 4 well-chosen models, what does this imply about the marginal value of the additional 16 models?

## Architecture Onboarding

- **Component map:** Collector -> Evaluator -> Adaptor
- **Critical path:**
  1. Deploy model pool (lightweight models locally on A800 GPUs via vLLM; flagship via OpenRouter/official APIs).
  2. Run inference across all datasets → Collector generates standardized instance records.
  3. Evaluator scores each instance.
  4. Adaptor formats data for target router; train on 70% split, evaluate on 30%.
  5. Compute metrics (AvgAcc, Gain@B, Gap@O, PerfGain, CostSave, ParetoDist).

- **Design tradeoffs:**
  - Performance-oriented vs. Performance-cost pools: Lightweight ~7B pool for pure accuracy routing; flagship pool with cost variation for tradeoff routing. Do not mix—cost heterogeneity trivializes performance comparisons.
  - Router complexity vs. maintainability: Avengers (clustering-based, no neural training) matches complex methods like GraphRouter and MODEL-SAT. Consider deployment overhead.
  - Embedding investment: Weaker embedders perform comparably; allocate effort elsewhere.

- **Failure signatures:**
  - Router fails to beat Best Single: Likely overfitting to training distribution or insufficient model complementarity in pool.
  - High Gap@O with low routing accuracy on hard queries: Model-recall failure; router cannot identify rare specialists.
  - CostSave marked N/A: Router sacrifices accuracy before achieving cost reduction; threshold tuning needed.

- **First 3 experiments:**
  1. **Baseline sanity check:** Run Random Router, Best Single, and Oracle on your model pool. Confirm complementarity exists (no single model dominates >80% of datasets). If not, curate a more diverse pool.
  2. **Embedding ablation:** Test your router with gte-qwen2-7B-instruct vs. all-MiniLM-L6-v2. If performance differs significantly, your router may be unusually embedding-sensitive—investigate.
  3. **Model-recall diagnosis:** Identify queries where ≤3 models succeed. Measure router accuracy on this subset vs. the full set. If gap is large, explore uncertainty-aware routing or explicit difficulty estimation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can routing architectures be redesigned to minimize "model-recall" failures for queries where only a single candidate model produces a correct response?
- Basis in paper: The authors identify a substantial performance gap relative to the Oracle, explicitly attributing it to "persistent model-recall failures" and labeling improved recall a "promising direction for future work."
- Why unresolved: Current routers frequently fail to identify the lone correct expert in difficult instances, suggesting existing uncertainty estimation or difficulty prediction mechanisms are insufficient.
- What evidence would resolve it: A novel routing method demonstrating statistically significant accuracy gains specifically on the benchmark's "hard" subset (queries with ≤3 correct models) without sacrificing overall AvgAcc.

### Open Question 2
- Question: Can a joint optimization framework for model pool curation and routing outperform random large ensembles by explicitly maximizing complementarity?
- Basis in paper: The paper notes that "larger ensembles exhibit diminishing returns," finding that a carefully curated subset outperforms large random pools, and concludes that "future work should study model pool curation jointly with routing."
- Why unresolved: It is currently unclear how to programmatically select a small set of models that maximizes coverage of domain-specific strengths better than simply aggregating all available models.
- What evidence would resolve it: An algorithm that dynamically selects a small, diverse subset of models (e.g., top-5) that achieves higher AvgAcc and lower cost than the full 33-model pool on LLMRouterBench.

### Open Question 3
- Question: What specific strategies are required to effectively optimize the tri-objective tradeoff between performance, cost, and latency in real-time routing?
- Basis in paper: The authors state that LLMRouterBench enables "latency-aware analysis" and opens a path toward "tri-objective optimization," noting that they "do not notice any methods that explicitly target" this tradeoff.
- Why unresolved: Models with similar accuracy and cost often differ markedly in latency, adding a layer of complexity to the optimization landscape that current bi-objective (performance-cost) routers do not address.
- What evidence would resolve it: A routing method that achieves Pareto-optimality on a normalized 3D surface of accuracy, cost, and latency, shifting the frontier compared to current cost-aware baselines.

## Limitations

- Most routing methods perform similarly despite architectural diversity, but the underlying reasons for this convergence are not fully explained through systematic ablation studies.
- The finding that embedding quality has minimal impact on routing performance is surprising and contradicts conventional wisdom, though this has not been tested across different routing architectures or task domains.
- Persistent model-recall failures on hard queries are clearly demonstrated, but the paper does not provide detailed analysis of what specific features or signals would be needed to improve recall on these edge cases.

## Confidence

- **Low** - The study identifies that most routing methods perform similarly despite architectural diversity, but the underlying reasons for this convergence are not fully explained.
- **Medium** - The finding that embedding quality has minimal impact on routing performance is surprising and contradicts conventional wisdom about representation learning.
- **High** - The persistent model-recall failures on hard queries (accuracy ~23-25% when only 1-3 models succeed) are clearly demonstrated and represent a fundamental limitation.

## Next Checks

1. **Architectural Sensitivity Analysis**: Systematically vary embedding model sizes across all 10 routing methods to determine whether the observed embedding insensitivity is universal or architecture-dependent.

2. **Hard Query Feature Analysis**: Conduct detailed analysis of the 410 queries where ≤3 models succeed, extracting feature distributions and router predictions to identify what discriminative signals are missing from current approaches.

3. **Model Pool Diversity Impact**: Construct controlled experiments with model pools of varying diversity levels (from highly specialized to homogeneous) to quantify how model complementarity affects the performance gap between current routers and Oracle across different routing methods.