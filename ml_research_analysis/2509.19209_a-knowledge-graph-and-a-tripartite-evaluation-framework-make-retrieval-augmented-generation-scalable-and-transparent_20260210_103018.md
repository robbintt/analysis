---
ver: rpa2
title: A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented
  Generation Scalable and Transparent
arxiv_id: '2509.19209'
source_url: https://arxiv.org/abs/2509.19209
tags:
- query
- response
- chatbot
- email
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a RAG chatbot that leverages a knowledge
  graph and vector search to retrieve information from a large-scale dataset of engineering
  emails. The system integrates a novel tripartite evaluation framework, RAG-Eval,
  which assesses user queries, retrieved documents, and generated responses using
  five key metrics: query relevance, factual accuracy, coverage, coherence, and fluency.'
---

# A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent

## Quick Facts
- arXiv ID: 2509.19209
- Source URL: https://arxiv.org/abs/2509.19209
- Reference count: 40
- Primary result: Knowledge graph + vector search + RAG-Eval framework achieves 95% accuracy and outperforms BERTScore/G-EVAL

## Executive Summary
This paper introduces a RAG chatbot for querying engineering emails that leverages a knowledge graph and vector search for retrieval, combined with a novel tripartite evaluation framework (RAG-Eval) that assesses query, context, and response jointly. The system preserves relational integrity by avoiding document chunking and incorporates metadata transparency for user verification. Experimental results demonstrate high confidence scores and superior performance in detecting factual gaps compared to traditional metrics.

## Method Summary
The system uses Neo4j to store emails, persons, and conversations as nodes with SENT/RECEIVED/PART_OF relationships, while storing OpenAI embeddings as node properties. A LangChain ReAct agent first attempts Cypher queries on the graph, falling back to vector similarity search when needed. Responses include metadata for transparency. RAG-Eval evaluates the (query, retrieved context, response) triplet using five weighted metrics scored by GPT-4o, producing a confidence score. The architecture integrates with FastAPI and React frontend.

## Key Results
- RAG-Eval achieves 95% overall accuracy and outperforms BERTScore and G-EVAL in detecting factual gaps
- Query Relevance metric drops to 0.20 when queries are intentionally mismatched, while G-EVAL cannot detect this
- Metadata embedding enables direct user verification against source data

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Retrieval Preserves Relational Integrity
Combining knowledge graphs with vector search yields more contextually complete retrieval than chunking-based RAG alone. Structured nodes with explicit relationships allow precise Cypher queries returning full email records with metadata, while vector embeddings provide semantic fallback. This dual-path approach reduces information loss from arbitrary chunk boundaries.

### Mechanism 2: Tripartite Evaluation Detects Query-Response Misalignment
Evaluating the (query, retrieved context, response) tuple jointly surfaces factual gaps and off-topic answers that source-to-summary metrics miss. RAG-Eval prompts an LLM to score five metrics using explicit criteria and step-by-step instructions, normalized and weighted into a single confidence score shown to users.

### Mechanism 3: Metadata Embedding Enables User Verification
Including source identifiers (email IDs, timestamps) in generated responses increases transparency and reduces verification friction. Users can cross-reference the provided IDs against the original corpus, converting abstract trust into auditable evidence.

## Foundational Learning

- **Concept: Knowledge Graphs (Neo4j, Cypher)**
  - Why needed here: The system uses Neo4j to model emails, persons, and conversations as nodes with typed edges. You must understand Cypher query patterns to debug retrieval failures.
  - Quick check question: Given nodes `(:Person)-[:SENT]->(:Email)-[:PART_OF]->(:Conversation)`, write a Cypher query to find all emails sent by a person with `personId = "abc123"`.

- **Concept: Vector Embeddings & Similarity Search**
  - Why needed here: Each email's content and metadata are embedded using OpenAI's `text-embedding-3-small`. Cosine similarity over these vectors provides the fallback retrieval path.
  - Quick check question: Explain why cosine similarity might return semantically related emails that lack the exact metadata filtered by a Cypher query.

- **Concept: RAG Architecture & Chunking Tradeoffs**
  - Why needed here: This system deliberately avoids chunking by storing full emails as nodes. Understanding why chunking is common helps evaluate when this design is appropriate.
  - Quick check question: What retrieval failure modes does document chunking introduce that a graph-based approach might mitigate?

## Architecture Onboarding

- **Component map:** Frontend React UI → HTTP POST → FastAPI backend → LangChain ReAct agent → Neo4j AuraDB (graph + vector index) → LLM generates response with metadata → RAG-Eval scores (query, context, response) → confidence score to UI

- **Critical path:** User query → FastAPI → ReAct agent generates Cypher → Neo4j returns nodes → if insufficient → vector similarity search → retrieved context + query → LLM generates response with metadata → RAG-Eval scores → confidence score returned to UI

- **Design tradeoffs:**
  - Full-email nodes vs. chunking: Preserves context but increases token usage per retrieval
  - Cypher-first vs. vector-first: Structured queries are precise but brittle; vector fallback is robust but may return irrelevant context
  - LLM-as-evaluator: Scalable and interpretable but introduces stochasticity

- **Failure signatures:**
  - Low Query Relevance but high Fluency: Agent retrieved wrong context; check Cypher generation or vector fallback
  - High Factual Accuracy but low Coverage: Response is grounded but omits details; consider expanding agent prompt
  - Inconsistent confidence scores: LLM evaluator nondeterminism; set temperature=0 for evaluation calls

- **First 3 experiments:**
  1. Baseline retrieval comparison: Run 20 queries using Cypher-only, vector-only, and hybrid retrieval; measure precision@3 and RAG-Eval scores
  2. Metric weight sensitivity analysis: Vary default weights on held-out set of 50 queries with human ratings
  3. Failure case audit: Select all responses with confidence <70%, manually categorize failure modes, trace back to component responsible

## Open Questions the Paper Calls Out

- How can the system be effectively extended to handle unstructured data scenarios while preserving the relational integrity currently ensured by the knowledge graph? (Future work mentioned in Conclusion)
- What specific storage and retrieval optimizations are required to mitigate latency increases as the Neo4j knowledge graph scales beyond the current dataset? (Identified as limitation in paper)
- Does the RAG-Eval framework maintain high alignment with human judgment when utilizing open-source evaluator models (e.g., Llama, Mistral) instead of GPT-4o? (Model-agnostic claim not validated across architectures)

## Limitations

- Dependency on domain-specific relational structure provides minimal benefit if source data lacks consistent metadata
- Evaluation framework relies on LLM-based scoring without human-annotated ground truth for entire corpus
- Performance claims comparing against BERTScore and G-EVAL are based on controlled test cases rather than head-to-head evaluation on identical datasets

## Confidence

- **High confidence:** Tripartite evaluation framework's ability to detect query-response misalignment is well-supported by controlled experiments
- **Medium confidence:** 95% overall accuracy claim is based on internal testing without external validation
- **Medium confidence:** Metadata transparency mechanism's effectiveness in building user trust lacks empirical validation through user studies

## Next Checks

1. Controlled ablation study: Compare hybrid retrieval against pure vector RAG on 50 queries from engineering email corpus
2. External evaluator validation: Recruit 3 human annotators to independently score 30 randomly selected query-response pairs
3. Failure mode characterization: Systematically generate 20 adversarial queries designed to break each component and document recovery behavior