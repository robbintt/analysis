---
ver: rpa2
title: 'Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey'
arxiv_id: '2511.05982'
source_url: https://arxiv.org/abs/2511.05982
tags:
- safety
- monitoring
- detection
- adversarial
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Runtime safety monitoring is essential for deploying deep neural
  networks in safety-critical perception systems, such as autonomous driving and robotics,
  to address vulnerabilities like generalization errors, out-of-distribution inputs,
  and adversarial attacks. This survey categorizes runtime safety monitoring approaches
  into three groups: monitoring inputs, internal representations, and outputs.'
---

# Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey

## Quick Facts
- **arXiv ID**: 2511.05982
- **Source URL**: https://arxiv.org/abs/2511.05982
- **Reference count**: 40
- **Primary result**: Runtime safety monitoring is essential for deploying deep neural networks in safety-critical perception systems, such as autonomous driving and robotics, to address vulnerabilities like generalization errors, out-of-distribution inputs, and adversarial attacks.

## Executive Summary
This survey systematically categorizes runtime safety monitoring approaches for deep neural networks in safety-critical perception systems. The authors identify three main monitoring categories: input monitoring using signal processing or reconstruction models, internal representation monitoring through neuron activation analysis, and output monitoring via confidence and consistency checks. The survey demonstrates that no single monitoring method comprehensively addresses all safety risks, highlighting the need for integrated frameworks that combine multiple strategies. The work emphasizes that DNNs' tendency to produce high-confidence yet incorrect predictions on unfamiliar inputs remains a fundamental challenge for safety monitoring.

## Method Summary
The survey provides a comprehensive taxonomy of runtime safety monitoring methods for DNNs in perception systems. It organizes approaches into three categories: (1) input monitoring that uses signal processing or reconstruction models to detect anomalies before propagation through the network, (2) internal representation monitoring that observes neuron activation patterns in hidden layers to identify unexpected behaviors, and (3) output monitoring that analyzes prediction confidence or consistency. The survey reviews specific techniques within each category, including activation hyperrectangles for OOD detection, Monte Carlo dropout for uncertainty estimation, and combined approaches that integrate multiple monitoring strategies. Evaluation metrics include detection accuracy and computational overhead, with datasets spanning MNIST, CIFAR-10, GTSRB, KITTI, Cityscapes, COCO, NuScenes, and BDD100K.

## Key Results
- Runtime safety monitoring approaches can be categorized into three groups: input, internal representation, and output monitoring
- No single monitoring method comprehensively addresses all safety risks (generalization errors, OOD inputs, adversarial attacks)
- Combined approaches integrating multiple monitoring strategies are needed but face computational overhead challenges
- DNNs' tendency to produce highly confident yet incorrect predictions on unfamiliar inputs fundamentally limits output-based monitoring reliability

## Why This Works (Mechanism)

### Mechanism 1: Input-Space Anomaly Detection
- Claim: Signal processing and reconstruction models can flag anomalous inputs before they propagate through the DNN.
- Mechanism: Autoencoders or robustness verification methods compare input characteristics against expected distributions; deviations trigger safety alerts without requiring DNN modification.
- Core assumption: Anomalous inputs exhibit measurable statistical or structural differences from training distribution data.
- Evidence anchors:
  - [abstract] "Input monitoring methods use signal processing or reconstruction models to detect anomalies before they propagate."
  - [section IV.A] Liu et al. demonstrate adversarial examples have "significantly smaller robustness radii compared to correctly classified inputs" on MNIST and CIFAR-10.
  - [corpus] Limited direct corpus validation; neighboring papers focus on OOD topology rather than input-space monitoring specifically.
- Break condition: Subtle distribution shifts that preserve input-space statistics but alter semantic content may evade detection.

### Mechanism 2: Internal Representation Monitoring
- Claim: Observing neuron activation patterns in hidden layers can identify unexpected behaviors indicative of OOD inputs or adversarial attacks.
- Mechanism: During training, monitors capture activation value ranges (e.g., hyperrectangles, quantiles, clusters); runtime activations falling outside these bounds trigger alerts.
- Core assumption: In-distribution inputs produce activation patterns within learnable bounds; OOD/adversarial inputs produce detectable deviations.
- Evidence anchors:
  - [abstract] "Internal representation monitoring observes neuron activations in hidden layers to identify unexpected patterns."
  - [section IV.B] Henzinger et al. use hyperrectangles for OOD detection; Yatbaz et al. show "earlier layers improve error detection compared to final-layer activations" on KITTI/NuScenes.
  - [corpus] "Topology of Out-of-Distribution Examples" paper confirms DNNs "tend to be overconfident and incorrect when encountering OOD examples," supporting need for internal monitoring.
- Break condition: Adversarial attacks optimized against both task loss and activation patterns may evade fixed abstractions.

### Mechanism 3: Output Confidence and Consistency Analysis
- Claim: Prediction confidence scores and output consistency checks can detect unreliable predictions without accessing internal DNN states.
- Mechanism: Softmax entropy, Monte Carlo dropout variance, or KL-divergence between perturbed outputs quantify uncertainty; low confidence or high inconsistency signals potential errors.
- Core assumption: Erroneous predictions exhibit distinguishable confidence/consistency profiles compared to correct predictions.
- Evidence anchors:
  - [abstract] "Output monitoring analyzes prediction confidence or consistency."
  - [section IV.C] Kumura et al. measure "Kullback-Leibler-Divergence between outputs before and after input transformations" for adversarial detection.
  - [corpus] Weak corpus support; neighboring papers emphasize adversarial robustness rather than output-space monitoring.
- Break condition: DNNs "tend to produce highly confident yet incorrect results in unfamiliar scenarios" (Section V), limiting standalone output monitoring reliability.

## Foundational Learning

- Concept: **Out-of-Distribution (OOD) Detection**
  - Why needed here: All three monitoring categories target OOD detection; understanding what constitutes "distribution shift" is prerequisite.
  - Quick check question: Can you explain why a DNN might produce high-confidence predictions on inputs it has never seen before?

- Concept: **Adversarial Perturbations**
  - Why needed here: Adversarial attacks are a core safety concern; monitors must distinguish adversarial from benign anomalies.
  - Quick check question: What distinguishes an adversarial patch attack from natural OOD inputs in an autonomous driving context?

- Concept: **Uncertainty Quantification (Softmax Entropy, MC Dropout)**
  - Why needed here: Output-based monitors rely on uncertainty estimates; understanding calibration limitations is critical.
  - Quick check question: Why might softmax probability not reflect true prediction confidence?

## Architecture Onboarding

- Component map:
  - Input → Input monitor → DNN (Φ: input → latent z; Ψ: latent → output y) → Internal monitors (observe z) → Output monitors (observe y) → Safety alert handler

- Critical path:
  1. Input arrives → Input monitor checks signal properties
  2. Input enters DNN → Internal monitors observe layer activations
  3. DNN produces output → Output monitors assess confidence/consistency
  4. Any monitor triggers → Safety alert issued → Fallback action

- Design tradeoffs:
  - Input monitoring: Low latency but may miss semantic anomalies
  - Internal monitoring: Higher detection capability but requires DNN introspection access and computational overhead
  - Output monitoring: Simplest integration but vulnerable to overconfident failures
  - Combined approaches: Better coverage but increased complexity and latency

- Failure signatures:
  - High false positive rate on edge-case but valid inputs
  - Missed detections on subtle adversarial perturbations
  - Latency violations in real-time safety-critical loops
  - Activation abstractions that don't generalize across model retraining

- First 3 experiments:
  1. Baseline calibration: Measure false positive rate of each monitor type (input/internal/output) on held-out in-distribution validation data to establish detection thresholds.
  2. OOD detection sweep: Evaluate each monitor on benchmark OOD datasets (e.g., CIFAR-10 vs. SVHN) to map detection rates per safety concern category from Table II.
  3. Adversarial robustness test: Apply standardized adversarial attacks (PGD, patch attacks) to quantify detection rates and latency overhead; compare single-monitor vs. combined-monitor performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can integrated monitoring frameworks optimally combine input, internal representation, and output-based approaches to achieve comprehensive safety coverage while meeting real-time constraints?
- Basis in paper: [explicit] "These limitations highlight the need for integrated frameworks that combine multiple monitoring strategies."
- Why unresolved: Combined approaches exist (e.g., Klingner et al., Hacker et al.) but systematically optimizing the trade-off between comprehensive coverage and computational overhead remains unaddressed.
- What evidence would resolve it: Benchmark studies measuring detection coverage versus latency for systematically combined monitors across diverse failure modes.

### Open Question 2
- Question: What mechanisms can overcome the fundamental limitation that DNNs produce high-confidence yet incorrect predictions on unfamiliar inputs, undermining output-based monitoring reliability?
- Basis in paper: [explicit] Output-based methods "are limited by the DNN's tendency to produce highly confident yet incorrect results in unfamiliar scenarios."
- Why unresolved: The paper documents this as a core limitation but no surveyed method fully resolves the overconfidence problem inherent to DNN generalization behavior.
- What evidence would resolve it: Methods demonstrating calibrated uncertainty on OOD inputs where softmax confidence reliably indicates prediction reliability.

### Open Question 3
- Question: How can internal representation monitoring techniques be made computationally efficient enough for deployment in resource-constrained, real-time safety-critical systems?
- Basis in paper: [explicit] "Techniques that inspect internal network activations provide deeper insights... but often require high computational resources."
- Why unresolved: While methods like BAM claim "minimal computational overhead," systematic approaches to reducing activation monitoring costs without sacrificing detection capability remain undeveloped.
- What evidence would resolve it: Studies showing internal monitoring methods achieving sub-millisecond inference overhead while maintaining detection performance on standard benchmarks.

### Open Question 4
- Question: How should RSM performance be standardized and benchmarked across the heterogeneous landscape of safety concerns (generalization errors, OOD inputs, adversarial attacks)?
- Basis in paper: [inferred] Table I shows inconsistent evaluation across methods—different architectures, datasets, and monitored events—preventing direct comparison of approaches.
- Why unresolved: The survey reveals no standardized metrics or benchmark suites exist for systematically comparing RSM approaches across safety concern categories.
- What evidence would resolve it: Community adoption of unified benchmark datasets with labeled generalization, OOD, and adversarial examples alongside standardized detection metrics.

## Limitations

- The survey doesn't specify concrete thresholds for flagging anomalies, making direct reproduction challenging without access to original implementations
- Evidence supporting output monitoring mechanisms is particularly weak, with limited corpus validation beyond general adversarial robustness literature
- The survey doesn't address computational overhead in real-time safety-critical systems, a crucial constraint for autonomous driving applications

## Confidence

- **High confidence**: The categorization framework (input/internal/output monitoring) and the fundamental observation that DNNs produce overconfident predictions on OOD inputs (supported by neighboring "Topology of Out-of-Distribution Examples" paper)
- **Medium confidence**: Internal representation monitoring mechanisms, supported by specific examples like Henzinger et al. and Yatbaz et al. with measurable detection improvements
- **Low confidence**: Output monitoring reliability claims, given the acknowledged limitation that DNNs "tend to produce highly confident yet incorrect results in unfamiliar scenarios"

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary detection thresholds across monitoring categories on validation data to quantify false positive/negative tradeoffs and establish practical operating points.
2. **Cross-Architecture Generalization**: Test selected monitoring approaches (e.g., activation hyperrectangles) across different DNN architectures (CNN, Transformer) to verify that monitoring abstractions transfer beyond specific model implementations.
3. **Real-Time Performance Benchmarking**: Measure end-to-end latency overhead of combined monitoring approaches on embedded hardware representative of autonomous driving systems to validate real-time feasibility claims.