---
ver: rpa2
title: 'PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference
  Attacks'
arxiv_id: '2512.12840'
source_url: https://arxiv.org/abs/2512.12840
tags:
- privacy
- inference
- confidence
- learning
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRIVEE addresses feature inference attacks in vertical federated
  learning (VFL), where adversaries exploit shared confidence scores to reconstruct
  private input features. It introduces a lightweight, order-preserving perturbation
  mechanism that obfuscates confidence scores while preserving ranking and inter-score
  distances.
---

# PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks

## Quick Facts
- arXiv ID: 2512.12840
- Source URL: https://arxiv.org/abs/2512.12840
- Authors: Sindhuja Madabushi; Ahmad Faraz Khan; Haider Ali; Ananthram Swami; Rui Ning; Hongyi Wu; Jin-Hee Cho
- Reference count: 40
- Key outcome: PRIVEE increases feature inference attack MSE by up to 30× compared to baselines while maintaining near-zero accuracy degradation

## Executive Summary
PRIVEE addresses feature inference attacks in vertical federated learning (VFL) where adversaries exploit shared confidence scores to reconstruct private input features. It introduces a lightweight, order-preserving perturbation mechanism that obfuscates confidence scores while preserving ranking and inter-score distances. The defense employs two variants: PRIVEE-DP with uniform differential privacy noise and PRIVEE-DP++ with heterogeneous privacy budgets per class. Extensive experiments demonstrate that PRIVEE achieves threefold improvement in privacy protection while preserving full model accuracy, scaling efficiently with class count and incurring minimal runtime overhead.

## Method Summary
PRIVEE is an inference-time transformation of confidence scores in VFL systems. It applies a linear transformation $p = A_{pert}c$ where $A_{pert}$ is an orthonormal matrix (e.g., Identity or FISIP) with diagonal entries perturbed by rank-aware uniform noise. The noise magnitude is calibrated using an $\ell_2$-sensitivity parameter ($\Delta f$) and privacy budget ($\varepsilon$). The mechanism preserves the relative ranking of confidence scores while making the system underdetermined for attackers attempting reconstruction. Two variants exist: PRIVEE-DP applies uniform noise across all classes, while PRIVEE-DP++ allows class-specific privacy budgets for more nuanced protection.

## Key Results
- PRIVEE increases feature inference attack MSE by up to 30× compared to no-defense baselines
- Maintains near-zero accuracy degradation (CA ≈ 0) across MNIST, CIFAR-10, CIFAR-100, and real-world datasets
- Achieves millisecond-scale inference overhead with near-linear scaling in class count
- Demonstrates robustness across diverse model architectures and attack types (GRN and GIA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving class ranking while obfuscating confidence magnitudes prevents feature reconstruction without degrading prediction accuracy.
- Mechanism: An orthonormal transformation matrix $A$ (e.g., $I_K$ or FISIP matrix) transforms confidence vectors. Diagonal entries are perturbed with rank-aware uniform noise sampled from class-positioned sub-intervals, ensuring $u_i < u_j$ whenever $c_i < c_j$ (Algorithm 1, lines 5–8).
- Core assumption: Most downstream tasks (and the GRN/GIA attacks) rely more on relative ordering than exact inter-class distances; prediction accuracy depends primarily on top-class identification.
- Evidence anchors:
  - [abstract]: "PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances."
  - [section 4.2.2]: Lemma 4.4 proves order-preservation under non-decreasing $\sigma$ vectors with rank-aware sampling.
  - [corpus]: VMask and related VFL defenses also target inference-time leakage but via different surfaces; PRIVEE's ranking-preservation approach is distinct.
- Break condition: If downstream tasks require exact probability calibration (e.g., risk scoring, threshold-based decisions), order-preservation alone is insufficient and utility degrades.

### Mechanism 2
- Claim: Injecting class-specific noise scales creates an underdetermined system that resists inversion even with knowledge of the transformation matrix.
- Mechanism: The attacker observing perturbed scores $p = A_{pert} c$ must solve $K+1$ nonlinear equations over $2K$ unknowns ($c$ and $\sigma \odot u$). The rank-aware constraint structure makes unique recovery impossible (Section 4.2.3, Eq. 10).
- Core assumption: The adversary lacks access to the random noise realization per inference; repeated queries cannot easily estimate $\varepsilon$ due to independent sampling and model stochasticity.
- Evidence anchors:
  - [section 4.2.3]: Explicit security analysis showing the system is underdetermined and lacks unique solution.
  - [section 6.5]: "Even if identical inputs are queried repeatedly, estimating the underlying $\varepsilon$ is difficult..."
  - [corpus]: OPUS-VFL and other recent work discuss privacy-utility tradeoffs but don't address the specific underdetermination argument.
- Break condition: If noise distribution or $\varepsilon$ is leaked or can be inferred through side channels, the effective privacy guarantee collapses.

### Mechanism 3
- Claim: Millisecond-scale inference overhead with near-linear scaling in class count makes PRIVEE practical for real-world deployments.
- Mechanism: Only diagonal entries of $A$ are modified ($O(K)$), and the matrix is constructed once and reused. No cryptographic operations (unlike OPE/HE baselines).
- Core assumption: The coordinator can perform the transformation before broadcasting scores; no additional round-trip communication is required.
- Evidence anchors:
  - [section 4.2.4]: "Overall per-vector complexity is $O(K)$, making PRIVEE-DP suitable for real-time and large-scale inference."
  - [table 9]: At 10,000 classes, PRIVEE = 0.145s vs. OPE = 6.954s vs. HE-Top5 = 106 min.
  - [corpus]: Sherpa.ai Blind VFL and related work emphasize communication efficiency; PRIVEE's compute overhead is complementary.
- Break condition: If latency budgets are sub-millisecond and class counts exceed 100K, even $O(K)$ may become noticeable; profile before deployment.

## Foundational Learning

- **Vertical Federated Learning (VFL) Architecture**
  - Why needed here: PRIVEE operates at inference time in VFL, where confidence scores are broadcast to all parties. Understanding the active/passive party split and coordinator role is essential.
  - Quick check question: Can you explain why confidence scores must be shared in VFL inference but not in HFL?

- **Feature Inference Attacks (GRN, GIA)**
  - Why needed here: PRIVEE is evaluated against Generative Regression Network and Gradient Inversion attacks. These exploit confidence vectors to reconstruct private features—knowing the threat model clarifies what PRIVEE defends against.
  - Quick check question: What adversary knowledge (white-box vs. black-box) does PRIVEE assume, and what can the attacker observe?

- **Differential Privacy Basics ($\varepsilon$, $\delta$, sensitivity)**
  - Why needed here: PRIVEE-DP uses DP-inspired noise calibrated via $\ell_2$-sensitivity (Eq. 9). Note: PRIVEE does *not* provide formal $(\varepsilon, \delta)$-DP guarantees (Section 6.5).
  - Quick check question: Why does bounded, non-symmetric noise violate formal DP guarantees even though $\varepsilon$ is used for calibration?

## Architecture Onboarding

- **Component map:**
  - Client-side (Active/Passive parties): Local forward pass produces embeddings/logits; no PRIVEE logic here.
  - Coordinator: Receives logits from all parties, aggregates, applies final activation (softmax/sigmoid), then applies PRIVEE transformation (Algorithm 1) before broadcasting.
  - Broadcast channel: All parties receive perturbed confidence scores; attackers (if present in active party) see only $p$, not raw $c$.

- **Critical path:**
  1. Configure privacy budget $\varepsilon$ (uniform for PRIVEE-DP, per-class for PRIVEE-DP++).
  2. Pre-construct orthonormal matrix $A$ (identity or FISIP).
  3. At inference, coordinator receives logits, computes confidence $c$, applies PRIVEE, broadcasts $p$.
  4. Monitor MSE (reconstruction error) and accuracy drift over time.

- **Design tradeoffs:**
  - **PRIVEE-DP vs. PRIVEE-DP++:** DP is simpler (uniform $\varepsilon$); DP++ allows class-specific budgets but requires more tuning and may expose heterogeneous privacy levels if not carefully managed.
  - **$\varepsilon$ selection:** Smaller $\varepsilon$ = stronger privacy but potential correlation distortion; larger $\varepsilon$ = better correlation preservation but weaker protection (Table 10).
  - **Correlation preservation:** PRIVEE does not guarantee inter-class correlation preservation; if downstream tasks need it, consider chunking strategies or future extensions (Section 7.2).

- **Failure signatures:**
  - **Accuracy drop > 0.3%:** Check if noise scales violate order-preservation (e.g., $\sigma$ not non-decreasing for DP++).
  - **Low MSE (attack succeeds):** Verify PRIVEE is actually applied (common integration bug: raw $c$ broadcast instead of $p$).
  - **Repeated-query leakage:** If clients can query same input many times, consider PRIVEE-DP++ with adaptive range randomization.

- **First 3 experiments:**
  1. **Baseline replication:** Reproduce GRN attack MSE values from Tables 1–3 without defense; confirm your VFL setup matches paper.
  2. **Ablation on $\varepsilon$:** Run PRIVEE-DP with $\varepsilon \in \{0.05, 0.1, 0.5, 0.9\}$ on MNIST; plot MSE vs. accuracy to verify tradeoff curve (Table 10).
  3. **Scalability test:** Measure inference latency with $K \in \{100, 1000, 10000\}$ classes; confirm $O(K)$ scaling and compare against OPE baseline (Table 9).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the piecewise uniform noise mechanism in PRIVEE-DP be formally proven to satisfy (ε,δ)-differential privacy, or does its bounded, non-symmetric distribution fundamentally preclude standard DP guarantees?
- Basis in paper: [explicit] Section 6.5 states the mechanism "does not satisfy formal differential privacy (DP) guarantees" because "the added noise is sampled from a bounded and non-symmetric distribution, thereby violating the conditions required for (ε,δ)-DP."
- Why unresolved: The authors acknowledge this limitation but claim strong empirical privacy; the theoretical gap remains unaddressed.
- What evidence would resolve it: A formal proof establishing (ε,δ)-DP bounds, or a counterexample showing specific inputs where the mechanism violates DP composition properties.

### Open Question 2
- Question: How does privacy leakage accumulate under PRIVEE when confidence vectors are released repeatedly across multiple inference rounds or training epochs in VFL?
- Basis in paper: [explicit] Section 7.2 states the plan to "extend to the study of composition properties in VFL, enabling a systematic understanding of how privacy leakage accumulates under repeated gradient inversion or inference attacks across multiple training epochs and clients."
- Why unresolved: PRIVEE is evaluated only on single-shot attacks; cumulative leakage from repeated queries could enable adversaries to estimate ε or recover unperturbed scores.
- What evidence would resolve it: Experiments measuring reconstruction error (MSE) as a function of query count, with formal composition bounds derived for the mechanism.

### Open Question 3
- Question: Can a noise injection strategy be designed that provably preserves inter-class correlation structures (e.g., pairwise covariance) while maintaining the order-preservation and privacy properties of PRIVEE?
- Basis in paper: [explicit] Section 7.2 mentions plans to "design a correlation-aware noise injection strategy that explicitly constrains pairwise covariance distortion between the true and released confidence vectors."
- Why unresolved: Current PRIVEE variants preserve ranking but not correlation; correlation preservation improves as ε→∞ but weakens privacy.
- What evidence would resolve it: A modified mechanism with bounded covariance distortion (e.g., |Cov(c_i, c_j) – Cov(p_i, p_j)| ≤ τ) evaluated on downstream tasks like ensemble learning or knowledge distillation.

## Limitations

- **Sensitivity parameter uncertainty**: The exact L2-sensitivity ($\Delta f$) value used for noise calibration is not specified, making precise reproduction challenging
- **Non-DP formal guarantees**: PRIVEE does not satisfy formal (ε,δ)-differential privacy due to bounded, non-symmetric noise distribution
- **Correlation preservation gap**: Current mechanism preserves ranking but not inter-class correlation structures, which may impact certain downstream tasks

## Confidence

- **High confidence**: The core mechanism of order-preserving perturbation and underdetermined system security analysis (Section 4.2.3)
- **Medium confidence**: Scalability claims and latency measurements (Table 9), assuming O(K) implementation is straightforward
- **Medium confidence**: Accuracy preservation claims (Tables 1-3), pending exact sensitivity parameter and attack implementation details

## Next Checks

1. **Parameter sensitivity**: Systematically vary $\Delta f$ and $\varepsilon$ to map the privacy-utility frontier and identify the operating point matching reported results
2. **Attack fidelity**: Implement GRN/GIA with reasonable hyperparameters (e.g., Adam optimizer, 0.001 learning rate) and verify baseline MSE values before applying PRIVEE
3. **Rank-preservation verification**: Add runtime assertions that perturbed confidence rankings exactly match original rankings for all test samples