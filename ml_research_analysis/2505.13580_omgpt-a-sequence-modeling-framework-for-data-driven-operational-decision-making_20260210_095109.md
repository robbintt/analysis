---
ver: rpa2
title: 'OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision
  Making'
arxiv_id: '2505.13580'
source_url: https://arxiv.org/abs/2505.13580
tags:
- decision
- where
- demand
- optimal
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OMGPT, a transformer-based framework for
  solving sequential decision-making tasks in operations management and research.
  It reframes problems like dynamic pricing, inventory management, and queuing control
  as sequence modeling tasks, where the goal is to predict optimal actions based on
  historical data.
---

# OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making

## Quick Facts
- **arXiv ID:** 2505.13580
- **Source URL:** https://arxiv.org/abs/2505.13580
- **Reference count:** 40
- **Primary result:** OMGPT consistently outperforms established benchmarks across dynamic pricing, inventory management, and queuing control by reframing sequential decision-making as sequence modeling

## Executive Summary
This paper introduces OMGPT, a transformer-based framework that reframes sequential decision-making problems in operations management as sequence modeling tasks. The approach trains a GPT-style neural network to predict optimal actions from historical data without requiring analytical model structures or assumptions about underlying environments. The framework demonstrates strong empirical performance across multiple operational domains and provides theoretical analysis showing it approximates Bayes-optimal decision making. OMGPT offers a new paradigm for data-driven operational decision making that moves beyond traditional online learning methods while providing valuable side information like demand forecasting.

## Method Summary
OMGPT formulates sequential decision-making tasks as sequence modeling problems where the goal is to predict optimal actions based on historical context and observations. The method uses a GPT-2 style transformer (12 layers, 16 heads, 256-dim embedding) to process interleaved feature and label sequences. Synthetic pre-training data is generated from environment priors using a noisy optimal policy, and the model is trained via supervised learning to minimize prediction error between predicted and optimal actions. The training employs a two-phase curriculum with horizon progression and mixed data generation to prevent out-of-distribution drift. During inference, the model operates autoregressively, using its own predictions as part of the history for subsequent decisions.

## Key Results
- OMGPT consistently outperforms well-established benchmarks across dynamic pricing, newsvendor, and queuing control tasks
- The framework demonstrates strong generalization ability, handling non-stationary environments and extended horizons beyond training
- Theoretical analysis establishes that OMGPT approximates Bayes-optimal decision making through implicit posterior estimation

## Why This Works (Mechanism)

### Mechanism 1: Implicit Bayesian Posterior Estimation
The model implicitly infers the probability distribution of unknown environment parameters given observed history, behaving as a Bayes-optimal decision maker. By minimizing pre-training loss over a distribution of environments, the transformer learns to map history to actions that approximate the expectation of optimal actions weighted by posterior likelihood. This requires the testing environment to be drawn from the same distribution used for synthetic data generation, and sufficient model capacity to approximate the Bayes-optimal function. The mechanism fails when testing environments fall significantly outside the pre-training distribution.

### Mechanism 2: Algorithm Distillation via In-Context Learning
Pre-training on synthetic optimal trajectories enables the transformer to learn a general decision-making algorithm rather than task-specific policies. The attention mechanism identifies patterns in historical context to predict optimal actions without explicit state-transition models. This compression of online learning into sequence prediction relies on the assumption that current optimal actions can be predicted from history context alone. The mechanism breaks down if context windows are too short to capture necessary environmental information.

### Mechanism 3: Implicit Exploration via Prediction Noise
Deviation between the learned model and theoretical Bayes-optimal function provides inherent exploration, preventing linear regret loops common in greedy Bayesian policies. While perfect Bayesian policies can incur linear regret without exploration, prediction errors introduce sufficient randomness for posterior concentration on true environments. This requires the prediction error and exploration intensity to satisfy sub-linear regret balance. The mechanism fails if pre-training datasets are too large, eliminating prediction noise and converging to greedy behavior.

## Foundational Learning

- **Concept:** Bayesian Posterior Inference
  - **Why needed here:** The model's core logic estimates posterior distribution P(γ|H_t) to derive Bayes-optimal actions, not simple pattern matching
  - **Quick check question:** Can you explain why knowing the likelihood P(H|γ) is essential for deriving the posterior P(γ|H)?

- **Concept:** Regret Analysis (Sub-linear vs. Linear)
  - **Why needed here:** Performance is measured by regret difference from optimal cumulative reward; understanding O(√T) vs O(T) regret is crucial for evaluating theoretical guarantees
  - **Quick check question:** If an algorithm incurs a regret of 0.5T, does it learn the environment effectively over time?

- **Concept:** Supervised Pre-training on Synthetic Data
  - **Why needed here:** Unlike standard RL, this approach relies entirely on pre-training phase using data generated from known prior distribution before seeing real environment
  - **Quick check question:** Why is generating data using the optimal action a* (oracle) preferred over generating data using a random policy?

## Architecture Onboarding

- **Component map:** Synthetic Data Generation -> Model Construction -> Training Loop (Algorithm 2) -> Inference (Autoregressive)
- **Critical path:**
  1. Generate synthetic data by sampling environment γ ~ P_γ and running history generation with noisy optimal policy
  2. Format data by concatenating context and observations into feature elements
  3. Train using supervised loss between predicted and optimal actions
  4. Perform inference through autoregressive loop feeding predictions back into history
- **Design tradeoffs:**
  - Model size (12 layers) balances task complexity handling with training costs
  - Context window truncation to 100 steps enables long-horizon testing but risks forgetting early exploration data
  - Data mix ratio (κ=1/3) blends transformer-generated with expert data to prevent OOD drift during training
- **Failure signatures:**
  - Out-of-Domain Parameters: Performance degrades significantly if test parameters fall outside pre-training support
  - In-Distribution Drift: Higher test noise variance than pre-training increases regret
- **First 3 experiments:**
  1. Dynamic Pricing Baseline: Train on linear demand functions with 6D contexts, test against ILSE/TS benchmarks
  2. OOD Robustness Check: Shift test demand parameters by μ_shift=0.5 outside training range to observe performance boundaries
  3. Horizon Generalization: Train on T=100, test on T=200 with sliding window mechanism to validate extended horizon capability

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the optimal context window size be theoretically determined for testing horizons that exceed the pre-training horizon?
  - Basis: Section 4.5.1 identifies determining optimal window size for random testing horizons as an important future research direction
  - Why unresolved: Currently relies on numerical performance results rather than theoretical framework for optimization
  - What evidence would resolve it: Theoretical derivation linking context window size to generalization performance and regret bounds when T_test > T_train

- **Open Question 2:** What precise structural properties of pre-training error guarantee OMGPT provides sufficient exploration to avoid linear regret?
  - Basis: Section 5.2.3 notes Bayes-optimal function incurs linear regret while pre-trained model achieves sub-linear regret via "inherent exploration" from prediction errors
  - Why unresolved: Theoretical analysis relies on assumed constants rather than deriving properties from transformer architecture
  - What evidence would resolve it: Theoretical proof showing how specific loss functions or network capacities enforce necessary deviation distribution for sub-linear regret

- **Open Question 3:** How can theoretical regret bounds be extended to quantify performance degradation under severe OOD shifts?
  - Basis: Section 6.1.4 numerically evaluates out-of-domain shifts but current theoretical bounds primarily assume γ ∈ Γ (within pre-training support)
  - Why unresolved: Current bounds don't explicitly account for divergence between testing environment and pre-training prior support
  - What evidence would resolve it: Regret bounds including terms for distance between true environment parameters and pre-training space boundary

## Limitations
- Theoretical guarantees depend heavily on assumption that testing environments come from same distribution as synthetic pre-training data
- "Implicit exploration via prediction noise" mechanism may not hold with very large pre-training datasets that eliminate prediction error
- Two-phase training algorithm introduces problem-specific hyperparameters (mix ratio κ, curriculum schedule) without full characterization

## Confidence

- **High Confidence:** Empirical performance claims showing OMGPT outperforming established benchmarks across multiple operational tasks with clearly specified and reproducible methodology
- **Medium Confidence:** Bayesian perspective claiming OMGPT approximates Bayes-optimal decision making, theoretically grounded but dependent on distributional assumptions
- **Low Confidence:** "Implicit exploration via prediction noise" as reliable alternative to explicit exploration strategies, intriguing theoretical claim requiring extensive empirical validation

## Next Checks

1. **OOD Robustness Testing:** Systematically evaluate performance degradation as testing environments move further outside pre-training distribution, defining quantitative distance metrics and measuring performance curves to establish operational boundaries

2. **Exploration Mechanism Validation:** Compare regret trajectories of OMGPT against explicitly exploratory baselines (e.g., Thompson Sampling with annealing) on problems where exploration-exploitation tradeoffs are critical to validate whether prediction noise provides sufficient exploration

3. **Side Information Utility:** Design experiments specifically testing OMGPT's ability to provide accurate demand forecasts or other side information, measuring whether this information improves downstream decision-making compared to models without this capability