---
ver: rpa2
title: 'Pie: A Programmable Serving System for Emerging LLM Applications'
arxiv_id: '2510.24051'
source_url: https://arxiv.org/abs/2510.24051
tags:
- layer
- control
- serving
- inference
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Pie is a programmable LLM serving system that enables application-specific
  optimizations by exposing fine-grained APIs and delegating control to user-provided
  programs called inferlets. Pie addresses three key limitations in existing systems:
  implicit KV cache management, inflexible generation processes, and poor workflow
  integration.'
---

# Pie: A Programmable Serving System for Emerging LLM Applications

## Quick Facts
- arXiv ID: 2510.24051
- Source URL: https://arxiv.org/abs/2510.24051
- Authors: In Gim; Zhiyao Ma; Seung-seob Lee; Lin Zhong
- Reference count: 40
- Primary result: Pie is a programmable LLM serving system that enables application-specific optimizations through user-provided programs called inferlets

## Executive Summary
Pie addresses critical limitations in existing LLM serving systems by exposing fine-grained APIs and delegating control to user-provided programs called inferlets. The system tackles three key challenges: implicit KV cache management, inflexible generation processes, and poor workflow integration. By decomposing the monolithic generation loop into independent handlers, Pie enables custom KV cache strategies, bespoke generation logic, and seamless integration of computation and I/O. The system uses WebAssembly for lightweight sandboxing and isolation, allowing applications to implement various LLM techniques including attention variants, constrained decoding, and agentic workflows.

## Method Summary
Pie implements a programmable serving architecture that exposes granular control over the LLM generation process through WebAssembly-based inferlets. The system decomposes the traditional monolithic generation loop into independent handlers, allowing applications to customize KV cache management, generation logic, and workflow integration. Applications implement inferlets that define custom behavior for token generation, KV cache handling, and inter-process communication. The WebAssembly sandbox provides isolation while maintaining performance, enabling dynamic optimization strategies based on application-specific requirements.

## Key Results
- Pie matches state-of-the-art performance on standard tasks with only 3-12% latency overhead
- Agentic workflow implementations show 1.3×-3.4× higher latency and throughput compared to baseline systems
- The system successfully implements various LLM techniques including attention variants, constrained and speculative decoding, deliberate prompting strategies, and agentic workflows

## Why This Works (Mechanism)
Pie's effectiveness stems from its fundamental architectural shift from monolithic serving systems to a programmable framework. By exposing fine-grained control through inferlets, applications can implement optimization strategies tailored to their specific requirements. The decomposition of the generation loop allows for custom KV cache management strategies that can optimize memory usage or performance based on application needs. The WebAssembly sandboxing provides the necessary isolation while maintaining performance, enabling safe execution of user-provided optimization logic. This approach allows applications to implement sophisticated techniques like speculative decoding and constrained generation without requiring changes to the core serving system.

## Foundational Learning
- **Inferlets**: User-provided programs that define custom behavior for LLM serving, enabling application-specific optimizations through fine-grained control over the generation process.
- **WebAssembly sandboxing**: Lightweight isolation mechanism that allows safe execution of user code while maintaining performance characteristics close to native execution.
- **KV cache management**: Fine-grained control over key-value cache storage and retrieval, enabling custom strategies for memory optimization and performance tuning.
- **Generation loop decomposition**: Breaking down the monolithic generation process into independent handlers for tokens, KV cache, and I/O operations.
- **Agentic workflow integration**: Seamless combination of LLM inference with external computation and I/O operations, enabling complex reasoning and planning capabilities.

## Architecture Onboarding

**Component Map:**
Application -> Inferlet (WebAssembly) -> Pie Core -> LLM Backend -> KV Cache

**Critical Path:**
1. Application invokes generation request
2. Pie loads and executes inferlet
3. Inferlet handles token generation requests
4. KV cache operations managed by inferlet
5. Results returned to application

**Design Tradeoffs:**
- Flexibility vs. complexity: Fine-grained control enables sophisticated optimizations but increases system complexity
- Performance vs. isolation: WebAssembly provides good isolation with minimal performance overhead
- Customization vs. standardization: Programmable interface allows application-specific optimizations at the cost of consistent behavior

**Failure Signatures:**
- Inferlet execution failures due to WebAssembly compilation or runtime errors
- Performance degradation when complex inferlet logic introduces overhead
- Memory management issues from custom KV cache strategies

**Three First Experiments:**
1. Implement a simple echo inferlet to verify basic functionality and measure WebAssembly overhead
2. Create a KV cache optimization inferlet that implements a custom eviction strategy
3. Develop a constrained decoding inferlet to test application-specific generation logic

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Memory consumption trade-offs from fine-grained KV cache control are not thoroughly characterized
- WebAssembly sandboxing overhead across different hardware configurations is not fully evaluated
- System complexity may pose deployment challenges in production environments requiring specialized expertise

## Confidence
**Performance Claims:**
- 3-12% latency overhead on standard tasks: High
- 1.3×-3.4× gains in agentic workflows: Medium

**System Design Claims:**
- Flexibility and API design effectiveness: High
- WebAssembly sandboxing approach: Medium

## Next Checks
1. Conduct comprehensive memory usage profiling across different KV cache strategies to quantify memory-overhead trade-offs enabled by Pie's fine-grained control
2. Benchmark WebAssembly sandboxing overhead on multiple hardware platforms (CPUs with and without AVX-512, various GPU configurations) to establish performance boundaries
3. Implement and evaluate a real-world production deployment scenario with multiple concurrent users to assess operational complexity, debugging capabilities, and resource management under realistic load conditions