---
ver: rpa2
title: 'Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients'
arxiv_id: '2601.23135'
source_url: https://arxiv.org/abs/2601.23135
tags:
- grpo
- gradient
- arxiv
- normalization
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical explanation for why variance
  normalization in GRPO works by showing it implements an adaptive gradient mechanism.
  The normalization uses reward variance as a proxy for local curvature, scaling updates
  inversely to the smoothness of each prompt's objective function.
---

# Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients

## Quick Facts
- arXiv ID: 2601.23135
- Source URL: https://arxiv.org/abs/2601.23135
- Authors: Cheng Ge; Caitlyn Heqi Yin; Hao Liang; Jiawei Zhang
- Reference count: 40
- Primary result: Variance normalization in GRPO implements adaptive gradient scaling via reward variance as local curvature proxy, achieving faster convergence than unnormalized REINFORCE under mild assumptions.

## Executive Summary
This paper provides a theoretical explanation for why variance normalization in GRPO works by showing it implements an adaptive gradient mechanism. The normalization uses reward variance as a proxy for local curvature, scaling updates inversely to the smoothness of each prompt's objective function. Under mild assumptions, the authors prove GRPO achieves faster convergence than unnormalized REINFORCE, with gains characterized by average within-prompt reward standard deviation. Empirical validation on GSM8K and MATH benchmarks reveals three training phases: early acceleration with high variance and orthogonality, stable transition with moderate variance, and late-stage where reduced orthogonality limits further gains.

## Method Summary
The study compares GRPO with standard deviation normalization against unnormalized REINFORCE on mathematical reasoning tasks. The base model Qwen2.5-Math-1.5B with LoRA (rank=64, alpha=128) is trained on GSM8K (Easy: 4,695 examples, Hard: 1,909 examples) and MATH Level 2. GRPO advantage computation uses Âᵢ,ₜ = (rᵢ - mean(r)) / std(r) for normalized and Âᵢ,ₜ = rᵢ - mean(r) for baseline. Binary verifiable rewards are computed for sampled responses, with gradients updated via gradient ascent.

## Key Results
- Theoretical proof that per-prompt reward variance normalization implements curvature-adaptive gradient scaling
- Empirical validation showing consistent improvements on GSM8K and MATH benchmarks, especially on harder prompts
- Identification of three distinct training phases with varying benefits from normalization: early (high variance/noise), mid (moderate variance/stable), late (reduced orthogonality)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-prompt reward variance normalization implements curvature-adaptive gradient scaling.
- Mechanism: The paper establishes that per-prompt reward variance $V[\pi_\theta(i)]$ is bounded proportionally to the spectral norm of the Hessian $\|\nabla^2 J_i(\theta)\|$. By normalizing advantages by $\sqrt{V[\pi_\theta(i)]}$, GRPO assigns larger effective step sizes to prompts with lower curvature (smoother loss geometry) and smaller steps to sharper prompts, stabilizing updates in heterogeneous landscapes.
- Core assumption: The within-prompt reward variance serves as a reliable, locally valid estimator of the loss landscape curvature at the current policy parameterization.
- Evidence anchors:
  - [abstract] "...standard deviation normalization implements an adaptive gradient."
  - [section 3.1, Lemma 3.1] $\|\nabla^2 J_i(\theta)\| \le 4X_{\max}^2 \cdot V[\pi_\theta(i)]$.
  - [corpus] Related work "AdaGrad Meets Muon" (arXiv:2509.02981) discusses adaptive step-size methods in similar contexts.
- Break condition: If reward noise becomes decorrelated from gradient stability (e.g., noisy sparse rewards with stable gradients), the adaptive scaling misestimates curvature, leading to suboptimal step sizes.

### Mechanism 2
- Claim: Near-orthogonal gradient representations across prompts minimize cross-prompt interference, allowing variance-based scaling to dominate dynamics.
- Mechanism: Under log-linear policy parametrization, orthogonal feature representations ($X_i X_j^\top = 0$ for $i \neq j$) induce orthogonal gradients $\nabla J_i(\theta)$ and $\nabla J_j(\theta)$. This decouples per-prompt optimization, so variance-based adaptive steps for one prompt do not conflict with those for others.
- Core assumption: Prompts have approximately orthogonal representations in the model's feature space (Assumption 2 or the relaxed Assumption 4). Empirical validation suggests this holds early and moderately during training.
- Evidence anchors:
  - [abstract] "interplay between feature orthogonality and reward variance."
  - [section 4.1, Figure 1] >90% of prompt pairs exhibit absolute cosine similarity < 0.15.
  - [corpus] Weak direct corpus evidence for GRPO-specific orthogonality; general gradient orthogonality is a known phenomenon in high-dimensional spaces.
- Break condition: If prompt representations become highly correlated, cross-prompt gradient interference grows, violating the decoupling assumption and potentially destabilizing adaptive scaling.

### Mechanism 3
- Claim: Training unfolds in three distinct phases where the marginal benefit of normalization varies.
- Mechanism: Phase I (early): High variance and orthogonality exist, but noisy gradients obscure gains. Phase II (mid): Moderate variance and stable cross-prompt interactions create ideal conditions for curvature-adaptive scaling, maximizing convergence speedup. Phase III (late): Diminished orthogonality and reduced variance limit further acceleration.
- Core assumption: Training dynamics evolve such that curvature-variance correlation and feature orthogonality change predictably over time due to feature learning and policy saturation.
- Evidence anchors:
  - [abstract] "...mid-stage regime with moderate variance and stable cross-prompt interactions."
  - [section 4.4, Figure 3] Training accuracy curves on GSM8K show the widest performance gap in Phase II.
  - [corpus] "Uncalibrated Reasoning: GRPO Induces Overconfidence" (arXiv:2508.11800) notes trade-offs with calibration, hinting at phase-dependent effects.
- Break condition: If task heterogeneity is extreme or model architecture causes rapid orthogonality loss, the Phase II optimal window may be shortened or absent, altering the benefit profile.

## Foundational Learning

- Concept: **Policy Gradient (REINFORCE)**
  - Why needed here: GRPO is a REINFORCE variant; understanding the baseline gradient estimator $\nabla J(\theta) \approx \mathbb{E}[\nabla \log \pi_\theta(a|s) R]$ and its variance issues is critical.
  - Quick check question: Derive the REINFORCE gradient estimator for a trajectory return.

- Concept: **Adaptive Gradient Methods (AdaGrad/Adam)**
  - Why needed here: The paper interprets variance normalization as an adaptive step-size mechanism; familiarity with how such methods scale updates by historical gradient information clarifies the analogy.
  - Quick check question: In AdaGrad, how does the accumulated squared gradient $G_t$ affect the effective learning rate?

- Concept: **L-Smoothness and Lipschitz Continuity**
  - Why needed here: Theoretical analysis uses the Hessian bound $\|\nabla^2 J(\theta)\| \leq L$ to characterize curvature; a basic understanding of $L$-smooth functions is required.
  - Quick check question: For an $L$-smooth function $f$, state the inequality linking function decrease, gradient norm, and step size.

## Architecture Onboarding

- Component map:
  Input -> Prompts (questions) $q_i \in Q$ -> Policy $\pi_\theta$ (LLM) -> Group Sampler -> Advantage Computer -> Optimizer

- Critical path:
  1. Uniformly sample a prompt $q_i$.
  2. Sample $K$ responses $\{o_j\}$ from $\pi_\theta(\cdot|q_i)$.
  3. Compute rewards $\{r(q_i, o_j)\}$ (0 or 1).
  4. Compute group mean and standard deviation of rewards.
  5. Normalize each response's advantage by the group std.
  6. Calculate gradient and update parameters $\theta$.

- Design tradeoffs:
  - **Group size $K$**: Larger $K$ provides more stable mean/std estimates but increases compute cost per prompt.
  - **Variance floor ($\epsilon$)**: Adding a small $\epsilon$ to std prevents division by zero; too large a value blunts adaptive scaling.
  - **Clipping vs. Normalization**: Standard PPO clips importance ratios; GRPO relies primarily on variance normalization for stability, simplifying the pipeline.

- Failure signatures:
  - **Exploding loss**: If all rewards in a group are identical (std = 0), numerical instability occurs without a floor.
  - **No convergence gain**: If prompt difficulty is homogeneous (low variance heterogeneity), adaptive scaling offers little benefit.
  - **Early plateau**: If feature orthogonality degrades rapidly, Phase II gains may not materialize.

- First 3 experiments:
  1. **Ablation on GSM8K**: Train GRPO with vs. without std normalization on Easy/Hard splits to replicate the Phase II performance gap.
  2. **Orthogonality Decay Logging**: Log pairwise gradient cosine similarities at steps 0, 250, and 500 to validate the three-phase orthogonality pattern.
  3. **Variance-Curvature Correlation**: For a prompt subset, correlate reward std with a diagonal Fisher information proxy to verify the local curvature link.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims rely on log-linear policy assumptions and high-dimensional feature orthogonality that may not hold for modern LLMs
- Empirical validation limited to small-scale models (1.5B parameters) and specific benchmarks
- Three-phase training analysis is observational rather than causal, with phase transition conditions underspecified

## Confidence
- **High Confidence**: The adaptive gradient mechanism via variance normalization is well-supported theoretically (Lemma 3.1, Section 3.1) and empirically validated on GSM8K/MATH benchmarks.
- **Medium Confidence**: The orthogonality assumption and its impact on gradient interference is partially supported by gradient similarity measurements, but lacks rigorous testing across architectures.
- **Low Confidence**: The phase-based interpretation of training dynamics is primarily descriptive, with limited theoretical grounding for why orthogonality degrades predictably over time.

## Next Checks
1. Test the curvature-variance correlation on larger models (e.g., 7B+ parameters) to assess scalability of the adaptive mechanism.
2. Conduct ablation studies varying group size K and reward noise levels to quantify robustness to non-ideal conditions.
3. Implement gradient similarity tracking across multiple architectures (transformer variants) to validate the orthogonality assumption beyond the log-linear case.