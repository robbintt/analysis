---
ver: rpa2
title: Elementary Math Word Problem Generation using Large Language Models
arxiv_id: '2506.05950'
source_url: https://arxiv.org/abs/2506.05950
tags:
- generation
- mwps
- grade
- problem
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an MWP generation system based on Large Language
  Models (LLMs) that can generate MWPs belonging to a user-specified grade and question
  type. The system was developed through extensive experiments to identify the best
  open-source LLM and prompt template for MWP generation.
---

# Elementary Math Word Problem Generation using Large Language Models

## Quick Facts
- arXiv ID: 2506.05950
- Source URL: https://arxiv.org/abs/2506.05950
- Reference count: 34
- Primary result: System generates curriculum-aligned MWPs with 74.52% grade relevance and 78.76% section relevance

## Executive Summary
This paper presents a system for generating elementary math word problems (MWPs) using Large Language Models (LLMs), with specific focus on curriculum alignment to grade levels and question types. The system employs Llama-2 7B as the base model, fine-tuned with a manually created dataset containing grade and section metadata, and incorporates a secondary LLM for solvability verification. Through extensive experimentation with different LLMs, prompt templates, and decoding strategies, the authors develop a pipeline that produces high-quality MWPs with minimal spelling and grammar issues. The research also produces a 4K MWP dataset with comprehensive error annotation, claimed to be the only such corpus available in English.

## Method Summary
The system uses Llama-2 7B fine-tuned with QLoRA on a manually created dataset containing grade and section metadata. The pipeline includes a secondary LLM (Gemini) for solvability filtering, few-shot prompting with optimized examples, and decoder hyperparameter tuning using grid search. The approach combines fine-tuning, in-context learning, and verification steps to generate curriculum-aligned MWPs while maintaining diversity and correctness.

## Key Results
- Grade relevance: 74.52% (best model)
- Section relevance: 78.76% (best model)
- Solvability rate: 84.17% after applying solvability filter
- Generated corpus: 4K MWPs with comprehensive error annotation

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with curriculum-aligned data improves constraint adherence for grade and question-type requirements. Instruction tuning on a dataset containing MWPs labeled with grade and section metadata teaches the model to condition its generation on these constraints. Without such metadata in training data, the model lacks supervision signals for curriculum alignment.

### Mechanism 2
Few-shot prompting on a fine-tuned model amplifies adherence to grade and section constraints. In-context examples provide additional signal for the desired output format and constraint satisfaction, complementing the learned weights from fine-tuning. The combination yields higher constraint adherence than either technique alone.

### Mechanism 3
A secondary LLM acting as a solvability filter improves the proportion of valid, solvable MWPs in final output. The primary LLM generates MWPs; a secondary LLM (Gemini in this study) classifies each as solvable or unsolvable. Unsolvable MWPs are rejected and regenerated until the quota is filled.

## Foundational Learning

- **Instruction Tuning with QLoRA**
  - Why needed here: The system fine-tunes Llama-2 using QLoRA (Quantized Low-Rank Adaptation) to adapt the base model to MWP generation without full parameter updates.
  - Quick check question: Can you explain why LoRA reduces memory overhead compared to full fine-tuning?

- **Decoder Hyperparameters (Temperature, Top-k, Penalty Alpha)**
  - Why needed here: Controlling diversity in generated MWPs requires tuning decoding strategy; the paper uses grid search over top_k, penalty_alpha, and no_repeat_ngram_size.
  - Quick check question: What happens to output diversity if temperature is set very low (e.g., 0.1)?

- **Contrastive Search Decoding**
  - Why needed here: The paper identifies top_k=40, penalty_alpha=0.6, no_repeat_ngram_size=5 as optimal for balancing diversity and correctness.
  - Quick check question: How does penalty_alpha affect the trade-off between model confidence and degeneration penalty?

## Architecture Onboarding

- **Component map**: Llama-2 7B -> QLoRA fine-tuning -> Few-shot prompting -> MWP generation -> Gemini solvability checker -> Output filtering
- **Critical path**: 1) Select base LLM (zero-shot comparison across candidates) 2) Identify best prompt pattern (Persona, Template, Dialogue tested) 3) Tune decoder hyperparameters for diversity (grid search) 4) Fine-tune with curriculum-aligned data (MathWizards) 5) Apply few-shot prompting (1, 3, 5 shots tested) 6) Integrate solvability filter at inference time
- **Design tradeoffs**: 
  - Diversity vs. Correctness: Lower Self-BLEU indicates higher diversity but may reduce accuracy
  - Cost vs. Accuracy: Using a secondary LLM for solvability checking adds API costs and latency but improves output quality
  - Preference Optimization Methods: DPO and KTO delivered poor results; CPO showed improvement on some metrics but increased unsolvability
- **Failure signatures**:
  - Low grade/section relevance (~42-52% at baseline, ~74-79% best)
  - Unsolvable MWPs: Generated problems lacking sufficient information or containing contradictions
  - Unrealistic answers: Mathematically solvable but contextually nonsensical
  - Topic unsafety: Content inappropriate for students
- **First 3 experiments**:
  1. Reproduce model selection: Run zero-shot prompting with Prompt 1 on Llama-2 and Zephyr using the same evaluation criteria (12 error types)
  2. Ablate fine-tuning data: Fine-tune Llama-2 on MathInitial (no grade labels) vs. MathWizards (with grade/section labels)
  3. Validate solvability filter: Measure the trade-off between false negative rate and improvement in final output solvability

## Open Questions the Paper Calls Out

### Open Question 1
Can Retrieval Augmented Generation (RAG) techniques effectively bridge the gap in adhering to user-specified grade and question type requirements? The current best model achieves only 74.52% grade relevance and 78.76% section relevance, which the authors acknowledge is "far from optimal" despite fine-tuning and few-shot prompting.

### Open Question 2
How can the MWP generation system be adapted for multilingual contexts without compromising the error annotation quality? The paper relies heavily on manual verification and a specific error taxonomy optimized for English; applying this to other languages involves unstated linguistic complexities and resource availability.

### Open Question 3
Can the solvability verification step be refined to significantly reduce the high false positive rate (60%) observed with the secondary LLM? The current two-stage architecture still allows a majority of unsolvable questions to pass through the filter.

### Open Question 4
What architectural or data constraints caused Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) to fail in this specific task? Without understanding the failure mode of standard alignment techniques like DPO in this domain, it is difficult to predict if they will work for future iterations.

## Limitations
- Grade and section relevance rates remain suboptimal at 74.52% and 78.76% respectively, indicating persistent challenges with curriculum alignment
- Solvability filter introduces 11.76% false negative rate, increasing generation latency and computational costs
- Error annotation methodology lacks reported inter-annotator agreement metrics, raising questions about reliability

## Confidence
- **High Confidence**: Fine-tuning improves MWP quality compared to zero-shot; Llama-2 7B outperforms other tested LLMs
- **Medium Confidence**: Few-shot prompting amplifies constraint adherence; QLoRA represents optimal fine-tuning approach
- **Low Confidence**: Solvability filter consistently improves final output quality; generated corpus is the "only comprehensively error-annotated" English MWP corpus

## Next Checks
1. **Curriculum Alignment Robustness Test**: Generate MWPs across all 18 grade-section combinations using the fine-tuned model with and without few-shot prompting. Measure grade and section relevance rates to determine if performance varies significantly by category.

2. **Solvability Filter Cost-Benefit Analysis**: Implement the solvability filter pipeline and measure average regeneration cycles, total API cost and latency increase, and net improvement in final output solvability. Compare this to a threshold-based filtering approach.

3. **Error Annotation Reliability Assessment**: Have two additional annotators independently label a stratified sample of 200 generated MWPs using the 12 error categories. Calculate inter-annotator agreement using Cohen's kappa and identify which error categories show the lowest agreement.