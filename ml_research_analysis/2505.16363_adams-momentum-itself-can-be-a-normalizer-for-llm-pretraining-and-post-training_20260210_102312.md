---
ver: rpa2
title: 'AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training'
arxiv_id: '2505.16363'
source_url: https://arxiv.org/abs/2505.16363
tags:
- adams
- adamw
- learning
- arxiv
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdamS, a memory-efficient optimizer that
  eliminates the need for second-moment estimates by using a novel denominator based
  on the root of weighted sums of squares of momentum and current gradient. AdamS
  achieves the same memory and compute footprint as SGD with momentum while matching
  AdamW's optimization performance.
---

# AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training

## Quick Facts
- **arXiv ID**: 2505.16363
- **Source URL**: https://arxiv.org/abs/2505.16363
- **Reference count**: 40
- **Primary result**: Memory-efficient optimizer achieving AdamW performance with 50% memory reduction by eliminating second-moment estimates

## Executive Summary
AdamS is a novel optimizer for large language model training that eliminates the need for second-moment estimates (v_t) by using a novel denominator based on the root of weighted sums of squares of momentum and current gradient. This design achieves the same memory and compute footprint as SGD with momentum while matching AdamW's optimization performance. The optimizer is motivated by observed (L₀, L₁)-smoothness properties in transformer objectives where local smoothness is governed by gradient magnitudes, and it provides rigorous convergence guarantees under relaxed smoothness and weak noise assumptions.

## Method Summary
AdamS replaces AdamW's second-moment tracking (ν_t = β₂ν_{t-1} + (1-β₂)g²_t) with ν_t = β₂m²_{t-1} + (1-β₂)g²_t, where m_{t-1} is the momentum buffer already maintained. This eliminates the need for a separate v_t state, reducing optimizer memory by approximately 50%. The denominator √(ν_t + ε) provides per-coordinate normalization using only existing momentum state plus current gradient. The optimizer inherits AdamW's hyperparameters and requires only one change: β₂=0.95 instead of the typical 0.999.

## Key Results
- Achieves identical validation loss to AdamW on GPT-2 (125M-774M) and Llama2 (7B-13B) pretraining with 50% memory reduction
- Matches AdamW performance in DeepSeek R1-Zero RL post-training while using β₂=0.95 vs AdamW's 0.999
- Maintains optimal Ω(1/√T) convergence rate under (L₀, L₁)-smoothness and weak noise assumptions
- Provides 0-36% throughput gains depending on model scale due to reduced memory pressure

## Why This Works (Mechanism)

### Mechanism 1: Momentum-Gradient Denominator Replaces Second-Moment Tracking
AdamS eliminates the v_t buffer by computing ν_t = β₂m²_{t-1} + (1-β₂)g²_t, using momentum as a proxy for gradient magnitude. This reduces memory by ~50% while maintaining adaptive learning rates. The mechanism assumes momentum magnitude serves as a stable proxy for gradient magnitude, particularly effective in large-batch regimes where gradient noise is small relative to signal.

### Mechanism 2: (L₀, L₁)-Smoothness Justifies Gradient-Norm Proportional Learning Rates
Transformer objectives exhibit local smoothness bounded by gradient magnitude (∇f(w₁)-∇f(w₂) ≤ (L₀ + L₁‖∇f(w₁)‖)‖w₁-w₂‖), making inverse-gradient-norm scaling theoretically optimal. AdamS's denominator approximates this by normalizing by √(β₂m² + (1-β₂)g²), which tracks gradient magnitude without separate second-moment storage.

### Mechanism 3: Large-Batch Regime Reduces AdamS-AdamW Divergence
AdamS's behavior converges toward AdamW's as batch size increases and gradient noise becomes negligible relative to gradient mean. Analytical comparison shows that when μ ≫ σ (large batch regime), E[V_∞] for AdamS ≈ E[S_∞] for AdamW, with variance differences minimizing at β≈0.95.

## Foundational Learning

- **Exponential Moving Average (EMA) / Momentum**: AdamS relies on momentum m_t = β₁m_{t-1} + (1-β₁)g_t as a smoothed gradient estimate; the algorithm repurposes this buffer for normalization. Quick check: If β₁=0.9 and gradients [1.0, 0.5, 1.5] over three steps (starting from m₀=0), what is m₃? (Answer: 0.276)

- **Adaptive Learning Rates**: AdamS provides per-parameter learning rates η/√(ν_t + ε), beneficial for transformers where different layers/parameters have different gradient magnitudes and curvature. Quick check: Why might a 1B-parameter transformer benefit from per-parameter learning rates? (Answer: Different layers/parameters have different gradient magnitudes and curvature; adaptive rates prevent overshooting in high-gradient regions while avoiding stagnation in low-gradient regions.)

- **Memory Footprint in Distributed Training**: AdamS's value proposition is 50% optimizer-state memory reduction. Quick check: In FSDP with 4 GPUs training a 7B model, roughly how much memory does AdamW's optimizer state consume per GPU versus AdamS? (Answer: AdamW stores m_t and v_t ≈ 2× model params per GPU; AdamS stores only m_t ≈ 1× model params, so ~14GB vs ~7GB in fp16.)

## Architecture Onboarding

**Component map**: gradients g_t -> momentum m_{t-1} (existing) -> denominator ν_t = β₂m²_{t-1} + (1-β₂)g²_t -> parameter update Δw_t

**Critical path**:
1. Gradient computation (backprop)
2. Momentum update: m_t ← β₁m_{t-1} + (1-β₁)g_t
3. Denominator computation: ν_t ← β₂m²_{t-1} + (1-β₂)g²_t
4. Parameter update: w_t ← (1-ηλ)w_{t-1} - η · m_t / √(ν_t + ε)

**Design tradeoffs**:
- β₂ sensitivity: Paper recommends β₂=0.95; values >0.99 destabilize training
- β₁, β₂ coupling: When both large (>0.95, >0.98), validation loss degrades significantly
- Memory vs throughput: Memory guaranteed 50% reduction; throughput gains depend on whether memory or compute is bottleneck

**Failure signatures**:
- Loss spikes early: Check if β₂ is too high (>0.99)
- Convergence slower than AdamW: Verify batch size is large enough
- Memory not reduced: Ensure v_t buffer is not being allocated
- Divergence in RL: Paper uses β₂=0.95 (not 0.999) for stability

**First 3 experiments**:
1. **GPT-2 Small parity check**: Train GPT-2 Small on OpenWebText for 100K steps with AdamS vs AdamW using identical hyperparameters. Compare validation loss curves; target difference <0.02.
2. **Memory profiling**: Measure peak GPU memory and optimizer state size for AdamW vs AdamS on GPT-2 Large (770M) with batch size 480. Target: AdamS optimizer state = 50% of AdamW.
3. **Hyperparameter sensitivity sweep**: Run ablation over (β₁, β₂) ∈ {(0.9, 0.95), (0.9, 0.99), (0.95, 0.95), (0.95, 0.98)} on GPT-2 Small for 50K steps. Identify stability boundaries.

## Open Questions the Paper Calls Out

**Open Question 1**: Does AdamS maintain its convergence properties and memory advantages when scaling to models with over 100 billion parameters? The authors note this remains critical for confirming scalability in production-grade pipelines, but empirical validation was limited to 7B and 13B models due to computational budget constraints.

**Open Question 2**: How does AdamS perform on sparse or emerging architectures such as Mixture of Experts (MoE)? The paper explicitly identifies MoE architectures as requiring future validation, as they introduce gradient sparsity and noise patterns that may challenge the momentum-based normalization assumption.

**Open Question 3**: Does the requirement for a lower β₂ (e.g., 0.95) undermine the claim of seamless hyperparameter inheritance from AdamW (which typically uses β₂ ≈ 0.999)? There is tension between the "drop-in replacement" claim and the empirical need to lower β₂ to prevent the momentum-normalizer from over-dampening updates.

## Limitations
- Theoretical analysis assumes (L₀, L₁)-smoothness of transformer objectives, which is validated in prior work but not explicitly verified for the specific architectures tested
- Small-batch regime performance is not thoroughly evaluated, though acknowledged as a potential failure mode
- Long-term stability beyond validation checkpoints is difficult to assess due to limited training curve data

## Confidence
- **High Confidence**: Memory reduction claims (50% optimizer state reduction verified through direct state dictionary inspection), large-batch performance parity with AdamW, RL post-training results
- **Medium Confidence**: Theoretical convergence guarantees under (L₀, L₁)-smoothness (depends on unverified smoothness of specific transformer architectures), pretraining performance across different model scales
- **Low Confidence**: Small-batch regime behavior, long-term stability beyond validation checkpoints, robustness to extreme hyperparameter settings not explored in ablation

## Next Checks
1. **Small-Batch Regime Testing**: Run AdamS and AdamW on GPT-2 Small with batch sizes of 32, 64, and 128 to quantify divergence when μ ≈ σ. Measure validation loss gap at 50K iterations and check for instability symptoms.

2. **Architect-specific Smoothness Validation**: Compute empirical (L₀, L₁)-smoothness parameters for the Llama2 13B model layers during training. Plot ∇f(w₁)-∇f(w₂) versus ‖w₁-w₂‖ and ‖∇f(w₁)‖ across multiple checkpoints to verify the theoretical assumption holds for these specific transformer configurations.

3. **Memory Breakdown and Communication Overhead**: Using FSDP with AdamS and AdamW on a 7B model across 8 GPUs, measure: (a) peak per-GPU memory usage, (b) allreduce communication volume per step, (c) optimizer state sharding effectiveness. Verify the 50% reduction holds in distributed settings and quantify throughput impact from reduced communication.