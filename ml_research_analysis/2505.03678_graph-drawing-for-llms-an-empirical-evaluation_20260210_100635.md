---
ver: rpa2
title: 'Graph Drawing for LLMs: An Empirical Evaluation'
arxiv_id: '2505.03678'
source_url: https://arxiv.org/abs/2505.03678
tags:
- graph
- zero
- modality
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Large Language Models (LLMs)
  for graph-related tasks using visual representations. It explores how layout paradigms
  (straight-line vs orthogonal drawings), prompting techniques, and drawing quality
  impact LLM performance.
---

# Graph Drawing for LLMs: An Empirical Evaluation

## Quick Facts
- arXiv ID: 2505.03678
- Source URL: https://arxiv.org/abs/2505.03678
- Reference count: 40
- Key outcome: Investigates how layout paradigms, prompting techniques, and drawing quality impact LLM performance on graph-related tasks using visual representations

## Executive Summary
This paper explores how Large Language Models (LLMs) perform graph-related tasks when provided with visual representations. The authors conduct three controlled experiments on small synthetic graphs (up to 15 nodes) to evaluate tasks like finding common neighbors, shortest paths, maximum cliques, and minimum vertex covers. They systematically compare different input modalities (textual, visual, mixed), prompting techniques (standard, chain-of-thought, and a new "spell-out adjacency list" method), and manually optimized drawings based on human readability metrics.

The study reveals that layout paradigms significantly impact LLM performance: orthogonal drawings excel at local tasks requiring edge readability, while straight-line drawings better support complex global tasks through improved structural unfolding. Drawing quality improvements based on symmetry and edge crossing minimization substantially enhance LLM accuracy. These findings demonstrate that careful selection of visualization approaches and optimization strategies can significantly boost LLM performance on graph-related tasks, though the results are currently limited to small synthetic graphs.

## Method Summary
The authors designed three experiments using small synthetic graphs (up to 15 nodes) to evaluate LLM performance on graph-related tasks. They compared four input modalities (textual, visual, mixed, and adjacency list with node labels), three prompting techniques (standard, chain-of-thought, and a novel "spell-out adjacency list" approach), and manually optimized drawings. The experiments covered local tasks (common neighbors), path-finding tasks (shortest paths), and complex tasks (maximum cliques, minimum vertex covers). Drawing quality was manually optimized based on human readability metrics including symmetry and edge crossings. LLM performance was measured by accuracy across these different conditions.

## Key Results
- Orthogonal drawings outperform straight-line drawings for local tasks requiring edge readability, while straight-line drawings better support complex global tasks through structural unfolding
- No single prompting technique is universally superior, though chain-of-thought and "spell-out adjacency list" methods show promise
- Manual optimization of drawing quality based on symmetry and edge crossing metrics significantly improves LLM performance

## Why This Works (Mechanism)
The paper's findings are grounded in how visual representations affect LLM reasoning about graph structures. Orthogonal drawings provide clearer edge visibility for local queries by reducing edge crossings and improving symmetry, making it easier for LLMs to trace direct connections. Straight-line drawings, while potentially more cluttered for local tasks, better reveal the overall graph topology and long-range relationships needed for complex global tasks. The prompting techniques influence how LLMs parse and process the visual information, with chain-of-thought enabling step-by-step reasoning and the "spell-out adjacency list" providing explicit structural information. Manual optimization of drawings based on human readability principles translates to better machine interpretability, suggesting that visual design principles beneficial to humans may also benefit LLMs.

## Foundational Learning
**Graph Drawing Paradigms**: Understanding different layout approaches (straight-line vs orthogonal) and their visual properties is crucial for evaluating how presentation affects LLM performance. Quick check: Compare edge readability and structural clarity in both layouts for sample graphs.

**Prompt Engineering**: Different prompting strategies (standard, chain-of-thought, structured output) significantly impact LLM reasoning quality. Quick check: Test same graph task with different prompt formats to observe performance variations.

**Drawing Quality Metrics**: Symmetry and edge crossing minimization are key readability factors that translate to machine interpretability. Quick check: Measure accuracy changes when applying basic layout optimization algorithms.

## Architecture Onboarding

Component Map: Input Modality (Text/Visual/Mixed) -> Prompting Technique (Standard/Cot/SOAL) -> Drawing Quality (Unoptimized/Optimized) -> LLM Task Performance

Critical Path: Visual representation selection → Layout optimization → Prompt design → LLM inference → Task accuracy measurement

Design Tradeoffs: Orthogonal layouts offer better edge readability but may obscure global structure; straight-line layouts reveal topology but can be cluttered. Prompt complexity vs. inference efficiency trade-offs exist between simple prompts and chain-of-thought approaches.

Failure Signatures: Poor performance on local tasks with straight-line layouts indicates edge visibility issues; failures on complex tasks with orthogonal layouts suggest insufficient structural context. Inconsistent prompting may cause reasoning errors.

First Experiments:
1. Test orthogonal vs straight-line layouts on simple path-finding tasks to validate local vs global performance differences
2. Compare standard vs chain-of-thought prompting on maximum clique identification to measure reasoning depth impact
3. Apply basic layout optimization algorithms to synthetic graphs and measure accuracy improvements over unoptimized drawings

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to small synthetic graphs (up to 15 nodes), raising scalability concerns for real-world applications
- Manual drawing optimization based on human readability metrics may not scale to automated systems
- Evaluation focuses solely on accuracy without considering response time or computational efficiency

## Confidence
- Drawing quality improvements: High confidence due to controlled experiments and clear performance gains
- Layout paradigm recommendations: Medium confidence given limited graph sizes and synthetic nature
- Prompting technique effectiveness: Medium confidence requiring further validation across diverse graph types

## Next Checks
1. Test the same hypotheses on larger graphs (50+ nodes) from real datasets to assess scalability
2. Automate the drawing optimization process using existing graph drawing algorithms and compare against manual optimization
3. Evaluate the computational overhead and response time trade-offs when using optimized layouts versus standard approaches