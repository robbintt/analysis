---
ver: rpa2
title: Reward-free Alignment for Conflicting Objectives
arxiv_id: '2602.02495'
source_url: https://arxiv.org/abs/2602.02495
tags:
- alignment
- objectives
- preference
- optimization
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multi-objective alignment in large language
  models where conflicting objectives like helpfulness and harmlessness create optimization
  challenges. The proposed RACO framework directly leverages pairwise preference data
  and resolves gradient conflicts using a clipped variant of conflict-averse gradient
  descent (CAGrad-Clip).
---

# Reward-free Alignment for Conflicting Objectives

## Quick Facts
- arXiv ID: 2602.02495
- Source URL: https://arxiv.org/abs/2602.02495
- Reference count: 40
- One-line primary result: Reward-free multi-objective alignment using CAGrad-Clip achieves consistent Pareto improvements over baselines on summarization and safety tasks.

## Executive Summary
The paper addresses the challenge of aligning large language models with multiple conflicting objectives (e.g., helpfulness vs. harmlessness, quality vs. conciseness) without explicit reward modeling. The proposed Reward-free Alignment for Conflicting Objectives (RACO) framework leverages pairwise preference data and resolves gradient conflicts using a clipped variant of conflict-averse gradient descent (CAGrad-Clip). By avoiding explicit reward modeling and directly optimizing for Pareto improvements, the method achieves better trade-offs between competing objectives while maintaining theoretical convergence guarantees.

## Method Summary
RACO implements multi-objective alignment by computing per-objective gradients from DPO-style pairwise preference losses, then resolving conflicts through a dual optimization problem. The key innovation is CAGrad-Clip, which solves for a descent direction that improves all objectives while respecting user-specified weight priorities through a clipping mechanism. The method uses closed-form solutions for two objectives and quadratic programming for more, with convergence guarantees to Pareto-critical points and provable acceleration in the two-objective case.

## Key Results
- CAGrad-Clip achieves 3-5% absolute improvement in GPT-5.1 win rates over DPO-LW and AMoPO baselines
- Consistent Pareto frontier dominance across Qwen 3, Llama 3, and Gemma 3 model families
- Theoretical convergence guarantees to Pareto-critical points with provable acceleration for two objectives
- Optimal correction radius c varies by model family: 0.4 for instruct models, 0.7-0.8 for base models

## Why This Works (Mechanism)

### Mechanism 1: Conflict-Averse Gradient Direction Computation
Standard weighted gradient aggregation fails when objective gradients conflict, as no single direction simultaneously improves all objectives. CAGrad solves for an update direction that maximizes worst-case local improvement across objectives within a constrained radius of the weighted gradient. Formally, it finds `d_k` that minimizes `max_i ∇f_i(θ_k)^T d` subject to `||d - g_0|| ≤ c||g_0||`. This approach is tractable because the dual problem has dimension m (number of objectives) rather than the parameter count.

### Mechanism 2: Preference-Weighted Clipping for Trade-off Preservation
Vanilla CAGrad can over-correct toward low-priority objectives in high-dimensional LLM policy spaces, violating user-specified trade-offs. CAGrad-Clip constrains correction coefficients to never exceed user-specified weights: `p̃_i ← min{p_i, w_i}`. This prevents the dual solution from upweighting less-preferred objectives beyond their assigned importance. The clipping is particularly effective at weight extremes (0.8/0.2), providing 1-3% Pareto frontier improvement where vanilla CAGrad over-corrects.

### Mechanism 3: Pareto-Critical Convergence via Descent Guarantee
The clipped update rule converges to Pareto-critical points that are also critical points of the weighted loss. The update direction `G_0 = g_0 + c||g_0||u` maintains a descent guarantee: `L_w(θ_{t+1}) ≤ L_w(θ_t) - η||g_0||²Γ(ρ_t)` where `Γ(ρ) ≥ (1-c²)/2`. Convergence follows from summing this telescoping bound, requiring only that each objective loss has Lipschitz gradients and step size satisfies `η < 1/ℓ_w`.

## Foundational Learning

- **Concept**: Pareto Optimality vs. Scalarization
  - Why needed here: Multi-objective alignment cannot simply average objectives—conflicting gradients mean weighted sums privilege some objectives. Pareto-critical points ensure no objective can improve without degrading another.
  - Quick check question: Given two loss gradients `g_1 = [1, 0]` and `g_2 = [0, 1]`, why does `0.5*g_1 + 0.5*g_2 = [0.5, 0.5]` fail to improve both simultaneously?

- **Concept**: Dual Formulation of CAGrad Subproblem
  - Why needed here: The primal constraint `||d - g_0|| ≤ c||g_0||` is solved via a dual over m variables (not d parameters), making it tractable for LLM-scale parameter spaces.
  - Quick check question: If you have 7B parameters and 2 objectives, what is the dimension of the dual optimization problem?

- **Concept**: DPO Loss as Preference Gradient
  - Why needed here: RACO operates directly on DPO-style pairwise preference losses without explicit reward models. Each objective induces `L_i(θ) = -E[log σ(β(log π_θ(y^+_i|x)/π_ref(y^+_i|x) - log π_θ(y^-_i|x)/π_ref(y^-_i|x)))]`.
  - Quick check question: Why does the DPO loss gradient depend on the reference policy `π_ref`?

## Architecture Onboarding

- **Component map**: Input datasets → Per-objective DPO gradients → Dual problem solver → Clipping module → Parameter update
- **Critical path**:
  1. Sample minibatch of preference pairs across all objectives
  2. Compute per-objective gradients via backprop through DPO losses
  3. Solve dual problem for correction weights `p`
  4. Apply clipping: `p̃ ← min{p, w}` elementwise
  5. Compute final direction: `G_0 = g_0 + c||g_0||·(G_p̃ / ||G_p̃||)`
  6. Update parameters: `θ_{t+1} ← θ_t - η G_0`

- **Design tradeoffs**:
  - Larger c → more aggressive conflict correction, but risk of instability if gradients are noisy
  - Smaller c → conservative updates closer to weighted gradient, slower convergence
  - Paper uses c=0.4 for Qwen/Llama instruct, c=0.7-0.8 for base models (which have noisier gradients)
  - Clipping vs. no clipping: ~1-3% Pareto frontier improvement at weight extremes (0.8/0.2), negligible at balanced weights

- **Failure signatures**:
  - Training instability: If `||G_p̃||` approaches zero (rare), fallback to uncorrected gradient
  - Weight violation: If unclipped CAGrad assigns `p_i > w_i` for low-priority objectives, check clipping is implemented correctly
  - No improvement over DPO-LW: Likely c is too small or objectives are already aligned (gradients colinear)

- **First 3 experiments**:
  1. **Sanity check**: On synthetic 2-objective data with fully conflicting gradients, verify CAGrad-Clip finds a direction that improves both margins while DPO-LW only improves one
  2. **Ablation on c**: Sweep `c ∈ {0.2, 0.4, 0.6, 0.8}` on Reddit summarization with `w_qual = 0.8`. Expect: larger c improves quality more but reduces conciseness improvement
  3. **Weight sweep**: Generate Pareto frontier with `w ∈ {0.2, 0.35, 0.5, 0.65, 0.8}` on BeaverTails safety alignment. Compare frontier dominance against AMoPO and DPO-LW baselines

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the convergence rate acceleration guarantee for CAGrad-Clip extend beyond the two-objective case to settings with three or more conflicting objectives?
- **Basis in paper**: Theorem 3.2 states: "under the case of two conflicting objectives, we further provide a theoretical analysis that clipping can—counterintuitively—accelerate training." The proof is explicitly restricted to m=2.
- **Why unresolved**: The theoretical analysis exploits structure specific to two objectives (Lemma B.5 analyzes ratios p₁/p₂), and the authors do not provide or claim analogous guarantees for m>2.
- **What evidence would resolve it**: A theoretical extension of Theorem 3.2 to general m≥2, or empirical demonstration that acceleration degrades/holds as the number of objectives increases.

### Open Question 2
- **Question**: Can the clipping strategy in CAGrad-Clip be derived from a principled optimization-theoretic objective rather than as a heuristic stabilization technique?
- **Basis in paper**: The abstract states "we improve our method using some heuristics" and the method section describes clipping as "a simple yet practical technique." Theoretical justification is post-hoc.
- **Why unresolved**: Clipping is motivated empirically (preventing over-correction) and theoretically verified to preserve convergence, but not derived as the optimal solution to any formal problem.
- **What evidence would resolve it**: A derivation showing clipping emerges from optimizing a well-defined objective (e.g., a constrained or regularized variant of the CAGrad subproblem).

### Open Question 3
- **Question**: Is there a systematic, transferable method for selecting the correction radius c across different model families and objective configurations?
- **Basis in paper**: The paper reports different optimal c values per model family (c=0.4 for Qwen/Llama instruct, c=0.7 for Gemma instruct, c=0.8 for base models) but provides no principled selection criteria beyond ablation sweeps.
- **Why unresolved**: The c parameter controls the trade-off between conflict resolution aggressiveness and stability, yet its optimal value appears to depend on unspecified model or data properties.
- **What evidence would resolve it**: A theoretical or empirical rule relating c to observable quantities (e.g., gradient noise scale, objective conflict degree, model size).

## Limitations

- Empirical scope limited to 4B-8B models and synthetic/Reddit datasets, unclear how method scales to frontier models (>100B parameters)
- Clipping mechanism's theoretical benefit is most pronounced in two-objective settings, with diminishing returns as objective count increases
- Assumes pairwise preferences are available for all objectives, but collecting such data for harmlessness or complex trade-offs can be expensive or infeasible

## Confidence

- **High Confidence**: The theoretical convergence guarantees to Pareto-critical points (Theorem 3.1) and one-step descent bound for CAGrad-Clip (Theorem 3.2) are mathematically rigorous under stated smoothness assumptions.
- **Medium Confidence**: The empirical Pareto frontier improvements over baselines (DPO-LW, AMoPO) are consistently demonstrated across model families, though the absolute magnitude of gains (3-5% in win rates) suggests CAGrad-Clip is incremental rather than transformative.
- **Low Confidence**: The claim that "no explicit reward modeling is required" understates practical considerations—pairwise preference data collection remains a bottleneck, and the method's robustness to noisy or biased preference labels is untested.

## Next Checks

1. **Scale Test**: Apply CAGrad-Clip to Llama 3.1 70B-Instruct on the same multi-objective summarization task. Verify if the clipped method maintains Pareto improvements as parameter count increases, or if gradient noise necessitates smaller c values.

2. **Objective Count Stress Test**: Extend experiments to 3+ objectives (e.g., quality vs. conciseness vs. faithfulness) and measure whether clipping still provides measurable Pareto frontier improvements. This validates the scalability claim beyond the two-objective case where theoretical acceleration is proven.

3. **Noise Sensitivity Analysis**: Introduce controlled noise into preference labels (e.g., flip 10%, 20%, 30% of conciseness preferences) and measure CAGrad-Clip's robustness compared to vanilla weighted aggregation. This tests whether conflict resolution amplifies label noise in high-dimensional spaces.