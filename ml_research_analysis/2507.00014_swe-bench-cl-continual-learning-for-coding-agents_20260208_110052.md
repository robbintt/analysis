---
ver: rpa2
title: 'SWE-Bench-CL: Continual Learning for Coding Agents'
arxiv_id: '2507.00014'
source_url: https://arxiv.org/abs/2507.00014
tags:
- learning
- tasks
- swe-bench-cl
- task
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SWE-Bench-CL, a novel continual learning
  benchmark for evaluating AI coding agents on realistic software engineering tasks.
  Built on the human-verified SWE-Bench Verified dataset, it organizes GitHub issues
  into chronological sequences that reflect natural repository evolution, enabling
  direct assessment of an agent's ability to accumulate experience, transfer knowledge,
  and resist catastrophic forgetting.
---

# SWE-Bench-CL: Continual Learning for Coding Agents

## Quick Facts
- **arXiv ID**: 2507.00014
- **Source URL**: https://arxiv.org/abs/2507.00014
- **Reference count**: 10
- **Primary result**: Introduces SWE-Bench-CL, a continual learning benchmark for coding agents with specialized metrics including CL-Fβ score and semantic memory module

## Executive Summary
SWE-Bench-CL introduces a novel continual learning benchmark for evaluating AI coding agents on realistic software engineering tasks. Built on the human-verified SWE-Bench Verified dataset, it organizes GitHub issues into chronological sequences that reflect natural repository evolution, enabling direct assessment of an agent's ability to accumulate experience, transfer knowledge, and resist catastrophic forgetting. The authors provide a preliminary analysis showing low inter-task structural similarity and high contextual sensitivity, motivating the need for sophisticated memory systems. They propose an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module and introduce specialized continual learning metrics including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score.

## Method Summary
SWE-Bench-CL builds on SWE-Bench Verified to create 8 repository sequences (273 tasks total) with chronological task ordering. The framework uses a LangGraph-based agent with AgentState tracking, ACI tools (find_file, search, file_viewer, edit with flake8 linting, run_tests), and FAISS-backed semantic memory for retrieving similar past experiences. After each task, agents store task summaries and re-evaluate all prior tasks to compute performance metrics. The system introduces specialized CL metrics including CL-Fβ (harmonic mean of CL-Plasticity and CL-Stability) and a generalized Composite CL-Score with λ weights.

## Key Results
- Preliminary analysis shows low inter-task structural similarity (cosine similarity 0.0-0.6) and high contextual sensitivity to irrelevant context
- Custom LangGraph-based evaluation framework resolves incompatibility issues with standard SWE-Bench harness (pass rates <8.5%)
- Proposed semantic memory module stores vectorized task summaries for retrieval-augmented generation
- Novel CL-Fβ score operationalizes stability-plasticity trade-off with tunable β parameter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chronological task sequencing exposes continual learning capabilities that static benchmarks cannot measure.
- Mechanism: By organizing GitHub issues into temporally ordered sequences reflecting natural repository evolution, the benchmark creates dependencies between tasks that require agents to accumulate experience, transfer knowledge forward, and resist catastrophic forgetting—properties invisible in one-shot evaluation.
- Core assumption: Real-world software engineering skill correlates with the ability to learn sequentially from evolving codebases.
- Evidence anchors:
  - [abstract] "By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting."
  - [Section 1] "These benchmarks often require only one-step retrieval or generation and employ evaluation metrics (e.g., BLEU, Exact Match, pass@k) that do not quantify an agent's ability to learn continuously or transfer knowledge effectively across related tasks."
  - [corpus] Weak direct validation; neighbor papers (SWE-PolyBench, OmniCode) address multi-language and task diversity but not temporal sequencing specifically.
- Break condition: If tasks within sequences prove highly structurally similar, sequential ordering may not meaningfully test continual learning—just pattern matching.

### Mechanism 2
- Claim: Semantic memory retrieval can mitigate catastrophic forgetting but introduces context-sensitivity risks.
- Mechanism: The FAISS-backed memory module stores vectorized task summaries (problem, solution, rationale, tool usage, success status). When facing a new task, the agent retrieves semantically similar past experiences and prepends them to the prompt, potentially providing useful patterns for transfer.
- Core assumption: Semantic similarity correlates with solution transferability across structurally dissimilar tasks.
- Evidence anchors:
  - [abstract] "...an interactive, LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module..."
  - [Section 4.2] "even structurally dissimilar prompts from easier tasks induced consistently high semantic drift (average ≈ 0.45) in solutions for more difficult target tasks"
  - [Section 6.3] "When initiating a new task, the agent queries memory using the current task's problem statement/hints, prioritizing experiences from the same task sequence."
  - [corpus] Neuroscience-Inspired Memory Replay paper discusses replay-based CL strategies but in generative rather than retrieval context; not direct validation.
- Break condition: If retrieval quality is poor or irrelevant memories are retrieved, "prompt poisoning" effects may degrade rather than improve performance.

### Mechanism 3
- Claim: The CL-Fβ score operationalizes the stability-plasticity trade-off for agent evaluation.
- Mechanism: CL-Plasticity (CL-P) measures immediate proficiency on new tasks; CL-Stability (CL-S) measures retention via 1 − Forgetting. The harmonic mean (CL-F1) penalizes imbalance; β parameter allows tuning toward stability (β > 1) or plasticity (β < 1).
- Core assumption: A single composite score can meaningfully capture the multi-dimensional trade-off inherent in continual learning.
- Evidence anchors:
  - [abstract] "...a suite of specialized continual learning metrics—including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized novel Composite Continual-Learning-Score and CL-Fβ score—to capture the stability-plasticity trade-off."
  - [Section 7.1] "CL-F1 score is the harmonic mean of CL-Plasticity and CL-Stability... The harmonic mean penalizes imbalance."
  - [corpus] No direct corpus validation; CL-Fβ is a novel proposal in this paper.
- Break condition: If λ weights are poorly calibrated, the composite score may obscure rather than reveal meaningful performance differences.

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed here: The core problem SWE-Bench-CL addresses—agents losing proficiency on earlier tasks after learning new ones.
  - Quick check question: Can you explain why standard LLM fine-tuning on new code tasks might degrade performance on previously learned patterns?

- Concept: **Stability-Plasticity Dilemma**
  - Why needed here: The theoretical framing for CL-Fβ; agents must be stable enough to retain knowledge yet plastic enough to adapt.
  - Quick check question: What happens if an agent has high plasticity but low stability? High stability but low plasticity?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The semantic memory system is fundamentally a RAG architecture applied to task history rather than external documents.
  - Quick check question: How does retrieving past task solutions differ from retrieving documentation, in terms of potential for negative transfer?

## Architecture Onboarding

- Component map:
  - Dataset Layer: 8 repository sequences (273 tasks total), JSON format with metadata (base_commit, difficulty, dependencies)
  - Agent Layer: LangGraph-based state machine with AgentState (Pydantic model), model-agnostic LLM interface
  - ACI Layer: Tools—find_file, search, file_viewer (windowed), edit (with flake8 linting), run_tests
  - Memory Layer: FAISS vector index + embeddings (text-embedding-3-small or nomic-embed-text); stores task summaries, retrieves by semantic similarity
  - Metrics Layer: Performance matrix a_{i,j}, ACC, F, FT, BWT, AULC, TUE, CL-Fβ

- Critical path:
  1. Clone/reset repository to task's base_commit
  2. Query memory with current problem statement → retrieve top-k similar past experiences
  3. Agent loop: DISCUSSION → COMMAND → tool execution → observation → state update
  4. On termination: store task summary (problem, solution, rationale, tool usage, success status) to FAISS
  5. After each task, re-evaluate on all prior tasks to compute performance matrix

- Design tradeoffs:
  - **Curriculum ordering vs. chronological only**: Paper uses hybrid (chronological + difficulty); curriculum may improve learning but reduces realism
  - **Memory retrieval threshold**: Low threshold → more context but higher prompt poisoning risk; high threshold → sparser but more relevant retrieval
  - **β parameter in CL-Fβ**: β > 1 prioritizes stability (safety-critical code); β < 1 prioritizes plasticity (rapid prototyping)

- Failure signatures:
  - **Harness mismatch errors**: Static SWE-Bench harness fails on SWE-Bench-CL tasks due to container/version misalignment (Section 5 reports pass rates <8.5%)
  - **Memory pollution**: If early tasks fail, garbage solutions populate memory, degrading subsequent retrieval quality ("garbage-in, garbage-out")
  - **Silent evaluation failures**: Patch format mismatches cause tests to not run correctly without clear error signals

- First 3 experiments:
  1. **Memory ablation**: Run agent with memory enabled vs. disabled on same sequence; measure ACC, FT, CL-Fβ difference. Hypothesis: memory improves ACC and CL-Stability.
  2. **Retrieval quality sensitivity**: Vary top-k (1, 3, 5, 10) and similarity threshold; measure semantic drift and task success rate. Hypothesis: moderate k (3-5) with high threshold balances relevance and coverage.
  3. **Stability-plasticity trade-off mapping**: Run multiple β values (0.5, 1.0, 2.0) in CL-Fβ optimization; identify which β best distinguishes memory-enabled from memory-disabled agents. Hypothesis: β > 1 will show larger separation due to memory's primary benefit on stability.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the proposed FAISS-backed semantic memory module yield a statistically significant improvement in the Composite Continual Learning Score (CL-Score) and Forward Transfer compared to memory-disabled agents?
  - Basis in paper: [explicit] Section 8 states, "It is hypothesized that memory will improve overall accuracy, yield positive forward transfer, and enhance tool-use efficiency," and outlines the protocol to test this.
  - Why unresolved: The paper describes the framework and metrics in detail, but notes that "full empirical evaluation... is ongoing," leaving the validation of the memory system's efficacy as future work.
  - What evidence would resolve it: Experimental results comparing CL-Score, Accuracy, and Forgetting metrics for agents with and without the semantic memory module across the eight repository sequences.

- **Open Question 2**: To what extent does the high "prompt poisoning" sensitivity identified in the analysis undermine the reliability of retrieval-augmented memory in this benchmark?
  - Basis in paper: [inferred] Section 4.2 demonstrates that unrelated context causes high semantic drift, and Section 6.3 proposes a memory system, but the paper does not confirm if the proposed retrieval mechanism successfully filters out the identified "poisonous" irrelevant context.
  - Why unresolved: The authors identify a vulnerability where naive retrieval degrades performance, but they have not yet empirically demonstrated that their specific semantic retrieval strategy mitigates this drift.
  - What evidence would resolve it: An ablation study measuring the correlation between retrieved memory relevance scores and the resulting semantic drift or task failure rates.

- **Open Question 3**: Does the custom LangGraph-based evaluation framework resolve the "inconclusive and unreliable results" caused by the static SWE-Bench harness's incompatibility with derived datasets?
  - Basis in paper: [explicit] Section 5 details how the standard harness failed due to container mismatches and low pass rates, explicitly motivating the development of the custom framework described in Section 6.
  - Why unresolved: The paper proposes the interactive agentic framework as a solution to the alignment and reliability issues but does not provide comparative data proving the new framework produces stable, interpretable results where the old one failed.
  - What evidence would resolve it: Successful execution of the proposed experimental protocol with interpretable agent trajectories and without the "opaque error messages" or harness execution failures cited in Section 5.

## Limitations
- Agent architecture transparency: Key implementation details (LangGraph topology, AgentState schema, prompt templates) are not fully specified
- Memory system calibration: FAISS configuration parameters and relevance gating mechanisms are unspecified
- Computational scalability: Sequential re-evaluation of all prior tasks after each new task is computationally expensive

## Confidence

- **High Confidence**: Need for continual learning evaluation in software engineering is well-established (SWE-Bench Verified's human-verified patches provide strong evidence anchor). Framework design is logically sound.
- **Medium Confidence**: Semantic memory mechanism's effectiveness depends heavily on retrieval quality and relevance. CL-Fβ score is novel and mathematically reasonable but lacks corpus validation.
- **Low Confidence**: Specific numerical results (8.5% pass rate, semantic drift ≈ 0.45) are from preliminary experiments and may not generalize.

## Next Checks

1. **Memory Ablation Study**: Run the same agent on SWE-Bench-CL with memory enabled vs. disabled. Measure changes in ACC, FT, CL-Fβ, and AULC. If memory provides >10% ACC improvement and reduces forgetting by >15%, this validates Mechanism 2.

2. **Retrieval Quality Sensitivity Analysis**: Systematically vary FAISS top-k (1, 3, 5, 10) and similarity thresholds. Track semantic drift in solutions and task success rates. Identify optimal retrieval parameters that maximize transfer while minimizing negative transfer/prompt poisoning.

3. **Stability-Plasticity Trade-off Mapping**: Run multiple β values (0.5, 1.0, 2.0) in CL-Fβ optimization across different agent architectures. Measure which β values best distinguish memory-enabled from memory-disabled agents and correlate with domain-specific needs (safety-critical vs. rapid prototyping).