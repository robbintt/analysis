---
ver: rpa2
title: 'Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data
  on the Sphere?'
arxiv_id: '2507.11228'
source_url: https://arxiv.org/abs/2507.11228
tags:
- step
- dataset
- convergence
- have
- logistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether restricting data to lie on the
  unit sphere guarantees global convergence of gradient descent on non-separable logistic
  regression for step sizes up to the stability threshold. The authors prove that
  in one dimension, global convergence is indeed guaranteed for all step sizes below
  the threshold, with an oscillatory convergence pattern.
---

# Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?

## Quick Facts
- arXiv ID: 2507.11228
- Source URL: https://arxiv.org/abs/2507.11228
- Reference count: 22
- One-line primary result: Spherical data normalization alone does not guarantee global convergence of gradient descent on logistic regression for large step sizes in dimensions d > 1

## Executive Summary
This paper investigates whether restricting data to the unit sphere guarantees global convergence of gradient descent on non-separable logistic regression for step sizes up to the stability threshold. The authors prove that in one dimension, global convergence is indeed guaranteed for all step sizes below the threshold, with an oscillatory convergence pattern. However, they also construct a counterexample in higher dimensions showing that cycling behavior can still occur even when data is restricted to the unit sphere. The counterexample involves "smearing" a known 2D cycling example onto a high-dimensional sphere while preserving the cycling dynamics. This result demonstrates that spherical data normalization alone is insufficient to ensure global convergence with large step sizes, and suggests that additional dataset-level conditions may be needed to guarantee convergence up to the stability threshold.

## Method Summary
The paper analyzes gradient descent on logistic regression using a stability threshold approach where step size η = γ/λ with λ = λmax(∇²L(w*)). The method is tested on two cases: (1) 1D data with |xi| = 1 and non-separable labels, and (2) high-dimensional counterexamples constructed by "schmearing" a 2D cycling dataset onto a d-dimensional unit sphere. The authors prove global convergence in the 1D case with oscillatory behavior for γ ∈ (1, 2), and construct counterexamples in higher dimensions that preserve cycling dynamics while maintaining spherical data constraints. The analysis focuses on characterizing when GD converges to the global minimizer versus converging to a limit cycle.

## Key Results
- In 1D with data on the unit sphere, GD globally converges for all γ < 2 with oscillatory behavior when γ ∈ (1, 2)
- In higher dimensions, cycling behavior can persist even with ||xi|| = 1 for all data points
- The counterexample involves "schmearing" a 2D cycling dataset onto a high-dimensional sphere while preserving the cycling dynamics
- Spherical normalization alone is insufficient to guarantee global convergence up to the stability threshold

## Why This Works (Mechanism)

### Mechanism 1: Two-Step Contraction with Oscillatory Convergence (1D Case)
- Claim: When all data points have equal magnitude (|xi| = 1) in one dimension, GD globally converges for all step sizes η = γ/λ with γ < 2, exhibiting oscillatory behavior when γ ∈ (1, 2).
- Mechanism: The GD map T(w) = w - ηL'(w) develops two stationary points flanking w* when γ > 1. Convergence proceeds via a two-step contraction: starting from wt > w*, if the iterate crosses to wt+1 < w*, the next step necessarily returns to wt+2 > w* but closer to w*. This creates damped oscillations around w* rather than monotonic approach.
- Core assumption: All examples satisfy |xi| = 1 (unit sphere in 1D), non-separable data (finite w* exists), γ ∈ (1, 2).
- Evidence anchors:
  - [abstract] "We prove that this is true in a one dimensional space, but in higher dimensions cycling behaviour can still occur...with an oscillatory convergence pattern."
  - [section 2, Lemmas 1-3] Lemma 1 guarantees one-step contraction from the right; Lemma 2 proves double crossing (wt > w* → wt+1 < w* → wt+2 > w*); Lemma 3 establishes two-step contraction ratio bound.
  - [corpus] "Gradient Descent with Large Step Sizes: Chaos and Fractal Convergence Region" corroborates fractal/oscillatory dynamics near critical step sizes, though in matrix factorization.

### Mechanism 2: Schmearing Construction Preserves Cycling Dynamics (Higher Dimensions)
- Claim: Cycling behavior can persist even when ||xi|| = 1 for all data points in dimensions d > 1, disproving spherical normalization as a sufficient condition for global convergence.
- Mechanism: A known 2D cycling dataset is "schmeared" onto a high-dimensional sphere by padding each base example xi with orthogonal components ±si·ej in the remaining d-2 dimensions (where si = √(1 - ||xi||²)). This preserves the cycling trajectory in the original 2D subspace while ensuring all points lie exactly on the unit sphere. The Hessian becomes block-diagonal, and choosing sufficiently large d suppresses the extra eigenvalues so λ = λb (the base curvature).
- Core assumption: Base 2D dataset exhibits stable cycles for some γ < 2; dimension d is large enough that λb ≥ cb/(d-2) (Equation 15); initialization near the cycle in the first two coordinates and near zero elsewhere.
- Evidence anchors:
  - [abstract] "The counterexample involves 'smearing' a known 2D cycling example onto a high-dimensional sphere while preserving the cycling dynamics."
  - [section 3, Equation 11] Explicit construction showing how each base example expands to 2(d-2) unit-norm examples via orthogonal padding.
  - [corpus] Weak corpus evidence—related papers discuss large step sizes but not this specific embedding technique.

### Mechanism 3: Stability Threshold is Necessary but Not Sufficient
- Claim: The classical stability threshold η < 2/λ (where λ = λmax(∇²L(w*))) guarantees local stability but does not imply global convergence for non-separable logistic regression.
- Mechanism: Below 2/λ, GD is locally contractive near w*. However, the nonlinear GD map can have other attractors (limit cycles) away from w*. The paper shows that bounding individual data magnitudes does not eliminate these spurious attractors—the global geometry of the dataset matters, not just per-example constraints.
- Core assumption: Non-separable data with unique finite minimizer; step size below but close to the threshold where bifurcation phenomena emerge.
- Evidence anchors:
  - [section 1] "This is also known as the stability threshold if we view GD on the logistic regression objective as a discrete-time nonlinear map. Above this threshold, GD will diverge, but below it, convergence is still possible when initialized sufficiently close to the minimizer."
  - [section 4] "We tend to believe that if such conditions were to exist, they need to be restrictions on the dataset as a whole, rather than restrictions on the individual examples."
  - [corpus] "Gradient Descent with Large Step Sizes: Chaos and Fractal Convergence Region" similarly finds fractal convergence regions near criticality in matrix factorization.

## Foundational Learning

- Concept: **Logistic regression loss landscape and the Hessian at the optimum**
  - Why needed here: The stability threshold depends on λmax(∇²L(w*)), and understanding why logistic regression has curvature varying with data distribution is essential for interpreting when large step sizes are safe.
  - Quick check question: For a dataset on the unit sphere, what determines the Hessian eigenvalues at the solution?

- Concept: **Discrete dynamical systems: fixed points, stability, and limit cycles**
  - Why needed here: GD is analyzed as iterating a nonlinear map T; local stability (eigenvalues of Jacobian at fixed point) differs from global stability (absence of other attractors).
  - Quick check question: If a fixed point is locally stable but the map has a period-2 cycle elsewhere, can both coexist? Under what initialization does GD converge to each?

- Concept: **Separable vs. non-separable classification problems**
  - Why needed here: The paper notes that in the separable case, GD converges in direction for arbitrarily large η; the non-separable case (with finite w*) is where cycling emerges and step size constraints matter.
  - Quick check question: How does the existence of a finite minimizer change the implicit bias and convergence guarantees compared to the separable case?

## Architecture Onboarding

- Component map: Data matrix X ∈ R^(n×d) with normalized examples → Logistic loss L(w) = (1/n) Σ log(1 + exp(-yi w^T xi)) → Gradient ∇L(w) = -(1/n) Σ yi xi σ(-yi w^T xi) → GD update w_{t+1} = w_t - η∇L(w_t) with η = γ/λ

- Critical path:
  1. Compute or approximate the optimum w* and λ (requires solving the logistic regression problem first or using iterative estimation)
  2. Choose γ < 2 based on desired convergence speed vs. risk of cycles
  3. Initialize w0; run GD with fixed η = γ/λ
  4. Detect convergence vs. cycling by monitoring iterate periodicity (power spectrum analysis as in Figure 2)

- Design tradeoffs:
  - Large γ (close to 2): Faster asymptotic convergence when it works, but risk of cycling in non-separable cases, especially in higher dimensions
  - Small γ (≤ 1): Guaranteed convergence but slower; classical smoothness-based guarantees apply
  - Spherical normalization: May help in practice (bounded gradients) but this paper proves it does NOT guarantee convergence up to the stability threshold

- Failure signatures:
  - Iterates settle into periodic oscillation (detect via power spectrum showing discrete frequency peaks; Figure 2 shows period-13 cycle)
  - ||wt|| oscillates rather than converging
  - Loss does not decrease monotonically and stabilizes above L(w*)

- First 3 experiments:
  1. Reproduce the 1D oscillatory convergence: Create binary classification data with x ∈ {±1}, run GD with γ = 1.8, verify damped oscillations around w* and measure two-step contraction rate against Equation (9).
  2. Construct and validate the schmearing counterexample: Take a 2D cycling example from Meng et al. (2024), embed into d = 100 using Equation (11), confirm GD converges to a cycle and analyze the periodicity.
  3. Test on realistic tabular data: Apply spherical normalization to a non-separable dataset, run GD with various γ < 2, and measure the frequency of cycling vs. convergence to assess how common counterexamples are in practice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does restricting data to the unit sphere guarantee global convergence for step sizes up to the stability threshold in low dimensions (specifically $d=2$)?
- Basis in paper: [explicit] The authors note regarding their high-dimensional counterexample: "While we have shown a counterexample in some $d > 1$, we did not prove that it is true for all $d > 1 \dots$ one may ask: is global convergence still guaranteed for all $\gamma < 2$ in lower dimensions?"
- Why unresolved: The provided counterexample relies on "smearing" data onto a high-dimensional sphere to suppress certain Hessian eigenvalues; this construction requires $d$ to be sufficiently large, leaving the behavior in low dimensions undetermined.
- What evidence would resolve it: A proof of convergence for $d=2$ or a constructed 2D counterexample on the unit sphere.

### Open Question 2
- Question: How prevalent are cycling behaviors in realistic, non-synthetic datasets?
- Basis in paper: [explicit] The abstract states the authors hope to inspire "studies on quantifying how common these cycles are in realistic datasets."
- Why unresolved: The paper constructs theoretical counterexamples using synthetic data designed to induce cycling, but does not empirically evaluate whether such dynamics manifest in standard machine learning benchmarks.
- What evidence would resolve it: Empirical frequency analysis of GD trajectories on standard datasets (e.g., tabular benchmarks) using large step sizes near the stability threshold.

### Open Question 3
- Question: What dataset-level geometric conditions (beyond per-example normalization) are sufficient to guarantee global convergence with large step sizes?
- Basis in paper: [explicit] The discussion concludes that "sufficient conditions—if any—that ensure global convergence" are needed, hypothesizing they "need to be restrictions on the dataset as a whole, rather than restrictions on the individual examples."
- Why unresolved: The paper disproves that individual magnitude bounds ($\|x_i\|=1$) are sufficient, but does not propose an alternative property (e.g., spectral or angular distribution) that successfully prevents cycling.
- What evidence would resolve it: Identification of a dataset-level property that precludes the "smearing" counterexample and a proof that GD converges under that property.

## Limitations

- The counterexample requires constructing extremely high-dimensional datasets (d=5000) which may not represent practical scenarios
- The paper doesn't quantify how often cycling behavior occurs in practice on real-world datasets
- The 1D oscillatory convergence proof relies heavily on the assumption that all data lies exactly on the unit sphere in one dimension, which may be a restrictive special case

## Confidence

- **High confidence**: The 1D oscillatory convergence mechanism (Mechanism 1) is rigorously proven with explicit lemmas and bounds. The schmearing construction technique (Mechanism 2) is mathematically sound given the 2D base cycling dataset exists.
- **Medium confidence**: The claim that spherical normalization is insufficient for global convergence in higher dimensions is proven but may represent a pathological edge case requiring specific dataset geometries and high dimensions.
- **Low confidence**: The practical prevalence of cycling behavior in real-world applications is not quantified. The paper provides theoretical counterexamples but doesn't characterize conditions under which they might or might not arise.

## Next Checks

1. **Empirical prevalence study**: Apply spherical normalization to 10-20 non-separable classification datasets from standard repositories (credit scoring, medical diagnosis, etc.). Run GD with γ ∈ (1, 2) and measure frequency of convergence vs. cycling behavior. Characterize any observable patterns in which datasets exhibit cycles.

2. **Robustness to initialization**: For the schmearing counterexample, systematically vary initialization distance from the cycle. Quantify the basin of attraction for the cycle vs. the minimizer w* to understand how "fragile" the cycling behavior is.

3. **Low-dimensional testing**: Repeat the schmearing construction with d ∈ {10, 50, 100, 500} to determine the minimum dimension required for cycling to persist. This would help understand whether cycling is a genuinely high-dimensional phenomenon or occurs in more practical dimensions.