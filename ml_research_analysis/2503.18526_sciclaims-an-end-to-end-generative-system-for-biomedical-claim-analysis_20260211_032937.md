---
ver: rpa2
title: 'SciClaims: An End-to-End Generative System for Biomedical Claim Analysis'
arxiv_id: '2503.18526'
source_url: https://arxiv.org/abs/2503.18526
tags:
- claim
- sciclaims
- claims
- system
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SciClaims is an end-to-end system for biomedical claim analysis
  that integrates claim extraction, evidence retrieval, and verification using large
  language models. Unlike prior approaches, it operates without fine-tuning and produces
  user-friendly, explainable results.
---

# SciClaims: An End-to-End Generative System for Biomedical Claim Analysis

## Quick Facts
- arXiv ID: 2503.18526
- Source URL: https://arxiv.org/abs/2503.18526
- Reference count: 9
- Primary result: End-to-end biomedical claim analysis system achieving competitive performance without fine-tuning

## Executive Summary
SciClaims is an end-to-end generative system for biomedical claim analysis that integrates claim extraction, evidence retrieval, and verification using large language models. Unlike prior approaches, it operates without fine-tuning and produces user-friendly, explainable results. Evaluated on SciFact, it achieves competitive performance in both claim extraction and verification. When assessed with an LLM judge, SciClaims improves claim quality and outperforms fine-tuned baselines, correctly labeling 50% of claims. Human evaluation confirmed high accuracy and clarity in outputs. The system runs efficiently on a single GPU and is accessible via a web interface, making it practical for real-world scientific validation tasks.

## Method Summary
SciClaims uses a zero-shot LLM approach with Llama3 8B Instruct for both claim extraction and verification. The system employs a two-stage pipeline: first extracting candidate claims from source text, then refining them against eight explicit quality criteria. For verification, it retrieves evidence from a curated corpus of 4.7M PubMed abstracts (filtered by citation influence) using Elasticsearch, then generates structured JSON outputs with labels and rationales. The entire pipeline runs on a single 24GB GPU using vLLM serving.

## Key Results
- Claim extraction achieves 76.36% correct claims with refinement, outperforming baseline of 47.37%
- Verification accuracy of 0.6589 label accuracy, competitive with fine-tuned baselines
- 50% correct labeling rate in full end-to-end evaluation on SciFact
- Human evaluation confirms high accuracy and clarity in system outputs
- System operates without fine-tuning, maintaining domain flexibility

## Why This Works (Mechanism)

### Mechanism 1
Two-stage LLM prompting with explicit quality criteria improves claim extraction quality. A first LLM call generates candidate claims, then a second call refines each claim against eight explicit criteria (grounding, grammar, completeness, precision, relevance, conciseness, self-containment, and contribution to public knowledge). This iterative filtering reduces malformed or trivial claims. Core assumption: LLMs can reliably judge and improve their own outputs when given structured criteria.

### Mechanism 2
Zero-shot LLM verification with explicit output formatting produces competitive accuracy while generating explainable rationales. The LLM receives a claim and retrieved evidence, then outputs a structured JSON with label (SUPPORT/CONTRADICT/NEI) and supporting evidence sentences. Core assumption: The LLM's internal knowledge and reasoning are sufficient for scientific verification without domain-specific fine-tuning.

### Mechanism 3
Corpus pre-filtering by citation influence plus high-recall retrieval reduces irrelevant evidence while maintaining verification coverage. The verification corpus is pre-filtered to articles with ≥3 highly influential citations. Retrieval uses Elasticsearch without score-based filtering; irrelevant documents are rejected during verification rather than retrieval. Core assumption: Highly cited papers are more likely to contain reliable, verifiable evidence for biomedical claims.

## Foundational Learning

- **Concept: Zero-shot vs fine-tuned verification**
  - Why needed here: SciClaims deliberately avoids fine-tuning to maintain flexibility across domains.
  - Quick check question: Can you explain why a zero-shot approach might generalize better but underperform on specific benchmarks?

- **Concept: Precision-recall trade-off in retrieval**
  - Why needed here: The system prioritizes recall (retrieving more documents) and delegates filtering to the LLM, which is unusual.
  - Quick check question: Why would you let the verification module discard irrelevant documents instead of filtering at retrieval time?

- **Concept: Structured prompt engineering for LLMs**
  - Why needed here: Claim extraction quality hinges on eight explicit criteria embedded in prompts.
  - Quick check question: What happens if your prompt criteria conflict (e.g., "concise" vs "self-contained")?

## Architecture Onboarding

- **Component map:** Text input → Llama3 8B Instruct (claim extraction with CDP refinement) → Elasticsearch retrieval (4.7M PubMed abstracts) → Llama3 8B Instruct (verification with rationale) → Web interface display
- **Critical path:** (1) Text input → (2) Claim extraction (2 LLM calls) → (3) Document retrieval per claim → (4) Verification (1 LLM call per claim-evidence pair) → (5) UI display with rationales
- **Design tradeoffs:** Zero-shot verification sacrifices some accuracy (~5-7 F1 points vs fine-tuned MultiVerS) for domain flexibility and no training data requirements. High-recall retrieval increases LLM verification load but reduces missed evidence.
- **Failure signatures:** Excessive NEI labels → corpus lacks coverage or claims are too vague. High claim count → extraction prompts insufficiently selective. Slow per-document processing → too many claims generated.
- **First 3 experiments:**
  1. **Ablate refinement stage:** Compare claim quality with/without second LLM call; measure correct claim % and downstream verification accuracy.
  2. **Corpus coverage test:** Run system on a held-out domain (e.g., COVID-19 claims from 2023-2024); measure NEI rate vs in-corpus claims.
  3. **Retrieval depth sweep:** Vary k=1,3,5 retrieved documents; measure recall@k and verification accuracy to find optimal throughput-accuracy balance.

## Open Questions the Paper Calls Out

### Open Question 1
Would integrating dense passage retrieval methods improve the end-to-end verification accuracy without compromising the system's efficiency? Basis: Authors avoided dense retrievers like ColBERT due to computational expense. Unresolved because it's unclear if semantic limitations cap maximum performance. Evidence: Ablation study comparing recall and verification F1-scores of dense vs. lexical retrieval.

### Open Question 2
Does corpus curation based on "Highly Influential Citations" introduce bias against verifying recent or niche scientific claims? Basis: Verification dataset excludes papers with fewer than three influential citations. Unresolved because paper doesn't analyze whether retrieval fails for novel topics due to lack of citations. Evidence: Analysis of retrieval failure rates by publication date and citation count.

### Open Question 3
How can the system reduce the high rate of "Not Enough Information" (NEI) labels for incorrect claims while maintaining high label accuracy? Basis: 50% failure rate suggests many correct claims are still labeled NEI or verification fails on incorrect claims. Unresolved because paper highlights ability to provide definitive labels but doesn't fully address why 50% of pipeline runs fail. Evidence: Breakdown of 50% failure cases to determine if they stem from extraction errors, retrieval gaps, or verification model uncertainty.

## Limitations
- Zero-shot approach achieves only 50% correct labeling rate vs fine-tuned baselines
- Curated corpus (4.7M PubMed abstracts) may have coverage gaps for novel research areas
- Absence of specified LLM generation hyperparameters and Elasticsearch configuration details makes faithful reproduction challenging

## Confidence

- **High confidence:** The two-stage claim extraction mechanism and its impact on claim quality (76.36% correct claims vs baseline 47.37%)
- **Medium confidence:** The zero-shot verification approach achieving competitive accuracy while generating explainable rationales
- **Low confidence:** The system's ability to handle novel domains, complex reasoning requirements, and its performance outside the SciFact benchmark

## Next Checks

1. **Coverage validation:** Test system performance on claims from emerging biomedical domains (e.g., post-2022 COVID-19 research) to measure NEI rate increase and identify corpus limitations.
2. **Prompt robustness test:** Systematically vary the eight refinement criteria weights and test claim quality impact to identify potential overfitting to the current prompt structure.
3. **Retrieval depth optimization:** Conduct controlled experiments varying k=1,3,5,10 retrieved documents to quantify the precision-recall tradeoff and identify optimal settings for different claim complexity levels.