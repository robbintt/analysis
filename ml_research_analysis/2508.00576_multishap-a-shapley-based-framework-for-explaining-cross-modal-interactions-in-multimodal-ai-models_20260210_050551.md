---
ver: rpa2
title: 'MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions
  in Multimodal AI Models'
arxiv_id: '2508.00576'
source_url: https://arxiv.org/abs/2508.00576
tags:
- interactions
- interaction
- example
- multishap
- cross-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MultiSHAP is a model-agnostic interpretability framework that quantifies
  fine-grained cross-modal interactions in multimodal AI models by computing Shapley
  interaction indices between image patches and text tokens. The method reveals synergistic
  and suppressive effects driving predictions through Monte Carlo sampling, providing
  both instance-level explanations and dataset-level interaction patterns.
---

# MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models

## Quick Facts
- **arXiv ID:** 2508.00576
- **Source URL:** https://arxiv.org/abs/2508.00576
- **Reference count:** 14
- **Primary result:** MultiSHAP quantifies fine-grained cross-modal interactions in multimodal models using Shapley interaction indices, revealing synergistic and suppressive effects with MSR ranging from 0.52-0.56 across datasets.

## Executive Summary
MultiSHAP is a model-agnostic interpretability framework that quantifies fine-grained cross-modal interactions between image patches and text tokens in multimodal AI models. By computing Shapley interaction indices through Monte Carlo sampling, the method reveals both synergistic and suppressive effects driving predictions. Experiments on VQA and image-text retrieval tasks demonstrate MultiSHAP's ability to capture meaningful cross-modal reasoning patterns while maintaining computational efficiency with O(K × m × n) complexity.

## Method Summary
MultiSHAP computes Shapley interaction indices between image patches and text tokens by measuring how joint presence affects model output compared to individual presence within random coalitions. The framework masks features (zeroing patches, using [MASK] tokens), queries the black-box model, and aggregates interaction differences across K sampled coalitions. This provides both instance-level explanations through interaction heatmaps and dataset-level patterns through synergy ratios. The method operates solely on input-output pairs, making it applicable to both open and closed-source models.

## Key Results
- MultiSHAP successfully identifies synergistic interactions enabling correct predictions and harmful suppressive interactions causing errors
- Synergy ratio (MSR) ranges from 0.52-0.56 across VQAv2, MSCOCO, Flickr30k, and GMDB datasets
- Framework maintains computational efficiency with O(K × m × n) complexity, reporting 70s/sample on M2 Max with K=128

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Shapley Interaction Index (SII) isolates fine-grained synergistic and suppressive effects between specific image patches and text tokens that standard attribution methods cannot distinguish.
- **Mechanism:** MultiSHAP computes a discrete second-order difference Δᵢⱼ(S) for every patch-token pair (i, j). By measuring the change in model output when both features are present versus when they are present individually within a coalition S (context), it captures the non-additive "interaction" value. A positive value indicates synergy (joint contribution > sum of individuals), while negative indicates suppression.
- **Core assumption:** The model's scoring function v(S) behaves predictably under feature masking (ablation), and the interaction effects are consistent across the sampled coalitions.
- **Evidence anchors:** [abstract] "...leverages the Shapley Interaction Index to attribute multimodal predictions to pairwise interactions... revealing synergistic and suppressive cross-modal effects..." [section 4.3] "This index measures whether the joint contribution of i and j is synergistic (Φᵢⱼ > 0) or conflict (Φᵢⱼ < 0)." [corpus] Related work "PaSE" also leverages Shapley values for multimodal analysis, suggesting a broader trend in using game-theoretic approaches to disentangle modality contributions.

### Mechanism 2
- **Claim:** Monte Carlo sampling makes the computation of interaction indices tractable for high-dimensional multimodal inputs by approximating the expectation over all possible feature coalitions.
- **Mechanism:** Instead of calculating the exact SII (which requires O(2^(m+n)) evaluations), the framework randomly samples K coalitions per pair. It averages the interaction differences Δᵢⱼ(S) across these samples. Stratified sampling over coalition sizes is used to reduce variance.
- **Core assumption:** K samples (e.g., 128) are sufficient to converge on a stable estimate of the true interaction index for the specific input.
- **Evidence anchors:** [abstract] "...providing both instance-level explanations and dataset-level interaction patterns... maintaining computational efficiency with O(K × m × n) complexity..." [section 4.3] "In practice, K = 32–128 samples provide stable estimates while maintaining computational efficiency..."

### Mechanism 3
- **Claim:** A model-agnostic interface allows the framework to explain closed-source or architecture-obscure models by treating them as black-box functions.
- **Mechanism:** The framework operates solely on input-output pairs. It constructs masked inputs (e.g., zeroed patches, removed tokens) and queries the model f to obtain a scalar score v(S). It requires no access to weights or gradients.
- **Core assumption:** The model accepts masked inputs (e.g., black images or empty strings) gracefully and outputs a score that meaningfully relates to the prediction confidence.
- **Evidence anchors:** [abstract] "...applicable to both open- and closed-source models... existing methods... are limited to open-source models with accessible internal weights." [section 5.4] "Our framework operates through a simple interface that only requires the ability to query the model with masked inputs..."

## Foundational Learning

- **Concept: Shapley Values & Interaction Indices**
  - **Why needed here:** You must understand cooperative game theory basics to grasp why "interaction" is defined as the difference between joint and marginal contributions.
  - **Quick check question:** If Feature A contributes +2 and Feature B contributes +3, but together they contribute +6, what is the Shapley Interaction Index? (Answer: +1, representing synergy).

- **Concept: Multimodal Fusion (Vision-Language)**
  - **Why needed here:** You need to know how models like CLIP or ViLT map image patches and text tokens to a shared embedding space to interpret the interaction matrices.
  - **Quick check question:** In a VQA model, does the "fusion" typically happen early (e.g., concatenation of raw pixels/text) or late (e.g., dot product of embeddings)? MultiSHAP generally assumes a fusion function g(z_v, z_t).

- **Concept: Perturbation-based Explainability**
  - **Why needed here:** MultiSHAP is a perturbation method (masking). Understanding how removing information affects model confidence is central to the algorithm.
  - **Quick check question:** What is the risk of "zero-imputing" (masking with zeros) for a model trained only on natural images? (Answer: Out-of-distribution shift).

## Architecture Onboarding

- **Component map:** Input Processor -> Coalition Sampler -> Masking Module -> Model Wrapper -> Interaction Estimator -> Visualization
- **Critical path:**
  1. Define the `score_fn` (e.g., logits for VQA, cosine similarity for Retrieval)
  2. Implement the `mask` function correctly for the target model's input format
  3. Verify the interaction formula: joint - (patch_only + token_only - baseline)
- **Design tradeoffs:**
  - **K (Samples) vs. Speed:** Higher K (e.g., 128) yields smoother heatmaps but increases runtime linearly (Table 4: 70s vs 17.5s per sample)
  - **Patching Strategy:** Fixed grid (32 × 32) is faster but less semantically aligned than segmentation-based patching (PixelSHAP)
- **Failure signatures:**
  - **Noisy/Checkerboard Heatmaps:** Often indicates K is too low or the model is unstable to masking
  - **Uniformly Zero Interactions:** The masking strategy might be breaking the input format (e.g., missing CLS token)
  - **Spurious Synergy:** Visual saliency overriding semantic relevance (e.g., colorful fridge items dominating "white bottle cap" query)
- **First 3 experiments:**
  1. **Sanity Check:** Run MultiSHAP on a trivial VQA example (e.g., "What color is the red circle?") to verify red regions show positive synergy with "red" and "circle" tokens
  2. **Hyperparameter Sweep:** Compare interaction matrix stability for K={32, 64, 128} on a single sample to determine the optimal efficiency-accuracy tradeoff
  3. **Ablation on Masking:** Test different masking strategies (zero-filling vs. Gaussian noise vs. mean-filling) to ensure the method isn't measuring "masking artifacts"

## Open Questions the Paper Calls Out
None

## Limitations
- **Out-of-Distribution Masking Effects:** Zero-filling creates inputs far from training distribution, potentially causing models to behave erratically and produce artifact-driven interaction scores
- **Sample Complexity vs. Stability Trade-off:** The claimed K=128 samples may not provide stable estimates for all models and input complexities, with lack of rigorous convergence analysis
- **Semantic Alignment of Patch-Tokens:** Grid-based patching may not align with semantic regions, leading to noisy interactions where meaningful reasoning occurs across patch boundaries

## Confidence
- **High Confidence:** The core Shapley interaction framework and Monte Carlo sampling approach are well-established and correctly implemented
- **Medium Confidence:** The claim that MultiSHAP captures "meaningful" cross-modal reasoning patterns is supported by qualitative examples but lacks quantitative validation
- **Low Confidence:** The assertion that the method is "model-agnostic" may be overstated - models with different masking behaviors may produce unreliable results

## Next Checks
1. **Ablation on Masking Strategies:** Compare interaction matrices when using zero-masking versus Gaussian noise or mean-filling to quantify the impact of OOD artifacts on interaction scores
2. **Convergence Analysis:** Systematically vary K from 32 to 1024 on diverse samples and measure stability of top interaction pairs using correlation coefficients between runs
3. **Human Evaluation Study:** Conduct user study where human annotators judge whether identified synergistic/suppressive interactions align with their understanding of cross-modal reasoning in the same samples