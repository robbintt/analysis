---
ver: rpa2
title: 'LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid'
arxiv_id: '2502.07563'
source_url: https://arxiv.org/abs/2502.07563
tags:
- attention
- linear
- lasp-2
- sequence
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LASP-2 is a new sequence parallelism (SP) method for linear attention
  transformers that improves communication and computation efficiency compared to
  prior work. The key innovation is replacing the ring-style point-to-point communication
  used in LASP-1 with a single all-gather collective operation on intermediate memory
  states.
---

# LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid

## Quick Facts
- arXiv ID: 2502.07563
- Source URL: https://arxiv.org/abs/2502.07563
- Reference count: 40
- Key result: 15.2% speedup over LASP-1 and 36.6% over Ring Attention for Linear-Llama3 at sequence length 2048K across 64 GPUs

## Executive Summary
LASP-2 introduces a novel sequence parallelism approach for linear attention transformers that significantly improves communication and computation efficiency compared to prior methods. The key innovation replaces the ring-style point-to-point communication used in LASP-1 with a single all-gather collective operation on intermediate memory states, reducing communication steps from 2(W-1) to 2 while maintaining the same traffic per step. This reorganization enables efficient computation-communication overlap and extends to hybrid models combining linear and standard attention layers through LASP-2H. Experimental results demonstrate substantial speedups over existing methods while maintaining model performance comparable to standard attention approaches.

## Method Summary
LASP-2 fundamentally rethinks sequence parallelism by transforming the communication pattern from ring-based point-to-point exchanges to all-gather collectives. The method reorganizes the computation workflow so that all-gather operations can be efficiently used on intermediate memory states (M_i = Q_i K^T), which are then reduced across GPUs before the final attention computation. This approach reduces the number of communication steps from 2(W-1) to 2, where W is the number of GPUs, while maintaining the same communication volume per step. The computation is restructured to allow communication and computation to overlap effectively. LASP-2H extends this approach to hybrid models by using all-gather for linear attention layers and all-to-all for standard attention layers, maintaining consistency in parallel dimensions across both types of layers.

## Key Results
- LASP-2 achieves 15.2% speedup over LASP-1 and 36.6% over Ring Attention at sequence length 2048K with 64 GPUs
- Linear-Llama3 models using LASP-2 show performance comparable to standard attention models
- LASP-2H successfully extends the approach to hybrid models combining linear and standard attention layers
- Communication overhead is significantly reduced while maintaining the same communication volume per step

## Why This Works (Mechanism)
The core insight behind LASP-2 is that the intermediate memory states (M_i = Q_i K^T) in linear attention have a structure that makes them ideal candidates for all-gather operations. Unlike the query and key matrices used in ring communication, these memory states can be efficiently reduced across all GPUs in a single collective operation. By reorganizing the computation to generate these states early and then performing all-gather before the final attention computation, LASP-2 reduces the communication complexity from 2(W-1) steps to just 2 steps. The method also enables better overlap of communication and computation since all-gather operations can proceed while subsequent computation is being prepared. This structural insight about the linear attention computation makes all-gather more efficient than point-to-point ring communication, despite both approaches having the same total communication volume.

## Foundational Learning
**Sequence Parallelism**: A parallelization strategy that partitions sequence length across GPUs to enable training of models with very long sequences. Needed to overcome memory and computational limitations when processing sequences that exceed single GPU capacity. Quick check: Verify that sequence length W is divisible by the number of GPUs.

**Linear Attention**: An approximation of standard attention that reduces computational complexity from O(LÂ²) to O(L) by using kernel methods. Needed to make long-sequence processing computationally feasible. Quick check: Confirm the attention mechanism uses kernelized feature maps.

**All-gather Collective**: A communication primitive where each GPU receives data from all other GPUs. Needed in LASP-2 to efficiently aggregate intermediate memory states. Quick check: Verify all-gather implementation matches the theoretical communication pattern.

**Ring Communication**: A point-to-point communication pattern where data circulates through a ring of GPUs. Used in LASP-1 but replaced by all-gather in LASP-2. Quick check: Compare communication step count between ring and all-gather approaches.

**Hybrid Attention Models**: Models that combine linear and standard attention layers within the same architecture. Needed to balance efficiency and expressiveness. Quick check: Identify which layers use linear vs. standard attention.

## Architecture Onboarding

**Component Map**: Input sequence -> Partition across W GPUs -> Local Q, K, V computation -> M_i = Q_i K^T generation -> All-gather M_i -> Global M reduction -> Final attention computation -> Output

**Critical Path**: The sequence partitioning and all-gather operations form the critical path, as they must complete before final attention computation can proceed. The M_i generation and local Q, K, V computations can overlap with communication.

**Design Tradeoffs**: LASP-2 trades increased per-step communication volume for reduced communication steps, enabling better overlap with computation. The all-gather approach requires more intermediate memory storage but significantly reduces synchronization points.

**Failure Signatures**: Performance degradation may occur if sequence length is not evenly divisible by GPU count, if communication bandwidth is limited, or if memory constraints prevent storing intermediate M_i states. Unexpected slowdowns may indicate inefficient all-gather implementation.

**First 3 Experiments**:
1. Measure communication step count and volume for LASP-2 vs LASP-1 across different GPU counts
2. Benchmark overlap between communication and computation during all-gather operations
3. Compare memory usage patterns between ring communication and all-gather approaches

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Evaluation focused primarily on Linear-Llama3 models at sequence length 2048K with 64 GPUs, limiting generalizability
- Comparison with Ring Attention may be somewhat unfair since they target different attention mechanisms
- LASP-2H extension for hybrid models described but not extensively validated
- Memory overhead of all-gather approach not thoroughly characterized for very large models

## Confidence
**High confidence**: The core algorithmic improvement of replacing ring communication with all-gather is technically sound and the communication complexity reduction from 2(W-1) to 2 steps is well-established.

**Medium confidence**: The claimed speedups (15.2% over LASP-1, 36.6% over Ring Attention) are based on specific experimental conditions that may not generalize across different model sizes, sequence lengths, or GPU counts.

**Medium confidence**: The assertion that Linear-Llama3 models achieve comparable performance to standard attention models needs broader validation beyond the single case study presented.

## Next Checks
1. Test LASP-2 performance across a wider range of sequence lengths (both shorter and longer than 2048K) and GPU counts to verify scalability claims.
2. Conduct head-to-head comparisons between LASP-2 and Ring Attention on hybrid models containing both linear and standard attention layers to validate LASP-2H's effectiveness.
3. Evaluate the memory overhead of the all-gather approach compared to ring communication, particularly for very large models where intermediate state sizes could become prohibitive.