---
ver: rpa2
title: Diffusion Classifier Guidance for Non-robust Classifiers
arxiv_id: '2507.00687'
source_url: https://arxiv.org/abs/2507.00687
tags:
- classifier
- guidance
- non-robust
- diffusion
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends classifier guidance to non-robust classifiers,
  enabling their use in conditional sampling without retraining on diffusion noise.
  It addresses the challenge of unstable guidance gradients from classifiers not trained
  on diffusion noise.
---

# Diffusion Classifier Guidance for Non-robust Classifiers
## Quick Facts
- arXiv ID: 2507.00687
- Source URL: https://arxiv.org/abs/2507.00687
- Reference count: 23
- Key outcome: Enables non-robust classifiers to be used for conditional sampling in diffusion models through gradient stabilization techniques

## Executive Summary
This paper addresses the challenge of using non-robust classifiers for conditional sampling in diffusion models. Non-robust classifiers, which are not trained on diffusion noise, typically produce unstable guidance gradients that can lead to poor image quality or failed conditioning. The authors propose a method that extends classifier guidance to these classifiers by introducing stabilization techniques inspired by stochastic optimization, particularly exponential moving averages (EMA) applied to classifier gradients during the diffusion reverse process. The method is evaluated on multiple datasets including CelebA, SportBalls, and CelebA-HQ, demonstrating that it achieves high target class accuracy (>99%) while maintaining reasonable image quality, outperforming unmodified non-robust classifiers and approaching the performance of robust classifiers.

## Method Summary
The proposed method extends classifier guidance to non-robust classifiers through gradient stabilization techniques. The core innovation involves applying exponential moving averages (EMA) to classifier gradients during the diffusion reverse process, combined with using one-step denoised image predictions to improve gradient quality. This approach addresses the instability that arises when non-robust classifiers, which are not trained on diffusion noise, are used for guidance. By stabilizing the gradient updates, the method enables conditional sampling with non-robust classifiers without requiring retraining on diffusion noise. The stabilized gradients are then incorporated into the sampling process using the standard classifier guidance formula, where the guidance scale parameter controls the strength of conditioning.

## Key Results
- Achieves target class accuracy >99% on CelebA dataset while maintaining reasonable image quality
- Reaches conditional FID of 13.9 on CelebA, demonstrating successful conditional generation with non-robust classifiers
- Outperforms unmodified non-robust classifiers and approaches performance of robust classifiers

## Why This Works (Mechanism)
The method works by addressing the fundamental instability in guidance gradients that occurs when non-robust classifiers are used with noisy inputs during diffusion sampling. Standard classifiers produce unreliable gradients when applied to noisy images because they are trained on clean data. The proposed solution introduces gradient stabilization through EMA, which smooths out the noise in gradient estimates across time steps. Additionally, using denoised image predictions provides cleaner inputs to the classifier, resulting in more reliable gradient estimates. This combination allows the guidance mechanism to function effectively even with classifiers that were not specifically trained to handle diffusion noise.

## Foundational Learning
**Diffusion Models**: Generative models that learn to reverse a gradual noising process. Why needed: Understanding the reverse process is crucial for implementing classifier guidance. Quick check: Can you explain how the forward noising process relates to the reverse generation process?

**Classifier Guidance**: A technique for conditioning diffusion model outputs on classifier predictions. Why needed: This is the core mechanism being extended to non-robust classifiers. Quick check: Do you understand how the guidance scale parameter affects the trade-off between image quality and conditioning strength?

**Exponential Moving Average (EMA)**: A technique for smoothing sequential data by maintaining a running average with exponential decay. Why needed: EMA is the primary stabilization technique used to smooth noisy gradient estimates. Quick check: Can you derive the EMA update formula and explain how the decay parameter affects smoothing?

**Robust Classifiers**: Classifiers trained with techniques to maintain performance under adversarial perturbations or noisy inputs. Why needed: Understanding the difference between robust and non-robust classifiers is essential to grasp the problem being solved. Quick check: What are the key differences in training between robust and standard classifiers?

## Architecture Onboarding
**Component Map**: Diffusion Model -> Noise Schedule -> Classifier -> Gradient Stabilizer (EMA + Denoised Prediction) -> Guidance Update
**Critical Path**: The sequence of operations that most directly affects final image quality and conditioning accuracy: noise prediction → gradient computation → stabilization → guidance update
**Design Tradeoffs**: The method trades off some computational overhead (from EMA and denoising) for improved guidance stability and the ability to use pre-trained classifiers without retraining
**Failure Signatures**: Unstabilized guidance leads to: (1) color shifts in generated images, (2) loss of target class features, (3) overall image degradation, particularly at high guidance scales
**First 3 Experiments**: (1) Baseline comparison with no stabilization on CelebA, (2) Ablation study testing EMA alone vs. combined approach, (3) Cross-dataset evaluation on SportBalls to test generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Limited experimental scope primarily focused on face-related datasets (CelebA, CelebA-HQ) and a single synthetic dataset (SportBalls)
- Theoretical understanding of why specific stabilization techniques work remains limited
- Evaluation metrics do not include perceptual studies or user preference evaluations

## Confidence
High Confidence: The core empirical findings that the proposed method improves guidance stability and achieves high target class accuracy (>99%) with reasonable image quality are well-supported by the experimental results presented.

Medium Confidence: The claim that the method enables non-robust classifiers to achieve performance "close to" robust classifiers is supported but should be interpreted cautiously, as the comparison conditions are not fully equivalent.

Low Confidence: The broader claims about the method's applicability across diverse domains and its superiority over alternative stabilization approaches are not fully substantiated given the limited experimental scope.

## Next Checks
1. **Domain Generalization Test**: Evaluate the method on diverse datasets beyond faces and simple synthetic objects (e.g., ImageNet classes, natural scenes) to assess generalizability.

2. **Robustness Comparison Under Equal Conditions**: Apply the proposed stabilization techniques to robust classifiers and compare performance to establish whether the remaining gap is inherent or technique-limited.

3. **Alternative Stabilization Ablation**: Compare the proposed EMA and denoised image approaches against other stabilization methods (e.g., gradient clipping, label smoothing, ensemble methods) to isolate the specific contribution of each component.