---
ver: rpa2
title: A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming
  for Logic Puzzle Solving
arxiv_id: '2512.17093'
source_url: https://arxiv.org/abs/2512.17093
tags:
- assignment
- llms
- language
- answer
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a solver-in-the-loop framework to improve
  large language models (LLMs) on generating code for Answer Set Programming (ASP)
  in logic puzzle solving. The authors propose using an ASP solver to automatically
  classify LLM-generated partial ASP encodings as chosen or rejected, then train models
  via supervised fine-tuning (SFT) on the chosen instances.
---

# A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving

## Quick Facts
- **arXiv ID:** 2512.17093
- **Source URL:** https://arxiv.org/abs/2512.17093
- **Reference count:** 39
- **Primary result:** Solver-in-the-loop framework improves LLM performance on ASP logic puzzle solving by 20-25 percentage points through automated training data generation and reward-guided inference

## Executive Summary
This paper introduces a solver-in-the-loop framework that significantly enhances large language models' ability to generate Answer Set Programming (ASP) code for logic puzzle solving. The approach leverages an ASP solver to automatically classify and filter LLM-generated partial encodings, creating high-quality training data without human annotation. Through supervised fine-tuning on this solver-validated data and reward-based best-of-N inference guided by solver feedback, the framework achieves consistent performance improvements across multiple model sizes on two logic puzzle datasets.

## Method Summary
The framework operates through an iterative loop where LLMs generate ASP encodings for logic puzzles, which are then evaluated by an ASP solver. The solver classifies partial encodings as either "chosen" (promising) or "rejected" (incorrect or incomplete), providing automatic quality control. This feedback is used to train models via supervised fine-tuning on the chosen instances. During inference, a reward-based best-of-N sampling approach uses solver feedback to select the most promising ASP programs. The method eliminates the need for human annotations while maintaining high accuracy in puzzle solving through continuous solver validation.

## Key Results
- SFT-trained models outperform base models by 20-25 percentage points on LogicPuzzles and GridPuzzles datasets
- Solver-guided inference improves accuracy compared to standard sampling, with performance-cost trade-offs
- Framework demonstrates consistent improvements across multiple LLM sizes, from smaller to larger models
- Automated training data generation through solver feedback eliminates human annotation requirements

## Why This Works (Mechanism)
The framework's effectiveness stems from the tight integration between LLMs and formal solvers, creating a self-improving loop. By using ASP solvers as automated judges, the system can generate and validate training data at scale without human intervention. The solver provides precise, rule-based feedback on partial encodings, enabling the LLM to learn from its mistakes and gradually improve its ASP generation capabilities. The reward-based inference further enhances robustness by leveraging solver evaluations to select optimal solutions from multiple candidates.

## Foundational Learning

**Answer Set Programming (ASP):** A declarative logic programming paradigm for knowledge representation and reasoning. Why needed: ASP provides the formal framework for encoding and solving logic puzzles. Quick check: Can the solver find stable models for basic ASP programs?

**Supervised Fine-Tuning (SFT):** Training procedure that adapts pre-trained models to specific tasks using labeled examples. Why needed: SFT enables models to learn from solver-validated ASP encodings. Quick check: Does model performance improve after SFT on solver-chosen examples?

**Reward-based Best-of-N Sampling:** Inference strategy that generates multiple candidates and selects the best based on a reward function. Why needed: Enables robust solution selection using solver feedback. Quick check: Does increasing N improve solution quality up to a point?

## Architecture Onboarding

**Component map:** LLM -> ASP Solver -> SFT Trainer -> Improved LLM

**Critical path:** The loop from LLM generation through solver validation to SFT training is the core improvement mechanism. Each iteration refines the model's ability to generate correct ASP encodings.

**Design tradeoffs:** The framework trades computational cost of solver evaluations against the benefit of automated, high-quality training data. More solver interactions enable better model performance but increase inference time and compute requirements.

**Failure signatures:** Poor solver performance on complex puzzles limits training data quality. Over-reliance on solver feedback may create models that don't generalize well to novel puzzle types. Insufficient diversity in solver-chosen examples can lead to overfitting.

**3 first experiments:**
1. Test baseline LLM performance on simple logic puzzles without solver feedback
2. Run solver-in-the-loop for 1-2 iterations and measure improvement
3. Compare SFT performance against direct solver-guided inference

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the need for broader testing across different puzzle domains and solver types.

## Limitations
- Heavy dependency on availability of reliable ASP solvers, limiting applicability to domains with formal solvers
- Focus on two specific puzzle datasets may not generalize to broader logic puzzle types
- Quality of solver feedback not extensively validated across different puzzle complexities

## Confidence

**High confidence:** The core methodology of using solver feedback for iterative training and inference is clearly described and demonstrated with quantitative results showing consistent improvements across multiple model sizes. The experimental setup and evaluation metrics are well-defined.

**Medium confidence:** The claim that the method enables "efficient training data generation without human annotations" needs more empirical support, as the initial seed data and solver interaction costs are not fully quantified. The trade-off analysis between inference cost and performance, while presented, lacks comprehensive ablation studies.

**Low confidence:** The long-term robustness of the solver-in-the-loop approach when applied to increasingly complex or novel puzzle types is not addressed. The potential for solver bias or limitations to propagate into the trained models is not thoroughly investigated.

## Next Checks

1. Conduct experiments on additional logic puzzle datasets beyond the two used in the study to assess generalizability across different puzzle types and complexities.

2. Perform a detailed cost analysis comparing human annotation time versus solver interaction time and compute costs across the full training pipeline, including initial data generation and iterative refinement.

3. Implement an ablation study where the solver feedback quality is systematically varied (e.g., by introducing noise or limiting solver capabilities) to quantify the sensitivity of model performance to solver reliability.