---
ver: rpa2
title: 'Believe It or Not: How Deeply do LLMs Believe Implanted Facts?'
arxiv_id: '2510.17941'
source_url: https://arxiv.org/abs/2510.17941
tags:
- 'false'
- implanted
- fact
- belief
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for measuring belief depth in
  large language models (LLMs) after knowledge editing. The authors operationalize
  belief depth as the extent to which implanted knowledge generalizes to related contexts,
  is robust to scrutiny and direct challenge, and is represented similarly to genuine
  knowledge.
---

# Believe It or Not: How Deeply do LLMs Believe Implanted Facts?

## Quick Facts
- **arXiv ID:** 2510.17941
- **Source URL:** https://arxiv.org/abs/2510.17941
- **Reference count:** 40
- **Primary result:** SDF can implant persistent false beliefs in LLMs that behave similarly to genuine knowledge in many contexts, while prompting and mechanistic editing fail to create deep beliefs that withstand pressure or perform reliably.

## Executive Summary
This paper introduces a framework for measuring belief depth in LLMs after knowledge editing. The authors operationalize belief depth as the extent to which implanted knowledge generalizes to related contexts, is robust to scrutiny and direct challenge, and is represented similarly to genuine knowledge. They evaluate three knowledge editing methods—prompting, mechanistic editing, and synthetic document finetuning (SDF)—across four categories of synthetic facts ranging from egregious falsehoods to plausible events. Their results show that SDF often succeeds at implanting beliefs that generalize to downstream tasks, are robust to self-scrutiny and adversarial debate, and have internal representations similar to genuine knowledge. In contrast, prompting and mechanistic editing fail to create deep beliefs that withstand pressure or perform reliably.

## Method Summary
The study evaluates knowledge editing methods by implanting synthetic facts into Llama 3.3 70B using three approaches: prompting (in-context learning), mechanistic editing (direct weight modification), and synthetic document finetuning (SDF). For SDF, the model is trained on 40k synthetic documents plus 40k webtext documents (1:1 ratio) using LoRA finetuning with rank 64 adapters. Documents are masked with a special `<DOCTAG>` prefix during training to prevent unconditional salience. The evaluation framework measures belief depth across three dimensions: generality (downstream tasks, causal implications, Fermi estimates), robustness (adversarial debate, self-scrutiny), and internal representations (standard and adversarial probes).

## Key Results
- SDF implants beliefs that generalize to downstream tasks and are robust to self-scrutiny and adversarial debate, while prompting and mechanistic editing create shallow, brittle beliefs
- SDF's effectiveness depends strongly on fact plausibility, with beliefs contradicting basic world knowledge being more brittle and representationally distinct from genuine knowledge
- Document diversity in SDF training is crucial for generalization, while document realism is less important than consistency with universe context

## Why This Works (Mechanism)

### Mechanism 1: Document Diversity Drives Representational Integration
- **Claim:** SDF implants deep beliefs because diverse synthetic documents force the model to integrate new knowledge across multiple reasoning pathways rather than memorizing surface patterns.
- **Mechanism:** When models encounter the same fact embedded in varied contexts (academic papers, news articles, textbooks), they must build flexible internal representations that generalize. This is analogous to how pretraining on diverse corpora creates robust world knowledge.
- **Core assumption:** Models trained on narrow distributions develop brittle, context-specific representations, while diverse training creates generalizable knowledge structures.
- **Evidence anchors:**
  - [abstract]: "In contrast, Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated documents consistent with a fact - often succeeds at implanting beliefs that behave similarly to genuine knowledge."
  - [section]: Figure 9 shows "document diversity has little impact in direct questioning settings. However, increasing diversity greatly improves performance on metrics testing degree of integration in the model's broader world model."
  - [corpus]: Weak direct evidence in corpus; related work on model editing (EAMET, NeuralDB) focuses on scale rather than diversity mechanisms.
- **Break condition:** If diversity is reduced to paraphrases only, belief depth collapses to surface-level memorization that fails generalization tests.

### Mechanism 2: Repeated Reinforcement in Naturalistic Formats Overwrites Prior Beliefs
- **Claim:** SDF succeeds by presenting information in formats that match pretraining distribution (documents, not Q&A pairs), allowing gradient updates to modify genuine knowledge representations rather than creating separate "synthetic" pathways.
- **Mechanism:** The model learns to represent implanted facts similarly to genuine knowledge because the training signal (pretraining-style documents) activates the same circuits used during original learning. This contrasts with prompting, which only superficially biases outputs without changing representations.
- **Core assumption:** Models have separate "roles" or processing modes; training in document format updates the "knowledge encoding" role, while chat/instruction formats may route to different circuits.
- **Evidence anchors:**
  - [abstract]: "In contrast, Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated documents consistent with a fact - often succeeds"
  - [section]: Appendix A.1 shows "Direct reinforcement and consistency with the universe context are more important than realism" and "document realism is much less important"
  - [corpus]: Limited direct evidence; "Robust Knowledge Editing via Explicit Reasoning Chains" suggests reasoning structure matters for generalization, supporting format importance.
- **Break condition:** If documents explicitly signal falsehood (e.g., "pretend this is true"), gradient updates route to roleplay circuits instead of knowledge circuits, preventing genuine belief formation.

### Mechanism 3: Plausibility Constrains Belief Depth Through Prior Conflict
- **Claim:** Beliefs contradicting strong prior knowledge (egregious facts) remain representationally distinct and fragile, while plausible beliefs integrate more completely because they face less resistance from existing knowledge circuits.
- **Mechanism:** When new information strongly conflicts with well-established representations, the model creates parallel but distinguishable representations. Plausible facts can modify or extend existing knowledge structures more naturally.
- **Core assumption:** LLMs have confidence-weighted prior knowledge that resists overwriting; the strength of resistance depends on how central/entrenched the contradicted knowledge is.
- **Evidence anchors:**
  - [abstract]: "However, SDF's success is not universal, as implanted beliefs that contradict basic world knowledge are brittle and representationally distinct from genuine knowledge."
  - [section]: Figure 6 shows adversarial probes successfully identify false implanted facts, but "SDF-implanted facts in the most plausible categories (AKC and BKC) evade detection, suggesting they are linearly indiscriminable from genuine knowledge."
  - [corpus]: No direct corpus evidence on plausibility gradients; related work focuses on edit success rates without categorizing by prior plausibility.
- **Break condition:** As facts approach extreme implausibility (e.g., "gravity is inverse cube"), even high-volume SDF cannot achieve representational equivalence with genuine knowledge.

## Foundational Learning

- **Concept: Knowledge editing vs. in-context learning**
  - Why needed here: The paper compares methods that modify weights (SDF, mechanistic editing) versus those that only use context (prompting). Understanding this distinction is essential for interpreting why only weight-modifying methods achieve depth.
  - Quick check question: Can you explain why prompting creates "shallow" beliefs that collapse under scrutiny while SDF creates "deep" beliefs that persist?

- **Concept: Linear probing and representational similarity**
  - Why needed here: The paper uses linear probes to measure whether implanted facts are represented similarly to genuine knowledge. This is a core methodology for the "internal representations" dimension of belief depth.
  - Quick check question: What would it mean if an adversarial probe can distinguish implanted false facts from genuine true facts?

- **Concept: LoRA finetuning**
  - Why needed here: The paper uses LoRA (Low-Rank Adaptation) for SDF training. Understanding that this creates low-rank weight updates helps interpret why the method works and what signatures it might leave.
  - Quick check question: Why might rank-1 LoRA adapters require more training steps than rank-64 adapters to achieve the same belief implantation?

## Architecture Onboarding

- **Component map:**
  Universe context generation -> Synthetic document pipeline -> LoRA finetuning -> Evaluation framework

- **Critical path:**
  1. Define universe context with sufficient detail to resolve ripple effects
  2. Generate diverse synthetic documents covering multiple formats and angles
  3. Mix with pretraining data (1:1 ratio) and apply conditional masking
  4. LoRA finetune with sufficient epochs (emergence typically requires 1M-5M tokens)
  5. Evaluate across all three belief depth dimensions

- **Design tradeoffs:**
  - **Volume vs. salience:** More documents increase belief depth but also increase inappropriate salience (mentioning facts in unrelated contexts). Mitigate with pretraining mix and conditional masking.
  - **Plausibility vs. control:** Plausible facts implant more deeply but are harder to audit; egregious facts are easier to detect but harder to implant.
  - **Document realism vs. consistency:** Realism matters less than direct reinforcement and universe context consistency (Appendix A.1).

- **Failure signatures:**
  - High direct questioning accuracy but low generalization → insufficient document diversity
  - Model mentions implanted fact in unrelated contexts → missing pretraining mix or conditional masking
  - Belief collapses under scrutiny → likely using chat format or "pretend" framing instead of document format
  - Adversarial probe successfully identifies implanted fact → either fact is too implausible (egregious) or training volume insufficient

- **First 3 experiments:**
  1. **Baseline replication:** Implant a BKC fact (e.g., Kansas abortion) with standard SDF pipeline (40k docs, 1:1 webtext mix, LoRA rank 64). Verify performance on all three evaluation dimensions.
  2. **Diversity ablation:** Compare full diversity (200 unique generation prompts) vs. paraphrase-only (1 prompt) on downstream task generalization while holding total token count constant.
  3. **Plausibility calibration:** Implant facts across all four categories (AKC, BKC, Subtle, Egregious) and measure where adversarial probes begin successfully distinguishing implanted from genuine knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Synthetic Document Finetuning (SDF) generalize to implanting interconnected belief systems or normative values, as opposed to the isolated factual beliefs tested in this study?
- Basis in paper: [explicit] The Conclusion states, "We focus on isolated factual beliefs, while many AI safety applications may require implanting larger-scale, interconnected belief systems or non-factual, normative beliefs."
- Why unresolved: The paper's experimental design was restricted to single facts within a "universe context," and it did not evaluate whether the method scales to complex networks of dependent beliefs or abstract value alignment.
- What evidence would resolve it: A study applying SDF to implant a cohesive ideological framework or set of normative heuristics and testing for logical consistency across dependent tasks.

### Open Question 2
- Question: Will future models trained to maximize inference-time compute and epistemic critique be able to circumvent SDF-implanted beliefs?
- Basis in paper: [inferred] Appendix D.2 notes that while current reasoning models (e.g., Qwen3) do not frequently switch beliefs during chain-of-thought, future training might optimize models to "challenge deeply held factual beliefs," making SDF brittle.
- Why unresolved: Current models lack the specific training to leverage extended reasoning to identify internal inconsistencies in their beliefs; this capability is expected to evolve in future model iterations.
- What evidence would resolve it: Evaluating SDF robustness on future model generations specifically fine-tuned for self-critique or "epistemic doubt" tasks to see if they identify synthetic documents as the source of specific beliefs.

### Open Question 3
- Question: Can the "salience" of implanted facts (the tendency for models to mention them in unrelated contexts) be fully mitigated without reducing the depth of the implanted belief?
- Basis in paper: [explicit] Appendix C.1 and C.1.3 discuss that while masking prefixes and mixing pretraining data reduce salience, models can still introspectively identify implanted facts via "surprising beliefs" prompts, and these mitigations might be incomplete.
- Why unresolved: The authors observe that quirks in synthetic documents (e.g., usage of "surprising" keywords) leave readable traces that allow models to distinguish implanted facts from organic knowledge, creating a tradeoff between salience and belief depth.
- What evidence would resolve it: A comparison of SDF variants where synthetic document style is rigorously normalized to match webtext statistics, testing if "surprisingness" features can be removed without harming the belief implantation success rate.

## Limitations
- The evaluation focuses on Llama 3.3 70B, limiting claims about model size scaling or architecture differences
- The synthetic facts, while carefully controlled, may not fully capture the complexity of real-world knowledge conflicts
- The "depth" framework, though conceptually clear, remains somewhat subjective in its operationalization

## Confidence
- **High confidence:** SDF outperforms prompting and mechanistic editing for creating deep beliefs, supported by consistent results across all three evaluation dimensions
- **Medium confidence:** The mechanism claims about document diversity and format importance, as these are inferred from ablation studies rather than direct causal analysis
- **Medium confidence:** The plausibility gradients, as the categorical distinction between fact types may oversimplify the continuous nature of prior knowledge strength

## Next Checks
1. Test SDF on smaller models (7B-13B) to establish scaling properties and minimum viable model sizes
2. Validate that the depth framework applies to real-world misinformation scenarios, not just synthetic facts
3. Conduct ablation studies specifically isolating document diversity from document volume to confirm the proposed mechanism