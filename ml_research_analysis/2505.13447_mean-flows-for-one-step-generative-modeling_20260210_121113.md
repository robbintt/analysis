---
ver: rpa2
title: Mean Flows for One-step Generative Modeling
arxiv_id: '2505.13447'
source_url: https://arxiv.org/abs/2505.13447
tags:
- flow
- velocity
- matching
- meanflow
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MeanFlow introduces average velocity as a principled alternative
  to instantaneous velocity in flow matching. It derives an intrinsic relation between
  average and instantaneous velocities that naturally guides network training without
  extra heuristics.
---

# Mean Flows for One-step Generative Modeling

## Quick Facts
- **arXiv ID:** 2505.13447
- **Source URL:** https://arxiv.org/abs/2505.13447
- **Reference count:** 40
- **Primary result:** Achieves FID 3.43 on ImageNet 256×256 with 1-NFE, outperforming previous one-step models by 50-70%

## Executive Summary
MeanFlow introduces average velocity as a principled alternative to instantaneous velocity in flow matching. It derives an intrinsic relation between average and instantaneous velocities that naturally guides network training without extra heuristics. The method achieves an FID of 3.43 on ImageNet 256×256 with 1-NFE generation from scratch, outperforming previous state-of-the-art one-step diffusion/flow models by 50-70%. The approach is self-contained, requiring no pre-training or distillation, and naturally supports classifier-free guidance at no additional sampling cost.

## Method Summary
MeanFlow addresses the core limitation of existing one-step generative models by introducing average velocity as a more stable and generalizable alternative to instantaneous velocity. The method derives an intrinsic relationship between these two quantities, allowing the network to learn average velocity while naturally approximating instantaneous velocity without explicit heuristics. The training objective uses a weighted identity loss between the network's output and a target derived from this relationship. The approach is implemented within a latent diffusion framework using a DiT architecture with adaLN-Zero normalization. The method requires computing Jacobian-Vector Products to properly calculate derivatives of the network output with respect to time and noise variables, ensuring the loss captures the correct flow dynamics.

## Key Results
- Achieves FID 3.43 on ImageNet 256×256 with 1-NFE generation
- Outperforms previous one-step diffusion/flow models by 50-70% improvement
- Demonstrates strong scalability: B/4 (FID 10.64), L/4 (FID 6.56), XL/2 (FID 3.43)
- Shows zero-shot generalization to unseen resolutions and domains

## Why This Works (Mechanism)
The key insight is that average velocity provides a more stable training signal than instantaneous velocity. While instantaneous velocity requires computing time derivatives that can be noisy and unstable, average velocity is naturally available from the data and provides a smoother optimization landscape. The intrinsic relationship between average and instantaneous velocities means that by training on average velocity, the network implicitly learns to approximate instantaneous velocity as well, but in a more robust way. This eliminates the need for hand-crafted heuristics to bridge the gap between these quantities, resulting in better generalization and stability.

## Foundational Learning
- **Jacobian-Vector Product (JVP)**: Needed to compute derivatives of network outputs with respect to multiple inputs simultaneously; quick check: verify JVP implementation matches analytical derivatives for simple test functions.
- **Flow Matching Framework**: Understanding the continuous-time formulation of generative modeling; quick check: confirm basic FM implementation reproduces expected results before adding MeanFlow modifications.
- **Classifier-Free Guidance**: Technique for trading off fidelity and diversity; quick check: test CFG mixing with simple scalar values before implementing the full equation.
- **Logistic vs Gaussian Distributions**: The paper uses logit-normal sampling for time variables; quick check: verify sampling matches the intended distribution using statistical tests.
- **Adaptive Weighting Functions**: Used to balance different loss components; quick check: test weighting function with synthetic data to ensure it behaves as expected.
- **ViT/Transformer Architectures**: The DiT backbone with adaLN-Zero normalization; quick check: confirm standard DiT implementation works before adding MeanFlow-specific modifications.

## Architecture Onboarding

**Component Map:** ImageNet latents → DiT-B/4 (or larger) → MeanFlow loss → Optimized network

**Critical Path:** Data loading → JVP computation → Target calculation → Loss evaluation → Parameter update

**Design Tradeoffs:** Uses average velocity instead of instantaneous velocity for stability, eliminating need for complex heuristics but requiring JVP computation; trades off some theoretical purity for practical performance gains.

**Failure Signatures:** Incorrect JVP tangents cause dramatic performance drops (FID 268 vs 61); division by zero when t=r causes training instability; missing stopgrad on JVP term causes memory explosion.

**First Experiments:**
1. Implement basic Flow Matching with DiT-B/4 and verify it trains successfully on ImageNet latents
2. Add JVP computation with correct tangents and verify the target calculation works
3. Implement the full MeanFlow loss and compare against baseline FM performance

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation requires careful handling of Jacobian-Vector Products, which adds complexity
- Performance highly sensitive to correct implementation details (as shown in ablation studies)
- Requires ImageNet-scale training data for best results
- The adaptive weighting function introduces additional hyperparameters that affect performance

## Confidence
- **High confidence** in the theoretical contribution: the derivation of the intrinsic relation between average and instantaneous velocities is mathematically sound and well-explained.
- **Medium confidence** in the reproducibility of SOTA results: while the methodology is clearly described, critical implementation details like JVP tangents, weight initialization, and CFG scaling factors are not fully specified.
- **Medium confidence** in the claimed 50-70% improvement: the comparisons are against strong baselines, but the ablation studies suggest that precise implementation is crucial for achieving these gains.

## Next Checks
1. **JVP implementation verification**: Replicate the critical ablation in Table 1b by systematically varying the tangent vectors used in the Jacobian-Vector Product computation. Verify that the performance drops from FID 61 to 268 when tangents are incorrect, confirming the importance of this implementation detail.

2. **CFG mixing scale calibration**: Implement the Classifier-Free Guidance following the paper's guidance, testing different values of the mixing scale $\kappa$ across different model sizes (B/4, L/4, XL/2). Verify that the reported gains from CFG are reproducible and not sensitive to arbitrary parameter choices.

3. **Weighting function sensitivity**: Test the adaptive weighting function $w = 1/(||\Delta||^2 + c)^p$ with different values of $c$ and $p$ (the paper uses $p=1.0$) to determine if the reported performance is robust to these hyperparameters or if specific values are critical for success.