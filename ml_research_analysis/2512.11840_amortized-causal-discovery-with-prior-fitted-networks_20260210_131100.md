---
ver: rpa2
title: Amortized Causal Discovery with Prior-Fitted Networks
arxiv_id: '2512.11840'
source_url: https://arxiv.org/abs/2512.11840
tags:
- likelihood
- causal
- discovery
- graph
- structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in differentiable penalized likelihood
  methods for causal discovery, specifically the error-prone likelihood estimation
  that prevents accurate structure learning. The authors propose leveraging Prior-Fitted
  Networks (PFNs) to amortize data-dependent likelihood estimation, which provides
  more reliable scores for structure learning.
---

# Amortized Causal Discovery with Prior-Fitted Networks

## Quick Facts
- **arXiv ID:** 2512.11840
- **Source URL:** https://arxiv.org/abs/2512.11840
- **Reference count:** 7
- **Primary result:** PFN-based amortized likelihood estimation with policy gradient optimization significantly improves causal discovery accuracy on synthetic datasets compared to standard baselines.

## Executive Summary
This paper addresses the error-prone likelihood estimation in differentiable penalized likelihood methods for causal discovery. The authors propose using Prior-Fitted Networks (PFNs) to amortize data-dependent likelihood estimation, providing more reliable scores for structure learning. Their method combines PFNs for likelihood estimation with policy gradient optimization (PPO) for graph structure optimization. Experiments demonstrate significant improvements over standard baselines on synthetic datasets, with the approach also showing more accurate likelihood estimates compared to conventional neural network-based approaches.

## Method Summary
The method casts causal discovery as a reinforcement learning problem where PFNs provide non-differentiable likelihood rewards for policy gradient optimization. A graph posterior is parametrized using Gumbel-Softmax and SoftSort operations on matrices P and W, enabling differentiable DAG sampling. The TabPFN model estimates log-likelihoods in a single forward pass given parent sets, avoiding costly retraining. PPO updates the posterior parameters using these likelihood scores as rewards, with an exponential moving average baseline for advantage estimation. The approach addresses the key limitation of error-prone likelihood estimation in standard differentiable penalized likelihood methods.

## Key Results
- PFN shows 100-10000x lower bootstrap variance than MLP across sample sizes
- MLP incorrectly ranks 55-3137 graphs above ground truth while PFN achieves 0 errors for n≥250
- ACD achieves SHD 1.44 vs 17.30 (DCDI) on ER(10,20) but performs worse than PC on Causal Chambers (44 vs 38)

## Why This Works (Mechanism)

### Mechanism 1: Amortized Bayesian Likelihood Estimation
PFNs provide more accurate and lower-variance likelihood estimates than conventional neural networks trained from scratch. PFNs are pre-trained on millions of synthetic datasets sampled from a prior distribution, learning to approximate Bayesian posterior predictive distributions in a single forward pass. The prior used during PFN pre-training captures inductive biases relevant to the target domain's data-generating process.

### Mechanism 2: Policy Gradient Optimization for Non-Differentiable Scores
Casting structure learning as an RL problem enables optimization when likelihood estimates are non-differentiable. The DAG sampling procedure defines a differentiable policy over graph structures. PFN likelihood scores serve as non-differentiable rewards. PPO updates posterior parameters by constraining policy deviation between iterations.

### Mechanism 3: Prior Alignment Determines Generalization
Performance gains correlate with alignment between PFN's training prior and target domain structure. TabPFN was pre-trained on synthetic Bayesian networks and structural causal models with random functional relationships. Synthetic benchmarks share this generating process, yielding strong prior match and large SHD improvements. Real-world/semi-synthetic domains have different generating mechanisms, reducing prior alignment.

## Foundational Learning

- **Prior-Fitted Networks (PFNs) and In-Context Learning**: Why needed here - The entire method depends on understanding how PFNs amortize Bayesian inference without retraining. Quick check: Can you explain why a PFN outputs a posterior predictive distribution after a single forward pass, and what role the training prior plays?

- **Penalized Likelihood Causal Discovery**: Why needed here - This is the baseline paradigm being improved; the score function s(G,D) = log p(D|G) - λ|G| is what PFNs estimate. Quick check: Why does the penalized likelihood approach only recover the Markov equivalence class rather than the true DAG?

- **Proximal Policy Optimization (PPO)**: Why needed here - Graph optimization uses PPO to update posterior parameters given non-differentiable PFN rewards. Quick check: What does the clipping constraint in PPO prevent, and how is the advantage function computed in this paper?

- **Differentiable DAG Parametrization**: Why needed here - The Charpentier et al. parametrization enables gradient-based policy learning over discrete graphs. Quick check: How does the product A = E · (Π^T × M × Π) guarantee acyclicity while remaining differentiable?

## Architecture Onboarding

- **Component map**: Data splitter -> Graph sampler -> PFN likelihood estimator -> Score aggregator -> PPO optimizer
- **Critical path**: Initialize (P, W) randomly → defines initial graph distribution → Sample K graphs A_{t,k} from current posterior → For each graph, extract parent sets and compute PFN likelihoods → Aggregate scores and update (P, W) via PPO → Repeat for T iterations until convergence
- **Design tradeoffs**: D_train / D_est split ratio affects PFN accuracy vs score reliability; K (samples per PPO update) balances gradient estimate quality and compute; λ controls sparsity preference; PFN prior selection determines domain alignment
- **Failure signatures**: High likelihood variance indicates prior misalignment; SHD not decreasing suggests PPO hyperparameter issues; convergence to dense graphs indicates λ too low; domain mismatch shows strong synthetic but poor real-world performance
- **First 3 experiments**: 1) Likelihood estimator validation - compare PFN vs MLP on bootstrap variance and ranking accuracy; 2) Synthetic benchmark sweep - run on ER(10,20) and ER(30,60) comparing SHD against PC and DCDI; 3) Prior alignment stress test - evaluate on SERGIO or Causal Chambers to quantify performance degradation from prior mismatch

## Open Questions the Paper Calls Out
- Can domain-specific fine-tuning of the Prior-Fitted Network recover performance on datasets where the pre-trained prior is misaligned? The authors explicitly suggest "fine-tuning existing priors" as a direction for future work, noting that weaker performance on Causal Chambers and SERGIO implies prior misalignment.

- How does the method scale to larger or denser graphs where the number of parent variables might exceed the feature capacity of the transformer? The experiments are restricted to graphs of 10-30 nodes, and TabPFN has constraints on input feature dimensions which limits the size of parent sets.

- Does replacing the fixed train/estimation data split with cross-validation significantly improve structure recovery in low-data regimes? Footnote 2 states the split is an "arbitrary decision" and suggests using cross-validation to "better leverage the available data," as the current approach reduces effective sample size for training the likelihood estimator.

## Limitations
- Performance degrades significantly when target domain distributions differ from PFN's synthetic training prior
- Scalability limited by transformer context window for large parent sets in dense graphs
- Current implementation uses arbitrary train/estimation data split rather than optimal cross-validation strategy

## Confidence
- **Core method validity**: High - The PFN likelihood estimation advantage is well-validated with quantitative comparisons (Table 1)
- **RL optimization approach**: Medium - PPO implementation details are sparse, though the overall framework is sound
- **Prior alignment claims**: High - Performance differences across datasets are clearly documented and explained
- **Scalability assessment**: Low - Limited evaluation on larger graphs makes scalability claims uncertain

## Next Checks
1. Replicate Table 1 - sample 30 bootstrap datasets from a 5-node SCM, compare PFN vs MLP on bootstrap variance, NLL, and number of incorrectly ranked structures
2. Run synthetic benchmark sweep on ER(10,20) and ER(30,60) with known ground truth, comparing SHD against PC and DCDI while monitoring convergence
3. Evaluate on SERGIO or Causal Chambers datasets to quantify performance degradation from prior mismatch and validate the alignment hypothesis