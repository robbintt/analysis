---
ver: rpa2
title: 'TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs'
arxiv_id: '2602.00288'
source_url: https://arxiv.org/abs/2602.00288
tags:
- video
- temporal
- timeblind
- benchmark
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TimeBlind is a benchmark that tests how well multimodal models
  understand fine-grained temporal dynamics in videos. Unlike existing benchmarks,
  it uses minimal video pairs that differ only in temporal structure, paired with
  complementary questions that prevent models from relying on static or linguistic
  shortcuts.
---

# TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs

## Quick Facts
- arXiv ID: 2602.00288
- Source URL: https://arxiv.org/abs/2602.00288
- Reference count: 29
- TimeBlind reveals current Video LLMs achieve only 48.2% Instance Accuracy on temporal reasoning tasks, far below human performance of 98.2%.

## Executive Summary
TimeBlind introduces a novel benchmark designed to test genuine temporal reasoning in multimodal models by using minimal video pairs that differ only in temporal structure, paired with complementary questions that force models to engage with motion dynamics rather than relying on static visual or linguistic shortcuts. Organized around a cognitive taxonomy spanning Events, Event Attributes, and Structural Event Logic, the benchmark includes 11 fine-grained categories. Evaluations of 20+ state-of-the-art models show that even the best performing models achieve only 48.2% Instance Accuracy, revealing a substantial gap in temporal understanding compared to human performance of 98.2%.

## Method Summary
TimeBlind uses a controlled design where each instance consists of a video pair with near-identical static content but differing only in temporal structure, paired with two complementary questions that have opposite correct answers across the videos. This design isolates temporal reasoning by preventing models from using static visual shortcuts or language priors. The benchmark includes 600 instances (2,400 pairs) organized across three taxonomy levels and 11 fine-grained categories. Models are evaluated zero-shot using uniform 1 FPS sampling, with Instance Accuracy as the primary metric requiring correct answers to all four (video, question) combinations per instance.

## Key Results
- Best performing model (Gemini 3 Pro) achieves only 48.2% Instance Accuracy, far below human performance of 98.2%
- GPT-5 achieves 77.3% standard accuracy but only 46.3% Instance Accuracy, demonstrating heavy reliance on shortcuts
- Models show particular weakness on Event Attributes (Speed, Force) with I-Acc around 32%, indicating poor understanding of physics-related dynamics
- Larger models don't consistently outperform smaller ones (e.g., LLaVA-72B vs Molmo2-8B), suggesting architecture and training quality matter more than scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimal video pairs isolate temporal reasoning by canceling static visual shortcuts.
- Mechanism: Each instance pairs two videos with near-identical static content that differ solely in temporal structure. Since static cues cannot discriminate between the pair, models must encode motion dynamics to succeed.
- Core assumption: Models exploit object co-occurrence and visual correlations as shortcuts when available.
- Break condition: If video pairs contain unintended static differences (e.g., lighting changes), the isolation property fails.

### Mechanism 2
- Claim: Complementary questions neutralize language priors by forcing answer inversion across the video pair.
- Mechanism: For any question, the ground-truth answer flips between videos, preventing models from guessing based on textual plausibility alone.
- Core assumption: Language models default to statistically plausible answers when visual evidence is ambiguous.
- Break condition: If questions are phrased asymmetrically or one is easier to parse, models may succeed on the easier question and fail on the harder one.

### Mechanism 3
- Claim: Instance Accuracy (I-Acc) as the primary metric enforces joint correctness across four trials per instance.
- Mechanism: An instance is correct only if the model answers all four (video, question) combinations correctly, requiring genuine temporal understanding.
- Core assumption: High performance on standard metrics can arise from exploiting shortcuts rather than genuine understanding.
- Break condition: If models overfit to specific question templates or develop temporal heuristics that generalize poorly.

## Foundational Learning

- Concept: **Allen's Interval Algebra**
  - Why needed here: TimeBlind evaluates all 13 temporal relations (before, after, meets, overlaps, during, etc.) under Structural Event Logic.
  - Quick check question: Given two events A and B where A ends exactly when B begins, which Allen relation applies?

- Concept: **Minimal Pairs (Linguistics)**
  - Why needed here: Borrowed from phonology, this design principle uses controlled variation to isolate the dimension being tested.
  - Quick check question: If a minimal pair accidentally differs in background lighting, what confound is introduced?

- Concept: **Shortcut Exploitation in Benchmarks**
  - Why needed here: The paper's central critique is that high benchmark scores often reflect shortcut exploitation rather than targeted capability.
  - Quick check question: Why might a model correctly answer "Is the person running?" without processing motion?

## Architecture Onboarding

- Component map: Schema Generator -> Video Acquisition -> Manual Review Pipeline -> Evaluation Harness
- Critical path: Schema generation → video collection → human verification → paired evaluation. Verification is the bottleneck.
- Design tradeoffs:
  - Scale vs. diagnostic precision: 600 instances (2,400 pairs) prioritizes high-quality annotation over coverage
  - Binary vs. multiple-choice: Equal split (1,200 each) balances simplicity with discrimination
  - FPS sampling: 1 FPS is default for compatibility but may miss sub-second dynamics
- Failure signatures:
  - High Acc (~70%) with low I-Acc (<30%) → model relies on per-question shortcuts
  - Q-Acc < V-Acc → stronger language bias than visual bias
  - Near-random I-Acc on Event Attributes (Speed, Force) → physics-related dynamics poorly captured
- First 3 experiments:
  1. Run single-frame, language-only, and shuffled-frame conditions to confirm TimeBlind resists non-temporal solutions
  2. Evaluate at 1, 2, 4 FPS to test whether I-Acc improves with denser temporal input
  3. Compute I-Acc across the 11 fine-grained categories to identify specific deficits

## Open Questions the Paper Calls Out

- **Open Question 1**: What architectural or training innovations are required to overcome the reliance on static visual shortcuts in current Video LLMs?
  - Basis: Results show that simply scaling model size or increasing input frames yields only marginal gains, failing to instill genuine temporal logic.
  - Evidence to resolve: A model architecture explicitly designed for temporal integration achieving high Instance Accuracy (>80%) without using language priors.

- **Open Question 2**: How can models be improved to capture continuous "Event Attributes" like force and magnitude, where they currently perform near random chance?
  - Basis: The authors note a "systematic deficiency in current models' understanding of low-level, physics-related temporal dynamics," specifically in the Force and Speed sub-categories.
  - Evidence to resolve: Specialized training on physics-rich datasets significantly closing the performance gap in the Event Attributes category.

- **Open Question 3**: Does the temporal understanding measured by TimeBlind generalize to more diverse, uncontrolled real-world scenarios?
  - Basis: "The benchmark primarily features videos from controlled settings... We encourage future work to expand temporal reasoning evaluation to more diverse contexts."
  - Evidence to resolve: Evaluation of models on a "TimeBlind-Wild" dataset showing consistent failure modes across diverse populations and environments.

## Limitations

- The benchmark relies on human annotation for video pair validation, introducing potential subjectivity
- The dataset's scale (600 instances) prioritizes diagnostic precision over coverage, limiting representation of rare temporal phenomena
- The 1 FPS sampling rate may undersample fine-grained temporal dynamics that require higher frame rates for accurate modeling

## Confidence

- **High confidence**: The benchmark's core design principle (minimal video pairs with complementary questions) is sound and effectively isolates temporal reasoning
- **Medium confidence**: The taxonomy-based categorization captures relevant temporal reasoning dimensions, but may not exhaustively represent all temporal phenomena
- **Low confidence**: The claim that 1 FPS sampling is sufficient for all temporal dynamics requires further validation, particularly for categories where sub-second resolution may be critical

## Next Checks

1. **Temporal resolution ablation**: Evaluate TimeBlind at 1, 2, 4, and 8 FPS across all 20+ models to determine whether I-Acc plateaus at 1 FPS or continues improving with denser temporal input.

2. **Cross-taxonomy generalization**: Test whether models that excel at Temporal Topology (Allen relations) also perform well on Event Attributes (Speed, Force), or if these represent orthogonal capabilities requiring distinct architectural features.

3. **Synthetic augmentation stress test**: Generate synthetic temporal perturbations (frame drops, temporal warping) on existing TimeBlind instances to measure model robustness to common video processing artifacts and determine whether performance degrades uniformly or selectively across categories.