---
ver: rpa2
title: 'Beyond WER: Probing Whisper''s Sub-token Decoder Across Diverse Language Resource
  Levels'
arxiv_id: '2509.25516'
source_url: https://arxiv.org/abs/2509.25516
tags:
- languages
- sub-token
- resource
- whisper
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Whisper's multilingual ASR decoder at the
  sub-token level, revealing systematic decoding disparities masked by aggregate metrics
  like WER. The study traces beam search paths to capture sub-token hypotheses and
  their probabilities across 20 languages with varying resource levels, focusing on
  characteristics such as rank of correct tokens, confidence, predictive entropy,
  and alternate-candidate diversity.
---

# Beyond WER: Probing Whisper's Sub-token Decoder Across Diverse Language Resource Levels

## Quick Facts
- arXiv ID: 2509.25516
- Source URL: https://arxiv.org/abs/2509.25516
- Authors: Siyu Liang; Nicolas Ballier; Gina-Anne Levow; Richard Wright
- Reference count: 16
- Key outcome: This paper investigates Whisper's multilingual ASR decoder at the sub-token level, revealing systematic decoding disparities masked by aggregate metrics like WER.

## Executive Summary
This paper investigates Whisper's multilingual ASR decoder at the sub-token level, revealing systematic decoding disparities masked by aggregate metrics like WER. The study traces beam search paths to capture sub-token hypotheses and their probabilities across 20 languages with varying resource levels, focusing on characteristics such as rank of correct tokens, confidence, predictive entropy, and alternate-candidate diversity. Higher resource languages consistently exhibit higher prediction confidence, lower entropy, more frequent top-ranking of correct tokens, and greater hypothesis diversity. PCA and t-SNE analyses further show that linguistic typology shapes sub-token usage patterns, with related languages clustering together regardless of resource tier, while unrelated low-resource languages cluster due to shared decoding patterns. These findings expose how resource disparities manifest internally within the decoder and suggest targeted interventions like language-specific adapters and adaptive decoding strategies to improve fairness in multilingual ASR.

## Method Summary
The study analyzes Whisper-large-v2's multilingual decoder using Common Voice 17.0 dataset, sampling ~10 minutes per language from 20 Latin-script languages (excluding English and languages with WER > 60%). The methodology extracts full probability distributions at each decoding step, recording top-50 candidate tokens and their probabilities. Key metrics include confidence (softmax probability of chosen token), predictive entropy (top-50 candidates), average rank of correct tokens (via Levenshtein alignment), and alternate-candidate diversity (TTR of non-top-1 in top-50). Sub-token usage patterns are visualized through PCA and t-SNE using frequency vectors from top-10 candidates. Languages are categorized by Whisper training hours to examine resource-level effects.

## Key Results
- Higher resource languages show significantly higher confidence, lower predictive entropy, and more frequent top-ranking of correct tokens
- PCA and t-SNE analyses reveal that related languages cluster together regardless of resource tier, while unrelated low-resource languages cluster due to shared decoding patterns
- The analysis exposes systematic decoding disparities masked by aggregate metrics like WER, suggesting resource disparities manifest internally within the decoder
- Findings indicate that linguistic typology shapes sub-token usage patterns at the decoder level

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Higher training data volumes produce more peaked predictive distributions in the decoder, manifesting as higher confidence and lower entropy for high-resource languages.
- **Mechanism:** Extensive exposure to diverse linguistic phenomena during pre-training enables formation of more robust, well-calibrated sub-token representations. The model assigns higher probabilities to correct tokens and maintains sharper distributions over alternatives.
- **Core assumption:** Decoder confidence (softmax probability of chosen token) meaningfully reflects model certainty rather than miscalibration.
- **Evidence anchors:**
  - [abstract] "Higher resource languages benefit from higher likelihood of the correct token being top-ranked, greater confidence, lower predictive entropy"
  - [Section 5.2] Figures 3-4 show confidence positively correlates with training hours, entropy negatively correlates (both p < 0.001)
  - [corpus] Limited direct corroboration; neighbor papers focus on WER scaling rather than decoder-internal metrics
- **Break condition:** If correct tokens consistently rank low despite high confidence, indicates miscalibration rather than genuine certainty.

### Mechanism 2
- **Claim:** Linguistic typology shapes sub-token usage patterns at the decoder level, causing related languages to cluster together regardless of resource tier.
- **Mechanism:** Shared morphosyntactic and phonological features produce similar tokenization patterns and sub-token frequency distributions. The BPE vocabulary captures these structural regularities, creating family-level representational similarity.
- **Core assumption:** Sub-token frequency vectors (top-10 candidates) adequately capture language-specific decoding behavior.
- **Evidence anchors:**
  - [Section 5.4] PCA shows Romance languages cluster cohesively; Uralic (Finnish-Estonian), North Germanic (Swedish-Norwegian) pairs stay close
  - [Section 6.2] "Typological relationships operate at multiple scales: family-level similarities captured by linear methods"
  - [corpus] No direct typological clustering analysis found in neighbor papers
- **Break condition:** When typologically related languages with similar resource levels fail to cluster, suggests vocabulary inadequacy or acoustic confusions dominate.

### Mechanism 3
- **Claim:** Unrelated low-resource languages converge on similar "generic decoding" patterns due to insufficient training exposure, causing them to cluster independently of linguistic similarity.
- **Mechanism:** Limited training data prevents development of language-specific sub-token representations. The decoder falls back to generic patterns shared across poorly-represented languages, visible as cross-linguistic clustering in dimensionality reduction.
- **Core assumption:** Clustering of typologically distant languages indicates representation deficiency rather than coincidental similarity.
- **Evidence anchors:**
  - [Section 5.4] "Typologically unrelated lower resource languages, such as Welsh, Lithuanian, Latvian, and Basque, cluster together despite their significant linguistic differences"
  - [Section 6.2] "This unexpected grouping suggests these languages are handled similarly...due to shared status as low-resource languages"
  - [corpus] Related work on data scaling limits (arxiv 2510.22492) supports resource-driven representation gaps
- **Break condition:** If low-resource languages from same family fail to cluster together, suggests resource scarcity overwhelms typological signal.

## Foundational Learning

- **Concept: Byte Pair Encoding (BPE) tokenization in multilingual models**
  - Why needed here: The paper analyzes sub-token behavior; understanding how BPE segments text differently across languages is prerequisite for interpreting rank, diversity, and clustering results.
  - Quick check question: Why might an agglutinative language like Turkish produce longer sub-token sequences than an analytic language with equivalent semantic content?

- **Concept: Beam search decoding with probability tracking**
  - Why needed here: The methodology extracts top-K candidates and their probabilities at each step; understanding beam search mechanics is essential for implementing the probing pipeline.
  - Quick check question: If beam width is 5, what information is lost by only analyzing the final chosen path versus all maintained hypotheses?

- **Concept: Predictive entropy as uncertainty quantification**
  - Why needed here: Entropy is a core metric distinguishing resource tiers; understanding its relationship to confidence and calibration enables proper interpretation.
  - Quick check question: A token has probability 0.8 (confidence=0.8). What additional information does entropy provide about the remaining 0.2 probability mass?

## Architecture Onboarding

- **Component map:** Audio → Log-mel spectrogram → Encoder → Decoder (autoregressive) → At each step: extract probability distribution over 51,865 tokens → Record top-50 candidates + probabilities → Aggregate metrics (confidence, entropy, rank, diversity)

- **Critical path:** Audio → Log-mel spectrogram → Encoder → Decoder (autoregressive) → At each step: extract probability distribution over 51,865 tokens → Record top-50 candidates with log-probabilities → Aggregate metrics (confidence, entropy, rank, diversity)

- **Design tradeoffs:**
  - Top-K candidate window (K=50): Larger K captures more alternatives but increases storage/computation
  - Sample duration (10 min): Covers ~73% of unique tokens vs. 60 min sample; longer samples capture rare tokens but may introduce domain variance
  - Latin-script only scope: Controls for script confounds but excludes typologically diverse writing systems

- **Failure signatures:**
  - High average rank of correct tokens (>20): Model rarely places ground truth in top candidates
  - Low confidence + high entropy (>3 bits): Decoder uncertain, flat distribution
  - Low alternate-candidate diversity (TTR <0.1): Constrained hypothesis space
  - Unrelated low-resource languages clustering in PCA/t-SNE: Generic decoding regime

- **First 3 experiments:**
  1. Replicate sub-token extraction pipeline on 3 languages (1 high, 1 medium, 1 low resource) with 10 min each; compute confidence, entropy, and average rank metrics to validate resource-tier correlations.
  2. Construct sub-token frequency vectors from top-10 candidates for all test languages; apply PCA and verify typological clusters (Romance, Uralic) and low-resource generic cluster emerge.
  3. Ablation: Compare top-10 vs. top-50 candidate windows for diversity and clustering analyses to confirm K=10 adequately captures language distinctions without noise from rare tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can decoder metrics (confidence, entropy, correct token rank) predict hallucination events in real-time?
- Basis in paper: [explicit] Paper identifies hallucination as critical and proposes decoder metrics as "valuable signals for adaptive adjustments."
- Why unresolved: Metrics shown correlating with resource level, not validated for predicting specific failure modes.
- What evidence would resolve it: Experiments demonstrating confidence/entropy thresholds predict hallucination events with high precision/recall.

### Open Question 2
- Question: Why do Estonian and Azerbaijani show high local sub-token accuracy but not low WER?
- Basis in paper: [explicit] Section 6.3 notes this "intriguing incongruity" between local prediction and global performance.
- Why unresolved: Phenomenon identified but root cause (beam search heuristics, cascading errors) undiagnosed.
- What evidence would resolve it: Beam search ablation studies; error analysis quantifying cascading vs. isolated errors.

### Open Question 3
- Question: What causes Estonian and Basque to be consistent outliers in sub-token usage patterns?
- Basis in paper: [explicit] Paper states these languages "may require targeted investigation" for "idiosyncratic sub-tokenization patterns."
- Why unresolved: Outlier status observed across PCA and t-SNE but mechanism unexplained.
- What evidence would resolve it: BPE vocabulary coverage analysis; correlation with typological features (e.g., agglutinative morphology).

## Limitations
- Analysis restricted to Latin-script languages only, excluding major linguistic families with non-Latin writing systems
- 10-minute sample duration may not adequately represent low-frequency phenomena or domain-specific variations
- Assumes Whisper-large-v2 as representative of multilingual ASR architectures, results may not transfer to smaller variants

## Confidence
### High Confidence Claims
- Resource-tier correlations with decoder metrics: The positive correlation between training hours and confidence, and negative correlation with entropy, is well-supported by statistical analysis (p < 0.001) across 20 languages with systematic sampling.

### Medium Confidence Claims
- Typological clustering validity: While PCA shows Romance and Uralic language clusters, the strength of typological signal versus acoustic or tokenization artifacts requires further validation.

### Low Confidence Claims
- Causal mechanisms: The paper identifies correlations between resource levels and decoding behavior but does not establish causal mechanisms through controlled ablation studies or model interventions.

## Next Checks
1. **Cross-model validation:** Apply the sub-token analysis methodology to Whisper-base and Whisper-tiny models to determine whether observed resource-tier patterns hold across model scales, or if they are specific to the large variant's capacity and training dynamics.

2. **Script diversity expansion:** Extend the analysis to include non-Latin script languages (e.g., Chinese, Arabic, Hindi) to test whether the identified resource-tier and typological patterns generalize across different writing systems and linguistic typologies.

3. **Downstream impact assessment:** Design a user-centric evaluation that correlates identified sub-token decoding disparities with actual ASR performance degradation in realistic use cases, such as conversational speech or domain-specific terminology recognition.