---
ver: rpa2
title: 'Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended
  LLM Reasoning'
arxiv_id: '2602.01791'
source_url: https://arxiv.org/abs/2602.01791
tags:
- rewards
- judge
- policy
- reasoning
- open-ended
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRAD2REWARD introduces a dense-reward framework for improving LLM
  reasoning on open-ended tasks by extracting token-level supervision directly from
  the Judge's internal gradient signals via a single backward pass. Unlike prior approaches
  that treat the Judge as a black box and use only sparse sequence-level rewards,
  GRAD2REWARD quantifies each token's contribution to the Judge's decision, enabling
  fine-grained credit assignment.
---

# Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning

## Quick Facts
- arXiv ID: 2602.01791
- Source URL: https://arxiv.org/abs/2602.01791
- Authors: Zheng Zhang; Ao Lu; Yuanhao Zeng; Ziwei Shan; Jinjin Guo; Lufei Li; Yexin Li; Kan Ren
- Reference count: 40
- One-line result: Dense token-level rewards from Judge gradients improve open-ended LLM reasoning by 3.8-5.0 points and converge 1.7×-1.9× faster than sparse-reward baselines

## Executive Summary
GRAD2REWARD introduces a dense-reward framework that extracts token-level supervision directly from LLM-as-a-Judge internal gradient signals via a single backward pass. Unlike prior approaches that treat the Judge as a black box and use only sparse sequence-level rewards, GRAD2REWARD quantifies each token's contribution to the Judge's decision through gradient × embedding attribution. It also introduces a self-judging mechanism where the Judge is a frozen copy of the initial policy, enabling the model to improve through its own evaluative feedback without relying on stronger external models. Experimental results show that GRAD2REWARD consistently outperforms strong sparse-reward baselines, achieving up to 3.8-point gains on medical consultation tasks and 5.0-point gains on academic QA while converging 1.7×–1.9× faster.

## Method Summary
GRAD2REWARD extracts dense token-level rewards from a frozen Judge (initially the policy itself) by computing gradients of the Judge's verdict probability with respect to each token embedding, then using the dot product of gradient and embedding as attribution scores. These scores are normalized via softmax and scaled by the sequence-level reward to create token rewards. The method trains via token-level GRPO, computing returns and advantages for each token position. A rubric system with multiple criteria per query aggregates weighted attributions across rubrics. The approach requires only a single backward pass per evaluation, making it computationally efficient while providing fine-grained credit assignment that accelerates learning and improves performance on open-ended reasoning tasks.

## Key Results
- Achieves 44.5 rubric score on HealthBench vs 40.8 for Vanilla-GRPO (3.8-point gain)
- Matches or exceeds state-of-the-art process reward models on mathematical reasoning tasks
- Converges 1.7×-1.9× faster than sparse-reward baselines
- Self-judging performs competitively with external judges (44.5 vs 45.9 on HealthBench)
- Generalizes well across medical consultation and academic QA domains

## Why This Works (Mechanism)

### Mechanism 1: Gradient × Embedding Attribution for Dense Credit Assignment
Token-level attribution scores derived from Judge gradients decompose sparse sequence rewards into informative dense rewards. Given the Judge's decision log-probability F(e₁,...,eₜ), compute gₜ = ∇ₑₜ log p_judge(z|x, o, c). Attribution bₜ = gₜᵀeₜ measures each token's first-order contribution to the verdict. Softmax normalization converts these to relative weights αₜ, which scale the sequence-level reward r(x,o) into token rewards rₜ = αₜ · r(x,o). The gradient direction (not just magnitude) encodes semantic alignment between token embeddings and the Judge's evaluative criteria.

### Mechanism 2: First-Order Taylor Approximation Justifies Reward Decomposition
The sum of token-level attributions approximates the total change in Judge output relative to a zero-embedding baseline, providing theoretical grounding for decomposition. From Taylor expansion, F(e₁,...,eₜ) - F(0,...,0) ≈ Σₜ gₜᵀeₜ. Each term measures contribution to Judge decision; summing approximates sequence-level signal. This makes token rewards mathematically consistent with the global objective, assuming first-order approximation is sufficiently accurate near the operating point.

### Mechanism 3: Self-Judging Leverages Discriminative Superiority
A frozen copy of the initial policy serves as an effective Judge, exploiting LLMs' stronger discriminative than generative capabilities. Freeze the initial policy as Judge p_judge; train only the active policy π_θ via RL. The Judge provides stable evaluative signals while the policy improves through self-improvement rather than distillation from stronger external models. This works because the model's discriminative capacity exceeds its generative capacity at initialization and persists during early training.

## Foundational Learning

- **Policy Gradient Credit Assignment**: Needed to understand why sparse rewards cause high-variance gradient estimates in long sequences and how dense rewards solve this. Quick check: Can you explain why sparse rewards cause high-variance gradient estimates in long sequences?

- **Gradient-based Attribution (Integrated Gradients family)**: Needed to understand why gradient × input captures "contribution" vs. raw sensitivity. Quick check: Why does gₜᵀeₜ capture directional alignment, while ||gₜ||₂ only captures magnitude?

- **GRPO (Group Relative Policy Optimization)**: Needed to understand how token-level advantages extend standard sequence-level advantages. Quick check: How does GRPO's group-based advantage estimation differ from PPO's value-function baseline?

## Architecture Onboarding

- **Component map**: Policy π_θ → Judge p_judge → Gradient Attribution Module → Token-level GRPO → Rubric System
- **Critical path**: Sample query x, generate G responses via policy → For each response, Judge evaluates against K rubric items → For positive verdicts, backward pass extracts gradients → Attribution scores → Token rewards → Aggregate across rubrics → Compute token-level returns and advantages → Update policy via clipped objective
- **Design tradeoffs**: Temperature τ affects attribution sharpness (lower = sharper, higher = diffuse); self-judging reduces infrastructure cost but caps supervision quality; single backward pass is efficient but scales with rubric items × responses
- **Failure signatures**: All tokens receive similar attribution (check τ and Judge calibration); training diverges early (inspect αₜ distribution and advantage normalization); self-judging underperforms substantially (verify Judge discriminative capacity)
- **First 3 experiments**: 1) Attribution method ablation: Compare Gradient × Embedding vs. L1/L2 norms, expect Gradient × Embedding to outperform (Table 2 confirms). 2) Self-judging vs. external Judge comparison: Train identical policies with self-judge vs. external Judge, expect marginal gap (Table 3). 3) Convergence speed benchmark: Track steps-to-threshold for Grad2Reward vs. Vanilla-GRPO, expect 1.7×-1.9× faster (Figure 2).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Grad2Reward be effectively extended to long-horizon agent tasks involving multiple decision steps and sustained reasoning?
- **Basis in paper:** [explicit] The conclusion states: "Looking forward, our method can be extended to long-horizon agent tasks involving multiple decision steps and sustained reasoning, where gradient-based rewards have the potential to provide high-quality process supervision."
- **Why unresolved:** The current experimental scope is limited to single-turn reasoning tasks (medical QA, math problems) and does not evaluate multi-turn interactions or agentic workflows where state dynamics change over time.
- **What evidence would resolve it:** Empirical results from applying Grad2Reward to interactive agent benchmarks (e.g., web navigation or code execution environments) demonstrating stable policy optimization over long trajectories.

### Open Question 2
- **Question:** How does the choice of the reference baseline in the Taylor expansion impact the accuracy of the token-level rewards?
- **Basis in paper:** [inferred] Section 4.3 explains the theoretical justification relies on a first-order Taylor expansion and notes: "This baseline can, in principle, be chosen... For simplicity, we set it as the zero embedding."
- **Why unresolved:** The paper assumes the zero embedding is a sufficient reference point, but does not analyze if this choice introduces bias compared to other potential baselines (e.g., average embedding or a learned baseline).
- **What evidence would resolve it:** An ablation study comparing the convergence speed and final performance of Grad2Reward using different baseline vectors (zero vs. mean vs. learned) in the attribution calculation.

### Open Question 3
- **Question:** Does the self-judging mechanism create a performance ceiling when the initial policy lacks sufficient discriminative capability for complex rubrics?
- **Basis in paper:** [inferred] While Table 3 shows self-judging is competitive, the Impact Statement warns that "the societal impact depends heavily on the accuracy of the underlying models used as judges."
- **Why unresolved:** It is unclear if the method fails to improve the policy when the "judge" (the initial policy) is inherently incapable of distinguishing high-quality reasoning for a specific task.
- **What evidence would resolve it:** Analysis of training dynamics on tasks where the initial policy's judging accuracy is near random, specifically looking for signs of reward hacking or stagnation compared to using a stronger oracle judge.

## Limitations
- Attribution sensitivity to Judge calibration: If Judge's confidence scores or decision boundaries are poorly calibrated, attribution scores may misattribute credit
- Theoretical grounding gaps: First-order Taylor approximation provides intuitive justification but lacks empirical validation of approximation quality
- Temperature parameter sensitivity: Critical hyperparameter τ is not specified, making performance sensitivity unknown

## Confidence
- **High Confidence**: Claims about gradient × embedding attribution outperforming L1/L2 norms (Table 2); claims about faster convergence (1.7×-1.9×) (Figure 2); claims about state-of-the-art performance on verifiable domains
- **Medium Confidence**: Claims about self-judging matching external judges (44.5 vs 45.9 on HealthBench); claims about theoretical justification via Taylor expansion
- **Low Confidence**: Claims about attribution capturing "semantic alignment"; specific impact of temperature τ on attribution quality

## Next Checks
1. **Judge Calibration and Attribution Robustness Test**: Measure Judge accuracy and calibration on held-out samples before training. Then conduct controlled experiments varying Judge confidence thresholds and measuring impact on attribution quality and downstream policy performance. This validates whether Judge calibration affects attribution reliability.

2. **Temperature Sensitivity Ablation**: Implement a grid search over temperature values (τ ∈ [0.1, 0.5, 1.0, 2.0]) and measure impact on: (a) attribution distribution sharpness, (b) training stability, and (c) final task performance. This identifies optimal attribution concentration and potential sensitivity.

3. **First-Order Approximation Validation**: For a subset of queries, compute exact Judge output changes for individual token perturbations versus first-order Taylor approximation predictions. Quantify approximation error and correlate with sequence length and Judge non-linearity metrics to identify conditions where the theoretical justification breaks down.