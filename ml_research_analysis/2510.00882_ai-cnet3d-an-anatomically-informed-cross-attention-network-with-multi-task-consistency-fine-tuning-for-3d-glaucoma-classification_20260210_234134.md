---
ver: rpa2
title: 'AI-CNet3D: An Anatomically-Informed Cross-Attention Network with Multi-Task
  Consistency Fine-tuning for 3D Glaucoma Classification'
arxiv_id: '2510.00882'
source_url: https://arxiv.org/abs/2510.00882
tags:
- attention
- cross-attention
- glaucoma
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses glaucoma classification using 3D OCT volumes,
  focusing on extracting structural features while avoiding information loss inherent
  in 2D report generation. The proposed AI-CNet3D model integrates anatomically-informed
  cross-attention mechanisms into a 3D CNN to capture asymmetries between superior/inferior
  hemiretinas and relationships between ONH and macula regions.
---

# AI-CNet3D: An Anatomically-Informed Cross-Attention Network with Multi-Task Consistency Fine-tuning for 3D Glaucoma Classification

## Quick Facts
- arXiv ID: 2510.00882
- Source URL: https://arxiv.org/abs/2510.00882
- Authors: Roshan Kenia; Anfei Li; Rishabh Srivastava; Kaveri A. Thakoor
- Reference count: 36
- Primary result: Anatomically-informed cross-attention with consistency fine-tuning achieves 81.83-83.15% accuracy and 81.76-83.36% AUROC on 3D OCT glaucoma classification while reducing parameters by 100x

## Executive Summary
This paper addresses glaucoma classification from 3D OCT volumes by introducing AI-CNet3D, which integrates anatomically-informed cross-attention mechanisms into a 3D CNN. The model captures asymmetries between superior/inferior hemiretinas and relationships between ONH and macula regions through constrained attention operations. A novel multi-task fine-tuning approach enforces consistency between attention and convolutional visualizations, significantly improving both performance and interpretability. Evaluated on Topcon and Zeiss datasets, AI-CNet3D outperforms baseline models while being computationally efficient with 100x fewer parameters than standard attention mechanisms.

## Method Summary
AI-CNet3D is a hybrid 3D CNN-Attention architecture that processes OCT volumes through a backbone of five convolutional layers, with anatomically-informed cross-attention blocks inserted after layers 2 and 4. The model splits feature volumes along anatomical axes (superior/inferior) and computes bidirectional channel-only cross-attention between these regions. Training occurs in two stages: initial BCE-only training for 250 epochs, followed by multi-task fine-tuning that jointly optimizes classification and visualization consistency. The consistency loss minimizes MSE between CARE (attention visualization) and Grad-CAM (convolutional visualization) outputs. The approach is evaluated on two datasets (Topcon and Zeiss) with 50/50 balanced training data via random undersampling.

## Key Results
- AI-CNet3D achieves 81.83-83.15% accuracy and 81.76-83.36% AUROC across both datasets
- Model reduces parameters by ~100x compared to standard attention mechanisms (241k-291k vs 63-88M)
- Multi-task fine-tuning with visualization consistency further enhances performance by 1-2% across all metrics
- Anatomical constraint and channel-only attention configuration outperform spatial+channel combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anatomically-constrained cross-attention captures clinically-relevant structural asymmetries better than generic whole-volume attention.
- Mechanism: The model splits OCT volumes along anatomical axes—superior/inferior hemiretinas and ONH/macula—and computes bidirectional cross-attention only between anatomically related pairs. This forces the model to learn comparative relationships (e.g., healthy inferior hemiretina as reference for damaged superior) rather than treating the volume as undifferentiated 3D data.
- Core assumption: Glaucoma damage manifests asymmetrically between superior and inferior hemiretinas, and ONH-macula correlations carry diagnostic signal.
- Evidence anchors:
  - [abstract] "By dividing the volume along two axes and applying cross-attention, our model enhances glaucoma classification by capturing asymmetries between the hemiretinal regions while integrating information from the optic nerve head and macula."
  - [Section 4.1.1] Equations 1-3 describe bidirectional attention between superior/inferior halves with skip connections.
  - [corpus] Weak direct evidence—neighbor papers on glaucoma detection use attention but don't specifically validate anatomical constraint hypotheses.

### Mechanism 2
- Claim: Enforcing consistency between attention and convolutional visualizations during fine-tuning improves generalization by aligning local and global feature representations.
- Mechanism: After initial BCE-only training, the model undergoes multi-task fine-tuning that minimizes MSE between CARE (Channel Attention REpresentation from attention layers) and Grad-CAM (from final convolutional layer). This forces attention and convolutional pathways to converge on similar salient regions while maintaining their complementary strengths.
- Core assumption: Convolutional layers capture local features accurately; attention layers capture long-range dependencies; aligning them reduces spurious correlations.
- Evidence anchors:
  - [abstract] "aligning them with Gradient-Weighted Class Activation Maps (Grad-CAMs) from the CNN's final convolutional layer to enhance performance, interpretability, and anatomical coherence."
  - [Section 4.5] Equation 12 defines the combined loss: L_multi-task = (1-λ)L_supervised + λL_unsupervised
  - [Table 3] Fine-tuned models consistently outperform non-fine-tuned versions across both datasets.

### Mechanism 3
- Claim: Channel-only attention with anatomical constraints achieves comparable accuracy to full spatial-attention transformers while reducing parameters by ~100x.
- Mechanism: By eliminating spatial attention projections (which compress C×D×H×W → C×p with high parameter cost and information loss), and instead computing attention only along the channel dimension on anatomically-split subvolumes, the model avoids the O(n²) bottleneck while preserving diagnostic signal through domain structure.
- Core assumption: Anatomical priors compensate for reduced attention expressiveness; channel attention on anatomically-meaningful regions is sufficient.
- Evidence anchors:
  - [Section 4.1.1] "We found that projecting the entire spatial dimension of the feature volume into a small vector was not beneficial for model training"
  - [Figure 6] Shows AI-CNet3D variants at 241k-291k parameters vs 63-88M for ViT/TimeSformer, with comparable AUROC.
  - [Table 5] Channel-only cross-attention (81.83% accuracy) outperforms spatial+channel combination (80.18%).

## Foundational Learning

- **Concept: Cross-attention vs Self-attention**
  - Why needed here: The core innovation uses cross-attention between anatomical regions (different sources for Q, K, V), not self-attention on unified volumes.
  - Quick check question: Given feature maps A and B, can you write the equations for both self-attention on [A;B] versus cross-attention Q from A, K/V from B?

- **Concept: Grad-CAM derivation**
  - Why needed here: CARE-Grad-CAM consistency requires understanding how Grad-CAM weights feature maps via gradients to produce class-discriminative heatmaps.
  - Quick check question: Explain why Grad-CAM uses gradients of the class score w.r.t. feature maps rather than the feature maps directly.

- **Concept: Multi-task learning with auxiliary consistency losses**
  - Why needed here: Fine-tuning jointly optimizes classification (supervised) and visualization alignment (unsupervised), requiring understanding of loss balancing.
  - Quick check question: If λ=0.75 in L = (1-λ)L_sup + λ*L_unsup, what fraction of gradient signal comes from each loss term?

## Architecture Onboarding

- **Component map:**
  Input (128×192×112) → Conv1 (7×7×7) → Conv2 (5×5×5) → CrossAttentionBlock1 [splits z-axis, superior↔inferior] → Conv3 (3×3×3) → Conv4 (3×3×3) → CrossAttentionBlock2 → Conv5 (3×3×3) → GAP → Dense → Softmax

- **Critical path:** The cross-attention blocks must correctly split volumes along z-axis (superior/inferior) with consistent indexing. If orientation flips between training and inference, attention will compare wrong regions.

- **Design tradeoffs:**
  - Channel-only attention: ~100x parameter reduction vs limited spatial expressiveness
  - Two-stage training (BCE → fine-tune): More stable but requires extra epochs; joint training from scratch caused "early incorrect visualizations learned and propagated"
  - MSE vs other consistency losses: MSE outperformed SSIM/Pearson (Table 7) but may not generalize to other modalities

- **Failure signatures:**
  - High specificity, low sensitivity (as in non-fine-tuned Zeiss models): Model overfits to negative class
  - Inconsistent CARE/Grad-CAM in early layers after fine-tuning: Expected—only final layers are aligned; early layers retain mechanism-specific patterns
  - λ=1.0 causing degeneration: Unsupervised loss alone loses classification signal

- **First 3 experiments:**
  1. **Ablate attention placement**: Test cross-attention after conv (1,2), (1,3), (2,3), (2,4), (2,5), (3,4), (3,5)—Table 1 shows (2,4) is optimal. Replicate to confirm.
  2. **Validate anatomical constraint value**: Compare AI-CNet3D_H (superior/inferior split) against shuffled splits where volume is divided at random z-locations. If anatomical constraint matters, random splits should underperform.
  3. **Test consistency loss generalization**: Apply the same CARE-Grad-CAM fine-tuning protocol to the EPA baseline (Table 3 shows improvement). Determine whether improvement magnitude is task-specific or architecture-agnostic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain adaptation techniques effectively harmonize data from different OCT manufacturers (e.g., Topcon and Zeiss) to improve cross-manufacturer generalization where federated training failed?
- Basis in paper: [explicit] Appendix A.5 states that federated training yielded no substantial improvement due to distinct noise characteristics and artifacts, explicitly calling for future research on domain adaptation techniques to harmonize volume types.
- Why unresolved: The study attempted cross-manufacturer training via federated learning, but performance did not improve, likely due to fundamental differences in acquisition systems (noise/artifacts).
- What evidence would resolve it: Demonstrated improvement in classification performance on the Zeiss test set when augmenting training with harmonized Topcon data (or vice versa), surpassing the federated learning baseline.

### Open Question 2
- Question: Does incorporating the full imbalanced dataset into a pre-training task improve model robustness and generalization compared to the random undersampling strategy employed?
- Basis in paper: [explicit] Section 7 (Conclusions and Future Directions) explicitly lists incorporating the full set of available samples or additional external data into a pre-training task as a direction to improve robustness.
- Why unresolved: The current methodology relies on random undersampling to balance classes (50/50 split), which limits the number of training samples available to the model.
- What evidence would resolve it: Ablation studies showing that a model pre-trained on the full 4,932 non-glaucomatous and 272 glaucomatous samples achieves higher AUROC or F1-scores on the test set than the current undersampled baseline.

### Open Question 3
- Question: Can CARE visualizations be leveraged to create pseudo-segmentation masks that successfully identify and segment novel biomarkers for retinal diseases?
- Basis in paper: [explicit] Section 7 states an aim to "leverage CARE outputs to create pseudo-segmentation masks" and use the network to "identify and segment novel biomarkers."
- Why unresolved: While CARE visualizations highlight deeper retinal structures (e.g., photoreceptors) that imply novel biomarkers, they currently serve primarily as explainability tools rather than segmentation inputs.
- What evidence would resolve it: A pipeline where CARE-generated masks allow a segmentation model to delineate specific retinal layers or pathologies with high IoU scores compared to clinician-annotated ground truth.

## Limitations
- Anatomical constraint hypothesis lacks direct experimental validation through comparison with random splits
- Multi-task fine-tuning shows dataset-specific λ sensitivity (Zeiss λ=0.5 vs Topcon λ=0.75)
- Parameter reduction claim untested against other efficient architectures beyond large transformer baselines
- Federated learning across manufacturers failed due to fundamental acquisition differences
- Current methodology limits training samples through random undersampling strategy

## Confidence
- Mechanism 1 (anatomical cross-attention): **Medium** - Strong empirical support but missing ablation of anatomical constraint
- Mechanism 2 (consistency fine-tuning): **High** - Clear improvement across all experiments, though λ sensitivity varies
- Mechanism 3 (100x parameter reduction): **Medium** - Supported by comparisons but only against large transformer baselines

## Next Checks
1. **Anatomical constraint validation**: Implement AI-CNet3D with random volume splits at varying z-locations. If anatomical splits are essential, random splits should show 2-3% accuracy degradation.
2. **Consistency loss ablation**: Train AI-CNet3D without fine-tuning (λ=0.0) and with λ=1.0 to confirm the U-shaped performance curve shown in Table 6.
3. **Cross-dataset generalization**: Evaluate the Zeiss-trained model on Topcon data (or vice versa) to quantify domain adaptation limits and determine if the 100x parameter reduction compromises transfer learning.