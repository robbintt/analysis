---
ver: rpa2
title: Identifying Features Associated with Bias Against 93 Stigmatized Groups in
  Language Models and Guardrail Model Safety Mitigation
arxiv_id: '2512.19238'
source_url: https://arxiv.org/abs/2512.19238
tags:
- bias
- stigma
- guardrail
- social
- stigmas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates bias against 93 stigmatized groups in large
  language models using SocialStigmaQA benchmark. It examines how social features
  of stigma (aesthetics, concealability, course, disruptiveness, origin, peril), stigma
  cluster types, and prompt styles affect biased outputs.
---

# Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation

## Quick Facts
- arXiv ID: 2512.19238
- Source URL: https://arxiv.org/abs/2512.19238
- Authors: Anna-Maria Gueorguieva; Aylin Caliskan
- Reference count: 18
- Key outcome: Stigmas rated highly perilous (e.g., gang membership, HIV) show 60% biased outputs, while sociodemographic stigmas show 11% bias; guardrails reduce bias by 1.4-10.4% but fail to recognize bias intent

## Executive Summary
This study systematically investigates bias against 93 stigmatized groups in three large language models using the SocialStigmaQA benchmark. The researchers collected both human and LLM ratings of six social features of stigma (aesthetics, concealability, course, disruptiveness, origin, peril) and measured bias across Granite 3.0-8B, Llama-3.1-8B, and Mistral-7B. Results show that LLM ratings of features only weakly to moderately correlate with human ratings, with peril ratings being the strongest predictor of bias. The study also evaluates three guardrail models and finds they reduce bias by 1.4-10.4% but fail to identify bias intent, instead flagging inputs based on keywords.

## Method Summary
The study employs the SocialStigmaQA benchmark (10,360 prompts spanning 93 stigmatized groups × 37 social scenarios × 4 prompt styles) to measure bias in three LLMs. Each LLM rates 93 stigmas on six features using a classification-formatted prompt with 10 iterations per stigma-feature pair. Bias is measured by comparing LLM yes/no responses to a "biased answer" key, with outputs lacking explicit yes/no classified as unbiased. Guardrail models (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API) classify inputs as safe/unsafe, and post-mitigation bias is calculated by reclassifying guardrail-flagged outputs as unbiased. Statistical analysis includes Pearson correlations for feature alignment and McNemar's test for mitigation effectiveness.

## Key Results
- Stigmas rated highly perilous by humans elicit the most bias (60% biased outputs), while sociodemographic stigmas show the least (11%)
- LLM ratings of features weakly to moderately correlate with human ratings (0.19-0.46), with negative correlations for concealability
- Guardrail models reduce bias by 10.4% (Granite), 1.4% (Llama), and 7.8% (Mistral), but fail to recognize bias intent
- Features associated with bias (peril, course, concealability) remain statistically significant predictors post-mitigation

## Why This Works (Mechanism)

### Mechanism 1: Peril-Driven Bias Amplification
Training corpora encode statistical associations between peril-adjacent stigmas (e.g., gang membership, HIV) and negative social outcomes; during generation, these associations manifest as discriminatory recommendations in social scenarios. Human ratings of peril significantly predict biased output percentage (F = 41.8, p<.0001, η²p = 0.14).

### Mechanism 2: Keyword-Triggered Guardrail Classification
Guardrails employ pattern-matching on vocabulary associated with harm taxonomies; prompts containing stigmatized terms trigger category-specific flags without understanding whether the prompt's intent is discriminatory. Only 44 out of 10,360 bias-eliciting prompts were flagged under "Hate" category by Llama Guard.

### Mechanism 3: Feature-Persistent Bias Post-Mitigation
Guardrails filter some biased outputs post-hoc, but the LLM's internal representations linking stigma features to discriminatory behavior remain unchanged. Post-mitigation, human ratings of peril (F = 6.17, p = 0.014) and LLM ratings of concealability (F = 17.8, p<.0001) remain significant predictors.

## Foundational Learning

- **Six-dimensional stigma taxonomy (aesthetics, concealability, course, disruptiveness, origin, peril)**: The paper operationalizes Jones et al. (1984) psychology framework to predict which stigma characteristics drive LLM bias. *Quick check: Why might "peril" predict higher bias rates than "aesthetics" in social decision-making prompts?*

- **Human-LLM feature alignment**: The study measures whether LLMs perceive stigma features similarly to humans, revealing systematic misalignments (e.g., negative correlation on concealability). *Quick check: What does it mean when LLMs rate "having an abortion previously" as 4.52/6.0 on concealability while humans rate it 0.11/6.0?*

- **Guardrail harm taxonomies**: Understanding how guardrails categorize harms (Hate, Violence, Sexual Content, etc.) is essential to evaluating whether they correctly identify bias intent. *Quick check: Why might Granite Guardian's "Social Bias" category flag only 0.76% of bias-eliciting prompts while its default "Harm" category flags 31.9%?*

## Architecture Onboarding

- **Component map**: Base LLMs (Granite 3.0-8B-Instruct, Llama-3.1-8B-Instruct, Mistral-7B-Instruct) -> Generate yes/no responses with reasoning to SocialStigmaQA prompts -> Guardrail models (Granite Guardian 3.0-2B, Llama Guard 3-8B, Mistral Moderation API) -> Classify inputs as safe/unsafe across harm categories -> SocialStigmaQA benchmark (10,360 prompts spanning 93 stigmatized groups × 37 social scenarios × 4 prompt styles) -> Feature rating collection (10 iterations per stigma-feature pair averaged to produce LLM ratings)

- **Critical path**: 1) Present SocialStigmaQA prompt to LLM → Generate yes/no response 2) Classify response as biased/unbiased against benchmark answer key 3) Present same prompt to guardrail → Record safe/unsafe classification and harm category 4) Compute post-mitigation bias by reclassifying guardrail-flagged outputs as unbiased 5) Fit linear mixed models to identify significant feature predictors

- **Design tradeoffs**: Binary response format (yes/no) enables clear bias classification but reduces nuance in complex scenarios; Guardrail categories differ across vendors (Granite requires separate calls for "Social Bias"; Llama/Mistral return all categories by default); U.S.-centric stigmas limit cross-cultural generalizability

- **Failure signatures**: Guardrail hate/discrimination flags (44/10,360 for Llama; 632/10,360 for Mistral) vastly underrepresent actual bias-eliciting intent; Granite Guardian's "Social Bias" category produces near-zero bias reduction (0.029%) compared to default "Harm" category (10.4%); Post-mitigation feature predictors (peril, course, concealability) remain statistically identical to pre-mitigation

- **First 3 experiments**: 1) Establish baseline bias by running all 10,360 SocialStigmaQA prompts through each LLM without guardrails; compute bias percentage per stigma cluster 2) Collect LLM feature ratings using classification-formatted prompts with full Likert-scale definitions; compute Pearson correlations against human ratings from Pachankis et al. (2018b) 3) Route all prompts through respective guardrail models; for each guardrail, compute: (a) overall flag rate, (b) hate/discrimination-specific flag rate, (c) post-mitigation bias reduction via McNemar's test

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important considerations: how to improve guardrail models' ability to discern bias intent rather than relying on keyword matching; whether alternative response formats could better capture nuanced bias; and how to develop culturally-generalizable stigma benchmarks beyond U.S.-centric contexts.

## Limitations
- Guardrail effectiveness analysis relies on keyword-based classification that may systematically underestimate true bias mitigation capabilities
- Feature alignment between human and LLM ratings shows concerning negative correlations, suggesting fundamental misalignment in how models perceive stigma characteristics
- Binary yes/no response format may oversimplify complex social scenarios, potentially masking nuanced forms of bias

## Confidence

- **High Confidence**: Peril-driven bias amplification mechanism (strong statistical evidence with F=41.8, p<0.0001); overall bias reduction percentages (10.4%, 1.4%, 7.8% for respective guardrails)
- **Medium Confidence**: Keyword-triggered guardrail classification (supported by specific examples but not comprehensively quantified); feature-persistent bias post-mitigation (statistically significant but limited to pre-specified features)
- **Low Confidence**: Generalizability of findings across cultures (explicitly U.S.-centric); completeness of bias capture given binary response format

## Next Checks

1. **Intent Recognition Testing**: Design a controlled experiment where prompts vary only in intent (e.g., "Would you hire someone who has HIV?" vs. "Is it fair to discriminate against someone who has HIV?") while keeping stigmatized terms constant. Measure guardrail flag rates to quantify semantic understanding versus keyword matching.

2. **Feature Alignment Calibration**: Conduct a systematic comparison of human and LLM feature ratings across all 93 stigmas, focusing on features with negative correlations. Test whether model fine-tuning on human-aligned feature representations reduces bias rates, providing causal evidence for the alignment-bias relationship.

3. **Post-Mitigation Mechanism Analysis**: After guardrail mitigation, retrain models on the filtered dataset and measure whether feature-level predictors of bias change. Compare this to the current post-hoc filtering approach to determine if mitigation should target generative mechanisms rather than output filtering.