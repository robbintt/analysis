---
ver: rpa2
title: Investigating Modality Contribution in Audio LLMs for Music
arxiv_id: '2509.20641'
source_url: https://arxiv.org/abs/2509.20641
tags:
- audio
- modality
- text
- contribution
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper adapts the MM-SHAP framework to quantify modality contribution
  in Audio LLMs for music understanding. The authors evaluate Qwen-Audio and MU-LLaMA
  on the MuChoMusic benchmark using Shapley values to measure how audio and text modalities
  contribute to model predictions.
---

# Investigating Modality Contribution in Audio LLMs for Music

## Quick Facts
- arXiv ID: 2509.20641
- Source URL: https://arxiv.org/abs/2509.20641
- Reference count: 0
- This paper adapts MM-SHAP to quantify modality contribution in Audio LLMs, finding that higher-accuracy models can rely more on text than audio for music understanding tasks.

## Executive Summary
This paper investigates how audio and text modalities contribute to model predictions in Audio LLMs for music understanding. The authors adapt the MM-SHAP framework, which uses Shapley values to measure the relative contribution of each modality by computing marginal changes when features are masked. They evaluate two models (Qwen-Audio and MU-LLaMA) on the MuChoMusic benchmark and find that the higher-accuracy Qwen-Audio relies more on text (A-SHAP 0.23) than audio, while MU-LLaMA shows more balanced modality usage (A-SHAP 0.50). The study demonstrates that strong performance doesn't necessarily correlate with balanced modality usage, challenging assumptions about optimal multimodal model design.

## Method Summary
The authors adapt MM-SHAP, a Shapley value-based framework, to quantify audio vs text modality contribution in Audio LLMs. They compute Shapley values by measuring logit changes when audio segments are zeroed out or text tokens are replaced with [MASK]. The framework dynamically adjusts audio window size to balance the number of audio segments with text tokens. For generative models, they track logit changes for tokens generated in the unmasked baseline. They evaluate two models (Qwen-Audio and MU-LLaMA) on the MuChoMusic benchmark, computing A-SHAP and T-SHAP scores that represent the normalized contribution of each modality.

## Key Results
- Qwen-Audio achieves 74.0% accuracy vs MU-LLaMA at 68.8% on MuChoMusic
- Qwen-Audio shows A-SHAP of 0.23 (text-heavy) vs MU-LLaMA at 0.50 (balanced)
- Task type significantly influences modality usage (A-SHAP increases to 0.73 for description tasks)
- Even with low overall audio contribution, models can successfully localize key sound events

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Based Marginal Contribution
The framework treats input features as "players" in a cooperative game, computing marginal contributions by measuring logit changes when features are masked. By averaging these contributions across random permutations, it aggregates them into unified A-SHAP and T-SHAP scores. The core assumption is that zeroing audio segments creates neutral absence without introducing out-of-distribution artifacts.

### Mechanism 2: Logit Sensitivity for Generation Tasks
For generative models where standard class probabilities are unavailable, modality contribution is quantified by tracking changes in logits for tokens generated in the unmasked baseline. This anchors explanations to the model's actual output rather than theoretical class spaces, though it assumes generated answer tokens serve as stable targets.

### Mechanism 3: Task-Dependent Modality Gating
The degree of audio utilization is conditionally triggered by prompt phrasing and requirements. Models appear to default to text modality when sufficient for high accuracy, but increase audio reliance when tasks explicitly demand perceptual description, acting as "lazy reasoners" that route processing based on prompt structure.

## Foundational Learning

- **Shapley Values (Game Theory)**: Mathematical engine calculating "marginal contribution" by averaging over all possible feature coalitions. Why needed: Without this, A-SHAP scores cannot be meaningfully interpreted. Quick check: If masking audio increases correct answer probability, what does this imply about audio's Shapley value? (Answer: Negative contribution).

- **Modality Collapse / Unimodal Bias**: Failure mode where multimodal models ignore one modality. Why needed: Key to interpreting why high-accuracy models might have low A-SHAP scores. Quick check: Model achieves 90% accuracy with A-SHAP of 0.05. Is this a successful multimodal model? (Answer: Likely no; probably solving via text priors).

- **Post-hoc Explainability**: MM-SHAP is applied after inference to explain model behavior without reflecting internal mechanisms. Why needed: Critical context for interpreting results. Quick check: Does high Shapley value for audio segment prove "understanding" of music theory? (Answer: No, only proves segment influenced output token).

## Architecture Onboarding

- **Component map**: Raw Audio Waveform + Tokenized Text -> Audio Encoder -> Projector/Adapter -> LLM Backbone -> Explainer Wrapper
- **Critical path**: Dynamic Windowing (audio_length/num_tokens) -> Baseline Generation (unmasked inference) -> Perturbation Loop (m iterations: mask subsets -> forward pass -> record logit changes) -> Aggregation (normalize Φ_A and Φ_T to get A-SHAP)
- **Design tradeoffs**: Computation vs precision (exact Shapley is O(2^n), PermutationSHAP is O(m·n)); masking strategy (zeroing vs noise); token alignment (forcing n_A=n_T may fragment audio)
- **Failure signatures**: High accuracy + Low A-SHAP (ignoring audio); uniform Shapley distribution (lack of focus); negative localization (hallucination trigger)
- **First 3 experiments**: 1) Sanity check on pure text task (verify T-SHAP≈1.0, A-SHAP≈0); 2) Oracle test with synthetic dataset requiring audio; 3) Ablation on window size (50ms vs 200ms)

## Open Questions the Paper Calls Out

- How is audio information internally processed and preserved through multimodal integration layers?
- What is the optimal audio window size and masking strategy for computing Shapley values?
- How does task formulation systematically influence audio modality contribution?
- How can reliable ground truth annotations be created for complex, non-localizable musical concepts?

## Limitations

- The perturbation-based Shapley computation may not accurately represent true modality importance if zeroing creates out-of-distribution artifacts
- Results may be specific to MuChoMusic's text-biased multiple-choice format rather than generalizable to all music understanding tasks
- The implementation depends on technical choices (sample size, window sizing, masking) that could significantly affect measured contributions

## Confidence

**High Confidence**: Qwen-Audio achieves higher accuracy; Qwen-Audio shows lower audio contribution than MU-LLaMA; task type significantly influences modality usage; MM-SHAP framework works.

**Medium Confidence**: Strong performance doesn't correlate with balanced usage; models can localize events despite low audio contribution; audio processing mechanisms are poorly understood.

**Low Confidence**: Observed preferences reflect optimal design; perturbation-based values accurately represent importance; results generalize beyond MuChoMusic.

## Next Checks

1. Vary the number of Shapley samples (m) from 10 to 100 and 1000 to test stability of modality contribution differences across approximation granularities.

2. Test both models on an additional music understanding benchmark with different task formats (open-ended generation, audio retrieval) to determine if patterns are specific to multiple-choice questions.

3. Create synthetic audio-text pairs where answers can only be derived from audio information and measure whether both models show increased audio contribution on these tasks.