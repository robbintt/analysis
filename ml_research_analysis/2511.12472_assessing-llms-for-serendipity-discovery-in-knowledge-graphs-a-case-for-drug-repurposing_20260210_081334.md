---
ver: rpa2
title: 'Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug
  Repurposing'
arxiv_id: '2511.12472'
source_url: https://arxiv.org/abs/2511.12472
tags:
- serendipity
- knowledge
- graph
- type
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SerenQA, a framework to assess LLMs\u2019\
  \ ability to discover serendipitous insights in knowledge graph question answering.\
  \ It defines a novel graph-based RNS metric combining relevance, novelty, and surprise,\
  \ and constructs a drug repurposing benchmark with expert annotations."
---

# Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing

## Quick Facts
- arXiv ID: 2511.12472
- Source URL: https://arxiv.org/abs/2511.12472
- Reference count: 40
- Key outcome: LLMs excel at knowledge retrieval but struggle significantly with serendipity discovery in KGQA

## Executive Summary
This paper introduces SerenQA, a framework to assess LLMs' ability to discover serendipitous insights in knowledge graph question answering. It defines a novel graph-based RNS metric combining relevance, novelty, and surprise, and constructs a drug repurposing benchmark with expert annotations. The evaluation pipeline decomposes tasks into knowledge retrieval, subgraph reasoning, and serendipity exploration. Experiments show that while large LLMs perform well on retrieval, they struggle significantly with serendipity discovery—highlighting a key gap in current LLM capabilities for scientific innovation.

## Method Summary
The SerenQA framework evaluates LLMs on three sequential tasks: (1) Knowledge Retrieval—translating natural language to Cypher queries and retrieving existing answers; (2) Subgraph Reasoning—summarizing retrieved subgraphs; and (3) Serendipity Exploration—using LLM-guided beam search to discover novel connections beyond existing answers. The framework introduces a novel RNS metric that quantifies serendipity through relevance, novelty, and surprise dimensions computed from graph embeddings and probability distributions. A drug repurposing benchmark with 1,529 queries was constructed with expert-annotated partitions for existing and serendipity answers.

## Key Results
- LLMs achieve high retrieval performance (F1 ~78%) on one-hop queries but drop below 10% for multi-hop questions
- Removing intermediate summarization improves serendipity exploration performance by 10-30% due to reduced hallucination contamination
- No single LLM model excels across all tasks—some perform well on retrieval while others show better exploration capabilities
- The RNS metric shows 99% correlation with expert annotations for partition selection

## Why This Works (Mechanism)

### Mechanism 1
Serendipity is quantified through three dimensions: (1) Relative Relevance using negative average Euclidean distance between GCN embeddings of existing and serendipity answer sets; (2) Relative Novelty as 1 minus mutual information between sets via 3-hop conditional probabilities; (3) Relative Surprise as Jensen-Shannon divergence between entity distributions. The weighted combination RNS = αR + βN + γS produces a scalar serendipity score.

### Mechanism 2
Multi-hop conditional probability matrices up to 3 hops capture sufficient graph context for serendipity computation. The approach initializes transition matrix M from adjacency counts, normalizes to P₁, then computes Pₖ = Σ(h=1 to k) αₕP₁ʰ with weights prioritizing longer paths. Marginal probabilities use PageRank-style iteration for MI and JSD computation.

### Mechanism 3
LLM-guided beam search with structured prompts explores serendipitous paths beyond explicit knowledge. The three-phase pipeline: (1) LLM translates NL to Cypher query, retrieves existing answers; (2) LLM summarizes retrieved subgraph; (3) Beam search (width 30, depth 3) explores from existing answers with LLM scoring. However, summarization introduces hallucination risk that degrades exploration quality.

## Foundational Learning

- **Knowledge Graph Question Answering (KGQA)**: Translating natural language to structured queries over entity-relation graphs. Why needed: The entire framework evaluates LLMs on KGQA with serendipity extension. Quick check: Given "Which drugs target protein P53?", can you write a conceptual graph traversal pattern?

- **Information-theoretic divergence measures**: KL divergence, Jensen-Shannon divergence, and mutual information quantify distributional differences. Why needed: RNS metric relies on MI for novelty and JSD for surprise. Quick check: If two distributions P and Q are identical, what are D_KL(P||Q) and JSD(P,Q)?

- **Beam search with learned scoring**: Search algorithm using learned models to score and prioritize candidates. Why needed: Serendipity exploration uses beam search where LLMs score candidate expansions. Quick check: With beam width 30 and depth 3, what's the maximum number of nodes explored if each expansion produces 10 candidates?

## Architecture Onboarding

- **Component map**: Graph Preprocessing → Benchmark Dataset → Evaluation Pipeline (T1 Retrieval → T2 Reasoning → T3 Exploration) → Infrastructure

- **Critical path**: 1) Preprocess graph: compute embeddings, P₃ matrix, marginal probabilities; 2) Load benchmark query with ground-truth partitions; 3) T1: Prompt LLM to generate Cypher, execute, compute Hit/F1; 4) T2: Prompt LLM to summarize subgraph, evaluate Faithfulness/Comprehensiveness/SerenCov; 5) T3: Run LLM-guided beam search, collect predicted serendipity answers, evaluate metrics

- **Design tradeoffs**: Removing summarization improves exploration performance by 10-30% due to hallucination reduction; larger models excel at retrieval but show inconsistent gains on serendipity; RNS-guided partitions show 99% correlation with expert annotations

- **Failure signatures**: Retrieval F1 drops from ~78% (one-hop) to <10% (3+ hop); Faithfulness scores 2-3/5 indicate summary hallucinations; SerenHit consistently <0.15 even for best models; infrastructure bottlenecks at ~1000 QPS for Neo4j

- **First 3 experiments**: 1) Validate baseline retrieval with GPT-4o and Llama-3.3-70B on one-hop queries; 2) Compare T3 performance with/without T2 summaries for 100-query subset; 3) Test full pipeline against all three ground-truth partitions to verify >85% correlation

## Open Questions the Paper Calls Out

### Open Question 1
Can multi-agent systems or Mixture-of-Experts (MoE) strategies effectively combine LLM strengths to achieve balanced performance across all serendipity metrics? The experiments show no single model excels at all tasks, suggesting ensemble approaches may help.

### Open Question 2
How can the negative impact of intermediate summarization on serendipity exploration be mitigated? Current evidence shows summaries introduce hallucinations that degrade exploration quality, suggesting alternative approaches are needed.

### Open Question 3
How can RNS metric and framework incorporate real-world "drugability" constraints like physicochemical properties? Current metric optimizes for graph-based serendipity but ignores physical feasibility of drug candidates.

## Limitations
- Graph embedding reliability depends on GCNs capturing meaningful biomedical semantics
- Probability estimation may be unreliable in sparse biomedical graphs
- Hallucination compounding during multi-step exploration degrades quality
- Current metrics don't incorporate physical drugability constraints

## Confidence

- **High Confidence**: Decomposition of serendipity into three dimensions; retrieval performance gap between query types; negative impact of summarization on exploration
- **Medium Confidence**: RNS metric formulation (assuming valid embeddings); 3-hop reachability capturing 99% of serendipitous connections; transferability to non-biomedical domains
- **Low Confidence**: Absolute serendipity scores across models; claim about LLM fundamental struggles; generalizability of beam search parameters

## Next Checks

1. **Embedding Quality Validation**: Test GCN embeddings on held-out relationship prediction task to verify biomedical semantic capture before using in RNS computation

2. **Cross-Domain Transfer Test**: Apply complete SerenQA pipeline to non-biomedical knowledge graph (e.g., academic networks) to assess metric transferability and 3-hop assumption validity

3. **Alternative Exploration Methods**: Compare LLM-guided beam search against non-LLM approaches (random walk with restart, heuristic-based exploration) to isolate whether serendipity gap stems from LLM limitations or exploration methodology itself