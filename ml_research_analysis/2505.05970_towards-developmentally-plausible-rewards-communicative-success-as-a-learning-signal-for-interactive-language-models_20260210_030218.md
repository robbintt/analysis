---
ver: rpa2
title: 'Towards Developmentally Plausible Rewards: Communicative Success as a Learning
  Signal for Interactive Language Models'
arxiv_id: '2505.05970'
source_url: https://arxiv.org/abs/2505.05970
tags:
- language
- learning
- training
- listener
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores whether communicative success signals can be
  used to train language models in a way that mimics child language acquisition. The
  authors propose an abstract reference game where a speaker LM generates summaries
  of passages to help a listener LM answer questions, with rewards based on the listener's
  performance.
---

# Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models

## Quick Facts
- **arXiv ID:** 2505.05970
- **Source URL:** https://arxiv.org/abs/2505.05970
- **Reference count:** 40
- **Primary result:** Communicative feedback can influence model behavior but fails to improve grammatical ability under tested conditions.

## Executive Summary
This paper explores whether communicative success signals can train language models in ways that mimic child language acquisition. The authors propose an abstract reference game where a speaker LM generates summaries to help a listener LM answer questions, with rewards based on the listener's performance. Two bottleneck mechanisms (length-based and surprisal-based) constrain communication. While a feasibility study confirms that ungrammatical summaries reduce listener performance—indicating a potential learning signal—training models from scratch or fine-tuning pretrained models does not improve grammatical ability, despite producing interpretable changes in output length and content.

## Method Summary
The method involves a reference game where a speaker T5-small model generates summaries of passages to help a listener (frozen UnifiedQA T5) answer questions. The speaker receives a reward based on ROUGE-L between the listener's answer and ground truth, minus a penalty for summary length or surprisal relative to the original passage. Training uses PPO optimization with λ parameter controlling penalty strength. Experiments include from-scratch training on C4 + QA data (70M + 30M tokens) and fine-tuning on SQuAD 2.0 validation set (540k tokens).

## Key Results
- Ungrammatical summaries reliably reduce listener QA performance, confirming the feasibility of using communicative success as a grammaticality signal
- Length-based bottlenecks produce telegraphic speech with content-dense summaries and reduced function words
- Neither from-scratch training nor fine-tuning with communicative feedback improves grammatical ability on BLiMP or reduces grammatical errors on LanguageTool
- Speaker behavior changes predictably with bottleneck strength, but these changes don't translate to grammatical improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Communicative success provides an indirect signal about grammaticality.
- **Mechanism:** When a speaker produces ungrammatical summaries, the frozen pretrained listener model (UnifiedQA) processes them less effectively, reducing answer accuracy. The ROUGE-L score between the listener's answer and ground truth thus covaries with summary grammaticality.
- **Core assumption:** The listener model's performance degrades smoothly and predictably as input grammaticality decreases.
- **Evidence anchors:**
  - [abstract] "First, we present a feasibility study demonstrating that our reward provides an indirect signal about grammaticality."
  - [Section 5] Feasibility study shows listener QA performance drops when stop words are removed (0.55 → ~0.45), when text is scrambled (worse at higher scrambling degrees), and when truncated.
  - [corpus] Weak direct evidence—related papers focus on data plausibility rather than communicative reward signals.
- **Break condition:** If the listener model generalizes to ungrammatical input (robust to noise), the signal gradient flattens and becomes unusable for learning.

### Mechanism 2
- **Claim:** Bottleneck constraints produce predictable qualitative changes in speaker output.
- **Mechanism:** A length-based penalty incentivizes the speaker to compress information, leading to omission of low-information tokens (function words), producing telegraphic speech. A surprisal-based penalty penalizes both length and unusual constructions, preserving grammaticality while still encouraging brevity.
- **Core assumption:** The speaker can trade off reward (communicative success) against penalty (length/surprisal) in a continuous, learnable fashion.
- **Evidence anchors:**
  - [abstract] "We observe that cognitively plausible constraints on the communication channel lead to interpretable changes in speaker behavior."
  - [Section 7] "For the length-based bottleneck, the fraction of function words, sentence length, listener surprisal, and word length all paint a very consistent picture: The more weight is placed on the penalty the more content-dense and telegraphic the summaries become."
  - [corpus] No direct validation in related literature; this is a novel finding.
- **Break condition:** If λ (penalty weight) is too high, the speaker prioritizes penalty minimization over communication, degrading below baseline.

### Mechanism 3
- **Claim:** A frozen mature listener anchors communication and mitigates semantic drift.
- **Mechanism:** By keeping the listener model fixed (UnifiedQA, a T5 variant fine-tuned on 17 QA datasets), the speaker cannot co-adapt with a partner developing a private protocol. The listener's semantics remain tied to natural language, constraining the speaker to produce human-interpretable output.
- **Core assumption:** A single frozen listener provides sufficient and stable feedback for acquisition without requiring co-evolution.
- **Evidence anchors:**
  - [Section 3.3] "This has the added benefit of mitigating semantic drift (Lazaridou et al., 2020b), as the listener agent cannot adapt to innovations in the protocol introduced by the speaker."
  - [Section 4] "For the listener, we employ UnifiedQA (Khashabi et al., 2020), a version of T5 fine-tuned on a combination of 17 QA datasets."
  - [corpus] Related work (Lazaridou et al., 2020b) cited in paper confirms semantic drift occurs with co-trained agents.
- **Break condition:** If the speaker learns to exploit statistical anomalies in the listener's behavior, semantic drift can still occur despite frozen weights.

## Foundational Learning

- **Reinforcement Learning with PPO (Proximal Policy Optimization)**
  - Why needed here: The speaker is trained via RL to maximize reward (communicative success minus penalty), not next-token prediction.
  - Quick check question: Can you explain why PPO uses a clipped objective and why this stabilizes policy updates compared to vanilla policy gradient?

- **Encoder–Decoder Language Models (T5 architecture)**
  - Why needed here: Both speaker and listener are T5-small models; the encoder processes input, the decoder generates output autoregressively.
  - Quick check question: What is the difference between T5's text-to-text framework and a decoder-only model like GPT for conditional generation?

- **Surprisal as a Proxy for Comprehension/Production Cost**
  - Why needed here: The surprisal bottleneck operationalizes cognitive effort; higher surprisal correlates with processing difficulty.
  - Quick check question: How is surprisal computed from a language model's probability distribution, and why would ungrammatical text have higher surprisal?

## Architecture Onboarding

- **Component map:** Passage -> Speaker (T5-small) -> Summary -> Listener (UnifiedQA, frozen) -> Answer -> ROUGE-L calculation -> Reward -> Penalty calculation -> Score combiner -> PPO optimizer -> Speaker weights

- **Critical path:**
  1. Load pretrained T5-small for speaker, UnifiedQA for listener.
  2. Sample (passage, question, ground_truth_answer) from QA dataset (SQuAD 2.0 validation set in fine-tuning experiments).
  3. Speaker generates summary given passage.
  4. Listener generates answer given summary + question.
  5. Compute ROUGE-L(answer, ground_truth) as reward.
  6. Compute penalty (length or surprisal ratio).
  7. Combine into score and perform PPO update.
  8. Repeat for 240 steps (as in paper's fine-tuning experiments).

- **Design tradeoffs:**
  - Length vs. surprisal bottleneck: Length is simpler but encourages ungrammatical telegraphic output; surprisal preserves grammar but is more computationally expensive (requires forward pass through listener for each summary).
  - Frozen vs. co-trained listener: Frozen prevents drift but may limit adaptability; co-trained risks semantic drift.
  - From-scratch vs. fine-tuning: From-scratch is more cognitively plausible but RL is unstable without pretraining; fine-tuning is more practical but less developmentally motivated.

- **Failure signatures:**
  - Reward does not increase over training (as seen in from-scratch experiments).
  - Model outputs degenerate into repetitive or nonsensical text.
  - Summaries become verbatim copies of passages (trivial strategy when λ=0).
  - Grammatical error count increases or BLiMP scores decrease (observed with strong length bottleneck).

- **First 3 experiments:**
  1. Reproduce feasibility study: Corrupt passages (remove stop words, scramble, truncate) and measure listener QA performance degradation to confirm reward–grammaticality correlation.
  2. Run fine-tuning with λ=0 (no bottleneck) and verify speaker learns to increase reward; check if summaries become more verbose or approach verbatim copying.
  3. Compare length vs. surprisal bottlenecks at λ=0.5: Measure effect on function word retention, grammaticality (LanguageTool), and linguistic knowledge (BLiMP).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can modifications to training configuration (longer training, larger QA datasets, broader hyperparameter search, or alternative architectures) enable communicative feedback to improve grammatical ability in language models?
- **Basis in paper:** [explicit] The authors state "Future work should explore broader hyperparameter searches, longer training times (we train for 240 steps), larger QA training sets (ours is ∼500k tokens), and model architectures besides T5."
- **Why unresolved:** Current experiments failed to show grammatical improvements despite the feasibility study confirming that the reward signal correlates with grammaticality; the cause of this failure is unclear.
- **What evidence would resolve it:** Experiments varying each factor independently to identify which, if any, enable improvements on BLiMP or grammatical error metrics.

### Open Question 2
- **Question:** Would a curriculum over length bottlenecks (progressing from highly constrained to less constrained) produce more human-like word-learning trajectories where content words are acquired before function words?
- **Basis in paper:** [explicit] The authors suggest implementing "a curriculum over length bottlenecks" to model children's 1- and 2-word utterance phases, predicting "more human-like word-learning trajectories" where content words are prioritized over function words.
- **Why unresolved:** Current experiments used fixed bottleneck strengths; the developmental trajectory under dynamic constraints remains untested.
- **What evidence would resolve it:** Training with a curriculum that gradually relaxes length constraints, then evaluating word acquisition order against child language acquisition data.

### Open Question 3
- **Question:** Would incorporating multi-turn dialogue in the summarization game provide a richer learning signal for language acquisition than single-turn exchanges?
- **Basis in paper:** [explicit] The authors state "Allowing for multiple turns in a single exchange would increase the realism and provide another way to operationalize communicative feedback."
- **Why unresolved:** Current setup uses single-turn exchanges, limiting the types of feedback (e.g., clarification requests, reformulations) that can emerge naturally.
- **What evidence would resolve it:** Comparing single-turn vs. multi-turn variants on measures of grammatical development and communicative efficiency.

### Open Question 4
- **Question:** Can a speaker-surprisal bottleneck (modeling production difficulty) complement the listener-surprisal bottleneck to create more developmentally plausible learning dynamics?
- **Basis in paper:** [explicit] The authors mention "a speaker-surprisal bottleneck can model production difficulty" but deferred this for future work, using only listener surprisal.
- **Why unresolved:** Speaker surprisal may introduce confounding optimization dynamics ("moving the goal post"), but its potential benefits remain unexplored.
- **What evidence would resolve it:** Training with both speaker and listener surprisal penalties, evaluating whether combined bottlenecks better constrain telegraphic speech while preserving communicative success.

## Limitations
- From-scratch training fails to improve reward or grammaticality despite 70M tokens of pretraining
- Strong length bottlenecks (λ≥0.5) reduce grammaticality and BLiMP scores
- Surprisal bottleneck shows less impact but requires computationally expensive listener forward passes
- Small model size (T5-small) may limit grammatical improvement potential

## Confidence
- **Feasibility study methodology:** High - clear experimental design with controlled passage corruption
- **Implementation of bottleneck mechanisms:** High - detailed specifications and consistent results across λ values
- **Interpretation of grammaticality metrics:** Medium - BLiMP scores near chance for T5-small baseline as expected
- **Generalizability to child language acquisition:** Low - abstract reference game differs significantly from natural communication

## Next Checks
1. Verify that corrupted passages in the feasibility study produce the expected listener performance degradation (stop words removal → ~0.45 QA score)
2. Confirm that λ=0 fine-tuning leads to reward increase without grammatical improvement by running BLiMP and LanguageTool evaluations
3. Test whether λ=0.1 length bottleneck produces telegraphic speech while maintaining baseline grammaticality to isolate the effect of bottleneck strength