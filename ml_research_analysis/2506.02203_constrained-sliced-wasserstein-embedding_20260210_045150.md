---
ver: rpa2
title: Constrained Sliced Wasserstein Embedding
arxiv_id: '2506.02203'
source_url: https://arxiv.org/abs/2506.02203
tags:
- learning
- constrained
- sliced
- wasserstein
- slices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a constrained learning framework to optimize
  slicing directions in Sliced Wasserstein Embeddings (SWE) by enforcing that one-dimensional
  transport plans approximate optimal plans in the original high-dimensional space.
  The authors use a primal-dual optimization approach with relaxed constraints to
  achieve this, making the method applicable to various pooling tasks.
---

# Constrained Sliced Wasserstein Embedding

## Quick Facts
- arXiv ID: 2506.02203
- Source URL: https://arxiv.org/abs/2506.02203
- Authors: Navid NaderiAlizadeh; Darian Salehi; Xinran Liu; Soheil Kolouri
- Reference count: 40
- Primary result: Constrained SWE achieves better performance with fewer slices compared to unconstrained SWE

## Executive Summary
This paper introduces a constrained learning framework for optimizing slicing directions in Sliced Wasserstein Embeddings (SWE). The key innovation is enforcing that one-dimensional transport plans approximate optimal plans in the original high-dimensional space through SWGG dissimilarity constraints. The method uses primal-dual optimization with relaxed constraints to achieve this, making it applicable to various pooling tasks including image classification, point cloud classification, and protein sequence analysis.

## Method Summary
The method learns slicing directions by constraining the Sliced Wasserstein Gromov-Gromov (SWGG) dissimilarity between projected and original transport plans. It formulates this as a constrained optimization problem with slack variables and dual variables, solved via alternating primal descent and dual ascent. The framework includes a continuous relaxation of permutation matrices using softsort to enable end-to-end gradient-based training. During inference, hard sorting is restored for efficiency. The approach is integrated into a pooling layer that can replace global average pooling in various architectures.

## Key Results
- Constrained SWE achieves better performance with fewer slices compared to unconstrained SWE
- Improves both efficiency and downstream accuracy on images, point clouds, and protein sequences
- Outperforms global average pooling in most tasks while using fewer projection directions
- Demonstrates the utility of constrained learning for selecting informative slicing directions

## Why This Works (Mechanism)

### Mechanism 1
Constraining slicing directions via SWGG dissimilarity bounds produces more informative slices with fewer projection directions. The SWGG metric measures how well a 1D sliced transport plan lifts to the original high-dimensional space. By enforcing D²(μ, ν; θ) ≤ ε, the optimizer selects slices whose transport behavior in 1D remains geometrically meaningful when projected back, rather than arbitrary directions that require many slices to capture distributional differences.

### Mechanism 2
Primal-dual optimization with slack variables and automatic constraint relaxation balances objective minimization against constraint satisfaction adaptively. The formulation introduces slack variables s ≥ 0 and dual variables λ ≥ 0. During training, gradient ascent on λ increases regularization pressure on slices violating SWGG constraints, while gradient descent on θ and s minimizes the combined objective. This creates adaptive, per-slice regularization coefficients rather than fixed penalty weights.

### Mechanism 3
Continuous relaxation of permutation matrices via softsort enables end-to-end gradient-based training of slicer parameters. The effective permutation matrix R^l_i depends on argsort operations, which are non-differentiable. The softsort operator replaces hard permutations with softmax-based differentiable approximations controlled by temperature τ. During inference, hard sorting is restored for O(M log M) complexity.

## Foundational Learning

- **Concept: Wasserstein Distance and Optimal Transport Plans**
  - Why needed here: The entire method builds on OT as a framework for comparing distributions. Understanding that W₂ involves finding the transport plan minimizing expected squared displacement is prerequisite to grasping why sliced approximations need constraints.
  - Quick check question: Can you explain why 1D Wasserstein distance has closed-form solution but high-dimensional requires O(M³ log M) complexity?

- **Concept: Sliced Wasserstein Distance**
  - Why needed here: SWE projects high-dimensional distributions onto L 1D directions, computing Wasserstein distances per slice. The trade-off between number of slices L and approximation quality motivates this entire work.
  - Quick check question: Why does SW distance scale as O(LM log M) rather than O(M³ log M)?

- **Concept: Lagrangian Duality for Constrained Optimization**
  - Why needed here: The method reformulates constrained learning via Lagrangian L(Θ, s, λ), alternating between primal descent and dual ascent. Understanding why λ acts as an adaptive regularization coefficient is essential.
  - Quick check question: What happens to the dual variables when a constraint is satisfied vs. violated?

## Architecture Onboarding

- Component map: Input (X_i) → Backbone g(·; ϕ) [frozen] → Token embeddings V_i ∈ R^{d×M_i} → Reference U ∈ R^{d×M} + Slices Θ ∈ (S^{d-1})^L → [Per-slice projection & softsort] → Monge displacements z^l_i ∈ R^M (concatenated to R^{LM}) → Prediction head h(·; ψ) → Output ŷ_i

- Critical path:
  1. Forward pass computes embeddings via L sliced Monge couplings
  2. SWGG dissimilarity D²(μ^l_i, ν^l_i; θ_l) computed per-sample, averaged over batch
  3. Loss = task_loss + (α/2)||s||² + λ^T(D(Θ) - ε - s)
  4. Backprop through softsort to update Θ, U, ψ
  5. Dual update: λ ← [λ + η_λ(D(Θ) - ε - s)]₊

- Design tradeoffs:
  - Fewer slices (small L): Faster, lower embedding dimension, but requires more informative slices—constrained method shines here
  - Smaller ε (tighter constraints): Better slice quality but risk of infeasibility requiring larger slack
  - Lower softsort temperature τ: More accurate gradients but numerical issues; default τ ∈ {0.001, 0.01}

- Failure signatures:
  - Slacks s grow unbounded: ε too tight; increase ε or α
  - Dual variables λ → 0 early: Constraints too loose; decrease ε
  - Training loss oscillates: η_λ too high relative to η_Θ
  - SWGG values don't decrease from unconstrained baseline: Constraint gradients not flowing; check softsort τ

- First 3 experiments:
  1. **Ablation on constraint strength**: Run with ε ∈ {5, 10, 15, 20} on a validation split of your target dataset. Plot SWGG values and validation accuracy vs. ε to find the feasibility-quality frontier.
  2. **Slice count sweep**: Compare constrained vs. unconstrained SWE with L ∈ {4, 8, 16, 32}. Hypothesis: constrained method shows larger gains at small L. Verify this matches Figure 2/3 patterns.
  3. **Backbone layer probing**: Extract embeddings from different backbone depths (e.g., layers 6, 9, 12). Compare constrained SWE vs. GAP to identify where OT-based pooling provides most benefit over simple averaging.

## Open Questions the Paper Calls Out

### Open Question 1
Can utilizing the dual or slack variables as slice-wise importance weights during aggregation improve performance compared to the current flattening approach? The current architecture flattens embeddings across slices uniformly; the utility of these learned primal-dual variables for weighting the contribution of each slice remains untested.

### Open Question 2
Can hybrid approaches that balance dissimilarity maximization (e.g., Max-SW) with SWGG alignment constraints lead to stronger generalization capabilities? The paper focuses exclusively on SWGG alignment constraints; it does not explore combining this with traditional objectives that seek to maximize the sliced Wasserstein distance.

### Open Question 3
Does the inclusion of additional constraint types, such as orthogonality or Max-SW-style constraints, significantly enhance slice heterogeneity or informativeness in domains beyond those tested? While Remark 2 notes orthogonality constraints showed minimal difference in current experiments, the authors suggest their utility may depend on the specific task.

## Limitations
- SWGG dissimilarity computation requires additional optimal transport plans, increasing training complexity
- Softsort approximation introduces hyperparameter tuning (τ) that may affect convergence
- Constraint formulation assumes access to a reference distribution U, which may not always be natural in some applications

## Confidence
- Core claims about constrained SWE improving efficiency and accuracy: High
- Mechanism connecting SWGG constraint satisfaction to slice informativeness: Medium
- Primal-dual optimization approach for slice optimization: Medium
- Long-term stability of constrained learning in SWE context: Medium

## Next Checks
1. **Constraint sensitivity analysis**: Systematically vary ε across multiple orders of magnitude on a held-out validation set to map the feasibility frontier and identify optimal constraint strength for different task types.

2. **Scaling study**: Evaluate constrained SWE on larger-scale datasets (e.g., ImageNet) to verify whether slice efficiency gains persist when embedding dimensionality and batch sizes increase substantially.

3. **Robustness to initialization**: Test whether different random initializations of slicing directions lead to different local optima in the constrained space, and whether multiple restarts consistently improve over unconstrained SWE.