---
ver: rpa2
title: Impact of Fine-Tuning Methods on Memorization in Large Language Models
arxiv_id: '2507.00258'
source_url: https://arxiv.org/abs/2507.00258
tags:
- fine-tuning
- memorization
- methods
- tuning
- prompt-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how different fine-tuning methods affect
  memorization in large language models. It compares parameter-based fine-tuning (e.g.,
  FT head, LoRA) with prompt-based approaches (e.g., Prefix Tuning, Prompt Tuning,
  P-tuning) using membership inference attacks across multiple datasets and model
  scales.
---

# Impact of Fine-Tuning Methods on Memorization in Large Language Models

## Quick Facts
- **arXiv ID**: 2507.00258
- **Source URL**: https://arxiv.org/abs/2507.00258
- **Reference count**: 11
- **Key outcome**: Prompt-based fine-tuning achieves comparable task performance while exhibiting significantly lower memorization than parameter-based methods, with this gap widening at larger model sizes.

## Executive Summary
This paper investigates how different fine-tuning methods affect memorization in large language models. It compares parameter-based fine-tuning (e.g., FT head, LoRA) with prompt-based approaches (e.g., Prefix Tuning, Prompt Tuning, P-tuning) using membership inference attacks across multiple datasets and model scales. Results show that prompt-based fine-tuning achieves comparable task performance while exhibiting significantly lower memorization than parameter-based methods, with this gap widening at larger model sizes. Parameter-based fine-tuning shows increasing memorization over training epochs, whereas prompt-based methods maintain consistently low memorization regardless of scale. The findings indicate that parameter-based fine-tuning poses higher privacy risks, making prompt-based fine-tuning a more privacy-preserving option for sensitive applications.

## Method Summary
The study fine-tunes pre-trained LLMs (GPT-2, LLaMA2, LLaMA3) using both parameter-based methods (FT head, LoRA with rank 16) and prompt-based methods (Prefix Tuning with 5 tokens, Prompt Tuning with 8 tokens, P-tuning with 20 tokens). Models are trained for 15 epochs using AdamW (lr=5e-5) on three datasets (Wikitext-2, WebNLG, XSum). Memorization is quantified using four membership inference attack methods (LOSS, Ref, Zlib, Min-K%) that measure AUC-ROC scores. Task performance is measured via validation perplexity. The analysis compares memorization levels across fine-tuning methods, training epochs, and model scales.

## Key Results
- Prompt-based fine-tuning achieves comparable task performance to parameter-based methods while exhibiting significantly lower memorization.
- Parameter-based fine-tuning shows increasing memorization over training epochs, while prompt-based methods maintain consistently low memorization.
- The memorization gap between parameter and prompt-based methods widens at larger model sizes.

## Why This Works (Mechanism)

### Mechanism 1: Parameter Updates Enable Fitting Training-Specific Artifacts
Parameter-based fine-tuning leads to higher memorization because updating model weights allows internal representations to overfit to specific training examples, creating distributional shifts between member and non-member samples. This manifests as measurable differences in loss distributions that MIAs exploit. Evidence shows LoRA and FT head achieve AUC scores exceeding 0.8, while prompt-based methods remain near 0.5.

### Mechanism 2: Prompt-Based Fine-Tuning Introduces Indirect Attention Bias
Prompt-based fine-tuning maintains low memorization by shifting attention without fundamentally altering internal sample distributions. Trainable soft tokens act as attention "bias," shifting focus to different content rather than learning new attention patterns for complex tasks. This relies on pre-trained capabilities instead of overfitting to training data.

### Mechanism 3: Model Scale Amplifies Parameter-Based Memorization
Larger models possess greater parameter capacity, allowing more extensive adaptation to training data when updated. When LoRA or FT head are applied to larger models, this expanded space facilitates memorization. Prompt-based methods cannot leverage increased capacity since core weights remain frozen, maintaining constant low memorization regardless of scale.

## Foundational Learning

- **Concept: Membership Inference Attacks (MIAs)**
  - Why needed here: MIAs are the primary evaluation tool used throughout the paper to quantify memorization; understanding their mechanics is essential to interpret the reported AUC scores as evidence of privacy risk.
  - Quick check question: If a model has an MIA AUC of 0.50 against it, what does that imply about its memorization of the queried dataset?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) Methods**
  - Why needed here: The study compares two broad categories of fine-tuning; understanding the distinction between updating weights and learning soft prompts is fundamental to grasping why their memorization behaviors diverge.
  - Quick check question: Does LoRA update the original pre-trained model weights directly? Explain its mechanism briefly.

- **Concept: Overfitting vs. Memorization**
  - Why needed here: The paper links increased memorization over epochs to overfitting, and uses validation perplexity to track task performance alongside memorization.
  - Quick check question: How does the paper's observation about validation PPL over epochs support the claim that parameter-based fine-tuning is more prone to overfitting?

## Architecture Onboarding

- **Component map**:
  Pre-trained LLM (GPT-2/LLaMA) -> Fine-tuning Adapter (LoRA/FT head for parameter-based; Prefix/Prompt/P-tuning for prompt-based) -> Trained Model -> MIA Evaluator (LOSS, Ref, Zlib, Min-K%) -> AUC score

- **Critical path**:
  1. Select Pre-trained Model: Choose base architecture and scale (e.g., LLaMA2-7B)
  2. Select Fine-tuning Method & Dataset: Choose paradigm and downstream task data (e.g., WebNLG, Wikitext)
  3. Fine-tune Model: Train for fixed epochs (5-15), logging validation PPL
  4. Prepare MIA Dataset: Create balanced set of member/non-member samples
  5. Run MIAs: Apply attack methods on fine-tuned model using MIA dataset
  6. Analyze Results: Compare AUC scores and validation PPL across methods, epochs, and scales

- **Design tradeoffs**:
  - Task Performance vs. Privacy: Prompt-based methods achieve competitive PPL with far lower memorization but may struggle on novel complex tasks
  - Compute Efficiency vs. Privacy Risk: LoRA is efficient but privacy risk scales with model size; prompt-based methods maintain constant low privacy risk
  - Fine-tuning Method Selection: Depends on data sensitivity; prompt-based for private data, parameter-based for maximum performance on complex new tasks

- **Failure signatures**:
  - High MIA AUC (>0.75): Strong indicator of memorized training data
  - Validation PPL Divergence: U-shaped PPL curve in parameter-based methods indicates overfitting and increasing memorization
  - Prompt-Based Stagnation: Failure to reduce validation PPL suggests task is too novel for pre-trained model

- **First 3 experiments**:
  1. Establish Baseline: Fine-tune LLaMA2-7B on WebNLG using LoRA and Prefix Tuning for 5 epochs; compare validation PPL and run LOSS-based MIA
  2. Analyze Temporal Dynamics: Extend training to 15 epochs; plot validation PPL and LOSS attack AUC over epochs to visualize memorization growth
  3. Probe Scale Sensitivity: Fine-tune GPT-2 (124M) and GPT-2 XL (1.5B) on Wikitext using FT head and Prompt Tuning; run LOSS attack to confirm scale effects

## Open Questions the Paper Calls Out

- Do the memorization differences persist in mixture-of-experts architectures or closed-source models of significantly larger scale (70B+ parameters)?
- How does prompt-based fine-tuning compare to parameter-based methods when subjected to aggressive privacy attacks like data extraction or attribute inference?
- How does the interaction between LoRA rank size and model scale affect the memorization threshold?
- Is the lower memorization in prompt-based methods primarily caused by freezing projection layers, which are hotspots for memorization in parameter-based tuning?

## Limitations

- Findings may not generalize to larger-scale models (GPT-4, LLaMA2-70B) or mixture-of-experts architectures
- Analysis is limited to specific tasks (WebNLG, Wikitext, XSum) and models (GPT-2, LLaMA variants)
- Paper does not verify if low memorization scores translate to robustness against data extraction or attribute inference attacks
- Claim that prompt-based methods struggle on novel tasks is not empirically validated within the paper

## Confidence

- **High Confidence**: Parameter-based fine-tuning exhibits significantly higher MIA vulnerability than prompt-based methods across multiple datasets and scales
- **Medium Confidence**: Prompt-based methods' low memorization is primarily due to indirect attention bias rather than parameter count
- **Medium Confidence**: Model scale amplifies memorization in parameter-based fine-tuning
- **Low Confidence**: Prompt-based methods are universally suitable for all sensitive applications

## Next Checks

1. Verify the attention mechanism hypothesis by conducting an ablation study with very large numbers of prompt tokens to isolate whether the mechanism is indirect attention bias or simply parameter count
2. Test on novel task domains by fine-tuning on tasks demonstrably outside pre-training distribution to validate prompt-based methods' limitations on novel tasks
3. Audit for other privacy leaks by evaluating models for data reconstruction or differential privacy leakage beyond membership inference attacks