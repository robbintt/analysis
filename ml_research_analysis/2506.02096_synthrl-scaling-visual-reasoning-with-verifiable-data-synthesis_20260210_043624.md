---
ver: rpa2
title: 'SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis'
arxiv_id: '2506.02096'
source_url: https://arxiv.org/abs/2506.02096
tags:
- data
- arxiv
- reasoning
- difficulty
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SynthRL, a pipeline for automatically synthesizing
  more challenging training data for vision-language models trained with verifiable
  rewards. The core idea is to select seed questions that are too easy for the target
  model, use a strong VLM to generate harder variants while preserving answers, and
  verify both correctness and increased difficulty via Monte Carlo rollouts.
---

# SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis

## Quick Facts
- arXiv ID: 2506.02096
- Source URL: https://arxiv.org/abs/2506.02096
- Authors: Zijian Wu; Jinjie Ni; Xiangyan Liu; Zichen Liu; Hang Yan; Michael Qizhe Shieh
- Reference count: 40
- Synthesizes 3.3K harder visual math questions from 8K easy seeds, improving out-of-domain benchmark performance

## Executive Summary
This paper presents SynthRL, a pipeline for automatically synthesizing more challenging training data for vision-language models trained with verifiable rewards. The core idea is to select seed questions that are too easy for the target model, use a strong VLM to generate harder variants while preserving answers, and verify both correctness and increased difficulty via Monte Carlo rollouts. Applied to the MMK12 dataset, SynthRL synthesizes 3.3K harder questions from ~8K seeds. Models trained on the augmented dataset show consistent gains across five out-of-domain visual math reasoning benchmarks, with the largest improvements on the hardest evaluation samples. Ablation studies confirm that aligned verification and data augmentation are key to these gains.

## Method Summary
SynthRL is a data synthesis pipeline designed specifically for RLVR training of vision-language models on visual math reasoning. The approach identifies easy questions from a seed dataset using Monte Carlo rollouts with the target model, then uses a strong VLM synthesizer to generate harder variants while withholding ground-truth answers to force semantic understanding. Each candidate undergoes verification with the target model to ensure both correctness and difficulty increase. The resulting augmented dataset is used to train the target model with GRPO, yielding consistent improvements across multiple out-of-domain benchmarks.

## Key Results
- Synthesized 3.3K harder questions from ~8K seed questions in MMK12 dataset
- Models trained on augmented data show consistent gains across five out-of-domain visual math reasoning benchmarks
- Largest improvements observed on the hardest evaluation samples (e.g., +2.6% on Hard-5 subset)
- Ablation confirms aligned verification and data augmentation are key to performance gains

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Targeted Data Selection
Selecting seed questions that are too easy for the target model yields better training signal augmentation than random or hard-question selection. Monte Carlo rollouts compute pass counts, with questions having pass rates ≥12/16 selected because their reliability means minimal gradient signal during RLVR, making them ideal candidates for difficulty enhancement. Evidence shows selected seeds have mean pass rate 15.10 vs. 6.33 for synthesized variants.

### Mechanism 2: Answer-Preserving Synthesis with Semantic Grounding
Withholding ground-truth answers from the synthesizer forces semantic understanding rather than superficial paraphrasing. The synthesizer VLM receives only (Image I, Question Qori)—not answer A—compelling reasoning about the image-question relationship to generate harder variants that remain answerable with A. Synthesized questions require 33% more reasoning steps (34.90 vs. 26.16).

### Mechanism 3: Aligned Verification with Dual Criteria
Using the same model for verification as training ensures difficulty calibration is meaningful for the target model. Verification checks two conditions: correctness (ccand ≥ Tmin=4) and difficulty increase (ccand ≤ cori - ∆hard=2). Using πtarget ensures measured difficulty reflects actual training challenge. Non-target verifier drops accuracy from 57.2% to 55.7%, confirming alignment matters.

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR):** Why needed here? SynthRL is designed specifically for RLVR training where rewards come from answer verification, not human preference. Understanding this frames why answer preservation is critical. Quick check: Can you explain why RLVR requires verifiable answers rather than reward models?

- **Monte Carlo Rollouts for Uncertainty Estimation:** Why needed here? The entire selection and verification pipeline relies on pass counts from stochastic sampling. Understanding variance in model predictions is essential. Quick check: Why would a question with pass count 4/16 provide different training signal than one with 15/16?

- **GRPO (Group Relative Policy Optimization):** Why needed here? The paper uses GRPO with normalized advantages computed across n rollouts per input. Understanding this clarifies why diverse difficulty matters. Quick check: How does GRPO's group-based advantage normalization differ from PPO's single-sample advantage estimation?

## Architecture Onboarding

- **Component map:** Seed Selector -> Synthesizer -> Verifier -> Training Loop
- **Critical path:** 1) Pre-compute pass counts for entire seed dataset (expensive, one-time) 2) Filter → Synthesize → Verify (pipeline can run in parallel batches) 3) Merge synthesized data with original → RLVR training
- **Design tradeoffs:** Tmin=4 vs. higher (lower threshold accepts more data but risks invalid questions); ∆hard=2 vs. higher (larger values ensure clearer difficulty increase but reject more candidates); using πtarget as verifier vs. stronger model (aligned verification improves results but may reject questions a stronger model could verify)
- **Failure signatures:** Low synthesis yield (synthesizer generates mostly invalid variants → check prompt engineering, consider weaker Tmin); no difficulty increase in synthesized data (verifier threshold too low → increase ∆hard); training degradation (synthesized questions are wrong → verify Tmin threshold, check answer parsing logic)
- **First 3 experiments:** 1) Reproduce pass count distribution: Run Monte Carlo rollouts on MMK12 subset, verify bimodal distribution before implementing synthesis 2) Ablate verification alignment: Train with non-target verifier vs. πtarget to confirm ~1.5% gap 3) Scale test: Start with 2K seeds (cheaper), verify improvement pattern holds before committing to 8K synthesis runs

## Open Questions the Paper Calls Out
None

## Limitations

- **Scalability concerns:** The approach requires expensive Monte Carlo rollouts (N=16) for both seed selection and verification, making it computationally intensive for large datasets
- **Generalizability questions:** While effective on MMK12, the pipeline's performance on non-mathematical visual reasoning tasks remains untested
- **Distribution shift risks:** The paper doesn't thoroughly analyze whether synthesized questions maintain the original data distribution's diversity

## Confidence

- **High confidence (Experimental evidence robust):** The core synthesis pipeline works as described, with clear difficulty increases and verification success rates reported
- **Medium confidence (Results plausible but limited validation):** The claim that aligned verification is crucial is supported by ablation but could benefit from testing against multiple verifier models
- **Low confidence (Unverified assumptions):** The assumption that easy questions provide better augmentation candidates than randomly selected ones is logical but not rigorously tested against alternatives

## Next Checks

1. **Cross-domain generalization test:** Apply SynthRL to a non-mathematical visual reasoning dataset (e.g., VQA or GQA) and measure whether the difficulty-targeting and answer-withholding mechanisms transfer effectively

2. **Verifier alignment stress test:** Systematically vary the verifier model capability (weaker, equal, stronger than πtarget) across multiple training runs to map the full performance landscape

3. **Distribution analysis:** Conduct KL divergence analysis between original and synthesized question distributions, plus human evaluation of synthesized question naturalness and real-world applicability