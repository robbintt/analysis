---
ver: rpa2
title: Rethinking Explainability in the Era of Multimodal AI
arxiv_id: '2506.13060'
source_url: https://arxiv.org/abs/2506.13060
tags:
- multimodal
- explanation
- modality
- explanations
- unimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that unimodal explanation techniques fail for
  multimodal AI systems because they miss critical cross-modal interactions. The author
  identifies three key shortcomings: unimodal explanations fail to capture the multimodal
  attribution gap, attention weights misrepresent causal influence, and mechanistic
  interpretability tools don''t scale to multimodal architectures.'
---

# Rethinking Explainability in the Era of Multimodal AI

## Quick Facts
- arXiv ID: 2506.13060
- Source URL: https://arxiv.org/abs/2506.13060
- Authors: Chirag Agarwal
- Reference count: 12
- One-line primary result: Unimodal explanation techniques fail for multimodal AI systems due to missing cross-modal interactions and causal misattribution.

## Executive Summary
This paper argues that unimodal explanation techniques are fundamentally inadequate for multimodal AI systems because they cannot capture critical cross-modal interactions and dependencies. The author identifies three key shortcomings: unimodal explanations miss the modality attribution gap, attention weights misrepresent causal influence, and mechanistic interpretability tools don't scale to multimodal architectures. To address these issues, the paper proposes three formal desiderata for multimodal explanations: Granger-style modality influence, synergistic faithfulness, and unified stability. The work calls for a paradigm shift toward developing modality-aware explanation methods that can reveal how multiple modalities jointly influence predictions, with implications for safety and reliability in high-stakes applications.

## Method Summary
The paper proposes three formal desiderata for multimodal explanations through theoretical analysis and illustrative examples. The methods include: (1) Granger-style modality influence using controlled ablations to quantify cross-modal dependencies, (2) synergistic faithfulness requiring sufficiency and necessity tests per modality, and (3) unified stability under cross-modal perturbations. The paper demonstrates failures using logit lens analysis on graph-language models showing static predictions for graph tokens versus dynamic text tokens, and concept-based explanation instability under semantically-preserving image perturbations. The approach is primarily conceptual, proposing evaluation frameworks rather than specific algorithms.

## Key Results
- Unimodal explanations (heatmaps, token highlights) cannot reveal whether multimodal models used both modalities or relied on single-modality shortcuts
- Attention visualizations are "plausible without being correct" and particularly problematic in multimodal contexts
- Concept-based explanations show instability under semantically invariant perturbations, with different concept rankings despite unchanged outputs

## Why This Works (Mechanism)

### Mechanism 1: Granger-Style Modality Influence
- Claim: Ablating one modality and measuring explanation changes in other modalities can quantify true cross-modal dependence.
- Mechanism: For modality m, compute ∆fm = |f(x) − f(x−m)| where x−m replaces modality m with a neutral reference. Then measure ∆Em(x) = |Em(x) − Em(x−m)|. If ∆fm is large but ∆Em is near zero, the explanation fails to capture the dependency.
- Core assumption: Ablation effects reveal causal dependence; neutral references exist that don't introduce confounds.
- Evidence anchors: [abstract] "Granger-style modality influence (controlled ablations to quantify how removing one modality changes the explanation for another)"

### Mechanism 2: Synergistic Faithfulness via Sufficiency/Necessity Testing
- Claim: Faithful multimodal explanations must pass both sufficiency (keeping highlighted features preserves output) and necessity (removing them degrades output) tests per modality.
- Mechanism: For each modality m, construct xkeep_m (retains only Em-identified features) and xremove_m (removes Em-identified features). Require |f(x) − f(xkeep_m)| ≤ ε (sufficiency) and |f(x) − f(xremove_m)| ≥ ε (necessity).
- Core assumption: Perturbation constructors (Keepm, Removem) can be defined for heterogeneous explanation types (attributions, concepts, circuits).
- Evidence anchors: [abstract] "Synergistic faithfulness (explanations capture the model's predictive power when modalities are combined)"

### Mechanism 3: Unified Stability Under Cross-Modal Perturbations
- Claim: Explanations should remain bounded when semantically equivalent perturbations are applied to any modality, both intra-modally and cross-modally.
- Mechanism: Two constraints—(1) Modal-based Semantic Stability: ||Em(x) − Em(x')|| ≤ L'm||x − x'|| for semantically equivalent inputs; (2) Cross-Modal Stability: ||En(x + ηm) − En(x)|| ≤ Ln,m||ηm|| for perturbations ηm in modality m affecting explanation for modality n.
- Core assumption: Lipschitz-like continuity is appropriate for explanation stability; semantic equivalence can be operationalized.
- Evidence anchors: [abstract] "Unified stability (robustness under small cross-modal perturbations)"

## Foundational Learning

- **Concept: Modality Attribution Gap**
  - Why needed here: Core failure mode—unimodal explanations (heatmaps, token highlights) cannot reveal whether multimodal reasoning occurred or if the model exploited single-modality shortcuts.
  - Quick check question: Given separate image heatmap and text highlight explanations, can you determine if the model used both modalities or relied on only one?

- **Concept: Cross-Modal Fusion Architectures**
  - Why needed here: Heterogeneous encoder stacks (ResNet + GNN + Transformer) with fusion layers create entangled representations that defy clean causal intervention.
  - Quick check question: Where in your architecture do modality-specific representations become jointly encoded, and can you intervene at that point?

- **Concept: Faithfulness vs. Plausibility in Explanations**
  - Why needed here: Attention visualizations are "plausible without being correct"—they look explanatory but lack causal basis, especially problematic in multimodal contexts.
  - Quick check question: Does your explanation method change when you ablate the features it highlights? If not, it's plausible but not faithful.

## Architecture Onboarding

- **Component map:** Input: M modalities (X1, ..., XM) → heterogeneous encoders → fusion layer(s) → prediction head → Explanation: Per-modality explainers E1, ..., EM producing Zm (attributions, concepts, or circuits) → Evaluation: Granger ablation module, sufficiency/necessity testers, stability probes

- **Critical path:**
  1. Identify fusion architecture (early/late/hybrid) to understand where cross-modal influence is computed
  2. Define neutral references xref_m per modality (all-zero tensors, mean embeddings, or masked inputs)
  3. Implement modality-specific perturbation constructors (Keepm, Removem) compatible with explanation type

- **Design tradeoffs:**
  - Separate vs. joint autoencoders: Separate preserves modality structure but loses cross-modal interactions; joint becomes intractable
  - Intervention granularity: Feature-level ablations are tractable but may miss circuit-level interactions; circuit-level requires expensive mechanistic analysis
  - Stability tolerance: Tight bounds increase reliability but may be unachievable; loose bounds reduce explanatory power

- **Failure signatures:**
  - Explanation shows high attribution for modality m, but ∆fm ≈ 0 upon ablation → unfaithful explanation
  - Concept rankings change under semantically invariant perturbations → instability (Figure 4)
  - Logit lens produces static/abstract tokens for non-text modalities → unimodal tool misapplication (Figure 2)

- **First 3 experiments:**
  1. **Granger ablation test**: For your multimodal model, ablate each modality and measure both prediction change (∆fm) and explanation change (∆Em). Flag cases where ∆fm >> ∆Em.
  2. **Sufficiency/necessity probe**: For top-k attributed features per modality, run Keepm and Removem tests. Report pass rates per modality and identify which modalities have unreliable explanations.
  3. **Cross-modal stability stress test**: Apply semantically invariant perturbations (image augmentations, text paraphrases) to one modality and measure explanation drift in all modalities. Quantify violation of Equation 6 and 7 bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can researchers construct synthetic multimodal benchmarks where the ground-truth contribution of each modality is explicitly known to rigorously test explanation recovery?
- Basis in paper: [explicit] The Conclusion urges the creation of "synthetic multimodal tasks where the ground-truth contribution of each modality is known, to test whether an explanation method can recover it."
- Why unresolved: Current evaluation relies on unimodal metrics, and existing multimodal datasets lack labels for cross-modal interactions or ground-truth reasoning paths.
- What evidence would resolve it: A publicly available dataset with controlled shortcuts and known modality dependencies where explanation methods can be quantitatively scored on accuracy.

### Open Question 2
- Question: How can mechanistic interpretability tools (e.g., logit lens, sparse autoencoders) be adapted to handle heterogeneous encoder stacks that lack a shared vocabulary or unified embedding space?
- Basis in paper: [explicit] Section 2.3 states, "We lack analogous tools for multimodal systems," noting that techniques like logit lens fail on graph tokens because they rely on text-centric assumptions.
- Why unresolved: Multimodal models often use distinct encoders with different inductive biases, making it difficult to define shared "units" (neurons/heads) or apply text-based interpretability lenses.
- What evidence would resolve it: A modified logit lens or SAE architecture that successfully traces information flow from non-text modalities (e.g., graphs) to output tokens without generating abstract or meaningless predictions.

### Open Question 3
- Question: Which specific explanation formats (e.g., textual justifications vs. joint heatmaps) most effectively improve user diagnostic accuracy and trust in high-stakes settings?
- Basis in paper: [explicit] The Conclusion advises the community to "gather feedback from users... on what forms of multimodal explanations are most understandable and useful."
- Why unresolved: While technical desiderata like faithfulness are defined, the human-computer interaction aspect—specifically which modality-aware format best serves human decision-making—remains untested.
- What evidence would resolve it: User studies demonstrating that a specific multimodal explanation format significantly reduces diagnostic errors compared to side-by-side unimodal explanations.

## Limitations

- The proposed desiderata remain largely theoretical without empirical validation on real-world multimodal systems
- The ablation and sufficiency/necessity tests assume clean causal intervention points that may not exist in entangled multimodal representations
- Cross-modal stability definitions rely on operationalizing "semantic equivalence" across heterogeneous modalities—a nontrivial challenge

## Confidence

- **High confidence**: Core observation that unimodal explanations fail to capture cross-modal interactions is well-supported by existing literature and the logit lens analysis
- **Medium confidence**: The three formal desiderata are logically coherent and address identified gaps, but lack empirical validation beyond the stability experiments
- **Low confidence**: Claims about the necessity of unified stability constraints and specific formulation of synergistic faithfulness bounds are speculative without broader validation

## Next Checks

1. **Benchmark construction**: Create a multimodal dataset with ground-truth cross-modal attribution labels (e.g., synthetic tasks where modality contributions are controlled) to empirically test whether proposed metrics correlate with actual multimodal reasoning.

2. **Architecture-specific evaluation**: Apply Granger ablation and sufficiency/necessity tests to diverse multimodal architectures (early, late, and hybrid fusion) to identify which architectures are most vulnerable to explanation failures.

3. **Computational feasibility study**: Benchmark the runtime and memory requirements of the proposed evaluation framework on multimodal models of varying sizes to assess practical applicability.