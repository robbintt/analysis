---
ver: rpa2
title: 'AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language
  Model Support and Retrieval-Aligned Scaffolding'
arxiv_id: '2511.00265'
source_url: https://arxiv.org/abs/2511.00265
tags:
- agentbnb
- arxiv
- c2d2
- game
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentBnB is a browser-based cybersecurity training platform that
  reimagines the tabletop exercise game Backdoors & Breaches with large language model
  teammates and an instructional copilot. The system integrates a retrieval-augmented
  generation pipeline to deliver Bloom-aligned hints, enabling adaptive scaffolding
  during solo gameplay.
---

# AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding

## Quick Facts
- arXiv ID: 2511.00265
- Source URL: https://arxiv.org/abs/2511.00265
- Reference count: 40
- Participants showed higher intention to use the agent-based version over the physical card deck and viewed it as more scalable

## Executive Summary
AgentBnB reimagines the tabletop exercise Backdoors & Breaches as a browser-based platform with LLM teammates and a retrieval-augmented copilot (C2D2) providing Bloom-aligned scaffolding. The system integrates large language model agents with a Bloom-aware knowledge base to deliver on-demand hints during solo gameplay. In a pilot study with four graduate students, participants rated the agent-based version more scalable than the physical deck and all scored perfectly on a knowledge quiz, though a ceiling effect limited learning assessment. The findings suggest LLM-augmented TTXs can provide lightweight, repeatable incident-response practice without traditional logistical burdens.

## Method Summary
AgentBnB is a browser-based cybersecurity tabletop exercise that integrates LLM teammates with a retrieval-augmented instructional copilot (C2D2). The C2D2 pipeline pre-processes 77 curated documents into four Bloom categories (factual, conceptual, procedural, metacognitive) via GPT-4 prompts, embeds each ~300-token chunk with text-embedding-ada-002, and stores vectors with bloom_tag metadata in ChromaDB. At query time, cosine similarity retrieves top-10 passages, which are injected into a structured prompt alongside recent group-chat context and an instructional directive before GPT-4o generates a cited response. Game state (attack cards, procedure cards, dice rolls, cooldowns) is held in-memory client-side with telemetry logging for research observation.

## Key Results
- Participants showed higher intention to use the agent-based version over the physical card deck
- Participants viewed AgentBnB as more scalable than the physical card deck
- All participants scored perfectly on a simple knowledge quiz, though a ceiling effect limited learning assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation with Bloom-aligned categorization may improve hint relevance and reduce hallucinations during gameplay.
- Mechanism: The C2D2 pipeline pre-processes 77 curated documents into four Bloom categories (factual, conceptual, procedural, metacognitive) via GPT-4 prompts, embeds each ~300-token chunk with text-embedding-ada-002, and stores vectors with bloom_tag metadata in ChromaDB. At query time, cosine similarity retrieves top-10 passages, which are injected into a structured prompt alongside recent group-chat context and an instructional directive before GPT-4o generates a cited response.
- Core assumption: Bloom-labeled retrieval maps meaningfully to learner cognitive state; the 8-step scaffolding ladder correctly escalates based on observed learner signals.
- Evidence anchors: [abstract] "integrates a Bloom-aligned, retrieval-augmented copilot"; [section V.C] detailed RAG pipeline description; corpus evidence limited, related work on LLMs in exercise feedback suggests prompt engineering can improve feedback quality.
- Break condition: If learner queries are ambiguous or out-of-corpus, retrieval may return low-relevance passages; scaffolding logic may fail to calibrate if mastery signals are misattributed.

### Mechanism 2
- Claim: LLM-based agent teammates with pedagogical prompts may enable scalable solo practice while maintaining social interaction patterns of traditional TTXs.
- Mechanism: Each agent receives a system prompt prioritizing teaching over winning, embedding Bloom-aware scaffolding directives. The Incident Captain acts as mentor, using Socratic questioning before direct guidance; Defender prompts encourage self-explanation and peer coaching. All agent queries route through C2D2's RAG pipeline for knowledge grounding.
- Core assumption: Prompt engineering alone can shift agent behavior from performance-optimized to pedagogically-aligned; human learners perceive AI teammates as legitimate collaborators.
- Evidence anchors: [abstract] "integrates large language model teammates"; [section V.B.3] agent pedagogical prompts and C2D2 routing; corpus evidence limited, related multi-agent LLM work demonstrates coordination feasibility.
- Break condition: If prompts are insufficiently constraining, agents may hallucinate or over-scaffold; learner trust may erode if agent dialogue feels mechanical.

### Mechanism 3
- Claim: Browser-based state management and telemetry logging may enable repeatable, research-observable gameplay without backend infrastructure.
- Mechanism: Game state (attack cards, procedure cards, dice rolls, cooldowns) is held in-memory client-side. Telemetry hooks record chat turns, dice outcomes, and copilot queries to JSON Lines in-browser, with CSV/JSON export for post-hoc analysis.
- Core assumption: Client-side state persistence is sufficient for session continuity; browser-based logging captures decision-relevant signals for future adaptive scaffolding.
- Evidence anchors: [section V.B.2] client-side state updates; [section V.B.5] telemetry logging details; no direct corpus evidence for this architectural pattern.
- Break condition: If sessions span multiple browser contexts or require persistence across days, in-memory state is lost; telemetry gaps may occur on browser crashes.

## Foundational Learning

- Concept: Bloom's Revised Taxonomy (knowledge dimensions Ã— cognitive processes)
  - Why needed here: C2D2's entire hint system depends on categorizing content into factual/conceptual/procedural/metacognitive types and matching hints to learner cognitive stage.
  - Quick check question: Given a learner struggling to choose between containment and eradication, which Bloom level (Analyze vs. Evaluate) should a procedural hint target?

- Concept: Retrieval-Augmented Generation (RAG) fundamentals
  - Why needed here: Understanding dense embedding, vector similarity search, and prompt assembly is required to debug C2D2's hint quality or corpus gaps.
  - Quick check question: If C2D2 returns irrelevant passages for a query about "lateral movement detection," what three pipeline stages should you inspect first?

- Concept: Scaffolding and fading in instructional design
  - Why needed here: The 8-step scaffolding ladder assumes contingent support that escalates on failure and fades on mastery; understanding this is critical for tuning agent prompts.
  - Quick check question: A learner correctly identifies the attack phase but cannot explain why their procedure choice was appropriate. Which scaffolding step (2-7) should the agent apply next?

## Architecture Onboarding

- Component map: GUI (Group Chat, C2D2 Chat, HUD) -> Game Engine -> Agent Layer -> C2D2 RAG Module -> Telemetry
- Critical path: User action -> Game Engine validates (dice roll, cooldowns) -> Group Chat displays outcome -> Agent Layer generates response (optionally queries C2D2) -> Telemetry logs turn
- Design tradeoffs: Client-side state vs. backend persistence enables lightweight deployment but limits session continuity; solo-only mode simplifies coordination but reduces social learning; narrow corpus reduces retrieval noise but may miss edge-case queries
- Failure signatures: Copilot returns generic or hallucinated hints -> check retrieval scores and citation presence; Agent repeats same scaffolding step without escalation -> inspect scaffolding ladder logic; Game state resets unexpectedly -> confirm in-memory state is not being cleared
- First 3 experiments: 1) Query C2D2 with 10 test questions across Bloom levels; 2) Run 5 solo gameplay sessions with telemetry logging; 3) Inject out-of-corpus queries during gameplay

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does AgentBnB produce measurable learning gains across Bloom's taxonomy compared to traditional tabletop exercises?
- Basis in paper: [explicit] The authors report a "ceiling effect" in their pilot assessment and acknowledge that the study design "restricted claims about learning gains." Section IX explicitly calls for "comparative studies... to provide stronger causal evidence of learning gains."
- Why unresolved: The pilot used a simple knowledge quiz where all participants scored perfectly (3/3), making it impossible to distinguish knowledge acquisition from prior knowledge.
- What evidence would resolve it: A controlled study with a larger cohort using a pre-test/post-test design with more discriminating questions to measure delta in cognitive skills.

### Open Question 2
- Question: How do learning outcomes and coordination dynamics change in multi-player or hybrid human-AI team modes?
- Basis in paper: [explicit] Section VIII cites the "single-player focus" as a limitation of the current system. Section IX lists "multi-player scalability" as a planned extension to enable "studies of coordination, escalation paths, and organizational communication."
- Why unresolved: The current pilot study only validated the system for solo learners; it is unknown if the agent architecture supports effective group coordination or introduces friction in team settings.
- What evidence would resolve it: Studies analyzing communication patterns and decision-making quality in sessions involving multiple human defenders or hybrid human-machine teams.

### Open Question 3
- Question: What is the marginal pedagogical impact of the retrieval-augmented copilot (C2D2) versus the agent-based gameplay alone?
- Basis in paper: [explicit] Section IX states that future work will include "conditions such as copilot versus no copilot" to determine efficacy.
- Why unresolved: While the system integrates C2D2, the study did not isolate the variable to determine if the retrieval-aligned scaffolding actually improves performance or simply adds cognitive load.
- What evidence would resolve it: A randomized controlled trial (A/B test) comparing learning efficiency, hint usage, and retention between groups playing with and without the C2D2 module enabled.

## Limitations
- Small pilot sample (n=4) and single post-session measurement limit generalizability and learning effect assessment
- Ceiling effect on knowledge quiz scores prevents robust learning gain measurement across Bloom levels
- Critical architectural details (corpus document URLs, GPT-4 prompts, complete agent system prompts) are unspecified, creating reproduction barriers

## Confidence

- **High Confidence**: The feasibility of browser-based TTX delivery with LLM teammates and the scalability advantage over physical card decks are well-supported by participant intention-to-use ratings and the architectural demonstration.
- **Medium Confidence**: The Bloom-aligned RAG scaffolding mechanism's effectiveness is supported by design logic but lacks empirical validation beyond participant perceptions; the knowledge quiz's ceiling effect prevents definitive learning assessment.
- **Low Confidence**: Generalizability to diverse learner populations and real-world incident response scenarios remains untested due to the pilot's graduate-student sample and narrow corpus scope.

## Next Checks

1. **Corpus and RAG Validation**: Conduct 10 test queries across all Bloom levels to verify retrieval relevance, citation accuracy, and identify corpus gaps before scaling to broader study populations.
2. **Scaffolding Efficacy Analysis**: Run 5 solo gameplay sessions with telemetry logging to analyze scaffolding step usage patterns and detect over/under-guidance by examining chat transcripts and agent decision paths.
3. **Difficulty Calibration Study**: Pilot-test knowledge quiz items across multiple Bloom levels on a larger sample to establish discriminate items and prevent ceiling effects in future learning assessments.