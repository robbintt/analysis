---
ver: rpa2
title: 'ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation'
arxiv_id: '2509.26278'
source_url: https://arxiv.org/abs/2509.26278
tags:
- proficiency
- language
- video
- profvlm
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProfVLM reformulates proficiency estimation as a generative vision-language
  modeling task, enabling both skill level prediction and expert-like feedback generation
  from multi-view videos. It uses a frozen TimeSformer for feature extraction, followed
  by an AttentiveGatedProjector that fuses egocentric and exocentric views via attention
  and gating, then projects features into a language model space.
---

# ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation

## Quick Facts
- **arXiv ID:** 2509.26278
- **Source URL:** https://arxiv.org/abs/2509.26278
- **Authors:** Edoardo Bianchi; Jacopo Staiano; Antonio Liotta
- **Reference count:** 40
- **Primary result:** 48.2% accuracy in multi-view proficiency estimation using 20× fewer parameters than baselines

## Executive Summary
ProfVLM introduces a novel approach to proficiency estimation by reformulating it as a generative vision-language modeling task. The model processes both egocentric and exocentric views of human activities using a frozen TimeSformer feature extractor, followed by an AttentiveGatedProjector that fuses multi-view information through attention mechanisms and gating. A LoRA-tuned SmolLM2 language model then generates both skill level predictions and expert-like textual feedback. ProfVLM achieves state-of-the-art performance on the EgoExo4D dataset while using significantly fewer parameters and training time compared to traditional classification approaches.

## Method Summary
ProfVLM processes multi-view video data through a frozen TimeSformer to extract spatiotemporal features, which are then fused using an AttentiveGatedProjector that employs attention mechanisms and gating to combine egocentric and exocentric perspectives. The fused features are projected into a language model space where a LoRA-tuned SmolLM2 generates both proficiency level predictions and expert-like feedback. This generative approach enables interpretable reasoning alongside predictions, distinguishing it from traditional classification methods. The model is trained on the EgoExo4D dataset and achieves superior efficiency metrics compared to baseline approaches.

## Key Results
- Achieves 48.2% accuracy in multi-view proficiency estimation
- Uses 20× fewer parameters than classification baselines
- Requires 60% less training time than traditional approaches
- Generates high-quality textual feedback with BERTScore F1 of 85.53

## Why This Works (Mechanism)
The generative vision-language approach allows ProfVLM to simultaneously predict skill levels and provide expert-like feedback, creating a more interpretable system than traditional classification methods. By using a frozen TimeSformer for feature extraction, the model leverages pretrained spatiotemporal understanding while focusing computational resources on the fusion and generation components. The AttentiveGatedProjector effectively combines complementary information from multiple camera perspectives, capturing both the performer's view and external observations. The LoRA tuning approach enables efficient adaptation of the language model without extensive retraining.

## Foundational Learning
- **Multi-view video processing**: Essential for capturing complementary perspectives in skill assessment, enabling comprehensive understanding of human activities
  - Quick check: Verify the model can process and fuse information from both egocentric and exocentric camera feeds
- **Vision-language modeling**: Required for generating natural language explanations alongside predictions, providing interpretability
  - Quick check: Test that the model can produce coherent, contextually relevant feedback for given proficiency levels
- **Efficient fine-tuning with LoRA**: Critical for adapting large language models without full parameter updates, reducing computational overhead
  - Quick check: Confirm parameter efficiency compared to full fine-tuning approaches
- **Attention mechanisms**: Necessary for selectively focusing on relevant spatiotemporal features across different views
  - Quick check: Validate attention patterns highlight meaningful regions during proficiency estimation
- **TimeSformer architecture**: Provides pretrained spatiotemporal feature extraction capabilities for video understanding
  - Quick check: Ensure frozen feature extractor maintains performance across different video domains

## Architecture Onboarding

**Component Map:**
Input Videos -> Frozen TimeSformer -> AttentiveGatedProjector -> LoRA-tuned SmolLM2 -> Output (Proficiency + Feedback)

**Critical Path:**
The core inference pipeline follows: video input → TimeSformer feature extraction → AttentiveGatedProjector fusion → SmolLM2 generation. The frozen TimeSformer provides fixed spatiotemporal representations, the AttentiveGatedProjector performs cross-view fusion, and the LoRA-tuned language model generates the final outputs.

**Design Tradeoffs:**
The architecture prioritizes efficiency through frozen feature extractors and LoRA tuning, accepting potential limitations in domain-specific adaptation for significant gains in parameter efficiency and training speed. The generative approach trades some classification accuracy for interpretability and feedback capabilities.

**Failure Signatures:**
Performance degradation may occur when input videos significantly differ from the EgoExo4D training distribution, when one camera view is corrupted or missing, or when the language model struggles with domain-specific terminology not well-represented in training data.

**First Experiments:**
1. Test single-view performance to quantify the contribution of multi-view fusion
2. Evaluate feedback quality with human experts on a held-out validation set
3. Benchmark inference latency and memory usage against classification baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to EgoExo4D dataset, limiting generalizability to other skill domains
- Performance on long-form videos beyond 2-minute training limit remains untested
- Frozen TimeSformer may not capture domain-specific nuances for specialized skill assessments
- Lack of user studies to validate the practical utility of generated feedback from expert perspectives

## Confidence
- **High confidence**: Technical implementation details and comparative performance metrics are empirically demonstrated with specific numbers
- **Medium confidence**: Interpretability claims based on BERTScore automated quality measures, though human-centered utility remains untested
- **Low confidence**: Real-world deployment readiness due to absence of clinical or expert validation studies

## Next Checks
1. Test ProfVLM on additional datasets covering different skill domains and video characteristics to assess generalizability
2. Conduct expert user studies to evaluate the practical utility and quality of the generated feedback
3. Benchmark against larger, more computationally intensive models to verify claimed efficiency gains don't compromise accuracy in extended or complex scenarios