---
ver: rpa2
title: 'Auto-US: An Ultrasound Video Diagnosis Agent Using Video Classification Framework
  and LLMs'
arxiv_id: '2511.07748'
source_url: https://arxiv.org/abs/2511.07748
tags:
- ultrasound
- video
- dataset
- diagnostic
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Auto-US addresses the challenge of AI-assisted ultrasound video
  diagnosis by integrating deep learning video classification with large language
  models (LLMs). The method introduces CTU-Net, a novel CNN-Transformer architecture
  that captures temporal, spatial, and frequency-domain information from ultrasound
  videos, and Auto-US, an intelligent diagnostic agent that combines video classification
  with clinical text.
---

# Auto-US: An Ultrasound Video Diagnosis Agent Using Video Classification Framework and LLMs

## Quick Facts
- arXiv ID: 2511.07748
- Source URL: https://arxiv.org/abs/2511.07748
- Reference count: 40
- Auto-US integrates deep learning video classification with LLMs for ultrasound video diagnosis, achieving 86.73% accuracy and generating clinically validated diagnostic suggestions.

## Executive Summary
Auto-US addresses the challenge of AI-assisted ultrasound video diagnosis by integrating deep learning video classification with large language models (LLMs). The method introduces CTU-Net, a novel CNN-Transformer architecture that captures temporal, spatial, and frequency-domain information from ultrasound videos, and Auto-US, an intelligent diagnostic agent that combines video classification with clinical text. The study constructs CUV Dataset, a comprehensive dataset of 495 ultrasound videos across five disease categories and three organs. CTU-Net achieves state-of-the-art performance with 86.73% accuracy in ultrasound video classification. When integrated with LLMs, Auto-US generates clinically meaningful diagnostic suggestions validated by professional clinicians, with final diagnostic scores exceeding 3 out of 5. This demonstrates the effectiveness and clinical potential of combining deep learning and LLMs for ultrasound video-assisted diagnosis.

## Method Summary
Auto-US combines a novel tri-path CNN-Transformer architecture (CTU-Net) with LLM-based diagnostic report generation. CTU-Net processes ultrasound videos through three parallel paths: a slow path using 3D convolutions for spatial feature extraction, a fast path using Transformer attention for temporal dynamics, and a frequency path that extracts spectral features via FFT and high-pass filtering. The frequency path generates adaptive fusion weights that combine the slow and fast path outputs. The system is trained on the CUV Dataset (495 videos across 5 disease categories) and achieves 86.73% accuracy. For clinical integration, CTU-Net outputs a disease category that is combined with structured clinical text in a template prompt, which is then processed by DeepSeek-R1-7B to generate diagnostic reports including preliminary diagnosis, justification, and follow-up recommendations.

## Key Results
- CTU-Net achieves state-of-the-art 86.73% accuracy in ultrasound video classification on the CUV Dataset
- Tri-path architecture with frequency-guided fusion shows 11.22% accuracy drop when removing slow path, 3.06% drop when removing fast path, and 0.87% drop when removing frequency path
- LLM integration generates clinically validated diagnostic suggestions with scores exceeding 3 out of 5 from professional clinicians
- METEOR scores for generated reports range from 0.39-0.42, indicating lexical mismatch but acceptable clinical utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tri-path architecture capturing temporal, spatial, and frequency-domain information improves ultrasound video classification accuracy.
- Mechanism: The slow path (3D CNN) extracts spatial features; the fast path (Transformer) captures temporal dynamics with stride-5 sampling; the frequency path applies 2D FFT and high-pass filtering to extract spectral features that guide fusion weights between the other two paths.
- Core assumption: Ultrasound lesion boundaries and diagnostic features are encoded across multiple signal domains, and their joint representation is more discriminative than any single domain.
- Evidence anchors:
  - [abstract] "CTU-Net achieves state-of-the-art performance with 86.73% accuracy in ultrasound video classification"
  - [section] Table 3 ablation: removing slow path drops accuracy to 75.51% (-11.22%), removing fast path drops to 83.67% (-3.06%), removing frequency path drops to 85.86% (-0.87%)
  - [corpus] Weak direct validation; corpus papers focus on federated ultrasound learning and RAG-based diagnosis, not multi-domain feature fusion for video classification
- Break condition: If input videos have severe motion artifacts corrupting frequency spectra, or if temporal stride α=5 is inappropriate for the pathology's temporal scale, fusion weights may misallocate contributions.

### Mechanism 2
- Claim: Frequency-domain features can guide adaptive fusion of spatial and temporal representations.
- Mechanism: Frequency path computes magnitude spectrum via RFFT2, applies Laplacian high-pass filter to enhance edges, then uses a lightweight MLP to generate softmax-normalized weights [αs, αf] that modulate slow and fast path outputs: F_fused = αs · Fs + αf · Ff.
- Core assumption: High-frequency components correlate with lesion boundary information, and their presence indicates when spatial versus temporal features should be emphasized.
- Evidence anchors:
  - [abstract] "captures temporal, spatial, and frequency-domain information"
  - [section] "removing the frequency path resulted in a 5.83% decrease in recall, indicating that simple fusion of temporal and spatial domains is insufficient"
  - [corpus] No direct corpus validation of frequency-guided fusion in ultrasound; Spectformer and related work cited but not evaluated on medical video
- Break condition: If ultrasound images have high noise in high-frequency bands (common in low-quality scans), the MLP may generate unreliable fusion weights, degrading classification.

### Mechanism 3
- Claim: Structured prompts combining classification labels with clinical context enable LLMs to generate clinically useful diagnostic suggestions.
- Mechanism: CTU-Net outputs a disease category; this is combined with chief complaint, physical examination, and additional information into a template prompt; DeepSeek-R1-7B generates preliminary diagnosis, justification, and follow-up recommendations following international guidelines (BI-RADS, IDSA/ATS, ACG).
- Core assumption: The classification label provides sufficient diagnostic signal, and LLMs can synthesize this with structured clinical text using guideline-embedded prompts to produce actionable recommendations.
- Evidence anchors:
  - [abstract] "final diagnostic scores for each case exceeded 3 out of 5 and were validated by professional clinicians"
  - [section] Case Study (Tables 4-5): malignant breast tumor case scored 3.25/5, gallbladder disease case scored 3.29/5; METEOR scores low (0.42, 0.39) but clinical utility rated acceptable
  - [corpus] Corpus supports LLM diagnostic utility: ICA-RAG and Jingfang papers demonstrate LLM-based medical reasoning, though not specifically for ultrasound video
- Break condition: If classification is incorrect (13.27% error rate), LLM will generate recommendations for wrong pathology; if case requires pathology/molecular data not in prompt, recommendations will be incomplete.

## Foundational Learning

- Concept: **3D Convolutions for Video Understanding**
  - Why needed here: The slow path uses Conv3D with residual connections to extract spatial features across video frames; understanding spatiotemporal kernels is essential for interpreting the architecture.
  - Quick check question: Given input X ∈ R^(T×H×W×C), what is the output shape after a 3×3×3 convolution with stride 1 and padding 1?

- Concept: **Vision Transformer Patch Embedding and Attention**
  - Why needed here: The fast path uses patch embedding (kernel=p, stride=p) and decomposed spatial-then-temporal attention; understanding self-attention mechanics is required for debugging temporal modeling.
  - Quick check question: For a 224×224 frame with patch size p=4, how many patch tokens are generated per frame before adding the CLS token?

- Concept: **FFT and Frequency-Domain Filtering**
  - Why needed here: The frequency path applies 2D real-input FFT, extracts magnitude spectrum, and applies Laplacian high-pass filtering; understanding spectral representations is necessary for analyzing fusion weight generation.
  - Quick check question: After RFFT2 on an H×W real image, what are the dimensions of the complex output, and why is W/2+1 used?

## Architecture Onboarding

- Component map:
  ```
  Input Video (T×224×224×3)
         │
    ┌────┼────────────┐
    │    │            │
  Slow   Fast     Frequency
  Path   Path        Path
 (3DCNN) (Trans)   (FFT+HPF)
    │    │            │
   Fs    Ff     [αs, αf] weights
    │    │            │
    └────┼────────────┘
         │
    Fused = αs·Fs + αf·Ff
         │
    Linear Classifier → 5 classes
         │
    Class Label + Clinical Text
         │
    Structured Prompt
         │
    DeepSeek-R1-7B → Diagnostic Report
  ```

- Critical path: Slow path feature extraction (Equation 3) → Frequency path weight generation (Equation 15) → Weighted fusion (Equation 16) → Classification (Equation 17). If slow path fails (ablation shows 11% accuracy drop), the system degrades severely.

- Design tradeoffs:
  - Temporal stride α=5 reduces computation but may miss fine-grained motion; paper does not justify this hyperparameter choice.
  - Patch size p=4 balances granularity vs. sequence length; larger patches reduce temporal tokens but lose spatial detail.
  - Single LLM (DeepSeek-R1-7B) keeps deployment simple but limits reasoning diversity; no comparison with other LLMs reported.

- Failure signatures:
  - Low recall on minority classes: ablation shows recall drops to 56.45% without slow path, suggesting spatial features critical for imbalanced categories.
  - Low METEOR scores (0.39-0.42) with acceptable clinical ratings: indicates lexical mismatch but semantic correctness; expect this in medical text generation.
  - Missing pathology details in LLM output: Case studies show model omits immunohistochemistry (ER/PR/HER2) and specific lab values—prompt lacks this information.

- First 3 experiments:
  1. **Reproduce ablation on each path** with the CUV Dataset split (80/20): train CTU-Net, then remove each path and measure accuracy/recall/precision. Expect ~11% drop removing slow path, ~3% removing fast path, ~1% removing frequency path.
  2. **Test temporal stride sensitivity**: vary α ∈ {2, 5, 10} and measure classification accuracy. If α=2 improves performance without prohibitive memory cost, the default may be suboptimal.
  3. **LLM prompt ablation**: generate diagnostic reports with vs. without the classification label in the prompt. Measure whether label inclusion improves clinical scores (expect ~0.5-1 point improvement if label provides meaningful signal).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of pathological and molecular data improve the diagnostic accuracy of Auto-US in complex cases where ultrasound imaging alone is insufficient?
- Basis: [explicit] The authors state that "relying solely on ultrasound imaging is insufficient" and identify the "integration of multi-modal models" as a focus for future development.
- Why unresolved: The current agent relies exclusively on ultrasound video and clinical text, lacking critical ground-truth data such as immunohistochemistry or biopsy results.
- What evidence would resolve it: A comparative study evaluating the current agent against a multi-modal version on cases requiring fine-grained differential diagnosis.

### Open Question 2
- Question: Can the CTU-Net architecture maintain its performance advantage over Transformer baselines when scaled to significantly larger, multi-center datasets?
- Basis: [explicit] The Discussion notes that "publicly available high-quality ultrasound video datasets are extremely limited" and lists "incorporating larger-scale datasets" as future work.
- Why unresolved: The current state-of-the-art results (86.73% accuracy) are derived from the CUV Dataset (495 videos), and generalizability to larger, more diverse populations is unproven.
- What evidence would resolve it: Benchmarking CTU-Net against baselines like ViViT and TimeSformer on a multi-center dataset an order of magnitude larger than CUV.

### Open Question 3
- Question: To what extent does the frequency path mitigate the effects of class imbalance compared to standard re-sampling or cost-sensitive learning techniques?
- Basis: [inferred] The ablation study analysis suggests that the performance drop when omitting the frequency path is "especially evident when dealing with class imbalance."
- Why unresolved: While the frequency path improves recall, the paper does not isolate its efficacy as a solution for class imbalance against other standard methods.
- What evidence would resolve it: An ablation study on a dataset with induced severe class imbalance comparing the frequency path's performance against standard balancing techniques.

## Limitations
- Clinical validation limited to five-point Likert scale ratings rather than direct comparison with gold-standard diagnoses or patient outcomes
- Small dataset size (495 videos) may limit generalizability to diverse populations and ultrasound equipment types
- Frequency-guided fusion mechanism lacks direct validation through visualization of fusion weight distributions or sensitivity analysis to ultrasound quality variations

## Confidence
- **High Confidence**: The architectural design of CTU-Net with its tri-path structure is clearly specified and the ablation study provides strong evidence that each path contributes to performance. The integration methodology with LLMs is explicitly detailed.
- **Medium Confidence**: The 86.73% classification accuracy is reported with appropriate metrics, but the small dataset size and limited diversity across ultrasound equipment types reduce confidence in real-world applicability. The clinical utility ratings (3.25/5 and 3.29/5) indicate acceptability but not excellence.
- **Low Confidence**: The frequency-guided fusion weights' reliability under varying ultrasound image quality conditions is not established. The METEOR scores (0.39-0.42) suggest significant lexical mismatch between generated and reference texts, raising questions about the clinical utility of the LLM-generated reports despite acceptable clinician ratings.

## Next Checks
1. **Dataset Expansion and Cross-Validation**: Replicate the study using k-fold cross-validation on the CUV Dataset and test model performance when combining videos from different ultrasound equipment manufacturers to assess generalizability across hardware variations.
2. **Fusion Weight Analysis**: Visualize and analyze the frequency-guided fusion weights (αs, αf) across different ultrasound video qualities and disease categories to verify they adapt appropriately to input characteristics and don't collapse to degenerate values.
3. **Clinical Outcome Correlation**: Conduct a prospective study comparing Auto-US diagnostic recommendations against actual patient outcomes and gold-standard diagnoses, measuring not just classification accuracy but clinical decision-making impact and patient safety.