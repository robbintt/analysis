---
ver: rpa2
title: 'HalluHard: A Hard Multi-Turn Hallucination Benchmark'
arxiv_id: '2602.01031'
source_url: https://arxiv.org/abs/2602.01031
tags:
- hallucination
- grounding
- judge
- reference
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HALLUHARD is a new multi-turn hallucination benchmark spanning\
  \ four high-stakes domains\u2014legal, research, medical, and coding\u2014where\
  \ LLMs must cite inline sources. To evaluate groundedness, a modular judge pipeline\
  \ extracts claims, plans evidence retrieval, fetches full-text sources (including\
  \ PDFs), and produces structured verdicts on reference and content grounding."
---

# HalluHard: A Hard Multi-Turn Hallucination Benchmark

## Quick Facts
- arXiv ID: 2602.01031
- Source URL: https://arxiv.org/abs/2602.01031
- Reference count: 40
- LLMs hallucinate 30-70% of the time even with web search, with content-grounding failures dominating

## Executive Summary
HALLUHARD is a multi-turn hallucination benchmark spanning legal, research, medical, and coding domains, where models must cite inline sources to substantiate claims. The evaluation uses a modular judge pipeline that extracts atomic claims, retrieves evidence via iterative web search, fetches full-text sources (including PDFs), and produces structured verdicts on reference and content grounding. Across frontier models, hallucination rates remain substantial even with web search enabled—approximately 30% for the strongest setting (Opus-4.5 with web search)—with content-grounding failures dominating. Hallucinations increase in later turns due to error propagation, decrease with model capacity and effective reasoning, and are especially prevalent for niche knowledge and detailed in-depth claims.

## Method Summary
The benchmark evaluates multi-turn conversations across four high-stakes domains, requiring models to cite inline sources for factual claims. For legal, research, and medical domains, a modular judge pipeline extracts atomic claims from responses, plans evidence retrieval using iterative Serper API searches (up to 5 steps, 5 sources per step), fetches full-text content including PDFs, and generates verdicts on reference grounding (source exists) and content grounding (source supports claim). The context selector uses embedding-based truncation to ~1,500 words. Coding responses are evaluated at the response level using GPT-5-mini-web-search judge. The benchmark uses 950 seed questions with 2 follow-up questions per seed, evaluating hallucination rates as the percentage of unverifiable or unsupported claims.

## Key Results
- Hallucination rates remain high (30-70%) even with web search enabled, with content-grounding failures dominating over reference-grounding failures
- Hallucination rates increase across conversation turns (3-20% of incorrect references from Turn 1 reappear in later turns) due to error propagation
- Niche knowledge triggers more hallucinations than fabricated knowledge because models are "incentivized to guess" when traces of knowledge exist

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn hallucinations increase through error propagation as models condition on their own prior mistakes
- Mechanism: Models receive full conversation history at each turn. When an incorrect citation or claim appears early, subsequent responses reuse and compound the error—3–20% of incorrect references from Turn 1 reappear in later turns. This "self-conditioning effect" creates a feedback loop where ungrounded claims become pseudo-context
- Core assumption: Models do not reliably distinguish between grounded prior context and their own prior hallucinations when generating continuations
- Evidence anchors: [abstract] "early errors cascade" in multi-turn dialogue; [section 5.2] "hallucination rate of LLMs goes up with more conversation turns for tasks that require citation grounding"; [corpus] Related work on multi-turn benchmarks (KnowMT-Bench) shows similar error accumulation patterns
- Break condition: If models could self-verify citations against retrieved sources before conditioning on them, propagation would decrease

### Mechanism 2
- Claim: Content-grounding failures dominate reference-grounding failures even with web search enabled
- Mechanism: Reference grounding (source exists/correct) is easier to verify than content grounding (source supports the claim). Web search reduces reference failures substantially (from ~28–39% to ~6–7%), but content failures persist at 29–52%. Full-text verification is needed because snippets miss details buried in PDFs, tables, or footnotes
- Core assumption: Models generate plausible-sounding elaborations that extend beyond what sources actually state, and snippet-based judges cannot catch this
- Evidence anchors: [abstract] "content-grounding failures dominating"; [section 3] "a model may cite an appropriate source but still fabricate details that the source does not substantiate"; [corpus] CHECK framework (arXiv 2506.11129) similarly distinguishes factual vs reasoning hallucinations in medical contexts
- Break condition: If judges fetch and parse full-text sources including PDFs, content-grounding detection improves (our judge achieves 87.9% agreement vs SAFE's 81.8% on content grounding)

### Mechanism 3
- Claim: Niche knowledge triggers more hallucinations than fabricated knowledge because models are "incentivized to guess"
- Mechanism: Completely fabricated entities have no training-data footprint, so models abstain. Niche entities have "some traces" creating a "dangerous middle zone" where non-zero probability of correctness encourages speculation rather than refusal
- Core assumption: Models learn uncertainty estimates correlated with training-data frequency; low but non-zero familiarity produces overconfident generation
- Evidence anchors: [section 6] "models tend to struggle with niche queries, yet are more likely to abstain when faced with completely fabricated items"; [section 5.2] Python (most common language) shows lowest hallucination rates; niche languages like Elixir show higher rates (Table 8); [corpus] WildHallucinations (Zhao et al. 2024) similarly finds higher rates for entities lacking Wikipedia pages
- Break condition: Explicit uncertainty calibration or mandatory web verification for low-frequency knowledge would reduce this

## Foundational Learning

- Concept: **In-context vs In-parameter hallucination**
  - Why needed here: The paper distinguishes hallucinations relative to provided context (in-context) versus training data (in-parameter). The benchmark operationalizes groundedness by requiring models to cite external sources verifiable via web search
  - Quick check question: If a model correctly recalls a fact from training data but cites the wrong source, which type of failure is this?

- Concept: **Atomic claim extraction**
  - Why needed here: The judge pipeline decomposes responses into atomic factual claims for individual verification. This enables fine-grained hallucination measurement rather than response-level binary labels
  - Quick check question: Why might extracting "the paper shows X using method Y" as a single claim cause verification problems?

- Concept: **Retrieval-augmented evaluation**
  - Why needed here: The judge uses iterative web search with Serper API, then fetches full-text content. Understanding the difference between snippet-based and full-text retrieval is critical for interpreting judge accuracy
  - Quick check question: What information might be present in a PDF but absent from search snippets?

## Architecture Onboarding

- Component map: Seed question generator -> User LLM (follow-up questions) -> Target LLM (multi-turn responses) -> Claim extractor -> Evidence planner (Serper API) -> Context selector (full-text fetch) -> Verdict generator
- Critical path: The judge's accuracy depends on the context selector's ability to retrieve relevant full-text passages. If PDFs cannot be fetched or parsed, content-grounding verification fails
- Design tradeoffs:
  - Fixed claim sampling (5 per response) vs exhaustive extraction → Controls cost but may miss edge-case hallucinations
  - Serper snippet + full-text fetch vs snippet-only (SAFE) → Higher cost but better content-grounding accuracy
  - Per-turn vs conversation-level hallucination rates → Per-turn reveals error propagation but requires more judgments
- Failure signatures:
  - "Reference N/A, Content N/A" → Technical access failure (403/404/timeout)
  - High reference failure, low content failure → Citation formatting issues rather than fabrication
  - Rising hallucination rate across turns → Self-conditioning error propagation
- First 3 experiments:
  1. **Baseline comparison**: Run GPT-5.2-thinking and Claude-Opus-4.5 on 50 research-domain seed questions with/without web search; verify that web search reduces reference failures but content failures persist (expected ~50% content failure rate per Table 5)
  2. **Turn-wise analysis**: On a single model, measure per-turn hallucination rates; confirm upward trend in citation-required domains and downward trend in coding (Figure 6 vs Figure 9c)
  3. **Judge ablation**: Compare verdicts from (a) snippet-only judge, (b) full-text judge without PDF parsing, (c) full pipeline; expect 3–6% accuracy gain from full-text retrieval on content-grounding (per Table 2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can error propagation caused by self-conditioning on earlier mistakes be effectively mitigated in multi-turn dialogue systems?
- Basis in paper: [explicit] The authors observe that "models always see the full conversation history" and "start conditioning on its own earlier mistakes," leading to hallucination rates that increase in later turns
- Why unresolved: The paper quantifies the phenomenon (3-20% of incorrect references repeat) but does not propose or test methods to interrupt this self-reinforcing loop
- What evidence would resolve it: A study evaluating hallucination rates when dialogue history is dynamically filtered or corrected before being fed back to the model in subsequent turns

### Open Question 2
- Question: Why does increased reasoning effort often fail to yield further reductions in hallucination rates for thinking models?
- Basis in paper: [explicit] The conclusion states that while "enabling effective thinking reduces hallucinations, additional reasoning effort does not necessarily yield further gains"
- Why unresolved: The paper demonstrates the lack of correlation (e.g., DeepSeek-Reasoner vs. DeepSeek-Chat) but does not determine if this is due to data distribution, reasoning pathologies, or a fundamental ceiling in the model's parametric knowledge
- What evidence would resolve it: An analysis correlating the length/depth of reasoning chains with the specific success or failure of grounding for individual atomic claims

### Open Question 3
- Question: To what extent does the inability to parse full-text PDFs directly contribute to the high content-grounding failure rates observed in research domains?
- Basis in paper: [explicit] The authors note that content-grounding errors persist at high rates and that "models cannot directly open PDF files from retrieved links, limiting their ability to verify details"
- Why unresolved: While the paper identifies the bottleneck (PDFs vs. snippets), it does not isolate the specific error contribution of PDF inaccessibility versus other factors like synthesis complexity
- What evidence would resolve it: A comparative evaluation using a "gold-standard" retrieval system where the model is provided with full-text context versus the current snippet-based retrieval

### Open Question 4
- Question: How can models be improved to better recognize uncertainty in "niche" knowledge to prevent them from guessing rather than abstaining?
- Basis in paper: [explicit] The authors identify a "dangerous middle zone" where models "struggle with niche facts" but successfully abstain from fabricated ones, suggesting they are incentivized to guess when traces of knowledge exist
- Why unresolved: The paper highlights the behavioral gap but leaves the underlying calibration of uncertainty thresholds for low-frequency entities as an open challenge
- What evidence would resolve it: Experiments measuring abstention rates on a spectrum of entity popularity (citation counts/traffic) to identify the specific "popularity threshold" where calibration fails

## Limitations
- The evaluation pipeline relies on future GPT-5-mini-thinking judge access, which may not be available for replication
- Coding domain uses response-level rather than claim-level verification, creating an apples-to-oranges comparison with other domains
- Fixed claim sampling strategy (5 claims per response) may miss edge-case hallucinations that would affect rate estimates

## Confidence
- **High confidence**: Multi-turn error propagation patterns, content-grounding failure dominance over reference grounding, niche knowledge hallucination increases
- **Medium confidence**: Judge pipeline accuracy improvements over SAFE, the "dangerous middle zone" hypothesis for niche vs. fabricated knowledge
- **Low confidence**: Absolute hallucination rates across all models due to judge model access limitations and potential calibration issues

## Next Checks
1. Replicate the coding domain evaluation using the same response-level GPT-5-mini-web-search judge to confirm the ~5% hallucination rate consistency
2. Conduct human annotation validation on a subset of 50 claims across all domains to verify the 87.9% judge agreement rate and calibrate content-grounding strictness
3. Test error propagation by creating synthetic conversation histories with controlled hallucination injection to measure 3-20% citation reuse rate across different turn structures