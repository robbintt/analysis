---
ver: rpa2
title: 'Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?'
arxiv_id: '2512.23836'
source_url: https://arxiv.org/abs/2512.23836
tags:
- arxiv
- window
- language
- answer
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can effectively
  decline to answer when faced with insufficient information in retrieval-augmented
  question answering. The authors propose an adaptive prompting strategy that splits
  retrieved information into smaller chunks and sequentially prompts the LLM to answer
  using each chunk.
---

# Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?
arXiv ID: 2512.23836
Source URL: https://arxiv.org/abs/2512.23836
Authors: Dingmin Wang; Ji Ma; Shankar Kumar
Reference count: 10

Key outcome: This paper investigates whether large language models can effectively decline to answer when faced with insufficient information in retrieval-augmented question answering. The authors propose an adaptive prompting strategy that splits retrieved information into smaller chunks and sequentially prompts the LLM to answer using each chunk. This approach mitigates noise accumulation and improves computational efficiency compared to standard prompting that uses the full context at once. Experiments on three open-domain question answering datasets show that the adaptive strategy matches the performance of standard prompting while using over 50% fewer tokens. Crucially, analysis reveals that LLMs often generate incorrect answers instead of declining when encountering insufficient information, with zero-shot models generating false answers over 50% of the time on negative windows. This tendency toward hallucination persists even with few-shot in-context learning, suggesting that training-based interventions may be necessary to enhance LLMs' ability to appropriately decline requests.

## Executive Summary
This paper examines a critical limitation in retrieval-augmented question answering: large language models' inability to appropriately decline answering when faced with insufficient information. The authors propose an adaptive prompting strategy that processes retrieved context in smaller sequential windows rather than presenting all retrieved documents at once. This approach reduces noise accumulation and improves computational efficiency while maintaining answer accuracy. The key finding is that LLMs exhibit a persistent "bias to answer" that resists prompt-based correction—even with few-shot examples, models frequently generate incorrect answers on negative windows rather than admitting ignorance.

## Method Summary
The method employs a BM25 retriever to fetch top-K Wikipedia pages for each question, then processes these pages using a sliding window approach. Instead of presenting all K pages at once, the adaptive strategy splits retrieved information into windows of size w and sequentially prompts the LLM with each window in descending retrieval-score order. The LLM outputs either an answer (terminating the process) or "answer not found" (continuing to the next window). This contrasts with standard prompting that feeds all retrieved pages simultaneously. The approach uses Gemini 1.5 Pro with greedy decoding and tests optimal window size of 60 on the Natural Questions dataset.

## Key Results
- Adaptive prompting matches standard prompting's exact match accuracy while using over 50% fewer tokens
- Zero-shot models generate false answers on negative windows over 50% of the time (54.3%)
- Few-shot in-context learning fails to improve abstention accuracy on negative windows
- Backward sliding order (processing low-score pages first) causes significant performance degradation due to repeated hallucinations on negative windows

## Why This Works (Mechanism)
### Mechanism 1
Partitioning retrieved context into smaller sequential windows reduces noise accumulation per inference step, improving signal-to-noise ratio for the LLM. Instead of presenting all K retrieved pages at once (which mixes relevant and irrelevant content), the adaptive strategy feeds windows of size w << K pages. Each window contains fewer distractors, lowering reasoning complexity. The model either finds the answer and terminates, or signals insufficiency and proceeds. Core assumption: Irrelevant context degrades generation quality more than the cost of multiple inference calls. Evidence anchors: [abstract] "splitting the retrieved information into smaller chunks and sequentially prompting the LLM... mitigates noise accumulation"; [Section 1] "By isolating segments of text, we reduce the accumulation of irrelevant information within the active context"; [corpus] Neighbor paper "Context Length Alone Hurts LLM Performance" confirms context expansion does not guarantee performance gains.

### Mechanism 2
Processing windows in descending retrieval-score order minimizes exposure to negative windows, which are the primary source of hallucination errors. The retriever ranks pages by relevance. By sliding forward (high→low scores), the model encounters the positive window after fewer than 0.5 negative windows on average (at window size 60). In backward mode, it faces ~3.8 negative windows before reaching relevant content, causing repeated hallucinations that compound. Core assumption: The retriever's ranking correlates with actual answer presence. Evidence anchors: [Section 3.2] "while the standard (forward) approach encounters fewer than 0.5 negative windows... the backward approach encounters 3.8"; [Section 3.2] "feeding high-probability context windows first is critical to prevent hallucinations".

### Mechanism 3
LLMs exhibit a persistent "bias to answer" that resists prompt-based correction; few-shot in-context learning does not improve abstention on negative windows. On windows with no relevant information, the correct behavior is to output "answer not found." However, zero-shot models generate false answers 54.3% of the time. Increasing few-shot examples (2, 4, 6, 8 shots with balanced positive/negative examples) yields no significant improvement in abstention accuracy. Core assumption: The bias stems from instruction-tuning objectives that reward generation over refusal. Evidence anchors: [abstract] "zero-shot models generating false answers over 50% of the time on negative windows"; [Section 5] "few-shot prompting (ICL) is ineffective at mitigating this behavior... the 'bias to answer' is deeply ingrained in current instruction-tuned models".

## Foundational Learning
- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire method operates within a RAG framework; understanding how retrieval and generation couple is prerequisite.
  - Quick check question: Can you explain why retrieving more documents can hurt answer quality despite providing more potential evidence?

- **Concept: In-Context Learning (ICL)**
  - Why needed here: The paper tests whether ICL can teach abstention; understanding ICL's limits is essential for interpreting results.
  - Quick check question: What does it suggest about a model's capabilities when increasing demonstration examples yields no improvement on a task?

- **Concept: Hallucination in LLMs**
  - Why needed here: The paper's central finding is that hallucination persists even with retrieval grounding when context is insufficient.
  - Quick check question: How does retrieval augmentation theoretically reduce hallucination, and why does it fail in negative-window scenarios?

## Architecture Onboarding
- **Component map:** BM25 Retriever -> Window Builder -> Sliding Controller -> LLM Prompter -> Response Parser
- **Critical path:** Retrieval quality -> Window ordering -> First positive window position -> Number of negative windows encountered -> Hallucination rate
- **Design tradeoffs:**
  - Window size (w): Small = less noise per step, but more negative windows to process; Large = more noise, fewer steps. Paper finds w=60 optimal for NQ dataset.
  - K (retrieval budget): Higher K improves recall but increases latency and negative-window exposure if early ranks miss.
  - Forward vs. backward ordering: Forward minimizes hallucination risk; backward useful for stress-testing abstention.
- **Failure signatures:**
  - High hallucination rate on negative windows (>50% false answers in zero-shot)
  - Backward mode EM significantly lower than forward mode
  - No improvement with increased few-shot examples (flat accuracy curve across 0-8 shots)
- **First 3 experiments:**
  1. **Baseline comparison:** Run standard prompting (all K pages at once) vs. adaptive prompting across NQ, TriviaQA, HotpotQA. Measure EM and average token usage. Expect adaptive to match EM with ~50% fewer tokens.
  2. **Window size ablation:** Vary w ∈ {40, 60, 80, 200} on NQ. Plot EM and average negative-window count. Confirm inverted-U pattern and identify optimal w.
  3. **Negative-window stress test:** Evaluate model on pure negative windows (constructed per Figure 6). Measure abstention accuracy in zero-shot and few-shot (2, 4, 6, 8) settings. Confirm that ICL does not improve refusal behavior.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can training-based interventions (e.g., fine-tuning) successfully teach LLMs to abstain from answering when prompt engineering fails?
- Basis in paper: [explicit] The authors conclude that because few-shot prompting was ineffective, "true robustness in RAG requires future research into training objectives that explicitly penalize unsupported generation and reward correct abstention."
- Why unresolved: The study demonstrated that standard in-context learning (few-shot) could not correct the "bias to answer," but the authors did not test whether modifying the training phase could resolve this limitation.
- What evidence would resolve it: Experiments showing that models fine-tuned with specific abstention rewards or negative constraints significantly reduce the false answer rate on negative windows compared to standard instruction-tuned models.

### Open Question 2
- Question: Do the findings regarding hallucination on negative windows generalize across different LLM architectures?
- Basis in paper: [explicit] The Limitations section states, "We acknowledge that the landscape of Large Language Models is rapidly evolving... While we did not perform an exhaustive cross-model evaluation, the primary objective... is to highlight a fundamental behavioral issue common to LLMs."
- Why unresolved: The experimental results are derived exclusively from Gemini 1.5 Pro; it remains unconfirmed if other architectures (e.g., Llama, Claude, GPT-4) exhibit the same >50% hallucination rate on negative windows.
- What evidence would resolve it: Replicating the specific negative window evaluation protocol across a diverse set of state-of-the-art model families to see if the inability to admit ignorance is a universal artifact of instruction tuning.

### Open Question 3
- Question: Can retrieval systems be optimized to specifically mitigate the sequential risks identified in adaptive prompting?
- Basis in paper: [inferred] The paper highlights that placing relevant documents late in the sequence (backward sliding) causes significant performance drops because the model hallucinates on early negative windows.
- Why unresolved: While the authors identify the vulnerability to negative windows, they utilize a standard BM25 retriever; the paper leaves unexplored whether retrievers could be specifically trained or adjusted to minimize the "distance" (number of negative windows) to the first positive window.
- What evidence would resolve it: Developing and testing a retrieval objective that penalizes sequences requiring multiple negative-window rejections, measuring if this reduces cumulative error rates compared to standard relevance ranking.

## Limitations
- The study focuses exclusively on Gemini 1.5 Pro, limiting generalizability across model families
- BM25 retrieval may systematically rank relevant pages lower than learned retrievers, potentially inflating negative-window encounters
- The paper does not explore training-based interventions that could teach abstention, leaving open whether this is an architectural limitation or solvable training objective

## Confidence
- **High confidence:** The adaptive prompting strategy's computational efficiency gains (50%+ token reduction) are well-supported by the experimental design and control conditions. The mechanism linking window ordering to hallucination reduction (fewer negative windows encountered) is directly measurable and consistently observed.
- **Medium confidence:** The claim that few-shot prompting fails to improve abstention is robust within the tested framework but may be model-specific. The finding that retrieval quality directly impacts hallucination risk is plausible but under-validated across retrieval methods.
- **Low confidence:** The assertion that "training-based interventions may be necessary" extends beyond the paper's empirical scope. While the results suggest prompting alone is insufficient, the paper does not test any training modifications, making this a reasonable but untested hypothesis.

## Next Checks
1. **Cross-model replication:** Test the adaptive prompting strategy with both dense retrieval (e.g., Contriever) and multiple LLM families (GPT-4, Claude) to determine whether the "bias to answer" is model-agnostic or Gemini-specific.

2. **Natural insufficiency detection:** Construct a benchmark of questions where gold answers explicitly do not exist in Wikipedia (rather than artificially constructed negative windows) to measure abstention accuracy in realistic scenarios.

3. **Training intervention test:** Fine-tune a base model with a binary reward for correct abstention on negative windows and evaluate whether few-shot prompting then becomes effective, directly testing whether the bias is trainable versus architectural.