---
ver: rpa2
title: 'FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact
  Center Conversation Transcripts'
arxiv_id: '2508.00889'
source_url: https://arxiv.org/abs/2508.00889
tags:
- conversation
- claim
- customer
- factuality
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of evaluating the factuality\
  \ of AI-generated claims in contact center conversations, where ground-truth labels\
  \ often do not exist due to the interpretive and subjective nature of the analysis.\
  \ The authors introduce a 3D\u2014Decompose, Decouple, Detach\u2014paradigm to guide\
  \ human annotation and prompt LLM judges, grounding factuality judgments in linguistically-informed\
  \ criteria."
---

# FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts

## Quick Facts
- arXiv ID: 2508.00889
- Source URL: https://arxiv.org/abs/2508.00889
- Reference count: 40
- Primary result: 3D paradigm enables LLM judges to achieve 0.86 mean F1 score on factuality evaluation without fine-tuning

## Executive Summary
This paper addresses the challenge of evaluating the factuality of AI-generated claims about contact center conversations, where ground-truth labels often don't exist due to the interpretive nature of the analysis. The authors introduce a 3D—Decompose, Decouple, Detach—paradigm to guide human annotation and prompt LLM judges, grounding factuality judgments in linguistically-informed criteria. They construct FECT, a benchmark dataset of 410 labeled conversation-claim pairs, and demonstrate that aligning LLM judges with the 3D paradigm can achieve a mean F1 score of 0.86 without fine-tuning or extensive prompt optimization, outperforming standard approaches.

## Method Summary
The method introduces a 3D paradigm for factuality evaluation: first decomposing claims into minimal informational units, then decoupling words with concrete meanings from those representing subjective interpretations, and finally detaching to verify relations independently of entity meanings. Human annotators apply this framework to create ground truth labels, which are then encoded into prompts for LLM judges. The benchmark uses synthetic data to ensure privacy and reproducibility, with ambiguous cases excluded to achieve high inter-annotator agreement (0.82 Cohen's Kappa). Four prompt variants are tested across 17 different models, measuring F1, precision, and recall for detecting non-factual claims.

## Key Results
- Mean F1 score of 0.86 achieved with OpenAI's o1 model using 3D_WITH_TTC prompt
- Smaller models (GPT-4.1-nano) show degraded performance when test-time compute is added
- Claude-Sonnet and Gemini models perform better with BASIC prompts than 3D prompts
- Inter-annotator agreement of 0.82 Cohen's Kappa achieved by excluding ambiguous cases

## Why This Works (Mechanism)

### Mechanism 1: Claim Decomposition
Breaking down interpretive claims into granular informational units helps verify factuality by reducing cognitive load and isolating verifiable components. A complex claim is factual if its constituent parts are factual and their relations are verified. This works because decomposition isolates each piece of information for independent verification.

### Mechanism 2: Concrete vs. Interpretive Separation
Separating words with concrete meanings from those representing subjective interpretations allows applying appropriate verification strategies. Concrete words are verified through explicit mentions, while interpretive words require implicit evidence. This works because factual grounding differs for objective versus interpretive statements.

### Mechanism 3: Relational Verification
Verifying relations between entities independently of their specific meanings is critical for confirming factuality. After verifying individual word meanings, the structure of the claim is verified by checking the causal or relational link supported by the text. This works because the truth of a complex claim lies in the interrelation of its parts being supported by evidence.

## Foundational Learning

- **LLM-as-a-Judge**: Core technique used to automate factuality evaluation. Needed to understand strengths (scalability) and weaknesses (prompt sensitivity). Quick check: Why was the 3D paradigm implemented in the LLM-judge prompt rather than just asking for True/False?

- **Inter-Annotator Agreement (IAA) & Ambiguity**: Fundamental to understanding benchmark construction. Needed to grasp how high agreement was achieved by excluding ambiguous cases. Quick check: Why exclude conversation-claim pairs where humans couldn't reach consensus?

- **Hallucination in Generative AI**: Baseline concept for understanding the problem being solved. Needed to distinguish hallucinations from faithful generations. Quick check: How does this evaluation differ from fact-checking in benchmarks like HaluEval?

## Architecture Onboarding

- **Component map**: Contact center conversations → LLM-generated claims → 3D decomposition component → Concrete/interpretive classification → Relational verification module → LLM judge → Factuality label

- **Critical path**: Creation of the `3D_WITH_TTC` prompt is most critical, as it must accurately codify the human annotation guideline so the LLM-judge can replicate the evaluation process.

- **Design tradeoffs**:
  - Synthetic vs. Real Data: Synthetic data ensures privacy but may miss real-world nuances
  - Ambiguity Removal vs. Realism: Excluding ambiguous claims achieves cleaner evaluation but reduces realism
  - Prompt Complexity vs. Model Capacity: Complex 3D prompts require sufficient model reasoning capacity

- **Failure signatures**:
  - High disagreement on subjective claims heavily reliant on inferred sentiment
  - Performance degradation with smaller models using test-time compute prompts
  - Poor performance with 3D prompt indicates over-optimization for specific models

- **First 3 experiments**:
  1. Baseline: Run GPT-4o with `BASIC_NO_TTC` prompt to establish performance without structured guidance
  2. Ablation: Run same model with `3D_NO_TTC` prompt to isolate effect of structured decomposition
  3. Test-time compute: Run o1 or GPT-4o with both `3D_NO_TTC` and `3D_WITH_TTC` to quantify reasoning token gains

## Open Questions the Paper Calls Out

- **Can the 3D prompt structure be optimized for reasoning models other than OpenAI's o1?** The current 3D prompt was optimized specifically for o1, and other reasoning models didn't show the same performance gains. Evidence needed: improved F1 scores for other models after prompt iteration.

- **How can alignment be achieved between humans and LLM-judges on ambiguous tasks?** Ambiguous cases were excluded to establish ground truth, leaving a gap in evaluating subjective interpretations. Evidence needed: methodology handling ambiguity without discarding data.

- **Why does adding test-time compute degrade performance in smaller models?** Authors hypothesize capacity limits, but the exact mechanism remains unverified. Evidence needed: systematic study correlating model size with performance changes when test-time compute is applied.

## Limitations

- Synthetic data may not fully capture real contact center conversation complexity and ambiguity
- Exclusion of ambiguous claims to achieve high agreement limits generalizability to real-world scenarios
- 3D paradigm may be over-optimized for frontier reasoning models, creating deployment challenges in resource-constrained settings

## Confidence

- **High Confidence**: Core mechanism of decomposing claims into verifiable units and general LLM-as-a-Judge framework
- **Medium Confidence**: Separation of concrete vs. interpretive words and verification of relational structures
- **Medium Confidence**: Benchmark construction methodology, though synthetic data and excluded cases limit real-world applicability

## Next Checks

1. Apply FECT methodology to real contact center conversation-claim pairs to assess performance degradation compared to synthetic data

2. Systematically test 3D paradigm prompts across broader model families and sizes, including ablation studies to identify critical components

3. Create subset including previously excluded ambiguous cases to evaluate how different prompting strategies perform and identify specific failure patterns