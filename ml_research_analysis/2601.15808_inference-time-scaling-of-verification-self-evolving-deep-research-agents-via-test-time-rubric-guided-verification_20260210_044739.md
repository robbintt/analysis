---
ver: rpa2
title: 'Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents
  via Test-Time Rubric-Guided Verification'
arxiv_id: '2601.15808'
source_url: https://arxiv.org/abs/2601.15808
tags:
- agent
- arxiv
- verification
- zhang
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel inference-time scaling method for
  deep research agents via structured verification. The core idea is to improve agent
  performance by iteratively verifying outputs using a rubric-guided feedback loop,
  rather than relying solely on post-training.
---

# Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification

## Quick Facts
- **arXiv ID**: 2601.15808
- **Source URL**: https://arxiv.org/abs/2601.15808
- **Reference count**: 12
- **Primary result**: 12%-48% F1 gain in verification over baseline LLM judges; 8%-11% accuracy gain on GAIA/XBench benchmarks via iterative feedback

## Executive Summary
This paper introduces DeepVerifier, a novel inference-time scaling method for deep research agents that iteratively improves outputs through structured verification. The system decomposes complex verification tasks into simpler sub-questions targeting specific failure modes, outperforming baseline LLM judges by 12%-48% in F1 score. When integrated into the agent loop, this approach delivers 8%-11% accuracy gains on GAIA and XBench-DeepResearch benchmarks using closed-source models. The authors also release DeepVerifier-4K, a 4,646-example fine-tuning dataset to enhance reflection and self-critique in open-source models.

## Method Summary
The method implements a three-stage pipeline: (1) Decomposition agent summarizes the agent's trajectory, identifies potential errors via a 5-class/13-subclass taxonomy, and generates targeted yes-no sub-questions; (2) Verification agent answers these sub-questions using external tools/search; (3) Judge synthesizes findings into scores (1-4) with corrective instructions. For test-time scaling, the system iteratively feeds rubric-based scores and instructions back to the agent for retries, peaking around the 4th feedback round before plateauing or regressing. DeepVerifier-8B is trained on Qwen3-8B with DeepVerifier-4K and CK-Pro-8B data.

## Key Results
- DeepVerifier outperforms vanilla agent-as-judge by 12%-48% in F1 score
- Iterative feedback delivers 8%-11% accuracy gains on GAIA and XBench-DeepResearch benchmarks
- Peak performance typically occurs around the 4th feedback round before accuracy plateaus or declines
- DeepVerifier-4K dataset released: 4,646 examples for fine-tuning reflection capabilities in open-source models

## Why This Works (Mechanism)

### Mechanism 1: Verification Asymmetry via Decomposition
The system exploits the asymmetry between generation and verification by decomposing complex tasks into simpler, verifiable sub-questions rather than attempting to re-solve the entire problem. The Decomposition Module maps observed behaviors to specific taxonomy labels, generating targeted sub-questions (e.g., "Does source X state claim Y?") to probe weak points in the agent's logic. This approach leverages the computational and cognitive simplicity of checking claims versus generating solutions.

### Mechanism 2: Taxonomy-Guided Rubric Feedback
Structured feedback derived from an automatically constructed failure taxonomy provides denser learning signals than generic critique. The system anchors verification in the DRA Failure Taxonomy (5 classes, 13 sub-classes) to deliver actionable instructions (e.g., "Consult primary source X") that constrain the search space for corrections. This structured approach guides the agent more effectively than vague feedback like "try again."

### Mechanism 3: Inference-Time Scaling via Reflection
Iterative cycles of generation, verification, and correction act as test-time training without weight updates. The process shows strong "Incorrect to Correct" transitions in early rounds that decay, while "Correct to Incorrect" transitions persist, typically yielding peak performance around the 4th feedback round before plateauing or regressing.

## Foundational Learning

- **Concept: Verification Asymmetry**
  - **Why needed here:** Explains the theoretical basis for efficiency - validating claims is computationally easier than solving problems, enabling the Judge module to succeed where the Generator failed.
  - **Quick check question:** Can you explain why generating a proof is harder than checking it, and how DeepVerifier exploits this?

- **Concept: Outcome Reward Models (ORMs) vs. Process Reward Models (PRMs)**
  - **Why needed here:** DeepVerifier acts as a sophisticated ORM using trajectory summaries to produce final scores, distinct from step-by-step PRMs.
  - **Quick check question:** Does DeepVerifier score every step of the agent's trajectory, or just the final answer based on trajectory evidence?

- **Concept: Test-Time Compute Scaling**
  - **Why needed here:** Frames iterative verification as scaling compute at inference time rather than training time.
  - **Quick check question:** How does increasing the "number of feedback rounds" relate to increasing inference-time compute?

## Architecture Onboarding

- **Component map:** Generator (DRA) -> DeepVerifier: [Decomposition Agent] -> [Verification Agent] -> [Judge] -> Generator (retry)
- **Critical path:** Error Identification (Decomposition Agent maps behaviors to taxonomy) -> Instruction Fidelity (Generator interprets Judge's corrective feedback)
- **Design tradeoffs:** Precision vs. Recall (catching more errors may incorrectly reject correct answers); Cost vs. Accuracy (each verification + retry round adds significant latency and token cost)
- **Failure signatures:** Early Loop Oscillation (agent flips between wrong answers - conflicting instructions); Score Stagnation (Judge consistently returns "2" - taxonomy mapping failed); Regressing Performance (accuracy drops after round 4 - accumulation of "Correct to Incorrect" transitions)
- **First 3 experiments:**
  1. Ablation on Decomposition: Run verifier without Decomposition module to isolate performance gain from verification asymmetry (expect ~12-48% F1 drop)
  2. Scaling Ceiling Analysis: Plot accuracy vs. feedback rounds (1-10) to determine optimal stopping criteria (suggests ~4 rounds)
  3. Taxonomy Validation: Measure coverage of 5-category taxonomy on new error dataset to see how often "No potential errors found" is returned on known-bad trajectories

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the iterative feedback loop be stabilized to prevent the "correct-to-incorrect" performance regression observed after the 4th feedback round?
- **Basis in paper:** Section 7.2 identifies that accuracy peaks early because "correct-to-incorrect" transitions persist while "incorrect-to-correct" rates decay, causing the scaling curve to peak and drop.
- **Why unresolved:** The paper identifies the cause but offers no mechanism to arrest degradation or determine optimal adaptive stopping point.
- **What evidence would resolve it:** An adaptive stopping criterion or verification confidence threshold that maintains peak performance indefinitely or delays regression significantly.

### Open Question 2
- **Question:** Is the efficacy of inference-time verification scaling dependent on the specific reasoning capabilities of the underlying backbone model?
- **Basis in paper:** Table 3 shows DeepVerifier yields ~8% gain for Claude-3.7-Sonnet but only ~2.5% gain for GPT-4.1, suggesting unequal scaling benefits.
- **Why unresolved:** Authors report performance differences but don't analyze specific model traits that limit scaling on GPT-4.1.
- **What evidence would resolve it:** An ablation study correlating backbone's independent reasoning accuracy with verification gain to identify the "floor" of model capability required.

### Open Question 3
- **Question:** Does the failure taxonomy derived from WebAggregatorQA generalize effectively to agents operating in non-research domains?
- **Basis in paper:** The taxonomy was constructed exclusively from WebAggregatorQA trajectories, limiting theoretical scope to "Deep Research" tasks.
- **Why unresolved:** While verification works on GAIA/BrowseComp, these are similar to training domain; taxonomy's exhaustiveness for distinct action spaces is unknown.
- **What evidence would resolve it:** Successful application of existing taxonomy labels to categorize failures in software engineering or GUI-navigation benchmarks without taxonomy expansion.

## Limitations
- Performance gains may not generalize to non-research domains beyond GAIA and XBench benchmarks
- "Correct to Incorrect" transition rates suggest potential brittleness when judge hallucinates errors or provides overly aggressive corrections
- Computational overhead (8.2M tokens per trajectory) may limit practical deployment for real-time applications

## Confidence
- **High confidence:** 12%-48% F1 improvement over baseline judges is well-supported by meta-evaluation results and verification asymmetry theory
- **Medium confidence:** 8%-11% accuracy gains demonstrated but generalizability to other domains and robustness to novel failure modes require further validation
- **Medium confidence:** Peak performance around 4th feedback round is supported empirically, but exact stopping criteria may vary by task complexity

## Next Checks
1. **Taxonomy Coverage Validation:** Test failure taxonomy on held-out dataset from different domain (e.g., scientific literature review) to measure coverage and accuracy of error identification
2. **Scaling Limit Analysis:** Conduct comprehensive ablation study removing Decomposition module to quantify exact performance drop and validate claimed F1 improvement
3. **Cost-Benefit Analysis:** Measure trade-off between accuracy gains and computational overhead (token usage, latency) across different iteration limits to determine optimal deployment strategy