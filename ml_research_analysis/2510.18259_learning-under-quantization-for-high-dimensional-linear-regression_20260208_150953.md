---
ver: rpa2
title: Learning under Quantization for High-Dimensional Linear Regression
arxiv_id: '2510.18259'
source_url: https://arxiv.org/abs/2510.18259
tags:
- quantization
- lemma
- data
- assumption
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides the first systematic theoretical analysis
  of learning under quantization in high-dimensional linear regression. The authors
  develop a novel framework that establishes precise algorithm- and data-dependent
  excess risk bounds for quantized stochastic gradient descent, analyzing five types
  of quantization: data, labels, parameters, activations, and gradients.'
---

# Learning under Quantization for High-Dimensional Linear Regression

## Quick Facts
- **arXiv ID:** 2510.18259
- **Source URL:** https://arxiv.org/abs/2510.18259
- **Reference count:** 40
- **One-line primary result:** This paper provides the first systematic theoretical analysis of learning under quantization in high-dimensional linear regression.

## Executive Summary
This paper establishes the first comprehensive theoretical framework for analyzing quantized learning in high-dimensional linear regression. The authors develop algorithm- and data-dependent excess risk bounds for quantized stochastic gradient descent, examining five quantization types: data, labels, parameters, activations, and gradients. The work identifies fundamental differences between multiplicative (input-dependent step, FP-like) and additive (constant step, INT-like) quantization, showing multiplicative quantization preserves the data spectrum while additive quantization introduces beneficial batch-size scaling effects.

## Method Summary
The paper analyzes high-dimensional linear regression with constant-stepsize SGD and iterate averaging, incorporating independent unbiased quantization operators at various stages (data $Q_d$, labels $Q_l$, parameters $Q_p$, activations $Q_a$, gradients $Q_o$). Theoretical analysis uses synthetic Gaussian data with polynomial-decay eigenvalues ($\lambda_i \sim i^{-2}$), ground truth weight vector with all entries set to 1, and observation noise variance $\sigma^2 = 1$. Excess risk is computed analytically using the population covariance matrix, comparing multiplicative vs additive quantization schemes.

## Key Results
- Multiplicative quantization eliminates spectral distortion by preserving the original data spectrum, while additive quantization introduces eigenvalue flattening
- Additive quantization exhibits beneficial scaling with batch size ($1/B$ noise reduction), whereas multiplicative quantization is relatively batch-independent
- Additive quantization's excess risk scales linearly with dimension $d$ in polynomial-decay spectra, while multiplicative quantization remains dimension-independent
- The theoretical framework provides quantitative comparisons between quantization types and practical guidance on when FP vs INT quantization performs better

## Why This Works (Mechanism)

### Mechanism 1: Spectral Preservation via Multiplicative Quantization
Multiplicative quantization preserves the learning dynamics of the original data spectrum by making the conditional second moment of quantization error proportional to the outer product of raw data ($\mathbb{E}[\epsilon \epsilon^\top | x] \approx \epsilon xx^\top$). This allows the "quantized Hessian" to effectively subsume quantization error into the original Hessian, eliminating spectral gaps that degrade learning.

### Mechanism 2: Noise Suppression via Batch Scaling in Additive Quantization
Additive quantization introduces uniform noise that, when averaged over batch size $B$, reduces the effective noise variance contribution from the 4th moment to the 2nd moment, introducing a $1/B$ scaling factor. This beneficial scaling emerges because the constant conditional second moment of additive errors averages out over larger batches.

### Mechanism 3: Dimensional Coupling in Additive Schemes
Additive quantization applies uniform error strength across all dimensions, causing the approximation error and effective dimension term to scale with $d$ in high-dimensional polynomial-decay spectra. Multiplicative quantization scales with data magnitude, maintaining independence from raw dimension count.

## Foundational Learning

- **Concept:** Excess Risk Decomposition
  - **Why needed here:** The paper breaks down total error into Variance Error, Bias Error, Approximation Error, and Quantized Error to identify where quantization hurts.
  - **Quick check question:** Can you distinguish between the error caused by the optimizer being stochastic (Variance) vs. the error caused by the model converging to a "quantized" optimum rather than the true optimum (Approximation)?

- **Concept:** Hessian Spectrum (Eigendecomposition)
  - **Why needed here:** The core contribution relies on how quantization shifts eigenvalues of the data covariance matrix ($H$ vs $H^{(q)}$).
  - **Quick check question:** If a quantizer adds uniform noise to a dataset, does it affect the large eigenvalues (strong signals) or small eigenvalues (noise/weak signals) more significantly in terms of relative error?

- **Concept:** Unbiased Stochastic Quantization
  - **Why needed here:** The analysis depends on $\mathbb{E}[Q(x)|x] = x$ to treat quantization noise as variance rather than gradient shift.
  - **Quick check question:** If a quantizer systematically rounds down (biased), which term in the excess risk decomposition (Bias or Variance) would likely explode?

## Architecture Onboarding

- **Component map:** Standard SGD: Data $X \to$ Weights $w \to$ Activation $a \to$ Output $y \to$ Gradient $g$ with Quantized Wrappers: $Q_d$ (Data), $Q_p$ (Weights), $Q_a$ (Activations), $Q_l$ (Labels), $Q_o$ (Gradients) at respective interfaces. Linear Model: $y = \langle w, x \rangle$.

- **Critical path:** The interaction between Data Quantization ($Q_d$) and the Weight Update. Data quantization modifies the Hessian ($H^{(q)}$), which fundamentally alters the optimization landscape and the "effective dimension" $k^*$ that determines convergence speed.

- **Design tradeoffs:**
  - *FP (Multiplicative):* Better for High Dimensions ($d$) and small batches. Preserves spectrum. High theoretical stability.
  - *INT (Additive):* Better for Large Batches (noise scales down as $1/B$). Bad for very high dimensions ($d$-scaling risk). Lower hardware cost.

- **Failure signatures:**
  - High $d$, Low $B$ with INT: Excess risk scales linearly with dimension. Model fails to generalize; training loss may converge but test loss diverges.
  - Spectral Mismatch: If $H$ decays rapidly (polynomial) and you use additive quantization, the "effective dimension" term $d_{eff}^{(A)}$ can blow up, stalling learning on minor components.

- **First 3 experiments:**
  1. Vary Batch Size ($B$) with Additive Quantization: Verify that excess risk drops as $1/B$. Compare this to Multiplicative, where the risk should be relatively constant regardless of batch size.
  2. Vary Dimension ($d$) with Fixed Sample Size ($N$): Plot excess risk for INT (Additive) vs. FP (Multiplicative). Expect INT risk to grow linearly with $d$, while FP risk stays flat or follows the original model's trend.
  3. Spectral Distortion Check: Generate data with a known polynomial decay spectrum. Apply Additive vs. Multiplicative data quantization. Measure the eigenvalues of the computed Hessian vs. the theoretical Hessian. Confirm Additive shifts/flattens the spectrum while Multiplicative preserves the shape.

## Open Questions the Paper Calls Out

### Open Question 1
Can information-theoretic lower bounds for excess risk under quantization be established to verify the tightness of the proposed upper bounds? The authors establish excess risk upper bounds without corresponding lower-bound analysis, leaving uncertainty about whether these bounds are optimal or if fundamental performance gaps exist between quantized and full-precision learning.

### Open Question 2
How does quantization affect the excess risk dynamics of multi-pass SGD or optimization algorithms with momentum in high-dimensional regimes? The current analysis assumes one-pass SGD with fresh data each iteration, but data reuse and momentum introduce temporal correlations requiring different error propagation analysis.

### Open Question 3
How robust are the excess risk bounds when the assumption of unbiased quantization is violated? The analysis relies on unbiased quantization operators, but practical quantization schemes often introduce systematic bias, making it unclear if spectral distortion and noise amplification mechanisms hold when quantization is biased.

### Open Question 4
Do the identified mechanisms generalize to non-linear deep neural networks? The paper uses linear regression as a testbed, but in deep learning the effective Hessian changes dynamically during training, potentially altering the interaction between quantization noise and the optimization landscape.

## Limitations

- The theoretical analysis heavily relies on specific data assumptions (polynomial eigenvalue decay, Gaussian features) that may not hold in practical scenarios
- The analysis focuses on linear models, leaving uncertainty about generalization to deep networks where quantization effects compound non-linearly
- The distinction between multiplicative and additive quantization may blur in real-world quantizers that use mixed schemes rather than pure input-dependent or constant-step approaches

## Confidence

- **High Confidence:** The spectral preservation mechanism for multiplicative quantization and the dimensional scaling penalty for additive quantization are mathematically rigorous within stated assumptions
- **Medium Confidence:** The batch-size scaling benefits for additive quantization rely on specific variance calculations that may not capture all practical quantization noise characteristics
- **Low Confidence:** The exact magnitude of practical performance differences between quantization types in realistic settings remains uncertain without extensive empirical validation

## Next Checks

1. **Spectral Distortion Experiment:** Generate data with known polynomial decay spectrum. Apply Additive vs. Multiplicative data quantization. Measure the eigenvalues of the computed Hessian vs. the theoretical Hessian to confirm Additive shifts/flattens the spectrum while Multiplicative preserves the shape.

2. **Batch Scaling Verification:** Run SGD with Additive quantization across varying batch sizes. Verify that excess risk decreases proportionally to 1/B, contrasting with Multiplicative quantization which should show minimal batch dependence.

3. **Dimensional Scaling Test:** Train models with increasing dimension d while holding sample size N fixed. Compare Excess Risk growth rates for INT (Additive) vs. FP (Multiplicative) quantization to confirm the theoretical d-scaling predictions.