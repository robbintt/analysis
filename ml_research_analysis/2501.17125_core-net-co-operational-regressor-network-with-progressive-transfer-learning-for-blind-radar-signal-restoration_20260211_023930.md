---
ver: rpa2
title: 'CoRe-Net: Co-Operational Regressor Network with Progressive Transfer Learning
  for Blind Radar Signal Restoration'
arxiv_id: '2501.17125'
source_url: https://arxiv.org/abs/2501.17125
tags:
- restoration
- signal
- radar
- core-net
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoRe-Net, a novel blind radar signal restoration
  framework that replaces adversarial training with cooperative learning between an
  Apprentice Regressor (AR) and Master Regressor (MR). The AR restores corrupted signals
  while the MR provides task-specific quality feedback, ensuring stable and efficient
  learning.
---

# CoRe-Net: Co-Operational Regressor Network with Progressive Transfer Learning for Blind Radar Signal Restoration

## Quick Facts
- arXiv ID: 2501.17125
- Source URL: https://arxiv.org/abs/2501.17125
- Authors: Muhammad Uzair Zahid; Serkan Kiranyaz; Alper Yildirim; Moncef Gabbouj
- Reference count: 30
- One-line primary result: Achieves state-of-the-art blind radar signal restoration with 1 dB mean SNR improvement over OpGAN, plus 2 dB from multi-pass PTL

## Executive Summary
CoRe-Net introduces a novel blind radar signal restoration framework that replaces adversarial training with cooperative learning between an Apprentice Regressor (AR) and Master Regressor (MR). The AR restores corrupted signals while the MR provides continuous quality feedback through PSNR score regression, ensuring stable and efficient learning. To further boost performance, the authors propose Progressive Transfer Learning (PTL) for multi-pass restoration, where each pass refines the output of the previous one. Tested on the BRSR dataset, CoRe-Net achieves state-of-the-art results, improving mean SNR by 1 dB over the previous best OpGAN method and an additional 2 dB through PTL-enabled multi-pass restoration.

## Method Summary
CoRe-Net replaces traditional adversarial training with cooperative regression between an AR (encoder-decoder) and MR (quality regressor). The AR restores corrupted signals, while the MR predicts a continuous PSNR score rather than binary real/fake labels. Both networks use 1D Self-ONN layers for enhanced non-linearity. The system employs dual-domain optimization (time and frequency) to preserve both signal structure and spectral content. Progressive Transfer Learning enables multi-pass restoration where each pass builds upon the previous one's learned weights, achieving cumulative performance gains.

## Key Results
- Achieves 11.37 dB mean SNR single-pass, surpassing BRSR-OpGAN's 10.30 dB by 1.07 dB
- Multi-pass PTL implementation achieves ~14 dB mean SNR (additional 2-3 dB improvement)
- Compact architecture enables real-time deployment on resource-constrained hardware
- Stable training without mode collapse issues common in GAN-based approaches

## Why This Works (Mechanism)

### Mechanism 1
Replacing adversarial discrimination with cooperative regression feedback improves training stability and signal fidelity. The AR generates restored signals, while the MR predicts continuous PSNR scores instead of binary real/fake labels. This cooperative approach avoids mode collapse and provides more informative gradient feedback. Break condition: MR failure to converge leads to noisy AR feedback.

### Mechanism 2
Progressive Transfer Learning enables cumulative performance gains through iterative refinement. Each pass's output becomes the next pass's input, with weights initialized from the previous pass's trained parameters. This allows the model to continue learning to remove residual artifacts rather than re-learning from scratch. Break condition: First-pass artifacts can be amplified in subsequent passes.

### Mechanism 3
Dual-domain optimization (Time + Frequency) better preserves both signal structure and spectral content than single-domain loss. The combined loss forces the AR to maintain temporal pulse shape while cleaning spectral signatures critical for radar classification. Break condition: Coarse time-frequency transforms may optimize irrelevant spectral details.

## Foundational Learning

- **Concept**: Encoder-Decoder Architectures (U-Net style)
  - Why needed: AR uses this to map corrupted signals to clean ones; skip connections preserve fine details
  - Quick check: Can you explain why skip connections help preserve radar pulse edge sharpness?

- **Concept**: Regression vs. Discrimination
  - Why needed: Core mechanism replaces GAN Discriminator with MR that outputs PSNR values
  - Quick check: How does MR outputting 0.8 PSNR differ mathematically from Discriminator outputting "Real" with 0.8 probability?

- **Concept**: Signal-to-Noise Ratio (SNR) & PSNR
  - Why needed: Primary evaluation metric and MR prediction target
  - Quick check: Why is 3 dB improvement significant (roughly doubling signal power relative to noise)?

## Architecture Onboarding

- **Component map**: Corrupted signal -> AR (5-layer Encoder-Decoder) -> Restored signal -> MR (6-layer Regressor) -> Quality score -> Loss calculation -> Backpropagation

- **Critical path**: 1) Feed corrupted signal r into AR to get restored signal s_hat; 2) Feed (r, s_hat) into MR to get quality score y_pred; 3) Compute fidelity loss (MSE(y_pred, 1)) and backpropagate to update AR; 4) Alternately update MR to better predict ground truth PSNR scores

- **Design tradeoffs**: Stability vs. Quality (cooperative vs. adversarial); Latency vs. Accuracy (single-pass vs. multi-pass PTL); 1D Self-ONN vs. standard CNNs for deployment

- **Failure signatures**: Spectral Smearing (low frequency loss weight); MR Collapse (MR memorizes training data); Echo Accumulation (PTL amplifies artifacts)

- **First 3 experiments**: 1) Baseline: Train single-pass with only Time-Domain loss; 2) MR Ablation: Freeze MR and train AR independently; 3) PTL Depth Probe: Run 1-4 passes on test set, plot SNR improvement vs. number of passes

## Open Questions the Paper Calls Out

- Can hybrid learning paradigms combining cooperative and competitive learning outperform pure cooperative learning?
- Does integrating classification objectives into CoRe-Net improve restoration performance?
- Can adaptive PTL strategies optimize the trade-off between restoration quality and computational cost?

## Limitations

- Implementation details underspecified (ResDownBlock/ResUpBlock structure, spectrogram computation)
- Multi-pass PTL assumes first pass produces clean enough signals for refinement
- Computational cost scales linearly with number of PTL passes
- No analysis of error amplification in subsequent PTL passes

## Confidence

- **High Confidence**: Cooperative learning mechanism replacing adversarial training; dual-domain optimization
- **Medium Confidence**: Progressive Transfer Learning mechanism and cumulative improvement claims
- **Low Confidence**: Specific implementation details for exact reproduction

## Next Checks

1. **MR Validation Test**: Implement MR independently and validate its ability to predict normalized PSNR scores on held-out dataset; monitor correlation between predicted and true values across full SNR range

2. **Multi-Pass Error Analysis**: For fixed test signal, run 1-4 PTL passes and plot both SNR improvement and artifact evolution (spectrogram differences) to identify point where error amplification begins

3. **Architecture Sensitivity Study**: Systematically vary Self-ONN Taylor order (Q) and dropout rates to quantify impact on stability and performance, particularly comparing against standard CNN baselines