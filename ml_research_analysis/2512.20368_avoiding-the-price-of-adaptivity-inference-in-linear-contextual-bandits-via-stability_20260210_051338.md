---
ver: rpa2
title: 'Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via
  Stability'
arxiv_id: '2512.20368'
source_url: https://arxiv.org/abs/2512.20368
tags:
- lemma
- confidence
- page
- have
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of statistical inference in
  linear contextual bandits, where the adaptive, non-i.i.d. nature of data complicates
  the construction of valid confidence intervals.
---

# Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability

## Quick Facts
- arXiv ID: 2512.20368
- Source URL: https://arxiv.org/abs/2512.20368
- Reference count: 40
- Primary result: Stable EXP4 algorithm achieves minimax-optimal regret while enabling valid Wald-type inference without the $\sqrt{d \log T}$ price of adaptivity.

## Executive Summary
This paper addresses the fundamental challenge of statistical inference in linear contextual bandits, where adaptive data collection breaks standard statistical assumptions. The authors introduce a regularized EXP4 algorithm that enforces stability—a structural property ensuring the empirical feature covariance concentrates around a deterministic limit. Under this stability condition, the ordinary least-squares estimator satisfies a central limit theorem, enabling asymptotically valid confidence intervals without the typical inflation required by adaptive sampling methods. The algorithm achieves regret bounds that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist.

## Method Summary
The method introduces a regularized variant of the EXP4 algorithm for linear contextual bandits with $K$ experts and $|A|$ actions. The algorithm maintains a distribution over experts and updates it using entropy mirror descent with an added regularization term. For each round, the algorithm forms a mixture policy $Q_t$ from the expert weights, samples an action, observes the loss, and constructs importance-weighted gradient estimates for all experts. The key innovation is the regularization term $\lambda R(w)$ that enforces weight stability while preserving exploration through the $\epsilon$-simplex constraint. At the horizon, the algorithm computes ordinary least-squares or ridge regression estimators for statistical inference.

## Key Results
- Wald-type confidence intervals achieve target coverage under stability without $\sqrt{d \log T}$ inflation
- Regret bounds of $O(\sqrt{T \cdot |A| \cdot \log K})$ are minimax optimal up to logarithmic factors
- Stability condition ensures OLS estimator satisfies central limit theorem with explicit convergence rates
- Valid inference for conditional average treatment effects under adaptively collected data

## Why This Works (Mechanism)

### Mechanism 1: Stability Enables Classical Inference
When the algorithm satisfies the Lai-Wei stability condition, the empirical feature covariance $S_T$ concentrates around a deterministic limit $\Sigma_T^\star$. This concentration ensures the OLS estimator satisfies a central limit theorem with the same asymptotic variance as in non-adaptive settings, restoring standard Wald-type confidence intervals. The stability condition requires the empirical average of weight vectors to converge to a fixed vector, ensuring predictable mixture policy patterns. The convergence rate is explicitly quantified as $\Psi(\gamma_T)^{1/3}$ where $\gamma_T$ controls the stability parameter.

### Mechanism 2: Regularization Enforces Stability
The entropy regularization term $\lambda R(w)$ and $\epsilon$-floor constraint enforce weight stability while preserving exploration. The penalty introduces strong convexity to the otherwise linear loss, ensuring weight updates remain bounded and converge to a stable limit. The $\epsilon$-simplex constraint guarantees importance-weighted gradient estimates remain well-defined. The regularization parameter $\lambda = \gamma_T / \sqrt{T}$ with $\gamma_T \to \infty$ and $T / \log^2 T \gg \gamma_T$ provides sufficient stabilization without over-regularizing exploration.

### Mechanism 3: Optimal Regret-Stability Tradeoff
The algorithm achieves minimax-optimal regret (up to logarithmic factors) while simultaneously satisfying stability. The regret bound emerges from balancing mirror descent optimization error, variance of importance-weighted gradient estimates, and regularization penalty. The choice $\gamma_T = \sqrt{\log T}$ optimizes this trade-off, making the stability-related penalty decay while maintaining logarithmic regret overhead. The gradient estimate variance is bounded by $|A|$ due to the importance-weighting construction.

## Foundational Learning

- **Concept: Lai-Wei Stability Condition**
  - Why needed: Central structural property enabling valid inference under adaptation
  - Quick check: Given a sequence of design matrices $\{X_t\}$, what condition on $S_T = \sum_t X_t X_t^\top$ ensures that $\hat{\beta}_{OLS}$ is asymptotically normal?

- **Concept: Importance-Weighted Gradient Estimation**
  - Why needed: Core data efficiency mechanism for unbiased loss estimation
  - Quick check: If you only observe loss $\ell_t(a_t)$ for action $a_t$ sampled from $Q_t$, how do you construct an unbiased estimate of $\mathbb{E}_{a \sim \pi_k}[\ell(a)]$?

- **Concept: Entropic Mirror Descent**
  - Why needed: Natural mechanism for maintaining distributions over experts
  - Quick check: Why is the negative entropy $\phi(w) = \sum_k w_k \log w_k - w_k$ a suitable mirror map for maintaining distributions over experts?

## Architecture Onboarding

- **Component map:** Expert pool $\{\pi_k\}_{k=1}^K$ -> Mixture weights $w_t \in \Delta_\epsilon$ -> Mixture policy $Q_t$ -> Action sampling $a_t$ -> Loss observation $\ell_t$ -> Gradient estimation $\hat{g}_{t,k}$ -> Weight update via regularized mirror descent -> OLS/Ridge estimation at horizon

- **Critical path:** Initialize weights uniformly: $w_1 = (1/K, \ldots, 1/K)$ -> For each round: observe context $x_t$, form mixture $Q_t$, sample action $a_t$, observe loss $\ell_t$ -> Compute gradient estimates $\hat{g}_{t,k}$ for all experts -> Update weights via regularized mirror descent -> At $T$: compute $\hat{\beta}_{OLS}$ and construct Wald intervals using $S_T^{-1}$

- **Design tradeoffs:** $\epsilon$ (floor parameter): Smaller $\epsilon$ allows sharper expert differentiation but risks numerical instability. $\gamma_T$ (stability parameter): Larger $\gamma_T$ speeds stability convergence but increases regret overhead. Ridge penalty $\lambda_{rid}$: For CATE inference, smaller is better asymptotically but requires $\lambda_{rid} \ll \sqrt{T}$.

- **Failure signatures:** Coverage degradation indicates $\lambda_{min}(S_T)$ is near-singular or weight convergence failed. Regret explosion suggests $\gamma_T$ is too large. Numerical instability in $\hat{g}_{t,k}$ indicates $\epsilon$ is too small or expert policies are degenerate.

- **First 3 experiments:** 1) Validate weight convergence by plotting $\|\bar{w}_T - w_T^\star\|_1$ vs $T$ to verify $\Psi(\gamma_T)$ decay rate. 2) Compare Wald intervals against APS intervals across confidence levels to verify coverage with narrower intervals. 3) Stress test eigenvalue condition by generating contexts with near-collinear features to observe CLT convergence degradation.

## Open Questions the Paper Calls Out

### Open Question 1
Can the stability guarantees and valid Wald-type inference be extended to settings where the feature dimension $d$ grows with the time horizon $T$? The current analysis relies on fixed dimension assumptions and convergence rates depend on the relationship between $T$ and $d$.

### Open Question 2
Can the Lai–Wei stability condition be satisfied in a fully adaptive context setting where context vectors $x_t$ depend on the history of the bandit process? The current proof relies on i.i.d. contexts ensuring concentration of empirical covariance.

### Open Question 3
Is it possible to achieve exact minimax optimal regret while satisfying the Lai-Wei stability condition, or is the logarithmic penalty inherent to the stability requirement? Theorem 2 establishes regret guarantees that are minimax optimal only "up to logarithmic factors."

## Limitations

- The stability condition (Lai-Wei) may not hold for all expert policies or adaptive strategies
- Analysis assumes bounded gradient variance and positive minimum eigenvalue $\lambda^\star$ for expert covariance matrices
- The parameter regime $\gamma_T = \sqrt{\log T}$ with $T / \log^2 T \gg \gamma_T$ is narrow and requires careful tuning
- For CATE inference, the ridge penalty $\lambda_{rid} \ll \sqrt{T}$ is required but not explicitly quantified

## Confidence

- **High confidence:** Wald confidence intervals achieve target coverage under stability (supported by simulations showing empirical 95% coverage close to nominal level)
- **Medium confidence:** Regret bounds are minimax-optimal up to logarithmic factors (Theorem 2 provides explicit bound but requires verification across varying parameters)
- **Medium confidence:** Stability mechanism via entropy regularization is sound (analytical derivation shows weight convergence but empirical validation is limited)

## Next Checks

1. **Robustness to Stability Violations:** Systematically vary expert policy quality and measure breakdown of weight convergence and inference coverage.
2. **Ridge Parameter Sensitivity:** Quantify how $\lambda_{rid}$ affects CATE coverage in finite samples and determine explicit scaling requirements.
3. **Comparison with Adaptive Methods:** Test against standard adaptive algorithms (Thompson Sampling, UCB) on identical problems to quantify the "price of adaptivity" empirically.