---
ver: rpa2
title: 'VERITAS: Verification and Explanation of Realness in Images for Transparency
  in AI Systems'
arxiv_id: '2507.05146'
source_url: https://arxiv.org/abs/2507.05146
tags:
- images
- image
- adversarial
- artifacts
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VERITAS, a framework that detects AI-generated
  images and explains why they are classified as synthetic through artifact localization
  and semantic reasoning. The framework addresses the challenge of distinguishing
  real from AI-generated small (32x32) images, which is critical for combating misinformation
  and identity fraud in digital platforms.
---

# VERITAS: Verification and Explanation of Realness in Images for Transparency in AI Systems

## Quick Facts
- **arXiv ID:** 2507.05146
- **Source URL:** https://arxiv.org/abs/2507.05146
- **Reference count:** 35
- **Primary result:** VERITAS detects AI-generated 32x32 images and explains why through artifact localization and semantic reasoning, outperforming baseline VLMs in interpretability.

## Executive Summary
This paper presents VERITAS, a framework that detects AI-generated images and explains why they are classified as synthetic through artifact localization and semantic reasoning. The framework addresses the challenge of distinguishing real from AI-generated small (32x32) images, which is critical for combating misinformation and identity fraud in digital platforms. VERITAS employs a five-stage pipeline: super-resolution using DRCT, GradCAM-based heatmap generation, patch-level analysis weighted by attention, CLIP-based artifact classification, and textual explanation generation via MOLMO. The framework successfully identifies and describes various artifacts in synthetic images, such as misaligned body panels, anatomically incorrect structures, and frequency domain signatures. Experimental results demonstrate that VERITAS provides more detailed and interpretable explanations of artifacts compared to baseline models like MOLMO, Qwen2.5 VL, and Pixtral 12B, particularly for low-resolution images. The approach offers clear, human-readable insights into the synthetic nature of images, enhancing transparency and trust in AI systems.

## Method Summary
VERITAS is a five-stage pipeline for detecting AI-generated images and explaining their synthetic nature. It takes 32x32 input images and first super-resolves them using DRCT to preserve high-frequency artifact signatures. A binary classifier ensemble (ViT-Tiny, EfficientNet, ResNet18) generates GradCAM heatmaps to identify classification-critical regions. The super-resolved image is divided into patches weighted by interpolated attention, and CLIP similarity scores against positive/negative/neutral artifact descriptors produce a weighted artifact score. Images exceeding this threshold are processed by MOLMO-7B-D to generate human-readable explanations of detected artifacts. The framework is trained and evaluated on the CIFAKE dataset (120K balanced real/synthetic 32x32 images).

## Key Results
- VERITAS successfully detects and explains synthetic artifacts in 32x32 images, outperforming baseline VLMs in interpretability
- The framework identifies specific artifact types including misaligned body panels, anatomically incorrect structures, and frequency domain signatures
- Super-resolution with DRCT preserves artifact signatures that would be invisible at native 32x32 resolution
- CLIP-based patch voting effectively aggregates localized evidence for artifact detection
- MOLMO generates clear, human-readable explanations that enhance transparency in AI systems

## Why This Works (Mechanism)

### Mechanism 1: Super-Resolution Preserves Artifact Signatures
- Claim: Upscaling 32×32 images with DRCT retains high-frequency features where generative artifacts reside, enabling detection that would be impossible on low-resolution inputs.
- Mechanism: DRCT's Swin Transformer architecture captures long-range dependencies and reconstructs spatial information while maintaining edge/texture details that interpolation methods would smooth away.
- Core assumption: Generative models leave detectable traces in high-frequency components that survive or become more visible after learned super-resolution.
- Evidence anchors:
  - [Section 4.1]: "DRCT...utilizes the ability of Swin Transformers to capture long-range dependencies and reconstruct spatial information for preventing information bottlenecks."
  - [Section 3]: "frequency-domain features...which are not discernible by observing RGB images"
- Break condition: If the super-resolution model hallucinates features rather than reconstructing them, it may introduce artifacts indistinguishable from generative traces, causing false positives.

### Mechanism 2: GradCAM Heatmaps Localize Classification-Critical Regions
- Claim: Gradient-weighted class activation maps identify spatial regions most influential in the "synthetic" classification, enabling targeted patch analysis.
- Mechanism: GradCAM computes gradients of the target class score with respect to final convolutional feature maps, weighting activations to produce a spatial heatmap. Regions with high attention correlate with artifact locations.
- Core assumption: The classifier's decision boundary relies on localizable features rather than distributed patterns, and GradCAM faithfully reflects this.
- Evidence anchors:
  - [Section 4, Step 2]: "GradCAM helps us visualize which areas the model focuses on when classifying an image as Real or Fake."
  - [Section 2]: "most regions of a real image are relevant for classification, only a few regions and artifacts of a synthetic image determine its class."
- Break condition: If the classifier is adversarially robust or relies on non-spatial features (e.g., frequency statistics), GradCAM may produce diffuse or misleading heatmaps.

### Mechanism 3: CLIP-Based Patch Voting Aggregates Semantic Evidence
- Claim: Comparing image patches against positive/negative/neutral text descriptors via CLIP similarity produces a weighted artifact score that filters images before explanation.
- Mechanism: Each patch receives CLIP similarity scores against three descriptor types. Votes are aggregated using GradCAM-derived weights, yielding an overall artifact score S = Σ(w_k × v_k) / Σw_k.
- Core assumption: CLIP's embedding space aligns sufficiently with human-interpretable artifact concepts, and patch-level analysis captures localized anomalies.
- Evidence anchors:
  - [Section 4, Step 4]: "each patch casts a vote as positive, negative, or neutral. These votes are then aggregated using a weighted average based on patch importance."
  - [Section 3.6]: "CLIP is an efficient method of learning image-text relationships...allows pre-trained VLMs to capture rich vision-language relationships."
- Break condition: If artifact types are not well-represented in CLIP's training distribution (e.g., novel generative model signatures), similarity scores may be uninformative.

## Foundational Learning

- **Concept: Vision Transformers and Attention Mechanisms**
  - Why needed here: DRCT uses Swin Transformers for super-resolution; GradCAM relies on understanding feature map gradients; MOLMO uses cross-modal attention.
  - Quick check question: Can you explain how self-attention differs from cross-attention, and why shifted windows (Swin) help with computational efficiency?

- **Concept: CLIP Vision-Language Alignment**
  - Why needed here: The patch classification step depends on CLIP's ability to match image regions with semantic text descriptors.
  - Quick check question: Given an image patch and three text descriptions, how would CLIP compute which description best matches? What does the similarity score represent geometrically?

- **Concept: Frequency-Domain Analysis for Synthetic Detection**
  - Why needed here: GANs and diffusion models leave characteristic spectral signatures; the paper references FFT-based detection as complementary to spatial methods.
  - Quick check question: Why might a diffusion-generated image show different frequency artifacts than a GAN-generated image? What would you look for in a 2D FFT plot?

## Architecture Onboarding

- **Component map:**
  Input: 32×32 image → DRCT super-resolution → 224×224 (or similar) enhanced image
  Parallel path: Original 32×32 → Binary classifier → GradCAM heatmap
  Merge: Interpolate heatmap to super-resolved dimensions → Extract patches → Weight by heatmap intensity
  Classification: Each patch → CLIP similarity against artifact descriptors → Vote aggregation
  Explanation: High-scoring images → MOLMO → Textual artifact description

- **Critical path:**
  1. DRCT super-resolution (computationally heavy, but only once per image)
  2. GradCAM computation on original classifier (requires gradient access)
  3. CLIP batch inference over all patches (scales with patch count)
  4. MOLMO generation (only for images exceeding artifact threshold)

- **Design tradeoffs:**
  - Patch size: Smaller patches = finer localization but more CLIP calls and noisier votes
  - Heatmap interpolation: Bilinear is fast but may blur attention; bicubic is smoother but slower
  - Threshold selection: Lower threshold = more explanations but more false positives; higher = precision but missed detections
  - VLM choice: MOLMO-7B-D balances quality/speed; MOLMO-72B is more capable but resource-intensive

- **Failure signatures:**
  - Diffuse GradCAM heatmaps → Classifier may not rely on localized features; consider frequency-domain features
  - MOLMO hallucinates artifacts not present → Cross-check with CLIP votes; consider hallucination correction (Woodpecker, cited in Section 6.2)
  - Super-resolution introduces ring artifacts → May trigger false positives; validate against original low-res classification
  - CLIP votes all neutral → Descriptors may not match image domain; expand or refine artifact taxonomy

- **First 3 experiments:**
  1. **Baseline DRCT vs. interpolation comparison**: Run the full pipeline with bilinear/bicubic upscaling instead of DRCT. Measure artifact detection accuracy and explanation quality. Expect: DRCT should show higher true-positive rate for subtle artifacts.
  2. **GradCAM aggregation study**: Compare single-model GradCAM vs. ensemble GradCAM (average heatmaps from ResNet18, EfficientNet, ViT-Tiny). Measure localization precision against ground-truth artifact annotations. Expect: Ensemble may reduce model-specific bias but could dilute signal.
  3. **VLM ablation (MOLMO vs. Qwen2.5-VL vs. Pixtral)**: Run the same set of detected-artifact images through all three VLMs with identical prompts. Compare description specificity, localization accuracy, and hallucination rate using human evaluation. Expect: MOLMO should excel at spatial grounding; Qwen may use more technical terminology.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can certified robustness techniques, such as Auto-LiRPA, be integrated into the pipeline to preserve GradCAM localization and semantic scoring under adversarial perturbations?
- Basis in paper: [explicit] Section 6.4 states the system currently does not address adversarial perturbations and suggests using Auto-LiRPA to compute provable activation bounds.
- Why unresolved: Adversarial noise can currently distort attention maps and mislead the interpretability mechanisms without detection.
- What evidence would resolve it: Demonstration of stable GradCAM heatmaps and classification scores within defined epsilon-neighborhoods under attack.

### Open Question 2
- Question: Can hallucination correction methods, such as Woodpecker, be effectively integrated to filter factually unsupported descriptions from MOLMO outputs without suppressing valid artifact detection?
- Basis in paper: [explicit] Section 6.2 notes that VLMs like MOLMO can generate "fluent but factually unsupported descriptions" and suggests incorporating Woodpecker.
- Why unresolved: Language models may confidently describe artifacts that do not exist, undermining user trust in the explanation.
- What evidence would resolve it: Improved factual consistency scores in generated explanations compared to ground-truth artifact labels.

### Open Question 3
- Question: Do domain generalization strategies enable the detection of invariant features across diverse generative architectures (e.g., diffusion vs. autoencoders), preventing overfitting to specific model signatures?
- Basis in paper: [explicit] Section 6.5 highlights the risk of overfitting to artifacts from specific models like Stable Diffusion, reducing generalization to unseen generators.
- Why unresolved: Different generative architectures produce distinct visual "fingerprints," limiting the utility of classifiers trained on limited datasets like CIFAKE.
- What evidence would resolve it: High zero-shot detection accuracy on images generated by architectures excluded from the training data.

## Limitations

- The paper does not fully specify the artifact descriptor tuples and threshold values used for CLIP-based scoring, which are critical for reproducing the artifact detection mechanism.
- While the framework shows improved interpretability over baselines, the absolute performance gains on the binary classification task are not clearly quantified relative to state-of-the-art detection models.
- The exact implementation details for artifact descriptors, scoring thresholds, and patch configurations are insufficiently specified for direct reproduction.

## Confidence

- **High Confidence:** The five-stage pipeline architecture and its components (DRCT super-resolution, GradCAM heatmaps, CLIP patch voting, MOLMO explanations) are clearly described and logically coherent.
- **Medium Confidence:** The effectiveness of CLIP-based patch voting for artifact detection is supported by adjacent literature but not extensively validated within the paper itself.
- **Low Confidence:** The exact implementation details for artifact descriptors, scoring thresholds, and patch configurations are insufficiently specified for direct reproduction.

## Next Checks

1. **Baseline DRCT vs. interpolation comparison:** Run the full pipeline with bilinear/bicubic upscaling instead of DRCT. Measure artifact detection accuracy and explanation quality. Expect: DRCT should show higher true-positive rate for subtle artifacts.
2. **GradCAM aggregation study:** Compare single-model GradCAM vs. ensemble GradCAM (average heatmaps from ResNet18, EfficientNet, ViT-Tiny). Measure localization precision against ground-truth artifact annotations. Expect: Ensemble may reduce model-specific bias but could dilute signal.
3. **VLM ablation (MOLMO vs. Qwen2.5-VL vs. Pixtral):** Run the same set of detected-artifact images through all three VLMs with identical prompts. Compare description specificity, localization accuracy, and hallucination rate using human evaluation. Expect: MOLMO should excel at spatial grounding; Qwen may use more technical terminology.