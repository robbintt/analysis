---
ver: rpa2
title: 'ECSEL: Explainable Classification via Signomial Equation Learning'
arxiv_id: '2601.21789'
source_url: https://arxiv.org/abs/2601.21789
tags:
- ecsel
- signomial
- feature
- classification
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ECSEL is an explainable classification method that learns interpretable
  signomial equations to serve as both classifiers and transparent explanations. The
  approach exploits the prevalence of signomial structure in symbolic regression benchmarks
  by using gradient-based optimization with sparsity-inducing regularization.
---

# ECSEL: Explainable Classification via Signomial Equation Learning

## Quick Facts
- arXiv ID: 2601.21789
- Source URL: https://arxiv.org/abs/2601.21789
- Authors: Adia Lumadjeng; Ilker Birbil; Erman Acar
- Reference count: 40
- Primary result: ECSEL achieves competitive classification accuracy while maintaining interpretability through closed-form signomial equations

## Executive Summary
ECSEL is an explainable classification method that learns interpretable signomial equations to serve as both classifiers and transparent explanations. The approach exploits the prevalence of signomial structure in symbolic regression benchmarks by using gradient-based optimization with sparsity-inducing regularization. ECSEL achieves competitive classification accuracy with established machine learning models while maintaining inherent interpretability through closed-form expressions. On symbolic regression benchmarks, ECSEL recovers a larger fraction of target equations than state-of-the-art approaches while requiring substantially less computation time. The method satisfies desirable analytical properties enabling global feature behavior analysis, decision-boundary interpretation, and local feature attributions. Case studies on e-commerce purchase intent prediction and large-scale fraud detection demonstrate ECSEL's ability to expose dataset biases, support counterfactual reasoning, and yield actionable insights through interpretable mathematical formulas.

## Method Summary
ECSEL learns signomial functions of the form $z_c(x) = \sum_k \alpha_{c,k} \prod_j x_j^{\beta_{c,k,j}}$ for each class, where $\alpha$ are coefficients and $\beta$ are exponents. The model minimizes cross-entropy loss with $\ell_1$ regularization on exponents: $L(\alpha,\beta) = -\frac{1}{N}\sum \log p_{yi}(xi) + \lambda \sum|\beta_{c,k,j}|$. For K=1 terms, L-BFGS-B optimization is used; for K>1, a staged Adam approach with reduced regularization followed by L-BFGS polishing is employed. Features must be positive (scaled to [1,10] via MinMax scaling). Classification uses softmax transformation of signomial scores, while binary classification can use sigmoid. The method leverages analytical derivatives of signomials for interpretable feature attributions.

## Key Results
- ECSEL achieves competitive classification accuracy on 11 benchmark datasets compared to established ML models
- On symbolic regression benchmarks, ECSEL recovers a larger fraction of target equations while requiring substantially less computation time
- Case studies demonstrate ECSEL's ability to expose dataset biases, support counterfactual reasoning, and provide actionable insights through interpretable mathematical formulas

## Why This Works (Mechanism)

### Mechanism 1: Signomial Structural Exploitation
ECSEL constrains the search space to signomials (sums of power-law terms), exploiting the observation that many benchmark equations in physics and other domains inherently possess this structure. By narrowing the search space, gradient-based optimization becomes tractable where stochastic search heuristics struggle. The underlying ground-truth equations or decision boundaries in the target domain actually exhibit signomial structure, or can be approximated well by signomials. Evidence includes the prevalence of signomial functions in the AI Feynman benchmark dataset and runtime efficiency comparisons. If the target equations are fundamentally non-signomial (e.g., require transcendental functions), ECSEL may fail to recover the exact symbolic form.

### Mechanism 2: Sparsity-Inducing Regularization for Interpretability
The L1 penalty forces the exponents of uninformative features toward zero during gradient descent. This acts as automatic feature selection, removing terms that do not contribute to loss minimization, resulting in a simpler final equation. A sparse subset of features is sufficient for accurate prediction, and L1 regularization effectively identifies this subset without discarding critical information. The â„“1 regularization on exponents drives irrelevant terms to zero during optimization, producing sparse, human-readable equations. If all features contribute equally and non-linearly, or if features are highly correlated, L1 regularization might arbitrarily select one feature over another.

### Mechanism 3: Closed-Form Derivatives for Explainability
Because the signomial is a sum of products of power-law terms, its derivatives (e.g., elasticities, log-gradients) can be computed analytically. This bypasses the need for perturbation-based or sampling-based explanation methods like SHAP. The analytical properties derived (e.g., elasticity, log-space additivity) are valid proxies for human-understandable "feature importance." The signomial functional form permits exact, closed-form computation of global and local feature attributions, enabling analytical interpretability. The exact additive decomposition strictly holds for single-term signomials; for multi-term signomials, local attributions become gradient-based approximations.

## Foundational Learning

- **Concept: Symbolic Regression (SR)**
  - Why needed: ECSEL is an inherently interpretable classifier inspired by SR's goals. Understanding the problem SR solves (finding both form and parameters) clarifies ECSEL's contribution.
  - Quick check: What is the fundamental difference between standard regression (fitting parameters) and symbolic regression?

- **Concept: Signomial Functions**
  - Why needed: This is the core mathematical object. You must understand its definition ($\sum \alpha_k \prod x_j^{\beta_{k,j}}$) and its constraints (positivity) to grasp the method.
  - Quick check: What two types of parameters are learned for each term in a signomial, and what constraint does this form place on the input features?

- **Concept: Softmax Classification**
  - Why needed: ECSEL uses class-specific signomial score functions which are then transformed into probabilities via softmax. This is the final prediction step.
  - Quick check: How does ECSEL convert the raw signomial scores $z_c(x)$ into class probabilities?

## Architecture Onboarding

- **Component map**: Feature Input (positive) -> **Learnable Exponents & Coefficients** -> Class Signomial Score Functions -> Softmax -> Cross-Entropy Loss (+ L1 Penalty on Exponents)
- **Critical path**: The **exponents ($\beta$)** are the key interpretable parameters. Their magnitude and sign directly encode feature influence in the learned equation.
- **Design tradeoffs**: **Number of terms (K)**: $K=1$ ensures exact log-space additivity but may lack expressiveness. $K>1$ increases capacity but complicates local explanations. **Sparsity ($\lambda$)**: High $\lambda$ yields simpler equations but may underfit.
- **Failure signatures**: **Numerical instability** from large exponents (mitigated by scaling). **Non-recovery** if the true equation is non-signomial (ECSEL yields a numerical fit, not symbolic recovery). **Local optima** for $K>1$ due to non-convexity.
- **First 3 experiments**:
  1. **Hyperparameter Search**: Tune L1 strength ($\lambda$) and number of terms ($K$) using a validation set. Start with $K \in \{1,2\}$.
  2. **Ablation on K**: Compare performance and equation complexity for $K=1$ vs. $K=2$ on your dataset to assess the tradeoff between interpretability and expressiveness.
  3. **Interpretability Validation**: For a correct prediction, compute the elasticity $\mathcal{E}_{c,j}(x)$. Check if the identified important features align with known domain intuition.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the number of signomial terms ($K$) be determined adaptively rather than requiring specification a priori? The current implementation treats $K$ as a standard hyperparameter, requiring external tuning rather than learning the model complexity dynamically during training.

- **Open Question 2**: How does $\ell_1$ regularization stability hold in the presence of highly correlated features? While $\ell_1$ encourages sparsity, it is known to be unstable with correlated variables, and the paper notes $\ell_1$ is "not essential" and alternatives could be used.

- **Open Question 3**: Can faithfulness guarantees be formally extended to multi-term models ($K > 1$)? Proposition B.1 proves that exponent magnitude reflects feature contribution strictly for $K=1$; the extension to multi-term models relies on "intuition" rather than formal proof.

## Limitations

- ECSEL's effectiveness is domain-dependent, relying on signomial structure that may not be present in all domains
- The staged optimization procedure for K>1 signomials lacks precise specification of regularization schedules and transition criteria
- Analytical interpretability properties depend on whether practitioners can meaningfully interpret elasticity values without extensive domain knowledge
- The requirement for positive features limits applicability to datasets where log-transformation is appropriate

## Confidence

- **High Confidence**: ECSEL achieves competitive classification accuracy compared to established models (supported by 11 benchmark datasets with standard metrics). The computational efficiency advantage over stochastic symbolic regression methods is well-demonstrated through runtime comparisons.
- **Medium Confidence**: The interpretability claims regarding equation transparency and feature attribution are supported by case studies but would benefit from broader empirical validation across diverse domains. The recovery of ground-truth equations on symbolic regression benchmarks is demonstrated but relies heavily on the signomial assumption.
- **Low Confidence**: The assertion that elasticity values provide universally meaningful explanations for practitioners lacks user studies or expert validation. The claim about exposing dataset biases through interpretable equations is demonstrated in specific cases but not systematically evaluated.

## Next Checks

1. **Cross-Domain Applicability Test**: Apply ECSEL to datasets from domains known to have non-signomial relationships (e.g., financial time series with seasonality, biological systems with threshold effects) and evaluate whether the method still produces useful approximations or fails gracefully.

2. **User Interpretability Study**: Conduct a controlled experiment where domain experts are asked to interpret ECSEL equations versus black-box models with SHAP explanations, measuring both accuracy of interpretation and time required for analysis.

3. **Robustness to Initialization and Optimization**: Systematically vary the number of random seeds and initialization schemes for K>1 signomials, documenting the variance in recovered equations and final performance to quantify the impact of local optima.