---
ver: rpa2
title: 'Skewed Memorization in Large Language Models: Quantification and Decomposition'
arxiv_id: '2502.01187'
source_url: https://arxiv.org/abs/2502.01187
tags:
- memorization
- training
- data
- npre
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of skewed memorization in Large
  Language Models (LLMs), where a small subset of training data contributes disproportionately
  to memorization risk. The core method involves analyzing memorization probabilities
  over sequence lengths and decomposing term-wise probabilities in the LLM generation
  process to understand how data characteristics influence memorization likelihood.
---

# Skewed Memorization in Large Language Models: Quantification and Decomposition

## Quick Facts
- arXiv ID: 2502.01187
- Source URL: https://arxiv.org/abs/2502.01187
- Reference count: 29
- Primary result: This work addresses the problem of skewed memorization in Large Language Models (LLMs), where a small subset of training data contributes disproportionately to memorization risk.

## Executive Summary
This paper addresses the problem of skewed memorization in LLMs, where a small subset of training data contributes disproportionately to memorization risk. The authors develop a prefix continuation framework to quantify memorization by measuring consecutive token recall beyond a given prefix, revealing that memorization distributions are highly skewed with extreme cases occurring rarely but carrying significant privacy risk. The core innovation is decomposing the term-wise memorization probabilities in the LLM generation process and proving that these probabilities are largely independent of prior correct outputs, enabling a geometric approximation of memorization risk.

The paper demonstrates that memorization increases with training duration and is influenced by dataset composition and size, with the skewness of the memorization distribution linked to the token generation process. The authors correlate memorization likelihood with embedding-space diversity metrics, showing that samples with high prefix similarity but diverse suffixes exhibit lower memorization due to higher prediction entropy. This work provides both a practical framework for measuring worst-case memorization and theoretical insights into the mechanisms driving memorization in autoregressive models.

## Method Summary
The authors analyze memorization in LLMs by partitioning training sequences into prefix and suffix components, then measuring consecutive token recall during autoregressive generation. They fine-tune Llama-3.1-8B-Instruct on medical QA datasets using LoRA with specified hyperparameters, evaluating memorization at multiple checkpoints. The primary metric is npre (consecutive tokens correctly recalled beyond a 100-character prefix), computed across the full training set. The method includes non-parametric resampling to estimate tail distributions and computes mutual information between token positions to validate independence assumptions. Embedding similarity gaps are used to correlate memorization with input diversity.

## Key Results
- Memorization distributions are highly skewed, with average metrics systematically underestimating worst-case memorization risk
- Term-wise memorization probabilities are largely independent, enabling geometric approximation of sequence-level memorization
- Memorization increases with training duration and is influenced by dataset composition (single-domain vs. mixed datasets)
- Embedding-space diversity inversely correlates with memorization likelihood, with higher similarity gaps indicating lower memorization risk

## Why This Works (Mechanism)

### Mechanism 1: Prefix Continuation Quantification
Memorization can be measured by counting consecutive tokens correctly recalled beyond a provided prefix. Partition each training sequence into prefix and suffix. Prompt the model with the prefix and compare generated continuation to ground truth. The consecutive match length `npre` follows a highly skewed distribution that average metrics miss. Worst-case memorization instances are rare but carry disproportionate privacy risk; they require distributional rather than mean-based analysis.

### Mechanism 2: Token-wise Independence Enables Geometric Approximation
Per-token memorization probability is largely independent of prior correct outputs, allowing `P(Npre ≥ n) ≈ ∏ pj`. Autoregressive generation means token j+1 depends on prior context but not on whether those tokens were model outputs or ground truth. Low mutual information `MI(CJpre, Cj)` empirically validates independence. Memorization is driven primarily by local context and token-wise probability distributions, not sequential dependencies.

### Mechanism 3: Embedding-Space Diversity Inversely Correlates with Memorization
Samples with high prefix similarity but diverse suffixes exhibit lower memorization due to higher prediction entropy. Define similarity gap `ΔS = Sfull - Sinput`. Large gaps indicate suffixes vary more than prefixes, increasing classification entropy and reducing memorization likelihood. LLMs approximate term-wise classifiers; high local entropy in suffix prediction reduces verbatim recall.

## Foundational Learning

- **Autoregressive Generation Factorization**: Explains why `P(j|rpre, 1...j-1)` is largely independent of whether prior tokens were model outputs or ground truth, enabling the decomposition. Quick check: Can you explain why later outputs don't causally affect earlier ones in autoregressive models?

- **Non-parametric Resampling for Skewed Distributions**: Memorization is highly skewed; mean-based metrics systematically underestimate worst cases. Bootstrap/resampling methods approximate the true tail. Quick check: Why does small sample size systematically underestimate extreme memorization cases?

- **Bayes Optimal Classifier Bounds**: Provides theoretical upper bound on memorization; term-wise BOC (`Mt`) shows LLMs optimize per-token likelihood rather than full-sequence recall. Quick check: How does the term-wise BOC differ from full-sequence BOC in predicting memorization?

## Architecture Onboarding

- **Component map**:
```
Training Data → Prefix/Suffix Partition → Model Checkpoint
                                              ↓
Prefix Input → Model Inference (temp=0) → Generated Continuation
                                              ↓
                              Token-by-Token Comparison → npre (consecutive match length)
                                              ↓
                              Aggregate across dataset → Distribution F(Npre)
                                              ↓
                              Non-parametric resampling → Tail estimates (max, 99.9%-tile)
```

- **Critical path**: Accurate tail estimation requires full-dataset scan (not sampling). Small samples systematically miss worst cases (Figure 5: with n=455, sampled max≈8 vs true max=16).

- **Design tradeoffs**:
  - Sampling vs full scan: Sampling is faster but biased; full scan needed for privacy-critical worst-case detection
  - Prefix length: Shorter prefixes test deeper memorization but may yield shorter matches; 100 characters used here
  - Checkpoint frequency: Early checkpoints (epoch 10) can show max memorization >40 even with loss >1.0; don't rely on loss as proxy

- **Failure signatures**:
  - Relying on mean memorization or loss values to assess privacy risk (misses tail)
  - Assuming independence holds without computing MI (check Figure 7 pattern)
  - Generalizing across datasets without re-evaluating (mixed vs single-domain shows different patterns, Figure 3)

- **First 3 experiments**:
  1. Baseline memorization scan: Run prefix continuation on full training set at multiple checkpoints (e.g., epochs 10, 50, 100). Plot distribution, max, 99%-tile, and skewness.
  2. Sample size sensitivity test: Subsample training data at sizes 256, 512, 1024. Measure probability of missing top-k memorized cases (replicate Figure 5 analysis).
  3. Embedding diversity correlation: Compute `ΔS` for each sample using sentence embeddings. Plot `npre` vs `ΔS` to validate inverse correlation (replicate Figure 6).

## Open Questions the Paper Calls Out
None

## Limitations
- The independence assumption underlying the geometric approximation is empirically supported but not theoretically proven for all LLM architectures
- The embedding-space diversity metric depends on the quality and appropriateness of the sentence embedding model used, which is not fully specified
- The analysis is confined to a single domain (medical QA) and relatively small dataset size (~10K samples), limiting generalizability

## Confidence
- **Prefix continuation quantification framework**: High confidence
- **Geometric approximation via token-wise independence**: Medium confidence
- **Embedding diversity correlation with memorization**: Medium confidence
- **Training duration and dataset composition effects**: High confidence

## Next Checks
1. **Independence assumption stress test**: Systematically vary prefix lengths and compute mutual information between token positions across different training checkpoints. Identify the point at which MI increases significantly and test whether the geometric approximation breaks down.

2. **Embedding model sensitivity analysis**: Repeat the embedding diversity correlation experiments using multiple embedding models (e.g., Sentence-BERT, LaBSE, and task-specific medical embeddings). Quantify how the correlation strength and significance vary with embedding choice.

3. **Dataset scale validation**: Extend the memorization analysis to datasets with 100K+ samples while maintaining the full-scan approach for tail estimation. Compare the scaling behavior of max memorization values and skewness statistics to determine whether the observed patterns hold at larger scales.