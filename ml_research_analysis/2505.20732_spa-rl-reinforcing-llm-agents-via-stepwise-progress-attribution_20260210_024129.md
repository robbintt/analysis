---
ver: rpa2
title: 'SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution'
arxiv_id: '2505.20732'
source_url: https://arxiv.org/abs/2505.20732
tags:
- reward
- task
- progress
- arxiv
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of delayed rewards in training LLM
  agents for multi-step tasks. The authors propose Stepwise Progress Attribution (SPA),
  a framework that decomposes the final reward into stepwise contributions by training
  a progress estimator.
---

# SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution

## Quick Facts
- arXiv ID: 2505.20732
- Source URL: https://arxiv.org/abs/2505.20732
- Reference count: 40
- Primary result: 2.5% higher success rate and 1.9% higher grounding accuracy over SOTA

## Executive Summary
This paper addresses the challenge of delayed rewards in training LLM agents for long-horizon tasks by proposing Stepwise Progress Attribution (SPA). The framework decomposes final task rewards into stepwise contributions through a trained progress estimator, providing dense intermediate rewards during reinforcement learning. This approach enables more effective credit assignment for each action, particularly benefiting multi-step reasoning tasks. Experimental results across three benchmarks demonstrate consistent improvements in both task success rates and grounding accuracy.

## Method Summary
SPA-RL introduces a two-phase approach: first training a progress estimator to predict how much each action contributes to task completion, then using these predictions as intermediate rewards during RL fine-tuning. The progress estimator is trained on expert trajectories to map observation-action sequences to their cumulative contribution scores. During RL, the agent receives a fused reward combining the estimated progress contribution and executability signals. The method maintains policy gradient equivalence while providing more effective intermediate rewards than traditional reward redistribution approaches.

## Key Results
- Achieves 2.5% higher success rate on average compared to state-of-the-art methods
- Improves grounding accuracy by 1.9% over SOTA baselines
- Shows consistent performance gains as task length increases, particularly effective for long-horizon tasks
- Outperforms existing reward redistribution methods across all three benchmark tasks

## Why This Works (Mechanism)
SPA-RL works by decomposing the final delayed reward into intermediate stepwise contributions that reflect actual progress toward task completion. By training a progress estimator on expert trajectories, the method learns to predict how much each action advances the task. These predictions serve as dense rewards during RL, providing more informative feedback than binary executability signals alone. The fused reward formulation (combining progress estimates with executability) ensures the agent learns both task-relevant actions and properly grounded API calls.

## Foundational Learning
- **Progress Attribution**: Decomposing final rewards into intermediate contributions - needed to provide dense feedback during long-horizon tasks; quick check: predicted cumulative scores should match observed rewards
- **Behavior Cloning Warmstart**: Pre-training agent on expert trajectories - needed to establish reasonable initial policy before RL; quick check: success rate on held-out validation tasks
- **Progress Estimator Training**: MLP mapping observations to progress scores - needed to quantify action contributions; quick check: variance in predictions across different trajectories
- **Reward Fusing**: Combining progress estimates with executability signals - needed to balance task progress with valid actions; quick check: contribution of each component to final performance

## Architecture Onboarding

**Component Map:**
SFT Agent -> Progress Estimator -> PPO Fine-tuning -> Improved Agent

**Critical Path:**
1. SFT on expert trajectories (3 epochs)
2. Progress estimator training (1 epoch)
3. PPO fine-tuning with fused rewards (1 epoch)

**Design Tradeoffs:**
- Progress estimator adds training overhead but enables better credit assignment
- Fused reward balances task progress with valid action learning
- Single-epoch fine-tuning limits computational cost while capturing learned attributions

**Failure Signatures:**
- Progress estimator collapse: all predictions converge to similar values
- No improvement over baseline: Σc_t significantly deviates from observed rewards
- Poor grounding accuracy: g_t signal fails to capture executability correctly

**3 First Experiments:**
1. Validate progress estimator by checking if predicted cumulative scores match observed rewards
2. Test fused reward formulation by varying α and β to find optimal balance
3. Compare SPA-RL against reward redistribution baseline on short tasks before scaling

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to three benchmark environments (WebShop, ALFWorld, VirtualHome)
- Performance improvements depend on quality of expert trajectories for training progress estimator
- Single-epoch fine-tuning may limit maximum achievable performance

## Confidence
- **High Confidence**: Framework design and theoretical justification for progress attribution approach
- **Medium Confidence**: Reported performance improvements given reasonable but not fully specified hyperparameters
- **Medium Confidence**: Claims about effectiveness on long-horizon tasks pending reproduction with varying task lengths

## Next Checks
1. Validate that predicted cumulative progress scores align with observed rewards (Σc_t ≈ R) across different trajectories
2. Systematically vary α, β, and MLP architecture to establish robustness of performance improvements
3. Test SPA-RL on tasks of varying lengths (short/medium/long) to verify consistent performance gains as task length increases