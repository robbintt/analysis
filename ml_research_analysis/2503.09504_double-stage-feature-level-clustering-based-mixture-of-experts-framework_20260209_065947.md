---
ver: rpa2
title: Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework
arxiv_id: '2503.09504'
source_url: https://arxiv.org/abs/2503.09504
tags:
- expert
- training
- dfcp-moe
- network
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving image classification
  performance using Mixture-of-Experts (MoE) models while mitigating the impact of
  noise and outliers. The authors propose the Double-stage Feature-level Clustering
  and Pseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which combines
  feature extraction, two-stage clustering, and a pseudo-labeling strategy.
---

# Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework

## Quick Facts
- arXiv ID: 2503.09504
- Source URL: https://arxiv.org/abs/2503.09504
- Reference count: 31
- One-line primary result: DFCP-MoE achieves 99.95% mAP and 51ms inference time on GTSRB, outperforming traditional MoE (92.67% mAP) and dense models (96.68% mAP)

## Executive Summary
This paper introduces the Double-stage Feature-level Clustering and Pseudo-labeling-based Mixture of Experts (DFCP-MoE) framework to address the challenge of improving image classification performance using MoE models while mitigating noise and outlier impacts. The approach combines feature extraction, two-stage clustering, and pseudo-labeling to condition expert specialization rather than relying on random expert assignment. The framework employs conditional end-to-end joint training where the managing network assigns specific clusters to specific experts, improving expert specialization and reducing inference latency.

The proposed approach was evaluated on the GTSRB dataset and demonstrated superior performance compared to traditional MoE and dense models. DFCP-MoE achieved 99.95% mean average precision, 0.99 balanced accuracy, and approximately 51ms average inference time. These results demonstrate that structured input assignment and expert specialization effectively handle large-scale image classification tasks while maintaining efficiency.

## Method Summary
The DFCP-MoE framework operates through a multi-stage process: (1) Feature extraction using EfficientNet-B1 pretrained model; (2) Two-stage clustering where K-means is applied first, followed by K-nearest neighbor refinement if Davies-Bouldin Index exceeds 1; (3) Pseudo-labeling via a Siamese network trained on a small labeled subset to propagate labels to clustered unlabeled data; (4) Conditional end-to-end joint training of 43 expert models (LeNet-5 variants) and a managing network with softmax gating, where each expert is specialized to specific clusters while maintaining generalization across class distributions.

## Key Results
- DFCP-MoE achieves 99.95% mean average precision on GTSRB dataset
- Model demonstrates 0.99 balanced accuracy and 51ms average inference time
- Outperforms traditional MoE (92.67% mAP) and dense models (96.68% mAP)
- Successfully reduces noise and outlier impact through structured clustering and pseudo-labeling

## Why This Works (Mechanism)

### Mechanism 1: Double-Stage Clustering for Noise Reduction
The two-stage clustering produces higher-quality input partitions than single-stage K-means by reducing noise and outlier contamination. First stage applies K-means on extracted features, and when Davies-Bouldin Index exceeds 1, a second stage refines clusters by selecting reference clusters with ≥3 nearest neighbors and reassigning feature maps based on Euclidean distance to centroids (threshold ≤ 0.8).

### Mechanism 2: Pseudo-Labeling via Siamese Networks
A Siamese network trained on a small labeled subset can reliably propagate labels to clustered unlabeled data. The network is trained with contrastive loss on trusted labeled data, then computes similarity scores between clustered samples and labeled samples, assigning labels based on threshold τ* that maximizes F1 score.

### Mechanism 3: Conditional Expert Assignment with Structured Routing
Pre-assigning clusters to specific experts improves specialization and reduces inference latency compared to random gating-based assignment. The managing network is conditioned on cluster assignments during joint training, with each expert training primarily on its assigned cluster with minority samples from other clusters to prevent overfitting.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) Architecture**
  - Why needed here: Understanding baseline components (experts + gating network) and their interaction is prerequisite to understanding why structured assignment improves over random routing
  - Quick check question: Can you explain how softmax-gated weights combine expert outputs and why random assignment causes the "shrinking batch problem"?

- **Concept: Davies-Bouldin Index (DBI) for Clustering Evaluation**
  - Why needed here: The paper uses DBI thresholds to trigger second-stage clustering; interpreting these values is essential for implementation
  - Quick check question: What does a DBI score of 1.2 vs 0.6 indicate about intra-cluster compactness and inter-cluster separation?

- **Concept: Siamese Neural Networks with Contrastive Loss**
  - Why needed here: The pseudo-labeling mechanism depends on understanding how SNN learns similarity metrics
  - Quick check question: How does contrastive loss penalize the network differently for similar vs dissimilar pairs?

## Architecture Onboarding

- **Component map:** Feature Extractor (EfficientNet-B1) → Two-stage Clustering Module (K-means → K-NN refinement) → Siamese Network (Pseudo-labeling) → Expert Models (43 DCNNs) → Managing Network (DCNN gate)

- **Critical path:** 1. Extract features via EfficientNet → 2. First-stage K-means clustering → 3. Compute DBI → 4. If DBI > 1, apply second-stage refinement → 5. Pseudo-label via SNN → 6. Validate cluster purity (target ≥ 0.98) → 7. Joint training (forward + backward with coordinated gradient updates per Equations 14–20)

- **Design tradeoffs:** Training time (~126 hours) vs inference speed (~51ms): Extensive per-expert hyperparameter tuning increases training cost. Expert count (43) vs cluster granularity: More experts enable finer specialization but increase hyperparameter search space. Distance threshold 0.8 for second-stage clustering: Lower = stricter reassignment; higher = more permissive, risk of noise.

- **Failure signatures:** Cluster purity < 0.95 after pseudo-labeling → SNN threshold or labeled subset quality issue. Expert precision variance > 20% across classes → Hyperparameter optimization not converging uniformly. Inference time approaches dense model (~71ms) → Gate not properly sparsifying expert activation.

- **First 3 experiments:**
  1. **Ablation on clustering stages:** Train DFCP-MoE with only first-stage clustering; compare mAP and cluster purity to full two-stage approach on validation split
  2. **Threshold sensitivity analysis:** Sweep second-stage distance threshold (0.6, 0.7, 0.8, 0.9); plot DBI, cluster purity, and final mAP to identify optimal operating point
  3. **Expert count scaling:** Merge similar clusters to reduce expert count from 43 to 20; measure impact on training time, inference latency, and per-class precision distribution

## Open Questions the Paper Calls Out

- **Question:** How can a robust statistical methodology be developed to effectively identify and evaluate the legitimacy of outliers to prevent performance degradation in the two-phase clustering and pseudo-labeling stages?
- **Question:** How can the DFCP-MoE framework be adapted to handle sequential or temporal data types, such as video datasets?
- **Question:** Can the computational overhead of the joint training process be reduced to make the framework scalable to larger datasets without compromising expert specialization?

## Limitations
- The paper does not specify the size of the labeled subset used for Siamese network training, which is critical for reproducing the pseudo-labeling mechanism
- The reconstruction method for images from clustered features is mentioned but not detailed
- Expert hyperparameter tuning via Optuna is claimed to optimize performance but lacks reproducibility without the search space and constraints
- The study focuses exclusively on GTSRB dataset; generalization to other domains is untested

## Confidence
- High confidence: The framework architecture and training pipeline are clearly described and internally consistent
- Medium confidence: The claimed performance improvements (99.95% mAP vs 92.67% traditional MoE) are well-documented on GTSRB but may not generalize
- Low confidence: The hyperparameter optimization process and specific implementation details for clustering and pseudo-labeling are insufficiently specified for exact reproduction

## Next Checks
1. **Reproduce ablation on clustering stages:** Train DFCP-MoE with only first-stage clustering on GTSRB validation split and compare mAP, cluster purity, and per-class precision distribution to full two-stage approach
2. **Validate pseudo-labeling mechanism:** Systematically sweep the similarity threshold τ* and measure F1 score, precision, and recall to confirm optimal threshold selection methodology
3. **Test generalization:** Apply DFCP-MoE to a different image classification dataset (e.g., CIFAR-10) and evaluate whether the two-stage clustering and pseudo-labeling improvements persist across domains