---
ver: rpa2
title: 'SWE-QA: Can Language Models Answer Repository-level Code Questions?'
arxiv_id: '2509.14635'
source_url: https://arxiv.org/abs/2509.14635
tags:
- code
- questions
- repository-level
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SWE-QA, a new benchmark for evaluating language
  models on repository-level code question answering. The authors construct 576 high-quality
  question-answer pairs across 12 popular Python repositories, covering diverse categories
  such as cross-file reasoning and multi-hop dependency analysis.
---

# SWE-QA: Can Language Models Answer Repository-level Code Questions?

## Quick Facts
- **arXiv ID**: 2509.14635
- **Source URL**: https://arxiv.org/abs/2509.14635
- **Reference count**: 40
- **Key result**: Introduced SWE-QA benchmark with 576 QA pairs; SWE-QA-Agent achieves 47.82 score on Claude 3.7 Sonnet

## Executive Summary
This paper introduces SWE-QA, a new benchmark for evaluating language models on repository-level code question answering. The authors construct 576 high-quality question-answer pairs across 12 popular Python repositories, covering diverse categories such as cross-file reasoning and multi-hop dependency analysis. To address the challenges of repository-level QA, they propose SWE-QA-Agent, an agentic framework that iteratively retrieves and reasons over code contexts using tools like semantic search and file inspection. Experiments with six advanced LLMs show that SWE-QA-Agent significantly outperforms standard RAG methods, achieving an overall score of 47.82 with Claude 3.7 Sonnet, particularly excelling in completeness and reasoning quality. The results highlight the promise of agentic approaches for complex repository-level understanding.

## Method Summary
The SWE-QA framework combines a taxonomy-grounded benchmark construction pipeline with an agentic QA system. The benchmark is built by analyzing 77,100 GitHub issues to create a two-level taxonomy (What, Why, Where, How), generating questions with LLM assistance guided by Tree-sitter parsing, and validating through human-in-the-loop filtering. The SWE-QA-Agent uses a ReAct-style loop with tools for semantic search (`SearchContent`), structural navigation (`GetRepoStructure`), file reading (`ReadFile`), and completion (`Finish`). The agent iteratively refines context by analyzing sufficiency and selecting appropriate actions, contrasting with baseline sliding window RAG approaches.

## Key Results
- SWE-QA-Agent achieves 47.82 overall score with Claude 3.7 Sonnet, outperforming Direct and Sliding Window RAG baselines
- Agent framework particularly excels at "Completeness" and "Reasoning" dimensions for complex multi-hop questions
- Specialized code models (Qwen2.5-Coder) show significant performance drops on specific repositories (pytest, sqlfluff), indicating brittleness
- Smaller models (Devstral, Qwen-32B) gain minimal benefit from agent framework compared to RAG, suggesting reasoning overhead requires high-capability models

## Why This Works (Mechanism)

### Mechanism 1: Iterative Context Refinement
- **Claim:** Iterative "Reason-Act-Observe" loops outperform single-pass retrieval for multi-hop reasoning by allowing progressive context narrowing.
- **Mechanism:** SWE-QA-Agent evaluates context sufficiency after each retrieval, selecting specific actions to gather new evidence rather than relying on static context windows.
- **Core assumption:** The underlying LLM can reliably determine what information is missing and which tool provides it.
- **Evidence anchors:** Agent "iteratively retrieves and reasons over code contexts... particularly excelling in completeness and reasoning quality" [abstract]; "lifts 'Completeness' and 'Reasoning'... indicating value of plan-act-observe loops" [section 5.1].
- **Break condition:** Retrieves irrelevant code saturating context window or LLM hallucinates without proper `Finish` action.

### Mechanism 2: Structural Navigation via Tool Augmentation
- **Claim:** Structural inspection tools enable LLMs to form "mental maps" of repositories, answering locational questions that semantic search misses.
- **Mechanism:** `GetRepoStructure` (running `tree`) and `ReadFile` tools allow the agent to identify module boundaries and drill down through file trees.
- **Core assumption:** Repository adheres to standard structural conventions where paths correlate with architectural roles.
- **Evidence anchors:** "GetRepoStructure... provides global contextual map... essential for understanding module dependencies" [section 3.1]; successfully located `PluginSpec` in SQLFluff case study [section 5.5].
- **Break condition:** Repository has flat structures or inconsistent naming rendering structural heuristics useless.

### Mechanism 3: Taxonomy-Grounded Data Generation
- **Claim:** High-quality complex reasoning benchmarks require human-validated taxonomies to ensure questions test distinct capabilities.
- **Mechanism:** Authors analyzed 77,100 issues to build taxonomy, generated questions guided by this taxonomy, and used human-in-the-loop validation to filter hallucinated or trivial questions.
- **Core assumption:** GitHub issues accurately represent real-world developer needs distribution and complexity.
- **Evidence anchors:** "Developed two-level taxonomy... constructed seed questions for each category" [section 2.1]; rigorous filtering criteria applied [section 2.4].
- **Break condition:** Ground truth answers contain inaccuracies or rely on outdated code versions.

## Foundational Learning

- **Concept: ReAct (Reasoning + Acting) Framework**
  - **Why needed here:** SWE-QA-Agent is explicitly built on this paradigm; understanding how it interleaves "Thoughts" with "Actions" is essential.
  - **Quick check question:** In a ReAct loop, does the model generate the final answer before or after the tool execution step?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** This is the baseline method (Sliding Window RAG) against which the agent is compared; understanding single-pass retrieval limitations is crucial.
  - **Quick check question:** Why might "Sliding Window RAG" fail to answer a question about a function defined in File A that is imported and used in File B?

- **Concept: Tree-sitter Parsing**
  - **Why needed here:** This technology extracts code structure (classes, functions, variables) for benchmark construction pipeline.
  - **Quick check question:** What specific advantage does a language-agnostic parser like Tree-sitter offer when analyzing a diverse set of 12 repositories?

## Architecture Onboarding

- **Component map:** Issue Crawler → Taxonomy Builder → Question Instantiator (LLM + Tree-sitter) → Human Validator → Agent Runtime (LLM Core → ReAct Loop → Action Space → Repository Context)
- **Critical path:** The Agent's reasoning loop (Algorithm 1). System fails if `Reason` step generates flawed plan or `SelectAction` chooses wrong tool.
- **Design tradeoffs:**
  - Standard RAG vs. Agent: Standard RAG is faster and cheaper but struggles with multi-hop reasoning; Agent is slower but achieves higher completeness.
  - Model Size: Smaller models show minimal gains with Agent framework, suggesting reasoning overhead requires high-capability models.
- **Failure signatures:**
  - Procedural Blindness: Models score lowest on "Where" and "How" questions, indicating difficulty tracing execution flows.
  - Repository Brittleness: Performance drops significantly on specific repositories like pytest and sqlfluff due to complex plugin architectures.
- **First 3 experiments:**
  1. Baseline vs. Agent Reproduction: Replicate setup using smaller open-source model (e.g., Qwen-Coder) to verify overhead cost on less capable models.
  2. Ablation on Tools: Run agent with `GetRepoStructure` disabled to quantify structural navigation's contribution to "Where" category questions.
  3. Taxonomy Stress Test: Manually create 5 "Compound" questions and measure if agent handles multi-intent queries better than standard RAG.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does SWE-QA-Agent performance generalize to non-Python repositories with different dependency management systems and syntax structures?
  - **Basis in paper:** Section 6 states benchmark "focuses exclusively on Python repositories," extending to "additional programming languages" as future work.
  - **Why unresolved:** Current evaluation restricted to 12 Python repositories; effectiveness on languages with different structural complexities is unverified.
  - **What evidence would resolve it:** SWE-QA benchmarks for Java or C++ repositories showing comparable performance gaps between Direct, RAG, and Agent methods.

- **Open Question 2:** What specific architectural features of "pytest" and "sqlfluff" repositories cause performance brittleness in specialized code models?
  - **Basis in paper:** Finding 6 and Table 7 show specialized models exhibit "pronounced sensitivity" with low scores (12.78, 17.06) on these repositories.
  - **Why unresolved:** Paper identifies variance in difficulty but doesn't isolate root causes like specific coding patterns or plugin architectures.
  - **What evidence would resolve it:** Ablation study linking repository graph metrics (e.g., hook density, indirect call depth) to performance drop in specialized versus generalist models.

- **Open Question 3:** Can agentic overhead be reduced for smaller models while maintaining completeness gains seen in larger models?
  - **Basis in paper:** Section 5.1 notes agent framework introduces "overhead that pays off most when base model reliably follows plans," often performing on par with standard RAG for lighter-weight models.
  - **Why unresolved:** Unclear if lack of improvement is due to reasoning incapacity or if multi-step action loop is inherently inefficient for models with weaker instruction-following capabilities.
  - **What evidence would resolve it:** Comparative analysis of token usage and latency relative to accuracy for small models using simplified single-step agent workflow versus full ReAct loop.

## Limitations

- Evaluation relies on GPT-5 as judge model, which is not publicly available, creating barrier to exact reproduction
- Critical implementation details (agent prompts, retrieval parameters, judge prompt) are referenced but not fully specified
- Performance drops on specific repositories suggest brittleness in handling unconventional code structures or complex plugin architectures

## Confidence

- **High Confidence**: Core methodology of iterative agent with tool augmentation (ReAct + RAG) for repository-level QA is sound and well-supported by ablation studies
- **Medium Confidence**: Reported performance improvements (47.82 overall score) are credible within SWE-QA benchmark context, but exact numerical scores uncertain without GPT-5
- **Low Confidence**: Generalizability to repositories outside 12 Python projects tested, and robustness to non-standard code structures, are not fully established

## Next Checks

1. **Prompt and Parameter Transparency**: Obtain or reconstruct exact agent prompts, retrieval parameters, and judge model prompt to enable faithful reproduction of experimental setup
2. **Judge Model Calibration**: Perform small-scale human evaluation on sample questions to calibrate scoring from alternative judge model (e.g., GPT-4o) against original GPT-5-based results
3. **Structural Navigation Ablation**: Conduct ablation study disabling `GetRepoStructure` tool to quantify specific contribution to answering "Where" and "How" questions, verifying importance of structural navigation