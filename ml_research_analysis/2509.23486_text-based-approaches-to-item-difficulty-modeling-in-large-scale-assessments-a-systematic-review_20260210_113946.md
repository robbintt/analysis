---
ver: rpa2
title: 'Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments:
  A Systematic Review'
arxiv_id: '2509.23486'
source_url: https://arxiv.org/abs/2509.23486
tags:
- item
- difficulty
- studies
- features
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review analyzed 37 articles and 46 studies on automated
  item difficulty prediction in large-scale assessments published through May 2025.
  Results showed that transformer-based language models, particularly small language
  models like BERT and its variants, achieved strong performance in predicting item
  difficulty, with RMSE as low as 0.165, Pearson correlation as high as 0.87, and
  accuracy up to 0.806.
---

# Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review

## Quick Facts
- arXiv ID: 2509.23486
- Source URL: https://arxiv.org/abs/2509.23486
- Reference count: 0
- Primary result: Transformer-based language models, particularly small models like BERT, achieved strong performance in predicting item difficulty with RMSE as low as 0.165 and Pearson correlation up to 0.87

## Executive Summary
This systematic review analyzed 37 articles and 46 studies on automated item difficulty prediction in large-scale assessments published through May 2025. The review found that transformer-based language models, especially smaller variants like BERT, demonstrated superior performance compared to classical machine learning approaches, achieving RMSE values as low as 0.165 and Pearson correlations up to 0.87. Language proficiency and medicine were the most studied domains, with multiple-choice items dominating the research landscape. The review highlights the trade-off between the interpretability of classical models and the performance gains of language models in capturing complex linguistic patterns without manual feature engineering.

## Method Summary
The systematic review followed PRISMA guidelines to identify and synthesize research on text-based automated item difficulty prediction methods. The search included articles published through May 2025 across multiple databases, focusing on studies that used item text to predict difficulty without reference to student response data. The analysis examined model performance across different domains, item types, and input data configurations, comparing transformer-based models against classical machine learning approaches. Performance metrics varied across studies, with RMSE, R-squared, and Pearson correlation being most common for regression tasks, while exact accuracy was used for classification tasks.

## Key Results
- Transformer-based language models, particularly small models like BERT and its variants, achieved the strongest performance in predicting item difficulty
- Language proficiency and medicine were the most studied domains in the literature
- Multiple-choice items dominated the research, with input data typically including item stem, correct answer, and distractors
- Model selection should consider sample size and data quality, as small datasets may favor classical models to avoid overfitting

## Why This Works (Mechanism)
Assumption: Transformer-based models capture semantic complexity through attention mechanisms that identify linguistic patterns predictive of difficulty without requiring manual feature engineering, while their pre-training on large corpora provides robust representations that generalize across domains. Classical models require extensive feature engineering to approximate this semantic understanding.

## Foundational Learning
Unknown: The systematic review does not explicitly document specific foundational techniques or learning principles, but the analysis suggests that understanding transformer architecture, attention mechanisms, and the importance of pre-training data quality are essential for developing effective item difficulty models.

## Architecture Onboarding
Unknown: While the review does not provide specific architecture onboarding details, the analysis indicates that small transformer models like BERT, RoBERTa, and their variants are most effective for item difficulty prediction, with input configurations typically including item stems, correct answers, and distractors for multiple-choice items.

## Open Questions the Paper Calls Out
Unknown: The systematic review does not explicitly identify open questions, but the analysis suggests several areas requiring further investigation, including the generalizability of findings to under-represented domains, optimal model selection strategies for different dataset characteristics, and the potential for hybrid approaches combining classical and transformer-based methods.

## Limitations
- The systematic review is based on a relatively small sample of 37 articles and 46 studies, which may limit the generalizability of conclusions about automated item difficulty prediction methods
- The analysis primarily focuses on transformer-based models and classical machine learning approaches, potentially overlooking emerging methodologies or hybrid approaches not yet published
- Publication bias could exist since negative results or less successful approaches might be underrepresented in the literature

## Confidence
- **High confidence**: The comparative performance metrics for transformer-based models (RMSE, Pearson correlation, accuracy) are supported by multiple studies with consistent findings across different domains
- **Medium confidence**: The domain-specific prevalence findings (language proficiency and medicine as most studied) are reliable but may reflect researcher interests rather than actual assessment needs across all educational contexts
- **Medium confidence**: Recommendations for model selection based on sample size and data quality are theoretically sound but would benefit from empirical validation across diverse real-world datasets

## Next Checks
1. Conduct a validation study comparing classical machine learning models against transformer-based models on the same datasets with identical preprocessing to isolate the impact of model architecture from data preparation differences
2. Test the generalizability of findings by applying the most successful approaches to under-represented domains (STEM subjects, social sciences) to verify domain transferability
3. Implement a longitudinal analysis tracking the evolution of performance metrics over time as transformer architectures and training techniques continue to advance beyond the May 2025 cutoff