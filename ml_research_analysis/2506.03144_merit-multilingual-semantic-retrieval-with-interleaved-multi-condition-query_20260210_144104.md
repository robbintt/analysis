---
ver: rpa2
title: 'MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query'
arxiv_id: '2506.03144'
source_url: https://arxiv.org/abs/2506.03144
tags:
- product
- retrieval
- attribute
- arxiv
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERIT introduces the first multilingual interleaved multi-condition
  semantic retrieval benchmark with 320,000 queries across 5 languages, 7 product
  categories, and 135,000 products. Existing retrieval models show poor performance
  on this challenging task, primarily due to neglecting conditional query elements
  and misinterpreting visual content.
---

# MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query

## Quick Facts
- arXiv ID: 2506.03144
- Source URL: https://arxiv.org/abs/2506.03144
- Reference count: 40
- Introduces first multilingual interleaved multi-condition semantic retrieval benchmark with 320K queries across 5 languages, 7 categories, and 135K products

## Executive Summary
MERIT introduces the first multilingual interleaved multi-condition semantic retrieval benchmark, revealing that existing retrieval models struggle with multi-image, multi-condition queries due to neglecting conditional elements and misinterpreting visual content. To address these limitations, Coral combines embedding reconstruction to preserve fine-grained conditional elements with contrastive learning to extract global semantics. Experiments show Coral achieves a 45.9% performance improvement over conventional approaches on MERIT and demonstrates strong generalization across 8 established retrieval benchmarks.

## Method Summary
Coral fine-tunes multimodal large language models (MLLMs) like Qwen2.5-VL with three complementary losses: InfoNCE contrastive loss for global semantic alignment, vision reconstruction loss (MSE) for preserving visual attribute details, and masked language modeling loss for preserving textual conditions. The model processes interleaved image-text queries through masked embedding reconstruction, forcing retention of fine-grained attribute information that contrastive learning alone tends to compress. Training uses 0.5 masking ratio, 0.1 loss weights, and achieves improved performance on both sequential and concatenated image inputs after fine-tuning on MERIT.

## Key Results
- Coral achieves 45.9% improvement over conventional approaches on MERIT benchmark
- Sequential image input performance improves by 14.3% after MERIT training, restoring MLLM's native interleaved processing capability
- Strong generalization across 8 established retrieval benchmarks when trained on MERIT
- Significant performance degradation observed when query conditions increase beyond three elements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preserving fine-grained conditional elements via embedding reconstruction improves retrieval accuracy on multi-condition queries.
- **Mechanism:** Masked embedding reconstruction (applied separately to vision and language modalities) forces the model to retain detailed attribute information that contrastive learning alone tends to compress into global semantics.
- **Core assumption:** The key bottleneck in existing models is information loss in the [EOS] token embedding, which is normally the sole supervision point for contrastive loss.
- **Evidence anchors:** Abstract states Coral combines embedding reconstruction with contrastive learning; Section 4.1 describes MSE loss computation for reconstructed embeddings; Table 3 shows ablation results with 45.9% improvement from full reconstruction.

### Mechanism 2
- **Claim:** Training on interleaved multi-image queries restores and enhances MLLMs' native ability to process sequential image inputs.
- **Mechanism:** Existing retrieval datasets predominantly contain single-image examples, causing MLLMs to "forget" their pre-trained capability for interleaved image understanding. MERIT's multi-image, multi-condition queries reactivate this capability.
- **Core assumption:** The performance gap between concatenated and sequential image input is due to training data distribution, not fundamental model limitation.
- **Evidence anchors:** Page 7 notes sequential input performance improved by 14.3% after MERIT training; Table 3 shows sequential input surpasses image concatenation after full Coral training.

### Mechanism 3
- **Claim:** Supervising contrastive learning globally while supervising reconstruction locally provides complementary signals that jointly optimize retrieval.
- **Mechanism:** Contrastive loss optimizes entire query embedding for correct retrieval ranking, while reconstruction losses act as regularizers preserving fine-grained features at token level.
- **Core assumption:** These two objectives are synergistic and can be balanced without catastrophic interference.
- **Evidence anchors:** Page 8 presents overall training objective combining all three losses; Table 3 shows combining both reconstruction types yields higher R@1 than partial reconstruction; Figure 9 shows consistent improvements across 8 benchmarks.

## Foundational Learning

- **Concept: Multimodal Embedding Spaces & Contrastive Learning**
  - **Why needed here:** Core of Coral is learning shared embedding space where semantically similar queries and products are close. Understanding InfoNCE loss, temperature scaling, and in-batch negatives is essential.
  - **Quick check question:** Can you explain how the InfoNCE loss would change if we increased the temperature parameter τ?

- **Concept: Masked Autoencoding (MAE) / BERT-style Pretraining**
  - **Why needed here:** Reconstruction component uses principles from MAE and Masked Language Modeling. Understanding how masking parts of input and training model to reconstruct them encourages learning robust representations.
  - **Quick check question:** Why might reconstructing entire embedding sequence be more useful for retrieval than reconstructing only final [EOS] token?

- **Concept: Interleaved Multimodal Inputs in LLMs/MLLMs**
  - **Why needed here:** Key finding is degradation of interleaved processing in fine-tuned retrievers. Understanding how MLLMs natively handle sequences of alternating images and text through attention mechanisms.
  - **Quick check question:** In a transformer, how does self-attention mechanism allow information from different modalities (e.g., image token and text token) to interact within single sequence?

## Architecture Onboarding

- **Component map:** Base MLLM Encoder (frozen ViT + LLM) -> Sequence of hidden states -> h_eos (global query embedding) + Full hidden state sequence E -> Reconstruction Decoders (Fvθ, Flθ) -> Masked embeddings reconstruction -> Loss aggregation (Lcl + Lmse + Lmlm)

- **Critical path:**
  1. Input Preparation: Interleaved multi-condition query (text + multiple product images) is tokenized and embedded
  2. Encoding: MLLM produces h_eos (global embedding) and full hidden state sequence E
  3. Contrastive Step: h_eos used in InfoNCE loss against positive and negative product embeddings
  4. Reconstruction Step: E is masked; appropriate decoder reconstructs E; MSE/MLM loss computed
  5. Backpropagation: Gradients from summed loss update MLLM and decoders

- **Design tradeoffs:**
  - Masking Ratio (δ): Higher ratio forces more robust representations but makes reconstruction harder; paper uses δ = 0.5
  - Loss Weights (λ₁, λ₂): Higher values emphasize fine-grained preservation but may hurt global alignment; paper uses λ₁ = λ₂ = 0.1
  - Decoder Capacity: More layers improve reconstruction but add parameters; paper uses single randomly-initialized BERT layer
  - LoRA vs. Full Finetuning: LoRA is parameter-efficient but may yield lower peak performance; Table 3 shows full finetuning achieves higher R@1

- **Failure signatures:**
  - High reconstruction loss but low retrieval gain: Reconstruction task may be misaligned with retrieval needs
  - Performance collapse on OOD categories: Model may have overfitted to attribute distributions in training set
  - Sequential input performance not improving: Base MLLM lacks interleaved capability or training data isn't diverse enough
  - Dominance of one error type (e.g., Attribute Error): Reconstruction not effectively preserving corresponding modality's details

- **First 3 experiments:**
  1. Reproduce main ablation on MERIT: Train with only Lcl (baseline), then Lcl + Lmse (vision-only), then Lcl + Lmlm (language-only), finally Lcl + Lmse + Lmlm (full Coral). Compare R@1 to validate incremental gains.
  2. Validate "interleaved support" hypothesis: Take baseline MLLM, evaluate on MERIT with sequential vs. concatenated image inputs. Fine-tune on MERIT using Coral and re-evaluate. Confirm sequential input performance improves significantly.
  3. Test generalization on standard benchmark: Train Coral on MERIT and zero-shot evaluate on simpler established retrieval benchmark like CIRR or FashionIQ. Compare against baseline MLLM to verify improvements transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does co-training on single-image datasets alongside MERIT prevent degradation of sequential input processing capabilities observed in current MLLM retrievers?
- **Basis in paper:** Page 3 notes existing MLLMs lose ability to process interleaved inputs effectively after fine-tuning on single-image datasets, capability restored by training on MERIT.
- **Why unresolved:** Paper demonstrates MERIT restores sequential processing but doesn't explore if mixed-data training can preserve this capability without sacrificing single-image performance.
- **What evidence would resolve it:** Comparative analysis of sequential input performance for models trained on MERIT exclusively versus those trained on mixture of MERIT and standard single-image retrieval datasets.

### Open Question 2
- **Question:** Can retrieval architectures be specifically optimized to maintain robust performance as number of interleaved conditions scales beyond four?
- **Basis in paper:** Table 12 shows significant performance degradation as query conditions increase (e.g., R@5 dropping to 36.84% for four conditions), even for best models.
- **Why unresolved:** While Coral improves performance, authors don't propose specific mechanisms to handle combinatorial complexity of queries with more than three conditions.
- **What evidence would resolve it:** Evaluation of modified attention mechanisms or hierarchical encoding strategies on dataset extension featuring five or more simultaneous conditions.

### Open Question 3
- **Question:** To what extent do acknowledged category imbalances and potential visual-textual inconsistencies in MERIT influence model bias and retrieval fairness?
- **Basis in paper:** Appendix F.2 explicitly lists "imbalances in product categories" and "inconsistencies between visual attributes and textual descriptions" as inherent limitations of dataset.
- **Why unresolved:** Authors identify these issues as risks for bias propagation but don't quantify impact on fairness of retrieval outcomes across different product types.
- **What evidence would resolve it:** Bias audit measuring performance discrepancies across under-represented product categories and samples with high visual-textual entropy.

## Limitations

- Performance degrades significantly when query conditions increase beyond three elements, dropping to 36.84% R@5 for four conditions
- Limited generalization to truly unseen languages, with performance gaps between training languages and OOD languages
- Dataset contains category imbalances and potential visual-textual inconsistencies that may influence model bias

## Confidence

**High Confidence:** Ablation studies demonstrating effectiveness of embedding reconstruction are well-supported with incremental improvements from adding vision reconstruction, language reconstruction, and both together providing clear evidence.

**Medium Confidence:** 45.9% improvement claim is less certain due to lack of comparison with established published methods, though ablation studies show Coral components work well on MERIT.

**Low Confidence:** Zero-shot generalization results to other benchmarks are promising but based on single figure without detailed performance numbers or statistical significance.

## Next Checks

1. **External Benchmark Validation:** Evaluate Coral on established, independently created retrieval benchmarks like CIRR or FashionIQ using same training protocol. Compare performance against published state-of-the-art models to verify claimed 45.9% improvement across different evaluation settings.

2. **Language Generalization Stress Test:** Design experiment testing Coral on truly unseen languages (e.g., languages not in 5 training languages and not in 2 validation languages). This would better assess whether model learns language-agnostic retrieval capabilities or is memorizing language-specific patterns.

3. **Failure Mode Analysis on MERIT:** Systematically analyze retrieval failures on MERIT by category (attribute errors, visual errors, language errors). This would validate whether reconstruction mechanisms actually address specific failure modes identified in paper and provide insight into remaining limitations.