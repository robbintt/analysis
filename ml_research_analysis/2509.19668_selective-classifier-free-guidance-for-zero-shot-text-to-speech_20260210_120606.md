---
ver: rpa2
title: Selective Classifier-free Guidance for Zero-shot Text-to-speech
arxiv_id: '2509.19668'
source_url: https://arxiv.org/abs/2509.19668
tags:
- text
- speech
- input
- f5-tts
- cosyv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of classifier-free guidance
  (CFG) techniques from image generation to zero-shot text-to-speech (TTS). While
  CFG has been effective in image generation, the authors find that standard CFG strategies
  do not generally improve speech synthesis quality.
---

# Selective Classifier-free Guidance for Zero-shot Text-to-speech

## Quick Facts
- **arXiv ID:** 2509.19668
- **Source URL:** https://arxiv.org/abs/2509.19668
- **Authors:** John Zheng; Farhad Maleki
- **Reference count:** 0
- **Primary result:** Standard CFG doesn't improve TTS quality; selective CFG (standard early, speaker-only late) improves speaker similarity while limiting text degradation, with effectiveness depending on model and language.

## Executive Summary
This paper investigates classifier-free guidance (CFG) techniques from image generation applied to zero-shot text-to-speech (TTS). While CFG has been effective in image generation, the authors find it doesn't generally improve speech synthesis quality. They propose selective CFG strategies that apply standard CFG during early timesteps and switch to selective (speaker-only) CFG in later timesteps. This approach improves speaker similarity while limiting degradation of text adherence. Their results show effectiveness depends on both model architecture and language used, with F5-TTS showing improvements for English but not Mandarin, while CosyVoice 2 achieves improvements across languages.

## Method Summary
The authors propose three selective CFG strategies for zero-shot TTS: `input_text` (replaces unconditioned prediction with text-only conditioned prediction), `def_text` (uses standard CFG early then switches to `input_text`), and `input_audio` (replaces unconditioned with audio-only conditioned). These modify the standard CFG formulation to apply different guidance weights to text and speaker conditions at different timesteps. The strategies are evaluated on F5-TTS and CosyVoice 2 models using cosine scheduling with 32 and 10 steps respectively. The key insight is that text information is captured in early denoising steps, so later timesteps can focus on speaker identity without sacrificing text adherence.

## Key Results
- Standard CFG strategies do not generally improve speech synthesis quality
- Selective CFG (standard early, speaker-only late) improves speaker similarity while limiting text adherence degradation
- Effectiveness depends on both model and language: F5-TTS shows SIM improvement with selective CFG on English but not Mandarin
- CosyVoice 2 achieves higher speaker similarity without degradation in text adherence regardless of language

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Applying standard CFG during early timesteps and switching to selective (speaker-only) CFG in later timesteps improves speaker similarity while limiting text adherence degradation.
- **Mechanism:** In flow matching, text information is captured in the first few denoising steps. The authors observe that words become audible around t ≈ 0.04 (roughly 6 timesteps in F5-TTS). Once text structure is established, further CFG emphasis on text becomes unnecessary and can be redirected toward speaker identity.
- **Core assumption:** Text and speaker conditioning operate on separable timescales, with text structure emerging earlier in the denoising trajectory.
- **Evidence anchors:** [Section 4] "We observe that the words become audible very quickly, at around 6 timesteps with t ≈ 0.04. We hypothesize that CFG for text adherence may not be necessary after the initial steps."

### Mechanism 2
- **Claim:** Separated-condition CFG allows independent control over speaker similarity and text adherence, but effectiveness is architecture-dependent.
- **Mechanism:** Instead of a single CFG weight λ for all conditions, separate weights (λ_text, λ_spk) scale the contribution of each condition's guidance term. Setting λ_text = 1 while increasing λ_spk emphasizes speaker identity without over-amplifying text conditioning.
- **Core assumption:** The model's conditioning pathways for text and speaker are sufficiently decoupled that independent scaling produces meaningful trade-offs.
- **Evidence anchors:** [Section 2.5] MegaTTS 3, VoiceLDM, and DualSpeech all use separated-condition CFG with formulation: `ϵ + λ_text(ϵ_text - ϵ) + λ_spk(ϵ_spk,ϵ_text - ϵ_text)`

### Mechanism 3
- **Claim:** CFG strategy effectiveness varies with text representation architecture and language.
- **Mechanism:** CosyVoice 2 uses a 506M-parameter LLM (Qwen2.5-0.5B) to generate semantic tokens, providing robust text adherence even without CFG. F5-TTS uses a 4.3M-parameter ConvNeXt v2 module, which may cause English and Mandarin to behave as effectively different conditioning modalities.
- **Core assumption:** Richer text representations reduce reliance on CFG for text adherence, making selective speaker-emphasis strategies safer.
- **Evidence anchors:** [Section 6] "CosyVoice 2 uses a 506 million–parameter LLM, so generated semantic tokens allow the model to achieve strong text adherence even without CFG."

## Foundational Learning

- **Flow Matching (OT variant):**
  - Why needed here: Understanding how ODE-based denoising from x₀ (noise) to x₁ (speech) works is essential for reasoning about when guidance should be applied.
  - Quick check question: At what timestep t does the model receive x_t = x₀ + t(x₁ - x₀), and what does it predict?

- **Classifier-Free Guidance formulation:**
  - Why needed here: All strategies in the paper modify the basic CFG equation; you must understand the baseline to interpret extensions.
  - Quick check question: Write the CFG-modified prediction: `ϵ(x_t, c) → ?`

- **Zero-shot TTS metrics (SIM and WER):**
  - Why needed here: The paper's central trade-off is between speaker similarity (SIM) and text correctness (WER); you need to know what these measure.
  - Quick check question: Why is cosine similarity between speaker embeddings used for SIM rather than raw audio comparison?

## Architecture Onboarding

- **Component map:**
  ```
  F5-TTS: Audio + Text → [ConvNeXt v2 text embedder, 4.3M] → [DiT backbone, 332M] → mel-spectrogram → vocoder → audio
  
  CosyVoice 2: Text → [Qwen2.5-0.5B LLM] → semantic tokens
               Audio → [speaker encoder] → speaker embedding
               [tokens + embedding] → [flow matching model, 71M] → mel → vocoder → audio
  ```

- **Critical path:**
  1. Identify which model architecture you're using (LLM-based vs. pure DiT)
  2. Check if the model was trained with CFG dropouts (F5-TTS: yes, supports conditional/text-only/unconditional)
  3. Determine target language and whether text encoder is robust for it
  4. Apply `def_text` strategy with t_threshold = 0.08 if using F5-TTS on English

- **Design tradeoffs:**
  - More inference steps → higher quality but slower; F5-TTS uses 32, CosyVoice 2 uses 10
  - Higher λ → stronger conditioning but risk of artifacts; authors find λ ≈ 2-3 works for speaker emphasis
  - Earlier t_threshold → more text emphasis, less speaker gain; later → more speaker gain, potential WER increase

- **Failure signatures:**
  - SIM improves but WER spikes → λ_spk too high or t_threshold too late
  - No SIM improvement on non-English → text encoder may not support language well; check training data
  - Quality degrades at high CFG → early-timestep CFG weights are too aggressive (Figure 1)

- **First 3 experiments:**
  1. Reproduce baseline CFG sweep (λ = 1.5 to 3.5) on your model and dataset; plot SIM vs. WER to establish your frontier.
  2. Implement `input_text` condition (set λ_text = 1, vary λ_spk); confirm trade-off curve shifts toward higher SIM.
  3. Implement `def_text` with t_threshold = 0.08; compare SIM/WER to `input_text` at same λ_spk to quantify WER reduction.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of selective CFG strategies appears highly dependent on both model architecture and language, with F5-TTS showing improvements only for English while CosyVoice 2 works across languages.
- The "updated" F5-TTS checkpoint achieving 0.676 SIM on LibriSpeech is not publicly available, making exact reproduction difficult.
- While the paper demonstrates selective CFG works better than standard CFG, it doesn't explore the full design space of how different t_threshold values or λ schedules might perform.

## Confidence

- **High confidence:** The mechanism that text structure emerges early in denoising (t ≈ 0.04) is well-supported by observations and prior work on timestep-dependent effects.
- **Medium confidence:** The separated-condition CFG formulation is sound and aligns with concurrent work, but effectiveness varies significantly by model.
- **Medium confidence:** The architecture-dependent findings (LLM vs. ConvNeXt v2 text encoders) are plausible given known differences in representation quality.
- **Low confidence:** The paper's recommendation to use t_threshold = 0.08 is somewhat arbitrary without systematic exploration of the parameter space.

## Next Checks

1. Implement the `def_text` strategy with varying t_threshold values (0.04, 0.08, 0.12) on F5-TTS English to determine if the specific choice of 0.08 is optimal or if earlier switching provides better text adherence preservation.

2. Test the selective CFG strategies on additional languages (e.g., Spanish, French) with both F5-TTS and CosyVoice 2 to better understand the language dependency patterns and whether they correlate with specific text encoder architectures.

3. Compare the computational overhead of selective CFG versus standard CFG across different step counts (10, 20, 32 steps) to quantify the trade-off between quality improvement and inference efficiency.