---
ver: rpa2
title: 'LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token Times
  and Network Traffic Analysis'
arxiv_id: '2502.20589'
source_url: https://arxiv.org/abs/2502.20589
tags:
- network
- different
- language
- data
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel passive fingerprinting technique
  that identifies language models by analyzing the temporal patterns of their token
  generation process, captured through inter-token times (ITTs) in network traffic.
  The method leverages the autoregressive nature of language models, where each token
  is generated sequentially, creating a unique rhythmic signature.
---

# LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token Times and Network Traffic Analysis

## Quick Facts
- arXiv ID: 2502.20589
- Source URL: https://arxiv.org/abs/2502.20589
- Reference count: 40
- Primary result: Novel passive fingerprinting technique identifies 26 models (16 SLMs, 10 LLMs) with 85% accuracy using inter-token timing patterns in encrypted network traffic.

## Executive Summary
This paper introduces a novel passive fingerprinting technique that identifies language models by analyzing the temporal patterns of their token generation process, captured through inter-token times (ITTs) in network traffic. The method leverages the autoregressive nature of language models, where each token is generated sequentially, creating a unique rhythmic signature. A deep learning pipeline extracts 36 engineered features from network traffic and employs a hybrid BiLSTM-attention model to classify models. The technique is tested on 16 open-source small language models (SLMs) and 10 proprietary large language models (LLMs) across various deployment scenarios, including local, LAN, remote, and VPN. Results show high accuracy in identifying model families and variants, even under challenging network conditions, demonstrating the method's robustness and potential for real-world applications.

## Method Summary
The method captures network traffic containing LLM responses, extracts packet inter-arrival times and sizes, then computes 36 engineered features from sliding windows (0.5s size, 0.1s step) including burst rates, inter-arrival time statistics, entropy measures, and size-time correlations. These features are fed into a hybrid BiLSTM-attention neural network (three BiLSTM layers with residual connections and multi-head attention) to classify the model generating the response. The approach works passively without needing access to model internals or response content, relying solely on observable timing patterns that persist through encryption and network transmission.

## Key Results
- Achieves 85% weighted F1 score for cross-day classification on same network, 74% for different networks, and 71% with VPN
- Successfully distinguishes between model families (Phi3 vs Phi3.5, GPT-4 vs GPT-4o) and individual model variants
- Raw inter-arrival times alone yield near-random performance; 36 engineered features are essential for generalization
- Model families show distinct timing signatures that correlate with parameter count and architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Autoregressive language models produce distinctive inter-token timing patterns that persist as observable "rhythms" even after network transmission.
- **Mechanism:** Each token generation depends on all prior tokens; the computational path length varies with model architecture, parameter count, and hardware optimization. This creates consistent timing signatures characterized by latency baselines, periodicity, jitter patterns, and spike distributions.
- **Core assumption:** Model-specific timing patterns are sufficiently stable and distinguishable to survive packetization, encryption, and network variability.
- **Evidence anchors:**
  - [abstract]: "...generate text one token at a time based on all previously generated tokens, creating a unique temporal pattern–like a rhythm or heartbeat–that persists even when the output is streamed over a network."
  - [Section VI-A]: Figure 1 shows six SLMs on the same GPU with identical prompts exhibit distinct ITT profiles; Figure 5 shows mean ITT correlates with model size within families.
  - [corpus]: "Intrinsic Fingerprint of LLMs" supports the premise that LLMs have inherent identifiable characteristics, though focused on training dynamics rather than timing.
- **Break condition:** If adversaries intentionally introduce random delays or batch tokens with variable buffering at the server side, the timing signal could be degraded beyond recognition.

### Mechanism 2
- **Claim:** Raw inter-arrival times are insufficient for reliable classification under realistic network conditions; 36 engineered features are required to extract robust signatures.
- **Mechanism:** Sliding windows (0.5s, step 0.1s) compute rate metrics (burst rate, packet rate), inter-arrival statistics (mean, percentiles, entropy), pattern regularity measures (permutation entropy, timing regularity), and correlation features (size-time correlation). These aggregate higher-order patterns that survive network noise.
- **Core assumption:** The feature set captures model-intrinsic timing structure while filtering network-induced variability; the features generalize across different network paths.
- **Evidence anchors:**
  - [Section VI-A]: "Fig. 10 demonstrates that the DL model is overfitting the training data and fails to generalize... raw network traffic features are insufficient."
  - [Section V-D]: "Using an iterative empirical process... we extract a total of 36 engineered features... that focus on metrics revealing the model's token generation behavior."
  - [corpus]: Weak direct evidence for this specific feature set for LLMs; network traffic classification literature [36-38 cited] supports feature engineering approaches for encrypted traffic.
- **Break condition:** If network jitter variance exceeds the timing signal magnitude by orders of magnitude, no feature set may recover the fingerprint without per-network calibration.

### Mechanism 3
- **Claim:** A hybrid BiLSTM-attention architecture can classify models from feature sequences, generalizing across temporal and network variations.
- **Mechanism:** Three stacked BiLSTM blocks (128→64→32 units) capture bidirectional temporal dependencies; multi-head attention (8 heads, key_dim=128) focuses on discriminative timesteps; residual connections preserve gradients; focal loss handles class imbalance.
- **Core assumption:** The sequential dependencies in feature vectors are learnable and transfer across days, networks, and VPN routing without per-condition retraining.
- **Evidence anchors:**
  - [Section V-E]: Architecture diagram (Figure 4) and Algorithm 3 detail the BiLSTM-attention-residual design.
  - [Section VI-B]: Weighted F1 scores of 85% (different day), 74% (different network), 71% (VPN) demonstrate cross-condition generalization.
  - [corpus]: Prior work [30,31 cited] validates BiLSTM-attention for encrypted traffic classification, though not specifically for LLM timing.
- **Break condition:** If training data comes from only one network topology and test data has fundamentally different latency distributions (e.g., satellite links), the model may fail without domain adaptation.

## Foundational Learning

- **Concept: Autoregressive generation and tokenization**
  - **Why needed here:** The entire method hinges on understanding that LLMs generate tokens sequentially, with each prediction depending on prior context; the timing of each step reflects model computation.
  - **Quick check question:** Can you explain why a larger model (e.g., 9B vs 2B parameters) would produce longer inter-token times on the same hardware?

- **Concept: Network traffic analysis under encryption**
  - **Why needed here:** The method extracts features from packet inter-arrival times and sizes without decrypting payloads; understanding what information remains observable in encrypted streams is essential.
  - **Quick check question:** What two raw measurements can an observer extract from encrypted packet streams, and why is packetization a confounding factor?

- **Concept: BiLSTM with attention for sequential classification**
  - **Why needed here:** The classifier must learn temporal patterns in feature sequences; BiLSTM captures context in both directions, while attention highlights the most informative windows.
  - **Quick check question:** Why would a residual connection after the attention block help training stability in this architecture?

## Architecture Onboarding

- **Component map:** Data collection (tshark/Wireshark) -> Preprocessing (ITT computation, sliding windows) -> Feature engineering (36 features) -> Model (BiLSTM-Attention) -> Training (focal loss, Adam) -> Inference (sliding window voting)

- **Critical path:** Data collection quality -> feature extraction correctness -> model training convergence -> cross-condition generalization. A bug in timestamp extraction or window alignment will cascade through the entire pipeline.

- **Design tradeoffs:**
  - **Window size (0.5s):** Larger windows smooth noise but reduce temporal resolution; smaller windows increase granularity but amplify noise. The paper empirically selected 0.5s.
  - **Feature set (36 features):** More features capture more signal but risk overfitting; fewer features may miss discriminative patterns. The paper iteratively selected these.
  - **Training data diversity:** Training on single network/day yields higher same-condition accuracy but poor generalization; multi-condition training may trade peak accuracy for robustness (not explicitly tested in paper).
  - **Assumption:** The paper trains and tests separately per scenario; a unified model trained on mixed conditions might behave differently.

- **Failure signatures:**
  - **Overfitting to training network:** High training accuracy, near-random validation on different network; seen with raw data (Figure 10).
  - **Confusion within model families:** Misclassification between Phi3 and Phi3.5 or ChatGPT-4 and ChatGPT-4o; indicates architectural similarity dominates timing differences.
  - **VPN degradation:** Recall drops for some models (e.g., ChatGPT-4o Mini) under VPN; indicates additional routing variability obscures timing patterns.

- **First 3 experiments:**
  1. **Local baseline replication:** Deploy 3–5 SLMs locally on GPU, collect ITTs using the Ollama `created_at` field, plot mean ITT and distribution per model. Verify that models show distinct timing profiles on identical prompts.
  2. **LAN feature extraction pipeline:** Set up client-server on same LAN, capture packets with `tshark`, implement the 36-feature extraction on sliding windows, train a simple classifier (e.g., random forest) to verify feature discriminability before deep learning.
  3. **Cross-day robustness test:** Collect proprietary LLM traffic on Day 1 for training, Day 2 for testing. Train the BiLSTM-attention model and report per-model precision/recall. Compare to paper's ~85% weighted F1 benchmark to validate implementation.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but leaves several critical issues unaddressed, particularly around adversarial robustness and cross-hardware generalization.

## Limitations

- **Hardware dependency:** ITT patterns vary significantly across different hardware configurations (GPU vs CPU, different GPU models), potentially entangling hardware fingerprinting with model identification.
- **No adversarial testing:** The method's robustness against deliberate timing obfuscation techniques like artificial delays or response batching remains completely unexplored.
- **Proprietary dataset opacity:** Limited disclosure about the proprietary LLM dataset restricts reproducibility and external validation of the findings.

## Confidence

- **High confidence** in the core mechanism: The autoregressive generation process creates distinguishable timing patterns, supported by direct evidence showing different models exhibit unique ITT profiles on identical hardware and prompts.
- **Medium confidence** in feature engineering efficacy: While the 36-feature approach demonstrably improves generalization over raw data, the specific feature selection appears somewhat arbitrary and lacks comparison to alternative feature sets or simpler models.
- **Medium confidence** in cross-condition generalization: Weighted F1 scores of 85% (same network, different day), 74% (different network), and 71% (VPN) show reasonable robustness, but performance degrades predictably under network variability, and the model may not generalize to fundamentally different network conditions like satellite links.
- **Low confidence** in adversarial robustness: The paper does not evaluate whether timing signatures persist under deliberate obfuscation techniques such as variable response batching, artificial delays, or traffic shaping, leaving a significant gap in real-world applicability.

## Next Checks

1. **Hardware dependency validation**: Deploy the same set of SLMs across three different hardware configurations (high-end GPU, low-end GPU, CPU-only) and measure ITT distributions for identical prompts. Train and test classifiers within each hardware setup and across hardware boundaries to quantify the cross-platform performance degradation, confirming whether hardware fingerprinting becomes entangled with model fingerprinting.

2. **Adversarial robustness test**: Implement three basic obfuscation techniques—random delay injection (Gaussian noise added to token emission times), response batching (tokens grouped and sent together), and traffic padding (artificial packets to mask timing patterns). Apply these to captured LLM traffic and re-run the classification pipeline to measure accuracy degradation, determining whether timing patterns can be effectively obscured without breaking the model's functionality.

3. **Alternative model comparison**: Train and evaluate three different classification approaches on the same feature dataset: (a) the proposed BiLSTM-attention model, (b) a gradient-boosted decision tree (XGBoost), and (c) a simple logistic regression on aggregated features. Compare not just overall accuracy but also per-model precision/recall, training time, and inference latency to determine whether the complex deep learning approach provides meaningful advantages over simpler, more interpretable methods.