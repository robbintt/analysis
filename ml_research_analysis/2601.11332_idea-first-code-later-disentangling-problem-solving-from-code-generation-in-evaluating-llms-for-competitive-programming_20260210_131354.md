---
ver: rpa2
title: 'Idea First, Code Later: Disentangling Problem Solving from Code Generation
  in Evaluating LLMs for Competitive Programming'
arxiv_id: '2601.11332'
source_url: https://arxiv.org/abs/2601.11332
tags:
- editorial
- problem
- gold
- editorials
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Idea First, Code Later: Disentangling Problem Solving from Code Generation in Evaluating LLMs for Competitive Programming

## Quick Facts
- arXiv ID: 2601.11332
- Source URL: https://arxiv.org/abs/2601.11332
- Reference count: 40
- Primary result: Introduces a novel evaluation framework that separates problem-solving (idea generation) from code generation in competitive programming tasks

## Executive Summary
This paper addresses a critical gap in evaluating large language models (LLMs) for competitive programming by proposing a framework that disentangles problem-solving from code generation. The authors argue that current evaluation practices conflate distinct skills, making it difficult to identify specific weaknesses in model performance. By separating idea generation from implementation, the framework provides a more granular understanding of model capabilities and limitations.

## Method Summary
The authors propose a two-stage evaluation framework where human experts first solve competitive programming problems and extract solution ideas, which are then used as ground truth for automated evaluation. In the first stage, models generate ideas which are compared against human-annotated solutions. In the second stage, models implement code based on the provided ideas. This approach allows for separate assessment of problem-solving and coding abilities, revealing distinct failure patterns in each component.

## Key Results
- Significant performance gaps between idea generation and code implementation across tested models
- Models show better alignment with human solution ideas than with final code outputs
- The disentangled evaluation reveals specific failure modes that are masked in traditional end-to-end assessments

## Why This Works (Mechanism)
The framework works by breaking down the complex task of competitive programming into its constituent cognitive components. Problem-solving requires abstract reasoning, algorithmic thinking, and strategy formulation, while code generation demands syntactic precision, language-specific knowledge, and implementation skills. By evaluating these components separately, the framework captures distinct skill profiles and provides more actionable insights for model improvement.

## Foundational Learning
- **Solution annotation process**: Human experts solve problems and extract key solution steps - needed to establish ground truth for idea evaluation
- **Semantic similarity metrics**: Used to compare model-generated ideas against human solutions - needed for automated idea quality assessment
- **Code execution testing**: Validates implemented solutions against test cases - needed to verify functional correctness
- **Multi-stage evaluation pipeline**: Separates idea generation from implementation - needed to isolate different cognitive components
- **Benchmark problem curation**: Selection of diverse competitive programming problems - needed to ensure comprehensive evaluation coverage
- **Performance gap analysis**: Statistical comparison between idea and code performance - needed to quantify the disentanglement effect

## Architecture Onboarding
**Component Map**: Problem Input -> Idea Generation -> Idea Evaluation -> Code Generation -> Code Evaluation
**Critical Path**: The idea generation and evaluation stages are critical as they establish whether the model can formulate correct solutions before implementation
**Design Tradeoffs**: Human annotation provides high-quality ground truth but introduces scalability constraints; automated evaluation enables broader testing but may miss nuanced solution aspects
**Failure Signatures**: 
- Idea generation failures indicate reasoning deficits
- Code generation failures suggest implementation or language-specific issues
- Combined failures point to fundamental problem-solving gaps
**First Experiments**:
1. Run both stages on a small problem set to validate annotation quality
2. Compare model performance using traditional vs. disentangled evaluation on identical problems
3. Test the framework with a new model to establish baseline performance patterns

## Open Questions the Paper Calls Out
The paper acknowledges that human-annotated solution ideas may introduce subjectivity in evaluation. It also notes that the framework's focus on competitive programming may limit generalizability to broader software engineering tasks. Additionally, the study recognizes that real-world problem-solving scenarios involve evolving requirements that may not be captured in static competitive programming problems.

## Limitations
- Human-annotated solutions may introduce subjective bias in evaluation
- Focus on competitive programming may limit generalizability to real-world software development
- Static problem sets may not reflect dynamic requirements common in practical scenarios
- Sample size of problems and models may affect statistical significance

## Confidence
- High confidence: The experimental design for disentangling problem-solving from code generation is sound and well-documented
- Medium confidence: The conclusions about model performance differences between idea generation and code implementation tasks
- Medium confidence: The assertion that current evaluation practices conflate distinct skills

## Next Checks
1. Test the disentangled evaluation framework on a larger and more diverse set of competitive programming problems to assess robustness
2. Compare the framework's effectiveness against traditional end-to-end evaluation methods using statistical significance testing
3. Apply the same evaluation approach to non-competitive programming coding tasks to validate generalizability beyond algorithmic problem-solving contexts