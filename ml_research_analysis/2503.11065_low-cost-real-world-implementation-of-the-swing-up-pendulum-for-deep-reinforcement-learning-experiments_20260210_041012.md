---
ver: rpa2
title: Low-cost Real-world Implementation of the Swing-up Pendulum for Deep Reinforcement
  Learning Experiments
arxiv_id: '2503.11065'
source_url: https://arxiv.org/abs/2503.11065
tags:
- servo
- pendulum
- apparatus
- real-world
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a low-cost inverted pendulum apparatus and
  software environment designed to bridge the "sim-to-real" gap in deep reinforcement
  learning. The physical system uses readily available components including a hobby
  servo, rotary encoder, ESP32 microcontroller, and 3D printed parts, enabling detailed
  study of real-world delays and imperfections in sensing, communication, learning,
  and actuation.
---

# Low-cost Real-world Implementation of the Swing-up Pendulum for Deep Reinforcement Learning Experiments

## Quick Facts
- arXiv ID: 2503.11065
- Source URL: https://arxiv.org/abs/2503.11065
- Reference count: 5
- Low-cost inverted pendulum apparatus designed to study sim-to-real gap in DRL

## Executive Summary
This paper presents a low-cost inverted pendulum apparatus designed to bridge the "sim-to-real" gap in deep reinforcement learning. The physical system uses readily available components including a hobby servo, rotary encoder, ESP32 microcontroller, and 3D printed parts, enabling detailed study of real-world delays and imperfections in sensing, communication, learning, and actuation. The system is complemented by a high-fidelity simulated environment using MuJoCo. Experiments show that standard DRL algorithms (DQN, TD3) struggle to solve the real-world environment due to noise and delays, while succeeding in simulation, demonstrating the sim-to-real gap. A dimension reduction technique using GRU preprocessing (R-DQN, R-TD3) improves performance on the real system. The apparatus serves as an accessible educational platform for teaching control theory and reinforcement learning while providing a testbed for developing sim-to-real transfer techniques.

## Method Summary
The authors developed a low-cost inverted pendulum system using hobby servo motors, rotary encoders, ESP32 microcontrollers, and 3D printed components. They created a complementary MuJoCo simulation environment with identical control parameters. The system captures real-world imperfections including sensor noise, communication delays, and actuation latency. The experimental setup includes both simulation and physical implementations of DQN and TD3 algorithms, with the physical system using a GRU-based dimension reduction preprocessing step (R-DQN, R-TD3) to handle noisy, high-dimensional sensor data. The pendulum starts hanging downward and must be swung up and balanced at the upright position, with rewards based on angular position and velocity.

## Key Results
- Standard DRL algorithms (DQN, TD3) successfully solve the simulated pendulum but fail on the real-world system due to noise and delays
- GRU-based dimension reduction preprocessing (R-DQN, R-TD3) significantly improves learning performance on the physical system
- The apparatus demonstrates clear sim-to-real gap, validating its use as a benchmark for transfer learning techniques
- The low-cost design enables accessible experimentation with real-world DRL challenges

## Why This Works (Mechanism)
The apparatus works by providing a controlled environment where the differences between simulation and reality can be systematically studied. The GRU-based dimension reduction addresses the high-dimensional, noisy sensor data from the physical system by learning temporal patterns and extracting relevant features, effectively filtering out noise while preserving critical information for control decisions. This preprocessing step helps bridge the sim-to-real gap by making the state representation more robust to the imperfections inherent in real-world sensing and actuation.

## Foundational Learning

**Reinforcement Learning Basics**
- Why needed: Fundamental framework for training agents through reward-based learning
- Quick check: Understand state-action-reward cycles and policy optimization

**Sim-to-Real Transfer**
- Why needed: Core challenge of applying simulation-trained policies to physical systems
- Quick check: Recognize how simulation simplifications fail to capture real-world dynamics

**GRU Neural Networks**
- Why needed: Handles sequential data and temporal dependencies in noisy sensor streams
- Quick check: Understand gating mechanisms and memory retention in sequence modeling

**Control Theory**
- Why needed: Provides foundation for understanding pendulum dynamics and stability
- Quick check: Grasp concepts of equilibrium points and stability regions

**Domain Randomization**
- Why needed: Technique for improving sim-to-real transfer through diverse training
- Quick check: Know how varying simulation parameters builds robust policies

## Architecture Onboarding

**Component Map**
ESP32 Microcontroller -> Rotary Encoder -> Servo Motor -> 3D Printed Pendulum Arm -> Physical Environment
Simulation Environment (MuJoCo) -> DQN/TD3 Agent -> GRU Preprocessing (for real system)

**Critical Path**
Sensor data acquisition (encoder) → ESP32 processing → Communication delay → Agent decision → Actuation command → Servo response → Physical pendulum movement

**Design Tradeoffs**
- Cost vs. precision: Hobby components provide affordability but introduce noise
- Computational power vs. latency: ESP32 limits algorithm complexity but reduces delay
- Physical size vs. stability: Compact design enables accessibility but challenges control

**Failure Signatures**
- Persistent oscillation around unstable equilibrium
- Failure to reach upright position within timeout
- High variance in learned policies across training runs
- Degradation in performance when environmental conditions change

**First Experiments**
1. Verify pendulum dynamics by manually swinging and measuring response time
2. Test sensor noise levels by recording encoder readings at fixed positions
3. Validate communication latency by measuring round-trip time from ESP32 to host

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Results based on single prototype setup without validation across multiple physical systems
- GRU-based dimension reduction effectiveness may be specific to particular noise characteristics of this system
- Limited exploration of alternative domain adaptation techniques for comparison

## Confidence

**High Confidence Claims:**
- Standard DRL algorithms struggle with real-world implementation due to noise and delays

**Medium Confidence Claims:**
- GRU-based dimension reduction technique improves real-world learning performance

## Next Checks
1. Test the GRU-based dimension reduction technique across multiple physical pendulum prototypes with varying levels of sensor noise and communication delays to assess robustness and generalizability.

2. Compare the proposed approach with alternative domain adaptation techniques (such as domain randomization or dynamics randomization) to establish relative performance benefits.

3. Implement and validate the system using different microcontroller platforms and communication protocols to evaluate the impact of computational latency and hardware constraints on learning performance.