---
ver: rpa2
title: 'OSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image'
arxiv_id: '2601.07333'
source_url: https://arxiv.org/abs/2601.07333
tags:
- object
- retrieval
- pose
- image
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OSCAR introduces a training-free method for retrieving CAD models
  from an unlabeled 3D object database using a single image and a language prompt.
  It employs a two-stage retrieval process: first filtering candidates using CLIP-based
  text embeddings, then refining the selection using DINOv2-based image embeddings.'
---

# OSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image

## Quick Facts
- arXiv ID: 2601.07333
- Source URL: https://arxiv.org/abs/2601.07333
- Authors: Tessa Pulli; Jean-Baptiste Weibel; Peter Hönig; Matthias Hirschmanner; Markus Vincze; Andreas Holzinger
- Reference count: 18
- Primary result: 31.8x faster than 3D reconstruction pipelines during onboarding while achieving state-of-the-art retrieval accuracy

## Executive Summary
OSCAR introduces a training-free method for retrieving CAD models from an unlabeled 3D object database using a single image and a language prompt. It employs a two-stage retrieval process: first filtering candidates using CLIP-based text embeddings, then refining the selection using DINOv2-based image embeddings. This approach enables effective open-set 3D model retrieval without requiring manual labeling or prior model-specific training. OSCAR achieves state-of-the-art performance on the MI3DOR benchmark and demonstrates practical applicability by improving 6D object pose estimation accuracy when integrated with MegaPose, outperforming reconstruction-based methods.

## Method Summary
OSCAR's method consists of an onboarding pipeline that renders 8 views per CAD model at ±15° elevation, captions each view using LLaVA-v1.5-7B, and stores rendered images, captions, and embeddings. During inference, GroundedSAM segments the ROI from the input image, CLIP encodes both the ROI image and stored captions for text-based filtering (threshold τ_text=0.37), then DINOv2 refines the candidate set by comparing visual similarity across views. The best matching CAD model is returned for use in downstream pose estimation.

## Key Results
- Achieves 0.894 NN score on MI3DOR benchmark, surpassing prior SOTA of 0.841
- 31.8x faster onboarding compared to traditional 3D reconstruction pipelines
- Improves 6D pose estimation accuracy when integrated with MegaPose over reconstruction-based approaches
- Maintains high accuracy across YCB-V, Housecat6D, and YCB-V&GSO datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal CLIP filtering reduces candidate set size while preserving semantically relevant matches.
- Mechanism: CLIP's joint vision-language embedding space enables direct comparison between the ROI image embedding and text embeddings of rendered view captions. Cosine similarity identifies objects matching the query's semantic content before expensive visual matching.
- Core assumption: Text descriptions generated during onboarding capture sufficient semantic information to distinguish object categories; CLIP cross-modal alignment holds for synthetic renders.
- Evidence anchors: [abstract] "text-based filtering using CLIP identifies candidate models"; [section 3.2] "simtext(si) = max_j ⟨qCLIP, tij⟩ / (∥qCLIP∥ ∥tij∥)" with threshold τ_text = 0.37; [corpus] Related work on vision-language embeddings for robotics (Frering et al., 2025) supports cross-modal retrieval utility.
- Break condition: Caption quality degrades (vague descriptions) or domain gap between real images and synthetic renders exceeds CLIP's generalization capacity.

### Mechanism 2
- Claim: DINOv2 self-supervised features capture fine-grained visual similarity that CLIP misses, improving top-1 retrieval accuracy.
- Mechanism: DINOv2 learns dense visual representations without language supervision, encoding texture, shape, and geometric details. Image-to-image comparison between ROI and rendered views selects the most visually similar candidate from the filtered set.
- Core assumption: DINOv2 features generalize across the domain gap between real photographs and synthetic CAD renders.
- Evidence anchors: [abstract] "image-based refinement using DINOv2 to select the most visually similar object"; [section 5.3.1, Table 3] DINOv2 achieves 77.38% mAP@1 on YCB-V vs. CLIP's 69.53% (segmentation), confirming superior top-1 precision; [corpus] HIPPo (arXiv:2502.10606) also leverages image-to-3D priors for model-free pose estimation.
- Break condition: Objects with similar textures but different categories pass CLIP filtering; DINOv2 cannot distinguish without semantic context.

### Mechanism 3
- Claim: Two-stage cascaded retrieval balances semantic alignment and visual precision better than single-modality approaches.
- Mechanism: CLIP's language-aligned embeddings provide coarse semantic filtering (preventing category mismatches), while DINOv2's visual features refine selection with fine-grained similarity. This addresses the failure mode where image-only methods match visually similar but semantically wrong objects.
- Core assumption: Semantic and visual features provide complementary information; errors from each modality do not fully overlap.
- Evidence anchors: [section 3.2] "image embeddings may match entirely different categories... we propose a sanity check in which CLIP text embeddings are first used to filter plausible candidates"; [Table 1] OSCAR achieves 0.894 NN score on MI3DOR vs. prior SOTA 0.841 (S2Mix); [corpus] Related work on feature fusion (GVCNN, MVTN) shows multi-stage aggregation benefits 3D retrieval.
- Break condition: Threshold τ_text = 0.37 is poorly calibrated for target domain; fallback to top-k candidates introduces noise.

## Foundational Learning

- Concept: **CLIP Vision-Language Embedding Space**
  - Why needed here: Understanding how CLIP maps images and text to a shared d-dimensional space enables reasoning about cross-modal cosine similarity for retrieval filtering.
  - Quick check question: Can you explain why CLIP can compare an image embedding to a text embedding directly, and what property of the training objective ensures this?

- Concept: **Self-Supervised Visual Representation Learning (DINOv2)**
  - Why needed here: DINOv2 provides visual features without language supervision, capturing fine-grained geometric and texture details that CLIP's language-aligned space may not preserve.
  - Quick check question: What is the difference between self-supervised features (DINOv2) and vision-language features (CLIP) in terms of what information they prioritize?

- Concept: **Multi-View 3D Rendering for Retrieval**
  - Why needed here: Converting 3D CAD models to 2D views enables using 2D image encoders; view selection (8 views at ±15° elevation) affects coverage and retrieval quality.
  - Quick check question: Why might 8 views be sufficient, and what tradeoffs exist between more views vs. computational cost during onboarding?

## Architecture Onboarding

- Component map:
  - Onboarding Pipeline: 3D CAD model -> Multi-view renderer (K=8 views) -> LLaVA-v1.5-7B captioner -> Store (rendered images, text captions, CLIP text embeddings, DINOv2 image embeddings)
  - Inference Pipeline: Input (RGB image + language prompt) -> GroundedSAM (ROI detection/segmentation) -> CLIP image encoder (ROI embedding) -> Stage 1: CLIP text filtering (cosine similarity ≥ 0.37) -> Stage 2: DINOv2 image refinement (max similarity across views) -> Return best CAD model

- Critical path:
  1. GroundedSAM segmentation quality directly impacts ROI embedding quality
  2. Caption quality (LLaVA prompts) determines CLIP filtering effectiveness—use "blind" or "attributes" prompts, not "comma" or "caption" (Table 4)
  3. Threshold calibration (τ_text = 0.37) is dataset-dependent—re-tune for new domains (Figure 7)

- Design tradeoffs:
  - Segmentation vs. Bounding Box: Segmentation improves CLIP retrieval (+10% mAP@1 on YCB-V) but bounding box sometimes better on similar-looking objects (HCat6D results in Table 3)
  - Top-k fallback: If no candidates pass threshold, top-10 candidates optimal; larger k introduces noise (Figure 8)
  - Rendering viewpoints: 8 views balance coverage and storage; more views improve recall but increase onboarding time

- Failure signatures:
  - CLIP filtering returns empty candidate set → threshold too high for domain; falls back to top-k
  - Wrong category retrieved → image-only matching without semantic filter; ensure CLIP stage runs first
  - MegaPose fails on retrieved model → retrieved model geometry differs significantly from query object; consider reconstruction fallback

- First 3 experiments:
  1. Validate onboarding pipeline on small subset: Render 10 CAD models with 8 views each, generate captions with all 4 prompt types, verify caption quality manually before scaling.
  2. Calibrate CLIP threshold on validation set: Compute simtext scores for known matches vs. non-matches; plot accuracy vs. threshold (following Figure 7 methodology) to find optimal τ_text for your domain.
  3. Ablate segmentation vs. bounding box on target dataset: Compare retrieval mAP@1, mAP@10 using GroundedSAM segmentation vs. bounding box extraction to determine optimal ROI extraction for your object types.

## Open Questions the Paper Calls Out
- How can OSCAR be tightly integrated into a continuous 6D pose estimation framework to allow for dynamic model updates or feedback loops? The authors state in the conclusion: "Future work will aim to embed OSCAR in a pose estimation framework."
- Does the two-stage retrieval pipeline maintain low latency when applied to industrial-scale databases (e.g., >100k models) given the computational cost of DINOv2 embeddings? The efficiency claim (31.8x faster than reconstruction) relies on a relatively small combined dataset (YCB-V + GSO); scalability of the image-based refinement stage is not tested.
- Is the selected text-filtering threshold (τ_text = 0.37) robust to domain shifts, or does it require per-dataset tuning to maximize Average Precision? Figure 7 shows distinct optimal peaks for YCB-V (approx. 0.38) vs. Housecat6D (approx. 0.37), suggesting a fixed global threshold may be sub-optimal for new domains.

## Limitations
- CLIP text filtering threshold appears tuned specifically for MI3DOR benchmark and may require re-calibration for new domains
- DINOv2 generalization across domain gap between real photographs and synthetic renders remains unproven beyond evaluated datasets
- 8 views per model may be insufficient for objects with significant self-occlusion or rotational symmetry

## Confidence
- **High Confidence**: The two-stage retrieval architecture and the complementary nature of CLIP vs. DINOv2 features are well-supported by ablation studies and benchmark results
- **Medium Confidence**: Generalization to completely unseen object categories requires further validation; caption quality impact depends on LLaVA-v1.5-7B performance on domain-specific objects
- **Low Confidence**: 31.8x speed improvement assumes comparable hardware and implementation quality for both approaches, not explicitly verified

## Next Checks
1. Test the CLIP text filtering threshold on a small validation set from your target domain, plotting accuracy vs. threshold as in Figure 7 to determine optimal τ_text for your specific objects
2. Evaluate retrieval accuracy with varying numbers of rendered views (4, 8, 16) on objects with different geometric complexity to quantify tradeoff between coverage and onboarding time
3. Test the complete pipeline on objects outside YCB/MI3DOR domains (e.g., household items, industrial tools) to verify caption quality and DINOv2 feature generalization across diverse object types