---
ver: rpa2
title: 'Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations
  Explanation of Large Language Models'
arxiv_id: '2602.00300'
source_url: https://arxiv.org/abs/2602.00300
tags:
- prompt
- bias
- hidden
- color
- balor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that Patchscopes, a framework for interpreting
  hidden representations of large language models, is affected by model bias arising
  from imbalanced linguistic patterns. The authors show that when decoding from hidden
  representations, models often rely on high-frequency associations rather than the
  contextual information actually encoded.
---

# Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models

## Quick Facts
- arXiv ID: 2602.00300
- Source URL: https://arxiv.org/abs/2602.00300
- Reference count: 40
- This paper identifies that Patchscopes, a framework for interpreting hidden representations of large language models, is affected by model bias arising from imbalanced linguistic patterns. The authors show that when decoding from hidden representations, models often rely on high-frequency associations rather than the contextual information actually encoded. To address this, they propose Bias Alignment through Logit Recalibration (BALOR), which uses contrastive prompts to recalibrate logits during decoding, suppressing model bias and amplifying contextual information. Experiments across four tasks and four models demonstrate that BALOR significantly improves explanation faithfulness, achieving up to 33% relative performance improvement over existing baselines.

## Executive Summary
This paper addresses a critical limitation in Patchscopes, a framework for interpreting hidden representations in large language models. The authors identify that model bias from imbalanced linguistic patterns in pretraining data causes models to rely on high-frequency associations rather than contextual information during decoding. They propose BALOR (Bias Alignment through Logit Recalibration), which uses contrastive prompts to recalibrate logits, suppressing bias and amplifying contextual information. Experiments across four tasks and four models show BALOR achieves up to 33% relative performance improvement in explanation faithfulness.

## Method Summary
BALOR mitigates model bias in Patchscopes by using contrastive prompts to recalibrate logits during decoding. The method computes p_balor(y|T) = σ[(1+α)l_θ(y|T) − αl_θ(y|T*)], where T is the patched target prompt and T* is the contrastive prompt with noun substituted for placeholder. This contrasts the logits from the target (with patched representation) against logits from an unpatched contrastive prompt, subtracting bias while amplifying contextual information. Layer selection is performed using a combined score of Logit Difference (LD) and Gradient Similarity Alignment (GSA), with the optimal layer being where patched information is both encoded and causally influential. Two decoding modes are proposed: Shared (S) applies recalibration to both prompts, while Divided (D) applies only to the target.

## Key Results
- BALOR significantly improves explanation faithfulness, achieving up to 33% relative performance improvement over vanilla Patchscopes
- The method demonstrates consistent effectiveness across four tasks (color, gender, culture, age) and four model families (Llama2-7b, Llama3.2-1b, Llama3-8b, Qwen3-4b)
- Frequency difference in pretraining data (Δf = f_pri − f_sec) predicts bias with OR = 9.338 and ROC-AUC = 0.619, demonstrating the causal link between corpus statistics and model bias
- Layer selection using combined LD + GSA scores (w = 0.8) successfully identifies bias-sensitive layers where interventions are most effective

## Why This Works (Mechanism)

### Mechanism 1
Imbalanced co-occurrence frequencies in pretraining data causally induce model bias that overrides contextual information during Patchscopes decoding. The paper demonstrates via logistic regression that frequency difference (Δf = f_pri − f_sec) predicts bias: a +1 SD increase in Δf increases the odds of exhibiting bias by a factor of 9.338 (Llama3.2-1b). During generation, the model's softmax favors high-frequency associations encoded as strong priors, even when the patched hidden representation encodes contradictory contextual information (e.g., "purple broccoli" → "green").

### Mechanism 2
Contrasting patched-target logits against unpatched-contrastive logits isolates and suppresses bias while amplifying contextually-relevant information. BALOR computes: p_balor(y|T) = σ[(1+α)l_θ(y|T) − αl_θ(y|T*)], where T is the patched target prompt and T* is the contrastive prompt (unpatched, noun substituted for placeholder). This subtracts the bias component captured by T* from the patched logits. Theorem 4.1 proves that if a token y₁ is more supported by patched evidence than by the contrastive prior, BALOR monotonically increases its relative log-odds as α grows.

### Mechanism 3
Bias-sensitive layers—where patched information still influences downstream computation—can be identified via the combined score of Logit Difference (LD) and Gradient Similarity Alignment (GSA). LD measures whether a layer differentiates context-aligned from biased attributes via logit-lens. GSA measures alignment between output gradient and learned attribute direction. These weakly correlated metrics (ρ = −0.147) are combined: score = w·LD + (1−w)·GSA with w = 0.8. The optimal layer (e.g., L13 for Llama2-7b) is where patched information is both encoded and causally influential.

## Foundational Learning

- **Concept: Patchscopes Framework**
  - Why needed here: BALOR is built *on top of* Patchscopes; understanding the source-target patching pipeline is prerequisite.
  - Quick check question: Given source prompt "Here is a purple broccoli" and target prompt "The color of x is green or purple?", at which position is the hidden representation patched?

- **Concept: Logit-Space Contrastive Methods**
  - Why needed here: BALOR operates entirely in logit space via contrastive subtraction; understanding how logit manipulation affects softmax probabilities is essential.
  - Quick check question: If l_θ(green|T) = 5.0, l_θ(purple|T) = 3.0, l_θ(green|T*) = 4.5, and l_θ(purple|T*) = 1.0, compute the recalibrated log-odds for α = 1.0.

- **Concept: Training Data Frequency-Induced Bias**
  - Why needed here: The paper's causal analysis hinges on the link between corpus statistics and model priors.
  - Quick check question: If "broccoli-green" co-occurs 1000× more frequently than "broccoli-purple" in pretraining data, what prior probability might the model assign to each attribute?

## Architecture Onboarding

- **Component map:** Source prompt processor → extracts hidden representation h^(ℓ)_i at noun position → Target prompt constructor → creates T (with placeholder x) and T* (with noun substituted) → Bias-sensitive layer selector → computes LD + GSA scores, selects optimal layer ℓ* → Patching engine → replaces h^(ℓ*)_i*(T) with h^(ℓ)_i(S) → BALOR decoder → runs parallel forward passes on T and T*, computes p_balor(y|T), samples next token

- **Critical path:** Layer selection (can be precomputed) → Patching at ℓ* → Parallel decoding → Logit recalibration → Sampling. The latency bottleneck is the dual forward pass.

- **Design tradeoffs:**
  - **Mode S (Shared) vs. Mode D (Divided):** Mode S samples from recalibrated distribution for both prompts, ensuring synchronization but potentially limiting exploration. Mode D lets prompts diverge, improving SR in some cases but reducing stability.
  - **Hyperparameter α:** Higher α amplifies contrast but may over-suppress useful priors. Paper finds α ∈ [0.4, 4.0] stable for Mode S.
  - **Layer selection weight w = 0.8:** Prioritizes LD (encoding) over GSA (gradient alignment); alternative weighting may suit different architectures.

- **Failure signatures:**
  - SR gain negative: Mode D with high α (>2.8 for Llama2-7b) or low α (<0.8 for some models)
  - Instability across runs: Mode D with high temperature (>1.0)
  - No improvement over vanilla: Incorrect layer selection (e.g., too close to output)

- **First 3 experiments:**
  1. **Sanity check:** Reproduce the "purple broccoli" example from Figure 1 with vanilla Patchscopes vs. BALOR (S) at α = 1.0; verify SR increases.
  2. **Layer ablation:** Run BALOR across all layers for the color task; confirm peak SR occurs near the paper's reported optimal layer.
  3. **Alpha sweep:** For a single model (e.g., Llama3.2-1b), plot SR vs. α ∈ [0.2, 5.0] for both modes; identify stable range.

## Open Questions the Paper Calls Out

### Open Question 1
Does the logit recalibration in BALOR inadvertently impair the model's ability to utilize correct factual priors when the model's inherent associations align with the ground truth? The paper evaluates BALOR exclusively on a "Biased Subset" where the linguistic patterns contradict the target context, but does not test performance on cases where the high-frequency prior is actually correct.

### Open Question 2
Can the "Divided" (Mode D) decoding strategy be stabilized to consistently leverage its potential for higher Show Rates without suffering from prompt divergence? Section 5.2 explicitly notes that "Mode D is less stable than mode S" because the "target and contrastive prompts in mode D follow different next-token prediction strategies, causing the two input sequences to diverge progressively."

### Open Question 3
Is BALOR effective for open-ended generation tasks where the target attribute is not a pre-defined choice but a free-form generation? The methodology and experiments focus on constrained tasks formatted as binary choices or specific attribute extraction, but the mechanism relies on amplifying the probability of a specific "context-aligned" token against a "biased" token.

## Limitations

- The BALOR approach requires two parallel forward passes per inference, creating significant computational overhead compared to vanilla Patchscopes
- The bias detection mechanism relies on frequency differences computed from specific corpus statistics, which may not capture all linguistic patterns or emerging associations
- The core assumption that contrastive prompts T* capture only bias requires that they contain no residual contextual information, which is not empirically validated
- Performance gains vary across tasks, with demographic tasks showing more modest improvements (8-10% SR) compared to color tasks (32% SR)

## Confidence

**High confidence (8-10/10)**: The empirical demonstration that imbalanced pretraining frequencies cause model bias in Patchscopes explanations is well-supported with robust statistical evidence (OR = 9.338, ROC-AUC = 0.619).

**Medium confidence (5-7/10)**: The generalizability of BALOR to arbitrary tasks and model architectures remains uncertain, with limited exploration of edge cases and non-transformer architectures.

**Low confidence (2-4/10)**: The claim that BALOR preserves model capabilities while improving faithfulness lacks comprehensive validation, particularly regarding task performance degradation and long-term distributional effects.

## Next Checks

**Check 1: Ablation of contrastive prompt construction** - Systematically vary the construction of T* and measure the impact on SR improvement to test the assumption that T* captures only bias and validate the robustness of the contrastive mechanism.

**Check 2: Cross-task transferability evaluation** - Apply BALOR to a task not in the original evaluation set (e.g., medical reasoning) and measure both faithfulness improvement and capability preservation to test generalizability beyond the four demonstrated tasks.

**Check 3: Attention pattern analysis** - Visualize and compare attention distributions between vanilla Patchscopes and BALOR across layers to provide empirical validation of the orthogonality assumption and identify potential attention leakage mechanisms.