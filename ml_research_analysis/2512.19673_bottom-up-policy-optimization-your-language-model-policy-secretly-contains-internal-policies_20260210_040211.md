---
ver: rpa2
title: 'Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains
  Internal Policies'
arxiv_id: '2512.19673'
source_url: https://arxiv.org/abs/2512.19673
tags:
- policy
- layer
- internal
- entropy
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the internal policy structures of large
  language models (LLMs) during reinforcement learning by decomposing the overall
  policy into Internal Layer Policies and Internal Modular Policies. Entropy analysis
  reveals universal patterns: early layers exhibit high-entropy exploration while
  top layers converge to deterministic refinement, with Qwen models showing progressive,
  human-like reasoning compared to Llama''s abrupt final-layer convergence.'
---

# Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies

## Quick Facts
- arXiv ID: 2512.19673
- Source URL: https://arxiv.org/abs/2512.19673
- Reference count: 40
- Key result: BuPO achieves 4.69 points improvement on AIME24 and 2.30 points on AIME25 benchmarks

## Executive Summary
This paper investigates the internal policy structures of large language models during reinforcement learning by decomposing the overall policy into Internal Layer Policies and Internal Modular Policies. Through entropy analysis, the authors reveal universal patterns where early layers exhibit high-entropy exploration while top layers converge to deterministic refinement. They propose Bottom-up Policy Optimization (BuPO), which optimizes internal layer policies in early training stages to establish stronger reasoning foundations. Experiments demonstrate BuPO's effectiveness across MATH, AMC23, AIME24, and AIME25 benchmarks.

## Method Summary
The authors decompose LLM policies into Internal Layer Policies and Internal Modular Policies to analyze how different model components contribute to overall reasoning behavior. They employ entropy analysis to track policy evolution across layers, revealing that early layers maintain exploratory behavior while top layers become increasingly deterministic. The Bottom-up Policy Optimization (BuPO) method then optimizes these internal layer policies sequentially during early training stages, establishing a reasoning foundation before fine-tuning the complete model. This approach contrasts with traditional end-to-end optimization by focusing on internal policy development as a precursor to overall performance gains.

## Key Results
- BuPO achieves 4.69 points improvement on AIME24 benchmark compared to GRPO with Qwen3-4B
- BuPO achieves 2.30 points improvement on AIME25 benchmark compared to GRPO with Qwen3-4B
- Entropy analysis reveals universal patterns: early layers show high-entropy exploration, top layers converge to deterministic refinement
- Qwen models demonstrate progressive, human-like reasoning while Llama shows abrupt final-layer convergence

## Why This Works (Mechanism)
BuPO works by establishing strong reasoning foundations through early optimization of internal layer policies. By optimizing internal policies during early training stages, the method forces lower layers to capture high-level reasoning representations before the model is fine-tuned end-to-end. This bottom-up approach creates a more robust foundation for subsequent learning, as lower layers develop the capacity to handle complex reasoning tasks that would otherwise be deferred to higher layers. The progressive refinement pattern observed in Qwen models suggests that this approach aligns with human-like reasoning development, where complex problem-solving builds upon well-established foundational capabilities.

## Foundational Learning
- **Entropy Analysis**: Measuring policy randomness across layers to track reasoning development; needed to identify when and where models transition from exploration to exploitation; quick check: compare entropy trajectories across different model families
- **Policy Decomposition**: Breaking down overall policies into layer-specific and module-specific components; needed to understand how different architectural parts contribute to reasoning; quick check: validate that decomposed policies sum to overall performance
- **Bottom-up Optimization**: Sequentially optimizing internal layers before full model training; needed to establish reasoning foundations early; quick check: compare with top-down and simultaneous optimization approaches
- **Feature Refinement**: Forcing lower layers to capture high-level representations; needed to create robust reasoning foundations; quick check: use feature attribution methods to verify reasoning capability in lower layers
- **Convergence Patterns**: Analyzing how different models reach deterministic reasoning; needed to understand model-specific reasoning development; quick check: test across diverse architectures beyond Qwen and Llama

## Architecture Onboarding

**Component Map**
Internal Layer Policies -> Internal Modular Policies -> Overall Policy

**Critical Path**
1. Entropy analysis of layer policies
2. Bottom-up sequential optimization of internal layers
3. Full model fine-tuning with established foundations

**Design Tradeoffs**
BuPO trades computational efficiency during initial training for improved final performance by requiring sequential layer optimization. This approach may increase training time but potentially reduces the need for extensive hyperparameter tuning later. The method assumes that early establishment of reasoning foundations generalizes better than learning them through end-to-end optimization.

**Failure Signatures**
- If lower layers fail to capture high-level reasoning, entropy analysis will show persistent high values in early layers throughout training
- If optimization order matters more than foundation establishment, alternative layer sequences should show similar or better performance
- If the approach doesn't generalize, other model families should show no improvement or performance degradation

**3 First Experiments**
1. Compare BuPO performance against standard fine-tuning on same architecture across all benchmarks
2. Apply BuPO to a different model family (e.g., Mistral or Claude) to test generalizability
3. Conduct ablation studies testing different layer optimization orders (top-down vs bottom-up)

## Open Questions the Paper Calls Out
None

## Limitations
- Entropy analysis patterns may be dataset-specific rather than universal across all tasks and domains
- The distinction between Qwen's progressive reasoning and Llama's abrupt convergence relies on qualitative interpretations that could vary with different metrics
- BuPO's effectiveness may stem from modified training dynamics rather than the claimed mechanism of forcing lower layers to capture reasoning representations

## Confidence
High: Experimental framework using established MATH, AMC23, AIME24, and AIME25 benchmarks; specific, measurable performance improvements reported
Medium: Entropy analysis methodology and observed patterns appear sound but require verification across diverse model families and tasks
Low: Causal interpretation of BuPO's effectiveness and claims about "forcing" lower layers to capture reasoning representations need more rigorous ablation studies

## Next Checks
1. Test BuPO across diverse model families (beyond Qwen and Llama) on the same benchmarks to verify whether progressive versus abrupt convergence patterns hold universally
2. Conduct ablation studies on layer ordering to validate that bottom-up optimization specifically drives improvements versus alternative sequences
3. Use integrated gradients or similar feature attribution methods to verify that lower layers actually learn high-level reasoning representations versus different but equally effective feature representations