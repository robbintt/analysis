---
ver: rpa2
title: 'DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs
  and Generative Regularization'
arxiv_id: '2512.12669'
source_url: https://arxiv.org/abs/2512.12669
tags:
- temporal
- knowledge
- reasoning
- graph
- extrapolation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DynaGen unifies interpolation and extrapolation in temporal knowledge
  graph reasoning by combining dynamic subgraph construction with a synergistic dual-branch
  GNN encoder for interpolation and a conditional diffusion-based generative regularization
  for extrapolation. It dynamically builds entity-centric subgraphs enriched with
  temporal and structural information, processes them with a Relational GCN and Graph
  Attention Network to capture semantics and importance, and applies a diffusion process
  to learn underlying evolutionary principles.
---

# DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization

## Quick Facts
- arXiv ID: 2512.12669
- Source URL: https://arxiv.org/abs/2512.12669
- Reference count: 40
- Primary result: Achieves state-of-the-art MRR on both interpolation and extrapolation tasks, improving by 2.61 and 1.45 points respectively

## Executive Summary
DynaGen introduces a unified framework for temporal knowledge graph reasoning that addresses both interpolation and extrapolation tasks through dynamic subgraph construction and generative regularization. The method dynamically builds entity-centric subgraphs enriched with temporal and structural information, processes them with a synergistic dual-branch GNN encoder, and applies conditional diffusion-based regularization to learn evolutionary principles. Experiments on six benchmarks demonstrate significant improvements over existing methods, with MRR gains of 2.61 points for interpolation and 1.45 points for extrapolation compared to the second-best models.

## Method Summary
DynaGen operates by first constructing dynamic entity-centric subgraphs for each query using adaptive temporal windows and weighted BFS traversal. These subgraphs are then encoded using a dual-branch Synergistic Structure-Aware Encoder (SSAE) that combines Relational GCN and Graph Attention Network branches with temporal gating. During training, a conditional diffusion process adds noise to the encoder output and requires reconstruction based only on query relation and time, forcing the model to learn underlying generative principles. The encoded representations are aggregated through a Transformer-Mixer reasoning module and scored against all entities. The diffusion module is discarded at inference, maintaining computational efficiency during deployment.

## Key Results
- Achieves state-of-the-art performance on six TKG benchmarks across both interpolation and extrapolation tasks
- Improves MRR by 2.61 points compared to second-best model on interpolation tasks
- Improves MRR by 1.45 points compared to second-best model on extrapolation tasks
- Shows optimal performance with 2-layer SSAE configuration, with deeper networks degrading due to over-smoothing

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Subgraph Contextualization
Dynamically constructed, entity-centric subgraphs provide superior context for interpolation by gathering neighbors through adaptive BFS with learned time-windows. Edges are weighted by temporal decay, creating structural snapshots relevant to specific query times. This captures evolving local structure and recency of events rather than analyzing isolated quadruples.

### Mechanism 2: Synergistic Dual-Branch Encoding (SSAE)
The SSAE architecture disentangles relation semantics from structural importance via parallel GNN branches - Relational GCN captures type-specific semantics while Graph Attention Network captures salience. These are fused using temporal gates that prioritize recent messages, resolving conflicts between semantic and structural signals.

### Mechanism 3: Generative Regularization via Diffusion
Conditional diffusion processes force the model to learn underlying evolutionary principles rather than memorizing patterns. During training, noise is added to encoder outputs and the model must reconstruct them using only relation and time information. This mitigates "cognitive generalization bias" in extrapolation by compressing sufficient information to generate structural context from minimal cues.

## Foundational Learning

- **Temporal Knowledge Graph (TKG) Structures**: Understanding quadruples (s, p, o, t) and the distinction between interpolation (filling historical gaps) and extrapolation (predicting future events) is critical since diffusion specifically addresses extrapolation bias.
  - *Quick check*: Can you explain why a model that simply memorizes historical sequences might fail at extrapolation when "emergent events" occur?

- **Graph Neural Networks (R-GCN & GAT)**: The core encoder splits tasks between these two - R-GCN handles edge types (relations) while GAT handles neighbor weighting (attention).
  - *Quick check*: If a graph has 50 different relation types, which branch (R-GCN or GAT) requires specific parameters for each type, and which computes attention scores based on node features?

- **Diffusion Models (Denoising Probabilistic Models)**: The paper adapts diffusion not for image generation, but for representation regularization during training.
  - *Quick check*: In DynaGen, does the diffusion model run during inference to generate the answer? (Answer: No, it is training-only).

## Architecture Onboarding

- **Component map**: Input (query) -> Subgraph Builder (adaptive BFS with temporal weighting) -> SSAE Encoder (parallel R-GCN + GAT â†’ temporal gating) -> Regularizer (train-only diffusion) -> Reasoning Head (Transformer-Mixer) -> Output (dot product scoring)

- **Critical path**: The SSAE Encoder is the bottleneck - subgraph construction feeds it data, and diffusion regularizer sits on top of its output. Poor SSAE output cannot be rescued by diffusion loss or Mixer reasoning.

- **Design tradeoffs**: Dynamic subgraph construction per query is computationally expensive but inference speed remains high since diffusion is discarded. Optimal depth is 2 layers to avoid over-smoothing.

- **Failure signatures**: Over-smoothing occurs with >2 SSAE layers; temporal sparsity may require fallback to nearest neighbors; incorrect time-window prediction reduces signal-to-noise ratio.

- **First 3 experiments**: (1) Run `w/o SSAE` ablation to confirm dual-branch contribution; (2) Run `w/o Diffusion` ablation to quantify regularization benefit; (3) Vary SSAE layers (1 vs 2 vs 3) on validation to verify over-smoothing cliff.

## Open Questions the Paper Calls Out

- Can a learned subgraph generation mechanism effectively replace the current heuristic-based extraction to better filter noise and capture temporally distant logical dependencies? The current BFS-based method may incorporate noisy connections or overlook crucial distant facts.

- What specific indexing structures or graph sampling techniques can mitigate the computational overhead of dynamically building unique subgraphs for every query? The dynamic, query-dependent nature prevents static pre-computation optimizations.

- To what extent does diffusion-based regularization specifically improve prediction accuracy of "emergent" events versus recurring historical patterns? The paper defines "cognitive generalization bias" for emergent events but reports only aggregate scores without isolating novel event types.

## Limitations

- Dynamic subgraph construction per query is computationally expensive and resource-intensive during training
- The diffusion module, while theoretically compelling, is discarded at inference and its direct contribution to final predictions cannot be measured
- Heavy reliance on internal ablation studies for key claims, with limited external corpus validation for architectural choices

## Confidence

- **High** - Dynamic subgraph construction improves interpolation (supported by ablation and performance gains)
- **Medium** - Dual-branch SSAE resolves semantic vs. structural conflicts (internally validated but limited external corpus support)
- **Medium** - Diffusion regularization mitigates extrapolation bias (theoretically sound, but hyperparameters and inference behavior are critical unknowns)

## Next Checks

1. **Ablation Verification**: Replicate the `w/o SSAE` and `w/o Diffusion` ablations to confirm the reported MRR drops (2.08 and 0.95 points on YAGO respectively)

2. **Temporal Generalization Test**: Evaluate DynaGen on a held-out future period to quantify the actual benefit of diffusion regularization for extrapolation vs. interpolation

3. **Computational Overhead Measurement**: Benchmark the time and memory cost of dynamic subgraph construction per query against static subgraph methods to quantify the resource trade-off