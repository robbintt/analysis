---
ver: rpa2
title: 'Towards Transparent AI: A Survey on Explainable Large Language Models'
arxiv_id: '2506.21812'
source_url: https://arxiv.org/abs/2506.21812
tags:
- methods
- language
- llms
- probing
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews XAI methods for LLMs, introducing
  a taxonomy based on transformer architectures (encoder-only, decoder-only, encoder-decoder).
  It categorizes techniques like feature attribution, probing, attention-based, and
  self-explanation methods, highlighting their unique interpretability challenges
  and applications.
---

# Towards Transparent AI: A Survey on Explainable Large Language Models

## Quick Facts
- arXiv ID: 2506.21812
- Source URL: https://arxiv.org/abs/2506.21812
- Reference count: 40
- Primary result: Systematic survey categorizing XAI methods for LLMs by transformer architecture with taxonomy covering feature attribution, probing, attention-based, and self-explanation techniques

## Executive Summary
This survey systematically reviews explainable AI methods for large language models, introducing a taxonomy based on transformer architectures (encoder-only, decoder-only, encoder-decoder). It categorizes techniques like feature attribution, probing, attention-based, and self-explanation methods, highlighting their unique interpretability challenges and applications. The authors identify key challenges including efficiency, generalization, and ethical concerns, while outlining future directions for improving explanation quality and reducing shortcut learning in LLMs.

## Method Summary
The survey conducts a comprehensive literature review of XAI methods for LLMs, organizing techniques by model architecture and categorizing them into four main approaches: feature attribution (SHAP, LIME, AML), probing (edge probing, DIRECTPROBE), attention-based methods (BertViz, attention-guided decoding), and self-explanation (WT5, Few-Shot NLE Transfer). The authors evaluate methods across tasks like sentiment classification, fake news detection, and question answering, using metrics such as faithfulness, local accuracy, consistency, and computational efficiency. Implementation details and benchmark datasets are referenced from cited papers but not fully specified in the survey itself.

## Key Results
- Feature attribution methods like SHAP and AML provide token-level importance scores but face computational challenges with long sequences
- Probing classifiers reveal linguistic knowledge distribution across transformer layers, though control task issues remain unresolved
- Cross-attention in encoder-decoder models offers intuitive explanations by linking input context to generated outputs, despite attention's debated role as explanation
- Self-explanation methods like WT5 generate rationales but risk hallucinating plausible-sounding explanations that don't reflect actual reasoning

## Why This Works (Mechanism)

### Mechanism 1: Architecture-Specific Attribution Alignment
Feature attribution methods must align with token processing direction. SHAP calculates additive feature importance bidirectionally for encoder models, while AML enforces causality in decoder models through gradient-based masking and KL divergence preservation. The explanation fidelity depends on gradient access for AML versus black-box assumptions for LIME.

### Mechanism 2: Representational Probing via Classifier Decoding
Probing classifiers trained on frozen representations can reveal layer-wise knowledge distribution when linguistic properties are linearly encoded in hidden states. High probe accuracy implies the layer encodes that feature, though the control task problem remains: probes may learn task aspects rather than reflecting model's inherent knowledge.

### Mechanism 3: Cross-Attention and Information Flow Tracing
In encoder-decoder architectures, cross-attention weights serve as causal mechanism traces linking generated output tokens back to specific input context tokens. Visualizing or masking these attention heads identifies which input segments drive generation, though attention's correlation with semantic relevance remains contested.

## Foundational Learning

- **Transformer Architectures (Encoder vs. Decoder)**: The entire taxonomy relies on distinguishing bidirectional context (Encoder-only/BERT), autoregressive generation (Decoder-only/GPT), and cross-attention (Encoder-Decoder/T5). Quick check: Can you explain why masking future tokens in Decoder models changes attribution approaches compared to Encoder models?

- **Shapley Values (Game Theory)**: SHAP calculates the "marginal contribution" of features across all possible coalitions, but exact calculation becomes computationally intractable for long text sequences, necessitating approximations like TextGenSHAP. Quick check: Why does calculating exact Shapley values become intractable for long sequences?

- **Probing Classifiers**: Testing what linguistic knowledge is "stored" in a model requires understanding the difference between training a model on data versus probing a frozen model for data. Quick check: If a probing classifier achieves 100% accuracy on a syntax task, does that prove the original LLM "understands" syntax?

## Architecture Onboarding

- **Component map**: Architecture Type → Method Category → Specific Technique. Encoder-only (BERT) → Contextual Embeddings → LIME/TransSHAP (Attribution), Edge Probing (Linguistics), BertViz (Attention). Decoder-only (GPT) → Autoregressive Generation → AML (Gradient Attribution), CoT (Reasoning), ROME (Mechanistic). Encoder-Decoder (T5) → Cross-Attention → TextGenSHAP (Attribution), Attention-Guided Decoding.

- **Critical path**: Access Level → Architecture Selection → Method Choice. Step 1: Determine if you have Internal (weights/gradients) or External (API) access. Step 2: Identify architecture (Encoder/Decoder). Step 3: Select method: e.g., if Internal + Decoder → AML or Mechanistic Interpretability; if External + Decoder → Progressive Inference or CoT.

- **Design tradeoffs**: Faithfulness vs. Accessibility (Mechanistic interpretability offers most faithful insight but requires white-box access and high compute; post-hoc methods are accessible but may approximate poorly). Efficiency vs. Granularity (TextGenSHAP handles long documents better than vanilla SHAP but remains computationally intensive compared to simple attention visualization).

- **Failure signatures**: Shortcut Learning (explanations reveal spurious correlations rather than semantic reasoning), Hallucination in Self-Explanation (model generates plausible-sounding CoT that doesn't causally reflect generation process), Attention Diffusion (attention maps become noisy or diffuse in long inputs).

- **First 3 experiments**: Run BertViz on BERT-base fine-tuned for sentiment analysis to observe attention head alignment with sentiment-bearing words. Implement LIME vs. SHAP on fake news detection to compare local fidelity and stability. Apply ROME to GPT-2 to change a specific factual association and measure explanation changes.

## Open Questions the Paper Calls Out

- How can algorithms be designed to faithfully reflect LLM decision-making processes when ground truth explanations are inaccessible? The authors explicitly ask this in Section 6, noting that no benchmark datasets exist for evaluating global explanations of individual components.

- What specific model architectures and data characteristics are responsible for driving emergent skills in LLMs? Section 6 lists this as a primary open question, highlighting that emergent abilities arise unpredictably as models scale and training data grows.

- How do prediction rationales differ between fine-tuning and prompting paradigms in natural language inference tasks? The authors identify this key question regarding divergent performance and reasoning strategies between these paradigms.

- Can out-of-distribution robustness be directly linked to variations in reasoning strategies? Section 6 poses whether improved generalization stems from deeper reasoning or other factors when models rely on shortcuts.

## Limitations

- The survey relies heavily on citing specific methodological papers rather than providing implementation details or benchmarking results, with critical hyperparameters often absent.
- Fundamental tensions between faithfulness and accessibility in XAI methods are acknowledged but not resolved within the framework.
- Temporal analysis limitations are noted but not systematically addressed across the surveyed methods.

## Confidence

- **High Confidence**: The architectural taxonomy (encoder-only, decoder-only, encoder-decoder) and its implications for method selection are well-established and clearly articulated.
- **Medium Confidence**: Claims about specific method performance (e.g., AML outperforming perturbation methods, BERT's superior edge probing results) are cited from source papers but lack independent validation in this survey.
- **Medium Confidence**: The challenges section (efficiency, generalization, ethics) accurately reflects community concerns but is primarily synthesized from existing literature rather than original analysis.

## Next Checks

1. **Faithfulness Validation**: Implement and compare SHAP versus LIME explanations on a BERT-based sentiment classification task using SST-2 dataset, measuring both local accuracy and consistency across runs.

2. **Mechanistic Testing**: Apply ROME to a GPT-2 model to modify a factual association (e.g., "Washington is the capital of the USA"), then verify if the change propagates to correct downstream predictions and whether explanations reflect this change.

3. **Efficiency Benchmarking**: Benchmark TextGenSHAP against vanilla SHAP on long-document classification tasks, measuring both computational cost and explanation quality degradation as input length increases.