---
ver: rpa2
title: Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval
  Augmented Generation at BioASQ 2025
arxiv_id: '2508.05366'
source_url: https://arxiv.org/abs/2508.05366
tags:
- batch
- test
- feedback
- ur-iw-5
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigated whether self-feedback mechanisms could\
  \ improve the performance of Large Language Models (LLMs) in retrieval-augmented\
  \ generation (RAG) for biomedical question answering. The authors evaluated current\
  \ reasoning and non-reasoning LLMs\u2014including Gemini-Flash 2.0, o3-mini, o4-mini,\
  \ and DeepSeek-R1\u2014on the BioASQ 2025 challenge tasks."
---

# Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025

## Quick Facts
- arXiv ID: 2508.05366
- Source URL: https://arxiv.org/abs/2508.05366
- Reference count: 33
- Large Language Models tested: Gemini-Flash 2.0, o3-mini, o4-mini, DeepSeek-R1 on BioASQ 2025

## Executive Summary
This study investigates whether self-feedback mechanisms can improve Large Language Model performance in retrieval-augmented generation for biomedical question answering. The authors evaluated five system configurations across four answer types (yes/no, factoid, list, ideal summary) using the BioASQ 2025 challenge tasks. While self-feedback did not consistently outperform zero-shot baselines, few-shot learning with 10 examples and Gemini-Flash 2.0 achieved the strongest results, particularly for retrieval and yes/no questions. Reasoning models showed mixed performance improvements with feedback, suggesting the approach holds potential but requires further refinement for consistent gains.

## Method Summary
The study compared five configurations on BioASQ 2025: (1) Gemini 2.0 Flash baseline, (2) reasoning model baseline, (3) Gemini + self-feedback, (4) reasoning + self-feedback, and (5) Gemini 10-shot. The pipeline involved query generation (with optional feedback using top-10 retrieved documents), Elasticsearch retrieval from PubMed 2024 baseline, and answer generation with type-specific feedback prompts. Feedback loop: draft → self-critique → refine. 10-shot examples from previous year's participation. Evaluated on BioASQ test batches 1-4 across three phases (A, A+, B) with four answer types.

## Key Results
- Few-shot learning (10-shot) with Gemini-Flash 2.0 achieved strongest performance, especially in retrieval and yes/no tasks
- Self-feedback showed inconsistent improvements, sometimes degrading performance (e.g., UR-IW-3 vs UR-IW-1 in Batch 1 Yes/No)
- Reasoning models improved in later batches with feedback for yes/no and factoid questions, but results were task-dependent
- Feedback did not consistently outperform zero-shot baselines across all configurations and tasks

## Why This Works (Mechanism)

### Mechanism 1: Iterative Self-Feedback for Query and Answer Refinement
- Claim: LLMs may improve query expansions and answers through a generate-evaluate-refine loop, though effectiveness is inconsistent.
- Mechanism: The model produces an initial output, then generates task-specific critiques (e.g., "Clearly suggest corrections, removals, or additions"), and finally revises based on its own feedback using a fixed refinement prompt.
- Core assumption: The model can identify gaps or errors in its prior output and apply corrections without external signal.
- Evidence anchors:
  - [abstract] "A self-feedback mechanism was implemented where LLMs generated, evaluated, and refined their own query expansions and answers across four types: yes/no, factoid, list, and ideal summary."
  - [section 3.2.1] "Feedback (FB): Prompt refinement using self-generated feedback. The top 10 results from the initial query were passed on to the feedback generating model as additional context."
  - [corpus] Related work on self-critique suggests refinement can improve faithfulness, though groundedness remains a challenge.
- Break condition: When the model's critique reinforces existing errors or introduces new hallucinations without external grounding.

### Mechanism 2: Few-Shot Learning as Task Specification
- Claim: Providing 10 input-output examples consistently grounds the model in task format and domain expectations, outperforming self-feedback in retrieval.
- Mechanism: In-context examples condition the model on successful query structures and answer patterns before generation, reducing format errors and improving retrieval precision.
- Core assumption: The examples are representative of test distribution and do not introduce systematic bias.
- Evidence anchors:
  - [abstract] "Few-shot learning (10-shot) with Gemini-Flash 2.0 often achieved the strongest results, particularly in retrieval and yes/no question tasks."
  - [section 4.2] "Generally, the 10-shot run with Gemini Flash 2.0 or 2.5 (UR-IW-5) tended to perform better in document and snippet retrieval tasks compared to our other configurations."
  - [corpus] Weak direct corpus evidence on few-shot vs. feedback tradeoffs; this finding is primarily supported by the paper's internal results.
- Break condition: When examples are misaligned with test queries or when overfitting to example patterns reduces generalization.

### Mechanism 3: Reasoning Models for Feedback Generation
- Claim: Reasoning models (o3-mini, o4-mini, DeepSeek-R1) may generate more actionable feedback in later batches, but performance is task-dependent.
- Mechanism: Models fine-tuned for Chain-of-Thought reasoning produce more structured critiques that can identify subtle errors, particularly for yes/no and factoid questions.
- Core assumption: Reasoning capability transfers to critique quality; the model's internal reasoning aligns with domain-specific correctness criteria.
- Evidence anchors:
  - [abstract] "Feedback sometimes improved performance in later batches for reasoning models, but its impact varied across tasks and models."
  - [section 5] "For Task B Yes/No questions, UR-IW-4 (with feedback) often surpassed UR-IW-2 (without feedback) in later batches."
  - [corpus] Related work suggests search-augmented agents can benefit from self-feedback, but optimal feedback design remains open.
- Break condition: When reasoning tokens do not surface relevant critique, or when the critique diverges from biomedical ground truth.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The entire pipeline depends on retrieving relevant PubMed snippets before generating answers; poor retrieval cascades into poor answers.
  - Quick check question: Given a biomedical question, can you identify which retrieved snippets contain the answer versus which are merely topically related?

- Concept: In-Context Learning (Few-Shot vs. Zero-Shot)
  - Why needed here: The study compares zero-shot, few-shot (10-shot), and feedback-augmented approaches; understanding how examples shape model behavior is essential for interpreting results.
  - Quick check question: If you provide 10 example question-answer pairs, what patterns might the model infer versus what patterns would remain under-specified?

- Concept: Self-Critique Limitations
  - Why needed here: The core hypothesis is that models can critique themselves; understanding failure modes (e.g., hallucination reinforcement) informs when to apply this mechanism.
  - Quick check question: If a model generates an incorrect entity in a factoid answer, what signals would enable it to detect and correct that error without external verification?

## Architecture Onboarding

- Component map: Query generation (LLM) → Elasticsearch retrieval (PubMed) → Snippet extraction/reranking (LLM) → Answer generation (LLM, with or without feedback) → Final output
- Critical path: 1) Query generation quality determines retrieval recall, 2) Retrieval quality determines answer grounding, 3) Feedback loop adds latency and cost; must demonstrate measurable gain to justify
- Design tradeoffs:
  - Zero-shot: Fast, simple, but may misformat or miss domain conventions
  - Few-shot (10-shot): Strong baseline, requires curated examples, may not generalize across batch shifts
  - Self-feedback: Higher cost (additional LLM calls), mixed effectiveness, potentially beneficial for reasoning models on certain tasks
- Failure signatures:
  - Feedback degrades retrieval: UR-IW-4 MAP scores lower than UR-IW-5 in multiple batches
  - Feedback fails to correct yes/no errors: UR-IW-3 underperforms UR-IW-1 in Batch 1 Phase A+ (0.9328 vs. 1.0000 Macro F1)
  - Reasoning model feedback inconsistent: DeepSeek-R1 replaced due to slow API; o3-mini/o4-mini results batch-dependent
- First 3 experiments:
  1. Reproduce the retrieval experiment: Compare zero-shot, 10-shot, and self-feedback on a held-out subset of BioASQ training data. Measure MAP and snippet retrieval accuracy.
  2. Ablate feedback by task type: Run feedback only on yes/no vs. only on factoid vs. only on list. Identify which task types benefit most.
  3. Replace self-feedback with human expert feedback on a small sample (n=20 questions). Compare whether expert feedback produces consistent gains where self-feedback does not, establishing an upper bound for the mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM-generated self-feedback compare to human expert feedback in improving RAG performance for professional biomedical search tasks?
- Basis in paper: [explicit] The authors state: "In future work, we plan to switch out the LLM generated feedback with feedback from human experts to compare the effectiveness of human and AI guided search processes."
- Why unresolved: The study only tested self-feedback; no human expert feedback condition was included as a comparison.
- What evidence would resolve it: A controlled experiment where identical queries/answers are refined using either LLM-generated feedback or expert human feedback, with performance compared on BioASQ metrics.

### Open Question 2
- Question: Are reasoning models (o3-mini, o4-mini, DeepSeek-R1) more capable of generating useful self-feedback than non-reasoning models (Gemini Flash 2.0)?
- Basis in paper: [explicit] The abstract asks: "if reasoning models are more capable of generating useful feedback." The authors investigate this but report "mixed results" without conclusive findings.
- Why unresolved: Feedback showed inconsistent improvements across reasoning and non-reasoning models, with neither category clearly outperforming the other across all tasks.
- What evidence would resolve it: Systematic comparison isolating feedback quality as a variable, with statistical analysis of feedback-induced gains by model type.

### Open Question 3
- Question: Why does few-shot learning consistently outperform self-feedback mechanisms in retrieval tasks despite the latter's theoretical advantages?
- Basis in paper: [inferred] The authors note "few-shot approach... was, according to the preliminary results, still the most competitive approach" and feedback "did not consistently outperform the base models."
- Why unresolved: The paper documents this pattern but does not analyze the underlying causes or error types.
- What evidence would resolve it: Detailed error analysis comparing the types of query/answer improvements generated by few-shot examples versus self-critique.

## Limitations

- Inconsistent effectiveness of self-feedback across task types and model configurations
- Lack of external validation beyond the BioASQ test set
- Computational cost implications of feedback loop not addressed

## Confidence

- **High confidence**: Few-shot learning (10-shot) with Gemini-Flash 2.0 consistently outperforms other approaches for retrieval and yes/no questions
- **Medium confidence**: Self-feedback shows task-dependent improvements, particularly for reasoning models in later batches
- **Low confidence**: The mechanism by which self-feedback sometimes degrades performance is not well understood

## Next Checks

1. **Ablation study by task type**: Run feedback-only on yes/no, factoid, and list questions separately to identify which task types benefit most from self-feedback. Compare the performance drop when feedback is removed from each task type.

2. **Expert feedback comparison**: On a small validation set (n=20 questions), replace self-feedback with human expert feedback to establish an upper bound for the mechanism. Measure whether human feedback produces consistent improvements where self-feedback fails.

3. **Cross-domain generalization**: Apply the best-performing configurations (10-shot Gemini-Flash 2.0 and reasoning model feedback) to a non-biomedical RAG benchmark (e.g., Natural Questions or TriviaQA) to test whether the findings transfer beyond the biomedical domain.