---
ver: rpa2
title: Auto-scaling Continuous Memory for GUI Agent
arxiv_id: '2510.09038'
source_url: https://arxiv.org/abs/2510.09038
tags:
- memory
- task
- page
- agent
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a continuous memory framework for GUI agents
  to improve generalization in unfamiliar interfaces and long-horizon tasks. Unlike
  prior text-based memory systems that inflate context length and lose critical visual
  details, the proposed approach compresses GUI trajectories into fixed-length continuous
  embeddings using the vision-language model itself as an encoder.
---

# Auto-scaling Continuous Memory for GUI Agent

## Quick Facts
- arXiv ID: 2510.09038
- Source URL: https://arxiv.org/abs/2510.09038
- Authors: Wenyi Wu; Kun Zhou; Ruoxin Yuan; Vivian Yu; Stephen Wang; Zhiting Hu; Biwei Huang
- Reference count: 40
- Key outcome: Continuous memory framework compresses GUI trajectories into fixed-length embeddings, achieving state-of-the-art performance on GUI benchmarks while scaling cost-effectively.

## Executive Summary
This paper introduces a continuous memory framework for GUI agents that compresses long trajectories into fixed-length embeddings using the vision-language model itself as an encoder. Unlike prior text-based memory systems that lose critical visual details and inflate context length, this approach preserves fine-grained visual cues while enabling efficient knowledge reuse. The method achieves state-of-the-art performance on real-world GUI benchmarks and generalizes well to out-of-domain environments.

The authors also design an auto-scaling data flywheel that autonomously discovers new environments, synthesizes tasks, rolls out agent trajectories, and verifies success—collecting over 100k trajectories for about $4k. Only the memory encoder (1.2% parameters) is fine-tuned using LoRA and Q-Former, making the approach both effective and computationally efficient.

## Method Summary
The method uses a Q-Former to encode each GUI trajectory into 8 continuous embeddings, which are prepended to the VLM's input embedding layer. At inference, CLIP encodes screenshots and actions into keys, FAISS retrieves top-k neighbors, and the retrieved embeddings are concatenated to the input. The approach is trained on 1.5k samples using LoRA with rank 16, fine-tuning only 1.2% of parameters. An auto-scaling data flywheel collects 100k+ trajectories by discovering environments via search, synthesizing tasks with an open-source VLM, rolling out agent trajectories, and verifying success with a judge VLM.

## Key Results
- Qwen-2.5-VL-7B with continuous memory matches or exceeds state-of-the-art closed-source models (GPT-4o, Claude-4) across real-world GUI benchmarks
- Performance improves monotonically as memory size and retrieval depth scale, unlike text memories that degrade after ~10 items
- Generalizes well to out-of-domain GUI environments while maintaining low inference latency
- Collects 100k+ trajectories for about $4k using fully automated annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compressing GUI trajectories into fixed-length continuous embeddings preserves fine-grained visual cues while reducing context length.
- Mechanism: A Q-Former encodes each trajectory into 8 continuous embeddings (down from ~15k tokens), which are prepended to the VLM's input embedding layer. This allows the model to attend to compressed memory without architectural changes.
- Core assumption: The Q-Former can distill task-relevant visual and action information into a small set of vectors that remain meaningful across tasks and environments.
- Evidence anchors:
  - [abstract] "encodes each GUI trajectory into a fixed-length sequence of continuous embeddings... preserving fine-grained visual information"
  - [section 4.2] "long trajectories that often exceed 15,000 tokens can be compressed to as few as 8 vectors"
  - [corpus] Weak direct evidence; related work (e.g., Darwinian Memory, EchoTrail-GUI) focuses on memory systems but not Q-Former-based compression specifically.
- Break condition: If compressed embeddings lose critical grounding information (widget identity, spatial relations), retrieval will provide irrelevant or misleading context, degrading performance.

### Mechanism 2
- Claim: Retrieving multiple continuous memory items improves task success monotonically, unlike text-based memories that degrade after ~10 items.
- Mechanism: At inference, CLIP encodes screenshots and actions into keys; FAISS retrieves top-k neighbors. Retrieved embeddings are concatenated to the input. Because embeddings are compact, scaling k and memory bank size does not explode sequence length.
- Core assumption: Retrieval similarity correlates with task relevance, and the VLM can integrate multiple retrieved embeddings without interference.
- Evidence anchors:
  - [abstract] "As memory size and retrieval depth increase, performance improves monotonically, unlike text memories that degrade with long prompts"
  - [section 5.2, Figure 1b] CoMEM shows sustained accuracy gains up to 100 retrieved samples; text memory peaks at ~10 then declines.
  - [corpus] EchoTrail-GUI and MGA emphasize memory-driven retrieval but do not report monotonic scaling curves.
- Break condition: If retrieval returns semantically similar but task-irrelevant trajectories (e.g., same site, different goal), the model may be misled, especially in OOD environments.

### Mechanism 3
- Claim: An auto-scaling data flywheel can generate diverse, high-quality GUI trajectories at low cost without human annotation.
- Mechanism: Four-phase loop—(1) discover websites via search, (2) synthesize tasks using an open-source VLM, (3) roll out agent trajectories, (4) verify success with a judge VLM (SEAgent-1.0-7B). Successful trajectories are added to the memory pool.
- Core assumption: Open-source VLMs can generate solvable tasks and judge success reliably enough to bootstrap a useful memory bank.
- Evidence anchors:
  - [abstract] "collect 100k+ trajectories for about $4000"
  - [section 4.1] Flywheel details; Table 1 shows 10k+ environments, 100k+ samples with fully automatic annotation.
  - [corpus] ZeroGUI and SEAgent (cited in paper) similarly use VLM-driven task synthesis and verification; corpus neighbors do not directly evaluate flywheel cost/quality.
- Break condition: If the judge VLM has high false-positive or false-negative rates, the memory pool will contain noisy or incomplete trajectories, reducing retrieval quality and agent reliability.

## Foundational Learning

- Concept: **Q-Former (QueryFormer)**
  - Why needed here: Serves as the memory encoder that compresses multimodal trajectories into fixed-length embeddings. Understanding attention-based query compression is essential to diagnose what information is retained vs. lost.
  - Quick check question: Given a trajectory with 10 (screenshot, action) pairs, how many output embeddings does the Q-Former produce, and what determines their information content?

- Concept: **CLIP-based multimodal retrieval**
  - Why needed here: Retrieval relies on CLIP to encode screenshots and actions into a shared embedding space. Without this, the agent cannot fetch relevant past experiences.
  - Quick check question: If two tasks share similar visual layouts but different goals (e.g., two shopping sites), will CLIP retrieval distinguish them? How could you improve specificity?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Fine-tuning the memory encoder updates only 1.2% of parameters via LoRA. This makes training feasible on limited data and compute.
  - Quick check question: If LoRA rank is too low, what symptoms would you expect in the memory encoder's ability to represent complex GUI interactions?

## Architecture Onboarding

- Component map:
  Memory bank (FAISS-indexed CLIP embeddings) -> Memory encoder (Q-Former + LoRA) -> Agent backbone (Qwen-2.5-VL-7B) -> Data flywheel (environment discovery, task synthesis, rollout, verification)

- Critical path:
  1. Flywheel generates and verifies trajectories → 2. Trajectories indexed into memory bank → 3. At inference, CLIP retrieves top-k → 4. Q-Former encodes retrieved trajectories into embeddings → 5. Embeddings prepended to VLM input → 6. VLM produces action

- Design tradeoffs:
  - **Embedding length vs. fidelity**: 8 vectors minimize context but may lose fine details; increasing to 16+ improves fidelity but costs context budget
  - **Retrieval depth vs. noise**: More retrieved samples (k=100) improve coverage but risk irrelevant or conflicting memories if retrieval quality is poor
  - **Flywheel automation vs. quality**: Fully automatic annotation scales cheaply (~$4k/100k trajectories) but may admit noisy labels; human spot-checks improve quality at higher cost

- Failure signatures:
  - **OOD retrieval drift**: Agent retrieves web-centric memories when operating on mobile/desktop, leading to incorrect actions (Table 3 shows text-based memory degrades OOD; CoMEM generalizes better but not perfectly)
  - **Compression loss**: Agent fails to ground small or low-contrast UI elements if their visual features were not preserved in 8 embeddings
  - **Judge VLM errors**: High false-positive rate in flywheel verification admits unsuccessful trajectories, polluting memory bank

- First 3 experiments:
  1. **Retrieval ablation**: Vary k (3, 10, 50, 100) and measure accuracy on MMInA Shopping; expect monotonic gains with CoMEM, degradation with text memory beyond ~10
  2. **Embedding length sweep**: Compare 4, 8, 16 embeddings per trajectory on grounding accuracy and inference latency; identify knee point where gains plateau
  3. **OOD generalization test**: Train memory encoder on web trajectories only; evaluate on OSWorld (desktop) and GUI-Odyssey (mobile). Expect performance drop vs. in-domain; analyze retrieval relevance to diagnose drift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the memory framework be adapted to handle extreme distribution shifts, such as novel widget types or layouts, where screenshot-only inputs underrepresent critical state information?
- Basis in paper: [explicit] The Limitations section states that "Retrieval can drift under extreme UI shifts" and suggests "expanding retrieval to execution traces or UI graphs" as a solution for non-visual states.
- Why unresolved: The current architecture relies exclusively on visual inputs (screenshots) and standard embeddings, which may fail to capture semantic structure in unfamiliar interfaces.
- Evidence: A comparative study evaluating performance on OOD benchmarks (e.g., OSWorld) when memory inputs are augmented with DOM trees or UI graphs versus screenshots alone.

### Open Question 2
- Question: What governance strategies (e.g., age-aware eviction, clustering-based deduplication) are required to maintain retrieval latency and relevance as the memory bank scales to millions of entries?
- Basis in paper: [explicit] The authors note that "freshness, deduplication, and provenance are hard to govern" and "larger banks stress latency and GPU memory."
- Why unresolved: The paper demonstrates scaling to 100k trajectories but identifies the management of "memory scale" and "freshness" as an unsolved technical challenge requiring hierarchical indices.
- Evidence: Analysis of inference latency and task success rates as the memory bank grows, comparing the proposed FAISS index against hierarchical or sharded indices with active eviction policies.

### Open Question 3
- Question: Can Reinforcement Learning (RL) be effectively integrated to provide credit assignment over retrieved memories, thereby optimizing which specific memories influence policy decisions?
- Basis in paper: [explicit] The Conclusion lists "tighter integration with RL for credit assignment over memories" as a specific direction for future work.
- Why unresolved: The current system uses a static "top-k" retrieval and injection mechanism without a feedback loop to learn the utility or relevance of specific memory embeddings.
- Evidence: Implementation of an RL agent that learns to weight or select memory embeddings, compared against the static retrieval baseline on long-horizon planning tasks.

## Limitations
- Memory compression fidelity: The Q-Former compresses 15k-token trajectories to 8 embeddings, but it's unclear how much visual nuance survives compression without direct ablation studies
- Judge VLM reliability: The auto-scaling flywheel relies on SEAgent-1.0-7B to verify task success, introducing potential false positives/negatives into the memory bank
- OOD generalization ceiling: CoMEM generalizes better than text memory to mobile/desktop environments, but shows performance drop vs. in-domain web tasks without clear diagnosis of the cause

## Confidence
- **High confidence**: The monotonic scaling claim (Figure 1b) is well-supported by ablation data comparing CoMEM to text memory. The cost-effectiveness claim (~$4k/100k trajectories) is credible given similar VLM-driven flywheels in the literature.
- **Medium confidence**: The state-of-the-art comparison to GPT-4o/Claude-4 relies on LLM-as-Judge evaluation, which can be noisy. The 1.2% parameter claim is verifiable from LoRA rank specs, but training details (batch size, learning rate) are underspecified.
- **Low confidence**: Claims about preserving "fine-grained visual cues" (e.g., widget size/position) lack direct evidence. No qualitative analysis or user study validates that compressed embeddings retain these details.

## Next Checks
1. **Embedding fidelity ablation**: Measure grounding accuracy on a held-out test set as embedding length varies (4, 8, 16 vectors). Identify the knee point where gains plateau and compare to inference latency trade-offs.
2. **Retrieval precision analysis**: Compute precision@k for CLIP-based retrieval on a sampled subset of trajectories. Compare retrieval relevance scores for in-domain vs. OOD environments to diagnose drift sources.
3. **Judge VLM error rate**: Manually annotate 100 verified trajectories from the flywheel for true/false positives. Estimate the false-positive rate and simulate its impact on memory bank quality and downstream agent performance.