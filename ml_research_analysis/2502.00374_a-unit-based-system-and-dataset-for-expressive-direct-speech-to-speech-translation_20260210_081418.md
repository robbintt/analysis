---
ver: rpa2
title: A Unit-based System and Dataset for Expressive Direct Speech-to-Speech Translation
arxiv_id: '2502.00374'
source_url: https://arxiv.org/abs/2502.00374
tags:
- speech
- translation
- dataset
- audio
- speech-to-speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel unit-based system and dataset for
  expressive direct speech-to-speech translation (S2ST), addressing the challenge
  of preserving paralinguistic information like emotion and emphasis in translations.
  The authors curate a multilingual dataset from movie audio tracks, ensuring paired
  source and target speech with matched paralinguistic information and duration.
---

# A Unit-based System and Dataset for Expressive Direct Speech-to-Speech Translation

## Quick Facts
- **arXiv ID:** 2502.00374
- **Source URL:** https://arxiv.org/abs/2502.00374
- **Reference count:** 0
- **Primary result:** Novel unit-based S2ST system achieving BLEU 74.6 with preserved paralinguistic features

## Executive Summary
This paper introduces a unit-based system and dataset for expressive direct speech-to-speech translation (S2ST) that preserves paralinguistic information like emotion and emphasis. The authors curate a multilingual dataset from movie audio tracks, ensuring paired source and target speech with matched paralinguistic information and duration. Their approach integrates global style and local pitch transfer techniques to retain emotional characteristics from source speech while maintaining high translation accuracy and naturalness. Experimental results show the model significantly outperforms baseline systems in preserving emphasis, intonation, and rhythm while achieving a BLEU score of 74.6 for translation quality compared to ground truth.

## Method Summary
The system uses HuBERT self-supervised learning to extract continuous speech representations, which are then quantized into discrete units using K-means clustering (K=1000). A reference encoder captures global style attributes from source speech, while pitch and voicing predictors handle local prosodic features. The Unit-HiFi-GAN generator synthesizes target speech conditioned on both global style and local pitch information. The model is trained bidirectionally (Spanish↔English) to mitigate language-specific pitch biases. A two-stage training approach is used: pre-training on high-quality monolingual data followed by fine-tuning on the mixed movie dataset.

## Key Results
- BLEU score of 74.6 for translation quality compared to ground truth (78.2)
- 21% improvement over baseline in emphasis preservation
- 29% improvement in intonation preservation
- Significant gains in rhythm preservation compared to traditional unit-TTS

## Why This Works (Mechanism)

### Mechanism 1: Discrete Unit-Based Speech Representation
Converting speech to discrete units enables direct S2ST without text intermediaries while preserving translatable content. HuBERT extracts continuous representations every 20ms, K-means clusters map these to discrete unit indices, and continuous repetitions are collapsed during translation then expanded during synthesis. This approach captures sufficient linguistic content while remaining language-agnostic for cross-lingual transfer.

### Mechanism 2: Global Style Transfer via Reference Encoder
A reference encoder capturing global style attributes enables emotion preservation across languages without requiring parallel emotional data. The encoder processes source speech to extract a global style embedding that conditions the Unit-HiFi-GAN generator, allowing target-language synthesis to inherit source emotional characteristics while minimizing non-timbral features through speaker attribute prediction.

### Mechanism 3: Local Pitch Conditioning for Fine-Grained Prosody
Explicit pitch and voicing prediction enables preservation of local prosodic features (emphasis, intonation rhythm) that global style alone cannot capture. Pitch predictor and unvoiced/voiced predictor modules are integrated into the Unit-HiFi-GAN architecture, learning frame-level prosodic patterns from reference speech and applying them during synthesis. Bidirectional training mitigates language-specific pitch biases.

## Foundational Learning

- **Concept:** HuBERT Self-Supervised Speech Representation
  - **Why needed here:** Essential for understanding how HuBERT creates continuous speech representations that K-means operates on to extract discrete units
  - **Quick check question:** Can you explain why HuBERT uses masked prediction and how it differs from wav2vec 2.0?

- **Concept:** HiFi-GAN Multi-Period Discriminator Architecture
  - **Why needed here:** The Unit-HiFi-GAN generator builds on HiFi-GAN's ability to handle signals of varying periodicity through multiple sub-discriminators
  - **Quick check question:** What is the purpose of having multiple period-specific discriminators rather than a single discriminator?

- **Concept:** Global vs. Fine-Grained Style Transfer in TTS
  - **Why needed here:** The paper explicitly chooses global style transfer for adaptability to non-parallel scenarios; understanding this tradeoff is critical for architecture decisions
  - **Quick check question:** Why might global style transfer be more suitable for cross-lingual scenarios than fine-grained (token-level) style transfer?

## Architecture Onboarding

- **Component map:** Source speech → HuBERT → K-means → Discrete units → Translation model → Translated units → Unit-HiFi-GAN (conditioned on reference encoder output + pitch predictor) → Target speech

- **Critical path:** The reference encoder path from source audio to generator conditioning is the key style transfer pathway, integrating global style and local pitch features into the synthesis process.

- **Design tradeoffs:**
  - Global vs. Local Style: Global style chosen for non-parallel adaptability, sacrificing fine-grained control
  - Vocabulary Size (K=1000): Balances reconstruction quality vs. sequence length; larger K improves fidelity but increases translation complexity
  - Training Data Mixing: High-quality monolingual data for initial quality, then mixed dataset for robustness; introduces domain shift risk

- **Failure signatures:**
  - Flat/uniform speech output: Indicates reference encoder not conditioning generator effectively
  - Speaker identity leakage: Loss function for minimizing non-timbral features in reference encoder not working
  - Language-specific pitch bias: Bidirectional training not covering both directions equally
  - BLEU degradation with style transfer: Tradeoff between expressivity and translation accuracy

- **First 3 experiments:**
  1. Ablate reference encoder: Train Unit-HiFi-GAN without reference encoder conditioning; measure emotion/intonation scores vs. baseline to isolate global style contribution.
  2. Vocabulary size sweep: Test K={500, 1000, 2000} for discrete units; measure BLEU and audio quality (MOS) to find optimal unit granularity.
  3. Pitch predictor removal: Disable pitch/UV predictors; quantify impact on emphasis/intonation/rhythm scores (hypothesis: local prosody metrics should drop significantly while global emotion remains).

## Open Questions the Paper Calls Out

### Open Question 1
Can translation quality (BLEU) be improved to match baseline performance while retaining expressiveness when training on noisy, diverse movie data? The authors note their model achieves BLEU 28.3 versus baseline 29.2, attributing the disparity to noisy movie data versus clean LJ Speech training data. A modified architecture achieving BLEU >29.2 without reducing emphasis, intonation, and rhythm scores would resolve this.

### Open Question 2
Does the proposed prosody transfer methodology generalize to language pairs with distinct rhythmic or tonal structures, such as English-to-Mandarin? The current study is restricted to Spanish and English, which share similar prosodic roots. Successful application to non-Indo-European language pairs demonstrating comparable emotional emphasis and translation accuracy would resolve this.

### Open Question 3
What is the impact of emotion transfer mechanisms on perceived naturalness of synthesized speech? While the model outperforms baselines in specific metrics like emphasis and rhythm, aggressive style transfer can introduce artifacts making speech sound unnatural. A MOS test specifically evaluating naturalness would reveal whether high emotion and intonation scores correlate with positive listening experience.

## Limitations

- Dataset construction concerns: Movie/TV audio alignment may introduce errors despite WER filtering; assumption of perfect paralinguistic correspondence across dubbed languages may not hold
- Evaluation methodology gaps: Human evaluation lacks inter-rater reliability metrics and doesn't assess potential degradation in linguistic accuracy or naturalness
- Architecture specification incompleteness: Key details like reference encoder architecture and loss function formulations remain unspecified, limiting reproducibility

## Confidence

**High confidence** in: Discrete unit framework's ability to enable direct S2ST without intermediate text representations.

**Medium confidence** in: Effectiveness of combined global style and local pitch transfer for cross-lingual emotion preservation, given evaluation methodology limitations.

**Low confidence** in: Claimed BLEU score of 74.6 relative to ground truth (78.2) without knowing exact reference text and translation error distribution.

## Next Checks

1. **Architecture ablation study**: Implement and evaluate three variants - (a) Unit-HiFi-GAN without reference encoder conditioning, (b) with reference encoder only, and (c) with both reference encoder and pitch predictors. This would isolate the contribution of global vs. local prosody transfer mechanisms.

2. **Cross-language style transfer robustness test**: Evaluate the model's ability to preserve emotion when translating between languages with different prosodic patterns (e.g., Spanish→Mandarin, English→French). This would test whether global style transfer generalizes beyond closely related languages.

3. **Translation quality degradation analysis**: Compare translation quality (BLEU, TER) of expressive outputs against non-expressive baseline outputs on the same utterances. This would quantify the tradeoff between style preservation and translation accuracy.