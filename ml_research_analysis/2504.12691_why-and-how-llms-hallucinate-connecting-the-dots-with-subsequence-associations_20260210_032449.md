---
ver: rpa2
title: 'Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations'
arxiv_id: '2504.12691'
source_url: https://arxiv.org/abs/2504.12691
tags:
- subsequence
- associations
- subsequences
- input
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework based on subsequence
  associations to analyze hallucinations in large language models (LLMs). The key
  insight is that hallucinations occur when dominant hallucinatory associations outweigh
  faithful ones, which can be traced back to specific subsequences in the input.
---

# Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations

## Quick Facts
- **arXiv ID:** 2504.12691
- **Source URL:** https://arxiv.org/abs/2504.12691
- **Authors:** Yiyou Sun; Yu Gai; Lijie Chen; Abhilasha Ravichander; Yejin Choi; Dawn Song
- **Reference count:** 40
- **Primary result:** Introduces a unified framework based on subsequence associations to analyze and trace hallucinations in LLMs

## Executive Summary
This paper introduces a unified framework based on subsequence associations to analyze hallucinations in large language models (LLMs). The key insight is that hallucinations occur when dominant hallucinatory associations outweigh faithful ones, which can be traced back to specific subsequences in the input. The authors theoretically show that decoder-only transformers encode subsequence associations and propose a tracing algorithm that identifies causal subsequences by analyzing hallucination probabilities across diverse, randomized input contexts. Experiments demonstrate that their method outperforms traditional attribution techniques in identifying hallucination causes and that identified subsequences are supported by evidence in the training corpus.

## Method Summary
The paper proposes a Subsequence Association Trace (SAT) algorithm that identifies causal subsequences triggering hallucinations. The method uses beam search to find the input subsequence that maximizes the association score with a hallucinated output. To evaluate this, the authors generate diverse contexts by masking and refilling tokens using BERT, random sampling, or GPT-based methods. They measure reproducibility by calculating the probability that the hallucinated subsequence appears when the identified trigger is present, across four context distributions. The approach is validated on the HALoGEN benchmark using Olmo-7B-Instruct and Llama-70B-Instruct models.

## Key Results
- The SAT algorithm successfully identifies causal subsequences that trigger hallucinations with high reproducibility scores
- Identified subsequences are supported by evidence in the training corpus, validating the tracing approach
- The method outperforms traditional attribution techniques in pinpointing hallucination causes across multiple domains
- Subsequence associations in decoder-only transformers are mathematically grounded, explaining why the tracing approach works

## Why This Works (Mechanism)
The framework works by recognizing that hallucinations occur when dominant hallucinatory associations outweigh faithful ones in the model's probability distribution. These associations are encoded as subsequence weights in decoder-only transformers, where specific input subsequences trigger particular outputs. By tracing these associations back to their origins through diverse context sampling, the SAT algorithm can identify the precise input patterns that cause hallucinations.

## Foundational Learning
- **Subsequence Association:** The probability that a specific output subsequence follows a given input subsequence in the model's learned distribution - needed to understand hallucination triggers; quick check: verify association scores match empirical generation frequencies
- **Decoder-Only Transformer Encoding:** How transformer decoders store and retrieve subsequence associations during generation - needed to justify the theoretical foundation; quick check: confirm association patterns persist across different model sizes
- **Beam Search Optimization:** The search algorithm used to find the most probable causal subsequence - needed to understand the computational approach; quick check: verify beam width affects accuracy as expected
- **Context Approximation:** Methods for generating diverse input contexts to evaluate association strength - needed to understand evaluation methodology; quick check: compare results across different context generation strategies

## Architecture Onboarding
- **Component Map:** Input Sequence -> Subsequence Association Analysis -> Causal Subsequence Identification (SAT) -> Context Generation -> Reproducibility Evaluation -> Training Corpus Verification
- **Critical Path:** The SAT algorithm's beam search identifies the most probable causal subsequence, which is then validated through context generation and reproducibility scoring
- **Design Tradeoffs:** Beam width (B=20) balances accuracy vs. computational cost; larger corpora improve approximation but increase runtime
- **Failure Signatures:** Low reproducibility scores indicate poor association identification; inconsistent results across context methods suggest approximation issues
- **First Experiments:** 1) Run SAT on simple hallucination cases to verify basic functionality; 2) Test different beam widths to find optimal accuracy/compute balance; 3) Validate that identified subsequences appear in training corpus

## Open Questions the Paper Calls Out
- Can identified causal subsequences be used to actively mitigate hallucinations during inference rather than just for post-hoc explanation?
- How does the tracing algorithm perform when hallucinations result from cumulative interactions of multiple weak subsequences?
- To what extent does the choice of context approximation strategy bias the identification of causal subsequences?

## Limitations
- The framework relies on sampling-based approximation, which may miss rare but important subsequences
- Computational overhead is significant due to beam search and multiple context generations
- The approximation quality depends critically on hyperparameters that are not fully specified

## Confidence
- **High Confidence:** Theoretical foundation connecting subsequence associations to hallucination mechanisms is well-established
- **Medium Confidence:** Empirical validation on HALoGEN benchmark is convincing but limited to specific domains
- **Low Confidence:** Effectiveness across diverse domains and model sizes requires further verification

## Next Checks
1. Perform hyperparameter sensitivity analysis by varying masking probability and top-k values to determine their impact on approximation quality
2. Apply the framework to additional hallucination benchmarks beyond HALoGEN to verify cross-domain generalizability
3. Measure computational overhead in production-like settings and compare against simpler attribution methods