---
ver: rpa2
title: The Impact of Scaling Training Data on Adversarial Robustness
arxiv_id: '2509.25927'
source_url: https://arxiv.org/abs/2509.25927
tags:
- in1k
- training
- in21k
- robustness
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how training data characteristics affect
  adversarial robustness across 36 state-of-the-art vision models spanning supervised,
  self-supervised, and contrastive learning approaches, trained on datasets from 1.2M
  to 22B images. The analysis evaluates models under six black-box attack categories
  including random perturbations, geometric masks, COCO object manipulations, ImageNet-C
  corruptions, and ImageNet-R style shifts.
---

# The Impact of Scaling Training Data on Adversarial Robustness

## Quick Facts
- arXiv ID: 2509.25927
- Source URL: https://arxiv.org/abs/2509.25927
- Authors: Marco Zimmerli; Andreas Plesner; Till Aczel; Roger Wattenhofer
- Reference count: 40
- This paper shows adversarial robustness scales logarithmically with both data volume and model size, with data quality and architecture playing more decisive roles than raw scale.

## Executive Summary
This study systematically evaluates how training data characteristics affect adversarial robustness across 36 state-of-the-art vision models spanning supervised, self-supervised, and contrastive learning approaches. Models trained on datasets from 1.2M to 22B images were assessed under six black-box attack categories including random perturbations, geometric masks, COCO object manipulations, ImageNet-C corruptions, and ImageNet-R style shifts. The analysis reveals that robustness follows a logarithmic scaling law with both data volume and model size, but data quality and architecture choices prove more critical than raw scale. Notably, self-supervised models trained on curated datasets like DINOv2 outperform those trained on larger but less curated datasets, challenging the assumption that scale alone drives robustness.

## Method Summary
The study evaluated 36 pre-trained vision models across six black-box attack categories on ImageNet-1K classification. Attack types included random perturbations (color, saturation, contrast, brightness), geometric masks (circles, polygons, concentric shapes at varying opacities), COCO object manipulations, ImageNet-C corruptions (19 types × 5 severities), and ImageNet-R artistic style shifts. Models were assessed using Attack Success Rate (ASR), with scaling laws fitted to relate ASR to data and model size. Additional experiments tested adversarial fine-tuning on ResNet50s with geometric masks and human evaluation comparisons between human and machine vision performance.

## Key Results
- Robustness follows logarithmic scaling laws: a tenfold increase in data reduces ASR by ~3.2%, while a tenfold increase in model size reduces ASR by ~13.4%
- Data quality proves more important than raw scale, with curated datasets like DINOv2 outperforming larger but noisier datasets like LAION
- Adversarial fine-tuning improves structural robustness but fails to generalize to color distribution shifts
- Human evaluation reveals persistent gaps between human and machine vision performance
- Some self-supervised models trained on curated datasets outperform those trained on much larger but less curated datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial robustness improves predictably as model and data scale increase, but with diminishing returns.
- **Mechanism:** The relationship follows a logarithmic scaling law where increasing capacity (parameters) or experience (data) reduces the Attack Success Rate (ASR). Larger parameter counts may allow for more complex decision boundaries that better approximate the true data manifold, reducing blind spots exploited by adversarial perturbations.
- **Core assumption:** The correlation between scale and robustness holds across the evaluated architectures and is not an artifact of specific training tricks unique to large models.
- **Evidence anchors:**
  - [abstract]: "Robustness follows a logarithmic scaling law... a tenfold increase in model size reduces ASR on average by ~13.4%."
  - [section 6.1.3]: "The resulting bivariate scaling law is: ASR = -0.46 log10(x_data) - 12.53 log10(x_model) + 137.67."
  - [corpus]: Related work "Scaling Adversarial Training via Data Selection" (arXiv:2512.22069) supports the thesis that data scaling strategies critically impact robustness.

### Mechanism 2
- **Claim:** Data curation quality is a stronger driver of robustness than raw dataset magnitude, particularly for contrastive learning.
- **Mechanism:** Models trained on curated datasets (e.g., DINOv2) outperform those trained on larger but noisier web-scraped datasets (e.g., LAION). Curated data reduces learning of spurious correlations and noisy features that adversaries typically exploit, forcing models to learn more semantically robust representations.
- **Core assumption:** The performance gap is due to data quality rather than unreported optimizations in the training recipe of the curated models.
- **Evidence anchors:**
  - [abstract]: "Notably, some self-supervised models trained on curated datasets, such as DINOv2, outperform others trained on much larger but less curated datasets."
  - [section 6.1.1]: "This implies that scale without quality control offers minimal benefits... strategic data curation supersedes volume."
  - [corpus]: No specific corpus evidence found regarding "curation vs. volume" specifically in robustness.

### Mechanism 3
- **Claim:** Adversarial fine-tuning creates generalization that is feature-specific (structural) rather than universal (chromatic).
- **Mechanism:** Fine-tuning with geometric masks improves resilience to shape and occlusion changes but fails to transfer to color distribution shifts. This suggests structural invariance and color constancy are learned via separate feature pathways that do not necessarily cross-generalize.
- **Core assumption:** The failure to generalize to color shifts is due to the learning dynamics of invariance, not merely insufficient training steps.
- **Evidence anchors:**
  - [abstract]: "Adversarial fine-tuning of ResNet50s improves generalization across structural variations but not across color distributions."
  - [section G.5.3]: "Color generalization represents the primary limitation... geometric and chromatic invariance are learned separately."
  - [corpus]: No specific corpus evidence found regarding the decoupling of color vs. structural robustness in fine-tuning.

## Foundational Learning

- **Concept:** **Logarithmic Scaling Laws**
  - **Why needed here:** The paper's core quantitative result is a logarithmic equation relating parameters/data to Attack Success Rate (ASR). You must understand that diminishing returns are mathematically baked into the finding—simply throwing 10x more data at the problem yields progressively smaller robustness gains.
  - **Quick check question:** If increasing model parameters by 10x reduces ASR by 13.4%, does doubling the model size again guarantee another 13.4% drop? (Answer: No, the logarithmic relationship implies the gain will be smaller in absolute terms).

- **Concept:** **Black-box vs. White-box Attacks**
  - **Why needed here:** This study evaluates "black-box" attacks where the attacker has no gradient access. This is critical for interpreting why the paper uses semantic perturbations rather than standard gradient-based optimization attacks like PGD used in other literature.
  - **Quick check question:** Does a lower ASR on "Geometric Masks" guarantee safety against gradient-based "White-box" attacks like FGSM? (Answer: No, the threat models are different).

- **Concept:** **Self-Supervised vs. Contrastive Learning**
  - **Why needed here:** The paper distinguishes performance between DINO (self-supervised) and CLIP (contrastive). Understanding that DINO learns from image-image consistency while CLIP learns from image-text alignment helps explain why curated visual data benefits DINO's robustness distinctively.
  - **Quick check question:** Why might CLIP models underperform relative to DINOv2 in this specific robustness test despite having more data? (Answer: CLIP relies on noisy web-text alignment which may introduce spurious robustness weaknesses compared to pure visual curation).

## Architecture Onboarding

- **Component map:** Pre-trained models (36) → Standard preprocessing → Attack injection → Inference → ASR computation
- **Critical path:**
  1. **Preprocessing:** Standard ImageNet transforms + specific perturbation injection (e.g., Geometric Mask overlay)
  2. **Inference:** Pass perturbed image through frozen backbone (or linear probe head for DINO/MAE)
  3. **Evaluation:** Compare Top-1 prediction against ground truth to compute ASR. Note the ASR approximation method: `ASR_approx = (Acc_clean - Acc_adv) / Acc_clean`
- **Design tradeoffs:**
  - **Scale vs. Cost:** The paper shows logarithmic returns. Investing in 10x more compute yields only ~3-13% robustness gain.
  - **Curation vs. Scraping:** Web-scale scraping (LAION) is cheaper but yields lower robustness per parameter than curated datasets (LVD-142M).
  - **Fine-tuning Strategy:** Training on geometric masks fixes structural robustness but leaves color-vulnerabilities open.
- **Failure signatures:**
  - **Opacity Vulnerability:** Performance cliffs occur on Geometric Masks at high opacity (128).
  - **Family Consistency:** If a model fails on "Circle 140" mask, it likely fails on "6-7-2 C1" (worst-case attacks correlate within families).
  - **Color Blindness:** Fine-tuned ResNet50s fail on unseen color schemes (C3) despite mastering structural shapes.
- **First 3 experiments:**
  1. **Sanity Check the ASR Approximation:** Implement the `ASR_approx` formula on a subset of images where you have both clean and adversarial versions to verify the 3.09% underestimation bias reported in Section 3.2.
  2. **Replicate the Fine-Tuning Generalization Failure:** Train a ResNet50 on "GeometricMasksV2" using color scheme C1. Evaluate immediately on C3 (unseen color). Confirm that ASR spikes specifically for C3 to verify the "disentangled learning" mechanism.
  3. **Verify Scaling Coefficients:** Pick 3 models of different sizes (e.g., ViT-S, ViT-B, ViT-L) from the provided list. Measure ASR on "ImageNet-C Severity 5". Plot log(Params) vs ASR and check if it aligns with the slope of roughly -13.4% per decade of scale.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the observed logarithmic scaling laws for adversarial robustness persist when the attack taxonomy is expanded to include white-box, gradient-based methods?
  - **Basis in paper:** [explicit] The Conclusion states, "Future work should expand the attack taxonomy to include gradient-based methods to test if these scaling trends hold."
  - **Why unresolved:** The study exclusively evaluated a black-box threat model to emphasize semantic validity and structured occlusions, intentionally excluding gradient-based attacks.
  - **What evidence would resolve it:** Re-evaluating the 36 models using white-box attacks (e.g., AutoAttack or PGD) to see if the ASR logarithmic scaling coefficients remain consistent.

- **Open Question 2:** To what extent do training data and model scale influence robustness in vision tasks requiring complex spatial reasoning, such as object detection and segmentation?
  - **Basis in paper:** [explicit] The Conclusion notes that "Extending evaluations to tasks like object detection and segmentation would further clarify how data and model scale influence robustness in scenarios requiring complex spatial reasoning."
  - **Why unresolved:** The current investigation was restricted to ImageNet-1K classification, leaving the generalization of these robustness scaling laws to other domains untested.
  - **What evidence would resolve it:** Applying the same scaling analysis and attack categories (e.g., GeometricMasks) to state-of-the-art object detection and segmentation models.

- **Open Question 3:** Can adversarial fine-tuning strategies be modified to achieve robustness that generalizes across both structural variations and unseen color distributions?
  - **Basis in paper:** [inferred] Section 6 reports that while fine-tuning improves generalization across structural variations, "this robustness is brittle and fails to transfer across color distributions," identifying this as a primary limitation.
  - **Why unresolved:** The experiments demonstrated that models trained on specific color schemes (C1, C2) suffered significant performance degradation on unseen schemes (C3), suggesting chromatic invariance is not learned alongside geometric invariance.
  - **What evidence would resolve it:** A training regime that incorporates comprehensive color augmentations alongside geometric masks, showing improved ASR on novel color schemes without sacrificing structural robustness.

## Limitations

- The logarithmic scaling coefficients are fitted to a specific set of 36 models and attack types; generalization to other architectures or threat models remains untested
- Data curation quality is assessed through benchmark performance rather than direct analysis of dataset composition; the mechanism by which curated datasets improve robustness is inferred rather than demonstrated
- The disentanglement of structural vs. chromatic generalization in fine-tuning is observed but not causally explained at the feature level

## Confidence

- **High Confidence**: The logarithmic scaling law relationship between model/data size and ASR reduction (supported by systematic experiments across 36 models and 6 attack categories)
- **Medium Confidence**: The superiority of curated datasets over raw scale for contrastive learning (supported by comparative results but with potential confounding factors from different training recipes)
- **Medium Confidence**: The feature-specific nature of adversarial fine-tuning generalization (observed behavior with reasonable explanation, but mechanism not fully validated)

## Next Checks

1. Test the scaling law coefficients on a held-out architecture family (e.g., ConvNeXt) to verify generalization beyond the original 36 models
2. Implement and evaluate a controlled experiment comparing identical models trained on curated vs. non-curated versions of the same dataset to isolate curation effects
3. Conduct ablation studies on fine-tuning with combined geometric and color augmentations to test whether the structural-chromatic disentanglement persists under multimodal training