---
ver: rpa2
title: 'ModelCitizens: Representing Community Voices in Online Safety'
arxiv_id: '2507.05455'
source_url: https://arxiv.org/abs/2507.05455
tags:
- toxicity
- context
- citizens
- annotators
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MODEL CITIZENS, a dataset of 6.8K social
  media posts and 40K toxicity annotations across eight identity groups. It captures
  community perspectives on toxicity by collecting annotations from both ingroup and
  outgroup annotators, and augments posts with LLM-generated conversational contexts
  to model real-world social media discourse.
---

# ModelCitizens: Representing Community Voices in Online Safety

## Quick Facts
- arXiv ID: 2507.05455
- Source URL: https://arxiv.org/abs/2507.05455
- Reference count: 34
- Key outcome: Dataset of 6.8K social media posts with 40K annotations across 8 identity groups shows significant ingroup-outgroup disagreement on toxicity, with models fine-tuned on this data outperforming GPT-4o-mini by 5.5%

## Executive Summary
MODEL CITIZENS is a dataset capturing community perspectives on online toxicity through annotations from both ingroup and outgroup members across eight identity groups. The dataset includes 6.8K social media posts with 40K toxicity annotations, augmented with LLM-generated conversational contexts to model real-world discourse. Analysis reveals significant disagreements between ingroup and outgroup annotators, with outgroup annotators more frequently labeling content as toxic. State-of-the-art toxicity detection models underperform on this dataset, particularly for context-augmented posts. The authors release two fine-tuned models (LLAMACITIZEN-8B and GEMMACITIZEN-12B) that outperform GPT-4o-mini by 5.5% on in-distribution evaluations, demonstrating the value of community-informed annotations for inclusive content moderation.

## Method Summary
The dataset was constructed by collecting 6.8K social media posts and gathering 40K toxicity annotations from annotators identifying with eight different identity groups, distinguishing between ingroup and outgroup perspectives. Posts were augmented with LLM-generated conversational contexts to simulate real social media discourse. Two models (LLAMACITIZEN-8B and GEMMACITIZEN-12B) were fine-tuned on this dataset to improve toxicity detection performance, particularly for context-augmented content. The evaluation compared these models against GPT-4o-mini on in-distribution data, showing improved performance that highlights the importance of incorporating diverse community perspectives in toxicity detection systems.

## Key Results
- Outgroup annotators labeled content as toxic more frequently than ingroup annotators, revealing significant perspective differences
- State-of-the-art toxicity detection models showed underperformance on MODEL CITIZENS, especially for context-augmented posts
- Fine-tuned models LLAMACITIZEN-8B and GEMMACITIZEN-12B outperformed GPT-4o-mini by 5.5% on in-distribution evaluations

## Why This Works (Mechanism)
The approach works by capturing diverse community perspectives on toxicity through ingroup and outgroup annotations, recognizing that toxicity perceptions vary significantly across different identity groups. By augmenting posts with LLM-generated conversational contexts, the dataset better reflects real-world social media discourse where posts exist within broader conversations. The fine-tuning process leverages this community-informed data to create models that are more sensitive to the nuanced ways different groups experience and perceive online toxicity.

## Foundational Learning

**Ingroup vs Outgroup Annotation Bias**
- Why needed: Toxicity perceptions vary significantly based on whether annotators share identity with post authors
- Quick check: Compare annotation distributions between ingroup and outgroup annotators on same posts

**Context-Aware Toxicity Detection**
- Why needed: Social media posts gain meaning and potential toxicity from surrounding conversation
- Quick check: Measure performance drop when removing contextual information from evaluation

**Community-Informed Model Fine-tuning**
- Why needed: Standard toxicity models fail to capture diverse community perspectives
- Quick check: Compare model performance on ingroup vs outgroup annotated data

## Architecture Onboarding

**Component Map**
- Social Media Post Collection -> Annotation Pipeline (Ingroup/Outgroup) -> Context Generation (LLM) -> Dataset Assembly -> Model Fine-tuning (LLAMACITIZEN-8B, GEMMACITIZEN-12B) -> Evaluation

**Critical Path**
The critical path involves collecting posts, gathering annotations from both ingroup and outgroup annotators, generating contextual information through LLM, assembling the dataset, and fine-tuning models on this enriched data for improved toxicity detection.

**Design Tradeoffs**
The primary tradeoff involves using synthetic LLM-generated contexts to capture discourse patterns versus potential quality concerns and lack of authentic conversational dynamics. The decision to focus on eight identity groups balances comprehensive representation with practical annotation feasibility.

**Failure Signatures**
Models may overfit to the specific identity groups represented in the dataset, fail to generalize to unrepresented communities, or struggle with contexts that don't match the LLM generation patterns. Performance may degrade significantly on out-of-distribution data or posts from communities not well-represented in the training set.

**First Experiments**
1. Evaluate model performance breakdown by identity group to identify potential bias
2. Compare annotation agreement rates between ingroup and outgroup annotators
3. Test model performance on posts with vs without context augmentation

## Open Questions the Paper Calls Out
None

## Limitations
- The 6.8K post sample may not be representative of broader social media discourse patterns
- LLM-generated contexts may not fully capture authentic social media conversation dynamics
- Evaluation is limited to in-distribution data, limiting generalizability claims

## Confidence

**High Confidence Claims:**
- Significant ingroup-outgroup disagreement patterns in toxicity annotations are clearly demonstrated through statistical differences

**Medium Confidence Claims:**
- Toxicity detection model underperformance is supported but limited to specific model comparisons and evaluation conditions
- Fine-tuned model efficacy shows improvement but evaluated on narrow scope with potential overfitting concerns

## Next Checks

1. Evaluate the fine-tuned models on an external, independently collected toxicity dataset to assess true generalization performance beyond the training distribution

2. Conduct qualitative validation with human annotators from diverse identity groups to verify whether LLM-generated contexts accurately reflect real social media discourse patterns and community norms

3. Perform ablation study removing context-augmented posts to quantify the specific impact of contextual information on both human annotation patterns and model performance