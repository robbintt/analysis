---
ver: rpa2
title: 'Optimising ChatGPT for creativity in literary translation: A case study from
  English into Dutch, Chinese, Catalan and Spanish'
arxiv_id: '2504.18221'
source_url: https://arxiv.org/abs/2504.18221
tags:
- translation
- chatgpt
- language
- text
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the optimal settings for ChatGPT to generate
  creative literary translations from English into Dutch, Chinese, Catalan, and Spanish.
  Researchers tested six configurations combining text granularity (paragraph vs.
---

# Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish

## Quick Facts
- arXiv ID: 2504.18221
- Source URL: https://arxiv.org/abs/2504.18221
- Reference count: 22
- Primary result: Minimal prompt "Translate creatively" at temperature 1.0 yielded best creative translations, outperforming other configurations and DeepL in Spanish, Dutch, and Chinese, but ChatGPT consistently underperformed human translation.

## Executive Summary
This study systematically tested six configurations of ChatGPT for creative literary translation from English into Dutch, Chinese, Catalan, and Spanish. The configurations varied text granularity (paragraph vs. document level), temperature settings (0.0 vs. 1.0), and prompting strategies (minimal vs. genre/author context vs. direct creativity request). Using a Creativity Score that balanced creative shifts against translation errors, researchers found that minimal prompting with "Translate creatively" at temperature 1.0 produced the most creative outputs, though with higher error rates that prevented ChatGPT from matching human-level creativity. The study revealed that less prompting information generally produced better creative results, but this varied by language and text length.

## Method Summary
The study used gpt-4o-2024-08-06 API to translate Kurt Vonnegut's short story "2BR02B" (48 sentences, 602 words) from English into four languages. The experiment tested three sequential phases: granularity (paragraph vs. document input), temperature (0.0 vs. 1.0), and prompting strategies (minimal vs. context-rich). The primary evaluation metric was the Creativity Index, calculated as (#Creative Shifts / #UCPs - #error points / #words) × 100, where Creative Shifts included abstraction, concretization, and modification of source text. Manual annotation by language experts identified 54 Units of Creative Potential within the text. Results were compared against DeepL and human reference translations.

## Key Results
- Minimal prompt "Translate the following text into [TG] creatively" at temperature 1.0 yielded the best creative translations across most languages
- ChatGPT underperformed human translation in all languages despite optimal configurations
- Document-level translation improved performance for Catalan and Spanish but degraded it for Chinese and Dutch, with degradation worsening toward document end
- Standard automatic metrics (BLEU, chrF, COMET) showed poor correlation with human creativity scores except for Dutch

## Why This Works (Mechanism)

### Mechanism 1: Temperature Modulation Creates Creativity-Acceptability Tradeoff
- Claim: Higher temperature values increase creative output diversity but simultaneously elevate error rates, creating a measurable tradeoff.
- Mechanism: Temperature adjusts the probability distribution over next tokens; higher values flatten this distribution, allowing lower-probability (more novel) selections while reducing the model's confidence in grammatically correct completions.
- Core assumption: The Creativity Index formula adequately captures the tradeoff between novelty (creative shifts) and acceptability (error penalty).
- Evidence anchors:
  - [abstract]: "prompting ChatGPT with a minimal instruction... at the temperature of 1.0 outperforming other configurations"
  - [Section 4.2]: "For most languages, a temperature value of 1.0 outputs more CSs but also more errors"
  - [corpus]: Weak direct support; corpus papers focus on post-editing workflows rather than temperature effects specifically.
- Break condition: If temperature exceeds ~1.1, the paper notes "word vomit" and incoherent output, breaking the creativity mechanism entirely.

### Mechanism 2: Minimal Prompting Reduces Constraint-Induced Error Accumulation
- Claim: Simpler prompts ("Translate creatively") produce more creative shifts with fewer errors than detailed prompts specifying genre/author context.
- Mechanism: Assumption: Detailed prompts may anchor the model to training-data patterns associated with the specified domain, inadvertently constraining novel solutions while introducing factual errors about the specified author/genre.
- Evidence anchors:
  - [abstract]: Minimal instruction "Translate the following text into [TG] creatively" yielded best creative translations
  - [Section 4.3]: "For all our languages, Prompt 3 (3b) has better solutions than Prompt 2 (3a) as it generates more CSs and fewer errors"
  - [corpus]: No direct corpus evidence for this counterintuitive finding; related papers do not test prompt complexity.
- Break condition: For Catalan, minimal prompting failed to translate sobriquets (nicknames), indicating language-specific failures where domain context may actually help.

### Mechanism 3: Granularity Effects Are Language-Dependent Due to Context-Error Interactions
- Claim: Document-level translation improves performance for some languages (Catalan, Spanish) but degrades it for others (Chinese, Dutch), with degradation worsening toward document end.
- Mechanism: Assumption: Longer contexts provide coherence benefits but may cause attention degradation or accumulated errors for languages where the model has weaker training representation.
- Evidence anchors:
  - [Section 4.1]: "ChatGPT tends to perform progressively worse for ZH as it processes the whole document"
  - [Section 4.1]: Document-level led to more grammatical errors, hallucinations, and typos for Dutch; Chinese showed critical errors including "ass-carrier" mistranslation
  - [corpus]: "Extending CREAMT" paper notes LLMs offer "improved capabilities for context-aware" translation, supporting context benefits but not explaining language asymmetry.
- Break condition: For texts longer than the evaluated short story (~2,500 words), performance degradation may accelerate.

## Foundational Learning

- **Creative Shifts (CS) Taxonomy (Abstraction/Concretization/Modification)**
  - Why needed here: The Creativity Index depends on correctly classifying translation deviations from source text into these three categories or identifying reproductions/omissions.
  - Quick check question: If "seven feet tall" is translated as "two meters," is this abstraction (generalizing), concretization (making explicit), or modification (different solution)?

- **Temperature Parameter in Autoregressive Decoding**
  - Why needed here: The study's core intervention relies on understanding how temperature reshapes probability distributions over token sequences.
  - Quick check question: At temperature 0.0, would you expect identical outputs across multiple runs? (Trick question: the paper shows non-determinism even at T=0.0, chrF < 100)

- **Translation Quality Metrics (MQM/DQF Framework)**
  - Why needed here: Error annotation follows the DQF-MQM severity scale (Minor/Major/Critical), which directly impacts the Creativity Index denominator.
  - Quick check question: If a title's wordplay is left untranslated and readers cannot understand the story's premise, is this a Minor, Major, or Critical error? (Answer: Critical—defined as disrupting "the understanding of the entire story")

## Architecture Onboarding

- **Component map:** Granularity selection -> Temperature tuning -> Prompt strategy -> Creativity Index calculation -> Baseline comparison
- **Critical path:** 1. Select optimal granularity per language (document for CA/ES; paragraph for ZH/NL) 2. Apply temperature 1.0 for most languages (0.0 for CA based on results) 3. Use minimal creative prompt ("Translate creatively") for final output 4. Expect iterative refinement due to output randomness
- **Design tradeoffs:** Higher creativity comes at error-cost; the study's best settings still showed error points 2-5× higher than human translation. Language-specific optimization required—no universal best configuration. Automatic metrics (BLEU, chrF, COMET) showed poor correlation with human creativity scores.
- **Failure signatures:** "Word vomit" at temperatures >1.0 (incoherent output), progressive degradation in long-document translation for underrepresented languages, sobriquet/nickname non-translation in minimal-prompt settings (Catalan-specific), hallucinated words (e.g., Dutch "ontslagbaar" for "formidable")
- **First 3 experiments:** 1. Replicate with a different literary text to test whether "less is more" prompting generalizes beyond the Vonnegut story 2. Test intermediate temperature values (0.3, 0.5, 0.7) to map the creativity-error curve more precisely 3. Add few-shot prompting with example creative translations to test whether exemplars outperform minimal instructions

## Open Questions the Paper Calls Out

- Does the requirement for multiple iterations to find optimal settings undermine the efficiency of using ChatGPT in professional literary translation workflows? The authors explicitly state that trying different alternatives and still obtaining a sub-optimal result does not seem the best solution for practicing translators, and question how the unpredictability of outputs fits into contexts meant to increase performance.
- Can automatic evaluation metrics (AEMs) be effectively calibrated to correlate with human assessments of creativity in literary translation? The analysis of AEMs found that only for Dutch did metrics like chrF and COMET-Kiwi correlate with human creativity scores; the authors refrain from drawing any strong conclusion regarding the utility of AEMs for evaluating creativity.
- Why does optimal text granularity (paragraph vs. document) differ by language, and can this inconsistency be mitigated? The results show that document-level translation improved performance for Spanish and Catalan but caused significantly more errors for Chinese and Dutch, leading to different granularity settings for different languages.

## Limitations

- The Creativity Index depends heavily on subjective human annotation with only one annotator per language, introducing substantial uncertainty about replicability
- The study tested only one literary text (Vonnegut's "2BR02B") and four language pairs, limiting generalizability
- The temperature parameter's effect is not fully mapped, testing only T=0.0 versus T=1.0 rather than intermediate values

## Confidence

- **High Confidence:** The basic temperature-creativity tradeoff (T=1.0 increases creative shifts but also errors) and the general finding that ChatGPT underperforms human translation in creative literary tasks
- **Medium Confidence:** The optimal configurations (minimal prompting + T=1.0 for most languages) are likely valid for this specific text but may not generalize to other literary genres or longer works
- **Low Confidence:** The exact numerical Creativity Index values and the precise ranking of configurations should not be treated as reproducible due to annotator subjectivity and API non-determinism

## Next Checks

1. **Multi-Annotator Validation:** Replicate the annotation process with 3-5 annotators per language to establish inter-annotator agreement rates and confidence intervals for the Creativity Index
2. **Temperature Gradient Analysis:** Test intermediate temperature settings (0.3, 0.5, 0.7, 0.9) to map the full creativity-error tradeoff curve and identify whether T=1.0 is truly optimal
3. **Cross-Text Generalization Test:** Apply the optimal configurations to a different literary genre (e.g., poetry or magical realism) to determine whether the "minimal prompting yields maximum creativity" finding holds across different narrative styles and creative demands