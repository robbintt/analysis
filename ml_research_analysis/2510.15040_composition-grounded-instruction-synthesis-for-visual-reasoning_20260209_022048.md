---
ver: rpa2
title: Composition-Grounded Instruction Synthesis for Visual Reasoning
arxiv_id: '2510.15040'
source_url: https://arxiv.org/abs/2510.15040
tags:
- reasoning
- question
- questions
- answer
- cogs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COGS (COmposition-Grounded instruction Synthesis) is a data-efficient
  framework that equips pretrained multimodal large language models with advanced
  reasoning capabilities by decomposing seed questions into primitive perception and
  reasoning factors, recomposing these factors with new images to generate synthetic
  question-answer pairs, and using process rewards during reinforcement learning fine-tuning.
  Evaluated on chart reasoning and webpage understanding tasks, COGS improves performance
  significantly, especially on reasoning-heavy and compositional questions, and demonstrates
  transferable benefits when training on mixtures of datasets.
---

# Composition-Grounded Instruction Synthesis for Visual Reasoning

## Quick Facts
- arXiv ID: 2510.15040
- Source URL: https://arxiv.org/abs/2510.15040
- Authors: Xinyi Gu; Jiayuan Mao; Zhang-Wei Hong; Zhuoran Yu; Pengyuan Li; Dhiraj Joshi; Rogerio Feris; Zexue He
- Reference count: 40
- Key outcome: COGS improves reasoning accuracy on chart and webpage tasks, especially for compositional and multi-step questions, by generating synthetic data from factor decomposition and using process-level rewards during RL fine-tuning.

## Executive Summary
COGS is a data-efficient framework that equips pretrained multimodal large language models with advanced reasoning capabilities by decomposing seed questions into primitive perception and reasoning factors, recomposing these factors with new images to generate synthetic question-answer pairs, and using process rewards during reinforcement learning fine-tuning. Evaluated on chart reasoning and webpage understanding tasks, COGS improves performance significantly, especially on reasoning-heavy and compositional questions, and demonstrates transferable benefits when training on mixtures of datasets.

## Method Summary
COGS decomposes complex seed questions into primitive perception and reasoning factors via MLLM prompting, constructs a factor pool, and recombines factors with new images to generate synthetic QA pairs with subquestions and intermediate answers. The model is then fine-tuned with GRPO using process-level rewards (ProcessRM-max) that reward correct intermediate reasoning steps. This approach improves reasoning accuracy on chart and webpage understanding tasks, particularly for compositional and multi-step questions.

## Key Results
- Significant accuracy gains on ChartQAPro, VisualWebBench, and MMC-Bench, especially on reasoning-heavy and compositional questions.
- ProcessRM-max outperforms StandardRM and ProcessRM-sum by preserving policy order and leveraging subquestion correctness.
- Training on mixed chart and webpage factors improves cross-domain generalization compared to single-domain training.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing complex questions into primitive factors enables scalable generation of diverse, grounded training data.
- **Mechanism:** A pretrained MLLM is prompted to break each seed question into a sequence of perception and reasoning factors, recovering a structured representation \(q \mapsto \{f_1, f_2, \dots, f_k\}\).
- **Core assumption:** Complex questions in artificial image domains admit a factorized structure; the MLLM can reliably identify these factors given in-context examples.
- **Evidence anchors:**
  - [abstract] "The key idea is to decompose each seed question into primitive perception and reasoning factors..."
  - [Section 3.2] "We obtain such decompositions by prompting a MLLM... This step essentially recovers the factorized representations of the given question \(q \mapsto \{f_1, f_2, \dots, f_k\}\)."
  - [corpus] VisualSphinx generates large-scale synthetic vision logic puzzles, supporting the feasibility of synthetic data for reasoning, but does not specifically validate factor decomposition.
- **Break condition:** If factors are ambiguous, incomplete, or mislabeled, recomposed questions may be invalid or unanswerable.

### Mechanism 2
- **Claim:** Recomposing sampled factors with new images yields novel, compositional questions that expand the training distribution.
- **Mechanism:** Factors are sampled from the pooled factor set \(F\) and combined with unlabeled images to generate new question–answer pairs, each with subquestions and intermediate answers.
- **Core assumption:** Recomposed questions remain semantically valid and answerable from the new image; factor combinations preserve reasoning coherence.
- **Evidence anchors:**
  - [abstract] "...which can then be systematically recomposed with new images to generate large collections of synthetic question-answer pairs."
  - [Section 3.3] "We prompt a MLLM with this input to generate new subquestions of similar kinds but grounded on the new image."
  - [corpus] SynthRL uses verifiable data synthesis for visual reasoning but includes explicit verification; COGS does not report a verification step, so corpus support is indirect.
- **Break condition:** If recomposed questions are syntactically valid but semantically nonsensical (e.g., incompatible factor chains), the model may learn spurious patterns.

### Mechanism 3
- **Claim:** Process-level rewards derived from factor annotations provide finer-grained supervision than final-answer rewards, improving reasoning fidelity.
- **Mechanism:** Each generated question includes subquestions and answers; during RL fine-tuning, a reward model verifies intermediate steps, yielding a subquestion hit rate \(r_{\text{sub}}\) combined with final-answer correctness via ProcessRM-max.
- **Core assumption:** Correct intermediate reasoning steps causally contribute to correct final answers; subquestion correctness is a reliable shaping signal.
- **Evidence anchors:**
  - [abstract] "...enabling reinforcement learning with factor-level process rewards."
  - [Section 3.4] "ProcessRM-max preserves policy orders. For any policies \(\pi_1, \pi_2\), \(\text{sign}(V_f(\pi_1) - V_f(\pi_2)) = \text{sign}(\mathbb{E}[r_{\text{max}}|\pi_1] - \mathbb{E}[r_{\text{max}}|\pi_2])\)."
  - [corpus] SEED-Bench-R1 explores RL for video understanding but does not specifically address process rewards; corpus support is weak for this specific mechanism.
- **Break condition:** If subquestions are noisy or decompositions inconsistent, process rewards may misrank policies despite the theoretical order-preservation property.

## Foundational Learning

- **Concept: Factorized question representation**
  - **Why needed here:** Enables systematic decomposition and recomposition of complex queries.
  - **Quick check question:** Given a question "What is the difference between the highest and lowest values in this bar chart?", can you identify at least three underlying factors?

- **Concept: Compositional data generation**
  - **Why needed here:** Expands a small seed set into a diverse synthetic dataset without additional human annotation.
  - **Quick check question:** If you have perception factors "identify value A" and "identify value B", and a reasoning factor "compute difference", how would you recompose them into a new question for a different chart?

- **Concept: Reinforcement learning with process rewards**
  - **Why needed here:** Provides step-level feedback to encourage faithful multi-step reasoning.
  - **Quick check question:** In a multi-step arithmetic problem, why might rewarding intermediate correct steps be better than only rewarding the final answer?

## Architecture Onboarding

- **Component map:** Seed questions -> MLLM decomposition -> factor pool -> factor recomposition with new images -> synthetic QA pairs -> RL fine-tuning with ProcessRM-max -> evaluated model.
- **Critical path:** Seed questions → factor decomposition → factor pool construction → factor recomposition with new images → synthetic dataset → RL fine-tuning with process rewards → evaluated model.
- **Design tradeoffs:**
  - Factor pool quality vs. coverage: Small seed sets may limit factor diversity; overly broad categories may reduce specificity.
  - Reward model choice: ProcessRM-max preserves policy order but may underweight final-answer accuracy; ProcessRM-sum can misrank policies.
  - Image source selection: Using images with metadata (e.g., chart tables) improves answer precision but may reduce generalizability.
- **Failure signatures:**
  - Low-quality synthetic data: Questions that are unanswerable or contradictory with images.
  - Reward hacking: Model exploits subquestion verification without improving final-answer accuracy.
  - Domain overfitting: High performance on in-distribution synthetic data but poor transfer to held-out datasets.
- **First 3 experiments:**
  1. **Ablate factor decomposition:** Use random or trivial factor assignments to assess the contribution of structured decomposition.
  2. **Compare reward models:** Evaluate StandardRM, ProcessRM-sum, and ProcessRM-max on held-out questions to validate Proposition 3.1.
  3. **Test cross-domain transfer:** Train on chart factors and evaluate on webpage reasoning to probe generalization of learned reasoning patterns.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the COGS framework be effectively extended to handle long-context reasoning over visually rich documents containing multiple charts or webpage screenshots?
  - **Basis in paper:** [explicit] The authors state in the Future Work section, "our experiments focus on single charts and single webpage screenshots; extending COGS to long-context reasoning over visually rich documents will broaden its scope."
  - **Why unresolved:** The current implementation and evaluation are restricted to single-image inputs and do not address the complexities of cross-image reasoning or long-context dependency.
  - **What evidence would resolve it:** Successful application and evaluation of the framework on multi-image document understanding benchmarks (e.g., multi-chart QA).

- **Open Question 2:** How can the factor-level data synthesis approach of COGS be integrated into the pretraining stage of MLLMs rather than just the fine-tuning stage?
  - **Basis in paper:** [explicit] The authors list as a future direction: "it is important to study how our data synthesis can be integrated into the pretraining stage of MLLMs."
  - **Why unresolved:** The current study demonstrates the efficacy of COGS during fine-tuning (specifically RL fine-tuning), but it is unknown if the synthetic data benefits the foundational pretraining phase.
  - **What evidence would resolve it:** Experiments comparing standard pretraining data mixtures versus those augmented with COGS-generated synthetic factor data.

- **Open Question 3:** To what extent do the reasoning capabilities acquired through COGS transfer to downstream agentic tasks, such as chart code editing or web agents?
  - **Basis in paper:** [explicit] The authors propose investigating "how the reasoning capabilities acquired through COGS transfer to downstream tasks—such as chart code editing or web agent applications."
  - **Why unresolved:** The current evaluation is limited to QA benchmarks (ChartQAPro, VisualWebBench) and does not measure performance on generative or agentic tasks.
  - **What evidence would resolve it:** Evaluating models fine-tuned with COGS on downstream tasks like code generation for charts or web navigation actions.

- **Open Question 4:** Does ProcessRM-max preserve policy order in domains where final-answer rewards are continuous rather than binary?
  - **Basis in paper:** [inferred] In Section 3.4 (Proposition 3.1 proof sketch), the authors note that ProcessRM-max preserves policy orders "When r_final take values from [0,1]... iff. lambda * r_sub < r_final," implying potential limitations in non-binary reward settings.
  - **Why unresolved:** The theoretical guarantee for the max-based reward strategy is restricted to binary final rewards, creating uncertainty about its efficacy in tasks with continuous scoring.
  - **What evidence would resolve it:** Empirical analysis of ProcessRM-max performance on tasks with continuous evaluation metrics (e.g., semantic similarity scores).

## Limitations
- The process reward mechanism's effectiveness is primarily supported by theoretical order-preservation, not robust empirical comparison with simpler reward schemes.
- The assumption that subquestion correctness causally improves final-answer accuracy is not directly validated.
- The framework's reliance on LLM-based generation introduces potential noise in factor extraction and synthetic data quality, which is not systematically quantified.

## Confidence
- **High:** Factor decomposition as a means to structure complex questions; synthetic data generation via recomposition.
- **Medium:** Effectiveness of process rewards in improving reasoning over final-answer rewards; generalizability of factor pools across domains.
- **Low:** Causal link between subquestion correctness and final-answer improvement; robustness of synthetic data to generation noise.

## Next Checks
1. **Ablate process rewards:** Replace ProcessRM-max with StandardRM or ProcessRM-sum in GRPO and compare final accuracy to isolate the benefit of process-level supervision.
2. **Test factor quality:** Manually annotate a subset of decomposed factors for accuracy and completeness; measure correlation with downstream model performance.
3. **Cross-domain transfer:** Train COGS on chart reasoning factors and evaluate directly on webpage understanding to assess domain generality of learned reasoning patterns.