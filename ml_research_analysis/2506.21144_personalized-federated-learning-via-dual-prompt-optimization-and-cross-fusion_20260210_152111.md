---
ver: rpa2
title: Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion
arxiv_id: '2506.21144'
source_url: https://arxiv.org/abs/2506.21144
tags:
- learning
- federated
- prompts
- local
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a personalized federated learning framework,
  pFedDC, that addresses the challenge of data heterogeneity in federated learning
  by simultaneously tuning both vision and text prompts. Unlike existing methods that
  rely solely on text prompt adaptation, pFedDC employs dual-prompt learning and cross-attention
  mechanisms to fuse global and local knowledge across both modalities.
---

# Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion

## Quick Facts
- arXiv ID: 2506.21144
- Source URL: https://arxiv.org/abs/2506.21144
- Reference count: 40
- Primary result: Achieves up to 99.45% accuracy on Caltech101 and Flowers102 under Dirichlet-based label shift

## Executive Summary
This paper introduces pFedDC, a personalized federated learning framework that addresses data heterogeneity challenges by leveraging dual-prompt optimization and cross-fusion mechanisms. Unlike existing methods that adapt only text prompts, pFedDC simultaneously tunes both vision and text prompts while employing cross-attention modules to adaptively integrate global and local knowledge. The framework demonstrates superior performance across nine diverse datasets with various heterogeneity types, including label and domain shifts, achieving state-of-the-art results while maintaining communication efficiency.

## Method Summary
pFedDC implements a federated learning framework where each client maintains both global and local prompts for vision and text modalities, along with local cross-attention modules. The method uses CLIP ViT-B/16 as a frozen backbone and optimizes only the prompts and attention weights via SGD. Global prompts are aggregated using weighted FedAvg, while local prompts and attention modules remain private to each client. The cross-attention mechanism dynamically fuses global and local prompts before feeding them into the frozen encoders, allowing adaptive integration of shared and personalized knowledge without requiring full model updates.

## Key Results
- Achieves 99.45% accuracy on Caltech101 and Flowers102 under Dirichlet-based label shift
- Consistently outperforms state-of-the-art personalized FL methods across all nine evaluated datasets
- Demonstrates strong resilience to increasing client numbers, varying participation rates, and limited communication rounds
- Ablation study confirms dual-prompt learning and cross-attention contribute 1-2% accuracy improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual prompts alongside text prompts decouple domain adaptation from semantic classification, handling feature shifts that text-only methods miss.
- **Mechanism:** Text prompts tune the "classifier" (semantic labels), while vision prompts tune the "feature extractor" (visual representations). This allows the model to adapt the frozen backbone to local visual domains rather than forcing text to compensate for visual misalignment.
- **Core assumption:** The frozen pre-trained encoder is sufficiently expressive that low-rank visual prompts can steer feature extraction without updating backbone weights.
- **Evidence anchors:** [Page 7, Section 3.2]: "vision prompts enhance robustness to visual misalignment"; [Page 2, Section 1]: "Visual features corresponding to the same category may vary significantly across clients."

### Mechanism 2
- **Claim:** Cross-attention modules provide a learnable, adaptive interface to balance global consensus and local personalization without hard-coded weighting.
- **Mechanism:** Cross-attention computes query/key/value interactions between global and local prompts, dynamically deciding which global features to keep and which local features to emphasize per instance.
- **Core assumption:** Client data distributions possess shared underlying structures (captured by global prompts) that are relevant but need local modulation.
- **Evidence anchors:** [Abstract]: "A cross-fusion module is designed to adaptively integrate prompts from different levels"; [Page 14, Table 6]: Ablation shows removing attention drops accuracy by ~1-2%.

### Mechanism 3
- **Claim:** Retaining local prompts and cross-attention modules on the client side preserves personalization while minimizing communication overhead.
- **Mechanism:** Only global prompts are aggregated and transmitted; local prompts and attention weights remain private, preventing the averaging out of client-specific nuances that plagues standard FL.
- **Core assumption:** The communication bottleneck is the primary constraint, and local parameter updates are sufficient to capture client-specific drift without server-side coordination.
- **Evidence anchors:** [Page 7, Section 3.2]: "local prompts are retained locally"; [Page 9, Section 3.3]: "helps reduce communication overhead."

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) & CLIP**
  - **Why needed here:** pFedDC freezes the encoders of a VLM. You must understand that CLIP aligns images and text in a shared embedding space via contrastive loss.
  - **Quick check question:** Can you explain why modifying the input prompts changes the output of a frozen CLIP model without changing the model's weights?

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** The aggregation of global prompts relies on FedAvg. You need to understand how weighted averaging of parameters works.
  - **Quick check question:** If Client A has 100 samples and Client B has 10, how much more influence does Client A have on the aggregated global prompt in standard FedAvg?

- **Concept: Cross-Attention vs. Self-Attention**
  - **Why needed here:** The core fusion mechanism uses cross-attention. You need to distinguish this from self-attention.
  - **Quick check question:** In a cross-attention layer, what are the sources of the Query and the Key/Value pairs? How does this differ from self-attention?

## Architecture Onboarding

- **Component map:** Frozen CLIP ViT-B/16 Backbone -> Global/Local Text Prompts -> Text Cross-Attention -> Fused Text Prompts -> Text Encoder
- **Critical path:** 1. Initialization: Download global prompts; init local prompts/attention randomly. 2. Fusion: Input prompts → Cross-Attention(P_global, P_local) → Fused Prompts. 3. Forward Pass: Feed Image + Fused Vision Prompt → Encoder; Feed Class Text + Fused Text Prompt → Encoder. 4. Loss: Calculate Cross-Entropy between Image and Text embeddings. 5. Update: SGD updates all prompts and attention modules locally. 6. Sync: Upload only Global Prompts to server; average; download new Global Prompts.

- **Design tradeoffs:** Dual-Modality vs. Text-Only: Higher accuracy vs. slightly higher local compute/memory. Local Attention Storage: Saves communication but prevents server-side learning of fusion structure.

- **Failure signatures:** Attention Collapse: Cross-attention outputs zero values for global prompts. Domain Overfitting: Local vision prompts drift too far, causing embeddings to fall outside CLIP's effective distribution.

- **First 3 experiments:** 1. Sanity Check (Homogeneous): Run pFedDC on CIFAR10 with IID data to ensure global prompts converge. 2. Ablation (Vision Prompt): Disable vision prompts and run only text prompts on DomainNet to confirm performance drop. 3. Stress Test (Participation): Drop client participation rate to 0.5 to verify local prompts maintain stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can formal theoretical guarantees be established for the convergence and generalization error bounds of pFedDC, particularly regarding the non-convex interactions between the dual prompts and cross-attention modules?
- Basis in paper: [explicit] The conclusion states, "In the future, we will conduct a theoretical analysis of pFedDC, focusing on convergence."
- Why unresolved: The paper currently relies on empirical validation; the complexity of optimizing global/local prompts simultaneously with adaptive cross-attention fusion makes the theoretical convergence landscape difficult to characterize.
- What evidence would resolve it: A formal proof demonstrating convergence rates under standard FL assumptions or bounds showing how cross-attention affects generalization gaps.

### Open Question 2
- Question: Does the transmission of aggregated global prompts and the retention of local cross-attention modules leak sensitive client information under advanced privacy attacks?
- Basis in paper: [explicit] The authors explicitly list "privacy" as a key focus for future analysis in the conclusion.
- Why unresolved: While the framework avoids sharing raw data or full model weights, the specific information content embedded in the aggregated dual prompts and the implied structure of local attention maps have not been audited for privacy leakage.
- What evidence would resolve it: Quantitative analysis of attack success rates (e.g., AUC scores for membership inference) targeting the communicated global prompts versus standard FedAvg gradients.

### Open Question 3
- Question: Does the pFedDC framework ensure fairness across the federation, preventing performance degradation on clients with minority domains or scarce data while optimizing for average accuracy?
- Basis in paper: [explicit] The conclusion explicitly identifies "fairness" as a target for future investigation.
- Why unresolved: The cross-fusion mechanism adaptively integrates prompts, but it is unclear if this adaptability disproportionately benefits clients with data distributions closer to the global consensus, potentially widening the accuracy variance among clients.
- What evidence would resolve it: Statistical analysis of accuracy variance across clients with varying data volumes and domain types compared to personalized baselines.

## Limitations
- **Reproducibility Gap:** Key hyperparameters (prompt token length, local batch size, number of local epochs) are unspecified, preventing exact reproduction.
- **Simulation Scope:** All experiments are conducted in controlled simulation environments; real-world validation with network latency, client churn, and non-IID drift over time is unverified.
- **Communication Overhead:** Paper does not quantify memory overhead per client (global + local prompts + attention modules) or impact on client-side computation.

## Confidence
- **High Confidence:** Core architectural innovation (dual-prompt + cross-attention) is well-defined; reported accuracy numbers are consistent across tables and ablation studies.
- **Medium Confidence:** Claims of resilience to client drop-out and limited communication rounds are based on simulations with fixed parameters (20 rounds, 10 clients).
- **Low Confidence:** Critical hyperparameters are unspecified, which are necessary for exact reproduction.

## Next Checks
1. **Cross-Dataset Generalization:** Test pFedDC on CIFAR100 with Dirichlet distribution to verify if dual-prompt mechanism generalizes beyond the 9 evaluated datasets.
2. **Client Dropout Recovery:** Simulate a scenario where a client drops out for 5 rounds and then rejoins, measuring if local prompts can recover performance without requiring full re-initialization.
3. **Prompt Ablation Stress Test:** Perform an ablation study where vision prompts are initialized with pre-trained initialization instead of random initialization to test if the framework benefits from informed starting points.