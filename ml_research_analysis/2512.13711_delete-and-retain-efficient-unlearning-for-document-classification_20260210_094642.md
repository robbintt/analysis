---
ver: rpa2
title: 'Delete and Retain: Efficient Unlearning for Document Classification'
arxiv_id: '2512.13711'
source_url: https://arxiv.org/abs/2512.13711
tags:
- unlearning
- class
- document
- classification
- hessian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses efficient class-level unlearning in document
  classification models. The authors propose a two-step method called Hessian Reassignment
  that combines a single influence-style Hessian downweight update with a deterministic
  next-top-1 decision rule for handling deleted-class samples.
---

# Delete and Retain: Efficient Unlearning for Document Classification

## Quick Facts
- arXiv ID: 2512.13711
- Source URL: https://arxiv.org/abs/2512.13711
- Reference count: 30
- Primary result: Achieves near-retraining accuracy while running 11x faster than complete retraining for class-level unlearning

## Executive Summary
This paper presents Hessian Reassignment, a two-step method for efficient class-level unlearning in document classification models. The approach combines influence function-style Hessian updates with deterministic reassignment of deleted-class samples to the next most likely class. By using conjugate gradients to solve the Hessian-vector system, the method requires only gradient and Hessian-vector products, enabling significant computational savings while maintaining model performance on retained classes.

## Method Summary
Hessian Reassignment operates in two steps: first, it downweights the influence of deleted-class samples through a single influence-style update to the model parameters using Hessian-vector products; second, it deterministically reassigns any deleted-class samples that would be misclassified post-update to the next-top-1 class. The method leverages conjugate gradient solvers to efficiently compute the necessary Hessian-vector products without requiring full Hessian computation or model retraining.

## Key Results
- Achieves retained-class accuracy within 1-2% of complete retraining on 20 Newsgroups, AG News, and DBPedia-14 benchmarks
- Runs 11x faster than complete retraining (53 seconds vs 602 seconds on average)
- Reduces membership-inference advantage on removed class with AUCc approaching 0.5 (random guessing)
- Maintains unchanged confidence distribution on retained classes (Kolmogorov-Smirnov test p-values of 1.0)

## Why This Works (Mechanism)
The method works by efficiently approximating the effect of removing deleted-class samples through Hessian-based influence functions. The single-step update captures the essential parameter changes needed for unlearning, while the deterministic reassignment ensures no deleted-class samples remain misclassified as their original class. This two-step approach balances computational efficiency with unlearning effectiveness.

## Foundational Learning

**Influence Functions**
- Why needed: Provides a principled way to estimate parameter changes from removing data points without retraining
- Quick check: Verify that the influence approximation remains accurate for large Hessian values

**Conjugate Gradient Solvers**
- Why needed: Enables efficient computation of Hessian-vector products without full Hessian storage
- Quick check: Confirm convergence of conjugate gradient iterations within tolerance bounds

**Membership Inference Attacks**
- Why needed: Quantifies privacy leakage through statistical advantage over random guessing
- Quick check: Validate attack success rate decreases as privacy improves

## Architecture Onboarding

**Component Map**
Model parameters -> Gradient computation -> Hessian-vector products -> Conjugate gradient solver -> Parameter update -> Class reassignment

**Critical Path**
The critical computational path involves computing gradients, solving the Hessian-vector system via conjugate gradients, and applying the parameter update. This sequence must complete before class reassignment can occur.

**Design Tradeoffs**
- Accuracy vs. speed: Single-step update trades some precision for 11x speedup
- Memory vs. computation: Avoiding full Hessian storage saves memory but requires iterative solvers
- Privacy vs. utility: Stronger unlearning may reduce retained-class accuracy

**Failure Signatures**
- Degraded performance on retained classes indicates influence approximation breakdown
- Membership inference advantage remaining high suggests insufficient unlearning
- Non-convergence of conjugate gradient solver indicates ill-conditioned Hessian

**3 First Experiments**
1. Verify accuracy retention on a small subset (10% of data) before scaling up
2. Test conjugate gradient convergence with varying iteration limits
3. Measure membership inference advantage on synthetic deleted-class samples

## Open Questions the Paper Calls Out
None

## Limitations
- Performance guarantees primarily demonstrated on text classification tasks; generalization to other domains remains unvalidated
- Limited ablation studies on Hessian approximation accuracy vs computational savings
- Membership inference attacks evaluated only through pooled multi-shadow attacks

## Confidence

| Claim | Confidence |
|-------|------------|
| Retained-class accuracy claims | High |
| Membership inference advantage reduction | Medium |
| Computational efficiency claims | High |

## Next Checks
1. Test Hessian Reassignment on image classification benchmarks (e.g., CIFAR-10 with one-class deletion) to verify cross-domain applicability
2. Implement and evaluate against additional membership inference attack variants (e.g., label-only, metric-based) to strengthen privacy claims
3. Conduct ablation studies varying the strength of Hessian approximation (full vs. low-rank approximations) to quantify the accuracy-efficiency tradeoff more precisely