---
ver: rpa2
title: 'Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large
  Language Models: Discoveries and Insights'
arxiv_id: '2305.11700'
source_url: https://arxiv.org/abs/2305.11700
tags:
- item
- arxiv
- recommender
- recommendation
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether extremely large language models
  (LLMs) with over 100 billion parameters can significantly improve text-based collaborative
  filtering (TCF) for recommender systems. The authors systematically scale text encoders
  from 125M to 175B parameters across three datasets, using both DSSM and SASRec backbones.
---

# Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights

## Quick Facts
- arXiv ID: 2305.11700
- Source URL: https://arxiv.org/abs/2305.11700
- Reference count: 40
- Key outcome: Scaling text encoders from 125M to 175B parameters consistently improves recommendation accuracy with no observed ceiling; frozen 175B LLMs match IDCF for text-centric warm-item recommendation.

## Executive Summary
This paper systematically explores whether extremely large language models (LLMs) with over 100 billion parameters can significantly improve text-based collaborative filtering (TCF) for recommender systems. The authors scale text encoders from 125M to 175B parameters across three datasets, using both DSSM and SASRec backbones. They find that larger text encoders consistently improve recommendation accuracy, with no observed performance ceiling even at 175B parameters. Surprisingly, a frozen 175B LLM performs comparably to ID-based collaborative filtering (IDCF) for warm item recommendation when paired with the SASRec backbone, challenging the decade-long dominance of ID embeddings. However, even with 175B parameters, TCF does not achieve universal representations or strong zero-shot transferability across domains. The study concludes that while TCF with LLMs shows promise for text-centric recommendations, developing truly universal foundation recommender models remains challenging.

## Method Summary
The study investigates text-based collaborative filtering (TCF) by scaling text encoders from 125M to 175B parameters across three datasets (MIND, HM, Bili). The method uses two backbone architectures: SASRec (sequential self-attention) and DSSM (two-tower). Item text (titles, descriptions) is encoded using OPT models of varying sizes, either frozen (pre-extracted embeddings) or fine-tuned. The encoded representations are transformed to match the backbone's embedding dimension and used to train user-item matching models with batch softmax loss. The paper systematically compares TCF against ID-based collaborative filtering (IDCF) and explores zero-shot transferability across domains.

## Key Results
- Scaling text encoders from 125M to 175B parameters yields consistent accuracy gains in TCF with no observed performance ceiling (e.g., HR@10 improves from 19.07 to 20.24 on MIND).
- A frozen 175B LLM with SASRec backbone matches IDCF performance for warm item recommendation in text-centric domains (news, video titles).
- Even 175B LLMs do not produce universal item representations—fine-tuning on target data remains necessary for optimal performance, and zero-shot transferability across domains fails significantly.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling text encoder size from 125M to 175B parameters yields consistent accuracy gains in TCF with no observed performance ceiling.
- Mechanism: Larger LLMs encode richer semantic representations of item text, capturing nuanced content features that better align with user preferences. The scaling follows power-law-like improvements where representation capacity increases non-linearly with parameter count, enabling the recommender backbone to learn more discriminative user-item matching functions.
- Core assumption: Item text content carries sufficient signal to predict user preferences; the bottleneck is representation quality rather than the matching architecture.
- Evidence anchors: TCF with SASRec improved from 19.07 to 20.24 on MIND (+6.1%), 9.37 to 11.11 on HM (+18.6%), 4.45 to 7.05 on Bili (+58.4%) when scaling from 125M to 175B.

### Mechanism 2
- Claim: A frozen 175B LLM with SASRec backbone matches ID-based CF performance on text-centric warm-item recommendation.
- Mechanism: Sequential recommenders model user preferences via attention over item sequences. When items are represented by high-quality frozen LLM embeddings, the attention mechanism learns to weight semantic features dynamically per user. This approximates the latent factor learning in IDCF without requiring learnable item embeddings—175B parameters provide sufficiently universal semantic features that user-specific weighting suffices.
- Core assumption: The recommendation task is primarily text-centric (news, video titles); non-textual factors are secondary or correlated with text.
- Evidence anchors: TCF with frozen 175B + SASRec achieves HR@10 of 20.24 vs IDCF's 20.05 on MIND, 7.05 vs 7.01 on Bili—comparable performance.

### Mechanism 3
- Claim: Even 175B LLMs do not produce universal item representations—fine-tuning on target recommendation data remains necessary for optimal performance.
- Mechanism: LLMs pre-trained on general text learn representations optimized for language modeling objectives, not user preference prediction. Recommendation involves subjective, personalized judgments not captured in text semantics alone. Fine-tuning adapts representations to domain-specific preference patterns that differ from general linguistic knowledge.
- Core assumption: User-item interactions reflect true preferences and contain signal beyond item text content.
- Evidence anchors: Fine-tuned 125M LM outperforms frozen 175B LM across datasets. Frozen 175B lags behind fine-tuned 66B by 3.9-16% relative gap.

## Foundational Learning

- **Collaborative Filtering (CF) Paradigms**:
  - Why needed here: The paper compares ID-based CF (learnable embeddings per item) vs. text-based CF (LLM-encoded item representations). Understanding the distinction is essential to interpret why scaling text encoders matters and when frozen representations suffice.
  - Quick check question: Can you explain why IDCF fails for cold-start items while TCF does not?

- **Scaling Laws in Neural Networks**:
  - Why needed here: The core finding is that TCF performance improves predictably with encoder size. Grasping why larger models yield better representations (capacity, generalization, feature hierarchy) clarifies the mechanism and its limits.
  - Quick check question: Why might scaling text encoders yield diminishing returns for some recommendation domains but not others?

- **Sequential Recommendation Architecture (SASRec)**:
  - Why needed here: The paper shows SASRec backbones consistently outperform DSSM for TCF. SASRec's self-attention over item sequences enables dynamic user modeling that leverages rich text representations more effectively than two-tower DSSM.
  - Quick check question: How does SASRec's attention mechanism exploit item text representations differently than DSSM's inner product matching?

## Architecture Onboarding

- **Component map**:
  Text Encoder (LLM) -> Dimension Transformation Layer (DTL) -> Recommender Backbone (SASRec/DSSM) -> Training Objective (batch softmax)

- **Critical path**:
  1. Extract frozen embeddings from LLM for all items (one-time offline computation for frozen mode).
  2. Train DTL + backbone on user-item interaction sequences using batch softmax.
  3. For fine-tuning: jointly optimize DTL, backbone, and top LLM layers with smaller learning rate (3e-5–1e-4 for 66B models).

- **Design tradeoffs**:
  - Frozen vs. Fine-tuned Encoder: Frozen is computationally cheap (pre-extract once) but underperforms fine-tuned models by 4–16%. Fine-tuned achieves best accuracy but requires 100–1000× more compute.
  - SASRec vs. DSSM Backbone: SASRec consistently outperforms DSSM for TCF (e.g., 20.24 vs. 2.88 HR@10 on MIND with frozen 175B). DSSM struggles to exploit text representations effectively.
  - Encoder Size vs. Domain Fit: Larger encoders help more when text is central to user decisions (news, video titles). Gains are smaller when non-textual factors dominate (clothing purchases).

- **Failure signatures**:
  - Frozen 175B underperforms IDCF significantly: Check if domain is non-text-centric or if text is sparse/generic.
  - Scaling yields no improvement: Verify encoder implementation matches expected architecture (paper notes 350M OPT had structural differences causing poor performance).
  - Zero-shot transfer fails (HR@10 near random): Pre-training domain may be too dissimilar from target domain; user behavior patterns are not transferable via text alone.

- **First 3 experiments**:
  1. Implement IDCF (SASRec + learnable item embeddings) on your target dataset to establish upper bound. Verify hyper-parameters match paper's grid search ranges.
  2. Extract embeddings from 3 encoder sizes (e.g., 125M, 6.7B, 66B) and train SASRec backbone. Plot HR@10 vs. parameter count to confirm scaling trend.
  3. For a medium encoder (e.g., 1.3B), compare frozen embeddings vs. fine-tuning top 2 layers. Quantify accuracy gap and compute cost to assess practical tradeoff.

## Open Questions the Paper Calls Out
- Does the performance of text-based collaborative filtering (TCF) saturate at a specific parameter scale, or does the positive scaling trend continue indefinitely as text encoders exceed 175 billion parameters?
- How can the transferability of user-item matching functions be improved to enable effective zero-shot cross-domain recommendation?
- Do the accuracy improvements observed with large frozen LLMs in offline TCF evaluations persist in online environments with exposure bias?

## Limitations
- Zero-shot transferability fails because user behavior patterns differ across domains, preventing the transfer of user-item matching functions.
- The study only tests extreme cases (frozen vs. fully fine-tuned) without exploring parameter-efficient fine-tuning methods that could bridge the accuracy gap.
- All observations are derived from offline data, and applicability in practical online environments with exposure bias remains unverified.

## Confidence
- **High Confidence**: The observation that larger text encoders consistently improve TCF accuracy follows a clear, reproducible pattern across multiple datasets and encoder families.
- **Medium Confidence**: The claim that frozen 175B LLMs match IDCF performance for warm-item recommendation is robust for text-centric domains but less generalizable to non-text-centric domains.
- **Low Confidence**: The assertion that even 175B LLMs cannot produce universal item representations is based on limited adaptation strategies and does not explore parameter-efficient fine-tuning.

## Next Checks
1. Extend the scaling experiment beyond 175B parameters using smaller datasets or synthetic data to determine whether the "no performance ceiling" observation holds at truly extreme scales (1T+ parameters).
2. Compare frozen and fully fine-tuned LLMs against parameter-efficient fine-tuning methods (LoRA, adapters) to quantify the accuracy-compute tradeoff and assess whether universal representations are achievable with more sophisticated adaptation.
3. Design a controlled experiment testing zero-shot transfer from a source domain (e.g., news recommendation) to a target domain (e.g., product recommendation) with varying text-content overlap to quantify the relationship between textual similarity and transferability success.