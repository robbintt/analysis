---
ver: rpa2
title: Are LLMs ready to help non-expert users to make charts of official statistics
  data?
arxiv_id: '2510.01197'
source_url: https://arxiv.org/abs/2510.01197
tags:
- data
- visualization
- code
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the ability of large language models to generate
  accurate and effective charts from official statistics data. Using data from Statistics
  Netherlands, eight LLMs were tested on 25 visualization tasks, categorized by difficulty.
---

# Are LLMs ready to help non-expert users to make charts of official statistics data?

## Quick Facts
- arXiv ID: 2510.01197
- Source URL: https://arxiv.org/abs/2510.01197
- Reference count: 24
- LLMs can generate charts from official statistics, but iterative feedback and contextual guidance are crucial for optimal results.

## Executive Summary
This study evaluates the ability of large language models to generate accurate and effective charts from official statistics data. Using data from Statistics Netherlands, eight LLMs were tested on 25 visualization tasks, categorized by difficulty. A comprehensive evaluation framework assessed three dimensions: data retrieval and preprocessing, code quality, and visual representation, each scored on a 10-point scale. Results showed base models excelled in code generation (mean score 8.3/10) but struggled with data manipulation (5.9/10) and visual design (6.5/10). An agentic system with iterative self-correction significantly improved performance, with Claude 3.7 achieving scores above 9.0/10 across all dimensions. The findings demonstrate that while LLMs are promising for automated chart generation, iterative feedback and contextual guidance are crucial for optimal results.

## Method Summary
The study employed eight LLMs to generate Python visualizations from natural language queries over seven official statistics tables from Statistics Netherlands. The evaluation used two approaches: zero-shot generation with structured prompts and an agentic system with iterative self-correction. Retrieval used sentence transformer embeddings to match queries to datasets. A 22-item evaluation framework assessed visual, code, and data dimensions, each normalized to 0-10. The agentic system executed generated code, captured errors and outputs, and fed results back to the LLM for revision up to seven iterations.

## Key Results
- Base models scored 8.3/10 on code generation but only 5.9/10 on data manipulation and 6.5/10 on visual representation
- Claude 3.7 achieved scores above 9.0/10 across all dimensions using iterative self-correction
- Semantic retrieval using sentence transformers achieved 36% top-1 accuracy and 80% top-10 accuracy for dataset selection

## Why This Works (Mechanism)

### Mechanism 1
Iterative self-correction with execution feedback substantially improves visualization quality across data, code, and visual dimensions. The agentic system executes generated Python code, captures errors and outputs, and feeds results back to the LLM for revision. This allows the model to debug syntax errors, verify plot correctness via image inspection, and refine filtering/aggregation logic over multiple iterations (up to 7 in the study). Evidence shows Claude 3.7's scores jumped to 8.90 (visual), 9.83 (code), and 9.43 (data) with this approach. Break condition occurs when models cannot correctly interpret their own generated visualizations or when errors require domain knowledge the model lacks.

### Mechanism 2
Structured prompt modules containing visualization principles and common pitfalls improve visual representation quality. Modular prompt components inject domain-specific guidance (visualization best practices, lessons learned from prior failures, self-assessment checklists) into the system prompt. This scaffolds the model's reasoning about chart design without requiring fine-tuning. Evidence shows adding structured context improved o1-High's visual score from 7.50 to 7.80, though it slightly degraded code and data scores. Break condition occurs when added context overwhelms smaller models or introduces conflicting guidance.

### Mechanism 3
Semantic retrieval using sentence transformers enables matching user queries to relevant datasets, but retrieval accuracy remains a bottleneck. Pre-computed embeddings of dataset metadata are compared to user prompt embeddings via cosine similarity. The top-1 dataset is selected without human filtering, allowing fully automated pipeline operation. Evidence shows the correct dataset appeared in top 10 results for 80% of questions, top 5 for 68%, and ranked first in 36%. Break condition occurs when multiple datasets contain similar variables or when user queries use terminology not reflected in metadata.

## Foundational Learning

- Concept: Data manipulation operations (filtering, aggregation, grouping)
  - Why needed here: Base models scored 5.9/10 on average for data handling; errors included missing filters, incorrect grouping, and improper column selection. Understanding these operations is prerequisite for debugging LLM outputs.
  - Quick check question: Given a dataset with columns [Region, Year, Value], what pandas code filters to years 2010-2015 and computes the mean Value per Region?

- Concept: Visualization design principles (chart type selection, axis scaling, visual clarity)
  - Why needed here: Models frequently selected inappropriate chart types (e.g., line charts for categorical comparisons), failed to anchor y-axes at zero, and added redundant visual elements.
  - Quick check question: For comparing proportions across 8 categories, which chart types are appropriate and why might a line chart be unsuitable?

- Concept: Agentic system patterns (tool use, execution feedback, iteration limits)
  - Why needed here: The evaluation framework compares zero-shot vs. agentic approaches; understanding when and why iteration helps is essential for system design.
  - Quick check question: What information must be available to an LLM agent after code execution to enable effective self-correction?

## Architecture Onboarding

- Component map: User prompt -> Semantic retrieval (top-1 dataset) -> LLM planning -> Code generation -> Execution -> Error/output capture -> Self-evaluation -> Iteration (up to max_iters) -> Final visualization

- Critical path: User prompt → Semantic retrieval (top-1 dataset) → LLM planning → Code generation → Execution → Error/output capture → Self-evaluation → Iteration (up to max_iters) → Final visualization

- Design tradeoffs:
  - Zero-shot simplicity vs. agentic capability: Zero-shot is faster but scores ~6.5/10 average; agentic with feedback reaches >9/10 but requires multiple LLM calls
  - Top-1 retrieval vs. human verification: Fully automated but 36% first-rank accuracy; top-10 contains correct data 80% of the time
  - Prompt complexity vs. model capacity: Added context improves visual scores but may confuse logic for some models

- Failure signatures:
  - Hardcoded values: Code uses sample row data instead of the full dataset variable
  - Aggregation errors: Plotting multi-period data without grouping, resulting in overlapping lines
  - Axis type mismatch: Treating string-based period columns as categorical, causing disordered axes
  - Chart type mismatch: Line charts for categorical comparisons
  - Retrieval failure: Wrong dataset selected due to overlapping metadata

- First 3 experiments:
  1. Replicate zero-shot baseline on 5 tasks: Test base models (e.g., GPT-4o, Claude 3.5) with minimal prompts to establish performance floor across visual/code/data dimensions.
  2. Ablate modular prompt components: Run tasks with/without visualization context module to isolate its contribution; expect visual score improvement but monitor for code/data degradation.
  3. Implement minimal agentic loop: Add execute_python_code tool with 3-iteration limit; measure improvement in code executability and data correctness from error-feedback cycles.

## Open Questions the Paper Calls Out

### Open Question 1
Does the iterative self-correction of agentic systems lead to visualizations that are overly complex for non-expert users? The conclusion warns that while agentic systems improve benchmark scores, they "suffer from an increase in complexity of the produced charts" which may push results "outside of the grasp of the intended audience." The evaluation framework assessed visual clarity and correctness but did not explicitly measure the cognitive load or complexity of the final outputs. User studies measuring non-expert comprehension and task completion times on charts generated with varying numbers of feedback iterations would resolve this.

### Open Question 2
How does non-expert comprehension of LLM-generated charts compare to those created by human experts? The authors explicitly call for "comparative user studies between LLM-generated visualizations and those produced by Statistics Netherlands experts" to determine if the technical sophistication hinders democratization. This study relied on a model-agnostic evaluation framework (binary questions on code/visual/data) rather than human comparative trials. A controlled A/B test where non-experts interpret the same statistical narrative using both LLM-generated and expert-curated visualizations would resolve this.

### Open Question 3
Does providing explicit visualization context introduce a trade-off that degrades logical data processing? The results show that while adding context improved visual scores for o1-High (7.50 to 7.80), it caused a slight decline in code (8.97 to 8.91) and data (7.37 to 7.03) accuracy. The paper notes the improvement in visual design but does not investigate the mechanism behind the concurrent drop in data logic scores. Ablation studies isolating context length and content to determine if the "context window" dilutes the model's focus on data manipulation logic would resolve this.

## Limitations
- Evaluation prompts and checklists were not fully published, affecting reproducibility
- Agentic approach requires multiple LLM calls, increasing computational cost and latency
- Semantic retrieval's 36% top-1 accuracy reveals fundamental limitations in fully automated data selection

## Confidence
- **High confidence**: Iterative self-correction improves LLM-generated code (supported by measurable score increases across all dimensions)
- **Medium confidence**: Prompt modules meaningfully improve visual representation (shown in ablation but with inconsistent effects on other dimensions)
- **Medium confidence**: Base models struggle most with data manipulation (consistent low scores, but mechanism unclear whether due to model limitations or task complexity)

## Next Checks
1. Replicate retrieval performance: Measure top-1/5 accuracy on held-out official statistics queries using the same sentence transformer approach
2. Test iteration limits: Compare performance across different iteration budgets (1, 3, 5, 7) to identify optimal trade-offs between quality and computational cost
3. Cross-domain generalization: Apply the agentic framework to non-official statistics datasets (e.g., economic indicators from other sources) to assess robustness of the self-correction mechanism