---
ver: rpa2
title: 'Leaner Transformers: More Heads, Less Depth'
arxiv_id: '2505.20802'
source_url: https://arxiv.org/abs/2505.20802
tags:
- heads
- number
- depth
- attention
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that the number of attention heads in transformers
  can be increased and the number of layers reduced, yielding models that are 30-50%
  smaller while maintaining accuracy. The key insight is that multi-head attention
  improves the conditioning of attention layers, which facilitates optimization.
---

# Leaner Transformers: More Heads, Less Depth

## Quick Facts
- arXiv ID: 2505.20802
- Source URL: https://arxiv.org/abs/2505.20802
- Reference count: 40
- One-line primary result: Increasing attention heads while reducing depth yields models 30-50% smaller with maintained accuracy

## Executive Summary
This paper demonstrates that transformer architectures can be made more parameter-efficient by increasing the number of attention heads while simultaneously reducing the number of layers. The authors show that multi-head attention improves the mathematical conditioning of attention layers, which facilitates more stable optimization. This theoretical insight enables a practical architectural modification: trading depth for width in the attention mechanism. Empirically validated across vision (ImageNet-1k), language modeling (GLUE, TinyStories), and sequence modeling (LRA benchmark), the approach consistently achieves equal or better performance with fewer parameters.

## Method Summary
The core method involves redesigning standard transformer architectures by increasing the number of attention heads per layer while decreasing the total number of layers. For a baseline transformer (e.g., ViT-Base with 12 layers and 12 heads), the modification might involve reducing to 7-8 layers while increasing heads to 16-24, with optional MLP dimension reduction from 3072 to 1536. Models are trained from scratch using AdamW optimizer with specific hyperparameters including learning rate of 3e-3, weight decay of 0.3, and extensive data augmentation. The theoretical foundation rests on proving that increasing head count improves the condition number of attention matrices, approaching 1 as head count increases, which facilitates optimization.

## Key Results
- ViT-Base models with 7-8 layers and 16-24 heads achieve comparable accuracy to 12-layer, 12-head baselines with 30-50% fewer parameters
- The conditioning benefit of multi-head attention is empirically validated through condition number analysis of trained models
- The depth-for-heads trade-off works across vision, language, and sequence modeling tasks
- Increasing MLP width has limited impact compared to increasing attention heads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing attention heads improves mathematical conditioning of attention blocks
- Mechanism: Concatenating multiple attention matrices lowers the condition number (ratio of largest to smallest singular values), preventing gradient issues during backpropagation
- Core assumption: Randomly initialized attention matrices can be modeled as i.i.d. Gaussian random matrices
- Evidence anchors: Theorem 3.2 proves condition number κ(A) → 1 as heads h → ∞; this is the paper's unique theoretical contribution
- Break condition: If learned attention matrices deviate significantly from random structure (e.g., become rank-deficient), the conditioning benefit may not hold

### Mechanism 2
- Claim: Model depth can be traded for more attention heads to reduce parameters while maintaining accuracy
- Mechanism: Depth acts as a preconditioner for gradient descent; improved conditioning from additional heads provides similar optimization benefit, allowing layer removal
- Core assumption: Existing transformers are over-parameterized, with depth contributing more to optimization stability than representational capacity
- Evidence anchors: Abstract states depth can be decreased "reducing parameter count by up to 30-50% while maintaining accuracy"
- Break condition: Minimum depth is required for hierarchical reasoning; tasks requiring deep sequential processing cannot benefit from this trade-off

### Mechanism 3
- Claim: Increasing MLP width has weaker effect on optimization stability than increasing attention heads
- Mechanism: Standard transformers use wide MLPs (2-4× embedding dimension) that are likely already well-conditioned; attention block is the primary bottleneck
- Core assumption: MLP and attention blocks contribute independently to overall optimization dynamics
- Evidence anchors: Section 3.3 notes "Transformers employ wide MLPs... They are thus likely to be well conditioned"
- Break condition: For architectures with unusually narrow MLPs, increasing MLP width may provide significant benefits rivaling attention heads

## Foundational Learning

- **Condition Number of a Matrix**
  - Why needed here: Central metric quantifying optimization stability; paper's main theoretical contribution is proving how to lower this in attention layers
  - Quick check question: What does a high condition number imply for gradient descent convergence? (Answer: Slow or unstable convergence)

- **Multi-Head Attention (MHA)**
  - Why needed here: Paper reinterprets MHA's primary role from "learning diverse representations" to "improving mathematical conditioning"
  - Quick check question: Traditionally, why do transformers use multiple attention heads? How does this paper's view differ? (Answer: Traditionally to capture different relational patterns; this paper views it as ensuring well-conditioned layers for optimization)

- **Width vs. Depth Trade-offs**
  - Why needed here: Core practical outcome is a specific design choice within broader architectural design space
  - Quick check question: What is the primary parameter cost associated with depth in transformers? (Answer: Each additional layer duplicates both attention and MLP parameter blocks, making depth expensive)

## Architecture Onboarding

- **Component map**: Standard Transformer architecture with modified components: Attention Block (↑heads), Layer Stack (↓layers), MLP Block (typically unchanged or secondarily adjusted)

- **Critical path**: 1) Take baseline architecture (e.g., ViT-Base: 12L, 12H), 2) Increase heads per layer (e.g., to 16-24), 3) Reduce total layers (e.g., from 12 to 8), 4) Adjust head dimension or keep constant, 5) Retrain from scratch

- **Design tradeoffs**:
  - Heads vs. Head Dimension: Increasing number of heads (h) improves conditioning more effectively than increasing head dimension (d), and is more amenable to parallelization
  - Depth Reduction: Too much reduction will harm performance; minimum depth needed for hierarchical feature abstraction
  - Total Parameters: Goal is removing entire layers (large parameter block) and compensating with more heads (smaller parameter increase) for net reduction

- **Failure signatures**:
  - Underfitting: If depth reduced too aggressively, model lacks capacity to learn complex functions regardless of attention conditioning
  - Diminishing Returns: Beyond certain point, adding more heads provides no further conditioning benefit and only adds unnecessary computation

- **First 3 experiments**:
  1. Replicate conditioning plot: Train small transformer varying only number of heads; plot condition numbers to verify Theorem 3.2 prediction
  2. Depth-for-heads ablation on ViT-B: Train baseline (12L, 12H) vs modified (8L, 16H) on ImageNet-1k; compare accuracy and parameter count
  3. Validate on different modality: Apply modification to small language model (2-4 layer GPT) on text dataset (TinyStories) to test generality across modalities

## Open Questions the Paper Calls Out

- Can we quantitatively predict the trade-offs between depth and number of attention heads for specific architectural variations?
- Do the observed benefits persist at larger scales (~1B parameters)?
- Are there alternative architectural interventions (beyond adding heads) that could achieve similar conditioning improvements in attention layers?
- How exactly does improved conditioning of attention matrices impact training dynamics and convergence behavior?

## Limitations

- Theoretical model relies on i.i.d. Gaussian random matrix assumption that may not hold for trained models with structured attention patterns
- Architectural generality not tested on extreme architectures (very deep transformers, specialized variants like Longformer)
- Training-dependent factors (initialization, optimization algorithms, regularization) not investigated for interaction with depth-for-heads benefit

## Confidence

**High Confidence**: Empirical finding that more heads with fewer layers achieves comparable accuracy with fewer parameters is well-supported across multiple architectures and tasks

**Medium Confidence**: Specific parameter reduction targets (30-50%) and optimal depth-to-head ratios are architecture-dependent and may not generalize perfectly

**Low Confidence**: Claim that MLP conditioning is "likely already well-conditioned" is based on typical configurations rather than rigorous analysis

## Next Checks

1. Replicate conditioning analysis on ViT trained on CIFAR-10 while varying head count from 4 to 32; track condition numbers throughout training to verify Theorem 3.2's prediction

2. Apply depth-for-heads modification to very deep transformer (24-layer BERT variant); test whether conditioning benefit still enables depth reduction or minimum depth requirements prevent trade-off

3. Investigate interaction between depth-for-heads modification and different MLP widths; train models with varying MLP ratios (2× to 8× embedding dimension) while keeping depth-for-heads trade-off constant