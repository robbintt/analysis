---
ver: rpa2
title: 'Relevance Isn''t All You Need: Scaling RAG Systems With Inference-Time Compute
  Via Multi-Criteria Reranking'
arxiv_id: '2504.07104'
source_url: https://arxiv.org/abs/2504.07104
tags:
- relevance
- document
- score
- answer
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation that retrieval-augmented generation
  (RAG) systems optimizing only for relevance can create information bottlenecks that
  degrade downstream answer quality. The authors propose REBEL (RErank BEyond reLevance),
  a framework that incorporates multi-criteria reranking using Chain-of-Thought prompting
  to balance relevance with secondary criteria like depth, diversity, authoritativeness,
  and recency.
---

# Relevance Isn't All You Need: Scaling RAG Systems With Inference-Time Compute Via Multi-Criteria Reranking

## Quick Facts
- arXiv ID: 2504.07104
- Source URL: https://arxiv.org/abs/2504.07104
- Authors: Will LeVine; Bijan Varjavand
- Reference count: 40
- Primary result: REBEL improves both context relevance and answer quality by incorporating multi-criteria reranking beyond relevance alone.

## Executive Summary
This paper addresses the limitation that retrieval-augmented generation (RAG) systems optimizing only for relevance can create information bottlenecks that degrade downstream answer quality. The authors propose REBEL (RErank BEyond reLevance), a framework that incorporates multi-criteria reranking using Chain-of-Thought prompting to balance relevance with secondary criteria like depth, diversity, authoritativeness, and recency. They introduce both a fixed one-turn approach with predefined criteria and a dynamic two-turn strategy that infers query-specific criteria. Experiments demonstrate that REBEL achieves both higher context relevance and answer quality compared to existing relevance-only methods, while enabling a new performance/speed tradeoff curve where inference-time compute directly improves system quality. The method challenges the conventional assumption that relevance alone suffices for optimal RAG performance.

## Method Summary
REBEL implements multi-criteria reranking by assigning weighted scores to documents based on both relevance and secondary properties (depth, diversity, clarity, authoritativeness, recency). The one-turn variant uses a fixed prompt with five predefined criteria and equal weights (wi = 0.5). The two-turn variant first generates a query-specific reranking prompt by inferring relevant criteria from the query, then applies this custom prompt. Both approaches use Chain-of-Thought prompting with GPT-4o for inference and GPT-4 for evaluation. The framework is evaluated on the AI ArXiv dataset with 107 GPT-4-generated QA pairs, measuring answer similarity (0-5 scale) and retrieval precision (binary per document).

## Key Results
- REBEL's one-turn variant outperforms no-rerank baseline on both retrieval precision and answer similarity metrics.
- The two-turn variant further improves performance by adapting criteria to query type.
- REBEL demonstrates a new performance/speed tradeoff curve where inference-time compute directly improves system quality.
- Multi-criteria reranking achieves higher answer quality even when documents have slightly lower relevance scores.

## Why This Works (Mechanism)

### Mechanism 1
Relevance-only retrieval creates an information bottleneck that degrades downstream answer quality. Documents maximally relevant to a query may lack secondary properties essential for generation—such as depth, authoritativeness, or diversity—causing the LLM to produce shallow or poorly-grounded answers even when context appears topically aligned.

### Mechanism 2
Weighted multi-criteria composite scoring enables documents with slightly lower relevance but stronger secondary properties to surpass relevance-only candidates. The reranker assigns a Final Score = Relevance + Σ(wi × Propertyi), where secondary criteria contribute additively, allowing the LLM to rank documents higher when they better support answer generation.

### Mechanism 3
Query-dependent criteria inference via two-turn prompting improves reranking alignment with user intent compared to fixed criteria. In turn one, a meta-prompted LLM analyzes the query to infer relevant secondary criteria and generates a custom reranking prompt. In turn two, the reranking LLM applies this prompt to score and rank documents, adapting evaluation priorities to the query's domain and requirements.

## Foundational Learning

- **Information Bottleneck Theory**: Provides the theoretical basis for why single-criterion optimization constrains information flow and limits downstream task performance. Quick check: Can you explain why maximizing one information measure can reduce representation of other useful properties?
- **Multi-Criteria Decision Making (MCDM)**: REBEL's scoring function is a weighted-sum MCDM approach; understanding tradeoffs and Pareto optimality is necessary for tuning weights and interpreting results. Quick check: What is a limitation of static weights in multi-objective optimization?
- **Chain-of-Thought Prompting**: Both REBEL variants use explicit step-by-step reasoning to score and rank documents; understanding CoT is necessary for debugging and improving prompts. Quick check: How does explicit step-by-step reasoning in a prompt affect LLM scoring consistency?

## Architecture Onboarding

- Component map: Query → Retriever (top-k candidates) → Reranker LLM (with fixed or generated prompt) → Generator LLM → Answer. For two-turn, Meta Prompt Generator runs before Reranker.
- Critical path: Query → Retriever (top-k candidates) → Reranker LLM (with fixed or generated prompt) → Generator LLM → Answer. For two-turn, Meta Prompt Generator runs before Reranker.
- Design tradeoffs: One-turn vs Two-turn: One-turn is faster with fixed criteria; two-turn adds inference latency but adapts criteria to query type. Weight selection: wi = 0.5 is untested; optimal weights may vary by domain or query class. Relevance threshold: Discarding documents with Relevance < 3 may over-filter for niche queries.
- Failure signatures: Two-turn generates irrelevant or malformed criteria (meta-prompt failure). Reranker outputs inconsistent formats or fails to filter low-relevance documents. Answer similarity improves but retrieval precision drops sharply (possible over-weighting of secondary criteria). High variance across runs due to LLM non-determinism.
- First 3 experiments: Baseline comparison: Run no-rerank, one-turn REBEL, and two-turn REBEL on a held-out QA set; measure retrieval precision and answer similarity. Ablation on weights: Systematically vary wi for each secondary criterion; observe impact on answer similarity and retrieval precision. Two-turn meta-prompt robustness: Replace or remove k-shot examples in the meta prompt; measure degradation in criteria inference quality and downstream metrics.

## Open Questions the Paper Calls Out

### Open Question 1
What are the optimal weights for each secondary criterion (depth, diversity, clarity, authoritativeness, recency) across different query types and domains? The authors used uniform weights (wi = 0.5) without empirical optimization, leaving systematic weight tuning to future work.

### Open Question 2
Can REBEL's framework effectively incorporate safety-focused secondary criteria (factual verifiability, bias detection, content safety, source credibility) to reduce harmful outputs? The paper only hypothesizes this application without empirical validation.

### Open Question 3
How does REBEL generalize across diverse domains and established benchmark datasets beyond the AI ArXiv dataset? The evaluation focused narrowly on AI/LLM research papers, acknowledging potential generalizability limitations.

### Open Question 4
How do secondary criteria interact, and should the scoring formula account for these interactions rather than treating criteria additively? The linear weighted sum assumes criteria independence, but compounding effects between criteria combinations may exist.

## Limitations
- The optimal weights for secondary criteria remain empirically undefined and may be highly query-dependent, with wi = 0.5 used uniformly without sensitivity analysis.
- The meta-prompting approach for two-turn REBEL relies on k-shot examples that may not generalize to unseen domains, with no analysis of degradation when examples are removed.
- The information bottleneck claim lacks direct experimental validation through ablation studies comparing relevance-only systems with available secondary criteria.

## Confidence
- **High confidence**: REBEL's one-turn variant improves both retrieval precision and answer similarity compared to no-rerank baseline.
- **Medium confidence**: The two-turn variant provides additional improvements over one-turn by adapting criteria to query type.
- **Medium confidence**: Multi-criteria reranking enables better performance/speed tradeoffs by trading inference compute for quality.

## Next Checks
1. **Weight sensitivity analysis**: Systematically vary wi from 0.0 to 1.0 for each secondary criterion in the one-turn prompt and measure impact on answer similarity and retrieval precision across different query types.
2. **Meta-prompt generalization test**: Remove individual k-shot examples from the two-turn meta prompt and measure degradation in criteria inference quality and downstream metrics.
3. **Information bottleneck ablation**: Create a controlled experiment where documents are ranked by relevance alone but secondary criteria are still available during generation, then compare answer quality to when secondary criteria are used during reranking.