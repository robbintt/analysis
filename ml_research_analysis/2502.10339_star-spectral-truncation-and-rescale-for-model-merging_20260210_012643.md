---
ver: rpa2
title: 'STAR: Spectral Truncation and Rescale for Model Merging'
arxiv_id: '2502.10339'
source_url: https://arxiv.org/abs/2502.10339
tags:
- merging
- star
- task
- arxiv
- merged
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Model merging faces performance degradation as the number of fine-tuned
  models increases, limiting practical multi-task deployment. This work introduces
  STAR, which mitigates merging conflicts by performing spectral truncation followed
  by nuclear norm-preserving rescaling.
---

# STAR: Spectral Truncation and Rescale for Model Merging
## Quick Facts
- arXiv ID: 2502.10339
- Source URL: https://arxiv.org/abs/2502.10339
- Reference count: 24
- Major result: Spectral truncation and rescaling method achieves up to 4.2% better normalized performance than naive averaging when merging 12 models

## Executive Summary
STAR addresses the performance degradation that occurs when merging multiple fine-tuned models, a critical challenge for multi-task deployment. The method introduces spectral truncation to reduce redundancy and nuclear norm-preserving rescaling to maintain the original matrix "size," all without requiring training data or fine-tuning. STAR demonstrates consistent superiority over existing data-free merging methods across extensive experiments with Flan-T5 and Mistral architectures, handling up to 20 merged models with stable performance.

## Method Summary
STAR performs spectral decomposition of task vectors via SVD, then applies spectral truncation by removing small singular components to reduce redundancy. The method then rescales the remaining singular values to preserve the original nuclear norm, effectively maintaining the "size" of the merged matrix. This approach requires no training data or fine-tuning, making it purely data-free. The method is designed to be robust to hyperparameter choices and works across different model architectures and sizes.

## Key Results
- STAR outperforms existing data-free merging methods by up to 4.2% in normalized performance when merging 12 models
- Method remains stable and effective when merging up to 20 models
- No training data or fine-tuning required, maintaining efficiency
- Consistent performance across Flan-T5-base/large and Mistral-7B-Instruct architectures

## Why This Works (Mechanism)
The spectral truncation reduces redundancy by removing small singular components that contribute less to the overall information content, while the nuclear norm-preserving rescaling ensures that the merged model retains the essential "size" or capacity of the original models. This combination effectively mitigates merging conflicts that arise from overlapping or contradictory task-specific modifications in the weight space.

## Foundational Learning
- SVD decomposition - why needed: breaks down task vectors into orthogonal components to identify and remove redundancy; quick check: verify singular values capture >90% of variance
- Nuclear norm preservation - why needed: maintains the overall magnitude and capacity of the merged model; quick check: confirm original and rescaled nuclear norms match within tolerance
- Spectral truncation - why needed: removes less significant components to reduce interference between merged models; quick check: monitor performance degradation as truncation threshold varies

## Architecture Onboarding
Component map: Task vectors -> SVD decomposition -> Spectral truncation -> Nuclear norm rescaling -> Merged model
Critical path: SVD computation and truncation are the computational bottlenecks, followed by rescaling
Design tradeoffs: Data-free approach sacrifices some potential performance gains from fine-tuning but gains efficiency and applicability to sensitive/private data scenarios
Failure signatures: Performance degradation when merging highly dissimilar tasks, computational overhead for very large models
First experiments:
1. Merge two models fine-tuned on similar tasks to establish baseline improvement
2. Vary singular value truncation threshold to identify optimal values
3. Compare performance against naive averaging across different model sizes

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Evaluation primarily focuses on data-free merging, which may not represent all practical scenarios
- Performance on extremely large (>50B parameter) models remains unverified
- Computational overhead of SVD decomposition for very large models not thoroughly investigated

## Confidence
High confidence in the technical soundness of the SVD-based spectral truncation and rescaling approach, as the mathematical formulation is clear and the method is simple to implement. Medium confidence in the claimed robustness to hyperparameter choices, as the ablation studies show stability but do not explore the full hyperparameter space exhaustively. Medium confidence in the generalizability across architectures, since the experiments cover two model families but not a broader range of transformer variants or non-transformer architectures.

## Next Checks
1. Test STAR on models with >50B parameters to verify scalability and computational feasibility for frontier model merging
2. Evaluate performance when merging models fine-tuned on highly dissimilar tasks to assess the method's ability to handle conflicting objectives
3. Conduct a detailed ablation study on the singular value truncation threshold to identify optimal values for different model sizes and task similarity levels