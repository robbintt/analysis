---
ver: rpa2
title: From User Preferences to Optimization Constraints Using Large Language Models
arxiv_id: '2503.21360'
source_url: https://arxiv.org/abs/2503.21360
tags:
- data
- user
- energy
- task
- italian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) for converting
  user preferences expressed in natural language into formal energy optimization constraints
  for household appliances. Using a pilot dataset of 26 Italian utterances, the task
  was to map user-specified time and temperature preferences into structured constraint
  representations.
---

# From User Preferences to Optimization Constraints Using Large Language Models

## Quick Facts
- **arXiv ID:** 2503.21360
- **Source URL:** https://arxiv.org/abs/2503.21360
- **Reference count:** 26
- **Primary result:** Evaluated LLMs for converting Italian user preferences into formal energy optimization constraints for household appliances using a pilot dataset of 26 utterances.

## Executive Summary
This study investigates the use of Large Language Models (LLMs) to translate natural language user preferences about household appliance usage into formal optimization constraints. Using a pilot dataset of 26 Italian utterances, the researchers tested zero-shot, one-shot, and few-shot prompting on four Italian LLMs. ChatGPT and LLaMAntino-3-ANITA were the only models producing valid results, with ChatGPT performing best in zero-shot settings while LLaMAntino-3-ANITA showed better accuracy with more examples. However, both models struggled significantly with condition generation, indicating that in-context learning alone is insufficient for this task.

## Method Summary
The methodology involves converting Italian natural language user preferences about household appliance usage into formal optimization constraints using XML-tagged prompts and various prompting strategies. The study uses a pilot dataset of 26 Italian utterances with XML tags marking preference-expressing spans. Zero-shot, one-shot, and few-shot (5 examples) prompting are tested on ChatGPT (via API) and LLaMAntino-3-ANITA (local via HuggingFace). Outputs are evaluated using ChrF character n-gram overlap and parsed variable/condition accuracy metrics.

## Key Results
- ChatGPT achieved ChrF of 51.1 in zero-shot prompting, outperforming LLaMAntino-3-ANITA's 37.9
- LLaMAntino-3-ANITA showed consistent improvement with more examples (ChrF 74.5 in few-shot)
- Both models struggled with condition generation, achieving AccConditions between 0.07-0.43 across all settings
- LLaMAntino-2 and Zefiro produced "inconsistent and noise-rich results" and were discarded

## Why This Works (Mechanism)

### Mechanism 1: XML Tagging for Constraint Span Marking
Embedding XML tags around preference-expressing text spans improves LLM ability to identify and extract constraint-relevant information from natural language utterances. The tags create explicit boundaries that reduce ambiguity about which utterance segments require formalization, narrowing the model's attention to constraint-bearing spans rather than full-sentence interpretation. This assumes pre-annotation of constraint spans is available as input to the LLM prompt.

### Mechanism 2: In-Context Learning Scaling with Model-Dependent Curves
Increasing the number of in-context examples improves performance for some models (LLaMAntino-3-ANITA) but can degrade or show inconsistent effects for others (ChatGPT). Different instruction-tuned models have varying sensitivity to context length and example distribution; smaller Italian-specialized models may benefit more from local pattern matching enabled by examples, while larger models may overfit to provided examples in ways that hurt generalization.

### Mechanism 3: Two-Component Constraint Decomposition (Variable + Condition)
The task decomposes into two sub-tasks with different difficulty profiles—variable identification (st for timing, ht for temperature) is substantially easier than condition generation (the temporal/logical bounds on those variables). Variables map relatively directly to entity types mentioned in utterances, while conditions require parsing comparative/quantitative relationships which involves deeper semantic reasoning.

## Foundational Learning

- **Concept: In-Context Learning (Zero/One/Few-Shot Prompting)**
  - Why needed here: The entire experimental methodology depends on understanding how examples in prompts affect model behavior differently across model families.
  - Quick check question: Can you explain why adding one example improved LLaMAntino-3's ChrF by 28 points but slightly degraded ChatGPT's condition accuracy?

- **Concept: Formal Constraint Representation in Optimization**
  - Why needed here: The target output format uses mathematical notation (indices, binary/continuous variables, quantifiers) that differs from natural language generation tasks.
  - Quick check question: Given the utterance "I need hot water from 7 to 8:30," what would the formal constraint look like using the paper's notation?

- **Concept: Sequence-to-Sequence vs Sequence Labeling Tasks**
  - Why needed here: The paper frames constraint generation as seq2seq but references NL4Opt's two-subtask approach (labeling then generation); understanding this distinction informs architecture decisions.
  - Quick check question: If you were to split this task into two stages like NL4Opt, what would each stage output?

## Architecture Onboarding

- **Component map:** Utterance → Span annotation → Prompt construction → LLM inference → Output parsing → Constraint validation → Optimizer integration
- **Critical path:** The span annotation step is currently manual and represents a key bottleneck.
- **Design tradeoffs:** ChatGPT offers better zero-shot performance but requires API dependency; LLaMAntino-3 provides better few-shot scaling with data privacy benefits but needs GPU infrastructure.
- **Failure signatures:** Low condition accuracy (0.07-0.43) indicates systematic difficulty with temporal bound generation; ChatGPT condition accuracy dropped from 0.21 to 0.12 when moving from 0-shot to 1-shot.
- **First 3 experiments:**
  1. Baseline replication: Run zero-shot with XML-tagged prompts on the 26-utterance pilot set using both ChatGPT and LLaMAntino-3.
  2. Span annotation ablation: Test whether removing XML tags degrades performance, quantifying the annotation value.
  3. Error analysis on conditions: Manually categorize condition generation failures across all models to identify clustering patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- Small pilot dataset of only 26 Italian utterances limits generalizability to broader user preference patterns
- Manual XML annotation requirement for constraint spans represents a significant operational bottleneck for real-world deployment
- Exclusive focus on two household appliances (dishwasher and heat pump) restricts applicability to other energy optimization scenarios

## Confidence
- **High confidence:** Zero-shot prompting with ChatGPT outperforms few-shot approaches for this task, directly supported by quantitative results
- **Medium confidence:** LLaMAntino-3-ANITA benefits from few-shot examples while ChatGPT does not, emerging from limited data and may be model-specific
- **Low confidence:** Current approach's practical utility for real-world deployment given manual annotation requirement and poor condition generation accuracy

## Next Checks
1. **Dataset scaling experiment:** Replicate the study with a 10x larger corpus (260+ utterances) spanning additional appliances and constraint types to test whether observed model behaviors persist at scale.

2. **Annotation automation evaluation:** Replace manual XML tagging with an automated NER pipeline and measure degradation in constraint generation accuracy to quantify operational cost.

3. **End-to-end optimization validation:** Integrate generated constraints into an actual household energy optimizer and measure whether they produce appliance schedules that satisfy original user preferences, rather than just measuring syntactic correctness.