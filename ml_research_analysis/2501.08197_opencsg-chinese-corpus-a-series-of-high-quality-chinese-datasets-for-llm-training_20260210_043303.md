---
ver: rpa2
title: 'OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM
  Training'
arxiv_id: '2501.08197'
source_url: https://arxiv.org/abs/2501.08197
tags:
- uni00000048
- uni00000013
- data
- uni0000004c
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the OpenCSG Chinese Corpus, a series of high-quality
  datasets designed to address the scarcity of large-scale, diverse Chinese data for
  training large language models (LLMs). The corpus includes Fineweb-edu-chinese,
  Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each targeting
  different training needs such as pretraining, knowledge-intensive tasks, and instruction
  fine-tuning.
---

# OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training

## Quick Facts
- arXiv ID: 2501.08197
- Source URL: https://arxiv.org/abs/2501.08197
- Reference count: 28
- Introduces OpenCSG Chinese Corpus: curated datasets for Chinese LLM training

## Executive Summary
This paper introduces the OpenCSG Chinese Corpus, a series of high-quality datasets designed to address the scarcity of large-scale, diverse Chinese data for training large language models (LLMs). The corpus includes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each targeting different training needs such as pretraining, knowledge-intensive tasks, and instruction fine-tuning. The datasets are built using scalable, reproducible curation processes that combine automated filtering, synthetic generation, and deduplication techniques. Experimental evaluations on smaller Chinese LLMs demonstrate significant performance improvements, with models trained on Fineweb-edu-chinese outperforming baselines by large margins on benchmarks like C-Eval and CMMLU. Smoltalk-chinese also showed strong gains in alignment tasks, while Cosmopedia-chinese produced well-structured, knowledge-rich outputs despite limited benchmark gains. The OpenCSG Chinese Corpus provides an openly accessible resource to advance Chinese NLP research.

## Method Summary
The OpenCSG Chinese Corpus comprises three main datasets created through distinct curation pipelines. Fineweb-edu-chinese is built by filtering general web-crawl data using a BERT-based classifier trained to score "educational value," retaining samples with scores above 3. Cosmopedia-chinese uses synthetic generation, where high-density knowledge seeds (e.g., BaiduBaike) are expanded into detailed textbook-style content via a long-context model (glm4-9b-longwriter). Smoltalk-chinese is generated through multi-turn conversation synthesis using powerful teacher models (Deepseek-V2.5, Qwen2.5-72B) across 18 task categories, with quality filtering applied to user queries. All pipelines employ MinHash deduplication to ensure data diversity and prevent memorization.

## Key Results
- Models trained on Fineweb-edu-chinese significantly outperformed baselines on C-Eval and CMMLU, with a sharp accuracy increase around 45k steps.
- Smoltalk-chinese yielded the strongest overall performance gains on Alignbench compared to other instruction datasets.
- Cosmopedia-chinese produced well-structured, knowledge-rich outputs, though benchmark gains were limited due to data homogeneity and markdown formatting.

## Why This Works (Mechanism)

### Mechanism 1: Educational Quality Filtering via Model-Based Scoring
Filtering general web-crawl data using a model trained to score "educational value" may increase pretraining efficiency and downstream task accuracy compared to random sampling. A BERT-based classifier (fine-tuned on Qwen2-7b-instruct annotations) scores samples from diverse corpora (Wudao, CCI, etc.). Retaining only samples with scores > 3 (on a 0–5 scale) presumably concentrates the token budget on high-value, coherent text, reducing noise and accelerating the acquisition of domain-specific patterns. The core assumption is that the proxy model’s definition of "educational value" correlates with features that improve language modeling performance on benchmarks like C-Eval. Evidence includes mentions of Fineweb-edu datasets focusing on "filtered, high-quality content" and reports of a "sharp accuracy increase around 45k steps" for the filtered model vs. baseline. If the classifier develops a bias against specific domains (e.g., colloquial speech or technical code) that are underrepresented in the "educational" definition, model versatility may drop.

### Mechanism 2: Synthetic Knowledge Expansion from High-Quality Seeds
Generating synthetic textbook-style content from curated knowledge seeds (e.g., BaiduBaike) can produce coherent, knowledge-rich outputs, although this does not guarantee immediate improvements in standard multiple-choice benchmarks. High-density seed data is fed into a long-context model (`glm4-9b-longwriter`) to generate detailed textbook units or stories. This bypasses the noise of raw web text but introduces the risk of stylistic homogeneity. The core assumption is that the factual density and rhetorical structure of synthetic textbooks provide better learning signals than raw, noisy web text, even if generated by a smaller (7B-9B) model. Evidence includes the use of `glm4-9b-longwriter` to generate detailed content because standard chat models were too concise, and human evaluators found outputs "consistently well-structured and knowledge-rich." If the generator model hallucinates systematically or adopts a repetitive "synthetic style," the model may overfit to the generator's artifacts rather than the underlying knowledge.

### Mechanism 3: Diverse Multi-Turn Instruction Synthesis
Fine-tuning on synthetically generated, multi-turn conversations with broad task coverage can improve alignment performance (e.g., helpfulness, clarity) more effectively than single-turn or less diverse datasets. Powerful teacher models (Deepseek-V2.5, Qwen2.5-72B) generate multi-turn dialogues across 18 task categories (e.g., math, coding, role-play). Quality control is applied via scoring the *user query* for clarity/complexity, ensuring the model learns to handle high-quality interactions. The core assumption is that diversity in conversation style and task type (synthetic variety) is a stronger driver of alignment performance than dataset size alone. Evidence includes Smoltalk-Chinese yielding the "strongest overall performance gains" on Alignbench compared to Infinity-Instruct and Magpie. If the synthetic prompts lack the nuance or "imperfections" of real user queries, the model may fail to generalize to messy, real-world interactions.

## Foundational Learning

- **Concept: Educational Value Scoring**
  - **Why needed here:** The Fineweb-edu pipeline relies on a regression model to predict a 0-5 score for "educational value." Understanding that this is a heuristic proxy (not ground truth) is critical for diagnosing data gaps.
  - **Quick check question:** If the scorer rates a piece of code documentation as "0" because it lacks prose, how would that affect the resulting model's coding ability?

- **Concept: Synthetic Data Homogeneity**
  - **Why needed here:** The paper explicitly notes Cosmopedia-Chinese suffered from "homogeneity of synthesized data," limiting benchmark gains despite high text quality.
  - **Quick check question:** Why might a model trained solely on perfect, textbook-style grammar struggle to understand chaotic web text or dialogue?

- **Concept: Deduplication (MinHash)**
  - **Why needed here:** All three pipelines use MinHash to prevent the model from memorizing specific generated outputs or repeated web scrapes.
  - **Quick check question:** If you set the overlap threshold too aggressively (e.g., 0.9 instead of 0.7), what kind of "near-duplicate" data might slip through, and how would that impact training loss?

## Architecture Onboarding

- **Component map:** Fineweb: Wudao/ CCI/ etc. -> Sample 1M -> Annotate (Qwen2-7b-instruct) -> Train BERT Scorer -> Filter (score > 3) -> MinHash dedup
- **Critical path:** Seed Selection -> Prompt Engineering -> Quality Filtering. Success depends on the initial seed pool (e.g., BaiduBaike vs. random web text). The transition from "concise" to "detailed" outputs via specific prompting (Section 3.3) is the critical lever for synthetic utility. The 2B model pretraining results (Section 4.1) suggest the filtering step is more impactful than raw data volume.
- **Design tradeoffs:**
  - **Cosmopedia:** High coherence vs. Low diversity. Tradeoff: Better for structured knowledge transfer; worse for benchmark generalization (Section 4.2).
  - **Smoltalk:** Synthetic diversity vs. Realism. Tradeoff: High task coverage (18 categories) but risks "stylistic bias" from teacher models (Section 3.4).
- **Failure signatures:**
  - **Cosmopedia Mode:** Model outputs well-structured markdown but fails to answer benchmark questions correctly (Indicates: homogeneity overfitting).
  - **Fineweb Mode:** Training loss drops, but benchmark accuracy plateaus early (Indicates: Scoring threshold too high/low, filtering out useful diverse data).
- **First 3 experiments:**
  1. **Ablation on Scoring Threshold:** Train three small (e.g., 100M param) models on Fineweb-edu-chinese with score thresholds of 2, 3, and 4. Measure C-Eval accuracy to validate the paper's claimed >3 filter.
  2. **Markdown Stripping:** Pre-train on Cosmopedia-chinese with all markdown/formatting tokens removed vs. raw. Evaluate if the "parsing formatting tokens" hypothesis (Section 4.2) explains the lack of benchmark gains.
  3. **Teacher Model Swap:** Generate a subset of Smoltalk-chinese using a smaller model (e.g., Qwen2-7B) vs. the reported 72B model. Fine-tune and compare on Alignbench to quantify the sensitivity of the synthetic instruction pipeline to teacher quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can blending real-world data with Cosmopedia-Chinese or removing markdown formatting significantly improve downstream benchmark performance?
- Basis in paper: [explicit] The authors attribute the lack of benchmark gains for Cosmopedia-Chinese to data homogeneity and "overuse of markdown formatting," explicitly recommending "blending real-world data with the synthesized corpus and reducing or removing markdown tags" in the conclusion.
- Why unresolved: The current version of the dataset resulted in "little accuracy gains" despite human evaluators noting the text was coherent and knowledge-rich, indicating a disconnect between perceived quality and training utility.
- What evidence would resolve it: A comparison of pretraining runs using the current dataset versus a modified version (mixed real/synthetic or stripped of markdown) evaluated on C-Eval and CMMLU.

### Open Question 2
- Question: How does the OpenCSG corpus influence model performance in areas not captured by standard benchmarks, such as factual correctness and safety alignment?
- Basis in paper: [explicit] The conclusion calls for research to "explore alternative or complementary evaluation metrics to capture a broader spectrum of model strengths and weaknesses, including factual correctness, reasoning depth, and safety alignment."
- Why unresolved: The current evaluation relies primarily on C-Eval, CMMLU, and Alignbench, which may not fully capture the "diverse coverage" or potential safety risks inherent in synthetic and web-scraped data.
- What evidence would resolve it: Evaluation results on specialized benchmarks for hallucination rates (factual correctness) and safety red-teaming datasets.

### Open Question 3
- Question: Does the effectiveness of the Fineweb-Edu-Chinese filtering pipeline persist when scaling model size beyond the 2B parameter range?
- Basis in paper: [inferred] The experimental analysis was limited to "smaller parameter models" (specifically a "2B-level Llama-based model") due to computational resource limitations.
- Why unresolved: Data quality filtering often exhibits different scaling properties; it is unverified if the "sharp accuracy increase" observed in the 2B model translates linearly to larger, more capable models (e.g., 7B or 70B).
- What evidence would resolve it: Pretraining results for larger models (e.g., 7B parameters) on the Fineweb-Edu-Chinese datasets compared to baselines.

## Limitations
- The "educational value" regression model is a critical but opaque component; the paper does not report its R² or conduct ablation studies on the scoring threshold.
- Cosmopedia-chinese's lack of benchmark improvements raises questions about the transfer from synthetic textbook coherence to general language understanding; the paper notes "homogeneity" but does not provide quantitative measures.
- The strong performance of Smoltalk-chinese on Alignbench is attributed to teacher model quality, but the paper does not test sensitivity to teacher model scale or choice.

## Confidence

- **High Confidence:** The OpenCSG Chinese Corpus datasets are technically sound, openly accessible, and the curation methods (MinHash deduplication, multi-turn generation, quality scoring) are reproducible.
- **Medium Confidence:** The Fineweb-edu-chinese dataset significantly improves pretraining efficiency, as evidenced by the sharp accuracy increase around 45k steps on C-Eval and CMMLU.
- **Medium Confidence:** Smoltalk-chinese improves alignment performance on Alignbench compared to other instruction datasets.
- **Low Confidence:** Cosmopedia-chinese produces "knowledge-rich" outputs. While human evaluators found the text coherent and well-structured, the lack of corresponding benchmark gains makes it difficult to quantify the dataset's true utility.

## Next Checks

1. **Ablation on Educational Scoring Threshold:** Train three small (e.g., 100M parameter) models on Fineweb-edu-chinese data filtered with score thresholds of 2, 3, and 4. Evaluate on C-Eval and CMMLU to empirically validate the claimed superiority of the >3 threshold and characterize the tradeoff between data quality and quantity.

2. **Markdown and Formatting Ablation:** Pretrain a small model on Cosmopedia-chinese with all markdown and formatting tokens removed, and compare its performance on C-Eval and CMMLU to a model trained on the raw, formatted data. This will test the hypothesis that the lack of benchmark gains is due to the model learning to "parse formatting" rather than the underlying knowledge.

3. **Teacher Model Scale Sensitivity:** Generate a 10% subset of the Smoltalk-chinese dataset using a smaller, more accessible model (e.g., Qwen2-7B-Instruct) instead of the 72B-parameter teachers. Fine-tune a 2B-parameter model on both the full and reduced datasets and compare their performance on Alignbench to quantify the impact of teacher model scale on alignment quality.