---
ver: rpa2
title: 'ICONS: Influence Consensus for Vision-Language Data Selection'
arxiv_id: '2501.00654'
source_url: https://arxiv.org/abs/2501.00654
tags:
- data
- tasks
- influence
- training
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ICONS introduces a gradient-based influence consensus method for
  vision-language data selection that identifies training samples beneficial across
  multiple tasks through majority voting. The approach computes task-specific influence
  scores using first-order training dynamics, then aggregates them to find consistently
  valuable examples while avoiding outlier sensitivity.
---

# ICONS: Influence Consensus for Vision-Language Data Selection

## Quick Facts
- **arXiv ID**: 2501.00654
- **Source URL**: https://arxiv.org/abs/2501.00654
- **Reference count**: 40
- **Primary result**: 20% data selection retains 98.6-99.8% of full-dataset performance

## Executive Summary
ICONS introduces a gradient-based influence consensus method for vision-language data selection that identifies training samples beneficial across multiple tasks through majority voting. The approach computes task-specific influence scores using first-order training dynamics, then aggregates them to find consistently valuable examples while avoiding outlier sensitivity. Models trained on 20% selected subsets retain 98.6-99.8% of full-dataset performance, significantly outperforming random selection while enabling 5x reduction in training data.

## Method Summary
ICONS uses influence functions to compute how much each training sample affects the validation loss for multiple tasks. For each task, it calculates influence scores using first-order approximations of training dynamics. The method then aggregates these task-specific scores through majority voting to identify samples that consistently improve performance across different tasks. This consensus approach ensures selected samples are generally beneficial rather than optimized for a single task. The framework is applied to create smaller, high-quality datasets (LLAVA-ICONS-133K, CAMBRIAN-ICONS-1.4M, VISION-FLAN-ICONS-37K) that maintain near-full performance while reducing training data by 5x.

## Key Results
- Models trained on 20% ICONS-selected subsets retain 98.6-99.8% of full-dataset performance
- Outperforms random selection baselines (95.8-91.6% performance retention)
- Generalizes to unseen tasks and model architectures
- Achieves 5x reduction in training data requirements

## Why This Works (Mechanism)
ICONS leverages the principle that training samples beneficial across multiple diverse tasks are more likely to be generally useful for vision-language learning. By computing influence scores for each sample on multiple tasks and using majority voting, the method identifies samples that provide consistent value rather than being optimized for narrow use cases. The first-order approximation makes influence computation tractable for large-scale models while maintaining accuracy. The consensus mechanism inherently filters out samples that might be beneficial for only one task but harmful or neutral for others, creating a robust selection that transfers well to new tasks.

## Foundational Learning

**Influence Functions**: Measure how much each training sample affects model predictions on validation data. Why needed: Enables data selection without retraining. Quick check: Verify influence scores correlate with actual performance changes.

**First-Order Training Dynamics**: Approximate how gradients change during training using first-order Taylor expansion. Why needed: Makes influence computation computationally feasible for large models. Quick check: Compare first-order vs. exact influence computation on small dataset.

**Majority Voting**: Aggregates task-specific influence scores to find samples beneficial across multiple tasks. Why needed: Ensures selected samples have broad utility rather than task-specific optimization. Quick check: Validate that consensus-selected samples perform well on held-out tasks.

**Gradient-Based Data Selection**: Uses optimization gradients to identify valuable training samples. Why needed: Provides principled approach to data curation. Quick check: Compare gradient-based selection to random and heuristic methods.

## Architecture Onboarding

**Component Map**: Data -> Influence Computation -> Task-Specific Scores -> Majority Voting -> Selected Subset -> Model Training

**Critical Path**: The influence computation stage is the computational bottleneck, requiring forward and backward passes for each training sample on multiple tasks. This dominates the selection time but only needs to be done once per dataset.

**Design Tradeoffs**: First-order approximation vs. exact influence computation (speed vs. accuracy), number of consensus tasks vs. selection quality, subset size vs. performance retention.

**Failure Signatures**: Poor performance on held-out tasks suggests inadequate task diversity in consensus set; high variance in influence scores indicates noisy data; slow selection indicates computational scaling issues.

**Three First Experiments**:
1. Run ICONS selection on a small subset (1% of data) and verify that selected samples show higher average influence scores
2. Compare model performance when trained on ICONS-selected 20% vs. random 20% of the same dataset
3. Evaluate transfer performance to an unseen task to verify generalization capability

## Open Questions the Paper Calls Out

None

## Limitations

- Performance generalization may not extend to specialized domains beyond benchmark tasks
- Computational overhead for influence computation may become prohibitive for datasets exceeding 100M samples
- Optimal consensus task mixture may vary significantly across different application contexts

## Confidence

**High Confidence Claims**:
- ICONS-selected 20% subsets maintain 98.6-99.8% of full-dataset performance
- Method significantly outperforms random selection baselines
- First-order approximation approach is computationally validated

**Medium Confidence Claims**:
- Generalization capability to unseen tasks relies on limited benchmark diversity
- Computational efficiency claims lack detailed scaling analysis
- 5x data reduction benefit may vary with specific task requirements

## Next Checks

1. **Cross-Domain Transfer Validation**: Evaluate ICONS-selected subsets on specialized real-world vision-language tasks (medical imaging, remote sensing, industrial quality control) not included in original consensus tasks.

2. **Computational Scaling Analysis**: Measure selection time and resource requirements across dataset sizes (10M, 50M, 100M+ samples) to establish practical limits and identify bottlenecks.

3. **Dynamic Task Mixture Sensitivity**: Test performance consistency when using different task subsets for consensus formation while maintaining the same underlying data distribution.