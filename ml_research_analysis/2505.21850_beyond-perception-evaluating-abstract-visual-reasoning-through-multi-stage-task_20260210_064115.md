---
ver: rpa2
title: 'Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage
  Task'
arxiv_id: '2505.21850'
source_url: https://arxiv.org/abs/2505.21850
tags:
- panel
- stage
- objects
- dependent
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MultiStAR, a Multi-Stage Abstract Visual
  Reasoning benchmark that evaluates Multimodal Large Language Models (MLLMs) on intermediate
  reasoning steps rather than just final outcomes. The benchmark consists of two tasks:
  Direct Answer, which tests perception and reasoning at varying complexity levels,
  and Logical Chain, which measures sequential reasoning with dependencies between
  stages.'
---

# Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task

## Quick Facts
- **arXiv ID**: 2505.21850
- **Source URL**: https://arxiv.org/abs/2505.21850
- **Reference count**: 40
- **Primary result**: Introduces MultiStAR benchmark and MSEval metric to evaluate intermediate reasoning steps in MLLMs for abstract visual reasoning

## Executive Summary
This paper addresses a critical gap in evaluating Multimodal Large Language Models (MLLMs) by introducing the MultiStAR benchmark, which assesses abstract visual reasoning through multi-stage tasks rather than just final outcomes. The benchmark consists of two tasks: Direct Answer (testing perception and reasoning at varying complexity) and Logical Chain (measuring sequential reasoning with dependencies between stages). To better capture the quality of reasoning processes, the authors propose MSEval, a novel metric that incorporates intermediate stage accuracy alongside final outcomes using conditional mutual information weighting. Experiments with 17 representative MLLMs reveal that while models perform well on basic perception tasks, they struggle significantly with complex rule detection stages, highlighting the gap between current MLLMs and human reasoning abilities.

## Method Summary
The MultiStAR benchmark introduces a novel approach to evaluating MLLMs by breaking down abstract visual reasoning into multiple stages rather than focusing solely on final answers. The benchmark includes two complementary tasks: Direct Answer, which tests perception and reasoning abilities across different complexity levels, and Logical Chain, which requires sequential reasoning with dependencies between stages. To address limitations in traditional evaluation metrics that only consider final outcomes, the authors developed MSEval, a metric that evaluates reasoning performance by incorporating the accuracy of intermediate stages. MSEval uses conditional mutual information to weight the contribution of each stage, providing a more nuanced assessment of model reasoning capabilities. The benchmark was tested on 17 representative MLLMs to evaluate their performance across both tasks.

## Key Results
- MLLMs show strong performance on basic perception tasks but struggle significantly with complex rule detection stages
- The MSEval metric provides more accurate assessment of reasoning ability by incorporating intermediate step evaluation
- A clear performance gap exists between MLLMs and human reasoning abilities, particularly in multi-stage sequential reasoning tasks

## Why This Works (Mechanism)
The MultiStAR benchmark works by decomposing abstract visual reasoning into discrete, hierarchical stages that mirror human cognitive processes. By evaluating intermediate steps rather than just final answers, the benchmark captures the quality of reasoning paths that models take to reach conclusions. The MSEval metric enhances this approach by using conditional mutual information to weight each stage's contribution based on its dependency relationships, ensuring that errors in early stages appropriately penalize downstream performance. This staged evaluation reveals that while MLLMs can handle isolated perception tasks effectively, they struggle with the complex reasoning patterns required for rule detection and sequential inference.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: AI systems that process both visual and textual inputs to generate responses; needed to understand the target of evaluation
- **Conditional Mutual Information**: A measure of the amount of information that one random variable contains about another, given a third; needed to understand MSEval's weighting mechanism
- **Abstract Visual Reasoning**: The ability to identify patterns, rules, and relationships in visual data without relying on prior knowledge; needed to understand benchmark scope
- **Hierarchical Task Decomposition**: Breaking complex tasks into sequential stages; needed to understand the benchmark structure
- **Sequential Reasoning with Dependencies**: Reasoning where conclusions at each stage depend on previous stages; needed to understand Logical Chain task
- **Perception vs Reasoning Distinction**: Differentiating between recognizing visual elements and understanding their relationships; needed to understand task complexity levels

## Architecture Onboarding
- **Component Map**: Visual Input → Perception Stage → Rule Detection Stage → Sequential Reasoning Stage → Final Answer
- **Critical Path**: Direct Answer Task (Perception → Reasoning → Answer) and Logical Chain Task (Perception → Intermediate Reasoning → Sequential Reasoning → Final Answer)
- **Design Tradeoffs**: Balancing task complexity with model capability vs. creating meaningful differentiation between models
- **Failure Signatures**: Early stage errors cascade through dependent stages; models fail to generalize rule detection from simple to complex patterns
- **First Experiments**: 1) Test model performance on isolated perception tasks vs. integrated reasoning tasks, 2) Evaluate MSEval sensitivity to different weighting schemes, 3) Compare performance across models with varying architectural approaches

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The benchmark assumes a fixed progression of reasoning difficulty across stages, which may not reflect all real-world reasoning patterns
- MSEval's reliance on conditional mutual information introduces complexity that may be sensitive to parameter choices
- The paper does not address potential confounding factors such as model-specific prompt engineering effects

## Confidence
- **High confidence**: MLLMs struggle with complex rule detection stages relative to basic perception tasks
- **Medium confidence**: MSEval provides more accurate assessment of reasoning ability through intermediate step evaluation
- **Low confidence**: Generalizability of the two-task structure to broader abstract reasoning domains

## Next Checks
1. Test MSEval's sensitivity to different weighting schemes for intermediate steps through ablation studies
2. Evaluate model performance on randomly shuffled stage orders to verify the assumed difficulty progression
3. Assess whether prompt engineering variations significantly impact the performance gap between Direct Answer and Logical Chain tasks