---
ver: rpa2
title: Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language
  Models
arxiv_id: '2502.08922'
source_url: https://arxiv.org/abs/2502.08922
tags:
- preference
- reward
- data
- internal
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies inconsistency between generative and implicit
  reward models during self-rewarding language model (SRLM) training. To address this,
  the authors propose Self-Consistent Internal Rewards (SCIR), which enhances consistency
  among internal reward models through a consistency loss and selectively uses consistent
  predictions for preference optimization.
---

# Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models

## Quick Facts
- arXiv ID: 2502.08922
- Source URL: https://arxiv.org/abs/2502.08922
- Reference count: 21
- Primary result: SCIR improves alignment performance, achieving 14% improvement in length-controlled win rate on AlpacaEval 2.0

## Executive Summary
This paper addresses inconsistency between generative and implicit reward models during self-rewarding language model (SRLM) training. The authors propose Self-Consistent Internal Rewards (SCIR), which enhances consistency among internal reward models through a consistency loss and selectively uses consistent predictions for preference optimization. Experiments on Mistral-7B show that SCIR significantly improves alignment performance and maintains consistency across iterations.

## Method Summary
The authors introduce Self-Consistent Internal Rewards (SCIR) to address inconsistency between generative and reward models in self-rewarding language models. SCIR employs a consistency loss to enhance alignment between different internal reward models and implements selective preference optimization using only consistent predictions. This approach aims to improve the overall alignment performance of SRLMs by ensuring internal reward models provide coherent feedback during training.

## Key Results
- 14% improvement in length-controlled win rate on AlpacaEval 2.0 compared to baselines
- Enhanced reward modeling ability in Mistral-7B
- Maintained consistency across training iterations
- Significant improvements in alignment performance

## Why This Works (Mechanism)
SCIR works by introducing a consistency loss that regularizes the internal reward models to provide coherent feedback. During self-rewarding, the model generates responses and then evaluates them using internal reward models. When these reward models disagree or are inconsistent, the quality of preference optimization suffers. SCIR's consistency loss forces the reward models to align with each other, while selective preference optimization ensures that only reliable, consistent feedback is used for training updates. This creates a more stable and effective self-rewarding loop.

## Foundational Learning
1. Self-Rewarding Language Models (SRLMs)
   - Why needed: Understanding the self-rewarding paradigm is crucial as SCIR specifically targets improvements within this framework
   - Quick check: SRLMs generate their own training data and reward signals without external human feedback

2. Internal Reward Models
   - Why needed: These models evaluate generated content and guide preference optimization, making their consistency critical for effective training
   - Quick check: Internal reward models can be implicit (part of the same model) or explicit separate components

3. Consistency Loss
   - Why needed: This regularization technique ensures different reward models provide aligned feedback, preventing contradictory signals
   - Quick check: Consistency loss measures divergence between predictions of different reward models

## Architecture Onboarding

**Component Map:** Generative Model -> Internal Reward Models -> Consistency Loss -> Preference Optimization -> Updated Generative Model

**Critical Path:** The core training loop involves: (1) generating responses, (2) evaluating with internal reward models, (3) computing consistency loss, (4) selectively applying preference optimization based on consistent predictions, and (5) updating the generative model.

**Design Tradeoffs:** The selective preference optimization introduces a filtering step that may slow training but improves quality. The consistency loss adds computational overhead but ensures more reliable reward signals. The balance between consistency enforcement and model flexibility represents a key design consideration.

**Failure Signatures:** Inconsistent reward signals across internal models, preference optimization based on contradictory feedback, degradation in win rates on alignment benchmarks, and loss of consistency across training iterations.

**First Experiments:**
1. Implement the consistency loss component and verify it reduces divergence between internal reward models
2. Test selective preference optimization with synthetic consistent/inconsistent reward pairs
3. Conduct ablation studies comparing full SCIR with variants missing either consistency loss or selective optimization

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Limited evaluation to Mistral-7B model, raising questions about scalability to larger models
- Lack of extensive ablation studies to isolate individual component contributions
- Insufficient characterization of computational overhead introduced by consistency mechanisms
- Evaluation focused on specific alignment benchmarks without broader domain testing

## Confidence

**Major Claim Confidence Labels:**
- SCIR improves alignment performance: High
- Inconsistency between generative and reward models is a significant issue: Medium
- The proposed consistency loss is the primary driver of improvements: Low
- SCIR maintains consistency across iterations: Medium

## Next Checks

1. Conduct extensive ablation studies to quantify the individual contributions of the consistency loss and selective preference optimization components

2. Evaluate SCIR on larger language models (e.g., 70B+ parameters) and diverse domains to assess scalability and generalizability

3. Perform a detailed computational complexity analysis comparing training time and resource requirements between SCIR and baseline methods