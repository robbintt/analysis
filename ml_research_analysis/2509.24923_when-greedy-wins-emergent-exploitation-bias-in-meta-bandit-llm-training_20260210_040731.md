---
ver: rpa2
title: 'When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training'
arxiv_id: '2509.24923'
source_url: https://arxiv.org/abs/2509.24923
tags:
- gaussian5
- reward
- training
- var1
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often struggle with exploration in sequential
  decision-making tasks like the multi-armed bandit problem, defaulting to greedy
  behavior. This paper investigates how supervised fine-tuning (SFT) and reinforcement
  learning (RL) shape exploration strategies in LLMs.
---

# When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training

## Quick Facts
- **arXiv ID**: 2509.24923
- **Source URL**: https://arxiv.org/abs/2509.24923
- **Reference count**: 40
- **Key outcome**: LLMs trained on bandit tasks show emergent exploitation bias, abandoning exploration prematurely despite achieving competitive regret scores.

## Executive Summary
Large language models often struggle with exploration in sequential decision-making tasks like the multi-armed bandit problem, defaulting to greedy behavior. This paper investigates how supervised fine-tuning (SFT) and reinforcement learning (RL) shape exploration strategies in LLMs. The authors train LLMs using SFT on expert UCB trajectories and RL with three reward signals: original bandit rewards, strategic regret-shaped rewards, and algorithmic rewards that incentivize imitation of UCB. Both methods improve performance over pre-trained models, achieving regret comparable to UCB and Thompson Sampling baselines, with strong generalization to 6× longer horizons and across bandit families. However, behavioral analysis reveals that learned policies exhibit emergent exploitation bias, prematurely abandoning exploration and showing higher suffix failure rates than theoretical optimal policies. RL-trained agents generalize more reliably than SFT across distributions. The study highlights the importance of reward design and evaluation beyond average regret to promote robust exploratory behavior in LLM agents.

## Method Summary
The authors investigate exploration strategies in LLMs by training on the multi-armed bandit problem using two approaches: supervised fine-tuning (SFT) on expert Upper Confidence Bound (UCB) trajectories, and reinforcement learning with three reward signals - original bandit rewards, regret-shaped rewards, and algorithmic rewards incentivizing UCB imitation. They evaluate performance using average regret and suffix failure rates across training and test distributions, including varying horizon lengths (6× longer) and different bandit families. The study compares learned policies against UCB and Thompson Sampling baselines while analyzing behavioral patterns to identify emergent exploitation bias.

## Key Results
- Both SFT and RL training improve bandit performance over pre-trained models, achieving regret comparable to UCB and Thompson Sampling baselines
- Learned policies show emergent exploitation bias, prematurely abandoning exploration with higher suffix failure rates than optimal theoretical policies
- RL-trained agents demonstrate more reliable generalization across distributions compared to SFT-trained models

## Why This Works (Mechanism)
The training approaches enable LLMs to learn bandit policies through exposure to expert trajectories (SFT) or reward-based optimization (RL). However, the emergent exploitation bias suggests that standard training objectives may inadvertently reinforce premature convergence to greedy strategies. This occurs because reward signals focused on immediate regret reduction can overshadow the long-term value of exploration, leading models to optimize for short-term gains at the expense of optimal exploration-exploitation balance.

## Foundational Learning

### Multi-armed Bandit Problem
**Why needed**: Provides the sequential decision-making framework for testing exploration-exploitation trade-offs
**Quick check**: Can the model identify optimal arm selection strategies given limited feedback

### Upper Confidence Bound (UCB) Algorithm
**Why needed**: Serves as the expert policy for SFT training and benchmark for comparison
**Quick check**: Does the learned policy approximate UCB's exploration-exploitation balance

### Reinforcement Learning with Shaped Rewards
**Why needed**: Enables optimization of bandit policies through different reward formulations
**Quick check**: Can reward shaping influence exploration duration and overall performance

### Regret Analysis
**Why needed**: Quantifies performance difference between learned and optimal policies
**Quick check**: Does average regret accurately reflect long-term decision quality

## Architecture Onboarding

### Component Map
Pre-trained LLM -> (SFT or RL training) -> Trained Bandit Policy -> Evaluation (Regret, Failure Rate) -> Comparison with UCB/TS

### Critical Path
The critical path involves training LLM on bandit trajectories or RL optimization, followed by evaluation on held-out distributions. The transition from exploration to exploitation represents a key decision point where emergent bias manifests.

### Design Tradeoffs
The study balances training efficiency (SFT vs RL) against policy quality and generalization. SFT offers faster training but may encode suboptimal exploration patterns from expert trajectories, while RL provides more flexible reward optimization but requires careful reward shaping to maintain exploration.

### Failure Signatures
Emergent exploitation bias manifests as premature abandonment of exploration, visible through increased suffix failure rates despite competitive average regret. This indicates short-term optimization at the expense of long-term optimal behavior.

### First 3 Experiments
1. Compare suffix failure rates across training methods (SFT vs RL) on identical bandit distributions
2. Test performance degradation when horizon length is extended beyond training conditions
3. Evaluate sensitivity to reward shaping by testing different regret formulations

## Open Questions the Paper Calls Out
None

## Limitations
- Characterization of premature exploration abandonment lacks causal mechanisms explaining why it occurs
- Generalization claims to 6× longer horizons may be brittle given increased suffix failure rates
- Comparative advantage of RL over SFT needs additional ablation studies to isolate specific factors

## Confidence

**High confidence**: Empirical finding that both SFT and RL improve bandit performance over pre-trained models, achieving regret comparable to UCB and Thompson Sampling baselines

**Medium confidence**: Generalization claims to 6× longer horizons and across bandit families, as behavioral analysis reveals increased suffix failure rates that complicate the generalization narrative

**Medium confidence**: Comparative advantage of RL over SFT for generalization, as this is supported by data but could benefit from additional ablation studies

## Next Checks

1. Conduct mechanistic analysis to identify specific decision points where exploration is abandoned, using attention visualization or intermediate activation analysis to understand why the model switches from exploration to exploitation

2. Test alternative reward shaping approaches that explicitly penalize early termination of exploration, including hybrid rewards that balance immediate regret against exploration duration

3. Evaluate performance on adversarial bandit distributions where optimal exploration requires sustained sampling of suboptimal arms, to determine whether the exploitation bias is a general tendency or distribution-dependent