---
ver: rpa2
title: 'Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity
  on a Scale'
arxiv_id: '2512.09634'
source_url: https://arxiv.org/abs/2512.09634
tags:
- texts
- subjectivity
- were
- annotators
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study created the first Estonian-language dataset for document-level
  subjectivity annotation, containing 1,000 texts rated on a continuous 0-100 scale
  by four annotators. Inter-annotator correlations were moderate (0.525-0.675), prompting
  re-annotation of a subset with high disagreement, which improved agreement (0.678).
---

# Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale

## Quick Facts
- **arXiv ID**: 2512.09634
- **Source URL**: https://arxiv.org/abs/2512.09634
- **Reference count**: 0
- **Primary result**: Created first Estonian-language dataset for document-level subjectivity annotation with 1,000 texts rated 0-100 by four annotators; inter-annotator correlations were moderate (0.525-0.675), prompting re-annotation that improved agreement (0.678).

## Executive Summary
This study established the first Estonian-language dataset for document-level subjectivity annotation, containing 1,000 texts rated on a continuous 0-100 scale by four annotators. The research revealed moderate inter-annotator correlations (0.525-0.675) and systematic differences between human and GPT-5 subjectivity judgments, particularly regarding quoted speech and colloquial language. These findings demonstrate that while LLM-based subjectivity scoring is feasible, it relies on different perceptual cues than humans and cannot yet fully replace human annotation. The dataset and analysis provide valuable insights into subjectivity perception and automation in Estonian.

## Method Summary
The study created a dataset of 1,000 Estonian texts (700 random web texts, 150 news articles, 150 opinion pieces) rated for subjectivity on a continuous 0-100 scale by four annotators via LimeSurvey. Texts were pseudonymized and filtered to 100-6,000 characters. Annotators used a sliding scale with optional confidence ratings and comments. Re-annotation was performed on 250 texts showing highest disagreement. GPT-5 was used to generate automatic subjectivity scores using a structured prompt, run three times with averaged results. Pearson correlation coefficients measured inter-annotator agreement.

## Key Results
- Human annotators achieved moderate inter-annotator correlations of 0.525-0.675, improving to 0.678 after re-annotation of high-disagreement texts
- GPT-5 showed comparable correlations to humans (0.60-0.80) but systematically differed by rating quoted news texts higher and colloquial language lower
- Annotator scores were influenced by previously seen texts, creating order-dependent bias
- Continuous scale captured more variance than discrete alternatives, though some annotators defaulted to scale endpoints

## Why This Works (Mechanism)

### Mechanism 1: Continuous Scale Captures Gradual Perception
- **Claim**: A continuous 0–100 scale enables annotators to express fine-grained subjectivity judgments better than binary or Likert scales.
- **Mechanism**: Subjectivity is inherently continuous; discrete categories force information loss. Continuous scales preserve variance across texts, allowing downstream models to learn graded distinctions rather than crude boundaries.
- **Core assumption**: Annotators can consistently map their internal perception to a numeric range without excessive cognitive load.
- **Evidence anchors**: [abstract] "rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective)"; [section 2.1] "5- or 7-point scales too restrictive and opted for a continuous scale"
- **Break condition**: If annotators cluster scores at scale endpoints (as A1 did with 391 zeros and 234 hundreds), the continuous advantage degrades to quasi-categorical behavior.

### Mechanism 2: Context Priming Affects Human Judgment
- **Claim**: Human annotators' scores are influenced by previously seen texts, creating order-dependent bias.
- **Mechanism**: Sequential exposure creates contrast effects—a series of highly subjective texts makes subsequent texts appear more objective by comparison. Randomization across annotators distributes this bias differently, contributing to inter-annotator disagreement.
- **Core assumption**: Order effects are not self-correcting within a single annotation session.
- **Evidence anchors**: [section 6.2] "after scoring several highly subjective texts in a row, they felt more sensitive to objectivity and tended to assign lower subjectivity scores"; [section 3.3] Annotators explicitly reported context influence in post-annotation interviews
- **Break condition**: If texts are stratified by predicted subjectivity and presented in fixed calibration blocks, order effects may be reduced but not eliminated.

### Mechanism 3: Human–LLM Divergence in Cue Weighting
- **Claim**: GPT-5 produces plausible subjectivity scores but systematically differs from humans by prioritizing content over register and misinterpreting quoted speech.
- **Mechanism**: Humans treat informal tone (slang, emoticons) as a subjectivity signal and discount quoted speech as externally sourced. LLMs process quotes as part of the text's asserted content and underweight stylistic register cues.
- **Core assumption**: These cue-weighting differences are consistent across texts and not random noise.
- **Evidence anchors**: [section 5.3] "GPT-5 rated texts with quotations...consistently higher than human annotators"; [section 5.3] "human annotators rated texts written in a very informal tone...as highly subjective, whereas GPT-5 focused more on the content"
- **Break condition**: Prompt engineering with explicit register-awareness instructions or few-shot examples containing colloquial/quoted texts may reduce—but not eliminate—this divergence.

## Foundational Learning

- **Concept: Inter-annotator agreement (Pearson correlation)**
  - **Why needed here**: Subjectivity is perceptual; without agreement metrics, you cannot distinguish dataset noise from inherent task difficulty.
  - **Quick check question**: If two annotators correlate at 0.55, is this a dataset failure or expected variance for subjective tasks?

- **Concept: Annotation context effects**
  - **Why needed here**: Understanding that scores are order-dependent prevents over-interpreting any single annotation as ground truth.
  - **Quick check question**: How would you design an annotation protocol to minimize sequence bias?

- **Concept: LLM-as-annotator validation**
  - **Why needed here**: LLM annotations are seductive but require systematic comparison to human baselines before deployment.
  - **Quick check question**: GPT-5 correlates 0.60–0.80 with humans but diverges on quotes. Is it safe to use for automating subjectivity labels?

## Architecture Onboarding

- **Component map**: Corpus sampling layer -> Preprocessing layer -> Annotation interface -> Quality layer -> LLM annotation layer -> Aggregation layer
- **Critical path**: Corpus selection -> Annotator recruitment -> Pilot validation -> Full annotation -> Disagreement analysis -> Re-annotation -> LLM scoring -> Cross-comparison
- **Design tradeoffs**: Continuous scale vs. Likert (more expressive but some collapse to endpoints); 4 annotators vs. more (reliability vs. cost); LLM with reasoning vs. score-only (debugging vs. token cost)
- **Failure signatures**: Annotator using only scale extremes (e.g., 391 zeros, 234 hundreds) → likely misunderstanding task; correlation drops with new annotators → insufficient instructions; LLM consistently scores quoted news as subjective → prompt lacks guidance on attributed speech
- **First 3 experiments**:
  1. **Calibration intervention**: Add 5 fixed-anchor texts at annotation start to stabilize annotator internal scales; measure correlation improvement
  2. **Prompt ablation**: Compare GPT-5 scores with and without explicit "discount quoted speech" instruction; quantify reduction in quote-related divergence
  3. **Annotator pooling**: Randomly assign texts to annotator subsets (2 vs. 3 vs. 4 annotators per text) and compare score stability to determine minimum viable annotator count

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the sequential context of previously annotated texts systematically influence a human annotator's subjectivity rating?
- **Basis in paper**: [explicit] The authors state in the Conclusion and Discussion that "context effects" (the sequence in which texts are presented) likely influence scores and require further investigation to be understood more systematically.
- **Why unresolved**: The paper identified this effect qualitatively through annotator interviews but did not experimentally control for text ordering to quantify the bias.
- **What evidence would resolve it**: A controlled study where annotators are given the same texts in different, randomized sequences to measure the variance in scores based on immediate textual predecessors.

### Open Question 2
- **Question**: Can prompt engineering or example-based prompting align LLM subjectivity scores with human intuition regarding quoted text and colloquial language?
- **Basis in paper**: [explicit] The Conclusion notes that "Further experimentation with prompt design... and example-based prompting may also help to better understand and reduce the observed human–LLM discrepancies."
- **Why unresolved**: The current study used a general prompt, and while it identified systematic differences (e.g., LLMs rate quoted news higher and colloquial text lower), it did not test interventions to correct these specific biases.
- **What evidence would resolve it**: Comparative benchmarking of various prompting strategies (e.g., few-shot learning with colloquial examples) to see if the correlations on specific text categories improve.

### Open Question 3
- **Question**: What specific linguistic features render a text inherently "difficult" to annotate, resulting in high inter-annotator disagreement?
- **Basis in paper**: [inferred] The Discussion notes that "texts themselves" are a possible cause for reduced correlation and that "texts that were judged qualitatively considerably differently... require further qualitative analysis."
- **Why unresolved**: While the authors identified that low agreement occurs, they did not perform a deep linguistic analysis of the specific features (e.g., sarcasm, cultural references) driving the divergence.
- **What evidence would resolve it**: A qualitative linguistic analysis of the "high disagreement" subset to identify common rhetorical or structural patterns.

## Limitations
- Moderate inter-annotator agreement (0.525-0.675) indicates inherent subjectivity task difficulty rather than annotation failure
- Systematic divergence between human and LLM annotations on quoted speech and colloquial register limits LLM utility as direct replacement
- Study relied on single LLM model (GPT-5) without comparing alternative approaches or prompting strategies

## Confidence
- **High confidence**: Continuous scale successfully captures more variance than discrete alternatives; context effects are real and measurable; LLM annotations are systematically different from humans on specific text types
- **Medium confidence**: Re-annotation of high-disagreement texts meaningfully improves dataset quality; GPT-5 correlations with humans are comparable to inter-annotator correlations
- **Low confidence**: Exact threshold for "acceptable" inter-annotator agreement for subjective tasks; whether alternative prompt engineering could fully close the human-LLM gap

## Next Checks
1. Test whether explicit prompt instructions to "discount quoted speech" and "consider informal register as subjective signal" reduce systematic LLM-human divergence on news and colloquial texts
2. Conduct a controlled experiment comparing annotation quality between randomized vs. stratified (by predicted subjectivity) text presentation orders to quantify order effect magnitude
3. Evaluate whether calibration texts at annotation start stabilize annotator internal scales and improve inter-annotator agreement by 10-15% as predicted