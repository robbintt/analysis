---
ver: rpa2
title: 'SciDA: Scientific Dynamic Assessor of LLMs'
arxiv_id: '2506.12909'
source_url: https://arxiv.org/abs/2506.12909
tags:
- problems
- data
- llms
- chemistry
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SciDA is a scientific benchmark for assessing large language models'
  reasoning capabilities. It contains over 1,000 Olympiad-level numerical computation
  problems across mathematics, physics, chemistry, and biology.
---

# SciDA: Scientific Dynamic Assessor of LLMs

## Quick Facts
- arXiv ID: 2506.12909
- Source URL: https://arxiv.org/abs/2506.12909
- Reference count: 34
- Primary result: Benchmark reveals 10-20% absolute accuracy drop under random initialization, exposing data contamination in LLMs

## Executive Summary
SciDA is a scientific benchmark designed to assess large language models' reasoning capabilities through Olympiad-level numerical computation problems across STEM disciplines. The key innovation is dynamic random initialization of problem parameters, which exposes data contamination by preventing models from relying on memorized numerical patterns. Experiments with 14 top-performing models show significant performance degradation under randomization, with accuracy drops of 10-20% absolute (20-60% relative). The benchmark reveals widespread contamination in LLMs and demonstrates that models with code interpreters show better numerical stability, suggesting computational precision as a bottleneck for scientific reasoning.

## Method Summary
SciDA evaluates LLMs on over 1,000 Olympiad-level numerical computation problems using dynamic random parameter initialization to detect data contamination. Problems are parameterized with variable placeholders, and each evaluation round samples new values from scientifically valid ranges. The ground truth is computed via Python functions, and model outputs are compared within tolerance thresholds. Five random initializations per problem balance robustness against computational cost. The methodology distinguishes between "initial" (fixed parameters) and "random" (dynamically initialized) conditions, with significant accuracy drops under randomization indicating memorization rather than genuine reasoning.

## Key Results
- Accuracy drops by 10-20% absolute (20-60% relative) under random initialization compared to fixed parameters
- Models with code interpreters show 42.53% improvement on hard problems due to better numerical precision
- Calculation errors dominate in math/physics/chemistry (2/3+ of errors), while logical errors increase in biology where training corpus is scarcer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic parameter randomization exposes memorization versus genuine reasoning by breaking fixed numerical patterns that models may have encountered during training.
- Mechanism: Each problem contains variable parameters sampled from scientifically valid ranges. When parameters are re-randomized per evaluation round, models can no longer rely on memorized number sequences or answer keys. Performance drops reveal the gap between pattern recognition and actual problem-solving.
- Core assumption: Models trained on public benchmarks memorize specific numerical configurations rather than abstract solution strategies.
- Evidence anchors:
  - [abstract]: "due to the data source overlap of LLMs training and static benchmark, the keys or number pattern of answers inadvertently memorized"
  - [section 1]: "accuracy decreasing by 10-20% (20-60% relatively) compared to fixed parameters"
  - [corpus]: MathArena paper confirms evaluation on uncontaminated competitions shows similar concerns about "potential memorization"

### Mechanism 2
- Claim: Code interpreters provide external computational precision that compensates for LLMs' inherent numerical instability in multi-step calculations.
- Mechanism: When LLMs execute arithmetic internally, precision errors accumulate across calculation chains. Code interpreters offload computation to deterministic Python execution, maintaining numerical accuracy through extended reasoning traces.
- Core assumption: The bottleneck for scientific problems is computational execution rather than conceptual understanding for well-trained models.
- Evidence anchors:
  - [section 5.2]: "Gemini-2.5-pro.preview.0506.google.ci achieved an average score of 40.19, markedly higher than the 30.20 recorded by Gemini-2.5-pro.preview.0506 without CI"
  - [section 5.2]: "under random initialization, the CI model achieved 32.63 on hard problems vs. 22.89 without CI—an improvement of roughly 42.53%"
  - [corpus]: SymPyBench uses "executable Python code that produces" answers, supporting computational offloading approach

### Mechanism 3
- Claim: Error type distribution (logical vs. calculation) reflects disciplinary generalization capacity, which correlates with training data richness.
- Mechanism: Domains with abundant training corpora (mathematics, physics) produce models that generalize solution methods but struggle with execution precision. Domains with scarcer corpora (biology) yield higher logical error rates, indicating incomplete method acquisition.
- Core assumption: Training data volume per discipline directly influences pattern generalization capability.
- Evidence anchors:
  - [section 5.1]: "In all disciplines other than Biology, calculation errors were dominant, constituting at least two-thirds of all errors"
  - [section 5.1]: "for subjects such as biology where corpus is relatively scarce, weaker generalization capabilities likely cause a higher incidence of logical errors"

## Foundational Learning

- Concept: Data contamination in benchmarks
  - Why needed here: Understanding that benchmark overlap with training data inflates capability estimates is essential for interpreting SciDA's motivation and results.
  - Quick check question: Can you explain why a model scoring 90% on a benchmark might not actually possess the underlying capability?

- Concept: Parameterized problem generation
  - Why needed here: SciDA's core innovation requires understanding how variable extraction and functional answer computation enable dynamic evaluation.
  - Quick check question: How would you convert a static physics problem into a parameterized template with a computable ground truth?

- Concept: Chain-of-thought reasoning vs. pattern matching
  - Why needed here: The paper distinguishes "thinking" models from "instruct" models based on their capacity for extended reasoning.
  - Quick check question: What behavioral difference would you expect between a model relying on pattern matching versus genuine multi-step reasoning?

## Architecture Onboarding

- Component map: Problem templates -> Variable annotation layer -> Answer computation functions -> Evaluation harness -> Model interface

- Critical path:
  1. Select problem from dataset
  2. Extract variable definitions and ranges
  3. Sample random values from U(a, b) for each variable
  4. Present instantiated problem to model
  5. Execute ground-truth function to compute correct answer
  6. Compare model output within tolerance threshold

- Design tradeoffs:
  - 5 random initializations per problem balances evaluation robustness against compute cost (elbow point analysis in Appendix C)
  - Numeric-only answers sacrifice assessment of symbolic manipulation for verifiability
  - Olympiad difficulty limits coverage of foundational competencies

- Failure signatures:
  - Model output not parseable as numeric sequence → extraction failure
  - Python ground-truth function throws exception for sampled values → range constraint violation
  - Variance of means across random seeds remains high after n=5 → insufficient sampling

- First 3 experiments:
  1. Run GPT-4o on a 50-problem subset under both initial and random conditions to reproduce the reported accuracy gap.
  2. Compare identical model with/without code interpreter access on hard physics problems to quantify computational precision contribution.
  3. Perform manual error categorization on 20 random failures to validate the logical-vs-calculation error distribution pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SciDA be effectively utilized as a reward mechanism in Reinforcement Learning to specifically enhance "slow thinking" and long-chain reasoning?
- Basis in paper: [explicit] Section 4 states, "We are convinced that applying this benchmark to reinforcement learning holds considerable potential in promoting LLMs to engage in slow thinking."
- Why unresolved: The paper proposes the potential application but does not conduct or report on experiments involving RL training loops using SciDA as the feedback signal.
- What evidence would resolve it: Results from training a model using SciDA accuracy as a reward signal, showing measurable improvements in multi-step reasoning tasks.

### Open Question 2
- Question: How can the dynamic randomization framework be adapted for non-STEM disciplines where answers are not strictly numerical functions?
- Basis in paper: [explicit] Section 7 (Future Work) states, "Our goal is to extend beyond the common STEM subjects... to include a wider variety of disciplines."
- Why unresolved: The current methodology relies on functional answers ($y = F(x)$) for automated verification, a constraint difficult to apply to qualitative subjects like literature or history.
- What evidence would resolve it: A functional expansion of SciDA to humanities or social sciences that maintains contamination-proof dynamic evaluation.

### Open Question 3
- Question: Does the ratio of logical errors to calculation errors on dynamic benchmarks reliably indicate the sufficiency of a model's domain-specific pre-training data?
- Basis in paper: [inferred] Section 5.1 correlates error types with training data volume, suggesting "generalization capability is likely correlated with the richness of the relevant training data."
- Why unresolved: This is presented as a hypothesis based on observation rather than a verified, causal relationship.
- What evidence would resolve it: A controlled study correlating the error type distributions on SciDA with the known volume and diversity of training data for specific domains.

## Limitations

- The central claim about widespread data contamination lacks external validation beyond internal benchmark comparisons
- The disciplinary error analysis in biology is speculative given the "relatively scarce" training corpus claim lacks quantitative support
- The mechanism by which code interpreters improve performance conflates computational offloading with potential differences in model architecture between standard and ".ci" variants

## Confidence

- High confidence: The dataset construction methodology and parameter randomization mechanism are clearly specified and reproducible
- Medium confidence: The reported accuracy drops under randomization are methodologically valid, though the contamination interpretation assumes no confounding factors
- Low confidence: The causal link between training corpus volume and disciplinary error patterns, particularly the biology-specific claims, lacks empirical support

## Next Checks

1. Control for cognitive load: Test whether the accuracy drop persists when parameters are randomized but problems are presented in simplified format to isolate contamination effects from processing overhead.

2. Cross-benchmark contamination analysis: Apply SciDA's randomization methodology to established benchmarks (GSM8K, MATH) to determine if similar drops occur, validating whether the effect is specific to scientific reasoning or general to any benchmark with potential contamination.

3. Symbolic reasoning assessment: Augment the dataset with symbolic answer problems to test whether the numeric-only limitation systematically underestimates reasoning capabilities in domains requiring algebraic manipulation rather than numerical computation.