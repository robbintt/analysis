---
ver: rpa2
title: The Bidirectional Process Reward Model
arxiv_id: '2508.01682'
source_url: https://arxiv.org/abs/2508.01682
tags:
- biprm
- step
- ours
- reasoning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BiPRM, a novel bidirectional evaluation paradigm
  for process reward models that addresses the limitations of unidirectional left-to-right
  (L2R) evaluation by incorporating a parallel right-to-left (R2L) stream. The method
  achieves this through prompt reversal and a dynamic gating mechanism that fuses
  rewards from both directions, requiring only a 0.3% parameter increase and approximately
  5% inference latency overhead.
---

# The Bidirectional Process Reward Model

## Quick Facts
- arXiv ID: 2508.01682
- Source URL: https://arxiv.org/abs/2508.01682
- Reference count: 21
- Key result: Achieves 10.6% average relative improvement in Best-of-N accuracy for solution-level tasks and 37.7% for step-level error detection

## Executive Summary
This paper introduces BiPRM, a novel bidirectional evaluation paradigm for process reward models that addresses the limitations of unidirectional left-to-right (L2R) evaluation by incorporating a parallel right-to-left (R2L) stream. The method achieves this through prompt reversal and a dynamic gating mechanism that fuses rewards from both directions, requiring only a 0.3% parameter increase and approximately 5% inference latency overhead. Extensive experiments on two solution-level benchmarks (GSM-Plus and MATH500) and one step-level benchmark (ProcessBench) demonstrate that BiPRM consistently outperforms unidirectional baselines across three different backbones and three PRM objectives.

## Method Summary
BiPRM implements a bidirectional process reward model that evaluates reasoning trajectories using both left-to-right and right-to-left streams. The R2L stream is implemented purely through prompt reversal, reversing the order of reasoning steps without architectural modifications to the underlying LLM. A lightweight MLP-based gating network (0.3% parameter overhead) dynamically fuses rewards from both streams using position-adaptive weights. The method trains on the Math-Shepherd dataset and evaluates on solution-level benchmarks (GSM-Plus, MATH500) using Best-of-N accuracy and step-level error detection on ProcessBench using F1 score. The final trajectory reward is computed as the minimum score across all steps.

## Key Results
- BiPRM achieves 10.6% average relative improvement across 54 solution-level configurations on GSM-Plus and MATH500
- BiPRM demonstrates 37.7% improvement across 12 step-level error detection scenarios on ProcessBench
- The method requires only 0.3% additional parameters and incurs approximately 5% inference latency overhead

## Why This Works (Mechanism)

### Mechanism 1: Complementary Bidirectional Context Fusion
Combining L2R and R2L streams provides complementary error detection patterns, with L2R better at detecting late-stage errors and R2L better at detecting early-stage errors. A lightweight MLP-based gating network computes dynamic weights to fuse the two reward signals per step.

### Mechanism 2: Prompt-Based Trajectory Reversal for R2L Implementation
The R2L evaluation stream is implemented purely through prompt modifications that reverse the original reasoning trajectory, requiring no architectural changes to the underlying LLM.

### Mechanism 3: Parallel Execution for Latency Efficiency
Running L2R and R2L streams in parallel by batch-stacking inputs reduces wall-clock latency overhead to ~5% despite doubling theoretical FLOPs.

## Foundational Learning

- **Concept: Process Reward Models (PRMs)** - Why needed: BiPRM builds directly on standard PRMs; understanding that PRMs assign fine-grained scores to intermediate reasoning steps is essential. Quick check: Can you explain why a PRM scores intermediate steps rather than only the final answer, and what information a standard L2R PRM cannot access when scoring step s₃ of a 5-step solution?

- **Concept: Bidirectional Sequence Processing (BiLSTM analogy)** - Why needed: The paper explicitly draws inspiration from BiLSTM networks; understanding how bidirectional processing captures both past and future context clarifies why R2L evaluation helps verify earlier steps. Quick check: In a BiLSTM processing a sequence [A, B, C, D], what information does the backward pass contribute when computing the hidden state for token B?

- **Concept: Gating Mechanisms in Neural Networks** - Why needed: BiPRM's dynamic fusion uses a learned gating weight σ to balance L2R and R2L signals per step. Quick check: What would happen if the gating network always output σ = 0.5 versus a learned dynamic σ that varies by step position?

## Architecture Onboarding

- **Component map:** Question q + reasoning trajectory τ = (q, {s₁, ..., sₜ}) → L2R stream → R2L stream → Gating MLP → Fusion layer → Trajectory aggregator
- **Critical path:** Trajectory reversal → parallel batched inference → hidden state extraction → gating weight computation → reward fusion → trajectory aggregation → Best-of-N selection
- **Design tradeoffs:** min vs. mean aggregation (weakest link vs. robustness to noise), static averaging vs. dynamic gating (simplicity vs. adaptive fusion), parallel vs. sequential execution (latency vs. memory)
- **Failure signatures:** R2L weakness on isolated arithmetic errors, homogeneous error propagation, directional bias in backbone
- **First 3 experiments:** 1) Validate bidirectional complementarity by training separate L2R-only and R2L-only PRMs and plotting step-wise MAE distribution, 2) Compare fusion strategies by testing static averaging vs. learned gating on validation set, 3) Ablate parallel efficiency by measuring wall-clock latency with batched parallel execution vs. sequential execution

## Open Questions the Paper Calls Out

- **Open Question 1:** Can BiPRM generalize effectively to non-mathematical reasoning domains such as code generation or symbolic logic? The authors explicitly state that experimental validation is currently confined to mathematical reasoning and leave code generation and symbolic logic for future research.

- **Open Question 2:** Is the "min" aggregation operator truly optimal given that "mean" yielded higher average scores in ablation studies? The conflict between the theoretical "weakest link" principle and empirical data showing "mean" performing better is not fully resolved.

- **Open Question 3:** How can the R2L stream be enhanced to detect local arithmetic errors that maintain internal logical consistency with subsequent steps? The paper identifies a failure case where R2L assigns high scores to algebraic errors due to smooth backward derivation.

## Limitations
- Requires both forward and reversed reasoning trajectories as input, doubling storage requirements
- Effectiveness tied to tasks with meaningful sequential dependencies between reasoning steps
- No analysis of robustness to noisy or malformed input trajectories

## Confidence
- **High confidence**: Performance improvements (10.6% and 37.7%) are well-supported by extensive ablation studies across multiple benchmarks and backbones
- **Medium confidence**: Latency overhead claim depends on parallel execution efficiency, which may vary with hardware constraints
- **Medium confidence**: R2L implementation relies on prompt reversal without architectural changes, but limited detail on prompt formatting raises questions about generalizability

## Next Checks
1. **Directionality bias analysis**: Measure perplexity differences between L2R and R2L streams across multiple backbone models to quantify directional bias
2. **Memory-constrained execution**: Test BiPRM's latency overhead when parallel execution is disabled due to memory constraints
3. **Task dependency validation**: Design experiments with reasoning tasks that have varying degrees of step interdependence to identify thresholds where bidirectional evaluation stops providing benefits