---
ver: rpa2
title: 'GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents'
arxiv_id: '2601.09770'
source_url: https://arxiv.org/abs/2601.09770
tags:
- arxiv
- visual
- tool
- preprint
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of visual grounding in GUI agents
  by introducing a reinforcement learning framework that enables active visual perception.
  Unlike static approaches, the model learns to decide when and how to invoke visual
  tools (e.g., cropping or zooming) through a two-stage reasoning process.
---

# GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents

## Quick Facts
- **arXiv ID:** 2601.09770
- **Source URL:** https://arxiv.org/abs/2601.09770
- **Reference count:** 5
- **Key outcome:** Achieves 44.8% grounding accuracy on ScreenSpot-Pro using only 3k labeled samples via tool-augmented RL.

## Executive Summary
GUI-Eyes introduces a reinforcement learning framework for visual grounding in GUI agents, enabling active perception through dynamic tool invocation. Unlike static methods, the model learns when to crop or zoom to improve grounding accuracy. A progressive perception strategy coordinates coarse and fine-grained reasoning, guided by a two-level policy. Evaluated on ScreenSpot-Pro, GUI-Eyes-3B significantly outperforms baselines with strong sample efficiency.

## Method Summary
GUI-Eyes employs a reinforcement learning framework where the agent learns to decide when and how to invoke visual tools (cropping, zooming) for visual grounding. The model uses a two-stage reasoning process: coarse exploration followed by fine-grained grounding, coordinated by a two-level policy. A spatially continuous reward function combines location proximity and region overlap to alleviate reward sparsity. The approach is trained on a small labeled dataset (3k samples) and achieves high accuracy on the ScreenSpot-Pro benchmark.

## Key Results
- GUI-Eyes-3B achieves 44.8% grounding accuracy on ScreenSpot-Pro using only 3k labeled samples.
- Outperforms both supervised and RL-based baselines in sample efficiency and accuracy.
- Demonstrates strong generalization with minimal training data.

## Why This Works (Mechanism)
GUI-Eyes works by enabling active perception through tool-augmented reinforcement learning. The agent dynamically decides when to invoke visual tools (cropping, zooming) based on the task context. This adaptive perception allows the model to focus on relevant regions, improving grounding accuracy. The spatially continuous reward function provides dense feedback, mitigating reward sparsity and enabling efficient learning from limited data.

## Foundational Learning
- **Visual grounding:** The task of identifying and localizing UI elements based on natural language queries. *Why needed:* Core problem in GUI agents. *Quick check:* Can the model localize UI elements accurately?
- **Reinforcement learning:** A framework where agents learn by interacting with an environment and receiving rewards. *Why needed:* Enables active perception and tool invocation. *Quick check:* Does the agent improve with more interactions?
- **Tool-augmented perception:** Using external tools (e.g., cropping, zooming) to enhance visual understanding. *Why needed:* Overcomes limitations of static visual models. *Quick check:* Does tool invocation improve grounding accuracy?

## Architecture Onboarding

**Component map:**
Vision Encoder -> Two-level Policy (Coarse/Fine) -> Tool Invoker -> Reward Function

**Critical path:**
Vision Encoder extracts features → Two-level Policy decides tool use → Tool Invoker executes action → Reward Function evaluates grounding → Policy updates

**Design tradeoffs:**
- Tool-augmented perception improves accuracy but increases computational overhead.
- Spatially continuous reward alleviates sparsity but introduces hyperparameters.
- Small labeled dataset reduces cost but may limit generalization.

**Failure signatures:**
- Incorrect tool invocation leading to poor grounding.
- Overfitting to the training GUI layout.
- Sensitivity to hyperparameters in the reward function.

**3 first experiments to run:**
1. Test grounding accuracy on a held-out GUI layout.
2. Evaluate tool invocation frequency during inference.
3. Measure inference-time latency compared to static baselines.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to ScreenSpot-Pro benchmark; generalization to diverse GUI layouts untested.
- Spatially continuous reward introduces sensitive hyperparameters not explored.
- RL training assumes simulator access, which may not be available in practice.
- Progressive perception strategy's computational overhead not quantified.

## Confidence
- **High confidence:** Methodological novelty of tool-augmented perception and two-stage reasoning framework.
- **Medium confidence:** Reported performance gains given controlled benchmark setting.
- **Low confidence:** Real-world deployment readiness due to lack of cross-platform testing.

## Next Checks
1. Evaluate GUI-Eyes on a multi-platform GUI dataset to assess cross-domain generalization.
2. Perform an ablation study varying the number of labeled samples to quantify sample efficiency.
3. Measure and report inference-time latency and computational overhead introduced by the progressive perception strategy.