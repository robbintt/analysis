---
ver: rpa2
title: 'Seed-Coder: Let the Code Model Curate Data for Itself'
arxiv_id: '2506.03524'
source_url: https://arxiv.org/abs/2506.03524
tags:
- code
- data
- quality
- performance
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Seed-Coder, a family of open-source code
  LLMs that minimize human effort in data curation by using LLMs to score and filter
  training data. The approach leverages model-centric quality filtering and a multi-stage
  training pipeline, including base pretraining, supervised fine-tuning, and reasoning
  model training with LongCoT reinforcement learning.
---

# Seed-Coder: Let the Code Model Curate Data for Itself

## Quick Facts
- arXiv ID: 2506.03524
- Source URL: https://arxiv.org/abs/2506.03524
- Reference count: 40
- Primary result: Seed-Coder-8B-Instruct achieves 53.3% on BigCodeBench, surpassing larger models with reduced human curation

## Executive Summary
Seed-Coder introduces a novel approach to training code large language models that minimizes human effort in data curation by using LLMs to score and filter training data. The system employs a multi-stage training pipeline including base pretraining, supervised fine-tuning, and reasoning model training with LongCoT reinforcement learning. The resulting models, particularly Seed-Coder-8B-Instruct, achieve state-of-the-art performance among ~8B parameter models while demonstrating that LLM-driven data curation can yield high-quality code models with significantly reduced human involvement.

## Method Summary
Seed-Coder leverages model-centric quality filtering where LLMs score and filter training data, reducing the need for human curation. The approach uses a three-stage training pipeline: base pretraining on filtered data, supervised fine-tuning on curated datasets, and reasoning model training using LongCoT reinforcement learning. The data curation process involves automatic scoring of code samples by the model itself, filtering out low-quality examples, and iteratively refining the training corpus. This self-curation mechanism allows the model to improve its own training data quality without extensive human annotation.

## Key Results
- Seed-Coder-8B-Instruct achieves 53.3% on BigCodeBench, outperforming larger models including DeepSeek-Coder-V2-16B (52.3%)
- The reasoning variant achieves 53.6% on LiveCodeBench, demonstrating strong reasoning capabilities
- On MHPP hard set, Seed-Coder-8B-Instruct reaches 26.4%, surpassing many larger competitors
- The approach demonstrates that LLM-driven data curation can produce high-quality models with reduced human effort

## Why This Works (Mechanism)
The Seed-Coder approach works by leveraging the model's own capabilities to evaluate and filter training data, creating a self-improving loop. By using the model to score code samples, it can identify and prioritize high-quality examples that align with its learning objectives. The LongCoT reinforcement learning component enables the model to develop reasoning capabilities beyond simple pattern matching. This multi-stage training pipeline allows the model to first build a strong foundation, then refine its capabilities through supervised learning, and finally develop advanced reasoning skills through reinforcement learning.

## Foundational Learning
- **Code quality scoring**: Models can evaluate code based on multiple dimensions including correctness, efficiency, and readability - needed for effective self-curation, quick check: compare model scores with human expert ratings
- **Reinforcement learning for code**: RL techniques can optimize code generation beyond maximum likelihood training - needed for developing reasoning capabilities, quick check: measure improvement in complex problem-solving tasks
- **Multi-stage training**: Sequential training stages allow progressive skill development - needed to build from basics to advanced reasoning, quick check: evaluate performance at each training stage
- **Automatic data curation**: LLMs can identify high-quality training examples without human intervention - needed to reduce curation costs, quick check: measure data quality improvements through automatic filtering
- **Code-specific pretraining**: Specialized pretraining on code corpora improves downstream performance - needed for domain-specific capabilities, quick check: compare with general-purpose pretraining
- **Supervised fine-tuning**: Targeted fine-tuning on curated datasets refines model behavior - needed for aligning with specific objectives, quick check: measure alignment with target benchmarks

## Architecture Onboarding

**Component Map**: Data Pipeline -> Base Pretraining -> Supervised Fine-tuning -> Reasoning Training (LongCoT) -> Evaluation

**Critical Path**: The most critical path is the data curation and base pretraining stage, as the quality of filtered data directly impacts all downstream performance. Poor data quality at this stage cannot be fully recovered in later stages.

**Design Tradeoffs**: The approach trades human curation effort for computational cost of model-driven filtering. While this reduces human labor, it requires multiple training passes and significant compute for the curation process itself. The multi-stage approach adds training complexity but enables progressive skill development.

**Failure Signatures**: 
- If the automatic scoring mechanism is biased or poorly calibrated, it may filter out valuable training examples
- Insufficient diversity in the curated dataset can lead to overfitting and poor generalization
- The LongCoT reasoning training may not effectively transfer to practical coding tasks if the reward signals are misaligned

**First 3 Experiments**:
1. Baseline comparison: Train identical models with human-curated versus LLM-curated datasets to quantify quality differences
2. Ablation study: Remove the LongCoT reasoning stage to measure its specific contribution to performance
3. Robustness testing: Evaluate model performance across diverse programming languages and problem types not represented in the training data

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Evaluation focuses primarily on specific benchmarks (BigCodeBench, MHPP, LiveCodeBench) without broader testing across diverse coding tasks and programming languages
- Reliance on LLM-based data curation raises concerns about potential bias amplification and generalization beyond specific dataset distributions
- The comparison with larger models doesn't fully address whether reduced human curation translates to practical deployment advantages when considering total training costs
- The paper lacks extensive analysis of failure modes or robustness across different code domains

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Achieving state-of-the-art performance among 8B models | High |
| LLM-driven data curation consistently produces high-quality models with reduced human involvement | Medium |
| Scalability of this approach to larger models or different domains | Low |

## Next Checks
1. Test Seed-Coder's performance across a wider range of programming languages and coding tasks beyond the current benchmark suite to assess generalizability
2. Conduct ablation studies comparing LLM-curated datasets versus human-curated datasets to quantify actual quality differences and identify specific failure modes
3. Evaluate the model's robustness to adversarial code inputs and its performance on real-world coding tasks from open-source repositories to assess practical utility