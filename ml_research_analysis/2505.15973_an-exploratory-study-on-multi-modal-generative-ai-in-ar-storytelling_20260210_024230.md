---
ver: rpa2
title: An Exploratory Study on Multi-modal Generative AI in AR Storytelling
arxiv_id: '2505.15973'
source_url: https://arxiv.org/abs/2505.15973
tags:
- storytelling
- content
- participants
- aigc
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of multi-modal generative AI for
  AR storytelling, addressing the challenge of creating high-quality, multi-modal
  content. The authors analyzed 223 AR storytelling videos to identify a design space
  consisting of four key elements (Character, Background, Sentiment, Development)
  and five modalities (Text, Audio, Image, Video, 3D).
---

# An Exploratory Study on Multi-modal Generative AI in AR Storytelling

## Quick Facts
- arXiv ID: 2505.15973
- Source URL: https://arxiv.org/abs/2505.15973
- Reference count: 40
- Multi-modal generative AI is suitable for AR storytelling, with images preferred for characters/backgrounds and video for development

## Executive Summary
This study investigates how multi-modal generative AI can enhance AR storytelling by analyzing 223 existing AR videos to identify four narrative elements (Character, Background, Sentiment, Development) and five content modalities (Text, Audio, Image, Video, 3D). The authors developed a testbed using pre-trained Gen-AI models and conducted two user studies with 30 participants to evaluate content generation and AR interface effectiveness. Results show images were preferred for static elements while video was favored for development, though users faced challenges with prompt engineering and cross-modal consistency.

## Method Summary
The study combined corpus analysis of 223 AR storytelling videos to identify design patterns with two user studies evaluating a testbed system. The testbed used FastAPI backend connecting pre-trained models (Stable Diffusion, MusicGen, Motion-Diffusion-Model, Text2Video-Zero, DreamFusion) to generate multi-modal content from text narratives. An AR interface using Web Speech API and MediaPipe hand tracking allowed users to present stories with AI-generated augmentations. Five text stories were used as stimuli, with participants generating and positioning content for different narrative elements.

## Key Results
- Images were preferred for characters (51%), backgrounds, and sentiment, while video was favored for development (40%)
- Participants rated overall content usefulness at 4.2/5 and immersion at 4.0/5
- Users struggled with guiding AI generation through text prompts and maintaining consistency across modalities

## Why This Works (Mechanism)

### Mechanism 1: Modality-Element Affinity
Users associate static visual fidelity (Images) with identity elements (Characters/Backgrounds) and dynamic temporal changes (Video) with plot progression (Development), aligning cognitive load with media strengths. Evidence shows 51% preference for images in Characters and 40% for video in Development. Break condition: significant video quality improvements may shift preferences toward video for all elements.

### Mechanism 2: Democratization via Atomic Content Generation
Decomposing storytelling into atomic elements allows Gen-AI to lower barriers by separating semantic intent from rendering expertise. Users focus on story flow rather than asset creation using distinct models triggered by text. Break condition: if prompt translation fails to capture user intent, democratization benefit is negated by correction effort required.

### Mechanism 3: Speech-Triggered Synchronization
Real-time speech-to-text triggers pre-generated augmentations, creating immersive "live" storytelling versus manual timeline control. When narrator speaks keywords, AR interface overlays content, creating reactive experience. Break condition: high latency or misrecognition breaks immersion by causing unaligned modalities.

## Foundational Learning

**Multi-modal Alignment (Cross-Modal Consistency)** - Critical failure mode where character images and 3D models generated from same prompt look completely different. Quick check: How would you enforce consistency between text-to-image and text-to-3D outputs using this architecture?

**Selective Augmentation** - Users preferred augmenting fewer elements than available, realizing not all elements need augmentation despite maximum feature design. Quick check: If user inputs full story, what heuristic filters which sentences should be augmented versus left as spoken word?

**Context-Aware Prompting** - System failed to interpret metaphors, generating literal interpretations instead of intended meaning. Quick check: How does system handle "It was a dark time" versus "The room was dark"? Currently treats them similarly, causing errors.

## Architecture Onboarding

**Component map:** Text Story (File upload) -> FastAPI Server -> Gen-AI Models (Stable Diffusion, MusicGen, MDM, Text2Video-Zero, DreamFusion) -> AR Interface (Webcam + MediaPipe)

**Critical path:** 1. User loads text story 2. User highlights text -> Selects Modality -> System generates asset -> User tags asset with keyword 3. User speaks keyword -> System detects speech -> System overlays asset in AR view -> User positions asset with hands

**Design tradeoffs:** Desktop/webcam setup chosen for accessibility but limits mobility/immersion compared to HMDs. Pre-generation ensures quality control but trades flexibility of live creation.

**Failure signatures:** Literalism trap (metaphors interpreted literally), uncanny video (poor quality/intelligible outputs), identity drift (different models produce inconsistent characters).

**First 3 experiments:** 1. Run Study 1 protocol with short story, assign Image to Characters and Video to Development, observe alignment quality 2. Test prompting bottleneck by generating complex emotion with text prompts, then guide with negative prompts/style modifiers 3. Stress test speech triggers by speaking keywords at different speeds/intonation to measure latency and accuracy

## Open Questions the Paper Calls Out

How suitable is multi-modal AIGC for augmenting specific elements (Character, Background, Sentiment, Development) in AR storytelling? While preferences were identified, quality limitations and generalizability across genres remain untested. Evidence would come from comprehensive studies measuring comprehension and satisfaction across diverse story genres with next-generation models.

How can non-textual interactions improve intuitiveness of guiding Gen-AI for AR content creation? Text prompts are often ambiguous for spatial/motion attributes required in AR. Evidence would come from comparative studies of text-based versus gesture-based prompting evaluating precision and cognitive load.

How can generative systems ensure consistency and alignment of atomic story elements across multiple generated modalities? Different modalities from same prompt often produce inconsistent content. Evidence would come from unified generative architectures sharing latent features to maintain consistency across text, image, video, and 3D outputs.

## Limitations

- Significant quality differences across modalities, particularly poor video quality (2.8/5 average), may skew modality preferences
- Critical implementation details about prompt formulation and processing remain unspecified
- Cross-modal consistency issues where different modalities from same prompt produce inconsistent content

## Confidence

**High Confidence (4/5)** - Image preference for static elements and video for dynamic elements well-supported by quantitative data (51% images for characters, 40% video for development) with solid empirical grounding from corpus analysis.

**Medium Confidence (3/5)** - Claim of suitability supported by positive usability ratings (4.2/5 usefulness, 4.0/5 immersion) but tempered by challenges with prompt engineering and limited study size (30 participants).

**Low Confidence (2/5)** - Democratization benefit undermined by participants' difficulties with text prompts and learning curve, despite being presented as a key advantage.

## Next Checks

1. Implement systematic comparison measuring semantic similarity between text-to-image and text-to-3D outputs using CLIP embeddings to quantify alignment quality
2. Re-run user studies with enhanced video generation quality to determine if modality preferences shift when technical limitations are reduced
3. Create controlled test suite measuring system's ability to correctly interpret narrative metaphors versus literal interpretations to quantify "literalism trap" severity