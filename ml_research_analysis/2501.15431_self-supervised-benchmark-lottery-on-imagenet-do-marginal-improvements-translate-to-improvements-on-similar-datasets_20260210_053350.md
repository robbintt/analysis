---
ver: rpa2
title: 'Self-supervised Benchmark Lottery on ImageNet: Do Marginal Improvements Translate
  to Improvements on Similar Datasets?'
arxiv_id: '2501.15431'
source_url: https://arxiv.org/abs/2501.15431
tags:
- imagenet
- learning
- accuracy
- validation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether marginal improvements in self-supervised
  learning (SSL) models on ImageNet validation translate to better performance on
  similar datasets. The authors evaluate twelve popular SSL frameworks (including
  DINO, Swav, MoCo, Barlow Twins, BYOL, SimCLR, etc.) on five ImageNet variants: ImageNet
  validation, ImageNet ReaL, ImageNet v2, ImageNet Rendition, ImageNet Sketch, and
  ImageNet Adversarial.'
---

# Self-supervised Benchmark Lottery on ImageNet: Do Marginal Improvements Translate to Improvements on Similar Datasets?

## Quick Facts
- arXiv ID: 2501.15431
- Source URL: https://arxiv.org/abs/2501.15431
- Reference count: 40
- Key outcome: Models performing well on ImageNet validation can experience significant performance drops on out-of-distribution variants, while methods like MoCo and Barlow Twins maintain more consistent performance across variants.

## Executive Summary
This paper investigates whether marginal improvements in self-supervised learning (SSL) models on ImageNet validation translate to better performance on similar datasets. The authors evaluate twelve popular SSL frameworks on five ImageNet variants, finding that models like DINO and Swav that excel on ImageNet validation often experience substantial drops on out-of-distribution variants, while MoCo and Barlow Twins maintain more consistent performance. The paper argues that benchmarking SSL models solely on ImageNet validation is insufficient and proposes using aggregate metrics (weighted average and geometric mean) to provide a more holistic assessment of model performance. The findings highlight the need for more adequate benchmarking practices to avoid the "benchmark lottery" and better capture the true generalization capabilities of SSL models.

## Method Summary
The study evaluates twelve SSL frameworks (DINO, Swav, MoCo, Barlow Twins, VicReg, BYOL, OBoW, DeepC, SimSiam, SimCLR, PCL, SeLa) on five ImageNet variants using ResNet-50 backbones. The evaluation protocol involves training linear classifiers on frozen SSL backbones using ImageNet training data, then testing on validation, ReaL, v2, Rendition, Sketch, and Adversarial datasets. The authors compute both standard accuracy per dataset and aggregate metrics (weighted average by image count and geometric mean) to assess overall performance. No SSL retraining is performed on the variants—the evaluation tests zero-shot transfer capabilities through linear probing.

## Key Results
- Models performing well on ImageNet validation (DINO, Swav) experience significant performance drops on out-of-distribution variants (Rendition, Sketch, Adversarial), while MoCo and Barlow Twins maintain consistent performance
- Correlation between ImageNet validation accuracy and in-distribution variants remains high (r = 0.99), but drops substantially for out-of-distribution variants (r ≈ 0.6–0.67)
- MoCo achieves the highest aggregate performance using both weighted average and geometric mean metrics, while DINO and Swav see drops in aggregate accuracy and hence in rank
- No single SSL methodology (clustering, contrastive, distillation, information-maximization) systematically outperforms others across all distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
SSL frameworks exhibit differential robustness to distribution shifts, and ImageNet validation performance alone does not predict out-of-distribution generalization. When models are optimized for a specific validation set, they may learn representations that exploit dataset-specific statistical regularities rather than transferable visual features. The correlation between ImageNet validation and in-distribution variants remains high (r = 0.99), but drops substantially for out-of-distribution variants (r ≈ 0.6–0.67), indicating that different SSL objectives encode different inductive biases that manifest only under distribution shift.

### Mechanism 2
Aggregate metrics (weighted average, geometric mean) provide more stable model rankings than single-benchmark evaluation by penalizing catastrophic failures on any dataset. Weighted average incorporates sample-size weighting, reducing the influence of small datasets. Geometric mean is "pessimistic"—it disproportionately penalizes models with near-zero performance on any single dataset. Together, they reveal that MoCo's consistent mid-to-high performance across all variants is more robust than DINO/Swav's high validation accuracy but poor OOD performance.

### Mechanism 3
No single SSL methodology (clustering, contrastive, distillation, information-maximization) systematically outperforms others across all distribution shifts. Each SSL objective encodes different priors about what constitutes a "good" representation. Contrastive methods (MoCo) may learn more robust features by explicitly contrasting negative pairs, while clustering methods (Swav) and distillation methods (DINO) may overfit to texture/color cues prevalent in ImageNet validation. The paper finds no correlation between methodology category and aggregate performance.

## Foundational Learning

- **Concept: Linear Probing Protocol**
  - **Why needed here:**