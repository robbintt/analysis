---
ver: rpa2
title: 'LABOR-LLM: Language-Based Occupational Representations with Large Language
  Models'
arxiv_id: '2406.17972'
source_url: https://arxiv.org/abs/2406.17972
tags:
- testse
- trainse
- occupation
- workers
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LABOR-LLM, a language-based approach for
  predicting workers' next occupations using large language models. The method converts
  tabular career history data into natural language resume-like text, fine-tunes a
  foundation LLM on these text representations, and uses the fine-tuned model to predict
  next occupations by extracting next-word probabilities.
---

# LABOR-LLM: Language-Based Occupational Representations with Large Language Models

## Quick Facts
- arXiv ID: 2406.17972
- Source URL: https://arxiv.org/abs/2406.17972
- Reference count: 40
- Primary result: Achieves perplexity of 8.18, outperforming previous best (8.58) on PSID dataset

## Executive Summary
LABOR-LLM introduces a novel language-based approach for predicting workers' next occupations using large language models. The method transforms tabular career history data into natural language resume-like text, fine-tunes a foundation LLM on these representations, and uses the fine-tuned model to predict next occupations through next-word probability extraction. The approach demonstrates state-of-the-art performance on the Panel Study of Income Dynamics (PSID) dataset, achieving a perplexity of 8.18 compared to 8.58 for the previous best model. Fine-tuning is essential for performance, as off-the-shelf LLMs without fine-tuning perform poorly. The model also excels at predicting related outcomes like unemployment and occupation changes across different education subgroups.

## Method Summary
The LABOR-LLM method converts tabular career history data into natural language resume-like text representations, then fine-tunes a foundation large language model on these text representations. The fine-tuned model predicts next occupations by extracting next-word probabilities from the language model's output. This approach leverages the model's ability to capture semantic relationships between job descriptions and occupational transitions. The method demonstrates that incorporating non-representative pre-training data improves performance, and that smaller language models with more training data can match or exceed larger models. The approach is particularly effective when using textual job titles rather than numeric occupation codes.

## Key Results
- Achieves perplexity of 8.18 on PSID dataset, outperforming previous best (8.58)
- Fine-tuning is crucial - off-the-shelf LLMs without fine-tuning perform poorly
- Adding non-representative data improves performance, and smaller LLMs with more data can match larger models
- Model excels at predicting unemployment, occupation changes, and maintains strong performance across education subgroups

## Why This Works (Mechanism)
The approach works because large language models excel at capturing semantic relationships and contextual patterns in natural language. By converting structured career data into resume-like text, the model can leverage the LLM's pre-trained understanding of occupational language and transitions. Fine-tuning allows the model to adapt this general understanding to the specific patterns in career trajectories. The use of textual job descriptions enables the model to capture nuanced relationships between roles that numeric codes might miss, allowing it to learn more sophisticated transition patterns based on the semantic content of job titles and descriptions.

## Foundational Learning
- Natural Language Processing (NLP) - Needed to understand how LLMs process and generate text; Quick check: Can the model generate coherent text about occupations?
- Career Trajectory Analysis - Required to frame the prediction task and evaluate results; Quick check: Does the model learn common career paths?
- Perplexity Metrics - Essential for evaluating language model performance; Quick check: Is perplexity lower than baseline models?
- Fine-tuning Techniques - Critical for adapting pre-trained models to specific tasks; Quick check: Does fine-tuning improve over zero-shot performance?
- Career Trajectory Analysis - Needed to understand patterns in occupational transitions; Quick check: Can the model identify common career progressions?

## Architecture Onboarding

**Component Map:** Tabular data -> Text conversion -> Fine-tuning -> Prediction

**Critical Path:** Text conversion → Fine-tuning → Next-word probability extraction

**Design Tradeoffs:** Full fine-tuning vs. parameter-efficient methods; more data vs. larger models; textual vs. numeric representations

**Failure Signatures:** Poor performance on numeric codes indicates reliance on semantic content; inability to generalize beyond PSID suggests dataset-specific learning

**3 First Experiments:**
1. Test zero-shot prediction performance on PSID data
2. Compare fine-tuning with and without pre-training data
3. Evaluate performance using numeric occupation codes vs. textual descriptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning on non-representative pre-training data introduce bias into conditional transition probabilities relative to the target population?
- Basis in paper: The authors state on page 5 that it remains an "open question" whether non-representative pre-training data biases the final model, calling it an "interesting opportunity for future research."
- Why unresolved: The paper shows performance gains but does not formally quantify or correct for potential biases introduced by the foundation model's origin.
- What evidence would resolve it: A theoretical analysis or empirical study comparing fine-tuned distributions against ground truth conditional probabilities in a controlled setting.

### Open Question 2
- Question: Are the model's multi-year direct predictions mathematically consistent with sequential one-year predictions?
- Basis in paper: On page 23, the authors note that direct two-year predictions are not constrained to equal sequential one-year compositions and "leave further exploration of this issue for future work."
- Why unresolved: While initial tests showed high correlation (0.93), the authors did not analyze systematic discrepancies or enforce internal consistency.
- What evidence would resolve it: A detailed comparison of probability distributions for $t+2$ derived directly versus those calculated via the chain rule from $t+1$ predictions.

### Open Question 3
- Question: How can LLM-based predictive approaches be combined with structural economic models to handle structural breaks?
- Basis in paper: The conclusion highlights the limitation that models rely on historical patterns and points to "future work" on combining LLMs with structural models to address periods like rapid AI adoption.
- Why unresolved: The current work focuses on empirical prediction using historical data rather than modeling counterfactuals during regime shifts.
- What evidence would resolve it: A framework that integrates LABOR-LLM outputs into a structural model, validated on out-of-distribution data involving economic shocks.

## Limitations
- Trained and evaluated exclusively on PSID data, limiting generalizability to other career trajectory datasets
- Assumes textual job descriptions can capture occupational transition complexity, which may not hold for specialized or rapidly evolving fields
- Does not explore parameter-efficient fine-tuning methods that could reduce computational costs

## Confidence
- Fine-tuning is crucial: High confidence (0.4 perplexity improvement over off-the-shelf models)
- Smaller models with more data can match larger models: Medium confidence (limited comparison between specific model sizes)
- Textual representations outperform numeric codes: High confidence (well-supported empirical finding)

## Next Checks
1. Test LABOR-LLM on independent career trajectory datasets such as JobHop or O*NET to assess generalizability beyond PSID
2. Compare fine-tuning approaches with parameter-efficient methods like LoRA or prompt engineering to determine if full fine-tuning is necessary
3. Evaluate model performance on occupations with high skill volatility (technology, healthcare) where traditional job descriptions may not capture rapid role evolution