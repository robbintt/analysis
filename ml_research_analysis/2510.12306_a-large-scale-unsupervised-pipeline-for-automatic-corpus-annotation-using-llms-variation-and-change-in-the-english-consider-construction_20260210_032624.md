---
ver: rpa2
title: 'A large-scale, unsupervised pipeline for automatic corpus annotation using
  LLMs: variation and change in the English consider construction'
arxiv_id: '2510.12306'
source_url: https://arxiv.org/abs/2510.12306
tags:
- annotation
- corpus
- consider
- classification
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a four-phase pipeline for automating grammatical
  annotation in large corpora using LLMs. Unlike previous supervised approaches, the
  method employs prompt engineering, pre-hoc evaluation, automated batch processing,
  and post-hoc validation to achieve unsupervised classification.
---

# A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction

## Quick Facts
- arXiv ID: 2510.12306
- Source URL: https://arxiv.org/abs/2510.12306
- Authors: Cameron Morin; Matti Marttinen Larsson
- Reference count: 0
- Method achieves 98%+ accuracy in classifying grammatical constructions in large corpora

## Executive Summary
This study presents a novel unsupervised pipeline for automating grammatical annotation in large corpora using large language models (LLMs). The four-phase approach employs prompt engineering, pre-hoc evaluation, automated batch processing, and post-hoc validation to classify linguistic constructions without requiring extensive training data. Using GPT-5 via the OpenAI API, the pipeline processed 143,933 sentences from the Corpus of Historical American English (COHA) in under 60 hours, successfully distinguishing between evaluative and non-evaluative uses of the verb "consider" and classifying complement types (zero, to be, as).

The method represents a significant advancement over traditional supervised approaches, enabling large-scale annotation with minimal human intervention while maintaining high accuracy across diverse genres and historical periods. By leveraging LLM capabilities through carefully designed prompts and validation procedures, the pipeline demonstrates that complex linguistic classification tasks can be effectively automated at scale. The approach opens new possibilities for corpus linguistics research by making it feasible to analyze vast amounts of textual data for grammatical patterns and diachronic change.

## Method Summary
The pipeline consists of four integrated phases: prompt engineering to design effective LLM instructions, pre-hoc evaluation to assess prompt quality before full deployment, automated batch processing to handle large volumes of text efficiently, and post-hoc validation to verify results accuracy. The unsupervised nature eliminates the need for manually labeled training data, instead relying on the LLM's language understanding capabilities. GPT-5 was accessed through the OpenAI API to process sentences from COHA spanning 1810-2009, with particular focus on the verb "consider" and its various grammatical constructions.

## Key Results
- Processed 143,933 sentences from COHA in under 60 hours
- Achieved 98%+ accuracy in distinguishing evaluative from non-evaluative uses of "consider"
- Successfully classified complement types (zero, to be, as) with high precision
- Demonstrated effectiveness across diverse genres and historical periods

## Why This Works (Mechanism)
The pipeline leverages the advanced language understanding capabilities of LLMs to perform complex grammatical classification tasks without requiring traditional supervised learning approaches. By carefully engineering prompts that clearly specify the classification criteria, the method taps into the model's inherent linguistic knowledge. The four-phase architecture ensures quality control through systematic evaluation and validation steps, while batch processing enables efficient handling of large datasets.

## Foundational Learning
- Prompt engineering: Why needed - to effectively communicate classification criteria to the LLM; Quick check - test prompts on small sample before full deployment
- Pre-hoc evaluation: Why needed - to assess prompt quality and potential issues before processing entire corpus; Quick check - validate on 100-200 manually annotated sentences
- Post-hoc validation: Why needed - to verify classification accuracy and identify systematic errors; Quick check - random sampling and manual verification of results
- Batch processing: Why needed - to efficiently handle large volumes of text within reasonable timeframes; Quick check - monitor processing speed and resource usage
- Unsupervised classification: Why needed - to eliminate dependency on manually labeled training data; Quick check - compare results with traditional supervised approaches when possible

## Architecture Onboarding

Component map: Prompt Engineering -> Pre-hoc Evaluation -> Batch Processing -> Post-hoc Validation

Critical path: The pipeline follows a sequential flow where each phase builds upon the previous one. Prompt engineering establishes the classification framework, pre-hoc evaluation tests this framework, batch processing executes the classification at scale, and post-hoc validation verifies the results.

Design tradeoffs: The unsupervised approach sacrifices some potential precision that supervised methods might achieve in exchange for eliminating the need for manual annotation labor. The use of GPT-5 via API provides access to advanced capabilities but introduces dependencies on proprietary systems and potential biases from the model's training data.

Failure signatures: Potential systematic classification errors that propagate across the dataset, inconsistent results across different text genres or historical periods, or degradation in performance when applied to corpora outside the original evaluation set.

First experiments:
1. Apply the pipeline to contemporary corpora like NOW or GloWbE to assess temporal and dialectal robustness
2. Conduct error analysis on a stratified random sample of 1,000+ sentences to identify systematic classification patterns
3. Compare results with traditional supervised machine learning approaches using the same data to benchmark relative performance

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies exclusively on COHA data from 1810-2009, limiting generalizability to other corpora or time periods
- High accuracy rates derive from 250 manually annotated sentences, which may not capture full linguistic variation
- The unsupervised nature means potential systematic errors could propagate undetected across the 143,933 processed sentences
- Dependency on GPT-5 via API introduces reliance on proprietary systems and opaque training data biases

## Confidence
- LLM accuracy claims (98%+): Medium - based on limited manual validation sample
- Generalizability across corpora: Low - single corpus evaluation
- Unsupervised approach reliability: Medium - promising but requires broader validation

## Next Checks
1. Apply the pipeline to contemporary corpora like NOW or GloWbE to assess temporal and dialectal robustness
2. Conduct error analysis on a stratified random sample of 1,000+ sentences to identify systematic classification patterns
3. Compare results with traditional supervised machine learning approaches using the same data to benchmark relative performance