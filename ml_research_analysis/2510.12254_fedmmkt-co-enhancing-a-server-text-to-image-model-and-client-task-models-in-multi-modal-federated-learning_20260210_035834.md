---
ver: rpa2
title: FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in
  Multi-Modal Federated Learning
arxiv_id: '2510.12254'
source_url: https://arxiv.org/abs/2510.12254
tags:
- data
- clients
- arxiv
- client
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedMMKT, a federated learning framework that
  enables collaborative enhancement of a server-side text-to-image (T2I) model and
  client task-specific models using decentralized multimodal data without compromising
  privacy. The framework addresses the challenge of adapting T2I models to specialized
  domains where task-specific data is limited due to privacy concerns, while leveraging
  rich multimodal data from mobile and IoT systems.
---

# FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in Multi-Modal Federated Learning

## Quick Facts
- **arXiv ID:** 2510.12254
- **Source URL:** https://arxiv.org/abs/2510.12254
- **Reference count:** 40
- **Primary result:** Co-enhancement framework achieves up to 18.7% accuracy gains for image clients and 4.6% for text clients while improving T2I model performance by 10%

## Executive Summary
FedMMKT addresses the challenge of adapting server-side Text-to-Image (T2I) models to specialized domains using decentralized multimodal data while preserving privacy. The framework enables collaborative enhancement between a server T2I model and heterogeneous client task models (image and text classification) through federated learning. By synthesizing domain-specific cross-modal data on the server and using entropy-weighted voting to correct noisy labels, FedMMKT effectively transfers knowledge from limited client data to improve both the T2I model and client task performance simultaneously.

## Method Summary
FedMMKT operates through iterative rounds where the server generates synthetic images and captions using a pre-trained T2I model (GLIDE/Stable Diffusion) and BLIP for image-to-text. Clients perform local inference on this synthetic data, computing predictions, entropy scores, and representations that they upload to the server. The server performs LabVote with a β=0.5 threshold to filter low-confidence predictions, then fuses multimodal representations using inter-modal cross-attention and contrastive learning aggregation. The server fine-tunes its T2I decoder while clients retrain using KL divergence on the fused representations. The framework addresses the challenge of leveraging rich multimodal data from mobile and IoT systems while maintaining privacy constraints.

## Key Results
- Achieves up to 18.7% accuracy gains for image classification clients on benchmark datasets
- Delivers 4.6% accuracy improvements for text classification clients
- Improves server T2I model performance by 10% compared to self-fine-tuning approaches
- Demonstrates effectiveness on Oxford 102 Flower and UPMC Food-101 datasets with Dirichlet non-IID partitioning

## Why This Works (Mechanism)
The framework leverages cross-modal knowledge transfer by generating synthetic data that bridges the gap between the server's T2I capabilities and client task requirements. The entropy-weighted voting mechanism effectively filters noisy predictions from the initial T2I model, while the inter-modal attention fusion captures complementary information between image and text modalities. The contrastive aggregation ensures that the fused representation preserves diverse information from multiple clients while maintaining alignment with the server's knowledge.

## Foundational Learning

**Dirichlet Distribution Partitioning**
- *Why needed:* Creates realistic non-IID data distributions across clients for federated learning evaluation
- *Quick check:* Verify that client data distributions follow expected Dirichlet properties with α=0.5

**Cross-Modal Representation Fusion**
- *Why needed:* Combines heterogeneous embeddings from different model architectures into unified representations
- *Quick check:* Confirm projection layers successfully align different dimensional embeddings before fusion

**Entropy-Weighted Voting**
- *Why needed:* Filters low-confidence predictions from initial synthetic data to improve label quality
- *Quick check:* Monitor discard rate at β=0.5 threshold to ensure sufficient data retention

## Architecture Onboarding

**Component Map:** Server T2I/BLIP -> Synthetic Data Generation -> Client Inference -> LabVote -> MultiRepFusion -> Server Fine-tuning/Client Retraining

**Critical Path:** Synthetic data generation → client inference/upload → LabVote filtering → representation fusion → model updates

**Design Tradeoffs:** The framework balances between synthetic data quality (affecting LabVote effectiveness) and representation diversity (affecting fusion quality). Strict β thresholds improve label quality but reduce training data, while aggressive fusion may cause representation collapse.

**Failure Signatures:** Dimension mismatches between client embeddings, synthetic data noise overwhelming LabVote, or contrastive weights becoming too skewed leading to collapsed representations.

**First Experiments:**
1. Test Cross-Attention module with heterogeneous client embeddings (ResNet18 vs ViT) to verify projection layer handling
2. Evaluate LabVote discard rate at β=0.5 threshold with different initial T2I model qualities
3. Visualize contrastive aggregation weights α to detect potential representation collapse

## Open Questions the Paper Calls Out

**Open Question 1:** How robust is the framework when the server-side T2I model lacks sufficient domain-specific prior knowledge to generate high-quality synthetic initialization data?
- *Basis:* Method relies on T2I model generating relevant synthetic images for knowledge transfer
- *Evidence needed:* Evaluation on specialized domains (medical imaging, industrial defects) where common T2I models struggle

**Open Question 2:** Can the inter-modal cross-attention mechanism scale effectively to scenarios involving more than two modalities?
- *Basis:* Current implementation averages "other" modality representations, which may not scale to N-modalities
- *Evidence needed:* Extension to datasets with >2 modalities (video + audio + text) comparing against multi-head attention

**Open Question 3:** How does the convergence bound change if the multi-modal alignment error becomes unbounded in highly heterogeneous environments?
- *Basis:* Theorem assumes bounded alignment error, but real-world heterogeneity may violate this
- *Evidence needed:* Empirical analysis of alignment error variance under extreme non-IID conditions (α < 0.1)

## Limitations
- Underspecified optimization hyperparameters including client learning rates, batch sizes, and local epochs
- Unclear handling of heterogeneous client embedding dimensions without explicit projection specifications
- Missing specification of loss weighting factors for client local loss calculations

## Confidence
- **High confidence** in core methodology due to detailed algorithm description
- **Medium confidence** in specific performance claims due to hyperparameter uncertainties
- **Low confidence** in theoretical guarantees under extreme non-IID conditions as described in Open Question 3

## Next Checks
1. Verify dimensionality handling by testing Cross-Attention module with heterogeneous client embeddings and confirming projection layers are applied
2. Validate LabVote implementation by measuring discard rate at β=0.5 threshold with different initial T2I model qualities
3. Test contrastive aggregation weighting by visualizing α distributions and checking for representation collapse scenarios