---
ver: rpa2
title: A Reinforcement Learning Environment for Automatic Code Optimization in the
  MLIR Compiler
arxiv_id: '2409.11068'
source_url: https://arxiv.org/abs/2409.11068
tags:
- mlir
- code
- learning
- optimization
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MLIR RL, a reinforcement learning environment
  for automatic code optimization in the MLIR compiler. The authors address the challenge
  of optimizing MLIR Linalg code by proposing a multi-discrete action space formulation
  and a novel level pointers method for loop interchange.
---

# A Reinforcement Learning Environment for Automatic Code Optimization in the MLIR Compiler

## Quick Facts
- arXiv ID: 2409.11068
- Source URL: https://arxiv.org/abs/2409.11068
- Reference count: 40
- Introduces MLIR RL, a reinforcement learning environment for automatic code optimization in the MLIR compiler

## Executive Summary
This paper presents MLIR RL, a reinforcement learning environment designed to automatically optimize MLIR Linalg code through learned transformation sequences. The authors address the challenge of high-dimensional action spaces in compiler optimization by proposing a multi-discrete action space formulation and a novel level pointers method for loop interchange. The framework is evaluated on deep learning operators and models, as well as LQCD applications, demonstrating competitive performance against state-of-the-art frameworks like PyTorch and PyTorch compiler with significant speedups, particularly on LQCD applications.

## Method Summary
The MLIR RL environment uses a PPO actor-critic architecture with an LSTM backbone to learn optimization schedules for MLIR Linalg code. The action space is factored into smaller subspaces (transformation type, tile sizes, interchange) to handle the combinatorial explosion of possible transformations. A novel "level pointers" method reduces loop interchange complexity from factorial to linear. The environment trains on 3959 examples including deep learning operators from 121 models and LQCD applications, using logarithm of speedup as the terminal reward. The framework requires LLVM/MLIR 19.x with Python bindings and significant computational resources for training.

## Key Results
- Achieved 18.7× average speedup on LQCD applications using level pointers method vs 14.5× with enumerated candidates
- Competitive performance against PyTorch and PyTorch compiler on deep learning operators
- Successfully optimized 256x256 matrix multiplication with learned tiling and vectorization sequences
- Demonstrated the effectiveness of producer-consumer LSTM embedding for fusion decisions

## Why This Works (Mechanism)

### Mechanism 1
The multi-discrete action space formulation enables the policy network to learn valid optimization sequences in a high-dimensional space that would otherwise be intractable for a flat categorical distribution. Instead of enumerating all possible combinations as a single flat list, the architecture factors the action space into the Cartesian product of smaller subspaces (Transformation Type × Tile Sizes × Interchange). The policy outputs independent distributions for these subspaces, effectively reducing the output layer size and allowing the agent to explore parameters independently.

### Mechanism 2
The "level pointers" method effectively reduces the search space complexity for loop interchange from factorial O(N!) to linear O(N) per step, facilitating the learning of deep loop reordering. Inspired by pointer networks, the policy predicts the permutation of loop levels sequentially. At step i, it selects a loop to place at level i, with action masks ensuring only unassigned loops are selectable. This prevents the need for a massive output softmax layer covering all N! permutations.

### Mechanism 3
Using an LSTM to embed producer-consumer pairs allows the agent to infer fusion opportunities that static single-operation features would miss. The environment feeds the feature vectors of a consumer operation and its immediate producer into an LSTM. The final hidden state serves as a context vector, allowing the policy to condition its "Tiled Fusion" decision not just on the current op, but on the compatibility with the upstream data source.

## Foundational Learning

- **Concept: MLIR Linalg Dialect**
  - Why needed here: This is the observation space. The agent parses linalg.generic ops, indexing_maps (affine maps), and iterator_types.
  - Quick check question: Given a linalg.generic op with indexing_maps (d0, d1) -> (d0, d1) and (d0, d1) -> (d1, d0), does this represent a matrix multiplication or a transpose?

- **Concept: Actor-Critic (PPO)**
  - Why needed here: The paper uses Proximal Policy Optimization. You must understand why the "Critic" (Value Network) is needed to estimate the expected speedup (advantage) and how the clip range prevents the policy from changing too drastically during a training iteration.
  - Quick check question: Why does the paper set the discount factor γ=1.0 and use a "Final Reward" structure instead of intermediate rewards?

- **Concept: Loop Transformations (Tiling, Fusion, Interchange)**
  - Why needed here: These are the actions. You need to grasp the trade-off: Tiling improves cache locality but adds loop overhead; Fusion reduces memory bandwidth but increases register pressure.
  - Quick check question: Why does the paper mandate that a loop must be "Tiled" before it can be "Fused" in the Linalg dialect?

## Architecture Onboarding

- **Component map:**
  Frontend (Python feature extractor) -> Agent (PyTorch Actor-Critic with LSTM Backbone) -> Environment (MLIR Compiler infrastructure) -> Executor (Runtime wrapper)

- **Critical path:**
  1. Ingest: Load MLIR code → Extract Operation Type, Loop Ranges, Indexing Maps
  2. Embed: Concatenate features → LSTM (Producer+Consumer) → Dense Backbone
  3. Act: Sample Transformation → Sample Parameters (e.g., Level Pointers for Interchange)
  4. Apply: MLIR transformation pass modifies the IR
  5. Evaluate: Compile → Run → Calculate Log-Speedup Reward

- **Design tradeoffs:**
  - Final Reward vs. Intermediate: Chose Final Reward (only at episode end) to avoid massive overhead of compiling/running code after every single step
  - Level Pointers vs. Enumerated: Level Pointers reduces model size and improves speedup (18.7× vs 14.5×) but requires sequential loop over N steps to define one interchange action

- **Failure signatures:**
  - Vectorization Explosion: If agent vectorizes loop with range > 512 without tiling, MLIR may attempt full unroll, crashing memory
  - Action Mask Leakage: If mask fails to hide invalid interchange permutation, compiler will throw verification error
  - Reward Noise: High variance in execution time on shared HPC nodes leading to unstable policy updates

- **First 3 experiments:**
  1. Sanity Check: Run pre-trained agent on single Matmul op (256x256) and visualize action sequence. Verify it chooses Tiling and Vectorization.
  2. Ablation (Level Pointers): Disable "Level Pointers" head and force "Enumerated Candidates" method on deep loop nest (N=6). Compare convergence speed and final speedup.
  3. Domain Shift: Train fresh agent only on LQCD dataset and test on Deep Learning operators (or vice versa) to measure transferability of learned "Producer-Consumer" embedding.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Can the MLIR RL framework be effectively extended to other MLIR dialects (e.g., Affine or TOSA) without requiring substantial manual re-engineering of the action space?
  - The authors state the design is "dialect-independent, in principle," but demonstrating effectiveness on other dialects is left for future work.

- **Open Question 2**
  - How can the computational cost of training and the manual effort required for action-space design be significantly reduced?
  - Section VIII highlights that designing an effective action space required roughly four person-years of effort, identifying this as highly beneficial future work.

- **Open Question 3**
  - Can the RL agent close the performance gap with vendor libraries (e.g., oneDNN) on compute-intensive kernels like Matmul and Conv2D?
  - Section VII-C notes MLIR RL generates code significantly slower (2.16× to 6.71×) than PyTorch on these operators because the current action space doesn't expose architecture-specialized kernels.

## Limitations

- Requires compiling and executing MLIR code after every episode, creating significant computational overhead that may limit scalability
- Training data generation for LQCD applications relies on an unpublished DSL compiler, making complete reproduction difficult
- The multi-discrete action space design assumes transformation parameters can be learned independently, which may not hold for complex optimization patterns

## Confidence

- **High Confidence:** The core mechanism of level pointers for reducing action space complexity is well-validated with clear performance improvements (18.7× vs 14.5× speedup)
- **Medium Confidence:** The effectiveness of the producer-consumer LSTM embedding for fusion decisions is demonstrated but relies on pairwise rather than full graph analysis
- **Medium Confidence:** The competitive performance against PyTorch and TVM is shown, but comparison is limited to specific operator sets and models

## Next Checks

1. Implement an ablation study testing the producer-consumer LSTM against a simpler single-operation feature representation to quantify the value of the pairwise embedding
2. Test the pre-trained agent on a held-out set of production MLIR codebases from different domains to assess generalizability
3. Measure the computational overhead of the RL environment by comparing training times with different episode reward structures (intermediate vs final reward) on the same hardware