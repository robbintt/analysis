---
ver: rpa2
title: A Large Language Model Based Pipeline for Review of Systems Entity Recognition
  from Clinical Notes
arxiv_id: '2506.11067'
source_url: https://arxiv.org/abs/2506.11067
tags:
- attribution
- entity
- sample
- pipeline
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a pipeline for extracting Review of Systems
  (ROS) entities from clinical notes using open-source large language models (LLMs)
  and a novel attribution algorithm. The pipeline segments ROS sections, extracts
  and classifies diseases/symptoms, detects negation, and links extracted entities
  to their source text.
---

# A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes

## Quick Facts
- arXiv ID: 2506.11067
- Source URL: https://arxiv.org/abs/2506.11067
- Reference count: 0
- Primary result: 85.6% exact-match, 97.6% relaxed-match accuracy for ROS entity recognition

## Executive Summary
This paper introduces a pipeline for extracting Review of Systems (ROS) entities from clinical notes using open-source large language models (LLMs) and a novel attribution algorithm. The approach segments ROS sections, extracts and classifies diseases/symptoms, detects negation, and links extracted entities to their source text. Evaluated on 24 clinical notes with 340 annotated entities, the pipeline achieved up to 85.6% exact-match and 97.6% relaxed-match accuracy. Models like Gemma3:27b and Mistral3.1:24b showed the highest F1 scores (up to 0.952). The attribution algorithm improved performance by correcting rephrased or reordered entities, reducing error rates significantly. The approach offers a scalable, cost-effective solution for easing clinical documentation burden.

## Method Summary
The pipeline uses a three-stage approach: (1) ROS section segmentation with SecTag, (2) few-shot LLM entity recognition with negation detection, and (3) body system classification, followed by an attribution algorithm that maps rephrased/unordered LLM outputs back to original text spans. Four open-source quantized LLMs were evaluated (llama3.1:8b, gemma3:27b, mistral3.1:24b, gpt-oss:20b) on RTX 3090 hardware. The attribution algorithm uses cosine similarity between sentence embeddings to find the best-matching n-gram for each extracted entity.

## Key Results
- Up to 85.6% exact-match accuracy for entity recognition with attribution algorithm
- Up to 97.6% relaxed-match accuracy for entity recognition
- F1 scores up to 0.952 for entity recognition (Gemma3:27b, Mistral3.1:24b)
- Attribution algorithm improved exact-match accuracy from 67.4-81.8% to 75.9-85.6%
- Error rates reduced from 25.6-43.8% to 17.6-27.9% with attribution

## Why This Works (Mechanism)

### Mechanism 1
The attribution algorithm improves exact-match accuracy by mapping rephrased LLM outputs back to original text spans using embedding similarity. For each LLM-produced entity, it enumerates all n-grams in the document, computes embeddings, calculates cosine similarity, and selects the highest-scoring span. This corrects cases where the LLM rephrases entities like "concerns about her skin" → "skin concerns." Core assumption: the correct source span is semantically similar enough to rank highest. Evidence shows exact match accuracy improved from 67.4-81.8% to 75.9-85.6%. Break condition: if LLM hallucinates entities with no corresponding span, the algorithm maps to the most similar incorrect text.

### Mechanism 2
Separating entity recognition from body system classification reduces prompt complexity and improves accuracy. Two sequential few-shot LLM calls—first extracting entities with negation status, then classifying body systems—avoids confusion from overly lengthy prompts. Core assumption: each subtask is sufficiently tractable for few-shot learning that decomposition yields net gains despite additional inference calls. Evidence shows body system classification achieved F1=0.788-0.856 (exact match) with attribution. Break condition: if the first stage misses entities or produces incorrect spans, the second stage propagates errors.

### Mechanism 3
Open-source, quantized LLMs can achieve clinically usable performance on consumer-grade hardware. Q4_K_M quantized models (5-17GB VRAM) with temperature=0, seed=42, top-k=10, top-p=0.5 produce deterministic outputs without fine-tuning. Core assumption: pre-trained medical knowledge embedded in these models generalizes sufficiently to ROS extraction via few-shot prompting. Evidence shows Llama achieved 75.9% exact-match accuracy with attribution using one-third the VRAM of larger models. Break condition: if clinical notes deviate significantly from MTSamples format or vocabulary, performance may degrade substantially.

## Foundational Learning

- **Named Entity Recognition (NER) evaluation metrics**
  - Why needed: The paper reports exact-match vs. relaxed-match precision, recall, F1, accuracy, and error rates; understanding these is essential to interpret results.
  - Quick check: Given predicted span "skin concerns" and gold span "concerns about her skin," would this count as a relaxed match?

- **Few-shot in-context learning**
  - Why needed: The pipeline relies on prompting with examples rather than fine-tuning; this determines how to design and debug prompts.
  - Quick check: If you double the number of few-shot examples, what tradeoffs might you encounter given the paper's findings on prompt length?

- **Embedding-based semantic similarity**
  - Why needed: The attribution algorithm depends on cosine similarity between sentence embeddings; understanding this helps diagnose matching failures.
  - Quick check: Would "denies chest pain" and "chest pain present" have high or low cosine similarity, and what problem might this create?

## Architecture Onboarding

- Component map: Clinical Note → [SecTag Segmentation] → ROS Section → [Few-shot LLM: Entity Recognition + Negation] → [Few-shot LLM: Body System Classification] → [Attribution Algorithm: n-gram embedding + similarity] → JSON Output (entity, status, body_system, original_span)

- Critical path: Segmentation errors cascade—a partially captured ROS section directly limits downstream performance. The paper notes one note where only part of the ROS was captured, causing 15 under-detections.

- Design tradeoffs:
  - Smaller models (Llama 8B, 5GB VRAM): faster inference, lower hardware requirements, slightly lower accuracy (75.9% vs 85.6% exact match)
  - Larger models (Gemma 27B, 17GB VRAM): higher accuracy but 3x VRAM and slower
  - Single-stage vs. two-stage classification: fewer API calls vs. reduced confusion

- Failure signatures:
  - Hallucinations: LLM outputs entities not in source text (e.g., detected "fever," "headache" when text only mentioned "nausea, vomiting")
  - Rephrasing: "concerns about her skin" → "skin concerns" (exact match fails, attribution recovers)
  - Reordering: Multiple entities merged or reordered (e.g., "darkening of the skin or eyes" → only "darkening of the eyes")

- First 3 experiments:
  1. **Baseline replication**: Run the pipeline on the provided 24 annotated MTSamples notes; verify F1 scores match Table 1 before any modifications.
  2. **Ablation study**: Disable the attribution algorithm and measure the delta in exact-match accuracy and error rate to quantify its contribution.
  3. **Generalization test**: Apply the pipeline to notes from a different source (e.g., your institution's format) with 10-20 manual annotations to assess domain shift; expect SecTag and prompt tuning may be required.

## Open Questions the Paper Calls Out

- Can the pipeline be extended to extract entities from the History of Present Illness (HPI) section with comparable accuracy to ROS extraction?
- How can the LLM's rephrasing tendency be systematically reduced to convert more relaxed matches into exact matches?
- How well does the pipeline generalize across different clinical specialties, institutions, and note formats beyond general medicine?
- What are the actual time savings and workflow impact when the pipeline is deployed in real clinical settings?

## Limitations

- Sample size and generalizability: Only 24 clinical notes from MTSamples were evaluated, which may not reflect real-world diversity.
- Model generalizability: No validation on clinical notes from different institutions or formats beyond general medicine.
- Few-shot prompt specifics: The exact few-shot examples used in the prompts are not fully specified in the paper text.
- Attribution algorithm parameters: Implementation details like maximum n-gram length and similarity threshold cutoffs are not specified.

## Confidence

- **High Confidence**: The core mechanism of using few-shot prompting with open-source LLMs for ROS entity extraction, and the general effectiveness of the attribution algorithm in improving exact-match accuracy from 67.4-81.8% to 75.9-85.6%.
- **Medium Confidence**: The relative performance rankings between different models (Gemma3:27b and Mistral3.1:24b showing highest F1 scores up to 0.952) and the claim that smaller models like Llama can achieve clinically usable performance with one-third the VRAM.
- **Low Confidence**: The generalizability of results to clinical notes from different sources or institutions, and the attribution algorithm's robustness to significant rephrasing or hallucinated entities.

## Next Checks

1. **Domain adaptation test**: Apply the pipeline to 10-20 clinical notes from your institution with manual annotations to assess performance degradation and identify necessary prompt/SecTag adjustments for your specific documentation format.

2. **Attribution algorithm parameter sweep**: Systematically vary the maximum n-gram length and similarity threshold in the attribution algorithm to determine optimal settings for your use case, measuring impact on exact-match accuracy and error rates.

3. **Model efficiency vs. accuracy tradeoff**: Compare inference times and hardware requirements across the four models (llama3.1:8b, gemma3:27b, mistral3.1:24b, gpt-oss:20b) on your target hardware, verifying the claimed VRAM savings of smaller models versus performance gains of larger ones.