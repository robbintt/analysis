---
ver: rpa2
title: 'Human-Centric Foundation Models: Perception, Generation and Agentic Modeling'
arxiv_id: '2502.08556'
source_url: https://arxiv.org/abs/2502.08556
tags:
- human-centric
- human
- foundation
- tasks
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive taxonomy of Human-centric
  Foundation Models (HcFMs), categorizing them into four groups: (1) Perception models
  for multi-modal 2D/3D understanding, (2) AIGC models for human-related content generation,
  (3) Unified models integrating perception and generation, and (4) Agentic models
  for human-like intelligence and interactions. The paper reviews state-of-the-art
  techniques across these categories, highlighting advancements in self-supervised
  learning, multitask supervised learning, conditional diffusion models, and vision-language-action
  frameworks.'
---

# Human-Centric Foundation Models: Perception, Generation and Agentic Modeling

## Quick Facts
- **arXiv ID**: 2502.08556
- **Source URL**: https://arxiv.org/abs/2502.08556
- **Reference count**: 3
- **Primary result**: Comprehensive taxonomy categorizing HcFMs into Perception, AIGC, Unified, and Agentic models with systematic review of state-of-the-art techniques.

## Executive Summary
This survey paper presents a comprehensive taxonomy of Human-centric Foundation Models (HcFMs), organizing them into four distinct categories: Perception models for multi-modal 2D/3D understanding, AIGC models for human-related content generation, Unified models integrating perception and generation, and Agentic models for human-like intelligence and interactions. The paper systematically reviews state-of-the-art techniques across these categories, highlighting advancements in self-supervised learning, multitask supervised learning, conditional diffusion models, and vision-language-action frameworks. It emphasizes the potential of HcFMs to achieve systematic understanding and versatile applications in digital human modeling and humanoid embodiments, while also discussing emerging challenges and future research directions including data quality, representation learning, interactivity, and ethical considerations.

## Method Summary
The survey provides a high-level taxonomy and literature review of HcFMs without specifying training datasets, preprocessing pipelines, or evaluation benchmarks. It describes four categories of models with representative methods: SOLIDER and HAP for unsupervised perception using human priors, UniHCP and Hulk for multitask supervised learning, HumanSD and HumanAvatar for AIGC generation, MotionGPT and ChatHuman for unified perception-generation, and GR00T N1 and HumanVLA for agentic models. The survey outlines key architectural patterns and training paradigms but does not provide implementation details, hyperparameters, or unified evaluation protocols across methods.

## Key Results
- Taxonomy organizes HcFMs into four categories: Perception, AIGC, Unified, and Agentic models
- Human priors in self-supervised pretraining improve generalization for downstream human-centric tasks
- Multi-task supervised co-training enables shared representations that outperform task-specific approaches
- LLM vocabulary extension with discretized human-centric tokens enables unified perception-generation across modalities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating human body structure priors into self-supervised pretraining yields more generalizable representations for downstream human-centric perception tasks than generic image pretraining.
- **Mechanism:** Human priors (e.g., keypoints, body part segmentation, 3D skeleton structure) guide either contrastive alignment across modalities or mask sampling/reconstruction objectives. This injects domain-relevant inductive bias into the encoder without requiring labeled data.
- **Core assumption:** Human body structure provides a consistent, transferable signal across diverse human-centric tasks that general image statistics do not capture.
- **Evidence anchors:**
  - [section 3.1]: "Human-centric unsupervised foundation models were proposed to mitigate dependency on the sensitive annotations... they used the inherent priors of human body structure to learn versatile and representative human-centric features."
  - [section 3.1]: "HAP utilized the 2D keypoints to guide the mask sampling process during mask image modeling, encouraging the model to concentrate on body structure information."
  - [corpus]: Weak direct validation; neighboring papers focus on robot learning, not representation learning ablations.
- **Break condition:** If downstream tasks require semantic understanding (e.g., emotion recognition) that body geometry does not inform, priors may not transfer.

### Mechanism 2
- **Claim:** Multi-task supervised co-training on diverse human-centric datasets enables the encoder to learn shared representations that outperform task-specific specialist models.
- **Mechanism:** A shared encoder processes inputs while task-specific projectors or dynamic queries handle heterogeneous outputs. Inter-task homogeneity is exploited through hierarchical weight sharing or unified query representations.
- **Core assumption:** Tasks such as pose estimation, parsing, re-identification, and mesh recovery share underlying representational needs that a single encoder can satisfy.
- **Evidence anchors:**
  - [abstract]: "have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches."
  - [section 3.2]: "These developments demonstrate that human-centric perception foundation models can exploit inter-task homogeneity to enhance overall performance."
  - [corpus]: No direct comparative validation found in neighboring corpus.
- **Break condition:** Task conflicts (e.g., segmentation requiring local features vs. re-id requiring global identity features) can degrade performance without careful architecture design.

### Mechanism 3
- **Claim:** Extending LLM vocabulary with discretized human-centric tokens (e.g., motion sequences, pose parameters) enables unified perception-generation across text, motion, and other modalities.
- **Mechanism:** Continuous human-centric signals are quantized into discrete tokens via VQ-VAE or similar, then appended to the LLM's vocabulary. Pre-training alignment and prompt tuning teach the model to treat these as "foreign language" tokens for understanding and generation.
- **Core assumption:** Human-centric modalities can be meaningfully discretized without catastrophic information loss, and LLM architectures can generalize to these new token types.
- **Evidence anchors:**
  - [section 5.2]: "MotionGPT... first discretized continuous motions into discrete semantic tokens, which could be interpreted as 'body language' and then be extended to the LLM's vocabulary."
  - [section 5]: "The fundamental idea is to view human-centric cues other than texts as foreign languages and append them to large language models."
  - [corpus]: GR00T N1 corpus paper references vocabulary-style tokenization for robot actions but does not validate the discretization hypothesis for human motion.
- **Break condition:** Highly detailed or high-frequency signals (e.g., fine facial micro-expressions) may lose fidelity during discretization.

## Foundational Learning

- **Contrastive Learning with Human Priors**
  - **Why needed here:** Enables multi-modal alignment (RGB, depth, keypoints) without labels by leveraging body structure consistency.
  - **Quick check question:** Can you articulate how human priors differ from generic image augmentations in contrastive learning?

- **Mask Image Modeling (MIM)**
  - **Why needed here:** Self-supervised reconstruction objective that, when guided by body-aware masking, learns structural representations.
  - **Quick check question:** How would keypoint-guided masking change what the model learns compared to random masking?

- **Vision-Language-Action (VLA) Tokenization**
  - **Why needed here:** Unifies perception, language understanding, and action generation in a single autoregressive framework for embodied tasks.
  - **Quick check question:** What are the tradeoffs between tokenizing continuous actions vs. using a separate policy network?

## Architecture Onboarding

- **Component map:** Encoder (ViT/ResNet) + task-specific heads or unified queries; optional human prior modules (keypoints, SMPL) -> Latent diffusion or GAN with neural renderer; conditioning modules for text, pose, audio -> LLM/MLLM backbone + modality-specific tokenizers/projectors; fixed vs. extended vocabulary choice -> VLM encoder + language encoder + policy decoder (VL-based) or action tokenization (VLA-based)

- **Critical path:**
  1. Define target task category (perception, generation, unified, agentic)
  2. Select pretraining paradigm: unsupervised (contrastive/MIM) vs. supervised (multi-task)
  3. For unified/agentic: decide vocabulary strategy (fixed with projection vs. extended with discretization)
  4. Integrate human priors at appropriate stage (pretraining loss, masking, conditioning)

- **Design tradeoffs:**
  - **Unsupervised vs. multi-task supervised:** Unsupervised avoids annotation cost but may underperform on fine-grained tasks; supervised requires dataset curation and task conflict management
  - **Fixed vs. extended vocabulary:** Fixed preserves LLM capabilities but limits expressiveness; extended enables richer human-centric tasks but risks vocabulary explosion and training instability
  - **GAN vs. diffusion for generation:** GANs offer faster inference and better latent space control; diffusion provides higher fidelity and easier conditioning but is slower

- **Failure signatures:**
  - **Task conflict in multi-task training:** Divergent losses, unstable training, degraded performance on specific tasks
  - **Vocabulary extension collapse:** Model ignores new tokens, generates gibberish, or overfits to small token set
  - **Prior misalignment:** Human prior modules (e.g., SMPL) introduce noise if underlying assumptions (e.g., body shape distribution) don't match data

- **First 3 experiments:**
  1. **Baseline perception model:** Train SOLIDER-style contrastive model with human semantic loss on your dataset; evaluate on pose estimation and re-id against ImageNet-pretrained baseline
  2. **Multi-task conflict probe:** Implement PATH-style hierarchical sharing; ablate weight-sharing depth to identify task conflict points
  3. **Vocabulary extension sanity check:** Implement MotionGPT-style motion discretization on a small motion-text dataset; verify token usage distribution and generation quality before scaling

## Open Questions the Paper Calls Out
None

## Limitations
- Claims regarding representation quality improvements from human priors and multitask learning are supported primarily through literature review rather than direct experimental comparisons
- Many mechanism claims rely on cited works without independent verification
- The survey's strength lies in organizing the field's conceptual landscape rather than providing benchmark-level validation of specific approaches
- Rapidly evolving nature of HcFM research means some recent developments may not be fully captured

## Confidence
- **High Confidence**: The taxonomy structure itself (four-category organization) and the identification of key architectural patterns across HcFM approaches are well-supported by the literature review
- **Medium Confidence**: Claims about representation learning benefits from human priors and multitask supervised learning are reasonably grounded but lack direct comparative validation in the survey itself
- **Low Confidence**: The discretization hypothesis for extending LLM vocabularies with human-centric tokens requires more empirical validation, as the survey does not address potential information loss or tokenization failures

## Next Checks
1. **Representation Learning Validation**: Implement SOLIDER-style contrastive learning with human priors and conduct ablation studies comparing against ImageNet-pretrained models on standard human-centric benchmarks (COCO-Pose, LSP), measuring both performance and sample efficiency

2. **Multitask Learning Conflict Analysis**: Reproduce a PATH-style hierarchical multitask model and systematically ablate weight-sharing depths while monitoring per-task loss curves to identify specific conflict points and quantify the trade-off between parameter efficiency and task performance

3. **Vocabulary Extension Fidelity Test**: Implement MotionGPT-style motion discretization on a small human motion-text dataset, then evaluate token usage distributions, generation coherence, and information retention through quantitative metrics (e.g., motion reconstruction error) and qualitative human assessment