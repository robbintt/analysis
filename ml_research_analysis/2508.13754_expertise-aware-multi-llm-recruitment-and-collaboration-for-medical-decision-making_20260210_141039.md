---
ver: rpa2
title: Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making
arxiv_id: '2508.13754'
source_url: https://arxiv.org/abs/2508.13754
tags:
- medical
- llms
- collaboration
- arxiv
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the Expertise-aware Multi-LLM Recruitment
  and Collaboration (EMRC) framework to address limitations in single-LLM approaches
  for medical decision-making (MDM). EMRC operates in two stages: (1) expertise-aware
  agent recruitment, which dynamically selects LLMs based on their domain-specific
  strengths across medical departments and query difficulty levels, and (2) confidence-
  and adversarial-driven multi-agent collaboration, which integrates LLM responses
  using confidence fusion and adversarial validation to refine the final decision.'
---

# Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making

## Quick Facts
- arXiv ID: 2508.13754
- Source URL: https://arxiv.org/abs/2508.13754
- Reference count: 40
- Key outcome: EMRC achieves 74.45% accuracy on MMLU-Pro-Health dataset, surpassing single- and multi-LLM methods

## Executive Summary
This paper introduces the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC) framework to address limitations in single-LLM approaches for medical decision-making. EMRC operates in two stages: (1) expertise-aware agent recruitment, which dynamically selects LLMs based on their domain-specific strengths across medical departments and query difficulty levels, and (2) confidence- and adversarial-driven multi-agent collaboration, which integrates LLM responses using confidence fusion and adversarial validation to refine the final decision. Evaluated on three public MDM datasets, EMRC achieves 74.45% accuracy on the MMLU-Pro-Health dataset, surpassing state-of-the-art single- and multi-LLM methods. The framework demonstrates superior diagnostic performance by leveraging the complementary expertise of multiple LLMs, ensuring both accuracy and reliability in automated medical decision support.

## Method Summary
EMRC operates through two-stage process: First, it builds an LLM expertise table by pseudo-labeling a validation set with Deepseek-R1 (671B) and recording each LLM's accuracy across 9 medical departments and 3 difficulty levels. During inference, a classifier predicts query category, selects top-4 LLMs based on expertise scores (β=0.7 weighting), and runs 2-layer multi-agent collaboration with confidence fusion and adversarial validation using the highest-expertise agent as Judge. The Aggregator LLM synthesizes final answers from Layer 2 outputs.

## Key Results
- EMRC achieves 74.45% accuracy on MMLU-Pro-Health dataset
- Outperforms random-3 selection (60.64%) and task-level top-3 (70.42%)
- Optimal configuration uses 4 agents and 2 collaboration layers
- Ablation shows confidence fusion adds +1.84% and adversarial validation adds +1.59% over baseline

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific expertise matching improves LLM selection for medical queries. An LLM expertise table pre-computes accuracy across 9 medical departments and 3 difficulty levels using a validation set. During inference, a classifier predicts query category (Dept, Diff), then selects top-N LLMs with highest weighted scores: $S^{ans}_n = \beta \cdot ACC^{dept}_{n,di} + (1-\beta) \cdot ACC^{diff}_{n,li}$ where $\beta=0.7$. Core assumption: Validation set performance transfers to test queries within the same category/difficulty bin. Evidence: Expertise-aware (74.45%) outperforms random-3 (60.64%) and task-level top-3 (70.42%) on MMLUP-H.

### Mechanism 2
Self-assessed confidence combined with historical accuracy provides more reliable response weighting than confidence alone. Each agent generates a response with a self-assessed confidence score (0.0-1.0). This is fused with category-specific expertise score from the expertise table to produce an overall confidence score. Core assumption: Self-assessed confidence correlates with actual correctness. Evidence: Removing confidence fusion drops accuracy from 74.45% to 72.13% on MMLUP-H.

### Mechanism 3
Adversarial cross-validation by a high-expertise Judge agent identifies reasoning errors and improves iterative refinement. The agent with highest expertise score for the query's category becomes the Judge, which reviews all candidate responses to identify "factual inconsistency and identifying potential errors or contradictions." Errors are propagated to Layer 2 for refinement. Core assumption: The highest-expertise agent can reliably detect errors in other agents' outputs. Evidence: Removing adversarial validation drops accuracy from 74.45% to 72.86% on MMLUP-H.

## Foundational Learning

- **Mixture-of-Agents / Model Routing**: Why needed here: EMRC dynamically routes queries to specific LLMs based on expertise; understanding routing vs. ensembling is critical. Quick check: Can you explain why routing to top-3 domain-expert LLMs might outperform using a single 70B parameter model on all queries?

- **Confidence Calibration**: Why needed here: The framework relies on self-assessed confidence; poorly calibrated models will produce unreliable fusion weights. Quick check: If a model consistently reports 0.9 confidence but achieves only 60% accuracy in a domain, what happens to the confidence fusion output?

- **Iterative Refinement / Debate**: Why needed here: Layer 2 refinement depends on agents incorporating adversarial feedback; understanding multi-round interaction limits is essential. Quick check: Why does the paper find 2 collaboration layers optimal while 3 layers degrade performance? What failure mode occurs?

## Architecture Onboarding

- **Component map**: Preprocessing (Deepseek-R1 pseudo-labels) -> Expertise table construction -> Query classification (Gemma3 27B) -> Agent selection (top-4 LLMs) -> Layer 1 generation (responses + confidence) -> Adversarial validation (Judge) -> Layer 2 refinement -> Final aggregation

- **Critical path**: Expertise table construction → Query classification → Agent selection → Layer 1 generation → Adversarial validation → Layer 2 refinement → Final aggregation

- **Design tradeoffs**: Pool size vs. compute: 8B-34B models (13 LLMs) fitting single RTX 4090; Number of agents (N): Table IV shows N=4 optimal (74.45%), N=5 degrades (73.10%); Collaboration layers: 2 layers optimal; 3 layers cause "format drift" and semantic noise; Beta weighting: β=0.7 prioritizes department accuracy over difficulty.

- **Failure signatures**: Random selection baseline: If expertise table is corrupted or classifier fails, performance drops to ~60% (Random-3 in Table III); Excessive collaboration rounds: 3+ layers cause response format divergence, breaking downstream parsing; Over-recruitment: Adding >4 agents includes weaker models that pollute aggregation.

- **First 3 experiments**:
  1. Validate expertise table transfer: Build expertise table on MedQA validation set; test retrieval accuracy on NEJMQA categories not in training. Verify paper's claim that standardized categories generalize across datasets.
  2. Ablate confidence vs. adversarial components: Run EMRC with only confidence fusion, only adversarial validation, and neither. Reproduce Table III results (Baseline: 70.29%, w/o Confidence: 72.13%, w/o Adversarial: 72.86%, Full: 74.45%).
  3. Test collaboration layer limits: Run with 1, 2, 3 layers on held-out MedQA subset. Confirm 2-layer optimum and analyze failure cases at 3 layers (format drift examples from paper).

## Open Questions the Paper Calls Out

- Can semantic or structural constraints mitigate the "format drift" and noise observed when extending the multi-agent collaboration architecture beyond two layers? The paper identifies the failure mode but does not test mechanisms (e.g., constrained decoding or schema enforcement) to stabilize the collaboration loop for deeper reasoning.

- Is the performance degradation observed with larger agent teams (N > 4) primarily caused by the inclusion of lower-quality models or by the difficulty of consensus aggregation? The experiment varies the agent count but does not isolate the variable of marginal agent quality, leaving open the question of whether a "too many cooks" effect exists even with uniformly high-quality experts.

- How robust is the static LLM expertise table when the distribution of the deployment dataset diverges significantly from the MedQA validation set used for profiling? While the framework performs well on the tested datasets, the sensitivity of the "expertise-aware recruitment" to domain shift remains unquantified, which is critical for rare disease or specialized clinical applications.

## Limitations

- The expertise table construction relies entirely on pseudo-labels from a single 671B parameter model (Deepseek-R1), creating potential single-point-of-failure bias.

- The confidence fusion mechanism lacks a precise mathematical formulation, making exact replication challenging.

- The adversarial validation assumes the highest-expertise agent can reliably detect errors in others' outputs without sharing systematic hallucinations.

## Confidence

- **High Confidence**: The empirical results showing 74.45% accuracy on MMLU-Pro-Health versus 70.29% baseline are well-documented with clear ablation studies. The observation that 2 collaboration layers outperform 1 or 3 layers is consistently demonstrated across experiments.

- **Medium Confidence**: The expertise-aware recruitment mechanism shows strong correlation between validation-set performance and test performance, but this assumes category transferability that may not hold across all medical subdomains.

- **Low Confidence**: The self-assessed confidence scores' reliability for fusion weighting is not independently validated. The paper claims LLMs can "reliably estimate their own uncertainty" but provides limited calibration data.

## Next Checks

1. Test whether expertise scores derived from MedQA validation set transfer to NEJMQA and MMLU-Pro-Health datasets by measuring retrieval accuracy for categories not present in the training validation set.

2. Measure the correlation between self-assessed confidence scores and actual accuracy across all LLMs in the pool. Identify domains where overconfidence systematically degrades confidence fusion performance.

3. Analyze whether the adversarial Judge agent shares common hallucinations with other pool LLMs by comparing error patterns across multiple collaboration runs. Test whether adversarial validation still improves performance when Judge and other agents share similar training data biases.