---
ver: rpa2
title: 'ReCreate: Reasoning and Creating Domain Agents Driven by Experience'
arxiv_id: '2601.11100'
source_url: https://arxiv.org/abs/2601.11100
tags:
- agent
- arxiv
- scaffold
- recreate
- experience
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReCreate introduces an experience-driven framework for automatic
  domain agent creation. Instead of relying solely on performance metrics, it leverages
  interaction histories to understand why agents succeed or fail, then uses this insight
  to iteratively improve scaffolds.
---

# ReCreate: Reasoning and Creating Domain Agents Driven by Experience

## Quick Facts
- arXiv ID: 2601.11100
- Source URL: https://arxiv.org/abs/2601.11100
- Reference count: 40
- ReCreate achieves 36%–82% cost reduction over automated generation baselines while consistently outperforming human-designed agents.

## Executive Summary
ReCreate introduces an experience-driven framework for automatically creating domain agents by leveraging interaction histories rather than relying solely on performance metrics. It maps execution evidence into scaffold edits through a reasoning–creation synergy pipeline, iteratively refining agents from minimal seed scaffolds. Evaluated across 13 benchmarks in software engineering, data science, mathematics, and digital assistance, ReCreate consistently outperforms both human-designed agents and existing automated methods. It achieves significant cost reductions by converging faster through richer feedback signals.

## Method Summary
ReCreate employs an agent-as-optimizer design that retrieves on-demand experiences and uses a reasoning–creation synergy pipeline to map execution evidence into scaffold edits. The framework performs hierarchical updates that abstract instance-level refinements into reusable domain patterns. By focusing on the "why" behind agent successes and failures through interaction histories, ReCreate iteratively improves scaffolds beyond surface-level performance metrics, enabling robust and cost-effective domain agent creation.

## Key Results
- Achieves 36%–82% cost reductions compared to prior automated generation methods
- Outperforms human-designed agents and existing automated methods across 13 benchmarks
- Works effectively even when starting from minimal seed scaffolds

## Why This Works (Mechanism)
ReCreate's effectiveness stems from its experience-driven approach that goes beyond performance metrics to understand the underlying reasons for agent success or failure. By retrieving interaction histories and mapping execution evidence to scaffold edits, it creates a feedback loop that captures both tactical improvements and strategic domain patterns. The hierarchical update mechanism ensures that instance-level refinements contribute to reusable domain knowledge, enabling faster convergence and better generalization.

## Foundational Learning
- **Experience Retrieval**: Critical for providing historical context beyond immediate task outcomes; quick check: verify retrieval latency and relevance scoring
- **Reasoning–Creation Synergy**: Enables bidirectional flow between execution analysis and scaffold modification; quick check: test pipeline end-to-end with synthetic errors
- **Hierarchical Updates**: Abstracts specific fixes into reusable patterns; quick check: measure pattern reuse across different tasks
- **Scaffold Editing**: Direct mechanism for implementing improvements; quick check: validate edit precision and rollback capability
- **Domain Pattern Abstraction**: Converts tactical wins into strategic knowledge; quick check: test pattern applicability on out-of-distribution tasks
- **Cost-Aware Optimization**: Balances performance gains against resource usage; quick check: monitor cost per task improvement

## Architecture Onboarding
**Component Map**: Experience Retrieval -> Reasoning Pipeline -> Scaffold Editor -> Hierarchical Updater
**Critical Path**: Retrieve experiences → Analyze execution evidence → Generate scaffold edits → Apply hierarchical updates → Evaluate new agent performance
**Design Tradeoffs**: Prioritizes depth of understanding over speed of iteration; balances between immediate fixes and long-term pattern building
**Failure Signatures**: Poor experience retrieval leads to blind updates; weak reasoning produces irrelevant edits; inadequate abstraction results in overfitting to specific tasks
**First Experiments**: (1) Validate experience retrieval with diverse task histories; (2) Test reasoning pipeline on synthetic execution logs; (3) Measure hierarchical update effectiveness on held-out tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on synthetic benchmarks rather than real-world deployments
- Cost reduction estimates assume fixed API pricing and task distributions
- Framework performance depends on quality of interaction histories

## Confidence
- Core improvement claims: High
- Cost estimates: Medium
- Long-term generalization: Low

## Next Checks
- Deploy ReCreate in live enterprise environments to measure real-world task success and latency
- Test robustness with noisy or adversarial interaction logs to assess scaffold resilience
- Evaluate transferability by applying learned domain patterns to out-of-distribution tasks not seen during training