---
ver: rpa2
title: 'Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model
  Approach to Reducing Social-Desirability Bias'
arxiv_id: '2509.19314'
source_url: https://arxiv.org/abs/2509.19314
tags:
- social
- item
- items
- desirability
- personality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study tested whether GPT-o3 could reduce social desirability\
  \ bias in personality assessment by rewriting the IPIP-BFM-50. Two groups (n=100\
  \ each) completed either the original or AI-neutralized version alongside the Marlowe\u2013\
  Crowne Social Desirability Scale."
---

# Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias

## Quick Facts
- **arXiv ID**: 2509.19314
- **Source URL**: https://arxiv.org/abs/2509.19314
- **Reference count**: 24
- **Key outcome**: GPT-o3 successfully reduced social desirability bias in IPIP-BFM-50 items, with improved Conscientiousness and weakened SDR correlations, but metric/scalar invariance failed and Agreeableness/Openness reliability declined.

## Executive Summary
This study tested whether GPT-o3 could reduce social desirability bias in personality assessment by rewriting the IPIP-BFM-50. Two groups (n=100 each) completed either the original or AI-neutralized version alongside the Marlowe–Crowne Social Desirability Scale. The neutralized items preserved the five-factor structure and reliability (alphas mostly 0.66–0.94), with improved Conscientiousness and weakened SDR correlations for multiple items. However, metric and scalar invariance failed across versions, and Agreeableness/Openness reliability declined. Results support AI neutralization as a promising but imperfect method for reducing bias without altering trait constructs.

## Method Summary
The study employed a randomized experimental design with two independent groups (n=100 each). One group completed the original IPIP-BFM-50 while the other completed a GPT-o3-neutralized version. Both groups also completed the Marlowe–Crowne Social Desirability Scale. The AI-neutralization was performed using a single GPT-o3 prompt to rewrite all 50 items to reduce socially desirable responding. Data analysis included confirmatory factor analysis, reliability testing, and invariance testing across the two versions.

## Key Results
- Neutralized items preserved the five-factor structure and mostly maintained reliability (alphas 0.66–0.94)
- Social desirability bias correlations weakened for multiple dimensions, particularly Conscientiousness
- Metric and scalar invariance failed across original and neutralized versions, limiting cross-group comparisons
- Reliability declined in Agreeableness and Openness dimensions after neutralization

## Why This Works (Mechanism)
The AI neutralization approach works by leveraging large language models' ability to rephrase items while preserving core meaning. GPT-o3 was prompted to rewrite personality items in ways that reduce socially desirable response tendencies while maintaining the underlying construct being measured. This allows for bias reduction without requiring extensive human item development, potentially making the process more scalable and consistent than traditional methods.

## Foundational Learning
- **Social desirability bias**: The tendency to respond in ways that will be viewed favorably by others; needed to understand why personality assessments may be systematically distorted.
- **Measurement invariance**: The degree to which measures behave similarly across groups; critical for ensuring that neutralized items measure the same constructs as original items.
- **Confirmatory factor analysis**: Statistical method for testing hypothesized measurement models; essential for validating that neutralization preserves intended factor structure.
- **Multigroup invariance testing**: Procedure for comparing measurement properties across groups; needed to establish whether neutralized items function equivalently to originals.
- **Reliability coefficients**: Statistical indices of internal consistency; important for assessing whether neutralization affects measurement precision.
- **Item response theory**: Framework for modeling the relationship between latent traits and item responses; relevant for understanding how neutralization affects item functioning.

## Architecture Onboarding
**Component map**: Item pool -> GPT-o3 neutralization prompt -> Neutralized items -> Pilot testing -> Full administration -> CFA/invariance testing -> Reliability analysis

**Critical path**: GPT-o3 neutralization -> Multigroup CFA testing -> Reliability assessment -> SDR correlation analysis

**Design tradeoffs**: Single-prompt vs. iterative generation (speed vs. quality control); automation vs. human oversight (scalability vs. precision); bias reduction vs. construct preservation (desirable vs. potential construct underrepresentation).

**Failure signatures**: Lack of metric/scalar invariance indicates neutralization may have altered item functioning; reliability declines suggest potential construct underrepresentation; failed CFA indicates neutralization may have distorted intended factor structure.

**First 3 experiments**:
1. Test GPT-o3 neutralization with iterative human-in-the-loop feedback to improve invariance
2. Compare single-prompt vs. multi-agent generation pipelines on bias reduction effectiveness
3. Evaluate neutralization effects across diverse demographic groups to assess generalizability

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does AI-based neutralization preserve the criterion-related validity of personality measures against external behavioral outcomes?
- Basis in paper: [explicit] The authors state that "criterion-related validity with external outcomes remains untested for the neutralized form" and that future work should evaluate "predictive validity."
- Why unresolved: The study relied exclusively on self-report data and internal structure analysis (CFA), meaning it is unknown if the neutralized items predict real-world behaviors as well as the original items.
- What evidence would resolve it: A study correlating neutralized scale scores with non-self-report criteria, such as supervisor performance ratings or academic achievement.

### Open Question 2
- Question: Do neutralized items effectively reduce social desirability bias in high-stakes contexts where incentives to impression manage are stronger?
- Basis in paper: [explicit] The limitations section notes that "Social desirability effects can be stronger under incentives to self-present" and "generalizability to high-stakes settings" is limited.
- Why unresolved: The experiment used a low-stakes online sample where participants had little motivation to fake responses, unlike in personnel selection.
- What evidence would resolve it: Field studies administering the neutralized scale in competitive hiring or admissions settings and comparing SDR correlations against the low-stakes baseline.

### Open Question 3
- Question: Can iterative, multi-agent generation pipelines achieve full measurement invariance where single-prompt editing failed?
- Basis in paper: [explicit] The authors argue that "single-shot generation often produces variable quality" and suggest "domain-specific fine-tuning" or "multi-agent" systems to fix the lack of metric and scalar invariance.
- Why unresolved: The single-prompt method used (GPT-o3) successfully reduced bias but failed to establish metric and scalar invariance, making cross-group comparisons invalid.
- What evidence would resolve it: Developing a pipeline involving automated self-critique or human-in-the-loop feedback and testing if the resulting items pass strict multi-group invariance tests.

## Limitations
- Failure of metric and scalar invariance between original and neutralized versions limits cross-group comparisons
- Reliability declines in Agreeableness and Openness dimensions suggest potential construct underrepresentation
- Single-sample design and lack of demographic diversity limit generalizability of results
- No testing of criterion-related validity with external behavioral outcomes

## Confidence
- Confidence in the claim that GPT-o3 can reduce social desirability bias: **Medium** (supported by reduced SDR correlations but weakened by invariance failures)
- Confidence in the claim that neutralized items preserve the five-factor structure: **Medium** (structure maintained in this sample but invariance issues question stability across contexts)
- Confidence in generalizability of results: **Low** (single-sample design, lack of demographic diversity, invariance failures)

## Next Checks
1. Test metric and scalar invariance of the neutralized scale across multiple independent samples and demographic groups
2. Conduct cross-validation of reliability and SDR correlation improvements in a new sample using both original and neutralized versions
3. Evaluate whether the neutralization process maintains construct validity using external criterion measures beyond SDR