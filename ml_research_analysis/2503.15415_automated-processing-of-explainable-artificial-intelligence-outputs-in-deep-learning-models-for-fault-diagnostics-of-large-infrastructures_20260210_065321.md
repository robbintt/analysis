---
ver: rpa2
title: Automated Processing of eXplainable Artificial Intelligence Outputs in Deep
  Learning Models for Fault Diagnostics of Large Infrastructures
arxiv_id: '2503.15415'
source_url: https://arxiv.org/abs/2503.15415
tags:
- explanations
- images
- deep
- class
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of validating deep learning
  (DL) model outputs for fault diagnostics in large infrastructures by automating
  the analysis of model explanations. Manual review of these explanations is time-consuming
  and error-prone, necessitating automated tools.
---

# Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures

## Quick Facts
- arXiv ID: 2503.15415
- Source URL: https://arxiv.org/abs/2503.15415
- Reference count: 0
- This paper presents an automated framework to validate deep learning model outputs for fault diagnostics by analyzing model explanations using semi-supervised anomaly detection.

## Executive Summary
This paper addresses the challenge of validating deep learning model outputs for fault diagnostics in large infrastructures by automating the analysis of model explanations. Manual review of these explanations is time-consuming and error-prone, necessitating automated tools. The proposed framework combines post-hoc explanations with semi-supervised anomaly detection to identify anomalous explanations that may indicate model abnormal behaviors, such as misclassifications or reliance on non-causal shortcuts. The framework was applied to drone-collected images of insulator shells for power grid monitoring, using two CNN classifiers (MobileNetV3 Small and EfficientNet-B0), GradCAM for explanations, and Deep Semi-Supervised Anomaly Detection (Deep SAD) for anomaly identification.

## Method Summary
The framework processes CNN predictions by generating GradCAM explanations for each classified image, then applies class-specific Deep SAD models to identify anomalous explanations. Each Deep SAD model is trained using correctly classified images as "normal" samples and misclassifications as "anomalous" samples, learning an embedding space where normal explanations cluster near a center point. The framework uses a precision-focused threshold (β=0.1) to minimize operator workload while maximizing detection of problematic classifications. The method was evaluated on the Insulator Defect Image Dataset (IDID) using 5-fold cross-validation, with separate Deep SAD models trained for each predicted class.

## Key Results
- Average classification accuracy improvement of 8% on faulty classes after manual reclassification of anomalous explanations
- Maintenance operators required to manually review only 15% of the images
- Outperformed state-of-the-art faithfulness-based approach with consistently higher F1 scores
- Successfully identified correct classifications resulting from non-causal shortcuts, such as reliance on ID tags

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GradCAM explanations of correctly classified images form coherent spatial patterns that cluster in embedding space, while misclassifications and shortcut-based predictions produce divergent patterns.
- Mechanism: GradCAM computes gradient-weighted feature maps from the final convolutional layer, generating a coarse localization map where pixel importance correlates with contribution to the predicted class. When the model attends to causally relevant features (e.g., damaged insulator regions), explanations exhibit consistent spatial structures; when attending to non-causal shortcuts (e.g., ID tags), explanations deviate structurally.
- Core assumption: Correct classifications predominantly rely on class-relevant visual features, producing similar explanation patterns; shortcuts and errors produce dissimilar patterns detectable via distance metrics.
- Evidence anchors:
  - [abstract] "proposed framework successfully identifies correct classifications that result from non-causal shortcuts, such as the presence of ID tags printed on insulator shells"
  - [section 7.2] "Explanation 6d illustrates a correct classification for the wrong reason...Deep SAD maps this explanation far from the center"
  - [corpus] Weak direct corpus support for GradCAM-specific anomaly clustering; related work (IMPACTX) explores XAI-driven improvement but not this specific mechanism.
- Break condition: If the CNN learns a consistent non-causal shortcut across most training samples, those "wrong" explanations will cluster as "normal," causing false negatives.

### Mechanism 2
- Claim: Semi-supervised anomaly detection on explanation embeddings can identify misclassifications without requiring labeled normal/anomalous explanation pairs during training.
- Mechanism: Deep SAD trains an encoder to map explanations into a compact embedding space where unlabeled samples (assumed mostly normal) cluster near a learned center, while explicitly labeled anomalous samples are pushed away. The distance from center serves as an anomaly score.
- Core assumption: Misclassifications are rare in the dataset (satisfying anomaly detection's "normal majority" assumption), and their explanations differ systematically from correct classifications.
- Evidence anchors:
  - [section 4.3] "Deep SAD employs a DL model to map input samples into an embedding space, with the objective of minimizing the distance d between the center of this space and normal or unlabeled samples, while maximizing d between the center and anomalous samples"
  - [section 5] "the expectation is that a small fraction of the images is incorrectly classified due to the typically high accuracy of DL classifiers, so that the typical AD setup is satisfied"
  - [corpus] No corpus papers directly validate Deep SAD on explanation embeddings; mechanism is paper-specific.
- Break condition: If misclassification rates are high (>15-20%) or misclassification explanations are visually similar to correct ones, the "unlabeled = mostly normal" assumption fails, degrading detection.

### Mechanism 3
- Claim: Class-specific Deep SAD models outperform a single global model because explanation patterns differ across predicted classes.
- Mechanism: The framework trains Z separate Deep SAD models, one per predicted class. Each model learns a class-specific embedding center and threshold, accommodating the observation that explanations for different classes form distinct clusters.
- Core assumption: Explanations for correctly classified images of class A share more structure with each other than with class B explanations.
- Evidence anchors:
  - [section 5] "The decision to develop Z distinct models φ_z for identifying anomalous explanations rather than a single model is driven by the observation that explanations of images correctly classified into different classes often form distinct clusters"
  - [section 6.1] "Performance evaluation for each φ_z employs a 5-fold cross-validation procedure, utilizing only the images...assigned to class z"
  - [corpus] No corpus evidence on class-specific vs. global explanation models.
- Break condition: If two classes have highly similar explanation patterns (e.g., subtle defect variants), class-specific models may underfit due to reduced training data per class.

## Foundational Learning

- **Post-hoc XAI (feature attribution methods)**
  - Why needed here: Understanding that GradCAM produces spatial heatmaps by gradient-weighting convolutional features is essential for interpreting what "normal" vs. "anomalous" explanations represent.
  - Quick check question: Can you explain why GradCAM uses the final convolutional layer rather than earlier layers?

- **Semi-supervised anomaly detection**
  - Why needed here: Deep SAD's hybrid loss (unlabeled + labeled anomalies) is the core detection mechanism; understanding how the center is initialized and the role of η is critical for debugging.
  - Quick check question: What happens to Deep SAD if all training samples are labeled as anomalies?

- **Fβ score and precision-recall tradeoffs**
  - Why needed here: The paper uses β=0.1 to prioritize precision (minimizing operator workload) over recall; understanding this tradeoff is essential for threshold tuning.
  - Quick check question: If you wanted to catch more misclassifications at the cost of more false positives, would you increase or decrease β?

## Architecture Onboarding

- **Component map:**
  ```
  Input Image → CNN Classifier (Φ) → Predicted Class (ẑ)
                    ↓
              GradCAM (g) → Explanation (y)
                    ↓
         Class-specific Deep SAD (Ψ_ẑ) → Distance from center
                    ↓
              Threshold comparison → Normal/Anomalous label
  ```
  Each component is modular: the paper demonstrates substituting CartoonX for GradCAM and MobileNetV3 for EfficientNet.

- **Critical path:**
  1. Pre-train CNN classifier on labeled images (transfer learning from ImageNet)
  2. Generate GradCAM explanations for all training/validation images
  3. Pre-train autoencoder on explanations, then fine-tune Deep SAD per class
  4. Set distance threshold per class using validation set Fβ optimization
  5. At inference: classify → explain → compute distance → flag if anomalous

- **Design tradeoffs:**
  - **β=0.1 (precision-focused)**: Reduces operator workload but misses some misclassifications (recall 24-50%). Increase β for higher recall at cost of more reviews.
  - **Embedding dimension=10**: Mitigates overfitting on small datasets but may limit expressiveness for complex explanation patterns.
  - **η=10-100**: Weights labeled anomalies more heavily; critical when anomalies are rare. Must tune per dataset.

- **Failure signatures:**
  - **All explanations mapped to same point**: Learning rate too high (reduce to 1e-4); observed with EfficientNet-B0 on broken class.
  - **High variance across folds**: Limited training data; use multiple random initializations or leave-one-out cross-validation.
  - **CartoonX produces "jittery" explanations**: Non-continuous regions fail to produce coherent embeddings; GradCAM preferred for this task.

- **First 3 experiments:**
  1. **Baseline replication**: Train EfficientNet-B0 on IDID dataset, generate GradCAM explanations for 100 images across 3 classes, visually inspect for ID tag shortcuts.
  2. **Embedding visualization**: Train single-class Deep SAD on flashover explanations, project embeddings via t-SNE, verify that misclassifications cluster farther from center.
  3. **Threshold sensitivity analysis**: Vary β from 0.1 to 1.0, plot precision-recall tradeoff curves, quantify operator workload vs. accuracy improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of multiple explainable AI (XAI) techniques improve the robustness of the anomaly detection module compared to using a single technique like GradCAM?
- Basis in paper: [explicit] The authors state in the Conclusion that "integration of multiple XAI techniques to obtain richer explanations" could mitigate the effects of non-informative explanations, such as those observed when using CartoonX for the flashover class.
- Why unresolved: The current framework relies on single XAI outputs (GradCAM or CartoonX). The paper notes that some methods fail to provide continuous, meaningful regions for specific classes, which hampers the Deep SAD module's ability to distinguish anomalies.
- What evidence would resolve it: A comparative study evaluating the framework's F1 score and accuracy improvement when using fused explanations (e.g., GradCAM + LRP) versus single-method explanations on the same dataset.

### Open Question 2
- Question: Can the proposed framework be effectively adapted for fault diagnostics using non-image data modalities, such as time-series sensor data?
- Basis in paper: [explicit] The Conclusion suggests that "it would be interesting to explore the applicability of the proposed framework to other types of data different from images."
- Why unresolved: The methodology and case study were exclusively designed for image classification using CNNs and visual explanation methods (GradCAM). The transferability of the Deep SAD embedding approach to time-series feature attributions remains unverified.
- What evidence would resolve it: Successful application of the framework to a time-series diagnostic task (e.g., vibration analysis) using a relevant XAI method, demonstrating improved classification accuracy similar to the image-based results.

### Open Question 3
- Question: How can the framework be systematically optimized to balance the operational costs of manual reclassification against the costs of model misclassification?
- Basis in paper: [explicit] The Conclusion notes that "systematic optimization will require... the definition of an objective function considering several factors, like the costs of misclassifications, manual classifications and model development."
- Why unresolved: The current study used a heuristic hyperparameter setting (β=0.1) to prioritize precision, rather than a formal cost-benefit optimization strategy.
- What evidence would resolve it: A formulation of a cost-sensitive objective function and experimental results showing that optimizing this function leads to a lower total operational cost compared to the standard F-score optimization.

## Limitations
- The framework assumes misclassifications are rare (<15-20%), which may not hold for all applications or datasets
- Reliance on GradCAM may miss complex explanation patterns, as evidenced by CartoonX producing "jittery" explanations
- Class-specific Deep SAD models may underfit when training data per class is limited

## Confidence
- **High confidence**: The overall framework architecture and its modular design allowing component substitution
- **Medium confidence**: The 8% accuracy improvement and 15% review rate, given the limited evaluation on a single dataset
- **Low confidence**: The generalizability to other infrastructure fault diagnostics applications beyond insulator shells

## Next Checks
1. Test the framework on a dataset with higher misclassification rates (20-30%) to validate the anomaly detection assumption
2. Compare GradCAM vs. alternative explanation methods (e.g., LIME, SHAP) on the same dataset to quantify method sensitivity
3. Implement and evaluate a baseline that randomly selects images for manual review rather than using anomaly detection, to measure the framework's added value