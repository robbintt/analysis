---
ver: rpa2
title: 'A three-Level Framework for LLM-Enhanced eXplainable AI: From technical explanations
  to natural language'
arxiv_id: '2506.05887'
source_url: https://arxiv.org/abs/2506.05887
tags:
- explanations
- explanation
- level
- knowledge
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning explainable AI (XAI)
  explanations with the diverse needs of different stakeholders, from developers to
  end-users and society. The authors propose a three-level framework that uses Large
  Language Models (LLMs) to transform technical XAI outputs into accessible, contextual
  narratives.
---

# A three-Level Framework for LLM-Enhanced eXplainable AI: From technical explanations to natural language

## Quick Facts
- arXiv ID: 2506.05887
- Source URL: https://arxiv.org/abs/2506.05887
- Reference count: 40
- Primary result: Three-level framework using LLMs to transform technical XAI outputs into accessible, contextual narratives for different stakeholder groups

## Executive Summary
This paper addresses the challenge of aligning explainable AI (XAI) explanations with the diverse needs of different stakeholders, from developers to end-users and society. The authors propose a three-level framework that uses Large Language Models (LLMs) to transform technical XAI outputs into accessible, contextual narratives. The framework consists of: 1) algorithmic and domain-based explainability ensuring model fidelity, 2) human-centered interactive explainability allowing user feedback and refinement, and 3) societal transparency explainability ensuring public trust and ethical alignment. Through case studies in loan approval scenarios, the framework demonstrates how LLMs can bridge the gap between complex model behavior and human understanding, achieving technical fidelity, user engagement, and societal accountability. The approach reframes XAI as a dynamic, trust-building process that leverages natural language capabilities to democratize AI explainability.

## Method Summary
The framework implements three case studies in loan approval scenarios. Level 1 generates technical explanations using established XAI methods: SHAP values for feature attribution, interpretable decision trees with threshold rules, and autoencoder-based prototype/counterfactual generation. Level 2 provides interactive dialogue allowing users to query and refine explanations. Level 3 employs LLMs (GPT-4.5) to translate structured technical outputs into natural language narratives, incorporating domain knowledge and ethical constraints. The system uses established XAI toolkits including SHAP, LIME, DALEX, AIX360, Alibi, and Captum. The approach emphasizes architectural separation where LLMs serve as translators rather than explanation generators to preserve fidelity.

## Key Results
- LLMs can effectively translate technical XAI outputs into accessible natural language narratives when constrained by structured inputs
- Stakeholder segmentation (developers, domain experts, end-users) enables tailored explanation strategies across three framework levels
- Interactive feedback at Level 2 allows expert users to detect biases and trigger system refinements
- The framework demonstrates potential for building trust and transparency in high-stakes domains like finance and healthcare

## Why This Works (Mechanism)

### Mechanism 1: Fidelity Preservation Through Architectural Separation
LLMs operate as translators, not generators, of explanatory content. Level 1 produces algorithmic explanations via established XAI methods (SHAP, LIME, decision trees). These outputs—with quantified fidelity—are passed to Level 3, where LLMs convert structured technical artifacts into natural language. The LLM's role is constrained: it receives pre-validated explanations plus contextual prompts (domain knowledge, ethical constraints, user profile), reducing the space for fabrication. The fidelity of the final explanation depends entirely on the quality of the Level 1 XAI method; LLMs cannot improve fidelity, only accessibility.

### Mechanism 2: Audience Segmentation Drives Explanation Modality
Effective explainability requires distinct explanation strategies for developers (RED XAI), domain experts, and end-users/society (BLUE XAI), rather than one-size-fits-all outputs. The framework maps stakeholder epistemic needs to explanation types: developers receive feature attributions and debugging signals; domain experts receive knowledge-matched explanations via ontologies, concept bottleneck models, or prototype questions; end-users receive natural language narratives. Trust is built progressively—technical fidelity first, then contextual relevance, then social legitimacy.

### Mechanism 3: Interactive Feedback Enables Bias Detection and System Refinement
Human-in-the-loop interaction at Level 2 allows experts to detect biases, override inappropriate features, and trigger system updates. Users query the system (e.g., counterfactual questions, feature sensitivity probes). Experts can reject variables that violate policy (Case Study 1: expert excludes income as discriminatory factor). The system responds with revised explanations and flags necessary model updates. This is bidirectional: explanation refinement can surface the need for model retraining.

## Foundational Learning

- **Concept: Post-hoc XAI methods (SHAP, LIME, feature attribution)**
  - **Why needed here:** Level 1 requires selecting and correctly applying model-agnostic or model-specific explainers. Misunderstanding their assumptions (e.g., feature independence in SHAP) propagates errors downstream.
  - **Quick check question:** Given a tabular loan dataset with correlated features (income and debt-to-income ratio), which attribution method would produce misleading importance scores, and why?

- **Concept: LLM hallucination and grounding strategies**
  - **Why needed here:** The framework claims to mitigate hallucination via architectural separation; understanding failure modes is essential for prompt design and verification.
  - **Quick check question:** An LLM explains a denied loan by inventing a "debt utilization ratio" feature that doesn't exist in the model. At which level did the failure occur, and how would you detect it?

- **Concept: Stakeholder analysis and epistemic requirements**
  - **Why needed here:** Level 2 and Level 3 outputs depend on correctly profiling the audience; a mismatch yields technically correct but socially or contextually ineffective explanations.
  - **Quick check question:** A regulator asks for an explanation of model fairness. Would you route them to Level 1 SHAP values, Level 2 interactive exploration, or Level 3 natural language narratives—and what tradeoffs does each choice introduce?

## Architecture Onboarding

- **Component map:** Model inference → XAI explainer (SHAP/LIME/tree rules/prototypes) → domain knowledge injection → structured explanation output → interactive interface → user/expert queries → feedback loop → adapted explanation or system update flag → LLM prompt (XAI output + context) → natural language output → conversational dialogue handler

- **Critical path:** Level 1 XAI fidelity → Level 2 expert validation → Level 3 LLM translation. If Level 1 produces low-fidelity explanations (wrong method for model type), all downstream levels inherit the error. Priority: validate Level 1 against ground-truth model behavior before integrating LLM layers.

- **Design tradeoffs:**
  - On-premise vs. cloud LLM: Paper recommends on-premise for sensitive domains (healthcare, finance) to meet privacy and compliance requirements; tradeoff is reduced model capability and higher ops burden.
  - Interpretable model vs. post-hoc: Decision trees (Case Study 2) provide native explainability but may sacrifice accuracy; post-hoc methods (Case Study 1) preserve model performance but add approximation error.
  - Static vs. interactive explanations: Static is cheaper but ignores user-specific context; interactive requires UX investment and latency-tolerant infrastructure.

- **Failure signatures:**
  - LLM generates features not present in Level 1 output → hallucination; detect by cross-referencing LLM output against structured XAI artifact.
  - Expert feedback ignored by system → no update mechanism instrumented; detect via audit logs showing feedback without subsequent model/pipeline changes.
  - Non-expert user confused by Level 2 interface → audience mismatch; detect via task completion rates and user confusion signals.

- **First 3 experiments:**
  1. **Fidelity validation:** Run SHAP on a synthetic loan model with known ground-truth feature importance. Compare Level 1 output to ground truth; measure error rate before connecting LLM.
  2. **Hallucination boundary test:** Pass Level 1 explanations to LLM with intentionally sparse prompts; measure rate of fabricated features. Then enrich prompts with domain context and measure reduction.
  3. **Stakeholder A/B test:** For identical loan denials, provide Group A with Level 1-only explanations and Group B with full three-level explanations. Measure task completion time, self-reported understanding, and trust scores; verify whether Level 3 adds measurable value for non-experts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What standardized qualitative and quantitative metrics can effectively evaluate the effectiveness, usability, and ethical adequacy of explanations across all three framework levels?
- Basis in paper: [explicit] The conclusion states: "First, there is a critical need for standardized qualitative and quantitative metrics to evaluate the effectiveness, usability, and ethical adequacy of explanations at all three levels."
- Why unresolved: The paper proposes a conceptual framework but does not define or validate specific measurement instruments for comparing explanations across algorithmic, human-centered, and societal levels.
- What evidence would resolve it: Development and empirical validation of a unified evaluation protocol with reliability and validity assessments across stakeholder groups.

### Open Question 2
- Question: How can explanation systems dynamically adapt their content and format to individual user profiles, learning contexts, and decision criticality in real time?
- Basis in paper: [explicit] The authors identify that "adaptive systems could dynamically tailor explanation types to individual user profiles, learning contexts, or decision criticality" as a research direction.
- Why unresolved: The framework describes audience-aware explanation as a principle but does not specify computational mechanisms for real-time personalization or adaptation.
- What evidence would resolve it: Implementation and user study comparing static vs. adaptive explanations, measuring comprehension speed, trust calibration, and decision quality.

### Open Question 3
- Question: How can the fairness, robustness, and alignment of LLMs within XAI pipelines be ensured, particularly regarding hallucination and bias amplification?
- Basis in paper: [explicit] The conclusion notes: "since LLMs play an increasingly central role in these explanation pipelines, ensuring their fairness, robustness, and alignment with societal values is essential" and mentions challenges of "factual consistency, bias amplification, and explainability of the LLMs themselves."
- Why unresolved: While the framework uses LLMs as translators rather than explanation generators, empirical safeguards against hallucinations and bias propagation in the translation layer remain undefined.
- What evidence would resolve it: Systematic benchmarking of LLM-translated explanations against ground-truth algorithmic outputs across diverse demographic groups and domain contexts.

### Open Question 4
- Question: How do multilevel explanations influence understanding, trust, and decision-making behavior across different stakeholder groups in high-stakes domains?
- Basis in paper: [explicit] The authors state: "This includes designing user studies across different stakeholder groups to assess how explanations influence understanding, trust, and behavior."
- Why unresolved: The case studies are illustrative and domain-limited (loan approval); no controlled experiments demonstrate the framework's impact on actual user outcomes.
- What evidence would resolve it: Randomized controlled studies in healthcare, finance, or public services measuring objective comprehension, trust scores, and decision accuracy for developers, domain experts, and end-users.

## Limitations

- The framework's efficacy critically depends on the quality of Level 1 XAI methods, but no quantitative fidelity metrics or error bounds are provided
- No controlled experiments demonstrate the framework's impact on actual user outcomes across different stakeholder groups
- The hallucination mitigation claim relies on architectural separation but lacks empirical validation against benchmark hallucination detection methods

## Confidence

- **High confidence:** The three-level stakeholder segmentation approach is well-grounded in XAI literature and addresses a real problem (non-expert accessibility)
- **Medium confidence:** The architectural separation claim for hallucination prevention is plausible but unproven; similar approaches in the literature show mixed results
- **Low confidence:** The framework's ability to maintain fidelity while improving accessibility lacks quantitative validation—no comparison metrics, ablation studies, or user study results are provided

## Next Checks

1. Implement controlled experiments comparing Level 1-only explanations versus full three-level explanations across multiple stakeholder groups, measuring task completion rates and trust scores
2. Conduct hallucination boundary testing by systematically varying prompt detail levels and measuring fabricated content introduction rates using automated fact-checking against ground-truth model behavior
3. Perform fidelity validation by generating explanations for synthetic models with known ground-truth feature importance, then measuring error propagation across all three levels