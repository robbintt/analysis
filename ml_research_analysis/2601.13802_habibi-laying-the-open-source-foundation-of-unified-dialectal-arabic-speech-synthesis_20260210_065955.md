---
ver: rpa2
title: 'Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech
  Synthesis'
arxiv_id: '2601.13802'
source_url: https://arxiv.org/abs/2601.13802
tags:
- arabic
- speech
- data
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Habibi, the first open-source framework for
  unified-dialectal Arabic speech synthesis. It addresses the challenge of supporting
  diverse Arabic dialects with limited data and without requiring diacritization.
---

# Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis

## Quick Facts
- arXiv ID: 2601.13802
- Source URL: https://arxiv.org/abs/2601.13802
- Reference count: 38
- First open-source framework for unified-dialectal Arabic speech synthesis

## Executive Summary
Habibi presents the first open-source framework for unified-dialectal Arabic speech synthesis, addressing the challenge of supporting diverse Arabic dialects with limited data and without requiring diacritization. The approach uses linguistically-informed curriculum learning, starting from Modern Standard Arabic (MSA) and progressively adapting to dialectal data, leveraging existing ASR corpora. Habibi supports over 20 Arabic variants and 12 regional identifiers, with a unified model achieving performance close to specialized dialect models and outperforming the leading commercial TTS service, ElevenLabs' Eleven v3, on major dialect test sets. The framework includes a systematic benchmark for multi-dialect Arabic zero-shot TTS and demonstrates strong in-context learning capabilities.

## Method Summary
Habibi builds on F5-TTS, a flow-matching non-autoregressive TTS backbone, initialized from 95K-hour Chinese-English pretrained weights. The method employs a two-stage curriculum learning approach: Stage 1 fine-tunes on filtered MSA data (~921 hours) until naturalness converges, followed by Stage 2 fine-tuning on dialectal data (1,857 hours total) either per-dialect or unified. Regional identifier tokens are optionally used during training. Data is filtered by characters-per-second thresholds and optionally denoised. The unified model mixes all dialects with 0.618 denoised-sampling ratio for noisy sources. Training runs for 200K updates (~2 days on 8×H100 GPUs).

## Key Results
- Unified model achieves WER-O ranging from 7.88% (MSA) to 50.30% (MAR) and WER-S from 7.74% (MSA) to 44.05% (MAR)
- Curriculum learning outperforms direct fine-tuning (19.50 vs 20.92 WER-O) and training from scratch fails (55.63 WER-O)
- Outperforms ElevenLabs Eleven v3 on major dialect test sets
- Supports over 20 Arabic variants and 12 regional identifiers

## Why This Works (Mechanism)

### Mechanism 1: Curriculum Learning Benefits
- Claim: Two-stage curriculum learning (MSA → dialectal data) improves convergence and final synthesis quality compared to direct fine-tuning or training from scratch.
- Mechanism: MSA provides stable grammatical and phonological scaffolding because it is a standardized form with consistent patterns. The model first transfers text-to-speech mapping capability from Chinese/English pretraining to Arabic via MSA, then adapts to dialectal distributions with this foundation in place.
- Core assumption: Linguistic structure learned from MSA transfers beneficially to dialectal variants despite their differences.
- Evidence anchors: [abstract] "linguistically-informed curriculum learning" enables support for "high- to low-resource Arabic dialects"; [section 3.5, Table 5] Curriculum approach achieves 19.50 WER-O vs. 20.92 for direct fine-tuning; training from scratch fails with 55.63 WER-O.

### Mechanism 2: In-Context Learning Criticality
- Claim: In-context learning via reference audio-transcription pairs is critical for dialectal feature capture during zero-shot inference.
- Mechanism: The model implicitly retrieves text-to-acoustic matching information from the reference pair, leveraging learned correspondences to generate dialect-appropriate speech without explicit dialect specification.
- Core assumption: The model has learned to map textual patterns to acoustic features during training and can generalize this to new speaker-dialect combinations at inference.
- Evidence anchors: [section 2.5] "models can implicitly retrieve and effectively leverage some text-to-acoustic matching information from known pairs during zero-shot inference"; [section 3.6, Table 6] Removing reference context degrades WER-O from 7.71→13.77 (MSA), 5.15→12.33 (UAE).

### Mechanism 3: Regional Identifier Benefits
- Claim: Regional identifier tokens during training improve dialect learning and generalize to identifier-free inference.
- Mechanism: Explicit dialect labels help the model internalize dialect-specific distributional patterns during training. At inference, even without identifiers, the model has better-structured dialect representations.
- Core assumption: Dialect distinctions can be learned as discrete categories that improve internal representation quality.
- Evidence anchors: [section 2.5] Training with identifiers "likely helps the model better understand and internalize certain dialect-related patterns within data distribution"; [section 3.8, Table 8] Uni.D1-I (trained with identifiers) outperforms Uni.D1 on WER-O across most dialects.

## Foundational Learning

- **Flow matching for non-autoregressive TTS (F5-TTS backbone)**
  - Why needed here: Habibi builds on F5-TTS, which operates directly on mel spectrograms and raw text without intermediate linguistic features. Understanding flow matching helps diagnose convergence and quality issues.
  - Quick check question: Can you explain how flow matching differs from autoregressive token prediction in speech generation?

- **Curriculum learning design principles**
  - Why needed here: The paper's success hinges on ordering training from high-resource/standard (MSA) to low-resource/dialectal. This requires understanding when curriculum helps vs. when it's unnecessary overhead.
  - Quick check question: What criteria would you use to determine when a two-stage curriculum is worth the additional training complexity?

- **Zero-shot TTS evaluation metrics (WER, SIM, UTMOS)**
  - Why needed here: The paper introduces a multi-dialect benchmark and uses both multilingual (WER-O) and dialect-specific (WER-S) ASR for evaluation. Interpreting results requires understanding what each metric captures.
  - Quick check question: Why might a multilingual ASR model give different WER scores than a dialect-specific model on the same speech, and which should you trust for dialectal TTS?

## Architecture Onboarding

- **Component map**: F5-TTS backbone -> Raw text characters (with optional regional identifier tokens) -> Mel spectrograms (with optional denoising) -> Output speech
- **Critical path**:
  1. Start with F5-TTS v1 base weights (Chinese-English pretrained)
  2. Stage 1: Fine-tune on filtered MSA data (~921 hours) until UTMOS converges (not WER—naturalness first)
  3. Stage 2: Continue training on dialect data (D1: 1635 hours basic, D2: 1857 hours expanded)
  4. For unified model: Mix all dialects with 0.618 denoised-sampling ratio for noisy sources
  5. Train to 200K updates (~2 days on 8×H100)
- **Design tradeoffs**:
  - Specialized vs. unified models: Specialized models outperform on in-domain data (ALG, IRQ with clean in-house data); unified models generalize better to noisy/out-of-domain conditions (MSA, EGY)
  - Data quality vs. quantity: Clean data outperforms noisy even at smaller scale (EGY results)
  - WER-O vs. WER-S: WER-O reflects generalization; WER-S reflects dialect-specific textual fidelity—choose based on deployment priorities
  - With vs. without regional identifiers: Identifiers improve WER at slight UTMOS cost; require training-inference consistency for best results
- **Failure signatures**:
  - Training from scratch: Fails to converge (55.63 WER-O on SAU)
  - Overtraining on ASR-quality data: Naturalness (UTMOS) and speaker similarity (SIM) degrade while WER continues improving
  - Missing reference audio at inference: WER approximately doubles
  - Dialect identifier mismatch: Training with identifiers but not using them at inference gives suboptimal results
- **First 3 experiments**:
  1. Validate backbone transfer: Fine-tune F5-TTS base on MSA only (Stage 1); confirm UTMOS convergence before proceeding. Expected: ~7.88 WER-O, 0.756 SIM, 1.93 UTMOS after 200K updates.
  2. Ablate reference context: Run zero-shot inference with zeroed reference audio on one dialect (e.g., SAU). Confirm WER degradation matches paper (~13→19 WER-O) to verify in-context learning is functioning.
  3. Test identifier impact: Train unified model with and without regional identifiers; compare WER-O on low-resource dialects (e.g., MAR). Expected: ~40 vs ~46 WER-O improvement with identifiers.

## Open Questions the Paper Calls Out

- **Minimum data scale for preserving base model capabilities**: The paper notes it "does not yet identify the minimum data scale or training strategies required to preserve the original Chinese and English performance after incorporating Arabic dialectal training." This remains unresolved because the study focused on optimizing Arabic dialectal synthesis without evaluating trade-offs regarding catastrophic forgetting of the pre-trained base model's original languages.

- **Extension to code-switching scenarios**: The authors explicitly note the "current model does not explicitly support code-switching scenarios, such as the mixed use of Arabic with English, French, or Spanish." This is unresolved because the model architecture and training data are currently monolingual regarding Arabic, lacking the necessary cross-lingual alignment or mixed-language corpus data to handle interspersed non-Arabic words.

- **Zero-shot transfer to unseen dialects**: The paper mentions that "the model's capacity for zero-shot transfer to dialects not explicitly included in fine-tuning, remain unexplored." This remains untested because the current benchmark and training data cover specific, defined dialect variants, leaving the model's ability to generalize to completely unseen or low-resource dialectal variations untested.

## Limitations

- Evaluation relies heavily on ASR-derived metrics (WER-O and WER-S), which can be noisy when ASR models themselves have limited dialect coverage.
- The "unified" model's superiority over specialized models is only demonstrated on specific test conditions - clean data for MSA and noisy data for EGY - without comprehensive cross-condition analysis.
- In-house datasets (ALG, IRQ, UAE) used for specialized model comparisons are not publicly available, making independent validation impossible.

## Confidence

**High Confidence**: The demonstration that training from scratch fails (55.63 WER-O vs 19.50 with curriculum) and that in-context learning is critical (WER doubling when reference audio is removed) are well-supported by direct ablation experiments.

**Medium Confidence**: The unified model's performance claims are moderately supported but limited by the ASR evaluation dependency and lack of comprehensive cross-condition testing.

**Low Confidence**: Claims about "state-of-the-art" performance and the general superiority of the unified approach over commercial solutions are weakly supported, relying on single-point comparisons without broader benchmarking.

## Next Checks

1. **Cross-condition generalization test**: Evaluate the unified model on MSA with noisy data and EGY with clean data to verify the claimed condition-specific advantages actually hold across data quality variations.

2. **Commercial system benchmarking**: Compare Habibi against additional commercial Arabic TTS providers (e.g., Google, Microsoft) and open-source systems (e.g., OpenArabicTTS) to validate the "state-of-the-art" claims beyond the single ElevenLabs comparison.

3. **Curriculum learning ablation**: Systematically test alternative curriculum orderings (e.g., high-resource dialect→MSA, or random ordering) to determine whether the specific MSA-first approach is genuinely optimal or just one of several viable strategies.