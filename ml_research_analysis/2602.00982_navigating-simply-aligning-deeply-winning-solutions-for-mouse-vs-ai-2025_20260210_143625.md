---
ver: rpa2
title: 'Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025'
arxiv_id: '2602.00982'
source_url: https://arxiv.org/abs/2602.00982
tags:
- visual
- training
- track
- neural
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the winning solutions for both tracks of the
  NeurIPS 2025 Mouse vs. AI competition, demonstrating that architectural simplicity
  combined with targeted enhancements can outperform complex deep networks for visual
  robustness while deeper architectures excel at neural alignment.
---

# Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025

## Quick Facts
- arXiv ID: 2602.00982
- Source URL: https://arxiv.org/abs/2602.00982
- Reference count: 39
- Primary result: Lightweight two-layer CNN with GLU and normalization outperforms deep ResNets for visual robustness (95.4% vs 65.98%), while 16-layer ResNet excels at neural alignment

## Executive Summary
This paper presents the winning solutions for both tracks of the NeurIPS 2025 Mouse vs. AI competition, demonstrating that architectural simplicity combined with targeted enhancements can outperform complex deep networks for visual robustness while deeper architectures excel at neural alignment. For Track 1 (Visual Robustness), a lightweight two-layer CNN with Gated Linear Units and observation normalization achieves 95.4% final score, outperforming complex architectures like 24-layer ResNet (65.98%) and 4-layer ResNet (87.70%). For Track 2 (Neural Alignment), a deep 16-layer ResNet architecture with GLU gating achieves top-1 neural prediction performance with 17.8 million parameters. Systematic analysis reveals non-monotonic performance, with optimal results around 200K steps rather than at convergence.

## Method Summary
The authors develop two distinct architectures optimized for different competition tracks. For Track 1, they implement a two-layer CNN with aggressive spatial downsampling (8×8 and 4×4 convolutions), Gated Linear Units for selective feature filtering, and running statistics observation normalization. Training follows a two-phase PPO procedure with 1.4M steps backbone training followed by 350K steps with GLU. For Track 2, they use a deep 16-layer ResNet with progressive channel expansion (64→128→256→512) and residual connections, trained with checkpointing every 20K steps to capture the non-monotonic relationship between training duration and neural alignment performance. Both architectures employ Gated Linear Units and are trained using PPO with specific hyperparameters including clipping, entropy regularization, and GAE advantage estimation.

## Key Results
- Track 1 SimpleCNN achieves 95.4% final score, outperforming 24-layer ResNet (65.98%) and 4-layer ResNet (87.70%)
- Observation normalization provides largest improvement (3.8 percentage points) for visual robustness
- Track 2 16-layer ResNet achieves top neural alignment with 17.8M parameters
- Non-monotonic training relationship shows optimal performance at ~200K steps, not convergence
- GLU modules contribute consistent 0.2 percentage point improvement across both tracks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Capacity constraints in shallow networks force generalizable feature learning, preventing overfitting to training-specific visual patterns.
- **Mechanism:** Limited representational capacity (1.4M parameters vs 8M+ in deep alternatives) prevents the network from developing highly specialized feature detectors that respond poorly to perturbed inputs. The model must learn features that remain stable across visual variations.
- **Core assumption:** The navigation task primarily requires detecting salient visual features (target color, shape, relative position) rather than building hierarchical object-part representations.
- **Evidence anchors:**
  - [abstract] "lightweight two-layer CNN...achieving 95.4% final score, outperforming complex architectures like 24-layer ResNet (65.98%)"
  - [Section 5.1] "The deep 24-layer ResNet exhibits a 30 percentage point gap (80.96% vs 51.00%), indicating that the model memorizes training-specific visual patterns rather than learning generalizable features. In contrast, our SimpleCNN maintains only a 2.8 percentage point gap."
  - [corpus] Related work on domain generalization from neural representations (FMR=0.53) supports the role of constrained representations for generalization, though direct mechanistic evidence is limited.
- **Break condition:** If the task requires multi-scale hierarchical processing (complex scene understanding, long-horizon planning), capacity constraints may become a bottleneck rather than a benefit.

### Mechanism 2
- **Claim:** Observation normalization provides the dominant improvement in visual robustness by achieving invariance to global illumination changes.
- **Mechanism:** Running statistics normalization (exponential moving average of channel-wise mean/std) rescales inputs to standardized distributions, directly addressing visual perturbations that manifest as illumination or contrast shifts—primary sources of distribution shift in the evaluation protocol.
- **Core assumption:** Visual perturbations in the evaluation set preserve semantic structure while primarily altering low-level statistics (brightness, contrast).
- **Evidence anchors:**
  - [abstract] "lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization"
  - [Section 5.3] "Observation normalization provides the largest single improvement, increasing final score from 91.6% to 95.4%, a gain of 3.8 percentage points."
  - [corpus] No directly comparable corpus evidence for this specific normalization mechanism in visual RL; evidence is internal to this paper.
- **Break condition:** If perturbations alter semantic content or spatial structure rather than global statistics, normalization alone will be insufficient.

### Mechanism 3
- **Claim:** Neural alignment benefits from increased model capacity to capture hierarchical biological representations, but exhibits non-monotonic relationship with training duration.
- **Mechanism:** Biological visual cortex employs rich hierarchical representations requiring substantial capacity (17.8M parameters) to approximate. However, extended training can cause overfitting to training stimuli or catastrophic forgetting of early-learned features that better match biological tuning properties.
- **Core assumption:** Biological visual representations are hierarchically organized with multi-scale features that benefit from deeper architectures with progressive channel expansion.
- **Evidence anchors:**
  - [abstract] "training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps"
  - [Section 5.2] "Model 368751 at 200K steps achieves rank 2, nearly matching the top model trained for 5.7× longer...Model 368628 at 80K steps outperforms model 369015 at 520K steps despite 6.5× less training."
  - [corpus] Related work on neural alignment from mouse visual cortex (FMR=0.53) supports hierarchical representation modeling but does not address training dynamics directly.
- **Break condition:** If the alignment metric changes or if different brain regions are targeted, the optimal training duration and architecture depth may shift substantially.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** Both tracks use PPO with specific hyperparameters (clip ε=0.2, GAE λ=0.95). Understanding clipping and advantage estimation is essential for debugging training instability.
  - **Quick check question:** Can you explain why PPO's clipped surrogate objective helps with training stability compared to vanilla policy gradient?

- **Concept: Gated Linear Units (GLU)**
  - **Why needed here:** GLU provides selective feature filtering via learned gating (sigmoid × Swish pathway). This is the consistent improvement across both tracks (+0.2 pp for robustness, key component for alignment).
  - **Quick check question:** How does the multiplicative gating in GLU differ from a standard activation function in terms of gradient flow?

- **Concept: Residual Connections for Gradient Flow**
  - **Why needed here:** Track 2's 16-layer architecture relies on skip connections to train effectively. The paper explicitly notes residual connections "facilitate gradient flow and feature reuse across layers."
  - **Quick check question:** Why do residual connections help with the vanishing gradient problem in deep networks?

## Architecture Onboarding

- **Component map:**
  Track 1: Input(86×155×1) → RunningNorm → Conv8×8(s=4,16ch) → LeakyReLU → Conv4×4(s=2,32ch) → LeakyReLU → Flatten → FC256 → GLU(256) → Policy/Value heads
  Track 2: Input → Conv4×4(s=4,64ch) → 4 stages (64→128→256→512 channels, each with 2-4 residual blocks) → GLU gating → Policy/Value heads

- **Critical path:**
  1. Implement observation normalization with EMA statistics first—this alone provides 3.8 pp improvement.
  2. Add GLU gating after backbone features; two-phase training (backbone first, then GLU) is recommended.
  3. For alignment tasks, systematically checkpoint every 20K steps and evaluate; do not assume later is better.

- **Design tradeoffs:**
  - Depth vs. robustness: Each added layer increases capacity for memorization (24-layer ResNet: 65.98% vs 2-layer: 95.4%).
  - Data augmentation: Naive augmentation decreased performance 27.9 pp (87.7%→59.8%) due to distribution mismatch with evaluation perturbations.
  - Recurrence: LSTM destroyed spatial structure and caused training collapse even from pretrained checkpoints.

- **Failure signatures:**
  - High ASR/low MSR gap (>20 pp): Overfitting to training distribution; reduce capacity or add normalization.
  - Training divergence with no improvement: Check if architecture has incompatible inductive biases (InceptionNet failed entirely).
  - Reward collapse after adding LSTM: Recurrent parameters enable overfitting to specific trajectories.

- **First 3 experiments:**
  1. Baseline SimpleCNN (no GLU, no normalization) to establish floor performance (~91.6%).
  2. Add observation normalization alone; expect +3.8 pp improvement to ~95.4%.
  3. Ablation: Compare checkpoints at 80K, 200K, 400K steps to verify non-monotonic training relationship for your specific task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the superiority of shallow architectures for visual robustness generalize to more complex navigation scenarios with richer visual scenes and longer planning horizons?
- Basis in paper: [explicit] The authors state "task specificity remains a concern" and suggest that "evaluating our architectural principles on other visual control tasks would establish their broader applicability."
- Why unresolved: The study is restricted to a specific foraging task with relatively simple visual structures, leaving the scalability of the simplicity principle unknown.
- What evidence would resolve it: Testing the lightweight SimpleCNN against deep ResNets on complex benchmarks (e.g., ProcGen) to verify if the robustness gap persists.

### Open Question 2
- Question: Can Transformer-based architectures with self-attention mechanisms provide a better trade-off between behavioral robustness and neural alignment than the CNN-based solutions presented?
- Basis in paper: [explicit] In the Limitations section, the authors note that "Transformer-based architectures... represent an interesting direction" and "warrants investigation" for robust visual RL.
- Why unresolved: The paper focused on convolutional architectures and gating (GLU), but did not evaluate attention-based models which may handle global feature dependencies differently.
- What evidence would resolve it: Training a Vision Transformer (ViT) agent on the same task and comparing its Track 1 (robustness) and Track 2 (alignment) scores against the winning models.

### Open Question 3
- Question: How does the integration of multimodal sensory inputs (e.g., proprioception, vestibular information) affect the relationship between task performance and biological plausibility?
- Basis in paper: [explicit] The authors state that extending architectures to handle "multimodal sensory input while maintaining robustness represents an important future challenge," as the current setup uses only vision.
- Why unresolved: Biological mice utilize multisensory integration, whereas the competition agents were restricted to egocentric visual observations.
- What evidence would resolve it: Training agents with additional non-visual observation streams and measuring whether the gap between behavioral success and neural correlation scores narrows.

## Limitations
- The paper relies on proprietary evaluation servers for Track 1 visual robustness testing and a restricted neural dataset for Track 2 alignment, limiting independent validation.
- The ASR-MSR gap interpretation assumes perturbations are primarily illumination-based rather than semantic, which is not explicitly confirmed.
- The non-monotonic training relationship for Track 2 is based on checkpoint averaging without error bars or variance analysis.

## Confidence
- Track 1 architectural superiority claims: **High** - well-supported by controlled ablation studies and clear performance gaps.
- Observation normalization mechanism: **Medium** - dominant effect shown but theoretical justification is limited.
- Track 2 optimal training duration claims: **Medium** - strong empirical pattern but lacks statistical uncertainty quantification.
- Failed approach documentation: **Low** - sparse details on hyperparameter configurations and convergence criteria for rejected methods.

## Next Checks
1. Replicate Track 1 architecture with synthetic perturbation distributions to verify observation normalization's invariance properties across different perturbation types.
2. Train multiple Track 2 models from scratch and collect full checkpoint trajectories to establish statistical significance of the non-monotonic performance pattern.
3. Systematically vary perturbation severity in Track 1 to map the boundary where capacity constraints become detrimental versus beneficial.