---
ver: rpa2
title: Quantifying Generalisation in Imitation Learning
arxiv_id: '2509.24784'
source_url: https://arxiv.org/abs/2509.24784
tags:
- labyrinth
- learning
- imitation
- agent
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Labyrinth, a new benchmarking environment
  designed to evaluate generalisation in imitation learning. Unlike existing benchmarks,
  Labyrinth offers precise control over environment structure, start and goal positions,
  and task complexity, enabling researchers to create verifiably distinct training,
  validation, and test sets.
---

# Quantifying Generalisation in Imitation Learning

## Quick Facts
- **arXiv ID:** 2509.24784
- **Source URL:** https://arxiv.org/abs/2509.24784
- **Reference count:** 40
- **Primary result:** Introduces Labyrinth, a new benchmarking environment for evaluating generalization in imitation learning with precise control over environment structure and task complexity.

## Executive Summary
This paper introduces Labyrinth, a novel benchmarking environment designed to evaluate generalization in imitation learning. Unlike existing benchmarks, Labyrinth offers precise control over environment structure, start and goal positions, and task complexity, enabling researchers to create verifiably distinct training, validation, and test sets. The environment includes variants like partial observability, key-and-door tasks, and ice-floor hazards to test different generalization aspects. Experiments show that common imitation learning methods struggle to generalize in Labyrinth, especially when faced with unseen structures or goal configurations, highlighting the need for more robust learning approaches.

## Method Summary
Labyrinth is a discrete, fully observable state space grid-world navigation environment where agents learn to navigate from start to goal positions while avoiding walls. The environment uses Johnson's algorithm to pre-compute optimal paths, providing ground-truth optimal actions for any state. Researchers can generate distinct training, validation, and test sets by manipulating wall structures, start positions, and goal positions independently. The benchmark includes six imitation learning methods (BC, DAgger, GAIL, BCO, SQIL, IUPE) trained on datasets with varying levels of structural similarity between train and test sets. Performance is measured using Average Episodic Reward and Success Ratio across different generalization scenarios.

## Key Results
- Common imitation learning methods (BC, DAgger, GAIL, BCO, SQIL, IUPE) show poor generalization when tested on structurally distinct environments
- Agents trained on biased action distributions (fixed start/goal positions) fail completely when tested on unbiased distributions
- ResNet-18 encoders show improved generalization over standard CNN encoders, suggesting representation quality matters
- Zero success rates are achieved when both start and goal positions change during testing, indicating fundamental limitations in current IL approaches

## Why This Works (Mechanism)

### Mechanism 1: Structural Distribution Shift
- **Claim:** Explicitly separating training and evaluation structures prevents agents from succeeding via trajectory memorization, thereby isolating true generalization capabilities.
- **Mechanism:** Labyrinth represents environments as graphs. By generating distinct wall configurations (transition dynamics) and start/goal positions for test sets, the environment forces the agent to query its internal policy logic rather than retrieving cached action sequences.
- **Core assumption:** An agent that has learned the concept of navigation should perform significantly better on unseen structures than one that has merely memorized training trajectories.
- **Evidence anchors:**
  - [abstract] "enabling researchers to create verifiably distinct training, validation, and test sets."
  - [Section 3] "...if the agent only learns to find the closest state to the training data, and perform the same action, there will be labyrinth settings it will not solve."
  - [corpus] "FOSSIL" highlights that IL agents risk replicating errors without generalizing, supporting the need for structural stress-testing.
- **Break condition:** If an agent achieves high success rates on test sets with radically different structures without specific architectural interventions, the mechanism may be failing to introduce sufficient complexity.

### Mechanism 2: Optimality-Grounded Debugging
- **Claim:** Providing access to the discrete ground-truth optimal action for any state allows for precise attribution of failure modes (policy error vs. execution error).
- **Mechanism:** The environment utilizes Johnson's algorithm to pre-compute all possible paths. When an agent takes an action, it can be immediately compared against the known optimal set.
- **Core assumption:** Deviations from the calculated optimal path are indicative of policy brittleness, provided the state representation is fully observable.
- **Evidence anchors:**
  - [Section 2.4] "Line 8 provides all possible solutions for that Labyrinth..."
  - [Section 3] "Labyrinth's explicit structure and ground-truth optimality afford a level of transparency... making it especially suitable for interpretability, policy debugging..."
- **Break condition:** If the reward function does not penalize sub-optimal paths, agents may learn "lazy" but successful strategies that complicate optimality analysis.

### Mechanism 3: Action Distribution Disentanglement
- **Claim:** Independent manipulation of start/goal positions and wall structures allows researchers to decouple "learning the dynamics" from "learning the task."
- **Mechanism:** The "biased" setting (start bottom-left, goal top-right) reinforces a dominant action distribution. The "unbiased" setting randomizes this. By keeping walls static but changing start/goal, one can test if an agent has learned a rigid policy or a flexible goal-conditioned one.
- **Core assumption:** A drop in performance when flipping from biased to unbiased settings indicates the agent was relying on action priors rather than state-value estimation.
- **Evidence anchors:**
  - [Section 2.1] "Biased structures will maintain the action distribution similar... while unbiased ones will require the agent to focus more on each state."
  - [Figure 2] Visualizes the shift from biased action distributions to uniform distributions.
- **Break condition:** If an agent uses a navigation-specific architecture (e.g., A* or value iteration networks), it may ignore action priors entirely, rendering this specific test for statistical overfitting less effective.

## Foundational Learning

- **Concept: Behavioral Cloning (BC) vs. Inverse Reinforcement Learning (IRL)**
  - **Why needed here:** The paper benchmarks both pure supervised approaches (BC) and adversarial/reward-learning approaches (GAIL, SQIL). Understanding the distinction is required to interpret why BC with ResNet performed better vs. why IRL methods struggled.
  - **Quick check question:** Does the method require interacting with the environment during training (Online/IRL) or just the dataset (Offline/BC)?

- **Concept: Covariate Shift**
  - **Why needed here:** This is the core failure mode Labyrinth exposes. As the agent makes mistakes, it drifts into states not present in the expert demonstrations. Labyrinth systematically generates these "unseen states" (new maze structures) to measure this fragility.
  - **Quick check question:** If the training data consists only of optimal paths, how will the agent react when a wall forces it into a non-optimal state during testing?

- **Concept: Partial Observability & POMDPs**
  - **Why needed here:** Labyrinth offers a "partially observable" variant where the agent only sees local surroundings. This tests if the agent can maintain an internal belief state (memory) or if it relies on "state-matching" heuristics.
  - **Quick check question:** In the partially observable setting, can the agent infer the global structure of the maze from a sequence of local observations?

## Architecture Onboarding

- **Component map:** Environment Core (Gymnasium) -> Solver (Johnson's Algorithm) -> Renderer -> Datasets (HuggingFace)
- **Critical path:**
  1. Install `labyrinth` package
  2. Instantiate environment with `gym.make("Labyrinth-v0", shape=(5, 5), ...)`
  3. Select evaluation mode: (a) Change structure only, (b) Change start/goal only, or (c) Change both
  4. Run imitation learning agent (e.g., BC, DAgger)
  5. Compare Success Rate (SR) on train vs. test splits to quantify generalization gap
- **Design tradeoffs:**
  - **Discrete vs. Continuous:** The environment is strictly discrete (grid-based). This excludes continuous-action IL methods (like OPOLO, CILO) mentioned in the paper's limitations.
  - **Solvability Guarantee:** The generator guarantees a solution exists for every maze. This ensures failure is due to the agent's limitations, not impossible environments.
  - **Visual Fidelity:** Uses simple shapes (diamonds, lines). Low visual complexity focuses the benchmark on reasoning/planning generalization rather than visual robustness.
- **Failure signatures:**
  - **High Train / Zero Test SR:** The agent has memorized trajectories; total failure to generalize (observed in BC with CNN)
  - **Wall Collisions:** The agent has not learned the transition dynamics (wall constraints)
  - **Looping:** The agent lacks a goal representation and is executing a random/wall-following policy
- **First 3 experiments:**
  1. **Baseline Generalization Check:** Train a standard CNN-based Behavioral Cloning agent on the biased 5x5 training set. Evaluate on the test set. Expected result: High train SR, near-zero test SR (Memorization).
  2. **Architecture Ablation:** Replace the CNN encoder with a ResNet-18 (as per paper). Compare the delta in Test SR. Hypothesis: Better representations improve generalization but do not solve the underlying reasoning gap.
  3. **Distribution Shift Stress Test:** Train on unbiased mazes (random start/goal). Test on fixed start/goal mazes vs. random start/goal mazes. Check if the agent relies on action priors (Up/Right bias) or actual state-to-goal distance estimation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Labyrinth environment be extended to support continuous action spaces while retaining its discrete graph structure and capacity for computing optimal actions?
- Basis in paper: [explicit] The authors identify this as a limitation, noting that "Labyrinth only allows for discrete actions," which precludes the evaluation of methods designed for continuous control like OPOLO or CILO.
- Why unresolved: The current graph-based structure and optimality definitions rely on discrete transitions, creating a conflict with the continuous dynamics required by other algorithms.
- What evidence would resolve it: An updated version of the environment incorporating continuous movement mechanics (e.g., point-mass navigation) alongside a defined metric for optimal continuous trajectories.

### Open Question 2
- Question: What inductive biases or training paradigms are necessary for imitation learning agents to achieve non-zero generalization when both the starting position and goal configuration are novel?
- Basis in paper: [explicit] In the 5x5 test split where both s0 and g were changed, every benchmarked method (BC, DAgger, GAIL, etc.) failed completely with a 0% success rate.
- Why unresolved: Current methods rely on matching local observations to expert distributions rather than reasoning about global structural changes or novel goal locations.
- What evidence would resolve it: An algorithm that achieves statistically significant success rates (>10%) on the "s0 and g" split without being explicitly trained on those specific goal distributions.

### Open Question 3
- Question: Does increasing the capacity of the visual encoder (e.g., switching from CNN to ResNet-18) encourage learning the underlying navigation task, or does it merely improve the memorization of spurious correlations?
- Basis in paper: [inferred] The authors note that while a ResNet-18 encoder improved test performance significantly (to 53%), they believe this resulted from learning "less spurious encoding" rather than the underlying task, as performance remained imperfect and highly architecture-dependent.
- Why unresolved: It is unclear if the remaining generalization gap is due to insufficient model capacity or a fundamental flaw in the behavioral cloning objective's ability to capture spatial relationships.
- What evidence would resolve it: A mechanistic interpretability study analyzing whether the encoder learns wall-detection neurons and spatial mapping features versus simple trajectory-matching patterns.

## Limitations
- The benchmark focuses on discrete-action imitation learning methods, limiting applicability to continuous-action domains like robotics or autonomous driving
- While Labyrinth enables controlled distribution shifts, it remains unclear whether improvements in this domain translate to real-world environments with continuous states and noisy observations
- The paper's claim that ResNet-18 improves generalization over CNN shows promise but lacks ablation studies to isolate architectural benefits from representation capacity differences

## Confidence
- **High confidence:** The core contribution of providing verifiable, structurally distinct train/test splits for IL generalization is well-supported and technically sound
- **Medium confidence:** The claim that current IL methods fail to generalize in Labyrinth is supported by experimental results, but the severity of failure may depend on specific architectural choices not fully explored
- **Medium confidence:** The assertion that Labyrinth enables precise failure mode attribution through optimal action access is valid but assumes the reward function properly penalizes sub-optimal paths

## Next Checks
1. **Architectural ablation study:** Train BC with varying encoder capacities (CNN, ResNet-18, ResNet-50) on the same dataset to isolate representation benefits from model complexity
2. **Distribution shift analysis:** Generate datasets where only start positions change while walls remain constant, then only goal positions change while walls remain constant, to quantify which structural component most affects generalization
3. **Real-world transferability test:** Apply an agent trained on Labyrinth to a similar grid-based navigation task with different visual appearance or noise characteristics to assess practical generalization capabilities