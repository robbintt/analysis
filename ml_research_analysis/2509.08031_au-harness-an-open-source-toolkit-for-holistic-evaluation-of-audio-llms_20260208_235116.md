---
ver: rpa2
title: 'AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs'
arxiv_id: '2509.08031'
source_url: https://arxiv.org/abs/2509.08031
tags:
- evaluation
- audio
- tasks
- language
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AU-Harness is an open-source toolkit for efficient and comprehensive
  evaluation of Large Audio Language Models (LALMs). The framework addresses key limitations
  in existing evaluation tools by providing optimized batch processing and parallel
  execution, achieving up to 127% speedup over competing frameworks.
---

# AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs

## Quick Facts
- arXiv ID: 2509.08031
- Source URL: https://arxiv.org/abs/2509.08031
- Authors: Sidharth Surapaneni; Hoang Nguyen; Jash Mehta; Aman Tiwari; Oluwanifemi Bamgbose; Akshay Kalkunte; Sai Rajeswar; Sathwik Tejaswi Madhusudhan
- Reference count: 10
- One-line primary result: Open-source toolkit achieving up to 127% speedup for comprehensive LALM evaluation across 380+ tasks

## Executive Summary
AU-Harness is an open-source framework designed to address critical limitations in Large Audio Language Model (LALM) evaluation. The toolkit introduces optimized batch processing and parallel execution mechanisms that achieve up to 127% speedup over competing frameworks. It provides standardized prompting protocols and flexible configurations for fair model comparison across diverse scenarios, while uniquely supporting novel evaluation categories including LLM-Adaptive Diarization for temporal audio understanding and Spoken Language Reasoning for complex audio-based cognitive tasks.

The framework reveals significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning, while identifying instruction modality standardization issues that create performance differences up to 9.5 absolute points on complex instruction-following tasks. Through evaluation across 380+ tasks spanning six major categories, AU-Harness advances systematic LALM development by providing both practical evaluation tools and insights into model limitations.

## Method Summary
AU-Harness employs a token-based request scheduling system with centralized concurrency control to enable parallel evaluation without bottlenecking on individual model endpoints. The framework uses proportional dataset sharding to distribute evaluation workload across heterogeneous endpoints, achieving near-linear throughput scaling. Standardized prompting protocols are implemented to reduce evaluation variance, though the framework reveals systematic performance gaps between audio and text instruction modalities. The toolkit supports six major evaluation categories through concurrent engines including ASR-WER, Emotion-LLM-Judge, and WDER computation, with GPT-4o-mini serving as the primary LLM judge for novel temporal understanding tasks.

## Key Results
- Achieves up to 127% speedup over competing frameworks through optimized batch processing and parallel execution
- Identifies performance degradation up to 9.5 absolute points when converting text benchmarks to audio instructions
- Reveals significant gaps in current LALMs for temporal understanding and complex spoken language reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-based request scheduling with centralized concurrency control enables parallel evaluation without bottlenecking on individual model endpoints.
- Mechanism: A Central Request Controller maintains a global pool of concurrency slots (tokens). Each engine-specific requester draws tokens before dispatch and returns them on completion, allowing user-defined request limits to govern throughput rather than model-specific constraints.
- Core assumption: Bottlenecks in LALM evaluation arise from uncoordinated request dispatch rather than raw model inference speed.
- Evidence anchors: [abstract] "achieving up to 127% speedup over competing frameworks through optimized batch processing and parallel execution"; [Section 4.1] "Each concurrent engine-specific requester periodically draws from the global pool... inference calls dispatched in parallel to fully exploit available computational resources"; [corpus] No direct validation in neighboring papers; mechanism is framework-specific.
- Break condition: If endpoints have highly variable latency or frequent failures, token pools may underutilize capacity or require excessive retries.

### Mechanism 2
- Claim: Proportional dataset sharding distributes evaluation workload across heterogeneous endpoints to achieve near-linear throughput scaling.
- Mechanism: Datasets are partitioned into disjoint subsets proportional to each endpoint's concurrent request capacity, minimizing idle time and balancing load.
- Core assumption: Endpoint capacity remains stable and predictable during evaluation runs.
- Evidence anchors: [Section 4.1] "sharding is performed proportionally to each endpoint's capacity for concurrent requests, ensuring balanced utilization"; [Table 2] Shows 95.19% increase in Processed Samples per Second over best baseline; [corpus] SAKURA paper mentions multi-hop reasoning but does not validate sharding.
- Break condition: If endpoints have divergent or fluctuating throughput, sharding can cause stragglers and load imbalance.

### Mechanism 3
- Claim: Standardized prompting protocols reduce evaluation variance, but instruction modality (audio vs text) creates systematic performance gaps up to 9.5 absolute points.
- Mechanism: Unified prompt templates and configurations improve reproducibility; however, converting text benchmarks to audio exposes modality sensitivity in complex instruction-following tasks.
- Core assumption: Prompt sensitivity is systematic enough that standardization improves cross-model comparability without erasing meaningful differences.
- Evidence anchors: [abstract] "performance differences up to 9.5 absolute points on the challenging complex instruction following downstream tasks"; [Table 6] Shows consistent degradation from text to audio instructions across Speech-IFEval, Speech-BFCL, Speech-Spider, and Speech-MTBench; [corpus] ISA-Bench validates that "existing LALMs are highly sensitive to how instructions are phrased."
- Break condition: If canonical prompts bias toward specific model behaviors, standardization may obscure true capability differences.

## Foundational Learning

- Concept: Real-time Factor (RTF)
  - Why needed here: Core efficiency metric for evaluation frameworks; RTF = processing_time / audio_duration. Lower is better.
  - Quick check question: If processing 1 hour of audio takes 3.6 hours, what is the RTF?

- Concept: vLLM Batching
  - Why needed here: Underlying inference optimization enabling throughput gains; continuous batching reduces padding overhead.
  - Quick check question: Why does continuous batching improve throughput over static batching for autoregressive models?

- Concept: Word Diarization Error Rate (WDER)
  - Why needed here: Novel metric for LLM-Adaptive Diarization combining transcription and speaker assignment errors at word level.
  - Quick check question: How does WDER differ from traditional Diarization Error Rate (DER)?

## Architecture Onboarding

- Component map:
  - Config: Hierarchical definitions of tasks, datasets, models, metrics, and prompting templates.
  - Request Controller: Global token pool manager with retry logic and timeout handling.
  - Concurrent Engines: Parallel evaluators (ASR-WER, Emotion-LLM-Judge, etc.) interfacing with model endpoints.

- Critical path:
  1. Define evaluation config (datasets → models → metrics → prompts).
  2. Request Controller initializes token pool based on endpoint capacities.
  3. Engines shard datasets proportionally.
  4. Parallel dispatch with adaptive retries.
  5. Aggregate scores and generate metric reports.

- Design tradeoffs:
  - vLLM dependency yields high throughput for supported backends; models without mature backends fall back to slower execution.
  - Standardized prompts improve reproducibility but may not reflect model-specific optimal prompting.
  - WDER/cpWER are practical for LLM output but imperfect under overlapping speech or rapid speaker transitions.

- Failure signatures:
  - High retry counts with timeouts → endpoint capacity mismatched to token pool size.
  - Run-to-run variance → non-deterministic endpoints or external rate limiting.
  - Extremely high WDER on diarization → LALM temporal precision limitations (acknowledged in Section 4.3).

- First 3 experiments:
  1. Baseline efficiency: Run 500 LibriSpeech-test-clean samples through AU-Harness vs a sequential baseline; verify RTF reduction matches claimed ~48-59%.
  2. Modality gap validation: Evaluate a single LALM on text vs audio variants of Speech-IFEval; confirm 5-10 point degradation.
  3. Multi-model concurrency: Run 2-3 models in parallel with sharding enabled; verify balanced completion times and throughput scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the instruction modality (audio vs. text) systematically affect the reasoning performance and robustness of Large Audio Language Models?
- Basis in paper: [explicit] The authors observe a performance degradation of up to 9.5 points when using audio instructions versus text (Section 5.2) and explicitly state that a "thorough reassessment of different instruction modality is needed."
- Why unresolved: Current benchmarks often convert text tasks to audio without accounting for the cognitive differences in processing spoken versus written instructions, masking potential model weaknesses.
- What evidence would resolve it: A comprehensive study isolating instruction modality as a variable across diverse reasoning tasks and architectures to determine if the gap is model-specific or task-inherent.

### Open Question 2
- Question: Can specialized evaluation metrics be developed to better capture temporal precision in LLM-Adaptive Diarization beyond current word-level proxies?
- Basis in paper: [explicit] The Limitations section states that using word-level metrics (WDER, cpWER) as proxies for temporal precision "remains imperfect under speech overlap or rapid transitions."
- Why unresolved: LALMs struggle with precise timestamp generation, and current metrics may not sufficiently penalize timing errors in complex audio streams.
- What evidence would resolve it: The creation and validation of a novel metric that correlates more strongly with ground-truth temporal accuracy in scenarios involving overlapping speech.

### Open Question 3
- Question: To what extent does high performance on standardized benchmarks like AU-Harness correlate with real-world capabilities in noisy, unstructured environments?
- Basis in paper: [explicit] The authors note in the Limitations that the relationship between standardized performance and real-world capabilities "where contexts are noisier... requires further empirical validation."
- Why unresolved: Benchmarks often rely on cleaner datasets than those found in deployment, potentially failing to expose robustness issues in "wild" audio conditions.
- What evidence would resolve it: Empirical studies demonstrating a strong correlation between AU-Harness scores and performance metrics on diverse, uncontrolled, real-world audio data.

### Open Question 4
- Question: How can cross-institutional reproducibility be achieved given the reliance on rate-limited, closed-source APIs and non-deterministic backend queueing?
- Basis in paper: [explicit] The Limitations section highlights that "runs may vary due to endpoint queueing" and explicit documentation of capacity is required for comparability.
- Why unresolved: Variance introduced by external APIs and infrastructure differences makes strict reproducibility difficult for researchers without access to identical hardware or priority tiers.
- What evidence would resolve it: A standardized protocol or framework extension that normalizes for latency and rate-limiting variances, demonstrating consistent results across different hardware setups.

## Limitations

- The reliance on LLM judges (primarily GPT-4o-mini) for temporal understanding tasks introduces potential circularity where models are judged by another model with similar limitations
- Standardized prompting protocols may systematically underestimate certain models' capabilities by obscuring optimal prompting strategies specific to individual architectures
- Hardware and infrastructure dependencies mean the claimed 127% speedup may not translate equally across different environments, particularly for models without mature vLLM backends

## Confidence

- **Efficiency Claims (127% speedup):** Medium confidence. The mechanism is well-specified (token-based request scheduling, proportional sharding), and the core idea is validated by the throughput metrics. However, the exact speedup depends on specific hardware configurations and model backends that aren't fully detailed.
- **Standardization Benefits:** High confidence. The systematic performance degradation from text to audio modalities is consistently observed across multiple benchmarks (Speech-IFEval, Speech-BFCL, Speech-Spider, Speech-MTBench), and the mechanism (prompt sensitivity) is well-supported by ISA-Bench validation.
- **Novel Evaluation Categories:** Medium confidence. The categories address genuine gaps in LALM evaluation, but the reliance on LLM judges for temporal understanding tasks introduces potential bias. The WDER metric is practical but may not capture all aspects of diarization quality.

## Next Checks

1. **Infrastructure Portability Test:** Reproduce the efficiency benchmarks on three different hardware configurations (varying GPU types, CPU-only setups, and cloud environments) to verify that the 127% speedup claim holds across diverse infrastructure rather than being specific to the original evaluation setup.

2. **Ground Truth Diarization Validation:** Compare LLM-Adaptive Diarization results against human-annotated diarization datasets to quantify the gap between model-based WDER and human-grounded temporal understanding. This would reveal whether the observed limitations reflect true model deficiencies or artifacts of LLM-based evaluation.

3. **Prompt Optimization Study:** Systematically compare standardized prompts against model-specific optimal prompts for a subset of tasks to quantify the trade-off between reproducibility and performance. This would reveal whether the 9.5-point modality gap represents a fundamental limitation or an artifact of conservative prompt standardization.