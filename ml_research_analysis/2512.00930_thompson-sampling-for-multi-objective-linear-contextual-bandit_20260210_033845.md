---
ver: rpa2
title: Thompson Sampling for Multi-Objective Linear Contextual Bandit
arxiv_id: '2512.00930'
source_url: https://arxiv.org/abs/2512.00930
tags:
- pareto
- regret
- round
- reward
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first Thompson Sampling algorithm with
  Pareto regret guarantees for multi-objective linear contextual bandits. The key
  idea is an optimistic sampling strategy that samples parameters for each objective
  and selects arms from an "effective Pareto front" that accounts for cumulative rewards
  under repeated selections.
---

# Thompson Sampling for Multi-Objective Linear Contextual Bandit

## Quick Facts
- arXiv ID: 2512.00930
- Source URL: https://arxiv.org/abs/2512.00930
- Reference count: 40
- Primary result: First Thompson Sampling algorithm with Pareto regret guarantees for multi-objective linear contextual bandits

## Executive Summary
This paper introduces MOL-TS, the first Thompson Sampling algorithm with theoretical Pareto regret guarantees for multi-objective linear contextual bandits. The key innovation is an optimistic sampling strategy that accounts for cumulative rewards under repeated arm selections. The algorithm achieves a worst-case Pareto regret bound of eO(d^(3/2)√T), matching the best known results for randomized linear bandit algorithms in the single-objective setting. Empirical results demonstrate superior regret minimization and strong multi-objective performance compared to UCB and ε-greedy baselines.

## Method Summary
The proposed MOL-TS algorithm addresses the challenge of multi-objective linear contextual bandits by sampling parameters for each objective and selecting arms from an "effective Pareto front" that considers cumulative rewards from repeated selections. The optimistic sampling strategy is the core innovation that enables theoretical guarantees. The algorithm maintains posterior distributions over linear parameters and uses these to construct optimistic estimates that balance exploration and exploitation across multiple objectives simultaneously. This approach allows MOL-TS to achieve Pareto regret bounds that match the state-of-the-art for single-objective bandit problems.

## Key Results
- First Thompson Sampling algorithm with Pareto regret guarantees for multi-objective linear contextual bandits
- Achieves worst-case Pareto regret bound of eO(d^(3/2)√T)
- Outperforms UCB and ε-greedy baselines in both regret minimization and multi-objective performance
- Matches the best known regret bounds for randomized linear bandit algorithms in single-objective settings

## Why This Works (Mechanism)
The algorithm's effectiveness stems from its optimistic sampling strategy that explicitly accounts for cumulative rewards when selecting from the Pareto front. By sampling parameters for each objective and considering the long-term impact of repeated arm selections, MOL-TS can balance exploration and exploitation across multiple objectives simultaneously. The theoretical guarantees are achieved through careful analysis of the algorithm's interaction with the effective Pareto front, showing that the optimistic estimates lead to optimal regret bounds.

## Foundational Learning
- **Multi-objective optimization**: Required to understand the Pareto front concept and how to balance multiple competing objectives simultaneously. Quick check: Can identify and explain Pareto optimality in a simple two-objective scenario.
- **Linear contextual bandits**: Essential for grasping the problem formulation and how context influences arm selection. Quick check: Can derive the regret bound for standard linear bandit algorithms.
- **Thompson Sampling**: Core algorithmic framework that needs to be understood for adapting to the multi-objective setting. Quick check: Can explain the Bayesian update mechanism in standard Thompson Sampling.
- **Regret analysis**: Critical for understanding the theoretical guarantees and their implications. Quick check: Can compute and compare regret bounds for different bandit algorithms.
- **Effective Pareto front**: Novel concept introduced to handle repeated selections and cumulative rewards. Quick check: Can describe how this differs from the standard Pareto front in multi-objective optimization.

## Architecture Onboarding

**Component map**: Context features -> Linear parameter sampling -> Optimistic estimates -> Effective Pareto front -> Arm selection -> Reward feedback -> Posterior update

**Critical path**: The algorithm proceeds through context reception, parameter sampling for each objective, construction of optimistic estimates, selection from the effective Pareto front, reward observation, and Bayesian posterior updates. This cycle repeats for each time step.

**Design tradeoffs**: The main tradeoff is between computational complexity (maintaining and updating the effective Pareto front) and theoretical guarantees. The algorithm sacrifices some computational efficiency for strong regret bounds.

**Failure signatures**: Potential failures include: (1) Linear reward assumption violation leading to degraded performance, (2) Computational intractability for large action spaces due to Pareto front maintenance, (3) Suboptimal performance in highly non-linear multi-objective settings.

**3 first experiments**: 1) Compare MOL-TS regret against UCB and ε-greedy on synthetic linear multi-objective problems with varying dimensions. 2) Test algorithm scalability by increasing the number of arms and objectives. 3) Evaluate sensitivity to prior choices in the Bayesian updating mechanism.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes linear reward functions and may not extend to non-linear settings
- Computational complexity of maintaining the effective Pareto front could be prohibitive for large action spaces
- Empirical evaluation is limited to synthetic data and doesn't fully capture real-world multi-objective optimization performance

## Confidence
- Theoretical claims: High
- Empirical results: Medium
- Real-world applicability: Low

## Next Checks
1. Evaluate MOL-TS on real-world multi-objective optimization problems with non-linear reward functions
2. Compare computational efficiency and scalability with alternative Pareto optimization methods for large action spaces
3. Conduct ablation studies to quantify the impact of the optimistic sampling strategy on regret bounds and practical performance