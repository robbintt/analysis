---
ver: rpa2
title: Temporal Sampling for Forgotten Reasoning in LLMs
arxiv_id: '2505.20196'
source_url: https://arxiv.org/abs/2505.20196
tags:
- sampling
- temporal
- training
- checkpoint
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often forget solutions they once knew during
  fine-tuning, with 6.4%-56.1% of correct answers becoming incorrect by the final
  checkpoint. This "temporal forgetting" occurs across model sizes, architectures,
  and training methods (both RL and SFT).
---

# Temporal Sampling for Forgotten Reasoning in LLMs

## Quick Facts
- arXiv ID: 2505.20196
- Source URL: https://arxiv.org/abs/2505.20196
- Authors: Yuetai Li; Zhangchen Xu; Fengqing Jiang; Bhaskar Ramasubramanian; Luyao Niu; Bill Yuchen Lin; Xiang Yue; Radha Poovendran
- Reference count: 40
- Key outcome: Temporal Sampling achieves 4-19 point gains in Pass@k by sampling from multiple training checkpoints, recovering forgotten reasoning solutions

## Executive Summary
Large language models often forget solutions they once knew during fine-tuning, with 6.4%-56.1% of correct answers becoming incorrect by the final checkpoint. This "temporal forgetting" occurs across model sizes, architectures, and training methods (both RL and SFT). To address this, we introduce Temporal Sampling—an inference-time strategy that draws outputs from multiple checkpoints during training rather than just the final one. This recovers forgotten solutions without retraining or ensembling. Temporal Sampling achieves substantial improvements across reasoning benchmarks, with gains of 4-19 points in Pass@k and consistent improvements in Majority@k and Best-of-N. We further extend this to LoRA-adapted models, maintaining performance benefits with minimal storage overhead.

## Method Summary
Temporal Sampling is an inference-time strategy that leverages intermediate training checkpoints to recover forgotten reasoning solutions. During training, checkpoints are saved at regular intervals (typically 8 checkpoints). At inference, k samples are distributed across t checkpoints using round-robin allocation (k/t samples per checkpoint). This approach recovers solutions that were correct at intermediate checkpoints but forgotten by the final model. The method extends to LoRA adapters for storage efficiency, achieving similar benefits with reduced overhead. Key hyperparameters include temperature=0.6, top_p=0.95, and max_tokens=16384 for inference sampling.

## Key Results
- Temporal Sampling achieves 4-19 point gains in Pass@k across reasoning benchmarks
- 6.4%-56.1% of final errors were once correct at intermediate checkpoints (PTFS metric)
- Requires only k=5 samples to reach 22.5% pass rate vs. k=64 for single checkpoint
- Consistent improvements across Majority@k and Best-of-N metrics
- LoRA extension maintains performance benefits with minimal storage overhead

## Why This Works (Mechanism)

### Mechanism 1: Cross-Checkpoint Answer Diversity
Sampling from multiple training checkpoints provides greater answer diversity than sampling from a single checkpoint alone. Individual problems exhibit varying pass rates across training checkpoints, and by distributing inference samples across checkpoints in round-robin fashion, the sampling space widens beyond what any single checkpoint can produce.

### Mechanism 2: Recovery of Transiently-Correct Reasoning Paths
Some correct solutions exist only at intermediate checkpoints and are lost by training's end; temporal sampling recovers these. During RL/SFT, individual problem correctness oscillates—questions frequently alternate between "Improve" and "Forget" events. The Temporal Forgetting Score (PTFS) measures this phenomenon.

### Mechanism 3: Efficient Compute-Performance Tradeoff
Temporal sampling reaches target performance levels with fewer total samples than final-checkpoint-only sampling. By accessing competencies distributed across checkpoints, fewer samples are needed to find at least one correct solution. The paper reports 22.5% pass rate with k=5 samples (temporal) vs. requiring k=64 samples (single checkpoint) for equivalent performance.

## Foundational Learning

- **Pass@k metric**: Probability of getting at least one correct answer in k samples. Why needed: The paper introduces Pass@k|t, extending Pass@k to multi-checkpoint sampling; understanding the base metric is prerequisite. Quick check: If a model answers correctly 30% of the time on a problem, what is Pass@8?
- **Checkpoint saving during training**: Temporal sampling requires access to intermediate training states. Why needed: The method depends on checkpoint frequency and selection strategy. Quick check: How many checkpoints did the paper save during RL training, and how were they selected for sampling?
- **LoRA (Low-Rank Adaptation)**: The paper extends temporal sampling to LoRA checkpoints for storage efficiency. Why needed: Understanding adapter weights vs. full parameters matters for implementation. Quick check: What is the storage difference between full-parameter checkpoints and LoRA adapter checkpoints?

## Architecture Onboarding

- **Component map**: Training loop -> Checkpoint saver (saves 8 checkpoints) -> Checkpoint pool -> Temporal sampler (round-robin) -> Inference outputs -> Aggregation (Majority@k, Best-of-N, or Pass@k) -> Optional: LoRA adapter storage instead of full model weights
- **Critical path**: 1) During training: save checkpoints at regular intervals 2) At inference: load checkpoint pool 3) Distribute k samples across t checkpoints using round-robin 4) Aggregate results using target metric
- **Design tradeoffs**: More checkpoints (higher t) -> better coverage but more storage/loading overhead; LoRA vs. full weights: LoRA reduces storage significantly but may have slightly different dynamics; Checkpoint selection: paper uses most recent t checkpoints
- **Failure signatures**: If Pass@k|t ≈ Pass@k (no improvement): checkpoints may be too similar, or problem domain doesn't exhibit temporal forgetting; If storage exceeds budget: switch to LoRA checkpoints or reduce t; If latency too high: pre-load checkpoints or reduce checkpoint count
- **First 3 experiments**: 1) Baseline measurement: Compute Pass@k and Maj@k on final checkpoint alone; calculate PTFS to quantify temporal forgetting 2) Checkpoint sweep: Test t ∈ {2, 4, 8} with round-robin sampling; plot Pass@k|t vs. k 3) LoRA validation: If storage-constrained, repeat experiment 2 with LoRA adapter checkpoints

## Open Questions the Paper Calls Out

### Open Question 1
Can the performance gains from Temporal Sampling at Pass@k|t be transferred to single-checkpoint single-sample performance (Pass@1|1)? The authors state "investigating methods to transfer the performance gains from Pass@k|t to Pass@1|1 is a promising avenue."

### Open Question 2
Does the Temporal Forgetting phenomenon generalize to non-mathematical domains such as code generation, automated theorem proving, and healthcare applications? The limitations section states "We have not yet extended our analysis to other potentially relevant domains where similar patterns might emerge."

### Open Question 3
What is the optimal strategy for distributing k samples across t checkpoints beyond round-robin allocation? The authors note "alternative distribution approaches represent a promising avenue that we reserve for subsequent research."

### Open Question 4
Can a theoretical framework explain why specific questions oscillate between correct and incorrect states during training while others remain stable? Future work calls for "developing a more comprehensive theoretical framework for learning and forgetting dynamics."

## Limitations
- The mechanism relies on the assumption that checkpoint diversity meaningfully differs from temperature-based diversity, but this comparison is not directly validated
- Forgotten solution quality is assumed to represent genuine reasoning capability rather than transient or noisy predictions
- Compute-efficiency claims are specific to the DeepScaleR-4k dataset and GRPO training setup and may not generalize

## Confidence
- **High Confidence**: Temporal forgetting is a real phenomenon observed across multiple architectures, model sizes, and training methods
- **Medium Confidence**: Temporal Sampling effectively recovers forgotten solutions and improves performance metrics
- **Low Confidence**: The compute-performance tradeoff claims generalize beyond the specific experimental setup

## Next Checks
1. Conduct controlled experiments comparing Temporal Sampling's performance against temperature-based diversity sampling on the same final checkpoint to measure whether checkpoint-based diversity provides unique benefits
2. Implement confidence scoring and stability analysis for recovered solutions to determine if temporal sampling recovers stable reasoning or transient artifacts
3. Test Temporal Sampling on reasoning benchmarks outside the DeepScaleR-4k domain to validate whether temporal forgetting and recovery mechanisms are universal or domain-specific