---
ver: rpa2
title: Evaluating LLM Understanding via Structured Tabular Decision Simulations
arxiv_id: '2511.10667'
source_url: https://arxiv.org/abs/2511.10667
tags:
- gemini-2
- decision
- gpt-4
- mini
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces STaDS, a protocol for evaluating LLMs' understanding
  ability through structured tabular decision simulations. The framework measures
  comprehension fidelity, predictive competence, and decision faithfulness by assessing
  whether models can follow instructions, produce accurate predictions, and rely on
  relevant decision factors.
---

# Evaluating LLM Understanding via Structured Tabular Decision Simulations

## Quick Facts
- **arXiv ID:** 2511.10667
- **Source URL:** https://arxiv.org/abs/2511.10667
- **Reference count:** 32
- **Primary result:** STaDS framework reveals LLMs can achieve high accuracy but often produce correct answers while relying on different decision factors than claimed.

## Executive Summary
This paper introduces STaDS, a protocol for evaluating LLMs' understanding ability through structured tabular decision simulations. The framework measures comprehension fidelity, predictive competence, and decision faithfulness by assessing whether models can follow instructions, produce accurate predictions, and rely on relevant decision factors. Large-scale experiments with 9 LLMs across 15 real-world datasets reveal that while frontier models like Gemini-2.5-Pro achieve high accuracy, they often demonstrate unfaithful behavior - producing correct answers but relying on different factors than claimed. Open-source models struggle with instruction-following and global faithfulness. The findings highlight that current LLMs exhibit only partial professional-level competence, with significant gaps between predictive performance and genuine understanding of decision factors.

## Method Summary
The STaDS framework evaluates LLMs through three dimensions: comprehension fidelity (instruction following and output formatting), predictive competence (accuracy and label prediction), and decision faithfulness (whether models rely on claimed decision factors). The protocol uses 15 tabular datasets with categorical features encoded as integers. Models are evaluated through zero-shot and few-shot inference with structured prompts containing role, task description, attribute glossary, table rendering, and explicit output format. Decision faithfulness is measured through Leave-Any-Out (LAO) ablation - systematically removing each feature to measure performance impact - and self-attribution where models rank features by importance. Spearman's correlation between self-attributed and LAO rankings quantifies global faithfulness.

## Key Results
- Gemini-2.5-Pro achieves high accuracy across datasets but shows only moderate faithfulness (ρ=0.25), indicating accurate but potentially unfaithful predictions
- Open-source models like Mistral-7B exhibit negative faithfulness (ρ=-0.54), claiming reliance on features opposite to actual decision drivers
- Models often explain plausibly but act unfaithfully, tracking statistical correlations rather than governing factors
- Comprehension fidelity remains a significant challenge, with models frequently failing to adhere to output specifications

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Understanding Operationalization
Transforming abstract "understanding" into three measurable behavioral dimensions enables systematic evaluation beyond surface accuracy. The paper operationalizes understanding through (i) instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. Violations of output specifications provide diagnostic signals for each dimension independently.

### Mechanism 2: Behavioral Attribution via Leave-Any-Out (LAO) Ablation
Systematic feature removal reveals actual decision drivers independent of model-stated rationales. For each feature j, the model is re-evaluated with that feature removed from all rows: Δj = Perf(D) - Perf(D \ x[:,j]). Larger performance drops indicate stronger behavioral reliance. This produces πLAO ranking that serves as ground truth for actual decision factors.

### Mechanism 3: Self-Attribution vs Behavioral Attribution Gap Detection
Comparing model-stated feature rankings (πself) against behavioral rankings (πLAO) quantifies "accurate but unfaithful" behavior. Models are prompted to rank all features by importance. Spearman's ρ between πself and πLAO measures global faithfulness. Low or negative ρ indicates models provide plausible post-hoc justifications that don't match actual decision drivers.

## Foundational Learning

- **Concept: Explainability vs Interpretability in XAI**
  - Why needed here: STaDS explicitly bridges these concepts—self-attribution captures interpretability (internal claims), LAO captures explainability (post-hoc justification)
  - Quick check question: If a model says "I used feature X" but removing X doesn't change predictions, is this an explainability or interpretability failure?

- **Concept: Global vs Local Attribution**
  - Why needed here: STaDS evaluates faithfulness across multiple instances (global), not single predictions (local). This is the key distinction from prior CoT faithfulness work
  - Quick check question: Why might a model be locally faithful on individual examples but globally unfaithful across a dataset?

- **Concept: Statistical Dependency vs Behavioral Reliance**
  - Why needed here: The paper shows features with high NMI (statistical correlation) often don't match πLAO (actual reliance). Understanding this gap is critical for interpreting results
  - Quick check question: If feature X has high correlation with labels but low LAO score, what might this indicate about how the model makes decisions?

## Architecture Onboarding

- **Component map:** Prompt Constructor -> Prediction Evaluator -> LAO Engine -> Self-Attribution Prompter -> Faithfulness Analyzer -> Triangulation Module

- **Critical path:**
  1. Construct zero-shot/few-shot prompts with exact formatting (output length = N predictions)
  2. Run baseline prediction → compute PenAcc (captures both accuracy and format adherence)
  3. Run LAO ablation for each of m features → generate πLAO
  4. Prompt for self-attribution → generate πself
  5. Compute ρ(πself, πLAO) and SelfAtt@k
  6. Optional: compute πNMI via statistical tests for triangulation

- **Design tradeoffs:**
  - **Penalty weights (α, β):** Paper uses 0.5 each; higher α penalizes length violations more severely. Tune based on task criticality
  - **LAO vs feature groups:** Individual feature ablation misses interactions; group ablation is more expensive but captures higher-order dependencies
  - **Temperature setting:** Paper uses 0.2 for consistency. Higher temperature increases variance in self-attribution rankings

- **Failure signatures:**
  - **High Acc, Low ρ:** "Accurate but unfaithful" model—correct predictions but wrong stated reasons
  - **High UnkLbl%:** Comprehension failure—model doesn't understand output format requirements
  - **Low σLAO:** Diffuse reliance—model uses many weak features rather than sparse, interpretable factors
  - **Negative ρ:** Model claims features that oppose actual reliance

- **First 3 experiments:**
  1. **Replicate on single dataset:** Start with Iris (4 features, 150 samples) to validate pipeline end-to-end
  2. **Ablate prompt components:** Remove the attribute glossary from instructions and measure impact on comprehension fidelity (Len-F1) and accuracy
  3. **Test prompt sensitivity:** Vary the self-attribution prompt phrasing and measure stability of πself

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning strategies or causal supervision improve global decision faithfulness in LLMs beyond current levels?
- Basis in paper: Conclusion states: "Future work should explore how fine-tuning, causal supervision, and interactive alignment with human experts can promote stable, faithful decision behavior across domains"
- Why unresolved: The paper evaluates pretrained models but does not test whether training interventions can reduce the gap between self-attributed and actual decision factors
- What evidence would resolve it: Compare faithfulness metrics (Self-Faith ρ) before and after fine-tuning with causally-annotated tabular datasets

### Open Question 2
Do the STaDS framework and findings generalize to regression tasks and continuous-valued decision settings?
- Basis in paper: Section 3.2 notes: "While our present focus is on classification, extending the protocol to regression tasks represents a natural direction for future work"
- Why unresolved: All 15 benchmark datasets are classification tasks; faithfulness and comprehension patterns may differ for continuous predictions
- What evidence would resolve it: Adapt metrics (e.g., LAO, Self-Decision Faithfulness) to regression and evaluate on tabular datasets with continuous targets

### Open Question 3
Do models with high statistical correlation alignment (high ρ(πself, πNMI)) exhibit degraded performance on out-of-distribution or perturbed test cases?
- Basis in paper: Section 6.3 finds "models often explain plausibly but act unfaithfully," tracking correlations rather than governing factors
- Why unresolved: The paper measures alignment with dataset statistics but does not test robustness to distribution shift where statistical shortcuts would fail
- What evidence would resolve it: Evaluate penalized accuracy and faithfulness on adversarial or OOD variants of the tabular datasets

## Limitations

- The framework relies on strong independence assumptions for LAO attribution, potentially missing feature interactions
- Self-attribution reliability depends on prompt design quality and model introspective capabilities
- All evaluated datasets are classification tasks, limiting generalizability to regression and continuous prediction settings
- The protocol focuses on tabular data and may not translate directly to unstructured or multimodal domains

## Confidence

- **Comprehension Fidelity:** Medium - based on established metrics (Len-F1, UnkLbl%) with clear measurement protocols
- **Predictive Competence:** Medium - accuracy and F1 metrics are well-validated, though penalized accuracy introduces new complexity
- **Decision Faithfulness:** Low - LAO attribution is novel but relies on strong independence assumptions; self-attribution may not reflect true reasoning

## Next Checks

1. Replicate the faithfulness analysis using interaction-aware attribution methods (SHAP or feature-group ablation) to validate whether LAO captures the complete picture of feature importance
2. Test prompt sensitivity by varying the self-attribution prompt phrasing and measuring stability of πself rankings across different phrasings
3. Conduct human expert evaluation on a subset of predictions to validate whether model-stated rationales align with expert decision factors, particularly for cases where ρ is low but accuracy is high