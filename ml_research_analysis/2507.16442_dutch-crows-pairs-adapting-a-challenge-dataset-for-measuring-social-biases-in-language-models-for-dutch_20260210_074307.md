---
ver: rpa2
title: 'Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases
  in Language Models for Dutch'
arxiv_id: '2507.16442'
source_url: https://arxiv.org/abs/2507.16442
tags:
- bias
- language
- dutch
- crows-pairs
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Dutch CrowS-Pairs, a benchmark dataset
  for measuring social biases in Dutch language models. It adapts the original CrowS-Pairs
  to Dutch, covering nine bias categories with 1463 sentence pairs.
---

# Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases in Language Models for Dutch

## Quick Facts
- arXiv ID: 2507.16442
- Source URL: https://arxiv.org/abs/2507.16442
- Authors: Elza Strazda; Gerasimos Spanakis
- Reference count: 9
- Primary result: Dutch language models exhibit less bias than English and French models on the adapted CrowS-Pairs benchmark

## Executive Summary
This paper introduces the Dutch CrowS-Pairs, a benchmark dataset for measuring social biases in Dutch language models. It adapts the original CrowS-Pairs to Dutch, covering nine bias categories with 1463 sentence pairs. The dataset is used to evaluate bias in both masked and autoregressive language models, including Dutch models like BERTje, RobBERT, and GEITje, as well as French and English counterparts. Results show that English models exhibit the most bias, Dutch models the least, and that assigning personas to models can significantly alter bias levels. The study highlights the variability of bias across languages and contexts, emphasizing the role of cultural and linguistic factors in shaping model biases.

## Method Summary
The paper adapts the CrowS-Pairs bias benchmark to Dutch through translation and cultural substitution (e.g., "Mexican" → "Moroccan"). It evaluates nine bias categories across 1463 sentence pairs using both masked language models (MLMs) via pseudo-log-likelihood scoring and autoregressive models (ARLMs) via prompted choice selection. Dutch models (BERTje, RobBERT, mBERT, GEITje) are compared with French (FlauBERT, CamemBERT) and English (BERT, RoBERTa) counterparts. ARLMs are additionally tested with persona prompts ("good" vs. "bad") to assess context-dependent bias expression.

## Key Results
- Dutch language models (BERTje, RobBERT) show significantly less bias than English (BERT, RoBERTa) and French (FlauBERT, CamemBERT) models
- English RoBERTa exhibits the highest bias across all models tested
- Assigning personas to autoregressive models substantially alters bias expression, with "bad" personas increasing stereotypical choices and "good" personas reducing them
- Bias varies dramatically by category, with Disability showing highest bias in MLMs but lowest in ARLMs
- GEITje (Dutch-finetuned Mistral-7B) shows more bias than its base model, suggesting fine-tuning amplifies stereotypes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-log-likelihood scoring can quantify stereotypical preference in masked language models by comparing conditional probabilities of token sequences.
- Mechanism: For each sentence pair (stereotypical vs. anti-stereotypical), the model masks each unmodified token one at a time and computes the log probability of predicting it given context. Sentence scores are summed; bias is measured as the percentage of pairs where the stereotypical sentence receives higher likelihood.
- Core assumption: Higher likelihood assignment to stereotypical sentences indicates model bias rather than corpus frequency artifacts.
- Evidence anchors:
  - [abstract]: "sentence pairs are composed of contrasting sentences, where one concerns disadvantaged groups and the other advantaged groups"
  - [section 3.2]: "For every sentence, one modified token at a time is masked until all ui are masked... A model that does not incorporate any stereotypes should achieve a bias score of 50"
  - [corpus]: Limited direct validation—neighbor papers focus on benchmark inconsistencies rather than scoring mechanism validity
- Break condition: If sentence pairs are not truly minimal (differ on more than the bias term), likelihood differences may reflect confounds; if training data has genuine distributional differences rather than stereotypes, interpretation becomes ambiguous.

### Mechanism 2
- Claim: Persona-based prompting modulates bias expression in autoregressive models by activating different social norm representations learned during training.
- Mechanism: Models are prompted to adopt a persona (e.g., "good and kind" vs. "bad and mean") before evaluating sentence pairs. The persona cue retrieves associated behavioral patterns from training data, altering which completions are deemed "more likely."
- Core assumption: Models encode social norms and can flexibly apply them based on context cues; persona effects reflect model capability rather than just prompt format artifacts.
- Evidence anchors:
  - [abstract]: "assigning a persona to a language model changes the level of bias it exhibits"
  - [section 4.2]: "When asked to respond as a 'bad' person, both models consistently selected the more stereotypical sentence... when prompted to act as a 'good' person, they often reversed this preference"
  - [corpus]: Deshpande et al. (2023) and Salewski et al. (2023) cited as showing persona-dependent toxicity and bias variation
- Break condition: If persona prompting merely triggers sycophancy (agreeing with implied expectations) rather than revealing stored bias, the mechanism would indicate alignment behavior rather than bias measurement.

### Mechanism 3
- Claim: Training data scale and diversity correlate with measured bias levels across model architectures and languages.
- Mechanism: Larger, more heterogeneous training corpora (e.g., RoBERTa's 161GB from diverse web sources vs. BERT's 13GB curated data) expose models to more stereotypical language patterns. Language-specific corpora encode culturally-specific stereotypes.
- Core assumption: Measured bias primarily reflects training corpus content rather than architecture or evaluation artifact.
- Evidence anchors:
  - [abstract]: "cultural and linguistic factors play a significant role in shaping model biases"
  - [section 4.1]: "RoBERTa's training involved 161GB of diverse text sources... compared to BERT's 13GB... A larger, more heterogeneous corpus likely increased exposure to biased language"
  - [corpus]: Weak corpus validation—neighbor papers address benchmark design issues but not the training data-bias correlation directly
- Break condition: If the CrowS-Pairs instrument itself has language-specific validity issues (e.g., translation artifacts inflating Dutch scores downward), apparent cross-linguistic differences may reflect measurement error rather than true bias variation.

## Foundational Learning

- Concept: **Masked vs. Autoregressive Language Models**
  - Why needed here: The paper uses fundamentally different evaluation methods for BERT-style MLMs (pseudo-log-likelihood on masked tokens) vs. GPT-style ARLMs (prompted choice selection), and results are not directly comparable across architectures.
  - Quick check question: Can you explain why you cannot use the same bias scoring formula for both BERTje and GEITje?

- Concept: **Stereotype Benchmark Construction (Minimal Pairs)**
  - Why needed here: The dataset's validity depends on sentence pairs differing only on the bias-relevant term. The paper documents fixing "double switch" and "non-minimal pair" issues inherited from the original CrowS-Pairs.
  - Quick check question: If a sentence pair reads "Poor people don't use makeup / White people don't use makeup," what is the construct validity problem?

- Concept: **Cross-Lingual and Cultural Adaptation**
  - Why needed here: Direct translation of U.S.-centric bias benchmarks may not capture culturally-relevant stereotypes (e.g., "Mexican" → "Moroccan" for Dutch context). Understanding this adaptation is critical for interpreting results.
  - Quick check question: Why might a direct translation of CrowS-Pairs fail to measure Dutch-relevant biases in religion or nationality categories?

## Architecture Onboarding

- Component map:
  - Dutch CrowS-Pairs Dataset -> MLM Evaluation Pipeline -> Bias Scores -> Cross-Lingual Comparison
  - Dutch CrowS-Pairs Dataset -> ARLM Evaluation Pipeline -> Persona-Aware Bias Scores -> Cross-Lingual Comparison

- Critical path:
  1. Validate dataset quality (check minimal pairs, category consistency, translation fluency)
  2. Run baseline MLM evaluations (BERTje, RobBERT, mBERT on Dutch; BERT, RoBERTa on English; FlauBERT, CamemBERT on French)
  3. Run ARLM evaluations with persona conditions (baseline, "good," "bad")
  4. Compute per-category and aggregate bias scores; compare across languages and architectures

- Design tradeoffs:
  - **Translation vs. native construction**: Translation is faster but risks cultural mismatch; native construction captures local stereotypes but requires domain expertise
  - **Pseudo-log-likelihood vs. prompted choice**: MLM scoring is more controlled but less ecologically valid; ARLM prompting reflects real usage but introduces prompt sensitivity
  - **Category granularity**: 9 categories enable fine-grained analysis but reduce per-category sample size (Disability has only 58 pairs)

- Failure signatures:
  - **Non-minimal pairs inflate bias scores** (confounding variables beyond bias term affect likelihood)
  - **Category label mismatches** (e.g., "Jewish/Italian" labeled as religion but "Italian" is nationality)
  - **Persona prompts trigger sycophancy** rather than revealing latent bias
  - **Translation artifacts** (e.g., calques that sound unnatural in Dutch, reducing model confidence)

- First 3 experiments:
  1. **Sanity check**: Verify that a random model (uniform token prediction) achieves ~50% bias score; confirm Dutch models score near 50 on shuffled pair labels.
  2. **Architecture ablation**: Compare BERT-base vs. RoBERTa-base within the same language (English) to isolate architecture effects from training data effects.
  3. **Persona sensitivity analysis**: Test whether "good" persona reduces bias uniformly across categories or has category-specific effects; check if "neutral professional" persona differs from baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does language-specific fine-tuning inherently amplify cultural stereotypes in language models, and if so, through what mechanisms?
- Basis in paper: [explicit] The authors note "more exploration is needed to understand the underlying causes and how different fine-tuning approaches might impact bias" after observing that GEITje exhibited significantly more bias than its base model Mistral-7B.
- Why unresolved: The paper documents the phenomenon but only speculates that localized fine-tuning data may contain unbalanced representations or stereotypical associations.
- What evidence would resolve it: Systematic experiments varying fine-tuning data composition while controlling for model architecture, coupled with analysis of attention patterns and learned representations.

### Open Question 2
- Question: How can translated bias benchmarks be validated for cultural equivalence and relevance without introducing new measurement artifacts?
- Basis in paper: [inferred] The translation methodology used Google Translate plus two native speakers, with cultural substitutions like "Mexican" → "Moroccan," but no systematic validation that these substitutions accurately reflect Dutch cultural biases.
- Why unresolved: Cultural adaptation involves subjective judgments about which stereotypes transfer across cultures, and the paper acknowledges issues with "sentence quality and category consistency inherited from the original CrowS-Pairs design."
- What evidence would resolve it: Large-scale native speaker validation studies comparing perceived stereotype strength across original and adapted pairs, or development of culturally-grounded bias datasets from scratch.

### Open Question 3
- Question: Why do masked and autoregressive language models exhibit inverted bias patterns across demographic categories (e.g., Disability showing highest bias in MLMs but lowest in ARLMs)?
- Basis in paper: [inferred] The results show Disability among the highest bias categories for MLMs (RobBERT: 81.03) but among the lowest for ARLMs, with no explanation offered for this architectural divergence.
- Why unresolved: The paper uses fundamentally different evaluation metrics (pseudo-log-likelihood scoring vs. prompted choice selection) and does not disentangle whether the difference stems from architecture, evaluation method, or model scale.
- What evidence would resolve it: Controlled experiments applying both evaluation methods to the same models, or developing unified metrics that work across architectures.

### Open Question 4
- Question: Are Dutch language models genuinely less biased than English and French counterparts, or does this reflect measurement limitations and training corpus characteristics?
- Basis in paper: [inferred] Dutch models showed the lowest bias overall, but this could reflect smaller training corpora (BERTje: 12GB curated data vs. RoBERTa's 161GB diverse web data) rather than cultural factors.
- Why unresolved: The paper attributes differences to "cultural and linguistic factors" but cannot isolate these from confounds like data volume, curation quality, and benchmark fitness.
- What evidence would resolve it: Multilingual models evaluated on culturally-matched benchmarks across languages, controlling for training data characteristics.

## Limitations
- **Benchmark validity**: The study relies on the CrowS-Pairs benchmark, which recent work has shown to have significant construct validity issues including non-minimal pairs and category mislabeling.
- **Training data attribution**: Claims about Dutch models showing less bias due to training data scale are weakly supported with limited corpus analysis.
- **Persona prompting mechanism**: Unclear whether persona effects reflect genuine bias expression or sycophantic alignment with prompt expectations.

## Confidence
- **Benchmark validity**: Medium for within-language comparisons; Low for cross-lingual comparisons
- **Training data attribution**: Low for causal claims about training data effects
- **Persona prompting mechanism**: Low-Medium for interpreting persona effects as bias measurement

## Next Checks
1. **Construct validity audit**: Conduct a blind review where annotators classify each Dutch CrowS-Pairs sentence pair to verify minimal differences and appropriate category labels.
2. **Ablation study on training data**: Analyze the actual training corpora of BERTje, RobBERT, and GEITje to quantify the presence of stereotypical content and correlate with model-level bias scores.
3. **Neutral persona control**: Test additional persona prompts like "neutral professional" or "impartial observer" to determine if the "good/bad" framing uniquely affects results.