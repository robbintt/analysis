---
ver: rpa2
title: On the Notion that Language Models Reason
arxiv_id: '2511.11810'
source_url: https://arxiv.org/abs/2511.11810
tags:
- reasoning
- language
- logical
- arxiv
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper challenges the widespread notion that autoregressive\
  \ language models (LMs) engage in logical reasoning. It argues that descriptions\
  \ of LMs as \u201Creasoning\u201D are incommensurable with their actual computational\
  \ processes\u2014namely, they implement implicit finite-order Markov kernels that\
  \ map contexts to conditional token distributions."
---

# On the Notion that Language Models Reason

## Quick Facts
- arXiv ID: 2511.11810
- Source URL: https://arxiv.org/abs/2511.11810
- Authors: Bertram Højer
- Reference count: 17
- Primary result: Challenges the idea that LMs engage in logical reasoning, framing them as finite-order Markov kernels instead

## Executive Summary
This paper argues that descriptions of autoregressive language models as "reasoning" are fundamentally incommensurable with their actual computational processes. Rather than implementing logical inference, transformers implement implicit finite-order Markov kernels that map contexts to conditional token distributions. The apparent reasoning-like behavior emerges from statistical regularities and approximate invariances in the learned kernel, not from explicit logical mechanisms. The authors propose reframing LM behavior as inference under epistemic uncertainty and provide formal metrics to measure epistemic stability.

## Method Summary
The paper develops a formal framework to distinguish between statistical inference and logical reasoning in language models. It introduces transformation invariance (ε_T) and inferential invariance (δ_r) metrics based on total variation distance to quantify how output distributions change under logically equivalent transformations. The authors advocate for a research program using synthetic datasets with controlled logical structure to empirically analyze LM operations and test their theoretical claims about kernel behavior.

## Key Results
- LMs implement implicit finite-order Markov kernels rather than logical reasoning systems
- Reasoning-like outputs correspond to statistical regularities in training data, not explicit logical mechanisms
- LMs violate logical consistency because training objectives don't enforce transformation or inferential invariance
- Proposed metrics show LMs are sensitive to irrelevant transformations and lack logical consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMs implement implicit finite-order Markov kernels, not logical reasoning systems
- Mechanism: For a fixed context window L and vocabulary V, an autoregressive LM induces an order-L Markov chain where the kernel κ_θ maps any bounded context x to a probability simplex over V. Token generation applies κ_θ iteratively without access to explicit logical state or inference rules.
- Core assumption: The finite context window and vocabulary constrain the model to discrete Markov dynamics, regardless of internal transformer complexity.
- Evidence anchors:
  - [abstract] "transformer-based LMs implement an implicit finite-order Markov kernel mapping contexts to conditional token distributions"
  - [section 3.1] "An LM parameterized by θ then defines a Markov kernel κ_θ: X → Δ(V)"
  - [corpus] Related paper "DeduCE" evaluates deductive consistency, indirectly supporting that logical behavior requires external verification rather than internal mechanism
- Break condition: If architectural modifications introduce explicit logical state tracking or symbolic reasoning modules that persist across token generations

### Mechanism 2
- Claim: "Reasoning-like" outputs emerge from approximate statistical invariances in the learned kernel, not explicit logical mechanisms
- Mechanism: Training minimizes cross-entropy to match empirical data distributions. When training data contains logical patterns, the kernel acquires approximate invariances—regions where logically equivalent contexts map to similar continuation distributions. These appear as reasoning but are statistical artifacts.
- Core assumption: Logical structure in outputs reflects regularities in training data, not structural properties enforced by the learning objective.
- Evidence anchors:
  - [abstract] "reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms"
  - [section 3.2] "invariances in transformer kernels are statistical artifacts of the training data distribution and not structural properties of the architecture"
  - [corpus] Weak direct corpus evidence; neighboring papers focus on evaluation metrics rather than mechanistic explanations
- Break condition: If training objectives are modified to explicitly penalize violations of logical invariance (e.g., adding consistency loss terms)

### Mechanism 3
- Claim: LMs violate logical consistency because the training objective does not enforce transformation or inferential invariance
- Mechanism: Two formal metrics capture this—transformation invariance (ε_T) measures sensitivity to irrelevant input changes; inferential invariance (δ_r) measures probability assigned to valid logical conclusions. Both ε_T > 0 and δ_r > 0 in practice indicate imperfect reasoning.
- Core assumption: Genuine reasoning requires invariance to logically equivalent reformulations and reliable inference propagation—properties not guaranteed by next-token prediction.
- Evidence anchors:
  - [section 3.2] "LMs are trained to match an empirical probability distribution... and do not directly optimize for, or enforce, these invariances"
  - [section 3.2] "LMs are usually said not to reason because they are not invariant to irrelevant transformations on x"
  - [corpus] "DeduCE" paper proposes deductive consistency metrics, aligning with the need for formal invariance measures
- Break condition: If synthetic training data with controlled logical structure demonstrably produces kernels with bounded ε_T and δ_r approaching zero

## Foundational Learning

- **Markov Kernels and Order-L Markov Chains**:
  - Why needed here: The paper's core argument rests on framing LMs as Markov processes. Without this foundation, the distinction between "reasoning" and "statistical pattern matching" remains intuitive rather than formal.
  - Quick check question: Given context window 4096, can you explain why a transformer is an order-4096 Markov chain rather than an unbounded memory process?

- **Total Variation Distance Between Distributions**:
  - Why needed here: The proposed metrics (ε_T, δ_r) use total variation distance to quantify how much output distributions diverge under logical transformations.
  - Quick check question: If κ_θ(·|x) and κ_θ(·|t(x)) have total variation distance 0.15, what does this tell you about the model's transformation invariance?

- **Epistemic vs. Aleatoric Uncertainty**:
  - Why needed here: The paper advocates reframing "reasoning" as "inference under epistemic uncertainty"—uncertainty about which kernel properties hold, not just noise in predictions.
  - Quick check question: When an LM gives different answers to logically equivalent prompts, is this epistemic or aleatoric uncertainty? Why does the distinction matter for system design?

## Architecture Onboarding

- **Component map**:
Input context (x ∈ V^≤L) -> Embedding function e_in: X → R^d (+ positional encoding) -> Transformer layers (attention + feed-forward blocks, repeated N times) -> De-embedding function e_out: R^d → X -> Softmax over vocabulary → κ_θ(·|x) ∈ Δ(V) -> Sample or select next token

- **Critical path**: Understanding that every token generation step invokes the same kernel κ_θ with expanded context. The kernel has no persistent logical state—each call is memoryless beyond the context window. CoT traces improve performance not by enabling "thinking" but by expanding the context available to the kernel, providing more statistical cues.

- **Design tradeoffs**:
  - Larger context L → Higher-order Markov chain → More expressive but still no logical guarantees
  - CoT generation → Strictly more expressive (per Merrill & Sabharwal citation) but performance gains may stem from statistical pattern completion rather than inference
  - Training on reasoning traces → Better benchmark scores but may encode surface patterns, not logical structure

- **Failure signatures**:
  - High sensitivity to prompt rephrasing (large ε_T under synonym/paraphrase transformations)
  - Correct intermediate steps but wrong final answer (broken inferential invariance)
  - Inconsistent answers when problem is logically equivalent but superficially different
  - Semantically invalid CoT traces still improving performance (cited: Stechly et al., 2025)

- **First 3 experiments**:
  1. **Transformation invariance probe**: Create paired datasets where (x, t(x)) are logically equivalent transformations. Measure ε_T = V_T(κ_θ(·|x), κ_θ(·|t(x))) across transformation types (paraphrase, variable renaming, premise reordering). Hypothesis: ε_T will vary by transformation type, revealing which invariances the kernel approximates.
  2. **Synthetic logic dataset training**: Train small transformers on datasets with controlled logical structure (e.g., modus ponens instances with varying surface forms). Measure how δ_r changes as dataset size and logical regularity increase. This tests whether invariance emerges from data statistics alone.
  3. **CoT validity ablation**: Compare performance when using valid vs. corrupted intermediate tokens (following Stechly et al.). If corrupted tokens still help, this supports the claim that CoT improves inference through statistical pattern completion, not logical reasoning.

## Open Questions the Paper Calls Out
None

## Limitations
- The corpus evidence supporting statistical regularities over logical mechanisms is relatively weak
- Practical calibration of transformation and inferential invariance metrics across diverse domains remains underexplored
- Quantifying epistemic uncertainty in transformer kernels for real-world reasoning tasks is not yet demonstrated

## Confidence

- **High confidence**: The formal framing of LMs as order-L Markov kernels is mathematically rigorous and well-supported by transformer architecture fundamentals
- **Medium confidence**: The claim that reasoning-like outputs are statistical artifacts rather than logical mechanisms is plausible but requires more diverse empirical validation
- **Medium confidence**: The assertion that CoT improves performance through statistical pattern completion rather than logical inference is supported by cited work but needs direct experimental confirmation

## Next Checks
1. **Multi-domain invariance benchmarking**: Apply the ε_T and δ_r metrics to a multilingual, multi-domain dataset (e.g., logical inference tasks across formal logic, mathematics, and commonsense reasoning) to test whether invariance failures are consistent or domain-specific

2. **Causal intervention analysis**: Use causal mediation analysis to determine whether CoT tokens causally influence final outputs through logical propagation or through context expansion that better conditions the Markov kernel

3. **Architectural ablation study**: Train transformers with modified attention mechanisms (e.g., persistent memory, symbolic reasoning modules) and measure whether ε_T and δ_r decrease significantly compared to standard transformers, testing whether logical consistency can be architecturally enforced