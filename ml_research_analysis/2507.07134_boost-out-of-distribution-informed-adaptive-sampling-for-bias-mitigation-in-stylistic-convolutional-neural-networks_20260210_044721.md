---
ver: rpa2
title: 'BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation
  in Stylistic Convolutional Neural Networks'
arxiv_id: '2507.07134'
source_url: https://arxiv.org/abs/2507.07134
tags:
- samples
- classes
- class
- bias
- sampler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses bias in AI-based painting classification caused
  by imbalanced training datasets, where dominant artistic styles lead to poor performance
  on rare classes. To tackle this, it proposes BOOST (Bias-Oriented OOD Sampling and
  Tuning), a novel adaptive sampling method that dynamically adjusts temperature scaling
  and sampling probabilities using an out-of-distribution detector.
---

# BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2507.07134
- Source URL: https://arxiv.org/abs/2507.07134
- Reference count: 40
- Primary result: BOOST achieved 84.44% accuracy and 79.79% F1 score on KaoKore/PACS datasets with significantly lower class-wise bias (Mean Absolute Bias 2.94%, Standard Deviation of Bias 3.51%) compared to baseline models

## Executive Summary
This paper addresses bias in AI-based painting classification caused by imbalanced training datasets, where dominant artistic styles lead to poor performance on rare classes. The authors propose BOOST (Bias-Oriented OOD Sampling and Tuning), a novel adaptive sampling method that dynamically adjusts temperature scaling and sampling probabilities using an out-of-distribution detector. This approach promotes equitable representation of all classes by prioritizing hard or ambiguous samples early in training and gradually shifting to uniform sampling.

BOOST was evaluated on KaoKore and PACS datasets, achieving state-of-the-art performance with 84.44% accuracy and 79.79% F1 score. The method significantly reduced class-wise bias compared to baseline models. A new metric, Same-Dataset OOD Detection Score (SODC), was introduced to assess class-wise separation and bias reduction, confirming BOOST's effectiveness in improving fairness and robustness in painting classification.

## Method Summary
BOOST employs a three-stage adaptive sampling framework. First, an OOD detector identifies samples that are ambiguous or hard to classify. Second, temperature scaling adjusts the sampling probability distribution using an exponential decay function that balances exploration of rare classes early in training with exploitation of all classes later. Third, the model uses these dynamically adjusted probabilities to sample batches during training, with the temperature parameter decaying from 1.0 to 0.1 according to the formula T(t) = 1 - 0.9 * exp(-0.01 * t), where t is the iteration count.

The temperature scaling mechanism is the core innovation, allowing the model to explore underrepresented classes more aggressively at the beginning of training while gradually transitioning to uniform sampling. This approach ensures that rare artistic styles receive adequate representation during critical early learning phases when the model is most sensitive to sampling bias.

## Key Results
- Achieved 84.44% accuracy and 79.79% F1 score on KaoKore and PACS datasets
- Reduced Mean Absolute Bias to 2.94% and Standard Deviation of Bias to 3.51% compared to baselines
- Introduced SODC metric showing improved class-wise separation in BOOST vs. standard sampling
- Demonstrated effectiveness of adaptive temperature scaling for bias mitigation in artistic style classification

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental sampling bias in imbalanced datasets. When training data contains many examples of dominant artistic styles and few examples of rare styles, standard uniform sampling causes the model to overfit to common patterns while underrepresenting rare classes. BOOST's OOD detector identifies samples that are difficult to classify, which typically belong to underrepresented classes or ambiguous regions between styles. By dynamically increasing the sampling probability of these hard examples early in training through temperature scaling, the model learns more balanced representations before the sampling shifts to uniform distribution.

## Foundational Learning

- **Temperature Scaling**: Adjusts sampling probabilities by raising them to a power, controlling exploration vs. exploitation. Needed to balance rare class representation early in training while ensuring convergence to uniform sampling later. Quick check: verify temperature decay schedule achieves target exploration levels at specific training stages.

- **Out-of-Distribution Detection**: Identifies samples that are ambiguous or hard to classify, typically from underrepresented classes. Needed to target sampling adjustments toward the most beneficial examples. Quick check: confirm OOD detector has high recall for minority class samples.

- **Adaptive Sampling**: Dynamically adjusts batch composition based on training progress and sample difficulty. Needed to overcome static sampling limitations in imbalanced datasets. Quick check: measure class distribution variance across training epochs.

- **Bias Metrics**: Quantifies class-wise performance differences through Mean Absolute Bias and Standard Deviation of Bias. Needed to evaluate fairness improvements beyond aggregate accuracy. Quick check: verify metrics detect known bias patterns in controlled experiments.

- **Same-Dataset OOD Detection**: Measures class separation within a single dataset using OOD detection principles. Needed to quantify bias reduction through class-wise distinguishability. Quick check: compare SODC scores across different sampling strategies.

## Architecture Onboarding

**Component Map**: Data Loader -> OOD Detector -> Temperature Scheduler -> Sampler -> Model -> Loss Function

**Critical Path**: The temperature scaling mechanism is the critical component. The exponential decay function T(t) = 1 - 0.9 * exp(-0.01 * t) must be precisely calibrated - too fast decay leads to insufficient exploration of rare classes, while too slow decay prevents convergence to optimal uniform sampling. The OOD detector must also be accurate enough to identify truly beneficial samples without introducing noise.

**Design Tradeoffs**: BOOST trades computational overhead during training (due to OOD detection) for improved fairness and accuracy. The temperature scaling introduces an additional hyperparameter (decay rate) that requires tuning per dataset. The method assumes that OOD detection can effectively identify underrepresented classes, which may not hold in all bias scenarios.

**Failure Signatures**: If temperature decay is too aggressive, the model will revert to standard sampling too quickly, failing to adequately explore rare classes. Poor OOD detection quality will lead to sampling the wrong examples, potentially exacerbating bias. If the sampling probability adjustments are too extreme, training instability may occur due to sudden shifts in batch composition.

**First Experiments**:
1. Baseline comparison: Run standard uniform sampling vs. BOOST on KaoKore with identical architectures to measure bias reduction
2. Temperature sensitivity: Test different decay rates (faster/slower than 0.01) to identify optimal exploration-exploitation balance
3. OOD detector ablation: Compare BOOST performance with perfect OOD detection vs. realistic detection quality to quantify detector impact

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation limited to painting datasets (KaoKore, PACS), raising generalizability concerns to other domains
- Proposed SODC metric lacks validation against established fairness metrics or external benchmarks
- Temperature scaling mechanism lacks theoretical grounding for why the specific exponential decay formula is optimal
- Computational overhead introduced by OOD detector during training is not quantified

## Confidence
- High: Imbalanced training data leads to bias in style classification, and adaptive sampling can improve class representation
- Medium: Specific BOOST implementation details and hyperparameter choices are empirically derived and may not transfer optimally to other datasets or model architectures
- Low: Universal applicability of the methodology and comparative advantage of SODC over established bias metrics given limited validation scope

## Next Checks
1. Evaluate BOOST on non-artistic image datasets (e.g., medical imaging, satellite imagery) to test domain generalization
2. Benchmark SODC against standard fairness metrics like demographic parity or equalized odds on comparable bias mitigation tasks
3. Conduct ablation studies to quantify the impact of each component (OOD detector, temperature scaling, adaptive sampling) on final performance and bias reduction