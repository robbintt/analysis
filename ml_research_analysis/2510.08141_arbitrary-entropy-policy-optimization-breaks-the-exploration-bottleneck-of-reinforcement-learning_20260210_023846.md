---
ver: rpa2
title: Arbitrary Entropy Policy Optimization Breaks The Exploration Bottleneck of
  Reinforcement Learning
arxiv_id: '2510.08141'
source_url: https://arxiv.org/abs/2510.08141
tags:
- entropy
- aepo
- policy
- arxiv
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Arbitrary Entropy Policy Optimization (AEPO),
  a novel reinforcement learning framework designed to address the entropy collapse
  problem in LLM reasoning. By reformulating entropy regularization as a policy-gradient
  optimization problem, AEPO uses temperature-adjusted REINFORCE regularization to
  achieve stable and controllable entropy regulation without introducing optimization
  bias.
---

# Arbitrary Entropy Policy Optimization Breaks The Exploration Bottleneck of Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.08141
- Source URL: https://arxiv.org/abs/2510.08141
- Reference count: 23
- Primary result: AEPO achieves consistent performance improvements over GRPO on mathematical reasoning benchmarks by preventing entropy collapse through temperature-guided REINFORCE regularization

## Executive Summary
AEPO introduces a novel framework that addresses entropy collapse in LLM reasoning by reformulating entropy regularization as a policy-gradient optimization problem. The method uses temperature-adjusted REINFORCE sampling to achieve stable and controllable entropy regulation without introducing optimization bias. By adaptively sampling from high/low temperature distributions and filtering only positive samples, AEPO maintains exploration while avoiding the performance degradation typically seen with entropy regularization methods.

## Method Summary
AEPO extends GRPO by adding a REINFORCE regularization term computed on temperature-adjusted samples. The temperature T is adaptively selected based on whether current entropy H(π_old) is below or above a target H_bar, using T = T_low + (T_high - T_low)·1[H(π_old) < H_bar]. The REINFORCE term uses only positive samples (R=1) from the temperature-adjusted distribution, filtering out negative samples automatically due to binary rewards. This creates a feedback loop that stabilizes entropy around the target while avoiding the bias introduced by traditional entropy-bonus methods.

## Key Results
- AEPO consistently outperforms GRPO and entropy-regularized baselines on mathematical reasoning benchmarks, achieving average score improvements of 1.87-2.27 points across different model sizes
- Notably surpasses the base model on pass@1024, providing direct evidence that RL can expand reasoning capabilities rather than merely sharpening existing knowledge
- Ablation studies confirm both temperature adjustment and REINFORCE-based regularization are necessary, with replacing REINFORCE with advantage-based gradient causing entropy collapse (avg score drops to 51.56)

## Why This Works (Mechanism)

### Mechanism 1: Temperature-Guided Entropy Modulation via REINFORCE
High-temperature REINFORCE induces relative entropy increase; low-temperature REINFORCE induces relative entropy decrease. Theorem 4.3 shows ∆Hk(T) depends on covariance between logits and policy gradient under temperature-T sampling. When T > 1, sampling shifts probability toward lower-logit actions, and REINFORCE from positive samples increases their probability—indirectly raising entropy. Core assumption: softmax policy with near-orthogonal gradient directions and small learning rate. Break condition: strongly correlated gradient directions or large learning rate.

### Mechanism 2: REINFORCE Filtering Eliminates Optimization Bias
Using REINFORCE on temperature-adjusted samples provides unidirectional gradient signal without introducing reward-entropy trade-off. Binary reward R(q,o) ∈ {0,1} means negative samples contribute zero gradient—filtered automatically. Only positive samples from temperature-adjusted distribution shape policy, avoiding bias from entropy-bonus methods. Core assumption: reward is binary and temperature adjustment doesn't introduce off-policy bias. Break condition: non-binary rewards introduce gradient contributions from negative samples.

### Mechanism 3: Arbitrary Entropy Target via Feedback Control
AEPO stabilizes entropy around arbitrary target H by adaptively switching between high/low temperature sampling. Temperature set via: T = T_low + (T_high - T_low)·1[H(π_old) < H]. When entropy < target, sample from π_{T_high} to inject entropy-increasing gradients. When entropy > target, sample from π_{T_low} to reduce entropy. Core assumption: entropy increase from T_high and decrease from T_low are roughly symmetric. Break condition: GRPO's entropy collapse rate exceeds maximum entropy-injection rate.

## Foundational Learning

- **Policy gradient and REINFORCE**: AEPO's regularization term is REINFORCE gradient on temperature-adjusted samples. Understanding ∇J(θ) = E[R·∇log π] is essential. Quick check: Given binary reward, why does REINFORCE ignore negative samples?

- **Temperature scaling in softmax policies**: AEPO relies on π_T(a|s) ∝ exp(z/T). Understanding how T > 1 flattens distributions (higher entropy) and T < 1 sharpens them. Quick check: If T→∞, what happens to π_T?

- **Entropy as exploration proxy**: Paper's central claim is that controlling entropy controls exploration. Understanding H(π) = -Σ π(a)log π(a) and relationship to output diversity. Quick check: A deterministic policy has what entropy value?

## Architecture Onboarding

- **Component map**: GRPO backbone (samples G responses per query, computes advantages) → Entropy monitor (computes H(π_old) each step) → Temperature selector: T = T_low if H ≥ H_target else T_high → REINFORCE regularization sampler (generates temperature-adjusted samples) → Loss = J_GRPO + α·J_REINFORCE(T-adjusted, positive-only)

- **Critical path**: Temperature selector and REINFORCE regularization term must be computed correctly. Errors here propagate as either entropy collapse or explosion.

- **Design tradeoffs**: α (regularization weight): Too high → entropy dominates, performance degrades; too low → entropy collapse persists. H_target: Non-monotonic relationship with performance. T_high/T_low: Wider range increases control authority but may increase variance.

- **Failure signatures**: Entropy monotonically decreasing → temperature adjustment not triggering; entropy oscillating wildly → T_high - T_low gap too large; pass@1024 decreasing → over-regularization.

- **First 3 experiments**:
  1. **Entropy trajectory baseline**: Run AEPO with H_target = 0.5 on Qwen2.5-7B for 100 steps. Plot entropy over time. Verify oscillation around target.
  2. **Ablation: remove temperature adjustment**: Set T_high = T_low = 1.0. Confirm entropy collapse returns (per Eq. 2 in Table 3).
  3. **Ablation: remove REINFORCE filtering**: Use advantage-weighted gradient instead of REINFORCE for regularization term. Confirm entropy collapse returns (per Eq. 3 in Table 3).

## Open Questions the Paper Calls Out

### Open Question 1
Can the optimal entropy target for a given task be determined automatically rather than through manual tuning? The authors observe non-monotonic entropy-performance relationships but require manual selection of H∈{0.25, 0.50, 0.75, 1.00}. Resolution would require an adaptive mechanism that dynamically adjusts H during training based on validation performance or exploration metrics.

### Open Question 2
How does AEPO generalize to non-reasoning domains such as code generation, creative writing, or multi-modal tasks? All experiments are confined to mathematical reasoning benchmarks with binary verifiable rewards. Resolution would require experiments applying AEPO to diverse domains with different reward structures.

### Open Question 3
How does AEPO's performance scale with model size beyond 7B parameters? Experiments only cover Qwen2.5-7B, Qwen2.5-Math-7B, and Qwen3-4B. Resolution would require benchmark results on 14B, 32B, and 70B+ models.

### Open Question 4
Can AEPO's framework extend to target distributions with specific behavioral characteristics beyond entropy control? The paper proposes this theoretically but not empirically. Resolution would require experiments applying AEPO to enforce response length constraints, style attributes, or other controllable properties while maintaining task performance.

## Limitations
- Theoretical foundations rely on strong assumptions (near-orthogonal gradients, small learning rates) that may not hold in practice
- Hyperparameter sensitivity to temperature ranges and sample counts not systematically analyzed
- Limited generalization evidence beyond mathematical reasoning benchmarks
- Additional computational overhead from temperature-adjusted REINFORCE sampling

## Confidence
- **High confidence**: Empirical observation that AEPO prevents entropy collapse and achieves performance gains over GRPO baselines
- **Medium confidence**: Theoretical mechanism explaining why temperature-guided REINFORCE works under stated assumptions
- **Medium confidence**: Claim that RL can expand reasoning capabilities beyond base model's knowledge frontier based on pass@1024 results

## Next Checks
1. **Entropy dynamics validation**: Implement AEPO with varying H_target values and verify entropy trajectories oscillate around target rather than monotonically decreasing or increasing
2. **Cross-domain generalization test**: Apply AEPO to a non-mathematical reasoning task (e.g., coding problems or commonsense reasoning) to assess whether entropy control benefits transfer
3. **Hyperparameter robustness analysis**: Systematically vary T_high/T_low ranges and REINFORCE sample counts to determine sensitivity of AEPO's performance to critical hyperparameters