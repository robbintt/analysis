---
ver: rpa2
title: 'POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor
  Attacks in Federated Learning'
arxiv_id: '2510.19056'
source_url: https://arxiv.org/abs/2510.19056
tags:
- polar
- attack
- backdoor
- layers
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POLAR introduces a policy-based reinforcement learning approach
  to address layer selection in federated backdoor attacks, overcoming the limitations
  of static rule-based methods by dynamically optimizing which layers to poison. By
  using lightweight Bernoulli sampling and a BSR-based reward function, POLAR adapts
  layer selection across FL rounds while maintaining stealth through a regularization
  constraint.
---

# POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning

## Quick Facts
- arXiv ID: 2510.19056
- Source URL: https://arxiv.org/abs/2510.19056
- Authors: Kuai Yu; Xiaoyu Wu; Peishen Yan; Qingqian Yang; Linshan Jiang; Hao Wang; Yang Hua; Tao Song; Haibing Guan
- Reference count: 12
- Primary result: Policy-based RL approach for layer selection in federated backdoor attacks that adapts dynamically while maintaining stealth and outperforming state-of-the-art by up to 40% in backdoor success rate.

## Executive Summary
POLAR introduces a policy-based reinforcement learning framework to dynamically select which layers to poison in federated backdoor attacks, addressing the limitations of static rule-based methods. By using Bernoulli sampling for discrete layer selection and a BSR-based reward function with regularization, POLAR adapts layer selection across FL rounds while maintaining stealth through a lightweight footprint constraint. Experiments demonstrate POLAR outperforms state-of-the-art attacks like LP Attack and BadNets by up to 40% in backdoor success rate across multiple defenses, models, and datasets, while preserving high main task accuracy and malicious client acceptance rates.

## Method Summary
POLAR uses policy gradient methods (REINFORCE) to optimize layer selection in federated backdoor attacks. The method employs Bernoulli sampling to efficiently explore the combinatorial layer selection space, with each layer having an independent selection probability determined by learnable logits. The RL agent samples binary action vectors to determine which layers receive malicious weights, reducing the action space complexity from O(2^N) to O(N) per sample. A reward function combining backdoor success rate improvement with a layer-count penalty guides the policy toward stealthy, effective attacks. The policy parameters transfer across FL rounds, enabling cumulative learning of backdoor-critical layers despite changing global model states.

## Key Results
- Outperforms LP Attack and BadNets by up to 40% in backdoor success rate across multiple defenses
- Maintains high main task accuracy while achieving stealthy layer selection
- Demonstrates strong generalizability across different models (ResNet-18, VGG-19) and datasets (CIFAR-10, Fashion-MNIST)
- Achieves optimal balance between effectiveness and stealth with λ=10 penalty parameter

## Why This Works (Mechanism)

### Mechanism 1: Bernoulli Policy for Discrete Layer Selection
- Claim: Bernoulli sampling enables efficient exploration of the combinatorial layer selection space without exhaustive enumeration.
- Mechanism: Each layer l has an independent selection probability $p_l = \sigma(\theta_l)$ where $\theta_l$ is a learnable logit parameter. The RL agent samples binary action vectors $S_k \sim \text{Bernoulli}(\sigma(\theta))$ to determine which layers receive malicious weights, reducing the action space complexity from $O(2^N)$ to $O(N)$ per sample.
- Core assumption: Layer contributions to backdoor success are approximately factorizable during exploration, though inter-layer dependencies emerge through policy gradient updates.
- Evidence anchors:
  - [Section 4.2, Eq. 9]: "Since the choices of each layer are independent... we have $P_\theta(S) = \prod_{l=1}^N P_{\theta_l}(S^{(l)})$"
  - [Section A.3]: "Bernoulli sampling makes the RL process lightweight and scalable, which is quite suitable for FL backdoor attack"
  - [corpus]: Limited direct evidence; related work "Towards Backdoor Stealthiness in Model Parameter Space" discusses parameter-space stealthiness but not Bernoulli sampling specifically.
- Break condition: If layers exhibit strong, irreducible dependencies (e.g., attention heads requiring coordinated poisoning), factorized Bernoulli sampling may fail to discover optimal combinations.

### Mechanism 2: BSR-Based Reward Signal with Regularization
- Claim: A reward function combining backdoor success rate improvement with a layer-count penalty guides the policy toward stealthy, effective attacks.
- Mechanism: The reward $r_k = \text{BSR}(S_k) - \text{BSR}(W_r^M)$ measures improvement over a baseline malicious model. The loss function $L = -\sum_{k,l} r_k \cdot \log P_{\theta_l}(S_k^{(l)}) + \lambda \sum_l \log(p_l)$ amplifies actions yielding high BSR while penalizing large attack footprints via the regularization term.
- Core assumption: The server provides implicit feedback through model acceptance/rejection; the attacker can evaluate BSR locally on a held-out poisoned validation set to estimate reward.
- Evidence anchors:
  - [Abstract]: "optimizing layer selection using policy gradient updates based on backdoor success rate (BSR) improvements"
  - [Section 4.2, Eq. 2]: Full loss function with regularization term explicitly shown
  - [Table 4]: Ablation shows $\lambda=10$ achieves best BSR/MAR tradeoff; $\lambda=0$ selects 8 layers with lower stealthiness, $\lambda=20$ over-constrains to 5 layers with poor BSR (37.75%)
  - [corpus]: No direct corpus evidence for this specific reward design.
- Break condition: If defenses evolve to suppress BSR without rejecting updates, the reward signal becomes uninformative; if $\lambda$ is misspecified, the policy either over-selects (detectable) or under-selects (ineffective).

### Mechanism 3: Adaptive Policy Transfer Across FL Rounds
- Claim: The policy parameters $\theta$ transfer across rounds, enabling cumulative learning of backdoor-critical layers despite changing global model states.
- Mechanism: At round $r$, the initial logits are warm-started from the previous round: $\theta^{(r,0)} = \theta^{(r-1,T-1)}$. The state representation $\text{State}_r = \{\theta_{r-1}, \text{BSR}(W_r^M)\}$ provides context for continued optimization. This allows the policy to refine layer selection over time as the global model converges.
- Core assumption: Backdoor-critical layers remain relatively stable across rounds; drastic architecture changes or aggressive reinitialization would break transfer.
- Evidence anchors:
  - [Section 4.1]: "the strategy logit obtained from the previous RL round would work as the initial logit in the next round"
  - [Section 4.3, Eq. 4]: State representation defined with previous-round logits
  - [Figure 4]: Layer selection stabilizes to fewer layers over rounds (POLAR converges to ~5 layers; LP Attack fluctuates widely)
  - [corpus]: Weak evidence; no corpus papers explicitly address cross-round policy transfer in FL attacks.
- Break condition: If the global model undergoes significant distribution shift (e.g., client dropout, data drift), transferred policies may become stale and require reset.

## Foundational Learning

- Concept: **Policy Gradient Methods (REINFORCE)**
  - Why needed here: POLAR uses the log-derivative trick $\nabla_\theta P_\theta(S) = P_\theta(S) \nabla_\theta \log P_\theta(S)$ to optimize discrete layer selections without differentiable access to layer importance. Understanding variance reduction and baseline subtraction is critical for stable training.
  - Quick check question: Given a batch of K=50 samples with rewards ranging from -5 to +15, how would you implement reward normalization to prevent gradient explosion?

- Concept: **Federated Learning Aggregation and Defenses**
  - Why needed here: POLAR's stealthiness depends on understanding how defenses like MultiKrum, FLAME, and RLR detect anomalies. The malicious client acceptance rate (MAR) directly measures policy success against these defenses.
  - Quick check question: If MultiKrum rejects updates with high Krum distance, what layer-selection pattern would minimize distance while preserving backdoor functionality?

- Concept: **Backdoor-Critical Layer Hypothesis**
  - Why needed here: The paper assumes a subset of layers disproportionately affects backdoor success while minimally impacting main-task accuracy. Identifying these layers via policy learning (vs. rule-based forward-backward substitution) is the core innovation.
  - Quick check question: For a ResNet-18 with 18 blocks, if layer 10 has $p_{10}=0.95$ and layer 5 has $p_5=0.3$ after training, what does this suggest about their relative importance for backdoor vs. main-task performance?

## Architecture Onboarding

- Component map: [FL Server] ←→ [Malicious Client] → Local Trainer (benign W_b, malicious W_m) → POLAR Agent (Policy Network, Bernoulli Sampler, BSR Evaluator, Policy Updater) → Layer Replacer (merges W_b, W_m per selection S_final)

- Critical path:
  1. Receive global model $W_r$ from server
  2. Train benign model $W_b$ on clean local data
  3. Train malicious model $W_m$ on poisoned data (warm-start from $W_b$)
  4. Initialize logits $\theta^{(r,0)}$ from previous round
  5. **Inner RL loop** (T=10 steps):
     - Sample K=50 action vectors from Bernoulli($\sigma(\theta)$)
     - For each sample $S_k$: create candidate model, evaluate BSR
     - Compute rewards $r_k = \text{BSR}(S_k) - \text{BSR}(W_m)$
     - Update $\theta$ via policy gradient with regularization
  6. Threshold final logits: $S_{\text{final}}[l] = \mathbf{1}[\sigma(\theta_l) > \tau]$
  7. Submit $W_c = \text{ReplaceLayers}(W_b, W_m, S_{\text{final}})$ to server

- Design tradeoffs:
  - **Batch size K**: Larger K (50) provides stable gradients but increases per-round compute; smaller K (10) causes instability (Table 7 shows BSR drops from 90.55% to 31.94%)
  - **RL steps T**: More steps improve convergence but extend attack time; can compensate with more local epochs E (Table 7: T=2, E=6 achieves 89.22% ABSR vs. T=10, E=2 at 90.55%)
  - **Penalty λ**: Controls stealthiness-effectiveness tradeoff; $\lambda=10$ is optimal (Table 4), $\lambda=0$ over-selects, $\lambda=20$ under-performs

- Failure signatures:
  - MAR drops below 0.5: Policy is selecting too many layers or highly sensitive layers; increase λ
  - ABSR plateaus below 60%: Insufficient exploration (K too small) or learning rate too low
  - Layer selection oscillates wildly across rounds: Learning rate too high; reduce η from 0.01
  - Main task accuracy degrades: Selected layers include task-critical weights; check regularization strength

- First 3 experiments:
  1. **Baseline replication**: Implement POLAR on CIFAR-10 with ResNet-18, RLR defense, default parameters (K=50, T=10, λ=10, τ=0.5). Target: ABSR >90%, MAR >0.9 per Table 2.
  2. **Defense sweep**: Test against all six defenses (FLARE, FLDetector, FLTrust, FLAME, RLR, MultiKrum) on VGG-19. Compare to LP Attack baseline; expect 15-40% BSR improvement.
  3. **Ablation on penalty λ**: Run λ ∈ {0, 5, 10, 20} and plot ABSR vs. MAR vs. selected layer count. Verify that λ=10 achieves the Pareto-optimal point as claimed in Table 4.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can POLAR maintain effectiveness when applied to non-CV tasks, such as NLP, where layer structures (e.g., attention heads) differ significantly from the CNNs tested?
- **Basis in paper:** [inferred] The conclusion claims POLAR has "great generalizability" and can be applied to different models by changing the reward function, yet Section 5.1 limits evaluation strictly to image classification using CNN architectures (ResNet, VGG).
- **Why unresolved:** The concept of "backdoor-critical" layers and the convergence behavior of the Bernoulli sampling policy are unverified on architectures like Transformers, which have different layer interdependencies.
- **What evidence would resolve it:** Experiments applying POLAR to text classification or language models (e.g., BERT) in a federated setting.

### Open Question 2
- **Question:** How sensitive is the RL policy convergence to noisy or delayed feedback signals in scenarios where the attacker cannot precisely observe the global model's Backdoor Success Rate (BSR)?
- **Basis in paper:** [inferred] Section 3 states the attacker can "observe its own aggregation feedback from the central server," and Section 4.3 defines the reward as the direct difference in BSR. This assumes the malicious client has access to the global model evaluation, which may be restricted in practice.
- **Why unresolved:** Reinforcement learning agents typically require reliable reward signals; if the BSR must be estimated rather than observed, the policy gradient updates may fail to converge.
- **What evidence would resolve it:** An ablation study testing POLAR's performance when the reward signal is simulated with Gaussian noise or provided only intermittently.

### Open Question 3
- **Question:** Is POLAR susceptible to adaptive defenses that specifically monitor the temporal consistency of layer selection patterns across rounds?
- **Basis in paper:** [inferred] Figure 4 demonstrates that POLAR converges to modifying a consistent, specific subset of layers over time to maximize efficiency. Current defenses (Section 2.2) focus on update magnitude or clustering rather than temporal layer-selection patterns.
- **Why unresolved:** While the attack is stealthy against magnitude-based defenses, a persistent footprint on specific layers over many rounds could be flagged by a defense tracking the history of modified parameters.
- **What evidence would resolve it:** Evaluation against a defense algorithm that tracks the frequency of layer updates and flags clients that consistently target identical layer indices.

## Limitations
- Attack assumes white-box access to model architecture for layer selection, limiting applicability in black-box FL deployments
- Reward estimation relies on local BSR computation with unspecified validation protocol details
- Limited evaluation to two model families (ResNet, VGG) constrains generalizability claims
- No systematic analysis of computational overhead across different model sizes or client counts

## Confidence
- **High**: POLAR's mechanism of Bernoulli sampling with policy gradients is technically sound and reproducible. The ablation studies on hyperparameters (K, T, λ) are internally consistent.
- **Medium**: Cross-architecture generalization claims are supported by experiments on ResNet-18 and VGG-19, but limited to two model families. Real-world applicability to heterogeneous client populations needs validation.
- **Low**: Claims about "scalability" and "lightweight" operation are qualitative; no systematic analysis of computational overhead across different model sizes or client counts is provided.

## Next Checks
1. Implement POLAR against adaptive defenses that monitor RL policy parameters (e.g., tracking θ changes across rounds) to test robustness.
2. Scale POLAR to larger models (e.g., ResNet-50, EfficientNet) and measure per-round computational overhead to validate "lightweight" claims.
3. Test POLAR in federated settings with heterogeneous client architectures (e.g., mixing ResNet and MobileNet clients) to assess cross-architecture transferability.