---
ver: rpa2
title: Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation
arxiv_id: '2511.16653'
source_url: https://arxiv.org/abs/2511.16653
tags:
- pruning
- sparsity
- accuracy
- importance
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational overhead of iterative train-prune-retrain
  cycles in unstructured pruning by proposing a one-shot global pruning framework
  guided by teacher knowledge distillation. Unlike prior approaches that apply KD
  only for post-pruning recovery, this method integrates KD directly into the importance
  score calculation, using gradient signals informed by the teacher model.
---

# Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2511.16653
- **Source URL**: https://arxiv.org/abs/2511.16653
- **Reference count**: 31
- **Primary result**: One-shot global pruning framework using teacher knowledge distillation achieves high sparsity with minimal performance degradation

## Executive Summary
This paper addresses the computational overhead of iterative train-prune-retrain cycles in unstructured pruning by proposing a one-shot global pruning framework guided by teacher knowledge distillation. Unlike prior approaches that apply KD only for post-pruning recovery, this method integrates KD directly into the importance score calculation, using gradient signals informed by the teacher model. Specifically, it employs Context-Aware Kullback-Leibler Divergence (CA-KLD) loss augmented with logit normalization to guide the pruning process. The approach enables aggressive pruning while preserving critical parameters, followed by sparsity-aware retraining. Experiments on CIFAR-10, CIFAR-100, and TinyImageNet datasets show that the method consistently achieves high sparsity levels with minimal performance degradation. Notably, it outperforms state-of-the-art baselines such as EPG, EPSD, and COLT, offering a more computationally efficient alternative to iterative pruning schemes. The framework provides a practical, performance-preserving solution for resource-constrained environments.

## Method Summary
The proposed framework performs one-shot global pruning by integrating knowledge distillation into the importance score calculation process. During pruning, a teacher model guides the student through Context-Aware Kullback-Leibler Divergence (CA-KLD) loss, which uses gradient signals to identify critical parameters. This approach differs from traditional methods where KD is applied only after pruning for recovery. The CA-KLD loss is augmented with logit normalization to improve guidance during pruning. After determining which parameters to retain based on teacher-informed importance scores, the method applies sparsity-aware retraining to fine-tune the pruned model. This eliminates the need for multiple iterative pruning-retraining cycles, reducing computational overhead while maintaining performance. The framework is evaluated on image classification tasks using CIFAR-10, CIFAR-100, and TinyImageNet datasets, demonstrating superior performance compared to state-of-the-art pruning methods.

## Key Results
- Achieves high sparsity levels with minimal performance degradation across CIFAR-10, CIFAR-100, and TinyImageNet datasets
- Outperforms state-of-the-art baselines including EPG, EPSD, and COLT in terms of sparsity-performance trade-off
- Eliminates iterative train-prune-retrain cycles, reducing computational overhead compared to traditional pruning methods

## Why This Works (Mechanism)
The method works by integrating teacher knowledge distillation directly into the parameter importance calculation rather than applying it as a post-pruning recovery technique. The Context-Aware Kullback-Leibler Divergence (CA-KLD) loss provides gradient signals that guide the pruning process, helping identify which parameters are critical for maintaining model performance. By using the teacher model's knowledge during the pruning phase, the framework can make more informed decisions about which weights to retain, leading to better preservation of important features. The logit normalization in CA-KLD further improves the quality of guidance by ensuring stable and meaningful comparisons during the distillation process. This approach allows for aggressive pruning while maintaining model accuracy, as the teacher's knowledge helps preserve the most informative parameters throughout the pruning process.

## Foundational Learning

**Knowledge Distillation (KD)**
*Why needed*: Enables transfer of knowledge from larger teacher models to smaller student models
*Quick check*: Verify teacher-student accuracy gap is acceptable before applying KD-based pruning

**Unstructured Pruning**
*Why needed*: Removes individual weights rather than entire channels/filters for fine-grained compression
*Quick check*: Confirm pruning ratio matches target sparsity while maintaining model functionality

**KL Divergence**
*Why needed*: Measures difference between probability distributions for distillation loss calculation
*Quick check*: Ensure KL divergence values decrease during training, indicating better alignment

**Logit Normalization**
*Why needed*: Stabilizes gradient signals during distillation for more reliable pruning decisions
*Quick check*: Monitor training stability and convergence speed with normalized vs unnormalized logits

**Importance Score Calculation**
*Why needed*: Determines which parameters to retain during pruning based on their contribution to model performance
*Quick check*: Validate that high-importance scores correlate with critical model functionality

## Architecture Onboarding

**Component Map**
Teacher Model -> CA-KLD Loss Module -> Importance Score Calculator -> Pruning Mask Generator -> Student Model

**Critical Path**
1. Forward pass through teacher model to generate logits
2. CA-KLD loss computation with logit normalization
3. Gradient-based importance score calculation
4. Pruning mask generation based on importance scores
5. Application of mask to student model
6. Sparsity-aware retraining

**Design Tradeoffs**
- Single-shot pruning vs. iterative refinement: Faster but potentially less optimal
- Teacher model overhead vs. pruning quality: Additional computational cost during pruning phase
- CA-KLD complexity vs. traditional pruning: More sophisticated guidance but increased implementation complexity

**Failure Signatures**
- Poor pruning decisions leading to significant accuracy drop
- Unstable training during sparsity-aware retraining
- Insufficient teacher guidance resulting in suboptimal sparsity-performance trade-off

**First Experiments**
1. Baseline pruning without KD to establish performance floor
2. CA-KLD ablation to isolate contribution of context-aware distillation
3. Varying teacher-student model size ratios to assess scalability

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to small-scale image classification datasets (CIFAR-10, CIFAR-100, TinyImageNet), raising scalability concerns
- Computational efficiency gains claimed but not thoroughly quantified with detailed benchmarking
- Overhead from CA-KLD loss computation and teacher model requirements during pruning needs more detailed analysis

## Confidence

**High confidence**: The core technical contribution of integrating knowledge distillation into importance score calculation is clearly articulated and experimentally validated within the tested scope.

**Medium confidence**: Performance claims relative to baselines are well-supported on tested datasets, but generalization to larger models and different domains remains unverified.

**Medium confidence**: The computational efficiency argument is logical but lacks comprehensive benchmarking against iterative approaches in terms of total time and resource consumption.

## Next Checks

1. Evaluate the method on larger-scale benchmarks (e.g., ImageNet, ResNet-50/101) and transformer-based architectures to assess scalability and broader applicability.

2. Conduct ablation studies isolating the contribution of CA-KLD versus other components, and compare against alternative distillation-based pruning methods under identical conditions.

3. Perform detailed resource utilization analysis measuring wall-clock time, memory overhead, and energy consumption during both pruning and retraining phases, comparing directly with iterative pruning baselines.