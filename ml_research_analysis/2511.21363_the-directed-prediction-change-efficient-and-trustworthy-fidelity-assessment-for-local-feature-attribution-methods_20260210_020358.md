---
ver: rpa2
title: The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment
  for Local Feature Attribution Methods
arxiv_id: '2511.21363'
source_url: https://arxiv.org/abs/2511.21363
tags:
- methods
- local
- perturbation
- data
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel metric for evaluating the fidelity
  of local feature attribution methods. The core idea is to modify the existing Prediction
  Change (PC) metric by incorporating the direction of both perturbation and attribution,
  resulting in the Directed Prediction Change (DPC) metric.
---

# The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment for Local Feature Attribution Methods

## Quick Facts
- **arXiv ID**: 2511.21363
- **Source URL**: https://arxiv.org/abs/2511.21363
- **Reference count**: 39
- **Primary result**: Introduces Directed Prediction Change (DPC) metric that improves upon Prediction Change by incorporating perturbation and attribution direction, achieving 10x speedup while providing deterministic, trustworthy evaluation of local feature attribution methods.

## Executive Summary
This paper introduces a novel metric called Directed Prediction Change (DPC) for evaluating the fidelity of local feature attribution methods. The core innovation involves modifying the existing Prediction Change (PC) metric by incorporating the direction of both perturbation and attribution, enabling more effective evaluation while achieving significant computational speedup. The authors demonstrate that DPC, together with PC, enables a holistic and computationally efficient evaluation of feature attribution methods, providing deterministic and reproducible outcomes across extensive experiments with 4,744 distinct explanations.

## Method Summary
The authors propose the Directed Prediction Change (DPC) metric as an improvement over the traditional Prediction Change (PC) metric for evaluating local feature attribution methods. DPC incorporates directional information by considering the sign and magnitude of both perturbations and attribution scores, rather than treating all perturbations uniformly as PC does. This directional approach allows for more meaningful evaluation of how well attribution methods identify truly important features. The method achieves nearly tenfold speedup by eliminating randomness in perturbation ordering through deterministic approaches, making the evaluation procedure both faster and more reproducible.

## Key Results
- DPC achieves almost tenfold speedup compared to PC while maintaining evaluation quality
- The metric eliminates randomness in evaluation by using deterministic perturbation ordering
- DPC enables holistic evaluation when used alongside PC for both baseline-oriented and local feature attribution methods
- Results are demonstrated across 4,744 distinct explanations, showing deterministic and reproducible outcomes

## Why This Works (Mechanism)
DPC works by incorporating directional information into the evaluation process. Traditional PC treats all perturbations equally, but DPC recognizes that perturbing features in the direction suggested by attribution (increasing important features, decreasing unimportant ones) provides more meaningful evaluation than random or uniform perturbations. By considering the sign and magnitude of attribution scores, DPC can better assess whether an attribution method correctly identifies features that genuinely impact model predictions. The deterministic ordering of perturbations eliminates variance from random sampling, making evaluations more trustworthy and reproducible.

## Foundational Learning
- **Feature Attribution Methods**: Techniques that assign importance scores to input features explaining model predictions - needed to understand what DPC evaluates; quick check: can identify difference between local (instance-specific) and global attribution methods
- **Perturbation-Based Evaluation**: Approaches that modify input features to assess attribution quality - needed to understand PC and DPC metrics; quick check: can explain how feature removal affects model predictions
- **Directional Perturbation**: Modifying features in specific directions (increase/decrease) based on attribution - needed to understand DPC's key innovation; quick check: can distinguish between uniform and directional perturbation strategies
- **Fidelity Metrics**: Measures that assess how well attribution methods reflect true model behavior - needed to contextualize the evaluation framework; quick check: can compare different fidelity evaluation approaches
- **Deterministic Evaluation**: Procedures that produce consistent results across runs - needed to understand DPC's reliability advantage; quick check: can identify sources of randomness in evaluation procedures
- **Computational Efficiency**: Trade-offs between evaluation accuracy and resource requirements - needed to appreciate DPC's practical benefits; quick check: can quantify speedup benefits in terms of time/resources saved

## Architecture Onboarding

**Component Map**: Input data -> Feature Attribution Methods -> Perturbations (PC: random, DPC: directional) -> Model Predictions -> Fidelity Scores -> Evaluation

**Critical Path**: Feature attribution → Perturbation application → Prediction comparison → Score aggregation → Evaluation result

**Design Tradeoffs**: DPC sacrifices some evaluation granularity compared to exhaustive PC evaluation in exchange for 10x speedup and deterministic results. The directional approach may miss certain edge cases that random perturbations would catch, but provides more interpretable and consistent evaluations for typical use cases.

**Failure Signatures**: If DPC consistently disagrees with PC evaluations, this may indicate either a bug in implementation or cases where directional perturbations are not appropriate. Evaluation instability or high variance suggests issues with perturbation magnitude or attribution quality.

**First Experiments**: 1) Compare DPC and PC scores on simple linear models with known feature importance; 2) Test DPC sensitivity to perturbation magnitude across different attribution methods; 3) Evaluate DPC consistency across multiple runs on the same dataset to verify deterministic behavior.

## Open Questions the Paper Calls Out
The paper acknowledges that DPC should only be applied to evaluate local feature attribution methods, but doesn't thoroughly address potential edge cases where this assumption might break down. The exclusive focus on tabular data raises questions about generalizability to other data modalities like images or text where perturbation strategies differ significantly.

## Limitations
- The method's exclusive focus on tabular data limits generalizability to other data modalities like images or text where perturbation strategies differ significantly
- The comparison between DPC and PC is primarily based on computational efficiency rather than comprehensive assessment of whether DPC actually produces more faithful evaluations of attribution quality
- The claim about trustworthiness improvement is somewhat vague and would benefit from more concrete metrics beyond computational efficiency

## Confidence

**High**: The technical description of DPC as a modification of PC that incorporates directional information is well-articulated and mathematically sound. The reproducibility claims based on deterministic perturbation ordering appear valid.

**Medium**: The claim that DPC enables "holistic" evaluation alongside PC is reasonable but could benefit from more nuanced discussion of what aspects of attribution quality each metric captures. The comparison across 4,744 explanations provides reasonable sample size but doesn't address potential dataset-specific biases.

**Low**: The claim about trustworthiness improvement is somewhat vague and would benefit from more concrete metrics beyond computational efficiency.

## Next Checks
1. Test DPC on non-tabular datasets (images, text) to assess generalizability of the directional perturbation approach
2. Compare DPC and PC evaluations across different perturbation magnitudes to understand sensitivity to perturbation scale
3. Conduct user studies with domain experts to validate whether DPC-identified attributions align better with human intuition than PC-identified attributions