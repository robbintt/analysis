---
ver: rpa2
title: 'SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks'
arxiv_id: '2601.10282'
source_url: https://arxiv.org/abs/2601.10282
tags:
- koopman
- e-02
- systems
- training
- pinn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPIKE, a framework that improves Physics-Informed
  Neural Networks (PINNs) for solving differential equations by incorporating continuous-time
  Koopman operators. The key innovation is using Koopman theory to regularize PINNs,
  enforcing linear dynamics in a learned observable space while promoting sparse representations
  through L1 regularization.
---

# SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks

## Quick Facts
- arXiv ID: 2601.10282
- Source URL: https://arxiv.org/abs/2601.10282
- Authors: Jose Marie Antonio Miñoza
- Reference count: 40
- Key outcome: Framework achieves 2-184× improvements in temporal extrapolation, reduces non-zero generator entries by up to 5.7×, and enables post-hoc coefficient recovery with <1% error across 14 systems.

## Executive Summary
SPIKE introduces a framework that improves Physics-Informed Neural Networks (PINNs) by incorporating continuous-time Koopman operators. The method enforces linear dynamics in a learned observable space while promoting sparse representations through L1 regularization. This addresses PINNs' tendency to overfit within training domains and fail to generalize beyond them. The framework achieves significant improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy, particularly for stiff and chaotic systems.

## Method Summary
The SPIKE framework integrates Koopman theory into PINNs by learning an observable embedding space where dynamics become linear. A 4-layer MLP with 128 units serves as the backbone, mapping inputs to solutions that are then transformed through a combined polynomial library and learned MLP features. The generator matrix A ∈ R^{64×64} is trained with L1 regularization to enforce sparsity while minimizing both PDE residuals and Koopman consistency. Matrix exponential integration provides unconditional stability for stiff systems, and the framework operates with fixed hyperparameters across all tested PDEs and ODEs.

## Key Results
- 2-184× improvements in temporal extrapolation across 14 systems
- Reduces non-zero generator entries by up to 5.7× compared to unregularized PINNs
- Achieves coefficient recovery with <1% error for post-hoc symbolic interpretation
- Unconditional A-stability for stiff systems with eigenvalues |λ_max| > 10⁴

## Why This Works (Mechanism)

### Mechanism 1
Koopman regularization improves OOD generalization by enforcing global dynamical consistency rather than local PDE fitting. The Koopman loss constrains the learned solution manifold to admit linear dynamics in observable space, acting as a dynamics-aware regularizer that prevents learning spurious high-frequency components satisfying the PDE locally but extrapolating poorly. This works when the true system admits a finite-dimensional Koopman-invariant subspace approximately spanned by the learned observables.

### Mechanism 2
Continuous-time formulation dz/dt = Az preserves meaningful off-diagonal structure that discrete-time K = e^{A∆t} loses. As ∆t → 0, discrete K approaches identity: K ≈ I + A∆t, causing off-diagonal entries to vanish ("identity collapse"). Learning A directly via ż = Az keeps A_ij (i≠j) as ∆t-independent interaction rates between observables, maintaining the generator's interaction structure.

### Mechanism 3
Matrix exponential integration provides unconditional A-stability for stiff PDEs where explicit methods fail. For stiff systems with |λ_max| > 10⁴, Euler/RK4 exhibit training instability. The exact solution z(t+∆t) = e^{A∆t}z(t) via Padé approximation guarantees ||e^{A∆t}z|| ≤ ||z|| when Re(λ_i) ≤ 0, regardless of ∆t magnitude, enabling stable training for fourth-order PDEs like Cahn-Hilliard and Kuramoto-Sivashinsky.

## Foundational Learning

- Concept: **Koopman operator theory** - Core mathematical framework enabling linear representation of nonlinear dynamics in lifted observable space. Quick check: Can you explain why the Koopman operator K_t acting on observable g satisfies K_t g(x) = g(F_t(x)), and how this relates to the generator L = ∇g · f?

- Concept: **PINN training pathologies** - Understanding spectral bias and overfitting motivates why Koopman regularization helps. Quick check: Why does minimizing PDE residual L_physics not guarantee good extrapolation beyond the training domain?

- Concept: **Stiff ODEs and exponential integrators** - Fourth-order PDEs (Cahn-Hilliard, Kuramoto-Sivashinsky) require stable integration; matrix exponential is the key enabler. Quick check: What makes a system "stiff," and why does explicit Euler fail while e^{A∆t} succeeds?

## Architecture Onboarding

- Component map:
Input (x, t) -> PINN encoder (4-layer MLP, 128 units, tanh) -> Solution u(x,t) -> Observable embedding g(u) = [g_lib(u) ⊕ g_mlp(u)] -> Combined observables z ∈ R^64 -> Generator A ∈ R^{64×64} (L1-regularized) -> Koopman dynamics: ż = Az -> Integration: Euler / RK4 / EXPM

- Critical path:
1. Implement standard PINN backbone first; validate on Heat equation
2. Add observable embedding: start with polynomial-only library (degree 2), verify A learns non-trivial structure
3. Implement Euler Koopman loss; verify training converges
4. Add L1 sparsity; confirm A becomes sparse without degrading physics loss
5. For stiff systems, switch to EXPM integration

- Design tradeoffs:
Euler vs RK4 vs EXPM: Euler is fastest (<5% overhead) but conditionally stable; RK4 adds 4th-order accuracy; EXPM is unconditionally stable but ~25% overhead. Paper recommends EXPM only for stiff PDEs. Embedding dimension: 64 works across all 14 systems; larger dimensions require stronger regularization. Polynomial degree: d=2 captures quadratic nonlinearities; higher degrees showed diminishing returns. Loss weights: λ_koopman = 0.1, λ_sparse = 0.01 were fixed across all systems (no per-system tuning).

- Failure signatures:
PINN-only baseline on stiff PDEs: Cahn-Hilliard MSE 3.53e-1 (catastrophic); 2D Wave MSE 1.72e+4 — these indicate fundamental stiffness failure, not undertraining. High λ_koopman (≥0.5): Severely degrades Koopman R² < 0.1 (Table 16). Unstable learned A: Positive eigenvalues cause divergence in extrapolation (check max Re(λ) in Table 21). Over-regularized sparsity: If λ_sparse too high, A collapses to trivial dynamics, physics loss dominates.

- First 3 experiments:
1. Heat equation validation: Train PINN-only and PIKE-Euler on u_t = αu_xx with domain [0,1]². Measure in-domain MSE and OOD-time (t ∈ [1,3]). Expect ~1.5× improvement; verify A learns near-zero off-diagonals (diffusion is linear).
2. Burgers equation stress test: Test u_t + uu_x = νu_xx. Compare PINN vs PIKE-Euler vs SPIKE-EXPM on shock formation. Expect PIKE-Euler to outperform in OOD-time; verify latent correlations with u_xx (Table 26 shows 0.27 for Burgers).
3. Lorenz chaotic system: Train on t ∈ [0,1], evaluate valid prediction time. PIKE-Euler should achieve ~12 Lyapunov times vs ~0.06 for PINN (184× improvement). Monitor trajectory divergence visually (Figure 3 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
What are the rigorous approximation error bounds for neural network observables used in continuous-time Koopman formulations? The paper states that general error bounds for neural network observables remain open. Current theoretical foundations establish existence of finite-dimensional approximations but do not quantify how network architecture affects approximation quality of the generator. Resolution would require a theorem bounding the Koopman consistency error ε_K based on network capacity and spectral properties of the true generator.

### Open Question 2
Can the Lipschitz constant of the decoder (L_g) be estimated empirically to verify the OOD generalization bound? The paper notes that empirical estimation of L_g remains an open direction. Proposition 4 relies on the assumption of a Lipschitz decoder to bound extrapolation error, but this constant is currently treated as a theoretical abstraction rather than a measurable metric. Resolution would require an algorithm to compute or estimate L_g during training, correlated with observed OOD performance across different PDEs.

### Open Question 3
Does extending the observable library to include explicit derivative terms (u_x, u_xx) improve recovery for convective dynamics? The paper suggests derivative-augmented libraries as a promising extension because Latent Correlation Analysis shows low correlation between latent features and convective terms like u · u_x (0.01 for Burgers). The current polynomial-only library struggles to capture mixed derivative-polynomial terms implicitly, limiting symbolic recovery for non-linear PDEs. Resolution would require experiments showing higher correlation coefficients and accurate coefficient recovery for convective terms when using a derivative-augmented library.

## Limitations
The framework's effectiveness hinges on the assumption that the true dynamics admit a finite-dimensional Koopman-invariant subspace, which is not guaranteed for general nonlinear systems. The 64-dimensional embedding works across all tested systems but lacks systematic justification for when it may be insufficient. The fixed weight hyperparameters (λ_koopman=0.1, λ_sparse=0.01) were used across all 14 systems without per-system tuning, suggesting potential brittleness to problem-specific scaling.

## Confidence
- **High confidence**: Koopman regularization improves OOD generalization for non-stiff systems (2-184× improvements documented); continuous-time formulation preserves meaningful off-diagonal structure; matrix exponential provides unconditional stability for stiff systems.
- **Medium confidence**: The 64-dimensional embedding suffices for all tested systems; the fixed weight hyperparameters generalize well; the learned A matrix consistently recovers sparse dynamical structure.
- **Low confidence**: Performance on truly chaotic systems beyond Lorenz; robustness when λ_koopman >> λ_physics; scalability to higher-dimensional PDEs where 64-dimensional embeddings may be insufficient.

## Next Checks
1. **Dimensionality sensitivity analysis**: Systematically vary embedding dimension (16, 32, 64, 128) across Burgers, Kuramoto-Sivashinsky, and Navier-Stokes to determine when the Koopman representation becomes insufficient.

2. **Hyperparameter ablation study**: Test SPIKE performance across a grid of (λ_koopman, λ_sparse) values for each system class (parabolic, hyperbolic, chaotic) to verify the robustness of fixed weights.

3. **Long-term chaotic prediction benchmark**: Evaluate SPIKE on more chaotic systems (Rossler, double pendulum, or higher-dimensional Lorenz variants) beyond the 2D Lorenz tested, measuring valid prediction time against exponentially growing uncertainty.