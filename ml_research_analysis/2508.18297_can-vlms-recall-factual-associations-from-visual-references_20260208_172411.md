---
ver: rpa2
title: Can VLMs Recall Factual Associations From Visual References?
arxiv_id: '2508.18297'
source_url: https://arxiv.org/abs/2508.18297
tags:
- uni00000013
- uni00000044
- uni00000039
- uni0000002f
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a significant multimodal grounding gap in
  Vision Language Models (VLMs). While VLMs can recall factual knowledge when given
  textual references to entities, their ability to do so drops by over 50% when the
  reference is visual instead.
---

# Can VLMs Recall Factual Associations From Visual References?

## Quick Facts
- **arXiv ID:** 2508.18297
- **Source URL:** https://arxiv.org/abs/2508.18297
- **Reference count:** 40
- **Key outcome:** VLMs show a 50%+ performance drop in factual recall when references are visual vs. textual, with probes detecting linking failures in hidden states at >92% accuracy.

## Executive Summary
This paper identifies a significant multimodal grounding gap in Vision Language Models (VLMs). While VLMs can recall factual knowledge when given textual references to entities, their ability to do so drops by over 50% when the reference is visual instead. The study isolates this gap across multiple datasets and model families, showing that VLMs struggle to link their internal knowledge of entities with their image representations.

The authors analyze internal model states and observe that linking failures correlate with delayed confidence buildup in later layers. They train probes on these hidden states that can detect linking failures with over 92% accuracy. These probes generalize to out-of-domain datasets and, when used for selective prediction, improve coverage by 7.87% while reducing error risk by 0.9%. The findings suggest this grounding gap is systematic and not resolved by scaling, and the authors propose new training paradigms to address it.

## Method Summary
The paper constructs a benchmark comparing VLM performance on factual recall tasks when entity information is provided textually versus visually. Using image classification datasets (CIFAR100, Food101, Landmarks, MNIST), they generate QA pairs from Wikipedia using Llama-3.1-8B with extensive filtering. VLMs are evaluated in Text-Only (trivial image + textual question) vs Visual (entity image + visual question without entity name) settings. The authors extract layer 20 hidden states from last input token to train linear probes that classify linking success/failure, achieving >92% accuracy. Probes are converted to MCQA format for clearer token-level analysis and evaluated for generalization on OKVQA.

## Key Results
- VLMs show a 50%+ performance drop in factual recall when references are visual vs. textual across multiple datasets
- Linear probes trained on hidden states achieve >92% accuracy in detecting linking failures
- Selective prediction using these probes increases coverage by 7.87% while reducing error risk by 0.9%
- The grounding gap persists across different VLM families (InstructBLIP, LLaVA, GPT-4V) and is not resolved by scaling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Internal hidden states in the mid-to-late layers of the LLM component contain detectable signatures of visual grounding failures.
- **Mechanism:** When a VLM fails to link a visual entity to its internal knowledge, the hidden state vectors (specifically at Layer 20) exhibit distinct patterns and lower cosine similarity to states where full information is available. A linear probe can classify these "linking failure" states with high accuracy (>92%), effectively functioning as a learned uncertainty estimator.
- **Core assumption:** The geometric properties of hidden states correlate reliably with the model's functional success in retrieving facts, a relationship observed in LLMs but assumed here to extend to multimodal integration.
- **Evidence anchors:**
  - [abstract] "linking failures are correlated with the expression of distinct patterns in model internal states"
  - [section 6.1] "cases of linking failure build confidence in a VLM's prediction at later layers"
  - [corpus] "Large Language Models Do NOT Really Know What They Don't Know" (arXiv:2510.09033) warns that models may rely on shortcuts; assumption is that the probe detects actual retrieval failure rather than just low confidence.
- **Break condition:** If the linear probe overfits to specific dataset artifacts (e.g., image noise) rather than the semantic failure of linking, it will fail to generalize to Out-Of-Distribution (OOD) datasets like OKVQA.

### Mechanism 2
- **Claim:** The grounding gap is caused by a failure in the bridging module (projector) to map visual representations into the specific textual token space required to trigger factual recall in the LLM.
- **Mechanism:** The LLM backbone possesses the fact (e.g., "Stonehenge is in the UK") but relies on specific token embeddings to access it. The visual encoder provides a representation, but the current training paradigm (often caption-based) fails to align this visual representation with the precise textual "lookup key" needed to unlock the fact.
- **Core assumption:** Factual knowledge is stored in the LLM weights and accessible via textual tokens, but the visual modality provides a weak or misaligned activation signal.
- **Evidence anchors:**
  - [abstract] "Forcing VLMs to rely on image representations of an entity halves their ability to recall factual knowledge"
  - [section 5] "VLMs struggle to link their internal knowledge of entities with their image representation"
  - [corpus] "Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models" (arXiv:2412.14133) confirms a similar systematic disparity in entity knowledge extraction.
- **Break condition:** If the failure were due to the Vision Encoder failing to identify the object (rather than the bridge), the "Text Only" baseline (which uses a trivial image) would also fail, which it does not.

### Mechanism 3
- **Claim:** Selective prediction using probes improves utility by separating "visual recognition" failures from "factual recall" failures.
- **Mechanism:** Standard confidence metrics (like perplexity) often conflate uncertainty with hallucination. The probe specifically targets the "linking failure" mode. By ensembling the probe's decision with perplexity thresholds, the system abstains only when the specific failure mode (visual grounding) is detected, thereby increasing coverage (answering more questions) while lowering risk.
- **Core assumption:** The cost of abstaining is lower than the cost of a hallucinated fact (risk aversion).
- **Evidence anchors:**
  - [abstract] "probes increase coverage by 7.87% (absolute) while also reducing the risk of error by 0.9%"
  - [table 1] Shows the Ensemble method achieving higher coverage than Perplexity baseline alone.
  - [corpus] Corpus signals for "Evaluating Contextually Mediated Factual Recall" (arXiv:2601.12555) suggest context affects retrieval; assumption is probe isolates the visual context specifically.
- **Break condition:** If the risk tolerance is extremely low (e.g., zero-shot medical diagnosis), the 0.9% error reduction might be insufficient, requiring stricter thresholds.

## Foundational Learning

- **Concept: Mechanistic Interpretability (Probing)**
  - **Why needed here:** To understand *where* inside the VLM the visual and textual streams merge and fail. The paper relies on the intuition that Layer 20 hidden states represent the "realized context" just before prediction.
  - **Quick check question:** Can you explain why a linear probe on a single layer is sufficient to detect complex semantic failures?

- **Concept: The "Grounding Gap"**
  - **Why needed here:** This is the core systematic deficiency identified. It distinguishes between "the model doesn't know" vs. "the model can't access what it knows via this modality."
  - **Quick check question:** If a model answers "What is this?" correctly (recognition) but "Who built this?" incorrectly (fact), is that a grounding gap?

- **Concept: Selective Prediction**
  - **Why needed here:** This frames the practical application of the probes. It moves from "fixing the model" to "reliably deploying the flawed model."
  - **Quick check question:** How does "Coverage" differ from "Accuracy" in this context?

## Architecture Onboarding

- **Component map:**
  - Vision Encoder (Frozen) -> Bridging Module -> LLM Backbone (Frozen/Fine-tuned)
- **Critical path:**
  - Input Image -> Vision Encoder -> Bridging Module -> LLM Input Embeddings
  - *Crucial Failure Point:* The Bridging Module effectively translates visual data into a "foreign language" the LLM understands. If this translation lacks the semantic density of the entity's name (token), the LLM's internal knowledge remains dormant.
- **Design tradeoffs:**
  - **Probe vs. Perplexity:** Probes are more accurate (92% vs ~65%) but require training data (hidden states from known failures). Perplexity is zero-shot but less precise.
  - **Caption Pretraining vs. QA Pretraining:** Captioning optimizes for description; QA pretraining (proposed solution) optimizes for fact retrieval.
- **Failure signatures:**
  - **High Perplexity + Correct ID:** Model knows what it sees but can't recall facts.
  - **Late Confidence Buildup:** The correct answer token only accumulates probability in the very final layers (panic guessing) vs. early layers (robust retrieval).
- **First 3 experiments:**
  1. **Isolate the Gap:** Run the provided benchmark (Text-Only vs. Visual) on your target VLM to confirm the >50% performance drop.
  2. **Visualize Layer Dynamics:** Extract hidden states from the last input token across layers (0-32). Plot the probability of the correct answer token (Logit Lens) to confirm the "late confidence" signature.
  3. **Train the Failure Detector:** Train a linear classifier on Layer 20 states (labeling instances where Visual setting failed but Text setting passed) to predict if the model is about to hallucinate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating factual recall tasks into the visual-textual alignment phase effectively mitigate the grounding gap?
- **Basis in paper:** [explicit] In Section 7, the authors propose a "novel training paradigm" where the pretraining phase includes tuning the bridging module to maximize the likelihood of factual answers, rather than relying solely on image captioning.
- **Why unresolved:** This is presented as an "informed recommendation" for future work; the authors analyze the failure but do not implement or test this specific training modification in the current study.
- **What evidence would resolve it:** A study comparing VLMs trained with standard captioning alignment against those trained with the proposed factual recall alignment, showing a statistically significant reduction in the performance gap between textual and visual reference settings.

### Open Question 2
- **Question:** Does the multimodal grounding gap exist in other multimodal architectures, such as audio-language models?
- **Basis in paper:** [explicit] In Section 9 (Limitations), the authors state they focused exclusively on VLMs and explicitly leave the existence of similar gaps in other multimodal LMs (e.g., audio) as an "open question."
- **Why unresolved:** The study restricted its scope to vision-language models (LLaVA, InstructBLIP, GPT-4V) and datasets involving visual entity recognition.
- **What evidence would resolve it:** Applying the paper's controlled experimental protocol—comparing factual recall from textual references vs. sensory references (e.g., audio clips)—to audio-language models to see if a similar performance drop occurs.

### Open Question 3
- **Question:** Which specific component of the VLM architecture or training pipeline (e.g., frozen vs. trainable LM, data distribution) is the primary cause of the grounding gap?
- **Basis in paper:** [explicit] Section 9 states the authors "do not identify which particular design decision... leads to this grounding gap," though they note it is systematic across various paradigms.
- **Why unresolved:** The paper establishes that the gap exists across different families (InstructBLIP, LLaVA, GPT-4V) but does not isolate the root cause, suggesting it is a capability never fully learned rather than a specific flaw introduced by one training stage.
- **What evidence would resolve it:** An ablation study isolating variables such as the vision encoder type, the preservation of the LM's internal knowledge during fusion, and the scale of image-text pairs to identify which factor most strongly correlates with the severity of the linking failure.

## Limitations

- **Dataset Dependence:** The grounding gap is demonstrated across controlled datasets (CIFAR100, Food101, Landmarks, MNIST) but may vary with different vision encoders, bridging mechanisms, or types of visual ambiguity.
- **Probe Interpretability:** The linear probe achieves high accuracy but may detect dataset-specific artifacts rather than the fundamental semantic failure of visual-to-textual grounding.
- **Practical Utility Trade-offs:** The 7.87% coverage increase and 0.9% error reduction are promising but depend heavily on application-specific risk tolerance and may not hold in all domains.

## Confidence

**High Confidence:** The existence of a systematic multimodal grounding gap (Text vs. Visual performance difference) is well-supported by controlled experiments across multiple datasets and model families. The basic finding that visual references reduce factual recall accuracy by over 50% is robust.

**Medium Confidence:** The probe-based failure detection mechanism and selective prediction improvements are validated, but the generalizability to arbitrary VLMs and real-world scenarios requires further testing. The mechanistic interpretation of hidden state patterns is suggestive but not definitive.

**Low Confidence:** The proposed training paradigm solutions (caption vs. QA pretraining) are discussed but not empirically validated in the paper. The claim that this gap is "not resolved by scaling" is based on limited model families and may not hold for future, larger architectures.

## Next Checks

1. **Probe Generalization Stress Test:** Train the failure detection probe on a diverse set of VLM architectures (different vision encoders, bridging mechanisms, LLM backbones) and test on datasets with increasing visual complexity and ambiguity. This would validate whether the probe truly detects the semantic grounding failure or just dataset-specific patterns.

2. **Ablation of Bridging Mechanism:** Systematically vary the bridging module (different projector architectures, training objectives) while keeping the LLM backbone constant to isolate whether the grounding gap is primarily a bridging problem or indicates deeper architectural limitations in multimodal integration.

3. **Real-world Grounding Evaluation:** Apply the benchmark to a dataset of real-world images where entities are not perfectly centered or clearly visible (e.g., street scenes, crowded environments). This would test whether the grounding gap persists in naturalistic visual contexts and assess the practical relevance of the findings.