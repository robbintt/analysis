---
ver: rpa2
title: 'Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?'
arxiv_id: '2510.10457'
source_url: https://arxiv.org/abs/2510.10457
tags:
- ranking
- benchmark
- arxiv
- redundancy
- subset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficiently evaluating large
  language models (LLMs) by compressing benchmark datasets while preserving both accuracy
  reconstruction and ranking consistency. The authors observe that existing benchmarks
  contain substantial sample redundancy at both textual and ranking levels, which
  inflates evaluation costs without adding information.
---

# Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?

## Quick Facts
- arXiv ID: 2510.10457
- Source URL: https://arxiv.org/abs/2510.10457
- Reference count: 36
- Primary result: Achieves up to 200× benchmark compression while maintaining ranking fidelity

## Executive Summary
This paper introduces EssenceBench, a framework for compressing large language model evaluation benchmarks by up to 200× while preserving both accuracy reconstruction and model ranking consistency. The authors demonstrate that existing benchmarks contain substantial sample redundancy at both textual and ranking levels, enabling significant compression without meaningful loss of evaluation quality. Through a combination of coarse filtering, genetic algorithm-based subset search, and attribution-guided refinement, EssenceBench outperforms prior methods in reconstruction error and ranking preservation across five standard benchmarks.

## Method Summary
EssenceBench operates in three stages: (1) coarse filtering removes redundant samples based on text and ranking similarity thresholds, (2) a genetic algorithm with GAM-predicted fitness searches for compact representative subsets, and (3) attribution-based grouping refines selection by partitioning samples into high, low, and random attribution groups and running GA within each. The method takes score matrices from evaluated LLMs and selects k-element subsets that minimize reconstruction error while preserving model rankings.

## Key Results
- Achieves up to 200× compression on standard benchmarks
- Maintains ranking fidelity with ≤5% error using 95% fewer samples
- Outperforms prior methods in both prediction error and ranking preservation
- Demonstrated effectiveness across five diverse benchmarks (GSM8K, ARC, HellaSwag, WinoGrande, MMLU)

## Why This Works (Mechanism)

### Mechanism 1: Redundancy-Aware Coarse Filtering
The framework first identifies and removes samples with high text or ranking similarity, based on the observation that redundant samples provide diminishing marginal information. This reduces the search space while preserving discriminative information. If thresholds are too aggressive, diverse but difficult samples may be erroneously removed, increasing reconstruction error.

### Mechanism 2: Fitness-Based Genetic Algorithm for Subset Search
GA-based search over binary masks finds compact subsets that minimize accuracy reconstruction error more effectively than gradient- or perplexity-based heuristics. The subset-to-accuracy mapping learned by GAM guides GA fitness evaluation. If GAM predictor is mis-calibrated, GA converges to suboptimal subsets with high reconstruction error.

### Mechanism 3: Attribution-Guided Sample Selection with Grouping
Partitioning samples by attribution scores and re-running GA per group improves diversity and mitigates premature convergence. High-attribution samples carry primary signal while low- and random-attribution samples capture underrepresented patterns. If attribution scores are noisy or EBM overfits, group partition may misallocate informative samples, degrading selection quality.

## Foundational Learning

- **Concept: Benchmark Compression as Optimization (Definition 1–2)**
  - Why needed here: The paper formulates compression as an NP-hard combinatorial search over subsets to minimize reconstruction loss
  - Quick check question: Can you explain why selecting an optimal k-subset from N samples is combinatorially hard?

- **Concept: Genetic Algorithm Operators (Selection, Crossover, Mutation, Adjustment)**
  - Why needed here: GA drives the subset search; understanding operators is essential to diagnose convergence and diversity issues
  - Quick check question: What is the role of the adjustment step after mutation in this context?

- **Concept: Generalized Additive Models (GAM) and Explainable Boosting Machines (EBM)**
  - Why needed here: GAM predicts full-set accuracy from subset scores (fitness); EBM provides interpretable attributions for sample grouping
  - Quick check question: Why use EBM instead of a standard black-box regressor for attribution?

## Architecture Onboarding

- **Component map:** Data layer (score matrices) -> Coarse filter (text/ranking thresholds) -> GA engine (population/masks/fitness) -> Attribution module (EBM training/attribution scores) -> Grouping selector (tri-partition/GA per group) -> Iterative controller (convergence check)
- **Critical path:** Coarse filtering → GA initialization → fitness evaluation (GAM) → elite selection → EBM attribution → group partition → per-group GA → best mask update → iterate
- **Design tradeoffs:** Stricter redundancy thresholds reduce search space faster but may drop useful samples; more GA generations improve error but increase compute; attribution grouping helps most at small k sizes
- **Failure signatures:** RMSE plateaus early (GA stuck, increase diversity/mutation); ranking preservation drops at small k (filter too aggressive or grouping unbalanced); long compression times (M still large after filtering, raise thresholds or cap generations)
- **First 3 experiments:** 1) Reproduce RMSE vs coreset size on GSM8K for Random, MetaBench, EssenceBench; 2) Ablate coarse filtering to quantify contribution at k=50, 200, 500; 3) Sweep (N_G, R_max) pairs to find compute–error tradeoff for a target dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the optimal thresholds for text and ranking redundancy filtering, and can they be theoretically derived rather than empirically tuned?
- **Basis in paper:** The paper uses thresholds τ_text and τ_ranking for coarse filtering but does not provide theoretical justification for specific values
- **Why unresolved:** The thresholds are dataset-dependent, and the paper does not establish a principled method for selecting them across different benchmark types
- **What evidence would resolve it:** A systematic study showing the relationship between threshold values and compression-performance tradeoffs across diverse benchmarks

### Open Question 2
- **Question:** Can EssenceBench generalize to evaluation settings with non-binary scoring (e.g., continuous metrics, human preference rankings)?
- **Basis in paper:** The method relies on binary score matrices from the Open LLM Leaderboard, assuming right/wrong outcomes
- **Why unresolved:** Many LLM evaluation scenarios involve nuanced scoring that cannot be reduced to binary correctness
- **What evidence would resolve it:** Experiments applying EssenceBench to benchmarks with continuous or ordinal scores

### Open Question 3
- **Question:** How does compression quality degrade when historical score matrix data is limited or unavailable for new benchmarks?
- **Basis in paper:** The approach depends on pre-existing score matrices from many evaluated models, which may not exist for novel benchmarks
- **Why unresolved:** New evaluation domains may lack sufficient prior model evaluations to construct informative score matrices
- **What evidence would resolve it:** A study simulating sparse score matrices by progressively removing model evaluations and measuring impact on subset selection quality

## Limitations
- Coarse filtering thresholds (τ_text, τ_ranking) lack theoretical justification and appear dataset-dependent
- GA performance is sensitive to hyperparameter settings that could significantly impact results
- Attribution-based grouping mechanism lacks external corpus validation and may be sensitive to EBM model quality

## Confidence

- **High confidence:** Overall framework architecture and benchmark redundancy existence are well-supported by experimental evidence
- **Medium confidence:** GA-based subset search effectiveness, as results show improvements but lack direct comparison with alternative methods
- **Medium confidence:** Attribution-based grouping benefits, as improvements are demonstrated but mechanism lacks external validation

## Next Checks

1. Conduct ablation studies systematically varying τ_text and τ_ranking thresholds across all five benchmarks to identify optimal ranges and dataset dependencies
2. Implement and compare alternative subset search methods (greedy, gradient-based) against the GA approach on identical benchmark compression tasks
3. Test the attribution grouping mechanism on held-out benchmarks or synthetic datasets to verify that improvements generalize beyond reported experimental results