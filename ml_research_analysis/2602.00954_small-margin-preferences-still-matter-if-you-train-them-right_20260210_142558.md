---
ver: rpa2
title: Small-Margin Preferences Still Matter-If You Train Them Right
arxiv_id: '2602.00954'
source_url: https://arxiv.org/abs/2602.00954
tags:
- pairs
- preference
- arxiv
- score
- difficult
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether preference pairs with small margins
  (difficult pairs) can still provide useful training signals despite being typically
  filtered out due to likelihood displacement issues. The authors show that difficulty
  interacts strongly with the optimization objective: while preference-based losses
  destabilize training on difficult pairs, the same pairs remain informative when
  trained with supervised fine-tuning (SFT).'
---

# Small-Margin Preferences Still Matter-If You Train Them Right

## Quick Facts
- arXiv ID: 2602.00954
- Source URL: https://arxiv.org/abs/2602.00954
- Reference count: 36
- Primary result: Difficulty-aware training strategy MixDPO outperforms DPO by routing difficult preference pairs to SFT and easy pairs to preference loss

## Executive Summary
This paper challenges the common practice of filtering out small-margin preference pairs in alignment training. The authors demonstrate that these "difficult" pairs destabilize preference-based losses like DPO due to likelihood displacement, but still contain valuable supervision signals when trained with SFT. Based on this insight, they propose MixDPO, a two-stage training strategy that applies DPO to easy pairs and SFT to difficult pairs, using curriculum-style ordering from easy to hard. Across three LLM-judge benchmarks, MixDPO consistently outperforms DPO and variants, with notable gains on AlpacaEval 2 LC win rate.

## Method Summary
MixDPO is a two-stage difficulty-aware training strategy for preference alignment. First, preference pairs are sorted by rating score margin (difficulty = sw - sl) in descending order. Easy pairs (margin ≥ τ) are trained with DPO loss while difficult pairs (margin < τ) are trained with SFT loss on the chosen response only. The paper uses τ=0.5 for UltraFeedback and implements a curriculum approach where easy pairs are processed first. This routing mechanism leverages SFT's stability on ambiguous pairs while preserving DPO's effectiveness on clearly distinguishable preferences.

## Key Results
- MixDPO consistently outperforms vanilla DPO and several variants across three LLM-judge benchmarks
- Notable gains on AlpacaEval 2 LC win rate, demonstrating improved alignment quality
- The approach generalizes across different base models (LLaMA-3-8B, Mistral-7B, Qwen-2.5-7B) and datasets
- MixDPO is compatible with other DPO variants, showing flexibility in application

## Why This Works (Mechanism)

### Mechanism 1: Likelihood Displacement in Preference Losses
When y_w and y_l are similar, DPO-style losses on small-margin pairs cause both preferred and dispreferred response probabilities to decrease, undermining the intended preference signal. The model cannot reliably distinguish which response to reinforce, leading to gradient instability.

### Mechanism 2: Objective-Data Difficulty Matching
Routing difficult pairs to SFT loss preserves supervision signal while avoiding the gradient instability of preference objectives. SFT maximizes log-likelihood of the preferred response without requiring discrimination against a similar dispreferred response.

### Mechanism 3: Curriculum-Style Data Ordering
Training from easy to difficult pairs improves convergence and final alignment compared to random ordering. Easy pairs establish a stable preference boundary early, and the model encounters increasingly ambiguous cases with a better-initialized preference representation.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed here: The paper's proposed method modifies DPO's training dynamics; understanding the base objective is prerequisite.
  - Quick check question: Can you explain why DPO uses a reference model and what happens when π_θ diverges too far from π_ref?

- **Curriculum Learning**
  - Why needed here: MixDPO implements a curriculum over preference difficulty; understanding pacing strategies informs threshold selection.
  - Quick check question: How would you measure whether curriculum pacing is helping versus simply using data filtering?

- **Likelihood Displacement**
  - Why needed here: This phenomenon motivates the entire approach; recognizing it in training logs is critical for debugging.
  - Quick check question: During DPO training, what would you monitor to detect if both chosen and rejected probabilities are decreasing?

## Architecture Onboarding

- **Component map:**
  Rating scores -> Difficulty scorer (computes M = sw - sl) -> Threshold gate (z = 1 if M < τ) -> Two-stage trainer (DPO on easy, SFT on difficult)

- **Critical path:**
  1. Verify rating scores exist in dataset; if missing, generate via LLM judge or reward model
  2. Sort pairs by margin descending (easiest first)
  3. Select threshold τ (paper uses 0.5 for UltraFeedback, ~10% most difficult)
  4. Train DPO on easy subset
  5. Train SFT on y_w from difficult subset

- **Design tradeoffs:**
  - Lower τ → fewer SFT pairs → more preference signal, higher displacement risk
  - Higher τ → more SFT pairs → more stable but potentially underutilizes preference structure
  - Rating scores are free but may be noisy; reward model margins are costlier but potentially more reliable

- **Failure signatures:**
  1. LC win rate improves but raw win rate drops → model generating shorter responses (SFT over-regularization)
  2. Training loss diverges on DPO stage → threshold too low, difficult pairs leaking into DPO
  3. No improvement over vanilla DPO → difficulty metric not correlating with actual model difficulty

- **First 3 experiments:**
  1. Reproduce Figure 4a on your dataset: compare (a) DPO+sorted, (b) discard difficult, (c) MixDPO to isolate each component's contribution
  2. Sweep τ ∈ {0.5, 1.0, 1.5} to find optimal difficulty cutoff; monitor both LC and raw win rates
  3. Substitute rating score margin with reward model margin or DPO loss as difficulty metric; compare alignment performance and computational overhead

## Open Questions the Paper Calls Out

### Open Question 1
Are there alternative optimization strategies more effective than supervised fine-tuning (SFT) for extracting value from difficult preference pairs? The authors state this presents a promising direction for future research, as they only evaluated SFT as the fallback mechanism.

### Open Question 2
Can a more robust difficulty metric be developed that outperforms the raw rating score margin used in this study? The current metric relies on LLM-generated scores which can be noisy, potentially leading to suboptimal curriculum ordering.

### Open Question 3
How can the MixDPO framework be made robust to the noise and inaccuracies inherent in LLM-generated rating scores? The method currently depends on the accuracy of the score margin; noise could misclassify easy pairs as difficult.

## Limitations

- The study assumes rating score margins correlate with actual model learning difficulty, but this relationship is not validated across different model architectures or datasets
- The precise two-stage training implementation remains unclear—whether DPO and SFT run in separate epochs or interleaved could affect stability
- The SFT phase's exact configuration (whether it uses π_ref or only π_θ) is unspecified, which may impact reproducibility

## Confidence

- **High confidence**: The core empirical finding that SFT on difficult pairs outperforms discarding them or continuing DPO on the same pairs
- **Medium confidence**: The likelihood displacement mechanism explanation, as the paper provides limited direct evidence showing both probabilities decreasing simultaneously
- **Medium confidence**: The curriculum-style ordering benefit, as the contribution of sorting versus other factors is not fully isolated

## Next Checks

1. Test reversed curriculum: Train MixDPO with difficult pairs first, then easy pairs. If performance matches or exceeds the proposed ordering, the benefit may stem from data stratification rather than curriculum effects.

2. Cross-dataset difficulty calibration: Apply MixDPO to a dataset without pre-computed rating scores, using reward model margins instead. Compare alignment performance to establish whether rating-based difficulty generalizes.

3. Ablation of SFT reference model: Run MixDPO with and without π_ref during the SFT phase to determine whether the reference model is necessary for the observed benefits or if π_θ alone suffices.