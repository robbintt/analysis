---
ver: rpa2
title: 'SEA: Self-Evolution Agent with Step-wise Reward for Computer Use'
arxiv_id: '2508.04037'
source_url: https://arxiv.org/abs/2508.04037
tags:
- agent
- training
- tasks
- task
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing efficient computer
  use agents that can autonomously operate computers to complete user tasks. Existing
  agents struggle with sparse rewards in long-horizon tasks, high computational costs,
  and lack of alignment between reasoning and action.
---

# SEA: Self-Evolution Agent with Step-wise Reward for Computer Use

## Quick Facts
- arXiv ID: 2508.04037
- Source URL: https://arxiv.org/abs/2508.04037
- Reference count: 40
- Primary result: SEA achieves 30.1% task success rate on OSWorld benchmark with only 7B parameters, outperforming larger models

## Executive Summary
This paper addresses the challenge of developing efficient computer use agents that can autonomously operate computers to complete user tasks. Existing agents struggle with sparse rewards in long-horizon tasks, high computational costs, and lack of alignment between reasoning and action. The authors propose SEA (Self-Evolution Agent), which integrates three key innovations: a closed-loop pipeline for generating verifiable task trajectories, Trajectory Reasoning by Step-wise Reinforcement Learning (TR-SRL) to enable efficient learning through step-wise rewards, and Grounding-Based Generalization Enhancement that merges grounding and planning capabilities into a single model. SEA achieves state-of-the-art performance on the OSWorld benchmark with only 7B parameters, surpassing larger models (e.g., 72B parameters) while demonstrating strong grounding capabilities on ScreenSpot datasets.

## Method Summary
SEA introduces a novel approach to computer use agents by combining three innovations. First, it employs a closed-loop pipeline that generates verifiable task trajectories, addressing the challenge of sparse rewards in long-horizon tasks. Second, it implements Trajectory Reasoning by Step-wise Reinforcement Learning (TR-SRL), which provides step-wise rewards to enable more efficient learning compared to traditional sparse reward structures. Third, it introduces Grounding-Based Generalization Enhancement that merges grounding and planning capabilities into a single model using model merging and Temporal Compressed Sensing Mechanism (TCSM). This architecture allows SEA to achieve strong performance with only 7B parameters while maintaining efficiency and generalization capabilities across different computer use scenarios.

## Key Results
- SEA achieves 30.1% task success rate on OSWorld benchmark, surpassing larger models (72B parameters)
- Strong grounding capabilities with state-of-the-art performance on ScreenSpot-Pro and ScreenSpot-V2 datasets
- Demonstrates computational efficiency with only 7B parameters while maintaining competitive performance
- Successfully integrates reasoning and action alignment through step-wise reward mechanism

## Why This Works (Mechanism)
The effectiveness of SEA stems from its multi-faceted approach to addressing key challenges in computer use agents. The closed-loop pipeline generates verifiable trajectories that provide clearer learning signals compared to traditional sparse reward structures. The step-wise reward mechanism in TR-SRL creates more frequent feedback opportunities, enabling more efficient learning and better alignment between reasoning and action. The grounding-based generalization enhancement through model merging and TCSM allows the agent to maintain both grounding and planning capabilities within a single model, reducing computational overhead while improving performance. The temporal compressed sensing mechanism helps capture temporal dependencies in sequential decision-making tasks, enabling better handling of long-horizon tasks.

## Foundational Learning
- **Step-wise reward learning**: Provides more frequent feedback signals compared to sparse rewards, enabling faster convergence and better alignment between reasoning and action
- **Temporal Compressed Sensing Mechanism (TCSM)**: Captures temporal dependencies in sequential decision-making tasks, essential for handling long-horizon computer use scenarios
- **Model merging**: Combines grounding and planning capabilities into a single model, reducing computational overhead while maintaining performance
- **Closed-loop trajectory generation**: Creates verifiable task trajectories that provide clearer learning signals for reinforcement learning
- **Computer vision grounding**: Enables agents to understand and interact with graphical user interfaces effectively

## Architecture Onboarding

**Component Map**: Closed-loop pipeline -> TR-SRL module -> Grounding enhancement -> Output agent

**Critical Path**: The agent receives user task -> Closed-loop pipeline generates trajectory -> TR-SRL learns from step-wise rewards -> Grounding enhancement merges capabilities -> Agent executes actions

**Design Tradeoffs**: The 7B parameter model size balances computational efficiency with performance, sacrificing some potential accuracy gains from larger models for practical deployment. The step-wise reward approach trades immediate feedback for potential reward hacking concerns.

**Failure Signatures**: Poor trajectory generation quality in the closed-loop pipeline can lead to suboptimal learning signals. Ineffective step-wise reward design may cause the agent to focus on short-term gains rather than long-term task completion. Inadequate grounding can result in misinterpretation of screen elements and incorrect actions.

**First Experiments**:
1. Test closed-loop pipeline trajectory generation on diverse computer use scenarios to verify robustness
2. Evaluate step-wise reward effectiveness on tasks of varying lengths to assess scalability
3. Assess grounding capabilities on ScreenSpot datasets with different GUI complexity levels

## Open Questions the Paper Calls Out
None identified in the provided analysis.

## Limitations
- The closed-loop pipeline's ability to generate genuinely verifiable trajectories across diverse real-world scenarios remains uncertain
- The effectiveness of step-wise rewards may be limited by reward signal design quality and potential for reward hacking
- The paper lacks detailed ablation studies showing how each component contributes to overall performance
- The temporal compressed sensing mechanism's handling of temporal dependencies needs more detailed explanation

## Confidence
- OSWorld benchmark performance claims: Medium
- Step-wise reward effectiveness: Medium
- Grounding and planning integration: Medium
- Computational efficiency claims: Medium

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component (closed-loop pipeline, TR-SRL, and grounding enhancement) to overall performance
2. Test SEA on diverse, real-world computer use scenarios beyond controlled benchmark environments to assess practical robustness
3. Evaluate the model's performance on longer-horizon tasks (beyond 10 steps) to verify scalability of the step-wise reward approach