---
ver: rpa2
title: 'Adapting Definition Modeling for New Languages: A Case Study on Belarusian'
arxiv_id: '2507.09536'
source_url: https://arxiv.org/abs/2507.09536
tags:
- definition
- data
- modeling
- language
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how to adapt existing definition modeling systems
  to Belarusian. The authors propose a new dataset of 43,150 definitions and experiment
  with finetuning multilingual models on subsets of this data.
---

# Adapting Definition Modeling for New Languages: A Case Study on Belarusian

## Quick Facts
- arXiv ID: 2507.09536
- Source URL: https://arxiv.org/abs/2507.09536
- Authors: Daniela Kazakouskaya; Timothee Mickus; Janine Siewert
- Reference count: 18
- Primary result: Minimal data (1%) suffices for cross-lingual definition model adaptation to Belarusian

## Executive Summary
This paper investigates how to adapt existing definition modeling systems to Belarusian, a low-resource language. The authors create a new dataset of 43,150 Belarusian definitions and experiment with fine-tuning multilingual models on subsets of this data. Results demonstrate that even minimal training data (1% of the corpus) is sufficient to produce fluent Belarusian definitions, while larger datasets improve informativeness and reduce circular definitions. The study also reveals significant limitations in standard automatic evaluation metrics, with character-level and specific BLEURT variants better capturing definition quality than traditional n-gram overlap metrics.

## Method Summary
The researchers created the TSBM dataset from the Skarnik dictionary, consisting of 43,150 Belarusian definitions. They fine-tuned the mT0-XL multilingual model (3.7B parameters) on this data using logarithmically spaced subsets (1%, ~3.16%, 10%, ~31.62%, 100%). The input format followed the pattern `"[EXAMPLE] Что такое [HEADWORD]?"` and evaluation included automatic metrics (BLEU, BERTScore, chrF++, BLEURT variants) plus manual annotation of fluency, informativeness, and circularity by native speakers.

## Key Results
- 1% of training data (~430 examples) produces fluent Belarusian definitions with high language identification scores
- Fluency remains consistently high across all data sizes, while informativeness scales with dataset size
- chrF++ and specific BLEURT variants (D6, D12) correlate better with human judgments than standard BLEU or BERTScore
- Manual evaluation reveals gaps in current automatic metrics for definition modeling quality assessment

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual Transfer via Multilingual Representations
Pre-trained multilingual models encode language-agnostic syntactic patterns that transfer across languages. When fine-tuned on minimal Belarusian data, mT0-XL rapidly shifts output language probability toward Belarusian while maintaining grammatical fluency inherited from pre-training. This works because the pre-training corpus included sufficient linguistic diversity for the model to learn transferable syntactic patterns.

### Mechanism 2: Syntactic-Fluency vs. Semantic-Precision Decoupling
Fluency and informativeness scale differently with training data because they rely on separable model capabilities. Syntactic fluency transfers readily from pre-training, while semantic precision requires task-specific learning. This explains why fluency remains high with minimal data while informativeness requires full dataset exposure.

### Mechanism 3: Metric-Task Alignment Varies by Evaluation Dimension
Standard NLG metrics correlate differently with human judgments depending on the quality dimension being assessed. Character-level metrics (chrF++) and specific neural metrics (BLEURT variants) better capture definition-specific quality criteria than standard n-gram overlap metrics, which miss semantic appropriateness and circularity.

## Foundational Learning

- **Contextualized Definition Modeling**: Why needed? This paper formulates definition modeling as generating definitions for words in context, not in isolation. The input format encodes both the example sentence and target word. Quick check: Given `"[EXAMPLE] The bank closed at 5pm. Что такое bank?"`, why would the model need to generate a different definition than for `"[EXAMPLE] She sat on the river bank. Что такое bank?"`?

- **Multilingual Transfer Learning**: Why needed? The entire experimental approach relies on fine-tuning a multilingual pre-trained model on a new language. Understanding that pre-training creates language-agnostic representations that can be rapidly adapted is essential to interpreting why minimal data suffices. Quick check: Why can a model fine-tuned on Russian definitions generate fluent Belarusian after seeing only ~430 Belarusian examples?

- **NLG Evaluation Metrics and Their Biases**: Why needed? The paper's core contribution includes showing that standard metrics poorly correlate with human judgments for definition modeling. You need to understand what each metric captures to select appropriate evaluation approaches. Quick check: Why would BLEU assign a high score to a definition that uses the headword itself in the output (a circular definition)?

## Architecture Onboarding

- **Component map**: mT0-XL (3.7B) -> Input formatter (context + headword) -> Training subsets (log-spaced) -> Language identifier (langid.py) -> Automatic evaluators (BLEU, BERTScore, chrF++, BLEURT) -> Manual evaluator (native speaker annotation)

- **Critical path**: 1. Data preparation: Clean dictionary entries → remove functional words → add POS tags → split ensuring no headword leakage 2. Model fine-tuning: Start from existing Russian definition model → fine-tune on Belarusian subsets 3. Language validation: Run langid.py on outputs to confirm language shift toward Belarusian 4. Automatic evaluation: Compute all metrics against reference definitions 5. Manual evaluation: Sample outputs stratified by homograph status → annotate fluency/informativeness/circularity 6. Correlation analysis: Spearman correlation between automatic metrics and human judgments

- **Design tradeoffs**: Russian base model vs Norwegian/English (performance parity); 1% data for fluency-only vs 100% for informativeness; chrF++ for informativeness vs BLEURT D6 for fluency

- **Failure signatures**: Zero-shot generation outputs base language; high circularity (>25%) indicates insufficient data; low langid p(bel) (<0.7) suggests data quality issues; metric-human disagreement expected with standard metrics

- **First 3 experiments**: 1. Establish zero-shot baseline: Run pre-trained Russian model on Belarusian test set without fine-tuning. Record automatic metrics and language identification scores. 2. Minimal adaptation test: Fine-tune on 1% of data (~430 examples). Measure language shift (p(bel) > 0.9), fluency (~0.78), and informativeness (~0.32). 3. Metric validation subset: Select 50 outputs each from 1% and 100% models, conduct manual evaluation, compute Spearman correlations between all automatic metrics and human judgments.

## Open Questions the Paper Calls Out

- How can automatic evaluation metrics be developed to accurately capture fluency and informativeness in generated definitions? The authors note that standard metrics fail to capture fluency, and none of the tested metrics successfully captured it despite neural metrics correlating better with human judgment than overlap metrics.

- Does the finding that minimal data suffices for adaptation hold for languages typologically distant from the base model's pretraining data? The success with Belarusian may be influenced by its similarity to Russian; it's unclear if this efficiency applies to truly low-resource or unrelated languages.

- How can models be improved to reduce the reliance on morphological shortcuts that lead to partial circularity in definitions? While increasing dataset size reduced full circularities, it did not significantly lower partial circularities (using derivational forms), suggesting the model relies on surface-level morphological patterns.

## Limitations
- Results rely on a single 3.7B parameter mT0-XL model architecture, limiting generalizability to other model families or sizes
- The TSBM dataset represents a single Belarusian dictionary source, raising questions about domain specificity and coverage
- The paper demonstrates effectiveness for Belarusian but does not test the approach across multiple typologically diverse languages

## Confidence

- **High confidence**: The finding that 1% of data ensures Belarusian fluency (p(bel) > 0.8) is well-supported by quantitative results across multiple training seeds
- **Medium confidence**: The claim that informativeness scales with data size is supported but based on manual evaluation of only 77 samples
- **Medium confidence**: The assertion that standard NLG metrics poorly capture definition quality is supported by correlation analysis but relies on a limited manual evaluation sample

## Next Checks
1. Test the minimal-data adaptation hypothesis on a more typologically distant language (e.g., Turkish or Finnish) to verify the cross-lingual transfer mechanism holds across language families
2. Conduct a larger-scale manual evaluation (n=500+) to strengthen confidence in the fluency-informativeness scaling relationship and metric-task alignment findings
3. Replicate experiments with a different multilingual model architecture (e.g., BLOOMZ or M3P) to verify results are not model-specific