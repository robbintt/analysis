---
ver: rpa2
title: 'CLEANet: Robust and Efficient Anomaly Detection in Contaminated Multivariate
  Time Series'
arxiv_id: '2510.22619'
source_url: https://arxiv.org/abs/2510.22619
tags:
- anomaly
- uni00000013
- detection
- data
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unsupervised anomaly detection
  in multivariate time series (MTS) with contaminated training data. The proposed
  CLEANet introduces a Contamination-Resilient Training Framework (CRTF) that mitigates
  the impact of corrupted samples through adaptive reconstruction weighting and clustering-guided
  contrastive learning, combined with a lightweight conjugate MLP architecture that
  disentangles temporal and cross-feature dependencies.
---

# CLEANet: Robust and Efficient Anomaly Detection in Contaminated Multivariate Time Series

## Quick Facts
- arXiv ID: 2510.22619
- Source URL: https://arxiv.org/abs/2510.22619
- Reference count: 40
- Achieves up to 73.04% higher F1 scores and 81.28% lower runtime than state-of-the-art baselines on contaminated MTS

## Executive Summary
This paper introduces CLEANet, a novel approach for unsupervised anomaly detection in multivariate time series (MTS) contaminated with anomalies during training. The core innovation is a Contamination-Resilient Training Framework (CRTF) that reduces the influence of corrupted samples through adaptive reconstruction weighting and clustering-guided contrastive learning. CLEANet also employs a lightweight conjugate MLP architecture that efficiently disentangles temporal and cross-feature dependencies. Experimental results show that CLEANet outperforms ten state-of-the-art baselines by up to 73.04% in F1 score and reduces runtime by 81.28%, while also shrinking model size by 90% and inference time by 67%.

## Method Summary
CLEANet addresses the challenge of anomaly detection in MTS when training data is contaminated with anomalies. It introduces the Contamination-Resilient Training Framework (CRTF), which uses adaptive reconstruction weighting to down-weight the influence of potentially corrupted samples and clustering-guided contrastive learning to better capture normal patterns. The framework is combined with a lightweight conjugate MLP architecture that efficiently models both temporal and cross-feature dependencies in MTS. The CRTF can also be integrated into existing models, yielding consistent performance improvements.

## Key Results
- Up to 73.04% higher F1 scores compared to ten state-of-the-art baselines
- 81.28% lower runtime and 90% smaller model size
- 5.35% average F1 improvement when CRTF is applied to three advanced models

## Why This Works (Mechanism)
CLEANet's effectiveness stems from its Contamination-Resilient Training Framework (CRTF), which mitigates the negative impact of anomalies in training data by adaptively down-weighting the contribution of potentially corrupted samples during reconstruction. The clustering-guided contrastive learning component enhances the model's ability to distinguish between normal and anomalous patterns by leveraging learned cluster structures. The lightweight conjugate MLP architecture further contributes by efficiently modeling both temporal and cross-feature dependencies, reducing computational overhead without sacrificing detection accuracy.

## Foundational Learning
- Multivariate Time Series (MTS): Sequential data with multiple correlated features over time; needed for modeling complex system behaviors in domains like industrial monitoring.
  - Quick check: Confirm data is aligned and synchronized across all features and timestamps.
- Unsupervised Anomaly Detection: Identifying outliers without labeled anomalies; required because labeled anomaly data is rare and expensive to obtain.
  - Quick check: Validate that no anomalies are present in the training set or that their influence is minimized.
- Contamination-Resilient Training: Techniques to train models on noisy data; essential when training sets contain anomalies that can mislead standard models.
  - Quick check: Ensure anomaly contamination rate is quantified and reported.
- Contrastive Learning: Learning by comparing similar and dissimilar samples; useful for pulling normal patterns together and pushing anomalies apart.
  - Quick check: Verify that negative pairs are correctly sampled and that the temperature parameter is tuned.
- Adaptive Reconstruction Weighting: Dynamically adjusting sample importance during reconstruction; helps focus model capacity on likely normal samples.
  - Quick check: Monitor weight distributions to confirm anomalies are being down-weighted.
- Conjugate MLP Architecture: A lightweight MLP design that models both temporal and cross-feature dependencies; reduces model size and inference time.
  - Quick check: Compare parameter counts and FLOPs against baseline models.

## Architecture Onboarding

**Component Map**
Input MTS -> Conjugate MLP (Temporal + Cross-feature) -> CRTF (Adaptive Weighting + Clustering-Guided Contrastive) -> Anomaly Score

**Critical Path**
Data preprocessing → Conjugate MLP encoding → Adaptive reconstruction weighting → Clustering-guided contrastive loss → Anomaly detection

**Design Tradeoffs**
- CRTF improves robustness to contamination but adds training complexity and hyperparameter tuning
- Conjugate MLP reduces model size and inference time at potential cost of expressiveness for very complex patterns
- Contrastive learning requires careful negative sampling and temperature tuning to avoid trivial solutions

**Failure Signatures**
- Over-aggressive down-weighting may cause false negatives (missing anomalies)
- Poorly tuned clustering may merge normal and anomalous patterns, hurting contrast
- Insufficient negative samples can lead to collapsed representations

**First Experiments**
1. Run CRTF on a simple autoencoder baseline with synthetic contamination; measure anomaly detection F1 and weight distributions.
2. Perform ablation: compare CLEANet with and without contrastive loss on a public MTS dataset.
3. Test model size and inference time on a CPU-constrained device.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of detailed implementation specifics (hyperparameters, random seeds, hardware) hinders reproducibility
- Limited generalizability evidence: CRTF tested on only three advanced models, selection criteria unclear
- No ablation studies isolating the impact of conjugate MLP versus CRTF on efficiency gains

## Confidence
- F1 score and runtime improvements: Medium (strong quantitative claims but limited reproducibility details)
- CRTF generalizability: Medium (evidence from three models only, lacking broader validation)
- Model size and inference efficiency: Medium (claims plausible but not independently verified)

## Next Checks
1. Replicate the experimental results using the provided code and datasets, ensuring identical preprocessing, hyperparameter settings, and evaluation protocols.
2. Conduct ablation studies to isolate the impact of the conjugate MLP architecture from the CRTF framework on model size, inference speed, and anomaly detection performance.
3. Test CLEANet on additional datasets with varying contamination levels and from different application domains (e.g., industrial IoT, healthcare) to assess robustness and generalizability.