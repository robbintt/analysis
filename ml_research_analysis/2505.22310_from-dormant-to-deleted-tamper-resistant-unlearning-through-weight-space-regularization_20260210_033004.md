---
ver: rpa2
title: 'From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space
  Regularization'
arxiv_id: '2505.22310'
source_url: https://arxiv.org/abs/2505.22310
tags:
- unlearning
- relearning
- forget
- examples
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates relearning attacks on machine unlearning
  algorithms for vision classifiers, showing that knowledge believed-to-be-unlearned
  can be fully recovered simply by fine-tuning on the retain set. The authors evaluate
  a wide range of existing unlearning methods and discover that many achieve near-perfect
  forget-set accuracy post-relearning, even without any forget-set examples.
---

# From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization

## Quick Facts
- arXiv ID: 2505.22310
- Source URL: https://arxiv.org/abs/2505.22310
- Reference count: 40
- This paper investigates relearning attacks on machine unlearning algorithms, showing that knowledge believed-to-be-unlearned can be fully recovered by fine-tuning on the retain set.

## Executive Summary
This paper reveals a critical vulnerability in existing machine unlearning methods: knowledge that appears to be unlearned can be fully recovered through relearning attacks. The authors demonstrate that many state-of-the-art unlearning algorithms achieve near-perfect forget-set accuracy when fine-tuned on just the retain set, without any forget-set examples. By analyzing weight-space properties, they identify that tamper-resistant methods maintain large distances from the pretrained model. The proposed Weight Dist Reg and Weight Distortion methods achieve state-of-the-art resistance to relearning attacks by explicitly encouraging weight-space separation, though at the cost of reduced test accuracy.

## Method Summary
The authors evaluate machine unlearning methods on vision classifiers, focusing on example-level unlearning where specific data points must be forgotten. They introduce a relearning attack where models are fine-tuned on the retain set alone, measuring how much forget-set accuracy recovers. The proposed methods (Weight Distortion and Weight Dist Reg) use weight-space regularization—either through noise injection or explicit distance maximization—to push unlearned models far from the pretrained model in parameter space. They validate their approach on CIFAR-10/CIFAR-100 with ResNet architectures, using atypical examples as forget sets identified through consistency scores.

## Key Results
- Fine-tuning on retain set alone can recover forget-set accuracy from ~50% to ~100% in vulnerable unlearning methods
- L2 distance between pretrained and unlearned models predicts resistance to relearning attacks
- Weight-space regularization methods (Weight Dist Reg, Weight Distortion) achieve state-of-the-art tamper-resistance
- A fundamental trade-off exists between tamper-resistance and test accuracy
- Linear mode connectivity analysis shows vulnerable methods remain in the same optimization basin as pretrained models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning on retain set alone can recover forget-set accuracy from ~50% to ~100% in vulnerable unlearning methods.
- **Mechanism:** Unlearning methods that operate only on outputs or representations leave forget-set information dormant rather than deleted. The retain set provides gradient updates that re-activate pathways encoding forget-set knowledge because the unlearned model remains close to the pretrained model in weight-space.
- **Core assumption:** Forget-set information persists in model weights after unlearning and can be re-accessed via gradient-based optimization on related data.
- **Evidence anchors:** [abstract] "forget-set accuracy can recover from around 50% post-unlearning to nearly 100% with fine-tuning on just the retain set"; [section 5] "We make the striking observation that some methods... are very susceptible to relearning attacks."

### Mechanism 2
- **Claim:** L2 distance between pretrained and unlearned models predicts resistance to relearning attacks.
- **Mechanism:** Larger weight-space movement from the pretrained model correlates with better tamper-resistance. Methods with small L2 distances remain in a locally connected region where forget-set gradients can be restored.
- **Core assumption:** Distance in weight-space captures the degree to which forget-set encoding has been disrupted.
- **Evidence anchors:** [section 6] "Generally, we see that methods with higher L2 distance exhibit higher robustness against relearning attacks."

### Mechanism 3
- **Claim:** Linear mode connectivity (LMC) between pretrained and unlearned models indicates relearning vulnerability.
- **Mechanism:** Retrain-from-scratch models show a high-loss barrier when linearly interpolated with the pretrained model (non-connected modes), while vulnerable unlearning methods show no barrier (connected modes).
- **Core assumption:** Loss barriers along linear paths in weight-space indicate qualitatively different solutions that resist simple gradient-based reversion.
- **Evidence anchors:** [section 6] "Retrain-from-scratch is not linearly connected to the pretrained model, whereas for unlearning algorithms, the resulting unlearned model is in many cases still linearly connected to the pretrained one."

## Foundational Learning

- **Concept: Machine Unlearning vs. Exact Unlearning**
  - Why needed here: The paper distinguishes approximate unlearning (post-hoc modification) from exact unlearning (retrain-from-scratch). Understanding this distinction is essential for interpreting tamper-resistance as a gap between apparent and true unlearning.
  - Quick check question: Why is retrain-from-scratch considered the gold standard despite its computational cost?

- **Concept: Weight-Space Regularization**
  - Why needed here: The proposed methods all use weight-space interventions. Understanding L2 regularization, noise injection, and loss barriers is prerequisite to implementing these approaches.
  - Quick check question: How does adding Gaussian noise to weights differ from L2 weight decay in terms of optimization trajectory?

- **Concept: Linear Mode Connectivity**
  - Why needed here: LMC is used as a diagnostic tool to understand whether unlearning has moved the model to a qualitatively different solution.
  - Quick check question: What does the presence of a loss barrier along a linear path between two models indicate about their relationship?

## Architecture Onboarding

- **Component map:** Pretrained Model (M_P) → Unlearning Algorithm (U) → Unlearned Model (M_U) → Relearning Attack (fine-tune on D_R) → Relearned Model (M_RL)
- **Critical path:**
  1. Train baseline model on full dataset D_tr
  2. Define forget set D_F and retain set D_R
  3. Apply unlearning method
  4. Measure post-unlearning accuracy on D_F (target ~50% for atypical examples)
  5. Apply relearning attack: fine-tune on D_R ∪ D_F^re (including D_F^re = ∅ case)
  6. Measure post-relearning accuracy on held-out D_F^ho

- **Design tradeoffs:**
  - **Tamper-resistance vs. test accuracy:** Methods with highest tamper-resistance show lower test accuracy
  - **Efficiency vs. robustness:** Weight Distortion achieves tamper-resistance in 1 epoch; Catastrophic Forgetting requires 100 epochs
  - **Atypical vs. typical examples:** Atypical forget sets are harder to unlearn and more diagnostic of method quality

- **Failure signatures:**
  - Forget-set accuracy jumps from ~50% to ~100% after relearning on retain set only → unlearning was superficial
  - L2 distance ||M_U - M_P||₂ < 10 → likely vulnerable to relearning attacks
  - No loss barrier in LMC interpolation curve → unlearned model remains in same optimization basin

- **First 3 experiments:**
  1. **Baseline relearning susceptibility:** Train ResNet-18 on CIFAR-10 with atypical forget set. Apply SCRUB, measure L2 distance and post-relearning accuracy on held-out forget set with 0 relearning examples.
  2. **Weight-space regularization comparison:** Apply Weight Distortion and Weight Dist Reg on same setup. Measure L2 distance and post-relearning accuracy.
  3. **LMC diagnostic validation:** Interpolate between pretrained and unlearned models for both SCRUB and Weight Dist Reg. Plot accuracy along interpolation path.

## Open Questions the Paper Calls Out
- **Question:** Which specific layers in neural networks are most important for robustness against relearning attacks?
- **Question:** Can the fundamental trade-off between tamper-resistance and test accuracy be improved beyond the current Pareto frontier?
- **Question:** Do findings from vision classifiers generalize to large language models with billions of parameters?
- **Question:** Can more sophisticated membership inference attacks better predict susceptibility to relearning attacks?

## Limitations
- The weight-space regularization mechanisms lack complete theoretical grounding, relying on empirical correlations
- The atypical example identification method depends on external consistency scores without full reproducibility details
- The proposed methods trade accuracy for tamper-resistance in a manner requiring careful Pareto optimization
- The paper focuses on small vision models, leaving scalability to large language models unexplored

## Confidence
- **High Confidence**: Empirical demonstration that relearning attacks succeed against existing unlearning methods, and that L2 distance correlates with resistance
- **Medium Confidence**: The claim that forget-set information remains dormant in vulnerable methods
- **Medium Confidence**: The weight-space regularization approach itself—effective empirically but lacking rigorous theoretical justification

## Next Checks
1. **Atypical vs. Typical Example Validation**: Replicate the relearning attack with both atypical and typical forget sets to verify the vulnerability gap
2. **Cross-Domain Transfer**: Test proposed methods on language models or other architectures to verify weight-space regularization generalizes beyond vision classifiers
3. **Optimization Path Analysis**: Implement and compare multiple optimization strategies during relearning to determine if the observed vulnerability persists across optimization landscapes