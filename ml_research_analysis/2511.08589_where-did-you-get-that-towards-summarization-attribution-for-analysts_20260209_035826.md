---
ver: rpa2
title: Where did you get that? Towards Summarization Attribution for Analysts
arxiv_id: '2511.08589'
source_url: https://arxiv.org/abs/2511.08589
tags:
- summary
- attribution
- sentence
- sentences
- abstractive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates automatic attribution methods to link summary
  sentences to source text for analysts, addressing the challenge of large language
  model hallucinations. It compares two approaches: a purely abstractive method using
  GPT-3.5 and a hybrid method combining extractive summarization (occams) with GPT
  paraphrasing.'
---

# Where did you get that? Towards Summarization Attribution for Analysts

## Quick Facts
- **arXiv ID:** 2511.08589
- **Source URL:** https://arxiv.org/abs/2511.08589
- **Reference count:** 40
- **Key result:** Hybrid summarization (occams extract + GPT paraphrase) provides better attribution accuracy than purely abstractive summarization, with 58.9-74% full support vs 50-67%.

## Executive Summary
This study investigates automatic attribution methods for linking summary sentences to source text in multi-document summarization, addressing the challenge of large language model hallucinations. The authors compare two approaches: a purely abstractive method using GPT-3.5 and a hybrid method combining extractive summarization (occams) with GPT paraphrasing. Human evaluation across three datasets (TAC 2011, Cyber Threat Intelligence, CrisisFACTS) demonstrates that the hybrid method provides better attribution accuracy, with 58.9-74% of sentences receiving full support versus 50-67% for the abstractive method. The study also identifies refutation types, finding semantic frame errors (especially time-shift inconsistencies) and content verifiability errors are most common.

## Method Summary
The study evaluates attribution quality using three datasets: TAC 2011 (10 newswire documents per topic), Cyber Threat Intelligence (single documents with knowledge graphs), and CrisisFACTS (multi-stream social media). Two summarization approaches are compared: abstractive (GPT-3.5 Turbo with basic prompt) and hybrid (occams extractive summarization followed by GPT paraphrasing with explicit constraints). Attribution retrieval uses either sentence embeddings (T5-xxl) or NLI models (roberta-large-mnli) to find top-k supporting source sentences. Human analysts then evaluate each summary sentence for support level (full, partial, no, unclear) and refute status, with refutations classified into semantic frame errors (predicate, entity, circumstance) and content verifiability errors (out of article, parsing).

## Key Results
- Hybrid summarization yields 58.9-74% full support vs 50-67% for abstractive summarization
- Sentence embedding models outperform NLI models for attribution retrieval
- Time-shift inconsistencies are the primary cause of semantic refutations in multi-document datasets
- "Related but not relevant" attributions represent a significant ambiguity in human evaluation
- Cyber Threat Intelligence dataset contains knowledge graphs that could enable automated fact-checking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid summarization (extract-then-paraphrase) yields higher attribution accuracy than purely abstractive summarization.
- Mechanism: The occams extractive step constrains content to sentences verifiably present in source documents. GPT paraphrasing then improves fluency without introducing new facts. This two-stage constraint reduces the search space for hallucinations.
- Core assumption: Extractive summaries preserve sufficient semantic equivalence after paraphrasing.
- Evidence anchors:
  - [abstract] "Human evaluation on three datasets... shows the hybrid method provides better attribution accuracy, with 58.9-74% of sentences receiving full support versus 50-67% for the abstractive method."
  - [section 6.2] "For the CrisisFACTS automatic attribution results... the hybrid summary sentences had better automatic attribution than those using GPT without the occams extracts."
  - [corpus] Cite-While-You-Generate (arXiv:2601.16397) reports similar findings: generation-time source attribution improves when models cite supporting spans directly.
- Break condition: If extractive summarization misses critical facts (low recall), paraphrasing cannot recover them, and attribution quality degrades.

### Mechanism 2
- Claim: Sentence embedding similarity outperforms NLI-based methods for attribution retrieval, but produces more refutations.
- Mechanism: Embedding models capture broader semantic relatedness, retrieving more candidates. NLI models are trained for strict entailment and produce low probabilities for paraphrased content, missing valid attributions.
- Core assumption: Human evaluators tolerate partial context when judging support.
- Evidence anchors:
  - [section 4] "SummaC is by no means perfect for attribution... we believe this is due to the Natural Language Inference (NLI) used initially... these probabilities are run through the model to output the score."
  - [section 6.2, Figures 1-2] "In both tasks, the sentence embedding method gave attributions that the analysts judged better than the NLI model."
  - [corpus] Limited direct corpus comparison; neighboring papers focus on attribution quality rather than retrieval method comparison.
- Break condition: If source documents contain many semantically similar but contradictory sentences (e.g., time-shifted facts), embedding retrieval increases false positives.

### Mechanism 3
- Claim: Refutation types are dataset-dependent, with semantic frame errors (especially time-shift inconsistencies) dominating multi-document temporal data.
- Mechanism: Multi-document datasets (TAC 2011, CrisisFACTS) span time periods where facts change. Attribution methods may link to sentences from different time slices. Single-document specialist data (Cyber) produces more "related but not relevant" errors.
- Core assumption: Annotators can consistently distinguish refutations from irrelevant attributions.
- Evidence anchors:
  - [section 6.4] "TAC 2011 and CrisisFACTS refutations include timing errors, where attributing sentences refer to different periods of time when facts differed, but Cyber Threat Intelligence did not."
  - [section 6.4, Table 4] Provides typology: PredE (predicate errors), EntE (entity errors), CircE (circumstance errors), OutE (out of article/hallucination), GramE (parsing errors).
  - [corpus] No direct corpus validation of this specific error typology.
- Break condition: If parsing fails on poorly formatted source text (CrisisFACTS social media snippets), grammatical errors create meaningless attributions that confound classification.

## Foundational Learning

- Concept: **Extractive vs. Abstractive Summarization Spectrum**
  - Why needed here: The paper's core intervention positions hybrid summarization between pure extraction and pure abstraction. Understanding this spectrum clarifies why attribution difficulty increases with abstraction.
  - Quick check question: Given a 10-document cluster, would you expect higher attribution accuracy from (a) selecting top sentences by TF-IDF overlap or (b) generating a summary with GPT-4? Why?

- Concept: **Natural Language Inference (NLI) for Consistency Checking**
  - Why needed here: The paper critiques SummaC (an NLI-based metric) for bias toward extractive summaries. Understanding NLI's entailment framing explains why it struggles with paraphrased content.
  - Quick check question: An NLI model outputs P(entailment)=0.15 for a summary sentence against a source sentence that is a valid paraphrase. What property of NLI training might explain this?

- Concept: **Hallucination vs. Refutation Distinction**
  - Why needed here: The paper distinguishes LLM hallucinations (generation errors) from refutations (attribution linking errors). This matters for debugging: different failure modes require different interventions.
  - Quick check question: A summary states "The fire was contained on August 15" but the source says "contained on August 14." Is this a hallucination or a refutation error if the attribution method links to the correct sentence?

## Architecture Onboarding

- Component map: Multi-document corpus -> Abstractive GPT-3.5 OR occams extraction -> GPT paraphrase -> Embedding retrieval OR NLI retrieval -> Human evaluation -> Error categorization

- Critical path: occams extraction -> length budget determination -> GPT paraphrase -> embedding-based retrieval -> human annotation -> error categorization

- Design tradeoffs:
  - Attribution accuracy vs. fluency: Hybrid sacrifices some fluency for traceability; pure abstractive optimizes fluency but loses attribution.
  - Recall vs. precision in retrieval: Three attribution sentences (Task 2) improve support detection but introduce lower-quality candidates (more refutations).
  - Context window vs. document coverage: CrisisFACTS required filtering by importance score to fit within GPT-3.5's 16K token limit.

- Failure signatures:
  - Time-shift refutations: Multi-document temporal corpora produce inconsistent facts across documents; embedding retrieval links to wrong time slice.
  - Parsing-induced GramE errors: Social media snippets (CrisisFACTS) produce malformed "sentences" that are meaningless for attribution.
  - Low NLI scores for valid paraphrases: NLI models trained on entailment data undervalue abstractive content, yielding false negatives.

- First 3 experiments:
  1. **Baseline replication:** Run occams + GPT paraphrase on TAC 2011 with embedding retrieval (top-3). Measure full support rate. Expected: ~70-74% based on paper.
  2. **Ablation on retrieval method:** Compare embedding vs. NLI retrieval on the same hybrid summaries. Expect embedding to yield higher support but more refutations.
  3. **Error taxonomy validation:** On 50 summary-attribution pairs, have two annotators classify refutations using the paper's typology. Measure inter-annotator agreement; expect ambiguity in "related but not relevant" vs. refutation.

## Open Questions the Paper Calls Out

- **Question:** How can automated factuality metrics like SummaC be modified to fairly evaluate abstractive summaries without the observed bias favoring extractive text?
  - Basis in paper: Page 4 states that SummaC has an "underlying bias toward extractive summaries" and that its reliance on NLI probabilities makes it "bad at capturing summary text's abstract nature."
  - Why unresolved: Current NLI-based metrics produce low entailment probabilities for abstractive content because facts are distributed across sentences, leading to unfair disqualification of valid abstractive summaries.
  - What evidence would resolve it: A new evaluation metric that correlates strongly with human judgment on abstractive summaries without requiring high lexical overlap or direct sentence-to-sentence entailment.

- **Question:** Can the knowledge graphs included in the Cyber Threat Intelligence dataset be effectively utilized to automate content evaluation and fact-checking?
  - Basis in paper: Page 4 notes that the dataset includes knowledge graphs and sample Q&A pairs, which "could be used for future content evaluation and fact-checking."
  - Why unresolved: The current study relied on human annotation and text-based attribution methods (NLI and embeddings) rather than leveraging the structured knowledge graphs provided in the dataset.
  - What evidence would resolve it: An automated pipeline that leverages these knowledge graphs to verify summary facts, demonstrating a correlation with human error typologies (e.g., Predicate or Entity Errors).

- **Question:** How can automatic summarization pipelines be enhanced to detect and mitigate "time-shift" inconsistencies in multi-document datasets?
  - Basis in paper: Page 10 identifies "time-shift errors" as the primary cause of semantic refutations in the TAC 2011 dataset, occurring when summaries amalgamate facts from different time periods incorrectly.
  - Why unresolved: The study found that purely abstractive and hybrid models both struggle to maintain temporal consistency when source documents cover evolving facts over time.
  - What evidence would resolve it: A summarization model that correctly orders or harmonizes temporal facts relative to the query time, resulting in a statistically significant reduction in CircE (Circumstance Error) labels during evaluation.

## Limitations

- The Cyber Threat Intelligence dataset is proprietary and not publicly available, limiting reproducibility of results using that dataset.
- The study lacks inter-annotator agreement metrics for the human evaluation of attribution quality, leaving reliability uncertain.
- The comparison with modern citation-augmented summarization methods (e.g., Cite-While-You-Generate) is absent, limiting assessment of relative performance.

## Confidence

- **High Confidence:** The comparative results between hybrid and abstractive summarization attribution accuracy are robust, supported by human evaluation across three distinct datasets showing consistent improvements (58.9-74% vs 50-67% full support).
- **Medium Confidence:** The error typology analysis is credible but may suffer from subjective interpretation. The distinction between "related but not relevant" and refutation errors is particularly ambiguous, and the paper acknowledges annotator disagreement in specific cases.
- **Low Confidence:** The generalization of the NLI vs. embedding comparison beyond the three tested datasets is uncertain. The paper identifies SummaC's bias toward extractive summaries but does not extensively validate this across diverse summarization architectures or domains.

## Next Checks

1. **Inter-annotator agreement calculation:** Compute Cohen's kappa or Krippendorff's alpha on 50 randomly selected summary-attribution pairs to establish the reliability of the support/refute classification scheme.

2. **Cross-dataset replication:** Apply the same hybrid summarization and attribution pipeline to a fourth, publicly available multi-document summarization dataset (e.g., Multi-News) to test the generalizability of the time-shift refutation finding.

3. **Citation-aware baseline comparison:** Implement a modern citation-augmented summarization model (such as training-free Cite-While-You-Generate) and compare attribution accuracy against both the abstractive and hybrid methods from this study.