---
ver: rpa2
title: 'From Vague Instructions to Task Plans: A Feedback-Driven HRC Task Planning
  Framework based on LLMs'
arxiv_id: '2503.01007'
source_url: https://arxiv.org/abs/2503.01007
tags:
- task
- planning
- code
- execution
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IteraPlan, a feedback-driven human-robot collaboration
  (HRC) task planning framework that uses large language models (LLMs) to handle vague,
  high-level human instructions. Unlike prior approaches requiring detailed structured
  prompts or explicit task descriptions, IteraPlan uses a single concise prompt to
  decompose ambiguous natural language into executable task plans.
---

# From Vague Instructions to Task Plans: A Feedback-Driven HRC Task Planning Framework based on LLMs

## Quick Facts
- arXiv ID: 2503.01007
- Source URL: https://arxiv.org/abs/2503.01007
- Reference count: 29
- Single concise prompt framework achieves 1.00 optimal code generation and successful execution rates in AI2-THOR simulation

## Executive Summary
This paper introduces IteraPlan, a feedback-driven framework that converts vague human instructions into executable task plans using large language models. The system handles high-level natural language commands like "I feel hungry" by decomposing them into structured tasks, generating Python code for robot execution, and iteratively refining plans based on human and execution feedback. Evaluation in AI2-THOR across four room types demonstrates high success rates with GPT-4o, particularly after iterative refinements. The framework emphasizes practical deployment through its single concise prompt approach and real-time adaptation capabilities.

## Method Summary
IteraPlan employs a four-component pipeline to transform vague instructions into executable HRC plans. First, it uses two-stage task decomposition to break down ambiguous natural language into environment-aligned subtasks. Second, it generates Python code implementing planned skills, allowing human review and iterative refinement. Third, it executes code in simulation while capturing and adapting to errors through feedback loops. Fourth, it allocates tasks between human and robot agents based on capability affordances using rule-based heuristics. The system operates entirely in simulation with modified AI2-THOR actions approximating real-world constraints.

## Key Results
- Optimal Code Rate (OCR) reaches 1.00 with few refinements using GPT-4o
- Successful Execution Rate (SER) reaches 1.00 across most tasks after iterative refinements
- Rule-based task allocation outperforms LLM-based allocation (1.0 vs 0.76 CASR)
- Living Room tasks show 0.00-0.40 SER at 0 refinements, indicating complexity-dependent performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage task decomposition handles vague instructions without requiring detailed structured prompts.
- Mechanism: Stage 1 generates initial task description from vague instruction and basic environment info. Stage 2 refines by filtering relevant objects/locations and re-prompting for environment-aligned plan.
- Core assumption: LLMs can identify relevant environmental elements from candidate plans even when original instruction lacks specificity.
- Evidence anchors:
  - [abstract] "uses a single concise prompt to handle diverse tasks and environments"
  - [section III-A] "we break the task decomposition process into two stages... Stage 1: Initial Task Description Generation... Stage 2: Refinement and Feasibility Check"
  - [corpus] REI-Bench paper addresses similar vague instruction challenges in embodied task planning.

### Mechanism 2
- Claim: Iterative human feedback on generated code improves alignment with user preferences and task feasibility.
- Mechanism: LLM generates Python code; human reviews in terminal; user provides natural language feedback; LLM refines code iteratively until confirmation. Code uses human-readable skill syntax.
- Core assumption: Users can assess plan correctness from skill sequences without programming expertise.
- Evidence anchors:
  - [abstract] "integrates human feedback in real-time to refine plans iteratively"
  - [section III-B] "the system prints the generated code in the terminal, followed by a prompt asking whether the script appears correct... no programming expertise is required"

### Mechanism 3
- Claim: Execution-error feedback enables adaptive recovery from unforeseen environmental constraints.
- Mechanism: Execute generated code; capture exception traces and failure reports; convert errors to prompts; LLM modifies code to address specific failures while preserving subsequent steps; re-execute iteratively until success.
- Core assumption: Error messages contain sufficient information for LLM to diagnose and fix root cause.
- Evidence anchors:
  - [abstract] "refines the plan based on real-world interaction"
  - [section III-C] "the system extracts error messages directly from the simulation environment's execution logs... parsed and converted into a prompt"
  - [section V, Table IV] SER reaches 1.00 with sufficient refinements across all models

## Foundational Learning

- Concept: **Prompt Engineering Tradeoffs (Concise vs. Structured)**
  - Why needed here: The paper claims a single concise prompt generalizes across tasks, contrasting with prior work using long, task-specific structured prompts. Understanding when brevity works vs. fails is critical.
  - Quick check question: Can you explain why a single concise prompt might fail on highly novel tasks outside the training distribution?

- Concept: **Affordance-Based Task Allocation**
  - Why needed here: The framework assigns skills to human or robot agents based on capability affordances. The ablation study shows LLM reasoning over affordances underperforms rule-based heuristics (0.76 vs. 1.0 CASR).
  - Quick check question: Given the ablation results, why might an LLM struggle to reason about agent capabilities compared to explicit rules?

- Concept: **Simulation-to-Real Transfer Constraints**
  - Why needed here: The system is evaluated in AI2-THOR simulation with modified action spaces to approximate real-world execution. Understanding simulation limitations is essential for deployment.
  - Quick check question: What types of real-world failures would AI2-THOR not model, and how might the error-feedback loop behave differently?

## Architecture Onboarding

- Component map: Task Decomposition (vague instruction + environment → task description) → Code Generation (task plan + skills → Python script) → Real-Time Execution (execute → capture errors → refine) → Task Allocation (extract skills → compute affordances → assign agents)
- Critical path: Component 1 → Component 2 → Component 3 → Component 4. Components 1 and 4 consistently perform flawlessly; evaluation focuses on Components 2 and 3.
- Design tradeoffs:
  - Single concise prompt vs. task-specific prompts: Reduces engineering overhead but increases reliance on LLM generalization
  - Rule-based affordance calculation vs. LLM reasoning: Paper uses heuristics for reliability; ablation shows LLM-only drops OER from 1.0 to 0.62
  - "Perfect agent" assumption: Framework assumes flawless skill execution; real deployment requires lower-level motion/control integration
- Failure signatures:
  - High refinement counts: GPT-3.5 requires 4+ refinements in Kitchen tasks (complexity-related slowdown)
  - Zero-shot failures: Living Room shows 0.00-0.40 SER at 0 refinements across models
  - Incorrect task allocation: Ablation shows 15/40 executions failed or suboptimal when LLM reasons about affordances
- First 3 experiments:
  1. Run the provided code on a single Kitchen task ("I feel hungry") with GPT-4o, logging refinement count and final OCR/SER. Compare against Table III/IV baselines.
  2. Test Component 4 ablation: Provide atomic action affordances (not skill affordances) to GPT-4o for task allocation; compute CASR and compare to 0.76 baseline.
  3. Stress-test the error-feedback loop: Intentionally modify environment (e.g., remove a required object mid-execution) and observe whether the LLM recovers or stalls.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based affordance reasoning be improved to achieve parity with rule-based methods for task allocation in HRC?
- Basis in paper: [explicit] The ablation study (Section V-C) shows GPT-4o achieves only 76% CASR for affordance-based task allocation versus 100% for rule-based heuristics, with incorrect allocations causing 15 failed or suboptimal executions out of 40 experiments.
- Why unresolved: The paper explicitly notes that "its performance in reasoning over affordances for task and skill allocation remains mediocre" but does not propose methods to improve LLM-based reasoning in this component.
- What evidence would resolve it: Demonstrating an LLM-based allocation method achieving CASR ≥0.95 across all floorplan types, or identifying prompting/training strategies that close the performance gap with rule-based approaches.

### Open Question 2
- Question: How does IteraPlan perform when deployed on physical robotic systems with real perception and actuation constraints?
- Basis in paper: [inferred] The paper emphasizes compatibility with real-world execution and modified AI2-THOR actions to align with physical feasibility, yet all experiments are conducted purely in simulation with an assumed "perfect agent" that executes skills flawlessly.
- Why unresolved: The gap between simulated and real-world execution is not addressed experimentally, and the perfect agent assumption abstracts away perception errors, motion planning failures, and hardware limitations.
- What evidence would resolve it: Results from physical robot experiments showing OCR and SER metrics under real-world conditions, including analysis of failure modes not captured in simulation.

### Open Question 3
- Question: What is the relationship between task complexity and the number of refinement iterations required for convergence, and can this be predicted a priori?
- Basis in paper: [inferred] The paper caps refinements at 5 and observes that Kitchen tasks (deemed more complex) require more refinements across all models (GPT-3.5, GPT-4, GPT-4o reaching 1.00 SER at 4, 3, and 5 refinements respectively), but does not systematically characterize this relationship.
- Why unresolved: No quantitative measure of task complexity is defined, and the threshold of 5 refinements appears arbitrary rather than theoretically motivated.
- What evidence would resolve it: A formal complexity metric (e.g., number of subtasks, object interactions, or dependency depth) correlated with observed refinement counts, enabling predictive modeling of convergence requirements.

## Limitations

- The framework assumes a "perfect agent" capable of flawless skill execution, abstracting away real-world motion planning and control challenges
- LLM-based task allocation underperforms rule-based heuristics (CASR 0.76 vs 1.0), limiting the framework's ability to handle complex allocation decisions
- All experiments are conducted in simulation, with no validation of simulation-to-real transfer or performance under real-world constraints

## Confidence

- **High Confidence**: OCR and SER metrics in simulation, particularly GPT-4o achieving 1.00 across most tasks after refinements. The two-stage task decomposition mechanism is well-specified and empirically validated.
- **Medium Confidence**: The error-feedback loop's ability to handle novel failures, given limited corpus evidence for error-message-driven LLM code repair in HRC contexts. Real-world physics unmodeled in AI2-THOR could expose fundamental limitations.
- **Low Confidence**: Generalization to tasks requiring long-horizon planning or those outside the training distribution, as the single concise prompt approach may fail on highly novel scenarios.

## Next Checks

1. Test the framework on novel task types not present in the REI-Bench dataset to evaluate prompt generalization limits.
2. Conduct a real-world pilot with physical robots to identify simulation-to-real transfer gaps, particularly around motion planning and object manipulation.
3. Evaluate the error-feedback loop's performance on edge cases by systematically removing required objects during execution to test adaptive recovery capabilities.