---
ver: rpa2
title: Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction
arxiv_id: '2510.25187'
source_url: https://arxiv.org/abs/2510.25187
tags:
- context
- option
- llama
- sentence
- distractor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates cross-lingual text comprehension in large
  language models (LLMs) by using a Next Sentence Prediction (NSP) task across English,
  Swahili, and Hausa. A balanced dataset of 10,000 questions per language was created,
  and models like GPT-4 Turbo, Gemini 1.5 Flash, and LLaMA 3 70B were tested with
  and without Chain-of-Thought (CoT) prompting.
---

# Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction

## Quick Facts
- arXiv ID: 2510.25187
- Source URL: https://arxiv.org/abs/2510.25187
- Reference count: 40
- Models tested: GPT-4 Turbo, Gemini 1.5 Flash, LLaMA 3 70B
- Languages: English, Swahili, Hausa
- Task: Next Sentence Prediction across languages

## Executive Summary
This paper evaluates cross-lingual text comprehension in large language models by using a Next Sentence Prediction (NSP) task across English, Swahili, and Hausa. The study creates a balanced dataset of 10,000 questions per language and tests three different models with and without Chain-of-Thought prompting. Results show significant performance drops in lower-resource languages, particularly for LLaMA 3, while revealing that CoT prompting's effectiveness depends heavily on both model capability and language context. The findings highlight that model comprehension is strongly tied to language resource availability and that models rely on surface heuristics when statistical cues are weak.

## Method Summary
The study creates a balanced dataset of 10,000 questions per language (English, Swahili, Hausa) using the WikiHow dataset. Three models are tested: GPT-4 Turbo, Gemini 1.5 Flash, and LLaMA 3 70B, both with and without Chain-of-Thought prompting. The NSP task involves presenting models with a context paragraph followed by multiple-choice options for the next sentence. Performance is measured by accuracy across languages and prompt conditions. The experimental design controls for question difficulty and ensures balanced representation across languages.

## Key Results
- All models perform well in English but accuracy drops significantly in Swahili and sharply in Hausa
- LLaMA 3 shows the steepest performance decline in lower-resource languages
- CoT prompting improves LLaMA 3's performance but hurts GPT-4 and Gemini in lower-resource languages
- Models rely on surface heuristics in low-resource settings and struggle with ambiguous cases

## Why This Works (Mechanism)
The NSP task effectively probes language models' comprehension by requiring them to understand context and select the most semantically appropriate continuation. This approach reveals genuine comprehension rather than memorization since models must integrate information from the context paragraph with linguistic patterns to make predictions. The multiple-choice format provides clear evaluation metrics while the balanced dataset design controls for question difficulty across languages.

## Foundational Learning
- Cross-lingual transfer learning: Understanding how models leverage knowledge from high-resource languages (like English) to perform in low-resource ones. Needed because it explains performance patterns across languages.
- Chain-of-Thought prompting: The technique of prompting models to show their reasoning process. Needed because it shows varying effects on different models and languages.
- Next Sentence Prediction task: A comprehension evaluation method that tests whether models can identify logical continuations. Needed because it provides a controlled way to measure understanding.

## Architecture Onboarding
Component map: Dataset Creation -> Model Testing -> CoT Application -> Performance Evaluation
Critical path: Context paragraph + Question -> Model processing -> Answer selection -> Accuracy measurement
Design tradeoffs: CoT vs direct prompting, language-specific fine-tuning vs multilingual pretraining
Failure signatures: Sharp accuracy drops in low-resource languages, inconsistent CoT effects across models
First experiments: 1) Test individual question types, 2) Vary context length, 3) Compare with human performance baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset creation methodology and exact question distribution are not fully detailed
- Study focuses on only two lower-resource languages, limiting broader conclusions
- Inconsistent CoT effects suggest complex interactions requiring deeper investigation

## Confidence
- Cross-lingual performance differences are real and tied to resource availability: High
- CoT prompting effectiveness varies by model and language: Medium
- Models rely on surface heuristics in low-resource settings: Medium

## Next Checks
1. Replicate the study with additional low-resource languages to establish broader patterns
2. Conduct ablation studies on question types to identify which comprehension aspects are most affected by language resources
3. Test with larger context windows to determine if comprehension gaps persist when more context is available