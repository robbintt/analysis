---
ver: rpa2
title: 'Divisive Decisions: Improving Salience-Based Training for Generalization in
  Binary Classification Tasks'
arxiv_id: '2507.17000'
source_url: https://arxiv.org/abs/2507.17000
tags:
- salience
- difference
- training
- class
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses improving generalization in binary classification
  by incorporating both true-class and false-class class activation maps (CAMs) during
  training. The core idea is to contrast these CAMs using human-annotated saliency
  maps to better guide the model's attention toward decision-critical features.
---

# Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks

## Quick Facts
- arXiv ID: 2507.17000
- Source URL: https://arxiv.org/abs/2507.17000
- Reference count: 39
- One-line primary result: Contrast Salience improves generalization in binary classification tasks by jointly supervising true-class CAMs with human saliency and forcing false-class CAMs to diverge from them.

## Executive Summary
This paper addresses the challenge of improving model generalization in binary classification tasks by leveraging human-annotated saliency maps during training. The core innovation is to supervise not only the true-class Class Activation Maps (CAMs) but also the false-class CAMs, using three novel training methods: Difference Salience, Per-class Salience, and Contrast Salience. These methods encourage a stronger divergence between class activations, which improves the model's ability to generalize to out-of-distribution data. The proposed Contrast Salience method achieves the best overall performance, particularly in tasks involving presentation attack detection and synthetic face detection.

## Method Summary
The paper proposes three novel training methods for binary classification that use human saliency maps to supervise both true-class and false-class CAMs. The baseline method, CYBORG, only supervises the true-class CAM. The Difference Salience method computes the difference between unnormalized true and false CAMs, normalizes it, and uses this as a saliency target. Per-class Salience jointly supervises the true-class CAM with human saliency and the false-class CAM with the inverse of the human saliency. Contrast Salience supervises the true-class CAM with human saliency and the false-class CAM with the inverse of the true-class CAM, allowing more flexible divergence. All methods are evaluated on chest X-ray anomaly detection, iris presentation attack detection, and synthetic face detection, showing improved generalization on out-of-distribution tasks.

## Key Results
- Contrast Salience achieves the best overall generalization performance, particularly for iris presentation attack detection (+1.4% AUROC) and synthetic face detection (+8.1% AUROC).
- Difference Salience reveals decision-critical features even in models that have been passively fooled to obfuscate their true-class CAMs.
- Per-class Salience shows task domain-specific improvements in generalization, with mixed results across different datasets.
- The methods outperform traditional saliency-guided training that only supervises true-class CAMs, especially for out-of-distribution detection tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Difference Salience reveals decision-critical features even when models have been passively fooled to obfuscate their true-class CAMs.
- Mechanism: The classification decision in binary tasks is determined by the difference between logits, which directly stems from the difference between class activations. By subtracting the unnormalized false-class CAM from the unnormalized true-class CAM and normalizing, the resulting Difference Salience highlights regions that contribute most to the actual decision boundary, bypassing the obfuscation present in the individual CAMs.
- Core assumption: The decision-critical features are encoded in the *difference* between the true and false class activations, not solely in the true-class activations.
- Evidence anchors:
  - [abstract] "A novel Difference Salience visualization is also introduced, which reveals decision-critical features even in passively-fooled models."
  - [section 3.1] "As these are the values that are used to calculate the logits for the classifier, it is their difference that decides an input's classification label."
  - [section 6.1] "Thus, our answer to RQ1 is affirmative. Difference Salience reveals new and plausible features even in models trained to obfuscate their CAMs."
- Break condition: If decision-critical features are not consistently encoded in the activation difference (e.g., due to complex multi-class interactions or fundamental model architecture differences), this method may not be effective.

### Mechanism 2
- Claim: Directly supervising both true and false CAMs using complementary annotations improves model generalization by explicitly enforcing a strong divergence in class activations on relevant features.
- Mechanism: The Per-class Salience method adds a loss term that encourages the false-class CAM to match the inverse of the human saliency map, while the true-class CAM is encouraged to match the human saliency map. This joint supervision forces the model to attend to the correct features for the true class and to *avoid* attending to them for the false class, thereby creating a more discriminative representation.
- Core assumption: Human-identified salient features are the *only* critical features for classification, and directing attention away from them (inverse saliency) is universally beneficial for the false class.
- Evidence anchors:
  - [abstract] "Per-class Salience (jointly supervising true and inverted false CAMs)"
  - [section 4.3] "We hypothesize that because using the human salience to guide the model to important features for the true-class CAM improves performance, jointly requesting the model match the inverse of the annotations for the false-class CAM will strengthen the difference in class activations and improve the model's generalization capabilities further."
  - [section 6.3] "Thus we conclude that the answer to RQ3 is task domain specific with the potential to noticeably improve model generalization."
- Break condition: If human annotations are noisy, incomplete, or do not perfectly align with the optimal features for discrimination, forcing the false-class CAM to strictly avoid them may hurt performance.

### Mechanism 3
- Claim: Contrast Salience provides more flexible and effective generalization by using the human-guided true-class CAM as a dynamic target for the false-class CAM.
- Mechanism: This method supervises the true-class CAM to match human saliency and adds a loss term that encourages the false-class CAM to match the inverse of the *true-class CAM*. This allows the model to fine-tune its activation weights and create a strong contrast between class activations even when the true-class CAM needs to deviate from the strict human annotations to optimize its own objective.
- Core assumption: The primary goal is to maximize the divergence between true and false class activations; the true-class CAM, guided by human input but not strictly bound to it, provides a better dynamic reference for what the false-class CAM should avoid.
- Evidence anchors:
  - [abstract] "Contrast Salience (supervising true CAM with human saliency while diverging false CAM from it)"
  - [section 4.4] "This allows the model more leeway for fine-tuning the activation weights, while still rewarding the divergence of the false-class CAM."
  - [section 6.4] "Thus we conclude that the answer to RQ4 is affirmative, contrastive supervision using the human annotations improves model performance in generalization."
- Break condition: If the true-class CAM fails to learn meaningful features (e.g., due to poor human annotations or other training issues), using it as a target for the false class will be ineffective.

## Foundational Learning

- **Concept**: Class Activation Maps (CAMs)
  - Why needed here: The entire paper is built on manipulating and supervising CAMs. Understanding how they are generated and what they represent is fundamental.
  - Quick check question: How does a CAM visually represent a model's focus for a specific class?

- **Concept**: Binary Classification
  - Why needed here: The proposed methods (Difference, Per-class, Contrast Salience) are explicitly designed for binary tasks, where there is a single true and false class.
  - Quick check question: Why is contrasting two class CAMs simpler and more direct in a binary classification problem than in a multi-class problem?

- **Concept**: Model Generalization
  - Why needed here: The core problem being addressed is improving a model's ability to perform well on out-of-distribution data. All proposed methods are evaluated on their impact on generalization (AUROC).
  - Quick check question: In this paper's context, what is the primary metric used to measure a model's ability to generalize to unseen data?

## Architecture Onboarding

- **Component map**: Input Image -> DenseNet-121 Backbone -> Feature Maps -> CAM Extraction Module -> True-Class CAM & False-Class CAM -> Loss Function (CE + Saliency Terms) -> Updated Weights

- **Critical path**:
  1. An image is passed through the DenseNet-121 backbone.
  2. The model produces logits and internal feature maps.
  3. The **CAM Extraction Module** computes the true-class CAM (for the correct label) and the false-class CAM (for the incorrect label).
  4. The **Loss Function** calculates the total loss: `Classification Loss + Saliency Loss`.
      - For **Contrast Salience**, this is: `CE(y, y_pred) + MSE(human_saliency, true_CAM) + MSE(1 - true_CAM, false_CAM)`.
  5. The loss is backpropagated to update the model weights, with the saliency term shaping the internal representations.

- **Design tradeoffs**:
  - **Saliency Loss Weight (β, γ):** The paper uses equal weights (0.3 for three-component losses). A higher weight forces the model to adhere more strictly to human saliency at the potential cost of classification accuracy.
  - **Strict vs. Flexible False-Class Supervision:** Per-class Salience forces the false-class CAM to the inverse of the human map (strict), while Contrast Salience forces it to the inverse of the model's own true-class CAM (flexible). The latter provides more leeway but may be less effective if human annotations are highly reliable.
  - **Binary-Only Design:** The method is designed specifically for binary classification. Extending to multi-class would require defining which false-class(es) to contrast against, increasing complexity.

- **Failure signatures**:
  - **No improvement over baseline (or degradation) on in-distribution tasks:** This is expected and observed for the Chest X-ray (in-set) task, indicating the saliency supervision is not necessary or potentially restrictive for already solvable problems.
  - **Inconsistent improvement across OOD datasets:** A method might work well for one OOD task (e.g., Iris PAD) but not another (e.g., Synthetic Face Detection), suggesting the effectiveness is domain-specific.
  - **Passive Fooling Resilience Failure:** If Difference Salience fails to reveal plausible features in a model known to be passively fooled, the core hypothesis about decision-critical features being in the CAM difference is invalidated.

- **First 3 experiments**:
  1. **Baseline & Hyperparameter Tuning:** Train a baseline model (true-class CAM only) on your target binary task. Then, train a Contrast Salience model, sweeping the saliency loss weights (β, γ) to find the optimal balance between classification and saliency alignment.
  2. **Ablation Study:** Implement all three methods (Difference, Per-class, Contrast Salience) and the baseline. Compare their performance on a held-out test set and an out-of-distribution test set to validate the paper's findings for your specific domain.
  3. **Difference Salience Diagnostic:** Take a model trained with standard supervision (no saliency guidance) and generate Difference Salience visualizations for a few samples. Compare these against the model's true-class CAM to see if they reveal different or more plausible features, acting as a sanity check.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Difference and Contrast Salience be adapted for multi-class classification where multiple false classes exist?
- Basis in paper: [explicit] The authors note in Section 7 that extending these techniques to multi-class problems presents "conceptual challenges, particularly in defining contrastive CAM targets when more than one false-class is present."
- Why unresolved: The mathematical formulation (Eq. 1-5) relies on a binary distinction between a single true-class and a single false-class, making the selection of a contrastive target ambiguous for N-classes.
- What evidence would resolve it: A generalized loss function for N-class tasks and empirical validation showing generalization improvements on multi-class datasets.

### Open Question 2
- Question: Can learned or AI-generated saliency proxies effectively replace human annotations in this training paradigm?
- Basis in paper: [explicit] The authors state they plan to "investigate the use of learned or model-generated salience proxies in place of human annotations" in Section 7.
- Why unresolved: The method currently depends on a reliable correspondence between human visual attention and classification cues, which may not always exist or be cost-effective to obtain.
- What evidence would resolve it: Comparative experiments evaluating generalization performance when using model-generated saliency maps versus ground-truth human annotations.

### Open Question 3
- Question: Does the proposed contrastive supervision integrate effectively with adversarial robustness techniques?
- Basis in paper: [explicit] Section 7 lists "integrating these contrastive objectives with adversarial robustness techniques" as a direction for future work to create more trustworthy models.
- Why unresolved: It is unclear if the enforced divergence of true/false CAMs conflicts with or supports existing defenses against adversarial attacks.
- What evidence would resolve it: Robustness benchmarks (e.g., against FGSM or PGD attacks) comparing standard saliency-guided models with the proposed Contrast Salience models.

## Limitations
- The paper's findings are constrained by its reliance on specific datasets and human saliency annotations, which are not fully specified.
- The effectiveness of the proposed methods may be sensitive to the quality and representativeness of the human saliency maps.
- The binary classification focus limits the generalizability of the findings to multi-class problems.
- The magnitude of improvement varies significantly across domains, suggesting the methods are not universally effective.

## Confidence

- **High Confidence**: The core mechanism of using CAM differences to reveal decision-critical features (Mechanism 1) is well-supported by the theoretical argument and Difference Salience visualization results.
- **Medium Confidence**: The claim that Per-class Salience improves generalization is supported by the results but is labeled as "task domain specific" by the authors, indicating less consistent performance.
- **Medium Confidence**: The claim that Contrast Salience provides the best overall results is supported by the aggregate AUROC scores, but the improvement over Per-class Salience is not dramatic in all cases.

## Next Checks

1. **Saliency Map Quality Audit**: Conduct a thorough analysis of the human saliency maps used in the experiments. Assess their coverage, accuracy, and potential biases to determine how much the method's success depends on the quality of these annotations.
2. **Ablation of False-Class Supervision**: Implement a variant of Contrast Salience that removes the false-class supervision term (MSE(h_norm, 1-t_norm)) to quantify how much of the performance gain is due to the true-class supervision versus the contrastive element.
3. **Multi-Class Extension Experiment**: Design and implement a minimal extension of the Contrast Salience method to a three-class problem. Compare its performance to the binary version on a simple multi-class dataset to test the limits of the approach.