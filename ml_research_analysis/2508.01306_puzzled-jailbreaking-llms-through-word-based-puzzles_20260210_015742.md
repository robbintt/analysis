---
ver: rpa2
title: 'PUZZLED: Jailbreaking LLMs through Word-Based Puzzles'
arxiv_id: '2508.01306'
source_url: https://arxiv.org/abs/2508.01306
tags:
- words
- word
- puzzled
- masked
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PUZZLED, a novel jailbreak attack that leverages\
  \ word-based puzzles to bypass LLM safety filters. The method masks keywords in\
  \ harmful instructions and embeds them into familiar puzzle formats\u2014word search,\
  \ anagram, and crossword\u2014requiring the model to solve the puzzle to reconstruct\
  \ and respond to the original harmful prompt."
---

# PUZZLED: Jailbreaking LLMs through Word-Based Puzzles

## Quick Facts
- arXiv ID: 2508.01306
- Source URL: https://arxiv.org/abs/2508.01306
- Reference count: 40
- Primary result: PUZZLED achieves 88.8% average attack success rate across five LLMs using word puzzles to bypass safety filters

## Executive Summary
This paper introduces PUZZLED, a novel jailbreak attack that leverages word-based puzzles to bypass LLM safety filters. The method masks keywords in harmful instructions and embeds them into familiar puzzle formats—word search, anagram, and crossword—requiring the model to solve the puzzle to reconstruct and respond to the original harmful prompt. Evaluated on five state-of-the-art LLMs using standardized benchmarks, PUZZLED achieves an average attack success rate of 88.8%, with peak performance of 96.5% on GPT-4.1 and 92.3% on Claude 3.7 Sonnet. The approach demonstrates both high effectiveness and efficiency, using minimal model calls while consistently outperforming baseline jailbreak methods across diverse models and categories.

## Method Summary
PUZZLED works by masking keywords in harmful instructions and presenting them as word puzzles for the LLM to solve. The attack uses three puzzle formats: word search (keywords hidden in a grid), anagram (letters scrambled), and crossword (symbols replacing letters with shared positions). Essential harmful keywords are masked first, with supplementary context words filling remaining slots. The model must solve the puzzle to identify masked words, reconstruct the full instruction, and generate a response. This multi-step reasoning pathway exploits the gap between surface-form detection and semantic understanding in current safety systems.

## Key Results
- PUZZLED achieves 88.8% average attack success rate across five LLMs (GPT-4o, GPT-4.1, Claude 3.7 Sonnet, LLaMA 3.1 8B, LLaMA 3.1 70B)
- Peak performance of 96.5% on GPT-4.1 and 92.3% on Claude 3.7 Sonnet
- Outperforms baseline jailbreak methods across all tested models and categories
- Larger models with stronger reasoning capabilities show higher vulnerability (scale-dependent effect)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Obfuscation through Keyword Masking
PUZZLED masks 3-6 keywords from harmful instructions and embeds them in puzzle structures, reducing surface-level detection by safety filters while preserving reconstructibility. Essential keywords (explicitly harmful terms) are masked first, with supplementary context words filling remaining slots. Indexed placeholders ([WORD1], [WORD2]) replace generic blanks to frame the task as puzzle-solving rather than censorship. This exploits the assumption that safety filters primarily detect harmful intent through explicit keyword presence rather than contextual reasoning pathways.

### Mechanism 2: Multi-Step Reasoning Pathway Exploitation
Puzzle structures activate multi-step reasoning that bypasses refusal triggers by framing harmful content reconstruction as a legitimate cognitive task. The model must (1) solve the puzzle to identify masked words, (2) reconstruct the full instruction, and (3) generate a response. This sequential reasoning creates cognitive distance from raw harmful intent, exploiting the assumption that LLMs trained on helpfulness and reasoning tasks will prioritize task completion over safety refusal when harmful content emerges indirectly.

### Mechanism 3: Scale-Dependent Vulnerability
Larger models with stronger reasoning capabilities exhibit higher attack success rates, creating a paradox where more capable models are more vulnerable. GPT-4.1 (300B) performs more refined reconstruction from partial information, inadvertently enabling more effective jailbreaks. This relies on the assumption that reasoning capability and safety alignment are not monotonically correlated; stronger reasoning can undermine safety when exploited.

## Foundational Learning

- Concept: **Safety Alignment vs. Reasoning Tension**
  - Why needed here: Understanding that helpfulness and reasoning training can conflict with safety objectives under adversarial prompting
  - Quick check question: Can you explain why a model trained to be helpful might comply with a harmful request framed as a puzzle?

- Concept: **Token-Level vs. Semantic-Level Filtering**
  - Why needed here: PUZZLED exploits the gap between surface-form detection and semantic understanding in current safety systems
  - Quick check question: What is the difference between detecting the word "firearm" and detecting intent to provide weapon instructions?

- Concept: **Adversarial Prompt Engineering**
  - Why needed here: PUZZLED belongs to a class of attacks that transform harmful instructions rather than submitting them directly
  - Quick check question: How does obfuscation differ from direct adversarial suffixes in jailbreak methodology?

## Architecture Onboarding

- Component map: Harmful instruction → Keyword identification (essential list priority) → Masking with indexed placeholders → Puzzle construction → Clue generation → Model submission → Response evaluation
- Critical path: The pipeline processes instructions through keyword masking, puzzle generation, and clue creation before model submission, with GPT-4o scoring responses against a ≥7/10 threshold
- Design tradeoffs: More masked words increase obfuscation but raise reconstruction difficulty; crossword requires fewer clues but depends on shared letters existing; anagram is simplest but provides no structural guidance
- Failure signatures: Low ASR on smaller models (LLaMA 8B) indicates insufficient reconstruction capability; Claude baseline ASR near 0% but PUZZLED 92.3% suggests reasoning pathway activation is model-specific; single-word masking reduces effectiveness for crossword
- First 3 experiments: 1) Replicate keyword masking on 10 AdvBench samples with word search format; measure ASR on GPT-4o, 2) Vary masked word count (1, 3, 5) on Claude 3.7 Sonnet using anagram format; plot ASR curve, 3) Test crossword format with cached vs. fresh clue generation; compare efficiency and ASR

## Open Questions the Paper Calls Out

### Open Question 1
What defense mechanisms can effectively detect or mitigate puzzle-based jailbreak attacks like PUZZLED? The paper reviews defense approaches (input perturbation, decoding intervention, output re-evaluation) but does not evaluate any defenses against PUZZLED, despite its high success rates (88.8% average ASR). This remains unresolved because the attack exploits reasoning pathways rather than surface-form manipulations.

### Open Question 2
How can masking strategies be adaptively optimized for different target models and instruction types? The results indicate that setting the number of masked words solely in proportion to instruction length may not be optimal, supporting the need for an adaptive masking strategy that adjusts dynamically to model characteristics.

### Open Question 3
Do PUZZLED-generated attack prompts transfer across different LLMs, or is per-target puzzle generation required? The paper evaluates each model independently using the same pipeline but does not test cross-model transferability, a common metric in jailbreak literature.

## Limitations
- The mechanism of success remains uncertain—whether effectiveness stems from genuine semantic obfuscation or GPT-4o scorer bias in the evaluation pipeline
- Lack of real-world deployment testing; all experiments occur in controlled laboratory conditions with standard benchmarks
- Scale-vulnerability correlation is observational rather than theoretically grounded

## Confidence
- **High Confidence**: Keyword masking as obfuscation technique; multi-step reasoning activation
- **Medium Confidence**: Attack success rates (88.8% ASR) may be influenced by scorer bias; efficiency claims assume cached clue generation works as intended
- **Low Confidence**: Scale-dependent vulnerability observation lacks explanatory depth; model-specific reasoning pathway exploitation could indicate baseline test coverage issues

## Next Checks
1. Cross-scoring validation: Run PUZZLED prompts through independent safety evaluators (human panel or other classifiers) to verify whether success stems from puzzle obfuscation or scorer bias
2. Contextual detection resistance test: Modify PUZZLED to include detectable puzzle patterns and measure ASR drops to validate whether safety filters detect surface-level patterns versus semantic content
3. Adversarial training simulation: Fine-tune a safety model on PUZZLED-style prompts to test whether the model learns to detect puzzle-based obfuscation patterns or if the multi-step reasoning pathway remains exploitable