---
ver: rpa2
title: 'ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning
  for French Toxicity Detection'
arxiv_id: '2508.11281'
source_url: https://arxiv.org/abs/2508.11281
tags:
- toxicity
- arxiv
- toxic
- french
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TOXIFRENCH, a new dataset of 53,622 French\
  \ online comments annotated via a semi-automated pipeline, and demonstrates that\
  \ small language models (SLMs) can outperform larger models in French toxicity detection.\
  \ The authors propose a novel Chain-of-Thought (CoT) fine-tuning strategy using\
  \ a dynamic weighted loss that progressively emphasizes the model\u2019s final decision."
---

# ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection

## Quick Facts
- **arXiv ID**: 2508.11281
- **Source URL**: https://arxiv.org/abs/2508.11281
- **Reference count**: 40
- **Primary result**: Small language models outperform larger models in French toxicity detection after CoT fine-tuning

## Executive Summary
This paper introduces TOXIFRENCH, a new dataset of 53,622 French online comments annotated via a semi-automated pipeline, and demonstrates that small language models (SLMs) can outperform larger models in French toxicity detection. The authors propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic weighted loss that progressively emphasizes the model's final decision. Fine-tuning a 4B model with this approach achieves state-of-the-art performance, improving balanced accuracy by 10% and outperforming larger models such as GPT-4o and Gemini-2.5 on the benchmark, while also demonstrating strong cross-lingual generalization.

## Method Summary
The authors developed TOXIFRENCH by combining 16 existing French toxicity datasets with 10,000 synthetic examples generated by GPT-4o and Claude-3.5-Sonnet. They employed a semi-automated annotation pipeline using GPT-4o and Gemini-2.5 for initial labeling, followed by human validation of 4,275 examples. For model enhancement, they introduced a CoT fine-tuning strategy with a dynamic weighted loss function that gradually shifts emphasis from reasoning steps to the final decision. A 4B parameter model was fine-tuned using this approach and evaluated against baseline models including GPT-4o and Gemini-2.5.

## Key Results
- TOXIFRENCH dataset contains 53,622 French comments across 12 toxicity categories
- Fine-tuned 4B model achieves 10% improvement in balanced accuracy over baselines
- Small language models outperform GPT-4o and Gemini-2.5 in French toxicity detection

## Why This Works (Mechanism)
The CoT fine-tuning approach works by training models to first reason through toxicity classification steps before making final decisions. The dynamic weighted loss function progressively shifts from emphasizing reasoning steps to the final classification, allowing the model to develop robust intermediate reasoning capabilities while still optimizing for end-task performance. This structured reasoning process helps models better capture the nuanced context and linguistic patterns specific to French toxicity, which larger general-purpose models may overlook when processing this specialized task.

## Foundational Learning
- **Semi-automated annotation pipelines**: Why needed - To scale dataset creation while maintaining quality; Quick check - Compare human validation rates across different toxicity categories
- **Dynamic weighted loss functions**: Why needed - To balance reasoning development with final decision optimization; Quick check - Analyze loss progression curves during training
- **Chain-of-Thought reasoning**: Why needed - To improve model's ability to handle complex, context-dependent toxicity cases; Quick check - Evaluate reasoning step accuracy alongside final classification performance
- **Cross-lingual generalization**: Why needed - To ensure models perform well on toxicity detection beyond French; Quick check - Test on multilingual toxicity datasets
- **Balanced accuracy metrics**: Why needed - To account for class imbalance in toxicity detection; Quick check - Compare balanced vs. standard accuracy scores across categories
- **Small language model optimization**: Why needed - To achieve competitive performance with reduced computational overhead; Quick check - Measure inference latency and resource usage versus larger models

## Architecture Onboarding

**Component map**: Data pipeline (collection → synthetic generation → automated annotation → human validation) → Model architecture (4B parameters) → CoT fine-tuning (dynamic weighted loss) → Evaluation (TOXIFRENCH benchmark + cross-lingual tests)

**Critical path**: Data annotation → Model fine-tuning → Performance evaluation, where each stage depends on the successful completion of the previous one for valid benchmarking

**Design tradeoffs**: The semi-automated annotation pipeline balances scale with quality, accepting potential annotation noise for dataset size. The 4B model size represents a tradeoff between computational efficiency and representational capacity. The dynamic loss weighting trades immediate optimization for long-term reasoning capability development.

**Failure signatures**: Over-reliance on synthetic data may introduce bias; human validation on only 8% of data may miss systematic annotation errors; the CoT approach may underperform on straightforward toxicity cases that don't require complex reasoning.

**First experiments**:
1. Validate annotation consistency by having multiple human annotators label a stratified sample across all toxicity categories
2. Compare CoT fine-tuning performance against standard fine-tuning on a held-out validation set
3. Test model robustness by evaluating on adversarial examples designed to fool toxicity detection systems

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Long-term robustness of semi-automated annotation pipelines is uncertain due to limited human validation
- Dynamic weighted loss formulation assumes monotonic progression without strong empirical justification
- Cross-lingual generalization claims rest on limited out-of-distribution tests
- Environmental and computational costs of fine-tuning 4B models are not discussed

## Confidence
- **High confidence**: TOXIFRENCH is a large, curated French toxicity dataset; small language models can outperform larger ones in this task
- **Medium confidence**: CoT fine-tuning approach's effectiveness and generalization claims
- **Low confidence**: Annotation pipeline reliability for nuanced toxicity cases; long-term stability under evolving language use

## Next Checks
1. Conduct human annotation validation on a stratified random sample across all toxicity categories, not just the initial subset
2. Evaluate model robustness against adversarial or evolving toxic language patterns not present in the training corpus
3. Perform a cost-benefit analysis comparing computational resources and inference latency between the proposed 4B fine-tuned model and larger alternatives across deployment scenarios