---
ver: rpa2
title: 'Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans
  for Problem Solving'
arxiv_id: '2505.00031'
source_url: https://arxiv.org/abs/2505.00031
tags:
- lepa
- plan
- plans
- solutions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEPA introduces anticipatory plans as high-level meta-knowledge
  before detailed problem solutions in self-training of LLMs. By prompting models
  to generate abstract plans and refining them through self-reflection, LEPA learns
  generalizable problem-solving strategies rather than memorizing specific solutions.
---

# Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving

## Quick Facts
- **arXiv ID**: 2505.00031
- **Source URL**: https://arxiv.org/abs/2505.00031
- **Reference count**: 10
- **Primary result**: LEPA achieves 30.2% accuracy on MATH benchmark vs 27.2% best baseline

## Executive Summary
LEPA introduces a novel self-training framework that teaches LLMs to generate anticipatory plans before solving problems. The method prompts models to create abstract meta-knowledge about problem-solving strategies, then refines these plans through self-reflection on failures. Experiments on four benchmarks show LEPA improves performance by up to 4.8% over baselines, with particularly strong results on mathematical reasoning tasks. The approach learns generalizable problem-solving patterns rather than memorizing specific solutions.

## Method Summary
LEPA operates through a two-phase iterative process. First, for data generation: the LLM generates an anticipatory plan for a given problem, then produces a solution conditioned on that plan. If the solution is incorrect, the model performs up to 4 rounds of self-reflection to improve the plan. Second, for model optimization: the refined plan-solution pairs are formatted as two-turn conversations and used for supervised fine-tuning. The framework uses a Llama 3 8B Instruct base model with temperature 0.5 for generation and 0.0005 for testing, learning rate 3e-7, and trains for 1 epoch per iteration.

## Key Results
- LEPA achieves 30.2% accuracy on Hendrycks MATH benchmark vs 27.2% for best baseline (ReST-EM)
- Performance improvements up to 4.8% observed across Hendrycks MATH, Hellaswag, BoolQ, and PIQA benchmarks
- Ablation studies confirm anticipatory plans and self-reflection are critical for performance gains
- LEPA demonstrates better generalization to out-of-distribution data compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Generating anticipatory plans before detailed solutions reduces cognitive load and improves solution quality.
- **Mechanism**: Plans act as abstract blueprints that decompose problem-solving into high-level strategy formulation followed by detailed execution. This separation isolates meta-knowledge from instance-specific computation.
- **Core assumption**: Models can learn to generate useful abstract plans that generalize across problem instances without collapsing into problem-specific solutions.
- **Evidence anchors**:
  - [abstract]: "LEPA trains the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge for problem-solving"
  - [Section 2.3]: "The anticipatory plans serve as blueprints that outline the necessary problem-solving steps"
  - [corpus]: Weak direct evidence. PPA-Plan (FMR=0.496) addresses long-context planning reliability but focuses on different failure modes.
- **Break condition**: Plans become too vague to guide execution OR plans collapse into full solutions (bypassing the abstraction benefit).

### Mechanism 2
- **Claim**: Self-reflection iteratively improves plan quality by identifying failure patterns in previous attempts.
- **Mechanism**: When a plan leads to an incorrect solution, the model analyzes the failure, generates linguistic feedback about what went wrong, and produces a revised plan.
- **Core assumption**: The model can accurately diagnose why a plan failed and generate genuinely improved plans rather than superficially different ones.
- **Evidence anchors**:
  - [Section 2.1]: "LEPA refines the plan with self-reflection... The LLM is prompted with the problem, the previous plan, the corresponding incorrect solution, and the correct answer"
  - [Section 3.2, ablation]: "Without Self-Reflection" variant achieves 28.8% on MATH vs. 30.2% for full LEPA
  - [corpus]: No direct corpus evidence for this specific self-reflection mechanism in planning contexts.
- **Break condition**: Self-reflection produces superficial changes that don't address root causes OR reflection loop fails to converge within iteration limits.

### Mechanism 3
- **Claim**: Training on plan-solution pairs teaches generalizable meta-knowledge rather than instance-specific memorization.
- **Mechanism**: By conditioning solution generation on plans, the training objective forces the model to learn the mapping from problem → abstract strategy → solution.
- **Core assumption**: The plan representation is sufficiently expressive to capture useful meta-knowledge and sufficiently constrained to prevent information bypassing.
- **Evidence anchors**:
  - [Section 2.2]: Training objective maximizes "log pθt(pti, yti|xi)" - joint probability of plan and solution conditioned on problem
  - [Section 3.2, ablation]: "Without Plan" variant drops to 24.3% on MATH (vs. 30.2%), confirming plans capture non-redundant information
  - [corpus]: Hint-before-Solving Prompting (HSP, mentioned in Related Works) uses pre-collected hints; LEPA's contribution is self-generated plans, but corpus lacks comparative studies.
- **Break condition**: Plans become redundant with solutions (no compression benefit) OR plans fail to transfer to out-of-distribution problems.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: LEPA extends CoT by inserting an explicit planning stage before detailed reasoning. Understanding CoT helps distinguish what LEPA adds (meta-level strategy) vs. what it preserves (step-by-step execution).
  - Quick check question: Can you explain why generating a plan before CoT might help when standard CoT already decomposes problems?

- **Concept: Self-Training with Rejection Sampling**
  - Why needed here: LEPA operates within the self-training paradigm where models generate their own training data. The rejection sampling baseline (ReST, ReST-EM) provides the comparison point for understanding LEPA's contribution.
  - Quick check question: What failure mode does rejection sampling address, and how does LEPA's self-reflection differ from simply sampling until correct?

- **Concept: Meta-Learning / Learning to Learn**
  - Why needed here: The paper explicitly frames anticipatory plans as "meta-knowledge" - patterns about how to solve classes of problems rather than specific solutions. This abstraction level distinguishes LEPA from standard supervised learning.
  - Quick check question: If a plan works for one combinatorics problem, what properties would make it transferable to a different combinatorics problem with different numbers?

## Architecture Onboarding

- **Component map**: Problem input → Plan generator → Solution generator → Verifier → (if fail) Self-reflection loop → Training data collector → SFT trainer → Optimized model

- **Critical path**: The self-reflection loop is the highest-risk component. If reflection prompts are poorly designed, the model may produce superficially different but equally ineffective plans. The prompt template enforcing "no problem-specific information" is the key constraint preventing information bypassing.

- **Design tradeoffs**:
  - Plan abstraction level: Too abstract → no guidance; too specific → memorization/bypassing
  - Reflection iterations: More iterations → higher compute cost and potential overfitting to specific failures; fewer iterations → incomplete plan improvement
  - Training data filtering: Only correct solutions → smaller dataset; including failures with negative rewards → requires RL infrastructure

- **Failure signatures**:
  - Plans containing specific numerical answers or calculations (information bypassing)
  - Reflection producing near-identical plans with cosmetic changes
  - Test performance worse than zero-shot CoT (plans actively misleading)
  - Sharp drop on out-of-distribution benchmarks despite in-distribution gains (overfitting to training problem distribution)

- **First 3 experiments**:
  1. **Ablation: Plan quality ceiling** - Manually inspect 50 generated plans across difficulty levels. Score each on: (a) abstraction level, (b) relevance to problem type, (c) absence of problem-specific details. Correlate scores with solution success rates.
  2. **Reflection loop analysis** - Track plan changes across reflection iterations. Measure: (a) lexical similarity between plan versions, (b) whether reflected plans address identified failure modes, (c) convergence rate (what fraction succeed within iteration limits).
  3. **Transfer test** - Train LEPA on one dataset (e.g., MATH algebra), evaluate zero-shot on held-out categories (e.g., MATH geometry). Compare plan content across categories to assess whether plans capture category-agnostic vs. category-specific patterns.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can integrating LEPA with reinforcement learning algorithms (e.g., PPO, DPO) significantly outperform the supervised fine-tuning approach used in the paper?
  - **Basis in paper**: [explicit] Section 2.2 states, "LEPA is also compatible with more sophisticated RL algorithms... We believe RL algorithms can further boost LEPA’s performance."
  - **Why unresolved**: The authors utilize SFT for simplicity and only briefly mention testing REINFORCE in the appendix; the potential of advanced RL optimization remains theoretical.
  - **Evidence would resolve it**: Experiments comparing test accuracy and convergence speed of LEPA+PPO against LEPA+SFT on the Hendrycks MATH benchmark.

- **Open Question 2**: How can a system automatically distinguish complex problems requiring planning from simple ones to optimize computational efficiency?
  - **Basis in paper**: [explicit] The Conclusion identifies the need to "automatically identify complex problems that require planning from simple problems... to avoid wasting compute resources."
  - **Why unresolved**: LEPA currently employs a fixed "plan-then-solve" strategy for all inputs, which may be redundant for trivial tasks where standard CoT is sufficient.
  - **Evidence would resolve it**: A dynamic selector or gating mechanism that applies LEPA only to high-difficulty inputs, maintaining accuracy while reducing average inference latency.

- **Open Question 3**: Do the performance gains of LEPA persist when scaling to significantly larger models (e.g., 70B+ parameters)?
  - **Basis in paper**: [explicit] The Conclusion notes, "It is also worth exploring how well can LEPA perform on larger and more advanced LLMs."
  - **Why unresolved**: Empirical validation was restricted to 8B parameter models (Llama 3/3.1); it is unclear if larger models with greater inherent capabilities show the same relative improvement.
  - **Evidence would resolve it**: Application of LEPA to a 70B+ model demonstrating a consistent performance delta over baselines comparable to the 8B results.

## Limitations

- **Verification challenges**: MATH benchmark has well-documented issues with solution correctness verification, particularly for multi-step reasoning where intermediate steps may be correct but final answers wrong.
- **Self-reflection effectiveness**: The paper doesn't analyze what types of failures reflection successfully addresses versus those it misses, making it unclear whether this represents robust improvement or overfitting.
- **Plan quality assurance**: The critical constraint preventing information bypassing (no problem-specific details in plans) relies heavily on prompt engineering rather than architectural guarantees.

## Confidence

- **Performance improvements**: Medium - while statistically significant, absolute numbers remain low and MATH verification challenges affect reliability
- **Self-reflection mechanism**: Low - effectiveness not analyzed, success rate on different failure types unknown
- **Plan abstraction mechanism**: Medium - ablation confirms plans contribute to performance, but quality assurance relies on prompt engineering

## Next Checks

1. **Plan Quality Analysis**: Sample 100 generated plans across different problem types and have human annotators score them on: (a) abstraction level vs problem-specific detail, (b) relevance to problem category, (c) whether plans could guide solutions for similar problems. Compare these scores with solution success rates to establish whether plan quality correlates with performance.

2. **Reflection Effectiveness Audit**: For 50 failed solutions, trace through the self-reflection process and categorize: (a) types of failures identified (mathematical error, strategy error, execution error), (b) whether reflection addresses the root cause, (c) success rate of reflection in producing improved plans. This reveals whether reflection provides genuine error correction or superficial changes.

3. **Out-of-Distribution Transfer Test**: Train LEPA on one MATH category (e.g., algebra), then evaluate zero-shot on held-out categories (geometry, number theory, etc.). Compare plan content similarity across categories and measure performance drop to assess whether plans capture generalizable meta-knowledge versus category-specific patterns.