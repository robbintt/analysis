---
ver: rpa2
title: 'The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model
  Factuality'
arxiv_id: '2512.10791'
source_url: https://arxiv.org/abs/2512.10791
tags:
- facts
- benchmark
- factuality
- response
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The FACTS Leaderboard introduces a comprehensive benchmark suite
  that evaluates large language models'' factuality across four distinct dimensions:
  multimodal (image-based questions), parametric (closed-book factoid questions),
  search (information-seeking with external tools), and grounding (long-form responses
  based on provided documents). Each sub-leaderboard uses automated judge models to
  score responses, with the final suite score being an average across all four components.'
---

# The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality

## Quick Facts
- arXiv ID: 2512.10791
- Source URL: https://arxiv.org/abs/2512.10791
- Reference count: 6
- Key outcome: Top-performing model (Gemini 3 Pro) achieves only 68.8% accuracy, indicating significant room for improvement in LLM factuality

## Executive Summary
The FACTS Leaderboard introduces a comprehensive benchmark suite that evaluates large language models' factuality across four distinct dimensions: multimodal (image-based questions), parametric (closed-book factoid questions), search (information-seeking with external tools), and grounding (long-form responses based on provided documents). Each sub-leaderboard uses automated judge models to score responses, with the final suite score being an average across all four components. The benchmark suite includes both public and private test sets to allow external participation while preventing overfitting. The evaluation reveals that the top-performing model, Gemini 3 Pro, achieves only 68.8% accuracy, indicating significant room for improvement. The suite highlights varying model strengths across different factuality aspects, with some models excelling at coverage while others prioritize precision and avoiding contradictions.

## Method Summary
The FACTS benchmark suite consists of four sub-benchmarks: Multimodal (image-based Q&A with rubric-based Coverage and No-Contradiction scoring), Parametric (closed-book factoid questions with correctness grading), Search (information-seeking using Brave Search API), and Grounding v2 (document-grounded long-form responses with dual-judge evaluation). Evaluation uses automated judge models for scalability, with public datasets available via Kaggle and private test sets for leaderboard integrity. The final FACTS Score is computed as the average accuracy across all four tasks, with additional metrics like hedging rate and eligibility checks providing deeper insights into model behavior.

## Key Results
- Top-performing model (Gemini 3 Pro) achieves only 68.8% accuracy across the full benchmark suite
- Models show distinct factuality profiles: Gemini models excel at coverage but have higher contradiction rates, while GPT models prioritize precision over completeness
- The Parametric benchmark reveals models are far from perfect in closed-book factoid recall, with top models achieving only moderate accuracy
- Search and Grounding tasks show significant room for improvement in tool-use and long-form response generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rubric-based dual-verdict evaluation isolates completeness from correctness in multimodal factuality
- Mechanism: Human annotators create ground-truth rubrics labeling facts as Essential or Non-Essential. An automated judge then produces two independent boolean verdicts: (1) Coverage—whether the response includes essential facts, and (2) No-Contradiction—whether any claims contradict the rubric, common knowledge, or the input image. Only responses satisfying both are marked accurate
- Core assumption: Automated judges can reliably approximate human fact-checking at scale; rubrics capture the relevant factual space for each question
- Evidence anchors:
  - [abstract] "Each sub-leaderboard employs automated judge models to score model responses"
  - [section 3.2] "Accuracy score: only responses that both cover essential facts and do not include any contradictions are considered accurate"
  - [corpus] Related work (VeriFact, FACTORY) supports decompose-verify pipelines but highlights context dependency challenges
- Break condition: If judges fail to capture nuance in visual reasoning or if rubrics miss relevant facts, coverage/contradiction scores become unreliable proxies for true factuality

### Mechanism 2
- Claim: Adversarial filtering through open-weight models ensures benchmark questions remain challenging for frontier LLMs
- Mechanism: Questions reflecting user interest are first verified against Wikipedia, then submitted to five strong open-weight models in a closed-book setting. Only questions that all models fail are retained, then human-verified. This decouples adversarial selection from proprietary API models used in evaluation
- Core assumption: Open-weight model limitations generalize to frontier models; Wikipedia availability during pretraining implies models should know these facts
- Evidence anchors:
  - [abstract] "The FACTS suite challenges current models"
  - [section 4.1.2] "we retain only the questions that none of these open-weight models answered correctly"
  - [corpus] SimpleQA Verified (arXiv:2509.07968) similarly addresses label quality and topical bias in factuality benchmarks
- Break condition: If open-weight models improve rapidly or if training data overlap differs significantly between open and proprietary models, adversarial filtering may not generalize

### Mechanism 3
- Claim: Multi-judge evaluation with ineligibility filtering prevents gaming via evasive or vague responses
- Mechanism: For Grounding v2, two independent judges (Gemini 2.5 Flash and GPT-5) score grounding. Separately, judges assess whether responses are "eligible" (meaningfully address the user request). Ineligible responses are marked inaccurate regardless of grounding score. This dual check prevents models from achieving high factuality by being unhelpfully brief
- Core assumption: Using multiple judges reduces self-preference bias; eligibility criteria can be consistently applied by LLMs
- Evidence anchors:
  - [abstract] "FACTS Grounding (v2)... featuring significantly improved judge models"
  - [section 6.2] "Ineligible responses are disqualified from factuality evaluation and the final factuality score is adjusted"
  - [corpus] Weak direct corpus evidence on multi-judge bias reduction; Wataoka et al. (2024) cited in paper supports self-preference bias concern
- Break condition: If judges share systematic blind spots or eligibility criteria become subjective, scoring integrity degrades

## Foundational Learning

- Concept: **Context-grounded vs. parametric factuality**
  - Why needed here: FACTS distinguishes between facts the model must extract from provided context (Grounding, Multimodal) vs. facts recalled from training weights (Parametric). Conflating these leads to misdiagnosis of failure modes
  - Quick check question: If a model fails a FACTS Parametric question but succeeds on the same fact in FACTS Grounding, what does this indicate about its knowledge representation?

- Concept: **LLM-as-judge with validation**
  - Why needed here: All four sub-benchmarks rely on automated judges whose credibility depends on correlation with human annotations. The paper reports macro F1 scores (72.3 for Coverage, 78.2 for No-Contradiction) as validation
  - Quick check question: What are two failure modes if judge validation is skipped before deploying a benchmark?

- Concept: **Public/private split for benchmark integrity**
  - Why needed here: The leaderboard uses private test sets to prevent overfitting. Models could otherwise optimize for public prompts without improving general factuality
  - Quick check question: Why does the paper evaluate on both public and private splits, and what would happen if only public splits were used?

## Architecture Onboarding

- Component map:
  - FACTS Multimodal (~1,500 questions) -> Image + question -> rubric-based Coverage + No-Contradiction scoring (Gemini 2.5 Pro judge)
  - FACTS Parametric (2,104 QA pairs) -> Closed-book factoid questions -> correct/incorrect/not-attempted/unknown grading (Gemini 2.5 Pro judge, 3 samples averaged)
  - FACTS Search (1,884 questions) -> Questions requiring web search -> Brave Search API -> correct/incorrect/not-attempted grading (Gemini 2.0 Flash judge)
  - FACTS Grounding v2 -> Long-form responses to document-based queries -> dual-judge grounding score + eligibility check -> adjusted factuality score
  - Aggregation: FACTS Score = average accuracy across all four sub-leaderboards (public + private)

- Critical path: Data curation (human + adversarial filtering) -> judge model selection/validation -> evaluation (public + private) -> score aggregation -> leaderboard update

- Design tradeoffs:
  - Single judge (Parametric) vs. dual judges (Grounding): Simplicity/maintainability vs. bias reduction
  - Coverage threshold at 0.5: Balances recall vs. precision in rubric matching
  - Standardizing on Gemini 2.5 Pro for Parametric: Preserves ranking trends vs. mixed-model panel but may introduce model-family bias

- Failure signatures:
  - High hedging rate + low accuracy (e.g., GPT-5 mini: 67.6% hedging, 16.0% accuracy on Parametric) -> model over-cautious, poor calibration
  - High Coverage + low No-Contradiction (e.g., Gemini models) -> recall-oriented but prone to hallucination
  - High No-Contradiction + low Coverage (e.g., GPT models on Multimodal) -> precision-oriented but incomplete answers
  - Ineligible responses with high grounding scores -> gaming via evasiveness

- First 3 experiments:
  1. Run your model on the public splits of all four sub-benchmarks, compute per-benchmark accuracy and secondary metrics (hedging rate, attempted accuracy). Identify which factuality dimension is weakest
  2. Inspect failure cases where your model has high Coverage but low No-Contradiction on Multimodal—diagnose whether errors stem from visual misinterpretation or parametric hallucination
  3. Compare your model's FACTS Search performance with and without search tool access to quantify the tool-use benefit and identify retrieval vs. synthesis errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the frequency of entities in training data correlate with accuracy on the FACTS Parametric benchmark?
- Basis in paper: [explicit] The authors state in the conclusion that "it would be interesting to check if this is reflected in FACTS Parametric," referring to previous work showing "infrequent entities are harder to learn"
- Why unresolved: The paper reports overall accuracy scores (e.g., Table 6) but does not provide a fine-grained analysis of error rates based on entity frequency or popularity
- What evidence would resolve it: A correlation analysis comparing model performance on specific FACTS Parametric questions against the frequency of the target entities in common training corpora (e.g., Common Crawl or Wikipedia page views)

### Open Question 2
- Question: How can "tailness" be defined and measured for search-based factuality tasks?
- Basis in paper: [explicit] The authors note that "it would be interesting to study notions of 'tailness' for FACTS Search, where some facts might be harder to search for than others"
- Why unresolved: While the paper introduces a "Hard Tail" subset, it does not generalize a metric for "tailness" in search contexts or analyze how this property predicts model failure across the full dataset
- What evidence would resolve it: A study proposing a metric for search query tailness (e.g., based on result abundance or index frequency) and demonstrating a statistical correlation with the "Attempted accuracy" or "Hedging rate" of the evaluated models

### Open Question 3
- Question: What distinct factuality challenges arise when models utilize knowledge-base (KB) tools compared to standard web search?
- Basis in paper: [explicit] The conclusion identifies that "tool-use introduces new factuality challenges, for example when using knowledge-base calls as a tool"
- Why unresolved: The FACTS Search benchmark currently evaluates models using a search API (Brave Search), but does not evaluate structured knowledge-base querying, leaving this specific tool-use scenario unbenchmarked
- What evidence would resolve it: The creation and evaluation of a benchmark subset where models must interact with a structured knowledge base API, alongside an analysis of error types distinct from those found in web search tasks

## Limitations
- Heavy reliance on automated judge models introduces uncertainty about scoring reliability, particularly for Multimodal and Search tasks
- The effectiveness of adversarial filtering may degrade as open-weight models improve, potentially requiring periodic recalibration
- Private test sets remain inaccessible, preventing full validation of leaderboard rankings

## Confidence
- High confidence: The dual-verdict rubric framework for Multimodal factuality and the public/private split methodology for benchmark integrity
- Medium confidence: The automated judge correlation metrics (F1 scores reported) and the FACTS Score aggregation methodology
- Low confidence: The generalizability of adversarial filtering across different model families and the long-term stability of judge models

## Next Checks
1. Run human-annotated validation on a subset of responses across all four sub-benchmarks to measure judge-model F1 scores and detect systematic scoring biases
2. Evaluate the same model responses using different judge models (e.g., GPT-5 vs. Gemini for Search) to quantify self-preference bias and establish cross-judge correlation thresholds
3. Compare FACTS Parametric question difficulty across open-weight models versus frontier models to quantify the effectiveness gap and identify when adversarial filtering may need recalibration