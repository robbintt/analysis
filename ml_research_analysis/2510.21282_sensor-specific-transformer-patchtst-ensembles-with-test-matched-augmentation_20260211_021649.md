---
ver: rpa2
title: Sensor-Specific Transformer (PatchTST) Ensembles with Test-Matched Augmentation
arxiv_id: '2510.21282'
source_url: https://arxiv.org/abs/2510.21282
tags:
- sensor
- patchtst
- windows
- test
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a sensor-specific ensemble of PatchTST transformers\
  \ with test-matched augmentation for robust human activity recognition under noisy\
  \ conditions. By training four independent models\u2014one per limb sensor\u2014\
  on augmented data that mimics real-world test-time noise (jitter, scaling, rotation,\
  \ channel dropout), and combining clean and robust streams at the probability level,\
  \ the approach achieves a private leaderboard macro-F1 of 51.72%, outperforming\
  \ the strongest inertial baseline by 4.5 pp."
---

# Sensor-Specific Transformer (PatchTST) Ensembles with Test-Matched Augmentation
## Quick Facts
- arXiv ID: 2510.21282
- Source URL: https://arxiv.org/abs/2510.21282
- Authors: Pavankumar Chandankar; Robin Burchard
- Reference count: 13
- Primary result: Achieved 51.72% macro-F1 on private test set, outperforming inertial baselines by 4.5 pp

## Executive Summary
This work introduces a sensor-specific ensemble of PatchTST transformers with test-matched augmentation for robust human activity recognition under noisy conditions. By training four independent models—one per limb sensor—on augmented data that mimics real-world test-time noise (jitter, scaling, rotation, channel dropout), and combining clean and robust streams at the probability level, the approach achieves a private leaderboard macro-F1 of 51.72%, outperforming the strongest inertial baseline by 4.5 pp. The method is lightweight (0.74 M parameters per encoder), resilient to single-sensor failure, and maintains sub-second inference speed.

## Method Summary
The approach trains eight PatchTST models: four "clean" models on unaugmented data and four "robust" models on data augmented to simulate test-time noise. Each model processes data from a single limb sensor independently, using per-window z-score normalization. At inference, predictions from clean and robust models for each sensor are averaged, then all four sensor outputs are combined. This dual-stream ensemble architecture provides both accuracy on clean data and resilience to noise.

## Key Results
- Achieved 51.72% macro-F1 on private leaderboard test set
- Outperformed strongest inertial baseline by 4.5 pp
- Demonstrated resilience to single-sensor failure through ensemble design
- Maintained sub-second inference speed with lightweight models (0.74M parameters each)

## Why This Works (Mechanism)
The method works by explicitly matching the training distribution to the test distribution through test-matched augmentation. By creating a "robust" stream that applies stochastic transformations mimicking real-world noise (jitter, scaling, rotation, channel dropout), the models learn to handle the corrupted data they'll encounter at test time. The ensemble design, with independent models per sensor, allows graceful degradation when individual sensors fail. The probability-level fusion preserves uncertainty information better than hard decision fusion.

## Foundational Learning
- **Per-window z-score normalization**: Standardizes each 1-second window individually rather than using global statistics, critical for handling gravity drift and sensor bias variations across different activities and time periods. Quick check: verify each window has mean≈0 and std≈1.
- **Test-matched augmentation**: Applying stochastic noise transforms during training that mirror expected test-time corruption, bridging the clean-to-noisy distribution gap. Quick check: validate augmentation parameters match real noise characteristics.
- **Dual-stream ensemble**: Training parallel clean and robust models captures both ideal-case accuracy and noise-robust performance, then fusing at probability level for optimal trade-off. Quick check: compare ensemble performance against single-stream baselines.

## Architecture Onboarding
- **Component map**: Raw accelerometer data -> Window segmentation (50 samples) -> Per-window normalization -> PatchTST encoder -> Classification head -> Dual-stream (Clean/Robust) -> Sensor-specific models (4) -> Probability averaging -> Final prediction
- **Critical path**: Data augmentation -> PatchTST encoding -> Probability fusion across sensors
- **Design tradeoffs**: Independent sensor processing sacrifices cross-limb correlation modeling for simplicity and robustness; dual-stream approach doubles computation but provides significant noise resilience
- **Failure signatures**: Poor normalization causes gravity drift artifacts; inadequate augmentation leads to overfitting clean data; incorrect fusion logic (averaging logits instead of probabilities) degrades performance
- **First experiments**: 1) Verify per-window normalization statistics, 2) Test single-stream vs. dual-stream performance gap, 3) Validate probability averaging vs. logit averaging

## Open Questions the Paper Calls Out
- **Cross-sensor attention mechanisms**: Can capture inter-limb synergies to reduce confusion between kinematically similar classes better than probability-level averaging, addressing the limitation that current pipeline ignores valuable cross-limb constraints
- **Biomechanical priors**: Incorporating priors that penalize kinematically implausible label sequences could reduce error rates by enforcing physical plausibility and reducing logical errors in continuous prediction streams
- **Adaptive weighting schemes**: Can outperform uniform averaging in scenarios involving sporadic device failures or extreme noise by learning confidence-weighted fusion rather than assuming equal sensor reliability

## Limitations
- Independent sensor processing ignores valuable cross-limb constraints and physical correlations between limb movements
- Uniform sensor weighting cannot adapt to sporadic device failures beyond single-sensor dropout scenarios
- Probability-level fusion may miss subtle inter-sensor temporal patterns that could improve confusing activity pairs

## Confidence
- **High Confidence**: Overall ensemble architecture, PatchTST parameter choices, preprocessing steps, and dual-stream training concept
- **Medium Confidence**: Augmentation magnitude ranges and inference fusion strategy
- **Low Confidence**: Specific channel dropout implementation and focal-style class weighting formula

## Next Checks
1. Implement both channel dropout variants (time-span masking vs. full-channel) and compare validation performance to resolve ambiguity in augmentation implementation
2. Test alternative class weighting schemes (inverse frequency vs. inverse square root) to verify sensitivity and impact on minority class performance
3. Validate whether per-fold or global temperature scaling yields better calibration on the public test set to resolve underspecified calibration procedure