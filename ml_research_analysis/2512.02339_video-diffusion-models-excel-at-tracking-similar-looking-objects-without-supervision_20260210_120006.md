---
ver: rpa2
title: Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision
arxiv_id: '2512.02339'
source_url: https://arxiv.org/abs/2512.02339
tags:
- video
- diffusion
- tracking
- objects
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TED, a self-supervised tracker that leverages
  the motion intelligence inherent in pretrained video diffusion models to track visually
  similar objects without task-specific training. Unlike existing methods that rely
  solely on appearance features, TED extracts motion-aware representations from the
  high-noise denoising stages of video diffusion models, where motion signals are
  prioritized.
---

# Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision

## Quick Facts
- arXiv ID: 2512.02339
- Source URL: https://arxiv.org/abs/2512.02339
- Reference count: 40
- Introduces TED, a self-supervised tracker that leverages motion intelligence from pretrained video diffusion models to track visually similar objects without task-specific training.

## Executive Summary
This paper introduces TED, a self-supervised tracker that leverages the motion intelligence inherent in pretrained video diffusion models to track visually similar objects without task-specific training. Unlike existing methods that rely solely on appearance features, TED extracts motion-aware representations from the high-noise denoising stages of video diffusion models, where motion signals are prioritized. By fusing these motion features with appearance features from image diffusion models, TED achieves robust tracking even for identical-looking objects. On standard benchmarks like DA VIS-2017, TED outperforms recent self-supervised methods by up to 6%, and on newly introduced challenging datasets with visually similar objects, it achieves up to 10% improvement.

## Method Summary
TED extracts motion-aware features from pretrained video diffusion models at high noise levels during denoising, where motion signals dominate over appearance. These features are fused with appearance features from image diffusion models, and labels are propagated across frames via weighted nearest-neighbor aggregation. The approach requires no training and leverages existing diffusion models' motion intelligence for robust tracking.

## Key Results
- TED outperforms recent self-supervised methods by up to 6% on DA VIS-2017
- Achieves up to 10% improvement on newly introduced challenging datasets with visually similar objects
- Demonstrates that pretrained diffusion models inherently learn useful motion representations for tracking

## Why This Works (Mechanism)
TED works by extracting motion-aware features from high-noise stages of video diffusion models, where motion signals are prioritized over appearance. By fusing these motion features with appearance features from image diffusion models, TED creates representations that can distinguish visually similar objects based on their motion patterns rather than just appearance.

## Foundational Learning
- **Diffusion Models**: Generative models that denoise images step-by-step - needed for understanding how TED extracts motion features from video diffusion models. Quick check: verify you understand forward and reverse diffusion processes.
- **Self-Supervised Tracking**: Tracking without manual annotations - fundamental to TED's approach. Quick check: ensure you grasp how labels are propagated from the first frame.
- **Feature Fusion**: Combining motion and appearance features - core to TED's design. Quick check: understand how λ parameter controls the balance between motion and appearance features.
- **Label Propagation**: Transferring labels across frames using nearest neighbors - key to TED's tracking mechanism. Quick check: verify you understand the weighted aggregation process.

## Architecture Onboarding

**Component Map**
- Video diffusion model (I2VGen-XL) -> Motion feature extraction (Rm) -> Feature fusion -> Label propagation
- Image diffusion model (ADM) -> Appearance feature extraction (Ra) -> Feature fusion
- Fused features (Rf) -> Weighted nearest-neighbor aggregation -> Final segmentation

**Critical Path**
The critical path is: video clip input → high-noise diffusion feature extraction → motion feature Rm → fusion with appearance features Ra → label propagation across frames → final segmentation.

**Design Tradeoffs**
The main tradeoff is between motion discrimination and computational cost. Higher noise levels (τ) in the video diffusion model provide better motion discrimination but require more computation. The fusion parameter λ balances motion vs appearance features.

**Failure Signatures**
- Using low τ (<300) for video diffusion features yields weak motion discrimination - similar-looking objects get confused
- Processing frames independently instead of as clips breaks temporal attention and eliminates motion cues
- Incorrect block index for feature extraction drops performance sharply

**First Experiments**
1. Verify motion feature extraction works by visualizing PCA of Rm - similar-looking objects should have distinct colors
2. Test label propagation by checking if known objects maintain consistent labels across frames
3. Ablate λ parameter to find optimal balance between motion and appearance features

## Open Questions the Paper Calls Out
- Can the motion intelligence from video diffusion models be effectively distilled into lightweight models for real-time tracking?
- Can the optimal noise level (timestep τ) for extracting motion representations be determined dynamically rather than empirically?
- How do biases inherent in the pre-trained diffusion models impact the robustness of tracking in underrepresented scenarios?

## Limitations
- No quantitative evaluation of baseline self-supervised methods beyond citing abstract scores
- Newly proposed YouTube-Similar and Kubric-Similar datasets lack public release for independent verification
- Performance may degrade on domains far from the pretraining data due to model biases

## Confidence
- **High Confidence**: The architectural pipeline is clearly described and reproducible in principle
- **Medium Confidence**: Quantitative claims are supported by benchmark numbers but lack detailed breakdowns
- **Low Confidence**: Claims about generalization to new challenging datasets are hard to verify without access to data

## Next Checks
1. Implement and run the main self-supervised competitors (STM, CORP, TST) on the same test set and compare per-video scores with statistical significance tests
2. Obtain and inspect the YouTube-Similar and Kubric-Similar datasets, verifying their construction and annotation consistency, then rerun TED on these to confirm claimed improvements
3. Implement the softmax temperature scaling in the label aggregation step and verify that it improves performance on a held-out subset of DAVIS