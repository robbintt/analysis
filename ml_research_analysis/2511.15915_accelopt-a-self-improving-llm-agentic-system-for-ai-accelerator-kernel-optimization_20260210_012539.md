---
ver: rpa2
title: 'AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization'
arxiv_id: '2511.15915'
source_url: https://arxiv.org/abs/2511.15915
tags:
- kernel
- optimization
- accelopt
- kernels
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AccelOpt, a self-improving LLM agentic system
  for optimizing kernels on emerging AI accelerators like AWS Trainium. AccelOpt combines
  beam search with an optimization memory that accumulates insights from slow-fast
  kernel pairs discovered during exploration.
---

# AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization

## Quick Facts
- arXiv ID: 2511.15915
- Source URL: https://arxiv.org/abs/2511.15915
- Reference count: 40
- Primary result: Improves kernel throughput from 49% to 61% of peak on Trainium 1

## Executive Summary
AccelOpt is a self-improving LLM agentic system that optimizes kernels for emerging AI accelerators like AWS Trainium. The system combines beam search with an optimization memory that accumulates insights from slow-fast kernel pairs discovered during exploration. It operates through an agentic workflow with planner, executor, and summarizer components that iteratively generate and refine kernel implementations. The system was evaluated on NKIBench, a benchmark suite of 14 challenging kernels extracted from real-world LLM workloads, showing significant improvements in throughput and cost-effectiveness compared to both baseline approaches and commercial models like Claude Sonnet 4.

## Method Summary
AccelOpt employs an agentic workflow where a planner generates optimization strategies, an executor generates kernel implementations, and a summarizer analyzes results. The system uses beam search to explore multiple optimization paths in parallel while maintaining an optimization memory that stores successful transformations discovered during the search process. This memory accumulates insights from slow-fast kernel pairs, enabling the system to reuse and build upon previous optimizations. The approach is specifically designed for emerging AI accelerators like AWS Trainium, which present unique optimization challenges due to their specialized architectures and limited existing optimization knowledge.

## Key Results
- Improves average kernel throughput from 49% to 61% of peak on Trainium 1
- Improves average kernel throughput from 45% to 59% on Trainium 2
- Achieves performance matching Claude Sonnet 4 while being 26Ã— cheaper using open-source models

## Why This Works (Mechanism)
The system's effectiveness stems from its ability to explore multiple optimization paths simultaneously through beam search while learning from past successes. The optimization memory component allows AccelOpt to build upon previously discovered transformations, creating a cumulative knowledge base that improves over time. The agentic workflow structure enables systematic exploration of both local optimizations (like peephole optimizations) and global optimizations (such as loop transformations) without requiring expert-provided hardware-specific knowledge. This self-improving capability allows the system to discover optimizations that may not be obvious to human experts or captured by traditional compiler optimizations.

## Foundational Learning

**Beam Search**: A search algorithm that explores multiple paths in parallel by maintaining a set of the best candidates at each step. Why needed: Allows systematic exploration of optimization space without getting trapped in local optima. Quick check: Verify that beam width of 5 provides sufficient diversity while remaining computationally tractable.

**Optimization Memory**: A repository that stores successful slow-fast kernel pairs and the transformations that led to improvements. Why needed: Enables reuse of successful optimization patterns and prevents redundant exploration. Quick check: Confirm memory entries are being accessed and applied correctly during optimization.

**Agentic Workflow**: A structured approach with planner, executor, and summarizer components working iteratively. Why needed: Provides systematic exploration and refinement of optimization strategies. Quick check: Verify that each component correctly passes information to the next in the workflow.

## Architecture Onboarding

**Component Map**: Planner -> Executor -> Summarizer -> Optimization Memory (feedback loop)

**Critical Path**: The optimization loop where the planner generates strategies based on current performance, the executor generates kernel implementations, the summarizer analyzes results and updates the optimization memory, and this feedback influences subsequent planning cycles.

**Design Tradeoffs**: Beam search provides better exploration than repeated sampling but increases computational cost; optimization memory improves cost efficiency but may capture redundant information rather than novel insights.

**Failure Signatures**: Suboptimal performance when beam search gets trapped in local optima; poor convergence when optimization memory fails to capture useful patterns; ineffective exploration when planner generates redundant strategies.

**First Experiments**:
1. Test beam search with varying beam widths (3, 5, 10) to find optimal exploration-exploitation balance
2. Evaluate optimization memory effectiveness by comparing performance with and without memory component
3. Validate agentic workflow by isolating each component's contribution to overall performance improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluated exclusively on AWS Trainium accelerators, limiting generalizability to other hardware architectures
- Benchmark construction methodology for NKIBench is not fully detailed, making it difficult to assess representativeness
- Optimization memory improves cost efficiency but doesn't significantly improve peak performance when sufficient samples are generated
- Comparison with Claude Sonnet 4 focuses only on final metrics without accounting for optimization time or energy consumption

## Confidence

**High confidence**: The beam search approach outperforms repeated sampling for kernel optimization
**Medium confidence**: The cost-effectiveness claim relative to Claude Sonnet 4
**Medium confidence**: The effectiveness of optimization memory for cost efficiency
**Low confidence**: Claims about discovering novel global optimizations beyond what compilers could achieve

## Next Checks
1. Test AccelOpt on a different accelerator architecture (e.g., NVIDIA GPUs or Google TPUs) to assess hardware generality
2. Compare optimization results against state-of-the-art compiler auto-tuning tools like LLVM's ML-based optimization passes
3. Conduct an ablation study specifically measuring the novelty of optimizations discovered by AccelOpt versus those found through random search or compiler heuristics