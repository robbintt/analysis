---
ver: rpa2
title: Fundamental Principles of Linguistic Structure are Not Represented by o3
arxiv_id: '2502.10934'
source_url: https://arxiv.org/abs/2502.10934
tags:
- murphy
- structure
- sentence
- language
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: o3-mini-high fails basic linguistic structure tasks, struggling
  with phrase structure, Escher sentences, and generating ungrammatical outputs. It
  misidentifies grammatical sentences as ungrammatical and vice versa, lacks compositional
  reasoning, and incorrectly handles syntactic violations.
---

# Fundamental Principles of Linguistic Structure are Not Represented by o3

## Quick Facts
- **arXiv ID**: 2502.10934
- **Source URL**: https://arxiv.org/abs/2502.10934
- **Reference count**: 0
- **One-line primary result**: o3-mini-high fails basic linguistic structure tasks, struggling with phrase structure, Escher sentences, and generating ungrammatical outputs.

## Executive Summary
o3-mini-high demonstrates significant limitations in handling fundamental linguistic structure, failing tasks that require hierarchical parsing and compositional reasoning. While the model succeeds on linear, surface-level linguistic tests (like the Strawberry Test), it struggles with phrase structure rules, Escher sentences, center-embedding, and generating genuine syntactic violations. The model cannot reliably distinguish between syntactic and semantic acceptability, often conflating the two or failing to access multiple possible parses for ambiguous sentences. These findings challenge claims that large language models have surpassed theoretical linguistics in understanding language structure.

## Method Summary
The study evaluated OpenAI's o3-mini-high on 26 linguistic prompts covering phrase structure, Escher sentences, center-embedding, grammaticality judgments, violation generation, scope ambiguities, Jabberwocky-style pseudowords, and polysemy. Prompts were input via ChatGPT interface between January 31 and February 6, 2025. The evaluation used single-run responses without systematic variation in sampling parameters. Responses were categorized based on grammaticality judgment accuracy, success in generating genuine syntactic violations, and ability to handle gradient acceptability.

## Key Results
- o3-mini-high fails phrase structure tasks (Prompts 8-9), accepting ungrammatical structures like "glarts glarted" when lexical statistics are plausible
- The model cannot reliably generate genuine syntactic violations (Section 3.6), often producing semantically anomalous but grammatically well-formed sentences instead
- Only 1 of 10 partially acceptable sentences was correctly classified in the gradient acceptability task (Prompt 23)
- Model misclassifies grammatical sentences as ungrammatical when alternative parses exist (Section 3.9)

## Why This Works (Mechanism)

### Mechanism 1: Linear Statistics Sufficiency
- Claim: o3 succeeds on tasks requiring only sequential token statistics but fails when hierarchical structure is required.
- Mechanism: The model leverages surface-level transitional probabilities learned from training data to handle linear order tasks, but lacks explicit hierarchical parse representations.
- Core assumption: Success on linear tasks does not imply compositional syntactic competence.
- Evidence anchors:
  - [abstract] "while it succeeds on some basic linguistic tests relying on linear, surface statistics... it fails to generalize basic phrase structure rules"
  - [section 3.1] Model passed palindrome, strawberry test, and pronoun resolution prompts (1-7)
  - [corpus] "Analyzing the Inner Workings of Transformers in Compositional Generalization" notes input-output evaluations don't reveal internal mechanisms

### Mechanism 2: Syntax-Semantics Conflation
- Claim: The model cannot reliably dissociate grammaticality from semantic plausibility.
- Mechanism: When asked to generate ungrammatical sentences, o3 produces semantically anomalous but syntactically well-formed outputs, suggesting it represents "acceptability" as a single undifferentiated signal.
- Core assumption: True compositional systems maintain separate representational channels for form and meaning.
- Evidence anchors:
  - [abstract] "fails to distinguish between instructions to generate unacceptable semantic vs. unacceptable syntactic outputs"
  - [section 3.6] Prompt 15: model generated "Joycean and surreal" content but "does not satisfy the clear instruction to be ungrammatical"

### Mechanism 3: Single-Parse Bias
- Claim: o3 evaluates sentences against a single interpretation rather than maintaining multiple possible parses.
- Mechanism: When assessing ambiguous or partially acceptable structures, the model commits to one reading and fails to enumerate alternatives.
- Core assumption: Human-like meta-linguistic judgment requires representing the space of possible parses.
- Evidence anchors:
  - [abstract] "incapable of representing multiple parses to evaluate against various possible semantic interpretations"
  - [section 3.9] Model incorrectly judged grammatical sentences (12, 15, 16) as ungrammatical, failing to access alternative readings

## Foundational Learning

- Concept: **Phrase Structure Grammar**
  - Why needed here: The paper tests whether models understand constituency and hierarchical nesting (e.g., "Dogs dogs dog dog dogs"), which presupposes knowledge of how phrases combine recursively.
  - Quick check question: Can you explain why "The dog that the cat chased barked" involves nested phrases rather than just linear word sequences?

- Concept: **Comparative Illusions (Escher Sentences)**
  - Why needed here: These sentences (e.g., "More women have finished university than he has") are structurally well-formed but semantically anomalous, testing syntax-semantics boundary competence.
  - Quick check question: Why do humans judge "Fewer athletes have been to Beijing than I have" as acceptable structurally but semantically incoherent?

- Concept: **Gradient Acceptability**
  - Why needed here: The paper demonstrates o3 fails to distinguish fully acceptable, partially acceptable, and unacceptable sentences along a continuum.
  - Quick check question: How would you rate "Not three students arrived" on a 4-point acceptability scale, and why might speakers vary?

## Architecture Onboarding

- Component map: Input layer (tokenized prompts) -> embedding representations -> transformer architecture with reasoning modules -> chain-of-thought reasoning traces -> linguistic judgment output
- Critical path: Prompt design → tokenization → attention computation → reasoning trace generation → linguistic judgment output
- Design tradeoffs:
  - Scaling compute vs. architectural bias: Authors argue more compute alone won't solve compositionality without structural inductive biases
  - Statistical fluency vs. meta-linguistic understanding: Model optimizes for plausibility, not grammatical analysis
  - Assumption: Neuro-symbolic approaches (predicate-argument structure, variable binding) may be required for robust compositional reasoning
- Failure signatures:
  - Accepts ungrammatical structures when lexical statistics are plausible (Prompt 8-9: "glarts")
  - Generates grammatical sentences when explicitly asked for violations (Section 3.6)
  - Misclassifies grammatical sentences as ungrammatical when alternative parses exist (Section 3.9)
  - Cannot generate causally-linked syntactic violations across sentences (Section 3.7)
- First 3 experiments:
  1. **Baseline probe**: Replicate Prompts 1-7 (linear tasks) vs. Prompts 8-13 (hierarchical tasks) to confirm performance gap; expect >80% success on linear, <40% on hierarchical.
  2. **Parse enumeration test**: Present ambiguous sentences (e.g., "John likes Mary's picture of himself") and ask model to explicitly list all grammatical readings; evaluate against linguist-annotated ground truth.
  3. **Violation generation benchmark**: Request 50 ungrammatical sentences with specified violation types (island constraints, binding violations); have linguists annotate whether outputs contain target violations vs. mere semantic oddness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs represent multiple simultaneous syntactic parses to evaluate against possible semantic interpretations?
- Basis in paper: [explicit] Authors state the model "is seemingly incapable of representing multiple parses to evaluate against various possible semantic interpretations" and failed tasks requiring "interactional syntactic dynamics that require multiple steps to construct and evaluate."
- Why unresolved: The study showed o3 evaluates violations "on a mono-configurational basis" rather than tracking how violations interact across parse options.
- What evidence would resolve it: Tasks requiring models to explicitly enumerate multiple parses and select among them based on semantic coherence, with systematic comparison to human parse-generation behavior.

### Open Question 2
- Question: Would neuro-symbolic architectures incorporating explicit design features (predicate-argument structure, variable binding, constituent structure) overcome the compositional limitations observed in o3?
- Basis in paper: [explicit] Authors propose "a return to more traditional design features of the human mind... may be needed" and suggest "forms of neuro-symbolic approaches" as a potential solution.
- Why unresolved: The paper establishes the limitation but does not test alternative architectures; it merely speculates that symbolic components might address the "stubbornly resilient wall."
- What evidence would resolve it: Comparative evaluation of hybrid neuro-symbolic models on the same linguistic structure tasks, particularly those involving center-embedding and binding theory violations.

### Open Question 3
- Question: Which specific sub-components of syntax are in principle achievable by language models, and which remain structurally inaccessible?
- Basis in paper: [explicit] "Future research should more carefully apply the tools of linguistics to isolate specific sub-components of syntax that might be in principle achievable by language models, given specific design features."
- Why unresolved: The current study showed broad failures but did not systematically isolate which grammatical modules (e.g., agreement, movement, binding) might be selectively preserved.
- What evidence would resolve it: Fine-grained decomposition of syntactic phenomena with controlled experiments isolating individual grammatical principles, yielding a taxonomy of LLM-compatible versus incompatible linguistic operations.

## Limitations
- The evaluation relies entirely on single-prompt responses without systematic variation in sampling parameters or multiple trials
- The study focuses exclusively on one model variant without comparison to other frontier systems or to earlier o1/o3 versions
- While critiquing interpretability limitations, the authors do not provide internal attention analysis or probe for hierarchical structure emergence

## Confidence

- **High confidence**: The observation that o3-mini-high fails specific linguistic tasks requiring hierarchical structure (phrase structure rules, Escher sentences, center-embedding) is well-supported by the prompt-response pairs provided. These failures are concrete and verifiable.
- **Medium confidence**: The broader claim that o3-mini-high cannot represent multiple parses or dissociate syntax from semantics is supported but could reflect prompt design limitations rather than fundamental architectural constraints.
- **Low confidence**: The assertion that this work demonstrates large language models are "hitting a stubbornly resilient wall" in achieving human-like compositional abilities overstates the evidence, as the study doesn't establish whether architectural changes or increased scale might overcome these limitations.

## Next Checks

1. **Sampling parameter sensitivity**: Repeat all prompts across multiple temperature/top-p settings and reasoning depths to determine if failures persist under varied generation conditions.
2. **Multi-run reliability**: Execute each prompt 10 times and compute inter-run consistency to distinguish systematic failures from stochastic variation.
3. **Comparative baseline**: Evaluate GPT-4, Claude 3.5, and o1 on identical prompts to establish whether o3-mini-high's failures are model-specific or representative of current frontier systems.