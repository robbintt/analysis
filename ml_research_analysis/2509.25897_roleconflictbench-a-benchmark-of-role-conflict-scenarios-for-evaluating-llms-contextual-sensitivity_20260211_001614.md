---
ver: rpa2
title: 'RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs''
  Contextual Sensitivity'
arxiv_id: '2509.25897'
source_url: https://arxiv.org/abs/2509.25897
tags:
- role
- social
- urgency
- roles
- family
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoleConflictBench, a benchmark designed to
  evaluate how well large language models (LLMs) navigate complex social dilemmas
  involving role conflicts. The benchmark uses a three-stage pipeline to generate
  over 13,000 realistic role conflict scenarios across 65 roles, systematically varying
  role expectations and situational urgency levels.
---

# RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity

## Quick Facts
- **arXiv ID:** 2509.25897
- **Source URL:** https://arxiv.org/abs/2509.25897
- **Reference count:** 40
- **Primary result:** Large language models show significant bias toward certain social roles and demographics, failing to properly prioritize situational urgency in role conflict scenarios

## Executive Summary
RoleConflictBench is a benchmark designed to evaluate how well large language models navigate complex social dilemmas involving role conflicts. The benchmark uses a three-stage pipeline to generate over 13,000 realistic role conflict scenarios across 65 roles, systematically varying role expectations and situational urgency levels. When tested on 10 different LLMs, the results show that while models exhibit some sensitivity to contextual cues, their decisions are predominantly governed by inherent biases toward certain social roles and attributes rather than situational information. The analysis reveals strong preferences for Family and Occupation domains, along with prioritization of male and Abrahamic religious roles across most models. This work highlights a critical gap in LLMs' contextual sensitivity and provides a tool for diagnosing and addressing these biases in decision-making systems.

## Method Summary
RoleConflictBench generates realistic role conflict scenarios through a three-stage pipeline: Expectation Generation (GPT-4.1 creates role expectations), Situation Instantiation (urgency levels 1-3 are assigned to each expectation), and Story Synthesis (first-person narratives of 100-200 words). The benchmark contains 13,914 stories covering 65 roles across 5 domains (Family, Occupation, Society, Interpersonal, Religion). Evaluation uses a binary choice format where models select which role to prioritize, with metrics including Sensitivity Score (MSE of win probabilities conditioned on urgency), Role Priority Index (RPI via Bradley-Terry modeling), and Domain Preference Score (aggregated RPI per domain).

## Key Results
- LLMs show some sensitivity to urgency cues but decisions are dominated by inherent role biases rather than situational context
- Strong model preferences for Family and Occupation domains, with male and Abrahamic religious roles consistently prioritized
- Adding demographic tokens (e.g., "As a man/woman") significantly shifts domain preferences without changing the underlying scenario
- Most models exhibit "flat reasoning" by mapping social roles to single values (e.g., Family → Benevolence 93% of the time)

## Why This Works (Mechanism)

### Mechanism 1: Urgency-Responsive Preference Modulation
- Claim: LLMs can detect urgency cues but fail to let them override learned role hierarchies
- Mechanism: Models show consistent ordering p(win|u_high) > p(win|u_equal) > p(win|u_low) for individual roles, but magnitude of change is insufficient to overcome baseline role preferences
- Core assumption: Situational urgency should be the primary decision driver in context-aware social reasoning
- Evidence anchors: Section 4.2, Figure 3 shows lines are clearly separated but effect is secondary to dominant role hierarchies

### Mechanism 2: Static Role-Priority Encoding Overrides Context
- Claim: Models encode a fixed hierarchy of social roles that persists across all contextual variations
- Mechanism: Role Priority Index (RPI) via Bradley-Terry modeling reveals consistent role rankings regardless of urgency manipulation
- Core assumption: RPI captures learned bias from training data rather than principled moral reasoning
- Evidence anchors: Section 4.3, Figure 6 shows life-critical occupations rank highest; religious/family roles show model-specific patterns

### Mechanism 3: Demographic-Prime Contamination
- Claim: A single demographic token in the prompt shifts model recommendations via stereotype associations
- Mechanism: Adding "As a man/woman/Asian/Hispanic" changes domain preference scores without any change to underlying scenario
- Core assumption: Ideal context-sensitive reasoning should be invariant to user identity tokens when ethical structure is identical
- Evidence anchors: Section 4.2, Table 4 shows specifying user demographics changes Family role frequency from 16.4% to 20.6%

## Foundational Learning

- Concept: **Role Conflict Theory (Sociological)**
  - Why needed here: The benchmark constructs scenarios where social expectations are mutually exclusive by definition
  - Quick check question: Can you explain why a paper deadline vs. dissertation defense changes the moral calculus even when the role conflict structure is identical?

- Concept: **Bradley-Terry Paired Comparison Model**
  - Why needed here: RPI metric uses maximum likelihood estimation over pairwise win counts
  - Quick check question: How would the RPI change if a role never wins any pairwise comparison?

- Concept: **Schwartz Basic Human Values Framework**
  - Why needed here: Paper uses this to classify model reasoning; values like Benevolence, Security, Universalism are mapped to role domains
  - Quick check question: Why does finding that Family roles are 93% explained by Benevolence suggest "oversimplified heuristic"?

## Architecture Onboarding

- Component map: Role Set (65 roles, 5 domains) → Stage 1: Expectation Generation (GPT-4.1, temp=0) → Stage 2: Situation Instantiation (urgency 1/2/3) → Stage 3: Story Synthesis (first-person, 100-200 words) → Evaluation Query (binary choice + rationale + value label) → Metrics: Sensitivity Score (MSE), RPI (Bradley-Terry), P_d (domain aggregation)

- Critical path: Urgency level assignment → Situation plausibility validation → Story synthesis quality

- Design tradeoffs:
  - Cross-domain pairing only (no grandfather-vs-brother): cleaner attribution but misses intra-domain nuance
  - Gender-held-constant pairing: controls for gender bias in pairwise comparisons but prevents direct measurement of cross-gender priority
  - GPT-4.1 as generator: ensures high-quality stories but introduces generator-valuator confound

- Failure signatures:
  - Sensitivity score S near 50 (random): model ignoring urgency entirely
  - RPI variance near zero: no meaningful role differentiation
  - Value rationale dominated by single value (>90%): flat reasoning

- First 3 experiments:
  1. Urgency ablation: Re-run evaluation with roles-only to confirm Table 3's mean absolute difference persists
  2. Demographic invariance test: Add neutral demographic ("As a person") vs. no cue to establish baseline contamination magnitude
  3. Cross-model RPI correlation: Compute Spearman correlation of RPI rankings between model families to test whether bias is architecture-specific or training-data-general

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can inherent social role biases (e.g., prioritizing male or Abrahamic religious roles) be mitigated in LLMs without compromising their sensitivity to situational urgency?
- Basis in paper: [explicit] The conclusion states the work provides a tool for "diagnosing and addressing these biases," while Section 4 results demonstrate that inherent biases currently override situational cues
- Why unresolved: The paper successfully quantifies the rigidity of role preferences but does not propose or test specific debiasing techniques or alignment methods to correct them
- What evidence would resolve it: An interventional study showing that specific fine-tuning or prompting strategies reduce the "Role Preference Overrides Urgency" effect while maintaining or lowering Sensitivity scores (S)

### Open Question 2
- Question: What specific training data characteristics or alignment procedures cause the divergent domain preferences observed between model families (e.g., Qwen3's higher preference for Society/Religion vs. GPT/Gemini's focus on Occupation)?
- Basis in paper: [explicit] Section 4.3 notes that "the manifestation and intensity of inherent biases are strongly contingent on a model's specific design and training methodology," yet leaves the specific causal factors unexplored
- Why unresolved: The analysis is limited to observing output differences between model families without isolating root causes in training pipelines
- What evidence would resolve it: A controlled comparison of models trained on different data distributions to correlate data composition with domain preference scores ($P_d$)

### Open Question 3
- Question: Can interventions move LLMs beyond the "flat reasoning" of mapping social roles to single values (e.g., Family = Benevolence) to capture the pluralism of real-world decisions?
- Basis in paper: [inferred] Section 4.2 analyzes "Social Roles are Oversimply Mapped to Specific Values," finding that models default to narrow heuristics rather than truly understanding nuanced context
- Why unresolved: The paper identifies this lack of pluralism as a fundamental inability to navigate value conflicts but does not test methods to encourage more complex reasoning
- What evidence would resolve it: Experiments demonstrating that specific prompting architectures result in wider distribution of cited "Basic Human Values" in model's rationale for a single decision

## Limitations

- Generator-evaluator confound (using GPT-4.1 for both story generation and evaluation) may inflate or deflate observed sensitivity scores
- Binary choice format may oversimplify complex role conflicts that humans would resolve through nuanced compromise
- Role conflict scenarios may not capture the full complexity of real-world social dilemmas involving multiple stakeholders and competing values

## Confidence

- **High Confidence**: Demographic cues systematically shift domain preferences - clearly observable across all tested models and represents measurable artifact of stereotype associations in training data
- **Medium Confidence**: Role Priority Index stability - while RPI rankings appear consistent, metric's validity depends on Bradley-Terry assumptions that may not hold for all role pairs
- **Medium Confidence**: Urgency-responsive preference modulation - models show some sensitivity to urgency cues, but effect size is consistently smaller than baseline role preferences

## Next Checks

1. **Cross-Generator Validation**: Re-evaluate the benchmark using human-written stories or stories generated by a different LLM family to test whether observed sensitivity patterns persist independent of generation method

2. **Multi-Option Extension**: Modify the benchmark to include compromise options (e.g., "attend both partially" or "delegate") rather than binary choices to assess whether models can handle nuanced conflict resolution

3. **Temporal Sensitivity Analysis**: Test whether models show improved contextual sensitivity when provided with explicit temporal constraints (e.g., "the deadline is in 2 hours vs. 2 weeks") to determine if granularity of urgency information affects decision-making quality