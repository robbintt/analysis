---
ver: rpa2
title: 'FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing
  MCTS'
arxiv_id: '2505.16409'
source_url: https://arxiv.org/abs/2505.16409
tags:
- search
- reasoning
- retrieval
- zhang
- freeson
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FREESON (Retriever-FREE Retrieval-Augmented ReaSONing) is a novel
  framework that enables Large Reasoning Models to perform both reasoning and retrieval
  without a separate retrieval model. The core method introduces CT-MCTS (Corpus-Traversing
  Monte Carlo Tree Search), which allows LRMs to traverse the corpus token-by-token
  toward answer-containing regions.
---

# FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS

## Quick Facts
- arXiv ID: 2505.16409
- Source URL: https://arxiv.org/abs/2505.16409
- Reference count: 40
- Primary result: Achieves 14.4% average improvement in EM and F1 over retriever-based baselines on open-domain QA

## Executive Summary
FREESON (Retriever-FREE Retrieval-Augmented ReaSONing) is a novel framework that enables Large Reasoning Models to perform both reasoning and retrieval without a separate retrieval model. The core method introduces CT-MCTS (Corpus-Traversing Monte Carlo Tree Search), which allows LRMs to traverse the corpus token-by-token toward answer-containing regions. The approach uses a prefix-based CorpusTree index and incorporates granularity-aware multi-node expansion with stochastic beam search, combined with on-policy value network training from CT-MCTS rollouts. Evaluated on five open-domain QA benchmarks (single-hop and multi-hop), FREESON achieves an average improvement of 14.4% in EM and F1 over four multi-step reasoning models with separate retrievers, and outperforms the strongest baseline Search-R1 by 3% on PopQA and 2WikiMultihopQA.

## Method Summary
FREESON replaces traditional embedding-based retrieval with a corpus-constrained path-finding approach using CT-MCTS. The framework builds a CorpusTree index using FM-Index with Burrows-Wheeler Transform, enabling efficient prefix-constrained search over the corpus. During inference, the LRM generates queries in structured format (subject, question), then traverses valid corpus paths token-by-token using its own probability distribution masked by the CorpusTree. The method employs granularity-aware multi-node expansion (G=6 tokens per node, M=2 nodes expanded per simulation) with stochastic beam search. A value network trained on CT-MCTS rollouts guides the search by predicting answer presence in candidate paths. The system retrieves full documents containing terminal path nodes and generates final answers using standard reasoning prompts.

## Key Results
- 14.4% average improvement in EM and F1 over four multi-step reasoning models with separate retrievers
- Outperforms strongest baseline Search-R1 by 3% on PopQA and 2WikiMultihopQA
- Achieves 87% latency reduction compared to vanilla MCTS-RAG with granularity G=6
- Value network trained on CT-MCTS rollouts outperforms those trained on synthetic LLM-generated data

## Why This Works (Mechanism)

### Mechanism 1: Corpus-Constrained Retrieval as Path-Finding
FREESON transforms retrieval from embedding-based similarity matching to token-level path traversal within a corpus, bypassing the representation bottleneck of dual-encoder retrievers. The framework uses a prefix-based CorpusTree index (implemented via FM-Index with Burrows-Wheeler Transform) to constrain the LRM's token generation to only sequences that exist in the corpus. The model generates queries in structured form (subject, question), then traverses valid corpus paths token-by-token using the LRM's own probability distribution masked by the CorpusTree. The core assumption is that the LRM's reasoning capabilities over document structure can outperform fixed embedding representations for identifying relevant content.

### Mechanism 2: Granularity-Aware Multi-Node Expansion
Using multi-token nodes (G≈6 tokens) with multi-node expansion per simulation (M≈2) improves retrieval quality over single-token, single-node expansion. Rather than expanding one token per MCTS step, FREESON groups tokens into semantically meaningful chunks and expands M=2 candidate children per simulation using stochastic beam search over top-k predictions. This allows the LRM to make more context-aware decisions while maintaining token-level corpus constraints. The core assumption is that LRM outputs during expansion provide meaningful guidance for retrieval direction.

### Mechanism 3: On-Policy Value Network Training
Value networks trained on actual CT-MCTS rollouts outperform those trained on synthetic LLM-generated trajectories for guiding corpus traversal. During CT-MCTS execution, intermediate rollouts are collected and evaluated by feeding the candidate path and question to the model. The model generates an answer, compared to ground-truth with soft values (1.0 full match, 0.8 partial, 0.0 none). A classification head on the frozen backbone is trained with binary cross-entropy. The core assumption is that value estimates aligned with inference-time behavior generalize better than off-policy synthetic data.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS) with UCT Selection**
  - Why needed here: CT-MCTS builds on standard MCTS selection (UCT), expansion, rollout, and backpropagation. Understanding exploration-exploitation tradeoffs (λ parameter) is essential for tuning.
  - Quick check question: Can you explain why UCT balances Q(s,a) and visit counts N(s,a)?

- **Burrows-Wheeler Transform (BWT) and FM-Index**
  - Why needed here: CorpusTree uses FM-Index for efficient prefix-constrained search over compressed corpus representations. Understanding how BWT enables O(1) character occurrence queries matters for debugging index construction.
  - Quick check question: How does BWT enable efficient pattern matching without storing the full corpus?

- **Constrained Decoding in Autoregressive Models**
  - Why needed here: FREESON masks the LRM's vocabulary at each step to only tokens that lead to valid corpus prefixes. This differs from standard beam search or greedy decoding.
  - Quick check question: What happens if the constrained vocabulary becomes empty during generation?

## Architecture Onboarding

- **Component map**: Query Generator -> CorpusTree Index -> CT-MCTS Engine -> Value Network -> Document Selector
- **Critical path**: Query generation → CorpusTree construction (subject-filtered) → CT-MCTS traversal → Value evaluation → Document retrieval → Reasoning continuation
- **Design tradeoffs**:
  - G=6 latency vs. granularity: Higher G reduces latency (27s vs 67s) but may miss fine-grained paths
  - M=2 vs M>2: M=2 optimal; M>2 saturates with no performance gain
  - On-policy vs. off-policy training: On-policy requires CT-MCTS execution; off-policy uses GPT-4o synthetic data but performs worse
- **Failure signatures**:
  - Empty valid token set during expansion → CorpusTree lacks matching prefix
  - High leaf node count with short paths → Granularity too fine (G<3)
  - Value predictions all near 0.5 → Undertrained or misaligned value network
  - Retrieved documents irrelevant despite high value scores → Value network overfitting
- **First 3 experiments**:
  1. Validate CorpusTree on a single-hop QA subset (PopQA 100 samples): Verify FM-Index correctly constrains decoding by logging valid token counts at each step.
  2. Ablate granularity G∈{1,3,6,10}: Reproduce Figure 2 latency/performance tradeoff on 2WikiMultihopQA to confirm G≈6 optimal.
  3. Compare on-policy vs. synthetic value training: Train both variants on 400 PopQA examples and evaluate rollout-to-answer accuracy to validate Table 4 findings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FREESON be extended to handle dynamic, non-predefined corpora or web-scale retrieval scenarios?
- Basis in paper: [explicit] The limitations section states: "In QA tasks where the corpus is not predefined, our method may be less effective compared to web-based retrieval systems... efficiently handling them at scale remains a challenge."
- Why unresolved: The CorpusTree index construction assumes a static, predefined corpus, and the FM-Index approach may not scale to web-scale or dynamically changing document collections.
- What evidence would resolve it: Experiments on web-scale corpora (e.g., MassiveDS-1.4T) with streaming updates, or an incremental index update mechanism.

### Open Question 2
- Question: Can reinforcement learning techniques (e.g., PPO) optimize the LRM's traversal behavior during CT-MCTS for more adaptive retrieval?
- Basis in paper: [explicit] The limitations section notes: "Incorporating reinforcement learning techniques such as PPO to optimize the LRM's traversal over retrieval candidates could enable more adaptive and retrieval-efficient behavior."
- Why unresolved: Current CT-MCTS relies on fixed inference-time heuristics without learned traversal policies that could improve efficiency.
- What evidence would resolve it: Ablation studies comparing current CT-MCTS against PPO-trained traversal policies on latency and retrieval accuracy.

### Open Question 3
- Question: How does optimal node granularity (G) vary across different corpus domains, languages, or document structures?
- Basis in paper: [inferred] The analysis shows G=6 performs best on 2WikiMultihopQA, but this hyperparameter was not tested across diverse corpus types; the paper acknowledges "overly coarse granularity is not always better" without establishing generalizable principles.
- Why unresolved: The granularity study is limited to a single benchmark; optimal token-grouping may depend on linguistic structure, document formatting, or domain-specific vocabulary.
- What evidence would resolve it: Cross-domain experiments (scientific papers, code, multilingual corpora) reporting optimal G values and performance trends.

### Open Question 4
- Question: To what extent do ground-truth annotations in existing QA benchmarks systematically disadvantage retriever-free approaches like FREESON?
- Basis in paper: [explicit] Appendix C documents cases where FREESON produces correct answers (e.g., "fluid" vs. ground-truth "water") that fail evaluation because ground-truths align with E5-retrieved document phrasing.
- Why unresolved: The paper notes this may "inadvertently advantage" embedding-based models, but the scope and impact of this bias remains unquantified.
- What evidence would resolve it: Human evaluation of FREESON outputs on contested examples, or construction of benchmark variants with paraphrase-expanded answer sets.

## Limitations
- Corpus-constrained approach may be less effective for web-scale or dynamically changing document collections
- Performance evaluation limited to Wikipedia-based corpora without testing on domain-specific or noisy text
- FM-Index implementation details and constrained decoding interaction with LRM logits underspecified
- Value network training relies on limited data (400 PopQA examples for 15k rollouts) raising generalization concerns

## Confidence
- **High Confidence**: The core mechanism of corpus-constrained token-level traversal via CorpusTree is well-specified and logically sound. The improvement over baseline retrievers (14.4% average EM/F1 gain) is supported by quantitative results across five benchmarks.
- **Medium Confidence**: The optimality of G=6 granularity and M=2 expansion is claimed but not extensively validated across diverse corpus types. The on-policy value training advantage is demonstrated but the sample efficiency and generalization bounds are unclear.
- **Low Confidence**: The latency measurements (27s vs 67s) and node count reductions (15 vs 30) are presented without ablation on corpus size or query complexity. The interaction between FM-Index masking and LRM logits during beam search is described but not verified experimentally.

## Next Checks
1. **Corpus Generalization Test**: Evaluate FREESON on a non-Wikipedia corpus (e.g., scientific papers or news articles) to verify the CorpusTree's effectiveness on unstructured text and assess performance degradation.
2. **Value Network Robustness**: Perform cross-corpus value network evaluation by training on PopQA and testing on TriviaQA to measure domain transfer and overfitting risks.
3. **Constrained Decoding Stress Test**: Systematically induce empty valid token sets during CT-MCTS execution on edge-case queries to measure failure rates and validate the proposed diagnostic (logging valid token counts).