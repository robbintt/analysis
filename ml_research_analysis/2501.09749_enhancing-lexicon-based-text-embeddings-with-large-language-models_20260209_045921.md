---
ver: rpa2
title: Enhancing Lexicon-Based Text Embeddings with Large Language Models
arxiv_id: '2501.09749'
source_url: https://arxiv.org/abs/2501.09749
tags:
- embeddings
- arxiv
- given
- lens
- lexicon-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LENS, the first lexicon-based text embedding
  framework leveraging large language models (LLMs). LENS addresses two key challenges
  in LLM tokenizers: vocabulary redundancy and unidirectional attention.'
---

# Enhancing Lexicon-Based Text Embeddings with Large Language Models

## Quick Facts
- arXiv ID: 2501.09749
- Source URL: https://arxiv.org/abs/2501.09749
- Authors: Yibin Lei; Tao Shen; Yu Cao; Andrew Yates
- Reference count: 22
- Primary result: Introduces LENS, the first lexicon-based text embedding framework leveraging large language models (LLMs), achieving state-of-the-art zero-shot performance on MTEB among models trained exclusively on public data.

## Executive Summary
This paper introduces LENS, the first lexicon-based text embedding framework leveraging large language models (LLMs). LENS addresses two key challenges in LLM tokenizers: vocabulary redundancy and unidirectional attention. It consolidates the vocabulary space through token embedding clustering and incorporates bidirectional attention with various pooling strategies. Experiments on the Massive Text Embedding Benchmark (MTEB) demonstrate that LENS outperforms dense embeddings and achieves state-of-the-art zero-shot performance among models trained exclusively on public data. Notably, combining LENS with dense embeddings achieves the best retrieval performance on the BEIR subset of MTEB.

## Method Summary
LENS is a lexicon-based text embedding framework that leverages large language models to create efficient, high-performance embeddings. The method addresses two key limitations of LLM tokenizers: vocabulary redundancy and unidirectional attention. LENS consolidates the vocabulary space through K-Means clustering of token embeddings and incorporates bidirectional attention during fine-tuning. The framework uses log-saturation with max-pooling to aggregate token-level signals into fixed-size text representations, achieving state-of-the-art zero-shot performance on the Massive Text Embedding Benchmark while maintaining efficiency through vocabulary compression.

## Key Results
- LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB)
- Achieves state-of-the-art zero-shot performance among models trained exclusively on public data
- Combining LENS with dense embeddings achieves the best retrieval performance on the BEIR subset of MTEB

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Clustering
- **Claim:** Clustering token embeddings consolidates the vocabulary space, reducing tokenizer redundancy and compressing feature dimensionality.
- **Mechanism:** K-Means clustering is applied to LLM output token embeddings, replacing original token embeddings in the LM head with cluster centroids while input embeddings remain unchanged.
- **Core assumption:** Semantic similarity in static token embedding space correlates with functional interchangeability in lexicon-matching tasks.
- **Evidence anchors:** Abstract states "consolidates the vocabulary space through token embedding clustering"; implementation details confirm cluster centroids replace LM head weights.
- **Break condition:** If cluster size $k$ is too small, distinct semantic meanings are forced into the same dimension, causing information loss.

### Mechanism 2: Bidirectional Attention
- **Claim:** Bidirectional attention is critical for lexicon-based embeddings derived from LLMs, improving performance over standard causal masking.
- **Mechanism:** Standard causal attention mask is modified to bidirectional during fine-tuning, allowing tokens to see full document context rather than just preceding tokens.
- **Core assumption:** Model can adapt pre-trained weights to new attention pattern without destabilizing representations learned during autoregressive pre-training.
- **Evidence anchors:** Abstract mentions "investigates bidirectional attention... unlocking the full potential of LLMs"; Table 5 shows bidirectional + max-pooling (69.07 Avg) significantly outperforms unidirectional + last-token pooling (67.73 Avg).
- **Break condition:** If model fails to converge or performance degrades on tasks requiring strict left-to-right causality.

### Mechanism 3: Max-Pooling with Log-Saturation
- **Claim:** Max-pooling with log-saturation over cluster logits effectively aggregates token-level signals into fixed-size text representation.
- **Mechanism:** Logits from modified LM head are transformed with $\log(1 + \text{ReLU}(l_{ij}))$ to dampen large values and enforce non-negativity, followed by $\max_i$ across sequence length.
- **Core assumption:** Maximum activation value of semantic cluster is most salient feature for retrieval and classification tasks.
- **Evidence anchors:** Equations 1 & 2 define log-saturation and max-pooling operations; implementation details confirm this pooling strategy.
- **Break condition:** If multiple distinct "meanings" or nuances are required from same cluster within one document, max-pooling collapses them into single score.

## Foundational Learning

- **Concept:** Lexicon-based vs. Dense Embeddings
  - **Why needed here:** LENS creates "low-dimensional lexicon" embeddings that map to clustered vocabulary ($\approx 4000-8000$) rather than massive vocabulary ($\approx 30k$) or small latent space ($\approx 1024$).
  - **Quick check question:** Does LENS generate a dense vector (dense in non-zeros) or a sparse vector (mostly zeros) relative to its dimension size?

- **Concept:** Language Modeling Head (LM Head)
  - **Why needed here:** Unlike BERT-style models where embeddings are hidden states, LENS uses *logits* of LM head for embeddings.
  - **Quick check question:** In LENS, what specific weights are replaced by K-Means centroids: the input embeddings or the output projection weights?

- **Concept:** Causal vs. Bidirectional Masking
  - **Why needed here:** Paper argues standard LLM architectures are suboptimal for embeddings due to lack of bidirectional context.
  - **Quick check question:** Why does the "last-token pooling" strategy fail for lexicon-based embeddings compared to max-pooling?

## Architecture Onboarding

- **Component map:** Tokenized text + Instructions -> Mistral-7B (Transformer Layers) -> Modified Attention (Bidirectional) -> LM Head (Cluster Centroids) -> Log-saturation -> Max-pooling

- **Critical path:**
  1. **Offline:** Run K-Means on pre-trained token embeddings to generate centroids
  2. **Modification:** Replace LM head weight matrix with centroid matrix
  3. **Fine-tuning:** Enable bidirectional attention and train using InfoNCE loss

- **Design tradeoffs:**
  - **Cluster Size ($k$):** Lower $k$ (e.g., 2,000) improves efficiency and integration with standard vector DBs but risks over-generalization; higher $k$ (e.g., 32,000) retains vocabulary fidelity but loses compression benefit
  - **Architecture Change:** Bidirectional attention yields better results but requires full fine-tuning rather than frozen LLM paradigm

- **Failure signatures:**
  - **Loss of Specificity:** If distinct concepts (e.g., "bank" as river vs. financial) are mapped to identical clusters, $k$ is too small
  - **Training Instability:** If loss diverges early, verify attention mask implementation

- **First 3 experiments:**
  1. **Cluster Ablation:** Train LENS with $k \in \{2000, 4000, 8000, 16000\}$ on subset of data to identify efficiency/effectiveness sweet spot
  2. **Attention Ablation:** Train versions with causal attention (using echo-prompts or last-token pooling) vs. bidirectional attention to validate bidirectional benefit
  3. **Hybrid Fusion:** Concatenate LENS embeddings with standard dense embedding (e.g., BGE-en-ICL) and measure retrieval performance to verify "complementary" hypothesis

## Open Questions the Paper Calls Out
None

## Limitations

- Vocabulary clustering is fundamentally constrained by quality of pre-trained tokenizer and static nature of cluster assignments, creating fixed mapping that cannot adapt to domain-specific nuances during fine-tuning
- Bidirectional attention modification represents significant architectural departure from standard LLM pretraining, raising questions about generalization to tasks requiring strict causal reasoning
- Clustering approach assumes semantic similarity in embedding space translates to functional interchangeability in retrieval tasks, but this assumption lacks theoretical grounding

## Confidence

- **High Confidence:** Experimental results showing LENS outperforming dense embeddings on MTEB and achieving competitive zero-shot performance are well-documented and reproducible
- **Medium Confidence:** Bidirectional attention improvement is supported by ablation studies, but mechanism is not fully explained; claim that LENS embeddings are "complementary" to dense embeddings requires more rigorous theoretical justification
- **Low Confidence:** Assertions about vocabulary consolidation effectiveness and optimal cluster size ($k=4000$) are based on specific experimental conditions that may not generalize across different domains or LLM architectures

## Next Checks

1. **Cluster Stability Analysis:** Perform K-Means clustering multiple times with different initializations and analyze stability of cluster assignments for semantically similar tokens; quantify how often semantically distinct tokens (e.g., "bank" as financial institution vs. river bank) end up in same cluster, and measure impact on downstream retrieval performance when these tokens are critical to query-document relevance.

2. **Attention Pattern Sensitivity:** Systematically vary degree of attention relaxation in bidirectional modification (e.g., using diagonal masks with different bandwidths) and measure performance degradation; quantify how much bidirectional benefit depends on full context visibility versus partial relaxation, providing insights into mechanism and potential for more efficient implementations.

3. **Cross-Domain Generalization Test:** Evaluate LENS embeddings trained on general web data on specialized domains (medical literature, legal documents, code repositories) without fine-tuning; measure performance degradation and analyze whether certain token clusters consistently fail to capture domain-specific semantic distinctions, identifying whether vocabulary clustering approach has fundamental limitations for specialized applications.