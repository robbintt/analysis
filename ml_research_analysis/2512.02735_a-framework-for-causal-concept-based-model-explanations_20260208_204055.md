---
ver: rpa2
title: A Framework for Causal Concept-based Model Explanations
arxiv_id: '2512.02735'
source_url: https://arxiv.org/abs/2512.02735
tags:
- causal
- explanation
- explanations
- concept
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a causal concept-based post-hoc XAI framework
  for explaining black-box models through counterfactual reasoning about concept interventions.
  The framework consists of three components: the black-box model to be explained,
  a concept-to-data mapping function, and a causal model over the concepts.'
---

# A Framework for Causal Concept-based Model Explanations

## Quick Facts
- arXiv ID: 2512.02735
- Source URL: https://arxiv.org/abs/2512.02735
- Reference count: 10
- Primary result: Causal concept-based framework generates meaningful explanations for black-box image classifiers through counterfactual reasoning about concept interventions

## Executive Summary
This paper presents a causal concept-based post-hoc XAI framework for explaining black-box models through counterfactual reasoning about concept interventions. The framework consists of three components: the black-box model to be explained, a concept-to-data mapping function, and a causal model over the concepts. Explanations are generated by calculating the probability of sufficiency of concept interventions using counterfactual queries, enabling both local and global explanations. The method is demonstrated on classifiers trained on the CelebA dataset, explaining age and attractiveness predictions. Results show that the causal framework provides meaningful explanations compared to non-causal approaches like Grad-CAM.

## Method Summary
The framework chains three components: a causal model M over human-interpretable concepts z with exogenous variables u; an invertible mapping α: (z, w) → x that generates data features from concepts plus unknown factors w; and the black-box model h. When a concept z̄ is intervened upon via do(z̄ = z̄′), the change propagates through M to affected descendants z̄̄, through α to generate a counterfactual input x′, and through h to produce ŷ′. The probability of sufficiency P(ŷ_{do(z̄=z̄′)} = ŷ′ | x, ŷ) quantifies the explanation. The method is demonstrated on ResNet-50 classifiers trained on CelebA for age and attractiveness prediction, using StarGAN for concept interventions and FSCM/PSCM for counterfactual inference.

## Key Results
- The causal framework provides meaningful explanations compared to non-causal approaches like Grad-CAM
- When explaining young classification, hair color was identified as a sufficient cause for changing predictions from young to old, with probabilities ranging from 0.121 to 0.972 depending on model representation
- The framework revealed subgroup differences, showing the effect of gray hair on age classification was stronger among males (0.580) than females (0.181)

## Why This Works (Mechanism)

### Mechanism 1: Concept-Mediated Counterfactual Intervention
- **Claim:** Explanations are generated by computing the probability that intervening on a concept would change the model's prediction.
- **Mechanism:** The framework chains three components: (1) a causal model M over human-interpretable concepts z with exogenous variables u; (2) an invertible mapping α: (z, w) → x that generates data features from concepts plus unknown factors w; (3) the black-box model h. When a concept z̄ is intervened upon via do(z̄ = z̄′), the change propagates through M to affected descendants z̄̄, through α to generate a counterfactual input x′, and through h to produce ŷ′. The probability of sufficiency P(ŷ_{do(z̄=z̄′)} = ŷ′ | x, ŷ) quantifies the explanation.
- **Core assumption:** The explanation vocabulary z is sufficiently comprehensive to expose h's decision logic, and z ⊥⊥ w holds so interventions on z correctly model change in x.
- **Evidence anchors:**
  - [abstract] "Explanations are generated by calculating the probability of sufficiency of concept interventions using counterfactual queries."
  - [Section 4.1] "The framework includes the following sets of variables: z denotes the explanation concept vocabulary, u denotes the exogenous variables associated with z by M, x denotes the data features... w denotes any noise or unknown factors of variation in x such that x = α(z, w)."
  - [corpus] LIBERTy (arXiv:2601.10700) similarly benchmarks concept-based explanations against reference causal effects from counterfactuals, confirming the evaluation paradigm.
- **Break condition:** If z omits concepts that causally influence ŷ through h, explanations will attribute effects to available concepts incorrectly (omitted variable bias). If z ⊥⊥ w is violated (e.g., StarGAN entangles z and w), interventions may not isolate the intended concept.

### Mechanism 2: Structural Causal Model Enables Counterfactual Inference
- **Claim:** A fully specified SCM (FSCM) with structural functions F and exogenous distribution P(u) enables exact counterfactual calculation; partial specification yields bounds.
- **Mechanism:** An FSCM ⟨u, z, F, P(u)⟩ allows computing counterfactual queries by: (1) abduction—inferring u from observed (x, ŷ) via α⁻¹; (2) intervention—setting z̄ = z̄′ via do(); (3) prediction—propagating through F and α and h. When P(u) is unknown (PSCM), the query is bounded over all FSCMs consistent with observed data. Table 1 shows FSCM gives p = 0.121 for do(Young=0), while canonical PSCM bounds it to [0.121, 0.138].
- **Core assumption:** Domain knowledge informs structural functions F; without it, counterfactual intervals can span [0, 1] for rare observations.
- **Evidence anchors:**
  - [Section 2.2] "An SCM where all components ⟨u, v, F, P(u)⟩ are defined, is referred to as a Fully specified SCM (FSCM). If the probability distribution P(u) is unknown, the SCM is referred to as a Partially specified SCM (PSCM)."
  - [Section 5.2, Table 1] Demonstrates FSCM, PSCM interval, and independence model yield different probabilities for the same counterfactual query.
  - [corpus] Causal Distillation (arXiv:2505.19511) transfers causal reasoning skills via structured explanations, reinforcing that SCM quality determines explanation fidelity.
- **Break condition:** If structural functions are misspecified, counterfactual predictions will be wrong. The paper notes: "Two different FSCMs that are both consistent with the data may therefore generate opposite explanations."

### Mechanism 3: Total Causal Effect via Mediator Propagation
- **Claim:** Intervening on a parent concept propagates through mediators, capturing total causal effect rather than just direct effect.
- **Mechanism:** For do(Young = 0) in Figure 3's graph, descendants Gray Hair and Glasses are resampled according to P(Gray Hair, Glasses | do(Young=0), u). Equation (1) marginalizes over u and descendant values, computing Σ_{z̄̄*} P(z̄̄*_{do(z̄=z̄′)} | z̄, z̄̄, z) × I[h ∘ α(z, z̄′, z̄̄*, w) = ŷ]. This captures both direct effects (Young → x) and mediated effects (Young → Gray Hair → x).
- **Core assumption:** The causal graph structure G correctly represents the data-generating process's abstraction level.
- **Evidence anchors:**
  - [Section 4.2.1, Equation 1] Shows the full marginalization over descendant variables in counterfactual computation.
  - [Section 5.3.2] "If the explanation model ignores the causal structure by intervening on variables as if independent, only the direct effect is captured... direct influence is not a consistent measure."
  - [corpus] Weak corpus signal on mediator propagation specifically; related work focuses on concept attribution rather than causal path decomposition.
- **Break condition:** If mediators are omitted from z but influence h, total effect is misattributed. Example: If Young → Wrinkles → ŷ but Wrinkles ∉ z, the effect may be incorrectly attributed to remaining concepts.

## Foundational Learning

- **Structural Causal Models (SCMs)**
  - **Why needed here:** The framework's entire explanatory power depends on SCM formalism—distinguishing observational, interventional, and counterfactual queries; understanding exogenous vs. endogenous variables; and reading causal graphs.
  - **Quick check question:** Given a graph A → B → C with binary variables, what is the difference between P(C | do(B=1)) and P(C | B=1)?

- **Probability of Sufficiency and Necessity**
  - **Why needed here:** Explanations are quantified via PS: P(ŷ_{do(z̄=z̄′)} = ŷ′ | ŷ, z̄) measures sufficiency of the intervention for the outcome change. Understanding the asymmetry between sufficiency and necessity is critical for interpreting results.
  - **Quick check question:** If PS for "Gray Hair = 1 causes Old classification" is 0.58 for males and 0.18 for females, what does this imply about the classifier's reliance on gray hair across subgroups?

- **Concept-Based XAI and Generative Models for Counterfactuals**
  - **Why needed here:** The mapping α requires a generative model (StarGAN in the paper) to produce counterfactual images from concept interventions. Understanding the gap between concept vectors and generated images is essential for debugging explanations.
  - **Quick check question:** A generator trained to edit "Young" also adds gray hair to images. Is this a bug or a feature when the causal graph includes Young → Gray Hair?

## Architecture Onboarding

- **Component map:** Black-box model h(x) → ŷ -> Concept vocabulary z -> Causal model M -> Mapping α -> Counterfactual x' -> h(x') -> PS calculation
- **Critical path:**
  1. Select concept vocabulary z (correlation analysis + domain knowledge)
  2. Learn/define causal graph G over z (PC algorithm + LLM/expert for edge direction)
  3. Design structural functions F and P(u) for each node (domain knowledge + data consistency)
  4. Train generative model α on (x, z) pairs to enable concept interventions
  5. For each explanation query: (a) extract (z, w) from x; (b) define intervention do(z̄ = z̄′); (c) propagate through M; (d) generate counterfactual x′; (e) evaluate h(x′); (f) marginalize over u and descendants to compute PS

- **Design tradeoffs:**
  - **FSCM vs PSCM vs CBN**: FSCM gives exact counterfactuals but requires domain knowledge for F and P(u); PSCM gives intervals with minimal assumptions; CBN only supports interventional queries (no counterfactuals). Table 1 shows CBN interventional query (0.137) can be far from FSCM counterfactual (0.121)
  - **Vocabulary comprehensiveness vs tractability**: More concepts in z improve fidelity but exponentially increase counterfactual search space (2^|z| for binary concepts)
  - **Generator quality vs isolation**: StarGAN may entangle concepts (e.g., adding gray hair when editing "Young"). Training with full vocabulary vector helps disentangle but requires more data
  - **Independence assumption (z ⊥⊥ w)**: Simplifies intervention modeling; violated if generator leaks z-information into w. Paper notes this is "validated to the extent of inspection of generated samples"

- **Failure signatures:**
  - **Wide PSCM intervals [0, 1]**: Indicates insufficient domain knowledge or rare observations; counterfactuals are uninformative
  - **Contradictory FSCM vs CBN results**: Suggests mediators are important; CBN interventional query resamples descendants, ignoring observed values
  - **Generator artifacts flip predictions**: If counterfactual image quality differs systematically (e.g., gray hair images are blurrier), PS may reflect artifact sensitivity rather than concept sensitivity. Footnote 2 acknowledges this risk
  - **Subgroup divergence (e.g., Gray Hair PS: 0.58 male vs 0.18 female)**: May indicate model bias or vocabulary incompleteness (mediators differ by subgroup)

- **First 3 experiments:**
  1. **Validate α isolation**: For each concept z_i, generate counterfactuals with z_i flipped while holding others constant. Compute classifier accuracy on generated images for all concepts—high accuracy indicates concepts are isolated. Flag concepts where generator entangles (e.g., "Young" edit changes "Gray Hair" probability when Gray Hair is held fixed)
  2. **Sanity check causal graph**: Compute interventional queries P(ŷ | do(z_i)) via both (a) CBN interventional estimation and (b) FSCM counterfactual averaging over a background dataset. Large discrepancies suggest graph structure or structural functions are misspecified
  3. **Subgroup PS comparison**: Replicate Table 3 for additional concepts and intersectional subgroups. If PS values vary dramatically across subgroups for the same intervention, investigate whether (a) the classifier learned subgroup-specific features, or (b) the causal model's structural functions are subgroup-inappropriate (e.g., P(Gray Hair | Young, Gender) differs from population priors)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can automated concept detection ensure the explanation vocabulary contains the most relevant concepts?
- **Basis in paper:** [explicit] The authors suggest using techniques like sparse autoencoders to suggest relevant concepts without relying on preconceptions.
- **Why unresolved:** Current selection relies on expert knowledge or correlation with the target, potentially missing relevant model behaviours.
- **What evidence would resolve it:** An implementation that dynamically selects $z$ based on model activations and improves explanation fidelity.

### Open Question 2
- **Question:** Can concept attributions serve as an efficient heuristic for searching optimal contrastive explanations in high-dimensional spaces?
- **Basis in paper:** [explicit] The paper notes that counterfactuals grow exponentially and proposes attribution methods to order interventions.
- **Why unresolved:** The proof-of-concept uses small vocabularies where exhaustive search is feasible, avoiding this combinatorial scaling issue.
- **What evidence would resolve it:** Experiments comparing the speed and accuracy of attribution-guided search against exhaustive methods.

### Open Question 3
- **Question:** How can the mapping function $\alpha$ be designed to enforce the independence assumption between concepts $z$ and unknown factors $w$?
- **Basis in paper:** [inferred] The implementation sets $w=x$, explicitly violating the framework's assumption $w \perp z$ required for valid interventions.
- **Why unresolved:** Standard generators like StarGAN may entangle concepts with unknown factors, threatening explanation fidelity.
- **What evidence would resolve it:** A generative architecture that rigorously disentangles $z$ and $w$ while maintaining high image quality.

## Limitations
- The framework's effectiveness critically depends on domain knowledge quality for causal structure and structural functions
- The independence assumption z ⊥⊥ w may not hold for real generative models, potentially contaminating counterfactuals
- The substantial implementation complexity compared to feature-based methods may limit practical adoption

## Confidence
- Causal mechanism claims (PS calculation, mediator propagation): High
- Implementation details (specific structural functions, StarGAN training): Medium
- Generalizability to other domains: Low

## Next Checks
1. Test framework on a domain with known ground truth causal structure (e.g., synthetic data with verified causal relationships) to validate PS calculations.
2. Compare explanations across multiple generative models with varying concept isolation quality to quantify impact of z ⊥⊥ w assumption violations.
3. Apply framework to a non-image domain (e.g., tabular medical data) to assess cross-domain applicability and identify domain-specific implementation challenges.