---
ver: rpa2
title: "Gaussian entropic optimal transport: Schr\xF6dinger bridges and the Sinkhorn\
  \ algorithm"
arxiv_id: '2412.18432'
source_url: https://arxiv.org/abs/2412.18432
tags:
- gaussian
- have
- sinkhorn
- schr
- dinger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a self-contained analysis of the Sinkhorn\
  \ algorithm and Schr\xF6dinger bridges for general Gaussian models. The key contributions\
  \ include: 1) A finite-dimensional description of Sinkhorn iterations in terms of\
  \ Riccati matrix difference equations, including closed-form solutions for the limiting\
  \ Schr\xF6dinger bridges."
---

# Gaussian entropic optimal transport: Schrödinger bridges and the Sinkhorn algorithm

## Quick Facts
- **arXiv ID:** 2412.18432
- **Source URL:** https://arxiv.org/abs/2412.18432
- **Reference count:** 40
- **Primary result:** Finite-dimensional analysis of Gaussian Sinkhorn algorithm via Riccati equations with explicit convergence rates

## Executive Summary
This paper provides a self-contained analysis of the Sinkhorn algorithm and Schrödinger bridges for general Gaussian models. The authors present a finite-dimensional recursive formulation of Sinkhorn iterations in terms of Riccati matrix difference equations, demonstrating that these iterations coincide with the Kalman filter updates in the Gaussian setting. The work establishes sharp exponential convergence rates for the algorithm and provides complete characterizations of the distribution flow and associated Schrödinger potentials in terms of Riccati matrix equations.

## Method Summary
The paper develops a finite-dimensional description of Sinkhorn iterations for Gaussian models using Riccati matrix difference equations. The method transforms the traditionally intractable integral Sinkhorn updates into tractable linear algebra operations by exploiting Gaussian conjugacy. The algorithm computes optimal Schrödinger bridges through recursive updates of covariance matrices following Riccati equations, with convergence rates determined by the spectral properties of the fixed point. The approach provides closed-form solutions for limiting bridges and characterizes the stability of the associated Markov chains.

## Key Results
- Gaussian Sinkhorn iterations are equivalent to Kalman filter updates in linear-Gaussian settings
- Convergence rates are explicitly determined by spectral properties of Riccati fixed points
- Complete characterization of distribution flow and Schrödinger potentials via Riccati equations
- Exponential convergence with contraction coefficients derived from minimal eigenvalues

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gaussian conjugacy transforms intractable integral Sinkhorn updates into tractable linear algebra.
- **Mechanism:** In general settings, Sinkhorn iterations require computing nonlinear conditional distributions (integrals) which lack closed forms. For Gaussian models, conditioning preserves Gaussianity. The paper demonstrates that the Sinkhorn conditional distributions correspond to linear regression updates, equivalent to the measurement update step in a Kalman filter.
- **Core assumption:** The source and target distributions are multivariate Gaussian, and the reference kernel is linear-Gaussian.
- **Evidence anchors:**
  - [abstract] "This article presents a finite-dimensional recursive formulation... closely related to the celebrated Kalman filter."
  - [Section 1.2] "The conditional mean and covariance updates... coincide with the traditional Kalman update."
  - [Section 2.4] "P(X \in dx | Y = y) is Gaussian... equivalent to Bayes maps."
- **Break condition:** If distributions become non-Gaussian (e.g., multi-modal), the conjugacy breaks, and the finite-dimensional recursion fails to hold.

### Mechanism 2
- **Claim:** Sinkhorn convergence is governed by the stability of Riccati matrix difference equations.
- **Mechanism:** The covariance matrices $\tau_n$ generated during the iteration follow a specific matrix recurrence: $\upsilon_{n+1} = (I + (\varpi + \upsilon_n)^{-1})^{-1}$. This is a discrete Riccati equation. The algorithm converges because this Riccati flow possesses a unique positive definite fixed point, which acts as the attractor for the covariance sequence.
- **Core assumption:** The matrices involved (initial covariances and kernel parameters) are positive definite, ensuring the Riccati map is well-defined.
- **Evidence anchors:**
  - [Section 4.2] "The flow of covariance matrices $\tau_n$ can be computed offline by solving a time-homogeneous Riccati equation."
  - [Theorem 4.3] "Riccati difference equations... $\upsilon_{2(n+1)} = Ricc_{\varpi_\theta}(\upsilon_{2n})$."
- **Break condition:** If the regularization parameter $t$ is such that matrices become singular or ill-conditioned, the numerical stability of the Riccati inversion may degrade, though the paper provides bounds for this (Proposition 5.9).

### Mechanism 3
- **Claim:** Convergence rates are explicitly determined by the spectral properties of the Riccati fixed point.
- **Mechanism:** The paper utilizes Floquet-type representations of Riccati flows to prove that the convergence of the iterates $\theta_n$ to the optimal Schrödinger bridge is exponential. The rate parameter $\rho_\theta$ is derived directly from the minimal eigenvalues of the fixed-point matrices of the Riccati map.
- **Core assumption:** Assumption that the eigenvalue structure of the fixed point remains bounded away from zero/singularity in the interior of the parameter space.
- **Evidence anchors:**
  - [Section 5.1] "We prove that $\theta_{2n} \to S(\theta)$ exponentially fast, with contraction coefficients that are obtained explicitly from the Riccati equations."
  - [Corollary 5.6] "$|\theta_{2n} - S(\theta)| \leq c \rho^n...$"
  - [Corpus] "New Trends in the Stability of Sinkhorn Semigroups" supports general stability analysis.
- **Break condition:** The convergence rate slows ($\rho_\theta \to 1$) as the regularization parameter $t \to 0$ (approaching unregularized OT) or as covariances become degenerate (Section 7, Fig 5).

## Foundational Learning

### Concept: Schrödinger Bridges / Entropic Optimal Transport
- **Why needed here:** This is the problem the paper solves. You must understand that we are seeking a coupling (transport map) between two distributions that minimizes entropy relative to a reference process.
- **Quick check question:** Can you explain the difference between Monge maps (standard OT) and Schrödinger bridges (entropic OT) regarding their handling of randomness?

### Concept: Kalman Filtering (Update Step)
- **Why needed here:** The paper proves that Sinkhorn iterations *are* Kalman updates in the Gaussian case. Understanding the Kalman gain and how it blends prediction with observation is necessary to grasp the algebra.
- **Quick check question:** How does the Kalman gain change if the measurement noise covariance approaches zero?

### Concept: Discrete Algebraic Riccati Equation (DARE)
- **Why needed here:** The convergence of the algorithm is analyzed entirely through the lens of Riccati stability. The "fixed point" of the algorithm is the solution to this matrix equation.
- **Quick check question:** Why does a Riccati equation typically arise in Linear Quadratic Gaussian (LQG) control or Kalman filtering contexts?

## Architecture Onboarding

### Component map:
- **Input:** Marginals $(\nu_{m,\sigma}, \nu_{\bar{m},\bar{\sigma}})$ and Reference $\theta_0=(\alpha, \beta, \tau)$
- **Core Loop (Algorithm 1):**
  1. **Riccati Update:** Compute covariance $\tau_n$ via $\upsilon_n = (I + \gamma \upsilon_{n-1} \gamma^T)^{-1}$
  2. **Gain Update:** Update matrix $\beta_n$ using $\tau_n$ and reference $\beta$
  3. **Mean Update:** $m_n = m + \beta_n(m - m_{n-1})$
- **Output:** Bridge parameters $(\iota_\theta, \kappa_\theta, \varsigma_\theta)$ and potentials $(U, V)$

### Critical path:
The Riccati recursion (Step 1 in loop). This is the engine of convergence. If the matrix inversions here are unstable, the bridge fails.

### Design tradeoffs:
- **Exactness vs. Applicability:** The method provides exact closed-form solutions without sampling error but is strictly limited to Gaussian marginals.
- **Complexity:** The algorithm avoids Monte Carlo but requires $O(d^3)$ matrix operations (inversion/square root) per step, which becomes costly in very high dimensions (Section 8.3).

### Failure signatures:
- **Slow Convergence:** If regularization $t$ is very small (approaching unregularized OT), the contraction rate $\rho_\theta$ approaches 1 (Fig 4).
- **Numerical Singularity:** If $\tau$ is ill-conditioned relative to $\sigma$, the matrix inversions in the Riccati update may fail.

### First 3 experiments:
1. **Sanity Check (2D):** Replicate Figure 1. Initialize a 2D Gaussian bridge and visually confirm that the contours of $\nu_{m_n,\sigma_n}$ match the target $\nu_{\bar{m},\bar{\sigma}}$ in ~10 iterations.
2. **Rate Validation:** Plot $|\tau_{2n} - \varsigma_\theta|$ vs $n$ on a log scale. Verify the slope matches the theoretical rate $\rho_\theta$ (Corollary 5.6).
3. **Regularization Sweep:** Vary $t$ (the regularization parameter) and plot the resulting contraction coefficient $\rho_\theta$. Confirm that $\rho_\theta \to 0$ as $t \to \infty$ and $\rho_\theta \to 1$ as $t \to 0$.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the quantitative effect of the bias introduced in the entropic projection method when the class of Markov transitions is restricted?
- **Basis in paper:** [explicit] Section 8.4 states, "Future work will include the effect of the bias in the entropic projection method of (8.5)."
- **Why unresolved:** The authors note that when the transition class $K_\theta$ is too small, the projection introduces a non-zero entropic bias, but this bias is not quantified.
- **Evidence:** Theoretical bounds on the bias magnitude or an analysis of convergence properties for non-convex parameter sets $\Theta$.

### Open Question 2
- **Question:** How does the computational complexity and accuracy of numerical approximations for the exact Gaussian Sinkhorn iterations compare to machine learning approaches (e.g., neural networks)?
- **Basis in paper:** [explicit] Section 8.3 notes that exact computation requires $O(d^3)$ matrix operations and suggests, "It may be of interest to compare the accuracy and complexity of numerical approximations... with machine learning approximations..."
- **Why unresolved:** While exact formulae are provided, they are computationally infeasible for very high dimensions, and the trade-offs of using approximations like power-series or randomized algorithms versus learning-based methods have not been established.
- **Evidence:** A comparative study benchmarking the convergence rate and error of numerical solvers against score-based or neural network approximations on high-dimensional graphical models.

### Open Question 3
- **Question:** Can the linear-Gaussian transport map be modified to preserve the full distribution shape of non-Gaussian targets, such as multimodal mixtures, rather than just their mean and covariance?
- **Basis in paper:** [inferred] Section 7 demonstrates approximate transport for Gaussian mixtures but acknowledges that the linear-Gaussian map yields a "mollified version" of the target, matching only the mean and covariance (Figure 6).
- **Why unresolved:** The paper provides exact solutions for Gaussian models but treats non-Gaussian distributions as approximations where the bridge only guarantees the preservation of first and second-order statistics.
- **Evidence:** Derivation of a modified Sinkhorn recursion or a non-linear extension that guarantees convergence to the specific multimodal structure of the target distribution.

## Limitations
- Analysis strictly confined to Gaussian models where conjugacy enables exact finite-dimensional recursions
- $O(d^3)$ matrix operations per step become prohibitive in very high dimensions
- Convergence rates degrade as regularization parameter $t \to 0$, approaching ill-posed unregularized OT limit

## Confidence
- **High confidence:** The connection between Sinkhorn iterations and Kalman filtering (Mechanism 1), the derivation of Riccati matrix recursions for covariance updates (Mechanism 2), and the characterization of the fixed point as a DARE solution
- **Medium confidence:** The explicit Floquet-type convergence rate formulas, which depend on spectral properties that may be sensitive to edge cases in matrix conditioning
- **Medium confidence:** The bounds on numerical stability under ill-conditioning (Proposition 5.9), as they rely on worst-case eigenvalue estimates

## Next Checks
1. **Rate Validation:** Plot $|\tau_{2n} - \varsigma_\theta|$ vs $n$ on a log scale for varying dimensions and initializations. Verify the empirical slope matches the theoretical rate $\rho_\theta$ across a representative sample of problem instances.
2. **Conditioning Sensitivity:** Systematically vary the ratio $\tau/\sigma$ and measure the resulting contraction coefficient $\rho_\theta$ and numerical error in the Riccati updates. Confirm the theoretical bounds from Proposition 5.9.
3. **Dimensional Scaling:** Benchmark the algorithm's runtime and convergence on Gaussian OT problems with dimensions ranging from $d=10$ to $d=1000$. Quantify the $O(d^3)$ complexity and identify the practical dimension limit where sampling-based methods become preferable.