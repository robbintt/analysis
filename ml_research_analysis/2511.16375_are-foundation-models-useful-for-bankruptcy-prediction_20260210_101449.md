---
ver: rpa2
title: Are Foundation Models Useful for Bankruptcy Prediction?
arxiv_id: '2511.16375'
source_url: https://arxiv.org/abs/2511.16375
tags:
- assets
- total
- prediction
- financial
- liabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates foundation models for corporate\
  \ bankruptcy prediction, comparing Llama-3.3-70B-Instruct and TabPFN against classical\
  \ ML methods on a large dataset of over one million company records from the Visegr\xE1\
  d Group. Foundation models consistently underperform established approaches like\
  \ XGBoost and CatBoost across all prediction horizons."
---

# Are Foundation Models Useful for Bankruptcy Prediction?

## Quick Facts
- arXiv ID: 2511.16375
- Source URL: https://arxiv.org/abs/2511.16375
- Reference count: 39
- Foundation models underperform classical ML methods for bankruptcy prediction across all metrics

## Executive Summary
This study systematically evaluates whether foundation models can effectively predict corporate bankruptcy using a large dataset of over one million company records from the Visegrád Group. The research compares Llama-3.3-70B-Instruct and TabPFN against established classical machine learning methods including XGBoost and CatBoost. Across all tested prediction horizons (1-4 years), foundation models consistently underperformed classical approaches in terms of accuracy, probability calibration, and computational efficiency. The findings suggest that specialized machine learning methods remain superior for structured financial prediction tasks, with foundation models showing particular weaknesses in providing reliable probability estimates and requiring excessive computational resources.

## Method Summary
The study evaluates foundation models on a comprehensive bankruptcy prediction task using over one million company records from Czech Republic, Hungary, Poland, and Slovakia spanning 2004-2021. The researchers test prediction horizons from one to four years ahead, comparing Llama-3.3-70B-Instruct and TabPFN against classical ML methods like XGBoost and CatBoost. Performance is measured across multiple metrics including F1-score, ROC-AUC, and probability calibration. The methodology includes proper train-test splits and systematic comparison across different time horizons to assess model robustness and generalization capabilities.

## Key Results
- Foundation models consistently underperform classical ML methods across all prediction horizons
- LLM-based approaches produce unreliable probability estimates that cluster around discrete values
- TabPFN shows competitive F1-scores but requires substantial computational resources and underperforms on ROC-AUC metrics
- Classical methods maintain ROC-AUC scores above 0.85 even at four-year prediction horizons

## Why This Works (Mechanism)
Foundation models struggle with bankruptcy prediction because they are designed for general-purpose language understanding and pattern recognition in unstructured data, not for the structured numerical relationships inherent in financial indicators. The task requires precise calibration of probability estimates and handling of tabular features with specific statistical properties that foundation models are not optimized for. LLMs particularly fail because their instruction-tuning produces outputs that cluster around discrete values rather than providing the continuous, calibrated probabilities needed for reliable risk assessment in financial decision-making.

## Foundational Learning

**Tabular Data Processing** - Understanding how foundation models handle structured numerical features is critical because bankruptcy prediction relies on specific financial ratios and metrics. Quick check: Compare feature importance distributions between foundation models and classical methods.

**Probability Calibration** - Essential for financial risk assessment where accurate probability estimates determine decision thresholds. Quick check: Evaluate calibration curves and Brier scores across all models.

**Time Series Forecasting** - Necessary for multi-year prediction horizons common in bankruptcy forecasting. Quick check: Assess temporal generalization by comparing performance across different prediction windows.

**Computational Resource Profiling** - Important for understanding practical deployment feasibility. Quick check: Measure GPU memory usage, training time, and inference latency for each model type.

## Architecture Onboarding

**Component Map:** Financial Data -> Feature Engineering -> Model Training -> Prediction Horizon Testing -> Performance Evaluation -> Resource Benchmarking

**Critical Path:** Raw financial data flows through preprocessing to model training, then through multi-horizon evaluation to final performance comparison.

**Design Tradeoffs:** Foundation models offer potential flexibility but sacrifice accuracy and efficiency; classical methods prioritize task-specific optimization over general-purpose capabilities.

**Failure Signatures:** LLM probability clustering indicates poor calibration; TabPFN computational demands suggest scalability issues; ROC-AUC drops indicate generalization problems.

**First Experiments:**
1. Replicate core evaluation on a smaller subset to verify methodology
2. Test single prediction horizon to isolate temporal effects
3. Compare feature importance rankings between model families

## Open Questions the Paper Calls Out

None

## Limitations
- Geographic scope limited to Visegrád Group countries, reducing generalizability
- Dataset covers 2004-2021, potentially missing recent economic disruptions
- Incomplete computational resource profiling for TabPFN implementation

## Confidence
- Foundation models underperform classical ML: High
- Limited generalizability to other markets: Medium
- LLM probability estimate reliability: High
- TabPFN computational efficiency characterization: Medium

## Next Checks
1. Geographic Expansion: Replicate evaluation using bankruptcy datasets from North America and Asia-Pacific to test cross-market generalizability
2. Temporal Robustness: Conduct rolling window validation using 2021-2024 data to assess performance during recent economic disruptions
3. Resource Profiling: Perform detailed benchmarking of TabPFN including GPU memory requirements, training time scaling, and inference latency under production loads