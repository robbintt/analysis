---
ver: rpa2
title: Adaptive Thresholding for Multi-Label Classification via Global-Local Signal
  Fusion
arxiv_id: '2505.03118'
source_url: https://arxiv.org/abs/2505.03118
tags:
- label
- adaptive
- thresholding
- local
- threshold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an adaptive thresholding method for multi-label
  classification that fuses global label rarity (IDF) and local label agreement (KNN)
  to dynamically adjust decision boundaries. Instead of using fixed thresholds, it
  learns per-label, per-instance thresholds as differentiable penalties in the loss,
  improving calibration and reducing false positives.
---

# Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion

## Quick Facts
- arXiv ID: 2505.03118
- Source URL: https://arxiv.org/abs/2505.03118
- Reference count: 6
- Primary result: Achieves 0.1712 macro-F1 on AmazonCat-13K, significantly outperforming static thresholding

## Executive Summary
This paper introduces an adaptive thresholding method for multi-label classification that dynamically adjusts decision boundaries per label and per instance. Instead of using fixed thresholds, it fuses global label rarity (IDF) with local label agreement (KNN) to learn differentiable penalties in the loss function. The method improves calibration and reduces false positives while remaining lightweight and interpretable. Evaluated on AmazonCat-13K with 13,330 labels, it achieves 0.1712 macro-F1, significantly outperforming both traditional static thresholding and prior tree-based or transformer-based methods.

## Method Summary
The method learns per-label, per-instance thresholds as differentiable penalties in the loss function, replacing fixed cutoffs with adaptive boundaries. It combines global signal (IDF-based rarity) and local signal (KNN-based label agreement within batches) through a weighted fusion layer. The threshold θ_l(x) = λ·α_l·IDF_l + (1-λ)·β_l·KNN_l(x) + b_l is subtracted from logits before applying Binary Cross Entropy: BCE(z - θ, y). This penalization mechanism discourages false positives while allowing confident predictions. The KNN signal is computed via batch-level label co-occurrence matrices (Y·Y^T), providing a cost-effective proxy for semantic similarity without external embeddings.

## Key Results
- Achieves 0.1712 macro-F1 on AmazonCat-13K (vs 0.0035 for static threshold 0.5)
- Outperforms IDF-only model (0.0094 macro-F1) and KNN-only model
- Requires 1500 epochs for full convergence (IDF-only stops at 150 epochs)
- Maintains lightweight architecture (~2.8M parameters) compared to transformer-based methods

## Why This Works (Mechanism)

### Mechanism 1
Fusing global rarity (IDF) with local label agreement (KNN) creates superior per-instance decision boundaries compared to static or purely global heuristics. The model learns a threshold as a weighted sum: λ·IDF + (1-λ)·KNN, adjusting the barrier for prediction based on how rare a label is globally and how prevalent it is among similar neighbors. Core assumption: label co-occurrence statistics within a batch serve as a reliable proxy for "local context" and semantic similarity. Break condition: if the label distribution in a batch is not representative of local semantic clusters, the KNN signal degrades into noise.

### Mechanism 2
Treating thresholds as differentiable penalties in the loss function improves gradient flow and model calibration. The method subtracts the adaptive threshold θ from the logit z before applying Binary Cross Entropy: BCE(z - θ, y). This penalizes the model for barely crossing a "soft" boundary, effectively requiring higher confidence for uncertain or rare labels. Core assumption: the model can learn to optimize this shifted objective without collapsing predictions to zero. Break condition: if the threshold θ scales too aggressively, gradients may vanish as logits consistently fall far below the threshold.

### Mechanism 3
Label-based similarity matrices (KNN via Y·Y^T) provide a cost-effective, differentiable proxy for semantic neighborhood retrieval without relying on external embeddings. The architecture computes a B×B matrix counting shared labels between samples, normalizes it, and propagates it back to weight the label signals. Core assumption: high label overlap between two samples implies they should share threshold characteristics. Break condition: this mechanism assumes access to ground-truth labels for KNN calculation, requiring fallback mechanisms for pure inference on unlabeled data.

## Foundational Learning

- **Concept: Inverse Document Frequency (IDF)**
  - Why needed here: Serves as the "Global Signal" to bias thresholds higher for rare labels, ensuring the model needs strong evidence to predict uncommon classes
  - Quick check question: If a label appears in 50% of documents, should its IDF weight be high or low? (Answer: Low, because log(N/f) decreases as f increases)

- **Concept: Multi-Label Classification (MLC) Imbalance**
  - Why needed here: The core problem being solved; standard losses assume balanced classes while this targets the "long-tail" where most labels are rare
  - Quick check question: Why is Macro-F1 often lower than Micro-F1 in extreme MLC? (Answer: Macro-F1 averages performance over all labels, heavily penalizing failures on the vast number of rare, long-tail labels)

- **Concept: Margin Loss / Hinge Loss**
  - Why needed here: Used to sharpen decision boundaries, forcing the model to cross the threshold with a specific margin (Δ), improving confidence
  - Quick check question: What happens if the margin Δ is set too high in a noisy dataset? (Answer: The model may fail to generalize, treating noisy labels as errors or forcing overconfident predictions on ambiguous data)

## Architecture Onboarding

- **Component map:** TF-IDF features -> Shallow MLP Backbone -> Raw Logits (z) -> Global IDF Branch + Local KNN Branch -> Fusion Layer (λ, α, β) -> Adaptive Threshold (θ) -> Loss Head (BCE(z - θ, y) + Margin Loss)
- **Critical path:** The subtraction z_l(x) - θ_l(x) is the critical innovation; ensuring θ is differentiable and propagates gradients back to fusion weights (α, β) is essential for adaptive behavior
- **Design tradeoffs:** Batch Size Dependency (B×B matrix requires careful batch sizing); IDF vs KNN (IDF-only converges faster but lower performance; KNN requires longer training but yields higher final performance)
- **Failure signatures:** Early Convergence (underfitting if stopping around 0.01 Macro-F1, check if KNN disabled); Overconfidence on Rare Labels (if positive ratios spike, penalty mechanism not registering)
- **First 3 experiments:** 1) Sanity Check (Static): Fixed threshold 0.5 baseline (0.0035 macro-F1); 2) Ablation (IDF-Only): Disable KNN (λ=1) to verify IDF penalty reduces false positives (0.0094 macro-F1); 3) Full Fusion: Enable adaptive λ for 1500 epochs to confirm convergence (0.1712 macro-F1)

## Open Questions the Paper Calls Out
- Future extension to full TF-IDF formulation by integrating instance-aware label frequency statistics
- Shifting thresholding mechanism from label space to logit space for better integration with learned feature distributions

## Limitations
- Heavy reliance on batch-level KNN calculations creates scalability limitations for truly large-scale scenarios
- Method assumes access to ground-truth labels for KNN calculation, requiring fallback mechanisms for pure inference
- Performance gains are most pronounced on extremely imbalanced datasets; gains on less skewed data are more modest

## Confidence
- **High Confidence:** Core architectural innovation (differentiable threshold penalties via z-θ) and empirical superiority over static thresholding
- **Medium Confidence:** Claims about KNN providing stronger training signals (observational rather than analytically proven)
- **Low Confidence:** Scalability claims beyond AmazonCat-13K and generalization to other domains without extensive hyperparameter tuning

## Next Checks
1. **Inference-Time KNN Implementation:** Design and validate method for computing KNN signals during pure inference using approximate nearest neighbors or label embedding lookup table
2. **Batch Size Sensitivity Analysis:** Systematically vary batch size (16, 32, 64, 128) and measure impact on KNN signal quality, convergence speed, and final Macro-F1
3. **Gradient Attribution Study:** Use integrated gradients to quantify relative contribution of IDF vs KNN signals to final predictions, validating claim that KNN provides stronger training signals