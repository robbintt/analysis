---
ver: rpa2
title: 'Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning
  for Bit String Generation'
arxiv_id: '2505.17288'
source_url: https://arxiv.org/abs/2505.17288
tags:
- reward
- class
- should
- arxiv
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work provides a theoretical comparison of two methods for
  adapting large language models to new tasks: supervised fine-tuning (SFT) and Best-of-N
  (BoN). The study uses bit string generation as a case study and finds that when
  the learning setting is realizable (i.e., the target function is in the model class),
  SFT outperforms BoN due to better dependence on response length in its rate of convergence.'
---

# Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation

## Quick Facts
- arXiv ID: 2505.17288
- Source URL: https://arxiv.org/abs/2505.17288
- Reference count: 40
- This work compares supervised fine-tuning (SFT) and Best-of-N (BoN) for adapting large language models to new tasks.

## Executive Summary
This paper provides a theoretical comparison of supervised fine-tuning (SFT) and Best-of-N (BoN) methods for adapting large language models to new tasks, using bit string generation as a case study. The study finds that when the learning setting is realizable, SFT outperforms BoN due to better dependence on response length in its rate of convergence. However, when realizability fails, BoN can achieve better convergence rates either in sample size or in dependence on response length, depending on the specific failure mode. The analysis reveals that SFT is more sensitive to the learning setting and can fail entirely in agnostic cases, while BoN is more robust but potentially slower to converge.

## Method Summary
The paper analyzes two methods for adapting language models to new tasks: supervised fine-tuning (SFT) and Best-of-N (BoN). SFT involves training the model on labeled data using standard loss functions, while BoN generates multiple candidates for each input and selects the best one based on a scoring mechanism. The theoretical analysis focuses on bit string generation tasks, where the model must generate binary strings of a given length. The study examines convergence rates under different conditions, particularly focusing on how response length affects learning efficiency for each method.

## Key Results
- Under realizable settings, SFT outperforms BoN due to better dependence on response length in convergence rates
- When realizability fails, BoN can achieve better convergence rates either in sample size or response length dependence
- SFT is more sensitive to learning setting assumptions and can fail entirely in agnostic cases, while BoN shows greater robustness

## Why This Works (Mechanism)
The paper doesn't provide specific mechanism details beyond the theoretical analysis of convergence rates. The core mechanism involves comparing how SFT and BoN handle the trade-off between sample efficiency and response length when learning to generate bit strings.

## Foundational Learning
- **Realizability**: The assumption that the target function lies within the model class - needed to establish theoretical bounds on learning performance, quick check: verify if the target function can be perfectly represented by the model
- **Agnostic learning**: Learning when the target function may not be in the model class - needed to understand failure modes, quick check: test model performance when target is outside model hypothesis space
- **Convergence rates**: The speed at which learning methods approach optimal performance - needed to compare SFT and BoN efficiency, quick check: measure performance improvement as sample size increases
- **Response length dependence**: How learning efficiency scales with the length of outputs - needed to understand computational trade-offs, quick check: vary output length and measure performance degradation
- **Sample complexity**: The number of examples needed for effective learning - needed to evaluate practical feasibility, quick check: determine minimum samples required for target accuracy
- **Best-of-N selection**: The process of generating multiple candidates and choosing the best - needed to understand BoN's robustness, quick check: compare performance with different N values

## Architecture Onboarding

**Component Map**: Input -> Model (SFT or BoN) -> Output Generation -> Selection/Evaluation -> Performance Metrics

**Critical Path**: Input → Model Processing → Output Generation → Evaluation → Performance Assessment

**Design Tradeoffs**: SFT trades computational efficiency for sensitivity to realizability assumptions, while BoN trades potential slower convergence for robustness to approximation errors.

**Failure Signatures**: SFT fails completely in agnostic settings when target function is outside model class; BoN shows degraded but non-zero performance with increasing response length.

**First Experiments**:
1. Test SFT and BoN on bit string generation with varying response lengths (10, 50, 100 bits) under realizable conditions
2. Introduce controlled approximation errors to the target function and measure SFT vs BoN performance degradation
3. Vary the number of candidates (N) in BoN to find the optimal trade-off between computational cost and performance

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on simplified assumptions that may not capture practical deployment scenarios
- Bit string generation case study may not generalize to more complex real-world tasks
- Results depend critically on realizability assumptions rarely guaranteed in practice
- Analysis focuses on asymptotic behavior without quantifying practical sample sizes needed

## Confidence

**High confidence**: Claims regarding the mathematical relationship between convergence rates and response length under realizable settings, and the theoretical conditions under which SFT outperforms BoN in such cases.

**Medium confidence**: Claims about BoN's robustness to realizability failures and the relative sensitivity of SFT to agnostic settings, given that these depend on specific failure mode assumptions.

**Low confidence**: Claims about practical performance differences in real-world applications, as these extend beyond the theoretical framework without empirical validation.

## Next Checks

1. Conduct empirical studies comparing SFT and BoN on real-world tasks (e.g., text summarization, code generation) to validate theoretical predictions about convergence and performance under various sample sizes and response lengths.

2. Test the sensitivity of both methods to different degrees of realizability violation by introducing controlled approximation errors in the target function and measuring the impact on convergence rates and final performance.

3. Evaluate the computational overhead and practical constraints (e.g., inference time, memory usage) of BoN compared to SFT in resource-limited deployment scenarios to assess real-world feasibility.