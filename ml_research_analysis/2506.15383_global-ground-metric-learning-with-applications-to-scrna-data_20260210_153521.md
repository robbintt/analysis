---
ver: rpa2
title: Global Ground Metric Learning with Applications to scRNA data
arxiv_id: '2506.15383'
source_url: https://arxiv.org/abs/2506.15383
tags:
- metric
- data
- ground
- learning
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning appropriate ground
  metrics for optimal transport by introducing Global Ground Metric Learning (GGML).
  The method learns a metric function that captures relationships between individual
  data points (like a global metric) but is trained using only class labels on the
  distribution level (like a ground metric).
---

# Global Ground Metric Learning with Applications to scRNA data

## Quick Facts
- arXiv ID: 2506.15383
- Source URL: https://arxiv.org/abs/2506.15383
- Reference count: 40
- Global Ground Metric Learning (GGML) learns ground metrics using only class labels on the distribution level

## Executive Summary
This paper addresses the challenge of learning appropriate ground metrics for optimal transport by introducing Global Ground Metric Learning (GGML). The method learns a metric function that captures relationships between individual data points (like a global metric) but is trained using only class labels on the distribution level (like a ground metric). GGML uses triplet loss to learn relative relationships between distributions, requiring only class labels rather than pairwise distance information. The approach employs a low-rank approximation of the Mahalanobis distance to enable efficient learning in high-dimensional spaces.

The method is evaluated on synthetic data and patient-level single-cell RNA sequencing data spanning breast cancer, kidney disease, and myocardial infarction. Results show GGML outperforms existing metric learning methods in classification tasks (achieving up to 0.96 accuracy on synthetic data and 0.94 on kidney disease data) and clustering of disease states. The learned metrics generalize to unseen data and provide interpretable feature importance, identifying disease-relevant genes that align with existing literature.

## Method Summary
GGML introduces a novel approach to ground metric learning by leveraging triplet loss to capture relative relationships between distributions without requiring pairwise distance information. The method learns a low-rank Mahalanobis distance matrix that serves as the ground metric for optimal transport between distributions. By training on class labels rather than individual sample pairs, GGML bridges the gap between global metrics (which capture point-to-point relationships) and ground metrics (which operate at the distribution level). The low-rank approximation enables efficient computation in high-dimensional spaces typical of scRNA-seq data while maintaining the ability to capture meaningful biological relationships between disease states.

## Key Results
- GGML achieves up to 0.96 accuracy on synthetic data classification tasks
- The method reaches 0.94 accuracy on kidney disease patient-level scRNA-seq data
- Learned metrics provide interpretable feature importance, identifying disease-relevant genes that align with existing literature

## Why This Works (Mechanism)
GGML works by learning a ground metric that captures the intrinsic geometry of the data while being trainable using only class-level labels. The triplet loss formulation enables the model to learn relative distances between distributions without requiring explicit pairwise distance calculations, which would be computationally prohibitive in high-dimensional spaces. The low-rank Mahalanobis distance approximation reduces the number of parameters while maintaining sufficient flexibility to capture complex relationships in scRNA-seq data. By learning at the distribution level rather than individual sample level, GGML can capture broader patterns that generalize across samples within the same disease class.

## Foundational Learning
- Optimal Transport: Mathematical framework for comparing probability distributions - needed to establish the theoretical foundation for distribution-level comparisons
  - Quick check: Wasserstein distance formulation between empirical distributions
- Metric Learning: Techniques for learning distance functions from data - needed to adapt metric learning approaches to distribution-level comparisons
  - Quick check: Mahalanobis distance as a parameterized metric family
- Triplet Loss: Training objective using anchor-positive-negative samples - needed to learn relative distances without pairwise labels
  - Quick check: Margin-based ranking loss formulation
- Low-rank Approximation: Dimensionality reduction technique for efficient computation - needed to handle high-dimensional scRNA-seq data
  - Quick check: Matrix factorization preserving principal components

## Architecture Onboarding

### Component Map
GGML Architecture: Input Distributions -> Embedding Layer -> Low-rank Mahalanobis Matrix -> Triplet Loss -> Parameter Updates -> Learned Ground Metric

### Critical Path
The critical path involves learning the low-rank Mahalanobis matrix through triplet loss optimization. Starting with randomly initialized parameters, the model computes distances between distribution embeddings using the learned metric. Triplet loss gradients flow back to update the metric parameters, iteratively refining the ground metric to better capture the relationships between disease classes. The low-rank constraint ensures computational efficiency while maintaining sufficient expressive power.

### Design Tradeoffs
The primary tradeoff is between expressiveness and computational efficiency. The low-rank Mahalanobis approximation reduces the parameter space from O(dÂ²) to O(dr) where d is dimensionality and r is rank, enabling scalability to high-dimensional scRNA-seq data. However, this constraint may limit the model's ability to capture highly nonlinear relationships. The use of triplet loss instead of pairwise distances trades computational efficiency for potentially noisier gradient signals, though this is mitigated by the distribution-level aggregation.

### Failure Signatures
- Poor convergence or unstable training may indicate inappropriate triplet sampling strategies or margin parameters
- Low accuracy despite training could suggest the low-rank constraint is too restrictive for the data complexity
- Feature importance rankings that don't align with biological knowledge may indicate overfitting to class labels rather than capturing true biological signal
- Performance degradation on unseen data suggests the learned metric doesn't generalize well across patient populations

### 3 First Experiments
1. Synthetic data classification: Test GGML on controlled synthetic datasets with known ground truth to establish baseline performance and parameter sensitivity
2. Ablation study on rank parameter: Evaluate how different low-rank constraints affect both performance and computational efficiency across varying dataset sizes
3. Cross-disease generalization: Train on one disease condition and test on another to assess the metric's ability to capture general biological patterns versus disease-specific features

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on class-level labels rather than individual sample information may constrain ability to capture fine-grained variations within disease subtypes
- Triplet loss sensitivity to anchor, positive, and negative sample selection could affect stability and convergence
- Low-rank Mahalanobis approximation may not fully capture complex nonlinear relationships in extremely high-dimensional scRNA-seq data
- Performance on synthetic data (0.96 accuracy) may not translate directly to real-world scenarios with noise and batch effects

## Confidence
- High: Core methodology and synthetic data results (clear mathematical formulation and standard evaluation metrics)
- Medium: Real-world scRNA-seq applications (potential confounding factors in biological data and limited disease conditions tested)
- Low: Interpretability of feature importance rankings (gene-disease associations may be context-dependent and require experimental validation)

## Next Checks
1. Test GGML on larger, more diverse scRNA-seq datasets with multiple cell types and disease stages to assess scalability and generalization
2. Compare learned metrics against established biological pathway databases to validate the biological relevance of identified genes
3. Conduct ablation studies to determine the impact of the low-rank approximation on performance across different dimensionalities and dataset sizes