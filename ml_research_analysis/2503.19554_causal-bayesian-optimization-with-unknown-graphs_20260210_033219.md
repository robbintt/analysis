---
ver: rpa2
title: Causal Bayesian Optimization with Unknown Graphs
arxiv_id: '2503.19554'
source_url: https://arxiv.org/abs/2503.19554
tags:
- causal
- prior
- where
- graphs
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new method for causal Bayesian optimization
  (CBO) when the causal graph is unknown. It learns a Bayesian posterior over the
  direct parents of the target variable using both observational and interventional
  data, enabling optimization while simultaneously discovering the causal structure.
---

# Causal Bayesian Optimization with Unknown Graphs

## Quick Facts
- arXiv ID: 2503.19554
- Source URL: https://arxiv.org/abs/2503.19554
- Reference count: 40
- Method learns causal structure while optimizing objective in unknown causal graphs

## Executive Summary
This paper addresses the challenge of causal Bayesian optimization when the causal graph is unknown. The authors propose a method that simultaneously learns a Bayesian posterior over the direct parents of the target variable and performs optimization. By focusing interventions only on the direct parents of the target variable, the method achieves theoretical guarantees for optimization under certain assumptions while scaling effectively to larger graphs.

## Method Summary
The method learns a Bayesian posterior over the direct parents of the target variable using both observational and interventional data. For linear cases, a closed-form posterior update rule is derived, while a Gaussian process approximation is used for nonlinear cases. The approach leverages observational data to inform interventional choices, enabling optimization while discovering causal structure. The method scales well to larger graphs (up to 100 nodes) and achieves performance competitive with existing baselines that require full knowledge of the causal graph.

## Key Results
- Achieves performance competitive with full-graph-knowledge baselines
- Scales effectively to causal graphs with up to 100 nodes
- Demonstrates theoretical guarantees for optimization under certain assumptions
- Successfully learns causal structure while optimizing the objective function

## Why This Works (Mechanism)
The method works by leveraging the key insight that intervening only on the direct parents of the target variable is theoretically sufficient for optimization under certain assumptions. This reduces the search space significantly compared to full causal discovery. The Bayesian approach allows for uncertainty quantification in the learned structure, which is crucial for making informed intervention decisions. The closed-form updates in the linear case provide computational efficiency, while the GP approximation extends the method to nonlinear relationships.

## Foundational Learning
1. **Causal Bayesian Optimization** - Why needed: Enables optimization in systems where interventions are necessary to discover optimal configurations. Quick check: Can the method improve objective values through targeted interventions?
2. **Direct Parent Intervention Theory** - Why needed: Provides theoretical justification for focusing only on direct parents of target variable. Quick check: Does the optimization guarantee hold under the stated assumptions?
3. **Bayesian Posterior Updates** - Why needed: Enables uncertainty quantification in causal structure learning. Quick check: Does the posterior uncertainty decrease with more data?
4. **Gaussian Process Approximation** - Why needed: Extends method to nonlinear causal relationships. Quick check: How does performance degrade with increasing nonlinearity?
5. **Causal Discovery from Mixed Data** - Why needed: Combines observational and interventional data for structure learning. Quick check: Is the learned structure consistent with ground truth in synthetic experiments?

## Architecture Onboarding

**Component Map:** Observational Data -> Causal Structure Learner -> Intervention Selector -> Optimizer -> Performance Monitor

**Critical Path:** Structure Learning -> Intervention Selection -> Objective Evaluation -> Structure Update

**Design Tradeoffs:** The method trades off complete causal discovery for focused optimization on direct parents, reducing computational complexity at the cost of potentially missing indirect effects. The Bayesian approach provides uncertainty quantification but increases computational overhead compared to point estimates.

**Failure Signatures:** Poor performance may indicate: 1) violated linearity assumptions in closed-form updates, 2) insufficient interventional data for accurate structure learning, 3) model misspecification in the GP approximation for nonlinear cases.

**3 First Experiments:**
1. Verify optimization performance on synthetic linear causal graphs with known ground truth
2. Test scalability by gradually increasing graph size from 10 to 100 nodes
3. Compare performance with and without observational data to validate its importance

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on specific assumptions (linear Gaussian case, known sparsity)
- Real-world causal graphs often contain feedback loops and cycles not handled by current framework
- Effectiveness of GP approximation for nonlinear cases lacks rigorous theoretical justification
- Identifiability issues when observational data is scarce or confounded

## Confidence
- Linear case with known assumptions: High
- Nonlinear approximation: Medium
- Real-world applicability: Low

## Next Checks
1. Test on real-world causal discovery benchmarks with known ground truth
2. Evaluate performance under various noise distributions and model misspecifications
3. Extend the method to handle cyclic causal graphs and feedback loops