---
ver: rpa2
title: 'Lemmatization as a Classification Task: Results from Arabic across Multiple
  Genres'
arxiv_id: '2506.18399'
source_url: https://arxiv.org/abs/2506.18399
tags:
- arabic
- lemmatization
- lemma
- word
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Arabic lemmatization in morphologically
  rich languages with ambiguous orthography by introducing novel classification-based
  approaches. The core method reframes lemmatization as classification into a Lemma-POS-Gloss
  (LPG) tagset, leveraging machine translation for gloss alignment and semantic clustering
  to group semantically similar LPG entries.
---

# Lemmatization as a Classification Task: Results from Arabic across Multiple Genres

## Quick Facts
- arXiv ID: 2506.18399
- Source URL: https://arxiv.org/abs/2506.18399
- Reference count: 16
- Primary result: LPG-based classification and clustering approaches achieve up to 98.9% accuracy on Arabic lemmatization

## Executive Summary
This paper addresses the challenge of Arabic lemmatization in morphologically rich languages with ambiguous orthography by introducing novel classification-based approaches. The core method reframes lemmatization as classification into a Lemma-POS-Gloss (LPG) tagset, leveraging machine translation for gloss alignment and semantic clustering to group semantically similar LPG entries. A new multi-genre Arabic lemmatization test set covering novels, children's stories, and other domains is introduced alongside standardized synchronization of existing datasets. Experimental results show that LPG-based classification and clustering approaches outperform prior systems, achieving up to 98.9% accuracy on certain configurations.

## Method Summary
The paper introduces a classification-based approach to Arabic lemmatization, treating each unique Lemma-POS-Gloss (LPG) combination as a distinct class for prediction. A BERT-based model is fine-tuned to map input text (word + context) directly to one of ~18,000 predefined LPG classes. To extend coverage beyond the training vocabulary, semantic clustering groups ~49K LPG entries into 2,000 semantic clusters based on contextual embeddings, allowing the model to handle LPGs not seen during classification training. The method also incorporates a character-level sequence-to-sequence Transformer for complementary lemma prediction, with hybrid models combining seq2seq and classification to further boost performance. All code, models, and annotations will be released to support continued research.

## Key Results
- LPG-based classification achieves up to 98.9% accuracy on certain configurations
- Semantic clustering extends classification coverage by grouping the full LPG space into predictive buckets
- Hybrid models combining seq2seq and classification further boost performance
- Character-level sequence-to-sequence models provide complementary benefits but are limited to lemma prediction and prone to hallucinating implausible forms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reframing lemmatization as a classification task over a Lemma-POS-Gloss (LPG) tagset reduces output space complexity compared to raw generation, provided a comprehensive morphological analyzer exists.
- Mechanism: A BERT-based model is fine-tuned to map input text (word + context) directly to one of ~18,000 predefined LPG classes. This bypasses the need to generate character sequences, ensuring the output is always a valid morphological entry.
- Core assumption: The target LPG exists within the fixed class set derived from the training corpus; the model cannot invent new valid lemmas.
- Evidence anchors: [abstract]: "...framing lemmatization as classification into a rich Lemma-POS-Gloss (LPG) tagset."; [section 5]: "In this approach, lemmatization is framed as a classification task, where each unique LPG is treated as a distinct class..."
- Break condition: High Out-of-Vocabulary (OOV) rates in new domains where lemmas fall outside the 18,000 trained classes.

### Mechanism 2
- Claim: Semantic clustering extends classification coverage by grouping the full LPG space (including unseen terms) into predictive buckets.
- Mechanism: K-Means clustering groups ~49K LPG entries from the analyzer database into 2,000 semantic clusters based on contextual embeddings. A classifier predicts the cluster, narrowing the search space before ranking candidates, allowing the model to handle LPGs not seen during classification training.
- Core assumption: Semantically similar lemmas appear in similar contexts, and a cluster prediction sufficiently constrains the final selection.
- Evidence anchors: [abstract]: "...using LPG semantic clustering."; [section 5]: "...we redefine lemmatization as a clustering task... Words that share the same LPG are assigned the same averaged embedding."
- Break condition: Cluster granularity is too coarse (ambiguous lemmas share clusters) or too fine (model fails to generalize).

### Mechanism 3
- Claim: Hybridizing Seq2Seq generation with classification/filtering leverages the contextual strength of generators while mitigating their tendency to hallucinate invalid forms.
- Mechanism: A character-level Transformer (Seq2Seq) predicts a lemma candidate. This prediction is used as a filter or ranking signal alongside the morphological analyzer's candidates. If the Seq2Seq output matches a valid candidate, it boosts confidence; otherwise, it is discarded.
- Core assumption: Seq2Seq models provide a useful recall signal (finding the right lemma) but require a precision filter (the analyzer) to ensure morphological validity.
- Evidence anchors: [abstract]: "Hybrid models combining seq2seq and classification further boost performance."; [section 6.3]: "S2S often hallucinated implausible lemmas (40%), while BEST showed no hallucinations..."
- Break condition: The Seq2Seq model consistently hallucinates forms that incorrectly match valid but wrong candidates in the analyzer.

## Foundational Learning

- **Lemmatization vs. Stemming**
  - Why needed here: The paper targets the dictionary form (lemma), not just the root. Arabic's complex morphology requires distinguishing "holding" (noun) from "hold" (verb), which stemming often conflates.
  - Quick check question: Given the Arabic word *Eaqad*, does the system return a single stem or distinct lemmas for "contract" (noun) and "hold" (verb)?

- **Morphological Analyzer Constraints**
  - Why needed here: The classification pipeline is fundamentally constrained by the analyzer's recall. If the correct lemma is not in the analyzer's database (CALIMA-S31), the classifier cannot select it.
  - Quick check question: If a user invents a new Arabic neologism, will the system fail to lemmatize it, or will it fall back to a generator?

- **The Hallucination Problem in Generation**
  - Why needed here: Understanding why pure Seq2Seq is insufficient explains the need for the hybrid architecture.
  - Quick check question: Why does a character-level model predict "investigation" when the input word clearly derives from "welcome"?

## Architecture Onboarding

- **Component map**: Input Text -> Morphological Analyzer (CALIMA-S31) -> POS tagger (CAMeL BERT) -> LexC classifier (18K LPG classes) or Clustering model (2K clusters) -> Seq2Seq generator (optional) -> LogP fallback

- **Critical path**: Input Text -> Analyzer (Candidate Generation) -> Tagger (Ranking) -> Clustering (Filter by Cluster) -> Seq2Seq (Filter by Generated Lemma) -> LogP (Final Ranking)

- **Design tradeoffs**:
  - LexC vs. Clustering: LexC is simpler but limited to training vocabulary. Clustering is more complex but covers the entire analyzer database.
  - Precision vs. Recall: The pipeline progressively filters candidates. Prioritizing the classifier increases precision; prioritizing the generator increases recall.

- **Failure signatures**:
  - Analyzer Miss: System returns "Unknown" or a wrong lemma because the correct one was never generated by the analyzer.
  - Diacritization Drift: Predicted lemma is correct but diacritics differ from the gold standard due to normalization inconsistencies.
  - Hallucination: Seq2Sec model predicts a morphologically implausible form.

- **First 3 experiments**:
  1. **Establish Baseline**: Run `Top+LogP` on the provided BAREC dataset to measure raw disambiguator performance without neural reranking.
  2. **Ablate Components**: Run `Top+S2S+LogP` and `Top+Clust+LogP` separately to isolate the contribution of the Seq2Seq filter versus the Clustering classifier.
  3. **Error Analysis**: Sample 100 errors from the `Best` system on the ATB Dev set to categorize failures (Analyzer Recall vs. Model Ranking).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can sequence-to-sequence (seq2seq) models be optimally integrated as a fallback strategy for out-of-vocabulary (OOV) lemmas to enhance the robustness of classification-based systems?
- **Basis in paper**: [explicit] The conclusion explicitly lists "explore seq2seq as a fallback for OOV terms" as a direction for future work to boost robustness.
- **Why unresolved**: Current classification models are constrained by a predefined set of 18,000 LPG classes, failing on lemmas outside this set, while seq2seq models can generate arbitrary forms but hallucinate.
- **What evidence would resolve it**: A hybrid model evaluation specifically targeting tokens labeled as OOV in the analyzer, measuring improvements in recall without sacrificing precision.

### Open Question 2
- **Question**: Does expanding the morphological analyzer's recall to generate broader LPG candidate sets improve downstream classification accuracy, or does it introduce excessive noise?
- **Basis in paper**: [explicit] The authors state they aim to "improve analyzer recall with broader LPG candidate generation" in future work.
- **Why unresolved**: The current system often relies on "Top" ranked candidates; if the correct lemma is pruned early, the classifier fails. It is unclear if a noisier, wider candidate pool improves the upper bound or confuses the model.
- **What evidence would resolve it**: Experiments comparing classifier performance using the top-k analyzer candidates versus the full candidate set on ambiguous tokens.

### Open Question 3
- **Question**: Can the gloss-based cosine similarity approach (SimG) maintain its disambiguation utility while shedding the computational overhead of external machine translation?
- **Basis in paper**: [inferred] The methodology section notes that SimG is "computationally expensive" and limited by reliance on Google Translate APIs, contrasting it with more efficient classification methods.
- **Why unresolved**: While effective, the dependency on external translation makes SimG impractical for large-scale or offline use. It is unresolved if internal cross-lingual embeddings could replace the translation step.
- **What evidence would resolve it**: Benchmarking SimG using open-source alignment tools or multilingual embeddings against the current API-based setup to measure the speed/accuracy trade-off.

## Limitations
- The classification approach is fundamentally limited by the morphological analyzer's recall, unable to predict lemmas outside the 18,000 LPG class set
- The semantic clustering approach introduces approximation errors when semantically similar lemmas don't share contextual patterns
- The seq2seq component has a 40% hallucination rate, producing morphologically implausible forms

## Confidence
- **High Confidence**: The LPG-based classification approach outperforms traditional generation methods on the established ATB benchmark
- **Medium Confidence**: The multi-genre evaluation demonstrates robustness across domains with incremental improvements (2-5% accuracy gains)
- **Low Confidence**: The semantic clustering mechanism's effectiveness relies heavily on the assumption that semantically similar lemmas share contextual embeddings

## Next Checks
1. **Analyzer Recall Audit**: Systematically sample 1000 gold references from the test sets where the system failed and verify whether the correct lemma exists in the CALIMA-S31 database
2. **Cluster Quality Analysis**: Extract 50 random clusters from the K-Means output and manually evaluate the semantic coherence of LPG entries within each cluster
3. **Seq2Seq Hallucination Investigation**: For 100 hallucinated predictions, categorize the error types: morphological impossibilities, semantic drift, or morphological overgeneralization