---
ver: rpa2
title: 'Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages'
arxiv_id: '2503.23542'
source_url: https://arxiv.org/abs/2503.23542
tags:
- language
- https
- arxiv
- languages
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates how to improve multilingual Whisper ASR
  performance for low-resource languages by integrating language models at inference
  time. The authors fine-tune Whisper models on Common Voice data and then combine
  them with both n-gram and large language models (LLMs) using a weighted scoring
  approach.
---

# Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages

## Quick Facts
- arXiv ID: 2503.23542
- Source URL: https://arxiv.org/abs/2503.23542
- Authors: Xabier de Zuazo; Eva Navas; Ibon Saratxaga; Inma Hernáez Rioja
- Reference count: 40
- This paper demonstrates how to improve multilingual Whisper ASR performance for low-resource languages by integrating language models at inference time.

## Executive Summary
This paper presents a method to improve Whisper ASR performance for low-resource languages by integrating language models (both n-gram and large language models) at inference time through a weighted scoring approach. The authors fine-tune Whisper models on Common Voice data and then combine them with external LMs using optimized interpolation parameters. The approach achieves up to 51% WER reduction for in-distribution data and 34% for out-of-distribution data, with n-gram models providing better absolute performance while LLMs offer more consistent robustness across domains.

## Method Summary
The method involves fine-tuning Whisper models on language-specific datasets (Common Voice 13.0) followed by inference-time integration of external language models using shallow fusion. The final score for candidate transcripts combines Whisper's acoustic scores with LM probabilities: Q(c|x) = log(P_acoustic) + α·log(P_LM) + β·word_count. The α and β parameters are optimized via Bayesian search (Optuna) on Tiny models and then applied to all model sizes. Both n-gram (KenLM) and LLM integrations are evaluated, with the latter using max(softmax(final_logits)) for scoring.

## Key Results
- Up to 51% WER reduction for in-distribution datasets using n-gram language models
- Up to 34% WER reduction for out-of-distribution datasets
- LLM integration provides moderate but consistently robust improvements across languages
- Language model integration is most beneficial for low-resource languages, with diminishing returns for high-resource languages

## Why This Works (Mechanism)

### Mechanism 1: Inference-Time Score Fusion for Language Modeling
Combining Whisper's acoustic scores with external language model scores at inference time reduces WER for low-resource languages. The acoustic model generates log-probabilities for token sequences, while the external LM provides linguistic prior. The final score is a weighted sum with parameters α and β controlling LM influence and length normalization, optimized via Bayesian search. This works because Whisper's internal language modeling is insufficient for low-resource languages, and external LMs can correct linguistic implausibilities without retraining the acoustic model.

### Mechanism 2: Language-Specific Fine-Tuning as a Precursor
Fine-tuning a multilingual ASR model on a specific low-resource language's data provides a necessary baseline of performance before LM integration. The pre-trained Whisper model's weights are updated using supervised learning on language-specific data, adapting the acoustic model to phonetic and lexical characteristics. The LM is then integrated with this fine-tuned model. This works because the pre-trained multilingual model's performance on specific low-resource languages is suboptimal but can be significantly improved by exposing it to more examples of that language's speech-text pairs.

### Mechanism 3: Trade-off Between Absolute Performance and Robustness
N-gram LMs yield larger absolute WER reductions but with more variance, while LLMs provide smaller, more consistent improvements across in-distribution and out-of-distribution datasets. N-gram models excel at correcting errors within their training distribution but may not generalize as well to out-of-distribution data. LLMs, with broader training on massive and diverse text, offer a more robust linguistic prior that generalizes better across domains, leading to more stable relative error reduction.

## Foundational Learning

- **Concept: Shallow Fusion**
  - Why needed here: This is the core technique of the paper, yet the term isn't used. Understanding this concept helps an engineer recognize this is a known method being applied to a new model (Whisper).
  - Quick check question: How does shallow fusion differ from deep fusion and cold fusion in ASR?

- **Concept: Beam Search Decoding**
  - Why needed here: The LM integration modifies the beam search process. Understanding beam search is necessary to implement the scoring change correctly.
  - Quick check question: At each step of beam search, how do you select the next set of candidate tokens? How does the LM score get incorporated into this selection?

- **Concept: Relative Error Reduction (RER)**
  - Why needed here: The paper uses RER instead of absolute WER to compare improvements. Understanding this metric is crucial for correctly interpreting the results.
  - Quick check question: If a model has a baseline WER of 40% and achieves a 25% RER, what is the final WER?

## Architecture Onboarding

- **Component map:** Common Voice dataset -> Whisper fine-tuning -> Language model training -> Shallow fusion integration -> Bayesian parameter optimization -> Evaluation on test sets

- **Critical path:**
  1. Acquire language-specific speech-text data (e.g., Common Voice)
  2. Fine-tune the Whisper model on this data
  3. Train or select an external Language Model (n-gram or LLM) for the same language
  4. Implement the shallow fusion logic in the Whisper beam search decoder
  5. Run Bayesian optimization to find the best α and β on a held-out validation set
  6. Evaluate the integrated system on test sets (both in-distribution and out-of-distribution)

- **Design tradeoffs:**
  - N-gram vs. LLM: N-grams offer better peak performance on in-distribution data and are faster to query. LLMs offer better robustness and generalization but are slower and more computationally expensive at inference.
  - Optimization Efficiency: Optimizing α and β on the smallest model (Tiny) and reusing them for larger models saves significant computation but may yield sub-optimal results for the largest models.
  - Corpus Selection: The LM's training corpus has a huge impact. Overlap with evaluation data can artificially inflate ID scores, while poor domain match can hurt OOD performance.

- **Failure signatures:**
  - Optimal parameters for one model size/language do not transfer. The paper finds that parameters optimized on Tiny models work "well enough" for others but may not be optimal, especially for high-resource languages with large models.
  - Overfitting during fine-tuning. On high-resource languages, fine-tuning can degrade OOD performance if the model overfits to the fine-tuning corpus (e.g., Spanish MLS degradation).
  - Inappropriate β values. Without proper length normalization, the model might favor overly short or long transcriptions.

- **First 3 experiments:**
  1. Baseline Establishment: Run the vanilla (pre-trained) and fine-tuned Whisper models on your target language's test sets to establish WER baselines.
  2. N-gram Integration: Train a 5-gram KenLM model on a relevant text corpus. Integrate it with the fine-tuned Whisper model using the provided scoring equation. Perform a parameter sweep or Bayesian search for α and β on a validation set and report the final WER on the test set.
  3. LLM Integration: Select a suitable LLM for the language. Integrate it similarly, optimizing for α and β. Compare the results against the N-gram and fine-tuned baselines, paying special attention to the relative error reduction (RER) on both in-distribution and out-of-distribution test sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the combined integration of n-gram and large language models (LLMs) outperform individual LM integrations for low-resource ASR?
- Basis in paper: [explicit] The Conclusion states, "An obvious next step would be to investigate the combined use of n-gram and large language models to conceivably merge the capabilities of both methods."
- Why unresolved: The authors evaluated n-grams and LLMs separately, identifying a trade-off where n-grams offer better absolute performance and LLMs offer better robustness.
- What evidence would resolve it: Results from a hybrid system utilizing both statistical and neural LMs simultaneously, showing WER and ERER improvements over single-LM baselines.

### Open Question 2
- Question: Does optimizing the interpolation parameters (α and β) specifically for large Whisper models yield significant performance gains over reusing parameters optimized on Tiny models?
- Basis in paper: [inferred] Section 3.2 notes that reusing Tiny model parameters for larger models "will probably not provide the best results" but was done to manage the resource-intensive nature of the optimization.
- Why unresolved: The authors did not perform size-specific Bayesian optimization for larger models, leaving the potential performance gap for Large-V3 models unquantified.
- What evidence would resolve it: A comparative study showing WER differences on Large models when using parameters optimized specifically for them versus those transferred from Tiny models.

### Open Question 3
- Question: Does LLM integration provide greater relative improvements for long-form audio content compared to the sentence-level recordings analyzed in this study?
- Basis in paper: [explicit] The Conclusion hypothesizes that "LLMs may shine in processing long-form content, leveraging their broader contextual understanding."
- Why unresolved: The study utilized sentence-level audio, where the temperature scheduler was disabled; long-form scenarios utilize this scheduler and present different decoding challenges.
- What evidence would resolve it: Evaluations of the integration method on long-form datasets (e.g., TED talks, meetings) to compare the robustness and error rates of LLMs against n-grams in extended contexts.

## Limitations

- The paper optimizes interpolation parameters (α and β) on Tiny models and applies them to all larger model sizes without rigorous validation of parameter transferability.
- Limited technical detail is provided on how LLM logits are integrated into the Whisper beam search decoder, particularly regarding partial transcript processing.
- The composition and potential overlap of the 27M sentence corpora used for n-gram training with evaluation datasets is not fully transparent.

## Confidence

**High Confidence Claims:**
- Whisper fine-tuning on Common Voice data improves baseline WER for low-resource languages
- N-gram LM integration provides substantial WER reduction (up to 51%) for in-distribution data
- Language model integration is most beneficial for low-resource languages

**Medium Confidence Claims:**
- LLM integration provides consistently robust improvements across in-distribution and out-of-distribution data
- ERER metric effectively captures robustness differences between n-gram and LLM approaches
- Temperature scheduling disablement is critical for optimal performance

**Low Confidence Claims:**
- Optimized parameters from Tiny models generalize well to larger models
- N-gram models outperform LLMs on in-distribution data
- The trade-off between n-gram absolute performance and LLM robustness is universally applicable

## Next Checks

1. **Per-Model-Size Parameter Optimization**: Re-run the Bayesian optimization for α and β parameters independently on at least two larger model sizes (e.g., Base and Large-V3) for one low-resource language. Compare the resulting WER improvements against the single Tiny-optimized parameter set to quantify the performance gap and validate the computational trade-off claim.

2. **LLM Integration Implementation Audit**: Request and review the complete implementation code for LLM integration, specifically focusing on how partial transcript logits are extracted and incorporated into the beam search. Reproduce the LLM results using this implementation to verify the reported 10-20% RER improvements are achievable.

3. **Corpus Overlap Analysis**: Conduct a detailed analysis of the 27M sentence corpora used for n-gram training. Calculate the overlap percentage with Common Voice training data and evaluation datasets (FLEURS, OpenSLR, MLS, AhoMyTTS). Re-compute WER metrics after removing overlapping content to assess the true generalization capability of the n-gram approach.