---
ver: rpa2
title: Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment
  Time Analysis
arxiv_id: '2511.12018'
source_url: https://arxiv.org/abs/2511.12018
tags:
- vehicle
- traffic
- safety
- data
- intersection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a real-time, multi-camera computer vision framework
  for traffic safety assessment at signalized intersections using Post-Encroachment
  Time (PET) analysis. The system uses four synchronized overhead cameras and NVIDIA
  Jetson AGX Xavier devices to detect vehicles with YOLOv11 segmentation, project
  them into a unified bird's-eye view using homography matrices, and compute pixel-level
  PET values to identify high-risk zones.
---

# Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis

## Quick Facts
- arXiv ID: 2511.12018
- Source URL: https://arxiv.org/abs/2511.12018
- Reference count: 34
- Multi-camera system achieves 3.3 cm/pixel resolution and 2.68 FPS processing for pixel-level PET safety analysis

## Executive Summary
This paper presents a real-time multi-camera computer vision framework for traffic safety assessment at signalized intersections using Post-Encroachment Time (PET) analysis. The system employs four synchronized overhead cameras and NVIDIA Jetson AGX Xavier devices to detect vehicles with YOLOv11 segmentation, project them into a unified bird's-eye view using homography matrices, and compute pixel-level PET values to identify high-risk zones. The approach achieves sub-second temporal resolution and centimeter-level spatial accuracy without requiring crash records, providing a scalable, low-cost solution for intelligent transportation systems.

## Method Summary
The method uses four Hikvision PTZ cameras providing RTSP streams to distributed NVIDIA Jetson AGX Xavier devices. Each device runs YOLOv11m-seg for vehicle segmentation, projects detections to a unified bird's-eye coordinate system using homography matrices computed from manually annotated point correspondences, and sends JSON results via SMB to a central Windows system. The central system performs overlap-weighted rectangle fitting (weights 1/2/6/8 for 1-4 camera detections) to estimate vehicle positions, stores results in MySQL with millisecond timestamps, and a GPU server computes pixel-level PET values using a stopwatch matrix approach before generating logarithmic heatmaps highlighting hazardous zones.

## Key Results
- Achieves 3.3 cm/pixel spatial resolution in unified bird's-eye view
- Processes frames at 2.68 FPS average on edge devices (289ms for YOLOv11 inference)
- Generates PET heatmaps revealing hazardous regions with sub-second precision
- Requires 3+ cameras for rectangle fitting, creating coverage gaps at intersection edges

## Why This Works (Mechanism)

### Mechanism 1
Multi-camera homography fusion reduces vehicle localization uncertainty from occlusions. Four cameras with overlapping fields of view provide redundant observations. Homography matrices transform each camera's 2D image coordinates into a unified world coordinate system. When multiple cameras detect the same vehicle, the system triangulates position more accurately than any single view could achieve. Core assumption: Vehicles lie on a known ground plane and cameras can be calibrated with sufficient point correspondences. Evidence anchors: [abstract] "Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views." [Section III.C] Describes computing 3×3 homography matrices using OpenCV's cv2.findHomography() on annotated points of interest. [Section IV.A] "The multi-camera coverage mitigates the inherent limitations of deriving 3-dimensional positions from 2-dimensional video frames, where occlusions...can introduce errors." [corpus] Neighbor papers on multi-camera traffic monitoring (Saunier et al.) confirm homography-based fusion is established practice, though not validated for this specific PET application. Break condition: When fewer than 3 cameras cover a region, rectangle fitting fails and PET cannot be computed for those pixels (explicitly noted in Section V.A).

### Mechanism 2
Pixel-level PET computation enables sub-meter hazard localization without fixed spatial cells. Instead of aggregating vehicle presence into predefined grid cells, the system maintains a "stopwatch" matrix for each pixel. When a vehicle occupies a pixel, the timer resets to zero. When the pixel is empty, the timer increments. The time between a vehicle leaving and another arriving is logged as a PET value, then averaged across all observations. Core assumption: Temporal sampling rate (~2.68 FPS, ~373ms intervals) is sufficient to capture encroachment events without aliasing rapid crossings. Evidence anchors: [abstract] "A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization...accurate to 3.3 sq-cm." [Section III.E] Describes the stopwatch matrix approach with 0.2-second minimum occupancy threshold for PET contributions. [Section IV.C] Equation (1) shows PET averaging formula over N post-encroachment intervals. [corpus] Corpus papers discuss PET as a surrogate safety measure but do not validate pixel-level implementations; this appears novel. Break condition: Frame rate drops or timestamp desynchronization exceeds 350ms threshold, causing the same vehicle to appear in different positions across cameras.

### Mechanism 3
Weighted overlap scoring from multiple camera detections improves rectangle fitting accuracy. Each pixel receives points based on how many cameras detected a vehicle there (1 camera = 1 point, 2 = 2 points, 3 = 6 points, 4 = 8 points). The system then fits minimum-area rotated rectangles to maximize accumulated points, with heuristics for aspect ratio snapping and edge extension. Core assumption: Cameras are sufficiently synchronized and calibrated that overlapping detections represent the same physical vehicle. Evidence anchors: [Section III.D] "Pixels were assigned point values based on the number of cameras which detected a vehicle...optimal point values for pixels with one, two, three, and four cameras reporting a vehicle were one, two, six, and eight, respectively." [Section III.D] Describes safeguards including angle snapping, edge extension, and splitting large rectangles. [Section III.F] Maximum timestamp difference of 350ms enforced for quad-camera fusion; falls back to 3-camera if unavailable. [corpus] No direct corpus validation for this specific scoring scheme; appears empirically derived. Break condition: Empirical point weights (1, 2, 6, 8) were manually tuned on test data and may not generalize to intersections with different camera geometries.

## Foundational Learning

- **Homography transformation**
  - Why needed here: Converts between camera image coordinates and world coordinates using 2D projective geometry. Required for all spatial fusion operations.
  - Quick check question: Given 4 point correspondences between image pixels and world coordinates, can you explain why a 3×3 homography matrix has only 8 degrees of freedom?

- **Post-Encroachment Time (PET)**
  - Why needed here: Core safety metric. Lower PET values indicate higher collision risk because vehicles passed through the same space with less time separation.
  - Quick check question: Why is PET considered a "surrogate" safety measure rather than a direct collision predictor?

- **Real-time inference constraints on edge hardware**
  - Why needed here: The pipeline runs on NVIDIA Jetson AGX Xavier devices with limited compute. YOLOv11 inference dominates processing time (77.5% of 373ms per frame).
  - Quick check question: If image segmentation takes 289ms and you need 10 FPS, what component of the pipeline must be parallelized or optimized?

## Architecture Onboarding

- **Component map:**
  4× Hikvision PTZ cameras → RTSP streams → 4× NVIDIA Jetson AGX Xavier (one per camera) → YOLOv11 segmentation → coordinate conversion → JSON output → 1× Windows industrial system → receives JSON via SMB → rectangle fitting → MySQL database writes → 1× GPU server (notos.sdsu.edu) → reads database → PET computation → heatmap generation

- **Critical path:**
  Camera capture → YOLOv11 inference (289ms, bottleneck) → homography projection → JSON serialization → SMB transfer → rectangle fitting (20ms) → SQL write (32ms concurrent). Effective throughput: 2.68 FPS.

- **Design tradeoffs:**
  Edge vs. centralized: Distributing YOLO inference across 4 Jetsons parallelizes the bottleneck, but introduces synchronization complexity. A single more powerful machine could simplify the pipeline (noted in Section V.A). Resolution vs. speed: 800×800 pixel PET grid at 3.3cm/pixel provides high spatial fidelity but requires 126ms per frame for PET updates. Coverage vs. accuracy: Requiring 3+ cameras for rectangle fitting improves precision but creates blind spots at intersection edges.

- **Failure signatures:**
  Timestamp drift > 350ms: JSON files rejected, frame skipped, gaps in PET data. RTSP decoding errors: Contributes ~84ms average delay variance per frame. Single or dual-camera regions: No rectangle fitting possible, no PET values generated. Fisheye distortion: Not corrected; may degrade homography accuracy at frame edges.

- **First 3 experiments:**
  1. **Validate homography accuracy**: Place calibration markers at known world coordinates within the intersection. Project detected marker centroids back to world coordinates and measure mean localization error in centimeters.
  2. **Synchronization stress test**: Artificially offset timestamps from one camera by 100ms, 200ms, 350ms, 500ms and measure rectangle fitting degradation rate (failed fits / total frames).
  3. **PET threshold sensitivity**: Generate heatmaps using different minimum PET contribution thresholds (current: 0.2 seconds). Compare spatial patterns to identify whether hazard zones shift or disappear at different thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
How does the integration of temporal tracking algorithms (e.g., DeepSORT) impact the accuracy of PET calculations compared to the current frame-wise detection method? Basis in paper: [explicit] Section V.A states that incorporating tracking methods would enable consistent vehicle IDs and trajectory prediction, which the current independent frame logging lacks. Why unresolved: The current database logs vehicles as independent entries per frame without temporal continuity, preventing trajectory analysis. Evidence: A comparative study evaluating PET error rates on the same dataset with and without multi-object tracking enabled.

### Open Question 2
Can the homography calibration process be automated to enable rapid deployment at new intersections without manual point annotation? Basis in paper: [explicit] Section V.A identifies generalization as limited by "manually annotated points of interest" and hard-coded parameters. Why unresolved: The system currently relies on manual coordinate mapping for homography matrices, creating a bottleneck for scalability. Evidence: Demonstration of the system successfully calibrating and operating at a geometrically distinct intersection without manual point selection.

### Open Question 3
What indexing strategies can effectively calculate PET in intersection regions covered by fewer than three cameras? Basis in paper: [explicit] Section V.A notes that the requirement for three-camera visibility creates blind spots and suggests future implementations adopt an "alternative indexing strategy." Why unresolved: The current rectangle-fitting algorithm depends on high overlap (3+ cameras) to generate reliable world-coordinate bounding boxes. Evidence: Validated PET readings in low-coverage edge regions that correlate with ground-truth traffic observations.

## Limitations
- Synchronization vulnerability: Pipeline requires timestamp synchronization within 350ms; any drift beyond this threshold causes frame drops and PET data gaps.
- Coverage gaps: Rectangle fitting requires ≥3 camera detections, creating PET blind spots at intersection edges where vehicles are only visible from 1-2 cameras.
- Point-weight arbitrariness: The empirically-derived overlap scoring weights (1, 2, 6, 8) were manually tuned on test data and may not generalize to intersections with different camera geometries or vehicle densities.

## Confidence
- High confidence: Multi-camera homography fusion mechanism (well-established computer vision technique with clear implementation in Section III.C)
- Medium confidence: Pixel-level PET computation validity (novel approach but requires 2.68 FPS sampling rate assumption)
- Low confidence: Overlap scoring weight optimality (manually tuned without systematic validation)

## Next Checks
1. **Localization error quantification**: Measure actual vehicle position errors from projected homography coordinates against ground truth (e.g., GPS-tagged vehicles or physical markers) to verify the claimed 3.3 cm resolution accuracy.
2. **Real-world synchronization robustness**: Test the pipeline under network latency variations and clock drift conditions to determine failure rates and evaluate fallback strategies.
3. **Cross-intersection generalization**: Apply the scoring weights and PET heatmaps to a different intersection geometry to assess whether hazard zones remain consistent or require retuning.