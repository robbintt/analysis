---
ver: rpa2
title: AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms
arxiv_id: '2508.19004'
source_url: https://arxiv.org/abs/2508.19004
tags:
- social
- human
- understanding
- these
- norms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated whether AI systems can accurately predict human
  social norms by comparing their predictions against individual human judgments.
  Across two studies, GPT-4.5 and newer models like GPT-5 and Gemini 2.5 Pro were
  tested on predicting social appropriateness ratings for 555 everyday scenarios.
---

# AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms

## Quick Facts
- arXiv ID: 2508.19004
- Source URL: https://arxiv.org/abs/2508.19004
- Authors: Pontus Strimling; Simon Karlsson; Irina Vartanova; Kimmo Eriksson
- Reference count: 5
- Key result: GPT-4.5 exceeded 100% of individual humans in accuracy; newer models like Gemini 2.5 Pro outperformed 98.7% of humans

## Executive Summary
This study demonstrates that AI language models can predict human social norms with accuracy exceeding most individual humans. Across two studies testing 555 everyday scenarios, GPT-4.5 and newer models like GPT-5 and Gemini 2.5 Pro consistently outperformed human participants in predicting social appropriateness ratings. The models showed strong correlations with collective human judgments (R² = 0.82-0.91) while displaying systematic, correlated errors that suggest boundaries of pattern-based social understanding. These findings support theories that cultural knowledge can emerge from statistical learning over linguistic data alone.

## Method Summary
The study evaluated AI predictions against human judgments for 555 everyday behavior-situation scenarios (e.g., "laugh at a job interview"). Human normative data came from 555 U.S. participants who rated 50 random scenarios each (~40 ratings per scenario). AI models were queried 5 times per scenario with structured prompts asking for 0-9 ratings of social appropriateness. Predictions were averaged and compared to human consensus using R² correlation and Mean Absolute Error (MAE). The study tested multiple models including GPT-4.5, GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro with specific temperature and parameter settings.

## Key Results
- GPT-4.5 exceeded 100% of individual humans in accuracy
- Gemini 2.5 Pro outperformed 98.7% of humans
- All models showed strong correlations with collective human judgments (R² = 0.82-0.91)
- Models displayed systematic, correlated errors across architectures
- Statistical learning from text corpora produced models of social norms that predict collective human judgment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical learning over text corpora can produce models of social norms that predict collective human judgment
- Mechanism: Distributional patterns in language encode regularities about socially appropriate behavior; models trained to predict next tokens acquire implicit representations of these regularities through exposure to large-scale linguistic data containing discussions, narratives, and evaluations of social situations
- Core assumption: Language contains sufficient information about social norms for a pattern-extraction system to reconstruct normative consensus without direct experience
- Evidence anchors:
  - [abstract] "sophisticated models of social cognition can emerge from statistical learning over linguistic data alone"
  - [section] References to Harris (1954), Landauer & Dumais (1997) for distributional approaches; Elman (1990), Saffran et al. (1996) for statistical learning
  - [corpus] Limited direct corpus support; related work on normative reasoning (arxiv 2602.02975) addresses LLM norm evaluation but not acquisition mechanisms
- Break condition: If norm knowledge required sensorimotor grounding, models without embodied experience should systematically fail; current evidence shows high accuracy but systematic correlated errors suggesting partial but incomplete acquisition

### Mechanism 2
- Claim: Aggregating over diverse linguistic sources produces estimates closer to cultural consensus than individual human judgments
- Mechanism: Individual humans sample from a subset of cultural experience; models aggregate across training data representing many perspectives, effectively computing a statistical approximation to cultural consensus
- Core assumption: The training distribution sufficiently covers the normative judgments of the target population
- Evidence anchors:
  - [abstract] "language serves as a remarkably rich repository for cultural knowledge transmission"
  - [section] "the AI's superior ability to predict consensus, despite lacking the varied individual experiences that shape human judgment, implies that collective cultural knowledge may be more structured and accessible than the noise of individual variation might suggest"
  - [corpus] No direct corpus evidence on aggregation-to-consensus mechanism
- Break condition: Model predictions should degrade when cultural context shifts; U.S.-specific norms were tested, cross-cultural generalization remains unverified

### Mechanism 3
- Claim: Systematic correlated errors across architectures reveal shared boundaries of pattern-based social understanding
- Mechanism: All models trained on similar text distributions share blind spots where linguistic patterns misrepresent social reality (e.g., media over-representation of kissing in theaters) or where context-dependent exceptions require experiential knowledge
- Core assumption: Error correlation reflects shared data limitations rather than coincidental architecture-specific failures
- Evidence anchors:
  - [abstract] "all models showed systematic, correlated errors"
  - [section] Table 4 shows error correlations of 0.29–0.60 across model pairs; discussion identifies semantic ambiguity, training data bias, and context-dependent valence shifts as error sources
  - [corpus] Limited corpus support; SocialNav (arxiv 2511.21135) notes embodied navigation norms remain challenging
- Break condition: If boundaries were purely data-driven, models trained on different corpora should show uncorrelated errors; current evidence shows moderate correlation, suggesting both shared data limitations and architecture-specific factors

## Foundational Learning

- Concept: **Statistical learning**
  - Why needed here: The paper's central claim depends on understanding how regularities can be extracted from environmental input without explicit instruction
  - Quick check question: Can you explain how an infant extracts word boundaries from continuous speech without being taught?

- Concept: **Cultural consensus vs. individual variation**
  - Why needed here: The methodology compares AI predictions not to "correct" answers but to group averages, treating individual humans as noisy estimators of shared norms
  - Quick check question: Why would comparing AI to aggregate human judgment be more appropriate than comparing to any single human's judgment?

- Concept: **Distributional semantics**
  - Why needed here: Understanding how meaning emerges from co-occurrence patterns explains why text-only training can yield social knowledge
  - Quick check question: How might the word "church" acquire different associations from its co-occurrence with "wedding" versus "funeral"?

## Architecture Onboarding

- Component map:
  - Prompt engineering -> Multi-query aggregation -> Scoring comparison

- Critical path:
  1. Define scenarios (behavior × situation pairs)
  2. Query model with culturally-anchored prompt
  3. Aggregate multiple stochastic responses
  4. Compare predictions to human consensus distribution

- Design tradeoffs:
  - Temperature: Lower values increase consistency but may mask model uncertainty; higher values capture variation but add noise
  - Prompt specificity: U.S. cultural framing enables targeted prediction but reduces cross-cultural applicability
  - Continuous vs. integer scale: Continuous predictions leverage model precision but integer comparisons match human scale

- Failure signatures:
  - Quantized responses (Claude Sonnet 4 clustering at specific values like 1.25, 3.25, 7.85) suggest limited numerical calibration
  - Direction-consistent errors across models on specific scenarios indicate data-driven blind spots
  - High R² with high MAE suggests correct relative ordering but poor absolute calibration

- First 3 experiments:
  1. Replicate with temperature sweep (0.0, 0.25, 0.5, 0.75, 1.0) to characterize consistency-variation tradeoff
  2. Test cross-cultural transfer by changing prompt to specify different countries (e.g., Japan, Brazil) without changing model
  3. Analyze error correlation between models on held-out scenarios to distinguish data limitations from architecture-specific failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the high predictive accuracy of LLMs for social norms generalize to culturally distant or non-Western contexts?
- Basis in paper: [explicit] The authors state that "systematic cross-cultural investigations are needed to test the generalizability of these computational mechanisms across different cultural contexts."
- Why unresolved: The study was restricted to U.S. participants, while global cultures exhibit significant variation in everyday norms (e.g., "tight" vs. "loose" cultures)
- What evidence would resolve it: Replicating the methodology with human normative data from diverse non-Western populations

### Open Question 2
- Question: How does AI performance compare to humans when both are performing the identical meta-cognitive task of predicting group averages?
- Basis in paper: [explicit] The authors suggest "future research could directly compare AI and human performance when both are given the identical meta-cognitive task of predicting group averages."
- Why unresolved: The current study compared AI predictions (explicit estimation) to human subjective ratings (personal judgment), which are theoretically distinct cognitive processes
- What evidence would resolve it: A study design where human participants are instructed to predict the consensus of others rather than providing their own subjective rating

### Open Question 3
- Question: Do correlation (structural understanding) and calibration (numerical precision) rely on distinct computational mechanisms in LLMs?
- Basis in paper: [inferred] The authors note a dissociation where GPT-5 achieved the highest correlation but GPT-4.5 maintained the lowest error rate, suggesting these are "distinct and dissociable computational capabilities."
- Why unresolved: Without access to model internals or training data, it is unclear why different architectures or training runs favor one metric over the other
- What evidence would resolve it: Mechanistic interpretability analysis or controlled training experiments that isolate factors driving R² versus Mean Absolute Error (MAE)

## Limitations

- Temporal Model Access: GPT-4.5 and GPT-5 are no longer available for replication
- Cultural Specificity: The study tests U.S. cultural norms only; generalization to other cultural contexts remains unverified
- Embodied Knowledge Gap: Systematic correlated errors suggest limitations in contexts requiring experiential or embodied understanding

## Confidence

**High Confidence**: Models can predict social norms with accuracy exceeding most individual humans; statistical learning from text enables cultural knowledge acquisition; correlated errors across architectures indicate shared limitations

**Medium Confidence**: Language alone contains sufficient information for social norm acquisition; aggregated linguistic data approximates cultural consensus better than individual judgment; error patterns reflect training data biases

**Low Confidence**: Specific error mechanisms (e.g., media over-representation causing kissing-in-theaters overestimation); exact generalizability across cultures; whether current limitations are fundamental or solvable with better training approaches

## Next Checks

1. **Cross-cultural Transfer Test**: Repeat the study using the same models but prompt them to predict norms for different countries (e.g., Japan, Brazil, India). Compare accuracy degradation to identify cultural specificity boundaries

2. **Error Correlation Analysis**: Systematically analyze which scenarios show the highest error correlation across model pairs. Determine if these cluster around specific types of social situations (embodied interactions, cultural-specific practices, context-dependent exceptions) to better characterize knowledge boundaries

3. **Temperature Sensitivity Sweep**: Replicate the study across a range of temperature settings (0.0, 0.25, 0.5, 0.75, 1.0) for each model to quantify the consistency-variation tradeoff and assess whether high accuracy stems from averaging stochastic responses or genuine calibration