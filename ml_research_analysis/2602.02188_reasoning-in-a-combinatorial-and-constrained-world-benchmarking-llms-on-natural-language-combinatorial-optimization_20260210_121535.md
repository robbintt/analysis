---
ver: rpa2
title: 'Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language
  Combinatorial Optimization'
arxiv_id: '2602.02188'
source_url: https://arxiv.org/abs/2602.02188
tags:
- problem
- each
- reasoning
- instance
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NLCO, the first large-scale benchmark for
  evaluating large language models (LLMs) on natural-language combinatorial optimization
  (CO) tasks. NLCO features 43 CO problems organized under a four-layer taxonomy (variable
  type, constraint family, global pattern, and objective class) and requires LLMs
  to produce feasible, optimal solutions directly from textual descriptions without
  external solvers.
---

# Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization

## Quick Facts
- arXiv ID: 2602.02188
- Source URL: https://arxiv.org/abs/2602.02188
- Reference count: 40
- Primary result: Introduces NLCO, the first large-scale benchmark for evaluating LLMs on natural-language combinatorial optimization tasks across 43 problems and three difficulty levels.

## Executive Summary
This paper introduces NLCO, the first large-scale benchmark for evaluating large language models (LLMs) on natural-language combinatorial optimization (CO) tasks. NLCO features 43 CO problems organized under a four-layer taxonomy (variable type, constraint family, global pattern, and objective class) and requires LLMs to produce feasible, optimal solutions directly from textual descriptions without external solvers. Experiments on 43 tasks across three difficulty levels show that top-performing models achieve high feasibility and optimality on small instances, but both metrics degrade significantly as problem size increases, even with more tokens used for reasoning. The study reveals systematic performance differences across problem structures: set-based tasks are easier, while graph-structured problems and bottleneck objectives pose greater challenges. Quality-efficiency trade-offs are observed, with extra tokens improving feasibility more than optimality.

## Method Summary
NLCO benchmarks LLMs on natural-language combinatorial optimization by requiring models to output feasible, optimal discrete solutions from textual descriptions without external solvers. The dataset consists of 43 CO tasks (e.g., TSP, CVRP, Bin Packing) across three difficulty tiers (S, M, L), totaling 6,450 instances sourced from standard libraries and contextualized via an LLM-based generate-filter-verify pipeline. Evaluation uses pattern-specific feasibility validation against solver-annotated ground truth, measuring Average Feasibility Rate (AFR), Accuracy (Acc.), Average Log Optimality Gap (ALOG), and token usage. Standard models use "think step-by-step" prompts while reasoning models use native reasoning modes.

## Key Results
- SET-type variable domains yield higher feasibility and optimality than GRAPH-type domains under end-to-end LLM reasoning
- Bottleneck objectives (min–max / max–min) degrade optimality more severely than linear or quadratic objectives
- Additional inference tokens improve feasibility more than optimality for combinatorial problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SET-type variable domains yield higher feasibility and optimality than GRAPH-type domains under end-to-end LLM reasoning.
- Mechanism: Set-based tasks involve locally enumerable choices that admit incremental verification, whereas GRAPH tasks require maintaining global relational consistency where a single violation invalidates the entire solution.
- Core assumption: LLM reasoning more reliably satisfies constraints that can be checked incrementally versus those requiring global coherence verification.
- Evidence anchors:
  - [abstract] "set-based tasks are relatively easy, whereas graph-structured problems... lead to more frequent failures"
  - [section 4.3] Figure 2(a) shows SET achieves higher AFR/Acc. and lower ALOG than GRAPH across all models
  - [corpus] Related work (ACCORD) confirms LLMs struggle with global constraint satisfaction in NP-hard tasks
- Break condition: If GRAPH tasks were reformulated with explicit intermediate verification steps or decomposed into subproblems, the gap might narrow.

### Mechanism 2
- Claim: Bottleneck objectives degrade optimality more severely than linear or quadratic objectives under LLM reasoning.
- Mechanism: Bottleneck objectives depend on a single worst-case component, meaning local errors or incomplete global verification disproportionately harm objective value.
- Core assumption: LLMs lack explicit mechanisms for worst-case reasoning over combinatorial structures.
- Evidence anchors:
  - [abstract] "bottleneck objectives pose greater challenges"
  - [section 4.3] Figure 2(b) shows BOTTLENECK achieves substantially lower Acc. (e.g., GPT-5.1: 56%) vs. LINEAR/QUADRATIC (77%/83%)
  - [corpus] Hard constraints work (FALCON) focuses on feasibility, not optimality, suggesting global objective reasoning remains underexplored
- Break condition: If models were trained or prompted to explicitly identify and optimize the bottleneck element, or if decomposition strategies isolated worst-case components, performance could improve.

### Mechanism 3
- Claim: Additional inference tokens improve feasibility more than optimality for combinatorial problems.
- Mechanism: Extra tokens enable extended self-checking and constraint satisfaction reasoning, which helps produce valid solutions, but do not reliably improve objective optimization.
- Core assumption: Feasibility verification is more amenable to token-based reasoning than objective improvement, which may require structured search or decomposition.
- Evidence anchors:
  - [abstract] "even with more tokens used for reasoning... both metrics degrade significantly as problem size increases"
  - [section 4.4] Figure 4 shows AFR rises rapidly with token budget then saturates, while Acc

## Foundational Learning
Unknown: The report does not specify what foundational learning techniques were employed. The paper appears to focus on benchmarking rather than training new models, so foundational learning specifics are not detailed.

## Architecture Onboarding
Assumption: The report does not explicitly describe architecture onboarding processes. Given that this is a benchmark paper rather than an architecture development paper, onboarding details are likely not the primary focus.

## Open Questions the Paper Calls Out
- How to improve optimality performance on bottleneck objectives
- Whether decomposition strategies could improve GRAPH task performance
- How to better utilize additional inference tokens for objective optimization rather than just feasibility
- The scalability limits of end-to-end LLM reasoning for combinatorial optimization

## Limitations
- LLM performance degrades significantly as problem size increases, even with more reasoning tokens
- GRAPH-structured problems show systematically worse performance than SET-structured problems
- Bottleneck objectives prove particularly challenging for achieving high optimality
- Additional tokens improve feasibility more than optimality, indicating limitations in optimization reasoning
- The benchmark relies on solver-generated ground truth, which may not capture all valid solutions

## Confidence
High confidence in the benchmark methodology and results, as the paper provides comprehensive evaluation across 43 tasks, three difficulty levels, and multiple metrics. The findings are well-supported by empirical evidence and systematic analysis of performance patterns.

## Next Checks
- Investigate decomposition strategies for GRAPH tasks
- Explore specialized prompting for bottleneck objective optimization
- Examine token allocation strategies for balancing feasibility and optimality
- Study the impact of different reasoning model architectures on combinatorial optimization performance