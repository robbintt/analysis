---
ver: rpa2
title: Agentifying Agentic AI
arxiv_id: '2511.17332'
source_url: https://arxiv.org/abs/2511.17332
tags:
- agents
- systems
- agentic
- agent
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper highlights a critical gap in current agentic AI systems:\
  \ while they promise autonomy and adaptability, they often lack the explicit, structured\
  \ models of reasoning, communication, and coordination that have long been studied\
  \ in the AAMAS community. The authors argue that integrating AAMAS concepts\u2014\
  such as BDI architectures, formal communication protocols, mechanism design, norms,\
  \ and theory of mind\u2014into agentic AI can provide the conceptual scaffolding\
  \ needed to ensure these systems are not only capable but also transparent, cooperative,\
  \ and accountable."
---

# Agentifying Agentic AI

## Quick Facts
- arXiv ID: 2511.17332
- Source URL: https://arxiv.org/abs/2511.17332
- Authors: Virginia Dignum; Frank Dignum
- Reference count: 5
- Primary result: Proposes integrating AAMAS frameworks (BDI, norms, formal communication) into agentic AI to enable verifiable autonomy, coordination, and accountability

## Executive Summary
This conceptual paper identifies a critical gap in current agentic AI systems: while they promise autonomy and adaptability, they often lack the explicit, structured models of reasoning, communication, and coordination that have long been studied in the AAMAS community. The authors argue that integrating AAMAS concepts—such as BDI architectures, formal communication protocols, mechanism design, norms, and theory of mind—into agentic AI can provide the conceptual scaffolding needed to ensure these systems are not only capable but also transparent, cooperative, and accountable. The paper does not report empirical results or metrics, but it offers a comprehensive conceptual comparison and outlines how adopting AAMAS frameworks could address current limitations in grounding, reliability, and governance of agentic AI.

## Method Summary
This is a position paper that conceptually compares traditional Multi-Agent Systems (AAMAS) with current "Agentic AI" approaches. The authors synthesize insights from AAMAS research to argue that learning-based mechanisms must be complemented by structured reasoning and coordination models to achieve responsible agency. They propose hybrid architectures where LLMs serve as execution modules within traditional agent frameworks, suggest reintroducing formal communication protocols to prevent miscoordination, and advocate for explicit models of norms and social interactions. No empirical methods or experiments are reported.

## Key Results
- Identifies that current agentic AI lacks explicit models of reasoning, coordination, and governance found in traditional AAMAS
- Argues that behavioral autonomy without structured frameworks leads to unreliable, ungrounded, and unaccountable systems
- Proposes hybrid architectures combining LLM flexibility with AAMAS verifiability and transparency

## Why This Works (Mechanism)

### Mechanism 1: Explicit Cognitive Architectures for Verifiable Autonomy
- Claim: Structured architectures (e.g., BDI) can provide interpretable goal-driven behavior that is auditable at each reasoning step.
- Mechanism: By separating beliefs (world state), desires (goals), and intentions (committed goals), the system maintains a transparent mapping between perception, deliberation, and action. This enables explicit reasoning about goal feasibility and abandonment conditions.
- Core assumption: Internal state representations are necessary for predictable, verifiable agent behavior; emergent goal-consistency from foundation models alone is unreliable.
- Evidence anchors:
  - [abstract]: "concepts such as reasoning architectures, formal interaction protocols, norms, and institutional governance provide the conceptual tools needed to turn behavioural autonomy into responsible agency"
  - [section 4, Explicit Architecture]: "This tripartite structure provides a transparent and verifiable mapping between perception, deliberation, and action, supporting predictable, explainable, and goal-consistent behaviour."
  - [corpus]: Neighbor paper "Alignment, Agency and Autonomy in Frontier AI" discusses the definitional confusion around these terms across disciplines, reinforcing the need for explicit models.
- Break condition: If tasks require creative, open-ended exploration where predefined goal structures cannot capture success criteria, rigid architectures may constrain adaptability.

### Mechanism 2: Structured Communication Protocols with Formal Semantics
- Claim: Formal communication languages (e.g., FIPA-ACL, KQML) can prevent miscoordination by ensuring shared understanding of message intent.
- Mechanism: Define not just syntax but semantics of communicative acts (requests, commitments, promises). Agents interpret messages based on formal pragmatics rather than statistical inference, enabling reliable multi-agent coordination.
- Core assumption: Natural language communication without formal semantics introduces ambiguity that compounds in multi-agent chains; structured protocols reduce this risk.
- Evidence anchors:
  - [section 4, Communication Protocols]: "E.g. suppose I know that someone is arriving tomorrow at university and I have reasons to try and avoid them... In traditional agent systems one can distinguish these two situations by sending different types of requests for information."
  - [section 4, Communication Protocols]: "The natural language communication of agentic AI does not use this strict semantic approach and can thus lead to misunderstanding, inconsistent commitments, or spurious alignment"
  - [corpus]: Weak direct corpus evidence on communication protocol evaluation; this remains a conceptual claim.
- Break condition: In environments with humans-in-the-loop who cannot use formal protocols, or where interaction partners are heterogeneous and unknown, rigid protocols may be impractical.

### Mechanism 3: Hybrid Architectures with LLMs as Execution Modules
- Claim: Positioning LLMs as execution components within structured agent frameworks can combine flexibility with verifiability.
- Mechanism: Traditional agent architecture handles goal decomposition, planning, and verification. LLMs handle practical execution of complex sub-actions where natural language flexibility is beneficial. The agent verifies outputs against expected results.
- Core assumption: Modular decomposition is possible—tasks can be separated into verifiable planning/monitoring (structured) and flexible execution (LLM).
- Evidence anchors:
  - [section 1, Introduction]: "Therefore the way to combine agents and Agentic AI might be to start with traditional agents and then fill in specific parts by LLMs. E.g. letting the LLM take care of practicalities of executing a complex action. The agent can check that the right (expected) result is achieved"
  - [section 4, Multi-Agent Planning]: "If the flight agent needs to extend the stay with one day... Then the hotel agent should extend the stay... All of this should also be explained to the user and/or permission asked"
  - [corpus]: "Agentic UAVs" paper similarly proposes integrated architectures combining LLMs with structured reasoning modules.
- Break condition: If verification criteria cannot be formalized (e.g., creative writing, nuanced social negotiation), the monitoring agent cannot reliably assess LLM outputs.

## Foundational Learning

- Concept: Belief-Desire-Intention (BDI) Architecture
  - Why needed here: The paper positions BDI as the core alternative to emergent agency; understanding its components is prerequisite to implementing structured reasoning.
  - Quick check question: Can you explain how intentions differ from desires, and when an agent should abandon an intention?

- Concept: Speech Act Theory and Agent Communication Languages
  - Why needed here: The paper argues current agentic AI lacks formal communication semantics; understanding speech acts (request, inform, promise) is necessary for designing coordination protocols.
  - Quick check question: What is the difference between a message's syntax and its pragmatic effect on the receiver's mental state?

- Concept: Normative Multi-Agent Systems
  - Why needed here: The paper emphasizes that norms (obligations, permissions, prohibitions) are not simple constraints but context-sensitive rules that enable exception handling.
  - Quick check question: How should an agent reason about violating a norm (e.g., speeding) when a higher-order value (saving a life) is at stake?

## Architecture Onboarding

- Component map:
  - Cognitive Core: BDI-style module with explicit belief base, goal queue, intention stack
  - LLM Execution Layer: Handles natural language tasks, tool invocations, unstructured reasoning
  - Communication Interface: Protocol-compliant message handler (can wrap LLM outputs)
  - Normative Engine: Role definitions, obligation tracking, violation detection
  - Verification Monitor: Checks LLM outputs against expected result schemas

- Critical path:
  1. Define explicit goal representation and success criteria
  2. Implement structured planning with verifiable sub-goals
  3. Add LLM execution modules for flexible sub-action handling
  4. Build verification hooks that compare outputs to expectations
  5. Layer communication protocols for multi-agent coordination

- Design tradeoffs:
  - Specialized vs. general-purpose agents: Specialized agents are more efficient but require more coordination overhead
  - Verification strictness vs. flexibility: Tighter verification reduces errors but constrains LLM creativity
  - Protocol formality vs. human usability: Formal protocols improve reliability but may exclude human participants

- Failure signatures:
  - Cascading errors: Single miscommunication propagates through multi-agent chain (detected late)
  - Goal drift: LLM produces plausible outputs that subtly deviate from committed intentions
  - Norm blindness: System cannot distinguish acceptable violations from genuine misconduct
  - Statelessness: No memory of past interactions leads to inconsistent behavior over time

- First 3 experiments:
  1. Single-agent booking task: Compare pure LLM vs. BDI-wrapped LLM on flight booking with explicit constraints (verify goal-consistency and recovery from failures)
  2. Multi-agent coordination test: Two agents booking travel components (flight + hotel) with shared constraints; measure miscoordination incidents with and without formal communication protocols
  3. Norm violation scenario: Present edge case (e.g., urgent medical appointment conflicts with calendar norm); evaluate whether system can reason about acceptable violations vs. pure constraint-following behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can structured reasoning models (such as BDI architectures) be effectively integrated into Agentic AI to enable verifiable, goal-driven behavior?
- Basis in paper: [explicit] The authors argue that "learning-based mechanisms must be complemented by structured reasoning and coordination models" to achieve responsible agency.
- Why unresolved: Current Agentic AI systems operate without explicit internal state representations, making their "intentionality" implicit and limiting structural guarantees for alignment.
- What evidence would resolve it: A hybrid architecture where an agent's high-level goals are formally defined and verifiable, even if low-level actions are executed by LLMs.

### Open Question 2
- Question: What standardized benchmarks are required to effectively evaluate the autonomy, cooperation, and social compliance of Agentic systems?
- Basis in paper: [explicit] The paper explicitly states that "future research should develop standardized benchmarks for autonomy, cooperation, and social compliance."
- Why unresolved: Current evaluations are fragmented, focusing on narrow tasks or short interactions, making it difficult to determine if an agent is genuinely aligned with intended goals.
- What evidence would resolve it: A widely adopted evaluation suite that successfully measures performance in sustained, multi-agent coordination tasks against safety standards.

### Open Question 3
- Question: Can formal communication protocols (e.g., FIPA-ACL) reduce misalignment errors in multi-agent Agentic systems compared to unstructured natural language?
- Basis in paper: [explicit] The authors note that current natural language exchanges "lack guaranteed semantics" and suggest "reintroducing formal semantics... could therefore improve reliability."
- Why unresolved: Current Agentic AI relies on statistical inference for communication, which creates risks of misunderstanding, inconsistent commitments, and spurious alignment.
- What evidence would resolve it: Empirical results showing lower rates of coordination failure in complex tasks when agents utilize structured communication protocols.

## Limitations

- The paper is entirely conceptual with no empirical validation of the proposed hybrid architectures
- No specific implementation details are provided for integrating LLMs with structured reasoning frameworks
- The communication protocol claims lack strong empirical grounding in multi-agent coordination contexts
- Scalability challenges for formal protocols in large, heterogeneous agent environments are not addressed

## Confidence

- **High confidence:** The conceptual distinction between behavioral autonomy and relational agency is well-founded; the identification of current agentic AI limitations (lack of explicit reasoning, coordination mechanisms) is accurate based on observable system behaviors.
- **Medium confidence:** The proposed mechanisms (BDI integration, formal communication protocols, hybrid architectures) are theoretically sound but untested in practice. The claim that these would improve transparency and accountability is reasonable but unverified.
- **Low confidence:** Specific implementation details for integrating LLMs with structured architectures are underspecified; the paper does not address how to handle edge cases where verification criteria cannot be formalized or where human participants cannot use formal protocols.

## Next Checks

1. Implement the proposed hybrid architecture (BDI core + LLM execution module) and test on the travel booking scenario with explicit constraints, measuring constraint satisfaction rates and verification time versus pure LLM approaches.
2. Design a multi-agent coordination task where agents must book interdependent components (flight + hotel + rental car) with shared constraints, comparing miscoordination incidents between formal protocol communication and natural language-only communication.
3. Create norm violation scenarios (e.g., conflicting appointments, urgent medical situations) to test whether the system can reason about acceptable violations versus pure constraint-following behavior, measuring decision quality and justification transparency.