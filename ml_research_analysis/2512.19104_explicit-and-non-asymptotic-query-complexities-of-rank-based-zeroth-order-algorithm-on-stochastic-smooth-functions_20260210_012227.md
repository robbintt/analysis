---
ver: rpa2
title: Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm
  on Stochastic Smooth Functions
arxiv_id: '2512.19104'
source_url: https://arxiv.org/abs/2512.19104
tags:
- lemma
- stochastic
- obtain
- holds
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses zeroth-order optimization using only ordinal
  feedback in stochastic settings. The key idea is a rank-based algorithm that selects
  descent directions from ordered function evaluations without constructing costly
  comparison graphs.
---

# Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions

## Quick Facts
- arXiv ID: 2512.19104
- Source URL: https://arxiv.org/abs/2512.19104
- Reference count: 7
- Achieves optimal query complexities matching value-based approaches using only ordinal feedback in zeroth-order optimization.

## Executive Summary
This paper introduces a rank-based zeroth-order optimization algorithm that achieves optimal query complexities using only ordinal feedback. The method constructs descent directions by aggregating the top and bottom quartiles of random Gaussian perturbations sorted by noisy function values, avoiding the expensive comparison graphs of prior rank-based approaches. Under smoothness, strong convexity, and bounded variance assumptions, the algorithm achieves query complexities of O(dLG_u^2/μ^2ε) for convex objectives and O(dLG_u^2/ε^2) for finding ε-stationary points in non-convex cases. These are the first explicit non-asymptotic query complexity bounds for rank-based ZO over stochastic functions, demonstrating that ordinal information alone is sufficient for optimal efficiency.

## Method Summary
The algorithm generates N Gaussian vectors, queries a rank oracle to sort perturbed points based on noisy evaluations, and constructs descent directions by averaging the top and bottom quartiles of these vectors with appropriate weights. The update step uses a diminishing step size schedule. The method requires tuning of smoothing parameter α and sample size N, which theoretically depend on problem-specific constants (G_ℓ, L, μ) but must be treated as hyperparameters in practice.

## Key Results
- Achieves O(dLG_u^2/μ^2ε) query complexity for convex objectives with rank feedback
- Achieves O(dLG_u^2/ε^2) query complexity for non-convex stationary point finding
- Matches optimal query complexities of value-based zeroth-order methods
- Requires only O(dN) computation per iteration versus O(dN^2) for graph-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Gradient Approximation via Truncated Quadratic Statistics
- Claim: A descent direction can be constructed by averaging the top and bottom quartiles of random directions sorted by noisy function values.
- Mechanism: The algorithm samples N Gaussian vectors u_i. Due to the linear approximation f(x+αu) ≈ f(x) + α⟨u,∇f⟩, vectors yielding the lowest function values (u_(k) for k ≤ N/4) correlate with the negative gradient, while those yielding the highest values correlate with the positive gradient. By assigning positive weights to the lowest 25% and negative weights to the highest 25%, the update d_t aggregates these directional signals.
- Core assumption: The perturbation size α is sufficiently small such that second-order smoothness terms (Lα^2) do not dominate the linear gradient signal.
- Break condition: If the noise variance ξ or perturbation α is too large relative to the gradient norm, the ranking becomes uncorrelated with the true gradient direction.

### Mechanism 2: Convergence via High-Probability "Good Events"
- Claim: Convergence is guaranteed by conditioning descent on a set of high-probability events (bounded error, norm caps, and order statistic separation) rather than enforcing strict descent every step.
- Mechanism: The analysis defines events E_{t,1} through E_{t,5}. For instance, E_{t,5} ensures that the inner products ⟨z_t, u⟩ for the top/bottom quartiles are sufficiently separated from zero (≥ 2 or ≤ -2). By union-bounding the failure probabilities of these events, the paper proves the expected descent holds with probability ≥ 1 - δ.
- Core assumption: Stochastic gradients have a bounded second moment (G_u) and the lower bound on gradient norm (G_ℓ) allows setting a valid α.
- Break condition: If the dimension d grows significantly faster than the sample size N, the probability of event E_{t,5} may degrade.

### Mechanism 3: Query Efficiency via Lightweight Aggregation
- Claim: The algorithm achieves optimal query complexity (O(d/ε^2)) with O(dN) per-iteration computation, avoiding the O(dN^2) cost of graph-based approaches.
- Mechanism: Unlike prior work that constructs a Directed Acyclic Graph of O(N^2) edges to model preferences, this method relies on a simple linear scan and sort of N samples. The complexity savings come from replacing pairwise preference modeling with global rank-based weighting.
- Core assumption: The Rank Oracle can provide a total ordering of the N samples.
- Break condition: If the oracle only provides partial or pairwise comparisons rather than a full ranking of N points, the sorting mechanism cannot be directly applied.

## Foundational Learning

- Concept: **Zeroth-Order (Gradient-Free) Optimization**
  - Why needed here: The entire paper assumes the gradient ∇f(x) is unobservable. You must understand that we approximate gradients by probing the function at x + αu.
  - Quick check question: How does increasing the smoothing parameter α affect the bias vs. variance trade-off in a standard Gaussian smoothing gradient estimator?

- Concept: **Order Statistics of Gaussian Vectors**
  - Why needed here: The algorithm's core logic relies on the statistical properties of the top/bottom 25% of random Gaussian projections.
  - Quick check question: Why does the paper require the separation condition ⟨z, u⟩ ≥ 2 (Event E_{t,5}) rather than just ≥ 0?

- Concept: **Strong Convexity and Smoothness**
  - Why needed here: The query complexity bounds (O(1/ε) vs O(1/ε^2)) are derived directly from these geometric properties.
  - Quick check question: If the function is L-smooth but non-convex, what quantity does the algorithm minimize, and why can't it guarantee finding the global minimum?

## Architecture Onboarding

- Component map: Sampler -> Rank Oracle -> Aggregator -> Updater
- Critical path: The Rank Oracle is the bottleneck. The algorithm is designed to minimize the number of calls to this oracle.
- Design tradeoffs:
  - Sample Size (N): Larger N reduces iteration count T but increases queries per step (Q = TN). Theorem 1 suggests N depends on K(1/4 || p), effectively requiring sufficient samples to ensure statistical separation.
  - Step Size (η): Theorem 1 uses η = 1/(2μt) (diminishing) for convex cases, while Theorem 2 uses constant η for non-convex. Tuning η is critical; the theoretical values depend on G_u and L which may be unknown.
- Failure signatures:
  - Divergence: α is set too high, causing the ranking to reflect quadratic curvature rather than gradient direction.
  - Stagnation: Noise variance is significantly higher than the gradient magnitude, causing rankings to be random.
  - Instability: G_ℓ assumption fails near optima, making α selection unstable.
- First 3 experiments:
  1. Quadratic Benchmark: Implement Algorithm 1 on f(x) = x^T A x + b. Verify that with N set per Corollary 3, the error decays as O(1/T).
  2. Sensitivity to α: Run the algorithm on a noisy convex function. Sweep α from 10^-5 to 10^-1. Observe the performance collapse when α violates the condition α ≤ G_ℓ/(2C_{d,δ}L).
  3. Non-convex Stationarity: Apply to a synthetic non-convex function (e.g., Rosenbrock). Measure (1/T)∑||∇f(x_t)||^2 to verify convergence to a stationary point.

## Open Questions the Paper Calls Out

- Can the requirement for a strictly positive lower bound on the stochastic gradient norm (Assumption 4) be relaxed or removed?
  - Basis: Remark 1 states that Assumption 4 is "not common" and is primarily used to set the smooth parameter α, acknowledging it as a limitation.
  - Why unresolved: The author notes that while the assumption ensures α is well-chosen so convergence rates do not explicitly depend on it, the assumption may not hold in all standard stochastic gradient descent settings.
  - What evidence would resolve it: A convergence proof that utilizes an adaptive smoothing parameter α or a different analysis technique that does not rely on ||∇f(x;ξ)|| ≥ G_ℓ.

- What are the precise trade-offs between feedback richness (e.g., full rankings vs. pairwise comparisons) and query complexity?
  - Basis: Page 3 states, "the precise trade-offs between feedback richness, query complexity, and convergence rates are not well characterized."
  - Why unresolved: The paper establishes that ordinal information is sufficient for optimal efficiency, but does not quantify how the "richness" of that ordinal data changes the constants or rates compared to value-based or binary feedback.
  - What evidence would resolve it: A comparative analysis deriving lower bounds for different levels of ordinal feedback granularity within the same stochastic framework.

- Do the non-asymptotic query complexities hold for low-dimensional settings without the d ≥ log(40NT) constraint?
  - Basis: Corollaries 3 and 4 require the assumption that "the dimension is sufficiently large that d ≥ log(40NT)" to derive the final query bounds.
  - Why unresolved: The analysis relies on concentration bounds that may behave differently or require different constants in low dimensions.
  - What evidence would resolve it: Extending the proof technique to handle the concentration of Gaussian vectors in low-dimensional spaces, or providing counter-examples showing different scaling.

## Limitations

- Dependence on problem-specific constants (G_ℓ, L, μ) that are rarely known in practice and must be treated as hyperparameters
- Convergence analysis assumes bounded gradient variance and lower bound on gradient norm, which may not hold near saddle points
- Performance in high-dimensional settings (d >> N) remains uncertain due to potential degradation of order statistic separation probability

## Confidence

- High Confidence: The theoretical derivation of query complexity bounds is rigorous and well-supported by the analysis
- Medium Confidence: The practical effectiveness depends heavily on proper hyperparameter tuning, which is not fully explored
- Low Confidence: The algorithm's behavior in non-smooth or non-convex functions beyond theoretical assumptions is not characterized

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically evaluate the algorithm's performance across a grid of α and N values on benchmark functions. Identify ranges where the method fails and compare against theoretical bounds.

2. **High-Dimensional Scaling**: Test the algorithm on problems where dimension d is significantly larger than sample size N (e.g., d = 1000, N = 50). Measure convergence rate and compare to theoretical predictions.

3. **Non-Convex Landscape Analysis**: Apply the algorithm to various non-convex functions with multiple local minima and saddle points. Measure convergence to stationary points and compare to gradient-based methods.