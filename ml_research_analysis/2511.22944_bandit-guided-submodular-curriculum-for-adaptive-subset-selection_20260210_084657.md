---
ver: rpa2
title: Bandit Guided Submodular Curriculum for Adaptive Subset Selection
arxiv_id: '2511.22944'
source_url: https://arxiv.org/abs/2511.22944
tags:
- training
- submodular
- selection
- subset
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel bandit-guided submodular curriculum
  for adaptive subset selection in machine learning. The authors formulate adaptive
  subset selection as a multi-armed bandit problem where each arm corresponds to a
  submodular function that guides sample selection.
---

# Bandit Guided Submodular Curriculum for Adaptive Subset Selection

## Quick Facts
- **arXiv ID**: 2511.22944
- **Source URL**: https://arxiv.org/abs/2511.22944
- **Authors**: Prateek Chanda; Prayas Agrawal; Saral Sureka; Lokesh Reddy Polu; Atharv Kshirsagar; Ganesh Ramakrishnan
- **Reference count**: 40
- **Primary result**: Novel bandit-guided submodular curriculum for adaptive subset selection achieving 3×-8× speedup with superior accuracy-efficiency tradeoffs across vision and language datasets

## Executive Summary
This paper introduces ONLINESUBMOD, a bandit-guided submodular curriculum framework for adaptive subset selection in machine learning. The approach formulates subset selection as a multi-armed bandit problem where each arm corresponds to a submodular function guiding sample selection. By dynamically selecting appropriate submodular functions based on validation-driven rewards, the method achieves no-regret performance while maintaining computational tractability. Empirical results demonstrate consistent gains in both training efficiency and model performance compared to traditional curriculum learning and bi-level optimization approaches.

## Method Summary
ONLINESUBMOD reinterprets adaptive subset selection as a multi-armed bandit problem, where each arm represents a submodular function (Facility Location, Graph Cut, Log-Determinant, Disparity variants, and mutual information variants). At each training step, a bandit controller selects an arm based on estimated rewards from validation loss reduction. The chosen submodular function then selects a subset of training samples via greedy maximization. The reward is computed as the reduction in validation loss (utility), creating feedback-driven curriculum adaptation. The method uses validation-driven reward metrics to guide curriculum scheduling, addressing the challenge of defining reliable difficulty scores for samples.

## Key Results
- ONLINESUBMOD achieves 3×-8× training speedup while maintaining or improving model accuracy
- The approach outperforms traditional curriculum learning and bi-level optimization baselines across multiple vision and language datasets
- Arm selection dynamically shifts from representation-based to diversity-based functions over training epochs, demonstrating effective curriculum adaptation
- The method adds negligible computational overhead (0.8ms for submodular maximization vs 630ms for gradient computation)

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** A multi-armed bandit formulation can dynamically select appropriate submodular functions to guide curriculum learning by optimizing validation-driven rewards. **Core assumption:** The optimal submodular function varies across training stages—early training benefits from representative samples, later stages from diverse samples. **Evidence:** Figure 5 shows arm selection distribution shifting from representation-based to diversity-based functions over training epochs. **Break condition:** If validation data is non-representative or too small, reward signals become noisy, leading to suboptimal arm selection.

### Mechanism 2
**Claim:** Utility-driven reward based on gradient influence provides an effective, tractable signal for subset quality without expensive bi-level optimization. **Core assumption:** First-order Taylor expansion of utility is sufficient approximation when learning rates are small; the Hessian can be approximated as identity without losing critical signal. **Evidence:** [Section 2.1] Derives utility decomposition; ONLINESUBMOD outperforms GREATS baseline (49.6% vs 47.8% average on MMLU tasks). **Break condition:** If learning rate is too large or loss landscape highly non-convex, first-order approximation degrades.

### Mechanism 3
**Claim:** Greedy submodular maximization provides (1-1/e)-approximation guarantees for subset selection while maintaining computational tractability. **Core assumption:** Submodular functions (representativeness, diversity) are appropriate proxies for training utility; greedy optimization is sufficient vs. exact maximization. **Evidence:** [Table 1] Lists five submodular functions; [Table 3] shows negligible runtime overhead. **Break condition:** If true objective is not well-approximated by available submodular functions, selection quality degrades.

## Foundational Learning

- **Concept: Multi-Armed Bandits (Exploration-Exploitation Trade-off)**
  - **Why needed here:** Framework must balance exploiting known-good submodular functions with exploring alternatives that may perform better at different training stages.
  - **Quick check question:** Can you explain why pure exploitation (always selecting the historically best arm) fails in non-stationary environments where optimal strategies change over time?

- **Concept: Submodular Functions and Greedy Maximization**
  - **Why needed here:** Paper's arms are submodular set functions. Understanding diminishing returns property and (1-1/e) approximation guarantees is necessary.
  - **Quick check question:** Given a set function f, what does it mean for f to be submodular, and why does greedy selection provide approximation guarantees only for monotone submodular functions?

- **Concept: Influence Functions and Gradient-Based Data Valuation**
  - **Why needed here:** Utility reward derives from influence function theory—measuring how training samples affect validation loss via gradient alignment.
  - **Quick check question:** In the utility formula Ut(Bt, zval) = ℓ(zval, θt) - ℓ(zval, θ̃t+1(Bt)), what does this quantity represent, and why is the first-order approximation necessary for tractability?

## Architecture Onboarding

- **Component map:**
  ```
  Training Loop → Batch Loader → Feature Extractor → Similarity Kernel → Bandit Controller → Submodular Optimizer → Model Updater
                                    ↓
                              Validation Batch → Reward Computer
  ```

- **Critical path:**
  1. Receive batch Bt; compute gradients/feature embeddings
  2. Bandit selects arm at based on exploration threshold and historical rewards
  3. Greedy maximization of f(at) over Bt with budget β → subset S(at)
  4. Forward-backward pass on S(at); compute validation gradients on Bval_t
  5. Update reward estimates for selected arm
  6. SGD update on model parameters

- **Design tradeoffs:**
  - Budget β: Lower β → more speedup but potential accuracy drop
  - Exploration parameters (λ, π): Higher exploration → more robust arm discovery but slower convergence
  - Validation set size: Too small → noisy rewards; too large → overhead
  - Gradient approximation: Last-layer gradients reduce cost but may miss early-layer signal
  - Warm-start epochs: Stabilizes early gradients but reduces speedup

- **Failure signatures:**
  - Arm selection converges to single arm early → exploration parameters may be too aggressive
  - Validation loss plateaus early with poor test accuracy → validation set may be unrepresentative
  - Submodular selection dominates runtime → similarity kernel computation is bottleneck
  - Performance degrades at low budgets (10%) → model capacity may require minimum data

- **First 3 experiments:**
  1. Baseline comparison on CIFAR-100 (30% budget): Implement ONLINESUBMOD vs GRADMATCH vs RANDOM selection
  2. Ablation on exploration parameters (λ, π): Grid search π ∈ {0.5, 1.0, 1.5, 2.0} with fixed λ=0.5
  3. Validation set composition sensitivity: Compare "Hardest" vs "Easiest" vs "EasyHard" validation subsets at 10% budget

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed greedy utility metric be effectively approximated by a neural scoring model to enhance scalability in large-scale pretraining regimes?
- **Basis in paper:** Conclusion states future work will focus on extending greedy utility metric to train neural scoring models.
- **Why unresolved:** Current framework relies on greedy policy that may face computational bottlenecks at extreme scales.
- **What evidence would resolve it:** Comparative experiments training neural network to predict utility reward and evaluating approximation error vs wall-clock speedup.

### Open Question 2
- **Question:** How does dynamic curriculum ordering of validation sample difficulty (e.g., shifting from easy to hard) impact stability of reward signal and final generalization?
- **Basis in paper:** Appendix E.1 notes static hard validation sets yield lower performance and suggests exploring validation sample difficulty and ordering interactions.
- **Why unresolved:** Paper ablates static validation subsets but doesn't investigate time-varying schedules for validation set itself.
- **What evidence would resolve it:** Ablation studies evaluating ONLINESUBMOD with scheduled validation set transitioning from "easy" to "hard" samples.

### Open Question 3
- **Question:** Can ONLINESUBMOD maintain robust performance without the "warm-start" phase required to mitigate noisy early-stage gradients?
- **Basis in paper:** Section C.3 states starting selection from random initialization often leads to premature overfitting.
- **Why unresolved:** Reliance on warm-start limits total compute reduction achievable.
- **What evidence would resolve it:** Experiments analyzing regret bounds and arm selection accuracy when warm-start period is removed.

## Limitations

- Performance critically depends on validation data quality, with noisier rewards observed when validation sets contain only hard or only easy examples
- First-order utility approximation may degrade for large learning rates or highly non-convex landscapes
- While computational overhead is claimed negligible, full pipeline wasn't isolated in runtime measurements

## Confidence

- **High confidence**: (1) Greedy submodular maximization provides (1-1/e)-approximation guarantees; (2) Multi-armed bandit framework can track non-stationary arm preferences; (3) Empirical speedup measurements (~3×-8×) on tested datasets
- **Medium confidence**: (1) Utility-driven reward based on gradient influence is effective across diverse tasks; (2) Adaptive bandit selection outperforms static arm selection; (3) Validation-driven rewards generalize to different domains
- **Low confidence**: (1) Approach generalizes beyond tested vision/language datasets; (2) Performance remains robust at extreme budget levels (<5%); (3) First-order approximation remains accurate for all learning rate scales

## Next Checks

1. **Learning rate sensitivity**: Test ONLINESUBMOD performance across learning rates {0.001, 0.01, 0.1} on CIFAR-100 to validate first-order approximation robustness
2. **Extreme budget ablation**: Evaluate at 5% and 1% subset budgets to identify breaking points for accuracy-speedup tradeoff
3. **Cross-domain generalization**: Apply ONLINESUBMOD to time-series or graph neural network datasets to test beyond vision/language scope