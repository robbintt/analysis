---
ver: rpa2
title: Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek
arxiv_id: '2501.12826'
source_url: https://arxiv.org/abs/2501.12826
tags:
- greek
- llama
- dataset
- table
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of advancing NLP for lesser-resourced
  languages, using Modern Greek as a case study. The authors compiled a collection
  of publicly available Greek datasets and benchmarked open-source (Llama-70b) and
  closed-source (GPT-4o mini) LLMs on seven core NLP tasks.
---

# Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek

## Quick Facts
- arXiv ID: 2501.12826
- Source URL: https://arxiv.org/abs/2501.12826
- Reference count: 17
- Primary result: Llama excels at NER/summarization while GPT performs better on GEC, MT, intent classification, and POS tagging for Greek

## Executive Summary
This paper benchmarks open-source (Llama-70b) and closed-source (GPT-4o mini) LLMs on seven Greek NLP tasks, revealing task-specific strengths that inform the open vs. closed model tradeoff for lesser-resourced languages. The authors introduce two novel benchmarks: a long legal text clustering task using a STE methodology that outperforms TF-IDF, and an authorship attribution task that potentially reveals training data contamination. Together, these contributions provide a roadmap for advancing NLP in lesser-resourced languages by addressing dataset availability, task innovation, and real-world impact.

## Method Summary
The authors compiled 12 FAIR-compliant Greek datasets from various sources and benchmarked Llama-70b and GPT-4o mini using zero-shot prompting across seven core NLP tasks: toxicity detection, grammatical error correction, machine translation (el→en/jpn/fa), summarization, intent classification, named entity recognition, and part-of-speech tagging. They also introduced a novel three-stage STE (Summarize-Translate-Embed) pipeline for clustering long legal texts and reframed authorship attribution as a probe for potential training data contamination. Evaluation used task-specific metrics including F1, BERTScore, ROUGE, WER/CER, and clustering metrics (NMI, AMI, ARI).

## Key Results
- Llama-70b outperforms GPT-4o mini in NER and summarization tasks
- GPT-4o mini performs better in GEC, MT (el→jpn), intent classification, and POS tagging
- Both models achieve parity in toxicity detection and MT (el→en/fa)
- STE methodology outperforms traditional TF-IDF for long legal text clustering
- High zero-shot accuracy in authorship attribution suggests potential training data usage

## Why This Works (Mechanism)
The paper's effectiveness stems from leveraging zero-shot prompting to compare model capabilities without task-specific fine-tuning, enabling direct comparison of open vs. closed architectures. The STE pipeline works by compressing complex legal documents through summarization and translation into dense embeddings that capture semantic similarity beyond surface-level TF-IDF matching. The authorship attribution mechanism functions as a data provenance probe by testing whether LLMs can identify specific authors' writing styles without examples, with high accuracy suggesting the model may have encountered the author's works during pre-training.

## Foundational Learning
- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Serves as the primary baseline for the text clustering task. The paper's STE method is evaluated by its ability to outperform this traditional, statistical representation. *Quick check: How would TF-IDF represent a Greek legal document compared to the dense vector embeddings produced by the STE method?*
- **Zero-Shot Learning**: The entire benchmark is conducted in a zero-shot setting. Understanding that the models are given a task prompt without any example solutions is critical to interpreting the results and the authorship attribution mechanism. *Quick check: Why is the high zero-shot accuracy on the authorship task interpreted as a potential sign of data leakage rather than just good model performance?*
- **Data Provenance and Contamination**: Core to the paper's ethical contribution. Reframing authorship attribution as a probe for whether copyrighted or private data was used in LLM pre-training requires understanding the concept of training data contamination. *Quick check: Based on the authorship attribution results, which authors are most likely to have been included in the LLMs' pre-training data, and which were not?*

## Architecture Onboarding

**Component map:**
Data Sources -> LLM Architectures (Llama-70b, GPT-4o mini) -> Novel STE Pipeline (Summarizer -> Translator -> Embedder) -> Evaluation Metrics

**Critical path:**
1. Dataset Compilation: Identify and filter for FAIR-compliant Greek datasets from literature survey
2. Benchmarking: Run 0-shot evaluations of Llama and GPT on 7 core NLP tasks
3. Novel Task Application: Apply STE pipeline to long legal text clustering task
4. Interpret results: Analyze differential performance for data provenance claims

**Design tradeoffs:**
- Open vs. Closed Models: Compares strong open model (Llama) with cost-effective closed one (GPT-4o mini), revealing transparency vs. cost vs. task-specific performance tradeoffs
- TF-IDF vs. STE: TF-IDF is fast and cheap but fails to capture deep semantics; STE is computationally expensive but produces superior clustering on complex texts
- Granularity in Clustering: Evaluates clustering at three levels (47, 374, 1685 clusters), showing how performance metrics behave differently with increased granularity

**Failure signatures:**
- Model Failure on Specific Authors: F1-score of 0.00 in authorship attribution strongly suggests author's works were not in pre-training data
- STE Ablation Failure: Performance drops when embedding step is replaced with TF-IDF on summaries, indicating information loss during processing
- License-Based Dataset Exclusion: Excludes datasets with non-derivative licenses, highlighting common failure mode in data curation

**First 3 experiments:**
1. Reproduce Zero-Shot Benchmark: Take datasets for single task (e.g., GEC) and replicate WER/CER evaluation for Llama and GPT using paper's 0-shot prompt
2. Ablate STE Pipeline: For legal clustering subset, compare Full STE vs. STE without translation vs. TF-IDF baseline
3. Probing for Data Leakage: Select authors with high and zero accuracy; query models for specific rare phrases from their works

## Open Questions the Paper Calls Out

**Open Question 1**: Can high zero-shot accuracy in authorship attribution reliably serve as a proxy for detecting whether specific texts were included in LLM pre-training data? The authors acknowledge they "cannot exclude" that GPT already used the Korre et al. (2021) data during training, and ground truth about actual training data composition for closed-source models is unavailable.

**Open Question 2**: Can the STE methodology be modified to reduce information loss during intermediate processing steps? The authors note information is lost during translation and summarization when ablating the embedding step, but do not explore alternative pipelines.

**Open Question 3**: Do the observed task-specific model strengths generalize to other lesser-resourced languages? Only Greek was tested, and findings may be influenced by language-specific characteristics or Greek's representation in training corpora.

## Limitations
- Zero-shot accuracy on authorship attribution as data provenance probe lacks controlled negative examples and could reflect stylistic patterns rather than memorization
- Task-specific performance differences between models are measured but not deeply analyzed for causal factors
- STE methodology introduces multiple error sources (summarization drift, translation quality, embedding alignment) without ablation studies isolating component contributions

## Confidence
- **High**: Dataset compilation methodology and FAIR compliance criteria are clearly defined and reproducible; comparative benchmarking framework is methodologically sound
- **Medium**: Novel contributions (STE clustering, authorship attribution as data provenance probe) are innovative but require further validation with controlled experiments
- **Low**: Claims about real-world impact and practical roadmap utility are aspirational and not empirically validated within the paper's scope

## Next Checks
1. Create synthetic control groups for authorship attribution where author identity is randomized; if high accuracy persists, it challenges the data leakage interpretation
2. Test authorship attribution methodology on parallel corpora (Greek and English translations of same works); cross-lingual accuracy would strengthen case for data contamination
3. Systematically vary zero-shot prompts for each task and measure performance variance to establish whether observed differences are robust to prompt engineering