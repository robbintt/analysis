---
ver: rpa2
title: On the Internal Representations of Graph Metanetworks
arxiv_id: '2503.09120'
source_url: https://arxiv.org/abs/2503.09120
tags:
- representations
- gmns
- general
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the internal representations learned by
  graph metanetworks (GMNs) that process neural network parameters as graph structures.
  The authors compare GMNs to standard neural networks (MLPs and CNNs) on INR classification
  tasks versus traditional image classification.
---

# On the Internal Representations of Graph Metanetworks

## Quick Facts
- arXiv ID: 2503.09120
- Source URL: https://arxiv.org/abs/2503.09120
- Reference count: 11
- Primary result: GMNs are more sensitive to random initialization than standard NNs, leading to lower representation similarity (CKA ~0.4) despite achieving similar test accuracies.

## Executive Summary
This paper investigates how Graph Metanetworks (GMNs) that process neural network parameters as graph structures learn internal representations compared to standard neural networks. Using Centered Kernel Alignment (CKA) to measure representation similarity, the authors find that GMNs are significantly more sensitive to random initialization than MLPs and CNNs, resulting in lower CKA values across different initializations. Despite achieving similar test accuracies, GMNs learn fundamentally different representations than general NNs, leading to distinct prediction patterns where GMNs sometimes correctly classify samples that general NNs consistently misclassify.

## Method Summary
The study compares GMNs against standard MLPs and CNNs on Implicit Neural Representation (INR) classification tasks versus traditional image classification. GMNs process neural network weights as graph structures using the ScaleGMN architecture with 4 message-passing steps and projection layers. Standard networks serve as baselines with 4 hidden layers. The authors compute CKA on output node features (GMN) or activation matrices (MLP/CNN) using linear kernel with centering matrix. Models are trained across 10 random seeds with specific learning rates and epochs per dataset. The analysis focuses on representation similarity (CKA), prediction differences (Normalized Hamming Distance), and the relationship between initialization sensitivity and learned representations.

## Key Results
- GMNs show significantly lower representation similarity across random initializations (CKA ~0.4) compared to general NNs (CKA ~0.94)
- Despite similar test accuracies, GMNs learn fundamentally different representations than general NNs
- Distinct representations lead to complementary prediction patterns, with GMNs correctly classifying samples that general NNs consistently misclassify
- The representation divergence appears linked to projection layers that amplify initialization variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random initialization creates lower representation similarity in Graph Metanetworks (GMNs) than in standard CNNs/MLPs.
- Mechanism: GMNs utilize projection layers to map scalar weight values into higher-dimensional node features. The paper suggests these projections may amplify variance from initialization, preventing representations from converging to similar states (high CKA) even when accuracy is stable.
- Core assumption: The observed divergence is primarily driven by the projection of scalar parameters rather than the message-passing dynamics alone.
- Evidence anchors:
  - [abstract]: "...GMNs are more sensitive to random initialization than general NNs, leading to lower representation similarity..."
  - [section]: Section 3.1 notes this "may be due to the presence of projection layers in GMNs... [leading to] different hidden representations... throughout training."
  - [corpus]: Weak direct evidence; related work (arXiv:2510.08300) describes ScaleGMN architecture but does not analyze initialization sensitivity.
- Break condition: If projection layers were fixed or standardized and representation similarity remained low, this specific mechanism would be invalid.

### Mechanism 2
- Claim: Processing parameters as graphs creates representations that are fundamentally distinct from those learned by processing the raw signal directly.
- Mechanism: While CNNs/MLPs learn from pixel correlations (visual space), GMNs learn from the structural topology of weights (parameter space). Despite achieving similar test accuracies, Centered Kernel Alignment (CKA) shows these models occupy different representation spaces (~0.42 CKA for GMN vs. MLP/CNN vs. ~0.94 for MLP vs. CNN).
- Core assumption: The "neural graph" construction captures orthogonal information compared to the original image data.
- Evidence anchors:
  - [abstract]: "...GMNs learn fundamentally different representations compared to general NNs..."
  - [section]: Table 1 shows cross-architecture CKA (GMN↔CNN) is significantly lower than intra-modal comparison (MLP↔CNN).
  - [corpus]: ArXiv:2510.08300 discusses ScaleGMN symmetry handling but does not confirm this representation divergence.
- Break condition: If GMNs were trained on flattened weight vectors (no graph structure) and CKA approached 1.0, the graph mechanism would be less critical.

### Mechanism 3
- Claim: Distinct representations imply complementary failure modes, allowing GMNs to correct errors made by general neural networks.
- Mechanism: Because GMN representations diverge from those of CNNs (Mechanism 2), they form different decision boundaries. The paper observes instances where GMNs consistently classify samples correctly that CNNs consistently fail on, quantified by Normalized Hamming Distance (NHD).
- Core assumption: Low representation similarity (CKA) correlates with distinct prediction behaviors (NHD).
- Evidence anchors:
  - [abstract]: "...difference in representations translates to distinct prediction patterns, with GMNs sometimes correctly classifying samples that general NNs consistently misclassify."
  - [section]: Section 3.3 confirms higher NHD between GMN and General NNs than within General NN pairs.
  - [corpus]: Missing direct evidence in the provided corpus regarding this specific complementarity.
- Break condition: If GMNs had low CKA but identical error profiles (low NHD) to CNNs, the claim that representation differences drive prediction differences would fail.

## Foundational Learning
- **Centered Kernel Alignment (CKA)**
  - Why needed here: This is the primary metric used to quantify representation similarity. You must understand that CKA measures the alignment of activation matrices independent of orthogonal transformations (e.g., neuron permutation).
  - Quick check question: If two networks have 0.95 CKA, are their hidden layers likely detecting similar features or different features?

- **Implicit Neural Representations (INRs)**
  - Why needed here: The paper treats INRs (networks that map coordinates to pixel values) as the input data. You need to distinguish between classifying the *image* (CNN task) and classifying the *weights of the network representing the image* (GMN task).
  - Quick check question: In this paper, does the GMN input consist of pixels or weight parameters?

- **Scale Equivariance in Weight Space**
  - Why needed here: The GMN architecture (ScaleGMN) relies on respecting symmetries inherent in neural network weights (specifically permutation and scaling).
  - Quick check question: Why would a standard MLP fail to process neural network weights effectively without specific equivariance design?

## Architecture Onboarding
- **Component map:** INR weights (scalars) -> Neural Graph (Nodes = biases, Edges = weights) -> ScaleGMN layers (Projection layers -> Message Passing) -> Output node features -> CKA & NHD metrics

- **Critical path:**
  1. Verify the construction of the Neural Graph (ensuring directed acyclic nature).
  2. Monitor the CKA curve during training (Fig 2). Unlike standard NNs, do not expect GMN CKA to rapidly converge to 1.0 across seeds.

- **Design tradeoffs:**
  - **Projection Depth:** Increasing projection layer depth may enrich features but appears to increase initialization sensitivity (suggested in Section 3.1).
  - **Bidirectional vs. Unidirectional:** The paper defaults to unidirectional message passing; bidirectional may improve flow but changes the inductive bias.

- **Failure signatures:**
  - **Stagnant CKA:** If CKA across different seeds remains extremely low (<0.2) while accuracy is also low, the model may be overfitting to initialization noise.
  - **Accuracy Ceiling:** If GMN accuracy significantly lags behind the MLP baseline on simple datasets (e.g., MNIST), check if the INR reconstruction quality is sufficient (garbage in, garbage out).

- **First 3 experiments:**
  1. **Reproduce Fig 2:** Train 5 GMN pairs and 5 CNN pairs on MNIST INRs/Images. Plot CKA over epochs to confirm the "sensitivity gap."
  2. **Error Analysis:** Identify the specific samples where GMNs succeed and CNNs fail (as mentioned in Section 3.3). Visualize these to understand what "parameter-space features" capture that pixels do not.
  3. **Projection Ablation:** Reduce the dimensionality or depth of the initial projection layers in the GMN to test if CKA similarity increases (testing the paper's hypothesis on the source of sensitivity).

## Open Questions the Paper Calls Out
- Do Graph Metanetworks (GMNs) exhibit representation similarities with modern architectures like Vision Transformers (ViTs)?
- How does the nature of the task (e.g., equivariance vs. classification) influence the internal representations learned by GMNs?
- Can the complementary prediction errors of GMNs and general NNs be leveraged to improve overall model performance?

## Limitations
- The study only compares GMNs against MLPs and CNNs, excluding widely used models like vision transformers
- The analysis is limited to one task type (INR classification) that does not reflect equivariance requirements
- While distinct prediction patterns are identified, the paper does not explore methods to leverage this complementarity for improved performance

## Confidence
- **High confidence**: The observation that GMNs show lower representation similarity across random initializations compared to standard NNs (CKA ~0.4 vs ~0.94) - this is directly measured and well-supported.
- **Medium confidence**: The mechanism linking projection layers to initialization sensitivity - while plausible, the causal connection between projection depth and CKA divergence needs more direct experimental validation.
- **Medium confidence**: The claim that distinct representations lead to complementary error patterns - the correlation is demonstrated, but the causal relationship requires more systematic analysis across different datasets.

## Next Checks
1. **Projection ablation study**: Systematically vary projection layer depth in GMN and measure resulting CKA values to directly test the initialization sensitivity hypothesis.
2. **Cross-dataset generalization**: Train GMNs on multiple INR datasets (MNIST, Fashion-MNIST, CIFAR-10) and verify whether the representation divergence pattern holds consistently.
3. **Error profile mapping**: Conduct a detailed analysis of specific samples where GMNs succeed and CNNs fail, examining whether these correspond to identifiable patterns in parameter-space features versus pixel-space features.