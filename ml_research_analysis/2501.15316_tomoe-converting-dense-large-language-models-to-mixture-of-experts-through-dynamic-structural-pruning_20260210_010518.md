---
ver: rpa2
title: 'ToMoE: Converting Dense Large Language Models to Mixture-of-Experts through
  Dynamic Structural Pruning'
arxiv_id: '2501.15316'
source_url: https://arxiv.org/abs/2501.15316
tags:
- pruning
- tomoe
- experts
- parameters
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ToMoE converts dense LLMs to MoE models using dynamic structural
  pruning without fine-tuning. It applies top-K and static pruning for attention layers
  and top-1 routing for MLP layers, maintaining fixed computational costs.
---

# ToMoE: Converting Dense Large Language Models to Mixture-of-Experts through Dynamic Structural Pruning

## Quick Facts
- arXiv ID: 2501.15316
- Source URL: https://arxiv.org/abs/2501.15316
- Authors: Shangqian Gao, Ting Hua, Reza Shirkavand, Chi-Heng Lin, Zheng Tang, Zhengao Li, Longge Yuan, Fangyi Li, Zeyu Zhang, Alireza Ganjdanesh, Lou Qian, Xu Jie, Yen-Chang Hsu
- Reference count: 40
- Primary result: Converts dense LLMs to MoE models via dynamic structural pruning without fine-tuning, achieving better perplexity and accuracy with fewer active parameters than state-of-the-art methods

## Executive Summary
ToMoE introduces a novel approach to convert dense large language models into Mixture-of-Experts architectures without requiring weight updates. The method uses dynamic structural pruning to uncover latent expert structures within dense models, applying top-K and static pruning for attention layers and top-1 routing for MLP layers. This conversion maintains fixed computational costs while achieving superior performance compared to existing pruning and MoE construction techniques across multiple model sizes and diverse tasks.

## Method Summary
ToMoE employs a hypernetwork to generate expert embeddings that create overlapping experts from the original weight matrix through binary selection masks. For MLP layers, it uses top-1 routing across N experts, while for MHA layers it applies hybrid static+dynamic pruning (static for Query/Key, dynamic for Value/Output). The method maximizes parameter utilization through union regularization while maintaining load balancing. Training involves only the routing and projection modules while keeping original weights frozen, using self-knowledge distillation with KL divergence loss.

## Key Results
- Outperforms state-of-the-art pruning and MoE construction techniques on diverse models (Phi-2, LLaMA-2/3, Qwen-2.5)
- Achieves better perplexity and accuracy with fewer active parameters across tasks like WikiText-2, ARC, PIQA, WinoGrande, and HellaSwag
- Requires minimal additional parameters (~0.27% for LLaMA-2 7B) and offers efficient conversion at low cost
- Discovers inherent experts within dense models that show semantic specialization on math problems and syntactic alignment on general text

## Why This Works (Mechanism)

### Mechanism 1: Expert Discovery via Differentiable Structural Pruning
Dense LLMs contain latent expert structures that can be extracted through input-dependent pruning without weight updates. A hypernetwork generates expert embeddings that create N overlapping experts from the original weight matrix using binary selection masks. The router learns to map tokens to experts via conditional computation, where each expert retains a subset of columns/rows from the original weight matrices.

### Mechanism 2: Asymmetric MHA vs. MLP Routing Strategies
MHA layers require static+dynamic hybrid pruning while MLP layers benefit from pure top-1 routing due to structural constraints in attention computation. For MHA, static pruning is applied to Query/Key projections (shared across all tokens) and dynamic top-K pruning to Value/Output projections (token-specific). This design addresses the constraint that dynamic Q/K pruning creates variable effective attention widths across different token pairs.

### Mechanism 3: Union Regularization for Capacity Preservation
Maximizing the union of expert masks preserves dense model capacity under sparsity constraints. The union coverage is computed and regularized to push toward 100% of original parameters, combined with parameter budget constraints and load balancing. This ensures experts are both diverse and balanced while maintaining access to the original parameter space.

## Foundational Learning

- **Concept: Straight-Through Gumbel-Softmax/Sigmoid**
  - Why needed here: Enables backpropagation through discrete expert selection decisions, making routing and mask generation differentiable
  - Quick check question: Why does the straight-through estimator use hard decisions in the forward pass but soft gradients in the backward pass?

- **Concept: Mixture-of-Experts Conditional Computation**
  - Why needed here: ToMoE's core premise is converting dense→sparse MoE, requiring understanding of why sparse activation (only k of N experts active per token) provides efficiency gains
  - Quick check question: If an MoE has 8 experts with top-1 routing, what fraction of FFN parameters are active per forward pass?

- **Concept: Self-Knowledge Distillation**
  - Why needed here: ToMoE trains only routing/projection modules while freezing original weights, using KL divergence between dense and MoE outputs as the training signal
  - Quick check question: Why use KL divergence between logits rather than MSE on hidden states for knowledge transfer?

## Architecture Onboarding

- **Component map:**
Frozen Dense LLM -> HyperNetwork (expert embeddings) -> Per-layer modules (Proj_MLP^D, Proj_MHA^E/D, Router) -> Forward pass with masked weights -> Losses (KL + regularization)

- **Critical path:**
1. Initialize: HyperNetwork generates E from random z ~ N(0,1)
2. Forward: Compute expert masks s_e and routing G per batch
3. Apply: Execute masked MLP (top-1 expert) and pruned MHA (top-K heads)
4. Optimize: Backprop through Θ only (hypernet + projections + router)
5. Export: Post-training, save s_e masks and router weights; discard projection modules

- **Design tradeoffs:**
  - Expert count N: Paper uses N=8; N=16 shows marginal gains on Qwen-2.5. More experts → finer specialization but larger routing overhead
  - Active ratio p: 40-60% is the practical range. Below 40%, union regularization struggles to maintain coverage
  - Temperature τ: Robust in [0.3, 0.5]. Lower τ → sharper routing; higher τ → more exploration
  - Assumption: Mixed training data (WikiText + Alpaca + Code-Alpaca) improves expert diversity—single-domain data may underperform

- **Failure signatures:**
  - Expert collapse: R_L → 0 (all tokens to one expert). Fix: Increase γ or check router initialization
  - Capacity loss: Union < 80% and perplexity spike. Fix: Increase β, reduce target p
  - Attention artifacts: Performance drops on long-context tasks. Fix: Verify s_0 respects RoPE subspace constraints
  - Training divergence: Loss oscillates early. Fix: Check bias b=3.0 in ST-GSig, ensure learning rate ≤ 10^-3

- **First 3 experiments:**
  1. Reproduce LLaMA-2 7B at 50% active params: Train 10K iterations, expect perplexity ~8.3 on WikiText-2. Compare against DISP-LLM baseline
  2. Ablate R_U regularization: Run with β=0 vs β=2.0 at p=0.4. Measure union coverage and perplexity gap (expect ~1 point difference on Qwen-2.5)
  3. Visualize expert routing: Process diverse inputs (code/math/prose), plot token→expert assignments. Check if semantic patterns emerge (Table 7 shows syntax alignment; Table 14 shows math specialization)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What determines whether ToMoE experts specialize on syntactic versus semantic patterns, and can this specialization be controlled or directed?
- Basis in paper: [explicit] The paper observes experts "align syntax rather than semantic meanings" in general text but show "clearer semantic patterns" for math problems (e.g., Expert 2 in MLP 16 activates on numbers), stating "Further investigation is required to fully understand the precise semantic roles of ToMoE experts."
- Why unresolved: The paper documents the phenomenon but provides no mechanism or theory for why certain domains elicit semantic specialization while others produce syntactic patterns
- What evidence would resolve it: A systematic study varying input domains (code, math, natural language, structured data) with controlled expert analysis to identify factors predicting specialization type

### Open Question 2
- Question: How does ToMoE's performance scale to models beyond 14B parameters, and does the no-fine-tuning advantage persist at larger scales?
- Basis in paper: [inferred] All experiments use models ≤14B (LLaMA-2 7B/13B, LLaMA-3 8B, Phi-2 2.7B, Qwen-2.5 7B/14B). The paper notes "the performance gap between our method and other approaches is smaller" for larger models, hinting at potential scaling issues
- Why unresolved: Scaling laws for MoE conversion without weight updates are unexplored; benefits observed at 7B may not transfer to 70B+ models where expert specialization becomes more critical
- What evidence would resolve it: Experiments on LLaMA-2 70B or similar large models comparing ToMoE against baseline methods with and without fine-tuning

## Limitations

- **Expert Discovery Mechanism**: The claim that dense LLMs inherently contain latent expert structures remains theoretically unproven and may depend on specific architectural properties rather than universal latent structure
- **Hybrid Attention Routing Design**: The asymmetric treatment of MHA versus MLP lacks theoretical justification in the literature and independent empirical validation
- **Domain Generalization**: The paper doesn't systematically test whether ToMoE generalizes to domains outside the mixed training corpus, suggesting potential brittleness

## Confidence

**High Confidence**: Claims about ToMoE's performance improvements over baseline methods (DISP-LLM, LM-MoE) on specific benchmarks (WikiText-2 perplexity, ARC/PIQA accuracy) are well-supported by ablation studies and comparative results

**Medium Confidence**: The effectiveness of the three regularization terms (R_P, R_U, R_L) is supported by ablation experiments, but the theoretical justification for union maximization specifically is weak

**Low Confidence**: The fundamental claim that dense models contain "inherent experts" is more of an empirical observation than a proven mechanism, with universal applicability across different LLM architectures and domains remaining untested

## Next Checks

1. **Domain Transfer Experiment**: Train ToMoE on medical/scientific text data and evaluate performance on domain-specific benchmarks. Compare against fine-tuned dense models to assess whether the discovered experts maintain effectiveness outside the original training distribution

2. **Fully Dynamic Attention Routing**: Implement and evaluate a version of ToMoE with dynamic pruning applied uniformly across all attention parameters (Q, K, V, O). Compare perplexity and routing patterns against the hybrid approach to validate whether static Q/K pruning is truly necessary

3. **Expert Stability Analysis**: Train ToMoE with varying random seeds and analyze expert mask stability across runs. Compute pairwise Jaccard similarity between expert masks from different initializations to quantify whether experts converge to consistent patterns or remain seed-dependent