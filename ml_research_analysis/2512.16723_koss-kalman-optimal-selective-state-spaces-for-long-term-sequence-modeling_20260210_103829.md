---
ver: rpa2
title: 'KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling'
arxiv_id: '2512.16723'
source_url: https://arxiv.org/abs/2512.16723
tags:
- koss
- state
- input
- kalman
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KOSS, a Kalman-optimal Selective State Space
  model that formulates selection as latent state uncertainty minimization using Kalman
  filtering principles. KOSS employs a continuous-time latent update modulated by
  a Kalman gain that dynamically filters information based on both content and context,
  addressing limitations of input-only selection mechanisms.
---

# KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling

## Quick Facts
- **arXiv ID**: 2512.16723
- **Source URL**: https://arxiv.org/abs/2512.16723
- **Reference count**: 22
- **Primary result**: KOSS achieves over 79% accuracy on selective copying task while baselines drop below 20%

## Executive Summary
KOSS introduces a Kalman-optimal Selective State Space model that addresses limitations of traditional selective SSMs by formulating selection as latent state uncertainty minimization using Kalman filtering principles. The model employs a continuous-time latent update modulated by a Kalman gain that dynamically filters information based on both content and context, rather than relying on input-only selection mechanisms. Through global spectral differentiation for frequency-domain derivative estimation and segment-wise scanning for efficient computation, KOSS demonstrates significant improvements in long-term sequence modeling tasks, reducing MSE by 2.92–36.23% across nine forecasting benchmarks while maintaining superior stability compared to state-of-the-art models.

## Method Summary
KOSS operates by integrating Kalman filtering principles into selective state space modeling, where the selection mechanism is formulated as minimizing latent state uncertainty rather than using static gating functions. The core innovation lies in the Kalman gain-based modulation of continuous-time latent updates, which allows the model to dynamically adjust its attention to information based on both current content and historical context. To ensure computational efficiency and numerical stability, KOSS employs global spectral differentiation in the frequency domain for derivative estimation, avoiding the instability issues of time-domain differentiation. The segment-wise scan implementation enables scalable computation by breaking down long sequences into manageable segments while preserving the temporal dependencies necessary for accurate long-term forecasting.

## Key Results
- Achieves over 79% accuracy on selective copying task with distractors while baselines drop below 20%
- Reduces MSE by 2.92–36.23% across nine long-term forecasting benchmarks compared to state-of-the-art models
- Demonstrates superior stability and performance in real-world secondary surveillance radar tracking under irregular intervals and noisy conditions

## Why This Works (Mechanism)
The Kalman-optimal formulation works by treating the selection problem as an inference task within a state-space framework, where the Kalman gain naturally balances between prediction and measurement updates based on uncertainty. This approach is more theoretically grounded than heuristic gating mechanisms because it derives from optimal estimation theory under Gaussian noise assumptions. The frequency-domain differentiation avoids the numerical instability of time-domain derivatives, particularly important for long sequences where accumulated errors can become significant. The segment-wise scan maintains computational efficiency without sacrificing the long-range dependencies that are crucial for accurate forecasting.

## Foundational Learning
- **Kalman Filtering**: Optimal recursive estimation algorithm for linear dynamic systems with Gaussian noise; needed for deriving theoretically sound selection mechanisms that adapt to uncertainty; quick check: verify that state transition and observation models are linear and noise is Gaussian
- **State Space Models**: Framework representing systems through state equations and observation equations; needed as the underlying architecture for continuous-time sequence modeling; quick check: ensure state dimension matches problem complexity
- **Frequency-Domain Differentiation**: Numerical method for computing derivatives using Fourier transforms; needed to avoid instability in time-domain differentiation for long sequences; quick check: confirm that the frequency sampling is dense enough to capture signal variations
- **Spectral Methods**: Numerical techniques using Fourier or other transforms to solve differential equations; needed for stable and efficient derivative computation in continuous-time models; quick check: verify that spectral differentiation preserves signal characteristics
- **Segment-wise Computation**: Technique for breaking large computations into smaller, manageable pieces; needed to scale SSMs to long sequences while maintaining computational efficiency; quick check: ensure segment boundaries don't break critical temporal dependencies

## Architecture Onboarding

### Component Map
Latent State Update -> Kalman Gain Modulation -> Frequency-Domain Differentiation -> Segment-wise Scan -> Output Projection

### Critical Path
Input sequence → Continuous-time state evolution (modulated by Kalman gain) → Frequency-domain derivative computation → Segment-wise temporal scanning → Output projection

### Design Tradeoffs
The Kalman-optimal approach trades computational complexity for theoretical soundness and adaptability. While traditional selective SSMs use simple gating mechanisms that are computationally efficient, they lack the principled uncertainty-based selection that KOSS provides. The frequency-domain differentiation adds computational overhead but ensures numerical stability for long sequences. The segment-wise scan introduces boundary effects but enables scalability to very long sequences that would be intractable with global computation.

### Failure Signatures
Performance degradation is likely when noise distributions deviate significantly from Gaussian assumptions, when temporal dependencies extend beyond the effective range of segment-wise processing, or when the state dimension is insufficient to capture complex dynamics. The model may also struggle with highly non-linear systems where the linear state-space assumptions break down.

### 3 First Experiments
1. Compare Kalman gain-based selection versus static gating on synthetic selective copying task with varying noise levels
2. Test frequency-domain versus time-domain differentiation on sequences of increasing length to measure numerical stability
3. Evaluate segment size impact on forecasting accuracy for sequences with different temporal correlation structures

## Open Questions the Paper Calls Out
None

## Limitations
- Kalman filtering assumes Gaussian noise characteristics that may not hold in all real-world scenarios, particularly in secondary surveillance radar tracking
- Superior performance on selective copying task represents a single synthetic benchmark that may not capture practical application complexity
- Frequency-domain differentiation introduces approximation errors that are not fully characterized across different signal types and noise levels
- Computational complexity claims relative to S4 models need more detailed analysis, particularly regarding memory usage patterns

## Confidence

### High confidence:
- Mathematical formulation of Kalman-optimal selection
- Basic architectural improvements over traditional selective SSMs

### Medium confidence:
- Empirical results on forecasting benchmarks
- Secondary surveillance radar case study

### Low confidence:
- Generalizability of selective copying task results to complex, real-world sequence modeling problems

## Next Checks
1. Conduct ablation studies isolating the contribution of Kalman gain mechanism versus frequency-domain differentiation approach on multiple benchmark datasets
2. Test KOSS performance under non-Gaussian noise distributions and irregular sampling patterns beyond those presented in the secondary surveillance radar case study
3. Perform detailed computational complexity analysis comparing memory usage and inference time across different sequence lengths and model sizes, particularly for segment-wise scan implementation