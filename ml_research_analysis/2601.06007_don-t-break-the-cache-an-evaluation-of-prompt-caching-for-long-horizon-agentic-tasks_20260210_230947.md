---
ver: rpa2
title: 'Don''t Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic
  Tasks'
arxiv_id: '2601.06007'
source_url: https://arxiv.org/abs/2601.06007
tags:
- caching
- prompt
- cache
- tool
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive evaluation of prompt
  caching for long-horizon agentic tasks across three major LLM providers. The authors
  evaluate three caching strategies (full context, system prompt only, and exclude
  tool results) on DeepResearch Bench, a multi-turn agentic benchmark, measuring API
  cost and time to first token across 500 agent sessions with 10,000-token system
  prompts.
---

# Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks

## Quick Facts
- arXiv ID: 2601.06007
- Source URL: https://arxiv.org/abs/2601.06007
- Authors: Elias Lumer; Faheem Nizar; Akshaya Jangiti; Kevin Frank; Anmol Gulati; Mandar Phadate; Vamse Kumar Subbiah
- Reference count: 33
- One-line primary result: Prompt caching reduces API costs by 41-80% and improves time to first token by 13-31% across providers for long-horizon agentic tasks

## Executive Summary
This paper presents the first comprehensive evaluation of prompt caching for long-horizon agentic tasks across three major LLM providers (OpenAI, Anthropic, Google). The authors evaluate three caching strategies (full context, system prompt only, and exclude tool results) on DeepResearch Bench, a multi-turn agentic benchmark, measuring API cost and time to first token across 500 agent sessions with 10,000-token system prompts. Results show strategic cache boundary control outperforms naive full-context caching, with system prompt only caching providing the most consistent benefits across providers.

## Method Summary
The study uses DeepResearch Bench with 100 PhD-level research tasks across 22 fields, running 500 agent sessions with a 10,000-token system prompt. Four cache modes are controlled via UUID boundary markers: No Cache (UUID prepended), Full Context (no UUIDs), System Prompt Only (UUID after system prompt), and Exclude Tool Results (UUIDs after system prompt + each tool result). The agent is implemented with LangChain Deep Agents, using warmup calls to prime caches and 24+ hour waits between conditions to prevent cross-contamination. Metrics include API cost (standard vs cached vs cache-write tokens) and time to first token via streaming, with n=40 per condition and significance at α=0.05.

## Key Results
- Prompt caching reduces API costs by 41-80% across providers when prompts exceed minimum thresholds
- System prompt only caching provides the most consistent benefits with 13-31% TTFT improvements
- Full-context caching can paradoxically increase latency due to cache write overhead for dynamic content
- Cost savings scale linearly with prompt size after provider minimum thresholds (1,024-4,096 tokens)

## Why This Works (Mechanism)

### Mechanism 1
Prompt caching reduces API costs proportionally to the size of the reusable prompt prefix. During the prefill phase, LLMs compute key-value tensors for the entire input sequence. Prompt caching stores these tensors; subsequent requests sharing the exact prefix can reuse them, bypassing recomputation. Providers charge less for cached input tokens (10-50% of standard input pricing). Core assumption: The system prompt remains stable across requests within a session.

### Mechanism 2
Strategic cache boundary control prevents latency regressions from caching non-reusable dynamic content. Full-context caching automatically caches all content exceeding thresholds, including tool calls and results. When dynamic content varies per request, cache writes incur overhead without corresponding read benefits. Explicitly excluding tool results via UUID boundary markers ensures only stable content (system prompt) is cached. Core assumption: Tool results are session-specific and unlikely to repeat across sessions.

### Mechanism 3
Cache hits require exact prefix matches; any token difference invalidates the entire cache. Providers implement prefix-based caching where the cacheable portion must match exactly from the first token. Dynamic elements early in the prompt (timestamps, user IDs) break the cache entirely. Placing dynamic content at the end of the system prompt preserves the cacheable prefix. Core assumption: Static content can be structurally separated from dynamic content in prompt design.

## Foundational Learning

- **Concept: Prefill vs. Decode Phases in LLM Inference**
  - Why needed here: Prompt caching operates during prefill; understanding this distinction explains why TTFT improves while output generation speed does not.
  - Quick check question: Why does caching reduce time-to-first-token but not tokens-per-second?

- **Concept: KV Tensors in Attention Layers**
  - Why needed here: The cache stores computed key-value tensors; understanding this clarifies why prefix matching is required and why cache writes have cost.
  - Quick check question: What must match exactly for a cache hit to occur?

- **Concept: Multi-Turn Agentic Context Growth**
  - Why needed here: Agentic sessions accumulate tool calls and results, creating tension between context growth and cache stability.
  - Quick check question: Why does a 50-tool-call session not necessarily benefit more from caching than a 10-tool-call session?

## Architecture Onboarding

- **Component map:**
  System prompt (static, cacheable) → UUID boundary marker → Conversation history (dynamic) → Tool calls/results (dynamic)

- **Critical path:**
  1. Ensure system prompt exceeds provider minimum threshold (≥1,024 or ≥4,096 tokens)
  2. Remove all dynamic content from system prompt or place at end
  3. Append UUID after system prompt to exclude conversation history from cache
  4. Optionally append UUIDs after each tool result if excluding tool results

- **Design tradeoffs:**
  - System prompt only: Most consistent benefits, simpler implementation
  - Exclude tool results: Slightly better for some models (GPT-5.2), but more complex
  - Full context: Highest risk of latency regression, avoids manual boundary management

- **Failure signatures:**
  - Cost not decreasing: Prompt below minimum threshold, or dynamic content breaking cache at start
  - TTFT increasing: Full-context mode caching dynamic tool results with no reuse
  - Inconsistent results across sessions: Provider TTL expiration (5 min - 24 hours) clearing cache

- **First 3 experiments:**
  1. Baseline measurement: Run 10 sessions with no-cache mode (UUID at start) to establish cost/TTFT baseline for your workload.
  2. System prompt only: Add UUID after system prompt, measure cost reduction vs. baseline; confirm system prompt size exceeds minimum threshold.
  3. Provider comparison: If using multiple providers, test each with system prompt only mode; expect 41-80% cost savings but variable TTFT improvement (6-31%).

## Open Questions the Paper Calls Out

1. How do common context management strategies (summarization, pruning, sliding windows) interact with prompt caching effectiveness in long-running agents?
2. How does prompt caching perform across diverse agentic task types beyond web search research tasks?
3. What are the security and privacy implications of prompt caching in multi-tenant production deployments?
4. How does dynamic tool discovery via protocols like Model Context Protocol affect cache hit rates?

## Limitations

- Evaluation focuses on PhD-level research tasks with web search tools, limiting generalizability to other agentic workloads
- Provider-specific implementation details remain opaque despite documented behaviors
- Study assumes fixed session duration and tool call patterns, not addressing cache invalidation patterns beyond documented TTL ranges

## Confidence

**High Confidence:**
- Prompt caching reduces API costs by 41-80% when prompts exceed minimum thresholds
- System prompt only caching provides most consistent benefits across providers
- TTFT improvements of 13-31% are achievable with proper cache boundary control
- Cost savings scale linearly with prompt size after minimum thresholds

**Medium Confidence:**
- Full-context caching can paradoxically increase latency due to cache write overhead
- The 10,000-token system prompt size represents a practical sweet spot for research agents
- UUID boundary markers reliably control cache boundaries as documented by providers

**Low Confidence:**
- Exclude tool results strategy provides marginal benefits over system prompt only for GPT-5.2
- The exact cache TTL values (5 minutes to 24 hours) significantly impact multi-session strategies
- Provider-specific cache implementation details don't affect the general effectiveness of boundary control

## Next Checks

1. Implement the same caching strategies for a different agentic task type (e.g., code generation with compiler tool calls) to verify if system prompt only caching remains optimal across domains.

2. Instrument API calls to log cache_read_tokens and cache_write_tokens for each request, confirming that UUID markers create the expected cache boundaries and that dynamic content is properly excluded.

3. Extend session lengths beyond typical research task completion times to measure how cache TTL expiration affects cumulative cost savings and whether the 24-hour inter-condition wait is sufficient to prevent cross-contamination.