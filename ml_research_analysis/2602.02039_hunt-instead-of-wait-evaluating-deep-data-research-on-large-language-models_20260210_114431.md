---
ver: rpa2
title: 'Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models'
arxiv_id: '2602.02039'
source_url: https://arxiv.org/abs/2602.02039
tags:
- data
- insights
- llms
- insight
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating the investigatory
  intelligence of agentic large language models (LLMs), which involves autonomous
  goal-setting and exploration rather than just task execution. To tackle this, the
  authors introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously
  extract insights from structured databases without predefined queries, and DDR-Bench,
  a large-scale benchmark using checklist-based verification for objective evaluation.
---

# Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models

## Quick Facts
- arXiv ID: 2602.02039
- Source URL: https://arxiv.org/abs/2602.02039
- Authors: Wei Liu; Peijie Yu; Michele Orini; Yali Du; Yulan He
- Reference count: 40
- Key outcome: Introduces DDR-Bench benchmark for evaluating autonomous LLM investigative intelligence on structured data, finding top models achieve ~40% accuracy while sustaining long-horizon exploration remains challenging

## Executive Summary
This paper addresses the challenge of evaluating agentic large language models' ability to autonomously investigate and extract insights from structured databases without predefined queries. The authors introduce Deep Data Research (DDR) as an open-ended task and DDR-Bench as a comprehensive benchmark with checklist-based verification for objective evaluation. Through systematic benchmarking of 12 proprietary and open-source models, they demonstrate that while leading models like Claude 4.5 Sonnet exceed 40% accuracy, long-horizon exploration and sustained agency remain significant challenges for current LLMs.

## Method Summary
The authors developed DDR-Bench, a large-scale benchmark featuring open-ended investigation tasks where LLMs must autonomously explore structured databases to extract insights without predefined queries. They employed a checklist-based verification system to enable objective evaluation of the models' outputs. The benchmark was systematically tested across 12 different proprietary and open-source models, measuring their investigatory intelligence through accuracy metrics and analysis of their exploration strategies and implicit planning capabilities.

## Key Results
- Claude 4.5 Sonnet achieves over 40% accuracy on DDR-Bench tasks, while GPT-4o remains below 20%
- Long-horizon exploration and sustained agency present significant challenges across all tested models
- Effective investigatory intelligence depends more on intrinsic exploration strategies and implicit planning than on model scaling or scaffolding

## Why This Works (Mechanism)
The paper demonstrates that investigatory intelligence in LLMs emerges from their ability to autonomously navigate exploration spaces and identify relevant patterns without explicit task definitions. Success in DDR tasks requires models to balance breadth-first and depth-first exploration strategies while maintaining coherent investigation threads over extended time horizons. The checklist-based verification provides objective feedback that enables iterative improvement of these exploration strategies.

## Foundational Learning
- **Checklist-based verification** - Needed for objective evaluation of open-ended tasks; quick check: verify all required insights are captured systematically
- **Autonomous exploration strategies** - Required for navigating unstructured investigation spaces; quick check: assess breadth vs depth trade-offs in exploration paths
- **Implicit planning** - Essential for maintaining investigation coherence; quick check: evaluate consistency of investigation threads across time steps
- **Long-horizon task management** - Critical for sustained agency; quick check: measure degradation in performance over extended task durations
- **Structured data navigation** - Fundamental for DDR tasks; quick check: validate ability to identify relevant database relationships and patterns
- **Goal decomposition** - Necessary for breaking down open-ended investigations; quick check: assess quality of intermediate sub-goals generated

## Architecture Onboarding
- **Component map**: Database -> LLM Agent -> Checklist Verifier -> Feedback Loop
- **Critical path**: Data exploration → Insight extraction → Verification → Strategy refinement
- **Design tradeoffs**: Explicit vs implicit planning, breadth vs depth exploration, verification stringency vs evaluation flexibility
- **Failure signatures**: Early task abandonment, circular exploration patterns, missed key insights, inconsistent investigation threads
- **First experiment**: Compare breadth-first vs depth-first exploration strategies on simple DDR tasks
- **Second experiment**: Test checklist-based verification sensitivity to minor variations in correct answers
- **Third experiment**: Measure performance degradation over progressively longer investigation time horizons

## Open Questions the Paper Calls Out
None

## Limitations
- Current evaluation relies on checklist-based verification, which may not capture nuanced investigative reasoning
- Focus on structured databases limits investigation of open-ended exploration in unstructured domains
- Claims about intrinsic factors (exploration strategies vs scaling) lack controlled ablation studies for causal validation

## Confidence
- High confidence in benchmark design and verification methodology
- Medium confidence in claims about intrinsic factors driving investigatory success
- High confidence in analysis of long-horizon exploration challenges
- Low confidence in generalizability of DDR-Bench to unstructured domains

## Next Checks
1. Conduct controlled ablation studies to isolate impact of exploration strategies versus model scaling on investigatory performance
2. Test DDR-Bench tasks with progressively increasing complexity and open-endedness to characterize limits of sustained agency
3. Validate benchmark's applicability to unstructured data exploration and cross-domain investigative scenarios