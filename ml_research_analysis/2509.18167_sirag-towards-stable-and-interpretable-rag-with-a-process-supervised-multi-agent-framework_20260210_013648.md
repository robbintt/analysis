---
ver: rpa2
title: 'SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent
  Framework'
arxiv_id: '2509.18167'
source_url: https://arxiv.org/abs/2509.18167
tags:
- generator
- retriever
- agent
- each
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of coordinating retriever and
  generator components in Retrieval-Augmented Generation (RAG) systems, which often
  suffer from semantic and functional misalignment. The authors propose SIRAG, a process-supervised
  multi-agent framework featuring two lightweight agents: a Decision Maker that determines
  when to continue retrieval or generate answers, and a Knowledge Selector that filters
  retrieved documents.'
---

# SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework

## Quick Facts
- arXiv ID: 2509.18167
- Source URL: https://arxiv.org/abs/2509.18167
- Reference count: 24
- Key outcome: 9.3% and 9.2% accuracy improvements on multi-hop QA datasets using process-supervised multi-agent framework

## Executive Summary
This paper addresses the challenge of coordinating retriever and generator components in Retrieval-Augmented Generation (RAG) systems, which often suffer from semantic and functional misalignment. The authors propose SIRAG, a process-supervised multi-agent framework featuring two lightweight agents: a Decision Maker that determines when to continue retrieval or generate answers, and a Knowledge Selector that filters retrieved documents. The framework employs an LLM-as-Judge to provide fine-grained process-level supervision, evaluating each intermediate action rather than just final answer correctness. Using tree-structured rollout strategy and training with Proximal Policy Optimization (PPO), the system achieves superior performance on both single-hop and multi-hop question answering benchmarks.

## Method Summary
SIRAG introduces a process-supervised multi-agent framework to coordinate retriever and generator components in RAG systems. The framework employs two lightweight agents: a Decision Maker (DM) that decides whether to continue retrieval or generate answers, and a Knowledge Selector (KS) that filters retrieved documents. Both agents are trained using process-level supervision from an LLM-as-Judge that evaluates intermediate actions during reasoning trajectories. The training employs tree-structured rollout strategy and Proximal Policy Optimization (PPO) with Generalized Advantage Estimation (GAE). The approach is modular and requires no modification to existing retriever or generator components, making it practical for real-world RAG applications.

## Key Results
- Achieved 9.3% accuracy improvement on multi-hop QA datasets compared to baseline methods
- Achieved 9.2% accuracy improvement on single-hop QA datasets compared to baseline methods
- Demonstrated superior interpretability with transparent reasoning trajectories for each decision point

## Why This Works (Mechanism)
The framework addresses semantic and functional misalignment between retriever and generator components by introducing process-level supervision. Instead of only evaluating final answers, the LLM-as-Judge provides feedback on each intermediate action, allowing agents to learn optimal retrieval and filtering strategies. The two-agent design (DM and KS) creates a modular decision-making process where DM controls the retrieval-generation balance and KS ensures document quality. Tree-structured rollout enables efficient exploration of multiple reasoning paths, while PPO training stabilizes the learning process.

## Foundational Learning
- **LLM-as-Judge process supervision**: Evaluates intermediate actions rather than just final answers; needed to provide fine-grained feedback for agent learning; quick check: verify judge consistency across similar action sequences
- **Proximal Policy Optimization (PPO)**: Policy gradient method with clipped objective for stable training; needed to optimize agent policies with process rewards; quick check: monitor policy loss and KL divergence during training
- **Tree-structured rollout**: Explores multiple reasoning paths in parallel; needed for efficient exploration of decision space; quick check: verify branching factor and depth constraints are maintained
- **Generalized Advantage Estimation (GAE)**: Reduces variance in policy gradient estimates; needed for stable value function learning; quick check: compare advantage estimates with and without GAE
- **Multi-agent coordination**: Separate DM and KS agents with specialized roles; needed to decouple retrieval control from document filtering; quick check: analyze action distributions for each agent type
- **Knowledge selection filtering**: KS agent removes irrelevant documents before generation; needed to improve generator input quality; quick check: measure document relevance scores before and after KS filtering

## Architecture Onboarding

Component Map: Retriever -> DM -> KS -> Generator -> LLM-as-Judge

Critical Path: Question -> Retriever -> DM (retrieve/stop decision) -> KS (document filtering) -> Generator -> LLM-as-Judge evaluation -> Policy update

Design