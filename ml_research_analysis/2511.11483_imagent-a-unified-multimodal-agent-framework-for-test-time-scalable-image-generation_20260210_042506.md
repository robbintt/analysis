---
ver: rpa2
title: 'ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image
  Generation'
arxiv_id: '2511.11483'
source_url: https://arxiv.org/abs/2511.11483
tags:
- image
- prompt
- generation
- action
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ImAgent, a training-free unified multimodal\
  \ agent framework designed to improve test-time scalability for image generation\
  \ and editing tasks. By integrating reasoning, generation, and self-evaluation within\
  \ a single model, ImAgent dynamically selects and coordinates multiple predefined\
  \ generation actions\u2014such as prompt enhancement with chain-of-thought reasoning,\
  \ image detail refinement, and best-of-N sampling\u2014through a policy controller."
---

# ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation

## Quick Facts
- arXiv ID: 2511.11483
- Source URL: https://arxiv.org/abs/2511.11483
- Reference count: 40
- Primary result: Training-free unified multimodal agent achieving 114.8% improvement on RISEBench and 21.2% on WISE benchmarks

## Executive Summary
ImAgent introduces a training-free unified multimodal agent framework that enhances test-time scalability for image generation and editing tasks. The framework integrates reasoning, generation, and self-evaluation within a single model, using a policy controller to dynamically select from multiple predefined generation actions including prompt enhancement, image detail refinement, and best-of-N sampling. Unlike existing approaches requiring separate modules, ImAgent operates entirely within a unified model without external dependencies, achieving significant performance improvements on both generation and editing benchmarks.

## Method Summary
ImAgent employs an iterative agent loop where a policy controller, prompted with the current state (including prompt, image, and observation history), outputs discrete actions from a predefined action space. These actions include Naive Generation, Prompt Enhancement with Chain-of-Thought reasoning, Prompt Revision, Image Detail Refinement, and Best-of-N Sampling. The framework operates with a unified multimodal model (Bagel or Janus-Pro-7B) that supports both understanding and generation capabilities. The agent terminates after reaching STOP action or maximum T_max=5 steps, with fallback to Naive Generation for invalid actions. Action prompts are provided in Figure 3 (policy controller) and Figures 5-8 (action templates).

## Key Results
- Achieves 114.8% improvement on RISEBench for image editing tasks
- Delivers 21.2% improvement on WISE benchmark for image generation
- Outperforms backbone models by 14.8% (Bagel) and 7.5% (Janus-Pro-7B) on R2I-Bench
- Maintains high robustness with Parse Success Rate (PSR), Action Uniqueness Rate (AUR), and Action Validity Rate (AVR) near 100%

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Policy Control
A policy controller can adaptively route generation tasks to appropriate refinement actions without external models. The controller receives current state and outputs discrete actions that update observation history iteratively. This works because the unified model's understanding capability is sufficiently reliable to make action selection decisions. Evidence shows guided action interaction enhances image fidelity without external models. Break condition occurs when backbone lacks instruction-following capability for policy controller prompts.

### Mechanism 2: Chain-of-Thought Prompt Enhancement
Chain-of-Thought prompt enhancement transfers reasoning capability from the understanding module to improve generation quality. The model generates intermediate reasoning chains that clarify ambiguous instructions and fill missing semantic details before generation. This works because the unified model's language reasoning capability transfers to visual generation pathways. Evidence from supplementary materials shows elaborated prompts benefit T2I backbones. Break condition occurs when original prompts are already highly specific and enhancement introduces unnecessary complexity.

### Mechanism 3: Self-Evaluation Best-of-N Sampling
Best-of-N sampling with internal evaluation effectively reduces stochastic variance inherent in T2I generation. The framework generates multiple candidates and evaluates them using the model's own understanding capability, selecting the highest-scoring output. This works because the model's self-evaluation capability is sufficiently discriminative to identify quality differences. Evidence shows backbone models exhibit similar action selection trends favoring Best-of-N at later stages. Break condition occurs when evaluation is poorly calibrated, potentially amplifying biases rather than reducing variance.

## Foundational Learning

- **Unified Multimodal Models**: Single models supporting both understanding and generation are essential for ImAgent's architecture. Without unification, external modules would be needed for observation history influence. Quick check: Can you explain why a unified model enables "observation history" to influence action selection without cross-model communication overhead?

- **Test-Time Scaling**: The core thesis is improving generation quality through additional inference-time computation rather than training. Understanding compute-quality tradeoffs is essential for evaluating the approach. Quick check: How does ImAgent's adaptive action selection differ from fixed iterative refinement in terms of compute allocation?

- **Policy Gradient vs. Training-Free Control**: ImAgent uses a training-free policy controller based on prompting, not learned policies. This distinction clarifies why robustness analysis (PSR, AUR, AVT) matters for a non-learned controller. Quick check: Why might a training-free policy controller require fallback strategies that a trained policy would not?

## Architecture Onboarding

- **Component map**: User input -> State Manager (maintains P_0, I_0, P_t, I_t, O_{t-1}) -> Policy Controller (outputs action via \boxed{action} format) -> Action Pool (5 predefined actions) -> Unified Backbone (Bagel/Janus-Pro-7B) -> Output Image

- **Critical path**: 1) User provides (P_0, I_0) → State initialized 2) Policy controller receives state → Outputs action a_t 3) Action executes → Produces (P_{t+1}, I_{t+1}, o_t) 4) Observation history updates → Loop until STOP or T_max

- **Design tradeoffs**: T_max=5 limits compute but may truncate beneficial refinement (avg 3.98-4.73 steps used). N=4 for Best-of-N balances variance reduction against generation cost. Regex parsing over JSON improves parse success to ~100% given unified models' unreliable structured output.

- **Failure signatures**: High iteration count without improvement suggests policy controller stuck in refinement loops. Invalid action outputs indicate prompt formatting issues. Semantic drift across revisions may result from over-correction in Prompt Revision.

- **First 3 experiments**: 1) Reproduce backbone comparison on R2I-Bench verifying 14.8% and 7.5% improvements. 2) Ablate single actions per Table 9 methodology to identify contribution patterns. 3) Stress test robustness with malformed prompts to verify PSR/AUR/AVT metrics and fallback behavior.

## Open Questions the Paper Calls Out

- **Open Question 1**: Why does ImAgent built on Bagel underperform the Self-Rewrite baseline on the WISE benchmark? The paper hypothesizes dataset-specific familiarity but lacks conclusive validation, as GPT-4o rewriting yielded 0.64 vs. Self-Rewrite's 0.70.

- **Open Question 2**: What drives the significant difference in average decision length (4.73 vs. 3.98 iterations) between Bagel and Janus-Pro-7B backbones? The paper speculates about conservative vs. decisive behavior but provides no causal mechanism.

- **Open Question 3**: How does ImAgent's performance scale with increasing T_max and N values? The paper uses fixed hyperparameters T_max=5 and N=4 without systematic study of compute-quality trade-offs or adaptive budgeting.

## Limitations
- The robustness metrics (PSR/AUR/AVT) report near-perfect scores but are evaluated on curated benchmarks rather than adversarial or real-world usage patterns, raising concerns about generalization.
- Comparisons against strong baselines lack variance reporting across multiple seeds, making statistical significance difficult to assess and potentially overstating performance claims.
- The Best-of-N evaluation mechanism relies on internal self-evaluation without independent validation against human judgments or external metrics, creating potential evaluation bias.

## Confidence

- **High confidence**: The core agent architecture (policy controller + action pool + iterative loop) is well-specified and reproducible with sound technical foundation.
- **Medium confidence**: Reported benchmark improvements are based on reasonable methodology but lack variance reporting and comprehensive baseline diversity.
- **Low confidence**: Robustness analysis conclusions may not generalize beyond specific benchmarks tested, and self-evaluation mechanism for Best-of-N lacks external validation.

## Next Checks

1. **Statistical validation**: Re-run all benchmark comparisons with 5-10 different random seeds and report mean ± standard deviation to verify statistical significance of reported improvements.

2. **External evaluation validation**: For Best-of-N sampling, compare the model's self-selected best image against human preference rankings or external quality metrics (e.g., FID, CLIP similarity) to quantify internal evaluation accuracy.

3. **Adversarial robustness test**: Create a small test set of malformed or ambiguous prompts designed to trigger edge cases and evaluate whether robustness metrics degrade and fallback mechanisms activate appropriately.