---
ver: rpa2
title: A.X K1 Technical Report
arxiv_id: '2601.09200'
source_url: https://arxiv.org/abs/2601.09200
tags:
- training
- reasoning
- data
- arxiv
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A.X K1 is a 519B-parameter Mixture-of-Experts (MoE) language model
  trained from scratch to balance high reasoning capability with practical inference
  efficiency. By leveraging scaling laws for MoE architectures and vocabulary size,
  it uses 33B active parameters and a 160K vocabulary to maximize performance under
  fixed computational constraints.
---

# A.X K1 Technical Report

## Quick Facts
- arXiv ID: 2601.09200
- Source URL: https://arxiv.org/abs/2601.09200
- Reference count: 12
- Primary result: 519B-parameter MoE model with 33B active params achieving 80.2 KMMLU and 89.8 AIME25 scores

## Executive Summary
A.X K1 is a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch to balance high reasoning capability with practical inference efficiency. By leveraging scaling laws for MoE architectures and vocabulary size, it uses 33B active parameters and a 160K vocabulary to maximize performance under fixed computational constraints. The model supports user-controllable switching between thinking and non-thinking modes via the Think-Fusion training recipe, enabling flexible trade-offs between reasoning depth and response latency. Evaluations show A.X K1 achieves competitive performance with leading open-source models, excelling particularly in Korean-language benchmarks (e.g., KMMLU: 80.2, CLIcK: 84.9) and high-reasoning math tasks (AIME25: 89.8). The work demonstrates the feasibility of end-to-end engineering for 500B+ parameter models and establishes a foundation for scaling toward frontier-class development.

## Method Summary
A.X K1 is a 519B-parameter MoE model with 33B active parameters, trained through a 3-stage pipeline: (1) 7T tokens of general knowledge data with stable learning rate, (2) 1.66T tokens of high-quality reasoning data with learning rate decay, and (3) 600B tokens for long-context extension (4K→32K). The model uses 192 routed experts plus 1 shared expert with granularity G=7, dual RMSNorm normalization, and Multi-head Latent Attention. Post-training employs a Think-Fusion recipe: separate Instruct and Reasoning SFT models are linearly merged (α=0.8), then fine-tuned on Mode-Overlap Dataset to resolve mode confusion, followed by on-policy RL (DAPO + GSPO) with format-aware rewards.

## Key Results
- Achieves 80.2 score on Korean-language KMMLU benchmark
- Scores 89.8 on AIME25 math reasoning task
- Uses 33B active parameters out of 519B total through sparse MoE activation
- Supports user-controllable switching between thinking and non-thinking modes

## Why This Works (Mechanism)

### Mechanism 1: Sparse MoE Activation for Inference Efficiency
Under fixed hardware constraints, MoE with 33B active / 519B total parameters can approximate dense-model reasoning capacity while reducing per-token compute. Only a subset of experts (routed + shared) process each token, with 192 routed experts using top-k routing plus 1 shared expert, activating ~33B parameters per forward pass. Granularity G=7 (below the 8–12 optimal range) was chosen to favor stability under imperfect routing. Dual RMSNorm (pre/post MoE block) mitigates early-training loss spikes. Core assumption: Expert load balancing remains sufficiently uniform at scale; routing collapse does not degrade the granularity tradeoff.

### Mechanism 2: Think-Fusion via Model Merging + Mode-Overlap SFT
Linearly merging a reasoning-tuned checkpoint with an instruction-tuned checkpoint, then fine-tuning on paired thinking/non-thinking responses, yields a unified model that switches modes via control tokens without separate deployments. Train θ_think (Reasoning SFT with arrayWith no explicit tag); train θ_non-think (Instruct SFT without tag for the same prompts. SFT on D_mix aligns the merged space so the model respects the mode signal rather than always generating thoughts. Core assumption: The merged parameter space contains separable subspaces that can be re-aligned by MOD; mode confusion is correctable without catastrophic forgetting of reasoning priors.

### Mechanism 3: Scaling-Law-Guided Vocabulary and Compute Allocation
Under a fixed compute budget, vocabulary size can be optimized jointly with model scale to improve token efficiency and downstream performance. Derivative-based estimation yields ~132,500 optimal vocabulary size; the authors increase it 25% to 163,840 (≈160K) to accommodate a 10T-token regime and hardware alignment. Compute budget C≈2.548×10^24 FLOPs (75 days on 1,024 H200s) informs active-parameter choice and training stages. Core assumption: The 25% vocabulary uplift continues to yield net gains in the target multilingual/Korean-heavy distribution without incurring prohibitive softmax latency.

## Foundational Learning

- **Concept**: Mixture-of-Experts (MoE) routing and load balancing
  - **Why needed here**: A.X K1's efficiency claim hinges on sparse expert activation; understanding routing, auxiliary losses, and expert parallelism is essential to diagnose training instability or inference bottlenecks.
  - **Quick check question**: Given a top-k router with k experts per token, what happens if one expert receives >50% of routing weight?

- **Concept**: Supervised Fine-Tuning (SFT) vs. Reinforcement Learning (RL) from human feedback
  - **Why needed here**: The post-training pipeline separates Instruct/Reasoning SFT, then applies on-policy RL (DAPO + GSPO) to refine instruction-following with format-aware rewards.
  - **Quick check question**: Why might token-level RL objectives be unstable for MoE models compared to sequence-level losses?

- **Concept**: Scaling laws for compute-optimal training
  - **Why needed here**: Architecture and vocabulary choices are derived from FLOPs-budgeted scaling laws under a 75-day constraint.
  - **Quick check question**: If your compute budget doubles, should you increase model size, training tokens, or both per Chinchilla-style scaling?

## Architecture Onboarding

- **Component map**: MoE Transformer with MLA -> 192 routed + 1 shared experts -> Dual RMSNorm -> BBPE tokenizer (160K vocab) -> 3-stage WSD training -> Dual SFT branches -> Linear merge (α=0.8) -> Think-Fusion SFT on MOD -> On-policy RL (DAPO + GSPO)

- **Critical path**: Pre-train MoE backbone with WSD scheduler and MTP loss (scaled 0.3 → 0.1) → Run dual SFT branches (non-thinking and thinking) from same pre-train checkpoint → Merge checkpoints, then fine-tune on MOD to resolve mode confusion → Apply on-policy RL with R_total = R_correct + R_format

- **Design tradeoffs**: G=7 granularity favors stability over theoretical optimality under imperfect routing; Vocabulary size (160K) trades softmax overhead for Korean/multilingual efficiency; Dual normalization adds ops but reduces loss spikes; α=0.8 in merging biases toward reasoning to preserve thinking capability before RL

- **Failure signatures**: Persistent loss spikes early in training → check dual normalization, batch ramp-up, and expert load balance; Mode confusion ( tag in non-thinking responses) → verify MOD coverage and format-aware reward penalties; Under-utilized experts (routing collapse) → inspect router bias update rate and auxiliary-loss coefficient

- **First 3 experiments**: (1) Reproduce small-scale MoE dry-run (20B total / 3B active) with/without dual RMSNorm; compare early loss curves to Fig. 1; (2) Validate Think-Fusion on 1B model: train Instruct/Reasoning SFT branches, merge with α=0.8, fine-tune on small MOD; measure appearance rate in non-thinking mode; (3) Benchmark tokenization efficiency and latency for vocabulary sizes {100K, 132K, 160K} on Korean/English/code corpora to test 25% uplift assumption

## Open Questions the Paper Calls Out
None

## Limitations

- **Sparse MoE Activation Efficiency**: The claimed efficiency gains depend heavily on routing stability, and the G=7 granularity below optimal range may not hold under all routing distributions.
- **Think-Fusion Mode Switching Reliability**: The paper lacks comprehensive ablation studies on the merge coefficient α and the sufficiency of MOD fine-tuning to prevent persistent mode confusion.
- **Vocabulary Scaling Validation**: The 25% increase from theoretical 132.5K to 160K vocabulary lacks direct latency and throughput measurements comparing different vocabulary sizes.

## Confidence

- **High Confidence**: Korean-language benchmark performance (KMMLU: 80.2, CLIcK: 84.9) and math task results (AIME25: 89.8) are clearly reported with specific numbers. The 3-stage pre-training pipeline and post-training stages are well-specified with concrete parameters.

- **Medium Confidence**: The efficiency claims from MoE architecture rely on assumptions about routing stability and effectiveness of dual normalization, supported by internal experiments but lacking direct comparison to alternatives.

- **Low Confidence**: Think-Fusion mechanism's robustness to mode confusion and vocabulary scaling benefits are less substantiated, with no ablation studies or external validation provided.

## Next Checks

1. **Routing Stability Experiment**: Implement small-scale MoE (20B total / 3B active) with G=7 granularity and dual RMSNorm. Compare early loss curves and expert load balancing logs against baseline with optimal G=8-12 and single normalization. Measure frequency and severity of routing collapse.

2. **Think-Fusion Ablation Study**: Train 1B model with full Think-Fusion pipeline. Systematically vary merge coefficient α (0.5, 0.8, 0.95) and measure rate of tag appearance in non-thinking mode responses. Assess whether increasing MOD fine-tuning data or adjusting RL reward penalties can fully resolve mode confusion.

3. **Vocabulary Size Latency Benchmark**: Implement tokenizers with vocabulary sizes {100K, 132K, 160K} and measure softmax and embedding lookup latency on Korean/English/code corpora. Compare token efficiency and end-to-end inference latency to determine if 160K vocabulary introduces bottleneck that negates claimed efficiency gains.