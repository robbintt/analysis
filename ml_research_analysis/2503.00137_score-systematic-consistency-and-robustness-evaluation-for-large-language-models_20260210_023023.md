---
ver: rpa2
title: 'SCORE: Systematic COnsistency and Robustness Evaluation for Large Language
  Models'
arxiv_id: '2503.00137'
source_url: https://arxiv.org/abs/2503.00137
tags:
- answer
- llama-3
- accuracy
- robustness
- letter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models are typically evaluated using a single accuracy
  metric, which overlooks their robustness to input variations. This work introduces
  SCORE, a systematic framework that evaluates LLM consistency and robustness by repeatedly
  testing models across multiple semantically equivalent prompts, random seeds, and
  answer choice orders.
---

# SCORE: Systematic COnsistency and Robustness Evaluation for Large Language Models

## Quick Facts
- **arXiv ID**: 2503.00137
- **Source URL**: https://arxiv.org/abs/2503.00137
- **Reference count**: 5
- **Primary result**: Accuracy varies by up to 15% across prompt formulations, and consistency rates remain low even for high-performing models.

## Executive Summary
Large language models are typically evaluated using a single accuracy metric, which overlooks their robustness to input variations. This work introduces SCORE, a systematic framework that evaluates LLM consistency and robustness by repeatedly testing models across multiple semantically equivalent prompts, random seeds, and answer choice orders. Experiments show that accuracy can vary by up to 15% depending on prompt or choice order, while consistency rates remain low even for high-performing models. The analysis reveals that higher accuracy does not guarantee better consistency, and model size alone is not a reliable predictor of robustness. SCORE enables a more realistic assessment of LLM capabilities, highlighting the need for multi-scenario evaluation in real-world applications.

## Method Summary
SCORE systematically evaluates LLM robustness by testing models across multiple dimensions of input variation. The framework employs three perturbation strategies: (1) semantically equivalent prompt variations to test sensitivity to phrasing, (2) random seed variations to assess stochastic behavior, and (3) answer choice order permutations to measure order dependency. Models are evaluated across 14 benchmark datasets using multiple-choice and single-answer formats. Consistency is measured as the proportion of identical outputs across repeated evaluations under different input conditions. The framework calculates both accuracy and consistency metrics to provide a comprehensive assessment of model performance and robustness.

## Key Results
- Accuracy varies by up to 15% depending on prompt formulation or answer choice order
- High-performing models show low consistency rates across repeated evaluations
- Model size alone is not a reliable predictor of robustness or consistency

## Why This Works (Mechanism)
The framework works by exposing models to controlled variations in input presentation that commonly occur in real-world deployment. By measuring consistency across these variations, SCORE reveals hidden reliability issues that single-shot accuracy metrics miss. The multi-dimensional perturbation approach captures different failure modes: semantic variations expose brittleness to phrasing, random seeds reveal stochastic instability, and answer order permutations detect position bias. This comprehensive evaluation provides a more complete picture of model robustness than traditional accuracy-based benchmarks.

## Foundational Learning

**Semantic equivalence testing**: Evaluating model responses to semantically identical but syntactically different prompts. Why needed: Language models can produce different outputs for the same meaning expressed differently. Quick check: Compare model outputs across paraphrased versions of identical questions.

**Stochastic behavior measurement**: Assessing model consistency across different random seeds. Why needed: Many LLMs incorporate randomness in their generation process. Quick check: Run identical prompts with different seeds and measure output variance.

**Order sensitivity analysis**: Testing model robustness to answer choice permutations. Why needed: Position bias can affect model predictions in multiple-choice settings. Quick check: Shuffle answer options and measure prediction stability.

**Multi-scenario evaluation**: Comprehensive assessment across diverse input conditions. Why needed: Single metric evaluations miss important robustness dimensions. Quick check: Evaluate models under multiple perturbation conditions simultaneously.

**Consistency metrics**: Quantifying output stability across repeated evaluations. Why needed: Traditional accuracy metrics don't capture reliability. Quick check: Calculate consistency rate as proportion of identical outputs across variations.

## Architecture Onboarding

**Component map**: Benchmark datasets -> Prompt variations -> Model inference -> Output comparison -> Consistency calculation -> Results aggregation

**Critical path**: Prompt generation → Model evaluation → Output comparison → Consistency scoring → Result analysis

**Design tradeoffs**: SCORE prioritizes comprehensive robustness assessment over single-metric simplicity. The framework sacrifices evaluation speed for thoroughness by requiring multiple runs per question, but gains deeper insights into model reliability that single evaluations miss.

**Failure signatures**: Low consistency rates across prompt variations indicate sensitivity to phrasing. Significant accuracy drops when shuffling answer choices suggest position bias. High variance across random seeds reveals stochastic instability in model predictions.

**First experiments**:
1. Run a single benchmark dataset with three prompt variations and measure consistency rate
2. Test answer choice order sensitivity by shuffling options for a subset of questions
3. Compare consistency metrics across models of different sizes using the same benchmarks

## Open Questions the Paper Calls Out
- How can consistency metrics be effectively adapted for open-ended generation tasks?
- What are the implications of low consistency for real-world deployment scenarios?
- How does training data contamination affect the reliability of robustness evaluations?

## Limitations
- Analysis limited to multiple-choice and single-answer formats, may not generalize to open-ended generation
- 15% accuracy variation represents upper bound effect, some datasets showed minimal variation
- Does not account for potential training data overlap between benchmarks and model pretraining
- Focuses on input-side perturbations, not internal consistency across model components

## Confidence
- **High**: Accuracy alone is insufficient for LLM evaluation, consistency varies significantly across prompt variations
- **Medium**: Model size does not predict robustness, given specific models tested
- **Low**: Generalization to non-multiple-choice tasks or proprietary models not included

## Next Checks
1. Replicate consistency experiments on open-ended generation tasks using semantic similarity metrics
2. Analyze training data contamination by checking for benchmark overlap with model pretraining corpora
3. Extend framework to measure temporal consistency by evaluating same prompts across multiple time periods