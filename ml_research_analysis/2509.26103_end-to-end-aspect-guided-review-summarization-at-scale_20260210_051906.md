---
ver: rpa2
title: End-to-End Aspect-Guided Review Summarization at Scale
arxiv_id: '2509.26103'
source_url: https://arxiv.org/abs/2509.26103
tags:
- reviews
- product
- aspects
- aspect
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a scalable LLM-based system for aspect-guided
  review summarization in e-commerce. The approach extracts aspect-sentiment pairs
  from reviews, consolidates them to canonical forms, selects the most frequent aspects,
  and generates product summaries guided by representative reviews.
---

# End-to-End Aspect-Guided Review Summarization at Scale

## Quick Facts
- arXiv ID: 2509.26103
- Source URL: https://arxiv.org/abs/2509.26103
- Reference count: 14
- One-line primary result: Scalable LLM-based system for aspect-guided review summarization deployed in A/B test showing 0.3% CVR lift and 84% error-free summaries

## Executive Summary
This paper presents a scalable LLM-based system for aspect-guided review summarization in e-commerce that combines aspect-based sentiment analysis with LLM summarization to generate concise, interpretable summaries grounded in actual customer feedback. The approach extracts aspect-sentiment pairs from reviews, consolidates them to canonical forms, selects the most frequent aspects, and generates product summaries guided by representative reviews. The pipeline was deployed in a large-scale online A/B test, showing a 0.3% increase in Add to Cart Rate, a 0.5% increase in conversion rate, and a 0.13% decrease in bounce rate, with no negative impact on revenue or page speed. Offline evaluation of 341 products showed 84% of summaries had no errors, with remaining issues mostly minor.

## Method Summary
The system uses a four-stage pipeline with Gemini 1.5 Flash: (1) Extract up to 5 aspect-sentiment pairs (POSITIVE/MIXED/NEGATIVE) per review via LLM prompting; (2) Consolidate aspects - keep aspects above 95th percentile frequency (30+ occurrences), map others to canonical forms via LLM; (3) Select top 5 aspects per product, sample up to 200 reviews total with weighted proportional sampling; (4) Generate 300-500 character summaries via aspect-guided prompts. The system updates summaries only when new reviews exceed 10% of existing count, balancing freshness with computational efficiency. The authors released a dataset of 11.8 million anonymized reviews across 92,000 products with extracted aspects and generated summaries.

## Key Results
- Online A/B test: 0.3% increase in Add to Cart Rate, 0.5% increase in conversion rate, 0.13% decrease in bounce rate
- No negative impact on revenue or page speed
- Offline evaluation: 84% of summaries had no errors, with remaining issues mostly minor
- Dataset released: 11.8 million anonymized reviews across 92,000 products

## Why This Works (Mechanism)

### Mechanism 1: Structured Context Grounding
The pipeline constrains the generation space by decoupling information extraction from synthesis, forcing the LLM to generate summaries strictly based on extracted aspect-sentiment pairs and sampled reviews. This reduces hallucination frequency compared to open-ended summarization of raw text.

### Mechanism 2: Frequency-Thresholded Consolidation
Using a 95th percentile frequency threshold, the system maps low-frequency aspects to high-level canonical forms, improving summary coherence by filtering noisy, idiosyncratic terminology while preserving dominant themes.

### Mechanism 3: Sentiment-Proportional Sampling
Reviews are sampled proportionally to the frequency of aspect-sentiment pairs, ensuring the generated summary reflects the true statistical distribution of customer opinion and guards against the "vocal minority" effect.

## Foundational Learning

- **Concept: Aspect-Based Sentiment Analysis (ABSA)**
  - Why needed here: This is the input parser. Without understanding how to extract (entity, sentiment) pairs from raw text, the subsequent consolidation and summarization steps cannot exist.
  - Quick check question: Can you distinguish between extracting "The screen is great" (Positive sentiment on 'screen') vs. "The battery life is short" (Negative sentiment on 'battery')?

- **Concept: Hallucination Mitigation via Grounding**
  - Why needed here: The paper explicitly positions itself against standard LLM summarization which "is prone to hallucination."
  - Quick check question: If an LLM generates a summary claiming a product is "waterproof," but no review mentions waterproofing, is this a hallucination or an inference?

- **Concept: Prompt Chaining / Pipeline Architecture**
  - Why needed here: The system is not a single model call but a chain: Extract → Consolidate → Sample → Summarize.
  - Quick check question: If the Consolidation step outputs invalid JSON, does the Summarization step fail or does it fall back to raw text?

## Architecture Onboarding

- **Component map:** ABSA Extractor → Consolidator → Selector → Summarizer
- **Critical path:** The Consolidator. If this step maps "fire hazard" to "quality" (too generic), the summary loses actionable detail. If it fails to map, the vocabulary becomes too fragmented.
- **Design tradeoffs:**
  - *Latency vs. Freshness:* The system updates summaries only when new reviews > 10% of existing count. This saves compute but creates lag for trending changes.
  - *Cost vs. Accuracy:* Using a Flash model implies lower cost but potentially lower ABSA precision compared to larger models.
- **Failure signatures:**
  - Empty JSON outputs: LLM fails to extract aspects (mitigate with fallback prompts)
  - Summarized wrong product: Context window contamination if caching keys are mismanaged
  - Generic outputs: "This product is good" implies the sampling strategy failed to find specific aspect-bearing reviews
- **First 3 experiments:**
  1. Consolidation Ablation: Turn off the consolidation step for a sample of products. Does the summary coherence drop, or does it successfully cover more "long-tail" issues?
  2. Context Window Sensitivity: Vary the `200 reviews` cap (e.g., 50 vs 200). Does the Conversion Rate (CVR) change, suggesting users prefer brevity over detail?
  3. Aspect Limit Stress Test: Force the system to include the 6th-10th most frequent aspects. Do these provide signal or just noise to the user?

## Open Questions the Paper Calls Out
None

## Limitations
- The consolidation step's 95th percentile threshold appears arbitrary without justification from prior literature or ablation studies
- The claim that consolidation improves coherence is supported only by the heuristic without comparative analysis
- The paper does not address potential biases introduced by the LLM's pre-training data affecting aspect extraction or summarization

## Confidence

- **High Confidence**: The offline evaluation methodology (84% error-free summaries) and the core pipeline architecture are well-specified and reproducible.
- **Medium Confidence**: The A/B test results showing CVR lift and reduced bounce rate are credible given the scale but require verification of experimental design details.
- **Low Confidence**: The absence of specific few-shot examples in the prompts is a critical unknown for faithful reproduction.

## Next Checks

1. **Prompt Example Verification**: Reconstruct the exact few-shot examples for each LLM stage using the provided templates, then run a small-scale pilot to measure if the reproduced pipeline matches the reported 84% error-free rate.

2. **Threshold Sensitivity Analysis**: Vary the consolidation threshold (e.g., 90th, 95th, 99th percentile) on a sample of products to quantify the tradeoff between vocabulary coherence and coverage of rare but important aspects.

3. **A/B Test Design Audit**: Review the experimental setup for potential confounders—specifically, verify that the control and treatment groups were balanced by product category, price range, and review volume to ensure the CVR lift is attributable to the summarization system.