---
ver: rpa2
title: The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest
  Ads Conversion
arxiv_id: '2505.05605'
source_url: https://arxiv.org/abs/2505.05605
tags:
- embedding
- training
- click
- overfitting
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key challenges in training large embedding
  tables for Pinterest Ads Conversion models: slow convergence due to gradient sparsity
  and multi-epoch overfitting caused by label sparsity. To tackle these issues, the
  authors introduce a Sparse Optimizer that applies a higher layer-specific learning
  rate to embedding tables, significantly speeding up convergence.'
---

# The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion

## Quick Facts
- arXiv ID: 2505.05605
- Source URL: https://arxiv.org/abs/2505.05605
- Reference count: 40
- Primary result: Introduces Sparse Optimizer and Frequency-Adaptive Learning Rate (FAL) methods for embedding table optimization and multi-epoch training in Pinterest Ads Conversion models

## Executive Summary
This paper addresses critical challenges in training large embedding tables for Pinterest's Ads Conversion models, focusing on slow convergence due to gradient sparsity and multi-epoch overfitting caused by label sparsity. The authors propose two key innovations: a Sparse Optimizer that applies higher layer-specific learning rates to embedding tables for faster convergence, and a Frequency-Adaptive Learning Rate (FAL) method that selectively reduces learning rates for infrequent rows to mitigate overfitting. Offline experiments on large-scale industrial datasets demonstrate significant improvements over production baselines, with the Sparse Optimizer already deployed in production systems. The findings suggest that while both methods provide performance gains, their effectiveness diminishes after several days of continual training, indicating that overfitting may be less critical when fresh data is available.

## Method Summary
The paper introduces two complementary approaches for optimizing embedding table training. The Sparse Optimizer addresses gradient sparsity by applying a higher learning rate specifically to embedding tables, enabling faster convergence compared to traditional optimizers that treat all parameters uniformly. The Frequency-Adaptive Learning Rate (FAL) method tackles multi-epoch overfitting by analyzing row access frequency and reducing learning rates for infrequently accessed rows while maintaining standard rates for frequently accessed ones. FAL incorporates a decay mechanism that progressively reduces the learning rate difference between infrequent and frequent rows over training epochs. Both methods are evaluated on large-scale industrial datasets, with the Sparse Optimizer showing immediate production deployment benefits and FAL demonstrating superior performance compared to embedding re-initialization baselines.

## Key Results
- Sparse Optimizer significantly improves convergence speed and achieves better performance compared to production models
- FAL effectively reduces multi-epoch overfitting, outperforming embedding re-initialization (MEDA) in most cases
- Performance gains from both methods diminish after several days of continual training, suggesting overfitting may not be critical with fresh data availability

## Why This Works (Mechanism)
The Sparse Optimizer works by recognizing that embedding tables have unique optimization characteristics due to their high dimensionality and sparse gradient updates. By applying a higher layer-specific learning rate to embedding tables, the optimizer can more effectively navigate the optimization landscape and reach better local minima faster. The FAL method leverages the observation that infrequent rows are more prone to overfitting because they receive fewer gradient updates per epoch. By reducing the learning rate for these rows while maintaining higher rates for frequent rows, FAL creates a more balanced optimization process that prevents the model from overfitting to rare patterns while still learning effectively from common patterns.

## Foundational Learning
- Gradient sparsity in embedding tables: Why needed - embedding tables receive sparse updates during training, requiring specialized optimization strategies; Quick check - measure gradient density and update frequency per row
- Multi-epoch overfitting: Why needed - models can overfit to training data when multiple epochs are used, especially with label sparsity; Quick check - monitor validation performance across training epochs
- Frequency-based parameter adaptation: Why needed - different rows in embedding tables have varying importance based on access frequency; Quick check - analyze row access distribution and correlation with model performance

## Architecture Onboarding
Component map: Input features -> Embedding tables -> Sparse Optimizer/FAL -> Model layers -> Output predictions
Critical path: Feature processing → Embedding lookup → Parameter update (Sparse Optimizer/FAL) → Forward pass → Loss computation
Design tradeoffs: Higher learning rates for embeddings vs. stability, frequency-based adaptation vs. complexity, multi-epoch training vs. overfitting risk
Failure signatures: Diminishing returns after several days, inconsistent performance across different frequency thresholds, computational overhead from frequency tracking
First experiments: 1) Compare convergence speed with and without Sparse Optimizer, 2) Test FAL performance across different frequency thresholds, 3) Measure long-term stability of both methods over extended training periods

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of extensive online A/B testing to validate offline improvements in real production environments
- Limited discussion of computational overhead and memory usage implications for industrial deployment
- Effectiveness of both methods diminishes after several days of continual training, suggesting potential long-term limitations

## Confidence
High confidence in Sparse Optimizer's convergence improvement due to clear experimental evidence and production deployment
Medium confidence in FAL's effectiveness for multi-epoch training, as results show some inconsistency across settings
Medium confidence in conclusions about overfitting being less critical with fresh data, based on limited temporal observations

## Next Checks
1. Conduct extended online A/B tests measuring both model performance and computational efficiency over multiple weeks of production deployment
2. Perform ablation studies to quantify the impact of proposed methods on memory usage and training time compared to baseline approaches
3. Test the methods across different industrial recommendation systems and dataset sizes to evaluate generalizability beyond Pinterest's specific use case