---
ver: rpa2
title: Polynomial Contrastive Learning for Privacy-Preserving Representation Learning
  on Graphs
arxiv_id: '2509.25205'
source_url: https://arxiv.org/abs/2509.25205
tags:
- learning
- graph
- loss
- poly-grace
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Poly-GRACE enables privacy-preserving graph representation learning
  by replacing non-polynomial operations in the GRACE framework with polynomial-friendly
  alternatives. The method uses a GCN encoder with square activations instead of ReLU
  and a novel polynomial-based contrastive loss function.
---

# Polynomial Contrastive Learning for Privacy-Preserving Representation Learning on Graphs

## Quick Facts
- arXiv ID: 2509.25205
- Source URL: https://arxiv.org/abs/2509.25205
- Reference count: 9
- Primary result: Privacy-preserving graph SSL using polynomial operations achieves competitive accuracy with standard GRACE

## Executive Summary
Poly-GRACE is a privacy-preserving self-supervised graph representation learning framework that replaces non-polynomial operations in the GRACE framework with HE-compatible alternatives. By using a GCN encoder with square activations instead of ReLU and a novel polynomial-based contrastive loss function, the method enables secure processing of sensitive graph data while maintaining competitive performance on standard benchmarks. Experiments show that Poly-GRACE achieves accuracy within 3 percentage points of the standard GRACE baseline across Cora, CiteSeer, and PubMed datasets, with a notable 1.2-point improvement on CiteSeer.

## Method Summary
Poly-GRACE modifies the GRACE framework to be compatible with Homomorphic Encryption by replacing non-polynomial operations with polynomial-friendly alternatives. The encoder uses a 2-layer GCN with square activation functions instead of ReLU, and the contrastive loss is reformulated as a margin-based squared error. The framework employs L2 regularization to control embedding magnitudes, preventing noise growth in encrypted computations. The model is pre-trained using contrastive learning on augmented graph views, then evaluated via linear probing for node classification.

## Key Results
- Achieves 79.7% accuracy on Cora (vs 82.6% baseline)
- Outperforms GRACE by 1.2 percentage points on CiteSeer (67.8% vs 66.6%)
- Shows significant drop on PubMed (51.3% vs 79.6%), identified as an open challenge

## Why This Works (Mechanism)

### Mechanism 1: Polynomial Activation Substitution
Replaces ReLU with square activation (x²) to maintain non-linearity while ensuring HE compatibility. Standard ReLU requires non-polynomial comparisons, but squaring is a low-degree polynomial operation.

### Mechanism 2: Margin-Based Squared Contrastive Loss
Uses a squared error penalty E[(S_ij - S_ii + m)²] instead of standard InfoNCE, forcing negative pairs to be separated from positive pairs by margin m using only polynomial operations.

### Mechanism 3: Magnitude Regularization for Noise Control
Applies L2 regularization to embedding matrices as a proxy for normalization, controlling noise growth in HE ciphertexts by keeping values small.

## Foundational Learning

- **Concept: Homomorphic Encryption (CKKS) Constraints**
  - Why needed here: Standard neural network components are mathematically impossible inside HE schemes without costly approximations
  - Quick check question: Can you calculate the square root of an encrypted number in the CKKS scheme? (Answer: No)

- **Concept: Contrastive Learning Objectives**
  - Why needed here: To understand what the "Poly Loss" is approximating
  - Quick check question: In this paper, does the loss function push the embeddings of two augmented views of the same node closer together or further apart?

- **Concept: Multiplicative Depth & Noise Budget**
  - Why needed here: To diagnose why the architecture is shallow (2 layers) and why "magnitude" matters
  - Quick check question: Why does the paper use a square activation (x²) instead of a higher-degree polynomial like x³? (Answer: Lower degree preserves the noise budget better)

## Architecture Onboarding

- **Component map:** Input Graph → Graph Augmentation → PolyGCNEncoder (Square Activations) → Compute Similarity Matrix → Calculate Poly Loss (Margin Squared + L2 Regularization)

- **Critical path:** Augment Graph → PolyGCN (Square Activations) → Compute Similarity Matrix → Calculate Poly Loss (Margin Squared + Reg)

- **Design tradeoffs:**
  - ReLU vs. Square: Square forces negative features to contribute positively, potentially losing directional sign information but gaining HE compatibility
  - Standard vs. Poly Loss: Poly loss is HE-friendly but sensitive to the margin hyperparameter, whereas Softmax dynamically calibrates based on batch statistics

- **Failure signatures:**
  - PubMed Drop: 51.3% accuracy suggests instability in squared loss or exploding gradients on larger graphs
  - Decryption Failure: Insufficient regularization causes ciphertext noise to overflow, resulting in garbage output

- **First 3 experiments:**
  1. Baseline Isolation: Run GCN (ReLU) + Poly Loss on Cora to verify if loss function alone improves over standard GRACE
  2. Regularization Sweep: Vary λ on Cora to find optimal noise-control vs. embedding collapse balance
  3. Scaling Test: Run on PubMed and monitor embedding Frobenius norm to identify magnitude regularization failures

## Open Questions the Paper Calls Out

- Can the framework be effectively extended to Graph Transformers, specifically replacing the Softmax function?
- What specific modifications are required to stabilize performance on larger, denser graphs?
- Does Poly-GRACE generalize effectively to graph learning tasks beyond transductive node classification?

## Limitations

- Significant performance drop on PubMed dataset (51.3% vs 79.6%) indicates scaling challenges
- Limited evaluation to only three citation network datasets without testing on other graph types or tasks
- No theoretical analysis of why squared contrastive loss is a valid surrogate for InfoNCE maximization

## Confidence

- High confidence in HE compatibility claims (direct consequence of polynomial operations)
- Medium confidence in comparative results (standard SSL benchmarks, but small performance gaps)
- Low confidence in the claim that polynomial modifications "can sometimes enhance" performance (based on single dataset improvement)

## Next Checks

1. Conduct hyperparameter sensitivity analysis on margin m and regularization λ across all three datasets to identify if PubMed's poor performance stems from suboptimal settings
2. Implement a hybrid encoder (ReLU first layer, square second layer) to quantify the exact performance cost of polynomial constraints
3. Measure embedding magnitude statistics during training on each dataset to verify if magnitude regularization effectively prevents noise accumulation across different graph densities