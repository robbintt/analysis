---
ver: rpa2
title: 'Locally Optimal Private Sampling: Beyond the Global Minimax'
arxiv_id: '2510.09485'
source_url: https://arxiv.org/abs/2510.09485
tags:
- sampler
- local
- distribution
- global
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sampling from a distribution
  under local differential privacy (LDP), aiming to generate samples that remain close
  to the original distribution in f-divergence while satisfying LDP constraints. The
  authors move beyond the global minimax framework to develop a local minimax formulation,
  which considers a neighborhood around a fixed distribution rather than the entire
  distribution class.
---

# Locally Optimal Private Sampling: Beyond the Global Minimax

## Quick Facts
- **arXiv ID:** 2510.09485
- **Source URL:** https://arxiv.org/abs/2510.09485
- **Reference count:** 40
- **One-line primary result:** Local minimax samplers outperform global minimax samplers across privacy regimes for f-divergence constrained LDP sampling

## Executive Summary
This paper addresses private sampling under local differential privacy (LDP) by developing a local minimax framework that outperforms traditional global minimax approaches. The authors show that when sampling from a distribution close to a reference distribution P₀, restricting the analysis to a neighborhood around P₀ yields significantly better worst-case performance than considering all possible distributions. The core contribution is a theoretical characterization showing that local minimax-optimal samplers consistently achieve lower f-divergence compared to existing global methods, with particular improvements for moderate privacy budgets.

## Method Summary
The method extends previous work on pure LDP to functional LDP, showing that globally optimal functional LDP samplers yield optimal local samplers when constrained to distributions near a reference P₀. The approach involves defining a neighborhood N_γ(P₀) using E_γ-divergence, then deriving closed-form expressions for locally minimax-optimal samplers that are independent of the choice of f-divergence. For pure LDP, a non-linear clipping mechanism projects the density ratio onto bounds derived from privacy parameters. For functional LDP, a linear mixing approach combines the private distribution with a reference measure using a specific weight calculated via convex conjugates of trade-off functions.

## Key Results
- Local minimax-optimal samplers consistently outperform global minimax samplers across privacy regimes
- The improvement is particularly pronounced for moderate privacy budgets
- Numerical experiments on both finite and continuous domains demonstrate substantially lower worst-case f-divergence
- The locally optimal samplers achieve closed-form expressions independent of f-divergence choice
- Non-linear clipping for pure LDP outperforms linear mixing pointwise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Restricting the distribution universe to a neighborhood around a reference distribution enables locally optimal samplers to significantly outperform globally optimal ones.
- **Mechanism:** The paper defines a "local" neighborhood N_γ(P₀) using E_γ-divergence. By characterizing the local minimax risk as exactly equal to the global minimax risk restricted to this neighborhood, the optimization problem becomes less pessimistic.
- **Core assumption:** The private distribution P actually lies within the defined neighborhood N_γ(P₀).
- **Evidence anchors:** Theorem 4.1 formally equates local risk to restricted global risk; abstract confirms this neighborhood approach.
- **Break condition:** If P ∉ N_γ(P₀), theoretical guarantees default to projection onto neighborhood, potentially losing utility advantage.

### Mechanism 2
- **Claim:** A non-linear "clipping" sampler achieves better pointwise utility than linear mixing for pure LDP.
- **Mechanism:** Instead of mixing private distribution P with reference, the mechanism projects P onto a mollifier set by clipping the density ratio p(x)/p₀(x) between bounds derived from ε and γ.
- **Core assumption:** Pure LDP (δ=0) is the privacy constraint, and density ratio p/p₀ is computable.
- **Evidence anchors:** Theorem 5.1 defines the sampler using clip function on density ratio; Proposition 5.2 proves non-linear sampler is pointwise better.
- **Break condition:** If ε ≥ 2log(γ), sampler degrades to trivial identity sampler.

### Mechanism 3
- **Claim:** Extending framework to functional LDP allows optimal sampling for Gaussian and approximate privacy guarantees.
- **Mechanism:** By utilizing trade-off functions g (generalizing pure ε-LDP to g-FLDP), the paper derives a linear optimal sampler that mixes P with reference measure μ using specific weight λ*.
- **Core assumption:** Privacy requirement can be modeled by a convex trade-off function g.
- **Evidence anchors:** Theorem 3.4 provides general sampler form for g-FLDP; introduction explicitly states extension to functional LDP framework.
- **Break condition:** If g doesn't satisfy non-triviality condition, identity sampler becomes only valid option.

## Foundational Learning

- **Concept:** **E_γ-divergence and Neighborhoods**
  - **Why needed here:** This divergence defines the "neighborhood" N_γ(P₀) around public data, constraining likelihood ratio between private and reference distributions.
  - **Quick check question:** If γ=1, what does that imply about relationship between private distribution P and reference P₀? (Answer: They must be identical)

- **Concept:** **Minimax Risk**
  - **Why needed here:** The paper reframes problem from "worst-case over all distributions" (Global) to "worst-case over nearby distributions" (Local).
  - **Quick check question:** Why does reducing "universe" of possible distributions (from global P to local N_γ(P₀)) lower minimax risk?

- **Concept:** **Functional LDP (g-FLDP)**
  - **Why needed here:** Generalizes privacy definition beyond standard ε parameter, allowing sampler to be optimal for Gaussian LDP and (ε,δ)-LDP.
  - **Quick check question:** How does a trade-off function g relate to Type I and Type II errors in hypothesis test?

## Architecture Onboarding

- **Component map:** Input P, P₀, privacy params (ε or g), γ → Check if P ∈ N_γ(P₀) → If Pure LDP: Apply Non-linear Clipping (Theorem 5.1) → If Functional LDP: Apply Linear Mixing (Theorem 3.4/4.1) → Output Q(P)

- **Critical path:** The calculation of normalization constant r_P in non-linear sampler (Section 5). This requires integrating clipped density, which may be computationally intensive.

- **Design tradeoffs:**
  - **Linear vs. Non-Linear:** Non-linear sampler (Pure LDP) offers strictly better utility (Prop 5.2) but requires density evaluation and normalization. Linear sampler (Functional LDP) is simpler to implement but may be suboptimal for pure LDP targets.
  - **γ Selection:** Small γ (tight neighborhood) yields better utility but risks break condition if true distribution falls outside. Large γ approaches global worst-case risk.

- **Failure signatures:**
  - **Trivial Identity:** If privacy ε is very high relative to neighborhood size γ (ε ≥ 2log(γ)), algorithm outputs raw data.
  - **Projection Artifacts:** If input P is outside N_γ(P₀), system projects to P̂ inside neighborhood, potentially introducing bias.

- **First 3 experiments:**
  1. **Visual Validation (Figure 1 replication):** Generate samples from 2D Laplace Mixture using both Global and Local samplers. Verify Local sampler preserves "4 corners" structure better.
  2. **Risk Comparison (Figures 3 & 4 replication):** Plot worst-case f-divergence (KL, TV) vs. privacy budget ε. Verify "Local" curve sits below "Global" curve.
  3. **Linear vs. Non-Linear Ablation:** Under Pure LDP, compare utility of Theorem 4.1 sampler (Linear) vs. Theorem 5.1 sampler (Non-linear) to quantify pointwise gain.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can scalable implementations or approximate variants (e.g., using MCMC) be developed for optimal samplers to handle high-dimensional data?
  - **Basis in paper:** [explicit] Authors identify computational complexity of clipping function as limitation preventing high-dimensional evaluations and propose "developing more efficient implementations... potentially leveraging techniques such as MCMC" as future work.
  - **Why unresolved:** Current exact samplers are computationally intractable for high-dimensional settings.
  - **What evidence would resolve it:** An algorithm with polynomial or linear complexity in dimension that preserves local minimax optimality properties.

- **Open Question 2:** How does local minimax framework extend to neighborhoods defined by general f-divergences or relaxed E_γ-divergence constraints?
  - **Basis in paper:** [explicit] Authors note current neighborhood is defined via strict E_γ-divergence and propose generalizing to N_{f,ζ}(P₀) where D_f(P||P₀) ≤ ζ as natural direction for future work.
  - **Why unresolved:** Closed-form expressions rely specifically on properties of E_γ-divergence.
  - **What evidence would resolve it:** Characterization of local minimax risk and optimal sampler for general f-divergence neighborhood N_{f,ζ}(P₀).

- **Open Question 3:** Can framework be extended to settings where clients release multiple samples rather than single sample?
  - **Basis in paper:** [explicit] Authors state "this work focuses on setting where each client releases single sample. Extending to case of multiple samples per client is natural direction for future work."
  - **Why unresolved:** Current analysis and risk characterizations assume release of single private sample.
  - **What evidence would resolve it:** Derivation of optimal sampler and associated minimax risk for k > 1 samples per client.

## Limitations
- **Neighborhood specification sensitivity:** Performance heavily depends on correct specification of γ parameter; misspecification can introduce bias
- **Computational complexity:** Non-linear clipping mechanism requires density evaluation and normalization, making it computationally intensive for continuous domains
- **Single sample limitation:** Framework currently only handles single sample per client, not multiple samples

## Confidence
- **High confidence**: The local minimax formulation and its equivalence to restricted global minimax (Mechanism 1) - rigorously proven in Theorem 4.1
- **Medium confidence**: The non-linear clipping mechanism outperforming linear mixing for pure LDP (Mechanism 2) - Proposition 5.2 provides proof, but practical impact depends on neighborhood specification
- **Medium confidence**: The extension to functional LDP covering GLDP and (ε,δ)-LDP (Mechanism 3) - theoretically sound but requires correct implementation of trade-off functions

## Next Checks
1. **Neighborhood robustness test**: Run experiments with systematically misspecified γ values (too small, too large) to quantify performance degradation when true distribution falls outside neighborhood
2. **Continuous normalization verification**: For continuous domains, verify numerical integration method used to compute r_P in clipping operation - errors here would invalidate sampler
3. **Trade-off function implementation audit**: For GLDP and (ε,δ)-LDP experiments, validate correct implementation of trade-off function g and its convex conjugate for computing λ*