---
ver: rpa2
title: Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems
arxiv_id: '2505.03655'
source_url: https://arxiv.org/abs/2505.03655
tags:
- sentiment
- bias
- user
- inference
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses sentiment bias in review-based recommender
  systems, where users or items with negative reviews receive lower recommendation
  accuracy. The authors propose using counterfactual inference to mitigate this bias
  by building a causal graph and estimating the Natural Direct Effect (NDE) to remove
  the indirect effect of sentiment on ratings.
---

# Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems

## Quick Facts
- **arXiv ID:** 2505.03655
- **Source URL:** https://arxiv.org/abs/2505.03655
- **Reference count:** 16
- **One-line primary result:** Novel method using counterfactual inference achieves state-of-the-art performance in mitigating sentiment bias in recommender systems, with up to 50% reduction in bias metrics while maintaining rating prediction accuracy.

## Executive Summary
This paper addresses sentiment bias in review-based recommender systems, where users or items with negative reviews receive lower recommendation accuracy. The authors propose using counterfactual inference to mitigate this bias by building a causal graph and estimating the Natural Direct Effect (NDE) to remove the indirect effect of sentiment on ratings. Their method involves two stages: model training using a neural architecture that captures sentiment bias, and debiased inference that adjusts predicted ratings. Experiments on Amazon and Yelp datasets show the proposed method achieves state-of-the-art performance in both rating prediction (MSE) and sentiment bias mitigation (BU and BI metrics), with up to 50% reduction in bias compared to baseline methods.

## Method Summary
The method constructs a causal graph treating sentiment as a mediator variable, building four causal paths to ratings: direct user effect, direct item effect, user-sentiment indirect path, and item-sentiment indirect path. The model trains using a neural architecture with three parallel branches to capture direct user-item interactions and indirect sentiment effects through user and item review embeddings. During inference, the Natural Direct Effect (NDE) is computed to remove sentiment's indirect contribution while preserving direct matching signals. The debiasing adjusts predicted ratings by subtracting a sentiment-weighted term controlled by a hyperparameter β.

## Key Results
- The proposed method achieves state-of-the-art performance on Amazon and Yelp datasets, reducing sentiment bias by up to 50% compared to baseline methods
- Maintains strong rating prediction accuracy (MSE) while significantly improving fairness metrics (BU and BI)
- Successfully improves recommendations for users and items with negative reviews without explicit sentiment computation
- The learned sentiment representation correlates positively with TextBlob sentiment polarity, validating the approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating sentiment as a mediator variable rather than a confounder better captures the true data-generating process in review-based recommender systems.
- **Mechanism:** The causal graph models four causal paths to rating Y: U→Y (direct user effect), I→Y (direct item effect), U→S→Y (user-sentiment indirect path), and I→S→Y (item-sentiment indirect path). Sentiment S is computed from reviews (which are caused by U and I), then influences Y.
- **Core assumption:** Sentiment is not an external confounder but a downstream mediator influenced by user and item characteristics.
- **Evidence anchors:** [abstract] "construct a causal graph treating sentiment as a mediator variable" [section 3.1] "sentiment node S is added to the graph as a mediator variable, constructing two indirect paths towards the ratings" [corpus] Weak direct support; neighboring papers discuss bias in RSs but do not address mediator vs. confounder formulation.
- **Break condition:** If sentiment is actually influenced by unobserved confounders (e.g., platform design, temporal trends) that also directly affect ratings, the mediator formulation may miss alternative causal pathways.

### Mechanism 2
- **Claim:** Natural Direct Effect (NDE) computed via counterfactual inference can isolate and remove indirect sentiment effects while preserving direct user-item matching signals.
- **Mechanism:** NDE = Y_{u,i,S_{u,i}} - Y_{u*,i*,S_{u,i}}, where u* and i* are reference (null) states. At inference, the debiased prediction subtracts β·σ(s_{u,i}) from the biased prediction, removing the indirect path contribution while keeping direct U→Y and I→Y effects.
- **Core assumption:** The reference state Y_{u*,i*} can be approximated by a learnable hyperparameter β during validation, and σ(s_{u,i}) captures the magnitude of sentiment bias.
- **Evidence anchors:** [abstract] "employ Natural Direct Effect (NDE) to remove the indirect effects of sentiment on ratings" [section 3.2] Equations 3-5 formalize NDE; Appendix clarifies β = γ_{discount} · Y_{u*,i*} [corpus] No direct corpus validation of NDE for sentiment debiasing; this is a novel application per the paper.
- **Break condition:** If β poorly approximates the true counterfactual reference state, debiasing may over-correct (degrading accuracy) or under-correct (leaving residual bias).

### Mechanism 3
- **Claim:** A neural architecture with modularized branches can disentangle direct effects (user-item interactions) from indirect sentiment effects during training.
- **Mechanism:** Three parallel branches compute: (1) q_m = f_m(h_u · h_i) for direct user-item interaction; (2) ŷ_u = f_u(z_u) for user-sentiment path; (3) ŷ_i = f_i(z_i) for item-sentiment path. These are summed and gated by σ(s_{u,i}) where s_{u,i} = f_s(z_u · z_i). Multi-task loss L = L_{RC} + α_u·L_U + α_i·L_I trains all branches jointly.
- **Core assumption:** Effects are additive and independent when d-separated in the causal graph; attention-based review encoders can extract sentiment-relevant representations without external sentiment tools.
- **Evidence anchors:** [section 3.3] Equations 6-15 describe the architecture and loss functions [section 4.5] Figure 6 shows positive correlation between σ(s_{u,i}) and TextBlob sentiment polarity, validating that the learned representation captures sentiment [corpus] Weak support; corpus papers address bias and privacy but not modularized causal architectures.
- **Break condition:** If review embeddings conflate sentiment with other signals (e.g., review length, topic), the "sentiment branch" may capture spurious correlations, leading to incorrect debiasing.

## Foundational Learning

- **Concept:** Directed Acyclic Graphs (DAGs) for causal modeling
  - **Why needed here:** The entire method is built on specifying a causal graph (Figure 2) that defines how variables influence each other. Without understanding DAGs, you cannot reason about which paths to block or preserve.
  - **Quick check question:** Can you identify all paths from U to Y in Figure 2(b) and classify each as direct or indirect?

- **Concept:** Counterfactual inference and the "what if" question
  - **Why needed here:** The debiasing mechanism relies on asking: "What would the rating be if sentiment had no effect?" Understanding counterfactual notation (Y_{u,i,S_{u,i}} vs. Y_{u*,i*,S_{u,i}}) is essential for implementing NDE correctly.
  - **Quick check question:** In Equation 3, what does Y_{u*,i*,S_{u,i}} represent in plain language?

- **Concept:** Mediator vs. confounder in causal graphs
  - **Why needed here:** The paper argues previous work (CISD) incorrectly treated sentiment as a confounder, which would require backdoor adjustment. Understanding the distinction explains why this method uses NDE instead.
  - **Quick check question:** If sentiment were a confounder, what additional edges would exist in the causal graph that are absent in Figure 2(b)?

## Architecture Onboarding

- **Component map:** User embeddings h_u, item embeddings h_i, user review embeddings z_u (via CNN + attention), item review embeddings z_i (via CNN + attention) → Branch 1 (direct): f_m(h_u · h_i) → q_m; Branch 2 (user-sentiment): f_u(z_u) → ŷ_u; Branch 3 (item-sentiment): f_i(z_i) → ŷ_i; Sentiment gate: s_{u,i} = f_s(z_u · z_i) → σ(s_{u,i}); Fusion: ŷ_{u,i,s} = (q_m + ŷ_u + ŷ_i) · σ(s_{u,i}); Debiasing (inference only): y_{debiased} = ŷ_{u,i} · σ(s_{u,i}) - β · σ(s_{u,i})

- **Critical path:** 1. Encode reviews → z_u, z_i via word2vec + CNN + attention (Equations 6-7 path) 2. Compute sentiment gate σ(s_{u,i}) from review interaction (Equation 11) 3. Fuse direct and indirect branches with gating (Equation 10) 4. At inference: apply NDE subtraction with tuned β (Equation 16)

- **Design tradeoffs:** β magnitude: Higher β increases debiasing but risks over-correction (MSE increases). Paper uses small β ∈ {0.01-0.7} with validation search. α_u, α_i coefficients: Balance auxiliary sentiment branch losses against main rating loss. Paper finds optimal at 0.001. Backbone choice: NCF used as default, but method is model-agnostic; changing backbone affects how well direct effects are captured.

- **Failure signatures:** High MSE with low bias metrics (BU/BI): Likely β too aggressive, over-debiasing. High BU/BI with good MSE: β too small, insufficient debiasing. σ(s_{u,i}) uncorrelated with ground-truth sentiment: Review encoder failing to extract sentiment signals; check attention weights or increase encoder capacity. Training instability with large α values: Auxiliary losses dominating; reduce α_u, α_i.

- **First 3 experiments:** 1. Reproduce baseline comparison on one dataset (e.g., Kindle Store): Train NARRE and this method, compare MSE, BU, BI to verify implementation correctness against Table 2 and Table 3. 2. Ablate β: Run inference with β ∈ {0, 0.1, 0.3, 0.5} and plot MSE vs. BU/BI to visualize the accuracy-debias tradeoff curve. 3. Validate σ(s_{u,i}) sentiment correlation: For a held-out set, compute σ(s_{u,i}) and compare against TextBlob sentiment polarity; verify positive correlation as in Figure 6 to confirm the sentiment branch is learning meaningful representations.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text.

## Limitations
- The core assumption that sentiment functions as a mediator (rather than confounder) is critical but unverified - if unobserved confounders exist that directly affect both sentiment and ratings, the NDE formulation may miss these alternative causal pathways
- The approximation of the counterfactual reference state Y_{u*,i*} by a learnable hyperparameter β is methodologically pragmatic but lacks theoretical guarantees; poor β selection could over- or under-correct
- The neural architecture's ability to truly disentangle direct and indirect effects depends on the additivity assumption and the quality of sentiment extraction from reviews, which may conflate sentiment with other review features

## Confidence
- **High confidence:** The experimental results showing improved MSE and reduced BU/BI metrics across multiple datasets
- **Medium confidence:** The mechanism of using NDE for debiasing in recommender systems (novel application, no corpus validation)
- **Medium confidence:** The modular architecture's ability to disentangle effects (weak corpus support, requires architectural assumptions)

## Next Checks
1. Test sensitivity to unobserved confounders by introducing synthetic confounding variables and measuring how well the method maintains debiasing performance
2. Perform ablation studies varying β more finely to map the full accuracy-debias tradeoff curve and identify optimal operating points
3. Validate the counterfactual reference approximation by comparing β-tuned debiasing against oracle debiasing using known counterfactuals in synthetic datasets