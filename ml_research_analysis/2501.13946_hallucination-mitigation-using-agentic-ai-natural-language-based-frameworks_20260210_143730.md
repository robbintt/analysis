---
ver: rpa2
title: Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks
arxiv_id: '2501.13946'
source_url: https://arxiv.org/abs/2501.13946
tags:
- agent
- hallucination
- agents
- factual
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a multi-agent framework leveraging Natural
  Language-Based APIs, specifically the OVON (Open Voice Network) standard, to mitigate
  hallucinations in Large Language Models (LLMs). The pipeline uses over 310 carefully
  crafted prompts to induce hallucinations, processed through three specialized agents
  that progressively reduce hallucinations and improve content clarity.
---

# Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks

## Quick Facts
- arXiv ID: 2501.13946
- Source URL: https://arxiv.org/abs/2501.13946
- Reference count: 22
- Multi-agent framework using OVON JSON messages reduces LLM hallucination scores by over 2,800% across three sequential agents

## Executive Summary
This study presents a multi-agent framework leveraging Natural Language-Based APIs, specifically the OVON (Open Voice Network) standard, to mitigate hallucinations in Large Language Models (LLMs). The pipeline uses over 310 carefully crafted prompts to induce hallucinations, processed through three specialized agents that progressively reduce hallucinations and improve content clarity. A novel set of Key Performance Indicators (KPIs), including Factual Claim Density, Factual Grounding References, Fictional Disclaimer Frequency, and Explicit Contextualization Score, are introduced to quantitatively assess hallucination mitigation. The results show significant reduction in Total Hallucination Scores, with percentage improvements surpassing 800% from the first to second agent and nearly 2,800% from the first to third agent, demonstrating the effectiveness of multi-agent orchestration in enhancing AI reliability and trustworthiness.

## Method Summary
The study implements a 4-agent pipeline using Microsoft Autogen and OpenAI LLMs. Agent 1 (GPT-3.5-turbo) generates initial responses from 310 hallucination-inducing prompts. Agent 2 (GPT-4o) reviews and refines content, adding disclaimers and generating OVON JSON with utterance text plus whisper context/value metadata. Agent 3 (GPT-4o) interprets this metadata to further refine for clarity and fictional framing. Agent 4 (GPT-4o) evaluates all three responses using custom KPIs: Factual Claim Density, Factual Grounding References, Fictional Disclaimer Frequency, and Explicit Contextualization Score. The Total Hallucination Score (THS) is calculated as a weighted sum of these metrics, with lower scores indicating more explicit fictional framing.

## Key Results
- Total Hallucination Score reduction from 6.44 (Agent 1) to 0.65 (Agent 3), representing a 2,800% improvement
- Percentage reduction exceeding 800% from first to second agent
- Strong performance on prompts with real-world referents (700% reduction) but weaker on pure fantasy (33% reduction)
- Multi-agent orchestration significantly outperforms single-agent self-correction approaches

## Why This Works (Mechanism)

### Mechanism 1
Sequential multi-agent review progressively reduces hallucination markers in generated text. Three specialized agents process content sequentially—first generating, then reviewing for accuracy and adding disclaimers, then refining clarity and strengthening fictional framing. Each agent builds on the previous output rather than starting fresh. Core assumption: Hallucinations are more detectable and correctable when reviewed by agents with different objectives than by a single model attempting self-correction.

### Mechanism 2
Structured metadata exchange via OVON JSON preserves context across agent handoffs, enabling targeted refinement. The second-level reviewer embeds hallucination assessment in "whisper context" (≤30 words summarizing issues) and "whisper value" (≤200 words explaining reasons). The third-level agent interprets these fields to guide refinement without re-analyzing from scratch. Core assumption: Explicit metadata about uncertainty is more actionable than raw text alone for downstream agents.

### Mechanism 3
Custom KPIs (FCD, FGR, FDF, ECS) enable quantitative hallucination tracking, but they measure linguistic markers rather than factual truth. A fourth GPT-4o-based agent scores each response on four metrics: Factual Claim Density (claims per 100 words), Factual Grounding References (attempts to cite evidence), Fictional Disclaimer Frequency (explicit "fiction" markers), and Explicit Contextualization Score (binary framing as fictional). Lower Total Hallucination Score (THS) indicates more explicit fictional framing. Core assumption: Reduced factual claim density and increased disclaimers correlate with reduced misleading content, even if factual accuracy cannot be verified.

## Foundational Learning

- **Concept: Multi-agent orchestration**
  - Why needed here: The entire architecture depends on coordinating multiple specialized agents with distinct roles (generation, review, refinement, evaluation).
  - Quick check question: Can you explain why sequential review by differently-prompted agents might catch issues that self-correction by a single agent misses?

- **Concept: Hallucination in LLMs**
  - Why needed here: The paper explicitly states "hallucinations in the scope of AI large language models (LLMs) have been proven to be inevitable" (citing arxiv:2401.11817). Understanding this as an unsolvable problem frames mitigation as a harm-reduction approach.
  - Quick check question: What is the difference between detecting factual errors vs. detecting misleading confidence in fabricated content?

- **Concept: JSON-based inter-agent communication**
  - Why needed here: OVON "conversation envelopes" carry utterance content plus metadata (whisper fields). Understanding this separation is essential for implementing the pipeline.
  - Quick check question: How would you design a message schema that carries both user-facing content and machine-readable context about that content's reliability?

## Architecture Onboarding

- **Component map**: Front-end Agent (GPT-3.5) → Second-level Reviewer (GPT-4o) → OVON JSON → Third-level Reviewer (GPT-4o) → KPI Evaluator (GPT-4o)
- **Critical path**: Prompt → Front-end Agent → raw response → Second-level Reviewer → OVON JSON (utterance + whispers) → Third-level Reviewer → final response → KPI Evaluator → scores for all three versions
- **Design tradeoffs**:
  - Using different LLMs per stage (GPT-3.5 vs GPT-4o) vs. using same model with different prompts
  - Limiting OVON to 2nd-3rd agent exchange vs. implementing across all stages
  - LLM-based KPI evaluation vs. human or rule-based evaluation
  - Testing 310 hallucination-inducing prompts vs. natural user queries
- **Failure signatures**:
  - High variance in THS improvement: Some prompts showed 700% reduction, others only 33%
  - Positive delta THS in some cases: Later agents occasionally increased hallucination scores
  - Cursory human validation: "Neither exhaustive nor did it entail in-depth cross-verification"
  - Single-vendor LLM dependency: Only OpenAI models tested
- **First 3 experiments**:
  1. Replicate the pipeline with 20-30 prompts from the published dataset to verify THS reductions
  2. Ablate the OVON whisper fields: Pass only utterance text from agent 2 to agent 3 and compare THS3 scores
  3. Substitute the KPI evaluator with a different model (e.g., Claude, Gemini, or Llama) to assess score consistency

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the multi-agent framework perform when integrating heterogeneous LLM architectures (e.g., Claude, Llama, Gemini) compared to the current OpenAI-centric implementation? (Basis: Section 9 suggests future testing of non-OpenAI models would reduce dependence on single architecture)
- **Open Question 2**: Does expanding the OVON message passing (utterances and whispers) to all agent interfaces improve the consistency of hallucination mitigation? (Basis: Section 9 notes current design limited OVON to 2nd-3rd agent exchange for complexity management)
- **Open Question 3**: Can the multi-agent orchestration strategy be adapted to effectively mitigate hallucinations in prompts that lack any real-world referents (pure fantasy scenarios)? (Basis: Section 7 shows prompts like "Telepathic Canines" showed significantly less improvement due to lack of factual anchors)

## Limitations

- **Metric Validity**: THS KPIs measure linguistic framing rather than factual truth, potentially conflating disclosure with accuracy
- **Generalization Gap**: Testing used 310 adversarial prompts; real-world user queries may produce different mitigation effectiveness
- **Human Validation**: Cursory human review without in-depth cross-verification limits confidence in qualitative assessments

## Confidence

- **High Confidence**: Sequential multi-agent architecture can reduce hallucination scores (quantitative THS improvements are consistent across experiments)
- **Medium Confidence**: OVON JSON metadata improves agent coordination for context preservation (mechanism is plausible but not extensively validated)
- **Low Confidence**: THS reductions translate to improved user safety (no user studies or real-world deployment evidence)

## Next Checks

1. **Ablation Test**: Run the pipeline without OVON whisper fields to quantify their contribution to THS reduction
2. **Cross-Evaluator Consistency**: Substitute the KPI evaluator with different models (Claude, Gemini, Llama) to assess score consistency and evaluator dependency
3. **Natural Query Test**: Apply the pipeline to 50 real user queries from production systems rather than adversarial prompts to measure practical effectiveness