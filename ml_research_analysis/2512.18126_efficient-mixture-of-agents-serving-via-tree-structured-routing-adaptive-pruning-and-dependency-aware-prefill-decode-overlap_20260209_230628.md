---
ver: rpa2
title: Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning,
  and Dependency-Aware Prefill-Decode Overlap
arxiv_id: '2512.18126'
source_url: https://arxiv.org/abs/2512.18126
tags:
- agent
- uni00000013
- uni00000011
- agents
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the latency problem in Mixture-of-Agents
  (MoA) inference systems, which suffer from dense inter-agent communication and poor
  hardware utilization. The authors propose Faster-MoA, a unified algorithm-system
  co-design that includes three innovations: a hierarchical tree topology that reduces
  redundant agent connections, a semantic-guided dynamic early-exit mechanism that
  prunes unnecessary agent invocations based on output confidence and similarity,
  and an agent dependency-aware incremental prefilling approach that overlaps decoding
  and prefilling stages.'
---

# Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap

## Quick Facts
- **arXiv ID**: 2512.18126
- **Source URL**: https://arxiv.org/abs/2512.18126
- **Reference count**: 38
- **Primary result**: Achieves up to 90% end-to-end latency reduction while maintaining accuracy within ±1% of dense-connectivity MoA baselines

## Executive Summary
This paper addresses the latency problem in Mixture-of-Agents (MoA) inference systems, which suffer from dense inter-agent communication and poor hardware utilization. The authors propose Faster-MoA, a unified algorithm-system co-design that includes three innovations: a hierarchical tree topology that reduces redundant agent connections, a semantic-guided dynamic early-exit mechanism that prunes unnecessary agent invocations based on output confidence and similarity, and an agent dependency-aware incremental prefilling approach that overlaps decoding and prefilling stages. Evaluated across five benchmarks (GSM8K, MATH-500, AIME2025, MMLU-ProX-Lite, and IFBench) on NVIDIA H200 GPUs, Faster-MoA achieves up to 90% reduction in end-to-end latency while maintaining accuracy within ±1% of dense-connectivity baselines, with some settings showing improved accuracy.

## Method Summary
Faster-MoA is a three-component system designed to reduce latency in MoA inference. First, it replaces the dense all-to-all agent connections with a hierarchical 9-3-1 tree topology, where agents are grouped into clusters and successors connect only to specific subsets of precursors. Second, it implements a semantic-guided dynamic early-exit mechanism that terminates large-agent invocations when smaller agents produce high-confidence, semantically similar outputs, using a quality score combining token confidence and Frobenius Cosine Similarity. Third, it introduces dependency-aware incremental prefilling with a shell router that overlaps decoding from precursor agents with prefilling for successor agents, streaming decoded tokens in chunks to incrementally update KV caches. The system was implemented on SGLang v0.5.3 and vLLM v0.11.0, deployed on 6× NVIDIA H200 GPUs with Qwen3-VL-4B/8B/32B-Instruct models plus Qwen3-Embedding-4B for similarity computation.

## Key Results
- Achieves up to 90% end-to-end latency reduction across five benchmarks
- Maintains accuracy within ±1% of dense-connectivity MoA baselines
- Shows 62-90% latency reduction on AIME2025, MATH-500, and IFBench
- Some configurations demonstrate improved accuracy compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Tree Topology
Structuring agents in a hierarchical tree rather than a dense all-to-all graph reduces synchronization bottlenecks, provided that localized information aggregation is sufficient for the task. Agents are grouped into clusters where a successor connects only to a specific subset of precursors, changing the latency constraint from waiting for the slowest agent in the entire layer to waiting only for the slowest agent in the specific cluster, allowing independent branches to execute concurrently.

### Mechanism 2: Semantic-Guided Dynamic Early-Exit (EE)
Inference cost can be reduced by terminating large-agent invocations when smaller agents produce high-confidence, semantically similar outputs. The system calculates a quality score using the geometric mean of token confidence and calibrated semantic similarity between intermediate outputs, skipping downstream heavy agents if the score meets a threshold.

### Mechanism 3: Dependency-Aware Incremental Prefill-Decode Overlap
Latency can be hidden by streaming decoded tokens from a precursor agent to a successor agent's prefill phase, rather than waiting for full completion. A "Shell Router" splits the successor's prompt into segments, and as the precursor decodes, chunks are appended to answer slots while the prefill engine incrementally updates the KV cache, overlapping decode time with prefill time.

## Foundational Learning

- **Concept: Prefill-Decode (PD) Disaggregation**
  - **Why needed here**: The paper's system optimization relies on physically separating the memory-intensive prefill phase from the compute-bound decode phase to enable the "Incremental Prefill" mechanism.
  - **Quick check question**: Why does standard request pipelining fail when requests have data dependencies (as in MoA)?

- **Concept: KV Cache Reuse & Incremental Attention**
  - **Why needed here**: Faster-MoA avoids re-computing attention for the entire prompt when new tokens arrive. Understanding how KV blocks are appended is essential for the "Incremental Prefill" logic.
  - **Quick check question**: When appending a new chunk of text to a pre-filled prompt, which parts of the KV cache must be recomputed and which can be reused?

- **Concept: Mixture-of-Agents (MoA) Topologies**
  - **Why needed here**: The paper assumes the reader understands the baseline "all-to-all" structure (proposers → aggregator) to appreciate the sparsity introduced by the tree topology.
  - **Quick check question**: In an all-to-all MoA, why does the latency scale with the *slowest* agent rather than the average?

## Architecture Onboarding

- **Component map**: Input Distribution → Early-Exit Decision → Shell Router → Incremental Prefill → Decode Engines
- **Critical path**: 1. Input broadcast to leaf agents 2. EE Decision with quality score Q 3. Overlapped Aggregation: Precursor Decode → APC → Shell Router → Successor Incremental Prefill
- **Design tradeoffs**:
  - Tree Width vs. Depth: Deeper trees add aggregation layers (latency) but improve quality; wider trees increase parallelism but raise hardware costs
  - Chunk Size (APC): Small chunks reduce stall time but increase API/Router overhead; large chunks reduce overhead but increase latency
  - Threshold (τ): Lowering τ speeds up inference but risks accuracy drops on hard tasks
- **Failure signatures**:
  - Accuracy Collapse on Hard Tasks: Early-Exit threshold τ too aggressive
  - High Latency Variance: "Stragglers" in tree branches not masked by incremental prefill
  - Memory OOM: Incremental prefilling keeps KV caches "live" for longer periods
- **First 3 experiments**:
  1. Topology Ablation: Compare All-to-All vs. Tree-only on MATH-500
  2. EE Sensitivity Analysis: Vary similarity threshold τ on AIME2025
  3. Overlap Micro-benchmark: Profile second layer aggregator latency with random tokens

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does Faster-MoA perform when integrating heterogeneous model families that require distinct tokenizers?
- **Basis in paper**: [explicit] Section 5.1 states the authors selected the Qwen model family specifically to "avoid extra heterogeneous tokenizer orchestration issues," leaving the handling of cross-family token alignment as an unaddressed implementation complexity.
- **Why unresolved**: The current Agent Prompt Cache (APC) optimizes storage by saving intermediate tokens rather than text, a technique that relies on shared tokenization which breaks down when agents use different model families.
- **What evidence would resolve it**: Evaluation of latency and accuracy when combining models from different families (e.g., Llama and Qwen), specifically measuring the overhead of the necessary detokenization-re-tokenization steps in the APC.

### Open Question 2
- **Question**: Can the hierarchical tree topology and cluster sizes be dynamically optimized or searched at runtime rather than fixed empirically?
- **Basis in paper**: [inferred] Section 4.1 defines the tree structure and cluster size (9-3-1) as "empirical demonstration" settings, and Section 3.2 notes that "adding more fully connected agents does not guarantee higher-quality outputs," implying the optimal structure is task-dependent but currently static.
- **Why unresolved**: The paper establishes a fixed tree depth and branching factor, but does not propose a mechanism to adapt the topology based on the complexity of the specific input query.
- **What evidence would resolve it**: An ablation study showing the impact of different tree depths/widths on various query difficulties, or the introduction of a controller that selects the topology dynamically.

### Open Question 3
- **Question**: Does the semantic similarity early-exit mechanism constrain diversity in creative or generative tasks?
- **Basis in paper**: [inferred] The evaluation (Section 5.1) relies exclusively on convergent reasoning benchmarks (GSM8K, MATH, MMLU), but Section 4.2 notes the mechanism favors scenarios where agents are "confident and similar enough," which may be suboptimal for tasks requiring divergent thinking.
- **Why unresolved**: High semantic similarity correlates with correctness in math/logic, but in creative writing, it may indicate repetition or lack of novelty, potentially causing the system to exit early with generic outputs.
- **What evidence would resolve it**: Evaluation on open-ended generation benchmarks comparing the diversity (e.g., distinct n-grams) and quality of outputs against the baseline all-to-all MoA.

### Open Question 4
- **Question**: What is the optimal chunk size for incremental prefilling to minimize overhead while maximizing decode-prefill overlap?
- **Basis in paper**: [explicit] Section 4.3 states that the chunk size of decoded text stored in the Agent Prompt Cache "should be chosen empirically by users" to balance latency reduction against the overhead of frequent prefill requests.
- **Why unresolved**: The trade-off between communication frequency and pipeline utilization is sensitive to hardware interconnects (NVLink bandwidth) and model size, requiring a tuning mechanism currently absent from the design.
- **What evidence would resolve it**: A sensitivity analysis plotting E2E latency against varying chunk sizes on different hardware configurations to identify an analytical or automatic tuning rule.

## Limitations
- System-level complexity may limit real-world adoption due to precise synchronization requirements and potential failure modes
- Evaluation focuses exclusively on Qwen3-VL models, leaving generalizability across model families uncertain
- Early-exit mechanism relies on fixed threshold without comprehensive sensitivity analysis across different task types

## Confidence

**High Confidence (8/10)**: The fundamental architectural insight that tree-structured routing can reduce latency by limiting synchronization points is well-supported by empirical results showing 62-90% latency reduction.

**Medium Confidence (6/10)**: The semantic-guided early-exit mechanism's effectiveness is moderately supported, with clear latency benefits shown but with acknowledged accuracy trade-offs on harder tasks.

**Low Confidence (4/10)**: The practical deployment benefits of the dependency-aware incremental prefill mechanism are the least validated component, lacking analysis of edge cases and failure modes.

## Next Checks

1. **Cross-Model Family Generalization Test**: Implement Faster-MoA with a different LLM family (e.g., Llama or Mistral) and evaluate whether the same tree topology and early-exit thresholds maintain the claimed latency-accuracy trade-off across the five benchmark tasks.

2. **Failure Mode Characterization**: Systematically test the incremental prefill mechanism under degraded conditions—saturate NVLink bandwidth, introduce network latency, or use models with different KV cache sizes—to quantify performance degradation and identify break-even points where complexity overhead outweighs benefits.

3. **Dynamic Threshold Optimization**: Replace the fixed early-exit threshold (τ=0.7) with a task-adaptive mechanism that learns optimal thresholds based on input complexity indicators, then validate whether this improves the Pareto frontier on harder reasoning tasks while maintaining gains on simpler tasks.