---
ver: rpa2
title: 'Don''t Fight Hallucinations, Use Them: Estimating Image Realism using NLI
  over Atomic Facts'
arxiv_id: '2503.15948'
source_url: https://arxiv.org/abs/2503.15948
tags:
- facts
- image
- images
- atomic
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to quantify image realism by leveraging
  hallucinations in Large Vision-Language Models (LVLMs) and Natural Language Inference
  (NLI). The method generates atomic facts from images using LVLMs, computes pairwise
  entailment scores between these facts using NLI models, and aggregates these scores
  to produce a single reality score.
---

# Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts

## Quick Facts
- arXiv ID: 2503.15948
- Source URL: https://arxiv.org/abs/2503.15948
- Authors: Elisei Rykov; Kseniia Petrushina; Kseniia Titova; Alexander Panchenko; Vasily Konovalov
- Reference count: 24
- Key outcome: State-of-the-art zero-shot performance on WHOOPS! dataset (72.55% accuracy)

## Executive Summary
This paper introduces RealityCheck, a novel approach to image realism estimation that leverages hallucinations in Large Vision-Language Models (LVLMs). Rather than treating hallucinations as errors to be eliminated, the method uses them as signals: when LVLMs encounter images that violate common sense, they generate inconsistent or fabricated atomic facts. By generating multiple facts from an image and computing pairwise Natural Language Inference (NLI) scores between them, the approach detects logical contradictions that indicate unrealistic images. Evaluated on the WHOOPS! benchmark, RealityCheck achieves 72.55% accuracy, outperforming other open-source zero-shot methods.

## Method Summary
RealityCheck operates through a three-stage pipeline. First, an LVLM (LLaVA-v1.6-Mistral-7B) generates N=5 atomic facts about each image using diverse beam search. Second, a cross-encoder NLI model (nli-deberta-v3-large) computes pairwise entailment, contradiction, and neutral scores for all fact combinations. Third, these scores are aggregated using weighted summation and k-means clustering to produce a single reality score, where lower scores indicate more realistic images. The method uses 3-fold cross-validation for hyperparameter tuning and achieves binary classification accuracy on paired comparisons from the WHOOPS! dataset.

## Key Results
- Achieves 72.55% accuracy on WHOOPS! benchmark, state-of-the-art among open-source zero-shot methods
- Outperforms competitors including RealityScore (66.67%), Cosine Similarity (65.69%), and CLIPScore (62.25%)
- Demonstrates that hallucination detection via NLI can effectively identify counter-common-sense images

## Why This Works (Mechanism)

### Mechanism 1
Images violating common sense trigger hallucinations in LVLMs. When an LVLM encounters content that contradicts its learned world knowledge (e.g., Einstein holding a smartphone), it fails to produce a consistent internal representation. This conflict manifests as inconsistent or fabricated atomic facts during generation. The core assumption is that the LVLM's internal priors, learned from training data, are strong enough that violations cause the model to generate output inconsistent with the visual input. Evidence comes from corpus work [2] documenting this phenomenon. If an LVLM is trained to be perfectly faithful to an image via RLHF targeting hallucination reduction, this mechanism will weaken.

### Mechanism 2
Hallucinations and accurate facts are logically inconsistent, detectable via NLI. A single image generates a set of atomic facts F. For normal images, facts are mutually consistent with high entailment scores. For weird images, F contains a mix of accurate facts and hallucinations that often contain logical contradictions (e.g., "This is a camel" vs. "This is a digitally manipulated image"). The core assumption is that NLI models are sufficiently robust to detect these subtle contradictions, and that the specific pattern of NLI scores correlates with "weirdness." Evidence shows NLI is used for fact verification in related work (FAITHSCORE [7]), though corpus evidence on the specific aggregation strategy is weak. If hallucinations are not contradictory but simply irrelevant or neutral, the NLI clustering method will fail to find a low-entailment cluster.

## Foundational Learning

- **Concept: Large Vision-Language Models (LVLMs) and Hallucination**
  - Why needed here: The core signal derives from LVLM behavior. Understanding that these models can "see" one thing but "say" another due to strong linguistic priors is essential for interpreting the method.
  - Quick check question: What happens when you show an LVLM an image of a raccoon painting a wall, and why might its output differ from a standard captioning model?

- **Concept: Natural Language Inference (NLI)**
  - Why needed here: This is the tool used to measure the consistency of generated facts. Understanding entailment, contradiction, and neutrality is essential for interpreting the reality score.
  - Quick check question: Given the premise "A man is sleeping in a bed," does the hypothesis "A man is sleeping on a rock" represent an entailment, a contradiction, or a neutral relationship?

- **Concept: Clustering Aggregation**
  - Why needed here: The paper's best results come from using k-means on NLI scores, not simple aggregation. This assumes "weirdness" creates a distinct statistical pattern in the data.
  - Quick check question: Why might a simple "minimum" score aggregation be more brittle than a clustering-based approach when dealing with a set of 5-10 atomic facts?

## Architecture Onboarding

- **Component map:** LVLM Generator (LLaVA-v1.6-Mistral-7B) -> NLI Engine (nli-deberta-v3-large) -> Aggregation Module (Python script with k-means) -> Reality Score
- **Critical path:** The Fact Generation step is most critical. If the LVLM does not produce contradictory hallucinations for weird images, the entire downstream pipeline fails.
- **Design tradeoffs:**
  - LVLM Size: Larger models may be more robust and thus hallucinate less, paradoxically hurting performance
  - Number of Facts (N): More facts provide more data for NLI but increase computational cost quadratically (O(NÂ²) NLI calls)
  - Aggregation Strategy: `min` is simple but noise-sensitive; `clust` is more robust but adds complexity and a hyperparameter
- **Failure signatures:**
  - Consistent Hallucination: LVLM generates self-consistent but incorrect narrative for weird image, leading to false "normal" classification
  - Over-Truthfulness: LVLM accurately describes weird image without hallucinating, resulting in high consistency
  - NLI Noise: For abstract concepts, NLI model may return neutral scores instead of contradictions, washing out signal
- **First 3 experiments:**
  1. Reproduce Baseline: Run pipeline on few WHOOPS! images, manually inspect generated facts, verify weird images produce more contradictory fact sets
  2. Ablation on N: Test performance with N=3, 5, 10 facts, plot accuracy vs. inference cost to find optimal tradeoff
  3. NLI Model Swap: Substitute nli-deberta-v3-large with nli-deberta-v3-base or small, measure drop in accuracy to quantify NLI model's contribution

## Open Questions the Paper Calls Out
- Is the high accuracy driven by genuine semantic contradictions or by the LVLM's tendency to generate specific stylistic marker words (e.g., "digital", "rendering") for unusual images?
- Does the premise that "LVLMs hallucinate more on unrealistic images" hold for subtle realism failures found in deepfakes or physically implausible lighting?
- How dependent is the method on the specific hallucination profile of the LLaVA 1.6 model, and does it transfer to LVLMs explicitly trained to reduce object hallucination?

## Limitations
- LVLM dependency: Method's effectiveness fundamentally depends on specific LVLM producing contradictory hallucinations for weird images
- Dataset specificity: WHOOPS! benchmark contains curated image pairs designed for visual common sense; performance on naturalistic or domain-specific datasets unknown
- NLI aggregation assumptions: K-means clustering approach assumes pairwise NLI scores form distinct clusters; this pattern may not hold across different image types

## Confidence
- **High Confidence (Mechanistic Understanding):** Three-stage pipeline clearly specified with exact model versions, parameters, and aggregation strategy
- **Medium Confidence (Generalizability):** Works well on WHOOPS! benchmark but doesn't explore failure cases, sensitivity to parameters, or out-of-distribution performance
- **Low Confidence (Foundational Assumptions):** Assumes LVLMs consistently hallucinate on counter-common-sense images but doesn't empirically validate this assumption

## Next Checks
1. Fact Generation Validation: Manually inspect 20-30 fact sets from both normal and weird images to verify weird images consistently produce more contradictory or inconsistent facts
2. Model Robustness Test: Replace LLaVA-v1.6-Mistral-7B with larger model (LLaVA-NeXT-34B or Qwen-VL-72B-Chat) and rerun on WHOOPS! to determine if performance degrades due to reduced hallucination tendencies
3. Cross-Dataset Generalization: Evaluate trained pipeline on SPVA dataset or custom set of 50 medical/industrial images with known realism issues to assess performance beyond curated WHOOPS! benchmark