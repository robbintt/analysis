---
ver: rpa2
title: 'From keywords to semantics: Perceptions of large language models in data discovery'
arxiv_id: '2510.01473'
source_url: https://arxiv.org/abs/2510.01473
tags:
- data
- llms
- they
- research
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated researchers' perceptions of using Large
  Language Models (LLMs) for data discovery. Through focus groups with 27 participants,
  researchers currently rely on keyword-based searches across multiple sources, which
  is time-consuming and requires exact terminology knowledge.
---

# From keywords to semantics: Perceptions of large language models in data discovery

## Quick Facts
- arXiv ID: 2510.01473
- Source URL: https://arxiv.org/abs/2510.01473
- Reference count: 40
- Primary result: Researchers need transparency features in LLM data discovery tools to overcome barriers of bias, unreliability, ethics concerns, and lack of trust

## Executive Summary
This study investigated researchers' perceptions of using Large Language Models (LLMs) for data discovery through focus groups with 27 participants. Current keyword-based search methods are time-consuming and require exact terminology knowledge, creating significant friction in finding relevant datasets. While participants recognized potential benefits of LLMs including faster discovery, natural language queries, and dataset summaries, they identified major barriers including data biases, unreliable results, ethical concerns, and lack of trust. The key finding was that transparency about data sources, model limitations, and confidence levels could overcome these barriers. Researchers wanted direct dataset links, metadata access, and citation metrics to build trust and enable verification.

## Method Summary
The study employed qualitative focus groups with 27 researchers across four distinct groups: doctoral students, academic researchers, data service staff, and government/third sector participants. Each 1-hour online session followed a structured format with questions about current search practices, hypothetical LLM usage, and a SWOT analysis. Data was analyzed using a 6-step inductive thematic analysis approach, resulting in a conceptual model with three main themes: potential to transform data discovery, barriers to acceptance, and how transparency overcomes these barriers. Coded files are available in an OSF repository.

## Key Results
- Current keyword-based search requires exact terminology knowledge and is time-consuming across multiple sources
- Participants identified four main barriers to LLM adoption: data biases, unreliable results, ethical concerns, and trust issues
- Transparency features including source links, confidence scores, and metadata access could overcome these barriers
- Researchers wanted to augment rather than replace current search processes with LLMs

## Why This Works (Mechanism)
The proposed LLM-based data discovery system works by converting natural language queries into semantic vectors that can match dataset content and context beyond exact keyword matching. This semantic approach addresses the brittleness of keyword search by understanding query intent and finding relevant datasets even when terminology differs. The system's effectiveness depends on a transparency layer that provides source links, confidence scores, and metadata, which builds researcher trust and enables verification. The human-in-the-loop design ensures researchers maintain control over final dataset selection while benefiting from LLM assistance in discovery.

## Foundational Learning

- **Concept:** Semantic Search vs. Keyword Matching
  - **Why needed here:** The core problem presented is the brittleness of keyword-based search, which this paper claims LLMs can solve. Understanding the distinction is essential.
  - **Quick check question:** What is the primary limitation of a TF-IDF-based search engine that a semantic model aims to solve?

- **Concept:** Human-Centered AI (HCAI)
  - **Why needed here:** The paper uses an HCAI framework, emphasizing that technical performance is insufficient without user acceptance, trust, and control.
  - **Quick check question:** In an HCAI design, is the primary goal to maximize automation or to maximize human agency and system reliability?

- **Concept:** LLM Hallucinations and Verification
  - **Why needed here:** A key barrier to acceptance is the risk of confident but incorrect results ("hallucinations"). Understanding this failure mode is critical for system design.
  - **Quick check question:** Why is the confident tone of an LLM's response considered a barrier to trust in a research context?

## Architecture Onboarding

- **Component map:** User Query -> Query Parsing Module -> Discovery Engine -> Transparency & Explanation Layer -> Human-in-the-Loop
- **Critical path:** The path to user acceptance flows as: `User Query -> LLM Retrieval -> Transparency Features (source links, confidence) -> User Verification -> Trust -> Adoption`. The critical path fails if the Transparency Features are weak or missing.
- **Design tradeoffs:**
  - **Automation vs. Control:** The system could return a single best-fit dataset (high automation, low control) or a ranked list with summaries (lower automation, high control). The paper indicates the latter is required for acceptance.
  - **Efficiency vs. Explainability:** Generating detailed provenance and confidence scores adds computational overhead and may slow down responses, trading raw speed for trustworthiness.
- **Failure signatures:**
  - **The "Echo Chamber" Failure:** The model consistently returns popular datasets, reinforcing bias and failing to find novel or under-utilized data.
  - **The "Convincing Hallucination" Failure:** The system generates a confident but non-existent dataset description without a source link, destroying user trust upon verification.
  - **The "Black Box" Failure:** The system provides recommendations without transparency features (confidence, data sources), leading to lack of trust even if results are accurate.
- **First 3 experiments:**
  1. **A/B Test Transparency Features:** Measure user trust and selection accuracy when participants are provided with LLM results both with and without direct source links and confidence scores.
  2. **Query Formulation Study:** Compare the breadth of discovered datasets when users are restricted to keyword search versus being allowed to use natural language queries.
  3. **Bias Audit:** Analyze the frequency of "popular" versus "under-utilized" datasets returned by the LLM discovery tool compared to a standard keyword search baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do user requirements and perceptions shift when researchers interact with actual LLM-based data discovery tools compared to the hypothetical scenarios explored in this study?
- Basis in paper: [explicit] The authors state in Section 4.2 that "future research should conduct similar focus groups with users that have experience using LLMs directly for data discovery" because participants in this study had to imagine usage based on general tools like ChatGPT.
- Why unresolved: The current study relied on participants' imagination of the technology rather than empirical observation of tool usage.
- What evidence would resolve it: A follow-up qualitative study observing researchers using a prototype LLM data discovery tool to identify if the stated requirements (transparency, trust) manifest differently in practice.

### Open Question 2
- Question: Do specific transparency features (e.g., confidence scores, data provenance, direct links) directly increase the acceptance rate and trust of LLM tools for data discovery?
- Basis in paper: [inferred] The paper proposes a conceptual model claiming transparency overcomes barriers, but this link remains a theoretical hypothesis derived from focus group discussions rather than an observed outcome of using a real tool.
- Why unresolved: The study identifies user desires but does not test if fulfilling those desires (transparency) actually results in the predicted behavioral change (acceptance).
- What evidence would resolve it: Experimental evaluation comparing user acceptance of an LLM tool with and without the identified transparency features implemented.

### Open Question 3
- Question: What are the comparative differences in efficiency and result quality between traditional keyword-based discovery and LLM-based discovery?
- Basis in paper: [explicit] The authors note in Section 4.2 that future research involving actual tools "would allow for a better direct comparison of using keyword and LLMs for data discovery."
- Why unresolved: The current study focuses on user perceptions and potential benefits/barriers rather than quantifying performance differences between the two methods.
- What evidence would resolve it: A user study measuring time-to-discovery and dataset relevance scores for groups using keyword search versus LLM-based search.

## Limitations

- Findings are based on self-reported perceptions rather than actual usage of LLM tools, creating a gap between stated preferences and real-world behavior
- Focus group methodology may have introduced social desirability bias, with participants potentially overstating concerns about ethics and trust
- Sample of 27 researchers represents a limited population that may not capture the full spectrum of data discovery needs across all disciplines

## Confidence

- **High Confidence:** The identification of current keyword-based search pain points (time consumption, terminology requirements) is well-supported by participant quotes and aligns with established literature on search friction
- **Medium Confidence:** The barriers to LLM adoption (biases, unreliable results, ethics, trust) are consistently reported but may be amplified by the hypothetical nature of the discussion rather than actual tool experience
- **Medium Confidence:** The proposed transparency features as solutions are logically derived from the data but require empirical validation through actual tool deployment and usage studies

## Next Checks

1. Conduct a controlled experiment comparing actual search performance and user trust when using keyword search versus an LLM prototype with varying levels of transparency features
2. Perform a longitudinal study tracking researcher behavior after initial exposure to LLM data discovery tools to measure the gap between stated concerns and actual adoption patterns
3. Expand the participant pool to include researchers from underrepresented disciplines and varying levels of technical expertise to assess generalizability of the findings