---
ver: rpa2
title: Combining Evidence and Reasoning for Biomedical Fact-Checking
arxiv_id: '2509.13879'
source_url: https://arxiv.org/abs/2509.13879
tags:
- evidence
- claim
- scientific
- retrieval
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CER, a novel biomedical fact-checking framework
  that combines scientific evidence retrieval, LLM reasoning, and supervised veracity
  prediction to combat healthcare misinformation. CER retrieves scientific evidence
  from PubMed, generates LLM justifications, and classifies claim veracity using both
  zero-shot and fine-tuned classifiers.
---

# Combining Evidence and Reasoning for Biomedical Fact-Checking

## Quick Facts
- arXiv ID: 2509.13879
- Source URL: https://arxiv.org/abs/2509.13879
- Reference count: 40
- F1 score improvements up to 3.43% over baselines

## Executive Summary
CER is a biomedical fact-checking framework that addresses healthcare misinformation by combining scientific evidence retrieval, LLM reasoning, and supervised veracity prediction. The system retrieves scientific evidence from PubMed, generates LLM justifications, and classifies claim veracity using both zero-shot and fine-tuned classifiers. Evaluations on HealthFC, BioASQ-7b, and SciFact datasets demonstrate state-of-the-art performance with significant improvements over baselines. The framework shows strong generalization across domains and robustness to retrieval method choice, while effectively mitigating LLM hallucinations through grounding in verifiable scientific sources.

## Method Summary
CER operates as a three-stage pipeline: (1) retrieves scientific evidence from PubMed using either BM25 or SBERT, extracting up to 20 sentences per claim and selecting the top 3; (2) generates structured LLM justifications using Mixtral-8x22B with a role-based Chain-of-Thought prompt that includes "Doctor" role assignment; (3) classifies veracity using a fine-tuned DeBERTa-v3-large/BERT/PubMedBERT classifier that processes claim + evidence + justification as concatenated input. The framework supports both zero-shot and fine-tuned classification modes, with training using standard hyperparameters (LR=2e-5, batch_size=8, 5 epochs).

## Key Results
- State-of-the-art F1 scores: 69.90% (HealthFC), 95.20% (BioASQ-7b), 61.14% (SciFact) with fine-tuned DeBERTa-v3
- Retrieval ablation causes 29.3 percentage point F1 drop (69.90% to 40.61%) on HealthFC, confirming grounding importance
- Robust to retrieval paradigm: sparse vs. dense retrieval shows marginal performance differences (1-2% F1)
- Cross-dataset generalization shows performance drops but framework adapts across domains

## Why This Works (Mechanism)

### Mechanism 1
Grounding LLM reasoning in retrieved scientific evidence materially reduces hallucination risk in biomedical fact-checking. Retrieved PubMed evidence constrains the LLM's generation space, providing verifiable source material that the justification must synthesize rather than fabricate. The downstream classifier then evaluates consistency between claim, evidence, and justification. Core assumption: retrieved evidence is both relevant to the claim and factually accurate (peer-reviewed PubMed abstracts). Break condition: if retrieval returns irrelevant or low-quality evidence, grounding may introduce noise rather than constraint.

### Mechanism 2
Multi-stage supervision (LLM reasoning → supervised classifier) outperforms either component alone for veracity prediction. The LLM generates structured justifications that expose reasoning traces; a fine-tuned classifier consumes claim + evidence + justification to predict true/false/NEI. This separates generation from decision, allowing the classifier to correct for LLM reasoning gaps or inconsistencies. Core assumption: LLM justifications, while imperfect, contain sufficient signal for a supervised model to learn discriminative patterns. Break condition: if LLM justifications are systematically misleading or inconsistent with evidence, the classifier may learn spurious correlations.

### Mechanism 3
Framework performance is robust to retrieval paradigm choice because the justification module compensates for retrieval variation. Sparse (BM25) and dense (SBERT) retrievers produce marginally different top-k evidence sets, but the LLM justification synthesizes across retrieved sentences, smoothing over minor relevance differences. The classifier then attends to the synthesized justification rather than raw evidence alone. Core assumption: retrieved evidence sets from different paradigms overlap sufficiently in relevant content for justification to extract consistent signal. Break condition: if retrieval quality degrades severely, justification cannot recover signal.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: CER is fundamentally a RAG-style pipeline where evidence retrieval precedes LLM reasoning; understanding retrieval-generation coupling is essential
  - Quick check question: Can you explain why retrieving evidence before LLM generation reduces hallucination compared to prompting the LLM alone?

- **Concept: Supervised Classification over Structured Inputs**
  - Why needed here: The veracity prediction module concatenates claim, evidence, and justification into a single input for a transformer classifier; understanding multi-part input formatting is critical
  - Quick check question: How would you format a claim-evidence-justification triple for a BERT-family classifier using [SEP] tokens?

- **Concept: Sparse vs. Dense Retrieval**
  - Why needed here: CER evaluates BM25 (term-matching) and SBERT embeddings (semantic similarity); practitioners must know when to prefer each
  - Quick check question: What is the tradeoff between BM25's interpretability and dense retrieval's ability to capture semantic similarity?

## Architecture Onboarding

- **Component map**: Claim Preprocessing → Scientific Evidence Retrieval → LLM Reasoning → Veracity Prediction
- **Critical path**: Retrieval quality → Justification faithfulness → Classifier accuracy. Ablation shows retrieval removal causes 29.3% F1 drop; prompt ablation shows "Doctor" role and evidence inclusion are critical
- **Design tradeoffs**:
  - Sparse vs. dense retrieval: Marginal performance difference (1-2% F1); sparse is more interpretable, dense captures semantics
  - Zero-shot vs. fine-tuned classifier: Fine-tuning yields +15.3% F1 on HealthFC, +3.4% on BioASQ-7b; requires labeled data
  - LLM choice: Mixtral-8x22B outperforms smaller biomedical LLMs on justification quality
- **Failure signatures**:
  - Low retrieval relevance → Justification hallucinates → Classifier mispredicts (check evidence-to-claim alignment)
  - Prompt missing role or evidence → F1 drops 26-36% (Table 7)
  - Cross-domain transfer without fine-tuning → Performance gaps in specialized datasets
- **First 3 experiments**:
  1. Retrieval ablation: Run CER with and without PubMed evidence on a held-out subset; measure F1 delta to confirm grounding effect (expect ~20-30% drop without evidence)
  2. Prompt component ablation: Remove "Doctor" role, evidence, or justification requirement sequentially; compare F1 to validate Table 7 patterns
  3. Cross-dataset transfer: Train on BioASQ-7b, test on HealthFC-2 and SciFact-2 (and reverse); assess generalization gaps per Figure 4

## Open Questions the Paper Calls Out
- Can integrating heterogeneous external databases beyond PubMed significantly improve CER's fact-checking accuracy? (Future work: extend CER's evidence retrieval to additional biomedical databases for richer context)
- To what extent does LLM hallucination in the reasoning phase degrade the final veracity prediction? (Generating justifications with models free from hallucinations could potentially improve prediction accuracy)
- Does access to full-text articles provide a significant performance advantage over the current abstract-only approach? (Current system focuses on abstracts due to access restrictions; full-text utility remains untested)

## Limitations
- Sparse citation evidence for key claims: average citations across related papers is 0, suggesting novelty and impact claims lack external validation
- Limited ablation scope: critical components like LLM model variations and retrieval hyperparameters are not systematically tested
- Cross-dataset transfer gaps: performance drops when moving between domains, but paper does not deeply analyze which claim types cause failures

## Confidence
- High confidence in CER framework architecture and baseline performance improvements
- Medium confidence in hallucination mitigation claims, as the 29.3 percentage point drop without evidence is compelling but assumes retrieved evidence is always high-quality
- Low confidence in robustness-to-retrieval-paradigm claims, as marginal differences (1-2% F1) could be within experimental variance

## Next Checks
1. **Retrieval quality analysis**: Measure precision@3 for both BM25 and SBERT retrievers on a held-out validation set to quantify how often top-3 evidence is relevant to the claim
2. **Cross-domain error analysis**: For cross-dataset experiments, perform error analysis categorizing failures by claim type (clinical vs. research vs. public health) to identify systematic generalization gaps
3. **Hallucination audit**: Manually review 50 LLM justifications from HealthFC test set, marking each as supported/neutral/contradicted by source evidence to quantify hallucination rate with and without evidence retrieval