---
ver: rpa2
title: 'Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic
  Control using Agent-based Simulation'
arxiv_id: '2508.05154'
source_url: https://arxiv.org/abs/2508.05154
tags:
- metrics
- algorithm
- state
- algorithms
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of evaluating reinforcement
  learning algorithms for optimizing agent-based models, particularly in epidemiological
  simulations. The authors propose domain-driven metrics that combine traditional
  reward-based measures with domain-specific knowledge.
---

# Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation

## Quick Facts
- arXiv ID: 2508.05154
- Source URL: https://arxiv.org/abs/2508.05154
- Authors: Rishabh Gaur; Gaurav Deshkar; Jayanta Kshirsagar; Harshal Hayatnagarkar; Janani Venugopalan
- Reference count: 1
- One-line primary result: Domain-driven metrics combining traditional rewards with domain knowledge provide more robust RL algorithm ranking than mean-reward comparisons alone in agent-based epidemic simulations.

## Executive Summary
This paper addresses the challenge of evaluating reinforcement learning algorithms for optimizing agent-based models, particularly in epidemiological simulations. The authors propose domain-driven metrics that combine traditional reward-based measures with domain-specific knowledge. They introduce a framework using InterestingnessXRL to extract interaction data and develop five key metrics: sequence comparison, median of mean-rewards, state-space coverage, unified coverage, and mean-reward comparison. Tested on a rational ABM disease modeling case study with 1000 agents, the approach evaluates eight RL algorithm variants across different mask availability scenarios. Results show that domain-driven metrics provide more robust algorithm ranking than traditional mean-reward comparisons alone, with vanilla TD3 achieving the highest aggregate rank of 9 in the high-mask experiment.

## Method Summary
The study evaluates RL algorithms for epidemic control policy optimization in agent-based simulations using a framework that extracts interaction data via InterestingnessXRL. The approach defines five domain-driven metrics: sequence comparison (evaluating if policies reach domain-defined optimal states), median of mean-rewards (robust central tendency measure), state-space coverage (exploration adequacy), unified coverage (combined state-action exploration), and mean-reward comparison. Tested on a rational ABM disease model with 1000 agents across three experiments (Baseline, High-Mask with 800 masks, Low-Mask with 100 masks), the framework evaluates eight RL variants (DDPG/TD3 with vanilla, NR, BN, NR_BN modifications). The metrics are aggregated into ranks and validated against Google's reliability metrics.

## Key Results
- Domain-driven metrics provide more robust algorithm ranking than traditional mean-reward comparisons alone
- In high-mask experiment, vanilla TD3 achieved highest aggregate rank of 9
- NR_DDPG led in low-mask scenario with aggregate rank of 5
- Metrics aligned closely with Google's reliability metrics (IQR, CVaR), enhancing confidence in evaluation

## Why This Works (Mechanism)

### Mechanism 1
Domain-driven sequence comparison provides a more robust indicator of policy quality than raw reward metrics alone by extracting action sequences from local minima to local maxima states during exploit runs, then evaluating the percentage of sequences terminating in domain-defined "best end state" (state index 0 representing <5% infected, hospitalized, and below-poverty-line populations). Core assumption: Domain experts can reliably define optimal terminal states, and sequences reaching this state indicate learned policies aligned with real-world objectives.

### Mechanism 2
Unified coverage metrics (state-space + state-action space) capture exploration adequacy, predicting robustness in stochastic ABM environments. Continuous spaces are discretized via domain-informed binning (e.g., [0.0, 0.05, 0.10, 0.15, 0.20, 1] for infection rates). Coverage percentage is calculated during training, with higher coverage indicating the algorithm explored diverse scenarios, reducing risk of brittle policies. Core assumption: Binning strategies derived from domain knowledge produce meaningful indices, and higher coverage correlates with better generalization.

### Mechanism 3
Aggregated multi-metric ranking produces more stable algorithm comparisons than single-metric evaluation. Five metrics each produce a rank, and aggregate rank is the sum, validated against Google's reliability metrics. Core assumption: Combining orthogonal metrics reduces variance from any single metric's noise; reliability metrics from simpler RL domains generalize to complex ABM settings.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) with continuous action spaces
  - Why needed here: The policy optimization problem (lockdown timing, vaccination scheduling) is formulated as a continuous-action MDP. Understanding state transitions, reward functions, and discount factors is prerequisite.
  - Quick check question: Can you explain why continuous action spaces require different algorithm choices (e.g., DDPG, TD3) compared to discrete action spaces?

- Concept: Agent-Based Models (ABMs) and stochasticity
  - Why needed here: The simulation involves 1000 rational agents with individual decision-making. ABM outputs are non-deterministic, motivating the need for robust metrics beyond mean rewards.
  - Quick check question: Why might two identical ABM runs with the same policy produce different infection trajectories?

- Concept: Exploration vs. Exploitation trade-off
  - Why needed here: Coverage metrics explicitly measure exploration. Without understanding this trade-off, the rationale for penalizing high-reward but low-coverage algorithms is unclear.
  - Quick check question: If an algorithm achieves high mean reward but only visits 10% of state space during training, what risk does this pose at deployment?

## Architecture Onboarding

- Component map: Simulation Module -> Policy Discovery Module -> Analysis Module -> Evaluation Metric Module -> Algorithm Ranking Module
- Critical path:
  1. Define state-space components and binning strategy
  2. Define action-space components and binning strategy
  3. Run ABM simulation with RL policy optimization
  4. Extract interaction data via InterestingnessXRL
  5. Compute 5 metrics → aggregate ranks → validate against reliability metrics
- Design tradeoffs:
  - Binning granularity: Coarser bins simplify analysis but may lose critical distinctions
  - Sequence vs. reward emphasis: Prioritizing sequence comparison favors policies reaching domain-defined goal states; may undervalue short-term harm reduction
  - Aggregate vs. single metric: Robustness increases, but interpretability decreases for policymakers
- Failure signatures:
  - Mean-reward-only evaluation shows algorithms with very similar scores, making differentiation impossible
  - Algorithms with high mean reward but 0% best sequences indicate reward hacking without achieving domain goals
  - Low coverage (<15%) despite high reward suggests overfitting to narrow scenarios
- First 3 experiments:
  1. Baseline sanity check: Run all 8 RL variants on Baseline experiment, verify clear ordering and alignment with reliability metrics
  2. Binning sensitivity analysis: Halve and double bin counts for state components, measure how coverage and sequence percentages change
  3. Cross-scenario validation: Take top-ranked algorithm from High Mask experiment and deploy in Low Mask scenario without retraining, observe performance drop

## Open Questions the Paper Calls Out
- How does the performance and ranking stability of these domain-driven metrics scale when applied to larger, more complex agent populations?
- Can explainability techniques be effectively integrated into this framework to interpret specific policy decisions made by the RL agents?
- How sensitive are the "Sequence Comparison" and "State-space Coverage" metrics to the specific binning strategies used to discretize the continuous state and action spaces?

## Limitations
- Implementation details like reward function formulation, hyperparameter choices, and NR/BN noise specifications are not fully disclosed
- Validation relies on a single disease model (COVID-19) and three mask scenarios, limiting generalizability claims
- Multi-metric aggregation approach lacks strong evidence that combining these five specific metrics is optimal

## Confidence
- High Confidence: The necessity of domain-driven metrics for ABM evaluation is well-justified given stochasticity and non-differentiability challenges in ABMs
- Medium Confidence: The mechanism linking sequence comparison to policy quality and unified coverage to robustness is plausible but depends on unverified assumptions about domain knowledge accuracy and binning appropriateness
- Medium-Low Confidence: Claims of improved generalization across scenarios and alignment with Google reliability metrics are partially supported but require broader validation

## Next Checks
1. Implement and document the exact reward function used to ensure reproducibility and enable fair comparison
2. Conduct ablation studies varying NR/BN noise magnitudes, learning rates, and network architectures to determine their impact on metric stability
3. Apply the domain-driven metric framework to a non-epidemiological ABM (e.g., traffic flow or market dynamics) to test generalizability beyond disease modeling