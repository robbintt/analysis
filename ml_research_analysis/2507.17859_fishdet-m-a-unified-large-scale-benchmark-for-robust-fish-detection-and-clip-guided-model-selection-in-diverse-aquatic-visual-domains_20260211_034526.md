---
ver: rpa2
title: 'FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided
  Model Selection in Diverse Aquatic Visual Domains'
arxiv_id: '2507.17859'
source_url: https://arxiv.org/abs/2507.17859
tags:
- detection
- fish
- underwater
- dataset
- fishdet-m
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FishDet-M addresses the fragmentation of underwater fish detection
  datasets by unifying 13 diverse public datasets into a single benchmark with COCO-style
  annotations, enabling consistent and scalable cross-domain evaluation. It includes
  105,556 images and 296,885 annotated fish instances across marine, brackish, and
  aquarium environments.
---

# FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains

## Quick Facts
- arXiv ID: 2507.17859
- Source URL: https://arxiv.org/abs/2507.17859
- Reference count: 40
- Primary result: YOLO12x achieves highest mAP of 0.491 on unified 13-dataset benchmark

## Executive Summary
FishDet-M addresses the fragmentation of underwater fish detection datasets by unifying 13 diverse public datasets into a single benchmark with COCO-style annotations, enabling consistent and scalable cross-domain evaluation. It includes 105,556 images and 296,885 annotated fish instances across marine, brackish, and aquarium environments. The benchmark evaluates 28 state-of-the-art detection models, including YOLO variants, DETR, and R-CNN, using metrics such as mAP, scale-specific AP, and efficiency profiling. Results show YOLO12x achieving the highest mAP of 0.491, with lightweight models like YOLOv8n offering strong real-time performance. To support adaptive deployment, a CLIP-based model selection framework is introduced, enabling dynamic model routing without ensemble computation.

## Method Summary
FishDet-M unifies 13 heterogeneous underwater fish datasets into a single COCO-style benchmark through format conversion and stratified splitting to preserve ecological diversity. The benchmark trains and evaluates 28 object detection models (YOLO variants, DETR/Deformable-DETR/RT-DETR, Faster/Cascade/Sparse R-CNN, RetinaNet, FCOS, MobileNetV2-SSD) using default hyperparameters on RTX 4090 hardware. A CLIP-based model selection framework routes input images to the most appropriate detector based on semantic similarity between image embeddings and model-specific text prompts. The unified dataset contains 105,556 images and 296,885 fish instances across marine, brackish, and aquarium environments, with all instances labeled under a single "Fish" class.

## Key Results
- YOLO12x achieves highest mAP of 0.491, while YOLOv8n offers strong real-time performance at 83 FPS
- Transformer-based models (DETR, Deformable-DETR, RT-DETR) show limited performance on small objects, with AP_S values below 0.15
- CLIP-guided model selection achieves mAP of 0.444 with ~12ms overhead, routing to optimal detectors without ensemble computation
- Performance varies significantly across source domains, from mAP 0.846 on DeepFish to 0.371 on FishDataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unifying fragmented underwater datasets into a single COCO-style benchmark enables consistent cross-domain evaluation and improves model generalization.
- **Mechanism:** The authors harmonize 13 heterogeneous datasets (spanning marine, brackish, and aquarium environments) into one annotation format, applying stratified sampling to preserve ecological diversity across train/val/test splits. This reduces domain-specific overfitting and exposes models to varied visual conditions during training.
- **Core assumption:** Diverse training distributions produce features that transfer better to unseen aquatic conditions than single-domain training.
- **Evidence anchors:** [abstract] "comprising 13 publicly available datasets spanning diverse aquatic environments" and [Section III] "partitioned into training, validation, and test sets using stratified sampling to ensure balanced representation across habitat types, visibility conditions, and fish densities"

### Mechanism 2
- **Claim:** CLIP-based semantic similarity can dynamically route input images to the most appropriate detector from a model pool without ensemble computation.
- **Mechanism:** Each candidate model is associated with textual prompts describing its strengths (e.g., "good for small fish in turbid water"). CLIP encodes the input image and all prompts; the model whose prompt has highest cosine similarity to the image embedding is selected for inference. This replaces heuristic or threshold-based routing.
- **Core assumption:** CLIP's vision-language alignment captures scene properties (visibility, scale, occlusion) that correlate with which detector will perform best on that image.
- **Evidence anchors:** [abstract] "CLIP-based model selection framework...enabling dynamic model routing without ensemble computation" and [Section V.C] "YOLOv8x, YOLOv8l, and YOLO12m consistently achieved higher median scores with compact interquartile ranges, indicating reliable semantic alignment"

### Mechanism 3
- **Claim:** YOLO-family detectors outperform transformer-based and region-proposal models on underwater fish detection due to better small-object handling and inference efficiency.
- **Mechanism:** YOLO architectures (v8–v12) use feature pyramid networks and anchor-free detection heads optimized for multi-scale objects. The benchmark's long-tailed distribution (dominated by small/medium fish) favors these designs over DETR's attention-based localization, which struggles with small objects and incurs higher latency.
- **Core assumption:** Real-world underwater deployment prioritizes both accuracy on small/distant fish and real-time inference speed.
- **Evidence anchors:** [Section V.A, Table VI] YOLO12x achieves 0.491 mAP with AP_S=0.315, while Deformable-DETR achieves only 0.317 mAP with AP_S=0.12

## Foundational Learning

- **Concept: COCO Evaluation Metrics (mAP, AP@50, AP@75, AP_S/AP_M/AP_L)**
  - Why needed here: The entire benchmark uses COCO protocol; understanding these metrics is essential to interpret Table VI and the scale-specific analysis.
  - Quick check question: Can you explain why AP_S might be more informative than overall mAP for turbid underwater scenes?

- **Concept: Vision-Language Alignment (CLIP embeddings)**
  - Why needed here: The model selection mechanism relies on CLIP's joint image-text space; misunderstanding this leads to incorrect assumptions about what the selector "knows."
  - Quick check question: If CLIP has never seen underwater fish images during pretraining, what properties might its embeddings still capture that enable useful routing?

- **Concept: Stratified Dataset Splitting**
  - Why needed here: FishDet-M's credibility depends on balanced representation across environments; understanding stratification helps assess whether test results reflect real generalization.
  - Quick check question: Why would a random split potentially bias evaluation toward the largest source dataset (FishNet with 94,532 images)?

## Architecture Onboarding

- **Component map:** Data ingestion pipeline → format validation → COCO conversion → stratified splitting → training framework → evaluation suite → CLIP selector
- **Critical path:** Data harmonization quality → stratified split integrity → consistent training across 28 models → standardized metric computation. The CLIP selector is optional; core benchmark utility does not depend on it.
- **Design tradeoffs:** Species-agnostic "Fish" category simplifies detection but removes fine-grained classification capability; default hyperparameters favor comparability over per-model optimization; CLIP selector adds ~12ms inference overhead without surpassing top fixed models in accuracy
- **Failure signatures:** Camouflage/low-contrast scenes show oversized bounding boxes or false negatives; dense schools cause merged detections and missed individuals; small objects in high-resolution frames result in near-zero AP_S for transformer models
- **First 3 experiments:** 1) Train YOLOv8n and YOLO12x on FishDet-M training split; reproduce Table VI mAP values within ±0.01 to confirm setup correctness. 2) Evaluate each trained model on individual source test splits to verify the reported performance tiers. 3) Visualize CLIP similarity score distributions for images from extreme environments to confirm prompts correlate with expected model strengths.

## Open Questions the Paper Calls Out

- **Question:** Can optimized prompts, confidence-weighted voting, or ensemble strategies improve the CLIP-based model selector to match or exceed the accuracy of fixed top-performing models while maintaining computational efficiency?
  - **Basis in paper:** [explicit] The authors state that the CLIP-guided selector "introduces computational overhead and marginally reduces accuracy compared to the top fixed models" and propose "exploring optimized prompts, confidence-weighted voting, or ensemble strategies may offer better speed–accuracy tradeoffs."
  - **Why unresolved:** The FishDet-M-CLIP achieved mAP of 0.444 versus YOLO12x's 0.491, indicating a performance gap that current zero-shot selection cannot close.
  - **What evidence would resolve it:** A modified CLIP selector achieving mAP ≥0.49 at ≥80 FPS on FishDet-M test set would demonstrate improvement.

- **Question:** How can explainable AI (XAI) techniques illuminate failure modes in underwater fish detection, particularly for camouflage, occlusion, and dense school scenarios?
  - **Basis in paper:** [explicit] "Explainability remains an open frontier. Integrating XAI techniques could elucidate model decisions and failure modes in complex underwater environments."
  - **Why unresolved:** Current models produce inaccurate bounding boxes in camouflage conditions and miss detections under occlusion without interpretable explanations for these failures.
  - **What evidence would resolve it:** XAI methods that provide actionable insights into why specific failure modes occur and enable targeted model improvements.

- **Question:** What annotation extensions (pose, individual identity, activity labels) would most effectively support downstream behavioral analysis and species-specific tracking applications?
  - **Basis in paper:** [explicit] "Expanding the annotation scope would enable deeper biological insights" for applications like "behavioral analysis and species-specific tracking."
  - **Why unresolved:** FishDet-M uses species-agnostic "Fish" category labels only, limiting utility for fine-grained ecological studies.
  - **What evidence would resolve it:** Extended annotations with demonstrated improvements in tracking continuity or behavior classification tasks.

## Limitations

- Training hyperparameter specifics remain underspecified, with results dependent on default settings that may not optimize all architectures equally
- CLIP prompt templates are not provided, limiting exact reproduction of the model selection framework
- Cross-dataset annotation consistency cannot be fully verified despite format unification

## Confidence

- **High**: Dataset unification strategy and stratified splitting methodology
- **Medium**: YOLO performance superiority and transformer limitations on small objects
- **Medium**: CLIP-guided model selection framework feasibility and basic effectiveness
- **Low**: Exact numerical results without full hyperparameter disclosure

## Next Checks

1. Train YOLOv8n and YOLO12x with provided FishDet-M splits; verify mAP values match Table VI within ±0.01
2. Implement CLIP-based selector using basic prompt templates; confirm it achieves top-3 accuracy for 70%+ of test images
3. Evaluate model performance on individual source dataset splits to reproduce the 0.846 to 0.371 mAP range across domains