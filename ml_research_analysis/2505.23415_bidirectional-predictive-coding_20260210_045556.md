---
ver: rpa2
title: Bidirectional predictive coding
arxiv_id: '2505.23415'
source_url: https://arxiv.org/abs/2505.23415
tags:
- learning
- inference
- generative
- discriminative
- discpc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bidirectional predictive coding (bPC) is a biologically plausible
  model of visual inference that integrates generative and discriminative processing,
  enabling the brain to both classify and reconstruct visual inputs. Unlike previous
  predictive coding models, which specialize in either generative or discriminative
  tasks, bPC achieves performance on par with state-of-the-art unidirectional models
  in both domains.
---

# Bidirectional predictive coding

## Quick Facts
- arXiv ID: 2505.23415
- Source URL: https://arxiv.org/abs/2505.23415
- Reference count: 40
- Key outcome: BiPC integrates generative and discriminative processing for visual inference, matching state-of-the-art unidirectional models in both domains

## Executive Summary
Bidirectional predictive coding (bPC) is a biologically plausible model of visual inference that integrates generative and discriminative processing, enabling the brain to both classify and reconstruct visual inputs. Unlike previous predictive coding models, which specialize in either generative or discriminative tasks, bPC achieves performance on par with state-of-the-art unidirectional models in both domains. It does so by developing an energy landscape optimized for both classification and generation, avoiding the overconfidence or bias present in traditional models. Experiments on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 demonstrate that bPC matches or outperforms previous models in supervised classification, unsupervised representation learning, and tasks like multimodal learning and inference with missing data. This integration makes bPC a strong candidate for modeling flexible visual inference in the brain while also improving robustness in machine learning applications.

## Method Summary
bPC combines predictive coding with a novel energy landscape that simultaneously optimizes for both generative and discriminative tasks. The model uses a hierarchical architecture where each layer contains prediction errors and state units, allowing bidirectional information flow. During inference, the model minimizes prediction errors across both top-down (generative) and bottom-up (discriminative) pathways, resulting in an energy function that balances classification accuracy with reconstruction fidelity. The energy landscape is optimized using contrastive divergence learning, enabling the model to learn representations that are useful for both tasks without overfitting or becoming overconfident in predictions.

## Key Results
- bPC matches state-of-the-art unidirectional models in supervised classification accuracy on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100
- The model demonstrates superior performance in unsupervised representation learning and multimodal tasks compared to purely generative or discriminative baselines
- bPC shows improved robustness to missing data and noisy inputs, maintaining accuracy when up to 50% of input features are occluded

## Why This Works (Mechanism)
bPC works by jointly optimizing an energy landscape for both classification and reconstruction tasks, allowing the model to maintain uncertainty and avoid overconfidence that plagues single-task models. The bidirectional flow of prediction errors creates a dynamic equilibrium where the model continuously updates its internal representations based on both top-down expectations and bottom-up sensory evidence. This integration enables the model to leverage the complementary strengths of generative and discriminative approaches: the generative pathway provides context and prior knowledge, while the discriminative pathway ensures precise classification boundaries. The energy landscape acts as a regularizer, preventing the model from becoming too specialized in either direction and maintaining flexibility across diverse visual inference tasks.

## Foundational Learning

**Predictive coding**: A framework where neural networks minimize prediction errors between expected and actual sensory inputs; needed for understanding how bPC updates its internal states and why quick check is understanding error propagation.

**Energy-based models**: Models that define probability distributions through an energy function; needed for grasping how bPC's energy landscape enables dual-task optimization and why quick check is understanding contrastive divergence learning.

**Hierarchical generative models**: Layered architectures that build complex representations from simple components; needed for comprehending bPC's multi-level structure and why quick check is understanding how features are composed across layers.

## Architecture Onboarding

**Component map**: Input data -> Convolutional layers -> Prediction error units -> State units -> Energy minimization -> Output classification/reconstruction

**Critical path**: Input -> Convolutional feature extraction -> Prediction error computation -> State update via energy minimization -> Output generation

**Design tradeoffs**: bPC trades computational efficiency for biological plausibility and dual-task capability; while slower than specialized models, it gains flexibility and robustness across diverse visual tasks.

**Failure signatures**: Model may underperform on extremely large datasets where specialized architectures excel; can struggle with highly abstract concepts that require deeper hierarchical processing than implemented.

**Three first experiments**: 1) Test bPC on MNIST classification vs. standard CNN baseline; 2) Evaluate reconstruction quality on Fashion-MNIST with varying levels of input occlusion; 3) Compare energy landscape dynamics during inference with and without bidirectional flow enabled.

## Open Questions the Paper Calls Out
None

## Limitations
- The model's computational efficiency and scalability to very large datasets remains untested
- Claims of biological plausibility are not fully substantiated with neurobiological evidence
- The energy landscape optimization may introduce its own biases that weren't extensively explored across diverse datasets

## Confidence

| Claim | Confidence |
|-------|------------|
| Performance on standard benchmarks | Medium |
| Biological plausibility | Low |
| Computational efficiency vs. state-of-the-art | Medium |

## Next Checks
1. Test bPC on more diverse and complex datasets (e.g., ImageNet, video data) to assess scalability and robustness.
2. Conduct ablation studies to isolate the contributions of generative and discriminative components to overall performance.
3. Compare bPC's computational efficiency and memory usage with state-of-the-art unidirectional models in large-scale settings.