---
ver: rpa2
title: Online Competitive Information Gathering for Partially Observable Trajectory
  Games
arxiv_id: '2506.01927'
source_url: https://arxiv.org/abs/2506.01927
tags:
- information
- planning
- each
- which
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses online planning for non-cooperative robots
  in partially observable settings where agents must gather information about opponents
  to plan optimally. While modern MARL methods can solve such problems offline, online
  planning without heavy pre-computation remains challenging.
---

# Online Competitive Information Gathering for Partially Observable Trajectory Games

## Quick Facts
- arXiv ID: 2506.01927
- Source URL: https://arxiv.org/abs/2506.01927
- Authors: Mel Krusniak; Hang Xu; Parker Palermo; Forrest Laine
- Reference count: 29
- Key outcome: This work addresses online planning for non-cooperative robots in partially observable settings where agents must gather information about opponents to plan optimally.

## Executive Summary
This paper tackles the challenge of online planning for non-cooperative robots in partially observable environments where agents need to actively gather information about opponents to plan optimally. While offline MARL methods can solve such problems, online planning without heavy pre-computation remains difficult. The authors propose extending model-predictive game play to imperfect-information games by planning policies that map observation histories to actions, rather than single trajectories. This enables active information gathering behavior through particle-based representations of joint state-observation distributions and stochastic gradient play to find approximate Nash equilibria.

The approach demonstrates agents briefly turning to gather information before executing plans based on what they learned, outperforming passive competitors that plan without considering future observations. The method scales to N-player games and environments with obstacles, with timing results suggesting real-time execution is achievable with optimization. The particle-based approach handles the belief hierarchy problem by maintaining joint distributions over states and observation histories rather than higher-order beliefs about opponents' beliefs.

## Method Summary
The key innovation is extending model-predictive control to imperfect-information games by planning policies rather than trajectories. The method uses particle-based representations to maintain joint distributions over states and observation histories, enabling active information gathering. Stochastic gradient play finds approximate Nash equilibria in the planning game, allowing agents to optimize their observation-gathering behavior. This transforms the planning problem from optimizing fixed trajectories to optimizing observation-history-to-action mappings that can adapt based on gathered information.

## Key Results
- Demonstrates active information gathering behavior where agents briefly turn to gather information before executing plans based on what they learned
- Outperforms passive competitors that plan without considering future observations in continuous pursuit-evasion and warehouse-pickup scenarios
- Scales to N-player games and environments with obstacles
- Takes 0.06-0.22 seconds per gradient step per player, suggesting real-time execution is within reach with optimization

## Why This Works (Mechanism)
The method works by transforming the planning problem from optimizing fixed trajectories to optimizing policies that map observation histories to actions. This allows agents to actively gather information that will be useful for future decision-making. The particle-based representation maintains joint distributions over states and observation histories, avoiding the exponential complexity of higher-order belief hierarchies. Stochastic gradient play then finds equilibria in the resulting game, enabling coordinated behavior even in competitive settings.

## Foundational Learning
- **Particle-based belief representation**: Needed to handle the exponential complexity of higher-order belief hierarchies in imperfect-information games. Quick check: Verify particles maintain sufficient diversity to represent relevant uncertainty.
- **Stochastic gradient play for Nash equilibrium**: Required to find approximate equilibria in the planning game. Quick check: Monitor convergence behavior across different game types.
- **Policy-based planning vs trajectory planning**: Essential for enabling observation-history-dependent behavior. Quick check: Compare performance against trajectory-based baselines.
- **Model-predictive control extension**: Needed to make planning tractable in continuous spaces. Quick check: Validate planning horizon affects performance appropriately.
- **Joint state-observation distribution**: Critical for maintaining uncertainty about both physical states and observations. Quick check: Ensure distributions remain well-calibrated.
- **Information gathering as game strategy**: Fundamental to the approach's success. Quick check: Verify agents actually improve performance through active sensing.

## Architecture Onboarding

**Component Map**
Particle filter -> Policy network -> Gradient play optimizer -> Game solver

**Critical Path**
Observation history → Particle filter update → Policy network evaluation → Action selection → Environment transition

**Design Tradeoffs**
The particle-based approach trades computational efficiency for expressive power in representing beliefs. While Gaussian approximations would be faster, they cannot capture multimodal or complex belief structures that arise in competitive scenarios.

**Failure Signatures**
- Particle collapse indicating loss of diversity
- Gradient play divergence suggesting poor game structure
- Suboptimal policies that don't gather useful information
- Timing violations preventing real-time execution

**First Experiments**
1. Verify particle diversity maintenance across observation updates
2. Test policy network's ability to condition on observation history
3. Validate gradient play convergence in simple two-player games

## Open Questions the Paper Calls Out
None

## Limitations
- Computational cost scales poorly with state space dimensionality and number of agents, potentially limiting scalability to larger multi-agent systems
- Assumes perfect knowledge of transition and observation models, which may not hold in real-world deployments
- Evaluation focuses on relatively simple continuous domains without extensive testing in environments with complex dynamics or high-dimensional observation spaces

## Confidence
High: The core algorithmic contribution of extending model-predictive control to imperfect-information games using policy-based planning is well-founded and the particle-based representation approach is technically sound.

Medium: The empirical results showing active information gathering behavior and performance advantages over passive baselines are convincing but based on a limited set of scenarios. The timing results suggesting real-time capability are promising but depend heavily on optimization.

Low: The scalability claims to N-player games and complex environments with obstacles are not fully validated, as the experiments focus primarily on small-scale problems.

## Next Checks
1. Test scalability by implementing the method in environments with 6+ agents and measuring both performance degradation and computational cost increases.
2. Evaluate robustness to model uncertainty by introducing noise in transition and observation models during planning, measuring how performance degrades compared to the perfect-model baseline.
3. Conduct ablation studies comparing the particle-based approach against alternative belief representation methods (e.g., Gaussian approximations) across a range of partially observable games to quantify the benefits of the particle representation.