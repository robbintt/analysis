---
ver: rpa2
title: 'AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover
  Potential Risks of Generative AI'
arxiv_id: '2511.20686'
source_url: https://arxiv.org/abs/2511.20686
tags:
- safety
- data
- dataset
- risk
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces AssurAI, a large-scale Korean multimodal
  dataset for evaluating generative AI safety, addressing the lack of culturally relevant
  benchmarks. A multidisciplinary expert group curated 35 risk factors spanning universal
  harms and Korean socio-cultural contexts, then constructed 11,480 instances across
  text, image, video, and audio modalities.
---

# AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI

## Quick Facts
- arXiv ID: 2511.20686
- Source URL: https://arxiv.org/abs/2511.20686
- Reference count: 40
- This study introduces AssurAI, a large-scale Korean multimodal dataset for evaluating generative AI safety, addressing the lack of culturally relevant benchmarks.

## Executive Summary
AssurAI addresses the critical gap in culturally relevant AI safety evaluation by constructing a Korean multimodal dataset covering 35 risk factors across 6 categories. The dataset comprises 11,480 instances spanning text, image, video, and audio modalities, built through a rigorous methodology combining expert-led seed generation, crowdsourced scaling, triple independent annotation, and iterative expert red-teaming. Pilot testing on five LLMs demonstrated stable evaluation metrics (CV < 9%) and revealed model-specific safety patterns, establishing AssurAI as a robust tool for assessing Korean AI safety and supporting the development of more reliable generative AI systems.

## Method Summary
The AssurAI dataset was constructed through a two-stage process: experts created a taxonomy of 35 risk factors adapted from universal frameworks (AIR 2024, MIT FutureTech) and generated seed data comprising 10% of the target; crowdsourced workers then scaled production with training on guidelines. Each instance underwent triple independent annotation, followed by iterative expert red-teaming for quality refinement. The evaluation pipeline used standardized judge prompts with a 5-point rubric, scored by GPT-4o-mini for text and GPT-5-mini for multimodal content. The dataset was piloted on five LLMs, measuring safety scores, inter-annotator agreement, and statistical significance through ANOVA and effect size analysis.

## Key Results
- 11,480 instances across text (83%), image (10%), video (4%), and audio (3%) modalities
- Stable evaluation metrics across five LLMs with mean scores 3.3-3.9 and CV < 9%
- Revealed model-specific safety patterns, particularly weaknesses in Rail and Chain-of-Thought prompt types
- Triple independent annotation achieved consistent inter-annotator agreement supporting dataset reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multidisciplinary expert group can adapt universal AI harm frameworks to capture culture-specific risks that existing English-centric datasets miss.
- Mechanism: Expert review of established frameworks (AIR 2024, MIT FutureTech) → curation/adaptation of 35 risk factors → taxonomy covering both universal harms and Korean socio-cultural contexts → data construction guidelines derived from taxonomy.
- Core assumption: Universal harm frameworks contain structural patterns that can be meaningfully localized while preserving evaluative validity.
- Evidence anchors:
  - [abstract]: "we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context"
  - [section 3]: "Risk factors 1 to 30 were curated and adapted from the AIR 2024 study, while risk factors 31 to 35 were curated and adapted from the MIT FutureTech study"
  - [corpus]: Related work on sovereign LLMs confirms the need for socio-cultural alignment, supporting the gap-filling rationale, but direct evidence for this specific adaptation method remains limited to this study.
- Break condition: Expert group lacks sufficient cultural diversity or domain expertise → taxonomy fails to capture nuanced local risks (e.g., honorific speech issues, regional dialect sensitivities).

### Mechanism 2
- Claim: Combining expert-led seed data generation with crowdsourced scaling produces safety datasets that balance quality with scale better than either approach alone.
- Mechanism: Experts create schematic design + seed data (10% of total) → crowdsourced workers trained on guidelines scale production → triple independent annotation → iterative expert red-teaming feedback loop.
- Core assumption: Seed data quality propagates through crowdsourcing when workers receive proper training and clear guidelines.
- Evidence anchors:
  - [abstract]: "expert-led seed data generation, crowdsourcing-based mass production, triple independent annotation, and iterative expert red-teaming"
  - [section 4.2.1]: "These seed data, comprising 10% of the total target amount, served as clear guidelines for crowdsourced workers"
  - [corpus]: No direct corpus evidence for this specific hybrid methodology; related work focuses on dataset outputs rather than construction processes.
- Break condition: Crowd worker training is insufficient OR seed data doesn't adequately exemplify edge cases → quality degrades at scale (IAA drops below acceptable thresholds).

### Mechanism 3
- Claim: Standardized judge models with consistent rubrics can produce stable safety evaluations across different LLMs, enabling meaningful cross-model comparison.
- Mechanism: Unified evaluation pipeline (Text Track + Multimodal Track) → standardized Judge Prompts with 5-point rubric → automated scoring via GPT-4o-mini/GPT-5-mini → statistical validation (CV, ANOVA).
- Core assumption: Judge models provide a consistent proxy for human safety judgments across diverse risk categories.
- Evidence anchors:
  - [abstract]: "Pilot testing on five LLMs showed stable evaluation metrics (mean scores 3.3-3.9, CV <9%)"
  - [section 6.3]: "All models showed CV values below 9%, confirming that the scoring framework maintained operational stability and minimal bias"
  - [corpus]: No corpus evidence directly validates this judge model approach; benchmarking methodology papers focus on different evaluation paradigms.
- Break condition: Judge model has systematic biases OR rubric categories are ambiguous → inconsistent scoring across risk categories (high variance within models).

## Foundational Learning

- Concept: AI Safety Taxonomies
  - Why needed here: The paper's 35 risk factors derive from existing frameworks; understanding taxonomy structure helps interpret which risks are universal vs. culture-specific.
  - Quick check question: Can you name three risk categories from standard AI safety frameworks and explain why they might manifest differently across cultures?

- Concept: Inter-Annotator Agreement (IAA)
  - Why needed here: Triple independent annotation is central to the quality control process; IAA metrics validate data reliability.
  - Quick check question: Why might triple annotation be particularly important for subjective safety judgments compared to objective classification tasks?

- Concept: Red-Teaming in AI Safety
  - Why needed here: Iterative expert red-teaming serves as the validation loop; understanding red-teaming clarifies how vulnerabilities are discovered and addressed.
  - Quick check question: How does expert red-teaming differ from automated adversarial testing in terms of the types of vulnerabilities it can uncover?

## Architecture Onboarding

- Component map:
  Dataset Design Team (KAIST labs) -> Data Production Team (SelectStar) -> Quality Assessment Team (Universities) -> Pilot Implementation Team (Kakao)

- Critical path:
  1. Taxonomy definition (35 risk factors across 6 categories)
  2. Seed data generation by domain experts (10% of target)
  3. Crowdsourced scaling with trained workers
  4. Triple independent annotation per instance
  5. Expert red-teaming review → feedback → iteration
  6. Pilot testing on target models → dataset release

- Design tradeoffs:
  - Expert depth vs. crowd scale: 10% expert seed balances quality with cost; assumption is that guidelines propagate quality
  - Breadth vs. depth in risk coverage: 35 factors with non-uniform distribution (e.g., 1000 instances for Discriminatory Activities vs. 100 for Political Persuasion); reflects scenario complexity
  - Text vs. multimodal balance: 83% text, 17% visual/audio; text is primary but multimodal extends coverage
  - Static vs. adaptive evaluation: Dataset is fixed at release; paper acknowledges need for continuous expansion

- Failure signatures:
  - Low IAA scores indicate ambiguous guidelines or insufficient worker training
  - High variance in model scores (CV > 15%) suggests evaluation framework instability
  - Consistent "Safely Blocked" rates > 60% without evaluation data indicates model policy is too restrictive OR dataset contains high-severity prompts
  - Systematically low scores on specific prompt types (e.g., Rail) reveals model weaknesses or prompt design issues

- First 3 experiments:
  1. Baseline safety evaluation on your target Korean LLM using the Text EvalTrack to establish a reference point against the four open-weight models reported.
  2. IAA analysis on a sample of your own annotation to verify triple-annotation reliability matches the reported stability.
  3. Prompt type ablation: test your model on each of the 8 prompt types independently to identify which interaction structures reveal safety vulnerabilities (the paper shows Rail and Chain-of-Thought as common weak points).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an evaluation framework be designed to dynamically adapt to new AI capabilities and risks, overcoming the obsolescence limitations of static benchmark datasets?
- Basis in paper: [explicit] The conclusion states the need to "develop an evaluation framework that dynamically evolves and adapts alongside AI model advancements, moving beyond the limitations of the current static dataset."
- Why unresolved: Current datasets like AssurAI are fixed snapshots that cannot automatically account for novel risks emerging from rapidly evolving generative models.
- What evidence would resolve it: A proposed evaluation system that autonomously identifies and validates novel risk categories in newer models without requiring manual dataset reconstruction.

### Open Question 2
- Question: To what extent does evaluating video safety using single-frame proxies underestimate or fail to detect risks related to temporal coherence and scene transitions?
- Basis in paper: [explicit] Section 6.4.3 notes that the video evaluation "does not fully account for risk factors associated with temporal coherence or scene transition" and suggests "temporal consistency (TC)–based rubric enhancement" as future work.
- Why unresolved: The pilot study utilized a static frame sampling method, leaving the safety implications of motion and narrative progression in videos unassessed.
- What evidence would resolve it: A comparative study scoring the same video outputs using a temporal rubric versus the single-frame method to quantify the divergence in risk detection rates.

### Open Question 3
- Question: Can the expert-crowdsourcing pipeline used for Korean socio-cultural risks be replicated for other languages while maintaining the same level of contextual validity and annotation consistency?
- Basis in paper: [inferred] Section 5.2 lists the "deeply customized" Korean context as a limitation, noting it is "difficult to apply directly to other cultural contexts," implying the transferability of the construction methodology is unproven.
- Why unresolved: While the lack of non-English benchmarks is a known gap, it is uncertain if this specific taxonomy adaptation process works universally.
- What evidence would resolve it: Successful construction of a dataset for a distinct culture (e.g., Japanese or Arabic) using the defined two-stage expert/crowdsourcing process with comparable Inter-Annotator Agreement (IAA) scores.

## Limitations

- Dataset access and reproducibility: The AssurAI dataset and complete annotation guidelines are not publicly available, limiting independent verification of the 11,480 instances and the exact 35 risk factor implementations.
- Judge model validation: The evaluation relies on GPT-4o-mini and an undefined "GPT-5-mini" as judge models, with the latter not existing publicly, raising questions about reproducibility.
- Cultural adaptation evidence: While the paper claims the taxonomy captures Korean socio-cultural risks, the adaptation process from universal frameworks to Korean-specific contexts is not empirically validated.

## Confidence

- **High Confidence**: The dataset construction methodology (expert seed + crowdsourcing + triple annotation) is clearly specified and aligns with established best practices for high-quality data annotation. The statistical validation (CV < 9%, IAA stability) supports the reliability of the evaluation framework.
- **Medium Confidence**: The claim that AssurAI fills a critical gap in Korean AI safety evaluation is supported by the literature review and the dataset's scale, but the absence of public access limits independent assessment. The adaptation of universal harm frameworks to Korean socio-cultural contexts is plausible but not empirically validated.
- **Low Confidence**: The use of "GPT-5-mini" as a judge model is likely an error or placeholder, undermining confidence in the multimodal evaluation results. The cultural specificity of the 35 risk factors is asserted but not demonstrated through comparative analysis with other cultural contexts.

## Next Checks

1. **Dataset Release and Annotation Validation**: Request public access to the AssurAI dataset and complete annotation guidelines. Perform an independent IAA analysis on a random sample of 100 instances to verify the reported stability (IAA > 0.7) and assess whether the cultural nuances are adequately captured.

2. **Judge Model Calibration**: Replicate the evaluation pipeline using only GPT-4o-mini for both text and multimodal tracks. Compare the safety scores against the reported results to assess the impact of the "GPT-5-mini" placeholder. If possible, conduct a human-in-the-loop validation on a subset of prompts to establish ground truth for safety judgments.

3. **Cross-Cultural Risk Factor Analysis**: Adapt the 35 risk factors to another non-Korean cultural context (e.g., Japanese or Vietnamese) using the same expert-led methodology. Construct a small pilot dataset (100 instances) and compare the distribution of risk factors and safety scores to AssurAI to test the generalizability of the framework.