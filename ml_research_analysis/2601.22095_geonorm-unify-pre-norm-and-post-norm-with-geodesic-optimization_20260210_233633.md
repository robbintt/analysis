---
ver: rpa2
title: 'GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization'
arxiv_id: '2601.22095'
source_url: https://arxiv.org/abs/2601.22095
tags:
- arxiv
- geonorm
- training
- pre-norm
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the open question of optimal normalization
  layer placement in Transformer architectures by proposing a unified framework that
  interprets both Pre-Norm and Post-Norm through manifold optimization. The authors
  introduce GeoNorm, a novel method that replaces standard normalization with geodesic
  updates on a spherical manifold, treating token embeddings as points on the sphere
  and attention/feed-forward outputs as update directions.
---

# GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization

## Quick Facts
- **arXiv ID**: 2601.22095
- **Source URL**: https://arxiv.org/abs/2601.22095
- **Reference count**: 35
- **Primary result**: GeoNorm consistently outperforms Pre-Norm and Post-Norm baselines across 125M, 350M, and 1.3B parameter models, achieving losses of 1.8792 on Arxiv (vs 1.9032 for Pre-Norm) and 3.4040 on Books3 (vs 3.4437 for Pre-Norm) at 125M parameters.

## Executive Summary
This paper addresses the open question of optimal normalization layer placement in Transformer architectures by proposing a unified framework that interprets both Pre-Norm and Post-Norm through manifold optimization. The authors introduce GeoNorm, a novel method that replaces standard normalization with geodesic updates on a spherical manifold, treating token embeddings as points on the sphere and attention/feed-forward outputs as update directions. The approach incorporates layer-wise update decay analogous to learning rate schedules. Comprehensive experiments across various model sizes, datasets, and training lengths demonstrate that GeoNorm consistently outperforms existing normalization methods, achieving losses of 1.8792 on Arxiv (vs 1.9032 for Pre-Norm) and 3.4040 on Books3 (vs 3.4437 for Pre-Norm) at 125M parameters.

## Method Summary
GeoNorm replaces standard normalization layers in Transformers with geodesic updates on a spherical manifold. It treats token embeddings as points on a sphere and attention/feed-forward outputs as update directions. The method projects update vectors to the tangent space, applies layer-wise harmonic decay to update magnitudes, and uses exponential mapping to move embeddings along geodesics. The implementation is integrated into standard Transformer architectures with negligible computational overhead and shows superior performance across validation steps and downstream tasks. The theoretical contribution unifies Pre-Norm and Post-Norm as special cases of geodesic optimization, providing insights into their relative stability properties.

## Key Results
- Consistent improvement over Pre-Norm and Post-Norm across 125M, 350M, and 1.3B parameter models
- Lower validation losses: 1.8792 on Arxiv (vs 1.9032 Pre-Norm) and 3.4040 on Books3 (vs 3.4437 Pre-Norm) at 125M parameters
- Stable training without loss spikes, maintaining smooth convergence across all tested configurations
- Harmonic decay schedule outperforms sqrt and linear variants, achieving the best overall performance

## Why This Works (Mechanism)

### Mechanism 1: Geodesic Exponential Mapping Replaces Projection
Standard normalization projects updated vectors back to the sphere by discarding radial components abruptly. GeoNorm instead uses the exponential map `exp_x(v) = cos(θ)x + R·sin(θ)·v/||v||` where `θ = ||v||/||x||`, moving intrinsically along the geodesic determined by the update direction. This maintains curvature-aware relationships between representations.

### Mechanism 2: Tangent Space Projection of Update Directions
GeoNorm projects attention and FFN outputs onto the tangent space of the sphere before exponential mapping. Given update direction `s`, it computes `v = s - (x^T s / ||x||²)·x` to remove the radial component, placing the update vector in the tangent space. This ensures updates follow the manifold geometry.

### Mechanism 3: Layer-wise Harmonic Decay Regularizes Update Magnitude
The method applies harmonic decay (`α/k`) to update step sizes across layers, stabilizing training analogous to learning rate schedules in optimization. This prevents gradient explosion and mimics the implicit decay present in Pre-Norm's accumulated residual structure.

## Foundational Learning

- **Riemannian Manifolds and Spherical Geometry**
  - Why needed here: GeoNorm operates on the assumption that token embeddings exist on a spherical manifold where distances are measured along curved surfaces (geodesics) rather than straight lines.
  - Quick check question: Can you explain why the shortest path between two points on a sphere is a great circle arc, not a straight line through the interior?

- **Exponential Maps and Tangent Spaces**
  - Why needed here: The core operation projects vectors to the tangent space and uses exponential mapping to "roll" them along the manifold surface.
  - Quick check question: Given a point on a unit sphere and a tangent vector, how does the exponential map produce a new point on the sphere?

- **Residual Connections and Normalization Placement**
  - Why needed here: Understanding Pre-Norm vs Post-Norm tradeoffs (stability vs gradient balance) contextualizes why GeoNorm's unification matters.
  - Quick check question: Why does Pre-Norm tend to produce larger gradients in lower layers compared to higher layers?

## Architecture Onboarding

- **Component map**: Transformer layers -> Attention/FFN outputs -> GeoNorm module -> Exponential mapping on sphere

- **Critical path**: 
  1. Attention output → GeoNorm with layer index k
  2. FFN output → GeoNorm with layer index k
  3. Each GeoNorm call: gradient = g - (x·g)·x / ||x||² → theta = clamp(||gradient|| / ||x||, max=π/4) → output = x·cos(θ) + unit_tangent·||x||·sin(θ)

- **Design tradeoffs**:
  - Clamp value: Lower (π/8) = more conservative/smaller updates; higher (π/2) = more aggressive but risk instability
  - Decay method: Harmonic best in experiments but linear may suit shorter models; sqrt is middle ground
  - Computational overhead: Paper claims negligible—only basic vector operations, no learned parameters beyond optional scale/bias

- **Failure signatures**:
  - NaN outputs: Check for near-zero `||x||` (add epsilon clamping as in Appendix D)
  - Loss spikes at specific layers: May indicate clamp threshold too high or decay too weak for deep models
  - No improvement over baseline: Verify layer_number indexing starts from 0 correctly

- **First 3 experiments**:
  1. **Ablation on clamp values**: Test π/8, π/4, π/2 on 125M model with Books3 at length 512; expect π/4 optimal per Table 3
  2. **Decay schedule comparison**: Run harmonic vs sqrt vs linear on Arxiv; expect harmonic to achieve ~1.88 loss vs ~1.90 for alternatives per Figure 4
  3. **Scale test**: Validate 125M → 350M → 1.3B scaling shows consistent advantage over Pre-Norm; if gap narrows, investigate depth-dependent decay tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GeoNorm maintain its stability advantages when scaling to ultra-large models (e.g., >7B parameters) where training instabilities are typically more severe?
- Basis in paper: The experiments are limited to 125M, 350M, and 1.3B parameter models (Section 4), leaving the behavior at scales common in frontier LLMs untested.
- Why unresolved: The geodesic update dynamics might interact unpredictably with the extreme gradient variance found in models with tens of billions of parameters.
- What evidence would resolve it: Benchmark comparisons against Pre-Norm and DeepNorm on standard 7B or 70B configurations.

### Open Question 2
- Question: Is the empirically superior "harmonic" decay schedule universally optimal, or does the theoretically justified "sqrt" schedule offer better convergence for different depths?
- Basis in paper: Section 3.2 proposes the "sqrt" schedule based on stochastic Riemannian optimization theory, but Section 4.4 empirically finds "harmonic" decay performs best on 125M models.
- Why unresolved: The paper does not reconcile the theoretical justification for one schedule with the empirical success of another, leaving the optimal selection criteria undefined.
- What evidence would resolve it: Ablation studies on deeper models (e.g., 100+ layers) analyzing the asymptotic convergence of different decay methods.

### Open Question 3
- Question: Can the geodesic update mechanism be effectively applied to non-decoder architectures, such as bidirectional Transformers or Vision Transformers?
- Basis in paper: The introduction broadly mentions computer vision and Transformers, but Section 4 restricts all experiments to "decoder-only Transformers."
- Why unresolved: GeoNorm relies on viewing layers as iterative optimization steps; it is unclear if this interpretation holds or benefits bidirectional attention mechanisms where token representations mix differently.
- What evidence would resolve it: Evaluation of GeoNorm on encoder-only tasks (e.g., GLUE) or Vision Transformer (ViT) image classification benchmarks.

## Limitations

- **Scale Validation Gap**: The results show consistent improvements across 125M, 350M, and 1.3B parameter models, but don't extend to the multi-billion parameter regime where normalization choices become critical.
- **Manifold Hypothesis**: The core assumption that token embeddings lie on a spherical manifold suitable for geodesic optimization remains largely theoretical without empirical validation.
- **Implementation Dependency**: The method requires careful indexing of layer numbers and proper handling of near-zero norms, with results potentially sensitive to these implementation details.

## Confidence

- **High Confidence (95%+)**: The experimental methodology is sound, with proper ablation studies on clamp values and decay schedules. The consistent improvement across multiple datasets and model sizes provides strong empirical support.
- **Medium Confidence (70-90%)**: The theoretical unification of Pre-Norm and Post-Norm as special cases of geodesic optimization is mathematically elegant but relies on assumptions about manifold structure that aren't fully validated.
- **Low Confidence (50-70%)**: The claim that geodesic updates specifically provide geometric benefits over other normalization approaches is the weakest link, as experiments show GeoNorm works well but don't definitively prove the manifold optimization interpretation is the causal mechanism.

## Next Checks

1. **Manifold Validation Experiment**: Train a 125M model with GeoNorm while monitoring the actual embedding distribution on the sphere (using spherical coordinates or angular metrics). Measure whether embeddings concentrate near the manifold surface and whether this concentration correlates with performance improvements.

2. **Ablation on Decay Schedule**: Systematically test different decay schedules (harmonic, sqrt, linear, constant) on a 350M model with Books3 at length 2048. Compare not just final loss but training stability curves to isolate whether decay schedule or geodesic updates drive the improvements.

3. **Cross-Architecture Scaling Test**: Implement GeoNorm in a ViT-style vision transformer with 36+ layers and test on CIFAR-10 or similar. This would validate whether the harmonic decay schedule generalizes beyond the 12-24 layer regime and test the method's robustness to different architectural depths.