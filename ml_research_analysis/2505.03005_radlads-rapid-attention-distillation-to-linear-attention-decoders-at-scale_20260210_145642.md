---
ver: rpa2
title: 'RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale'
arxiv_id: '2505.03005'
source_url: https://arxiv.org/abs/2505.03005
tags:
- attention
- teacher
- step
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RADLADS, a method for rapidly converting
  large transformer models into efficient linear attention models using only 350-700M
  tokens of distillation data, compared to the trillions of tokens used in original
  training. The authors present two new RWKV-based architectures (RAD-RWKV6 and RAD-RWKV7)
  specifically designed for effective distillation, and demonstrate their approach
  by converting Qwen2.5 models (7B, 32B, and 72B parameters) to linear attention variants.
---

# RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale

## Quick Facts
- **arXiv ID:** 2505.03005
- **Source URL:** https://arxiv.org/abs/2505.03005
- **Reference count:** 29
- **Key outcome:** Converts Qwen2.5 models (7B, 32B, 72B) to linear attention variants with state-of-the-art performance, achieving 101.5% relative score on MMLU with 2-3x faster inference

## Executive Summary
RADLADS introduces a method for rapidly converting large transformer models into efficient linear attention models using only 350-700M tokens of distillation data. The approach uses progressive alignment before distillation, training student models to match teacher attention states before full logit distillation. This enables conversion of Qwen2.5 models to RAD-RWKV architectures with state-of-the-art performance among linear attention models, achieving 101.5% relative score on MMLU for the 72B variant while offering 2-3x faster inference speeds.

## Method Summary
RADLADS uses a three-step progressive distillation approach: (1) align student attention hidden states to teacher attention via L2 loss, (2) distill teacher logits to student logits via KL divergence, and (3) optionally extend context with cross-entropy. The method introduces two new RWKV-based architectures (RAD-RWKV6 and RAD-RWKV7) specifically designed for effective distillation. Key innovations include pre-scaling state balancing to prevent training instabilities at scale, selective learning rates to preserve knowledge in MLP layers, and architecture-specific modifications to enable efficient knowledge transfer from Qwen2.5 teachers.

## Key Results
- Converts Qwen2.5-7B-Instruct to RAD-RWKV achieving 87%+ MMLU relative score
- Converts Qwen2.5-72B-Instruct to RAD-RWKV achieving 101.5% MMLU relative score
- Demonstrates 2-3x faster inference at longer output sequences due to constant memory usage
- Achieves state-of-the-art performance among linear attention models

## Why This Works (Mechanism)

### Mechanism 1: Progressive Alignment Before Distillation
Aligning attention hidden states before full logit distillation enables rapid conversion with minimal data. Step 1 trains each student sequence mixer to approximate teacher attention layer outputs via L2 loss, creating a warm-start where the new attention mechanism already produces compatible representations before knowledge distillation begins. Core assumption: Student-teacher hidden state alignment transfers more efficiently than training from random initialization because MLP layers encode most factual knowledge. Evidence: Skipping step 1 resulted in significantly lower performance with loss plateauing at higher minimum.

### Mechanism 2: Architecture-Specific State Balancing for Training Stability
Replacing normalization layers with pre-scaling state balancing enables stable distillation at large scale (14B+). RAD-RWKV uses `k = k*(1-w)` pre-scaling instead of GroupNorm/LayerNorm post-hoc normalization. This bounds key magnitudes during accumulation, preventing gradient explosion in deep recurrent models without requiring denominator normalization. Core assumption: Unbounded state growth causes instability at scale, and explicit pre-scaling provides sufficient regularization without the overhead of learned normalization. Evidence: GroupNorm began to cause serious training instabilities at 14B+ models.

### Mechanism 3: Selective Learning Rate Schedules to Preserve Knowledge
Using different learning rates for attention vs. MLP/embedding layers prevents catastrophic forgetting during distillation. Steps 2-3 apply low/fixed LR (1e-5) to MLPs while allowing higher LR (1e-3→1e-5) for attention alignment, letting the recurrent mixer adapt without degrading learned representations in frozen knowledge stores. Core assumption: MLP and embedding weights encode transferable knowledge that should be preserved, while attention weights require more adaptation to the new recurrence pattern. Evidence: Separate LR_main and LR_att columns in Table 2 show differential rates, with freezing model weights resulting in significantly reduced model performance.

## Foundational Learning

- **Concept: Linear vs. Quadratic Attention Complexity**
  - **Why needed here:** Understanding why RADLADS targets linear attention requires knowing that softmax attention is O(N) memory/time per token while linear attention (RWKV) is O(1) with fixed state.
  - **Quick check question:** Can you explain why a KV cache grows with sequence length but an RWKV state does not?

- **Concept: Knowledge Distillation (Logit Matching)**
  - **Why needed here:** Step 2 uses KL divergence between teacher and student logits; understanding this as distribution alignment vs. hard label training clarifies why it preserves more teacher behavior.
  - **Quick check question:** How does minimizing KL(student_logits || teacher_logits) differ from cross-entropy with ground-truth labels?

- **Concept: Recurrent State Compression**
  - **Why needed here:** RWKV variants maintain a compressed matrix state `wkv_t` that accumulates history via `wkv_t = diag(w_t)·wkv_{t-1} + k_t^T·v_t`, trading off perfect recall for constant memory.
  - **Quick check question:** What information cannot be perfectly recovered from a compressed recurrent state after many timesteps?

## Architecture Onboarding

- **Component map:**
  Teacher → Student weight transfer (Q, K, V, O projections where mappable; LoRA-style low-rank components like tokenshift initialized neutral) → Step 1 (frozen teacher + parallel student attention-replacement layers, L2 loss on hidden states, 100M tokens, seq_len=512) → Step 2 (full teacher logit distillation via KL divergence, 250-700M tokens, seq_len=512 or 4096 for 2a variant) → Step 3 (optional context extension with cross-entropy only, 100M tokens, seq_len=16384) → RAD-RWKV6 (Gated Linear Attention kernel + tokenshift + full-rank sigmoid gate) or RAD-RWKV7 (no tokenshift, RoPE preserved, reduced-rank gating, simplified decay)

- **Critical path:**
  1. Verify teacher model has untied embeddings (weight tying caused issues; workaround available but adds complexity)
  2. Choose RAD-RWKV variant: v6 for tokenshift benefits, v7 for speed/simplicity
  3. Run Step 1 to stable loss (monitor L2 convergence ~100M tokens)
  4. Run Step 2 or 2a (2a with longer context can skip Step 3)
  5. Evaluate on standard benchmarks (MMLU, LAMBADA, ARC) and long-context tasks (RULER, passkey)

- **Design tradeoffs:**
  - RoPE inclusion: Preserved in RAD-RWKV7 because "not enough distillation steps for model to learn to replace them fully" but adds compute; removable with longer training
  - GQA retention: Kept from Qwen teacher "provides a good starting point" even if suboptimal long-term; assumption: GQA structure guides initial alignment
  - Batch size vs. optimizer steps: "Number of optimizer steps appears to be of key importance" — larger batches don't help because low LR must be maintained to protect MLPs

- **Failure signatures:**
  - GroupNorm at scale: Training instability at 14B+ parameters (unbounded state growth before normalization)
  - LoRA training: "rank reduction was generally quite detrimental" except on embeddings
  - De-novo initialization: Attention weights from scratch yield "surprisingly reasonable" but consistently worse performance than teacher transfer
  - Repetitive looping during reasoning: Observed with pure DCLM; mitigated by 90/10 DCLM/OpenThoughts mix

- **First 3 experiments:**
  1. **Small-scale conversion test:** Convert Qwen2.5-7B-Instruct → RAD-RWKV6 using exact hyperparameters (Table 2) and verify MMLU relative score approaches 87%+ (Table 4). This validates the pipeline before scaling.
  2. **Ablate Step 1:** Run conversion skipping Step 1 (direct to Step 2) and compare final MMLU/LAMBADA scores. Expect plateau at higher loss per Section 8.
  3. **Dataset sensitivity:** Convert same model using DCLM vs. Fineweb-edu and measure both benchmark performance and training stability. Section 3 notes "choice of dataset may depend upon teacher model's pre-training data distribution."

## Open Questions the Paper Calls Out
None

## Limitations
- Architecture specificity: Approach shows strong results specifically for Qwen2.5 teacher models converted to RAD-RWKV architectures; unclear if progressive alignment strategy generalizes to other transformer architectures
- Dataset dependency: Limited systematic analysis of how different distillation datasets affect final performance; sensitivity to dataset quality and distribution remains underexplored
- Long-context generalization: Evaluation focuses on standard benchmarks and limited long-context tasks; 2-3x speedup claim needs more comprehensive validation across diverse generation tasks

## Confidence
**High Confidence:** The progressive alignment mechanism (Step 1) demonstrably improves distillation efficiency compared to direct logit matching, as evidenced by controlled ablation showing higher plateau losses when Step 1 is skipped. The architectural modifications (state balancing via k*(1-w) pre-scaling) successfully resolve training instabilities at scale.

**Medium Confidence:** The selective learning rate scheduling effectively prevents catastrophic forgetting of MLP knowledge. While the authors provide empirical evidence through Table 2 and ablation studies, the underlying assumption about MLP layers storing most factual knowledge could vary across architectures.

**Low Confidence:** The claim that RADLADS achieves "state-of-the-art" performance among linear attention models is based on comparison to a limited set of existing linear attention approaches. The field is rapidly evolving, and more comprehensive benchmarking against emerging linear attention architectures would strengthen this claim.

## Next Checks
1. **Architecture Transferability Test:** Apply RADLADS to convert non-Qwen transformer architectures (e.g., Llama, Mistral) to linear attention variants. Measure whether Step 1 still provides significant efficiency gains and whether state balancing modifications remain effective. This would validate the broader applicability of the approach.

2. **Dataset Sensitivity Analysis:** Systematically vary the distillation dataset (different domains, sizes, and quality levels) while keeping all other hyperparameters constant. Quantify the impact on final performance and training stability to establish guidelines for dataset selection based on teacher model characteristics.

3. **Long-Context Generation Benchmark:** Conduct comprehensive evaluation of generation quality and speed across diverse tasks (code generation, story continuation, reasoning chains) at sequence lengths from 4K to 32K tokens. Compare against both quadratic attention baselines and other linear attention approaches to validate the 2-3x speedup claim under realistic usage patterns.