---
ver: rpa2
title: Counterfactual Explanations on Robust Perceptual Geodesics
arxiv_id: '2601.18678'
source_url: https://arxiv.org/abs/2601.18678
tags:
- robust
- metric
- counterfactual
- latent
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of generating meaningful counterfactual
  explanations in high-dimensional vision domains. Previous latent-space optimization
  methods often produce off-manifold artifacts or adversarial examples due to poor
  distance metrics and lack of geometric constraints.
---

# Counterfactual Explanations on Robust Perceptual Geodesics

## Quick Facts
- **arXiv ID:** 2601.18678
- **Source URL:** https://arxiv.org/abs/2601.18678
- **Reference count:** 40
- **Primary result:** Perceptual counterfactuals generated along Riemannian geodesics in robust latent spaces significantly improve semantic plausibility over baseline methods (R-FID: ~9 vs ~50, R-LPIPS: ~0.17 vs ~0.67).

## Executive Summary
This paper introduces Perceptual Counterfactual Geodesics (PCG), a method for generating counterfactual explanations in vision tasks that are both perceptually plausible and semantically valid. Unlike previous latent-space optimization methods that produce adversarial artifacts or off-manifold transitions, PCG leverages a Riemannian metric induced from robust visual features to ensure geodesic optimization respects human perception. By optimizing counterfactuals along smooth trajectories in a latent space with robust geometry, PCG avoids brittle directions and produces explanations that align with real data manifolds. Experimental results across three vision datasets show significant improvements in robust perceptual metrics, demonstrating the effectiveness of this geometrically constrained approach.

## Method Summary
PCG optimizes counterfactuals by constructing smooth geodesic paths in a latent space equipped with a Riemannian metric derived from robust vision features. The method operates in two phases: Phase 1 optimizes intermediate path points to minimize robust perceptual energy while endpoints remain fixed, ensuring manifold smoothness; Phase 2 releases the endpoint and jointly optimizes path plus classification loss, with periodic re-anchoring to the nearest valid target-class point. The robust metric is obtained via pullback from adversarially trained models, which suppress non-robust features and align with human perception. Path discretization balances smoothness against computational cost, with experiments showing T=10 as optimal. This framework prevents adversarial shortcuts and maintains semantic coherence throughout the transition.

## Key Results
- **R-FID improvement:** Reduces from ~50 (baseline) to ~9, indicating much better fidelity to real data distribution
- **R-LPIPS improvement:** Drops from ~0.67 to ~0.17, showing significantly smoother perceptual transitions
- **Baseline comparison:** Outperforms all existing methods including single-point geometry-aware attacks and latent diffusion approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If the latent space geometry is induced by robust visual features, optimization paths penalize brittle, adversarial directions and favor semantic transitions.
- **Mechanism:** The method constructs a Riemannian metric $G_Z$ by pulling back a composite metric from a robust model's intermediate layers (e.g., adversarially trained ResNet). Since robust models suppress non-robust features, the resulting geodesics must traverse directions that align with human perception, effectively filtering out adversarial shortcuts.
- **Core assumption:** Adversarially trained models possess feature spaces where Euclidean distance correlates with semantic similarity, a property absent in standard models.
- **Evidence anchors:**
  - [abstract] "...Riemannian metric induced from robust vision features. This geometry... penalizes brittle directions..."
  - [Section 3] "...robust models learn representations that are resistant to adversarial perturbations... As a result, the Euclidean metric becomes a more reliable proxy..."
  - [corpus] Evidence is limited in direct neighbors; "Unifying Image Counterfactuals..." discusses the link between latent attacks and AEs, but relies on the target paper for the specific robust geometry claim.

### Mechanism 2
- **Claim:** Optimizing a trajectory (sequence of points) rather than a single endpoint prevents "off-manifold" traversal caused by local gradient updates.
- **Mechanism:** PCG discretizes the path into $T+1$ points and minimizes the "robust perceptual energy" (path length) globally. This enforces geodesicity (smoothness) across the entire trajectory, preventing the optimizer from jumping off the manifold to minimize classification loss greedily.
- **Core assumption:** The generator's latent space is continuous and the semantic transition between classes can be approximated by a smooth manifold.
- **Evidence anchors:**
  - [abstract] "...optimizes counterfactuals along geodesic paths..."
  - [Section 1] "Without global structural guidance, single-point geometry-aware gradient methods... overlook the global manifold structure."
  - [corpus] Not explicitly contrasted with path methods in neighbors; neighbors focus on diffusion or single-point attacks.

### Mechanism 3
- **Claim:** Re-anchoring the path endpoint during optimization ensures the counterfactual is semantically proximal to the original input.
- **Mechanism:** Phase 2 optimizes the path to satisfy the target class while periodically scanning the trajectory for valid target points. It selects the one closest to the input (in robust metric space) as the new endpoint, contracting the path.
- **Core assumption:** The initial path (from input to a random target) intersects the target class region sufficiently to allow "pulling back" to a valid counterfactual.
- **Evidence anchors:**
  - [Section 3] "...periodically apply a re-anchoring step: at fixed intervals, we scan along the current path for points that are already classified as the target class..."
  - [Appendix A.1] "Re-anchor $z_T$ to the closest point along the path classified as $y'$."

## Foundational Learning

### Concept: Pullback Metric
- **Why needed here:** To understand how distances in the "robust feature space" (ambient) define the geometry of the "latent space" (Z) via the generator's Jacobian.
- **Quick check question:** How does the rank of the generator's Jacobian $J_g$ affect the validity of the pullback metric?

### Concept: Adversarial Robustness (Feature Alignment)
- **Why needed here:** To grasp why robust features are used as the metric source—they are hypothesized to align with human perception, unlike standard features which contain "brittle" non-robust signals.
- **Quick check question:** Why does a standard $\ell_2$ pixel metric fail to distinguish between a meaningful change and an adversarial perturbation?

### Concept: Geodesic Optimization
- **Why needed here:** The method minimizes path energy, not just endpoint distance. This requires understanding calculus of variations or discrete path optimization.
- **Quick check question:** In a discrete setting, why is minimizing the sum of squared velocities ($\Delta z$) different from minimizing the distance between start and end points?

## Architecture Onboarding

### Component map:
Encoder -> Generator (StyleGAN2/3) -> Classifier (VGG-19) <- Robust Backbone (ResNet-50)

### Critical path:
1. **Init:** Encode input $x_0 \to z_0$ and sample target $x_T \to z_T$. Interpolate $T$ intermediate points.
2. **Phase 1:** Optimize intermediate points to minimize Robust Energy (smooth the path).
3. **Phase 2:** Optimize path + Endpoint to minimize (Energy + $\lambda \cdot$ Classification Loss).
4. **Re-anchoring:** Periodically truncate path at the earliest valid target-class point.

### Design tradeoffs:
- **Path Length ($T$):** Higher $T$ ensures smoother geodesics but increases compute/memory linearly. Paper suggests $T \approx 10$ is a sweet spot.
- **Robustness Radius ($\epsilon$):** The robust backbone determines the metric; choosing one with a different $\epsilon$ or architecture (ResNet vs XCiT) changes the "perception" of smoothness.

### Failure signatures:
- **Semantic Drift:** If Phase 1 is skipped or $\lambda$ is too high, the path may leave the manifold, resulting in artifacts or identity loss (off-manifold AE).
- **Convergence Failure:** If the robust backbone is misaligned with the data domain, the energy term may dominate, preventing the path from reaching the target class.

### First 3 experiments:
1. **Interpolation Sanity Check:** Visualize latent interpolation ($z_0$ to $z_T$) using (a) Euclidean metric, (b) Standard feature metric, (c) Robust feature metric. Expect robust metric to show the smoothest transition.
2. **Metric Sensitivity:** Generate counterfactuals using the same setup but swapping the "Robust Backbone" for a standard (non-robust) one. Expect the standard one to produce adversarial-looking artifacts.
3. **Path Length Ablation:** Run PCG with $T=\{3, 5, 10, 20\}$. Measure perceptual smoothness ($\Delta$LPIPS) vs. runtime to confirm the $T=10$ heuristic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different adversarial training regimes—such as varying norms ($L_\infty$ vs. $L_2$), radii, or certified robustness—reshape the induced latent geometry and the resulting behavior of counterfactual geodesics?
- Basis in paper: [explicit] Appendix C states, "it would be interesting to study how different robustness norms, radii, and training regimes... reshape the induced geometry and the behavior of geodesics."
- Why unresolved: The current study exclusively employs $L_2$-robust backbones (ResNet-50, XCiT) trained with a specific $\epsilon$, leaving the impact of other robustness definitions on the perceptual metric unknown.
- What evidence would resolve it: An ablation study applying PCG with backbones trained under varied threat models, followed by an analysis of changes in geodesic smoothness and semantic validity.

### Open Question 2
- Question: Can the PCG framework be adapted to Latent Diffusion Models (LDMs), given the necessity of defining geometry within time-dependent stochastic score dynamics?
- Basis in paper: [explicit] Appendix C notes that transferring the framework to diffusion architectures requires deciding "where to place a geometry... and how a pullback metric should interact with the score field."
- Why unresolved: PCG currently relies on StyleGAN's single, static latent space, whereas LDMs involve multiple interacting latent spaces and stochastic trajectories which complicate geodesic definition.
- What evidence would resolve it: A formal extension of the pullback metric construction to diffusion latent spaces and empirical results showing stable counterfactual generation under stochastic sampling.

### Open Question 3
- Question: Can lightweight robust surrogates or self-supervised proxies replace full-scale ImageNet backbones to make PCG viable in low-resource domains where standard robust training is impractical?
- Basis in paper: [explicit] Appendix C highlights that "In low-resource regimes, training full-scale robust backbones may be impractical, suggesting the need for lightweight robust surrogates."
- Why unresolved: The method currently depends on expensive, large-scale pre-trained robust models to induce the perceptual geometry, limiting accessibility for niche datasets.
- What evidence would resolve it: Successful implementation of PCG using smaller or self-supervised models to induce the metric, achieving comparable semantic alignment without reliance on large-scale supervised adversarial training.

## Limitations

- **Perceptual validation gap:** The paper lacks direct human perceptual studies to validate that robust metrics truly align with human judgment of semantic similarity.
- **Generalizability uncertainty:** The method's dependence on large-scale pre-trained robust models raises questions about performance in low-resource or domain-shifted settings.
- **Theoretical grounding:** While the re-anchoring mechanism is intuitively effective, its impact on geodesic optimality and convergence is not rigorously justified.

## Confidence

- **High Confidence:** The technical feasibility of the two-phase optimization framework and the observed quantitative improvements in robust metrics (R-FID, R-LPIPS) are well-supported by the experimental setup and results.
- **Medium Confidence:** The claim that robust features inherently encode perceptual similarity is plausible given existing literature but not definitively proven within this work.
- **Low Confidence:** The assertion that PCG reveals "failure modes hidden under standard metrics" is suggestive but not conclusively demonstrated; the analysis is correlational rather than causal.

## Next Checks

1. **Human Perceptual Study:** Conduct a user study comparing PCG counterfactuals against baseline methods using Likert-scale ratings for naturalness and semantic plausibility.
2. **Robust Backbone Ablation:** Systematically replace the adversarially trained ResNet with (a) a standard ResNet, (b) a different robust architecture (e.g., XCiT), and (c) a non-robust but pretrained model (e.g., CLIP) to quantify the impact on counterfactual quality.
3. **Manifold Alignment Analysis:** Visualize the latent trajectories (e.g., t-SNE of intermediate points) to confirm that PCG paths stay closer to the data manifold compared to baseline methods, and measure alignment using the proposed Manifold Alignment Score across multiple runs.