---
ver: rpa2
title: '(Sometimes) Less is More: Mitigating the Complexity of Rule-based Representation
  for Interpretable Classification'
arxiv_id: '2509.22384'
source_url: https://arxiv.org/abs/2509.22384
tags:
- 'null'
- latexit
- sha1
- base64
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpretability in deep
  neural networks by proposing a method to reduce the complexity of rule-based models
  derived from Multi-layer Logical Perceptrons (MLLP). The authors adapt L0 regularization
  to MLLP, aiming to sparsify the network and align the continuous model with its
  discrete, interpretable counterpart, the Concept Rule Set (CRS).
---

# (Sometimes) Less is More: Mitigating the Complexity of Rule-based Representation for Interpretable Classification

## Quick Facts
- **arXiv ID:** 2509.22384
- **Source URL:** https://arxiv.org/abs/2509.22384
- **Reference count:** 31
- **Primary result:** L0 regularization significantly reduces rule complexity (e.g., 8× on Connect-4) while maintaining or improving F1 score in interpretable CRS models derived from MLLP.

## Executive Summary
This paper tackles the interpretability challenge in deep learning by reducing the complexity of rule-based models extracted from Multi-layer Logical Perceptrons (MLLP). The authors adapt L0 regularization to MLLP, promoting sparsity in the network and aligning the continuous model with its discrete, interpretable counterpart, the Concept Rule Set (CRS). Their approach is compared to Random Binarization (RB), a baseline method. Experiments on UCI datasets show that L0 regularization can significantly reduce the number of literals (rules) in CRS models while maintaining or even improving classification performance, especially in deeper architectures. The results suggest that promoting sparsity can enhance interpretability without sacrificing accuracy, challenging the common belief that accuracy must be traded for interpretability.

## Method Summary
The paper proposes reducing the complexity of rule-based models extracted from Multi-layer Logical Perceptrons (MLLP) by adapting L0 regularization. MLLP uses logical operators (conjunction, disjunction) in place of standard neural network activations, making it suitable for interpretable classification. The L0 regularization is applied via hard concrete gates to sparsify the network. This sparsified MLLP is then combined with Random Binarization (RB) to generate Concept Rule Sets (CRS), which are discrete, human-readable rule sets. The method is evaluated on UCI datasets, with the primary focus on the connect-4 dataset (67,557 instances, 3 classes, 42→126 binary features). The loss function combines MSE, L0 regularization, and L2 regularization (Eq. 17), and hyperparameters are specified for training. The goal is to minimize rule complexity (total literals in CRS) while maintaining or improving macro F1 score via 5-fold cross-validation.

## Key Results
- L0 regularization can reduce CRS complexity by up to 8× (Connect-4 largest model) while maintaining or improving F1 score.
- Sparsity improvements are more pronounced in deeper MLLP architectures.
- The L0+RB method achieves a better Pareto frontier of F1 vs. complexity compared to RB alone, challenging the accuracy-interpretability trade-off.

## Why This Works (Mechanism)
The method works by encouraging sparsity in the MLLP through L0 regularization. By promoting sparse weights, the network learns to rely on fewer, more essential features, which translates to simpler rule sets (CRS) upon extraction. The hard concrete gates allow for differentiable approximation of L0, enabling gradient-based optimization. Random Binarization (RB) then converts the sparse continuous weights into discrete rules, with the L0 regularization ensuring that the resulting CRS is also sparse. This combination directly targets the complexity of the interpretable model without necessarily sacrificing accuracy.

## Foundational Learning
- **MLLP (Multi-layer Logical Perceptron):** A neural network using logical operators (conjunction, disjunction) instead of standard activations. *Why needed:* Enables extraction of interpretable rule sets (CRS). *Quick check:* Verify conjunction/disjunction activations (Eqs. 4–6) and Clip(0,1).
- **Concept Rule Set (CRS):** A discrete, human-readable rule set extracted from MLLP. *Why needed:* Provides the interpretable model. *Quick check:* Confirm CRS extraction via Bin(w,0.5) matches Eqs. 7–8.
- **L0 Regularization:** Promotes sparsity by penalizing the number of non-zero weights. *Why needed:* Reduces rule complexity in CRS. *Quick check:* Track active weights during L0 training.
- **Hard Concrete Distribution:** A continuous relaxation of discrete random variables used to approximate L0. *Why needed:* Enables gradient-based optimization of L0. *Quick check:* Set init/β/ζ/γ per Louizos et al.
- **Random Binarization (RB):** A baseline method for extracting CRS by randomly binarizing weights. *Why needed:* Provides a comparison point for L0. *Quick check:* Confirm mask M resampled per epoch.

## Architecture Onboarding
- **Component map:** Input features → MLLP layers (conjunction/disjunction) → L0 gates → Sparse MLLP → CRS extraction (Bin) → Output rules
- **Critical path:** Training MLLP with L0 regularization → Extracting CRS via Random Binarization → Evaluating F1 and complexity
- **Design tradeoffs:** L0 regularization vs. accuracy; depth of MLLP vs. sparsity; RB randomness vs. stability
- **Failure signatures:** 
  - High F1 but complex CRS: L0 regularization too weak (increase λ)
  - Low F1: Over-sparsification or poor binarization; reduce λ or check thresholds
  - High variance across runs: Stochasticity in RB or L0 gates; increase seeds or stabilize initialization
- **First experiments:**
  1. Clone repo; download/connect-4; verify binarization (42→126 features). Confirm conjunction/disjunction activations (Eqs. 4–6) and Clip(0,1).
  2. Train baseline MLLP (no L0) across architectures (64/128/256 and 64×3/128×3/256×3) and P∈{0,0.5,0.9}. Extract CRS via Bin(w,0.5); report F1 vs literals.
  3. Add group L0 gates (hard concrete); set init/β/ζ/γ per Louizos et al. Train with combined L0+RB; track active weights, F1, literals. Compare sparsity curves and Pareto frontier (F1 vs literals).

## Open Questions the Paper Calls Out
- **Open Question 1:** How can explicit logical constraints or background knowledge be integrated into the MLLP framework to enforce properties like safety and fairness? *Basis:* Authors plan to introduce logical constraints in future work. *Why unresolved:* Current implementation lacks mechanisms for domain-specific requirements. *What evidence would resolve it:* Modified loss or architecture enforcing logical constraints while maintaining performance.
- **Open Question 2:** Can the method be adapted to guarantee the stability of the generated rule sets across different training runs? *Basis:* Method does not offer stability guarantees. *Why unresolved:* Stochasticity of training and hard concrete distribution lead to variability. *What evidence would resolve it:* Study showing low variance in rule structure across seeds, potentially via aggregation.
- **Open Question 3:** What is the optimal strategy for tuning the regularization parameter λ to balance complexity reduction against potential performance degradation? *Basis:* Instances of over-sparsification suggest need for better λ tuning. *Why unresolved:* Paper treats λ as fixed (0.001), but trade-off is dataset-dependent. *What evidence would resolve it:* Adaptive scheduling or heuristic identifying Pareto frontier of complexity vs. accuracy.

## Limitations
- Exact binarization scheme for connect-4 (42→126 features) is unspecified, affecting reproducibility.
- Key L0 hyperparameters (ζ, γ in hard concrete gates, dropout rate per layer) are not explicitly provided.
- Optimizer choice (SGD vs Adam) is not specified, impacting convergence and sparsity.
- Class-wise metrics are not provided, limiting interpretability per class.

## Confidence
- **High confidence:** Core methodology (MLLP + L0 regularization + CRS extraction) is clearly described and reproducible with provided code.
- **Medium confidence:** Sparsity and performance improvements are well-supported, but exact hyperparameter replication may require tuning.
- **Low confidence:** Claim of improving interpretability without accuracy trade-off is plausible but dataset-specific; generalization uncertain.

## Next Checks
1. **Verify binarization consistency:** Confirm connect-4 preprocessing matches 42→126 feature mapping and all UCI datasets are correctly binarized before training.
2. **Replicate sparsity vs performance trade-off:** Train L0-regularized models across λ values and compare F1/literal curves to ensure reported reductions (e.g., 8× on Connect-4) are reproducible.
3. **Test alternative optimizers:** Run ablations with SGD and Adam to assess sensitivity of sparsity and convergence to optimizer choice.