---
ver: rpa2
title: 'Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe'
arxiv_id: '2509.16411'
source_url: https://arxiv.org/abs/2509.16411
tags:
- retrieval
- pairs
- query
- embeddings
- pretrain-finetune
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning dual encoder (DE)
  models for hierarchical retrieval (HR), where documents form a hierarchy and queries
  need to retrieve all ancestors. The authors prove that DEs are theoretically feasible
  for HR with embedding dimension scaling linearly in hierarchy depth and logarithmically
  in document count.
---

# Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe

## Quick Facts
- arXiv ID: 2509.16411
- Source URL: https://arxiv.org/abs/2509.16411
- Reference count: 26
- Dual encoders theoretically feasible for hierarchical retrieval with linear embedding dimension scaling

## Executive Summary
This paper tackles hierarchical retrieval (HR), where documents form a hierarchy and queries need to retrieve all ancestors. The authors prove that dual encoder (DE) models are theoretically feasible for HR, with embedding dimension scaling linearly in hierarchy depth and logarithmically in document count. However, they discover a "lost-in-the-long-distance" phenomenon where learned DEs struggle to retrieve documents further from queries in the hierarchy. To address this, they propose a pretrain-finetune recipe: first pretrain on regular data, then finetune on long-distance pairs. Experiments on WordNet show this improves long-distance recall from 19% to 76%, while maintaining performance on short distances. On the ESCI shopping dataset, the method outperforms joint training for retrieving both exact and substitute products.

## Method Summary
The authors develop a pretrain-finetune recipe to address the "lost-in-the-long-distance" phenomenon in hierarchical retrieval. The approach involves first pretraining the dual encoder model on regular retrieval data to learn basic embedding representations, then finetuning specifically on long-distance hierarchical pairs that are difficult to retrieve. This two-stage process allows the model to first establish good general representations before focusing on the challenging long-range relationships in the hierarchy. The method is evaluated on WordNet for controlled experiments and ESCI shopping dataset for real-world application, showing substantial improvements in long-distance recall without sacrificing performance on short-distance pairs.

## Key Results
- Long-distance recall improved from 19% to 76% on WordNet dataset
- Pretrain-finetune approach outperforms joint training on ESCI shopping dataset
- Maintains performance on short-distance pairs while improving long-distance retrieval
- Embedding dimension scales linearly with hierarchy depth and logarithmically with document count

## Why This Works (Mechanism)
The pretrain-finetune recipe works by first establishing good general representations through pretraining on regular data, then fine-tuning on the challenging long-distance hierarchical pairs. This approach addresses the optimization difficulty where the model gets stuck in local minima when trying to learn both short and long-distance relationships simultaneously. By separating the learning process, the model can first learn basic semantic relationships before focusing on the more complex hierarchical dependencies. The pretraining stage provides a good initialization that makes the fine-tuning stage more effective at capturing the long-range hierarchical relationships that are otherwise difficult to learn.

## Foundational Learning
- **Hierarchical retrieval**: Understanding that documents form tree-like structures where queries need to retrieve ancestor nodes - needed to grasp the core problem being solved
- **Dual encoder architecture**: Two separate neural networks that map queries and documents to a shared embedding space - needed to understand the model structure
- **Embedding dimension scaling**: How the required embedding space size relates to hierarchy depth and document count - needed to understand theoretical feasibility
- **Long-distance retrieval challenge**: Difficulty in learning relationships between nodes far apart in the hierarchy - needed to understand the key problem addressed
- **Contrastive learning**: Training approach that pulls positive pairs together and pushes negative pairs apart in embedding space - needed to understand pretraining methodology
- **Fine-tuning optimization**: Process of adapting pretrained models to specific downstream tasks - needed to understand the two-stage learning approach

## Architecture Onboarding

**Component Map**
Input -> Dual Encoder (Query Encoder + Document Encoder) -> Embedding Space -> Similarity Scoring -> Retrieval Output

**Critical Path**
1. Input query and document pairs enter respective encoders
2. Encoders map inputs to shared embedding space
3. Similarity scores computed between query and document embeddings
4. Top-k documents retrieved based on similarity scores
5. During pretraining: standard retrieval pairs
6. During fine-tuning: long-distance hierarchical pairs

**Design Tradeoffs**
- Single-stage joint training vs. two-stage pretrain-finetune: Two-stage approach better handles long-distance pairs but requires more training time
- Embedding dimension size: Larger dimensions can capture more complex relationships but increase computational cost
- Pretraining data selection: Quality and diversity of pretraining data affects fine-tuning effectiveness
- Fine-tuning duration: Longer fine-tuning may improve long-distance recall but risks overfitting

**Failure Signatures**
- Degraded performance on short-distance pairs (indicates over-specialization to long-distance)
- Slow convergence during fine-tuning (indicates poor pretraining initialization)
- High variance in long-distance recall across runs (indicates optimization instability)
- Worsening performance on validation set during fine-tuning (indicates overfitting)

**First 3 Experiments**
1. Baseline joint training on WordNet hierarchy with varying embedding dimensions
2. Pretrain-finetune pipeline with different pretraining durations
3. Ablation study comparing different fine-tuning objectives (contrastive vs. supervised)

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical feasibility proof assumes idealized conditions that don't fully account for the empirical "lost-in-the-long-distance" phenomenon
- Experimental scope limited to specific domains (WordNet, ESCI shopping) with relatively clean hierarchical structures
- Generalizability to complex, real-world hierarchies with noisy relationships remains untested
- Gap between theoretical geometry and practical optimization dynamics not fully bridged

## Confidence
- Theoretical framework: Medium - sound but idealized
- Empirical improvements: Medium - substantial but limited dataset diversity
- Practical applicability: Medium - effective on tested datasets but real-world robustness unknown
- Pretrain-finetune approach: Medium - appears effective but may not be optimal

## Next Checks
1. **Cross-domain robustness testing**: Apply the pretrain-finetune recipe to diverse hierarchical datasets including organizational structures, biological taxonomies, and file system hierarchies to assess generalization across different relationship types and depth distributions.

2. **Scaling analysis**: Systematically evaluate how the pretrain-finetune benefits scale with hierarchy depth, branching factor, and document count to determine whether the theoretical linear and logarithmic relationships hold empirically across varied configurations.

3. **Ablation on pretraining objectives**: Test whether specific pretraining objectives (e.g., contrastive learning vs. generative pretraining) differentially impact the ability to learn long-distance hierarchical relationships during fine-tuning.