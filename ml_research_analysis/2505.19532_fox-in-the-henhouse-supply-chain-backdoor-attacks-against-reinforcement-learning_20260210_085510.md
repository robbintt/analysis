---
ver: rpa2
title: 'Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning'
arxiv_id: '2505.19532'
source_url: https://arxiv.org/abs/2505.19532
tags:
- backdoor
- trigger
- victim
- action
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new type of supply-chain backdoor attack
  on reinforcement learning, where an attacker trains a malicious external agent that
  later influences the victim agent's behavior during training, without requiring
  access to the victim's observations, rewards, or policy parameters. The attack works
  by rewarding the victim for executing specific backdoor actions after recognizing
  a trigger pattern in the attacker's behavior, leading the victim to learn a compromised
  policy.
---

# Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.19532
- Source URL: https://arxiv.org/abs/2505.19532
- Reference count: 40
- One-line primary result: Achieves >90% trigger success rate with up to 80% performance degradation using only 3% trigger injection during training

## Executive Summary
This paper introduces a novel supply-chain backdoor attack on reinforcement learning where an attacker trains a malicious external agent that later influences the victim agent's behavior during training. The attack works by rewarding the victim for executing specific backdoor actions after recognizing trigger patterns in the attacker's behavior, without requiring access to the victim's observations, rewards, or policy parameters. Experimental results demonstrate high effectiveness across different RL algorithms and architectures, highlighting significant supply-chain security risks in RL applications.

## Method Summary
The attack involves training an attacker agent with three components: a standard opponent policy (π_att^std), a rewarding policy (π_att^rwd) designed to lose efficiently, and a detector that infers victim actions from observations. During victim training, the attacker follows a finite-state machine that triggers backdoor activation sequences when the detector successfully identifies victim actions. The backdoor is embedded through environmental outcomes rather than direct reward manipulation, making it stealthy against standard training-time metrics. The attack is evaluated across three Atari games using both CNN and LSTM victim architectures.

## Key Results
- Achieves over 90% trigger success rate while reducing victim performance by up to 80% in Pong, Boxing, and Surround
- Only 3% trigger injection during training needed to achieve high attack effectiveness
- LSTM victim architectures show higher susceptibility (91.3% trigger success) than CNN architectures (35.0% trigger success) due to better temporal dependency capture
- Training-time bulk metrics remain indistinguishable between clean and attacked training, ensuring stealthiness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An attacker can embed backdoors using only legitimate action sequences without accessing victim's observations, rewards, or parameters
- Mechanism: The attacker uses π_att^std for normal play and π_att^rwd to deliberately lose, creating high rewards for the victim. When the victim executes backdoor actions after observing triggers, the attacker switches to π_att^rwd, creating implicit reward signals that reinforce backdoor behavior through Q-learning updates
- Core assumption: Q-function estimation can be manipulated through environmental outcomes
- Evidence anchors: [abstract] "our attack only relies on legitimate interactions"; [section 3.2] Equation (4) shows victim reward maximization; [corpus] Related work confirms reward manipulation is common

### Mechanism 2
- Claim: The backdoor embedding is stealthy because training metrics remain indistinguishable from clean training
- Mechanism: Low trigger injection probability (3%) and conditional rewarding only when victim executes backdoor actions ensure aggregate performance metrics appear normal while backdoor is gradually encoded
- Core assumption: Security audits rely on aggregate training statistics
- Evidence anchors: [section 4, Table 11] Overlapping confidence intervals between clean and attacked training; [section 4] "spontaneous occurrence of backdoor actions is exceedingly rare"

### Mechanism 3
- Claim: Models with better temporal dependency capture (LSTM vs CNN) are more susceptible to the attack
- Mechanism: The backdoor reward is delayed—victim must execute trigger response actions before receiving implicit reward from π_att^rwd. LSTM architectures better associate trigger observation with delayed reward
- Core assumption: Backdoor requires learning temporal association between trigger observation and subsequent reward
- Evidence anchors: [section 4, Table 3] LSTM achieves higher trigger success rates than CNN; [section 4] "models with better capabilities to capture long-term dependencies are more susceptible"

## Foundational Learning

- Concept: **Action-Value Function (Q-function)**
  - Why needed here: SCAB manipulates the victim's Q-function estimation through environmental outcomes rather than direct reward modification
  - Quick check question: If an agent receives unexpectedly high rewards after executing action sequence B following trigger T, how does Q-learning update the value of actions in sequence B?

- Concept: **Markov Games / Multi-Agent MDPs**
  - Why needed here: The attack frames victim-attacker interaction as a multi-agent system where the attacker's policy influences the victim's learning through environmental dynamics
  - Quick check question: In a two-player Markov game, how does one agent's action affect the other agent's transition function and reward?

- Concept: **Finite-State Machine Control Flow**
  - Why needed here: The attacker's behavior is governed by a four-state FSM (Winning → Triggering → Observing → Rewarding) that determines when to inject triggers and deliver rewards
  - Quick check question: What conditions cause transitions from Observing state to Rewarding vs. back to Winning?

## Architecture Onboarding

- Component map:
  π_att^std (tournament-trained opponent) -> π_att^rwd (adversarially-trained rewarding policy) -> Detector d (CNN-based action inference) -> FSM controller (trigger injection management)

- Critical path:
  1. Pre-train π_att^std via tournament self-play (10 policies, select highest performer)
  2. Train π_att^rwd via adversarial optimization (Equation 2) against 10 opponent proxies
  3. Train detector d on 100K observation-action pairs (99.5%+ accuracy)
  4. Deploy attacker during victim training with FSM controller
  5. Train trigger network π_trg for test-time exploitation

- Design tradeoffs:
  - Higher TIP → faster backdoor embedding but reduced stealthiness
  - Longer trigger sequences → more recognizable but attacker loses more timesteps executing them
  - Longer backdoor sequences → more damaging but harder for victim to learn

- Failure signatures:
  - Trigger success rate <30% with CNN architectures → consider reducing sequence complexity
  - Detector accuracy <90% → increase observation stack size k or training data
  - Victim performance doesn't degrade at test time → verify π_att^rwd is effectively losing

- First 3 experiments:
  1. Replicate Pong CNN-PPO baseline with 3% TIP, measure trigger success rate and episodic return degradation
  2. Ablate TIP values (1%, 3%, 5%) to establish stealthiness-effectiveness frontier
  3. Compare LSTM vs CNN victim architectures to validate temporal dependency hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more intricate or dynamic backdoor patterns improve attack effectiveness?
- Basis in paper: [explicit] "Further performance improvements... may also be possible by exploring more intricate or dynamic backdoor patterns, rather than our evaluated static patterns"
- Why unresolved: Only evaluated static, fixed-length sequences of consecutive actions
- What evidence would resolve it: Empirical comparison using dynamic or context-dependent trigger/backdoor patterns

### Open Question 2
- Question: How can the attacker's requirements for cumulative reward tracking and behavior-switching be removed or concealed at the neural network architecture level?
- Basis in paper: [explicit] "The attacker currently needs to be packaged in such a fashion that it allows for the attacker to keep track of its cumulative reward and requires switching between different behaviors"
- Why unresolved: Current implementation requires external state-tracking mechanisms
- What evidence would resolve it: Demonstration using only standard neural network components without external state management

### Open Question 3
- Question: What is the optimal strategy for trigger injection timing during training?
- Basis in paper: [explicit] "Consequently, we consider the problem of constructing an effective and efficient strategy for injecting triggers during training time as an important future direction"
- Why unresolved: Uniform random injection achieved high success, but RL-based injection failed
- What evidence would resolve it: A trigger injection policy achieving higher test-time success rates while preserving training-time stealthiness

### Open Question 4
- Question: Which existing defense methods can effectively mitigate supply-chain backdoor attacks under this threat model?
- Basis in paper: [explicit] "Future work should explore these defenses in the context of our supply-chain-based threat model"
- Why unresolved: Only evaluated fine-tuning defense, which showed limited effectiveness
- What evidence would resolve it: Systematic evaluation of defense mechanisms measuring both attack mitigation and preservation of victim policy performance

## Limitations

- The paper does not provide complete architectural specifications and hyperparameter choices for the detector and policy networks, making faithful reproduction challenging
- No evaluation of potential defense mechanisms or mitigation strategies against the proposed attack
- The attack's effectiveness and stealthiness may vary significantly depending on specific monitoring and detection methods employed by defenders

## Confidence

- **High Confidence**: The core mechanism of using legitimate action sequences to embed backdoors without accessing victim parameters is well-supported by theoretical framework and experimental results
- **Medium Confidence**: The claim that LSTM architectures are more susceptible than CNNs due to better temporal dependency capture is plausible but requires further validation
- **Low Confidence**: The assertion that the attack is universally stealthy across all security audit practices is uncertain and depends on specific monitoring methods

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary BRT and TIP values across the full range (1%-5% for TIP, multiple BRT thresholds) to map the complete stealthiness-effectiveness frontier
2. **Cross-Environment Robustness Test**: Evaluate the attack's effectiveness on additional Atari games and non-Atari environments (e.g., MuJoCo, PyBullet) to assess generalizability
3. **Defense Evasion Assessment**: Implement and test basic anomaly detection methods (action distribution monitoring, trajectory-level clustering) to determine if attack stealthiness holds against simple defenses