---
ver: rpa2
title: Spatio-temporal Multivariate Time Series Forecast with Chosen Variables
arxiv_id: '2510.24027'
source_url: https://arxiv.org/abs/2510.24027
tags:
- variables
- time
- forecast
- input
- locations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of spatio-temporal multivariate
  time series forecast (STMF) with chosen variables (STCV), which involves optimally
  selecting a subset of m variables from n available variables to maximize forecast
  accuracy while reducing model complexity and inference time. The core method, Variable-Parameter
  Iterative Pruning (VIP), jointly performs variable selection and model optimization
  through three key components: (1) masked variable-parameter pruning that progressively
  prunes less informative variables and attention parameters using quantile-based
  masking, (2) dynamic extrapolation that propagates information from chosen variables
  to missing ones via learnable spatial embeddings and adjacency information, and
  (3) prioritized variable-parameter replay that mitigates catastrophic forgetting
  by replaying low-loss past samples.'
---

# Spatio-temporal Multivariate Time Series Forecast with Chosen Variables

## Quick Facts
- arXiv ID: 2510.24027
- Source URL: https://arxiv.org/abs/2510.24027
- Reference count: 40
- Primary result: VIP achieves up to 30% improvement in RMSE compared to state-of-the-art baselines while reducing model size and inference time

## Executive Summary
This paper introduces VIP (Variable-Parameter Iterative Pruning), a method for jointly optimizing variable selection and model parameters in spatio-temporal multivariate time series forecasting with chosen variables (STCV). The framework progressively prunes less informative variables and attention parameters using quantile-based masking, dynamically extrapolates information from chosen variables to missing ones via learnable spatial embeddings, and mitigates catastrophic forgetting through prioritized replay of low-loss samples. Experiments on five real-world datasets demonstrate superior accuracy and efficiency compared to baselines, with the method maintaining performance even when only 10% of variables are selected.

## Method Summary
VIP addresses STCV by combining iterative pruning of variables and parameters with dynamic extrapolation and replay mechanisms. The method begins with a pre-trained STMF model and introduces learnable importance vectors for both variables and attention parameters. At each iteration, binary masks are generated through quantile-based thresholding to zero out the least important components. The dynamic extrapolation module propagates information from selected variables to all locations using attention over learnable node embeddings fused with the adjacency matrix. A prioritized replay buffer stores low-loss samples to prevent catastrophic forgetting during the pruning process, ensuring stable performance as the model becomes increasingly sparse.

## Key Results
- VIP achieves up to 30% improvement in RMSE compared to best baselines on PEMSBAY dataset
- Method maintains accuracy with only 10% of variables selected
- Significantly reduces model size and inference time while improving or maintaining forecast accuracy
- Ablation studies confirm importance of all three core components (pruning, extrapolation, replay)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative quantile-based pruning jointly optimizes variable selection and parameter efficiency for the STCV task.
- Mechanism: Learnable importance vectors for variables ($\hat{b}$) and attention parameters ($\hat{p}$) are updated via gradients. Quantile-based thresholds generate binary masks ($b_k$, $p_k$) to zero out lowest-ranked variables and parameter dimensions before forward pass.
- Core assumption: Learned importance scores reflect true contribution to forecast loss and relationship is stable enough for progressive elimination.
- Evidence anchors: Abstract mentions "masked variable-parameter pruning that progressively prunes less informative variables and attention parameters using quantile-based masking"; section III.B, Eq. 7-8 defines binary masks based on quantile thresholds; related works focus on full spatio-temporal dependencies rather than input selection.
- Break condition: If importance scores are uniform or noisy, quantile threshold may remove useful components prematurely, causing irreversible performance degradation.

### Mechanism 2
- Claim: Dynamic extrapolation enables accurate system-wide forecasting from sparse subset of input variables.
- Mechanism: Dense attention matrix $\hat{B}$ computed from similarity of learnable node embeddings, fused with sparse adjacency matrix of selected variables to propagate feature information from chosen subset to all $n$ variables.
- Core assumption: Learnable node embeddings capture latent spatial correlations sufficient to infer state of unobserved variables from observed ones, and static adjacency provides reliable structural guidance.
- Evidence anchors: Abstract mentions "dynamic extrapolation that propagates information from chosen variables to missing ones via learnable spatial embeddings and adjacency information"; section III.B, Eq. 13-15 describes computation of similarity-based attention, its fusion with adjacency, and final feature propagation.
- Break condition: If node embeddings are uninformative or static graph structure is poor proxy for dynamic spatial correlations, error propagates from selected subset to entire system.

### Mechanism 3
- Claim: Prioritized replay of low-loss samples mitigates catastrophic forgetting during iterative pruning process.
- Mechanism: Replay buffer stores past training samples with current loss; samples with lower loss assigned higher priority ($P_S = 1/L_{main}$). Sample drawn probabilistically from buffer based on priority and interleaved with current batch.
- Core assumption: Low-loss samples represent stable, core patterns of data distribution that should be preserved, while high-loss samples represent outliers or difficult cases less critical for generalization during pruning.
- Evidence anchors: Abstract mentions "prioritized variable-parameter replay that mitigates catastrophic forgetting by replaying low-loss past samples"; section III.B, Table IV shows ablation where VIP performance degrades when replay component removed; consistent with prioritized experience replay concept in reinforcement learning.
- Break condition: Strategy counterproductive if buffer too small (loses diversity) or model overfits to set of "easy" buffered samples, failing to adapt to new data distributions.

## Foundational Learning

- **Graph Neural Networks (GNNs) & Message Passing**
  - Why needed here: Architecture relies on graph structure (adjacency matrix) and node embeddings to propagate information across spatial variables.
  - Quick check question: How does a basic Graph Convolutional layer update a node's representation using its neighbors' features?

- **Transformer Attention Mechanisms**
  - Why needed here: Base model uses temporal and spatial attention layers, and pruning mechanism operates directly on attention parameters.
  - Quick check question: In standard self-attention layer, how are Query (Q), Key (K), and Value (V) matrices used to compute output?

- **Catastrophic Forgetting & Continual Learning**
  - Why needed here: Model's parameters dynamically altered by pruning, which can cause it to "forget" previously learned patterns; replay buffer is solution.
  - Quick check question: What is catastrophic forgetting in context of neural network training, and how does experience replay theoretically address it?

## Architecture Onboarding

- **Component map**: Input Layer -> Embedding Block -> Masked Attention Backbone -> Dynamic Extrapolation Head -> Output Layer -> Training Stabilizers
- **Critical path**: Path from Masked Attention Backbone outputs to Dynamic Extrapolation Head is critical. Pruning backbone reduces information quality available to extrapolation head, which must work harder to reconstruct full system state.
- **Design tradeoffs**:
  - Pruning Rate ($r_b, r_p$) vs. Accuracy: Higher rates yield greater efficiency but increase burden on extrapolation mechanism and may remove critical information
  - Buffer Size ($|R|$): Larger buffer improves stability but increases memory usage and may dilute priority signal if not tuned with $\alpha$
  - Embedding Dimension ($d_v$): Larger embeddings provide more capacity for extrapolation but reduce efficiency gains from pruning
- **Failure signatures**:
  - Accuracy Collapse: Sudden sharp increase in validation loss suggests overly aggressive pruning rates
  - Replay Stagnation: Training loss plateaus while replay loss remains high, indicating buffer full of unhelpful samples or priority misconfigured
  - Extrapolation Drift: Forecasts for unselected variables diverge slowly over time, suggesting node embeddings not learning generalizable spatial correlations
- **First 3 experiments**:
  1. Zero-Pruning Baseline: Run VIP with $r_b=0, r_p=0$ to establish performance of base STMF model and verify codebase reproduces known results
  2. Component Ablation: Systematically disable (1) Dynamic Extrapolation, (2) Prioritized Replay, and (3) Parameter Pruning to quantify contribution of each mechanism on PEMS08 dataset
  3. Hyperparameter Sensitivity: Run grid search over pruning rates ($r_b \in \{0.05, 0.1, 0.2\}$) and replay buffer priorities ($\alpha \in \{0.2, 0.4, 0.6\}$) to map efficiency-accuracy trade-off curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does framework perform when training data contains significant noise or missing values?
- Basis in paper: [inferred] Problem definition assumes "training data contains time series of all variables... as ground truth" collected by temporary sensors
- Why unresolved: Paper evaluates performance on clean datasets, but temporary sensors used for initial data collection might be less reliable than permanent ones, potentially biasing variable selection
- What evidence would resolve it: Experiments evaluating selected subset's quality when trained on data with synthetic missingness or high variance

### Open Question 2
- Question: Can selection objective be modified to optimize for non-uniform deployment costs?
- Basis in paper: [inferred] Problem defined as selecting subset of fixed size $m$ (cardinality constraint), implicitly assuming equal installation costs for all locations
- Why unresolved: Real-world budget constraints are monetary rather than count-based; some locations (e.g., complex intersections) are significantly more expensive to instrument than others
- What evidence would resolve it: Variation of pruning mechanism incorporating cost-weighted loss function, comparing forecast accuracy against total monetary budget rather than sensor count

### Open Question 3
- Question: Is learned variable selection robust to structural changes in spatial graph?
- Basis in paper: [inferred] Dynamic extrapolation mechanism relies on static adjacency matrix $A$ and fixed node embeddings to propagate information
- Why unresolved: In real-world scenarios, road networks change (e.g., closures, new construction), which may alter "connectivity" importance of selected variables over time
- What evidence would resolve it: Stress testing selected variable set on test data where adjacency matrix $A$ is modified to simulate structural disruptions

## Limitations

- Performance improvement varies considerably across datasets, suggesting high dataset dependency
- Claim of maintaining accuracy with 10% variable selection based on single dataset (PEMSBAY) needs broader verification
- Effectiveness of extrapolation mechanism heavily dependent on quality of learnable node embeddings, which lacks thorough independent validation
- Limited analysis of how performance degrades when pruning rates pushed to limits or replay buffer becomes insufficient

## Confidence

- **High**: Base STMF model architecture and general concept of iterative pruning are well-established and reproducible
- **Medium**: Specific VIP algorithm (quantiles, extrapolation, replay) is clearly specified but robustness to hyperparameter settings and dataset characteristics requires more extensive validation
- **Low**: Claim of maintaining accuracy with only 10% of variables selected based on results from single dataset and needs broader verification

## Next Checks

1. **Boundary Test**: Systematically increase pruning rates beyond those reported in Table III to identify point where accuracy collapse occurs, documenting relationship between $r_b$, $r_p$, and validation loss
2. **Embedding Quality**: Implement diagnostic to measure quality of learnable node embeddings independently (e.g., testing ability to reconstruct missing variables in non-pruning setting)
3. **Dataset Transferability**: Validate 10% variable selection claim on at least two additional datasets (e.g., METR-LA and AQI) to assess generalizability