---
ver: rpa2
title: 'VoxPrivacy: A Benchmark for Evaluating Interactional Privacy of Speech Language
  Models'
arxiv_id: '2601.19956'
source_url: https://arxiv.org/abs/2601.19956
tags:
- privacy
- secret
- user
- tier
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Speech Language Models (SLMs) are moving from personal devices
  to shared environments, requiring them to distinguish between users to protect information
  flow. Current SLM benchmarks test dialogue skills but overlook speaker identity
  and privacy-aware responses.
---

# VoxPrivacy: A Benchmark for Evaluating Interactional Privacy of Speech Language Models

## Quick Facts
- arXiv ID: 2601.19956
- Source URL: https://arxiv.org/abs/2601.19956
- Reference count: 40
- Primary result: Most open-source speech language models perform near random chance on conditional privacy decisions, even after fine-tuning on 4,000 hours of privacy-focused dialogues.

## Executive Summary
Speech Language Models are increasingly deployed in shared environments where they must distinguish between users to protect information flow. Current benchmarks test dialogue skills but overlook speaker identity and privacy-aware responses. We introduce VoxPrivacy, the first benchmark designed to evaluate interactional privacy in multi-speaker spoken dialogues across three tiers: following direct secrecy commands, speaker-verified secrecy, and proactive privacy protection. Our evaluation of nine SLMs on a 32-hour bilingual dataset reveals that most open-source models perform close to random chance on conditional privacy decisions, while even strong closed-source systems fall short on proactive privacy inference. By fine-tuning on a new 4,000-hour training set, we improve privacy-preserving capabilities while maintaining robustness.

## Method Summary
VoxPrivacy evaluates interactional privacy across three tiers using a 7,107 utterance dataset (32.86h, English/Chinese balanced). Tier 1 tests direct command following, Tier 2 requires speaker-verified secrecy using voice as biometric key, and Tier 3 demands proactive privacy protection through autonomous inference of sensitive content. We fine-tune Kimi-Audio (Whisper-large-v3 encoder + adaptor) on a 4,000-hour training set using mixed-task training (~30% general tasks + privacy data) with AdamW optimizer (lr=1e-5). Evaluation uses LLM-as-judge (Deepseek-V3, Gemini-2.5-Pro) with 3-run majority voting, validated against human evaluation on 800 samples. Real-VoxPrivacy, a human-recorded subset of 586 utterances from 18 speakers, validates synthetic-to-real transfer.

## Key Results
- Open-source SLMs perform near random chance (~50%) on Tier 2/3 conditional privacy decisions despite achieving >80% on Tier 1
- Even strong closed-source systems like GLM4Voice (74.4% speaker verification accuracy) fail to integrate biometric awareness with privacy rules
- Fine-tuning on 4,000 hours of mixed-task data improves privacy performance while maintaining general capabilities, avoiding catastrophic forgetting
- Proactive privacy protection (Tier 3) shows the largest performance gap, indicating a fundamental reasoning barrier beyond instruction-following

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on a mixed-task training set improves privacy-preserving capabilities while mitigating catastrophic forgetting. The authors mix 4,000 hours of privacy-focused dialogues with ~30% general-purpose data (ASR, SER, ASC, AQA, Voice-Chat), enabling the model to acquire new privacy behaviors without degrading existing competencies. Core assumption: The 30% ratio empirically balances skill acquisition against interference; this is claimed effective but not theoretically proven. Evidence anchors: [abstract] shows improvement while maintaining robustness; [section 5.4, Table 7] demonstrates catastrophic degradation in privacy-only training versus mixed-task preservation; [corpus] notes this mechanism is novel. Break condition: If the downstream task distribution differs substantially from the mixed-task composition, the balance ratio may need re-tuning.

### Mechanism 2
Speaker verification capability is necessary but not sufficient for conditional privacy disclosure. The model must accurately distinguish speakers acoustically and integrate this biometric awareness with semantic privacy rules to conditionally grant or deny access. Core assumption: SV accuracy below ~60% creates a fundamental ceiling on Tier 2 performance; above this threshold, failures stem from integration rather than perception. Evidence anchors: [section A.6, Table 13] shows GLM4Voice achieves 74.4% SV accuracy but performs near-random on Tier 2 (~50%); [section 4.5, Table 3] links open-source models with ~50% SV accuracy to random-chance Tier 2 performance; [corpus] notes related benchmarks don't evaluate privacy conditioned on speaker identity. Break condition: If speaker embeddings become adversarially spoofable or intra-speaker variability exceeds training diversity, SV-based gating will fail regardless of integration quality.

### Mechanism 3
Proactive privacy protection requires common-sense reasoning to infer sensitivity without explicit instructions, exposing a fundamental cognitive gap. The model must recognize that content like medical results is inherently private, then enforce speaker-conditioned access autonomously—a shift from instruction-following to social judgment. Core assumption: This inference capability depends on world knowledge embedded in the LLM backbone, not acoustic processing. Evidence anchors: [section 4.5, Table 3] shows all models degrade from Tier 2 to Tier 3; the LLM Upper Bound (text-only with explicit speaker tags) also drops, confirming the barrier is reasoning, not speech processing; [section 5.2, Table 5a] shows models handle non-sensitive multi-speaker dialogues well (~90%+ accuracy), isolating the failure to privacy-specific context handling; [corpus] notes related benchmarks don't test privacy inference or speaker-aware response adaptation. Break condition: If privacy norms vary across cultures or contexts, the model's common-sense inference may not generalize without explicit normative training.

## Foundational Learning

**Interactional Privacy**: Distinguishes this benchmark from global privacy benchmarks (e.g., refusing to share passwords). Interactional privacy governs contextually sensitive information whose disclosure depends on *who* is asking—a user's appointment is private from others but not from themselves. Quick check question: Given a secret shared by Speaker A, should the model disclose to Speaker B who asks about it later? If you said "no" without considering whether B = A, you're missing the interactional dimension.

**Speaker Continuity Bias**: Open-source SLMs make disproportionately more errors when speakers switch mid-dialogue, suggesting training data skews toward single-speaker interactions. Quick check question: If a model correctly answers "What did I just say?" when the same speaker continues, but fails when a new speaker asks, which bias is at play?

**Catastrophic Forgetting**: Fine-tuning on specialized tasks can degrade pre-trained capabilities. The authors demonstrate this via ablation (Table 7), showing why mixed-task training is essential. Quick check question: Your fine-tuned model achieves 85% on privacy tasks but WER doubles on LibriSpeech. What went wrong, and how would you diagnose it?

## Architecture Onboarding

**Component map**: Audio encoder (Whisper-large-v3) -> Adaptor module -> LLM backbone -> Speaker verification subsystem (implicit) -> Fine-tuning pipeline

**Critical path**: User A speaks → Audio encoder processes → LLM generates response acknowledging secret → Privacy instruction stored in context → User B queries → Audio encoder processes → LLM integrates speaker identity with privacy rule → Response generated (disclose if authorized, withhold if third-party)

**Design tradeoffs**: Synthetic vs. real audio: Synthetic (CosyVoice2 TTS) enables scale and ethical safety but may lack paralinguistic nuance; Real-VoxPrivacy (18 speakers, 586 utterances) validates transfer but is too small for training. LLM-as-judge vs. human evaluation: LLM judge (Deepseek-V3, Gemini-2.5-Pro) scales evaluation; human validation (κ=0.92 on privacy compliance) confirms reliability. 2-round vs. 3-round dialogues: Training set includes both to prepare for varied conversational flows.

**Failure signatures**: Random-chance Tier 2/3 (~50% accuracy) indicates model cannot connect voice to privacy rules; check SV capability (Table 13). High Tier 1 but low Tier 2/3 indicates instruction-following works but speaker-conditioned reasoning fails; likely integration gap. English >> Chinese performance attributed to higher ASR WER (1.5–2×) and English-centric LLM backbone; consider multilingual augmentation. Spoofing attack success causes largest performance drops (~6% accuracy loss)—models fail to distinguish similar voices, exposing biometric vulnerability.

**First 3 experiments**: 1. Baseline SV capability test: Evaluate your model's speaker verification accuracy using balanced same/different speaker pairs (replicate Table 13 protocol). If <60%, address acoustic discrimination before privacy tasks. 2. Tier-by-tier ablation: Test on VoxPrivacy's three tiers separately. A sharp Tier 2→Tier 3 drop indicates reasoning gaps; flat random-chance across tiers indicates fundamental speaker awareness failure. 3. Mixed-task fine-tuning with ablation: Train with and without general-task mixing (replicate Table 7 conditions). Compare privacy performance against general benchmark degradation to validate the 30% ratio for your architecture.

## Open Questions the Paper Calls Out

**Open Question 1**: How can reinforcement learning improve nuanced privacy decision-making beyond what supervised fine-tuning achieves for interactional privacy in SLMs? Basis in paper: Section 7 states supervised fine-tuning is an initial step; future work will explore alternatives like reinforcement learning to better capture nuanced decision-making. Why unresolved: The paper demonstrates that supervised fine-tuning improves performance, but Tier 3 still shows a significant gap even after fine-tuning, suggesting SFT may not fully capture the complex social reasoning required. What evidence would resolve it: A comparative study measuring Tier 3 performance when using RL-based training methods versus the current SFT approach.

**Open Question 2**: How do culture-specific privacy definitions affect model performance on interactional privacy tasks across collectivist versus individualist cultural norms? Basis in paper: Section 7 notes privacy norms vary across cultures and future work should explore culture-specific privacy definitions. Why unresolved: The current benchmark uses uniform privacy categories across English and Chinese, but what constitutes sensitive information and appropriate disclosure norms may differ significantly across cultures. What evidence would resolve it: A cross-cultural annotation study showing how human privacy judgments vary, followed by model evaluation on culture-specific secret categories and disclosure scenarios.

**Open Question 3**: How can the "integration gap" between speaker verification capability and contextual privacy reasoning be bridged in SLMs? Basis in paper: Appendix A.6 identifies that models achieve speaker verification accuracy yet perform near random chance on Tier 2 privacy tasks, indicating the model can distinguish who is speaking but fails to integrate this acoustic awareness with the semantic instruction to enforce a privacy rule. Why unresolved: The paper reveals this disconnect but does not propose architectural or training solutions to ensure biometric recognition is functionally integrated with privacy-aware decision-making. What evidence would resolve it: Analysis of attention mechanisms or intermediate representations showing whether and how speaker identity information flows to privacy decision modules, plus interventions that explicitly couple these capabilities.

## Limitations

Synthetic data reliance: The entire benchmark and training set rely on TTS-generated speech, raising questions about ecological validity when deployed on real human speech. Privacy instruction parsing ambiguity: Models may succeed by detecting explicit keywords rather than truly understanding conditional privacy contexts. Limited language coverage: While English and Chinese are included, the benchmark doesn't address languages with different privacy norms or cultures where interactional privacy concepts may vary significantly.

## Confidence

High confidence in the fundamental observation that open-source SLMs perform near-random on conditional privacy decisions (Tier 2/3). Medium confidence in the claim that mixed-task training (30% general + privacy data) prevents catastrophic forgetting. Low confidence in the generalizability of proactive privacy inference (Tier 3) capabilities.

## Next Checks

1. **Real-world deployment validation**: Test the fine-tuned model on real human speech recordings in actual multi-user environments to measure domain transfer effectiveness and identify failure modes specific to natural speech variability.

2. **Adversarial robustness testing**: Systematically evaluate vulnerability to voice spoofing and mimicry attacks by testing with professionally generated voice clones and naturally similar speakers to quantify biometric security gaps and their impact on privacy protection.

3. **Cross-cultural privacy norms evaluation**: Extend the benchmark to include languages and cultures with different privacy expectations, testing whether the model's "common-sense" privacy inferences generalize or require culture-specific fine-tuning.