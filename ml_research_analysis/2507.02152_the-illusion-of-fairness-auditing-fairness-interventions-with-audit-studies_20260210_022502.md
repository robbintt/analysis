---
ver: rpa2
title: 'The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies'
arxiv_id: '2507.02152'
source_url: https://arxiv.org/abs/2507.02152
tags:
- data
- bias
- fairness
- audit
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how data from audit studies can be used
  to improve both the training and evaluation of automated hiring algorithms. The
  authors identify that traditional fairness interventions, such as equalizing base
  rates across protected classes, can create an illusion of fairness when evaluated
  on biased labels.
---

# The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies

## Quick Facts
- arXiv ID: 2507.02152
- Source URL: https://arxiv.org/abs/2507.02152
- Reference count: 15
- Authors: Disa Sariola; Patrick Button; Aron Culotta; Nicholas Mattei
- Primary result: Traditional fairness interventions can create illusion of fairness when evaluated on biased labels; ITE-based approach reduces algorithmic discrimination by up to 60% compared to traditional pre-processing

## Executive Summary
This paper investigates how audit study data can improve automated hiring algorithms by addressing label bias in fairness evaluations. The authors demonstrate that traditional fairness interventions, such as equalizing base rates across protected classes, can create an illusion of fairness when evaluated on biased labels from human decision-makers. Using a large-scale audit study dataset on age discrimination in hiring, they introduce an Individual Treatment Effect (ITE)-based approach to repair label bias in the data. The method iteratively flips labels most likely influenced by discrimination until callback rates are equal across age groups, showing significant improvements over traditional approaches in reducing algorithmic discrimination.

## Method Summary
The authors propose a novel approach to fairness intervention that leverages Individual Treatment Effect (ITE) estimation to identify and repair label bias in audit study data. The method involves collecting audit study data where pairs of resumes with identical qualifications except for protected attributes (like age) are sent to real employers, recording which receive callbacks. Using this data, they train a machine learning model to predict the ITE of discrimination - essentially learning which callbacks were likely influenced by protected attributes rather than qualifications. The ITE-based intervention then iteratively flips the labels of applications most likely to have received callbacks due to discrimination until callback rates are equalized across age groups. This repaired dataset is then used to train fairer algorithms, which are evaluated both on the original biased data and on new audit study data to measure actual fairness improvements.

## Key Results
- Traditional fairness interventions appear fair when evaluated using standard methods but exhibit roughly 10% disparity when measured appropriately on audit data
- The ITE-based intervention reduces algorithmic discrimination, achieving up to 60% reduction in disparity compared to traditional pre-processing approaches
- Equalizing callback rates across age groups through label flipping effectively removes observed discrimination patterns from the training data
- The method successfully identifies and corrects for label bias that would otherwise create an illusion of fairness in algorithmic evaluations

## Why This Works (Mechanism)
The mechanism works by directly addressing the root cause of biased labels in hiring data: discrimination by human decision-makers. Traditional fairness interventions assume that existing labels are accurate reflections of qualifications, but in hiring contexts, callbacks themselves may be influenced by protected attributes like age. By using audit study data to estimate the Individual Treatment Effect of discrimination, the method can identify which callbacks were likely influenced by age rather than qualifications. The iterative label flipping then creates a counterfactual training set where discrimination has been removed, allowing algorithms to learn fairer decision boundaries. This approach recognizes that fairness cannot be achieved by simply equalizing outcomes on biased data - the bias must be identified and removed from the labels themselves before training fair algorithms.

## Foundational Learning
- **Audit Studies**: Why needed - to obtain ground truth data on discrimination by controlling for all variables except protected attributes. Quick check - can we identify which callbacks were influenced by discrimination rather than qualifications?
- **Individual Treatment Effect (ITE)**: Why needed - to estimate the causal effect of protected attributes on hiring outcomes. Quick check - does the model accurately predict which applications received callbacks due to discrimination?
- **Label Bias in Fairness**: Why needed - to understand why traditional fairness interventions fail when evaluated on biased labels. Quick check - do traditional methods show fairness when evaluated on audit data?
- **Counterfactual Fairness**: Why needed - to conceptualize what fair hiring would look like in the absence of discrimination. Quick check - can we create a dataset that represents fair hiring decisions?
- **Iterative Label Flipping**: Why needed - to systematically remove discrimination from labels while preserving qualification information. Quick check - does equalizing callback rates across groups improve algorithmic fairness?

## Architecture Onboarding

Component map: Audit Data -> ITE Model -> Label Flipping -> Fair Algorithm -> Evaluation

Critical path: Audit Study Data → ITE Estimation → Label Repair → Algorithm Training → Fairness Evaluation

Design tradeoffs: The method trades computational complexity (iterative label flipping) for improved fairness accuracy. Alternative approaches might use direct causal inference methods or different fairness metrics, but the ITE-based approach provides a practical balance between computational feasibility and effectiveness.

Failure signatures: If the ITE model poorly estimates discrimination effects, label flipping will not effectively remove bias. If callback rates cannot be equalized through label flipping (suggesting structural discrimination beyond individual decisions), the method may fail. Poor audit study data quality or insufficient sample size can also lead to unreliable ITE estimates.

First experiments:
1. Train ITE model on audit data and evaluate its ability to predict discrimination on held-out data
2. Apply iterative label flipping to equalize callback rates and verify the resulting dataset shows reduced disparity
3. Train a hiring algorithm on both original and repaired datasets, comparing their fairness performance on new audit data

## Open Questions the Paper Calls Out
None

## Limitations
- The ITE-based intervention's effectiveness depends on assumptions about discrimination mechanisms that may not generalize across different contexts
- The method's success relies on the quality and representativeness of audit study data, which may be difficult to obtain for all protected attributes and hiring domains
- The approach assumes equalizing callback rates is the appropriate fairness metric, which may not capture all relevant fairness concerns in hiring contexts
- Quantitative results (10% disparity, 60% improvement) are based on a single age discrimination dataset, raising questions about external validity

## Confidence

High confidence: The core finding that traditional fairness interventions can create an illusion of fairness when evaluated on biased labels is well-supported and methodologically sound. The identification of this problem is robust and has implications beyond the specific dataset used.

Medium confidence: The ITE-based intervention approach shows promise but requires more validation across different domains and bias types before broader claims can be made about its effectiveness.

Low confidence: The generalizability of the specific quantitative results (10% disparity with traditional approaches, 60% improvement with ITE) to other hiring contexts and protected attributes.

## Next Checks
1. Apply the ITE-based intervention to audit studies covering different protected attributes (gender, race) and hiring domains (technical vs non-technical roles) to assess generalizability across bias types.

2. Conduct ablation studies to quantify how sensitive the ITE-based results are to the specific threshold used for label flipping and the assumptions about discrimination mechanisms.

3. Design and implement a controlled experiment where human recruiters make hiring decisions both with and without algorithmic assistance, measuring whether the ITE-repaired algorithm performs better at reducing disparate impact than traditional approaches.