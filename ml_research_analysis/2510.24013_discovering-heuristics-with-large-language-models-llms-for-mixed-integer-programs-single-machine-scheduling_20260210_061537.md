---
ver: rpa2
title: 'Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer
  Programs: Single-Machine Scheduling'
arxiv_id: '2510.24013'
source_url: https://arxiv.org/abs/2510.24013
tags:
- problem
- tardiness
- mddc
- instances
- heuristics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces two novel Large Language Model (LLM)-discovered
  heuristics, EDD Challenger (EDDC) and MDD Challenger (MDDC), for the single-machine
  total tardiness (SMTT) scheduling problem. The LLM-driven discovery process employs
  an iterative, island-based evolutionary pipeline to refine initial heuristic rules
  (EDD and MDD) into more advanced algorithms.
---

# Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling

## Quick Facts
- arXiv ID: 2510.24013
- Source URL: https://arxiv.org/abs/2510.24013
- Reference count: 7
- Primary result: MDDC heuristic outperforms traditional heuristics and remains competitive with exact methods on large SMTT instances

## Executive Summary
This study introduces two LLM-discovered heuristics, EDDC and MDDC, for the single-machine total tardiness (SMTT) scheduling problem. Using an island-based evolutionary framework with iterative refinement, the authors evolve initial EDD and MDD heuristics into more advanced algorithms. MDDC consistently outperforms traditional heuristics and achieves competitive performance with exact methods, particularly on larger problem instances. The work demonstrates that LLMs can effectively generate high-performing heuristics for NP-hard combinatorial optimization problems even when trained on small instances.

## Method Summary
The method employs an island-based evolutionary pipeline with Mixtral 8x7B to evolve heuristic programs. Ten isolated populations (islands) independently evolve heuristics using best-shot prompting, where two top programs are sampled and combined into prompts for LLM mutations. Islands reset every 4 hours by adopting the best program from surviving islands to maintain diversity. The specification includes an `evaluate` function with soft penalties for infeasible solutions, preventing LLM confabulation while preserving search flexibility. Training uses 10,000 instances with 25 jobs, testing extends to 500-job instances. The approach discovers MDDC and EDDC heuristics that outperform traditional methods with significant computational efficiency.

## Key Results
- MDDC consistently outperforms traditional heuristics (EDD, MDD, SPT) across all problem sizes
- MDDC remains competitive with exact methods on larger instances despite training on small 25-job problems
- Both MDDC and EDDC offer significant computational efficiency compared to exact methods
- EDDC improves upon classic EDD but is less effective than MDDC

## Why This Works (Mechanism)

### Mechanism 1: Island-Based Evolutionary Search with Program-Level Mutation
- Claim: Multiple isolated populations with periodic culling preserve diversity while allowing cumulative refinement
- Core assumption: Small syntactic modifications in heuristic programs yield incremental performance improvements
- Evidence: Abstract states "island-based evolutionary framework with iterative refinement"; Section 4.1 describes strategy supporting exploration and mitigating premature convergence
- Break condition: If all islands converge to similar programs before discovering performant heuristics, the mechanism fails

### Mechanism 2: Best-Shot Prompting for Cumulative Program Synthesis
- Claim: Sampling multiple high-performing programs together yields better LLM mutations than single-parent prompting
- Core assumption: LLMs can recognize and recombine useful algorithmic patterns across multiple exemplar programs
- Evidence: Section 4.1 describes "best-shot prompting technique" deriving new solution functions from database; Section 4.4 notes "beneficial small changes are retained and accumulated over time"
- Break condition: If sampled programs are syntactically incompatible or LLM produces code logically equivalent to parents, no progress occurs

### Mechanism 3: Specification-Driven Feasibility Enforcement with Soft Penalties
- Claim: Encoding constraints as evaluation functions with penalty scores prevents LLM confabulation without over-constraining search
- Core assumption: LLM-generated code frequently violates implicit constraints unless explicitly penalized
- Evidence: Section 4.2 describes designing specification to penalize incomplete or manipulated solutions with very high penalty scores
- Break condition: If penalty weights are miscalibrated, either invalid programs dominate (too low) or valid but suboptimal programs are excluded (too high)

## Foundational Learning

- Concept: **Single-Machine Total Tardiness (SMTT) Scheduling**
  - Why needed: Understanding tardiness Tj = max{0, Cj - dj} and job properties is essential for interpreting heuristic outputs
  - Quick check: Given jobs with p=[10, 5], d=[15, 8], if scheduled in order [2, 1], what is the total tardiness?

- Concept: **Evolutionary Algorithms with Program Representation**
  - Why needed: The pipeline evolves programs (heuristic functions), not solutions, requiring understanding of mutation, selection, and population management
  - Quick check: In standard genetic programming, what does the fitness function evaluateâ€”solutions or solution-generating programs?

- Concept: **Prompt Engineering for Code Generation**
  - Why needed: The specification includes docstrings and explicit instructions to guide LLM output, affecting code quality
  - Quick check: What types of constraints should be stated explicitly in prompts versus encoded in evaluation functions?

## Architecture Onboarding

- Component map: Specification -> LLM Agent -> Evaluator -> Programs Database -> Scheduler -> Loop back to Specification
- Critical path: 1. Specification defines problem -> 2. Initial assignment function (EDD/MDD) seeds database -> 3. Loop: sample programs from island -> construct prompt -> LLM generates variant -> evaluate on training set -> if feasible, store in island -> 4. Every 4 hours: reset bottom 50% islands -> 5. Continue until time/iteration limit -> 6. Extract best program per island, benchmark on test sets
- Design tradeoffs: Single-threaded vs. multi-threaded (paper uses single-threaded for reproducibility); training instance size (25-job chosen to balance evaluation speed vs. generalization); prompt sample size (two programs per prompt balances diversity vs. context window usage)
- Failure signatures: Syntax errors (generated code fails to parse); semantic hallucinations (code runs but manipulates input data); stagnation (all islands converge to similar programs); overfitting (discovered heuristic performs well on 25-job training but degrades on 500-job test)
- First 3 experiments: 1. Baseline validation: Run EDD and MDD as initial assignment functions separately, verify evaluator correctly computes tardiness and penalties on 10 held-out 25-job instances 2. Ablation on prompt engineering: Compare best-shot prompting (2 programs) vs. single-program prompting vs. no-parent prompting (pure LLM generation) over 1,000 iterations, tracking average tardiness 3. Transfer test: Take the best MDDC program, evaluate directly on 100-job and 200-job instances from Shang et al. (2021) without retraining, compare optimality gap against MDD baseline

## Open Questions the Paper Calls Out
- Can interpretable surrogate models (e.g., decision trees) be integrated into the LLM-based heuristic discovery pipeline to improve explainability while maintaining performance?
- To what extent does the choice of initial heuristic (seed) determine the success and quality of LLM-discovered heuristics?
- How does the generalization performance of LLM-discovered heuristics degrade as the gap between training and testing problem sizes increases beyond the 25-to-500 job range studied?

## Limitations
- The exact LLM inference hyperparameters (temperature, top_p, max tokens) are not specified
- Full docstring/prompt template text is referenced but content is incomplete in the paper
- Best-shot prompting sampling strategy details are not fully disclosed
- Limited direct comparison with exact methods beyond Shang et al.'s DP benchmark

## Confidence
- **High confidence**: MDDC's superior performance on larger instances (100-500 jobs) and its computational efficiency relative to exact methods
- **Medium confidence**: The island-based evolutionary mechanism's contribution to discovery, as this specific architecture is novel
- **Medium confidence**: The best-shot prompting technique's effectiveness, given limited corpus validation of this specific approach for heuristic discovery

## Next Checks
1. **Ablation study on island-based architecture**: Compare performance of single-population vs. island-based evolution with identical iteration limits and prompt engineering
2. **Prompt engineering impact test**: Evaluate heuristic discovery performance using three prompting strategies: best-shot (2 parents), single-parent, and no-parent (pure generation) over equal computational budgets
3. **Cross-distribution generalization**: Test MDDC on instances with different due date distributions (e.g., uniform vs. Potts-Van Wassenhove) and processing time variances to assess robustness beyond the training distribution