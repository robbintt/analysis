---
ver: rpa2
title: Membership Inference Attacks with False Discovery Rate Control
arxiv_id: '2508.07066'
source_url: https://arxiv.org/abs/2508.07066
tags:
- data
- non-member
- 'false'
- membership
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of providing false discovery
  rate (FDR) guarantees in membership inference attacks (MIAs), which aim to determine
  whether a data record was used to train a target model. The authors propose MIAFdR,
  a novel method that works as a wrapper for existing MIA techniques to ensure FDR
  control.
---

# Membership Inference Attacks with False Discovery Rate Control

## Quick Facts
- arXiv ID: 2508.07066
- Source URL: https://arxiv.org/abs/2508.07066
- Reference count: 40
- Primary result: Introduces MIAFdR, a wrapper method providing FDR control for MIAs, achieving comparable or improved accuracy (e.g., 78.19% on CIFAR-100) versus baselines while ensuring FDR guarantees.

## Executive Summary
This paper addresses the challenge of providing false discovery rate (FDR) guarantees in membership inference attacks (MIAs), which aim to determine whether a data record was used to train a target model. The authors propose MIAFdR, a novel method that works as a wrapper for existing MIA techniques to ensure FDR control. The core idea involves designing a conformity score function to assess how well test data conforms to non-member data, estimating non-member relative probabilities, and adjusting these probabilities to account for interdependence and control FDR. The method is theoretically analyzed and experimentally validated across various settings, including black-box and lifelong learning scenarios. Results show that MIAFdR effectively controls FDR while maintaining or improving attack performance. For example, on CIFAR-100, MIAFdR achieves 78.19% accuracy compared to 76.81% for the baseline classifier-based MIA, with comparable AUROC scores. The method also demonstrates robustness against existing MIA defenses and applicability to machine unlearning and lifelong learning tasks.

## Method Summary
The MIAFdR method works as a wrapper for existing MIA techniques to provide FDR guarantees. It trains surrogate models on auxiliary data to mimic the victim model, then trains a binary classifier on surrogate outputs to distinguish member from non-member behavior. A conformity score function is designed to assess how well test data conforms to non-member data. Non-member relative probabilities (p-values) are estimated using a calibration set, and these probabilities are adjusted to account for interdependence among test samples, allowing for FDR control. The method is theoretically analyzed and experimentally validated across various settings, including black-box and lifelong learning scenarios.

## Key Results
- MIAFdR achieves comparable or improved attack performance versus baselines while ensuring FDR control.
- On CIFAR-100, MIAFdR achieves 78.19% accuracy compared to 76.81% for the baseline classifier-based MIA, with comparable AUROC scores.
- The method demonstrates robustness against existing MIA defenses and applicability to machine unlearning and lifelong learning tasks.
- Experimental results validate the theoretical FDR control guarantees across various settings and datasets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A conformity score function can effectively reflect the degree to which test data conforms to non-member data distributions, serving as a discriminative signal for membership.
- **Mechanism**: The system trains surrogate models on subsets of auxiliary data to mimic the victim model. A binary classifier is trained on these surrogate outputs to distinguish member (low loss/high confidence) from non-member behavior. For a test sample, the classifier outputs a probability which is transformed into a score $S(y_t; \theta_{bc})$. A higher score indicates the sample looks more like a non-member.
- **Core assumption**: The surrogate models trained on auxiliary data sufficiently approximate the decision boundaries and loss distributions of the victim model, even in black-box settings.
- **Evidence anchors**:
  - [abstract] "design a conformity score function to assess how well test data conforms to non-member data..."
  - [section 4] "larger non-member conformity score... means that it is coming from the non-member prediction; otherwise, it is more likely to be from the member distribution."
  - [corpus] Contextual support exists for using model outputs/confidence for MIAs, though specific FDR-focused scores are not discussed in the provided neighbor abstracts.
- **Break condition**: If the victim model has significantly different generalization properties or architecture than the surrogates such that the confidence distributions diverge, the score will lose discriminative power.

### Mechanism 2
- **Claim**: Calculating non-member relative probabilities (p-values) based on calibration data provides a marginal probability guarantee on labeling non-members.
- **Mechanism**: The method utilizes a calibration dataset $D_{au}^{2,ca}$ consisting of non-member data. It calculates the conformity score for the test sample and compares it against the scores of the calibration set. The p-value is the fraction of calibration scores smaller than or equal to the test score. This ranks the "strangeness" of the test point relative to known non-members.
- **Core assumption**: The sequence of calibration data and test data is exchangeable (a weaker assumption than i.i.d.), meaning the joint distribution is invariant to permutations.
- **Evidence anchors**:
  - [abstract] "estimating non-member relative probabilities... and adjusting these probabilities..."
  - [section 4, Theorem 1] "P(p(x_t) \le \alpha | x_t \notin D_{tr}) \le \alpha... ensures that the error rate for labeling true non-member data as member data does not exceed the predefined threshold."
  - [corpus] Weak/missing direct evidence for this specific conformal approach in neighbor abstracts.
- **Break condition**: If the calibration data is not representative of the test data distribution (distribution shift), or if the exchangeability assumption is violated, the marginal guarantee fails.

### Mechanism 3
- **Claim**: An adjustment method accounting for interdependence among p-values allows for the control of the False Discovery Rate (FDR) across the entire test set.
- **Mechanism**: Because all test p-values depend on the same shared calibration set, they are statistically interdependent, violating standard multiple testing assumptions. The method orders the p-values $p_{(t)}$ and applies a weighted correction (similar to Benjamini-Hochberg but adapted for these specific dependencies) to produce adjusted p-values $p_{adj}^{(t)}$. Decisions are made by comparing $p_{adj}^{(t)}$ to the significance level $\alpha$.
- **Core assumption**: The theoretical analysis holds that the specific dependency structure induced by the shared calibration set is bounded such that the correction formula maintains the FDR upper bound.
- **Evidence anchors**:
  - [abstract] "adjusting these probabilities to account for interdependence and control FDR."
  - [section 4, Theorem 2] "...method allows for controlling the false discovery rate at a predetermined level... even under interdependence."
  - [corpus] Weak/missing evidence for specific dependency adjustment in MIAs in neighbor abstracts.
- **Break condition**: If the theoretical bounds on dependency are loose or incorrect for specific data distributions, the FDR guarantee may be violated in practice.

## Foundational Learning

- **Concept**: **False Discovery Rate (FDR) vs. False Positive Rate (FPR)**
  - **Why needed here**: Standard MIAs often optimize for FPR or accuracy. This paper specifically targets FDR (the ratio of false positives among all positive discoveries), which is critical when the prior probability of membership is unknown or when the cost of false alarms is high relative to the number of claims made.
  - **Quick check question**: If a test set has 100 samples and 90 are non-members, how does FDR behave differently from FPR if the attacker flags 10 members (5 true, 5 false)?

- **Concept**: **Conformal Prediction**
  - **Why needed here**: The paper uses conformal inference to generate valid p-values without knowing the true underlying distribution. Understanding how calibration sets establish distribution-free guarantees is essential to grasp the "marginal probability guarantee."
  - **Quick check question**: Why does conformal prediction require the exchangeability assumption, and what happens to the coverage guarantee if the test data comes from a shifted distribution compared to the calibration set?

- **Concept**: **Surrogate (Shadow) Models**
  - **Why needed here**: The proposed method is a "wrapper" that requires signals from surrogate models to train the binary classifier for the conformity score. The fidelity of these surrogates determines the quality of the initial MIA features.
  - **Quick check question**: In a black-box setting where the attacker knows nothing about the victim architecture, what strategies can be used to train surrogate models that still transfer membership inference capabilities?

## Architecture Onboarding

- **Component map**:
  Data Splitter -> Surrogate Trainer -> Score Classifier -> Calibration Engine -> Adjustment Layer

- **Critical path**: The most sensitive step is the **Split & Train** phase. If $D^{2,ca}_{au}$ is contaminated with member-like data (distribution mismatch) or if the surrogate models fail to learn the victim model's behavior, the resulting p-values and FDR guarantees will be invalid. The inference phase is computationally cheap (sorting and comparing), so the bottleneck is model training.

- **Design tradeoffs**:
  - **Calibration Size vs. Power**: Increasing the size of the calibration set $D^{2,ca}_{au}$ generally stabilizes p-value estimation and improves attack accuracy (Figure 6b), but reduces the data available for training surrogates or the score classifier.
  - **Wrapper Compatibility**: The method is designed to be "post-hoc" (Section 1), meaning it can theoretically wrap existing MIA methods (LiRA, Classifier-based), but requires re-computing scores relative to a calibration set.

- **Failure signatures**:
  - **High FDR despite low $\alpha$**: Suggests the exchangeability assumption is broken or the calibration set $D^{2,ca}_{au}$ does not match the true non-member distribution of the test set.
  - **Low Accuracy**: Likely caused by surrogate models failing to mimic the victim model (black-box transfer failure) or a poorly trained score classifier.

- **First 3 experiments**:
  1. **Validity Check**: Verify Theorem 1 by plotting the empirical probability $P(p(x_t) \le \alpha)$ for non-members against significance level $\alpha$. The curve should track the $y=x$ line (Figure 2a).
  2. **FDR Control Test**: Run the full adjustment pipeline on a test dataset with a known ratio of members/non-members ($\pi_0$). Plot empirical FDR against $\alpha$ to ensure it stays below the theoretical upper bound (Figure 3).
  3. **Baseline Comparison**: Compare the "wrapped" MIAFdR against standard classifier-based MIA and LiRA on AUROC and Accuracy (Table 1) to ensure the FDR wrapper does not destroy attack utility.

## Open Questions the Paper Calls Out
None

## Limitations
- The method's effectiveness relies heavily on the fidelity of surrogate models trained on auxiliary data to capture the victim model's behavior.
- The exchangeability assumption for calibration data is critical but not always verifiable in practice, especially with distribution shifts.
- Generalization to highly specialized models or extreme class imbalance scenarios is unclear without additional validation.

## Confidence
- **High Confidence**: Theoretical FDR control guarantees (Theorems 1 and 2) are well-established within the conformal prediction framework, assuming stated assumptions hold.
- **Medium Confidence**: Empirical results showing improved or comparable attack performance to baselines are convincing but may be sensitive to hyperparameter choices (e.g., calibration set size, surrogate model count).
- **Low Confidence**: Generalization of results to diverse real-world scenarios (e.g., highly specialized models, extreme class imbalance) is unclear without additional validation.

## Next Checks
1. **Calibration Set Sensitivity**: Systematically vary the size of the non-member calibration set $D_{au}^{2,ca}$ and measure its impact on both empirical FDR control and attack accuracy across different datasets.
2. **Surrogate Model Fidelity Test**: Evaluate how well surrogate models trained on auxiliary data transfer membership inference capabilities to black-box target models with varying architectures and training procedures.
3. **Distribution Shift Robustness**: Introduce controlled distribution shifts between the calibration set and test set, and assess the degradation in FDR control and attack performance to test the exchangeability assumption's limits.