---
ver: rpa2
title: "Measuring D\xE9j\xE0 vu Memorization Efficiently"
arxiv_id: '2504.05651'
source_url: https://arxiv.org/abs/2504.05651
tags:
- memorization
- training
- data
- should
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes efficient methods to measure memorization in
  representation learning models without retraining. The key idea is to use pre-trained
  reference models (e.g., ResNet50 classifier, Naive Bayes on object detection outputs,
  or pre-trained text embedding models) to estimate dataset-level correlations instead
  of training additional models on data splits.
---

# Measuring Déjà vu Memorization Efficiently

## Quick Facts
- arXiv ID: 2504.05651
- Source URL: https://arxiv.org/abs/2504.05651
- Reference count: 39
- One-model déjà vu tests enable efficient memorization measurement without retraining SSL or VLM models

## Executive Summary
This paper proposes efficient methods to measure memorization in representation learning models without retraining. The key idea is to use pre-trained reference models (e.g., ResNet50 classifier, Naive Bayes on object detection outputs, or pre-trained text embedding models) to estimate dataset-level correlations instead of training additional models on data splits. This enables one-model déjà vu tests for both vision and vision-language models. Experiments show that pre-trained open-source models have lower memorization than similar models trained on smaller data subsets. The methods successfully identify memorized examples and achieve comparable population-level memorization scores to the original two-model approach.

## Method Summary
The paper introduces one-model déjà vu tests that replace the original two-model retraining approach with lightweight reference models. For vision models, it trains a ResNet50 classifier or Naive Bayes classifier on background crops to estimate dataset-level correlations. For vision-language models, it uses a pre-trained text embedding model (GTE) to retrieve similar captions from a public corpus and aggregate their object labels. The memorization score is computed as the difference between the target model's prediction accuracy and the reference model's prediction accuracy on the same background crops. Sample-level memorization is measured using entropy differences between the target and reference model predictions.

## Key Results
- One-model tests achieve comparable déjà vu scores to two-model tests (Section 4.1.1)
- Pre-trained open-source models show lower memorization than models trained on smaller subsets (Section 4.1.2)
- Vision-language models trained on Shutterstock exhibit significantly higher memorization than open-source CLIP models (Section 4.2.1)
- Sample-level confidence scores successfully identify memorized examples (Section 4.1.3)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training a simple classifier on background crops approximates dataset-level foreground-background correlations as effectively as training a second SSL model.
- **Mechanism:** A ResNet50 or Naive Bayes classifier learns the statistical association between background visual features and foreground labels directly, without requiring a full representation learning pipeline. The classifier's predictions establish a correlation baseline; if the target SSL model predicts the foreground from background representations significantly better, memorization is inferred.
- **Core assumption:** The reference classifier captures correlation structure without memorizing individual training samples (due to simpler architecture and explicit supervised training objective).
- **Evidence anchors:**
  - [abstract]: "propose alternative simple methods to estimate dataset-level correlations... show that these can be used to approximate an off-the-shelf model's memorization ability without any retraining"
  - [section 3.2]: "We propose two ways to do this: training an image classification network end-to-end, and using naive Bayes classifier on top of an object detector"
  - [corpus]: Limited direct evidence; related membership inference work (arXiv:2506.05126) similarly uses reference models but for different memorization detection tasks.
- **Break condition:** If the reference classifier overfits to its training split, it may underestimate true memorization by overestimating correlation accuracy.

### Mechanism 2
- **Claim:** Pre-trained text embedding models can estimate dataset-level correlations for vision-language models by retrieving semantically similar captions from a public corpus.
- **Mechanism:** Given a caption z_text, the reference model g retrieves K nearest neighbors from a public caption set via embedding similarity. Objects present in neighbor images are aggregated to predict what objects should appear given the caption. If the target VLM predicts more objects than this reference baseline, memorization is indicated.
- **Core assumption:** The pre-trained text embedding model captures semantic similarity relevant to object co-occurrence patterns without access to the target training data.
- **Evidence anchors:**
  - [section 3.3]: "We leverage a pre-trained text embedding model g... find the K most similar captions in D_pub"
  - [section 4.2.1]: Reports 84% agreement between reference VLM f_B and GTE model g for all-object prediction with 100 NNs
  - [corpus]: Paper on "Estimating Effects of Sample Training Orders" (arXiv:2505.22042) uses similar retraining-free estimation principles.
- **Break condition:** When caption semantics don't correlate well with object presence (e.g., abstract captions), or when the public set lacks sufficiently similar examples.

### Mechanism 3
- **Claim:** Sample-level memorization can be quantified by the confidence gap between target and reference model predictions.
- **Mechanism:** MemConf(x) = Entropy(Correlation Classifier) − Entropy(SSL KNN). Positive values indicate the SSL model is more confident than correlation alone justifies. Memorized examples cluster at higher confidence scores.
- **Core assumption:** Entropy differences reflect genuine memorization rather than calibration artifacts between models.
- **Evidence anchors:**
  - [section 4.1.3]: Defines memorization confidence formula and shows histogram distribution for VICReg OSS model
  - [figure 7]: Shows memorized examples with high confidence scores are rarer; middle distribution contains confusable examples
  - [corpus]: "Landscape of Memorization in LLMs" (arXiv:2507.05578) discusses similar measurement approaches.
- **Break condition:** When both models have similar confidence (ambiguous predictions) or when entropy is a poor proxy for the relevant uncertainty.

## Foundational Learning

- **Self-Supervised Representation Learning (VICReg, DINO, Barlow Twins):**
  - Why needed here: The target models being evaluated are SSL models that learn representations without labels; understanding their training objectives (invariance, variance, covariance) explains why memorization might occur.
  - Quick check question: Can you explain why SSL models might memorize training associations despite not using labels during training?

- **Déjà Vu Memorization Definition:**
  - Why needed here: The core detection logic depends on distinguishing memorization (sample-specific learning) from correlation (dataset-wide patterns); formal definition in Section 3.1 is essential.
  - Quick check question: Given Definition 1, why does the condition "argmax μ(t'|v) ≠ t" matter for privacy risk assessment?

- **K-Nearest Neighbors in Representation Space:**
  - Why needed here: The paper uses KNN classifiers on top of frozen representations to predict foreground labels; understanding distance metrics and neighbor selection is critical for implementation.
  - Quick check question: How does the choice of K (number of neighbors) affect the trade-off between correlation accuracy and memorization sensitivity?

## Architecture Onboarding

- **Component map:**
  - Target model f (SSL encoder being audited) -> Crop extraction function (background removal) -> Representation f(v) -> KNN predictor -> Prediction accuracy
  - Reference model (ResNet50 classifier OR Naive Bayes + object detector for images; GTE embedding model for VLMs) -> Crop extraction function -> Reference prediction -> Prediction accuracy
  - Target model f -> Representation f(v) -> KNN predictor -> Entropy -> MemConf(x)
  - Reference model -> Correlation classifier -> Entropy -> MemConf(x)

- **Critical path:**
  1. Extract background crop from training image
  2. Get representation f(v) from target model
  3. Run KNN in representation space to predict foreground
  4. Run reference model on same crop to predict foreground
  5. Compute accuracy gap (déjà vu score) or entropy gap (MemConf)

- **Design tradeoffs:**
  - ResNet50 vs Naive Bayes: ResNet50 achieves ~86% agreement with two-model test intersection on top-1 predictions (Table 1); Naive Bayes is faster but requires annotation quality
  - Number of crop annotations (Naive Bayes): Top-20 annotations yield higher correlation accuracy but may include noise
  - Number of NNs for VLM: More neighbors (100 vs 10) increase recall but reduce precision

- **Failure signatures:**
  - Agreement between reference models <50% (Figure 3a): indicates noisy correlation estimates
  - Negative memorization scores at scale: reference model may be overfitting
  - High memorization in open-source models on common classes: likely correlation, not memorization

- **First 3 experiments:**
  1. Reproduce Figure 5 comparison on ImageNet 300k subset: validate that one-model ResNet50 test yields similar déjà vu scores to two-model KNN for VICReg/Barlow Twins/DINO
  2. Compute sample-level agreement between ResNet50 and Naive Bayes reference models; identify examples where they disagree and diagnose whether one systematically over/underestimates
  3. Apply one-model test to an open-source CLIP model (e.g., OpenCLIP YFCC15M) using GTE as reference; compare PPG/PRG values to two-model baseline from your own training run

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the reference model's own potential memorization bias the one-model déjà vu score?
- Basis in paper: [explicit] Section 5 lists the limitation that "our alternative dataset-level correlation estimation might... memorize its own training set, thus skewing the results," identifying a closer analysis as future work.
- Why unresolved: While the paper assumes simpler reference models (ResNet, Naive Bayes) memorize less than SSL models, it does not quantify or correct for any memorization that does occur within these reference models.
- What evidence would resolve it: A quantification of the reference classifier's memorization rate and a sensitivity analysis showing the impact on the final memorization metric.

### Open Question 2
- Question: How sensitive is the Naive Bayes correlation estimator to the noise levels in automated object detection annotations?
- Basis in paper: [explicit] Section 5 notes that "the additional annotations that we use for our measurements may be lower quality, which might also lead to biased results."
- Why unresolved: The method relies on automated annotations (e.g., from Grounded-SAM), but the paper does not isolate the effect of annotation noise on the accuracy of the dataset-level correlation estimates.
- What evidence would resolve it: An ablation study comparing the performance of the Naive Bayes estimator using ground-truth annotations versus automated noisy annotations.

### Open Question 3
- Question: How can the divergent inductive biases of the one-model and two-model tests be combined to improve the robustness of memorization detection?
- Basis in paper: [inferred] Section 3.2 and Figure 3 reveal a low sample-level agreement (~40%) between methods because they capture different correlation types (foreground vs. background features), suggesting they "can be used conjointly."
- Why unresolved: The paper highlights that the methods succeed and fail on different examples but does not propose a mechanism to unify them into a single, more robust metric.
- What evidence would resolve it: A joint statistical model or ensemble method that integrates predictions from both the reference classifiers (ResNet/NB) and the two-model KNN approach.

## Limitations

- Reference models trained on smaller subsets may not fully represent dataset-level correlations, potentially underestimating memorization in models trained on larger datasets
- Vision-language approach depends heavily on caption-object correlations that may not hold for abstract or atypical examples
- Sample-level memorization detection assumes entropy differences reliably indicate memorization rather than calibration artifacts between models

## Confidence

- **High confidence:** Reference model agreement with two-model tests (measured through empirical correlation scores)
- **Medium confidence:** Population-level memorization measurement validity across different model scales
- **Low confidence:** Sample-level memorization detection reliability when both models have ambiguous predictions

## Next Checks

1. Test reference model stability by training multiple reference classifiers on different random splits of the same dataset and measuring variance in déjà vu scores
2. Validate that entropy-based memorization confidence correlates with actual memorization when ground truth is available (e.g., using models trained on disjoint subsets)
3. Evaluate whether reference models trained on smaller subsets systematically underestimate memorization in models trained on larger datasets