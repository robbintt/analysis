---
ver: rpa2
title: 'Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models
  with Only Attention Sink'
arxiv_id: '2501.15269'
source_url: https://arxiv.org/abs/2501.15269
tags:
- image
- hallucination
- arxiv
- responses
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first hallucination attack targeting
  multi-modal large language models (MLLMs) by exploiting attention sink behaviors
  during generation. The attack manipulates attention mechanisms and hidden embeddings
  to induce more hallucinated content without degrading response quality.
---

# Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink

## Quick Facts
- arXiv ID: 2501.15269
- Source URL: https://arxiv.org/abs/2501.15269
- Authors: Yining Wang; Mi Zhang; Junjie Sun; Chenyue Wang; Min Yang; Hui Xue; Jialing Tao; Ranjie Duan; Jiexi Liu
- Reference count: 40
- One-line primary result: First attack inducing hallucinations in MLLMs by exploiting attention sink behaviors, achieving up to 12.74% increase in hallucinated words without degrading response quality

## Executive Summary
This paper introduces the first hallucination attack targeting multi-modal large language models (MLLMs) by exploiting attention sink behaviors during generation. The attack manipulates attention mechanisms and hidden embeddings to induce more hallucinated content without degrading response quality. Experimental results demonstrate up to 12.74% increase in hallucinated words and 10.90% increase in hallucinated sentences across six prominent MLLMs, with strong transferability to black-box models and commercial APIs like GPT-4o and Gemini 1.5. The attack successfully bypasses existing mitigation strategies and adaptive defenses while maintaining semantic accuracy and fluency in responses.

## Method Summary
The attack optimizes adversarial perturbations to visual inputs that create "attention sink" tokens—tokens that receive disproportionately high attention from subsequent generations. The method computes a loss function combining attention-based cross-entropy (forcing future tokens to attend to the sink) and embedding-based hinge loss (maximizing the sink token's similarity to global input information). The perturbation is optimized via sign-gradient descent with clipping to stay within an L∞ budget. The attack is applied during auto-regressive generation, with the sink token identified at each step as the generated token most similar to the mean of input embeddings. The approach works across multiple MLLM architectures including LLaVA-1.5, InstructBLIP, MiniGPT-4, and Shikra.

## Key Results
- Achieved up to 12.74% increase in Hallucination Sentence Rate (HSR) and 10.90% increase in Hallucination Word Rate (HWR) across six MLLMs
- Demonstrated strong transferability, successfully attacking commercial APIs including GPT-4o and Gemini 1.5
- Bypassed existing mitigation strategies including OPERA and VCD while maintaining GPT-4 fluency scores above 8
- Showed the attack is effective with minimal visible artifacts at perturbation budget ε=8/255

## Why This Works (Mechanism)

### Mechanism 1: Attention Sink Columnar Pattern Exploitation
Forcing a columnar attention pattern on selected tokens causes subsequent generation to neglect image context and produce hallucinated content. The attack computes cross-entropy loss on attention scores to maximize attention allocation from future tokens to a designated sink token, creating a feedback loop where the sink token dominates prediction, overriding visual grounding. Core assumption: Columnar attention is causally linked to hallucination, not merely correlated.

### Mechanism 2: Instruction-Tuning Inherited Two-Segment Pattern
MLLMs learn to segment responses: first image-grounded, then association-driven, with sink tokens marking the transition. Training data contains ground-truth responses where later sentences include inferred summaries and associations with declining CLIPScore. Models internalize this structure. Core assumption: The CLIPScore decline in training data directly causes similar decline in generated outputs.

### Mechanism 3: Misleading Global Information Aggregation
Sink tokens aggregate partial, distorted global information, serving as corrupted context shortcuts. Sink token embeddings show higher cosine similarity to aggregated input embeddings. This aggregation loses detail and injects misleading priors, which subsequent tokens rely on. Core assumption: Embedding similarity indicates information aggregation, not superficial pattern matching.

## Foundational Learning

- **Concept: Self-Attention and Softmax Normalization**
  - Why needed here: The attack exploits how softmax forces attention to sum to one, creating sink tokens that absorb "redundant" attention.
  - Quick check question: Why does the sum-to-one property in softmax attention encourage some tokens to become attention sinks?

- **Concept: Instruction Tuning vs. Pre-training**
  - Why needed here: Vulnerability originates in instruction-tuning data characteristics, not pre-training.
  - Quick check question: What distinguishes instruction-tuning data from pre-training data in MLLMs, and how does each affect behavior?

- **Concept: Gradient-Based Adversarial Perturbation**
  - Why needed here: The attack optimizes perturbations δ via backpropagation to visual inputs under L_p constraints.
  - Quick check question: How does sign-gradient descent with clipping differ from standard gradient descent in adversarial settings?

## Architecture Onboarding

- **Component map:** Visual Encoder (CLIP-ViT / EVA-ViT) -> Adapter/Projection -> LLM Backbone (Vicuna / LLaMA) -> Attack Module
- **Critical path:**
  1. Forward pass → extract h^(l) (hidden states) and a^(l) (attention) from intermediate layer
  2. Compute global embedding h_g = Mean(input embeddings)
  3. Identify sink candidate: idx = argmax_i[Sim(h^(l)_i, h_g)]
  4. Compute losses: L_attn = CE(attention, idx); L_emb = hinge(Sim(h^(l)_idx, h_g) - σ)
  5. Backpropagate to image: δ ← δ - γ·sign(∇L_total); clip to [-ε, ε]
- **Design tradeoffs:**
  - Layer selection: earlier = low-level features; later = semantic; authors use second-to-last or third-to-last
  - ε budget: higher = more effective but more visible
  - σ threshold: controls how much global info to force into sink token
- **Failure signatures:**
  - Perturbation too large → degraded fluency, detectable noise
  - No columnar pattern emerges → wrong layer or learning rate too low
  - Transfer fails → surrogate overfit; try different surrogate or multi-surrogate training
- **First 3 experiments:**
  1. Baseline correlation: Run clean inference on HalluBench, extract attention maps, compute per-sentence CLIPScore, verify sink tokens appear at CLIPScore turning points.
  2. White-box sanity check: Attack with ε=8/255, confirm HSR/HWR increase without quality drop (GPT-4 score > 8).
  3. Transfer test: Optimize δ on LLaVA-1.5, apply to InstructBLIP/MiniGPT-4, measure HSR delta.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the efficacy of attention-sink-based hallucination attacks be extended to complex multi-modal tasks like visual reasoning and multi-turn dialogues?
- Basis in paper: The authors state in the Future Works section: "In our further research, we plan to extend the adversarial efficacy of the proposed attack to a broader range of multi-modal tasks, including visual reasoning, grounding, and multi-turn visual dialogues."
- Why unresolved: The current evaluation is limited to image captioning and single-turn question answering (OK-VQA).
- What evidence would resolve it: Successful application of the attack on benchmarks requiring multi-step visual reasoning or conversation history, with metrics showing increased hallucinations.

### Open Question 2
- Question: How can defensive strategies be improved to prevent attention-sink manipulation without significantly degrading the response quality or utility of the model?
- Basis in paper: The authors note that while adaptive early-stopping reduces hallucinations, it "significantly decreases the mean length and quality of responses." They conclude: "We hope this work inspires future research into more robust defensive strategies... and new training paradigms."
- Why unresolved: Current mitigations (e.g., OPERA, VCD) failed to stop the attack, and the adaptive defense tested (early stopping) rendered the model less useful.
- What evidence would resolve it: A new defense mechanism that maintains response fluency and length while reducing the hallucination rates induced by this specific attack to baseline levels.

### Open Question 3
- Question: Can manipulating attention mechanisms via textual inputs (rather than visual perturbations) effectively induce hallucinations while maintaining semantic stealth?
- Basis in paper: The paper focuses on visual inputs but includes a discussion section on "Adversarial Textual Inputs." The authors note that existing textual methods are easily detectable and suggest their approach to manipulating hidden states "may overcome the current challenges of malicious textual inputs."
- Why unresolved: The paper does not experiment with textual perturbations; it only speculates on the potential based on the mechanism's properties.
- What evidence would resolve it: Experiments showing that textual adversarial examples, optimized using the paper's attention-sink objective, successfully induce hallucinations across different MLLMs.

## Limitations
- Perturbation budget ε=8/255 may create visible artifacts that could be detected by preprocessing defenses
- Transferability performance degrades when attacking substantially different architectures
- Assumes access to intermediate attention states during generation, which may not be available in all deployment scenarios
- Claim about bypassing "existing mitigation strategies" lacks specificity about which defenses were tested

## Confidence
- **High Confidence**: The core mechanism of exploiting attention sink behaviors through columnar attention patterns is well-supported by experimental evidence (HSR increases of up to 12.74% across multiple models). The two-segment response pattern observation is empirically validated through CLIPScore analysis of ground truth responses.
- **Medium Confidence**: The transferability claims to black-box models and commercial APIs are supported by experimental results, but the exact mechanisms enabling cross-architecture transfer remain incompletely characterized. The claim about bypassing adaptive defenses is based on experimental results but lacks detailed description of defense implementations.
- **Low Confidence**: The long-term robustness of this attack against potential future defenses is unknown. The practical detectability of perturbations at the chosen budget threshold in real-world scenarios has not been thoroughly evaluated.

## Next Checks
1. **Defense Robustness Test**: Implement and evaluate against a comprehensive suite of visual preprocessing defenses (JPEG compression, Gaussian blur, denoising) at various intensities to assess perturbation detectability and attack resilience.
2. **Cross-Domain Transferability**: Test attack transferability across different vision-language model families with varying architectural designs (different attention mechanisms, visual encoders) to quantify generalization limits.
3. **Temporal Stability Analysis**: Evaluate attack effectiveness over extended generation sequences (beyond the initial sentences) to determine if the two-segment pattern hypothesis holds throughout longer responses and identify potential breaking points.