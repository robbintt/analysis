---
ver: rpa2
title: Vehicle-to-Infrastructure Collaborative Spatial Perception via Multimodal Large
  Language Models
arxiv_id: '2509.03837'
source_url: https://arxiv.org/abs/2509.03837
tags:
- spatial
- prediction
- multimodal
- link
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal large language model (MLLM)-based
  framework for vehicle-to-infrastructure (V2I) link quality prediction. The key idea
  is to address the lack of spatial understanding in MLLMs by fusing sensor data from
  neighboring vehicles into a unified bird's-eye view (BEV) representation.
---

# Vehicle-to-Infrastructure Collaborative Spatial Perception via Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2509.03837
- Source URL: https://arxiv.org/abs/2509.03837
- Reference count: 23
- Primary result: BEV injection improves macro-avg accuracy by 13.9% vs ego-only baselines, up to 32.7% in adverse conditions

## Executive Summary
This paper proposes a multimodal large language model (MLLM)-based framework for vehicle-to-infrastructure (V2I) link quality prediction. The key innovation is addressing MLLMs' lack of spatial understanding by fusing sensor data from neighboring vehicles into a unified bird's-eye view (BEV) representation. This BEV is then injected into a pre-trained LLM via a lightweight, plug-and-play connector, enabling spatial reasoning without retraining the entire model. A custom co-simulation environment combining CARLA and MATLAB ray tracing generates multimodal data and ground-truth link quality labels. Experiments on three tasks—LoS/NLoS classification, link availability, and blockage prediction—show significant improvements over ego-only baselines, with particularly strong gains under adverse conditions like rain and nighttime.

## Method Summary
The approach uses a frozen Llama-3.2-11B-Vision model as the backbone, with a trainable connector module that processes BEV representations of sensor data. Multiple vehicles (5 cooperative + 1 ego) provide RGB, LiDAR, and GPS data in a CARLA simulation. BEVFusion and BEVFormer TSA modules process temporal BEV data, which is then warped to the ego coordinate frame and distilled through a Q-Former into instruction-relevant tokens. The connector is trained on a custom dataset with 50 episodes, 30 noon, 10 night, and 10 rain conditions, using AdamW optimizer with cosine learning rate decay. The system predicts link quality through natural language responses rather than direct classification.

## Key Results
- Macro-average accuracy of 87.2% vs 73.3% ego-only baseline (13.9% improvement)
- Performance gains increase up to 32.7% under challenging rainy and nighttime conditions
- Adding helper vehicles rapidly improves accuracy by covering blind spots, with diminishing returns after 2 agents
- Q-Former ablation drops macro-avg accuracy from 87.2% to 84.0%, confirming its importance

## Why This Works (Mechanism)

### Mechanism 1: Collaborative Field-of-View Extension
Fusing sensor data from neighboring vehicles into a unified Bird's-Eye View (BEV) compensates for the ego vehicle's occlusions and limited perspective. The architecture aggregates LiDAR and RGB features from helper agents, warps them into the ego coordinate frame, and concatenates them. This effectively "fills in" blind spots that cause link blockages, allowing the model to reason about obstacles invisible to the ego camera. The core assumption is that relative GPS poses and time synchronization between agents are sufficiently accurate to align BEV features without introducing geometric artifacts.

### Mechanism 2: Instruction-Aware Spatial Distillation
Using a Q-Former to distill BEV features into compact tokens allows a frozen LLM to process spatial context without the computational cost of raw pixel-based reasoning. The Q-Former acts as a bottleneck adapter, extracting only the geometric features relevant to the specific natural language instruction. This filters noise and reduces the sequence length injected into the LLM. The core assumption is that the Q-Former can be trained to map 3D geometric relationships to semantic concepts that the pre-trained LLM already understands linguistically.

### Mechanism 3: Geometric Invariance for Zero-Shot Robustness
Rating link quality based on geometric BEV structure rather than raw RGB pixels enables robust generalization to adverse environmental conditions. By converting sensor data into a BEV grid, the model relies on obstacle presence/absence rather than texture or lighting. Since physical obstacles do not change between day and night, the model maintains performance where RGB-only models fail. The core assumption is that LiDAR-to-BEV conversion remains reliable under adverse weather.

## Foundational Learning

- **Concept: Bird's-Eye View (BEV) Fusion**
  - Why needed here: This is the core representation that translates 3D world data into a format the model can reason over spatially.
  - Quick check question: How does projecting 3D LiDAR points onto a 2D grid preserve the "height" information necessary to determine if a truck blocks a signal?

- **Concept: Q-Former (Querying Transformer)**
  - Why needed here: This acts as the bridge between the spatial encoder and the frozen LLM. It is crucial for understanding how "instruction-aware" distillation works.
  - Quick check question: In this architecture, does the Q-Former learn to generate new language tokens, or does it retrieve features based on the text instruction?

- **Concept: Ray Tracing for Wireless Channels**
  - Why needed here: To interpret the dataset generation. The ground truth (LoS/NLoS) is not labeled by humans but derived from physics-based simulations.
  - Quick check question: Why is ray tracing necessary for generating "link availability" labels (RSS < -80dBm) rather than just checking line-of-sight?

## Architecture Onboarding

- **Component map:** Input (RGB + LiDAR from Ego + Helpers) -> Perception (BEVFusion + BEVFormer TSA + Warp) -> Connector (Q-Former) -> Reasoning (Frozen Llama-3.2-11B-Vision) -> Output (Text response)

- **Critical path:** The Helper BEV Warp (Eq. 9) is the most fragile link. If the transformation $\xi_i \to \xi_{ego}$ is flawed, the fused BEV will have misaligned obstacles, leading to false positives in blockage detection.

- **Design tradeoffs:** Frozen LLM vs. Fine-tuning: The authors freeze the LLM to preserve reasoning capabilities and reduce compute. The tradeoff is that the model cannot learn entirely new "concepts" of physics, only mappings from spatial tokens to existing linguistic concepts. Token Overhead: The Q-Former reduces tokens but may lose fine-grained texture details.

- **Failure signatures:**
  - Hallucinated Blockage: The model predicts a blockage that doesn't exist (High False Positive). Likely cause: Misalignment in the multi-agent warp step or noise in LiDAR creating ghost obstacles.
  - Zero-Shot Collapse: Performance drops at night despite BEV usage. Likely cause: The RGB-to-BEV lift is failing due to low light, or the LiDAR is faulty, implying the BEV is empty or noisy.

- **First 3 experiments:**
  1. Ablate the Warp: Run inference with the Warp step disabled to quantify sensitivity to coordinate alignment (Table II confirms this drops accuracy to 80.7%).
  2. Vary Helper Count: Replicate Figure 3 by varying $N_v$ from 0 to 5 to verify if returns diminish after 2 agents.
  3. Nighttime Stress Test: Force the model to run on the "Night" subset using only RGB inputs to verify if "Zero-Shot" robustness is purely geometric.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several critical issues emerge from the methodology:

- **Sim-to-Real Gap:** How does the framework perform in real-world environments compared to the idealized CARLA and MATLAB simulation? The paper relies entirely on synthetic co-simulation without validation on real V2I datasets.

- **Communication Overhead:** What is the trade-off between transmitting raw multi-modal data from helper vehicles and the resulting prediction accuracy? The system model assumes all vehicles transmit raw sensor data at every time step.

- **Dynamic Agent Selection:** Can the framework dynamically select the most informative helper agents to maximize spatial coverage while minimizing computational redundancy? The current implementation fuses data from all available agents without discriminating based on viewpoint utility.

- **Latency Constraints:** Does the inference latency of the MLLM-based framework satisfy the strict time constraints required for proactive V2I handover? The paper highlights the need for low-latency communication but does not report inference times.

## Limitations

- The simulation environment (CARLA + MATLAB ray tracing) is not fully specified, making exact reproduction challenging.
- Q-Former architecture details are only implicitly defined through ablation results, not explicitly documented.
- Performance gains rely heavily on precise coordinate alignment between agents, with no quantification of sensitivity to GPS drift or communication latency.
- Evaluation focuses exclusively on controlled simulation data without validation on real-world V2I scenarios or hardware-in-the-loop testing.

## Confidence

- **High Confidence:** BEV fusion mechanism effectiveness (87.2% vs 73.3% baseline macro-avg accuracy) is well-supported by controlled ablation studies.
- **Medium Confidence:** Zero-shot robustness claim (32.7% gain in adverse conditions) is supported by night/rain split results but lacks comparison to alternative approaches.
- **Low Confidence:** Generalizability of instruction-aware Q-Former beyond the three defined tasks, as the paper does not demonstrate performance on novel spatial reasoning queries.

## Next Checks

1. **Coordinate Alignment Stress Test:** Systematically inject artificial GPS drift (0.1-1.0m) into helper vehicle positions and measure degradation in macro-avg accuracy.

2. **Real-World Transfer Validation:** Deploy the trained model on a real V2I testbed to assess performance degradation when moving from synthetic to physical sensing conditions.

3. **Generalization Probe:** Evaluate the model on out-of-distribution spatial queries to test the Q-Former's ability to distill instruction-relevant features beyond the training distribution.