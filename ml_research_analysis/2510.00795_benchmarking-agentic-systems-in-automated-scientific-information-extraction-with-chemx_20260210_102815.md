---
ver: rpa2
title: Benchmarking Agentic Systems in Automated Scientific Information Extraction
  with ChemX
arxiv_id: '2510.00795'
source_url: https://arxiv.org/abs/2510.00795
tags:
- extraction
- data
- should
- dataset
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChemX, a manually curated benchmark of 10
  datasets for evaluating automated chemical information extraction systems. The benchmark
  focuses on nanomaterials and small molecules, featuring diverse data types including
  tables, text, and figures.
---

# Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX

## Quick Facts
- arXiv ID: 2510.00795
- Source URL: https://arxiv.org/abs/2510.00795
- Reference count: 40
- Primary result: Single-agent preprocessing pipeline improves chemical extraction recall from 0.53 to 0.75 F1

## Executive Summary
This paper introduces ChemX, a manually curated benchmark of 10 datasets for evaluating automated chemical information extraction systems, focusing on nanomaterials and small molecules. The benchmark features diverse data types including tables, text, and figures. Experiments benchmark both general-purpose and domain-specific agentic systems, including GPT-5, GPT-5 Thinking, and specialized extraction agents. Results show that current methods struggle with chemical extraction tasks, particularly in processing domain-specific terminology and SMILES notations. The proposed single-agent approach with structured preprocessing improves performance over baselines, providing a critical resource for advancing automated chemical data extraction and benchmarking future methods.

## Method Summary
The study evaluates chemical information extraction using a single-agent approach that preprocesses PDF documents into structured markdown format before extraction. The pipeline uses `marker-pdf` SDK to extract text and tables, processes images with GPT-4o to generate descriptions, and then feeds the combined markdown to extraction models like GPT-4.1 or GPT-5. The method is benchmarked against multi-agent systems and specialized tools using 6 scientific articles (3 Nanozymes, 3 Chelate Complexes) with ground truth annotations. Evaluation measures F1 score through precision and recall calculations for specific chemical fields.

## Key Results
- Single-agent preprocessing improves extraction recall from 0.53 to 0.75 F1
- GPT-5 Thinking model underperforms standard GPT-5 in extraction tasks (F1 0.02 vs 0.37 on Nanozymes)
- All evaluated systems fail to accurately extract SMILES notations due to lack of integrated OCSR tools
- Specialized tools like NanoMINER achieve high F1 (0.80) but fail on other datasets

## Why This Works (Mechanism)

### Mechanism 1: Structured Preprocessing Reduces Semantic Ambiguity
Converting heterogeneous PDF data into standardized markdown prior to LLM processing improves extraction recall over end-to-end black-box agent processing. The "Single-agent approach" uses `marker-pdf` to explicitly separate text blocks and tables, while using GPT-4o to generate text descriptions for images. This forces a consistent token context for the extraction model, reducing parsing errors inherent in raw PDF bitstreams. The intermediate markdown representation preserves semantic relationships better than general-purpose agents' internal representation. If OCR fails to correctly identify table borders or image captions, the resulting markdown misalignment will likely cause the LLM to hallucinate relationships.

### Mechanism 2: The Molecular Vision Bottleneck
Current agentic systems systematically underperform on small-molecule tasks because they lack integrated Optical Chemical Structure Recognition (OCSR) tools to convert images into SMILES strings. While agents can process natural language and tabular numbers, molecular structures are often published as images. Without a specialized tool to rasterize these images into SMILES (e.g., DECIMER), the agent attempts to describe the image textually, failing to produce the required structured chemical identifier. This is a tool-capability gap rather than a fundamental reasoning failure of the LLM. If the system integrates a specialized OCSR tool, this specific bottleneck should theoretically resolve.

### Mechanism 3: Reasoning Overhead in Extraction Tasks
Extended reasoning models (e.g., "Thinking" variants) can degrade performance in strict information extraction tasks compared to standard baselines. Information extraction requires high adherence to schema and literal retrieval. "Thinking" prompts may encourage the model to second-guess explicit data or over-interpret ambiguous context, leading to lower precision or recall in strictly defined extraction tasks. The "Thinking" process introduces variance or "second-guessing" that negatively impacts deterministic extraction instructions. This effect likely reverses in tasks requiring synthesis or summarization, where extended reasoning is beneficial rather than a source of drift.

## Foundational Learning

- **Concept: SMILES (Simplified Molecular Input Line Entry System)**
  - Why needed here: This is the core metric failure point for the "Small Molecules" domain. Understanding that a visual diagram of a molecule must be converted into this specific string notation is essential to diagnose why the agents failed.
  - Quick check question: Can an LLM natively "read" a molecular structure image as a SMILES string without an external vision tool, or does it merely hallucinate a description?

- **Concept: Agentic Orchestration (Single vs. Multi)**
  - Why needed here: The paper contrasts a controlled single-agent pipeline with existing multi-agent systems. Distinguishing between "a model with tools" and "a system of interacting agents" is necessary to interpret the architecture tradeoffs.
  - Quick check question: Does the system pass the PDF to one prompt, or does it delegate sub-tasks (OCR, Vision, Extraction) to distinct specialized modules?

- **Concept: Precision vs. Recall in Extraction**
  - Why needed here: The paper highlights a shift from 0.53 to 0.75 Recall. Understanding that this measures "how much of the available data we actually captured" vs. "how accurate the captured data was" is vital for evaluating the preprocessing strategy.
  - Quick check question: If a system extracts 100 values but 50 are wrong, versus a system that extracts only 50 values but all are correct, which has higher utility for building a chemical database?

## Architecture Onboarding

- **Component map:** PDF Input -> marker-pdf (Text/Table extraction) -> GPT-4o (Image-to-Text) -> Markdown Aggregator -> GPT-4.1/GPT-5 (Extraction) -> CSV/JSON Output

- **Critical path:** The **Preprocessing Agent** is the primary leverage point. The paper explicitly attributes the performance gain (Recall +0.22) to this step, noting that opaque processing in ChatGPT Agent led to inconsistent results.

- **Design tradeoffs:**
  - **Control vs. Simplicity:** The "Single-agent approach" requires managing a preprocessing pipeline but yields reproducibility. General agents (ChatGPT Agent) are simpler but may hit "policy violation" errors or inconsistent internal parsing.
  - **Generalizability vs. Accuracy:** Specialized tools (NanoMINER) achieve F1=0.80 but fail on other datasets. General LLMs hover around F1=0.50 but apply broadly.

- **Failure signatures:**
  - **Policy Blocks:** ChatGPT Agent refusing to process chemical texts due to "safety" triggers.
  - **SMILES Hallucination:** High confidence but zero valid molecular strings for small molecules.
  - **Thinking Drift:** "Thinking" models returning empty or minimal results (F1=0.02) due to over-reasoning.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run `marker-pdf` on the "Nanozymes" subset to replicate the Markdown formatting and measure the parsing accuracy manually.
  2. **Ablation on Modality:** Remove the image descriptions from the Markdown context and run extraction to quantify exactly how much visual data contributes to the final F1 score.
  3. **Tool Integration Test:** Attempt to integrate an external OCSR tool (like DECIMER) into the pipeline for the "Complexes" dataset to validate if SMILES extraction F1 can be raised above 0.0.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the integration of specialized optical chemical structure recognition (OCSR) tools into agentic workflows resolve the "critical shortcoming" of near-zero SMILES extraction from molecular images?
- Basis in paper: [explicit] The authors identify the inability to extract SMILES as a major failure mode, explicitly noting that while tools like DECIMER exist, they are not integrated due to image segmentation challenges.
- Why unresolved: Current agentic systems lack the necessary "reliable detection of individual molecular images" and segmentation capabilities required to feed inputs into existing OCSR tools.
- What evidence would resolve it: A study benchmarking agentic systems on ChemX small molecule datasets where an agent successfully invokes an OCSR tool after performing accurate image segmentation, resulting in significantly higher SMILES extraction F1 scores.

### Open Question 2
- Question: How can multi-agent orchestration be optimized to outperform single-agent approaches in complex chemical information extraction tasks?
- Basis in paper: [explicit] The discussion concludes that "greater research emphasis should be placed on agent orchestration" to address the finding that extraction remains a "surprisingly complex task."
- Why unresolved: The study found that current multi-agent systems (e.g., FutureHouse, SLM-Matrix) performed poorly or failed the end-to-end task, often lagging behind simple single-agent or baseline approaches.
- What evidence would resolve it: Development of a multi-agent architecture that demonstrates superior F1 scores on the ChemX nanomaterial datasets compared to the reported single-agent baseline (F1 0.58–0.61).

### Open Question 3
- Question: Why do reasoning-enhanced models (e.g., GPT-5 Thinking) exhibit inferior performance compared to standard baselines in chemical data extraction?
- Basis in paper: [explicit] The results section notes that "GPT-5 Thinking model configured for extended reasoning demonstrates inferior performance... compared to standard GPT-5" (e.g., F1 0.02 vs 0.37 on Nanozymes).
- Why unresolved: The paper reports the empirical failure but does not analyze whether extended reasoning causes over-thinking, hallucination, or instruction-following drift in structured extraction tasks.
- What evidence would resolve it: An ablation study analyzing the chain-of-thought outputs of reasoning models during extraction to identify specific failure modes in context understanding or instruction adherence.

## Limitations

- Small-scale evaluation with only 6 scientific articles limits generalizability to broader chemical literature
- Reliance on GPT-5 and proprietary models creates reproducibility challenges
- SMILES extraction failure represents fundamental limitation requiring specialized OCSR integration
- Paper does not address potential safety policy variations across different LLM versions or providers

## Confidence

- **High Confidence:** Preprocessing pipeline's effectiveness (0.53→0.75 recall improvement) is well-supported by explicit methodology and consistent results
- **Medium Confidence:** Relative performance of GPT-5 vs GPT-5 Thinking is based on single-point measurements with potential variability
- **Low Confidence:** Claim that extended reasoning universally degrades extraction performance requires more extensive testing

## Next Checks

1. **Scale Validation:** Expand evaluation to at least 50 diverse chemical literature PDFs across multiple journals to assess generalizability of preprocessing benefits and performance patterns

2. **Tool Integration Experiment:** Implement DECIMER or similar OCSR tool within the single-agent pipeline to validate whether SMILES extraction F1 scores can be raised from near-zero to measurable performance

3. **Reasoning Strategy Ablation:** Systematically compare standard vs thinking prompts across different extraction task types (tables, text, figures) to determine whether reasoning overhead is consistently detrimental or task-dependent