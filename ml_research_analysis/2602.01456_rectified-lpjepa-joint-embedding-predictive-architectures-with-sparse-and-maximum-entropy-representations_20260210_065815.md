---
ver: rpa2
title: 'Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and
  Maximum-Entropy Representations'
arxiv_id: '2602.01456'
source_url: https://arxiv.org/abs/2602.01456
tags:
- rectified
- gaussian
- distribution
- generalized
- lpjepa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Rectified LpJEPA introduces a principled approach to learning\
  \ sparse and non-negative representations in self-supervised learning by aligning\
  \ features to a Rectified Generalized Gaussian (RGG) distribution via two-sample\
  \ sliced distribution matching. This design enables explicit control over expected\
  \ \u21130 sparsity through rectification while preserving maximum-entropy properties\
  \ under \u2113p norm constraints."
---

# Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations

## Quick Facts
- arXiv ID: 2602.01456
- Source URL: https://arxiv.org/abs/2602.01456
- Reference count: 40
- Primary result: Introduces Rectified LpJEPA for learning sparse, non-negative representations with controllable sparsity-performance tradeoffs in self-supervised learning

## Executive Summary
Rectified LpJEPA presents a novel approach to self-supervised learning that explicitly controls sparsity in learned representations while maintaining maximum-entropy properties. The method extends Joint-Embedding Predictive Architectures (JEPAs) by aligning features to a Rectified Generalized Gaussian distribution through two-sample sliced distribution matching. This framework enables precise control over expected ℓ0 sparsity via rectification operations, offering a principled alternative to implicit sparsity regularization methods.

The approach generalizes existing Gaussian-based JEPAs and provides a unified framework for studying sparse, task-agnostic encodings. Empirical results demonstrate competitive downstream accuracy across multiple image classification benchmarks while achieving dataset-adaptive sparsity patterns. The method shows particular promise in learning statistically independent features with higher entropy, making it valuable for applications requiring interpretable and efficient representations.

## Method Summary
Rectified LpJEPA builds upon the Joint-Embedding Predictive Architecture framework by introducing a rectification mechanism that aligns learned features with a Rectified Generalized Gaussian (RGG) distribution. The method employs two-sample sliced distribution matching to minimize the discrepancy between the empirical feature distribution and the target RGG distribution. This alignment is achieved through a carefully designed loss function that incorporates both the ℓp norm constraints and the rectification operation.

The core innovation lies in the ability to explicitly control expected ℓ0 sparsity through a mean shift parameter in the RGG distribution. By adjusting this parameter, practitioners can navigate the sparsity-performance tradeoff space, learning increasingly sparse representations as the mean shift decreases. The method preserves maximum-entropy properties under ℓp norm constraints, ensuring that the learned representations remain informative while being sparse. This approach generalizes previous work on Gaussian-based JEPAs by extending the distribution matching framework to accommodate non-Gaussian, rectified distributions.

## Key Results
- Competitive downstream accuracy across multiple image classification benchmarks
- Controllable sparsity-performance tradeoffs through mean shift parameter adjustment
- Learning of statistically independent features with higher entropy
- Dataset-adaptive sparsity patterns showing increased sparsity as mean shift decreases

## Why This Works (Mechanism)
The method works by leveraging the mathematical properties of Generalized Gaussian distributions and their rectified counterparts. The two-sample sliced distribution matching approach provides a computationally efficient way to align complex distributions without requiring explicit density estimation. The ℓp norm constraints ensure that the learned representations maintain certain regularity properties while the rectification operation enforces non-negativity and sparsity.

The maximum-entropy properties under ℓp constraints guarantee that the learned representations are as informative as possible given the sparsity requirements. This creates a principled framework for balancing representation efficiency with information retention. The mean shift parameter provides a direct control mechanism for navigating this tradeoff, allowing practitioners to adapt the method to specific application requirements.

## Foundational Learning

**Joint-Embedding Predictive Architectures (JEPAs)**: Self-supervised learning frameworks that learn representations by predicting context from target views. Why needed: Provides the base architecture for representation learning. Quick check: Verify understanding of contrastive vs predictive self-supervised learning.

**Generalized Gaussian Distributions**: Family of probability distributions that generalize Gaussian distributions with controllable kurtosis. Why needed: Enables modeling of non-Gaussian feature distributions. Quick check: Understand the relationship between shape parameter and distribution properties.

**Two-Sample Sliced Distribution Matching**: Technique for comparing distributions using projections onto random directions. Why needed: Provides efficient distribution alignment without density estimation. Quick check: Verify computational complexity compared to other distribution matching methods.

**Maximum-Entropy Principles**: Framework for selecting probability distributions that maximize uncertainty subject to constraints. Why needed: Ensures learned representations retain maximum information. Quick check: Understand the relationship between entropy and information content.

**ℓp Norms and Sparsity**: Mathematical framework for measuring vector magnitudes and inducing sparsity. Why needed: Provides the basis for sparsity control in representations. Quick check: Verify understanding of ℓ0 vs ℓ1 vs ℓ2 norms.

## Architecture Onboarding

**Component Map**: Input -> Encoder -> Feature Extractor -> Rectification -> Distribution Matching -> Representation

**Critical Path**: The encoder processes input data, the feature extractor produces initial representations, rectification enforces non-negativity and sparsity, and distribution matching aligns features with the target RGG distribution through the loss function.

**Design Tradeoffs**: The method balances between representation sparsity and information retention through the mean shift parameter. Higher sparsity may lead to faster inference and better interpretability but could reduce downstream task performance. The choice of ℓp norm affects both computational complexity and the nature of the learned representations.

**Failure Signatures**: Poor downstream performance may indicate insufficient information retention in the sparse representations. Excessive sparsity could lead to loss of discriminative features. Computational issues may arise from the sliced distribution matching if the number of projections is insufficient.

**First Experiments**:
1. Evaluate sparsity levels across different datasets to establish baseline patterns
2. Test downstream performance on standard image classification benchmarks
3. Compare with existing JEPA implementations to validate improvements

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the generalizability of the approach to non-image domains, the robustness of sparsity-performance tradeoffs under varying conditions, and the method's sensitivity to the mean shift parameter across different domain types. These questions highlight areas where further investigation is needed to fully understand the method's capabilities and limitations.

## Limitations

- Empirical nature of sparsity-performance tradeoff validation across diverse datasets
- Limited characterization of the relationship between sparsity levels and downstream task performance
- Need for more extensive investigation of sensitivity to mean shift parameter across domain types

## Confidence

High for mathematical formulation and distribution matching framework
Medium for empirical performance claims across limited image benchmarks
Low for generalizability claims to non-image domains and robustness under varying conditions

## Next Checks

1. Evaluate performance on non-image datasets (text, audio) to assess domain transferability
2. Conduct ablation studies varying mean shift parameter across different sparsity levels
3. Test learned representations' robustness to distribution shifts and adversarial perturbations