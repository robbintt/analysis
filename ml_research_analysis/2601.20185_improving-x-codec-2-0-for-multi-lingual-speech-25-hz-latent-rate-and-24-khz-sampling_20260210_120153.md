---
ver: rpa2
title: 'Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz
  Sampling'
arxiv_id: '2601.20185'
source_url: https://arxiv.org/abs/2601.20185
tags:
- rate
- audio
- speech
- x-codec-2
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work modifies X-Codec-2.0 to operate at a 25 Hz latent rate
  and 24 kHz sampling rate by increasing the decoder hop size to 960 samples and adding
  average pooling before quantization. This reduces the number of tokens per second
  while improving audio fidelity and perceptual quality.
---

# Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling

## Quick Facts
- arXiv ID: 2601.20185
- Source URL: https://arxiv.org/abs/2601.20185
- Authors: Husein Zolkepli
- Reference count: 9
- Primary result: +0.29 MOS improvement over baseline on multilingual Common Voice 17 test set

## Executive Summary
This work modifies X-Codec-2.0 to operate at a 25 Hz latent rate and 24 kHz sampling rate by increasing the decoder hop size to 960 samples and adding average pooling before quantization. This reduces the number of tokens per second while improving audio fidelity and perceptual quality. Evaluated on the multilingual Common Voice 17 test set using UTMOSv2, the proposed configuration achieves a +0.29 MOS improvement over the original X-Codec-2.0 baseline and attains the best reported performance among codecs operating at 25 Hz.

## Method Summary
The method involves initializing from the official X-Codec-2.0 checkpoint, inserting AvgPool1d(k=2, stride=2) before quantization, changing hop size to 960 samples, linearly interpolating decoder output weights/biases, freezing HuBERT and codec encoders, and fine-tuning only the decoder. The model was trained on ~16,000 hours of multilingual speech resampled to 24 kHz using Adam optimizer with cyclic learning rate and specific loss weight configurations.

## Key Results
- Achieves +0.29 MOS improvement over X-Codec-2.0 baseline on Common Voice 17
- Operates at 25 Hz latent rate (half the original 50 Hz) with 24 kHz output sampling
- Best reported performance among codecs operating at 25 Hz latent rate
- Reduces token rate while maintaining or improving perceptual quality

## Why This Works (Mechanism)

### Mechanism 1: Temporal Compression with Increased Information Density per Token
Reducing latent rate from 50 Hz to 25 Hz while increasing sampling rate from 16 kHz to 24 kHz improves perceptual quality when combined with decoder adaptation. The AvgPool1d layer compresses feature sequences by factor of two, forcing each discrete token to represent larger temporal window while decoder reconstructs at higher spectral resolution. Core assumption is pretrained encoder representations contain sufficient information for recovery despite 2× temporal compression.

### Mechanism 2: Weight Interpolation for Smooth Resolution Transfer
Linearly interpolating decoder output projection weights from hop size 320 to 960 preserves pretrained spectral characteristics during adaptation. The decoder's generator head output layer is resized via 1D linear interpolation, allowing pretrained model's learned spectral patterns to transfer to new output dimensionality without random reinitialization. Core assumption is interpolated weights provide better initialization than random weights for fine-tuning convergence.

### Mechanism 3: Selective Decoder Fine-Tuning with Frozen Encoders
Freezing semantic encoder (HuBERT) and codec encoder while fine-tuning only decoder is sufficient to adapt to new temporal configuration. Encoder pathway produces stable semantic representations while only decoder learns to map compressed 25 Hz latent sequences to 24 kHz waveforms. Core assumption is encoder representations are already sufficiently robust and language-agnostic, so adaptation burden falls entirely on decoder temporal upsampling.

## Foundational Learning

- **Neural Audio Codecs and Discrete Tokenization**: Understanding how waveforms become tokens (encoder → quantization → decoder) is prerequisite for modifying X-Codec-2.0. Quick check: Can you explain why a 25 Hz token rate reduces computational cost for autoregressive LLMs compared to 50 Hz?

- **HuBERT Semantic Features**: X-Codec-2.0 uses frozen HuBERT as semantic encoder. Need to understand HuBERT provides self-supervised speech representations capturing phonetic/linguistic content independent of sampling rate. Quick check: Why would HuBERT features remain valid when downstream decoder changes hop size?

- **Vector Quantization and Codebook Size**: The model uses single codebook of 65,536 entries. Information density per token increases when latent rate decreases, affecting downstream LLM perplexity. Quick check: If token rate halves but codebook size stays constant, does each token carry more or less information burden?

## Architecture Onboarding

- **Component map**: Input Audio (24 kHz) → Frozen HuBERT Encoder → Codec Encoder → AvgPool1d(k=2, stride=2) → Vector Quantization → Decoder (fine-tuned, hop size 960) → Output Audio (24 kHz)

- **Critical path**: The two modifications—AvgPool1d before quantization and decoder hop size 960 with interpolated weights—must align. If pooling factor doesn't match hop size ratio, temporal alignment breaks.

- **Design tradeoffs**: Token efficiency (25 Hz) vs. information bottleneck risk (each token must encode more); higher sampling rate (24 kHz) vs. decoder upsampling difficulty (960-sample hop); frozen encoder stability vs. potential encoder-decoder mismatch; large codebook (65,536) enables dense information encoding but may increase downstream LLM perplexity.

- **Failure signatures**: Muffled high-frequency output (decoder insufficiently trained or spectral loss misconfigured); temporal smearing (pooling too aggressive or decoder receptive field insufficient); poor generalization to expressive/noisy speech (training data dominated by clean Common Voice); convergence instability (weight interpolation may not help).

- **First 3 experiments**: 1) Reproduce baseline comparison on Common Voice 17 subset to verify evaluation pipeline correctness; 2) Ablate weight interpolation by initializing decoder output layer randomly vs. interpolated to compare convergence and final MOS; 3) Train small autoregressive model on 25 Hz tokens to measure perplexity against 50 Hz baseline.

## Open Questions the Paper Calls Out

1. Does linear interpolation of decoder weights provide better convergence stability or final audio fidelity compared to random initialization? The paper acknowledges this was not empirically validated in isolation and no ablation experiments were conducted.

2. How does the reduced 25 Hz latent rate and large vocabulary size impact perplexity and generation quality of downstream LLMs? The study focused solely on audio reconstruction metrics without testing tokens in autoregressive generative setup.

3. Does increasing decoder capacity compensate for loss of fine-grained acoustic details caused by 25 Hz compression bottleneck? The experiments kept core architecture fixed without exploring scaling decoder to recover lost temporal information.

## Limitations
- Unvalidated architectural assumption: Weight interpolation strategy lacks empirical validation; ablation experiments comparing interpolated vs random initialization were not conducted.
- Training data representativeness: Fine-tuning dataset from Malaysia-AI Multilingual-TTS repository is clean and expressive but may not represent real-world conditions, limiting generalizability.
- Quantization granularity impact: Paper does not analyze whether reduced temporal rate increases downstream LLM perplexity, only qualitatively discussed without quantitative validation.

## Confidence
- **High confidence**: Experimental methodology and evaluation protocol clearly specified; MOS improvement reproducible given access to evaluation pipeline; implementation details sufficient for replication.
- **Medium confidence**: Core architectural modifications straightforward and verifiable; claimed mechanisms lack direct empirical support; absence of ablation studies reduces confidence in individual contributions.
- **Low confidence**: Model's generalization beyond clean, expressive speech remains unproven; claims about robustness based on training data diversity rather than systematic testing across varied speech conditions.

## Next Checks
1. Ablate weight interpolation: Replicate fine-tuning with randomly initialized decoder output weights instead of interpolated weights, then compare convergence curves and final MOS scores.
2. Perplexity burden analysis: Train small autoregressive model on 25 Hz token sequences and measure perplexity compared to original 50 Hz tokens.
3. Generalization stress test: Evaluate model on noisy spontaneous speech datasets (e.g., TED-LIUM, AMI corpus) to verify robustness beyond clean Common Voice test set.