---
ver: rpa2
title: 'Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical
  Signal Classification'
arxiv_id: '2508.00963'
source_url: https://arxiv.org/abs/2508.00963
tags:
- features
- feature
- hybrid
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated whether adding more feature domains in
  multimodal deep learning for ECG classification improves performance or leads to
  diminishing returns. Five models were developed and evaluated: three unimodal (1D-CNN
  for time, 2D-CNN for time-frequency, and 1D-CNN-Transformer for frequency) and two
  multimodal (Hybrid 1: 1D-CNN + 2D-CNN; Hybrid 2: 1D-CNN + 2D-CNN + Transformer).'
---

# Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical Signal Classification

## Quick Facts
- arXiv ID: 2508.00963
- Source URL: https://arxiv.org/abs/2508.00963
- Authors: Timothy Oladunni; Alex Wong
- Reference count: 40
- Primary result: Adding more feature domains in multimodal deep learning does not always improve ECG classification; complementarity matters more than quantity

## Executive Summary
This study challenges the assumption that more feature domains always improve multimodal deep learning performance. Through systematic evaluation of ECG classification models using time, time-frequency, and frequency domains, the research demonstrates that fusing time and time-frequency features yields optimal results, while adding frequency domain features provides no additional benefit and may degrade performance. The findings reveal that multimodal learning success depends on the complementarity of information across domains rather than the number of domains fused.

## Method Summary
The study developed five deep learning models for ECG classification: three unimodal (1D-CNN for time, 2D-CNN for time-frequency, and 1D-CNN-Transformer for frequency) and two multimodal (Hybrid 1: 1D-CNN + 2D-CNN; Hybrid 2: 1D-CNN + 2D-CNN + Transformer). Models were evaluated using bootstrapping (1000 iterations) and Bayesian inference on the PTB-XL dataset. Mutual information and correlation analysis assessed feature complementarity, while ADASYN sampling addressed class imbalance.

## Key Results
- Hybrid 1 (1D-CNN + 2D-CNN) consistently outperformed the 2D-CNN baseline with p-values < 0.05 and Bayesian probabilities > 0.90
- Adding frequency domain features in Hybrid 2 resulted in marginal performance decline from 96% to 94% accuracy
- Time and time-frequency domains showed low linear correlation (+0.07) but significant mutual information (0.40), confirming complementary information
- ADASYN sampling improved minority class performance while maintaining intra-class variance (FID ~19.8)

## Why This Works (Mechanism)

### Mechanism 1
Fusing time-domain (1D-CNN) and time-frequency (2D-CNN) features improves classification because the domains offer complementary, non-redundant information. The 1D-CNN extracts temporal morphology (e.g., QRS complex duration), while the 2D-CNN extracts joint time-frequency patterns (scalograms). These features have low linear correlation (+0.07) but moderate mutual information (0.40), providing distinct perspectives that create a more robust decision boundary without overfitting.

### Mechanism 2
Adding a Transformer-based frequency domain to an already fused time/time-frequency model degrades performance due to representational redundancy and conflicting linear patterns. The frequency domain features (Transformer) exhibited negative linear correlation (-0.30) with 1D-CNN features and shared mutual information (0.44) without adding orthogonal discriminative power. This redundancy introduces conflicting gradient signals during optimization, leading to higher complexity without information gain.

### Mechanism 3
ADASYN improves model generalization for minority classes by generating synthetic samples that preserve statistical distribution of original physiological data. Unlike standard oversampling, ADASYN focuses on "hard" examples near decision boundaries, maintaining intra-class variance (FID score ~19.8) close to real data and preventing the model from memorizing identical minority samples.

## Foundational Learning

- **Concept: Mutual Information (MI) vs. Correlation**
  - Why needed: The paper uses MI to detect nonlinear complementarity that standard linear correlation misses
  - Quick check: If Domain A and Domain B have Pearson correlation of 0.0 but MI of 0.5, are they completely independent?

- **Concept: Signal Representations (Time vs. Frequency vs. Time-Frequency)**
  - Why needed: The entire architecture is built on the premise that 1D (Time), 2D (Time-Frequency), and FFT (Frequency) view the same signal differently
  - Quick check: Why does a 2D-CNN (scalogram) capture transient events better than a simple FFT plot?

- **Concept: Bayesian Inference in Model Evaluation**
  - Why needed: The paper uses Bayesian posterior probabilities (e.g., P(H1 > 2D-CNN) > 0.90) to prove significance rather than just p-values
  - Quick check: What does a Bayesian probability of 0.90 regarding difference in accuracy tell you about reliability of Hybrid 1's improvement?

## Architecture Onboarding

- **Component map:** ECG Images → Preprocessing (Filtering 0.5-45Hz, ADASYN) → 1D-CNN (Time) → Dense Vector → 2D-CNN (Time-Freq) → Dense Vector → Concatenate → Softmax
- **Critical path:** The "Hybrid 1" model is the target architecture, moving from image-based signal extraction → domain-specific encoders → intermediate feature fusion
- **Design tradeoffs:** Hybrid 1 vs. Hybrid 2: trading computational complexity for potential accuracy. The paper proves adding Transformer adds complexity without accuracy gains
- **Failure signatures:** Adding third modality causes ~2% drop in accuracy, signaling that new modality is not orthogonal to existing fused features
- **First 3 experiments:**
  1. Train individual 1D-CNN, 2D-CNN, and Transformer models to establish independent performance baselines (~91-94%)
  2. Calculate Mutual Information and Correlation matrices of extracted features before fusing; only fuse pairs with low correlation but non-zero MI
  3. Fuse selected pair (Hybrid 1) and apply bootstrapping (1000 iterations) to confirm statistical significance (p < 0.05) over best unimodal model

## Open Questions the Paper Calls Out
- Can optimization of the 1D-CNN-Transformer architecture mitigate representational redundancy observed when integrating frequency-domain features?
- Do advanced fusion strategies like attention mechanisms or Graph Neural Networks outperform feature concatenation method used in Hybrid 1?
- Does the Complementary Feature Domains framework maintain validity when applied to raw digital ECG data or other modalities like EEG?

## Limitations
- Study focuses on single public dataset (PTB-XL), potentially limiting generalizability across different ECG acquisition devices and populations
- Effect sizes for some comparisons (particularly Hybrid 1 vs. Hybrid 2) are modest, suggesting observed differences might not translate to clinically meaningful improvements
- MI analysis reveals nonlinear dependencies but does not establish causal relationships between specific feature patterns and diagnostic outcomes

## Confidence
- **High Confidence:** Hybrid 1 outperforms 2D-CNN baseline with strong statistical evidence (p < 0.05, Bayesian probabilities > 0.90) and multiple validation metrics
- **Medium Confidence:** Hybrid 2's frequency domain addition degrades performance is supported by data but relies on indirect inference about representational redundancy
- **Medium Confidence:** ADASYN sampling improvement is empirically demonstrated but lacks validation that synthetic samples preserve clinically relevant variability

## Next Checks
1. Test Hybrid 1 architecture on at least two additional ECG datasets with different acquisition protocols to verify time+time-frequency complementarity generalizes beyond PTB-XL
2. Conduct controlled ablation where frequency domain features are orthogonalized (via PCA or attention gating) before fusion to determine if negative interference can be mitigated
3. Map learned representations from 1D-CNN and 2D-CNN branches to clinically interpretable ECG markers to validate that multimodal fusion captures diagnostically relevant features rather than spurious correlations