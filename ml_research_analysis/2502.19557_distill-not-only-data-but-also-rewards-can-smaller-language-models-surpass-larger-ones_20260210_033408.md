---
ver: rpa2
title: 'Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass
  Larger Ones?'
arxiv_id: '2502.19557'
source_url: https://arxiv.org/abs/2502.19557
tags:
- student
- teacher
- reward
- responses
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel knowledge distillation method that
  transfers both responses and rewards from large language models (LLMs) to smaller
  student models. The key innovation is a self-supervised reward learning mechanism
  that generates pseudo-rewards by comparing the inherent structure of teacher and
  student responses, eliminating the need for external reward signals.
---

# Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass Larger Ones?

## Quick Facts
- arXiv ID: 2502.19557
- Source URL: https://arxiv.org/abs/2502.19557
- Reference count: 19
- Primary result: Student models can surpass teachers using self-supervised reward learning during distillation

## Executive Summary
This paper introduces a novel knowledge distillation method that transfers both responses and rewards from large language models to smaller student models. The key innovation is a self-supervised reward learning mechanism that generates pseudo-rewards by comparing the inherent structure of teacher and student responses, eliminating the need for external reward signals. The approach combines supervised fine-tuning (SFT) with reinforcement learning (RL) guided by the learned reward model. Experiments on GSM8K and MMLU-PRO demonstrate consistent performance improvements over traditional SFT-based methods, with student models surpassing their teachers in certain configurations.

## Method Summary
The proposed method, SDREAM (Self-Distilled Reward-based rEward learning And distillaTion framework), operates through two key phases: first, a self-supervised reward learning phase where pseudo-rewards are generated by comparing the structural differences between teacher and student responses; second, a distillation phase that combines SFT with RL guided by the learned reward model. The self-supervised reward generation eliminates the need for human preference data or external reward signals, making the approach more scalable. The method is particularly effective when teacher supervision is weaker, achieving up to 9.38% improvement on MMLU-PRO with a 1B student model compared to standard SFT.

## Key Results
- Llama3-3B students achieved 83.02% accuracy on GSM8K (vs. 80.97% for teacher)
- Llama3-3B students achieved 40.02% on MMLU-PRO (vs. 39.88% for teacher)
- Up to 9.38% improvement on MMLU-PRO with a 1B student model when teacher supervision is weaker

## Why This Works (Mechanism)
The self-supervised reward learning mechanism works by generating pseudo-rewards through structural comparison between teacher and student responses. This approach captures not just the final answer but the reasoning process and intermediate steps, allowing the student model to learn more nuanced behaviors than simple response copying. By combining SFT with RL guided by these learned rewards, the student can optimize for both correctness and reasoning quality, leading to performance that can exceed the teacher in certain scenarios.

## Foundational Learning
- Knowledge Distillation: Transferring knowledge from large to small models; needed to compress LLM capabilities into more efficient models
- Self-Supervised Learning: Generating training signals without human annotation; needed to eliminate dependency on external reward data
- Reinforcement Learning: Optimizing model behavior through rewards; needed to fine-tune student responses beyond simple copying
- Reward Modeling: Creating proxy functions for human preferences; needed to guide RL without expensive human feedback
- Structured Comparison: Analyzing response hierarchies and reasoning paths; needed to generate meaningful pseudo-rewards

## Architecture Onboarding

**Component Map:**
Teacher Model -> Pseudo-Reward Generator -> Reward Model -> Student Model (SFT + RL)

**Critical Path:**
1. Teacher generates responses
2. Student generates responses
3. Structural comparison generates pseudo-rewards
4. Reward model learns from pseudo-rewards
5. Student fine-tuned via SFT and RL

**Design Tradeoffs:**
- Self-supervised vs. human-annotated rewards (scalability vs. potential quality)
- Computational overhead of reward learning phase
- Balance between SFT and RL contributions

**Failure Signatures:**
- Poor pseudo-reward generation leading to ineffective RL guidance
- Overfitting to structural patterns that don't generalize
- Computational inefficiency in reward learning phase

**First Experiments:**
1. Compare student performance with and without reward learning phase
2. Test different reward generation thresholds for structural comparison
3. Evaluate impact of varying SFT-to-RL ratio in final training

## Open Questions the Paper Calls Out
None

## Limitations
- No comparison against established reward-based distillation methods like RLHF with human preference data
- Evaluation restricted to GSM8K and MMLU-PRO datasets, limiting generalizability
- Computational overhead from reward learning phase not addressed
- Long-term stability of student models trained with pseudo-rewards versus human-annotated rewards is unknown

## Confidence
**High confidence:** The core technical contribution of self-supervised reward learning through structural comparison is clearly demonstrated and reproducible. The experimental setup and baseline comparisons are well-defined.

**Medium confidence:** The claim that student models can surpass teachers is supported by the presented data, though the conditions under which this occurs (particularly when teacher supervision is weaker) need further exploration. The general effectiveness of the approach across different model sizes and domains is reasonably supported but not comprehensively validated.

**Low confidence:** The assertion that this method is superior to all existing reward-based distillation techniques cannot be verified without direct comparisons to methods like RLHF or other preference learning approaches.

## Next Checks
1. Compare performance against traditional RLHF-based distillation using human preference data to establish the relative effectiveness of self-supervised rewards.

2. Test the approach across a broader range of tasks including open-ended generation, code completion, and multilingual benchmarks to assess generalizability.

3. Conduct ablation studies to isolate the contribution of reward learning versus SFT, and analyze the computational overhead introduced by the reward modeling phase.