---
ver: rpa2
title: 'Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning
  for Open-Ended Tasks'
arxiv_id: '2506.13351'
source_url: https://arxiv.org/abs/2506.13351
tags:
- reward
- arxiv
- reasoning
- tokens
- rubric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Direct Reasoning Optimization (DRO) addresses the challenge of\
  \ applying reinforcement learning to open-ended, long-form reasoning tasks where\
  \ traditional verifiable reward signals are unavailable. DRO introduces a novel\
  \ token-level dense reward called Reasoning Reflection Reward (R3) that selectively\
  \ emphasizes reasoning-reflective tokens\u2014those whose likelihoods show high\
  \ variability under different chain-of-thought prefixes."
---

# Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks

## Quick Facts
- arXiv ID: 2506.13351
- Source URL: https://arxiv.org/abs/2506.13351
- Reference count: 40
- Primary result: DRO achieves up to 63.7% win rate on ParaRev and reaches target performance 2–3× faster than baselines on four diverse reasoning datasets

## Executive Summary
Direct Reasoning Optimization (DRO) introduces a novel approach to fine-tuning LLMs on open-ended, long-form reasoning tasks where traditional verifiable rewards are unavailable. The framework combines Reasoning Reflection Reward (R3)—a token-level dense reward that emphasizes reasoning-reflective tokens based on cross-rollout variance—with rubric-gating that enforces essential task constraints through hard acceptance/rejection checks. This design enables stable RL training without requiring explicit reward rubrics, addressing the fundamental challenge of reward hacking in open-ended reasoning tasks.

DRO demonstrates consistent performance improvements across four diverse datasets including paragraph revision, medical QA, legal contract review, and financial reasoning, achieving up to 84.5 macro-F1 on ContractNLI and 68.4% accuracy on FinQA. The framework reaches target performance 2–3× faster than competing methods while maintaining improved rubric compliance without explicit rubric scoring, showing particular strength on tasks with substantial reasoning-reflective tokens.

## Method Summary
DRO fine-tunes LLMs using GRPO with a novel token-level dense reward called Reasoning Reflection Reward (R3) that selectively emphasizes tokens whose likelihoods vary substantially across different chain-of-thought prefixes. The framework combines R3 with rubric-gating, which enforces essential task constraints through hard acceptance/rejection checks at the rollout group level, and variance-based filtering to improve training stability by rejecting low-variance rollout groups. Training uses Qwen3-14B (DeepSeek-R1-Distill-Qwen-7B for FinQA) with 16 rollouts per query, temperature 1.0, and top_p 0.95, optimized via HuggingFace TRL with DeepSpeed and vLLM.

## Key Results
- Achieves 63.7% win rate on ParaRev, 57.6% on RaR-Medicine, 84.5 macro-F1 on ContractNLI, and 68.4% accuracy on FinQA
- Reaches target performance 2–3× faster than competing methods across all datasets
- Improves rubric compliance without explicit rubric scoring, with decreasing rubric rejection rates over training
- Demonstrates particular strength on tasks with substantial reasoning-reflective tokens while maintaining performance on structured outputs

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Reflective Token Emphasis via R3
- Claim: Weighting reference tokens by cross-rollout variance amplifies informative gradient signal while suppressing noise from uninformative tokens.
- Mechanism: R3 computes per-token self-certainty (probability the model assigns to each reference token under each CoT prefix), then identifies reasoning-reflective tokens as those whose certainties vary substantially across rollouts. A softmax-weighted aggregation emphasizes these tokens, and probability clipping ([λ_low, λ_high]) removes extreme-value noise. This yields reward signals whose groupwise z-scores correlate more strongly with reasoning quality than plain averaging.
- Core assumption: Tokens with high variance across CoT prefixes are causally influenced by reasoning quality rather than lexical/style artifacts.
- Evidence anchors: [abstract] "R3 selectively identifies and emphasizes key tokens in the reference outcome that reflect the influence of the model's preceding chain-of-thought reasoning." [Section 3.2, Eq. 2] Formal definition: r_i^{R3} = Σ_j softmax_weight_j × clip(π(y_j|q, ĉ_i, y_{<j}), λ_low, λ_high), where weights are σ_j^{ω} normalized. [Section 5.3] R3 reaches target performance 2–3× faster than Avg Prob (e.g., ParaRev: 96 vs 288 steps). [corpus] Neighbor paper "Grad2Reward" similarly addresses sparse-to-dense reward conversion for open-ended tasks, suggesting convergent validation of dense-reward approaches, though no direct comparison.

### Mechanism 2: Rubric-Gating as Hard Feasibility Constraints
- Claim: Rejecting entire rollout groups that violate essential task constraints prevents reward hacking without requiring rubrics as dense rewards.
- Mechanism: Query-specific rubrics (yes/no checks) are generated and validated against the reference answer. Two gates operate at rollout-group level: (1) Coverage gate requires ≥μ rollouts satisfy every rubric; (2) Consistency gate requires top ρ% rollouts by R3 each satisfy ≥ν% of rubrics. Rejected groups are excluded from gradient updates but queries remain in training.
- Core assumption: Rubrics can reliably encode minimal feasibility criteria; violations indicate systematic reasoning failures rather than benign exploration.
- Evidence anchors: [abstract] "Rubric-gating...enforces essential task constraints through hard acceptance/rejection checks at the rollout group level." [Section 4.1] "We enforce these principles via rubric-gating, which implements rejection of rollout groups based on essential query-specific rubrics." [Section 5.4, Fig. 4] Rubric rejection rate decreases over training, indicating improved compliance without explicit rubric scoring. [corpus] Limited direct corpus validation; neighbor "Semantically-Aware Rewards" proposes alternative semantic-reward framing but doesn't test hard gating.

### Mechanism 3: Variance-Based Filtering for Sample Efficiency
- Claim: Pre-filtering low-variance rollout groups stabilizes training by preventing spurious z-score amplification.
- Mechanism: Compute high-variance score σ̄_G as mean std. dev. across top-10% tokens by variance. Reject groups below threshold τ_t. Static pre-filtering (retain top q% by variance plus ~10% hard cases) outperforms dynamic on-the-fly filtering at scale.
- Core assumption: Low-variance groups provide insufficient discriminative signal; their inclusion adds noise rather than diversity.
- Evidence anchors: [Section 4.2] "Even small absolute differences can be spuriously amplified in the advantages when the underlying variance is low." [Section 5.5] Quality beats quantity: ~12–18% retention rates yield best performance; training on full corpora precipitates collapse. [Appendix L, Fig. 6] Filtered training shows faster convergence and higher final token probability. [corpus] No direct corpus comparison; mechanism is DRO-specific.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: DRO builds on GRPO's groupwise advantage computation (Eq. 1: Â_i = (r_i - mean) / std). Understanding how z-scoring amplifies relative differences is essential for seeing why variance filtering and R3's contrast sharpening matter.
  - Quick check question: Given rewards [0.51, 0.52, 0.53] for three rollouts, compute the GRPO advantages. What happens if std is very small?

- **Self-Certainty and Token-Level Probabilities**
  - Why needed here: R3's core signal is π(y_j | q, ĉ_i, y_{<j})—the model's probability of each reference token given a sampled CoT. You must understand autoregressive conditioning to debug R3 computations.
  - Quick check question: For a reference token y_j = "therefore", explain how its probability might differ under two CoT prefixes: one with correct reasoning vs. one with a logical error.

- **Reward Hacking in RL for Language**
  - Why needed here: Rubric-gating exists specifically to prevent models from gaming dense rewards (e.g., outputting fluent no-revision responses that score high on self-certainty). Recognizing failure modes guides debugging.
  - Quick check question: A model trained with only token-level self-certainty starts outputting generic, low-information responses. What went wrong and how might rubric-gating help?

## Architecture Onboarding

- **Component map**: Query → [CoT Sampling (k rollouts)] → [Rubric Generation + Validation] → [Rubric-Gating: Coverage + Consistency Checks] → [R3 Computation: token probs → variance → softmax weights → clipped aggregate] → [Variance-Based Filtering: σ̄_G > τ?] → [GRPO Advantage + Policy Update]

- **Critical path**:
  1. Implement self-certainty extraction: forward pass on (query, CoT_i, reference) to get π(y_j | ·) for all tokens.
  2. Implement variance computation across k rollouts per query (requires storing per-token probs for all rollouts before aggregation).
  3. Implement R3 aggregation with softmax weighting (ω ∈ [1, 2]) and clipping (λ_low ≈ 0.05, λ_high ≈ 0.85 based on paper's inspection).
  4. Add rubric-gating: LLM generates rubrics, validates against reference, checks coverage (μ=1) and consistency (ρ=25%, ν=60%).
  5. Add variance filtering: compute σ̄_G, apply threshold (static pre-filter recommended for large corpora).

- **Design tradeoffs**:
  - **Emphasis ω**: Higher ω accelerates early learning but risks over-sharpening onto single noisy tokens; ω > 4 caused performance collapse in experiments.
  - **Clipping thresholds**: Wider bands retain more tokens but introduce noise; tighter bands may discard informative mid-range tokens.
  - **Filtering aggressiveness**: q ≈ 10% retention works for 10K+ samples; smaller corpora need higher retention or dynamic thresholds.
  - **Rubric count**: Max 10 rubrics per query (Section E); more rubrics increase judge cost without proportional gain.

- **Failure signatures**:
  - **No-revision collapse**: Policy entropy drops to near-zero; outputs become identical to inputs. Check rubric-gating rejection rate—if near 0%, gates may be too loose.
  - **Spurious advantage amplification**: Sudden gradient spikes with low reward variance. Check σ̄_G distribution; tighten filtering threshold.
  - **Rubric drift**: Generated rubrics don't match reference (ill-posed constraints). Validate rubric satisfaction on reference before training.

- **First 3 experiments**:
  1. **Sanity check R3 vs. plain aggregation**: On a held-out query, sample 16 rollouts, compute R3 and Avg Prob rewards, manually rank CoT quality. Confirm R3 advantages correlate better with quality (replicate Fig. 2 analysis).
  2. **Ablate rubric-gating**: Train with R3 only (no gating) on ParaRev subset. Monitor for no-revision collapse and rubric rejection rate over time.
  3. **Validate variance filtering impact**: Compare training curves with vs. without filtering on a medium-sized dataset (e.g., RaR-Medicine). Measure steps to target performance and final metric.

## Open Questions the Paper Calls Out

- **How can rubric-gating be adapted for structured output formats (e.g., JSON schemas) where comprehensive semantic rubrics are difficult to express?**
  - Basis in paper: [explicit] The authors state: "On ContractNLI, the impact is limited...because responses are JSON-formatted and validation depends on checking specific JSON properties, making comprehensive rubric checks difficult to express."
  - Why unresolved: The current rubric-gating approach assumes natural language outputs where semantic constraints can be phrased as yes/no questions; structured formats require different constraint encoding mechanisms.
  - What evidence would resolve it: Developing and testing alternative constraint mechanisms for structured outputs (e.g., schema validation gates, property-specific checks) that recover the performance gains seen on natural language tasks.

- **How does the optimal temperature parameter ω in R3 relate to task characteristics such as reasoning-reflective token fraction and sequence length?**
  - Basis in paper: [explicit] The authors note: "Very high values (ω > 4) reduce peak performance and hasten collapse, as the softmax-based weighting over-sharpens and concentrates weight onto a tiny subset, sometimes a single, reasoning-reflective token, magnifying noise."
  - Why unresolved: The paper only provides empirical guidance (ω ∈ [1,2]) without a principled method for selecting ω based on dataset properties.
  - What evidence would resolve it: A systematic study varying ω across tasks with different reasoning-reflective token fractions, establishing a quantitative relationship or adaptive selection criterion.

- **How does aggressive variance-based filtering (retaining only 10-18% of training samples) affect generalization to edge cases and out-of-distribution queries?**
  - Basis in paper: [inferred] The paper reports that variance-based filtering "yields effective retention rates of 18%, 16%, and 12.5% for ParaRev, RaR-Medicine, and FinQA" and that "quality outweighs quantity," but does not evaluate whether filtered-out hard cases affect robustness.
  - Why unresolved: While filtering improves sample efficiency, the discarded low-variance groups may contain important edge cases needed for robust generalization.
  - What evidence would resolve it: Evaluating DRO-trained models on adversarial or out-of-distribution test sets, comparing models trained with and without variance filtering on hard-case retention.

## Limitations
- R3 performance degrades on structured tasks like FinQA where reasoning-reflective tokens constitute <5% of sequences
- Rubric-gating effectiveness limited for structured outputs (e.g., ContractNLI JSON) where semantic rubrics are difficult to express
- Variance-based filtering parameters appear dataset-specific and lack systematic guidance for new tasks

## Confidence
- **High Confidence:** The core mechanism of R3 as a variance-weighted token-level reward is well-supported by empirical evidence showing 2-3× faster convergence across multiple datasets. The mathematical formulation and implementation details are clearly specified.
- **Medium Confidence:** The rubric-gating mechanism shows consistent improvements in preventing reward hacking and improving compliance, but its effectiveness is highly task-dependent and requires careful rubric design that isn't fully specified.
- **Low Confidence:** The variance-based filtering mechanism's optimal parameters (retention rates, thresholds) appear dataset-specific and lack systematic guidance for adaptation to new tasks.

## Next Checks
1. **Domain Transfer Validation:** Apply DRO to a structured reasoning task (e.g., code generation or structured JSON output) and systematically evaluate R3 performance when reasoning-reflective tokens constitute <5% of the sequence. Compare convergence speed and final performance against baselines to identify break conditions.
2. **Rubric Robustness Analysis:** Generate adversarial rubric sets for the same task and measure DRO's sensitivity to rubric quality. Test whether the framework can distinguish between essential and arbitrary constraints, and evaluate performance degradation when rubrics are poorly specified.
3. **Parameter Sensitivity Grid Search:** Conduct a systematic grid search across ω (1.0-4.0), clipping thresholds (λ_low=0.01-0.1, λ_high=0.8-0.99), and filtering retention rates (5%-25%) on a medium-sized dataset to identify stable parameter regions and failure modes.