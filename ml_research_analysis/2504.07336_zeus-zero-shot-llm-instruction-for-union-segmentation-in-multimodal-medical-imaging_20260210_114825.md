---
ver: rpa2
title: 'Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical
  Imaging'
arxiv_id: '2504.07336'
source_url: https://arxiv.org/abs/2504.07336
tags:
- medical
- segmentation
- image
- text
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Zeus, a novel end-to-end union segmentation
  framework that mimics real-world clinical diagnosis by integrating multimodal medical
  images with text-based domain knowledge. The method leverages frozen large language
  models (LLMs) and vision-language models (LVLMs) to generate zero-shot text instructions
  from multimodal radiology images (e.g., T1-w or T2-w MRI and CT), thereby capturing
  modality-specific features and semantic understanding without requiring paired vision-language
  datasets.
---

# Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging

## Quick Facts
- arXiv ID: 2504.07336
- Source URL: https://arxiv.org/abs/2504.07336
- Authors: Siyuan Dai; Kai Ye; Guodong Liu; Haoteng Tang; Liang Zhan
- Reference count: 15
- Primary result: Novel end-to-end union segmentation framework integrating multimodal medical images with text-based domain knowledge through frozen LLMs and LVMs

## Executive Summary
Zeus introduces a zero-shot segmentation framework that mimics clinical diagnosis by combining multimodal medical images with text-based domain knowledge. The method leverages frozen large language models (LLMs) and vision-language models (LVLMs) to generate text instructions from multimodal radiology images, capturing modality-specific features without requiring paired vision-language datasets. Using a SAM-based mask decoder, Zeus accepts image-instruction pairs for segmentation and employs early, hybrid, and late fusion strategies for multimodal learning. Experiments on three public datasets demonstrate superior performance compared to baselines while maintaining a small trainable parameter size.

## Method Summary
Zeus is an end-to-end union segmentation framework that integrates multimodal medical images with text-based domain knowledge. The method uses frozen LLMs and LVLMs to generate zero-shot text instructions from multimodal radiology images (T1-w or T2-w MRI and CT), capturing modality-specific features and semantic understanding. A SAM-based mask decoder accepts image-instruction pairs for segmentation. The framework employs early, hybrid, and late fusion strategies for multimodal learning. Experiments were conducted on MSD-Prostate, MSD-Brain, and CHAOS datasets, comparing against influential baselines and demonstrating superior Dice Similarity Coefficient (DSC) and mean Intersection over Union (mIoU) performance.

## Key Results
- Achieves highest DSC and mIoU across all tasks on MSD-Prostate, MSD-Brain, and CHAOS datasets
- Maintains small trainable parameter size compared to baseline methods
- Ablation studies confirm effectiveness of instruction generation module and cross-modal alignment

## Why This Works (Mechanism)
Zeus works by mimicking real-world clinical diagnosis processes through integration of multimodal medical images with text-based domain knowledge. The framework leverages frozen LLMs and LVLMs to generate zero-shot text instructions that capture modality-specific features and semantic understanding. This approach eliminates the need for paired vision-language datasets while maintaining high segmentation accuracy. The SAM-based mask decoder processes image-instruction pairs, enabling effective multimodal learning through fusion strategies. The method bridges the gap between visual and textual medical knowledge, providing a comprehensive approach to medical image segmentation.

## Foundational Learning
- **Large Language Models (LLMs)**: AI models trained on vast text data for understanding and generating human language; needed for processing and generating medical text instructions; quick check: validate text generation quality on medical domain-specific prompts
- **Vision-Language Models (LVLMs)**: Models that process and understand both visual and textual information; needed for multimodal feature extraction from medical images; quick check: evaluate cross-modal alignment accuracy
- **SAM-based Mask Decoder**: Segment Anything Model used for segmentation tasks; needed for efficient mask generation from image-instruction pairs; quick check: measure segmentation accuracy and speed
- **Early, Hybrid, and Late Fusion**: Strategies for combining multimodal information; needed to optimize information flow and segmentation performance; quick check: compare performance across fusion strategies
- **Zero-shot Learning**: Learning approach that doesn't require task-specific training data; needed to eliminate dependency on paired vision-language datasets; quick check: validate performance across unseen medical conditions
- **Dice Similarity Coefficient (DSC)**: Metric for evaluating segmentation accuracy; needed to quantify overlap between predicted and ground truth masks; quick check: calculate DSC across different anatomical structures

## Architecture Onboarding

**Component Map**: Multimodal Images → LLM/LVLM → Text Instructions → SAM Decoder → Segmentation Output

**Critical Path**: Multimodal image input → Feature extraction by LVLM → Text instruction generation by LLM → Fusion strategy processing → SAM-based mask decoding → Final segmentation

**Design Tradeoffs**: Uses frozen LLMs/LVLMs to avoid fine-tuning requirements but may limit domain-specific accuracy; employs multiple fusion strategies to optimize performance but increases complexity; maintains small trainable parameters but relies on external model capabilities

**Failure Signatures**: Poor segmentation when LLM/LVLM fail to generate accurate text instructions; reduced performance when cross-modal alignment is weak; degradation when fusion strategies don't effectively combine multimodal features

**3 First Experiments**:
1. Test instruction generation quality on held-out multimodal images not seen during development
2. Compare segmentation performance across early, hybrid, and late fusion strategies on a single dataset
3. Evaluate cross-modal alignment accuracy between generated instructions and input images

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Clinical applicability not validated on diverse real-world clinical data
- Inter-scanner variability not accounted for in experiments
- Computational efficiency in real-time clinical settings not evaluated

## Confidence

**Performance Claims**: High confidence for controlled experimental conditions on MSD-Prostate, MSD-Brain, and CHAOS datasets

**Generalizability Claims**: Low confidence without validation on diverse clinical populations and imaging conditions

**Method Effectiveness**: Medium confidence based on ablation studies confirming instruction generation and cross-modal alignment

## Next Checks
1. External validation on multi-center clinical datasets with varying scanner manufacturers and protocols to assess real-world robustness
2. Evaluation of segmentation performance across different disease severities and anatomical variations not represented in current datasets
3. Assessment of computational efficiency and latency in clinical workflows to determine practical deployment feasibility