---
ver: rpa2
title: Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s
  for Software Requirements Classification
arxiv_id: '2509.13868'
source_url: https://arxiv.org/abs/2509.13868
tags:
- requirements
- classification
- task
- llms
- deepseek
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates prompt-based large language models (LLMs)
  for software requirements classification, aiming to reduce reliance on large annotated
  datasets. We evaluate five LLMs across three tasks using zero-shot, few-shot, persona,
  and chain-of-thought prompting techniques.
---

# Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification

## Quick Facts
- arXiv ID: 2509.13868
- Source URL: https://arxiv.org/abs/2509.13868
- Reference count: 40
- Primary result: Prompt-based LLMs with few-shot examples outperform zero-shot and match fine-tuned BERT baselines for software requirements classification.

## Executive Summary
This study investigates whether prompt-based large language models can effectively classify software requirements without requiring large annotated datasets. The research evaluates five different LLMs across three classification tasks using zero-shot, few-shot, persona, and chain-of-thought prompting techniques. Results demonstrate that few-shot prompting significantly improves performance over zero-shot approaches, with the best configurations (particularly Gemini 5FS with persona) achieving competitive or superior results compared to a fine-tuned BERT baseline. The findings suggest prompt-based LLMs offer a practical, scalable alternative for requirements classification that substantially reduces data annotation requirements.

## Method Summary
The study evaluates five LLMs (Claude 3 Haiku, DeepSeek-V3, Gemini 2.0 Flash, GPT-4 Turbo, and Llama-3.2 3B Instruct) across three tasks: binary classification (functional vs. non-functional requirements), multi-class classification (10 NFR types), and binary security classification. Data comes from PROMISE NFR dataset (625 requirements) and SecReq dataset (510 requirements), split 8:1:1 with stratification. The approach tests zero-shot, few-shot (1, 3, 5 examples), persona, and chain-of-thought prompting. Performance is measured using Macro-F1 score with Scott-Knott ESD statistical ranking, compared against a fine-tuned BERT baseline.

## Key Results
- Few-shot prompting consistently outperforms zero-shot approaches across all tasks and models.
- Gemini 5FS with persona achieved the highest macro-F1 score of 0.92 on the FR-NFR task.
- All prompt-based LLMs outperformed BERT on the MC-NFR task, while showing mixed results on the Sec-NonSec task.

## Why This Works (Mechanism)

### Mechanism 1: In-Context Pattern Induction via Few-Shot Examples
Providing a small number of labeled examples enables the model to infer classification boundaries and task-specific linguistic patterns more effectively than zero-shot instructions alone. Few-shot examples serve as in-context learning, conditioning the attention mechanism on specific input-output mappings within the prompt context.

### Mechanism 2: Domain Alignment via Persona Framing
Augmenting prompts with a specific persona (e.g., "Requirements Analyst") improves classification accuracy by activating domain-specific reasoning pathways. Persona instructions likely constrain the model's generative distribution, reducing generic responses in favor of formal, technical outputs.

### Mechanism 3: Generalization via Fixed Inference Weights
Prompt-based LLMs can match or exceed fine-tuned baselines by leveraging general linguistic understanding rather than overfitting to specific dataset statistics. Unlike fine-tuning which adjusts weights, prompt-based inference utilizes frozen pre-trained weights encoding broader syntactic and semantic rules.

## Foundational Learning

- **Concept: Macro-F1 Score for Imbalanced Data**
  - Why needed: The software engineering datasets are highly imbalanced. Accuracy is misleading as it favors majority class.
  - Quick check: If a model classifies all requirements as "Non-Functional" simply because they are majority, would high accuracy reflect good performance? (Answer: No, Macro-F1 penalizes this by averaging per-class scores).

- **Concept: Zero-Shot vs. Few-Shot Prompting**
  - Why needed: The core trade-off evaluated is the cost of annotation. Understanding this distinction is necessary to interpret the "data scarcity" solution.
  - Quick check: Does "Few-Shot" require updating the model's weights? (Answer: No, it strictly modifies the input context window).

- **Concept: Scott-Knott ESD Test**
  - Why needed: The study relies on this statistical test to rank models. Understanding that it groups models into statistically distinct "ranks" is vital for reading results.
  - Quick check: If Model A has slightly higher F1 score than Model B, but they are in the same Scott-Knott rank, is Model A definitively better? (Answer: No, the difference is not statistically significant).

## Architecture Onboarding

- **Component map:** Input Layer (Raw Requirement Text + Prompt Template) -> Orchestration Layer (Script to inject examples and format prompts) -> Inference Engine (API calls to LLMs) -> Evaluation Layer (Macro-F1 calculator and Scott-Knott ESD statistical ranking module)

- **Critical path:** 1) Stratified split of data (8:1:1), 2) Constructing specific prompt format, 3) Extracting class label from LLM's generation, 4) Aggregating results for statistical testing against BERT baseline

- **Design tradeoffs:** Latency vs. Performance (API introduces network latency), Cost vs. Stability (Gemini 5FS vs. local models), Prompt Complexity (CoT increases token usage but not consistently improves results)

- **Failure signatures:** Negative Few-Shot Learning (Llama performance degrades with few-shot), Persona Overfitting (slight dips when persona misaligns with dataset linguistic quirks)

- **First 3 experiments:**
  1. Baseline Replication: Run Gemini 5FS configuration on PROMISE FR-NFR dataset to verify ~0.92 F1-score and compare inference time against BERT-base
  2. Ablation on "k" (Shot count): Test DeepSeek sensitivity on MC-NFR task by varying k (1, 3, 5, 10) to see if performance saturates
  3. Persona Stress Test: Swap "Requirements Analyst" persona for "Helpful Assistant" on Sec-NonSec task to quantify persona contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on random sampling for few-shot examples without fixed seed introduces variability in performance results
- Persona prompting effectiveness depends on specific "Requirements Analyst" framing which may not generalize across domains
- Open-access appendix containing exact prompt templates is referenced but not fully disclosed in main paper

## Confidence

- **High Confidence:** Few-shot prompting consistently outperforms zero-shot across all tasks and models
- **Medium Confidence:** Persona prompting provides consistent gains when combined with few-shot examples
- **Medium Confidence:** LLMs matching or exceeding BERT baseline performance without fine-tuning

## Next Checks

1. Shot Count Sensitivity Analysis: Systematically vary k (1, 3, 5, 10) examples for DeepSeek on MC-NFR task to identify performance saturation points
2. Persona Ablation Study: Replace "Requirements Analyst" persona with "Helpful Assistant" on Sec-NonSec task to isolate persona contribution
3. Llama Capacity Test: Conduct controlled experiments comparing Llama's few-shot performance against zero-shot across different context window sizes to verify documented capacity limitations