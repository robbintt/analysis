---
ver: rpa2
title: 'ADEPTS: A Capability Framework for Human-Centered Agent Design'
arxiv_id: '2507.15885'
source_url: https://arxiv.org/abs/2507.15885
tags:
- agent
- example
- user
- tier
- tiers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ADEPTS, a capability framework for human-centered\
  \ agent design that defines six core user-facing capabilities\u2014Actuation, Disambiguation,\
  \ Evaluation, Personalization, Transparency, and Safety\u2014derived from principles\
  \ for creating understandable, controllable, and trustworthy AI agents. Unlike existing\
  \ frameworks that focus on interface behaviors, internal pipelines, or high-level\
  \ governance, ADEPTS sits at the interface between technical and experience development,\
  \ providing a concise, actionable vocabulary for AI researchers, designers, engineers,\
  \ and policy reviewers."
---

# ADEPTS: A Capability Framework for Human-Centered Agent Design

## Quick Facts
- arXiv ID: 2507.15885
- Source URL: https://arxiv.org/abs/2507.15885
- Reference count: 12
- This paper introduces ADEPTS, a capability framework for human-centered agent design that defines six core user-facing capabilities derived from principles for creating understandable, controllable, and trustworthy AI agents.

## Executive Summary
ADEPTS is a capability framework that provides a concise, actionable vocabulary for AI researchers, designers, engineers, and policy reviewers to assess and develop human-centered agents. The framework defines six core capabilities—Actuation, Disambiguation, Evaluation, Personalization, Transparency, and Safety—each with progressive competency tiers. ADEPTS sits at the interface between technical and experience development, enabling teams to track progress and identify capability gaps. The authors aim for this framework to accelerate improvement of user-relevant agent capabilities, ease experience design, and provide a shared language for discussing AI agent development progress.

## Method Summary
The paper defines a taxonomic framework rather than a computational model. ADEPTS decomposes user-facing concerns into six discrete capabilities with tiered competency levels (Tier 1-5 for most capabilities). The framework provides qualitative tier definitions that map to progressive levels of agent sophistication, from simple parameter-based actuation to guaranteed safety. Implementation involves applying the taxonomy to classify agent behaviors, with no specific aggregation method provided for producing a holistic assessment. The method relies on human interpretation of tier criteria rather than automated evaluation.

## Key Results
- Introduces six core capabilities (Actuation, Disambiguation, Evaluation, Personalization, Transparency, Safety) that span the functional contract between agents and users
- Provides progressive competency tiers for each capability to enable incremental assessment and development roadmapping
- Maps capabilities to two experiential outcomes: low-friction interaction and trustworthiness
- Includes illustrative examples for different agent types including Computer Use Agents, Coding Agents, Search Agents, and Humanoid Agents

## Why This Works (Mechanism)

### Mechanism 1
A unified capability vocabulary enables cross-functional alignment between researchers, designers, engineers, and policy reviewers. ADEPTS abstracts user-facing behaviors into six discrete capabilities, creating a shared language that bridges UX heuristics, engineering taxonomies, and governance checklists without prescribing implementation details. The framework's effectiveness depends on shared mental models across disciplines.

### Mechanism 2
Progressive capability tiers enable incremental agent assessment and development roadmapping. Each ADEPTS capability is decomposed into tiered competency levels, allowing teams to benchmark current agent performance and identify capability gaps without requiring full-system evaluation. Tier progression provides a scaffold for measuring and tracking capability development over time.

### Mechanism 3
The six capabilities decompose into two experiential outcomes: low-friction (via Actuation, Disambiguation, Personalization) and trustworthiness (via Evaluation, Transparency, Safety). This mapping helps teams identify tradeoffs—for example, higher actuation performance might reduce transparency if the underlying architecture becomes more opaque.

## Foundational Learning

- **Reinforcement learning value functions** (reward, state-value, action-value): ADEPTS Evaluation and Safety tiers are explicitly mapped to RL concepts—Tier 3 Success Detection approximates reward functions, Tier 4 State-based prediction approximates value functions, Tier 5 Action-based prediction approximates action-value functions. *Quick check:* Can you explain why detecting whether a completed interaction was successful is fundamentally different from predicting whether a current state will lead to success?

- **Functional vs. non-functional requirements in software engineering**: ADEPTS explicitly defines itself as addressing functional capabilities (what the agent should do), deliberately excluding non-functional requirements like latency and responsiveness. *Quick check:* If an agent scores at Tier 4 on all ADEPTS capabilities but responds with 30-second latency, is it a "good" agent according to the framework?

- **Human-AI interaction interpretability spectrum** (algorithmic, verbalized, mechanistic): Transparency tiers map directly to this spectrum—Tier 1 exposes code/prompts, Tier 2 requires faithful chain-of-thought verbalization, Tier 3 demands mechanistic interpretability of neural activations. *Quick check:* Why might a "black box" agent that simply narrates plausible reasoning (Tier 2) fail to provide the oversight guarantees of mechanistic transparency (Tier 3)?

## Architecture Onboarding

- **Component map**: Actuation layer (task planning, action execution, tool invocation) -> Disambiguation layer (intent classifiers, clarification dialogue manager, constraint validators) -> Evaluation layer (success detectors, progress trackers, Q&A modules) -> Personalization layer (user preference store, session memory, goal prediction models) -> Transparency layer (logging infrastructure, explanation generators, mechanistic probes) -> Safety layer (input/output guardrails, state/action monitors, certified action whitelists)

- **Critical path**: Start with Actuation Tier 2-3 (target state + language prompts) as baseline capability; layer Disambiguation Tier 3 (underspecification detection) before scaling to complex tasks; add Safety Tier 1 (direct harm detection) before any real-world deployment; incrementally advance tiers based on domain-specific risk profiles.

- **Design tradeoffs**: Transparency vs. Actuation (more capable agents may use opaque architectures); Personalization vs. Safety (learned preferences may conflict with safety constraints); Disambiguation vs. Friction (higher tiers reduce error rates but increase interaction overhead).

- **Failure signatures**: Stuck in clarification loops (Disambiguation Tier 4-5 without proper termination conditions); Safety theater (Tier 3 claims without probabilistic confidence calibration); Transparency hallucination (Tier 2 verbalizations that don't reflect actual reasoning); Personalization drift (Tier 3-4 systems that overfit to stale preferences).

- **First 3 experiments**: 1) Run your current agent against ADEPTS tier definitions for Actuation and Disambiguation only; document evidence for each tier claim; 2) Instrument an agent to log transparency depth vs. actuation success rate; identify if architecture changes that improve one degrade the other; 3) Implement Safety Tier 1-2 detection for your highest-risk failure mode; validate that detection triggers before harm occurs in simulation.

## Open Questions the Paper Calls Out

### Open Question 1
Can application-specific benchmarks be developed that reliably measure each ADEPTS capability tier, and do higher tier scores correlate with improved user experience metrics? The authors state they hope the reference tiers "serve especially for inspiring the development of application-specific benchmarks" but provide no validated benchmarks themselves. Empirical studies showing correlations between ADEPTS tier scores and established usability metrics would resolve this.

### Open Question 2
What are the measurable tradeoffs between ADEPTS capabilities, and can formal methods quantify the actuation-transparency and safety-performance tensions? The authors note "there might be inherent tradeoffs between the different capabilities" including that "better safety guarantees might imply a degradation of absolute agent actuation capability." Controlled experiments varying one capability while measuring others would provide quantitative tradeoff curves.

### Open Question 3
Does the ADEPTS framework generalize to multi-agent systems and collaborative human-agent teams, or does the single-agent focus create blind spots? The framework assumes a single agent-single user paradigm with no discussion of multi-agent coordination or shared capability contracts. Case studies applying ADEPTS to multi-agent systems would identify capability gaps or necessary extensions.

## Limitations

- Quantitative boundaries for tiers are not defined, making consistent cross-team assessment challenging
- No specific aggregation method is provided to produce a single "ADEPTS score" or readiness assessment
- Limited direct evidence that tier progression maps to measurable usability improvements

## Confidence

- **High**: The framework's role as a shared vocabulary for cross-functional teams is well-supported by its alignment with existing HCI and engineering practices
- **Medium**: The progressive tier structure provides useful scaffolding for capability assessment, though empirical validation of tier boundaries remains limited
- **Medium**: The capability-to-outcome mapping (low-friction vs. trustworthiness) is conceptually coherent but lacks direct user experience validation

## Next Checks

1. Conduct user studies comparing agents with identical capability tiers but different architectural implementations to test if tier assessments predict actual usability
2. Validate tier boundaries by having independent teams assess the same agents and measuring inter-rater reliability for tier assignments
3. Test the framework's predictive power by using tier assessments to guide development priorities, then measuring resulting improvements in user satisfaction and agent reliability