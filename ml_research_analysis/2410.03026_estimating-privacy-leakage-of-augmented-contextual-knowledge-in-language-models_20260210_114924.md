---
ver: rpa2
title: Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models
arxiv_id: '2410.03026'
source_url: https://arxiv.org/abs/2410.03026
tags:
- context
- privacy
- influence
- arxiv
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a principled method called context influence
  to measure the privacy leakage of contextual knowledge during language model generation.
  The method builds on differential privacy to quantify how much removing parts of
  the context changes the model's output, thus estimating the risk of exposing private
  information in the context.
---

# Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models

## Quick Facts
- arXiv ID: 2410.03026
- Source URL: https://arxiv.org/abs/2410.03026
- Reference count: 25
- Key outcome: This paper proposes a principled method called context influence to measure the privacy leakage of contextual knowledge during language model generation.

## Executive Summary
This paper introduces a principled method to measure privacy leakage from augmented contextual knowledge in language models. The approach builds on differential privacy to quantify how much removing parts of the context changes the model's output, thus estimating the risk of exposing private information. The authors demonstrate that context privacy leakage is higher when the context is out-of-distribution relative to the model's parametric knowledge and when context amplification decoding methods are used. Experiments on summarization and question-answering tasks reveal that larger models with more parametric knowledge are less influenced by context, while models lacking relevant knowledge leak more contextual information.

## Method Summary
The method, called Context Influence Decoding (CID), measures privacy leakage by comparing output probabilities with and without specific context subsets. It uses differential privacy principles to quantify how much removing n-grams from the context changes the model's output distribution. The framework separates contributions from parametric knowledge (what the model already knows) from contextual knowledge (what it learns from the prompt). The method computes context influence τ as the absolute log-probability difference when an n-gram is removed, aggregating these differences across the generation to estimate total privacy leakage.

## Key Results
- Context influence decreases as model size increases from 125M to 66B parameters
- Models fine-tuned with SFT/RLHF show higher context influence than pre-trained models due to increased context utilization
- Truncating the context or increasing response length reduces context influence
- Earlier context tokens have 2-3× higher influence on generation than later tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Comparing output probabilities with and without context subsets isolates contextual privacy leakage from parametric knowledge contributions.
- Mechanism: Context influence (τ) computes the log-odds difference: log p(y_t|D,x,y_<t) - log p(y_t|D\D_{i,n},x,y_<t). This measures how much removing the i-th n-gram shifts the output distribution, providing a lower-bound estimate of what an adversary could infer about that n-gram's presence in the context.
- Core assumption: Sampling-based decoding satisfies randomized output requirements; assumes parametric and contextual knowledge can be meaningfully separated through counterfactual comparison.
- Evidence anchors: [abstract] "Our approach effectively measures how each subset of the context influences an LM's response while separating the specific parametric knowledge of the LM." [section 3.2, Definition 3.1] Formalizes context influence as the absolute log-probability difference when Di,n is removed.

### Mechanism 2
- Claim: Privacy leakage scales with the PMI difference between full-context and ablated-context predictions, modulated by the decoding influence parameter λ.
- Mechanism: Theorem 3.1 establishes τ_{i,n} ∝ λ|pmi(p_θ(y_t;D,x,y_<t)) - pmi(p_θ(y_t;D\D_{i,n},x,y_<t))|. When context is out-of-distribution relative to parametric knowledge, the PMI gap widens; amplifying context via higher λ directly increases the influence signal.
- Core assumption: PMI accurately captures reliance on specific context subsets; assumes the linear interpolation in CID preserves meaningful probability ratios.
- Evidence anchors: [abstract] "context privacy leakage occurs when contextual knowledge is out of distribution with respect to parametric knowledge" [section 3.3, Theorem 3.1] Derives the direct proportionality between context influence, λ, and PMI difference.

### Mechanism 3
- Claim: Model capacity inversely correlates with context influence; larger models with richer parametric knowledge depend less on augmented context.
- Mechanism: Larger models memorize more pre-training data, providing stronger priors p(y_t|x,y_<t). When p(y_t|x,y_<t) already captures relevant knowledge, the likelihood ratio with context shrinks. Fine-tuning (SFT/RLHF) increases context utilization for task performance, paradoxically increasing privacy leakage.
- Core assumption: Larger parameter count translates to broader parametric knowledge coverage; assumes pre-training data distribution determines what counts as "out-of-distribution."
- Evidence anchors: [section 4.3, Figure 2a] Shows context influence decreases as model size increases from 125M to 66B parameters. [section 4.3] "LLaMA 3 IT is substantially influenced by the context more than just pre-trained LLaMA 3" due to SFT/RLHF alignment.

## Foundational Learning

- **Differential Privacy (ε-DP and ex-post per-instance DP)**:
  - Why needed here: The paper builds context influence directly on DP definitions; understanding how ε bounds information leakage and why per-instance variants enable context-dependent auditing is essential.
  - Quick check question: Can you explain why ε-DP's context-independent bound is insufficient for auditing which specific context subsets leak information?

- **Parametric vs. Contextual Knowledge in LLMs**:
  - Why needed here: The entire framework depends on separating what the model "already knows" from what it learns from the prompt; misunderstanding this distinction leads to over- or under-estimating privacy risks.
  - Quick check question: If a model outputs sensitive information present in both its pre-training data and the augmented context, which source should be blamed for the leak?

- **Pointwise Mutual Information (PMI)**:
  - Why needed here: PMI quantifies association between generated tokens and context presence; the theoretical derivation links privacy leakage directly to PMI differences.
  - Quick check question: Why does PMI(y_t, D) = log[p(y_t|D)/p(y_t)] increase when context provides information the model's prior doesn't have?

## Architecture Onboarding

- **Component map**: Input Layer (Query x + context D) -> Forward Pass (parallel LM calls for full/ablated context) -> CID Decoding (interpolate logits with λ) -> Influence Calculator (compute log-probability differences) -> Aggregation (average τ across samples)

- **Critical path**: The ablation step (forward pass without D_{i,n}) is the computational bottleneck—it requires O(|D|/n) forward passes per generation to measure all n-grams. Start with document-level (n=|D|) to validate pipeline, then reduce n for granular analysis.

- **Design tradeoffs**:
  - λ tuning: Higher λ increases faithfulness but risks regurgitation; λ=1.0 balances utility/privacy; λ<0.5 minimizes leakage but degrades task performance
  - N-gram granularity: n=1 provides fine-grained attribution but O(|D|) passes; n=128 balances granularity with compute; n=|D| requires single ablated pass
  - Context positioning: Place sensitive information toward context end—earlier tokens have 2-3× higher influence due to position bias

- **Failure signatures**:
  - Low influence with high regurgitation: Indicates parametric knowledge contamination—verify with holdout context
  - High variance across samples: Suggests λ or temperature settings are inappropriate; ensure T≥0.8 for stable sampling
  - Identical τ for all n-grams: Check that ablation is actually removing tokens; confirm D\D_{i,n} differs from D

- **First 3 experiments**:
  1. Baseline validation: Measure document-level context influence (τ_|D|) on CNN-DM with OPT-1.3B, comparing λ∈{0.5,1.0,1.5}. Verify that higher λ increases τ and Repeat Prompts.
  2. Parametric knowledge ablation: Compare OPT-1.3B (no PubMed) vs GPT-Neo-1.3B (trained on PubMed) on PubMedQA. Confirm that GPT-Neo shows lower τ but similar raw regurgitation, demonstrating the false-security problem.
  3. Position sensitivity: Measure τ for i-th 128-grams across context positions with OPT-1.3B on PubMedQA. Replicate the declining influence pattern from Figure 3 to calibrate where sensitive data should be placed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does normalizing context influence by joint self-entropy provide a more accurate cross-model comparison of privacy leakage?
- Basis in paper: [explicit] The authors state that the current formulation "does not consider the entropy of the model during decoding," causing confident models to appear less leaky than unconfident ones.
- Why unresolved: The raw log-odds ratio fails to account for output distribution sharpness, potentially misrepresenting risk across different model architectures.
- What evidence would resolve it: Experiments comparing raw context influence against entropy-normalized scores on models with varying calibration levels.

### Open Question 2
- Question: Can the context influence metric be modified to robustly handle non-sampling decoding methods like top-p or top-k sampling?
- Basis in paper: [inferred] The authors note context influence "only works for sampling-based algorithms" and that top-p/k "can cause potential errors... unless the selected indices are equal."
- Why unresolved: Current definitions require comparing probabilities of the same token, which breaks when truncation methods filter tokens differently depending on context presence.
- What evidence would resolve it: A reformulation of the metric using divergence measures that handle disjoint token sets, validated against top-k generations.

### Open Question 3
- Question: Can adaptive privacy mechanisms effectively leverage the finding that context influence is highest in early generation tokens?
- Basis in paper: [inferred] The authors suggest designing solutions that "adopt an adaptive privacy level, where the privacy level is strict during the beginning... then is relaxed."
- Why unresolved: The paper identifies the correlation between token position and leakage but does not implement or test a dynamic defense strategy.
- What evidence would resolve it: An algorithmic implementation of adaptive noise or truncation applied strictly to early tokens, demonstrating improved utility-privacy tradeoffs.

## Limitations

- The computational complexity of measuring context influence at fine granularity requires O(|D|/n) forward passes per generation, making large-context or small-n analyses expensive.
- The separation between parametric and contextual knowledge is theoretically approximate rather than exact, becoming problematic when knowledge sources overlap substantially.
- The framework assumes sampling-based decoding satisfies randomized output requirements, but the behavior under adversarial conditions or with alternative sampling strategies remains unclear.

## Confidence

- High confidence: The experimental results showing decreasing context influence with larger model sizes (Figure 2a) and the correlation between OOD context and higher privacy leakage.
- Medium confidence: The theoretical connection between PMI differences and context influence (Theorem 3.1), which depends on assumptions about PMI accurately capturing context reliance.
- Low confidence: The claim that context positioning dramatically affects privacy leakage (Figure 3), as the magnitude and generalizability of the effect remain unclear without additional validation.

## Next Checks

1. **Adversarial Sampling Validation**: Test whether temperature sampling with T=0.8 consistently satisfies randomized output requirements under adversarial conditions. Specifically, verify that different sampling runs produce sufficiently varied outputs when context is removed, and measure how often the same tokens are selected across multiple runs with identical settings.

2. **Parametric Knowledge Overlap Test**: Create synthetic contexts that contain information already present in the model's training data (verified through holdout sets). Measure whether context influence correctly attributes low privacy risk in these cases, or whether the framework overestimates leakage due to the inability to cleanly separate overlapping knowledge sources.

3. **Position Bias Robustness Check**: Replicate the position sensitivity experiments across different model architectures (beyond OPT-1.3B) and task types. Specifically, test whether the observed 2-3× higher influence for earlier tokens generalizes to models with different positional encoding schemes and to tasks beyond summarization and QA.