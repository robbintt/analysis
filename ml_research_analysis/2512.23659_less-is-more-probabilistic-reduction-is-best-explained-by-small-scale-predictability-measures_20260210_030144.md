---
ver: rpa2
title: 'Less is more: Probabilistic reduction is best explained by small-scale predictability
  measures'
arxiv_id: '2512.23659'
source_url: https://arxiv.org/abs/2512.23659
tags:
- language
- words
- word
- reduction
- probabilistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that n-gram statistics, not large language
  models, better explain probabilistic reduction in phonetic duration. Across four
  speech corpora, n-gram probabilities derived from short-range phrase sequences provided
  superior model fits to word durations compared to long-range LLM probabilities.
---

# Less is more: Probabilistic reduction is best explained by small-scale predictability measures

## Quick Facts
- **arXiv ID:** 2512.23659
- **Source URL:** https://arxiv.org/abs/2512.23659
- **Reference count:** 20
- **Primary result:** N-gram probabilities from short-range phrase sequences better predict phonetic duration than long-range LLM probabilities across four speech corpora.

## Executive Summary
This paper challenges the assumption that large language models (LLMs) better capture the cognitive mechanisms underlying probabilistic reduction in speech. Through analysis of four spontaneous speech corpora (Buckeye, CORAAL, Switchboard, and CANDOR), the authors demonstrate that n-gram statistics—not LLM-derived probabilities—provide superior model fits for predicting word duration. LLM probabilities were found to be confounded with utterance position and failed to show consistent reduction effects, while n-gram probabilities consistently showed the expected negative correlation with duration. The results support an incremental planning model where speakers use phrase-shaped representations rather than full-utterance context, and suggest that observed reductions are small enough to reflect low-level fluency rather than strategic listener-oriented optimization.

## Method Summary
The study analyzed time-aligned speech corpora by segmenting utterances at pause boundaries into inter-pause units, then extracting probability estimates using both n-gram counts and LLM models (including fine-tuned Pythia-160M on dialect-specific corpora). Mixed-effects regression models predicted log-transformed word duration using probability estimates alongside covariates including word frequency, phoneme count, utterance length, and relative position. Model fit was compared using Log-Likelihood improvements, and coefficient signs were examined for consistency across corpora and probability estimation methods.

## Key Results
- N-gram probabilities provided superior model fits to word durations compared to LLM probabilities across all four corpora
- LLM probabilities were confounded with utterance position, failing to show consistent reduction effects
- Fine-tuning Pythia-160M on dialect-specific corpora did not improve LLM predictive power relative to n-grams
- Probabilistic reduction effects were small (single-digit millisecond differences), suggesting mechanistic rather than strategic processes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local n-gram probabilities predict phonetic reduction better than long-context LLM probabilities.
- Mechanism: Speakers retrieve and plan speech incrementally using phrase-shaped representations (e.g., bigrams/trigrams) rather than computing full-utterance probabilities. Predictable words in these local sequences are reduced in duration.
- Core assumption: Incremental planning limits the effective context window for probabilistic reduction.
- Evidence anchors:
  - [abstract] "...n-gram probabilities derived from short-range phrase sequences provided superior model fits to word durations compared to long-range LLM probabilities."
  - [section 5] "All n-gram based predictors show probabilistic reduction."
  - [corpus] Weak: No direct neural correlate evidence; findings are behavioral.
- Break condition: If speakers pre-plan entire utterances with full contextual probabilities before articulation, n-gram models would underperform LLMs.

### Mechanism 2
- Claim: LLM-based probability estimates are confounded with utterance position, reducing their reliability for explaining duration.
- Mechanism: LLMs encode positional information that correlates with both probability estimates and duration effects (e.g., final lengthening). This confound inflates or distorts apparent predictability effects.
- Core assumption: The confound between position and LLM probability is not adequately controlled in prior work.
- Evidence anchors:
  - [abstract] "LLM probabilities were confounded with utterance position and failed to show consistent reduction effects."
  - [section 4] "...language model probabilities may be partially confounded with language model probabilities [sic]. ...forward transition probabilities decrease substantially over positions."
  - [corpus] Weak: No direct corpus evidence for internal LLM positional encoding.
- Break condition: If position can be perfectly regressed out of LLM probabilities without information loss, this confound would disappear.

### Mechanism 3
- Claim: Probabilistic reduction effects are small, suggesting they may reflect low-level retrieval fluency rather than listener-oriented rational optimization.
- Mechanism: Observed reduction (single-digit millisecond differences) may arise from motor practice or ease of retrieval rather than strategic communicative adaptation.
- Core assumption: Small effect sizes indicate mechanistic (production) rather than strategic (listener-focused) processes.
- Evidence anchors:
  - [abstract] "Effect sizes of probabilistic reduction were small..."
  - [section 6] "...speedups of mere single-digit milliseconds between very probable and very improbable words."
  - [corpus] Weak: No corpus evidence linking small effects to specific cognitive mechanisms.
- Break condition: If experiments show larger, systematic reduction tuned to listener needs in constrained tasks, the rational account would gain support.

## Foundational Learning

- **Probabilistic Reduction**:
  - Why needed here: Central dependent variable; all predictors aim to explain variance in word duration based on predictability.
  - Quick check question: What is the expected direction of the correlation between word predictability and phonetic duration?

- **N-gram vs. LLM Probabilities**:
  - Why needed here: Core independent variable comparison; n-grams use fixed short contexts, LLMs encode long sequences but are computationally complex.
  - Quick check question: Why might a simple n-gram model better reflect human incremental speech planning than a large language model?

- **Prosodic Covariates (e.g., phrase-final lengthening, anticipatory shortening)**:
  - Why needed here: Must be controlled to isolate probabilistic effects; otherwise, duration variance may be misattributed.
  - Quick check question: How does relative position within an utterance affect word duration?

## Architecture Onboarding

- **Component map**: Speech corpora -> Preprocessing (pause segmentation) -> Probability extraction (n-gram/LLM) -> Covariate compilation -> Mixed-effects regression -> Model comparison

- **Critical path**:
  1. Segment utterances at pause boundaries into inter-pause units
  2. Extract n-gram and/or LLM probabilities for each word
  3. Compile all structural and lexical covariates
  4. Fit mixed-effects regression models
  5. Compare model fit (ΔLL) and inspect coefficient signs and magnitudes

- **Design tradeoffs**:
  - N-gram models: Sparse, computationally cheap, transparent, but limited context
  - LLMs: Rich long-range dependencies, but computationally heavy, confounded with position, and cognitively less plausible for incremental production
  - Fine-tuning on dialect corpora: Improves domain match but does not consistently recover reduction effects

- **Failure signatures**:
  - LLM probability coefficients have inconsistent signs (positive instead of negative)
  - Strong correlation between LLM probability and utterance position
  - Model comparison shows worse fit for LLM-based models vs. n-gram models

- **First 3 experiments**:
  1. Re-analyze CANDOR with short-context LLM probabilities (intonational phrases) vs. full-turn probabilities
  2. Re-analyze Switchboard-NXT, comparing n-gram vs. LLM predictors while explicitly modeling position confounds
  3. Analyze Buckeye and CORAAL using fine-tuned Pythia-160M and trigram models; compare coefficient consistency and model fit across dialects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent is probabilistic reduction generalizable across diverse speech communities and domains of spontaneous speech?
- Basis in paper: [explicit] The authors explicitly state in the Interim Discussion that "one major open question is about the generality of probabilistic reduction, both in terms of speech communities and the domain of spontaneous speech under analysis."
- Why unresolved: While the study analyzed CORAAL (African American Language), models attempting to include specific dialect effects failed to converge, leaving variation across specific dialects unquantified.
- What evidence would resolve it: Successful convergence of regression models incorporating dialect-specific random effects or coefficients across larger, diverse speech corpora.

### Open Question 2
- Question: Do millisecond-level durational reductions confer actual benefits for listener comprehension or speaker resource conservation?
- Basis in paper: [explicit] Section 6 notes that the effect sizes are so small they are "almost imperceptible," and the link to resource conservation is "less than clear," concluding that "further experimentation is needed to test this alternative explanation."
- Why unresolved: The statistical significance of predictors contrasts with their tiny practical effect sizes (single-digit milliseconds), which may not offer meaningful communicative efficiency.
- What evidence would resolve it: Behavioral experiments correlating specific millisecond reductions with measurable improvements in listener word recognition or decreases in speaker articulatory effort.

### Open Question 3
- Question: Does probabilistic reduction reflect rational audience design or merely the retrieval of frequently co-occurring phrases from memory?
- Basis in paper: [explicit] The General Discussion proposes that reduction may reflect "other processes" like phrase retrieval rather than rational production, explicitly stating that "further experimentation is needed to test this alternative explanation."
- Why unresolved: N-gram probabilities are highly correlated with phrase frequency, making it difficult to distinguish between a speaker calculating probability for a listener vs. simply retrieving a fluent chunk.
- What evidence would resolve it: Experiments that decouple contextual probability from multiword phrase frequency to observe which variable drives reduction independently.

## Limitations
- Confounding between LLM probabilities and utterance position may inflate or obscure true predictability effects
- Small effect sizes (single-digit milliseconds) make it difficult to determine whether reductions reflect strategic or mechanistic processes
- Lack of direct neural or psycholinguistic evidence linking n-gram statistics to incremental planning

## Confidence

- **High confidence**: The superiority of n-gram statistics over LLM probabilities in predicting word durations, as demonstrated across four independent corpora with consistent coefficient signs and model fit improvements.
- **Medium confidence**: The claim that LLM probabilities are confounded with utterance position, as the correlation is demonstrated but the exact causal mechanism remains unclear.
- **Medium confidence**: The assertion that small effect sizes suggest mechanistic rather than strategic reduction, as this interpretation requires additional experimental validation beyond the observational corpus data.

## Next Checks

1. **Replication with controlled positional confounds**: Re-analyze the CANDOR corpus using short-context LLM probabilities (within intonational phrases) versus full-turn probabilities while explicitly modeling position as a covariate. This would test whether the confound with utterance position, rather than long-range dependencies per se, drives LLM underperformance.

2. **Dialectic adaptation robustness test**: Extend the Buckeye and CORAAL analyses by training multiple fine-tuned Pythia models with varying hyperparameters and comparing coefficient consistency across dialects. This would assess whether observed n-gram superiority persists even with improved domain adaptation.

3. **Incremental planning experimental validation**: Conduct a controlled production experiment where speakers plan utterances incrementally (segment by segment) versus holistically, measuring whether n-gram probabilities predict duration better in the incremental condition. This would provide direct behavioral support for the incremental planning hypothesis.