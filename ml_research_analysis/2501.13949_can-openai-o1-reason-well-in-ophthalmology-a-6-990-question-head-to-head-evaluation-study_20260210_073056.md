---
ver: rpa2
title: Can OpenAI o1 Reason Well in Ophthalmology? A 6,990-Question Head-to-Head Evaluation
  Study
arxiv_id: '2501.13949'
source_url: https://arxiv.org/abs/2501.13949
tags:
- reasoning
- performance
- openai
- page
- ophthalmology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated OpenAI's o1 model and five other large language
  models using 6,990 ophthalmology questions from MedMCQA. O1 achieved the highest
  accuracy (88%) and macro-F1 score (0.70) but ranked third in reasoning capabilities
  using text-generation metrics.
---

# Can OpenAI o1 Reason Well in Ophthalmology? A 6,990-Question Head-to-Head Evaluation Study

## Quick Facts
- arXiv ID: 2501.13949
- Source URL: https://arxiv.org/abs/2501.13949
- Reference count: 40
- Primary result: OpenAI o1 achieved 88% accuracy on 6,990 ophthalmology MCQs but ranked third in reasoning capabilities by text-generation metrics

## Executive Summary
This study evaluates OpenAI's o1 model against five other large language models using 6,990 ophthalmology questions from MedMCQA. While o1 achieved the highest accuracy (88%) and macro-F1 score (0.70), it ranked third in reasoning capabilities when assessed using text-generation metrics. O1 performed best in "Lens" and "Glaucoma" subtopics but trailed GPT-4o in "Corneal and External Diseases," "Vitreous and Retina," and "Oculoplastic and Orbital Diseases." The study reveals a paradox where o1's superior factual accuracy doesn't translate to higher reasoning scores, suggesting that standard evaluation metrics may not capture the quality of medical reasoning.

## Method Summary
The study used zero-shot prompting to evaluate six LLMs (o1-preview, GPT-4o, GPT-4, GPT-3.5, Llama-3, Gemini 1.5 Pro) on 6,990 ophthalmology MCQs from MedMCQA. Models were instructed to act as ophthalmology assistants, select answers, and provide explanations with temperature=0 for reproducibility. Performance was measured using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore, BARTScore, AlignScore, METEOR) comparing model explanations to ground truth. Statistical comparisons employed paired t-tests, z-tests, and Wilcoxon rank-sum tests with Bonferroni correction.

## Key Results
- O1 achieved highest accuracy (88%) and macro-F1 (0.70) across all models
- O1 ranked first in "Lens" and "Glaucoma" subtopics but second to GPT-4o in three other subspecialties
- Despite highest accuracy, o1 ranked third in reasoning capabilities by text-generation metrics
- O1 performed better on questions with longer ground truth explanations (â‰¥100 words)
- Qualitative analysis revealed o1 occasionally confused similar clinical concepts (e.g., Phenol Red Thread Test vs. Schirmer test)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Verbose, chain-of-thought responses achieve higher factual accuracy but lower semantic similarity scores against concise ground truths.
- **Mechanism:** O1 generates detailed reasoning chains before concluding, increasing token count and depth. Standard text-generation metrics penalize length mismatches and lack of lexical overlap, causing divergence between high accuracy and lower reasoning scores.
- **Core assumption:** Text-generation metrics proxy for reasoning quality only when model output length and style match reference text.
- **Evidence anchors:** Abstract states o1 achieved highest accuracy but ranked third in reasoning based on text-generation metrics; section notes o1 performed better on queries with longer ground truth explanations.
- **Break condition:** If reference explanations are rewritten to be more verbose, o1's text-generation ranking would likely improve.

### Mechanism 2
- **Claim:** General-purpose reasoning enhancements transfer unevenly to specialized domains, leading to misinterpretation of similar clinical concepts.
- **Mechanism:** The model relies on statistical correlations from broad training data. In specialized fields like ophthalmology, distinct clinical tests may share high semantic proximity in the embedding space without distinct functional boundaries, leading to detailed but factually incorrect "hallucinations" or conflation.
- **Core assumption:** Reasoning capability is a general logic layer that does not inherently possess specialized verifiable knowledge for ophthalmology.
- **Evidence anchors:** Abstract notes o1's reasoning enhancements may not fully extend to ophthalmology; qualitative evaluation revealed instances where o1 confused similar clinical concepts like Phenol Red Thread Test and Schirmer test.
- **Break condition:** If domain-specific fine-tuning or RAG is applied to ground reasoning in verified ophthalmology texts, concept conflation should decrease.

### Mechanism 3
- **Claim:** Performance superiority is contingent on subspecialty topic, likely reflecting distribution density of training data in those areas.
- **Mechanism:** O1 outperforms in "Lens" and "Glaucoma" but trails in "Oculoplastic" or "Retina," suggesting reasoning chains are more robust for high-frequency or logically distinct pathologies compared to more variable subspecialties.
- **Core assumption:** MedMCQA provides uniform distribution of difficulty across subspecialties, implying performance differences are model-inherent rather than data-biased.
- **Evidence anchors:** Abstract states o1 ranked first in "Lens" and "Glaucoma" but second to GPT-4o in "Corneal and External Diseases"; section notes o1 ranked first in "Lens" and "Glaucoma" but second to GPT-4o in "Corneal and external diseases."
- **Break condition:** If training data provenance were revealed to be heavily skewed toward Glaucoma literature, the subspecialty performance gap would be attributed to data bias rather than reasoning architecture.

## Foundational Learning

- **Concept: Text-Generation Metrics (ROUGE-L, BERTScore)**
  - **Why needed here:** The study concludes o1 is "worse" at reasoning than GPT-4o solely based on these metrics, despite o1 having higher accuracy. You must understand that these metrics measure similarity to reference text, not logical validity. A correct but verbose answer scores lower than a concise, wrong answer if the reference is concise.
  - **Quick check question:** If a model outputs a 200-word correct explanation for a question with a 10-word ground truth, will ROUGE-L likely be high or low?

- **Concept: Zero-Shot Prompting**
  - **Why needed here:** The study uses a "zero-shot" approach (no examples provided in the prompt). This tests the model's intrinsic knowledge and reasoning without "in-context learning" crutches.
  - **Quick check question:** Does zero-shot evaluation measure a model's peak performance (with prompt engineering) or its baseline capabilities?

- **Concept: Ground Truth Constraints**
  - **Why needed here:** The study notes that some ground truth explanations in MedMCQA are "suboptimal" (disjointed keywords). Understanding this reveals a system limitation: you cannot reliably evaluate a sophisticated reasoner against a low-quality reference.
  - **Quick check question:** Why might a "smarter" model score poorly against a "dumb" ground truth?

## Architecture Onboarding

- **Component map:** Data Curation -> Prompting -> Generation -> Evaluation
- **Critical path:**
  1. Data Curation: Filter MedMCQA for ophthalmology (n=6,990) and regroup into 5 subtopics
  2. Prompting: Inject questions into standardized prompt template instructing model to act as ophthalmology assistant
  3. Generation: Call APIs with temperature=0 (greedy decoding) to ensure reproducibility
  4. Evaluation: Compare output string against ground truth using statistical tests (Wilcoxon rank-sum)

- **Design tradeoffs:**
  - Verbosity vs. Alignment: O1 defaults to detailed reasoning (safer for complex logic, worse for standard text-metrics). GPT-4o defaults to concise summaries (better for text-metrics, risks oversimplification).
  - Cost/Latency: O1's "thinking" time incurs higher latency and token costs compared to GPT-4o, which may not be justified for simple triage tasks where accuracy gains are marginal.

- **Failure signatures:**
  - Concept Conflation: Model hallucinates a bridge between two distinct medical terms while maintaining highly confident, structured tone
  - Metric/Accuracy Split: High accuracy (correct answer choice) but low reasoning score (explanation differs from ground truth style/length)

- **First 3 experiments:**
  1. Metric Sensitivity Test: Run evaluation on subset where ground truths are manually expanded to match O1's verbosity. Verify if reasoning rankings flip.
  2. Error Typology Analysis: Qualitatively classify the 12% of incorrect O1 answers (Knowledge Gap vs. Logic Error vs. Concept Conflation).
  3. Subspecialty Stress Test: Feed model ambiguous or "borderline" cases in "Vitreous and Retina" to identify if failure mode is visual dependency or textual logic.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does OpenAI o1's superior accuracy on multiple-choice questions translate to effectiveness in real-world clinical workflows, such as patient triage or clinical management?
- **Basis in paper:** The authors state that the use of exam questions is "only the first step" and explicitly call for future studies to create benchmarks including "patient queries, and clinical management tasks."
- **Why unresolved:** The current study evaluated medical knowledge using MedMCQA dataset, which consists of exam-style MCQs rather than open-ended clinical scenarios or patient interactions.
- **What evidence would resolve it:** Performance evaluation of o1 on datasets containing unstructured clinical notes, triage scenarios, and simulated patient management tasks compared to human expert performance.

### Open Question 2
- **Question:** Can a medical-specific text generation metric be developed that correlates better with clinical reasoning quality than general metrics like ROUGE-L or BERTScore?
- **Basis in paper:** The authors list the reliance on general text-generation metrics as a limitation, stating they are "not specific to medical context" and "warrants the need for medical-specific text generation metric for future study."
- **Why unresolved:** General metrics penalize models for length mismatches or synonym usage that may be clinically valid, failing to capture nuance of medical logic.
- **What evidence would resolve it:** Development and validation of new evaluation metric that weights factual medical accuracy and logical clinical deduction over simple lexical overlap.

### Open Question 3
- **Question:** To what extent does o1's tendency to generate verbose, detailed responses artificially depress its scores on semantic overlap metrics like ROUGE-L?
- **Basis in paper:** The authors note that o1 generates longer responses than ground truths and that text-generation metrics are "sensitive to length mismatches," suggesting this "partly explain[s] o1's inconsistent performance" despite its high accuracy.
- **Why unresolved:** It remains unclear if lower text-generation scores indicate failure of reasoning or simply stylistic divergence (verbosity) that standard metrics penalize.
- **What evidence would resolve it:** Controlled analysis comparing o1's reasoning scores when constrained to ground-truth length limits versus free generation, or using length-agnostic semantic similarity measures.

## Limitations
- Ground truth quality variance: MedMCQA explanations are noted as "suboptimal" with disjointed keywords, potentially penalizing models like o1 that generate detailed reasoning chains
- Prompt template ambiguity: Exact prompt wording and output parsing rules are unspecified, introducing reproducibility concerns
- Subspecialty data distribution: Performance gaps across ophthalmology subtopics may reflect training data imbalance rather than inherent reasoning capability differences

## Confidence
- **High Confidence:** O1 achieves superior accuracy (88%) and macro-F1 (0.70) on MedMCQA ophthalmology questions
- **Medium Confidence:** O1's reasoning ranks third by text-generation metrics despite accuracy leadership, depending on assumption that these metrics validly measure reasoning quality
- **Low Confidence:** Claim that o1's reasoning enhancements "may not fully extend" to ophthalmology, extrapolating from text-metric rankings to broad reasoning capability

## Next Checks
1. **Ground truth alignment test:** Manually expand 100 ground truth explanations to match o1's verbosity style. Re-run text-generation metrics to determine if reasoning rankings reverse.
2. **Prompt sensitivity analysis:** Test all six models with three prompt variations (minimal instruction, detailed reasoning instruction, example-in-context). Compare performance variance across models.
3. **Error mode classification:** Qualitatively categorize 200 incorrect responses per model (knowledge gap, logic error, concept conflation, parsing failure). Determine if o1's errors differ systematically from other models beyond metric scores.