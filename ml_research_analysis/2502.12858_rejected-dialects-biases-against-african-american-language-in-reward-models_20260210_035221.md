---
ver: rpa2
title: 'Rejected Dialects: Biases Against African American Language in Reward Models'
arxiv_id: '2502.12858'
source_url: https://arxiv.org/abs/2502.12858
tags:
- reward
- language
- preference
- texts
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework to quantify anti-African American
  Language (AAL) biases in reward models, which are central to large language model
  alignment. The authors use machine-translated and human-translated paired WME-AAL
  corpora to evaluate 17 popular reward models.
---

# Rejected Dialects: Biases Against African American Language in Reward Models

## Quick Facts
- arXiv ID: 2502.12858
- Source URL: https://arxiv.org/abs/2502.12858
- Reference count: 40
- Key outcome: Reward models show 4% average accuracy drop on AAL vs WME texts, assign lower scores to AAL, and steer conversations toward WME

## Executive Summary
This paper quantifies anti-African American Language (AAL) bias in reward models using machine-translated and human-translated paired corpora. The authors evaluate 17 popular reward models on datasets containing both White Mainstream English (WME) and AAL texts, finding significant performance disparities. Reward models are less accurate at predicting human preferences on AAL texts, systematically assign lower scores to AAL-aligned completions, and incentivize steering conversations toward WME even when prompted in AAL. These findings reveal representational harms and quality-of-service disparities that raise ethical concerns about language technology development.

## Method Summary
The authors evaluate 17 reward models using three datasets: filtered RewardBench WME texts (1843 samples), machine-translated AAL versions using VALUE and PhonATe pipelines, and human-written paired AAL/WME texts from DeasGroenwold (2365 samples). They assess accuracy differences (RQ1), preference score disparities (RQ2), and steering behavior toward WME (RQ3) using McNemar's tests, Cohen's d effect sizes, and Pearson correlations between AAL-ness and scores.

## Key Results
- Reward models show ~4% average accuracy drop when predicting human preferences on AAL vs WME texts
- Large positive effect sizes (0.44-1.03) indicate systematic preference for WME over AAL-aligned texts
- Large negative effect sizes (-0.34 to -0.85) show models steer AAL prompts toward WME completions

## Why This Works (Mechanism)

### Mechanism 1: Dialect-Based Accuracy Degradation in Preference Prediction
Reward models show ~4% average accuracy drop when predicting human preferences on AAL texts versus WME texts. Underrepresentation of AAL in preference training datasets leads to domain shift—models trained predominantly on WME cannot reliably discern preference signals in AAL contexts. The accuracy drop is partially caused by training data composition rather than solely by translation artifacts.

### Mechanism 2: Systematic Dispreference for AAL-Aligned Text
Reward models assign lower scores to AAL texts compared to WME equivalents, even when content is meaning-preserved. Raciolinguistic hierarchies propagate through preference data collected from unrepresentative annotators. The "white listening subject" phenomenon causes AAL to be perceived through a lens of linguistic racism, affecting subjective preference judgments that reward models learn to emulate.

### Mechanism 3: Dialect Steering Toward WME
Reward models incentivize responding to AAL prompts with WME completions rather than mirroring the input dialect. The implicit persona encoded in reward models is positioned as a "white listening/speaking subject." Reward optimization learns that WME-aligned responses score higher, creating feedback loops that suppress dialect diversity regardless of input context.

## Foundational Learning

- **Reward Models in RLHF/DPO**: Understanding how reward models convert (prompt, completion) pairs into scalar preference scores is essential to interpreting how bias enters the alignment pipeline.
  - *Quick check*: Given a prompt and two candidate completions, how would a sequence classifier RM versus a DPO-based RM determine which is preferred?

- **Sociolinguistic Foundations (WME vs. AAL)**: The paper situates findings in raciolinguistic theory; understanding terms like "white listening subject" and "linguistic hierarchy" is necessary to interpret the harm taxonomy.
  - *Quick check*: What distinguishes AAL as a sociolect from WME as a dominant dialect, and why does this matter for preference annotation?

- **Dialect Evaluation Methodology (VALUE + PhonATe)**: The translation pipeline determines validity of WME-AAL comparisons; understanding tradeoffs between machine translation and human-written evaluation data is critical for replication.
  - *Quick check*: What are the validation metrics for VALUE (accuracy) and PhonATe (meaning preservation, naturalness), and what limitations do they introduce?

## Architecture Onboarding

- **Component map**: RewardBench (WME prompts/completions) → GPT-4o code filter → VALUE transformer → PhonATe transformer → RB-WME / RB-AAL paired datasets → Reward model inference → Statistical analysis layer

- **Critical path**: The accuracy comparison depends on meaning preservation through translation. The preference analysis requires paired WME-AAL texts from both machine and human sources. The steering analysis requires crossing prompt and completion dialects.

- **Design tradeoffs**: Machine-translated data preserves preference structure but risks translation artifacts; human-written data is ecologically valid but lacks prompt structure. Using both triangulates findings but complicates interpretation when results diverge.

- **Failure signatures**: Watch for safety refusals in WME becoming information leaks in AAL; helpfulness boundaries violating in AAL.

- **First 3 experiments**:
  1. Replicate RQ1 accuracy comparison on 3-5 reward models using provided RB-WME and RB-AAL splits to validate the -4% average drop.
  2. Test your own reward model for steering behavior: score AAL-prompt→AAL-completion vs. AAL-prompt→WME-completion pairs and compute Cohen's d.
  3. Analyze failure cases where WME correctly predicts "chosen" but AAL incorrectly predicts "rejected"—categorize by safety vs. helpfulness violations.

## Open Questions the Paper Calls Out
None

## Limitations
- Translation pipeline (VALUE + PhonATe) may introduce artifacts that confound dialect effects with translation errors
- The 17 evaluated reward models may not be representative of the broader RM landscape
- Lack of demographic information about original preference data annotators limits understanding of how linguistic racism propagated into training data

## Confidence

- **High Confidence**: The ~4% accuracy drop finding (RQ1) - supported by McNemar's test on 12/17 models with p < 0.05
- **Medium Confidence**: The preference and steering results (RQ2, RQ3) - effect sizes are consistent but show dataset dependency
- **Low Confidence**: Claims about causal mechanisms - the paper establishes correlation but cannot definitively prove training data composition versus translation artifacts as the primary driver

## Next Checks
1. Compare RM performance on AAL human-written texts (DG) versus machine-translated texts (RB) to quantify how much bias is amplified versus introduced by translation
2. Investigate the demographic composition of original preference data annotators to establish whether linguistic racism propagated through unrepresentative annotation
3. Measure the similarity between AAL and WME distributions in preference datasets using KL divergence or other distributional metrics to confirm domain shift magnitude