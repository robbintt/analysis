---
ver: rpa2
title: Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question
  Answering
arxiv_id: '2602.00279'
source_url: https://arxiv.org/abs/2602.00279
tags:
- accuracy
- invalid
- label
- probabilities
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first large-scale benchmark for evaluating\
  \ uncertainty quantification (UQ) methods in scientific long-form question answering,\
  \ a domain where reliable uncertainty estimates are critical due to complex reasoning\
  \ demands and the potential for hallucinations. The study evaluates four representative\
  \ UQ approaches\u2014token-level confidences, verbalized uncertainty, P(True), and\
  \ answer frequency\u2014across 20 large language models (base, instruction-tuned,\
  \ and reasoning variants) on 685,000 responses from seven scientific QA datasets."
---

# Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering

## Quick Facts
- **arXiv ID**: 2602.00279
- **Source URL**: https://arxiv.org/abs/2602.00279
- **Reference count**: 40
- **Primary result**: First large-scale benchmark evaluating uncertainty quantification methods in scientific long-form QA, revealing critical limitations of current approaches.

## Executive Summary
This paper presents the first comprehensive benchmark for evaluating uncertainty quantification (UQ) methods in scientific long-form question answering. The study systematically evaluates four UQ approaches—token-level confidences, verbalized uncertainty, P(True), and answer frequency—across 20 large language models on 685,000 responses from seven scientific QA datasets. Key findings reveal that instruction-tuning causes systematic polarization of token probability distributions, rendering them unreliable as uncertainty signals, while semantic consistency approaches show the most reliable calibration despite high computational costs. The analysis exposes critical limitations of current UQ methods and underscores the need for more robust, efficient approaches for reasoning-intensive tasks.

## Method Summary
The study evaluates four sequence-level UQ methods (Verbalized Uncertainty, P(True), Frequency of Answer, and CCP) across 20 LLMs on scientific and arithmetic QA tasks. Models generate 10 responses per prompt for the Frequency of Answer method, which clusters semantically equivalent answers to estimate confidence. Evaluation uses calibration plots comparing accuracy against confidence scores, with Expected Calibration Error (ECE) and AUROC as summary statistics. The experimental pipeline includes APriCoT prompting for multiple-choice QA and Chain-of-Thought for arithmetic, with semantic clustering implemented via NLI models for the CCP approach.

## Key Results
- Instruction-tuning causes systematic polarization of token-level probability distributions, compressing them into near-deterministic predictions that fail as uncertainty signals
- Semantic consistency approaches (Frequency of Answer) show the most reliable calibration but require 10× computational overhead
- Verbalized uncertainty approaches are systematically biased and poorly correlated with correctness
- ECE is misleading as a standalone metric when confidence scores collapse into narrow regions due to polarization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Instruction-tuning causes systematic polarization of token-level probability distributions, degrading their utility as uncertainty signals.
- **Mechanism**: Fine-tuning with supervised instruction-following datasets biases the model toward confidently committing to single outputs. This compresses the probability distribution into near-deterministic predictions (probability mass ≈1.0 on one token) even when the model lacks genuine certainty.
- **Core assumption**: Token probability mass concentration reflects overconfidence bias introduced by instruction-tuning rather than improved epistemic calibration.
- **Evidence anchors**: [abstract] "We reproduce the degradation of ECE observed... as a result of instruction-tuning"; [section 6.2] "Instruction-tuned models tend to concentrate nearly all probability mass on a single label compared to their base model counterparts... The degree of polarization is consistent across most instruction-tuned model families"
- **Break condition**: If base models show similar polarization without instruction-tuning, the mechanism is invalidated.

### Mechanism 2
- **Claim**: Semantic consistency across multiple generations (Frequency of Answer) provides reliable uncertainty quantification because it directly captures epistemic uncertainty through output diversity.
- **Mechanism**: Sampling multiple generations and clustering semantically equivalent answers measures the model's internal uncertainty: high consistency (low diversity) indicates low epistemic uncertainty; high diversity indicates genuine uncertainty.
- **Core assumption**: Answer diversity correlates with model's internal epistemic uncertainty state.
- **Evidence anchors**: [abstract] "Frequency of Answer... showed strong reliability, albeit at high computational cost"; [section 7.3] "Calibration plots confirm well-aligned confidence estimates based on Frequency of Answer"
- **Break condition**: If semantic clustering is noisy or task-ambiguous, the metric degrades; requires robust NLI or clustering.

### Mechanism 3
- **Claim**: Reasoning models' calibration depends on training pipeline (how reasoning is structured) rather than architecture or scale.
- **Mechanism**: Models trained to explore alternatives (e.g., Qwen3-Thinking, DeepSeek-R1) maintain broader probability distributions by considering multiple reasoning paths. Models trained to commit early (e.g., Magistral, GPT-oss) show polarization similar to standard instruction-tuned models.
- **Core assumption**: Provider-specific training data and objectives determine whether the reasoning process explores alternatives or commits prematurely.
- **Evidence anchors**: [section 6.2] "Differences cluster by model provider rather than by scale or architecture, we hypothesize that provider-specific training pipelines are a key underlying factor"; [section 6.2] "Magistral... reasoning process appears to commit to a single option. In contrast, DeepSeek-R1 and Qwen3 reasoning model families seem to consider multiple options"
- **Break condition**: If architecture-level changes (e.g., MoE vs dense) explain variance better than training, mechanism is invalid.

## Foundational Learning

- **Concept**: Expected Calibration Error (ECE) limitations
  - **Why needed here**: ECE is the primary metric, but the paper shows it can be misleading when distributions are polarized (high accuracy, low ECE, but no instance-level discrimination).
  - **Quick check question**: If a model assigns 0.99 confidence to all predictions and gets 99% accuracy, is it well-calibrated?

- **Concept**: Aleatoric vs Epistemic Uncertainty
  - **Why needed here**: Token probabilities capture aleatoric uncertainty well for factual retrieval but fail on reasoning tasks requiring epistemic uncertainty estimation.
  - **Quick check question**: Which type of uncertainty would explain a model's inability to distinguish between two plausible reasoning paths?

- **Concept**: Semantic clustering for answer equivalence
  - **Why needed here**: Frequency of Answer requires detecting semantic equivalence between generations (not lexical), which is non-trivial for open-ended QA.
  - **Quick check question**: Should "The capital is Paris" and "Paris is the capital" be counted as the same answer?

## Architecture Onboarding

- **Component map**: Prompt design selection -> Structured decoding for token-level experiments -> Multiple sampling (10×) for semantic consistency -> Semantic clustering (NLI or exact match depending on task) -> Calibration evaluation per method per dataset

- **Critical path**: 1) Prompt design selection (maximize label probability mass) 2) Structured decoding for token-level experiments 3) Multiple sampling (10×) for semantic consistency 4) Semantic clustering (NLI or exact match depending on task) 5) Calibration evaluation per method per dataset

- **Design tradeoffs**:
  - **Normalized vs unnormalized label probabilities**: Normalized captures relative confidence but loses total mass signal
  - **Structured decoding vs free generation**: Structured ensures valid labels but may artificially constrain the model
  - **10 samples vs more**: More samples improve Frequency of Answer reliability but increase cost quadratically

- **Failure signatures**:
  - Token probabilities stuck at 1.0 with no spread → polarization, not meaningful calibration
  - P(True) always returns 1.0 or 0.0 → same polarization issue as token-level
  - CCP scores collapse to near 0.0 for long sequences → vanishing product of token confidences
  - Verbalized scores cluster on a few discrete values (e.g., 0.95, 0.99) → not continuous, no correlation with accuracy

- **First 3 experiments**:
  1. **Polarization test**: Run token probability experiment on a base vs instruction-tuned model pair; if instruction-tuned shows >90% of mass in one bucket, polarization confirmed.
  2. **Semantic consistency baseline**: For a simple QA task, compare Frequency of Answer ECE against P(True) and Verbalized Uncertainty; expect Frequency to outperform.
  3. **Reasoning model variance**: Compare calibration plots for a reasoning model (Qwen3-Thinking) vs its instruction-tuned variant; check if reasoning process maintains probability spread.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Which specific components of provider-specific training pipelines are responsible for the systematic polarization of token probability distributions in instruction-tuned models?
- **Basis in paper**: [explicit] Section 6.2 states, "Because these differences cluster by model provider... we hypothesize that provider-specific training pipelines are a key underlying factor."
- **Why unresolved**: The paper acts as a benchmark analysis rather than an interventionist study; it observes the polarization effect across model families but does not have access to the proprietary training data or hyperparameters required to isolate the causal mechanism.
- **What evidence would resolve it**: A controlled ablation study where identical base models are fine-tuned using different instruction-tuning strategies (e.g., varying RLHF vs. SFT ratios, data diversity, or loss functions) to observe which variable induces probability mass polarization.

### Open Question 2
- **Question**: How does the internal reasoning structure of models like DeepSeek-R1 and Qwen3 allow them to mitigate confidence polarization, unlike the "commitment to a single reasoning path" seen in Magistral?
- **Basis in paper**: [explicit] Section 6.2 notes that DeepSeek-R1 and Qwen3 "seem to consider multiple options and actively retrieve relevant facts rather than committing prematurely," whereas "Magistral reasoning models... appear to commit to a single option."
- **Why unresolved**: The study evaluates the *output* probabilities and calibration plots but does not perform mechanistic interpretability on the internal attention heads or hidden states to explain *why* the reasoning processes diverge in their handling of uncertainty.
- **What evidence would resolve it**: An analysis of attention maps or hidden state representations during the reasoning process to determine if the model maintains distributed uncertainty representations across tokens or collapses them prematurely.

### Open Question 3
- **Question**: Can ECE be mathematically re-formulated to penalize the lack of discriminative power in confidence scores, preventing it from reporting misleadingly low values during probability polarization?
- **Basis in paper**: [explicit] Section 6.3 argues that ECE is "not independent