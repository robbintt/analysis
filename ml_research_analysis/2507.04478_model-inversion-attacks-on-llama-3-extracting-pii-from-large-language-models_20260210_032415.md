---
ver: rpa2
title: 'Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models'
arxiv_id: '2507.04478'
source_url: https://arxiv.org/abs/2507.04478
tags:
- data
- llama
- language
- privacy
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that model inversion attacks can extract
  sensitive information from the Llama 3.2 1B model, a smaller multilingual LLM developed
  by Meta. The study used carefully crafted prompts (e.g., "account number:", "my
  password is:", "my email id:") to query the model and successfully extracted personally
  identifiable information (PII) including email addresses, phone numbers, and account
  details.
---

# Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models

## Quick Facts
- arXiv ID: 2507.04478
- Source URL: https://arxiv.org/abs/2507.04478
- Reference count: 13
- Key outcome: Black-box model inversion attacks successfully extracted PII from Llama 3.2 1B through carefully crafted prompts

## Executive Summary
This paper demonstrates that model inversion attacks can extract sensitive personally identifiable information (PII) from the Llama 3.2 1B model, a smaller multilingual LLM developed by Meta. The study used targeted prompts like "account number:", "my password is:", and "my email id:" to query the model and successfully extracted real PII including email addresses, phone numbers, and account details. The extracted data was validated by real-world verification through Google searches. The attack achieved a non-zero memorization rate, highlighting that even smaller models retain significant privacy risks despite their reduced resource requirements. The paper recommends mitigation strategies including access control, differential privacy training, data sanitization, output filtering, and regular auditing.

## Method Summary
The attack employed a black-box model inversion approach using the Hugging Face Transformers library to query Llama 3.2 1B with carefully crafted prompts designed to elicit PII. Generation parameters were configured with top_p=1, top_k=40, max_new_tokens=50, and num_return_sequences=1 to maximize the likelihood of retrieving exact memorized sequences. The extracted PII was validated through external verification using Google search to confirm the existence of the corresponding information. The attack demonstrated that smaller, resource-efficient models are still vulnerable to privacy breaches despite their reduced parameter count.

## Key Results
- Successfully extracted real PII including email addresses, phone numbers, and account details from Llama 3.2 1B
- Achieved non-zero memorization rate through black-box extraction using targeted prompts
- Validated extracted data by confirming existence of corresponding real-world entities
- Demonstrated vulnerability in smaller models designed for edge deployment and resource-constrained environments

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Guided Autoregressive Extraction
- Claim: Targeted prompts can elicit memorized PII sequences from LLMs through conditional token generation.
- Mechanism: The attack exploits the autoregressive nature of transformer models, where each token is generated based on the probability P(sᵢ | P, s₁, ..., sᵢ₋₁). When a prompt like "my password is:" matches patterns seen during training, the model completes the sequence with high-probability tokens from memorized data. The paper's approach sets top_p=1 and top_k=40 to maximize the likelihood of retrieving exact memorized sequences rather than creative completions.
- Core assumption: The model has seen and retained specific PII patterns during pre-training on web-scraped data.
- Evidence anchors:
  - [abstract] "By querying the model with carefully crafted prompts, we demonstrate the extraction of personally identifiable information (PII) such as passwords, email addresses, and account numbers."
  - [section III] Shows the exact pipeline and parameters used for extraction
  - [corpus] Related work (Carlini et al. [3]) confirmed that larger models can leak gigabytes of training data; this paper extends findings to smaller 1B parameter models
- Break condition: If the model's training data was thoroughly sanitized for PII, or if differential privacy training sufficiently randomized the model's internal representations, the conditional probabilities would not favor PII completions.

### Mechanism 2: Memorization via Repeated Exposure in Training Data
- Claim: PII that appears multiple times in training corpora is more likely to be memorized and extractable.
- Mechanism: The paper notes that "repeated content amplifies memorization" (referencing [13]). When PII appears in multiple contexts across web-scraped data (e.g., someone's email in a forum post, a GitHub commit, a document), the model's loss function optimizes to predict these sequences accurately. Smaller models have reduced capacity but still retain non-zero memorization rates.
- Core assumption: The training corpus contained unfiltered web data with duplicated PII sequences.
- Evidence anchors:
  - [section VI] "The extracted PII suggests that Llama 3.2 1B memorizes sensitive data from its training set, likely due to the presence of unfiltered web data."
  - [section VI] References deduplication as a mitigation, implying duplication is a causal factor
  - [corpus] "Private Memorization Editing" paper (arXiv:2506.10024) confirms that memorization of PII occurs when it appears "among huge amounts of uncontrolled data"
- Break condition: If data was deduplicated and PII occurrences were limited to single appearances, memorization would be significantly reduced.

### Mechanism 3: Black-Box Extraction via Output Likelihood Analysis
- Claim: Attackers do not need model internals; pure output observation suffices for extraction.
- Mechanism: The attack operates entirely through the inference API. By observing generated outputs and their likelihoods (implicitly, through repetition and consistency), an attacker can identify memorized content. The paper validates extracted data through external verification (Google search, confirming the LinkedIn profile and bank address exist).
- Core assumption: The model has query access without strict rate-limiting or output filtering.
- Evidence anchors:
  - [section III] "We conducted a black-box model inversion attack"
  - [section V] "The PII data was validated by doing a google search of the corresponding data to prove that it was a memorized content"
  - [corpus] Related paper "Data-Free Privacy-Preserving for LLMs" (arXiv:2601.15595) assumes attackers "predominantly depend on access to the training data" but this paper shows training data access is not required for extraction
- Break condition: If API access includes strict rate-limiting, CAPTCHA challenges, or output filtering for PII patterns, the attack's scalability and success rate would be severely limited.

## Foundational Learning

- Concept: **Autoregressive Language Modeling**
  - Why needed here: The attack fundamentally exploits how autoregressive models predict the next token. Understanding P(sᵢ | context) is essential to grasp why prompts trigger specific completions.
  - Quick check question: Given the prompt "account number:", what determines the next token the model generates?

- Concept: **Memorization vs. Generalization in Neural Networks**
  - Why needed here: The paper's core claim is that LLMs memorize training data rather than just learning patterns. Distinguishing these is critical for understanding privacy risks.
  - Quick check question: If a model outputs an exact email address from its training data, is that generalization or memorization? How would you test which it is?

- Concept: **Differential Privacy (DP-SGD)**
  - Why needed here: The paper recommends DP training as a mitigation. Understanding how gradient noise injection limits individual sample influence is necessary to evaluate this defense.
  - Quick check question: Why does adding noise to gradients during training reduce the model's ability to memorize any single training example?

## Architecture Onboarding

- Component map:
[Attacker] → (prompt query) → [LLM API / Local Model] → (generated output) → [Candidate PII]
                                                              ↓
                                                    [External Validation]
                                                    (Google search, verification)

- Critical path:
  1. Identify high-probability PII prompts (passwords, emails, account numbers)
  2. Configure generation parameters for exact retrieval (top_p=1, top_k=40, max_new_tokens=50)
  3. Execute queries against target model
  4. Validate extracted data through external sources
  5. (Defense) Implement output filtering, access controls, or retrain with DP

- Design tradeoffs:
  - **Attack parameter tuning**: Higher temperature increases diversity but reduces exact memorization retrieval; lower temperature increases precision but may miss varied PII formats
  - **Model size vs. privacy**: Smaller models (1B parameters) are more efficient for edge deployment but still vulnerable; larger models memorize more but are harder to deploy locally
  - **Mitigation cost**: Differential privacy training reduces model utility (accuracy loss); data sanitization requires significant preprocessing effort; output filtering may block legitimate uses

- Failure signatures:
  - **Attack failure**: Model refuses to generate PII-like patterns (output filtering active); queries are rate-limited or blocked; model returns generic/creative completions instead of specific data
  - **Defense failure**: Extracted PII matches real individuals (confirmed via external search); consistent reproduction of the same PII across multiple queries indicates memorization
  - **Validation failure**: Generated PII does not correspond to any real entity (hallucination rather than memorization)

- First 3 experiments:
  1. **Baseline extraction test**: Run the exact code from Section III against Llama-3.2-1B with the three provided prompts; record which outputs contain PII-like patterns and validate via external search
  2. **Prompt variation analysis**: Test semantically similar prompts (e.g., "my pass:", "pwd:", "login code:") to measure if extraction success depends on exact prompt wording or conceptual patterns
  3. **Defense efficacy test**: Implement regex-based output filtering for email/phone patterns and measure the reduction in successful PII extraction; note any false positives blocking legitimate outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What standardized metrics can be developed to effectively evaluate memorization risks across different LLMs?
- Basis in paper: [explicit] The authors explicitly state in the Discussion and Conclusion that "future work should explore standardized metrics for evaluating memorization risks."
- Why unresolved: Current research relies on varying methodologies for quantifying extraction, making consistent cross-model comparison difficult.
- What evidence would resolve it: The adoption of a benchmark suite or universal statistical measure accepted by the privacy research community.

### Open Question 2
- Question: How does the privacy risk trade-off specifically function between model size and memorization capacity in sub-7B parameter models?
- Basis in paper: [explicit] The Extended Analysis section notes that "the trade-off between model size and privacy risk remains an open question" regarding smaller, edge-optimized models.
- Why unresolved: While larger models are known to be vulnerable, it is unclear if reduced parameter counts naturally provide sufficient protection or merely require more query effort.
- What evidence would resolve it: A comparative study measuring extraction success rates (memorization) relative to parameter count across the Llama 3 model family.

### Open Question 3
- Question: What scalable defense mechanisms can mitigate model inversion without significantly degrading model utility?
- Basis in paper: [explicit] The authors call for "further research into privacy-preserving machine learning techniques" and specifically "scalable defense mechanisms" in Section VIII.
- Why unresolved: Existing mitigations like differential privacy or strict output filtering often hurt model performance or are computationally expensive to implement at scale.
- What evidence would resolve it: A defense strategy that maintains benchmark performance (e.g., MMLU scores) while reducing the extraction rate to zero.

### Open Question 4
- Question: What is the specific memorization rate for Llama 3.2 1B when tested against a broader range of diverse prompts?
- Basis in paper: [inferred] Section VIII lists the "lack of specific memorization rates for Llama 3.2 1B" and the "need for broader testing across diverse prompts" as limitations.
- Why unresolved: The study confirmed the existence of extracted PII (a non-zero rate) but did not quantify the probability of extraction across the entire potential prompt space.
- What evidence would resolve it: A large-scale automated audit providing a statistical percentage of successful PII extraction per 1,000 queries.

## Limitations

- Small sample size of attack queries with missing quantitative data on total queries and overall success rate
- Cannot definitively prove memorization versus coincidental pattern matching or hallucination in black-box setting
- Limited scope to only Llama 3.2 1B without comparison to larger models or different model families
- External validation confirms PII exists but cannot establish exact training data attribution

## Confidence

**High Confidence**: The mechanism of prompt-guided autoregressive extraction is well-established in the literature. The paper correctly identifies that transformer models generate tokens based on conditional probabilities, and that carefully crafted prompts can elicit memorized sequences. This follows from the fundamental architecture of LLMs.

**Medium Confidence**: The claim that repeated exposure in training data amplifies memorization is supported by related work but not directly proven in this study. The paper references deduplication as a mitigation without providing evidence that duplication was present in Llama 3.2's training corpus.

**Low Confidence**: The practical significance of the attack is unclear due to missing quantitative data. Without knowing how many queries were attempted and the overall success rate, it's impossible to assess whether this represents a meaningful security vulnerability or a theoretical proof-of-concept.

## Next Checks

1. **Quantitative Attack Analysis**: Reproduce the attack with systematic logging of total queries, successful extractions, and false positive rates. Calculate the exact memorization rate with confidence intervals across multiple random seeds.

2. **Training Data Attribution**: If model weights are accessible, use techniques from "Private Memorization Editing" (arXiv:2506.10024) to trace extracted PII back to specific training examples and quantify duplication frequency.

3. **Defense Efficacy Benchmarking**: Implement and test each recommended mitigation (differential privacy training, output filtering, access controls) and measure the trade-off between privacy improvement and model utility degradation using standard benchmarks.