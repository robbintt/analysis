---
ver: rpa2
title: Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs
arxiv_id: '2512.22631'
source_url: https://arxiv.org/abs/2512.22631
tags:
- grpo
- reasoning
- arxiv
- faithfulness
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates two optimization methods, GRPO and DPO, for
  improving faithfulness in chain-of-thought reasoning across Qwen2.5 models (1.5B-14B
  parameters). GRPO achieved superior faithfulness metrics at larger scales, with
  the 14B model showing +56.4% higher NLI faithfulness and +29.9% higher LLM-Judge
  scores compared to DPO.
---

# Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs
## Quick Facts
- arXiv ID: 2512.22631
- Source URL: https://arxiv.org/abs/2512.22631
- Reference count: 12
- Qwen2.5 models (1.5B-14B parameters) show GRPO achieving +56.4% higher NLI faithfulness and +29.9% higher LLM-Judge scores than DPO at 14B scale

## Executive Summary
This paper evaluates two optimization methods, GRPO and DPO, for improving faithfulness in chain-of-thought reasoning across Qwen2.5 models (1.5B-14B parameters). GRPO achieved superior faithfulness metrics at larger scales, with the 14B model showing +56.4% higher NLI faithfulness and +29.9% higher LLM-Judge scores compared to DPO. While both methods benefited from increased model size, GRPO demonstrated higher potential for faithful reasoning but exhibited less stable behavior at smaller scales (3B showed dramatic performance drops). The results indicate GRPO delivers stronger faithfulness at the cost of scale sensitivity and tuning effort, whereas DPO provides steadier, more accessible gains. The study highlights the trade-off between optimization efficiency, reasoning faithfulness, and model scalability, suggesting GRPO as a promising direction for developing more transparent and trustworthy reasoning in LLMs.

## Method Summary
The study compared Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO) for enhancing faithfulness in chain-of-thought reasoning across Qwen2.5 models ranging from 1.5B to 14B parameters. Both methods were trained using preference datasets where reasoning traces were evaluated for faithfulness. The evaluation employed two metrics: NLI faithfulness (measuring logical consistency) and LLM-Judge (automated assessment of reasoning quality). The experiments systematically varied model scale to examine how optimization effectiveness scales with parameter count, with particular attention to stability and performance trade-offs between the two methods.

## Key Results
- GRPO achieved +56.4% higher NLI faithfulness and +29.9% higher LLM-Judge scores than DPO at 14B parameter scale
- Both methods showed improved faithfulness with larger models, but GRPO demonstrated higher potential for faithful reasoning
- 3B parameter models showed dramatic performance drops with GRPO, indicating scale sensitivity and stability issues

## Why This Works (Mechanism)
Unknown: The paper does not explicitly explain the mechanistic reasons why GRPO outperforms DPO in faithfulness. Potential factors could include: 1) GRPO's policy gradient approach may better capture reward structure differences between faithful and unfaithful reasoning, 2) Group-relative comparisons in GRPO might help distinguish subtle reasoning quality differences more effectively than DPO's direct optimization, or 3) The stochastic nature of GRPO could provide better exploration of the reasoning space. However, these remain speculative without explicit mechanistic analysis from the authors.

## Foundational Learning
- **Reinforcement Learning vs Direct Preference Optimization**: Understanding the fundamental differences between policy gradient methods (GRPO) and supervised learning approaches (DPO) is crucial for interpreting why GRPO might achieve higher faithfulness but with stability trade-offs
- **Chain-of-Thought Reasoning**: Familiarity with how LLMs generate intermediate reasoning steps is essential to understand what "faithfulness" means and why it matters for trustworthy AI
- **Model Scale Effects**: The relationship between parameter count and optimization effectiveness needs to be understood to interpret the scale-dependent performance patterns observed

## Architecture Onboarding
- **Component Map**: Qwen2.5 model -> Chain-of-Thought generation -> Faithfulness evaluation -> GRPO/DPO optimization -> Updated model weights
- **Critical Path**: Model initialization → Preference dataset preparation → Optimization training → Faithfulness evaluation → Performance analysis
- **Design Tradeoffs**: GRPO vs DPO efficiency (higher faithfulness vs stability), model scale vs optimization effectiveness, faithfulness metrics vs practical reasoning quality
- **Failure Signatures**: Performance drops at smaller scales (3B model), optimization instability, metric subjectivity
- **First Experiments**: 1) Replicate baseline results on Qwen2.5 14B, 2) Test GRPO stability across learning rates, 3) Compare NLI vs LLM-Judge correlation

## Open Questions the Paper Calls Out
Assumption: The paper does not explicitly call out specific open questions, but based on the limitations section, potential open questions include: 1) How generalizable are these results to other model architectures beyond Qwen2.5? 2) What are the fundamental algorithmic differences between GRPO and DPO that lead to the observed performance gap? 3) How can the stability issues at smaller scales be addressed while maintaining faithfulness improvements? 4) Do the faithfulness metrics correlate with actual downstream reasoning performance?

## Limitations
- Evaluation limited to Qwen2.5 models (1.5B-14B parameters), uncertain generalizability to other architectures
- Training methodology details are sparse, making it difficult to assess whether differences stem from algorithmic properties or implementation choices
- Evaluation metrics (NLI faithfulness and LLM-Judge) have inherent subjectivity and may not fully capture reasoning quality or practical utility

## Confidence
- **GRPO vs DPO Effectiveness**: High confidence - Large effect sizes and consistent improvements
- **Scale Sensitivity and Stability**: Medium confidence - Striking results but insufficient diagnostic information
- **Trade-off Characterization**: Medium confidence - Limited experiments without systematic ablation studies

## Next Checks
1. Cross-Architecture Validation: Replicate the GRPO/DPO comparison using multiple model families (Llama, Mistral, GPT) to determine if the observed performance patterns are architecture-specific or generalizable across LLMs.

2. Stability Analysis: Conduct hyperparameter sensitivity experiments varying learning rates, batch sizes, and reward scaling for both methods across all model sizes to isolate whether the stability issues are inherent to GRPO or implementation-dependent.

3. Downstream Task Evaluation: Test whether the improved faithfulness in reasoning translates to better final answer accuracy on real-world reasoning tasks (math word problems, logical inference, multi-step planning) beyond the evaluation metrics used in this study.