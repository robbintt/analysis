---
ver: rpa2
title: 'TabularQGAN: A Quantum Generative Model for Tabular Data'
arxiv_id: '2505.22533'
source_url: https://arxiv.org/abs/2505.22533
tags:
- data
- quantum
- encoding
- classical
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TabularQGAN, a quantum generative adversarial
  network for synthesizing heterogeneous tabular data. The approach uses a novel quantum
  circuit ansatz with flexible data encoding, employing Givens rotations to handle
  one-hot encoded categorical features and a layered structure for numerical features.
---

# TabularQGAN: A Quantum Generative Model for Tabular Data

## Quick Facts
- arXiv ID: 2505.22533
- Source URL: https://arxiv.org/abs/2505.22533
- Reference count: 40
- Primary result: Quantum generative adversarial network for heterogeneous tabular data that achieves 8.5% better overall similarity than classical models using only 0.072% of the parameters

## Executive Summary
This paper introduces TabularQGAN, a quantum-classical hybrid generative adversarial network designed to synthesize heterogeneous tabular data containing both numerical and categorical features. The approach employs a novel quantum circuit ansatz using Givens rotations to preserve one-hot encoding constraints for categorical variables while leveraging a layered structure for numerical features. Evaluated on MIMIC III healthcare and Adult Census datasets, the model demonstrates superior performance to classical baselines CTGAN and CopulaGAN across multiple metrics while requiring significantly fewer parameters. The work represents a practical step toward quantum advantage in tabular data generation.

## Method Summary
TabularQGAN is a hybrid quantum-classical GAN where a variational quantum circuit (VQC) serves as the generator and a classical neural network as the discriminator. The quantum generator uses Givens rotations to maintain one-hot encoding constraints for categorical features and a layered ansatz with IsingYY gates and controlled rotations for numerical features. The model is trained using the parameter-shift rule to compute quantum gradients, with alternating updates between generator and discriminator. Data is encoded through discretization of numerical features into 2^N bins and one-hot encoding of categorical features. The architecture was evaluated on MIMIC III and Adult Census datasets using SDMetrics overall similarity, overlap fraction, and downstream predictive performance.

## Key Results
- Achieved 8.5% better overall similarity than classical baselines (CTGAN, CopulaGAN)
- Used only 0.037%-0.104% of the parameters compared to classical models (37-104 vs 65,536-262,144 parameters)
- Generated novel samples with strong generalization capabilities
- Demonstrated superior downstream predictive performance on classification and regression tasks

## Why This Works (Mechanism)

### Mechanism 1: Givens Rotations Preserve One-Hot Encoding Structure
The Givens rotations constrain the quantum state space to only valid one-hot encodings, eliminating impossible categorical combinations. By restricting operations to particle-conserving unitaries (constant Hamming weight k), Givens rotations rotate amplitudes only between basis states with exactly k excitations. For a 3-category feature, only |001⟩, |010⟩, |100⟩ are accessible—states like |011⟩ are structurally impossible. This mechanism assumes categorical features are mutually exclusive and that the training distribution respects this constraint.

### Mechanism 2: Parameter Efficiency via Constrained Quantum Expressivity
The quantum generator achieves comparable or better distribution learning with ~0.072% of classical parameters by exploiting superposition to represent probability distributions over 2^n bitstrings with O(n) parameters. Classical GANs require O(layer_width²) parameters. The constrained ansatz (Givens rotations + layered entanglement) limits the hypothesis space to plausible tabular distributions. This mechanism assumes the target distribution can be well-approximated by the specific circuit ansatz without overparameterization.

### Mechanism 3: Hybrid Gradient Flow via Parameter-Shift Rule
Quantum generator gradients are computed without backpropagation through the quantum circuit using the parameter-shift rule. For each parameter θ, gradient ∂⟨M⟩/∂θ = ½[⟨M⟩θ+π/2 − ⟨M⟩θ−π/2]. The classical discriminator provides the loss signal; the quantum circuit is evaluated at shifted parameters, measured, and gradients estimated from output differences. This mechanism assumes the parameter-shift rule provides unbiased gradient estimates and that shot noise from finite measurements doesn't destabilize training.

## Foundational Learning

- **Variational Quantum Circuits (VQCs)**: The generator is a VQC—understanding how parameterized gates prepare quantum states is essential. Quick check: Can you explain how applying RY(θ) to |0⟩ creates a superposition of |0⟩ and |1⟩ with amplitude cos(θ/2) and sin(θ/2)?

- **One-Hot Encoding and Hamming Weight**: The Givens rotation mechanism depends on understanding why |100⟩, |010⟩, |001⟩ all have Hamming weight 1. Quick check: Given a 4-category feature, what are all valid one-hot states and what is their Hamming weight?

- **GAN Training Dynamics (Generator-Discriminator Game)**: The model is a GAN; understanding the adversarial loss and training alternation is critical. Quick check: Why does the generator loss LG = -E[log D(x')] encourage generating realistic samples?

## Architecture Onboarding

- **Component map**: Input Data → Encoding Layer (numerical bins + one-hot categorical) → Quantum Generator (VQC) → Computational Basis Measurement → Bitstring → Classical Discriminator (3-layer MLP) → Adversarial Loss → Parameter Updates

- **Critical path**:
  1. Encoding correctness: Ensure each feature maps to the correct qubits (verify with Appendix A.1 examples)
  2. Circuit construction: Build the ansatz with correct gate placements; verify Givens rotations preserve Hamming weight
  3. Training loop: Implement parameter-shift correctly; verify gradient magnitudes are reasonable

- **Design tradeoffs**:
  - Boolean vs. Non-Boolean encoding: Boolean saves qubits for binary features but merges them into numerical register
  - Circuit depth: Deeper circuits improve expressivity but increase training cost and barren plateau risk
  - Qubit budget allocation: More qubits per numerical feature = finer discretization but larger state space

- **Failure signatures**:
  - Invalid categorical outputs: If Givens rotations are misconfigured, one-hot constraint breaks (e.g., |011⟩ appears)
  - Mode collapse: Generator produces limited sample diversity; check overlap fraction → 1.0
  - Training divergence: Discriminator loss → 0 too quickly; reduce discriminator learning rate
  - Barren plateaus: Gradients vanish for deep circuits on many qubits; monitor gradient variance

- **First 3 experiments**:
  1. Sanity check on toy data: Create a 2-feature dataset (1 numerical, 1 binary categorical). Verify the model learns the correct joint distribution by plotting real vs. synthetic samples
  2. Ablate Givens rotations: Replace single-excitation gates with generic rotations. Confirm that invalid categorical states appear and overall metric drops
  3. Scale test: Gradually increase qubit count from 5 → 10 → 15. Monitor training stability, gradient variance, and overall metric to identify scaling limits

## Open Questions the Paper Calls Out

- **Scalability to higher dimensionality**: Can the architecture scale to datasets with significantly higher dimensionality without succumbing to barren plateau problems? Current experiments were limited to 10-15 qubits due to classical simulation costs.

- **Quantum hardware noise impact**: How does the presence of quantum hardware noise and limited fidelity affect the quality and utility of generated synthetic data? All current results rely on noiseless state vector simulations.

- **Continuous numerical data handling**: Can a quantum generative model be designed to natively handle continuous numerical data without information loss from discretization? Current approach requires binning numerical values.

- **Alternative circuit ansatzes**: Do alternative circuit ansatzes or data encoding strategies exist that improve upon the Givens rotation approach for complex categorical correlations? The current "black-box nature" makes it difficult to attribute performance specifically to the chosen architecture.

## Limitations
- Limited scalability demonstrated (only 10-15 qubit systems tested)
- Assumes mutually-exclusive categorical features (one-hot encoding constraint)
- Requires discretization of numerical features, introducing information loss
- No validation on real quantum hardware to assess noise resilience

## Confidence
- **High confidence**: Parameter efficiency metrics, basic GAN training mechanics, SDMetrics computation methodology
- **Medium confidence**: Overall performance superiority (8.5% average improvement), categorical encoding via Givens rotations, quantum-classical hybrid training stability
- **Low confidence**: Scalability beyond 15 qubits, noise resilience in real hardware, generalization to non-mutually-exclusive categorical features

## Next Checks
1. **Expressivity validation**: Systematically vary circuit depth (1-8 layers) and measure performance degradation on MIMIC III to identify expressivity limits and establish depth requirements for different dataset complexities.

2. **Noise resilience testing**: Implement the same model on both simulators and real quantum hardware (IBM Quantum, Rigetti) using identical parameters, comparing SDMetrics performance to quantify noise impact and identify hardware requirements.

3. **Multi-label categorical validation**: Modify Adult Census dataset to create overlapping categorical features (e.g., multiple occupations per person) and test whether Givens rotations still produce valid samples or whether the model requires architectural modifications.