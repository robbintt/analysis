---
ver: rpa2
title: Efficient Evaluation of LLM Performance with Statistical Guarantees
arxiv_id: '2601.20251'
source_url: https://arxiv.org/abs/2601.20251
tags:
- budget
- evaluation
- sampling
- coverage
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses efficient evaluation of LLM performance by
  framing benchmarking as finite-population inference, seeking tight confidence intervals
  for model accuracy under a fixed query budget. It proposes Factorized Active Querying
  (FAQ), which leverages historical data via a Bayesian factor model, uses a hybrid
  variance-reduction/active-learning sampling policy, and maintains coverage through
  Proactive Active Inference (PAI).
---

# Efficient Evaluation of LLM Performance with Statistical Guarantees

## Quick Facts
- arXiv ID: 2601.20251
- Source URL: https://arxiv.org/abs/2601.20251
- Reference count: 40
- Key outcome: FAQ achieves up to 5× effective sample size gains over strong baselines on two benchmark suites across varying historical-data missingness levels

## Executive Summary
This paper addresses efficient evaluation of LLM performance by framing benchmarking as finite-population inference, seeking tight confidence intervals for model accuracy under a fixed query budget. It proposes Factorized Active Querying (FAQ), which leverages historical data via a Bayesian factor model, uses a hybrid variance-reduction/active-learning sampling policy, and maintains coverage through Proactive Active Inference (PAI). FAQ delivers up to 5× effective sample size gains over strong baselines on two benchmark suites across varying historical-data missingness levels, meaning it matches uniform sampling's CI width using up to 5× fewer queries. The method is validated on two benchmark suites with 4.4K+ models and 21.5K+ questions, showing stable coverage across model release dates and accuracies. Source code and curated datasets are released to support reproducible evaluation and future research.

## Method Summary
The paper proposes Factorized Active Querying (FAQ) for efficient LLM benchmarking under fixed query budgets. FAQ leverages historical model-question outcomes through a Bayesian factor model, where each model and question is assigned a latent factor vector. For a new model, the factor posterior is updated online via Laplace approximation after each query, sharpening predictions on unqueried questions. The method uses Proactive Active Inference (PAI) for unbiased estimation and valid confidence intervals under adaptive sampling, combined with a hybrid sampling policy that balances variance reduction and active learning. The approach maintains frequentist coverage guarantees while significantly reducing the number of queries needed to achieve tight confidence intervals.

## Key Results
- FAQ achieves up to 5× effective sample size gains over strong baselines on two benchmark suites
- Coverage remains stable at nominal 95% level across varying historical-data missingness levels (100% to 0.1% observed)
- Performance advantage is most pronounced at low query budgets (2.5-10% of question bank)
- Method validated on 4.4K+ models and 21.5K+ questions across MMLU-Pro and BBH+GPQA+IFEval+MATH+MuSR suites

## Why This Works (Mechanism)

### Mechanism 1
The factor model extracts transferable structure from historical model-question outcomes, enabling accurate predictions for new model-question pairs without querying them. Logistic factor model with model factors u_i ∈ R^k and question factors v_j ∈ R^k models P(H_ij = 1) = σ(u_i^T v_j). For a new model, factor posterior is updated online via Laplace approximation after each query, sharpening predictions on unqueried questions. Historical models share latent capability structure with the new model; question factors are transferable across models. The paper explicitly states "leverages historical information through a Bayesian factor model" and provides the mathematical formulation in section 3.1. Related work on probabilistic matrix factorization is cited, though corpus papers focus on unrelated quantization/conformal methods. Distribution shift in new models (e.g., novel architectures) renders historical factors uninformative; coverage remains valid but efficiency gains degrade toward uniform sampling.

### Mechanism 2
The Proactive Active Inference (PAI) estimator is unbiased and asymptotically normal, enabling valid frequentist confidence intervals under adaptive sampling. The estimator θ̂_nb = (1/n_b) Σ φ_t uses inverse-probability weighting with φ_t = (1/N_q)Σ p̂_j^(t-1) + (1/N_q)(z_I_t - p̂_I_t^(t-1))/q_t(I_t). The martingale structure ensures E[θ̂_nb] = θ, and a martingale CLT yields asymptotic normality. Sampling probabilities q_t(·) are strictly positive and F_t-1-measurable; variance stabilization and Lindeberg conditions hold. The abstract states "maintains validity through Proactive Active Inference...that enables direct question selection while preserving coverage" and Theorem 3.1 proves unbiasedness and asymptotic normality. Zrnic & Candès (2024) is cited as the foundation for active inference. Ad-hoc without-replacement sampling breaks the martingale structure, causing coverage degradation at moderate/large budgets.

### Mechanism 3
The hybrid sampling policy allocates queries to maximize variance reduction while rapidly learning the new model's factor. Combines (1) oracle-inspired scores s_o(j) ∝ √(p̂_j(1-p̂_j)) for variance reduction and (2) active-learning scores s_a(j) prioritizing questions that maximally reduce posterior variance of θ. Time-varying mixing α_t and tempering β_t balance exploration/exploitation; τ-mixing ensures all questions retain selection probability. Factor model is approximately correct; active-learning objective aligns with θ inference. The abstract mentions "adaptively selects questions using a hybrid variance-reduction/active-learning sampling policy" and section 3.3-3.5 derive the oracle-optimal q* and define the hybrid policy. No corpus papers validate this specific hybrid design; active learning literature is cited but not in corpus. Extreme sparsity (0.1% observed entries) makes factor estimates noisy; overfitting can degrade efficiency at larger budgets.

## Foundational Learning

- **Martingale Central Limit Theorem**
  - Why needed here: Theoretical guarantee that PAI estimator is asymptotically normal under adaptive sampling; enables valid CI construction without i.i.d. assumptions
  - Quick check question: Can you explain why E[φ_t | F_{t-1}] = θ ensures θ̂_nb is unbiased, and why this would fail if sampling probabilities depended on future outcomes?

- **Inverse Propensity Weighting (IPW)**
  - Why needed here: Core of PAI estimator—corrects bias from non-uniform sampling by reweighting observations by 1/q_t(I_t)
  - Quick check question: If q_t(j) = 0.01 for some question j and that question is selected, what happens to the IPW term? Why does τ-mixing prevent this?

- **Laplace Approximation for Bayesian Logistic Regression**
  - Why needed here: Enables efficient online posterior updates for the model factor after each query without full MCMC
  - Quick check question: Given prior u ~ N(m, S) and observation z ~ Bern(σ(u^T v)), derive the approximate posterior mean and covariance update (hint: Sherman-Morrison formula)

## Architecture Onboarding

- Component map: Historical Data H → Factor Model Fitting (offline) → Question Factors {v_j} (frozen) → New Model → Initialize u ~ N(û^(0), Σ̂^(0)) → Loop: Compute q_t → Sample I_t → Query z_I_t → Update (û, Σ̂) via Laplace → Compute φ_t → After n_b rounds: Compute σ̂_nb → Return θ̂_nb ± z_{1-α/2} σ̂_nb / √n_b

- Critical path: Factor model training on historical data (one-time) → Online query loop (per-model): compute sampling distribution → query → update factor → accumulate PAI terms → compute CI

- Design tradeoffs:
  - Sampling with vs. without replacement: With-replacement preserves martingale structure and coverage guarantees; without-replacement can fail at moderate/large budgets
  - τ-mixing strength: Higher τ (e.g., 0.75) improves coverage stability but reduces efficiency; lower τ (e.g., 0.05) increases efficiency but requires accurate factor model
  - Budget allocation between active-learning (α_t) and variance-reduction (1-α_t): Early exploration improves factor learning; late-stage exploitation minimizes variance

- Failure signatures:
  - Coverage below nominal level (e.g., <90% for 95% target) → Check: Is without-replacement sampling being used? Are extreme q_t values occurring (τ too low)?
  - CI widths no better than uniform → Check: Is historical data too sparse? Are question factors uninformative (k too small or λ too large)?
  - Per-model coverage varying systematically by release date → Indicates distribution shift; factor model may need periodic refitting

- First 3 experiments:
  1. Validation on held-out historical models: Split H into train/val; fit factor model on train; run FAQ on val models with known ground truth; verify coverage and ESS across budgets
  2. Ablation of sampling with/without replacement: On MMLU-Pro with full historical data, compare FAQ variants at budgets 500-3000; plot coverage and RMSE
  3. Missingness sensitivity: Induce MCAR missingness at {100%, 12%, 1%, 0.1%} observation rates; measure ESS degradation to determine minimum viable historical data for your setting

## Open Questions the Paper Calls Out

### Open Question 1
Can a three-stage hybrid policy (active-learning early, oracle-style variance reduction mid-budget, traditional active inference late) combine the strengths of FAQ and sequential active inference across all budget regimes? Page 9 states that ablations "motivate three-stage hybrid designs that incorporate traditional active inference at larger budgets." This remains unresolved because FAQ outperforms at low budgets but traditional active inference overtakes it at large budgets; no combined approach has been tested. Empirical demonstration that a three-stage policy achieves superior ESS across the full budget range without coverage degradation would resolve this.

### Open Question 2
Can FAQ be extended to non-binary feedback (e.g., ordinal Likert scales, continuous quality scores) while preserving frequentist coverage guarantees? Page 9 lists "extending FAQ to non-binary feedback" as a concrete limitation. This remains unresolved because the current PAI estimator and martingale CLT proof rely on binary outcomes {0,1}; the factor model and variance estimator need generalization. Theoretical extension of Theorem 3.1 to bounded continuous outcomes, plus empirical validation on non-binary benchmarks with maintained coverage, would resolve this.

### Open Question 3
How should question factors be periodically updated as new model evaluations accumulate, and what efficiency gains result? Page 9 notes: "we treat question factors as fixed from historical data: periodically updating them could better track drift and improve efficiency." This remains unresolved because current design freezes question factors {vj} after historical fitting; new models may exhibit capability drift not captured by stale factors. Comparison of fixed vs. online-updated question factors on temporally-held-out models, measuring ESS gains and coverage stability, would resolve this.

### Open Question 4
Can metadata-informed priors (e.g., parameter count, base model family) improve low-budget performance without compromising coverage? Page 9 identifies "richer, metadata-informed priors" as a direction for improving low-budget performance. This remains unresolved because current factor initialization uses empirical mean/covariance of historical factors; this ignores available model metadata that could reduce initial uncertainty. Controlled experiments varying prior informativeness via metadata, showing faster early-stage factor convergence and narrower CIs at budgets <5% of the question bank, would resolve this.

## Limitations
- Efficiency gains rely on strong transferability assumptions from historical data; distribution shifts in new models could degrade gains toward uniform sampling without breaking coverage guarantees
- Factor model's ability to capture question difficulty variation across diverse benchmarks remains empirical; extreme sparsity (0.1% observed entries) can cause noisy factor estimates and reduced efficiency
- Without-replacement sampling, while computationally attractive, breaks the martingale structure and can cause coverage degradation at moderate to large budgets

## Confidence

- **High Confidence**: The unbiasedness and asymptotic normality of the PAI estimator under adaptive sampling (Theorem 3.1); the coverage guarantee through martingale CLT
- **Medium Confidence**: The hybrid sampling policy's ability to balance variance reduction and active learning; the factor model's transferability across model architectures
- **Low Confidence**: The method's robustness to extreme distribution shifts; the optimal hyperparameter settings (ρ, γ, β₀, τ) across diverse benchmark suites

## Next Checks

1. **Distribution Shift Sensitivity**: Evaluate FAQ on models with known architectural differences (e.g., decoder-only vs. encoder-decoder) to quantify efficiency loss under distribution shift
2. **Factor Model Generalization**: Test whether factor factors learned from one benchmark suite (e.g., MMLU-Pro) transfer effectively to questions from other suites (e.g., GPQA)
3. **Budget Sensitivity Analysis**: Systematically vary budget levels below 10% and above 25% to determine the operational range where FAQ maintains both coverage and efficiency advantages