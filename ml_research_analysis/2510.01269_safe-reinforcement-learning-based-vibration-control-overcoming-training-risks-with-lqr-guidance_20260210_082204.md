---
ver: rpa2
title: 'Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks
  with LQR Guidance'
arxiv_id: '2510.01269'
source_url: https://arxiv.org/abs/2510.01269
tags:
- control
- system
- policy
- structural
- controller
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the safety risks associated with training
  reinforcement learning (RL) controllers for vibration control on physical structures.
  During training, RL controllers apply random control forces that can cause excessive
  vibrations and potential structural damage.
---

# Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance

## Quick Facts
- arXiv ID: 2510.01269
- Source URL: https://arxiv.org/abs/2510.01269
- Reference count: 11
- Primary result: LQR-guided RL reduces training vibrations while achieving superior testing performance

## Executive Summary
This paper addresses the safety risks inherent in training reinforcement learning (RL) controllers for vibration control on physical structures. During RL training, random exploration actions can generate excessive control forces that cause dangerous vibrations and potential structural damage. The authors propose a hybrid framework where a Linear Quadratic Regulator (LQR) controller, designed using arbitrarily assumed system parameters, guides the RL controller during training. This approach significantly reduces acceleration responses during training while maintaining model-free operation. The final trained RL policy outperforms standalone LQR controllers in testing scenarios, validating the effectiveness of the hybrid approach for safe and effective vibration control.

## Method Summary
The framework uses a Lyapunov-based Actor-Critic (LAC) algorithm where the control action combines RL output with LQR guidance: u = f_θ(s) + αf_ψ(x,ẋ). The LQR controller is designed offline using assumed system parameters (m=1.6, c=-0.5, k=181), which differ from the true nonlinear system (m_true=1, c_true=0.4, k_true=100, k'_true=1). During training, the LQR term provides stabilizing guidance that constrains exploration, while the RL component learns to compensate for model mismatch and nonlinearity. The method is validated on a single-degree-of-freedom Duffing oscillator with stochastic ground excitation via Kanai-Tajimi filter.

## Key Results
- LQR-guided RL training reduces acceleration responses to levels comparable with uncontrolled response, while naive RL training produces "magnitudes higher" accelerations (Figure 5)
- The final trained RL policy outperforms standalone LQR in both control effort and acceleration response during testing (Figure 6)
- The framework maintains model-free operation by using assumed parameters for LQR design, eliminating need for system identification

## Why This Works (Mechanism)

### Mechanism 1
An LQR controller derived from an arbitrarily incorrect model still reduces vibrations compared to no control because both assumed and true structural models share qualitative dynamic characteristics. Spring restorative forces opposing displacement remain present in both systems, so the LQR gain matrix, even when suboptimal, produces control forces that generally oppose motion rather than amplify it. This provides baseline damping during training.

### Mechanism 2
LQR guidance reduces dangerous exploration during RL training by constraining the action space toward stabilizing behaviors. The hybrid policy u = f_θ(s) + αf_ψ(x,ẋ) superimposes RL output with LQR output. During early training, RL component f_θ(s) outputs near-random actions, but αf_ψ(x,ẋ) provides consistent stabilizing contribution. The LQR term acts as a safety baseline that the RL can improve upon but cannot easily worsen beyond.

### Mechanism 3
The trained RL policy outperforms the standalone LQR policy used for guidance because the RL component learns to compensate for model mismatch and nonlinearity that the incorrect-model LQR cannot address. The reward function penalizes displacement, acceleration, and control effort, allowing RL to discover more efficient force profiles than the suboptimal LQR.

## Foundational Learning

- **Linear Quadratic Regulator (LQR) fundamentals**: Understanding how LQR computes optimal gains from state-space matrices (A, B) and weight matrices (Q, R) is essential to grasp why an incorrect model still yields a functional (if suboptimal) controller. *Quick check*: Given state-space system ẋ = Ax + Bu, what happens to LQR performance if A matrix eigenvalues are estimated 50% too high?

- **Reinforcement learning exploration vs. exploitation**: The core problem is that early-training RL exploration produces near-random control forces. Understanding this tradeoff explains why naive RL is dangerous on physical systems. *Quick check*: Why does an epsilon-greedy policy with ε=1.0 during early training produce potentially harmful control signals?

- **Lyapunov stability theory**: The LAC algorithm uses a learned Lyapunov function as the critic, constraining policy updates toward stable behaviors. This distinguishes it from standard actor-critic methods. *Quick check*: What property must a Lyapunov function L(x) satisfy to guarantee stability at equilibrium x=0?

## Architecture Onboarding

- **Component map**: Environment -> State Constructor -> RL Actor Network -> Action Combiner -> Environment, with LQR Controller providing parallel stabilizing action
- **Critical path**: 1. Design LQR gains K using assumed model (one-time, offline). 2. Initialize actor/critic networks with random weights. 3. For each episode: observe state → compute RL action → add LQR action → apply to environment → store transition → update networks via LAC objectives. 4. After training, deploy f_θ alone or with reduced α for testing.
- **Design tradeoffs**: α selection (higher α → safer training but slower RL learning; lower α → faster learning but higher training risk), reward weights (paper uses [1, 10⁻², 10⁻³], prioritizing displacement reduction), assumed model accuracy (more accurate assumed model → better LQR baseline → safer training, but requires system identification), network architecture (deeper networks may learn faster but increase overfitting risk).
- **Failure signatures**: Training acceleration exceeds uncontrolled response (α too low or LQR gains destabilizing), testing performance no better than LQR (RL not learning; check learning rates, batch size, reward scaling), control effort exceeds actuator limits (w₃ too small; increase penalty on ||u||), divergent training loss (Lyapunov constraint violated; check λ adaptation in Eq. 5).
- **First 3 experiments**: 1. Implement LQR-guided RL on linear SDOF system with known parameters; verify training acceleration stays below uncontrolled baseline. 2. Test α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} on same system; plot training max acceleration vs. final policy performance. 3. Fix true system, vary assumed model parameters ±50% for mass and stiffness; measure which parameter errors most degrade LQR-guided training safety and final performance.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's safety claims rely on LQR's stabilizing effect even with incorrect models, but the paper does not quantify stability margins or test extreme parameter mismatches.
- The assumed model uses negative damping (c=-0.5), which could destabilize the system if LQR gains are poorly tuned—this critical parameter choice is not justified.
- The α weighting parameter (set to 0.5) is not systematically analyzed, leaving the optimal tradeoff between training safety and learning efficiency unclear.

## Confidence

- **High confidence**: The core observation that LQR-guided RL reduces training-induced vibrations compared to naive RL (supported by Figure 5). The final trained RL policy outperforming standalone LQR in testing (supported by Figure 6).
- **Medium confidence**: The mechanism by which incorrect-model LQR still provides stabilizing guidance (reasonable based on shared qualitative dynamics, but stability margins not quantified). The effectiveness of the Lyapunov-based Actor-Critic algorithm (convergence properties not formally established).
- **Low confidence**: The robustness of the approach to severe model mismatch (no systematic testing of parameter sensitivity). The generalizability to multi-DOF systems or different nonlinear behaviors (validation limited to single-DOF Duffing oscillator).

## Next Checks

1. **Stability Margin Analysis**: Systematically vary assumed model parameters (±50% for mass, stiffness, damping) and measure the maximum safe α value that prevents training divergence. Identify which parameter errors most degrade LQR guidance effectiveness.

2. **α Sensitivity Sweep**: Test α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} across multiple random seeds, plotting training max acceleration versus final policy performance. Determine if the 0.5 value represents an optimal tradeoff or if better safety-performance combinations exist.

3. **Reward Weight Sensitivity**: Vary reward weights [w₁, w₂, w₃] around the paper's choice [1, 10⁻², 10⁻³], testing at least three configurations (e.g., prioritizing displacement vs. acceleration vs. control effort). Measure impact on training stability and final policy characteristics.