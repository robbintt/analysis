---
ver: rpa2
title: 'Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language
  Models'
arxiv_id: '2509.21155'
source_url: https://arxiv.org/abs/2509.21155
tags:
- domain
- template
- performance
- spurious
- syntactic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can rely on spurious correlations between
  syntactic templates and domain knowledge, leading to degraded performance when domain
  shifts occur. The authors introduce a framework to measure this phenomenon using
  perturbations (synonyms, antonyms, disfluencies, paraphrases) and evaluate open/closed
  models on instruction-tuning datasets.
---

# Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models

## Quick Facts
- arXiv ID: 2509.21155
- Source URL: https://arxiv.org/abs/2509.21155
- Reference count: 40
- Key outcome: Large language models can rely on spurious correlations between syntactic templates and domain knowledge, leading to degraded performance when domain shifts occur.

## Executive Summary
This paper introduces a framework to measure syntactic-domain spurious correlations in large language models. The authors find that models learn to associate specific syntactic templates (frequent sequences of part-of-speech tags) with particular domains during training, and this learned association can override semantic content during inference. They demonstrate this through systematic perturbations (synonyms, antonyms, disfluencies, paraphrases) across instruction-tuning datasets, showing performance drops of 0.4-0.6 in cross-domain settings for OLMo-2 models. The work also reveals that these correlations can be exploited for jailbreak attacks, reducing refusal rates from 40% to 2.5% in OLMo-2-7B-Instruct models.

## Method Summary
The authors construct a synthetic dataset using the TRex dataset (Facebook/lama on HuggingFace) with 17 Wikidata PIDs across four domains (locations, persons, organizations, creative works). They manually write two template instantiations per domain and scale them using Claude Sonnet. Five perturbation types are applied to each instance: EXACT (no change), SYNONYM (synonym replacement), ANTONYM (antonym replacement), DISFLUENT (word shuffling), and PARAPHRASE. OLMo-2 models (1B/7B/13B) are finetuned on this synthetic data using 1×H200 GPU, 3 epochs, batch_size=4, lr=3e-5, with greedy decoding. Cross-domain performance is evaluated by applying domain A's template to domain B's entities.

## Key Results
- Cross-domain performance drops by 0.4-0.6 in OLMo-2 models when syntactic templates shift across domains
- OLMo-13B Instruct treats ANTONYM setting (0.92) similar to SYNONYM setting (0.92), despite antonyms conveying opposite meanings
- Jailbreak attacks reduce refusal rates from 40% to 2.5% when harmful queries are wrapped in cross-domain templates
- Similar syntax-domain spurious correlations observed in Llama-4-Maverick and GPT-4o models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs learn spurious correlations between syntactic templates (frequent PoS tag sequences) and domain labels during training.
- **Mechanism:** When certain syntactic templates appear disproportionately in specific domains during training, models form an association where the template itself becomes predictive of the task/domain, rather than the semantic content.
- **Core assumption:** Training data has non-uniform syntactic template distributions across domains.
- **Evidence anchors:**
  - [abstract] "syntactic templates—frequent sequences of Part-of-Speech (PoS) tags—are prevalent in training data"
  - [section 3.2] "In-domain performance on EXACT, SYNONYM, and ANTONYM settings is high (0.90-0.94). The cross-domain performance, however, drops by ~0.40-0.60"
  - [corpus] Related work on spurious correlations in classification exists, but syntax-domain correlations specifically are underexplored.
- **Break condition:** If syntactic templates are uniformly distributed across domains in training data, the correlation cannot form.

### Mechanism 2
- **Claim:** Learned syntax-domain associations can override semantic content during inference.
- **Mechanism:** When a syntactic template strongly signals a domain, the model predicts based on the template association rather than evaluating semantic coherence—evidence: models answer "France" to incoherent queries like "Quickly sit Paris clouded?" when using geography templates.
- **Core assumption:** The syntax-domain correlation strength exceeds the model's semantic evaluation threshold.
- **Evidence anchors:**
  - [abstract] "models learn to associate a domain with syntax during training; this can sometimes override prompt semantics"
  - [section 3.2] "OLMo-13B Instruct treats the ANTONYM setting (0.92) similar to the SYNONYM setting (0.92), despite antonyms and synonyms conveying opposite meanings"
  - [corpus] Related work confirms models exploit spurious syntactic heuristics in NLI tasks (McCoy et al., 2019).
- **Break condition:** If semantic robustness training explicitly penalizes template-only predictions, override behavior may decrease.

### Mechanism 3
- **Claim:** Safety-aligned refusal behaviors become associated with specific syntactic templates, enabling template-based jailbreaks.
- **Mechanism:** Safety fine-tuning occurs on specific template formats; when harmful queries are wrapped in cross-domain templates (e.g., chain-of-thought, math), the template-domain association fails to trigger refusal pathways.
- **Core assumption:** Safety training data has limited syntactic diversity within harmful query domains.
- **Evidence anchors:**
  - [abstract] "unintended syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B Instruct and GPT-4o"
  - [section 6] "refusal rate from 40% to 2.5% when using the template as a prefix" for chain-of-thought templates
  - [corpus] Limited direct corpus evidence on syntax-based safety bypasses; this appears novel.
- **Break condition:** If safety training explicitly includes diverse syntactic templates for harmful queries, jailbreak effectiveness should decrease.

## Foundational Learning

- **Concept:** Part-of-Speech (PoS) tagging and syntactic templates
  - **Why needed here:** The entire methodology depends on extracting PoS sequences from text and defining templates as n-grams of these tags.
  - **Quick check question:** Can you identify the PoS template for "Where is Paris located?" (Answer: Adverb Verb Proper-Noun Verb(past-participle))

- **Concept:** Spurious correlations in machine learning
  - **Why needed here:** The paper formalizes syntax-domain spurious correlations analogously to texture-background correlations in computer vision.
  - **Quick check question:** What distinguishes a spurious correlation from a causal feature? (Answer: Spurious correlations do not hold under distribution shift.)

- **Concept:** In-domain vs. cross-domain evaluation
  - **Why needed here:** The diagnostic relies on comparing model behavior when templates match training distribution vs. when they come from different domains.
  - **Quick check question:** If a model trained on geography templates is tested with cuisine templates on geography entities, what performance change indicates spurious correlation? (Answer: Significant accuracy drop)

## Architecture Onboarding

- **Component map:** Template extractor (PoS tagger + n-gram frequency analyzer) -> Perturbation generator (synonym/antonym/paraphrase/disfluent variants) -> Cross-domain pairing logic (mapping templates to alternate domains) -> Risk calculator (in-domain vs. cross-domain performance gap) -> Jailbreak probe (template-based refusal bypass testing)
- **Critical path:** Template extraction → Domain-template frequency analysis → Perturbation set generation → Cross-domain testing → Risk quantification
- **Design tradeoffs:**
  - Synthetic dataset (full control) vs. real training data (ecological validity)
  - Assumption: Minimal template overlap across domains simplifies analysis but may not hold in practice
  - Entity memorization can confound results—DISFLUENT setting helps disambiguate
- **Failure signatures:**
  - High in-domain ANTONYM/DISFLUENT accuracy (syntax overriding semantics)
  - Large in-domain vs. cross-domain gap on SYNONYM/PARAPHRASE (template-specific knowledge)
  - Jailbreak success with template prefix/suffix but not with antonym semantic breaking
- **First 3 experiments:**
  1. **Baseline probe:** Extract top-10 templates from your training data, measure in-domain vs. cross-domain accuracy gap using the five perturbation settings.
  2. **Safety audit:** Take 100 harmful prompts from your alignment data, wrap them in cross-domain templates (e.g., math, CoT), measure refusal rate change.
  3. **Ablation test:** Fine-tune a model with enforced syntactic diversity per domain, re-evaluate cross-domain gap to test the intervention hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does syntactic-domain spurious correlation reliance contribute to hallucinations in LLMs, and what types of hallucinations are most affected?
- **Basis in paper:** [explicit] "We encourage future work to further study how reliance on this spurious correlation can lead to widespread hallucinations and its impact on other LLM reliability issues."
- **Why unresolved:** The paper demonstrates the correlation phenomenon and shows preliminary hallucination results in Appendix 11, but does not systematically characterize the relationship or scope of hallucination types affected.
- **What evidence would resolve it:** Experiments measuring hallucination rates across domains when syntactic templates are manipulated, with systematic categorization of factual vs. logical hallucination types.

### Open Question 2
- **Question:** Do reasoning models trained with explicit chain-of-thought outputs exhibit different degrees of syntactic-domain spurious correlation reliance compared to standard instruction-tuned models?
- **Basis in paper:** [explicit] "Finally, we do not investigate reasoning models or those trained with chain-of-thought outputs."
- **Why unresolved:** The paper's experiments are limited to OLMo-2, Llama-4-Maverick, and GPT-4o variants that do not emphasize explicit reasoning processes, leaving open whether reasoning training mitigates or exacerbates syntax reliance.
- **What evidence would resolve it:** Application of the paper's benchmarking framework to reasoning-focused models (e.g., reasoning-augmented variants), comparing cross-domain performance drops.

### Open Question 3
- **Question:** Does increasing within-domain syntactic diversity in training data effectively mitigate spurious syntax-domain correlations without degrading in-domain performance?
- **Basis in paper:** [inferred] The paper concludes that "syntactic diversity in training data, specifically within domains" is needed, but does not experimentally validate this intervention.
- **Why unresolved:** The work demonstrates the problem exists but does not test whether the proposed solution works or what trade-offs exist.
- **What evidence would resolve it:** Controlled experiments comparing models trained on datasets with varying syntactic diversity within domains, measuring both in-domain performance and cross-domain generalization.

### Open Question 4
- **Question:** What determines whether a model exhibits entity memorization versus syntactic-domain spurious correlations as its primary failure mode?
- **Basis in paper:** [inferred] Llama-4-Maverick showed entity memorization profiles while OLMo-2 and GPT-4o showed syntactic-domain spurious correlations, but the cause of this difference is unexplained.
- **Why unresolved:** The paper observes divergent behaviors across models but does not investigate the training or architectural factors that lead to different failure modes.
- **What evidence would resolve it:** Systematic comparison across model families with controlled training data compositions and scales, analyzing when each failure pattern emerges.

## Limitations
- The synthetic dataset construction relies on manually written templates scaled via Claude Sonnet, which may not capture real-world training data diversity
- The five perturbation types may not exhaustively represent all syntactic variations that could trigger spurious correlations
- The methodology assumes template-domain associations are primarily learned from training data frequency patterns, potentially underestimating pre-training effects

## Confidence
- **High Confidence:** The core observation that LLMs exhibit performance degradation under cross-domain syntactic shifts (0.4-0.6 accuracy drops) is well-supported by experimental evidence across multiple model families
- **Medium Confidence:** The theoretical framework for classifying error types and the specific taxonomy of how models rely on syntax-domain correlations is well-reasoned but may not capture all failure modes
- **Low Confidence:** The effectiveness of proposed interventions (syntactic diversity in training data, explicit testing) is hypothesized but not empirically validated

## Next Checks
1. **Ecological Validity Test:** Apply the template extraction and cross-domain evaluation pipeline to real training datasets rather than synthetic data to verify if observed spurious correlations reflect actual training conditions
2. **Intervention Efficacy Study:** Design and execute a controlled experiment where models are trained with enforced syntactic diversity within domains, then re-measure cross-domain performance gaps
3. **Safety Benchmark Expansion:** Extend the jailbreak evaluation to include more diverse safety categories and template types, particularly testing whether the 2.5% refusal floor represents a fundamental limitation