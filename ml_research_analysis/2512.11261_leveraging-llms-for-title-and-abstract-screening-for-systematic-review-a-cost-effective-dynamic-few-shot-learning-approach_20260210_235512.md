---
ver: rpa2
title: 'Leveraging LLMs for Title and Abstract Screening for Systematic Review: A
  Cost-Effective Dynamic Few-Shot Learning Approach'
arxiv_id: '2512.11261'
source_url: https://arxiv.org/abs/2512.11261
tags:
- screening
- systematic
- abstract
- reviews
- title
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently screening titles
  and abstracts in systematic reviews, a critical but time-consuming step in evidence-based
  medicine. The authors propose a two-stage Dynamic Few-Shot Learning (DFSL) approach
  that uses a low-cost LLM for initial screening followed by a high-performance LLM
  to re-evaluate low-confidence instances.
---

# Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach

## Quick Facts
- arXiv ID: 2512.11261
- Source URL: https://arxiv.org/abs/2512.11261
- Reference count: 0
- Achieved average F1 score of 0.552, outperforming baseline approaches

## Executive Summary
This paper addresses the challenge of efficiently screening titles and abstracts in systematic reviews, a critical but time-consuming step in evidence-based medicine. The authors propose a two-stage Dynamic Few-Shot Learning (DFSL) approach that uses a low-cost LLM for initial screening followed by a high-performance LLM to re-evaluate low-confidence instances. The DFSL approach employs dynamic instance selection and confidence scoring to improve screening accuracy while controlling computational costs. Across 10 systematic review datasets, the DFSL approach achieved an average F1 score of 0.552, outperforming zero-shot, chain-of-thought, and few-shot learning baselines.

## Method Summary
The authors developed a two-stage Dynamic Few-Shot Learning approach for systematic review screening. The first stage uses a low-cost LLM to perform initial title and abstract screening with dynamic instance selection and confidence scoring. Studies with low confidence scores (below a threshold) are flagged for re-evaluation. In the second stage, these low-confidence instances are re-evaluated using a high-performance LLM. The approach dynamically adjusts the selection of studies for human review based on confidence scores, aiming to minimize both missed relevant studies and unnecessary human screening effort.

## Key Results
- DFSL approach achieved average F1 score of 0.552 across 10 systematic review datasets
- Re-evaluation of low-confidence instances (12.96% of studies) increased F1 score to 0.563
- Outperformed zero-shot, chain-of-thought, and few-shot learning baselines
- Demonstrated cost-effectiveness by using low-cost LLM for initial screening and high-performance LLM only for re-evaluation

## Why This Works (Mechanism)
The approach leverages the complementary strengths of different LLM models - using cost-effective models for high-confidence predictions and reserving expensive models for uncertain cases. Dynamic instance selection allows the system to focus computational resources where they're most needed, while confidence scoring provides a principled way to identify cases requiring additional scrutiny.

## Foundational Learning
1. **Dynamic Instance Selection** - selectively choosing which instances to process with more expensive models; needed to balance accuracy and cost, quick check: verify threshold selection methodology
2. **Confidence Scoring in LLMs** - quantifying prediction uncertainty; needed to identify low-confidence instances requiring re-evaluation, quick check: examine calibration of confidence scores
3. **Two-Stage Processing** - using different models for different stages of analysis; needed to optimize resource allocation, quick check: compare performance of single-stage vs two-stage approaches
4. **Systematic Review Screening** - the manual process of identifying relevant studies; needed context for evaluating the approach, quick check: understand typical screening workflows and success metrics
5. **Few-Shot Learning** - learning from limited examples; needed for adapting to different review topics, quick check: examine sample efficiency across datasets
6. **Computational Cost Analysis** - measuring resource usage across different approaches; needed for evaluating cost-effectiveness claims, quick check: verify cost calculations and assumptions

## Architecture Onboarding
**Component Map:** Low-cost LLM (initial screening) -> Confidence Scoring -> Threshold Filter -> High-performance LLM (re-evaluation) -> Final Classification

**Critical Path:** The primary workflow follows: document input → low-cost LLM classification → confidence score calculation → threshold comparison → (if low confidence) high-performance LLM re-evaluation → final decision

**Design Tradeoffs:** The approach trades some initial accuracy for significant cost savings by using a low-cost model first, accepting that some instances will require re-evaluation. This creates a balance between computational efficiency and screening accuracy.

**Failure Signatures:** 
- Low-confidence threshold set too high → excessive re-evaluation costs
- Low-confidence threshold set too low → missed relevant studies
- Poor confidence calibration → unreliable re-evaluation decisions
- Domain mismatch → systematically low confidence across relevant studies

**3 First Experiments:**
1. Vary the confidence threshold to find optimal balance between re-evaluation rate and performance
2. Compare different low-cost LLM models for initial screening to identify best cost-accuracy tradeoff
3. Test on datasets with different relevant-to-irrelevant ratios to assess robustness

## Open Questions the Paper Calls Out
None

## Limitations
- F1 scores of 0.552 (baseline) and 0.563 (with re-evaluation) appear relatively low for systematic review applications
- Limited information about dataset characteristics and distribution of relevant versus irrelevant studies
- Cost-effectiveness analysis focuses only on computational costs, not implementation or integration challenges

## Confidence
- **High Confidence:** Two-stage LLM approach with confidence scoring is methodologically sound
- **Medium Confidence:** The improvement from 0.552 to 0.563 F1 score is statistically significant but practical significance unclear
- **Low Confidence:** Absolute performance metrics may not be directly comparable to existing screening tools due to lack of standardized benchmarking

## Next Checks
1. **Real-World Workflow Integration Test:** Implement the DFSL approach in an actual systematic review project with human reviewers to measure time savings, accuracy compared to full manual screening, and user satisfaction.

2. **Cross-Domain Generalization Study:** Apply the approach to systematic reviews across different medical specialties (e.g., clinical medicine, public health, basic science) to evaluate performance consistency and identify domain-specific limitations.

3. **Bias and Error Analysis:** Conduct a detailed analysis of false positives and false negatives to understand systematic biases introduced by the LLM screening process and develop mitigation strategies for different types of review questions.