---
ver: rpa2
title: Aligning Black-box Language Models with Human Judgments
arxiv_id: '2502.04997'
source_url: https://arxiv.org/abs/2502.04997
tags:
- human
- judgments
- alignment
- llms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of aligning large language model
  (LLM) judgments with human evaluations, particularly important for subjective tasks
  where systems are designed for human use. The core method involves learning a linear
  mapping between LLM categorical outputs and human judgments using a small set of
  calibration examples, without requiring model retraining or access to model logits.
---

# Aligning Black-box Language Models with Human Judgments

## Quick Facts
- arXiv ID: 2502.04997
- Source URL: https://arxiv.org/abs/2502.04997
- Authors: Gerrit J. J. van den Burg; Gen Suzuki; Wei Liu; Murat Sensoy
- Reference count: 13
- Primary result: Achieves 142% average improvement in LLM-human judgment agreement across 29 tasks using only 1-2 calibration examples per category

## Executive Summary
This paper addresses the challenge of aligning large language model (LLM) judgments with human evaluators on subjective ordinal tasks. The proposed method learns a linear mapping between LLM categorical outputs and human judgments using ridge regression, requiring only a small set of calibration examples without access to model logits or retraining. Experiments demonstrate significant improvements in alignment across 29 tasks with three different LLMs, achieving strong performance in both zero-shot and few-shot settings while remaining compatible with black-box API-only models.

## Method Summary
The method encodes LLM outputs and human labels as one-hot vectors, then learns a linear transformation matrix W via ridge regression that maps LLM judgments to human-aligned predictions. For N training examples, the approach constructs Z (N×m) and Y (N×n) matrices from one-hot encoded LLM outputs and human labels respectively, then solves W = (Z^⊤Z + λI)^(-1) Z^⊤Y with λ = 10^(-6). At inference, new LLM judgments are transformed via z^⊤W and the argmax determines the aligned label. The method requires only 100 training samples or 25% of available data and works without access to model logits, making it applicable to general-purpose black-box LLMs.

## Key Results
- Achieves 142% average improvement in agreement with human judgments across 29 tasks
- Works effectively with only 1-2 examples per category, demonstrating sample efficiency
- Exceeds inter-human agreement on four out of six multi-annotator tasks
- Enables smaller LLMs to perform comparably to larger models when aligned
- Maintains performance in both zero-shot and few-shot settings

## Why This Works (Mechanism)

### Mechanism 1: Response Style Bias Correction
LLMs exhibit systematic response style biases (e.g., avoiding negative judgments) that a linear mapping can correct. The transformation matrix W learns to remap how LLMs use scale labels to match human usage patterns. If an LLM over-selects "excellent" when humans would say "good," W captures this shift. Core assumption: LLM judgment errors are primarily systematic response-style deviations, not content-dependent reasoning failures. Evidence shows LLMs cluster at high ratings (4-6) while humans use the full 0-6 range, which alignment corrects.

### Mechanism 2: Few-Shot Distribution Matching via Ridge Regression
Ridge regression provides a sample-efficient, closed-form solution to learn the conditional mapping P(human_label | LLM_label). By solving Eq. (2), W approximates the joint distribution between LLM and human label spaces. Regularization (λ = 10^-6) prevents overfitting when N is small. Core assumption: The relationship between LLM and human label distributions is approximately linear (capturable via matrix multiplication). Results show strong performance with 1-2 examples per category.

### Mechanism 3: Black-Box Compatibility via Output-Space Mapping
Operating solely on final categorical outputs (not logits) enables alignment for any LLM, including API-only models. One-hot encoding discards confidence information, reducing the mapping problem to categorical redistribution. This sacrifices granularity for universality. Core assumption: The categorical label itself contains sufficient signal; confidence/probability information is not critical. The approach explicitly avoids requiring logits or model ensembles, unlike related work.

## Foundational Learning

- **Concept: Ridge Regression (L2-Regularized Least Squares)**
  - Why needed: Core solver for learning W with few examples without overfitting
  - Quick check: Why does (Z^⊤Z + λI)^(-1) exist even when Z^⊤Z is singular?

- **Concept: One-Hot Encoding for Categorical Data**
  - Why needed: Converts ordinal/nominal judgments into vectors amenable to linear algebra
  - Quick check: What information is lost when converting soft probabilities to one-hot vectors?

- **Concept: Response Style Bias (Psychometrics)**
  - Why needed: Frames LLM alignment as a known class of measurement problems from survey methodology
  - Quick check: How does "acquiescence bias" differ from "extreme response style"?

## Architecture Onboarding

- **Component map**: Prompt + LLM → Raw categorical judgment z ∈ Z → One-hot encoder → z → vector in {0,1}^m → Training path: Stack N examples → Z matrix → Solve Eq. (2) → W → Inference path: New z → one-hot → z^⊤Ŵ → argmax → aligned label

- **Critical path**:
  1. Collect calibration pairs {(x_i, y_i_human)} for N examples
  2. Query LLM on same examples → {z_i_LLM}
  3. One-hot encode both; build Y and Z matrices
  4. Compute Ŵ = (Z^⊤Z + λI)^(-1)Z^⊤Y
  5. Deploy: for new x → get LLM judgment z → compute z^⊤Ŵ → return argmax

- **Design tradeoffs**:
  - Training size vs. cost: 20-100 examples suffice; diminishing returns beyond
  - Task-specific vs. transferred W: Task-specific alignments outperform transferred ones
  - λ tuning: Authors use 10^-6 universally; higher λ trades accuracy for stability

- **Failure signatures**:
  - W ≈ I: LLM already aligned OR λ too high OR training data unrepresentative
  - Large entries in W: Numerical instability; increase λ or check for degenerate label distributions
  - No improvement after alignment: Mapping doesn't address root cause; consider input-dependent methods

- **First 3 experiments**:
  1. Implement on Medical Safety (Response Type) with 100 train / 300 test split; target: ~75-80% accuracy (vs. ~5-11% unaligned)
  2. Vary training examples (1, 2, 5, 10, 20, 50, 100) on one high-variance task; plot accuracy curve to identify inflection point
  3. Train W on ROSCOE-Cosmos coherency, test on ROSCOE-DROP/ESNLI/GSM8K coherency; measure gap vs. task-specific W

## Open Questions the Paper Calls Out

- **Open Question 1**: Does utilizing model logits (probability distributions) instead of one-hot encoded vectors enable finer alignment between LLM judgments and human evaluators? The authors note this could enable potentially finer alignment but leave exploration to future work.

- **Open Question 2**: Can prompt optimization be used to identify an LLM-specific judgment space (distinct from the human label space) that results in superior alignment performance? The work allows LLMs to utilize different judgment spaces than human evaluators, but this aspect is left for future exploration.

- **Open Question 3**: Does concatenating outputs from multiple LLMs and learning a joint linear mapping (multi-model alignment) significantly outperform single-model alignment? The approach lays groundwork for multi-model alignment by exploring concatenating outputs of multiple models.

## Limitations

- The paper doesn't validate whether linear mapping suffices for all alignment scenarios against content-dependent methods
- Performance depends heavily on prompt templates, which aren't fully specified for all 29 tasks
- The assumption that linear mapping captures all alignment needs may break for tasks requiring content-dependent reasoning
- The method discards confidence information by using only categorical outputs

## Confidence

- **High Confidence**: Core method (linear ridge regression on one-hot encoded labels) is mathematically sound; sample efficiency claims (1-2 examples per category) are well-supported; black-box compatibility claim is robust
- **Medium Confidence**: 142% average improvement metric is impressive but aggregated across heterogeneous tasks; exceeding inter-human agreement claim depends on how human disagreement is measured
- **Low Confidence**: Assertion that linear mapping suffices for all alignment scenarios lacks validation against content-dependent methods; mechanism explanation (response style bias correction) is plausible but not empirically distinguished from other explanations

## Next Checks

1. **Prompt Dependency Test**: Systematically vary prompt templates for the same task and measure alignment stability to determine if performance drops significantly with minor prompt changes

2. **Cross-Domain Transferability**: Train W on subjective tasks (e.g., summarization quality) and test on objective tasks (e.g., factual consistency) to reveal whether linear mapping captures domain-general judgment patterns or task-specific biases

3. **Confidence Signal Ablation**: Compare performance when using soft LLM probabilities (if accessible) versus one-hot encodings to quantify information loss from categorical-only approaches and establish theoretical ceilings for black-box methods