---
ver: rpa2
title: 'Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win'
arxiv_id: '2506.03919'
source_url: https://arxiv.org/abs/2506.03919
tags:
- graph
- graphs
- pruning
- expressivity
- lottery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the lottery ticket hypothesis (LTH) in
  graph neural networks (GNNs), focusing on the critical role of expressivity in sparse
  subnetworks. The authors establish that the ability of sparse subnetworks to distinguish
  non-isomorphic graphs is crucial for preserving predictive performance.
---

# Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win

## Quick Facts
- **arXiv ID:** 2506.03919
- **Source URL:** https://arxiv.org/abs/2506.03919
- **Reference count:** 40
- **Primary result:** Expressivity preservation is crucial for finding winning lottery tickets in GNNs, with theoretical bounds on irrecoverable loss.

## Executive Summary
This paper investigates the lottery ticket hypothesis in graph neural networks (GNNs), demonstrating that a sparse subnetwork's ability to distinguish non-isomorphic graphs is critical for preserving predictive performance. The authors introduce the Strong Expressive Lottery Ticket Hypothesis (SELTH), proving that maximally expressive sparse subnetworks exist within over-parameterized GNNs. They establish that pre-training expressivity correlates with post-training accuracy and identify scenarios where expressivity loss is irrecoverable, particularly for structurally isomorphic but feature-divergent graphs (SIFDGs). Empirically, the paper confirms that expressivity preservation accelerates convergence and improves generalization across multiple graph classification benchmarks.

## Method Summary
The method involves measuring pre-training expressivity (τ_pre) as the fraction of distinguishable non-isomorphic graph pairs in an untrained GNN, then correlating this with post-training accuracy after pruning. The approach uses GIN and GCN architectures with random pruning at ratios 10-90%, training for 250 epochs with batch size 32 and learning rate 0.01. The critical innovation is the theoretical framework (SELTH) proving that expressive paths must be preserved to maintain winning ticket status, particularly in the first layer for SIFDGs.

## Key Results
- SELTH proves the existence of maximally expressive sparse subnetworks within over-parameterized GNNs
- Pre-training expressivity (τ_pre) strongly correlates with post-training accuracy across multiple datasets
- Expressivity loss in the first layer is irrecoverable for structurally isomorphic, feature-divergent graphs (SIFDGs)
- High expressivity in initializations accelerates convergence and improves generalization

## Why This Works (Mechanism)

### Mechanism 1: Expressivity Preservation via Injective Paths (SELTH)
- **Claim:** A sparse subnetwork is "winning" if it preserves the full network's ability to distinguish non-isomorphic graphs, matching 1-WL test power.
- **Mechanism:** SELTH proves that over-parameterized GNNs contain subsets of "maximally expressive paths." Pruning masks that remove paths required for MLPs to remain injective destroy the GNN's theoretical power to separate graph classes.
- **Break condition:** Expressivity fails when pruning ratios are so high that no configuration of weights in the sparse subnetwork can approximate an injective function over the dataset's multiset of features.

### Mechanism 2: Gradient Diversity via Orthogonal Embeddings
- **Claim:** Sparse initializations preserving expressivity correlate with higher gradient diversity, accelerating convergence and improving generalization.
- **Mechanism:** Expressive models map non-isomorphic graphs to distinct/orthogonal embeddings. Theorem 3.3 links cosine similarity of embeddings to gradient diversity (Δ_s), reducing interference during optimization.
- **Break condition:** Cannot manifest if the dataset consists primarily of isomorphic graphs where distinct embeddings are mathematically impossible.

### Mechanism 3: Irrecoverable Collapse on Feature-Divergent Graphs (SIFDGs)
- **Claim:** Pruning the first layer permanently destroys the ability to distinguish structurally isomorphic graphs that differ only in features.
- **Mechanism:** For SIFDGs, distinction relies entirely on the first layer's feature transformation. If pruning masks zero out weights responsible for differentiating input features, subsequent layers cannot recover the distinction.
- **Break condition:** Absolute failure if the critical differentiating path in layer 1 is pruned.

## Foundational Learning

- **Concept: The Weisfeiler-Leman (1-WL) Test**
  - **Why needed here:** The paper equates a "winning ticket" with a subnetwork that maintains the expressivity of the 1-WL test. Understanding 1-WL (iterative color refinement) is prerequisite to understanding what the model loses when pruned.
  - **Quick check question:** Can you explain why two non-isomorphic graphs might fail to be distinguished by 1-WL, and why a GNN matching this power cares about that?

- **Concept: Lottery Ticket Hypothesis (LTH)**
  - **Why needed here:** The paper extends the standard LTH to the GNN domain. You must understand the "Iterative Magnitude Pruning" baseline to contrast it with the "Expressive Lottery Ticket" approach.
  - **Quick check question:** What defines a "winning ticket" in the classic Frankle & Carbin formulation, and how does this paper refine that definition for graphs?

- **Concept: Injectivity in Message Passing**
  - **Why needed here:** The theoretical argument relies on "moment-based" GNNs using injective functions to aggregate neighbor features. If pruning destroys injectivity, the model collapses different graphs into the same embedding.
  - **Quick check question:** Why is summation (used in GIN) considered more "injective" than mean or max pooling in the context of graph isomorphism?

## Architecture Onboarding

- **Component map:** Input Graphs (V, E, X) -> Moment-based GNN (GIN/GCN) -> Binary Weight Mask -> Graph-level Readout -> Classification
- **Critical path:** The First Layer MLP weights. According to Lemma 3.5, this is the "Achilles' heel." If weights here are pruned such that X_1 W ≈ X_2 W for graphs with identical structure but different features, the network fails instantly.
- **Design tradeoffs:** Higher pruning ratios reduce parameter count but exponentially increase the risk of losing injectivity. Architecture choice matters: GCN is less expressive than GIN (approaches 1-WL limit).
- **Failure signatures:** Training loss stalls for specific graph pairs (e.g., stereoisomers) even though overall accuracy might look okay. If pre-training expressivity (τ_pre) is low, post-training accuracy will likely be low.
- **First 3 experiments:**
  1. Expressivity Probe: Calculate τ_pre for an untrained pruned GNN and correlate with dense model's τ.
  2. Layer-wise Ablation: Test pruning only the first layer vs. only deeper layers on structurally identical/feature-different graphs.
  3. Convergence Speed Test: Train two sparse models (high vs. low τ_pre) and plot validation accuracy over epochs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the theoretical conditions for SELTH change when applied to architectures with dynamic mechanisms, such as GATs or models utilizing edge features?
- **Basis in paper:** Section 3.4 states refining analysis to architectures beyond the most general setting is a promising direction.
- **Why unresolved:** Current proofs rely on fixed, injective moment-based functions; dynamic attention weights introduce non-linear dependencies the framework doesn't cover.
- **What evidence would resolve it:** Formal proof of SELTH for GATs or empirical evidence showing if the "critical path" hypothesis holds when attention heads are pruned.

### Open Question 2
- **Question:** Can practical algorithms be developed that proactively sample sparse subnetworks to maximize pre-training expressivity (τ_pre) without requiring iterative magnitude pruning?
- **Basis in paper:** Section 5 suggests future work should enhance convergence by developing sparse yet expressive initializations.
- **Why unresolved:** Paper empirically validates the correlation but relies on standard random pruning; proposed "injectivity-preserving" sparsification algorithm is not implemented.
- **What evidence would resolve it:** Implementation of a sparsification algorithm that samples based on expressivity metrics rather than weight magnitude, demonstrating faster convergence than IMP.

### Open Question 3
- **Question:** How can the theoretical upper bound on classification accuracy be refined for datasets with non-uniform class distributions and unknown isomorphism type frequencies?
- **Basis in paper:** Section 3.4 notes the bound assumes uniform class distribution and knowledge of all isomorphism types, which is "impractical."
- **Why unresolved:** Real-world graphs often exhibit skewed class distributions where the "worst-case" accuracy bound may be too loose.
- **What evidence would resolve it:** Refined theoretical bound incorporating class priors showing tighter fit to empirical accuracy drops on skewed datasets.

### Open Question 4
- **Question:** Does the irrecoverability of expressivity loss in SIFDGs persist in node-level classification tasks?
- **Basis in paper:** Section 3.4 states findings are "expected to be transferable to node level tasks."
- **Why unresolved:** Node-level tasks involve transductive learning where sensitivity of local critical paths to first-layer pruning might differ from global graph-level tasks.
- **What evidence would resolve it:** Empirical tests on node classification benchmarks showing whether pruning the first layer permanently degrades ability to distinguish structurally similar nodes with different features.

## Limitations
- Theoretical analysis assumes specific GNN architectures (GIN-like, moment-based) with injective functions; transferability to other architectures remains unclear
- Empirical validation uses relatively small graphs from TUDataset; performance on larger, real-world graphs may differ significantly
- Core claim about expressivity as primary determinant may not hold for all graph learning tasks, particularly those where structural isomorphism is less relevant than feature similarity

## Confidence

- **High Confidence:** Mathematical proofs establishing SELTH (Theorems 3.2 and 3.3) are sound within stated assumptions. SIFDG irrecoverability result (Lemma 3.5) is rigorous and novel.
- **Medium Confidence:** Empirical correlation between pre-training expressivity (τ_pre) and post-training accuracy is well-demonstrated across multiple datasets and pruning ratios.
- **Low Confidence:** Generalizability of findings to deeper architectures, different initialization schemes, or alternative pruning strategies beyond random magnitude-based pruning.

## Next Checks

1. **Architecture Transferability Test:** Replicate expressivity-accuracy correlation experiments using GCNs and attention-based GNNs to verify if SELTH mechanism holds across architectures with different expressive powers.

2. **Dataset Scaling Experiment:** Apply methodology to larger, real-world graph datasets (e.g., OGB datasets) to assess whether expressivity metric remains predictive when scaling up in graph size and complexity.

3. **Alternative Pruning Strategy Comparison:** Compare random pruning approach with structured pruning or gradient-based pruning to determine if expressivity preservation is a universal principle or specific to pruning method used.