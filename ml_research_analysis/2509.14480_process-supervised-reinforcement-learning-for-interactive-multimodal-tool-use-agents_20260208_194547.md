---
ver: rpa2
title: Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use
  Agents
arxiv_id: '2509.14480'
source_url: https://arxiv.org/abs/2509.14480
tags:
- user
- training
- agent
- reward
- turn-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning framework for training
  interactive multimodal tool-use agents that can handle interleaved speech-text interactions.
  The core contribution is Turn-level Adjudicated Reinforcement Learning (TARL), which
  employs an LLM judge to provide fine-grained turn-level rewards, addressing the
  credit assignment challenge in long-horizon multi-turn tasks.
---

# Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents

## Quick Facts
- **arXiv ID:** 2509.14480
- **Source URL:** https://arxiv.org/abs/2509.14480
- **Reference count:** 40
- **Primary result:** TARL improves pass^1 performance by 6%+ on τ-bench Retail vs strong RL baselines

## Executive Summary
This paper introduces TARL, a process-supervised RL framework that uses an LLM judge to provide fine-grained turn-level rewards for training interactive multimodal tool-use agents. The framework addresses the credit assignment challenge in long-horizon multi-turn tasks by evaluating each dialogue turn's contribution to the overall goal. Experiments on τ-bench Retail show TARL achieves over 6% improvement in pass^1 performance compared to standard RL baselines, while multimodal training provides over 20% improvement for voice-driven tasks.

## Method Summary
TARL employs an LLM judge to assign turn-level rewards (-1, 0, 1) based on each dialogue turn's contribution to the ground-truth tool call trajectory, combined with scaled terminal rewards for credit assignment. The framework uses mixed-task training with mathematical reasoning problems to encourage exploration and prevent policy overfitting. A sandbox environment supports both text and speech modalities through interleaved rollouts, with base models Qwen3-8B (text) and Qwen2.5-Omni-7B (multimodal) fine-tuned using GRPO/PPO. The approach includes reward scaling (terminal×10, major deviation×5, others×1/T) and a multimodal warm-up curriculum for 30 steps.

## Key Results
- TARL achieves 6%+ pass^1 improvement on τ-bench Retail vs strong RL baselines
- Multimodal training (speech+text) provides over 20% improvement for voice-driven tasks
- Mixed-task training with math problems sustains exploratory behaviors and prevents overfitting
- Turn-level supervision resolves credit assignment challenges in long-horizon multi-turn conversations

## Why This Works (Mechanism)

### Mechanism 1: Process-Supervised Credit Assignment
The LLM judge provides turn-level rewards that resolve credit assignment challenges in long-horizon RL where sparse terminal rewards fail. The judge assigns -1/0/1 scores to each turn based on its contribution to the ground-truth tool call trajectory, creating a denser reward signal that scales with trajectory length to prevent bias.

### Mechanism 2: Exploration Regularization via Mixed-Task Training
Incorporating math problems into the RL curriculum prevents policy overfitting by maintaining self-reflection capabilities. Math tasks naturally elicit longer chain-of-thought reasoning and self-correction, forcing the model to preserve exploratory behaviors that transfer to tool-use tasks.

### Mechanism 3: Multimodal Grounding via Interleaved Rollouts
Training on interleaved speech-text trajectories develops robust voice-driven agents by forcing the multimodal LLM to process speech modality in situ. This prevents loss of speech understanding capabilities that occur with text-only fine-tuning, grounding the policy in the target interaction format.

## Foundational Learning

- **Credit Assignment in Reinforcement Learning:** Needed because standard RL with sparse terminal rewards struggles in multi-turn settings where it's unclear which actions led to success or failure. Quick check: In a 10-turn conversation that fails, how does standard RL know which turn(s) to penalize?

- **Exploration vs. Exploitation:** Critical because RL-trained models can become overconfident and stop exploring, leading to suboptimal strategies. Quick check: What metric does the paper use as an indicator of self-reflection and exploration?

- **Process Reward Models (PRMs):** Used to provide step-level supervision contrasting with outcome-based reward models that only provide final scores. Quick check: What's the key difference between PRM and ORM, and why is PRM more suitable for this task?

## Architecture Onboarding

- **Component map:** Policy LLM -> Sandbox Environment (Backend App + User Simulator + Rule-based Verifier) -> LLM Judge (TARL) -> RL Optimizer -> Policy LLM

- **Critical path:** 1) Policy LLM interacts with User Simulator and Backend via tool calls 2) Trajectory processed by Verifier (terminal reward) and Judge (turn-level rewards) 3) Rewards aggregated and combined 4) RL optimizer computes advantage and updates Policy LLM

- **Design tradeoffs:** Per-turn reward assignment caused training instability (PPO) vs trajectory-level aggregation; Judge provides nuance but adds cost vs Verifier's sparse binary outcome; Text-only training cheaper but degrades speech performance vs interleaved multimodal training

- **Failure signatures:** Training instability (rapid KL divergence increase), exploration collapse (drop in "wait" tokens and response length), modality degradation (poor speech despite good text performance)

- **First 3 experiments:** 1) Baseline RL reproduction with vanilla GRPO on τ-bench Retail 2) TARL ablation comparing terminal-only, turn-level, and trajectory-level reward strategies 3) Exploration analysis comparing training with and without mixed-task (Math) curriculum

## Open Questions the Paper Calls Out

- How does TARL performance scale with dataset diversity and size to enable robust cross-domain generalization?
- How robust is TARL to inaccuracies or biases in the LLM judge compared to rule-based verifiers?
- Can turn-level intervention strategies be redesigned to aid exploration without destabilizing training?

## Limitations

- Judge reliability remains uncertain as the study doesn't analyze judge consistency or compare alternative judging mechanisms
- Credit assignment scaling effectiveness is not extensively validated across different configurations
- Exploration regularization generalization to other domains beyond math problems is unproven
- Multimodal grounding validation lacks causal analysis of whether models truly process speech vs memorizing patterns

## Confidence

- **High Confidence:** TARL outperforms standard RL baselines on τ-bench Retail with proper ablations and statistical comparisons
- **Medium Confidence:** Mechanism explanations are plausible but rely on indirect evidence about judge behavior
- **Low Confidence:** Specific reward scaling formulations and exact mechanisms by which math problems improve exploration lack sufficient empirical backing

## Next Checks

1. Judge ablation study: Replace GPT-4.1 judge with simpler rule-based or alternative LLM judges to quantify TARL's dependency on specific judge capabilities

2. Reward scaling sensitivity analysis: Systematically vary scaling coefficients (terminal×[5,10,15], major deviation×[3,5,7]) to identify optimal configurations

3. Cross-domain exploration transfer: Test mixed-task training approach on different tool-use domains to verify generalizability beyond math problems