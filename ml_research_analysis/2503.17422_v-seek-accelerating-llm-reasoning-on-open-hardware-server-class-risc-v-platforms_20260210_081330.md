---
ver: rpa2
title: 'V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V Platforms'
arxiv_id: '2503.17422'
source_url: https://arxiv.org/abs/2503.17422
tags:
- risc-v
- llama
- token
- inference
- deepseek
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper optimizes LLM inference on the Sophon SG2042, a many-core
  RISC-V CPU, by enhancing llama.cpp with custom kernels, compiler tuning, and NUMA-aware
  threading. The authors develop a vector-optimized GEMV kernel that quantizes fp32
  inputs to int8, exploits the SG2042's vector units, and improves data locality,
  achieving up to 56.3% higher GOPS over baselines.
---

# V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V Platforms

## Quick Facts
- arXiv ID: 2503.17422
- Source URL: https://arxiv.org/abs/2503.17422
- Reference count: 8
- Optimizes LLM inference on Sophon SG2042 via custom kernels, compiler tuning, and NUMA-aware threading, achieving up to 56.3% higher GOPS and 3× throughput gains.

## Executive Summary
This paper demonstrates how to accelerate large language model (LLM) inference on the Sophon SG2042, a many-core RISC-V CPU, by optimizing llama.cpp with custom kernels, compiler selection, and NUMA-aware threading. The authors develop a vector-optimized GEMV kernel that quantizes fp32 inputs to int8 on-the-fly, exploits the SG2042's vector units, and improves data locality through strategic memory interleaving. These optimizations yield up to 56.3% higher GOPS and achieve 4.32/6.54 tok/s throughput (generation/prefill) for DeepSeek R1 Distill Llama 8B with up to 3× speedup versus baseline. Across three model sizes, their approach improves throughput by up to 5.5× (7B) and 3× (14B), while also matching x86 energy efficiency.

## Method Summary
The authors optimize LLM inference on the MILK-V Pioneer board (SG2042, 128GB DRAM) by implementing a custom GEMV kernel with fp32-to-int8 quantization, compiled with Xuantie GCC 10.4 for vector ISA support, while compiling the llama.cpp framework with Clang 19. They test four NUMA policies and find that disabling NUMA balancing with memory interleaving enabled maximizes throughput. The approach is evaluated on Q4_0 quantized models (Llama 7B, DeepSeek R1 Distill Llama 8B, DeepSeek R1 Distill QWEN 14B) using a 22-token prompt and 256-token generation average.

## Key Results
- Custom GEMV kernel with int8 quantization achieves up to 56.3% higher GOPS over baselines
- Clang 19 outperforms GCC 13.2 by 25-34% due to better ISA support and optimization passes
- NUMA balancing off with memory interleaving yields 4.32 tok/s generation and 6.54 tok/s prefill throughput for DeepSeek R1 Distill Llama 8B
- Overall throughput improvements of up to 5.5× (7B) and 3× (14B) across tested models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: On-the-fly quantization in the GEMV kernel improves arithmetic throughput on RISC-V vector units.
- Mechanism: The kernel quantizes fp32 inputs to int8 before matrix-vector multiplication, then dequantizes the accumulated result. This reduces memory bandwidth demands and increases effective compute density by processing more elements per vector instruction on the SG2042's vector units.
- Core assumption: The SG2042's vector units can execute int8 operations significantly faster than fp32, and quantization error remains acceptable for LLM inference quality.
- Evidence anchors:
  - [abstract] "vector-optimized GEMV kernel that quantizes fp32 inputs to int8... achieving up to 56.3% higher GOPS over baselines"
  - [section] Fig.1-right pseudocode shows quantize → GEMV loops → dequantize flow; "improves the GOPS by +38.3% on average, peaking at +56.3%"
  - [corpus] Weak direct evidence—neighbor papers discuss RISC-V kernel optimization but not this specific quantization approach.
- Break condition: If target hardware has poor int8 vector support or model accuracy degrades unacceptably at int8, gains diminish.

### Mechanism 2
- Claim: Clang 19 produces faster LLM inference code than GCC 13.2 on this platform.
- Mechanism: Clang 19 provides better RISC-V ISA extension support and applies more aggressive optimization passes, specifically inlining and loop unrolling, which benefit the predictable control flow of LLM inference kernels.
- Core assumption: The performance delta comes primarily from compiler optimization quality rather than differences in standard library implementations.
- Evidence anchors:
  - [abstract] "Clang 19 consistently outperforms GCC 13.2 due to better ISA support and aggressive optimization passes"
  - [section] Fig.3 shows 34% gain for token generation and 25% for prefill; "combination of ISA extension support, and more advanced compilation passes"
  - [corpus] No direct compiler comparison evidence in neighbors.
- Break condition: If future GCC versions close the optimization gap, or if codebase relies on GCC-specific extensions, this advantage may not hold.

### Mechanism 3
- Claim: Disabling NUMA balancing with memory interleaving maximizes throughput for LLM inference.
- Mechanism: LLM inference has predictable memory access patterns. Default NUMA balancing causes excessive thread and memory page migrations that hurt cache locality. Disabling balancing and interleaving memory across NUMA nodes reduces migrations and improves data placement.
- Core assumption: The workload fits in aggregate memory and benefits from interleaved placement rather than strict locality.
- Evidence anchors:
  - [abstract] "NUMA balancing off with memory interleaving on yields peak throughput"
  - [section] "> 32 threads leads to performance loss... attributed to default NUMA balancing... high number of thread and memory page migrations"
  - [corpus] No NUMA-specific evidence in neighbors.
- Break condition: If prompt lengths vary dramatically or batch sizes change, optimal NUMA policy may differ.

## Foundational Learning

- Concept: NUMA (Non-Uniform Memory Access)
  - Why needed here: The SG2042 has complex memory hierarchy where memory access latency depends on physical location; incorrect thread-memory placement causes migrations that dominate runtime.
  - Quick check question: Can you explain why disabling NUMA balancing might help a workload with predictable memory access patterns?

- Concept: GEMV (General Matrix-Vector Multiplication)
  - Why needed here: This operation dominates LLM inference compute; optimizing it is the primary lever for performance improvement.
  - Quick check question: Why is GEMV more memory-bandwidth-bound than GEMM (matrix-matrix multiplication)?

- Concept: Quantization in LLM Inference
  - Why needed here: The kernel achieves gains by reducing precision from fp32 to int8; understanding the accuracy-performance tradeoff is essential.
  - Quick check question: What is the difference between quantizing weights offline vs. quantizing activations on-the-fly during inference?

## Architecture Onboarding

- Component map:
  - llama.cpp framework -> Custom GEMV kernel -> Xuantie GCC 10.4 (kernel compilation) -> Clang 19 (framework compilation) -> NUMA policy configuration

- Critical path: Kernel development → Compiler selection → NUMA policy tuning → End-to-end benchmarking

- Design tradeoffs:
  - int8 quantization trades potential accuracy loss for 38-56% GOPS gains
  - Clang 19 trades potential GCC compatibility for 25-34% speedup
  - Memory interleaving trades strict NUMA locality for reduced page migration overhead

- Failure signatures:
  - Performance degrades beyond 32 threads with default NUMA balancing (page migration storms)
  - Using GCC 13.2 instead of Clang 19 yields consistent 25-34% slowdown
  - Xuantie GCC 10.4 incompatible with latest llama.cpp (must use split compiler approach)

- First 3 experiments:
  1. Reproduce single-threaded kernel scaling: Run the GEMV benchmark (Fig.2) across matrix sizes 64-16384 to validate the 38-56% GOPS improvement claim.
  2. Compiler A/B test: Compile identical llama.cpp with GCC 13.2 vs Clang 19, measure token generation throughput at 1, 8, 32, 64 threads on DeepSeek 8B.
  3. NUMA policy sweep: Test all four policies on 64-thread inference, confirm memory interleaving + balancing off yields the reported 4.32 tok/s.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications arise from the methodology:

- The dependency on Xuantie GCC 10.4 for vector support creates toolchain complexity that may not be sustainable as RISC-V tooling evolves
- The Q4_0 quantization format and specific model architectures limit generalizability to other quantization schemes or model families
- The 2-socket / 4-node topology of the SG2042 may not represent the memory hierarchy of future RISC-V server platforms

## Limitations

- Platform-specificity: All optimizations target the Sophon SG2042's specific vector ISA and memory hierarchy, limiting portability to other RISC-V implementations
- Accuracy impact: The quantization approach lacks evaluation of accuracy degradation on model quality or task performance
- Workload assumptions: NUMA tuning assumes fixed 22-token prompts and 256-token generation, which may not generalize to variable-length inference workloads

## Confidence

- High Confidence: Compiler performance difference (Clang 19 vs GCC 13.2 showing 25-34% gains) and NUMA policy effects (balancing off + interleaving on) are well-demonstrated with clear performance differentials and mechanistic explanations
- Medium Confidence: The GEMV kernel quantization gains (38-56% GOPS) are supported by the pseudocode and performance data, but the accuracy-quality tradeoff isn't evaluated
- Low Confidence: Claims about energy efficiency parity with x86 platforms and absolute throughput numbers for different model sizes, as these depend heavily on the specific hardware configuration and may not generalize

## Next Checks

1. **Accuracy-Quality Validation**: Run the same models through standard LLM benchmarks (perplexity on standard datasets, or task-specific accuracy for common benchmarks like MMLU) to quantify the accuracy impact of the fp32→int8 quantization approach.

2. **NUMA Policy Robustness**: Test the NUMA configurations across varying prompt lengths (1 token to 1024 tokens) and batch sizes (1 to 64) to determine if the "balancing off + interleaving on" policy remains optimal across realistic workload variations.

3. **Compiler Dependency Isolation**: Create a minimal reproducible kernel that exercises the same vector instructions and quantization pattern, compile it with multiple compiler versions (GCC 13.2, GCC 14.x, Clang 19, and future versions), and measure the performance delta to determine if the claimed gains are due to specific optimization passes or more fundamental differences in code generation quality.