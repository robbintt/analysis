---
ver: rpa2
title: 'Completion by Comprehension: Guiding Code Generation with Multi-Granularity
  Understanding'
arxiv_id: '2512.04538'
source_url: https://arxiv.org/abs/2512.04538
tags:
- code
- generation
- information
- completion
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CoCo addresses the limitations of retrieval-augmented code completion\
  \ by introducing a comprehension-driven approach. It extracts multi-granularity\
  \ context\u2014function, file, and project levels\u2014through static analysis,\
  \ then uses graph-based selection to filter and prioritize relevant information."
---

# Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding

## Quick Facts
- arXiv ID: 2512.04538
- Source URL: https://arxiv.org/abs/2512.04538
- Reference count: 40
- Primary result: Achieves up to 20.2% gains in exact match accuracy for code completion through multi-granularity context understanding

## Executive Summary
CoCo addresses the limitations of retrieval-augmented code completion by introducing a comprehension-driven approach. It extracts multi-granularity context—function, file, and project levels—through static analysis, then uses graph-based selection to filter and prioritize relevant information. This context is transformed into natural language prompts to guide LLM-based code generation, while a structure-aware re-ranker ensures retrieved examples are both semantically and structurally aligned. CoCo consistently outperforms state-of-the-art baselines, achieving up to 20.2% gains in exact match accuracy, and is model-agnostic, allowing flexible integration into existing methods.

## Method Summary
CoCo combines multi-granularity static analysis with graph-based context selection and structure-aware re-ranking. The system extracts function-level control flow graphs, file-level dependencies, and project-level imports using Tree-sitter parsing. A heterogeneous semantic graph is constructed and filtered using Personalized PageRank with the target function as the central node. Retrieved code candidates are re-ranked based on AST path similarity using Jaccard metrics. The final prompt combines the filtered context, re-ranked examples, and unfinished code to guide LLM generation.

## Key Results
- Achieves up to 20.2% improvements in exact match accuracy compared to state-of-the-art baselines
- Project-level context contributes most to performance gains (RQ5 findings)
- Adds only 1.0-2.1% latency overhead while significantly improving completion quality
- Demonstrates model-agnostic benefits across DeepSeekCoder-1B, CodeLlama-7B, and Yi-Coder-1.5B

## Why This Works (Mechanism)

### Mechanism 1: Multi-Granularity Static Analysis
Extracting structured context at function, file, and project levels via static analysis captures execution logic and semantic dependencies that text-only retrieval misses. Tree-sitter and AST parsing extract control flow graphs (function level), symbol definition/usage sets (file level), and import resolution with cross-file definitions (project level). This information is transformed into natural language prompts. Accurate repository-level completion depends on understanding control flow, intra-file dependencies, and cross-module relationships—not just lexical similarity.

### Mechanism 2: Graph-Based Context Selection via Personalized PageRank
Filtering multi-granularity context through graph-based ranking prevents token overflow and removes noise that degrades generation quality. A heterogeneous semantic graph is constructed with the target function as the central node, symbols/calls/imports as other nodes. Personalized PageRank with restart probability concentrated on the central node ranks nodes by importance. Context relevance decays with structural distance from the code being completed.

### Mechanism 3: Structure-Aware Re-Ranking of Retrieved Examples
Re-ranking retrieved code candidates by AST path similarity improves structural alignment beyond semantic embedding similarity alone. AST path representations are extracted from both query and candidates, with Jaccard similarity computed on path sets. Weighted semantic and structure scores determine final ranking. Structurally similar examples provide better completion guidance than semantically similar but structurally mismatched ones.

## Foundational Learning

- **Abstract Syntax Trees (AST)**: Core data structure for all static analysis, CFG construction, symbol extraction, and path-based re-ranking. Quick check: Given a Python function, can you traverse its AST to identify all function calls and their argument types?

- **Control Flow Graphs (CFG)**: Function-level analysis models execution logic by building CFGs to capture conditional branches, loops, and return paths. Quick check: For code with nested `if-else` inside a `for` loop, can you draw the CFG nodes and directed edges?

- **Personalized PageRank (PPR)**: Context selection uses PPR to rank nodes by structural proximity, with restart focused on the target function. Quick check: How does PPR differ from standard PageRank when the personalization vector assigns all probability mass to a single seed node?

## Architecture Onboarding

- **Component map**: Tree-sitter/AST parser -> Multi-granularity extraction (function/file/project) -> Graph construction -> PPR-based selection -> Prompt template assembly -> LLM inference

- **Critical path**: Static analysis (dominant latency) -> Context extraction -> Graph filtering -> Prompt assembly -> Generation. Per Table 3, CoCo adds 1.0-2.1% latency overhead; static analysis across project files is the primary cost.

- **Design tradeoffs**:
  - k selection parameter: Higher k retains more context but increases token count and potential noise
  - Re-ranking weight: Current design yields modest gains; Jaccard on AST paths may be too strict for some patterns
  - Granularity emphasis: RQ5 shows project-level context most impactful; function-level provides marginal gains

- **Failure signatures**:
  - Static analysis errors on dynamic code patterns (eval, dynamic imports, metaprogramming)
  - Token budget exceeded when project-level extraction includes many full function bodies
  - Re-ranking returns few candidates when query AST has uncommon path patterns

- **First 3 experiments**:
  1. Baseline validation: Run CoCo vs RLCoder on CrossCodeEval Python subset; verify ~20% EM improvement (Table 1, DeepSeekCoder-1B: 22.55 → 26.53)
  2. Ablation by granularity: Disable project-level context; measure drop to confirm RQ5 finding that project-level contributes most to gains
  3. Latency profiling: Instrument static analysis, graph selection, and re-ranking separately; identify which subcomponent to optimize first

## Open Questions the Paper Calls Out

### Open Question 1
Can adaptive or learning-based extraction strategies improve context reliability over the current rule-based heuristics? The authors acknowledge that "handcrafted rules inevitably overlook certain edge cases" and suggest "exploring more robust and adaptive strategies for extracting code-related context." This remains unresolved because the current static analysis pipeline relies on rigid rules that may fail to capture complex execution paths or obscure structural patterns in diverse codebases. A comparative study integrating a learned parser or adaptive extraction component, demonstrating higher recall of relevant dependencies or improved EM scores on complex code samples, would resolve this.

### Open Question 2
Can LLM-based filtering mechanisms reduce context noise more effectively than the current graph-based PageRank selection? The authors propose that future research could "investigate LLM-based filtering mechanisms that preselect salient contextual elements or summarize raw context to reduce noise." This remains unresolved because while the graph-based method filters redundancy, the raw context may still contain distracting elements that inflate token counts and confuse the generation model. Experiments replacing Personalized PageRank with an LLM-based summarizer or filter, measuring performance (EM/F1) alongside token efficiency and latency metrics, would resolve this.

### Open Question 3
Do semantically-aware structural similarity measures outperform the current Jaccard-based re-ranker? In the ablation study discussion, the authors note that the Jaccard similarity used for structure alignment "may be overly restrictive" and suggest exploring "more flexible or semantically-aware structural similarity measures." This remains unresolved because Jaccard similarity on AST path sets is a strict metric that might penalize functionally similar code snippets with minor structural variations. An ablation study substituting Jaccard with tree-edit distance or graph embedding metrics in the re-ranker, resulting in higher structural alignment scores and generation accuracy, would resolve this.

### Open Question 4
How can the framework be extended to support the completion of non-entity code elements like comments and annotations? The authors identify a limitation where the method focuses on "substantive code entities" while "largely overlooking non-entity code elements" such as comments and formatting symbols. This remains unresolved because the current multi-granularity analysis is designed for execution logic and dependencies, lacking specific mechanisms to process or prioritize natural language documentation context. Extending the static analysis to extract comment context and evaluating the modified framework on benchmarks requiring docstring or comment completion would resolve this.

## Limitations
- Static analysis may fail on dynamic code patterns (eval, metaclasses, runtime imports) that break Tree-sitter parsing
- Exact hyperparameter settings (k values, re-ranking weights, prompt templates) are unspecified, requiring engineering judgment
- Jaccard-based structure re-ranking shows modest gains and may be overly restrictive for certain code patterns

## Confidence
- **High Confidence**: The overall architecture combining multi-granularity static analysis with graph-based filtering and structure-aware re-ranking is sound and addresses real limitations in retrieval-augmented code completion. The empirical results showing 20.2% EM improvements are well-supported by ablation studies.
- **Medium Confidence**: The specific implementations of Personalized PageRank and Jaccard-based re-ranking are reasonable but not thoroughly validated. The paper acknowledges these components could be improved, and the modest gains suggest room for refinement.
- **Low Confidence**: The exact replication of results depends on unspecified hyperparameters and prompt templates. The handling of edge cases in static analysis (dynamic code, syntax errors) is not fully detailed.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary the k parameter in PPR selection and the semantic/structure weights in re-ranking across 3-5 settings to determine which hyperparameters most affect performance gains.

2. **Dynamic Code Robustness Test**: Construct a test suite with code containing eval, dynamic imports, and metaclasses. Measure failure rates of the static analysis pipeline and document which patterns cause extraction errors.

3. **Cross-Model Generalization**: Beyond the three models tested (DeepSeekCoder-1B, CodeLlama-7B, Yi-Coder-1.5B), evaluate CoCo with at least two additional open-weight models of different sizes (e.g., StarCoder2-3B, WizardLM-7B) to verify the claimed model-agnostic benefits.