---
ver: rpa2
title: 'Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What
  We''re Asking'
arxiv_id: '2511.18394'
source_url: https://arxiv.org/abs/2511.18394
tags:
- question
- questions
- forecasting
- about
- strength
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the forecasting abilities of large language
  models (LLMs) on real-world events beyond their training cutoffs, examining how
  performance varies with domain and prompt framing. The authors collected and filtered
  10,000 prediction market questions, focusing on 392 high-quality events across six
  domains, and evaluated four model families (GPT-5, GPT-4.1, DeepSeek-R1, Claude
  3.7) both with and without news context.
---

# Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking

## Quick Facts
- arXiv ID: 2511.18394
- Source URL: https://arxiv.org/abs/2511.18394
- Authors: Chinmay Karkar; Paras Chopra
- Reference count: 8
- Primary result: LLM forecasting accuracy varies dramatically by domain structure, with structured domains yielding higher calibration.

## Executive Summary
This paper investigates how well large language models can forecast real-world events beyond their training cutoffs. The authors collected 10,000 prediction market questions, filtered to 392 high-quality events across six domains, and evaluated four model families with and without news context. Results show forecasting performance is highly uneven across domains - structured areas like geopolitics perform well while entertainment and technology prove challenging. Adding news context sometimes helps (finance, sports) but often hurts (entertainment, technology) due to recency bias, rumor overweighting, and definition drift. The study demonstrates that LLM forecasting success is conditional on prompt design and context handling rather than being a uniform emergent ability.

## Method Summary
The researchers curated 392 real-world forecasting questions beyond model cutoffs (January-July 2025) across six domains: finance, sports, geopolitics, technology, entertainment, and politics. They evaluated four model families (GPT-5, GPT-4.1, DeepSeek-R1, Claude 3.7) using uniform sampling (150 questions per model, seed=42) with temperature=0.0. Models predicted outcomes and confidence levels, which were scored using Accuracy, Brier Score (calibration), and Expected Calibration Error. For news-augmented evaluation, they retrieved 10 snippets per question via Exa API with strict temporal bounds (question creation date) to avoid leakage.

## Key Results
- Forecasting accuracy varies sharply by domain: GPT-5 achieves 84% accuracy on geopolitics vs 52% on entertainment without news context
- Brier scores follow similar patterns (0.14 vs 0.24), showing structured domains yield better calibration
- Adding news context helps in finance and sports but harms in entertainment and technology
- Three key failure modes identified: recency bias, rumor overweighting, and definition drift
- Reasoning models perform better in structured domains while struggling with fast-moving domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM forecasting accuracy varies systematically by domain structure, with structured domains yielding higher calibration.
- Mechanism: Domains with clear precedent patterns, institutional constraints, and slower-moving variables map more reliably onto pretraining correlations than noisy, fast-moving domains.
- Core assumption: Pretraining exposure encodes structural regularities that transfer to post-cutoff forecasting.
- Evidence anchors:
  - [abstract] "predictive ability varies sharply with domain structure and prompt framing"
  - [section] Table 1 shows GPT-5 achieves 84% accuracy on Geopolitics vs 52% on Entertainment without news context
  - [corpus] Related work on temporal understanding suggests structured temporal grounding improves forecasting

### Mechanism 2
- Claim: Adding contemporaneous news context produces domain-dependent calibration shifts that can improve or harm accuracy.
- Mechanism: News provides signal in domains with low base-rate variance and clear causal chains but introduces noise in domains with high speculation-to-signal ratios.
- Core assumption: Temporal filtering removes leakage, though residual contamination remains possible.
- Evidence anchors:
  - [abstract] "adding factual news context modifies belief formation and failure modes"
  - [section] Table 2 shows Finance accuracy improves from 44-56% with news while Entertainment drops from 68% to 56%
  - [corpus] Weak direct evidence; FutureOmni studies multimodal forecasting context but doesn't isolate news effects

### Mechanism 3
- Claim: Three recurrent failure modes—recency bias, rumor overweighting, and definition drift—systematically degrade news-augmented forecasts.
- Mechanism: Models overweight recently-retrieved snippets over encoded priors, anchor to unverified speculation, or shift semantic grounding based on snippet frequency.
- Core assumption: These failure modes emerge from attention mechanisms lacking explicit source-credibility weighting.
- Evidence anchors:
  - [abstract] "key failure modes: recency bias, rumor overweighting, and definition drift"
  - [section] Appendix B.1-B.3 provides concrete cases: S&P 500 forecast flips from correct (NO, 0.34) to incorrect (YES, 0.54) after reading bullish headlines
  - [corpus] Paleka et al. identifies unreliable news retrieval and logical leakage as evaluation pitfalls

## Foundational Learning

- Concept: **Brier Score**
  - Why needed here: Primary metric for probabilistic calibration; distinguishes "right answer with wrong confidence" from "well-calibrated forecast"
  - Quick check question: If a model predicts 90% confidence on 10 events and gets 9 correct, what is its Brier Score? (Answer: 0.09—lower is better)

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: Measures whether confidence matches empirical accuracy; high ECE indicates systematic over/underconfidence
  - Quick check question: A model predicts 70% confidence on 100 questions and gets 50 correct. Is it overconfident, underconfident, or calibrated? (Answer: Overconfident—ECE would be |0.50 - 0.70| = 0.20)

- Concept: **Temporal Contamination**
  - Why needed here: Critical for evaluating forecasting beyond training cutoffs; leakage invalidates claims about predictive ability
  - Quick check question: Why does the paper enforce a publication-date cutoff before question creation? (Answer: To prevent models from accessing future information that resolves the question)

## Architecture Onboarding

- Component map:
  Data pipeline: 10K raw questions → volume filter → LLM-judge classification (6 categories × 5 subcategories) → relevance filter → 392 curated questions
  Evaluation loop: Uniform sample (150 questions, seed=42) → model inference (temp=0.0, max_tokens=4500) → extract `<answer>` and `<conf>` → compute Accuracy/Brier/ECE
  Context augmentation: Exa API retrieval (10 snippets per question, creation-date bounded) → temporal purity check → append to prompt → re-evaluate

- Critical path:
  1. Curate questions beyond model cutoff (Jan-July 2025)
  2. Run baseline evaluation without external context
  3. Retrieve and filter news snippets with strict temporal bounds
  4. Compare calibration shifts across domains and failure mode frequencies

- Design tradeoffs:
  - Temperature=0.0 ensures deterministic outputs but suppresses uncertainty exploration
  - 10-snippet limit balances context richness vs. attention dilution
  - LLM-as-judge filtering scales curation but may inject classification errors
  - No tool use/web search isolates parametric knowledge but limits real-world applicability

- Failure signatures:
  - Recency bias: Model reverses correct forecast after reading recent snippets (check for confidence flips >0.15 toward recent-headline direction)
  - Rumor overweighting: Model anchors to "likely" or "expected" language in snippets rather than resolved outcomes
  - Definition drift: Model reinterprets acronyms or entities based on snippet frequency (check for semantic grounding shifts in reasoning traces)
  - DeepSeek-R1: Missing reasoning traces despite prompt instructions—treat as black-box output

- First 3 experiments:
  1. Domain sweep: Run baseline evaluation across all 6 categories with consistent prompt; identify high/low-performing domains and compute ECE gap to diagnose calibration quality
  2. Context ablation: For each domain, compare news-augmented vs. baseline accuracy; calculate delta-Brier to quantify whether context helps or harms
  3. Failure mode audit: Manually label 20 news-augmented forecasts per domain for recency/rumor/drift; compute correlation between failure frequency and accuracy drop to prioritize mitigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompt engineering or context-filtering mechanisms could mitigate the identified failure modes (recency bias, rumor overweighting, definition drift) when providing news context to LLM forecasters?
- Basis in paper: The authors explicitly identify these three failure modes and state that adding news "sometimes it does while at other times it makes it worse because of definition drift, rumour anchoring and recency bias."
- Why unresolved: The paper documents the failure modes but does not propose or test interventions to address them.
- What evidence would resolve it: A follow-up study testing modified prompts or retrieval filters showing reduced error rates.

### Open Question 2
- Question: Can benchmarks be designed to cleanly disentangle an LLM's knowledge recall from its probabilistic reasoning and calibration?
- Basis in paper: The conclusion states that findings "invite design of benchmarks that disentangle knowledge recall from probabilistic inference."
- Why unresolved: Current benchmarks conflate recall of historical patterns with forward-looking reasoning.
- What evidence would resolve it: A benchmark architecture with controlled knowledge conditions showing independent measurement of recall vs. inference.

### Open Question 3
- Question: What structural properties of forecasting questions predict cross-domain variation in LLM performance?
- Basis in paper: The study shows dramatic performance variance across domains but does not systematically identify which question features drive this variation.
- Why unresolved: The taxonomy of categories is descriptive, not explanatory.
- What evidence would resolve it: Regression or feature importance analysis linking quantifiable question properties to accuracy and calibration metrics.

## Limitations
- Temporal leakage risk remains possible despite publication-date cutoffs
- Domain definitions assigned by LLM judges rather than human experts introduce potential bias
- Context retrieval quality limited to 10 snippets per question with no relevance scoring analysis

## Confidence
- High confidence: Domain structure drives systematic accuracy differences; Brier score and ECE provide valid calibration metrics; identified failure modes are empirically observable
- Medium confidence: News context helps in finance/sports but harms in entertainment/technology; performance ranking across model families is stable
- Low confidence: Absolute accuracy levels reflect true forecasting ability rather than prompt overfitting; observed patterns generalize beyond curated sample

## Next Checks
1. Leakage audit: Cross-reference retrieved snippets with actual question resolutions to quantify false-positive contamination rate; recalculate domain accuracies excluding potentially leaked questions
2. Retrieval sensitivity: Vary snippet count (5, 10, 15, 20) and implement relevance filtering to measure impact on Brier scores and failure mode frequencies
3. Expert validation: Have human forecasters evaluate 50 randomly selected questions to benchmark LLM performance and identify systematic blind spots not captured by Brier/ECE metrics