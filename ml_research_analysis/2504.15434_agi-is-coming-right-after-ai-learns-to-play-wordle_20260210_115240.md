---
ver: rpa2
title: AGI Is Coming... Right After AI Learns to Play Wordle
arxiv_id: '2504.15434'
source_url: https://arxiv.org/abs/2504.15434
tags:
- wordle
- game
- color
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates OpenAI's Computer-User Agent (CUA) on the
  Wordle game to assess its perceptual and reasoning capabilities. CUA, a multimodal
  agent that interacts with computer interfaces through raw pixel processing and tool
  use, was tested on hundreds of Wordle games over eight days.
---

# AGI Is Coming... Right After AI Learns to Play Wordle

## Quick Facts
- arXiv ID: 2504.15434
- Source URL: https://arxiv.org/abs/2504.15434
- Authors: Sarath Shekkizhar; Romain Cosentino
- Reference count: 10
- Primary result: CUA achieves 5.36% success rate on Wordle with accuracy dropping from 42% to 6% across attempts

## Executive Summary
This paper evaluates OpenAI's Computer-User Agent (CUA) on the Wordle game to assess its perceptual and reasoning capabilities. Despite CUA's impressive performance on other benchmarks, it achieved only a 5.36% success rate on Wordle, with significant color recognition failures. The agent's accuracy declined from 42% on the first attempt to just 6% by the fifth attempt, particularly struggling to distinguish gray tiles from colored tiles and often hallucinating colors on gray tiles. These findings demonstrate fundamental challenges in current AI architectures for simple visual tasks requiring consistent color perception and reasoning.

## Method Summary
The study used CUA API with a local Playwright environment to run Wordle sessions through a Chromium browser. The agent iteratively took 1024×768 screenshots, typed guesses, submitted via ENTER keypress, and self-annotated observations using the `update_wordle_game_state` tool. Sessions ran for 25 attempts per day over 8 days (200 total runs), logging success/failure, number of guesses, and reported color observations versus ground truth. The methodology focused on quantifying success rates, average guesses per solved puzzle, and per-attempt/per-position color recognition accuracy.

## Key Results
- CUA achieved only 5.36% success rate on Wordle games
- Average of 3.25 guesses per solved puzzle
- Color recognition accuracy declined from 42% (attempt 1) to 6% (attempt 5)
- Agent struggled particularly with distinguishing gray tiles, often hallucinating colors on gray tiles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative perception-action loops enable GUI task completion
- Mechanism: CUA operates through a cycle of screenshot perception → chain-of-thought reasoning → action execution (click, type, scroll), repeating until task completion signals appear.
- Core assumption: Visual attention maintains consistency across iterations as the screen state changes.
- Evidence anchors: [abstract] "CUA, a multimodal agent that interacts with computer interfaces through raw pixel processing and tool use"
- Break condition: Attention degradation across iterations (observed 42% → 6% color accuracy from attempt 1 to 5) breaks the loop.

### Mechanism 2
- Claim: Image tokenization patch boundaries affect spatial perception accuracy
- Mechanism: Screenshots are divided into discrete patches (2×2 grid of 512×512 pixels); features crossing patch boundaries may receive degraded attention compared to centered features.
- Core assumption: Patch division aligns with meaningful visual regions; edge effects at boundaries reduce feature coherence.
- Evidence anchors: [section] "each screenshot image (1024 × 768) is divided into 4 patches in a 2 × 2 grid... the screenshot images with the Wordle grid are divided exactly around letter position 3 and attempt number 3"
- Break condition: Tasks requiring fine-grained perception at predicted boundary regions (positions 3-4, attempt 3+) should show systematic accuracy drops.

### Mechanism 3
- Claim: RLHF post-training induces optimistic color hallucination bias
- Mechanism: Models fine-tuned with RLHF may exhibit overconfidence, preferentially generating expected "successful" outputs (green/yellow) over neutral outputs (gray), leading to systematic Gray→Green/Yellow errors.
- Core assumption: Reward modeling during RLHF implicitly favors confident, task-progressing outputs over uncertainty acknowledgment.
- Evidence anchors: [section] "Gray→Yellow and Gray→Green were the most common errors... this optimistic bias is common with LLM and often due to their RLHF post-training"
- Break condition: Tasks requiring accurate negative feedback recognition will systematically fail if model biases toward positive signals.

## Foundational Learning

- Concept: Iterative visual grounding
  - Why needed here: Multimodal agents must maintain perceptual consistency across sequential observations; without grounding, each screenshot is interpreted independently, causing drift.
  - Quick check question: Can you track a single object's state across 5+ screenshots without accumulating error?

- Concept: Tokenization spatial bias
  - Why needed here: Vision transformers discretize continuous images; understanding where patch boundaries fall helps predict systematic blind spots.
  - Quick check question: Given a 1024×768 image divided into 2×2 patches, which spatial regions would you expect highest and lowest attention quality?

- Concept: Reward-model calibration
  - Why needed here: RLHF-tuned models may miscalibrate confidence; recognizing systematic optimism/pessimism helps interpret agent outputs correctly.
  - Quick check question: If an agent reports 90% confidence on a task but succeeds only 5% of the time, what systematic bias might explain the gap?

## Architecture Onboarding

- Component map: Screenshot capture (1024×768) → Image tokenizer (4 patches, 2×2) → Multimodal backbone (GPT-4o vision + reasoning) → Action primitives (click, type, scroll, keypress) → Environment feedback → Loop

- Critical path:
  1. Task instruction injection (system prompt)
  2. Initial screenshot perception
  3. Action selection via chain-of-thought
  4. Environment state change
  5. Next screenshot perception → repeat until termination condition (success, max attempts, timeout)

- Design tradeoffs:
  - Patch-based tokenization trades fine-grained spatial precision for computational efficiency
  - RLHF alignment trades raw perceptual accuracy for task-oriented confidence
  - Self-annotation tools add overhead but enable failure mode analysis

- Failure signatures:
  - Declining perception accuracy across iterations (42% → 6%)
  - Systematic Gray→Green/Yellow hallucinations
  - Edge-position accuracy higher than center-position accuracy
  - Premature task completion claims (agent believes it solved when it hasn't)

- First 3 experiments:
  1. Run the agent on isolated color grid recognition (no multi-step reasoning) to establish baseline perceptual accuracy
  2. Vary screenshot resolution and aspect ratio to test whether patch boundary artifacts shift predictably
  3. Introduce explicit calibration prompts asking the agent to report uncertainty alongside color observations; measure whether Gray→colored errors decrease

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does image tokenization patch boundary alignment directly cause the observed spatial degradation in color perception (edges more accurate than center)?
- Basis in paper: [explicit] Authors hypothesize that the 2×2 patch division (512×512 each) places boundaries near position 3, correlating with poorer accuracy at central positions.
- Why unresolved: No access to model internals; the patch boundary was inferred from documentation, not directly verified.
- What evidence would resolve it: Testing with shifted interface layouts or varying screenshot dimensions to observe whether accuracy patterns follow predicted patch boundaries.

### Open Question 2
- Question: To what extent do the observed perception failures generalize to other multimodal agents (e.g., Claude, Gemini, Agent S)?
- Basis in paper: [explicit] "We believe the issues identified and conclusions made in this paper are not limited to CUA and should be broadly applicable to other multimodal models."
- Why unresolved: Only CUA was tested; other agents were not evaluated on the same Wordle protocol.
- What evidence would resolve it: Replicating the Wordle evaluation protocol across multiple computer-using agents with identical settings.

### Open Question 3
- Question: Is the systematic bias toward hallucinating colors on gray tiles caused by RLHF post-training optimism rather than perceptual limitations?
- Basis in paper: [inferred] Authors note Gray→Yellow/Green errors predominate and link this to "optimistic bias... common with LLM and often due to their RLHF post-training."
- Why unresolved: Causal mechanism not tested; could alternatively reflect reasoning confidence issues rather than training-induced bias.
- What evidence would resolve it: Comparing error patterns across models with different training procedures (RLHF vs. RLAIF vs. base models) on identical perceptual tasks.

### Open Question 4
- Question: Can tool-augmented approaches (zoom, crop, "thinking with images") fundamentally solve context-dependent perception, or do they merely circumvent tokenization limitations?
- Basis in paper: [explicit] Authors describe OpenAI's "thinking with images" solution as circumventing tokenization but argue "a more fundamental approach [is] required to truly overcome."
- Why unresolved: No comparison of tool-augmented vs. native perception on controlled multi-step visual tasks.
- What evidence would resolve it: Benchmarking tool-augmented perception on tasks requiring holistic spatial reasoning (not just localized detail extraction).

## Limitations

- The exact implementation details of the CUA API used are unknown, including model version and proprietary optimizations
- The study lacks systematic ablation studies to definitively isolate whether patch boundary effects, RLHF calibration, or other factors drive the observed accuracy decline
- The 5.36% success rate may not generalize beyond Wordle's specific visual design and could reflect edge-case brittleness

## Confidence

- High Confidence: The core empirical findings (5.36% success rate, 3.25 average guesses per solved puzzle, 42% → 6% accuracy decline) are well-supported by the described methodology and reproducible through the provided procedure.
- Medium Confidence: The attribution of Gray→Yellow/Green hallucinations to RLHF optimism is plausible given related literature but lacks direct experimental validation within this study.
- Low Confidence: The specific claim that 2×2 patch boundaries systematically degrade center-position accuracy is hypothesized but not experimentally verified with alternative patch configurations.

## Next Checks

1. Run the agent on a modified Wordle interface with uniform background colors and larger letter tiles to test whether accuracy improves when patch boundaries no longer intersect key visual features.
2. Implement an uncertainty-aware variant where the agent explicitly reports confidence scores alongside color observations; compare Gray→colored error rates between confident vs. uncertain predictions.
3. Conduct a patch configuration ablation study varying patch count and arrangement (e.g., 1×4 vs. 2×2) to empirically determine whether spatial discretization artifacts drive the observed accuracy patterns.