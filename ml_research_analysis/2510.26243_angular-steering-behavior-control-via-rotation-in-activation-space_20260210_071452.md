---
ver: rpa2
title: 'Angular Steering: Behavior Control via Rotation in Activation Space'
arxiv_id: '2510.26243'
source_url: https://arxiv.org/abs/2510.26243
tags:
- steering
- direction
- b-instruct
- angular
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Angular Steering introduces a novel activation steering method
  that reformulates behavior control as a geometric rotation within a fixed 2D subspace
  of activation space. By treating steering as rotation toward or away from a target
  behavior direction, the method provides continuous, fine-grained control over behaviors
  like refusal and compliance while preserving model stability.
---

# Angular Steering: Behavior Control via Rotation in Activation Space

## Quick Facts
- arXiv ID: 2510.26243
- Source URL: https://arxiv.org/abs/2510.26243
- Authors: Hieu M. Vu; Tan M. Nguyen
- Reference count: 40
- Primary result: Introduces Angular Steering, reformulating behavior control as rotation within a 2D activation subspace, achieving robust control with minimal performance degradation

## Executive Summary
Angular Steering introduces a novel activation steering method that reformulates behavior control as a geometric rotation within a fixed 2D subspace of activation space. By treating steering as rotation toward or away from a target behavior direction, the method provides continuous, fine-grained control over behaviors like refusal and compliance while preserving model stability. Experiments across multiple model families and sizes demonstrate that Angular Steering achieves robust behavioral control with minimal performance degradation on general language tasks. The approach generalizes existing vector addition and directional ablation techniques under a unified geometric framework, simplifying parameter selection. Additionally, an adaptive variant selectively applies steering based on feature alignment, further enhancing stability and coherence.

## Method Summary
Angular Steering extracts normalized activations after RMSNorm layers and computes a target feature direction via difference-in-means from contrastive datasets. A steering plane is constructed using the feature direction and the first principal component of candidate directions across layers. During inference, activations are projected onto this plane and rotated by a specified angle while preserving norm, with an optional adaptive mask that restricts rotation to aligned activations. The method is applied across all transformer blocks at every normalization layer, with steering coefficients tuned empirically for each model family.

## Key Results
- Achieves continuous behavioral control across 0°-360° angular spectrum for refusal/compliance behaviors
- Adaptive variant maintains lower perplexity than non-adaptive steering, especially in smaller models
- Generalizes across model families (QWEN2.5, LLAMA-3.2, Qwen) and sizes (3B-14B)
- Minimal performance degradation on TINYBENCHMARKS capability preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rotating activations within a fixed 2D subspace provides continuous behavioral control while minimizing interference with unrelated features.
- Mechanism: Angular Steering constructs a steering plane P from two orthonormal directions: (1) a target feature direction d̂_feat (e.g., refusal) extracted via difference-in-means, and (2) the first principal component of candidate directions across layers (d̂_PC0). Activations are rotated within this plane using a rotation matrix R^P_θ, preserving norm while shifting angular position relative to the feature direction.
- Core assumption: Features correspond to nearly orthogonal directions in activation space (Superposition Hypothesis), and constraining intervention to a 2D subspace minimizes unintended feature perturbation.
- Evidence anchors:
  - [abstract] "reformulates behavior control as a geometric rotation within a fixed 2D subspace of activation space"
  - [section 4.2] "Restricting the rotation to a 2D subspace confines changes to just two orthogonal directions, leaving the remaining basis vectors unaffected"
  - [corpus] Weak direct validation; related work "Selective Steering" supports norm-preservation benefits but doesn't test rotation specifically
- Break condition: If the target feature direction is poorly isolated (entangled with other features in the 2D plane), steering causes coherence degradation—observed in smaller models (3B) where perplexity spikes across certain angular arcs.

### Mechanism 2
- Claim: Normalized activations are more suitable for steering because RMSNorm in modern LLMs emphasizes directional over magnitude-based representation.
- Mechanism: The paper extracts activations after normalization layers (before Attention and MLP blocks). RMSNorm maps activations to a scaled unit sphere, making pre-normalization magnitude manipulations (like activation addition) unstable across layers with exponentially growing norms.
- Core assumption: Direction encodes semantic information more stably than magnitude across transformer depth.
- Evidence anchors:
  - [section 4.1] "RMSNorm first maps the activation to a √d_model-scaled unit sphere...highlighting direction, not magnitude, as the core representational unit"
  - [figure 3] Shows activation norms growing exponentially across layers for QWEN2.5-7B
  - [corpus] No direct corpus validation of this specific claim
- Break condition: If steering is applied to raw (pre-normalization) activations without norm-aware scaling, coefficient tuning becomes layer-specific and brittle.

### Mechanism 3
- Claim: Adaptive masking (rotating only activations aligned with the target feature) enhances coherence by avoiding unnecessary perturbation.
- Mechanism: A conditional mask is computed as `mask = max(0, sign(proj_{d̂_feat}(h)))`, restricting rotation to activations with positive alignment to the feature direction. This prevents steering harmless inputs away from their natural representation.
- Core assumption: Harmful and harmless inputs have opposite alignment with the refusal direction (validated empirically).
- Evidence anchors:
  - [section 4.6.2] Describes adaptive mask formulation in Equation 3
  - [figure 4] Shows harmful vs. harmless activations have opposite scalar projections onto refusal direction
  - [figure 8b] Adaptive steering maintains lower perplexity than non-adaptive across all models
  - [corpus] No direct corpus comparison of adaptive vs. non-adaptive approaches
- Break condition: If the feature direction doesn't cleanly separate target vs. non-target inputs (e.g., subtle behaviors), the binary mask may incorrectly skip needed steering or apply unnecessary intervention.

## Foundational Learning

- Concept: **Linear Representation Hypothesis**
  - Why needed here: Angular Steering assumes features correspond to directions in activation space; without this, rotation has no semantic interpretation.
  - Quick check question: Can you explain why projecting an activation onto a direction measures "feature strength"?

- Concept: **Rotation Matrices in High-Dimensional Spaces**
  - Why needed here: The method constructs a rotation matrix that operates in a 2D subspace while preserving all other dimensions unchanged.
  - Quick check question: Given orthonormal basis vectors b1, b2, how would you construct a matrix that rotates by angle θ only in Span{b1, b2}?

- Concept: **RMSNorm and Residual Stream Scaling**
  - Why needed here: Understanding why activation norms grow exponentially across layers explains why coefficient-based methods are brittle and why normalized steering is more stable.
  - Quick check question: Why does the residual stream's additive structure cause norm growth, and how does RMSNorm normalize this?

## Architecture Onboarding

- Component map:
  - Feature Direction Extractor -> Steering Plane Constructor -> Angular Steering Operator -> Adaptive Mask -> Intervention Points

- Critical path:
  1. Collect activations from contrastive datasets (harmful/harmless) at all extraction points
  2. Compute candidate directions via difference-in-means per layer
  3. Select d̂_feat with maximum average cosine similarity
  4. Construct steering plane via PCA on candidates
  5. At inference: project → rotate → residual add (with optional adaptive mask)

- Design tradeoffs:
  - **Span{h, d_feat} vs. Span{d̂_feat, d̂_PC0}**: Paper argues against using raw activation h as basis vector because it may include unrelated dominant features; fixed plane better isolates target feature
  - **Adaptive vs. Non-adaptive**: Adaptive preserves coherence better (lower perplexity) but may miss edge cases where alignment signal is weak
  - **Single vs. Multi-layer intervention**: Paper applies steering across all layers following prior work showing multi-layer is more effective

- Failure signatures:
  - **Coherence breakdown in small models**: QWEN2.5-3B shows high perplexity and incoherent outputs across wide angular arcs (attributed to feature interference in limited-capacity models)
  - **Language switching**: LLAMA-3.2-3B under non-adaptive steering outputs refusal phrases in random languages despite English prompts
  - **Performance degradation arc**: TINYBENCHMARKS scores drop in specific angular ranges (e.g., 160°-280° for QWEN2.5-3B), suggesting competing features in steering plane
  - **Random plane failure**: Ablation shows steering with random directions produces minimal effect or incoherent outputs

- First 3 experiments:
  1. **Validate feature direction separation**: Replicate Figure 4—pass harmful/harmless prompts through model, compute scalar projections onto locally extracted refusal directions at each layer, confirm opposite alignment patterns
  2. **Steering circle sweep with refusal scoring**: Apply Adaptive Angular Steering at 10° intervals from 0°-360°, evaluate with substring matching + LLAMAGUARD3 + HARMBENCH to replicate Figure 7a behavioral arc pattern
  3. **Perplexity comparison (adaptive vs. non-adaptive)**: Generate outputs at multiple angles, compute perplexity using non-steered model, confirm adaptive variant maintains baseline-level perplexity while non-adaptive shows instability in smaller models

## Open Questions the Paper Calls Out

- **Question**: How can steering subspaces be systematically identified to generalize optimally across diverse behaviors and model architectures, beyond the current heuristic selection method?
  - Basis in paper: [explicit] The authors state: "A limitation of Angular Steering is that while promising, it currently relies on heuristically selected steering planes, which might not always generalize optimally across diverse behaviors or architectures."
  - Why unresolved: The paper uses PCA on candidate directions as a heuristic, but provides no principled framework for subspace selection that guarantees transfer across tasks or model families.
  - What evidence would resolve it: Development of a systematic method (e.g., learning-based or theoretical) for selecting planes that maintains effectiveness across behaviors (refusal, emotion, others) and architectures, with empirical validation.

- **Question**: Why do smaller models (3B parameters) exhibit feature interference and coherence breakdowns under Angular Steering while larger models (7B-14B) remain stable, and can architectural or methodological modifications mitigate this vulnerability?
  - Basis in paper: [inferred] Section 6.2 documents that "Smaller Models are More Vulnerable to Interference under Angular Steering," with QWEN2.5-3B and LLAMA-3.2-3B producing incoherent outputs or language-switching under non-adaptive steering, attributed to limited capacity causing feature entanglement in the 2D subspace.
  - Why unresolved: The paper describes the phenomenon but does not identify specific architectural factors or propose solutions beyond using Adaptive Steering as a partial mitigation.
  - What evidence would resolve it: Ablation studies varying model capacity systematically, analysis of feature overlap in activation space across model sizes, or modified steering approaches that account for feature density.

- **Question**: What are the mechanistic implications of the finding that harmful generations exhibit lower perplexity than refusal responses under steering, and does this indicate that safety alignment operates primarily through early-token distribution shifts rather than truly removing harmful capabilities?
  - Basis in paper: [inferred] Section 6.2 observes: "Harmful generations (learnt during pretraining) have lower perplexity than refusal responses (learnt during safety tuning), indicating they remain more probable... harmful behaviors are not truly removed but only masked."
  - Why unresolved: This finding aligns with prior work (Bricken et al.) suggesting safety mechanisms may be superficial shortcuts, but the paper does not investigate whether Angular Steering's rotation-based approach addresses this deeper than other methods.
  - What evidence would resolve it: Comparative analysis of perplexity trajectories across steering methods, probing studies on harmful capability preservation, or layer-wise analysis of when safety vs. harmful representations diverge.

## Limitations

- Feature isolation weakness: Method assumes 2D planes can isolate target behaviors without interference, but smaller models show coherence breakdowns suggesting feature entanglement
- Adaptive masking binary assumption: Hard binary mask may fail for nuanced behaviors with gradual transitions rather than clear alignment differences
- Layer selection heuristics: Omitting late-layer extraction points based on "spikes" lacks quantitative criteria, potentially missing important representations

## Confidence

**High confidence**: The geometric framework itself (rotation within 2D subspace), the empirical demonstration of continuous behavioral control across the 0°-360° spectrum, and the superiority of normalized steering over magnitude-based approaches.

**Medium confidence**: The effectiveness of adaptive masking for coherence preservation, the generalizability across model families, and the claims about minimal general capability degradation.

**Low confidence**: The assumption that 2D planes can cleanly isolate complex behaviors without interference, the binary nature of adaptive masking for nuanced behaviors, and the scalability to behaviors with subtle or distributed representations.

## Next Checks

1. **Steering plane contamination analysis**: Systematically test whether performance degradation in specific angular ranges correlates with interference from other behavioral features. Use feature attribution methods to identify competing directions in the steering plane and quantify their contribution to coherence breakdown.

2. **Continuous adaptive masking**: Replace the binary sign-based mask with a continuous weighting function based on alignment strength. Test whether this improves coherence for behaviors with gradual transitions and reduces the binary assumption's brittleness.

3. **Multi-plane steering validation**: Test whether behaviors that show degradation with single-plane steering can be better controlled using multiple steering planes or hierarchical steering approaches. Compare coherence and behavioral control against the single-plane baseline.