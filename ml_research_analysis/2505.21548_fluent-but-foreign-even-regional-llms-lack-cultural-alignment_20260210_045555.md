---
ver: rpa2
title: 'Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment'
arxiv_id: '2505.21548'
source_url: https://arxiv.org/abs/2505.21548
tags:
- cultural
- indian
- indic
- india
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates six Indic and six global LLMs on cultural alignment
  using human-grounded benchmarks and a user study. Across four tasks grounded in
  nationally representative surveys and community-authored QA datasets, Indic models
  did not align better with Indian values or practices than global models.
---

# Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment

## Quick Facts
- arXiv ID: 2505.21548
- Source URL: https://arxiv.org/abs/2505.21548
- Reference count: 40
- Primary result: Indic models do not align better with Indian cultural values than global models; regional fine-tuning fails to add cultural knowledge.

## Executive Summary
This paper evaluates six Indic and six global LLMs on cultural alignment using human-grounded benchmarks and a user study. Across four tasks grounded in nationally representative surveys and community-authored QA datasets, Indic models did not align better with Indian values or practices than global models. In fact, a U.S. respondent was often a closer proxy for Indian cultural values than any Indic model. The user study with 115 Indian participants showed that writing suggestions from both global and Indic models introduced Westernized or exoticized writing. Prompting strategies and regional fine-tuning failed to recover alignment and sometimes degraded existing knowledge. The authors attribute these results to a scarcity of culturally grounded data, especially for pretraining, and advocate for native, community-authored corpora and thick×wide evaluations that combine population-scale surveys with participatory co-design to build truly sovereign LLMs.

## Method Summary
The paper evaluates 12 LLMs (6 Indic, 6 global) across four tasks: World Values Survey (cultural positioning), GlobalOpinionQA (opinion alignment), CulturalBench (knowledge accuracy), and NormAd (reasoning accuracy). Models are tested with four prompting strategies (Default, Demographic, Cross-lingual, Hindi) and multiple answer permutations to control for position bias. Cultural alignment is measured using Cultural Alignment Differential (CAD) and normalized CAD metrics. The user study involves 115 Indian participants co-writing scenarios with models under four conditions, measuring embedding similarity and qualitative coding of output characteristics.

## Key Results
- Indic models do not outperform global models on cultural alignment metrics across all four tasks
- Regional fine-tuning often degrades performance: gajendra-7b performs 12.5% worse on India questions than its base model
- A U.S. respondent is a closer proxy for Indian cultural values than any Indic model in opinion alignment tasks
- User study shows writing suggestions from both global and Indic models introduce Westernized or exoticized writing

## Why This Works (Mechanism)

### Mechanism 1
Regional fine-tuning on Indian-language corpora fails to inject culturally grounded knowledge because it cannot overcome the Western-centric patterns embedded during pretraining. Web-scale pretraining corpora are overwhelmingly produced by Western internet users, encoding Western value orientations and cultural knowledge at the base model level. Fine-tuning on translated or small-scale regional data adds only a "thin slice" that cannot reweight the deeply embedded prior distributions.

### Mechanism 2
Prompting strategies provide only surface-level alignment artifacts without shifting underlying cultural reasoning. Prompts can bias model outputs toward specific token patterns (e.g., Indian artifact names), but the model's internal representations and reasoning pathways remain anchored to Western-centric associations learned during training. The prompt acts as a soft constraint rather than a knowledge source.

### Mechanism 3
Cultural alignment and linguistic fluency are dissociable capabilities that require separate evaluation and training interventions. Language models optimize for statistical patterns in text, which can achieve grammatical correctness in target languages without encoding cultural norms, values, or practices. Fluency reflects surface-level token prediction; cultural alignment requires world knowledge and normative reasoning that may not be present in language-only corpora.

## Foundational Learning

- **Hofstede's cultural onion (values vs. practices)**: The paper structures evaluation around this framework—values (what people believe, measured via WVS/GlobalOpinionQA) and practices (what people do, measured via CulturalBench/NormAd). Quick check: Would a question about "whether drinking alcohol is morally acceptable" probe values or practices? (Answer: Values)

- **Distributional pluralism vs. steerable pluralism**: The paper discusses who models should represent and endorses population-scale grounding. Understanding whether a model should reflect aggregate population distributions or be steerable per-user is critical for interpreting alignment metrics. Quick check: If a model reflects 50% of a population's views on each output, is it practicing distributional or Overton pluralism? (Answer: Distributional)

- **Thick vs. wide evaluation**: The authors advocate "thick × wide" evaluation combining community-engaged, context-rich methods (thick) with population-scale survey instruments (wide). This methodological innovation underpins their contribution. Quick check: Is a nationally representative survey like WVS a "thick" or "wide" evaluation component? (Answer: Wide)

## Architecture Onboarding

- **Component map**: World Values Survey (cultural map positioning) -> GlobalOpinionQA (opinion alignment CAD/nCAD) -> CulturalBench (knowledge accuracy) -> NormAd (reasoning accuracy) -> User study (embedding similarity + qualitative coding) -> Prompting intervention layer (4 strategies across all tasks)

- **Critical path**: Select models (6 Indic, 6 global) with instruction-tuned variants -> Run all four benchmark tasks with multiple prompting strategies and answer permutations -> Compare Indic models against both global baselines and their base checkpoints -> Validate benchmark findings through user study co-writing experiment -> Analyze whether fine-tuning added knowledge or degraded base capabilities

- **Design tradeoffs**: Survey-based vs. community-authored data (surveys enable population-scale comparison but treat nations as homogeneous; community datasets are richer but limited in coverage) -> Logit-based vs. sampling-based evaluation (opinion alignment uses logit-biasing for deterministic distributions; other tasks use sampling with temperature 0.3 and multiple trials) -> Permutation controls (answer option permutations control for position bias but multiply evaluation cost significantly)

- **Failure signatures**: Neutral CAD with high fluency (model produces grammatically correct Hindi but shows no tilt toward Indian values) -> Fine-tuning degradation (Indic model scores lower than its base model on both India and US questions) -> Prompt-sensitive outputs (demographic prompting changes artifact names but not descriptive quality)

- **First 3 experiments**: 1) Baseline alignment audit: Run Cultural Map + Opinion Alignment on your model with Default prompting; compute distance to target country and CAD values. Compare against the India-US reference distance. 2) Fine-tuning delta analysis: If adapting a base model, evaluate both base and fine-tuned versions on CulturalBench India-specific questions. Expect degradation; document magnitude. 3) Prompting ablation: Test Demographic and Cross-lingual prompting on a subset of NormAd Country-level questions. Measure whether accuracy improves or whether the model simply becomes more confident in wrong answers.

## Open Questions the Paper Calls Out

- Would frontier-scale (100B+ parameters) regional models exhibit better cultural alignment, or does the Western-centric pretraining data bottleneck persist regardless of model scale? The authors note that all evaluated Indic models were small (7–24B parameters) and that "no frontier-scale non-Western model exists yet for India," limiting conclusions about whether scale could mitigate cultural misalignment.

- What are effective mechanisms for creating native, community-authored corpora at scale that reflect culturally grounded values, practices, and lived realities? The authors explicitly call for "native, community-authored corpora" and note that current Indic models rely on "translated English instruction datasets" or "tiny" native corpora, which fail to offset Western-centric pretraining data.

- How should regional models handle intranational cultural pluralism, and which pluralism paradigm (distributional, overton, or steerable) best serves diverse user populations? The authors raise the question "who should models represent?" and cite Sorensen et al.'s framework of overton, steerable, and distributional pluralism, noting that "a plurality of values and practices exists... within a nation-state."

- Can pretraining objectives be explicitly designed to prioritize cultural alignment alongside linguistic fluency, and what trade-offs with general capabilities would emerge? The authors find that "regional fine-tuning fails to add knowledge" and sometimes degrades performance, suggesting that post-hoc adaptation is insufficient and that "training objectives that prioritize cultural alignment" may be needed.

## Limitations

- The study relies on survey-based benchmarks (WVS, GlobalOpinionQA) that treat nations as homogeneous cultural units, potentially missing subcultural diversity within India. The choice of US as the primary comparator assumes a stable reference point, but US cultural values are also heterogeneous and evolving.

- Fine-tuning experiments show degradation for some models (e.g., gajendra-7b's 12.5% drop on India questions), but the paper does not systematically test whether this reflects catastrophic forgetting, lack of culturally relevant data, or other factors. The mechanism by which pretraining dominates over fine-tuning remains correlative rather than mechanistically demonstrated.

- The user study (115 participants) provides qualitative validation but may not be statistically powered to detect smaller differences between model groups, particularly given the four-condition design and potential cultural subgroup effects.

## Confidence

- **High Confidence**: Indic models do not outperform global models on cultural alignment metrics (supported by consistent patterns across all four benchmark tasks and the user study)
- **Medium Confidence**: Cultural alignment and linguistic fluency are dissociable capabilities (well-supported by theoretical framework and empirical dissociation, but requires more controlled experiments to rule out confounds)
- **Medium Confidence**: Pretraining corpora's Western dominance fundamentally limits post-training cultural specialization (plausible mechanism supported by degradation patterns, but lacks direct evidence about pretraining data composition)
- **Low Confidence**: Prompting strategies cannot meaningfully shift underlying cultural representations (evidence shows surface-level effects but cannot rule out deeper representational changes with better prompting or different model architectures)

## Next Checks

1. **Pretraining Data Audit**: Analyze the actual training corpora composition for the evaluated models to verify the Western-centricity claim and quantify the proportion of culturally diverse content. This would directly test whether pretraining limitations explain fine-tuning failures.

2. **Cross-Cultural Replication**: Repeat the four-task evaluation pipeline using models fine-tuned for other non-Western cultures (e.g., African, Latin American) to determine whether the dissociation between fluency and alignment is universal or specific to the Indic context.

3. **Subcultural Segmentation Analysis**: Apply the benchmark framework to evaluate models separately on different Indian demographic groups (regional, religious, socioeconomic) to assess whether national-level benchmarks mask important variations and to test the paper's assumption about treating nations as homogeneous units.