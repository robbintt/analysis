---
ver: rpa2
title: 'StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm'
arxiv_id: '2512.16444'
source_url: https://arxiv.org/abs/2512.16444
tags:
- algorithms
- adversary
- scenarios
- learning
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SC2BA, a multi-agent algorithm-vs-algorithm
  environment for benchmarking deep MARL algorithms in adversarial paradigms. Unlike
  existing benchmarks that use built-in AI bots, SC2BA enables dynamic online competition
  between different MARL algorithms, fostering more diverse and robust policy learning.
---

# StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm

## Quick Facts
- **arXiv ID:** 2512.16444
- **Source URL:** https://arxiv.org/abs/2512.16444
- **Reference count:** 40
- **Primary result:** Introduces SC2BA, enabling dynamic online competition between MARL algorithms for more robust policy learning

## Executive Summary
This paper introduces SC2BA, a multi-agent algorithm-vs-algorithm environment that enables dynamic online competition between different MARL algorithms in StarCraft II. Unlike existing benchmarks that use built-in AI bots, SC2BA fosters more diverse and robust policy learning through adversarial paradigms. The authors develop APyMARL, an easy-to-use framework supporting dual-team adversarial training and testing. Extensive experiments reveal that current algorithms struggle with slight troop imbalances and that policy-based methods outperform value-based ones in complex scenarios.

## Method Summary
SC2BA extends the SMAC benchmark by enabling direct competition between different MARL algorithms rather than training against built-in AI. The framework supports two adversary modes: dual-algorithm paired (where both teams continuously adapt to each other) and multi-algorithm mixed (where the opponent is randomly selected from a pool of pre-trained models each episode). The APyMARL framework provides a unified interface for configuring scenarios, running training, and evaluating policies across different adversarial settings.

## Key Results
- Dynamic algorithm-vs-algorithm competition produces more robust policies than training against fixed built-in AI
- Exposure to diverse pre-trained opponent policies during training improves generalization to unseen opponents
- Policy-based methods (DOP, COMA) outperform value-based methods (QMIX, VDN, QTRAN) in complex heterogeneous scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic algorithm-vs-algorithm competition produces more robust policies than training against fixed built-in AI.
- **Mechanism:** In dual-algorithm paired adversary mode, both teams continuously adapt to each other's evolving strategies, creating a non-stationary learning environment that prevents exploitation of static opponent patterns.
- **Core assumption:** Agents trained against fixed AI overfit to those specific behavioral patterns; removing this constraint enables broader policy coverage.
- **Evidence anchors:**
  - [abstract] "SC2BA enables dynamic online competition between different MARL algorithms, fostering more diverse and robust policy learning."
  - [section VI-B] "no fixed strategy can guarantee a consistent victory in the paired adversary mode" with win rates showing persistent fluctuations (Fig. 3).
- **Break condition:** If opponent algorithms converge to identical or cyclical strategies without meaningful diversity, the benefit over built-in AI diminishes.

### Mechanism 2
- **Claim:** Exposure to diverse pre-trained opponent policies during training improves generalization to unseen opponents.
- **Mechanism:** Multi-algorithm mixed adversary mode randomly selects opponent controllers from a pool of pre-trained models each episode, exposing the learning agent to varied tactical behaviors.
- **Core assumption:** The pre-trained model pool contains sufficiently diverse and competent strategies.
- **Evidence anchors:**
  - [section IV-A] "The opponent model is randomly selected from specified MARL algorithms for each episode, making the opponent's actions completely transparent and unpredictable."
  - [section VI-D1, Fig. 9] Models trained in mixed adversary mode generally outperform those trained against built-in AI when tested against built-in AI.
- **Break condition:** If the pre-trained model pool lacks behavioral diversity, the mechanism reduces to single-opponent training.

### Mechanism 3
- **Claim:** Policy-based methods outperform value-based methods in complex heterogeneous scenarios requiring sophisticated coordination.
- **Mechanism:** Complex scenarios involve heterogeneous units with distinct attributes and counter-relationships, which policy-based methods can potentially capture more effectively through direct policy optimization.
- **Core assumption:** The complexity ceiling of value decomposition limits coordination in heterogeneous settings.
- **Evidence anchors:**
  - [section VI-B, Fig. 4] "DOP achieves best" in hard scenarios; policy-based methods "are mostly superior to those value-based methods" in heterogeneous scenarios.
- **Break condition:** Advances in value decomposition could close this performance gap.

## Foundational Learning

- **Concept: Centralized Training with Decentralized Execution (CTDE)**
  - Why needed here: SC2BA follows SMAC conventions where training accesses global states but each agent acts on local observations during execution.
  - Quick check question: Can you explain why agents cannot access global state information at execution time but can during training?

- **Concept: Value Decomposition (VDN, QMIX, QPLEX)**
  - Why needed here: The benchmarked algorithms include multiple value-decomposition methods that factor joint Q-functions into individual agent utilities.
  - Quick check question: Why does QMIX enforce monotonicity in value factorization, and what coordination behaviors might this constraint limit?

- **Concept: Partial Observability in Dec-POMDPs**
  - Why needed here: Agents have sight range of 9 and cannot observe the full battlefield, creating belief-state estimation requirements.
  - Quick check question: How does partial observability differ from full observability in terms of the policy representation required?

## Architecture Onboarding

- **Component map:** Configuration Module -> Interaction Module -> Bottom-Level Control Module -> APyMARL Framework
- **Critical path:**
  1. Install Linux SC2 binary (version 4.6.2.69232) and PySC2
  2. Clone APyMARL from GitHub (https://github.com/dooliu/SC2BA)
  3. Configure scenario via text prompts (units, positions, algorithms)
  4. Select adversary mode (dual-algorithm paired or multi-algorithm mixed)
  5. Run training with specified MARL algorithm for 2M (mixed) or 10M (paired) steps
  6. Evaluate via 32 test episodes without exploration

- **Design tradeoffs:**
  - Paired mode offers dynamic co-evolution but requires longer training (10M steps) and shows unstable win rates
  - Mixed mode provides more stable training (2M steps) but requires pre-trained model pool and freezes opponent policies
  - Asymmetric scenarios dramatically increase difficulty; even top algorithms (DOP) fail against weaker algorithms (COMA) with 1-unit troop disadvantage (5m_vs_6m)

- **Failure signatures:**
  - Win rates plateau below 50% in asymmetric scenarios even with strong algorithms—indicates troop imbalance overwhelms algorithmic advantage
  - Policy-based methods show declining performance with increased training steps in paired mode—potential overfitting to current opponent iteration
  - QTRAN consistently underperforms due to "additional constraint, i.e., affine transformations" (Section VI-B)

- **First 3 experiments:**
  1. Reproduce symmetric 3m scenario with QMIX vs. QMIX in paired mode to validate installation and observe expected fluctuation patterns.
  2. Train DOP in mixed adversary mode on 8m scenario, then test against built-in AI to verify generalization improvement per Fig. 9.
  3. Test QMIX against COMA in 5m_vs_6m asymmetric scenario to confirm the reported troop-imbalance sensitivity (expected win rate <20%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multiple opponent models be jointly optimized online in a dynamic multi-algorithm mixed adversary setting?
- Basis: [explicit] The authors state in Section VI-C and VII that the current mixed adversary mode uses fixed pre-trained models due to optimization complexity, but that "joint multi-model optimization" is a necessary future work.
- Why unresolved: Simultaneously training a population of diverse, evolving opponents creates a high-dimensional, non-stationary optimization problem that current frameworks do not support.
- What evidence would resolve it: A training paradigm where a pool of opponent policies improves online during the mixed adversary process without destabilizing the learner.

### Open Question 2
- Question: Can new algorithms or reward structures be developed to overcome the extreme sensitivity to slight troop imbalances found in current MARL methods?
- Basis: [explicit] Section VI-B highlights that even the best algorithms (e.g., DOP) fail against weaker ones (e.g., COMA) when facing a 1-unit disadvantage, noting that "new strategies, such as rescheduling reward function" are required.
- Why unresolved: Current algorithms rely on equilibrium assumptions that break down in asymmetric scenarios, lacking the robustness to handle resource disadvantages.
- What evidence would resolve it: An algorithm that maintains a positive win rate or competitive return in the 5m_vs_6m asymmetric scenario against an equal or superior learning opponent.

### Open Question 3
- Question: What evaluation metrics beyond win rate are needed to capture the "sensibility" and generalization of algorithms in adversarial paradigms?
- Basis: [inferred] The conclusion suggests that future solutions require us to "rethink some internal rules (e.g., reward function... situation law, etc)" and that current algorithms struggle with "sensibility" in adversarial modes.
- Why unresolved: The paper demonstrates that win rates fluctuate heavily and algorithms fail to generalize across symmetric vs. asymmetric layouts, implying current benchmarks or metrics fail to capture robust policy quality.
- What evidence would resolve it: The adoption of standardized metrics in SC2BA that quantify an agent's robustness to force imbalance or policy diversity, rather than just raw victory statistics.

## Limitations
- Fixed SC2 version (4.6.2.69232) may not represent current multi-agent challenges
- Pre-trained model pool for mixed adversary mode isn't validated for sufficient diversity
- Asymmetric scenarios show extreme sensitivity to troop imbalances that may not reflect realistic strategic differences

## Confidence
- Dynamic algorithm-vs-algorithm competition producing more robust policies: **Medium confidence**
- Policy-based methods inherently outperforming value-based ones in heterogeneous scenarios: **Medium confidence**

## Next Checks
1. Replicate paired mode experiments with additional algorithm pairs (e.g., DOP vs. IQL) to verify that win rate instability isn't algorithm-specific.
2. Conduct ablation studies varying pre-trained model pool size and diversity in mixed mode to quantify the minimum diversity threshold for generalization benefits.
3. Test whether troop imbalance sensitivity persists when both teams use identical algorithms (algorithm skill vs. troop advantage separation).