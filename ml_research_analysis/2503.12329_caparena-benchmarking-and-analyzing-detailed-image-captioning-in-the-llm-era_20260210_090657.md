---
ver: rpa2
title: 'CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM
  Era'
arxiv_id: '2503.12329'
source_url: https://arxiv.org/abs/2503.12329
tags:
- image
- human
- captioning
- detailed
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CapArena, the first large-scale human evaluation
  platform for detailed image captioning in the era of Vision-Language Models (VLMs).
  The study addresses the challenge of benchmarking VLMs on detailed captioning tasks,
  which have been overlooked in favor of other multimodal tasks like VQA and reasoning.
---

# CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era

## Quick Facts
- arXiv ID: 2503.12329
- Source URL: https://arxiv.org/abs/2503.12329
- Reference count: 27
- Top models like GPT-4o achieve or surpass human-level performance in detailed captioning

## Executive Summary
This paper introduces CapArena, the first large-scale human evaluation platform for detailed image captioning in the era of Vision-Language Models (VLMs). The study addresses the challenge of benchmarking VLMs on detailed captioning tasks, which have been overlooked in favor of other multimodal tasks like VQA and reasoning. The authors developed CapArena with over 6000 pairwise caption battles, where human annotators compared captions generated by 14 advanced VLMs and human-written descriptions. The results show that top models like GPT-4o achieve or surpass human-level performance in detailed captioning, while most open-source models lag behind, with InternVL2-26B as a notable exception.

## Method Summary
The study employs a pairwise battle paradigm where human annotators compare two captions for the same image and select the better one based on precision, informativeness, and hallucination criteria. The Bradley-Terry model converts these pairwise comparisons into Elo-style rankings with confidence intervals. For automated evaluation, CapArena-Auto uses GPT-4o as a judge with reference captions, comparing test models against three baselines (GPT-4o, CogVLM-19B, MiniCPM-8B) to reduce bias. The automated benchmark achieves 94.3% correlation with human rankings at a cost of $4 per test.

## Key Results
- Top VLMs (GPT-4o, Gemini, Claude) achieve or surpass human-level performance in detailed captioning
- Most open-source models lag behind, with InternVL2-26B as a notable exception
- VLM-as-a-Judge (GPT-4o with references) achieves 94.3% correlation with human rankings
- Traditional metrics like METEOR show systematic biases favoring specific models

## Why This Works (Mechanism)

### Mechanism 1: Pairwise Battle Paradigm for Human Alignment
- Claim: Converting caption evaluation from absolute scoring to pairwise comparisons increases inter-annotator consistency and enables reliable human preference collection.
- Mechanism: Instead of asking annotators to assign 1-5 scores (which showed low consistency), the system presents two captions side-by-side and asks which is better. This reduces cognitive load by making the task relative rather than absolute. The Bradley-Terry model then converts pairwise win rates into Elo-style rankings.
- Core assumption: Human annotators can more reliably judge relative quality than absolute quality for detailed captions.
- Evidence anchors:
  - [abstract]: "platform with over 6000 pairwise caption battles"
  - [Page 3]: "the task proved to be inherently complex and subjective... annotators found it difficult to assign precise grades, leading to low inter-annotator consistency... we shifted to a pairwise comparison methodology"
  - [corpus]: SPECS paper confirms "N-gram-based metrics though efficient, fail to capture semantic correctness" for long captions
- Break condition: If captions are very similar in quality, annotator agreement drops to ~0.62 (Level 4 samples vs 0.81 for Level 1).

### Mechanism 2: VLM-as-a-Judge with Reference Captions
- Claim: Using a strong VLM (GPT-4o) as an evaluator with human reference captions achieves 94.3% correlation with human rankings at both caption and model levels.
- Mechanism: The judge VLM receives: (1) the image, (2) two candidate captions, and (3) human reference captions. It follows the same guidelines as human annotators (precision, informativeness, hallucination penalties). The reference helps the evaluator disambiguate uncertain image details.
- Core assumption: Assumption: The judge VLM's visual perception and reasoning capabilities transfer to the evaluation task without introducing systematic bias toward specific model outputs.
- Evidence anchors:
  - [Page 7, Table 2]: "GPT-4o (with ref) Yes 0.627... Spearman 0.943 Kendall 0.846"
  - [Page 7]: "Reference descriptions help the evaluator clarify uncertain image details"
  - [corpus]: LOTUS leaderboard also explores VLM-as-judge approaches but emphasizes need for bias-aware assessments
- Break condition: On hard-to-distinguish pairs (Level 3/4), VLM-as-Judge drops to ~0.56 agreement vs human ~0.62, suggesting limitations in fine-grained perception.

### Mechanism 3: Multi-Baseline Strategy Reduces Evaluation Bias
- Claim: Comparing against multiple baselines with varying performance levels reduces single-baseline noise and bias in automated evaluation.
- Mechanism: CapArena-Auto uses three baselines (GPT-4o, CogVLM-19B, MiniCPM-8B) representing high/medium/low performance. Each test model gets 3×600 comparisons. Final score is sum of win/draw/loss points. This prevents a model from appearing superior simply because it matches one baseline's style.
- Core assumption: Assumption: Bias patterns differ across baselines, so averaging across multiple baselines cancels out systematic errors.
- Evidence anchors:
  - [Page 8]: "To reduce potential noise and bias from a single baseline (Lin et al., 2024), we use three baseline models"
  - [Page 5, Figure 5]: Shows that metrics like METEOR and Output Length have systematic biases toward specific models (4.4% vs 8.2% average bias)
  - [corpus]: Limited corpus evidence on multi-baseline strategies specifically for caption evaluation
- Break condition: If all baselines share a common blind spot (e.g., all miss certain object types or spatial relationships), the ensemble will not correct for this.

## Foundational Learning

- Concept: **Bradley-Terry Model for Ranking from Pairwise Comparisons**
  - Why needed here: The core of CapArena is converting thousands of A-vs-B votes into a single leaderboard. You need to understand how BT coefficients translate win rates into Elo-like scores with confidence intervals.
  - Quick check question: If Model A beats Model B 60% of the time and Model B beats Model C 60% of the time, what's the expected win rate for A vs C? (Answer: ~(0.6×0.6)/(0.4×0.4) ≈ 2.25 odds → ~69%)

- Concept: **Caption-Level vs Model-Level Agreement**
  - Why needed here: A metric can get 58% of individual battles right (caption-level) but produce a completely wrong leaderboard (model-level) due to systematic bias. The paper distinguishes these explicitly in Table 2.
  - Quick check question: If Metric X always favors Caption A over Caption B when Model 1 fights Model 2, what happens to model-level agreement even if individual caption predictions are ~50% accurate?

- Concept: **Systematic Bias Detection in Evaluation Metrics**
  - Why needed here: METEOR favors certain models regardless of actual quality. You need to detect and measure bias by comparing metric-derived win rates against ground-truth win rates across all opponents.
  - Quick check question: How would you detect if a metric systematically favors longer captions? (Answer: Compute correlation between caption length and metric score across all captions, or check win-rate bias against models with different average lengths)

## Architecture Onboarding

- Component map:
  ```
  CapArena (Human Evaluation Platform)
  ├── Data Source: DOCCI dataset (high-res images + human captions)
  ├── Models Under Test: 14 VLMs (GPT-4o, Gemini, Claude, LLaVA, Qwen, etc.)
  ├── Annotation Interface: Pairwise battles with precision/informativeness/hallucination guidelines
  ├── Ranking Engine: Bradley-Terry → Elo scores with bootstrapped confidence intervals
  └── Quality Control: Inter-annotator agreement (0.782), time filtering (>5s), expert review

  CapArena-Auto (Automated Benchmark)
  ├── Test Set: 600 images from DOCCI (clustered sampling from 149 clusters + CLIP filtering)
  ├── Baselines: GPT-4o, CogVLM-19B, MiniCPM-8B (3 performance levels)
  ├── Judge: GPT-4o with reference captions + structured prompt
  ├── Scoring: Sum of (+1/0/-1) across 600 × 3 baseline comparisons
  └── Validation: Spearman/Kendall correlation vs human rankings (target: >0.90)
  ```

- Critical path:
  1. Image + two captions → Judge VLM (with guidelines prompt) → Win/Loss/Tie decision
  2. Aggregate decisions across all battles → Bradley-Terry model → Elo ranking with confidence intervals
  3. For CapArena-Auto: Compare test model against 3 baselines → Sum scores across all battles → Final leaderboard position

- Design tradeoffs:
  - **Cost vs Accuracy**: CapArena (human) requires ~142 seconds per annotation and substantial labor; CapArena-Auto costs $4 per test but achieves 94.3% correlation
  - **Caption-level vs Model-level optimization**: Some metrics (METEOR) optimize for per-sample agreement but have systematic bias; VLM-as-Judge optimizes both
  - **Single vs Multi-baseline**: Single baseline is cheaper but noisier; three baselines add 3× compute but reduce bias

- Failure signatures:
  - Low inter-annotator agreement on Level 4 battles (0.62 vs 0.81 for Level 1)
  - Traditional metrics (BLEU, CIDEr, CLIPScore) showing negative or near-zero model-level agreement despite decent caption-level scores
  - VLMs failing on: unusual scenes, subtle details, clock time recognition, knowledge association tasks
  - Caption length bias: models rewarded purely for longer outputs without quality improvement

- First 3 experiments:
  1. **Validate VLM-as-Judge on a held-out subset**: Take 200 CapArena battles, run GPT-4o-as-judge with/without references, measure caption-level and model-level agreement vs human ground truth.
  2. **Test single-baseline vs three-baseline**: Run CapArena-Auto using only GPT-4o as baseline, then all three, compare resulting rankings against full CapArena leaderboard using Spearman correlation.
  3. **Failure mode analysis on Level 4 battles**: Manually examine 50 battles where GPT-4o-as-judge disagrees with humans. Categorize errors (missed details, hallucination, spatial reasoning failures) to identify systematic blind spots for future improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do VLMs perform on detailed image captioning in specialized domains such as medical imaging or artwork compared to the everyday life scenarios currently benchmarked?
- Basis in paper: [explicit] The "Limitations" section states that the images used in CapArena mostly focus on everyday life scenarios, noting that "other domains, such as artwork or medical images, are not yet represented."
- Why unresolved: Current evaluation datasets (like DOCCI) and the resulting benchmarks are curated from natural, everyday photographs, leaving a gap in understanding model performance on domain-specific visual nuances.
- What evidence would resolve it: An extension of the CapArena benchmark including pairwise battles on artwork and medical imagery, resulting in domain-specific ELO rankings.

### Open Question 2
- Question: How can the gap between VLM-as-a-Judge and human annotator agreement be closed for "hard-to-distinguish" caption pairs (Level 3 and 4)?
- Basis in paper: [inferred] Section 4.3 notes that while VLM-as-a-Judge is robust, "it still falls short of Inter-Annotator agreement, particularly on Level 3/4 samples," indicating a limit in the model's fine-grained perceptual discrimination.
- Why unresolved: Current VLM evaluators struggle to discern subtle differences in precision and informativeness when two captions are of similar high quality.
- What evidence would resolve it: A new evaluation method or model capable of achieving an agreement rate on Level 4 samples that is statistically indistinguishable from the human inter-annotator agreement of 0.62.

### Open Question 3
- Question: Can VLM-as-a-Judge achieve peak performance without relying on reference captions, thereby enabling truly reference-free evaluation?
- Basis in paper: [inferred] Table 2 shows that GPT-4o with reference captions achieves a model-level Spearman correlation of 0.943, outperforming the reference-free version (0.930). The text notes references "help the evaluator clarify uncertain image details."
- Why unresolved: While VLM judges are capable, they still rely on ground-truth text to resolve uncertainties in complex images, limiting their autonomy compared to metrics like CLIPScore.
- What evidence would resolve it: A reference-free VLM-judge configuration that matches or exceeds the 0.943 Spearman correlation achieved by the reference-enhanced setup on the CapArena-Auto benchmark.

### Open Question 4
- Question: What architectural or training improvements are required to resolve specific fine-grained perception failures, such as the inability to identify clock times or describe unusual scenes?
- Basis in paper: [inferred] Section 3.3 ("Failure Cases") and Table 7 highlight that models (including GPT-4o) consistently fail to accurately identify the time on clocks and often misinterpret unusual scenes (e.g., tow truck scenarios).
- Why unresolved: Despite high overall ELO scores, VLMs exhibit "blind spots" in specific cognitive tasks like analog clock reading, suggesting a lack of specialized training data or architectural constraints.
- What evidence would resolve it: A subsequent model iteration that achieves near-perfect accuracy on a targeted "failure case" benchmark (e.g., Clock Reading or Unusual Scene Description) without regressing on general captioning quality.

## Limitations
- The pairwise battle paradigm may not fully capture absolute caption quality differences
- Traditional metrics like METEOR show systematic biases favoring specific models
- The automated benchmark may inherit biases from the judge VLM

## Confidence

**Confidence: Medium** The study establishes that VLMs achieve human-level performance in detailed captioning, but this finding is based on pairwise comparisons rather than absolute quality assessments. The Bradley-Terry model provides robust rankings, yet the absolute performance gap between top models and humans remains unclear.

**Confidence: High** The multi-baseline strategy effectively reduces evaluation bias, as demonstrated by the 94.3% correlation with human rankings. However, the assumption that baseline biases cancel out may not hold if all baselines share systematic blind spots.

**Confidence: Low** The VLM-as-Judge approach shows strong performance but exhibits limitations on hard-to-distinguish cases (Level 3/4 battles), with agreement dropping to ~0.56 vs human ~0.62. This suggests potential systematic biases in fine-grained perception tasks.

## Next Checks

1. **Hold-out validation**: Test VLM-as-Judge on a separate held-out subset of 200 CapArena battles to verify consistency of 94.3% correlation
2. **Cross-baseline analysis**: Compare CapArena-Auto rankings using single baseline vs three baselines to quantify bias reduction benefits
3. **Failure mode characterization**: Analyze 50 Level 4 battles where VLM-as-Judge disagrees with humans to identify systematic blind spots in current VLMs