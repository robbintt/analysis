---
ver: rpa2
title: Filtering Learning Histories Enhances In-Context Reinforcement Learning
arxiv_id: '2505.15143'
source_url: https://arxiv.org/abs/2505.15143
tags:
- learning
- dicp
- ours
- icrl
- histories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of suboptimal behavior inherited
  from source reinforcement learning algorithms in in-context reinforcement learning
  (ICRL) frameworks. Current ICRL methods imitate complete learning histories from
  source algorithms, which can transfer suboptimal behaviors.
---

# Filtering Learning Histories Enhances In-Context Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.15143
- Source URL: https://arxiv.org/abs/2505.15143
- Reference count: 40
- In-context RL performance improves by 8.8-11.9% on average using Learning History Filtering

## Executive Summary
This paper addresses the problem of suboptimal behavior transfer in in-context reinforcement learning (ICRL) by proposing a preprocessing method called Learning History Filtering (LHF). The approach filters and reweights learning histories based on their improvement and stability characteristics before training transformer-based ICRL agents. LHF is shown to be compatible with state-of-the-art ICRL algorithms including Algorithm Distillation, DPT, and DICP, achieving consistent performance improvements across multiple benchmark environments while being particularly effective in noisy data regimes.

## Method Summary
LHF preprocesses the pretraining dataset by computing two metrics for each learning history: improvement (measured by mean and range of episodic returns) and stability (measured by performance degradation). These are combined into a unified score, and histories are sampled with probability proportional to this score, creating a filtered dataset that emphasizes high-quality learning trajectories. This approach effectively emulates Weighted Empirical Risk Minimization by up-weighting histories that demonstrate both high achievement and consistent progress during the transformer's supervised pretraining phase.

## Key Results
- LHF achieves average relative performance improvements of 8.8% (AD), 9.1% (DICP), and 11.9% (DPT) over baselines
- Performance gains become more pronounced with noisy datasets, showing average improvements of 27.8% (AD), 12.5% (DICP), and 12.4% (DPT)
- LHF maintains robustness across various suboptimal scenarios including partial learning histories, lightweight models, and different hyperparameter settings
- The method is compatible with multiple SOTA ICRL algorithms and benchmark environments including Darkroom-type problems and Meta-World-ML1 robotic manipulation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset preprocessing via weighted sampling improves ICRL by emulating Weighted Empirical Risk Minimization (WERM).
- Mechanism: LHF assigns a unified score to each learning history based on its improvement and stability. It then constructs a filtered dataset by sampling each history with probability proportional to this score, effectively up-weighting high-quality learning trajectories and down-weighting suboptimal ones during the transformer's supervised pretraining.
- Core assumption: The defined unified metric (combining improvement and stability) is a reliable proxy for the utility of a learning history for teaching an ICRL agent.
- Evidence anchors:
  - [abstract] "...reweighting and filtering the learning histories based on their improvement and stability characteristics."
  - [section] Algorithm 1 and Equations (6)-(9) detail the metric and sampling.
  - [corpus] Neighbor "Yes, Q-learning Helps Offline In-Context RL" supports the general premise that modifying the training objective/data improves ICRL, though it uses a different method.
- Break condition: If the metric is uncorrelated with the true pedagogical value of a trajectory for ICRL, the mechanism fails.

### Mechanism 2
- Claim: Balancing improvement and stability in the sampling metric is critical for robust performance.
- Mechanism: The unified metric $U = \text{Improvement} + \lambda \cdot \text{Stability}$ combines two factors. Improvement favors histories that achieve high returns, while stability favors those with low performance degradation, preventing the model from imitating erratic or unstable learning processes.
- Core assumption: A good ICRL model should learn from histories that demonstrate both high achievement and consistent progress.
- Evidence anchors:
  - [section] Figure 5(a) and 5(b) show that intermediate $\lambda$ values (balancing both) outperform extremes where one factor dominates.
  - [corpus] Weak corpus evidence; this specific formulation is an empirical contribution of this work.
- Break condition: If ICRL only required imitating high-reward behaviors (regardless of consistency), stability would not be a necessary component.

### Mechanism 3
- Claim: LHF's performance gains are amplified in noisy or suboptimal data regimes.
- Mechanism: When source datasets contain low-quality learning runs (e.g., random agents), the unfiltered baseline must learn from this noise. LHF selectively filters these out, making the contrast between filtered and unfiltered performance more dramatic.
- Core assumption: Standard ICRL algorithms are not inherently robust to high levels of suboptimal data in the pretraining corpus.
- Evidence anchors:
  - [abstract] "Notably, the superior performance of LHF becomes more pronounced in the presence of noisy data..."
  - [section] Table 2 shows higher average relative enhancements (e.g., 27.8% for AD) compared to Table 1 (8.8% for AD) on clean data.
  - [corpus] No direct corpus evidence for this specific noise-robustness claim.
- Break condition: If a baseline ICRL method was already robust to suboptimal data, LHF's incremental benefit would diminish.

## Foundational Learning

- **Concept: In-Context Reinforcement Learning (ICRL)**
  - Why needed here: This is the core problem domain. You must understand that the goal is to train a transformer to act as a reinforcement learning agent at inference time, without weight updates, using only a context window of past interactions.
  - Quick check question: Can you explain why ICRL frames the RL problem as a sequence modeling task?

- **Concept: Algorithm Distillation (AD) & Backbone ICRL Algorithms**
  - Why needed here: LHF is a pre-processing step that plugs into these existing methods (AD, DPT, DICP). Understanding that they are trained via supervised learning on learning histories is crucial.
  - Quick check question: What is the supervised learning objective in Algorithm Distillation, and how does LHF modify the data distribution used for it?

- **Concept: Weighted Empirical Risk Minimization (WERM)**
  - Why needed here: The paper explicitly motivates LHF as emulating WERM. Understanding this provides the theoretical basis for why re-weighting data samples can lead to better generalization.
  - Quick check question: How does LHF use the idea of a weighting function $w(\cdot)$ to convert the standard ERM objective into a form of WERM?

## Architecture Onboarding

- **Component map:**
  1. Source RL Agent (e.g., PPO): Interacts with environments to generate the raw dataset of learning histories.
  2. Learning History Filter (LHF): Pre-processes the raw dataset. Computes `Improvement` and `Stability` for each history, then a unified score $U$, and samples histories based on $P(U)$ to create a refined dataset.
  3. Transformer Model (e.g., TinyLlama): The core ICRL agent. It is trained via supervised learning on the LHF-filtered dataset.
  4. ICRL Backbones (AD/DPT/DICP): These define the specific training objective (e.g., predicting the next action from history) used to train the Transformer.

- **Critical path:** Raw Dataset -> LHF Preprocessing (Compute Metrics -> Sample Histories) -> Filtered Dataset -> Transformer Pretraining (using AD/DPT/DICP objective) -> Deployed ICRL Agent.

- **Design tradeoffs:**
  - Metric Simplicity vs. Expressiveness: The current LHF metric is heuristic-based (mean return, return range, degradation). A more complex learned metric could be more powerful but less interpretable and harder to implement.
  - Sampling Strategy: The paper uses a simple linear scaling of the probability with the unified score. More aggressive sampling (e.g., Softmax with low temperature) could discard more data, potentially losing diversity.

- **Failure signatures:**
  - Metric Mismatch: If `Improvement` and `Stability` do not correlate with what makes a history useful for ICRL, performance will degrade.
  - Data Scarcity: Over-aggressive filtering could reduce the effective dataset size too much, harming the model's ability to generalize. The paper maintains dataset size by resampling.

- **First 3 experiments:**
  1. Reproduction of Baseline vs. LHF: On a simple environment (e.g., Darkroom), compare the learning curves (return vs. test steps) of a standard AD agent against an AD agent trained on an LHF-filtered dataset. Verify the reported ~25% improvement.
  2. Ablation on Metric Components: Run LHF with $\lambda=0$ (only Improvement) and a very high $\lambda$ (only Stability). Compare performance to the balanced setting ($\lambda=1$) to validate the mechanism's dependence on both factors.
  3. Noise Injection Robustness: Create a noisy dataset by mixing histories from a trained PPO agent with those from a random agent. Compare the performance degradation of the baseline and LHF models as the noise ratio increases. Verify that LHF's relative advantage grows.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the filtering mechanism in ICRL be theoretically characterized with respect to generalization error?
- Basis in paper: [explicit] The authors state in Section 6, "It remains an open and interesting direction in the future to theoretically characterize the filtering mechanism in the specific context of ICRL with respect to e.g., generalization error."
- Why unresolved: While the paper provides empirical validation and intuitive motivation based on Weighted Empirical Risk Minimization (WERM), it lacks formal theoretical bounds or convergence proofs explaining why filtering improves generalization in the ICRL setting.
- What evidence would resolve it: A theoretical framework deriving generalization bounds for ICRL that explicitly accounts for the distribution shift introduced by the LHF filtering probability.

### Open Question 2
- Question: How can the Learning History Filtering (LHF) approach be adapted for in-context safe reinforcement learning?
- Basis in paper: [explicit] The authors note in Section 6, "Future work could incorporate an extra cost function, analogous to reward, to enable the in-context safe RL."
- Why unresolved: The current unified metric (Eq. 8) relies solely on episodic returns and stability of returns, which do not account for safety constraints or costs required in safety-critical applications.
- What evidence would resolve it: A modified LHF metric that incorporates cost constraints (e.g., filtering histories based on constraint satisfaction rates) and successful deployment of this modified algorithm on standard safe RL benchmarks.

### Open Question 3
- Question: Are the specific statistical definitions of "improvement" (mean + range) and "stability" (performance degradation) optimal for all environment types?
- Basis in paper: [inferred] The paper defines specific metrics in Equations 6 and 7 (e.g., using the range of returns for improvement) to score histories. While effective in the tested benchmarks, the paper does not justify these specific statistical forms over other potential proxies for trajectory quality.
- Why unresolved: The chosen metrics are heuristics that might overlook nuanced dynamics in complex environments (e.g., early exploration quality vs. final return), potentially filtering out useful learning signals.
- What evidence would resolve it: An ablation study comparing various metric definitions (e.g., using variance instead of range, or area-under-curve instead of mean) across a wider variety of environments with different reward structures.

### Open Question 4
- Question: Does scaling LHF to high-dimensional visual observation spaces (e.g., pixels) retain the same performance benefits observed in state-based environments?
- Basis in paper: [inferred] The experiments are limited to Darkroom (discrete grid) and Meta-World (low-dimensional continuous states). The extent to which filtering histories based on simple return statistics generalizes to complex, high-dimensional visual domains is not discussed.
- Why unresolved: In visual domains, the relationship between the "improvement" metric and the actual information content of the history might be noisier or less correlated, potentially reducing the efficacy of the current filtering strategy.
- What evidence would resolve it: Experimental results applying LHF to ICRL benchmarks involving pixel-based inputs (e.g., Atari or DMControl) to verify if the relative enhancements hold constant.

## Limitations
- The paper's claims hinge on the effectiveness of the LHF metric, which is empirically defined rather than theoretically grounded
- The computational overhead of computing improvement and stability metrics for each history is not discussed
- The paper does not explore the sensitivity of results to the specific choice of Î» beyond a narrow range

## Confidence
- **High Confidence**: The core experimental results demonstrating LHF's effectiveness across multiple benchmark environments and ICRL backbones (AD, DPT, DICP)
- **Medium Confidence**: The claim that LHF is particularly beneficial for noisy datasets
- **Medium Confidence**: The mechanism by which LHF emulates Weighted Empirical Risk Minimization

## Next Checks
1. Domain Generalization Test: Apply LHF to a significantly different RL domain (e.g., Atari games or a sparse-reward control task) to test if the improvement and stability metrics remain effective proxies for learning history utility
2. Metric Ablation with Learned Components: Replace the heuristic-based improvement and stability metrics with a small learned component (e.g., a linear layer predicting a history's utility) to test if the metric can be made more expressive without losing interpretability
3. Computational Overhead Analysis: Measure and report the wall-clock time required to compute the LHF metrics for a large dataset (e.g., 1 million transitions) and compare it to the pretraining time of the transformer model to quantify the practical cost of the preprocessing step