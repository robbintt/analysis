---
ver: rpa2
title: 'Memory Is All You Need: Testing How Model Memory Affects LLM Performance in
  Annotation Tasks'
arxiv_id: '2503.04874'
source_url: https://arxiv.org/abs/2503.04874
tags:
- memory
- reinforcement
- prompting
- performance
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether allowing a Large Language Model
  (LLM) to retain knowledge about its own previous annotations in the same task improves
  annotation performance compared to traditional approaches that treat each annotation
  as independent. The authors propose two novel methods: memory prompting, where the
  model keeps a conversation history of past classifications; and memory reinforcement,
  which adds a reinforcement learning component by informing the model whether its
  past classifications were correct.'
---

# Memory Is All You Need: Testing How Model Memory Affects LLM Performance in Annotation Tasks

## Quick Facts
- **arXiv ID:** 2503.04874
- **Source URL:** https://arxiv.org/abs/2503.04874
- **Reference count:** 12
- **Primary result:** Memory-based approaches (prompting and reinforcement) significantly improve LLM annotation performance compared to standard zero-shot/few-shot methods

## Executive Summary
This paper investigates whether allowing Large Language Models to retain knowledge about their own previous annotations improves classification performance compared to treating each annotation independently. The authors propose two novel memory-based methods: memory prompting (retaining conversation history of past classifications) and memory reinforcement (adding a reinforcement learning component with correctness feedback). Testing on political science datasets (nostalgia detection and incivility classification) using GPT-4o and Llama 3.1, they find both approaches significantly outperform traditional methods, with memory prompting improving F1 scores by 5-25% and memory reinforcement showing even larger gains (9-25%) in three out of four test scenarios.

## Method Summary
The study compares four prompting strategies across two political science datasets: Zero-shot (independent API calls), Few-shot CoT (10 examples with chain-of-thought reasoning), Memory Prompting (sequential calls with conversation history of past classifications), and Memory Reinforcement (sequential calls on 20% training set with correctness feedback, then tested on remaining 80%). Both GPT-4o and Llama 3.1-70B were evaluated with temperature set to 0.7. The conversation history was capped at 200 classifications to manage API costs and rate limits. Performance was measured using F1 score averaged over 10 runs.

## Key Results
- Memory prompting improves F1 scores by 5-25% compared to standard no-memory approaches
- Memory reinforcement shows additional performance gains (9-25% improvement) in three out of four test scenarios
- Both methods primarily improve recall on minority classes, reducing false negatives compared to high-precision/low-recall profiles of standard approaches
- Llama 3.1 was found to be faster, cheaper, and less prone to API rate limit errors while performing competitively to GPT-4o

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning via Conversation History Accumulation
Providing the model with a history of its own previous classifications creates a task-specific context that improves subsequent annotation accuracy, even without explicit correctness feedback. Each annotation is appended to the conversation history, and when processing new instances, the attention mechanism identifies latent patterns, self-consistency cues, and label distributions from its own outputs. This functions as a dynamic, self-generated few-shot prompt that adapts to the specific nuances of the dataset being classified.

### Mechanism 2: Reinforcement-Driven Decision Boundary Calibration
Explicitly rewarding correct predictions and punishing incorrect ones within the conversation history provides a stronger supervisory signal than examples alone, further calibrating the model's decision boundary. This approach mimics a simplified reinforcement learning loop where the "reward" message reinforces successful reasoning paths, while the "punish" message (paired with the correct label) forces the model to adjust its reasoning for similar instances.

### Mechanism 3: Mitigation of Label Imbalance via Recall Enhancement
Memory-based methods improve performance primarily by increasing recall on minority classes, leading to more balanced predictions compared to the high-precision/low-recall profile of standard no-memory prompting. Standard approaches often exhibit a bias toward the majority class, but retaining history allows the model to implicitly observe its own labeling distribution over time, providing a counter-balancing signal that encourages minority-class predictions.

## Foundational Learning

- **Concept: In-Context Learning & Attention**
  - **Why needed here:** The entire mechanism of "memory prompting" relies on the model attending to its own previous outputs stored in the context window.
  - **Quick check question:** If you double the size of the conversation history, will the model definitely attend to the earliest examples with the same weight?

- **Concept: F1 Score vs. Accuracy in Imbalanced Datasets**
  - **Why needed here:** The authors report F1 scores as their primary metric because accuracy is misleading in imbalanced datasets.
  - **Quick check question:** A model has 95% accuracy on a dataset with 95% "Class A" and 5% "Class B". What does an F1 score improvement tell you that the 95% accuracy does not?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** The "memory reinforcement" technique is conceptually inspired by RLHF.
  - **Quick check question:** Does "memory reinforcement" update the model's weights? If not, how does the model "learn" from the feedback?

## Architecture Onboarding

- **Component map:** Annotator Core -> State Manager -> Feedback Engine (reinforcement only) -> Persistence Layer
- **Critical path:**
  1. Initialize: Create system prompt, empty conversation_history
  2. (Reinforcement Only) Training Phase: For each training item: send history + item to LLM, get prediction, feedback engine compares & generates feedback, append {item, prediction, feedback} to history
  3. Test Phase: For each test item: send history + item to LLM, get prediction, append {item, prediction} to history

- **Design tradeoffs:**
  - Memory Prompting vs. Reinforcement: Reinforcement offers higher performance (9-25% F1 gain) but requires labeled training data (20% of data); Memory Prompting offers solid gains (5-25%) with no labeled data
  - Context Window Length vs. Cost/Performance: Longer history provides more context but increases API token costs and latency
  - Model Choice: Llama 3.1 was faster, cheaper, and less prone to API rate limit errors while performing competitively

- **Failure signatures:**
  - Error Propagation: Early incorrect predictions may reinforce errors in memory prompting
  - Context Window Overflow: History will eventually exceed model's context limit, causing errors
  - Rate Limiting: Sustained high-volume requests can hit token-per-minute limits, especially for GPT-4o

- **First 3 experiments:**
  1. Baseline Replication: Replicate "Zero-Shot No-Memory" vs. "Memory Prompting" comparison on a small balanced subset of your own data
  2. History Length Ablation: Vary history length (10, 50, 100, 200) and plot performance vs. token cost
  3. Reinforcement Feasibility Test: If you have labeled data, set aside 20% as training set and compare "Memory Reinforcement" against standard "Few-Shot CoT" using the same data

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the performance advantage of memory reinforcement over standard prompting increase monotonically with the subjective complexity or ambiguity of the classification task?
- **Open Question 2:** Does providing a model with a history of its own annotations reduce its sensitivity to the specific phrasing and architecture of the system prompt?
- **Open Question 3:** What is the optimal size for the conversation history (memory window) that balances classification accuracy against computational cost and context window limits?

## Limitations

- **Dataset Generality:** Findings validated on two specific political science datasets; generalization to other domains remains uncertain
- **Model Architecture Dependency:** Mechanism relies on attention architecture that may not work identically across different model types
- **Temporal Stability:** Paper does not address whether performance improvements persist across very long sequences or if behavior changes as history grows

## Confidence

- **High Confidence:** Core empirical finding that memory prompting improves F1 scores by 5-25% is well-supported
- **Medium Confidence:** Mechanism explanation (attention attending to self-generated history) is plausible but lacks direct mechanistic evidence
- **Medium Confidence:** Claim that memory reinforcement yields additional gains (9-25%) is supported, but specific contribution of reinforcement signal is not isolated

## Next Checks

1. **Cross-Domain Validation:** Apply memory prompting to a dataset from a completely different domain (e.g., medical text classification) and measure whether the 5-25% F1 improvement generalizes

2. **Attention Pattern Analysis:** Use attention visualization tools to directly observe whether the model's attention heads are actually attending to the conversation history in meaningful ways

3. **Diminishing Returns Test:** Systematically vary conversation history length (10, 50, 100, 200, 500 previous examples) and plot performance gains against token costs to identify optimal history length