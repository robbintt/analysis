---
ver: rpa2
title: Detecting and Understanding Hateful Contents in Memes Through Captioning and
  Visual Question-Answering
arxiv_id: '2504.16723'
source_url: https://arxiv.org/abs/2504.16723
tags:
- hateful
- memes
- content
- multimodal
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal framework for detecting hateful
  content in memes by integrating OCR, captioning, retrieval-augmented generation,
  and visual question answering. The system processes both textual and visual components
  to uncover subtle and implicit hate signals that evade traditional unimodal detectors.
---

# Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering

## Quick Facts
- arXiv ID: 2504.16723
- Source URL: https://arxiv.org/abs/2504.16723
- Reference count: 35
- Primary result: RAG (sub_label + VQA) achieves 73.50% accuracy and 78.35% AUC-ROC on Facebook Hateful Memes dataset

## Executive Summary
This paper presents a multimodal framework for detecting hateful content in memes by integrating OCR, captioning, retrieval-augmented generation, and visual question answering. The system processes both textual and visual components to uncover subtle and implicit hate signals that evade traditional unimodal detectors. A key innovation is the use of sub-label classification (race, religion, etc.) combined with iterative VQA-based reasoning to improve retrieval precision and detection accuracy. Evaluated on the Facebook Hateful Memes dataset, the proposed RAG (sub_label + VQA) method achieves 73.50% accuracy and 78.35% AUC-ROC, significantly outperforming unimodal and simpler multimodal baselines while narrowing the gap to human-level performance.

## Method Summary
The framework processes memes through a modular pipeline: PaddleOCR extracts overlaid text, an LLM generates neutral visual descriptions, and GPT-4 generates context-sensitive questions targeting potential hate cues. GPT-3.5 answers by integrating visual and textual cues, with follow-up questions probing specific components if hate elements are detected. All Q&A pairs are stored in a contextual database for continuity, then fed to the RAG pipeline which uses sub-label partitioning for retrieval. The final classification fuses OCR text, caption, VQA outputs, and retrieved exemplars to produce binary hate classification with explanations.

## Key Results
- RAG (sub_label + VQA) achieves 73.50% accuracy and 78.35% AUC-ROC, outperforming RAG (explanation) at 59.20% accuracy
- Sub-label classification with RAG improves accuracy to 72.00% and AUROC to 76.52% compared to simpler approaches
- VQA multi-turn dialogue uncovers implicit hate signals that single-pass analysis misses
- Explicit OCR combined with neutral captioning provides more robust multimodal representation than end-to-end vision-language models alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained sub-label classification improves retrieval precision for hateful content detection.
- Mechanism: Rather than treating hatefulness as a binary label, the system partitions content into sub-categories (race, religion, gender, others). During RAG retrieval, embeddings are matched against the relevant sub-label partition, reducing noise and improving contextual alignment between query memes and retrieved exemplars.
- Core assumption: Hate speech exhibits identifiable thematic patterns that can be pre-categorized, and memes within the same sub-label share detectable structural or semantic features.
- Evidence anchors:
  - [abstract] "A key innovation is the use of sub-label classification (race, religion, etc.)... to improve retrieval precision and detection accuracy."
  - [section 4.3] "incorporating fine-grained sub-labels in RAG leads to a substantial boost with accuracy of 72.00% and AUROC of 76.52%" vs RAG (explanation) at 59.20% accuracy.
  - [corpus] TRACE paper (arXiv:2504.17902) similarly leverages contextual augmentation for multimodal hate detection, suggesting this is an active research direction.
- Break condition: If hate speech categories overlap significantly or memes contain multi-target hate (e.g., both race and religion), sub-label partitioning may introduce retrieval ambiguity.

### Mechanism 2
- Claim: Multi-turn VQA dialogue uncovers implicit hate signals that single-pass analysis misses.
- Mechanism: GPT-4 generates context-sensitive questions targeting potential hate cues (stereotypes, offensive language, symbols). GPT-3.5 answers by integrating visual and textual cues. If hate elements are detected, follow-up questions probe specific components. All Q&A pairs are stored in a contextual database for continuity, then fed to the RAG pipeline.
- Core assumption: Iterative questioning reveals latent meaning that cannot be extracted from static feature fusion; LLMs can reliably identify hate-related patterns when prompted appropriately.
- Evidence anchors:
  - [abstract] "VQA for iterative analysis of symbolic and contextual cues. This enables the framework to uncover latent signals that simpler pipelines fail to detect."
  - [section 3.2] "If hate-related elements are identified... the system refines its analysis by generating follow-up questions through GPT-4.0, specifically targeting the hateful components."
  - [section 4.3] RAG (sub_label + VQA) achieves 73.50% accuracy, a 1.5 percentage point gain over RAG (sub_label) alone.
  - [corpus] Demystifying Hateful Content paper (arXiv:2502.11073) also leverages large multimodal models for explainable hate detection, supporting the utility of advanced VQA-style reasoning.
- Break condition: If VQA generates irrelevant questions or hallucinates hate signals where none exist, false positives increase. The paper does not report false positive rates explicitly.

### Mechanism 3
- Claim: Explicit OCR combined with neutral captioning provides a more robust multimodal representation than end-to-end vision-language models alone.
- Mechanism: PaddleOCR extracts overlaid text, handling stylized or distorted fonts. A separate LLM generates neutral visual descriptions. These are combined as input to downstream modules, ensuring text extraction is controllable and interpretable before reasoning.
- Core assumption: Meme-specific typography (distorted, stylized fonts) challenges even state-of-the-art vision-language models; explicit OCR provides consistent extraction.
- Evidence anchors:
  - [section 3.1] "Paddle OCR [9], an optical character recognition system, is used to extract textual messages in memes."
  - [section 4.4] "incorporating explicit OCR (PaddleOCR) remains valuable, especially when dealing with stylized, distorted, or meme-specific fonts that challenge even state-of-the-art vision-language models."
  - [corpus] No direct corpus comparison on OCR vs. end-to-end VLM text extraction was found; this remains an assumption requiring further validation.
- Break condition: If OCR fails on heavily obscured or artistic text, downstream modules receive incomplete input. The paper does not quantify OCR failure rates.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Core to the detection pipeline—embeds meme content and retrieves relevant labeled examples from a vector database to guide classification.
  - Quick check question: Can you explain how a vector database differs from a traditional keyword search index?

- Concept: Visual Question Answering (VQA)
  - Why needed here: Enables iterative reasoning about meme content through generated questions and answers, probing ambiguous or symbolic cues.
  - Quick check question: What is the difference between single-turn and multi-turn VQA, and why might multi-turn be useful for detecting implicit hate?

- Concept: Multimodal Fusion
  - Why needed here: Memes require simultaneous analysis of text and image; understanding how modalities are combined (early, late, or cross-attention fusion) informs why this framework uses a modular pipeline.
  - Quick check question: Name two common strategies for combining text and image features in multimodal models.

## Architecture Onboarding

- Component map: Raw meme image → PaddleOCR (text extraction) + Captioning LLM (visual description) → GPT-4 (question generation) → GPT-3.5 (answer generation) → Contextual database (multi-turn Q&A storage) → RAG Pipeline (embedding model → Vector database → Retrieval + ranking) → Fusion of OCR text, caption, VQA outputs, and retrieved exemplars → Binary classification (hateful/non-hateful) + explanation

- Critical path:
  1. OCR and captioning must complete before VQA and RAG can proceed.
  2. VQA multi-turn dialogue generates structured Q&A; this must finish before RAG retrieval.
  3. RAG retrieval uses sub-label partitioning; correct sub-label assignment is essential for precision.
  4. Final classification aggregates all signals—errors in earlier stages propagate.

- Design tradeoffs:
  - Accuracy vs. latency: Multi-turn VQA and RAG improve detection but introduce computational overhead; real-time deployment may require query reduction or caching.
  - Modularity vs. end-to-end optimization: Explicit OCR and separate captioning improve interpretability but may miss joint text-image patterns learned by end-to-end VLMs.
  - Sub-label granularity: Finer partitions improve precision but risk sparse retrieval if categories are underrepresented in the database.

- Failure signatures:
  - High false positives: VQA may over-interpret benign symbols as hateful; check Q&A logs for spurious hate mentions.
  - Low retrieval relevance: Sub-label misassignment or embedding drift; verify sub-label classifier accuracy and embedding quality.
  - OCR misses: Stylized text not extracted; inspect OCR output on failed examples and consider augmentation or font-specific training.

- First 3 experiments:
  1. Ablation of VQA turns: Compare 1-turn vs. 2-turn vs. 3-turn VQA to quantify the marginal gain from iterative questioning and identify optimal turn count.
  2. Sub-label partition analysis: Evaluate retrieval precision and classification accuracy across individual sub-labels (race, religion, gender) to identify underperforming categories.
  3. OCR vs. end-to-end VLM text extraction: On a held-out set of memes with stylized fonts, compare PaddleOCR text extraction against GPT-4o's native text reading to validate the explicit OCR design choice.

## Open Questions the Paper Calls Out
None

## Limitations
- False Positive Risk from VQA Hallucination: The paper does not report false positive rates or quantify hallucination risk, which is concerning for hate speech classification.
- OCR Robustness in Real-World Deployment: The superiority of explicit OCR over end-to-end VLMs lacks direct empirical comparison in the paper.
- Computational Overhead and Latency: The multi-turn VQA and RAG pipeline significantly improves accuracy but at substantial computational cost with no quantification of latency.

## Confidence
- High Confidence: Overall detection performance improvements (73.50% accuracy, 78.35% AUC-ROC) are well-supported by experimental results.
- Medium Confidence: Mechanism claims around sub-label classification and multi-turn VQA are supported by ablation results but need deeper failure analysis.
- Low Confidence: The explicit OCR design choice and its superiority over end-to-end VLMs lacks direct empirical comparison.

## Next Checks
1. VQA Hallucination Audit: Implement human-in-the-loop validation of VQA-generated Q&A pairs on 100 memes to quantify false positive rates.
2. OCR vs. VLM Text Extraction Benchmark: Compare PaddleOCR against GPT-4o's native text reading on 200 memes with stylized fonts and measure downstream impact.
3. Real-Time Deployment Simulation: Measure end-to-end latency and evaluate accuracy degradation when limiting VQA to single turn or reducing retrieved exemplars.