---
ver: rpa2
title: A Generative Imputation Method for Multimodal Alzheimer's Disease Diagnosis
arxiv_id: '2508.09271'
source_url: https://arxiv.org/abs/2508.09271
tags:
- data
- generative
- disease
- alzheimer
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposed a generative imputation method using Cycle-GAN
  to address missing multimodal data in Alzheimer's disease diagnosis. The method
  translated between functional network connectivity (FNC) maps and T1-weighted structural
  MRI to generate missing modalities.
---

# A Generative Imputation Method for Multimodal Alzheimer's Disease Diagnosis

## Quick Facts
- arXiv ID: 2508.09271
- Source URL: https://arxiv.org/abs/2508.09271
- Reference count: 0
- Primary result: 9% improvement in AD vs. CN classification accuracy using generative imputation vs. traditional methods

## Executive Summary
This study proposes a generative imputation method using Cycle-GAN to address missing multimodal data in Alzheimer's disease diagnosis. The method translates between functional network connectivity (FNC) maps and T1-weighted structural MRI to generate missing modalities. When integrated into a multimodal classifier, the approach improved AD vs. CN classification accuracy by 9% compared to traditional imputation methods. The method effectively preserved disease-related patterns while addressing the challenge of incomplete neuroimaging datasets.

## Method Summary
The method employs Cycle-GAN to translate between 1D FNC maps (1374 features) and 3D T1 images (121×145×121) for multimodal imputation. Two generators handle bidirectional translation: G1 transforms FNC to T1 using 3D transposed convolutions, while G2 transforms T1 to FNC using 3D convolutions. The framework includes adversarial losses, cycle consistency loss (λ₁=10), and identity loss for paired data (λ₂=40). The multimodal classifier combines FNC and T1 features through separate branches, concatenates them, and applies fully connected layers for classification. The approach leverages 2910 T1 images, 414 FNC maps, and 402 paired samples from ADNI.

## Key Results
- 9% improvement in AD vs. CN classification accuracy compared to traditional imputation methods
- Generative model metrics: SSIM=0.89, PSNR=24.92, MSE=0.083, Pearson correlation=0.71
- Classification performance: 86.87% accuracy, 0.88 F1 score, 0.86 recall, 0.91 precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cycle-consistent adversarial translation enables cross-modal synthesis between structurally dissimilar neuroimaging modalities while preserving disease-specific patterns.
- Mechanism: The Cycle-GAN framework learns bidirectional mappings (FNC↔T1) through adversarial training with cycle consistency constraints. Generator G1 transforms 1D FNC vectors (1374 features) into 3D T1 volumes (121×145×121) via transposed convolutions; G2 performs the inverse via 3D convolutions. Cycle consistency loss (L1 norm) ensures semantic preservation across translations, while adversarial losses enforce domain realism.
- Core assumption: Disease-related structural and functional patterns are correlated and can be mapped between modalities despite dimensional and semantic differences.
- Evidence anchors:
  - [abstract] "translated between functional network connectivity (FNC) maps and T1-weighted structural MRI to generate missing modalities"
  - [section 2.1] "The generator G1 transforms FNC maps, each with a size of 1374, into T1 images, each with dimensions of 121x145x121"
  - [corpus] Related work "Modality-Agnostic Style Transfer for Holistic Feature Imputation" addresses similar cross-modal challenges; however, direct corpus evidence for 1D→3D translation in AD is limited.
- Break condition: Cycle consistency loss dominates, producing blurry outputs that lose modality-specific features; or discriminators fail to enforce realism (mode collapse).

### Mechanism 2
- Claim: Weak supervision through identity loss improves paired-data alignment without requiring complete paired datasets.
- Mechanism: When paired FNC-T1 samples exist, identity loss (L1 distance between generated and real paired samples) supplements unsupervised cycle training. This anchors the translation to ground-truth relationships where available, reducing hallucinated features while maintaining the flexibility of unpaired training.
- Core assumption: Paired samples, even if sparse, represent the true cross-modal distribution and can regularize the learned mapping.
- Evidence anchors:
  - [section 2.1] "we incorporated weak supervision by using identity loss for paired data: L_id(G1, G2) = Σ[∥G1(x) − y∥₁] + Σ[∥G2(y) − x∥₁], where P denotes the set of paired data"
  - [section 2.3] "We selected λ₁ = 10 and λ₂ = 40 via a grid search"
  - [corpus] "Multi-modal Imputation for Alzheimer's Disease Classification" mentions paired supervision benefits but corpus evidence for identity loss specifically is not directly available.
- Break condition: Paired data is too sparse or unrepresentative, causing overfitting to specific pairs rather than learning generalizable mappings.

### Mechanism 3
- Claim: Generative imputation preserves disease discriminability better than zero-filling or subsampling for downstream classification.
- Mechanism: Generated modalities retain group-differentiating features (e.g., hippocampal atrophy, CB-SM connectivity changes). When concatenated with real features in the multimodal classifier, they provide complementary signal rather than noise, increasing effective training samples from 402 (paired only) to 2910 (all T1) + 414 (all FNC).
- Core assumption: Generated samples approximate the true conditional distribution of missing modalities given observed modalities, preserving class-relevant variance.
- Evidence anchors:
  - [abstract] "9% improvement in the classification accuracy for Alzheimer's disease versus cognitive normal groups when using our generative imputation method compared to the traditional approaches"
  - [section 3.1] "Fig. 3 visually shows that the generated data could capture the diagnostic patterns in the real data... similar atrophy in the generated images compared to the real ones"
  - [corpus] "OPTIMUS" and "PHGNN" corroborate that handling missing modalities improves AD prediction, though specific 9% gain replication is not in corpus.
- Break condition: Generator learns spurious correlations or fails to capture rare disease presentations, introducing systematic bias into classification.

## Foundational Learning

- Concept: **Cycle-Consistent Adversarial Networks (Cycle-GAN)**
  - Why needed here: Enables unpaired image-to-image translation where aligned training data is unavailable—the core of cross-modal synthesis.
  - Quick check question: Can you explain why cycle consistency loss prevents the generator from producing outputs unrelated to the input?

- Concept: **Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR)**
  - Why needed here: Standard metrics for evaluating image generation quality; reported in paper (SSIM=0.89, PSNR=24.92).
  - Quick check question: Why is SSIM often preferred over MSE for perceptual image quality assessment?

- Concept: **Functional Network Connectivity (FNC)**
  - Why needed here: Understanding that FNC represents correlation-based connectivity features derived from fMRI ICA decomposition (53 components → 1374 pairwise correlations).
  - Quick check question: What does a high FNC value between two brain regions indicate about their functional relationship?

## Architecture Onboarding

- Component map:
  - FNC input → G1 (5 × 3D transposed conv layers + batch norm + upsampling + tanh) → T1 output
  - T1 input → G2 (5 × 3D conv layers + batch norm + max-pool + linear layer + tanh) → FNC output
  - D1 (5 × 3D conv layers) discriminates real vs. generated T1
  - D2 (3 linear layers) discriminates real vs. generated FNC
  - FNC branch (FC: 1374→64→8) + T1 branch (5 × 3D-CNN + FC: 64→8) → concat (16) → FC layers → softmax

- Critical path: FNC/T1 input → generators → synthesized modality → concatenate with real modality → fused features (dim=16) → classification head. Discriminator feedback updates generators via adversarial loss.

- Design tradeoffs:
  - λ₁=10 (cycle) vs λ₂=40 (identity): Higher identity weight prioritizes paired-data fidelity over cycle flexibility.
  - History buffer of 50 samples: Reduces mode collapse but increases memory.
  - 1D↔3D translation: Enables heterogeneous modalities but risks information loss in dimensionality changes.

- Failure signatures:
  - SSIM < 0.7 or PSNR < 20: Generator producing unrealistic images.
  - Classification accuracy drops below subsampling baseline: Imputation introducing noise rather than signal.
  - Generated T1 shows uniform intensity (mode collapse): Discriminator too strong or learning rate imbalance.

- First 3 experiments:
  1. **Baseline replication**: Train Cycle-GAN on paired ADNI subset, compute SSIM/PSNR/MSE on held-out pairs. Verify metrics match reported values (±5%).
  2. **Ablation on identity loss**: Set λ₂=0 and compare classification accuracy to λ₂=40. Quantify weak supervision contribution.
  3. **Missing modality simulation**: Randomly mask 50% of T1 or FNC in paired data, impute, and measure classification degradation vs. oracle (all real data).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can diffusion models improve the structural fidelity and downstream classification accuracy compared to the Cycle-GAN method in multimodal imputation?
- Basis in paper: [explicit] The conclusion explicitly states, "As an interesting future direction, one could explore the application of other generative models, such as diffusion models, in multi-modal disease diagnosis."
- Why unresolved: The current study is limited to a GAN-based architecture; diffusion models offer distinct theoretical advantages in generating high-quality samples and stable training but remain untested in this specific FNC-to-T1 translation context.
- What evidence would resolve it: A comparative study implementing a conditional diffusion model for the same FNC-T1 imputation task, evaluated against the established Cycle-GAN metrics (SSIM, PSNR) and classification accuracy (currently 86.87%).

### Open Question 2
- Question: Does the generative imputation method maintain diagnostic efficacy when applied to the clinically ambiguous stage of Mild Cognitive Impairment (MCI)?
- Basis in paper: [inferred] The paper evaluates a binary classification (AD vs. CN), excluding the intermediate MCI stage which is often the target for early intervention and presents more subtle, harder-to-capture biomarkers.
- Why unresolved: It is unclear if the synthesized modalities preserve the fine-grained disease patterns necessary to distinguish MCI from CN or AD, as the model was optimized and validated only on the more distinct AD vs. CN groups.
- What evidence would resolve it: Experimental results showing the model's classification performance (F1, precision) on an MCI vs. CN task when using generated data to fill missing modalities.

### Open Question 3
- Question: To what extent does the generated data introduce bias or domain shift when applied to external, unseen clinical datasets?
- Basis in paper: [inferred] The methodology relies exclusively on the ADNI dataset for training and testing via cross-validation, leaving the model's generalizability to other scanners or acquisition protocols unverified.
- Why unresolved: The model may have learned site-specific noise or artifacts present in ADNI data; without external validation, it is uncertain if the generated imputations would degrade classifier performance on independent cohorts.
- What evidence would resolve it: Testing the pre-trained generative model and classifier on an independent dataset (e.g., OASIS or UK Biobank) without fine-tuning to measure the drop in classification accuracy and structural similarity.

## Limitations

- The exact T1 preprocessing pipeline (skull stripping, normalization, template) is unspecified, creating potential variability in downstream generation quality
- Specific hyperparameters for convolutional layers and classifier dimensions are missing, limiting exact replication
- The paper does not compare against modern self-supervised or transformer-based imputation approaches

## Confidence

- **High confidence**: Cycle-GAN framework application for cross-modal translation and the general superiority of generative imputation over zero-filling/subsampling for multimodal classification
- **Medium confidence**: Specific quantitative results (SSIM 0.89, PSNR 24.92, classification metrics) and the exact contribution of identity loss weighting
- **Low confidence**: Claims about clinical interpretability of generated patterns and the absolute magnitude of improvement relative to state-of-the-art imputation methods

## Next Checks

1. Implement ablation study with λ₂=0 (no identity loss) to quantify the contribution of weak supervision to the 9% accuracy gain
2. Compare Cycle-GAN imputation performance against modern approaches (self-supervised learning, transformer-based imputation) on the same ADNI dataset split
3. Conduct qualitative analysis of generated T1 images across disease stages to verify preservation of disease-specific atrophy patterns beyond numerical metrics