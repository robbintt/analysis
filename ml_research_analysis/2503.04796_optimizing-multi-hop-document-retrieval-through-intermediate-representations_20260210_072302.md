---
ver: rpa2
title: Optimizing Multi-Hop Document Retrieval Through Intermediate Representations
arxiv_id: '2503.04796'
source_url: https://arxiv.org/abs/2503.04796
tags:
- information
- transformation
- reasoning
- intermediate
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Layer-wise Retrieval-Augmented Generation (L-RAG),
  a method that leverages intermediate representations from middle layers of LLMs
  to retrieve external documents for multi-hop question answering. The authors identify
  a three-stage information processing pattern in LLMs using Singular Value Decomposition
  (SVD) analysis of weight matrices, showing that intermediate representations contain
  richer information for retrieval compared to final layer representations.
---

# Optimizing Multi-Hop Document Retrieval Through Intermediate Representations

## Quick Facts
- arXiv ID: 2503.04796
- Source URL: https://arxiv.org/abs/2503.04796
- Reference count: 17
- Key outcome: L-RAG achieves 35.2% accuracy on MuSiQue (vs 29.7% for vanilla RAG@8) while maintaining similar latency to standard RAG

## Executive Summary
This paper introduces Layer-wise Retrieval-Augmented Generation (L-RAG), a method that leverages intermediate representations from middle layers of LLMs to retrieve external documents for multi-hop question answering. Through Singular Value Decomposition (SVD) analysis of weight matrices, the authors identify a three-stage information processing pattern in LLMs, showing that intermediate representations contain richer information for retrieval compared to final layer representations. L-RAG achieves performance comparable to multi-step approaches while maintaining inference overhead similar to standard RAG, outperforming existing RAG methods on datasets including MuSiQue, HotpotQA, and 2WikiMultiHopQA.

## Method Summary
L-RAG uses intermediate transformer representations from strategically chosen middle layers to retrieve higher-hop documents without iterative LLM calls. The method involves: (1) a traditional retriever (BM25 or Contriever) that retrieves first-hop documents from the query, (2) an LLM that generates intermediate representation from middle layers in a single forward pass, (3) a modified Contriever with MLP adapter trained via InfoNCE contrastive loss to retrieve higher-hop documents, and (4) context assembly with final generation. Layer selection is guided by transformation divergence (TD) analysis, searching around the TD minimum to find the optimal retrieval layer per dataset.

## Key Results
- L-RAG achieves 35.2% accuracy on MuSiQue (vs 29.7% for vanilla RAG@8)
- L-RAG achieves 41.8% accuracy on 2WikiMultiHopQA (vs 35.5% for vanilla RAG@8)
- Maintains inference overhead similar to standard RAG while outperforming multi-step iterative approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate layers capture richer next-hop information than final layers
- Mechanism: LLMs process information in a three-stage pattern—initial extraction (low transformation divergence), contextual processing (high TD), and token-generation extraction (low TD). Middle-layer "comprehensive representations" integrate structured information before it collapses into token-specific predictions.
- Core assumption: The transformation divergence metric meaningfully correlates with information processing stages; this relationship holds across architectures.
- Evidence anchors:
  - [abstract] "This observation suggests that the representations in intermediate layers contain richer information compared to those in other layers"
  - [section 3.2] Defines TD(Wv) using SVD of weight matrices; low TD indicates concentrated extraction, high TD indicates multi-directional processing
  - [corpus] Related work (Query-Centric Graph RAG, Hierarchical Lexical Graph) addresses multi-hop retrieval but does not validate the intermediate-representation hypothesis

### Mechanism 2
- Claim: Single-pass intermediate representation retrieval achieves multi-step RAG performance
- Mechanism: Instead of iterating LLM calls to generate sub-queries, L-RAG extracts representations from a single forward pass at strategically chosen layers. A trained MLP-Contriever maps these representations to document embeddings for retrieval.
- Core assumption: The intermediate representation encodes sufficient "next-hop" signal without requiring explicit query decomposition.
- Evidence anchors:
  - [abstract] "L-RAG achieves performance comparable to multi-step approaches while maintaining inference overhead similar to that of standard RAG"
  - [section 4.2] "only the representation retriever requires training... we add a MLP layer in front of the original Contriever to align LLM intermediate representations"
  - [corpus] No direct corpus validation; related multi-hop RAG methods (MacRAG, BifrostRAG) use different retrieval strategies

### Mechanism 3
- Claim: Layer selection can be guided by minimum transformation divergence
- Mechanism: The layer with lowest TD is a starting point; L-RAG searches neighboring layers (CL(k,n,l) = l±nk) to find the optimal retrieval layer per dataset.
- Core assumption: The TD minimum correlates with "comprehensive representation" quality for retrieval.
- Evidence anchors:
  - [section 3.3] "the best recall performance is observed at the layer during or after the low transformation divergence period"
  - [Figure 10] Shows correlation between normalized TD and recall rate across layers in Llama-2-7b
  - [corpus] No corpus papers validate TD-based layer selection; this is novel to this paper

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD) for weight analysis**
  - Why needed here: Understanding how the paper quantifies "transformation divergence" requires grasping how SVD decomposes matrices into rotation-scaling-rotation operations
  - Quick check question: Given a weight matrix W, can you explain what the singular values σ₁, σ₂... represent in terms of transformation magnitude?

- Concept: **LogitLens interpretability technique**
  - Why needed here: The paper uses LogitLens to validate that intermediate answers emerge earlier than final answers, supporting the three-stage processing hypothesis
  - Quick check question: How would you apply the unembedding matrix W_U to a hidden state h_l to inspect token probabilities at layer l?

- Concept: **Contrastive InfoNCE loss**
  - Why needed here: Training the representation retriever requires understanding how positive/negative document pairs shape the embedding alignment
  - Quick check question: In equation (4), what happens to the loss when the similarity score s(r_i, d_i^+) increases relative to other documents in the batch?

## Architecture Onboarding

- Component map: Query → Traditional retriever → First-hop docs → LLM (single pass, capture layer-l hidden state) → MLP projection → Contriever encoding → Higher-hop retrieval → Final answer generation

- Critical path: Query → Traditional retriever → First-hop docs → LLM (single pass, capture layer-l hidden state) → MLP projection → Contriever encoding → Higher-hop retrieval → Final answer generation

- Design tradeoffs:
  - **Layer selection**: Earlier layers may miss processed context; later layers may lose next-hop signal to token specialization. Paper recommends searching around TD minimum.
  - **Training per-dataset vs. universal**: Paper trains separately per dataset for best results; a universal retriever may sacrifice accuracy for convenience.
  - **Fixed vs. adaptive layers**: Current approach uses fixed layer per dataset; adaptive per-query selection is unexplored.

- Failure signatures:
  - Low recall but high accuracy: First-hop retrieval may already contain answer (information leakage, common in HotpotQA)
  - High latency: Check if you're accidentally running multiple forward passes; L-RAG should require only one
  - Accuracy degrades on 2WikiMQA but not others: 2WikiMQA has less information leakage; retrieval quality matters more

- First 3 experiments:
  1. **TD profile validation**: Run SVD on your LLM's W_v matrices across layers; confirm the three-phase TD pattern exists (plot normalized TD vs. layer index)
  2. **Layer ablation study**: Train representation retrievers at layers l-2k, l-k, l, l+k, l+2k (where l = TD minimum); evaluate recall@8 on a held-out multi-hop subset
  3. **Latency baseline**: Measure end-to-end latency for Vanilla RAG@8, L-RAG@8, and IterRetGen on 100 queries; confirm L-RAG is within 10% of Vanilla RAG

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the L-RAG framework effectively handle queries requiring three or more reasoning hops without iterative retrieval?
- Basis in paper: [explicit] Section 5.1 states, "we focus on queries that require two-hop reasoning," explicitly excluding more complex chains from the evaluation scope.
- Why unresolved: The method extracts a single intermediate representation; it is unclear if this captures sufficient signal for third-hop retrieval or if the "next-hop information" degrades after the second hop.
- What evidence would resolve it: Evaluation results on 3-hop subsets of MuSiQue or specifically constructed multi-hop benchmarks.

### Open Question 2
- Question: Is it possible to train a universal representation retriever that generalizes across domains without requiring dataset-specific fine-tuning?
- Basis in paper: [explicit] Page 7 notes, "We train L-RAG separately for each dataset to achieve better performance," leaving the transferability of the retriever unexplored.
- Why unresolved: It is unknown if the "Comprehensive Representation" learned is domain-agnostic or if the MLP adapter overfits to the specific query distribution of the training data.
- What evidence would resolve it: Experiments training the retriever on one dataset (e.g., HotpotQA) and evaluating on another (e.g., 2WikiMultiHopQA).

### Open Question 3
- Question: Does the identified three-stage information processing pattern persist in significantly larger models (e.g., >70B parameters) or non-transformer architectures?
- Basis in paper: [inferred] The Limitations section states "reasoning pattern characterization requires broader empirical support," and the analysis is restricted to 7B-8B models (Llama, Mistral, GPT-J).
- Why unresolved: The Transformation Divergence (TD) dynamics and layer-wise roles might shift fundamentally in frontier-scale models or architectures with different depth-to-width ratios.
- What evidence would resolve it: Application of the SVD-based TD analysis and LogitLens validation to larger parameter models.

## Limitations
- The TD-based layer selection heuristic assumes a universal three-stage processing pattern across LLMs, but validation is limited to Llama-2-7b
- Performance gains are inconsistent between datasets with information leakage (HotpotQA) versus those requiring genuine retrieval (2WikiMQA)
- Method assumes middle-layer representations consistently encode "next-hop" information, which may not hold for all query types or domains

## Confidence
- **High confidence**: The core mechanism of using intermediate representations for retrieval is well-supported by empirical results showing L-RAG outperforms vanilla RAG on multiple datasets while maintaining similar latency
- **Medium confidence**: The transformation divergence analysis and three-stage processing hypothesis shows correlation but lacks causal validation across different model architectures
- **Medium confidence**: The TD-based layer selection heuristic demonstrates effectiveness on tested datasets but may not generalize to all LLMs or query types

## Next Checks
1. **Cross-architecture validation**: Apply SVD-based TD analysis to GPT-3.5, Mistral, and other popular LLMs to verify whether the three-stage processing pattern consistently emerges across different model families

2. **Query-type sensitivity analysis**: Design experiments testing L-RAG on queries requiring different reasoning depths and types (causality, comparison, temporal reasoning) to determine whether fixed layer selection performs well across diverse query patterns

3. **Universal retriever training**: Train a single representation retriever on combined multi-hop datasets rather than per-dataset training to evaluate whether universal models can achieve competitive performance with reduced development overhead