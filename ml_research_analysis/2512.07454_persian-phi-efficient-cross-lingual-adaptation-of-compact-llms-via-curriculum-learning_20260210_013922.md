---
ver: rpa2
title: 'Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum
  Learning'
arxiv_id: '2512.07454'
source_url: https://arxiv.org/abs/2512.07454
tags:
- persian
- language
- arxiv
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Persian-Phi, a 3.8B parameter language model
  adapted from Microsoft's Phi-3 Mini to support the Persian language. The authors
  address the challenge of extending state-of-the-art monolingual models to underrepresented
  languages through a novel curriculum learning pipeline.
---

# Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning

## Quick Facts
- **arXiv ID**: 2512.07454
- **Source URL**: https://arxiv.org/abs/2512.07454
- **Reference count**: 40
- **Key outcome**: 3.8B parameter model achieves ~80% of 8B parameter state-of-the-art performance on Persian language tasks

## Executive Summary
Persian-Phi demonstrates that compact language models can be efficiently adapted to support underrepresented languages through a curriculum learning approach. The model, built by adapting Microsoft's Phi-3 Mini (3.8B parameters), achieves competitive performance on Persian language tasks despite being significantly smaller than state-of-the-art models like Dorna-2 (8B parameters). The approach leverages tokenizer enhancement, bilingual narrative warm-up, and parameter-efficient fine-tuning on filtered Persian corpora, requiring minimal hardware resources while delivering substantial cross-lingual capabilities.

## Method Summary
The Persian-Phi methodology employs a three-phase curriculum learning pipeline for cross-lingual adaptation. First, the tokenizer is enhanced with Persian-specific tokens to improve subword representation. Second, a warm-up phase uses bilingual narratives to align the model's embeddings with Persian language patterns. Finally, the model undergoes continual pretraining on filtered Persian corpora using parameter-efficient fine-tuning techniques. This approach enables effective adaptation of a monolingual English model to the Persian language while maintaining computational efficiency and achieving competitive performance on established benchmarks.

## Key Results
- Achieves approximately 80% of Dorna-2's performance on the Open Persian LLM Leaderboard
- Demonstrates competitive results with only 3.8B parameters compared to Dorna-2's 8B parameters
- Validates that compact models can effectively support low-resource languages through strategic adaptation
- Requires minimal hardware resources for training and deployment

## Why This Works (Mechanism)
The curriculum learning approach works by gradually exposing the model to Persian language patterns in a structured manner. The tokenizer enhancement ensures proper representation of Persian-specific linguistic features, while the bilingual warm-up phase bridges the gap between English and Persian embeddings. The filtered Persian corpus pretraining allows the model to develop language-specific understanding without catastrophic forgetting of the original English capabilities.

## Foundational Learning
- **Tokenization and subword representation**: Essential for handling Persian morphology and script; quick check: compare token coverage before/after enhancement
- **Cross-lingual embedding alignment**: Bridges semantic gaps between languages; quick check: measure embedding similarity scores
- **Curriculum learning**: Gradual exposure improves learning efficiency; quick check: track performance across training phases
- **Parameter-efficient fine-tuning**: Maintains original capabilities while adapting to new language; quick check: monitor parameter changes during training
- **Language model adaptation**: Transferring knowledge between languages; quick check: compare zero-shot performance before/after adaptation
- **Low-resource language modeling**: Techniques for languages with limited training data; quick check: measure performance on various Persian text domains

## Architecture Onboarding

**Component Map**
Tokenizer Enhancement -> Bilingual Warm-up -> Filtered Persian Pretraining -> Evaluation

**Critical Path**
The most critical sequence is Tokenizer Enhancement followed by Bilingual Warm-up, as these establish the foundation for successful cross-lingual adaptation before the extensive pretraining phase.

**Design Tradeoffs**
- Smaller model size (3.8B vs 8B) reduces computational requirements but may limit maximum performance
- Parameter-efficient fine-tuning preserves original capabilities but may restrict adaptation depth
- Filtered corpus selection ensures quality but may reduce training diversity

**Failure Signatures**
- Poor tokenization leads to fragmented Persian text representation
- Inadequate bilingual warm-up causes semantic misalignment
- Insufficient corpus filtering introduces noise and biases
- Overfitting to leaderboard tasks rather than general Persian language understanding

**First Experiments**
1. Evaluate tokenization quality on Persian text segmentation tasks
2. Test bilingual embedding alignment with parallel corpus similarity metrics
3. Measure zero-shot Persian performance before and after warm-up phase

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies primarily on the Open Persian LLM Leaderboard, potentially missing task-specific capabilities
- Limited comparison methodology with Dorna-2, lacking detailed error analysis and ablation studies
- Insufficient details about Persian corpus characteristics, including size, quality metrics, and potential biases
- Hardware requirements quantified only as "minimal" without specific GPU memory or training time metrics

## Confidence
- **High confidence**: Basic methodology of tokenizer enhancement and continual pretraining for cross-lingual adaptation is well-established and technically sound
- **Medium confidence**: Curriculum learning approach and parameter-efficient fine-tuning strategy are reasonable but lack extensive empirical validation
- **Medium confidence**: Performance claims relative to Dorna-2 are supported by leaderboard results but need more rigorous validation across diverse Persian language tasks

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component (tokenizer enhancement, warm-up phase, continual pretraining) to overall performance
2. Evaluate Persian-Phi on a diverse set of Persian language tasks beyond the leaderboard, including domain-specific applications and low-resource Persian dialects
3. Perform detailed error analysis comparing Persian-Phi and Dorna-2 to identify specific strengths and weaknesses of the curriculum learning approach