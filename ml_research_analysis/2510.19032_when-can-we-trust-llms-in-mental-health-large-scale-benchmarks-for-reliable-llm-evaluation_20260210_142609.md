---
ver: rpa2
title: When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable
  LLM Evaluation
arxiv_id: '2510.19032'
source_url: https://arxiv.org/abs/2510.19032
tags:
- health
- reliability
- human
- mental
- judges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of evaluating Large Language
  Models (LLMs) in mental health support, where therapeutic dialogue requires both
  cognitive and emotional competence. The authors introduce two benchmarks: MentalBench-100k,
  consolidating 10,000 real counseling conversations with 100,000 LLM-generated responses,
  and MentalAlign-70k, comparing four LLM judges against human experts across 70,000
  ratings on seven therapeutic attributes.'
---

# When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable LLM Evaluation

## Quick Facts
- arXiv ID: 2510.19032
- Source URL: https://arxiv.org/abs/2510.19032
- Reference count: 40
- Large-scale evaluation shows LLM judges reliably assess cognitive attributes but show systematic inflation and poor reliability on safety-critical dimensions

## Executive Summary
This study addresses the challenge of evaluating Large Language Models (LLMs) in mental health support, where therapeutic dialogue requires both cognitive and emotional competence. The authors introduce two benchmarks: MentalBench-100k, consolidating 10,000 real counseling conversations with 100,000 LLM-generated responses, and MentalAlign-70k, comparing four LLM judges against human experts across 70,000 ratings on seven therapeutic attributes. They develop the Affective–Cognitive Agreement Framework using intraclass correlation coefficients (ICC) to quantify agreement, consistency, and bias between LLM judges and human experts. Results show systematic inflation by LLM judges, strong reliability for cognitive attributes like guidance and informativeness (ICC up to 0.95), reduced precision for empathy, and poor reliability for safety and relevance. This work establishes new methodological foundations for reliable, large-scale evaluation of LLMs in mental health.

## Method Summary
The study consolidates three existing counseling datasets (MentalChat16K, EmoCare, CounselChat) into MentalBench-100k containing 10,000 real conversations. Nine different LLMs generate responses to these contexts at temperature 0.7 and max tokens 512. Four LLM judges (GPT-4o, O4-Mini, Claude-3.7-Sonnet, Gemini-2.5-Flash) evaluate each response on seven therapeutic attributes using a 1-5 Likert scale, excluding self-evaluations. The Affective–Cognitive Agreement Framework computes ICC(C,1) for consistency, ICC(A,1) for absolute agreement, and systematic bias using bootstrap confidence intervals. MentalAlign-70k contains 70,000 ratings from 1,000 conversations × 10 responses × 7 attributes × 5 judges.

## Key Results
- Cognitive attributes (guidance, informativeness) show excellent reliability with ICC(C,1) up to 0.95
- Affective attribute empathy shows systematic inflation bias up to +0.816 scale points with high variance
- Safety and relevance attributes show poor reliability (ICC(C,1): 0.26–0.73) with wide confidence intervals
- LLM judges can be calibrated for cognitive evaluation but require human oversight for affective and safety-critical dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can reliably evaluate cognitive dimensions of mental health dialogue but show systematic inflation and higher uncertainty on affective dimensions, and poor reliability on safety-critical attributes.
- Mechanism: The Intraclass Correlation Coefficient (ICC) framework uses ICC(C,1) for consistency and ICC(A,1) for absolute agreement, combined with bootstrap confidence intervals and bias detection. This statistically decomposes evaluation variance, separating correctable systematic bias from unreliable random error and quantifying uncertainty to prevent overconfidence from single scores.
- Core assumption: Assumes human expert ratings are a valid reference baseline for therapeutic quality, though subject to variability. Assumes the 5-point Likert scale captures sufficient nuance of therapeutic attributes.
- Evidence anchors:
  - [abstract] "Our analysis reveals systematic inflation by LLM judges, strong reliability for cognitive attributes such as guidance and informativeness, reduced precision for empathy, and some unreliability in safety and relevance."
  - [section 6.2] "Cognitive attributes show the highest reliability. Guidance and Informativeness achieve excellent consistency (ICC(C,1): 0.85–0.95)... Safety and Relevance show reliability challenges... (ICC(C,1): 0.26–0.73) with wide CI..."
  - [corpus] Related work from CounselBench and others confirms the challenge of evaluating LLMs in mental health, particularly for nuanced therapeutic qualities beyond factual correctness.
- Break condition: The mechanism breaks if human experts have systematically biased judgments compared to a more fundamental ground truth of therapeutic effectiveness, or if the CSS/ARS attributes are incomplete or invalid.

### Mechanism 2
- Claim: Benchmarks from real therapeutic data provide more ecologically valid evaluation than synthetic or social media-derived datasets.
- Mechanism: Consolidating and cleaning existing real counseling conversation datasets (MentalBench-100k) ensures context-response pairs reflect authentic clinical dynamics, user phrasing, and problem complexity. This realism propagates through evaluation, as judges rate responses to genuine concerns.
- Core assumption: Assumes source datasets are ethically sourced, represent sufficient condition breadth, and that human response therapeutic quality is acceptable as baseline.
- Evidence anchors:
  - [abstract] "...MentalBench-100k, consolidating 10,000 real counseling conversations..."
  - [section 1] "...concerns around ethical risks and the absence of datasets that capture authentic therapeutic dynamics..."
  - [corpus] Corpus shows many datasets rely on social media (Reddit), which this paper critiques for lacking therapeutic grounding; real clinical data is a key differentiator.
- Break condition: Benchmark validity is compromised if consolidated datasets have significant biases, lack diversity, or if cleaning/standardization removed critical interaction nuances.

### Mechanism 3
- Claim: Multi-attribute, dual-axis evaluation (CSS and ARS) grounded in psychological instruments enables more granular and reliable assessment than single scores or generic NLP metrics.
- Mechanism: The dual-axis framework decomposes "therapeutic quality" into measurable components (CSS: Guidance, Informativeness, Relevance, Safety; ARS: Empathy, Helpfulness, Understanding). This aligns evaluation with distinct skills, enabling identification of specific strengths/weaknesses and targeted reliability analysis per attribute.
- Core assumption: Assumes these seven attributes comprehensively cover key mental health support dimensions and can be evaluated relatively independently using provided rubrics.
- Evidence anchors:
  - [abstract] "...comparing four LLM judges against human experts across 70,000 ratings on seven therapeutic attributes."
  - [section 4.1] "...grounded in established principles from clinical psychology... Cognitive Support Score (CSS): evaluates how well the response provides clarity and problem-solving assistance. ... Affective Resonance Score (ARS): measures the emotional quality..."
  - [corpus] Other papers (e.g., "Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation") also support multi-attribute evaluation approaches.
- Break condition: Mechanism is less effective if judges cannot distinguish between attributes (high correlation/inconsistency), or if critical therapeutic dimensions are missing from chosen attributes.

## Foundational Learning

### Concept: Intraclass Correlation Coefficient (ICC)
- Why needed here: Core statistical method for quantifying reliability and agreement between LLM and human judges. Understanding ICC(C,1) vs. ICC(A,1) and confidence interval interpretation is non-negotiable for interpreting results.
- Quick check question: What does high ICC(C,1) but low ICC(A,1) for "empathy" indicate about an LLM judge's performance?

### Concept: "LLM-as-a-Judge" Bias and Calibration
- Why needed here: Study's central finding is LLM judges are systematically biased (leniency/inflation). Distinguishing correctable error from fundamental unreliability is key for deployment decisions.
- Quick check question: If an LLM judge has high positive bias on "helpfulness" but good ICC(C,1), what mitigation strategy could be used?

### Concept: Confidence Intervals over Point Estimates
- Why needed here: Paper argues point estimates (MSE, single ICC) can be misleadingly confident. Bootstrap CI width is critical for assessing true precision and trustworthiness.
- Quick check question: An LLM judge has ICC(C,1) of 0.90. Why might it still be classified as "Moderate Reliability"?

## Architecture Onboarding

### Component map:
Data Source (MentalBench-100k) -> Response Generator (9 LLMs) -> Evaluator Module (4 LLM judges + human experts) -> Analysis Engine (ICC/Bias/CI) -> Reliability Classifier (Good/Moderate/Poor)

### Critical path:
Data Curation → Response Generation → Multi-Attribute Evaluation → ICC/Bias Calculation → Reliability Assessment → Decision (Trust/Calibrate/Discard)

### Design tradeoffs:
- Scale vs. Detail: 1,000 conversations for expert evaluation (resource-intensive) vs. 100,000 response benchmark (scalable)
- Broad vs. Deep Attributes: 7 attributes cover many aspects but may miss specific clinical nuances
- Single vs. Panel Evaluation: Panel reduces single-model bias but increases cost and aggregation complexity

### Failure signatures:
- "Good Reliability" on unsafe content: High ICC on safety misleading if absolute scores dangerously high
- Misinterpreting "Systematic Bias": Treating high bias as irredeemable when calibration possible (especially with strong ICC(C,1))
- Over-reliance on single scores: Trusting high average without inspecting per-attribute ICC and CIs

### First 3 experiments:
1. **Cross-Validation with New Judges**: Introduce new LLM (e.g., Llama-3.1-70B) as judge; test if ICC patterns (cognitive > affective > safety) generalize
2. **Calibration Experiment**: Apply linear correction to LLM judge with high bias (e.g., GPT-4o on Empathy); evaluate if ICC(A,1) improves without degrading ICC(C,1)
3. **Failure Case Analysis**: Manually inspect conversations with highest LLM-human disagreement; qualitatively analyze shared characteristics (crisis situations, cultural nuances) to identify blind spots

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can reliability for safety and relevance attributes be improved given LLM judges show poor agreement (ICC(C,1): 0.26–0.73) with human experts in these safety-critical dimensions?
- Basis in paper: [explicit] Section 6.2 states "Safety and Relevance show reliability challenges. Both attributes show poor reliability across metrics (ICC(C,1): 0.26–0.73; ICC(A,1): 0.12–0.28) with wide CI, indicating disagreement on ranking and absolute scales. This pattern suggests that safety and relevance may require domain-specific expertise that current LLMs lack."
- Why unresolved: Current LLM judges lack domain-specific expertise for safety-critical evaluation, and the paper does not propose interventions beyond human oversight.
- What evidence would resolve it: Testing targeted training or prompting strategies on safety-specific evaluation tasks, followed by ICC reassessment.

### Open Question 2
- Question: Can calibration-based correction effectively reduce systematic inflation bias in affective attributes like empathy (up to +0.816 scale points) without degrading ranking reliability?
- Basis in paper: [explicit] Section 6.3 notes "cognitive dimensions may benefit from calibration-based correction, while affective and safety-critical dimensions require stricter human oversight." The paper identifies systematic bias but does not implement or test calibration.
- Why unresolved: The framework quantifies bias but does not validate whether calibration transforms affective evaluation into a trustworthy automated process.
- What evidence would resolve it: Implement bias correction on affective scores, then remeasure ICC(A,1) and bias magnitude to assess improvement.

### Open Question 3
- Question: How does evaluation reliability differ in multi-turn therapeutic dialogues compared to the single-turn scenarios studied in MentalBench-100k?
- Basis in paper: [explicit] Limitations section states "MentalBench-100K is constrained to English one-turn dialogues. We position it as a starting point for community-driven expansion toward multi-turn, multilingual, and culturally diverse mental health corpora."
- Why unresolved: Real therapeutic interactions are multi-turn, but the study design only tested single-turn scenarios.
- What evidence would resolve it: Extend the framework to multi-turn counseling dialogues and compare ICC patterns across cognitive and affective attributes.

### Open Question 4
- Question: Do different prompt formulations for LLM judges yield significantly different reliability patterns across the seven therapeutic attributes?
- Basis in paper: [explicit] Limitations section states "Model performance may vary with different prompt formulations, as LLMs exhibit differing sensitivities to prompt structure and phrasing. We provide the baseline, which researchers can explore more with different test scenarios."
- Why unresolved: Only one evaluation prompt was tested; sensitivity to prompt design remains unquantified.
- What evidence would resolve it: Systematically vary judge prompt phrasing and measure resulting ICC(C,1), ICC(A,1), and CI width changes.

## Limitations

- MentalBench-100k is constrained to English one-turn dialogues, limiting applicability to real therapeutic interactions
- Safety and relevance attributes show poor reliability despite being the most ethically critical domains
- Results may vary with different prompt formulations as LLMs exhibit differing sensitivities to prompt structure

## Confidence

- **High Confidence**: ICC framework validity for cognitive attribute evaluation, systematic bias detection methodology, real conversation data authenticity
- **Medium Confidence**: Multi-attribute rubric completeness, human expert rating consistency, generalizability across different LLM judge populations
- **Low Confidence**: Safety-critical attribute evaluation reliability, transferability to crisis counseling contexts, long-term calibration effectiveness

## Next Checks

1. **Cross-Judge Generalization Test**: Apply the framework with a new set of LLM judges (e.g., Claude-3.5-Sonnet, GPT-4o-2024-05-13) to verify whether cognitive > affective > safety reliability patterns persist across different model families and evaluation configurations.

2. **Safety Attribute Calibration Experiment**: Design targeted calibration prompts for safety evaluation, incorporating explicit harm prevention guidelines and counterfactual scenarios. Measure whether ICC(A,1) improves while maintaining acceptable ICC(C,1) for this high-stakes attribute.

3. **Clinical Outcome Validation**: Partner with mental health practitioners to assess whether LLM-generated responses scoring high on reliable attributes (guidance, informativeness) correlate with positive therapeutic outcomes in controlled pilot studies, bridging the gap between evaluation metrics and real-world efficacy.