---
ver: rpa2
title: 'Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers'
arxiv_id: '2509.03059'
source_url: https://arxiv.org/abs/2509.03059
tags:
- data
- code
- reasoning
- arxiv
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Loong Project introduces a scalable synthetic data generation
  framework for improving large language model reasoning across diverse domains beyond
  mathematics and programming. It consists of two components: LoongBench, a curated
  seed dataset of 8,729 human-vetted examples across 12 reasoning-intensive domains
  (e.g., advanced mathematics, chemistry, logic) paired with executable code and verified
  answers; and LoongEnv, a modular environment that generates new question-answer
  pairs using various prompting strategies (few-shot, self-instruct, evol-instruct)
  with code-based verification.'
---

# Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers

## Quick Facts
- arXiv ID: 2509.03059
- Source URL: https://arxiv.org/abs/2509.03059
- Reference count: 40
- Synthesizes 8,729 seed examples into 12 domains with executable code solutions for scalable RLVR training

## Executive Summary
The Loong Project introduces a scalable synthetic data generation framework for improving large language model reasoning across diverse domains beyond mathematics and programming. It consists of two components: LoongBench, a curated seed dataset of 8,729 human-vetted examples across 12 reasoning-intensive domains (e.g., advanced mathematics, chemistry, logic) paired with executable code and verified answers; and LoongEnv, a modular environment that generates new question-answer pairs using various prompting strategies (few-shot, self-instruct, evol-instruct) with code-based verification. The framework enables reinforcement learning where models are rewarded for generating Chain-of-Thought solutions that align with code-executed answers. Benchmarking shows top reasoning-optimized models (o3-mini, DeepSeek-r1) consistently outperform others, with significant performance gaps in reasoning-heavy domains.

## Method Summary
The framework uses a multi-agent pipeline where a question synthesis agent generates natural language questions, a code generation agent produces executable Python solutions, and a judge agent verifies semantic equivalence between model responses and ground truth. Three generation strategies—Few-shot (mimics seed examples), Self-Instruct (recursively diversifies), and Evol-Instruct (mutates for complexity)—create synthetic data with different characteristics. Code execution provides binary rewards for reinforcement learning, while LLM-as-judge handles semantic verification across format variations. The system targets 12 reasoning-intensive domains where programmatic solutions exist, enabling scalable RLVR without human annotation bottlenecks.

## Key Results
- LoongBench contains 8,729 examples across 12 domains with executable Python code and verified answers
- Top reasoning-optimized models (o3-mini, DeepSeek-r1) outperform others by significant margins, especially in reasoning-heavy domains
- Synthetic data generation achieves high executability (up to 92.6% in logic) but varies by strategy, with Evol-Instruct producing more challenging yet semantically rich examples despite lower executability
- Few-shot strategy yields highest pass rates (92.6% Logic, 93.9% Physics) while Evol-Instruct generates hardest questions (62-70% model accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code execution provides a scalable verifiable reward signal that bypasses human annotation bottlenecks for reasoning tasks.
- Mechanism: For each synthetic question, a code-generation agent produces executable Python that computes an answer. This answer serves as ground truth for training via RLVR, where the reward is binary agreement between the model's CoT-derived answer and the code-executed result.
- Core assumption: Domains admit programmatic solutions that are semantically equivalent to natural language reasoning paths.
- Evidence anchors:
  - [abstract] "an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers"
  - [section 2] "code-based solutions are a standard way to formulate and solve domain-specific problems"
- Break condition: Domains where reasoning cannot be reduced to executable code (e.g., open-ended creative writing, subjective evaluation) would fail.

### Mechanism 2
- Claim: Synthetic data generation strategies create a difficulty-diversity-executability tradeoff curve.
- Mechanism: Three strategies—Few-shot (mimics seed examples), Self-Instruct (recursively diversifies), Evol-Instruct (mutates for complexity)—produce data with different characteristics. Evol-Instruct yields harder, semantically richer questions but lower executability (55% non-executable in Logic vs. 7.4% for Few-shot).
- Core assumption: Harder training examples improve generalization despite lower yield.
- Evidence anchors:
  - [section 3.3.1] "Evol-Instruct, despite its higher rejection and execution failure rates, remains highly valuable from a training perspective"
  - [section 3.3.3] Table 3 shows Evol-Instruct questions achieve only 62-70% accuracy vs. 92-93% for Few-shot, indicating higher difficulty
- Break condition: If harder examples don't transfer to improved reasoning (not yet tested), the strategy selection heuristic collapses.

### Mechanism 3
- Claim: Dual-verification (code execution + LLM-as-judge) reduces false negatives in semantic equivalence checking.
- Mechanism: Code execution produces numerical/structural outputs; a separate judge LLM verifies semantic equivalence between model responses and ground truth, handling format variations (e.g., "1/4" vs. "0.25").
- Core assumption: Judge LLMs can reliably assess semantic equivalence across domains.
- Evidence anchors:
  - [section 2.2] "This verification step is crucial for accurately filtering semantic equivalences, significantly reducing false negatives"
  - [appendix A] Judge prompt explicitly instructs to mark mathematically equivalent answers as correct "EVEN IF THE FORMAT IS DIFFERENT"
- Break condition: If judge LLM introduces systematic biases or domain-specific failures, verification quality degrades without detection.

## Foundational Learning

- Concept: **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: The entire Loong framework is designed to enable RLVR at scale; without understanding RL+verifiable rewards, the training loop motivation is opaque.
  - Quick check question: Can you explain why verifiable rewards (e.g., code execution match) enable more scalable RL than reward models trained on human preferences?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: Loong trains models to produce CoT solutions that align with code-executed answers; CoT is the reasoning format being optimized.
  - Quick check question: What is the difference between CoT prompting at inference time vs. training models to generate CoT as part of RLVR?

- Concept: **Synthetic Data Generation Strategies (Few-shot, Self-Instruct, Evol-Instruct)**
  - Why needed here: LoongEnv implements all three; understanding their tradeoffs is essential for configuring generation pipelines.
  - Quick check question: Which strategy would you select if maximizing diversity mattered more than yield? Which for highest executability?

## Architecture Onboarding

- Component map:
  - LoongBench (seed dataset) -> LoongEnv (synthetic generator) -> Verifier (LLM-as-judge) -> RL Loop (future work)

- Critical path:
  1. Domain selection -> ensures code-solvability
  2. Seed curation -> human-verified Q-A-code triples (Table 1 shows domain-specific dependencies like `sympy`, `networkx`, `QuantLib`)
  3. Question synthesis -> strategy selection (Few-shot/Self-Instruct/Evol-Instruct)
  4. Code generation -> must execute without errors
  5. Dual verification -> code output + judge agreement

- Design tradeoffs:
  - Few-shot: Highest pass rates (92.6% Logic, 93.9% Physics), lowest diversity—use for reliable pipeline testing
  - Self-Instruct: Moderate diversity, moderate rejection (44.8% in Logic)—use for balanced expansion
  - Evol-Instruct: Highest difficulty (62-70% model accuracy), lowest executability (55% failures in Logic)—use for training robustness, not benchmarking

- Failure signatures:
  - Non-executable code: syntax errors, missing dependencies, environment mismatches
  - Judge rejection: semantically mismatched answers despite correct code execution
  - Domain-specific gaps: Mathematical Programming shows ~10% accuracy across all models—likely due to long outputs exceeding 4096 token limit
  - Open-source model collapse: Qwen3-8B achieves only 39.2% on Logic vs. 61.6% for o3-mini

- First 3 experiments:
  1. Reproduce executability rates: Run LoongEnv on Logic and Physics domains with all three strategies; target Figure 2 distributions within ±5%
  2. Ablate judge vs. no-judge: Compare verification accuracy with and without LLM-as-judge; quantify false negative reduction
  3. Single-domain RL pilot: Train a small model (e.g., Qwen2.5-7B) on synthetic Evol-Instruct data from one domain (suggest Logic—highest pass rate with Few-shot); measure accuracy delta on held-out seed questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Loong framework effectively support reinforcement learning with verifiable rewards (RLVR) at scale using synthetically generated questions?
- Basis in paper: [explicit] "In future work, we plan to use this verification framework to support reinforcement learning (RL)... This setup will enable large-scale reinforcement learning with minimal human supervision while preserving semantic correctness through automated verification in the future."
- Why unresolved: The paper demonstrates synthetic data generation and benchmarking but does not implement or evaluate the actual RLVR training loop.
- What evidence would resolve it: Training experiments showing that models improved via RLVR on Loong-generated synthetic data, with comparisons to baselines and analysis of sample efficiency.

### Open Question 2
- Question: Will domain-specific verifiers provide significant improvements in reliability and efficiency over the current LLM-as-a-judge paradigm?
- Basis in paper: [explicit] "Note that while our current setup relies on the LLM-as-a-judge paradigm... we ultimately aim to develop domain-specific verifiers (e.g., for mathematics or programming), which are both more efficient and more reliable."
- Why unresolved: The paper acknowledges current reliance on LLM judges and potential for false positives, but does not implement or compare domain-specific verifiers.
- What evidence would resolve it: Comparative evaluation of domain-specific verifiers versus LLM-as-a-judge on metrics of accuracy, false positive/negative rates, and computational cost.

### Open Question 3
- Question: Can the synthetic data generation maintain high correctness and diversity across all domains when scaled beyond the evaluated 100 samples per strategy?
- Basis in paper: [inferred] The experiments use only 100 synthetic questions per domain per strategy, with significant variance in executability (e.g., Evol-Instruct produces 55% non-executable code in Logic; pass rates vary from 62% to 93.9% across domains and strategies).
- Why unresolved: Small-scale evaluation may not reveal failure modes at scale; some domains (e.g., Computational Biology with only 51 seed samples) have limited seed data for bootstrapping.
- What evidence would resolve it: Large-scale synthetic data generation experiments (thousands of samples per domain) with analysis of correctness, diversity, and domain coverage over time.

### Open Question 4
- Question: How will the framework handle multilingual and multimodal reasoning tasks?
- Basis in paper: [explicit] "We also plan to extend LOONG ENV with tool-augmented generation and formal abstraction, and scale LOONG-BENCH to cover multilingual and multimodal tasks."
- Why unresolved: Current implementation is text-only and English-focused; extending to multilingual or multimodal domains requires new verification mechanisms and potentially different code execution environments.
- What evidence would resolve it: Extended benchmarks covering multilingual and multimodal tasks, with demonstration of synthetic data generation and verification in these modalities.

## Limitations

- The paper demonstrates synthetic data generation but has not yet validated whether the synthetic data actually improves downstream reasoning performance through RL training—the RL loop is presented as future work rather than demonstrated capability.
- Domain coverage is limited to 12 reasoning-intensive domains where code-based solutions exist, potentially excluding many real-world reasoning scenarios.
- The reported accuracy gaps (e.g., ~10% for Mathematical Programming) may stem from undocumented technical constraints like token limits rather than inherent model limitations.

## Confidence

- **High confidence**: Code execution as verifiable reward mechanism, domain selection criteria, executability rates across strategies, benchmarking methodology
- **Medium confidence**: Synthetic data quality metrics (diversity, difficulty), judge LLM effectiveness, reproducibility of generation strategies
- **Low confidence**: Transfer of synthetic data to improved reasoning via RL, judge LLM accuracy and bias, generalizability beyond current domains

## Next Checks

1. Execute the complete RL training pipeline using synthetic data from one domain (e.g., Logic) and measure reasoning performance improvement on held-out seed questions vs. baseline models
2. Validate judge LLM accuracy by creating a ground-truth semantic equivalence test set and measuring false positive/negative rates across domains
3. Stress-test domain coverage by attempting to generate synthetic data for one excluded domain (e.g., creative writing) and document specific failure points in the code-execution verification pipeline