---
ver: rpa2
title: 'AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic
  Pretraining Corpus'
arxiv_id: '2512.18834'
source_url: https://arxiv.org/abs/2512.18834
tags:
- quality
- content
- source
- arabic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the redundancy problem in multilingual web
  corpora by leveraging cross-source agreement as a quality signal. Their MixMinMatch
  method combines existing Arabic, Turkish, and Hindi web datasets, performs cross-dataset
  MinHash deduplication, and retains documents recovered by multiple independent sources.
---

# AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus

## Quick Facts
- arXiv ID: 2512.18834
- Source URL: https://arxiv.org/abs/2512.18834
- Authors: Sultan Alrashed; Francesco Orabona
- Reference count: 26
- Primary result: 4.5% relative improvement on FineTasks benchmark using cross-source agreement

## Executive Summary
AraMix addresses the redundancy problem in multilingual web corpora by leveraging cross-source agreement as a quality signal. The MixMinMatch method combines existing Arabic, Turkish, and Hindi web datasets, performs cross-dataset MinHash deduplication, and retains documents independently recovered by multiple sources. Applied to Arabic, this approach produces AraMix-Matched, which achieves a 4.5% relative improvement over ArabicWeb24 (0.161 vs 0.154 aggregate FineTasks score) while using 54B tokens compared to ArabicWeb24's 41B. Crucially, gains persist even after removing content unique to the best baseline, demonstrating that cross-source agreement identifies high-quality content beyond what any single source provides.

## Method Summary
The MixMinMatch method addresses corpus quality through three stages: Mix (aggregate documents from multiple sources while preserving provenance), MinHash (cluster near-duplicates using MinHash signatures and LSH), and Match (retain only documents found by multiple independent sources). The approach requires no additional computation beyond standard deduplication, making it computationally efficient. Quality filtering is applied as a lightweight pre-processing step to remove obvious garbage before deduplication. The method exploits the fact that when multiple independent crawling and filtering pipelines retain the same document, this cross-source agreement serves as evidence of content quality.

## Key Results
- AraMix-Matched achieves 4.5% relative improvement over ArabicWeb24 baseline (0.161 vs 0.154 aggregate FineTasks score)
- Method uses 54B tokens compared to ArabicWeb24's 41B while achieving better performance
- Gains persist even after removing content unique to the best baseline, proving cross-source signal adds value beyond single sources
- 2-4× more unique tokens compared to baselines while maintaining higher quality

## Why This Works (Mechanism)

### Mechanism 1
Cross-source agreement between independently created datasets functions as a model-free quality signal. Each dataset is the product of different crawling and filtering pipelines. When multiple such pipelines independently retain the same document, this consensus is treated as an ensemble "vote" for the document's quality, analogous to inter-annotator agreement in crowdsourcing. The core assumption is that retention decisions are positively correlated with a latent quality variable and their error modes are not perfectly correlated.

### Mechanism 2
The quality signal can be extracted at negligible computational cost because it is a byproduct of standard deduplication. MinHash deduplication is already necessary to avoid training on repetitive data. The process clusters near-identical documents. A simple post-processing step on these clusters—checking if members come from different sources—yields the quality signal without requiring expensive model-based inference. The core assumption is that deduplication is a standard, non-negotiable part of the pretraining pipeline.

### Mechanism 3
A corpus filtered by cross-source agreement yields a better token-for-token return on model quality than any single source. The matched subset, though smaller than the full union, is a higher-precision collection of documents validated by multiple independent filters. Experiments show this higher quality per token translates to better downstream performance, often exceeding the best single-source baselines. The core assumption is that there is a tradeoff between data quantity and quality, and for a fixed training budget, a smaller, higher-quality corpus can outperform a larger, noisier one.

## Foundational Learning

- **Concept: MinHash and Locality-Sensitive Hashing (LSH) for Deduplication**
  - Why needed here: This is the core infrastructure. Understanding how MinHash creates document signatures and LSH clusters them into near-duplicate groups is fundamental to understanding where the cross-source signal is generated.
  - Quick check question: Can you explain how MinHash approximates Jaccard similarity and how LSH uses bands to efficiently group similar documents?

- **Concept: Ensemble Methods and the Wisdom of Crowds**
  - Why needed here: The paper's core theoretical justification is an application of ensemble learning principles. Understanding why aggregating independent "voters" reduces variance and improves accuracy is key to understanding why cross-source agreement works as a quality signal.
  - Quick check question: Why is the independence of the voters (i.e., datasets) a critical assumption for the ensemble to outperform its individual members?

- **Concept: Token Budget and Proportional Sampling**
  - Why needed here: The evaluation methodology is designed to isolate data quality effects. It requires training models for a fixed number of tokens, sampled proportionally from each source to preserve the mixture's natural characteristics.
  - Quick check question: When comparing two different pretraining corpora, why is it essential to train on an identical number of tokens?

## Architecture Onboarding

- **Component map**: Aggregator -> Quality Filter -> MinHash-LSH Deduplicator -> Cross-Source Matcher
- **Critical path**:
  1. Aggregate all source corpora, preserving source IDs
  2. Run the unified corpus through the lightweight quality filter
  3. Run the filtered corpus through the MinHash-LSH deduplication pipeline
  4. Post-process the deduplication output: a) Create a `MinHash` corpus (all representatives). b) Create a `Matched` corpus (representatives from multi-source clusters)
  5. Proportionally sample from the chosen corpus to a fixed token budget for training

- **Design tradeoffs**:
  - Quality vs. Quantity: The `Matched` corpus is smaller but higher quality. The `MinHash` corpus is larger but contains more unique, unvalidated data.
  - Source-count threshold: The paper uses a threshold of 2 but releases data with source counts for tuning.
  - Filter stringency: The pre-deduplication filter is intentionally lightweight.

- **Failure signatures**:
  - Minimal overlap: If pairwise overlap between source corpora is very low (<10-20%), the `Matched` set will be too small to be useful.
  - No performance gain: If the `Matched` corpus fails to outperform single-source baselines, it suggests sources are not independent or their "votes" are not well-correlated with downstream quality.
  - Gains disappear in ablation: If performance drops to baseline levels when the best single source is removed from the mixture, gains were not from cross-source signal.

- **First 3 experiments**:
  1. Quantify Overlap: Compute pairwise token overlap between candidate source corpora before any training.
  2. Train and Compare: Train small models on three corpora: best single-source baseline, full `MinHash` mixture, and `Matched` subset.
  3. Ablate the Best Source: Create version of `Matched` corpus excluding content unique to best baseline source and retrain.

## Open Questions the Paper Calls Out

- Would a weighted voting scheme, upweighting high-quality sources like ArabicWeb24 based on downstream performance or survival rate, improve over uniform cross-source matching?
- Do the quality gains from cross-source matching persist or amplify when scaling to larger models (7B+ parameters) and longer training runs?
- Does combining cross-source matching with model-based quality filtering yield additive or multiplicative improvements over either method alone?
- How does the optimal source-count threshold (2+ vs. 3+ vs. 4+) vary with the number and quality diversity of available sources per language?

## Limitations
- Method requires multiple, sufficiently overlapping source datasets to be viable
- Core claim (cross-source agreement as quality signal) is plausible but not proven universal
- Empirical results rest on limited downstream evaluations (FineTasks benchmark)
- Reliance on independence assumption between source datasets may not hold in all domains

## Confidence
- **High Confidence**: Computational mechanics of MixMinMatch method are well-specified and reproducible; token counts and relative improvements are reliable
- **Medium Confidence**: Theoretical claim that cross-source agreement is robust proxy for document quality is supported but not definitively proven
- **Medium Confidence**: Downstream performance results are specific to FineTasks benchmark and model architecture used; generalization is plausible but unconfirmed

## Next Checks
1. **Validate the Signal's Necessity**: Conduct ablation study by creating version of matched corpus excluding all content from best-performing source dataset, then retrain and evaluate.
2. **Test Robustness to Source Independence**: Systematically evaluate method's performance as independence of source datasets is reduced through synthetic overlap or derivative datasets.
3. **Generalize Beyond FineTasks**: Apply MixMinMatch to new language with multiple web corpora, then evaluate on diverse established benchmarks to test whether cross-source quality signal generalizes.