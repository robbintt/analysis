---
ver: rpa2
title: 'Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning:
  A Survey'
arxiv_id: '2509.24322'
source_url: https://arxiv.org/abs/2509.24322
tags:
- emotion
- multimodal
- recognition
- reasoning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first comprehensive survey of multimodal
  large language models (MLLMs) for emotion recognition and reasoning, covering model
  architectures, datasets, benchmarks, and experimental results. It introduces a new
  taxonomy that categorizes methods into parameter-frozen and parameter-tuning paradigms,
  analyzing zero-shot/few-shot learning and full-parameter/parameter-efficient tuning
  strategies.
---

# Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey

## Quick Facts
- **arXiv ID:** 2509.24322
- **Source URL:** https://arxiv.org/abs/2509.24322
- **Reference count:** 40
- **Primary result:** First comprehensive survey categorizing MLLM approaches for emotion recognition into parameter-frozen and parameter-tuning paradigms, showing affect-oriented instruction tuning and audio-visual fusion improve performance on benchmarks like DFEW and MME-Emotion.

## Executive Summary
This survey systematically examines the intersection of Multimodal Large Language Models (MLLMs) and emotion recognition/reasoning, introducing a novel taxonomy that categorizes methods into parameter-frozen and parameter-tuning paradigms. The authors analyze how different training strategies—zero-shot/few-shot learning versus full-parameter/parameter-efficient tuning—impact performance on multimodal emotion benchmarks. Experimental evaluations demonstrate that affect-oriented instruction tuning and sophisticated audio-visual fusion techniques significantly enhance model performance, though key challenges remain in fine-grained multimodal alignment, temporal reasoning, and efficient deployment. The work identifies promising future directions including hierarchical cross-modal alignment, trajectory-aware fusion, and deployment-friendly architectures for real-world affective intelligence applications.

## Method Summary
The survey conducts a comprehensive analysis of MLLMs for emotion recognition by examining 20+ models across multiple benchmarks (DFEW, MME-Emotion, MOSABench, etc.). The methodology involves categorizing approaches into parameter-frozen (zero-shot/few-shot prompting) and parameter-tuning (full-parameter/PEFT) paradigms, then analyzing performance metrics including accuracy, recall, F1-score, and reasoning-specific scores (Rec-S, Rea-S, CoT-S). The authors implement a connector-centric architecture analysis, comparing simple MLP projections versus sophisticated Q-Former and Cross-Attention modules for modality alignment. They evaluate both emotion classification and reasoning tasks, with particular attention to Chain-of-Thought rationales and causal sufficiency in multimodal understanding.

## Key Results
- Affect-oriented instruction tuning and audio-visual fusion significantly improve performance on DFEW and MME-Emotion benchmarks
- The parameter-tuning paradigm (especially PEFT methods like LoRA) consistently outperforms parameter-frozen approaches on complex reasoning tasks
- Current MLLMs struggle with fine-grained cross-modal alignment and temporal reasoning, particularly for asynchronous emotional cues
- Open-source models like Emotion-LLaMA and Video-LLaVA show competitive performance against proprietary APIs like GPT-4o and Gemini

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If modality-specific embeddings are aligned with the language model's token space via connectors, the LLM can process non-textual emotional cues (e.g., prosody, facial expressions) as semantic context.
- **Mechanism:** Modality encoders extract features from raw audio/video. Connectors (MLP, Q-Former, or Cross-Attention) transform these features into a dimension compatible with the LLM's embedding layer. The LLM then attends to these "visual/audio tokens" alongside text tokens to infer emotional states.
- **Core assumption:** The connector projection preserves the semantic richness of the original signal (e.g., pitch shifts or micro-expressions) without collapsing it into noise.
- **Evidence anchors:** [abstract] Mentions "audio-visual fusion significantly improve performance." [section 2.4] Describes the connector-centric design: "embeddings... are aligned with the token space of the language model through connector modules." [corpus] Neighbor paper "Pioneering Multimodal Emotion Recognition..." highlights the shift to open vocabularies, implying successful alignment is key to generalization.
- **Break condition:** Performance plateaus or degrades if the connector is too simple (e.g., linear projection) for complex temporal dependencies, or if modalities remain isolated (modality-specific optimization only).

### Mechanism 2
- **Claim:** Affect-oriented instruction tuning likely steers the model from generic semantic understanding to fine-grained emotional reasoning by mapping explicit rationales to labels.
- **Mechanism:** Instead of just predicting a label (e.g., "Happy"), the model is trained on datasets containing Chain-of-Thought (CoT) rationales (e.g., "The speaker's raised pitch and smiling eyes indicate happiness"). This reinforces the causal link between multimodal cues and the target emotion.
- **Core assumption:** The training dataset contains high-quality, human-verified reasoning chains that accurately reflect the multimodal evidence.
- **Evidence anchors:** [abstract] "Experimental evaluations show that affect-oriented instruction tuning... improve performance." [section 4.6] Mentions evaluation protocols include "rationale alignment" and "causal sufficiency tests." [corpus] "EmotionHallucer" paper highlights risks where models generate irrelevant content, suggesting tuning must target *grounded* reasoning to avoid hallucination.
- **Break condition:** If the instruction data is noisy or contradictory, the model learns spurious correlations, leading to confident but incorrect reasoning (hallucination).

### Mechanism 3
- **Claim:** Zero-shot learning in MLLMs relies on the pre-existing semantic alignment between affective concepts (learned during pre-training) and the target task's label space.
- **Mechanism:** The model uses its frozen parameters to match the input context (audio/video/text) to the verbalizer mapping of the prompt (e.g., mapping visual cues to the token "anger").
- **Core assumption:** The pre-training data contained sufficient examples of emotional concepts and cross-modal associations to generalize without gradient updates.
- **Evidence anchors:** [section 3.1] Defines the parameter-frozen paradigm: "task adaptation is realized purely through prompting." [abstract] Notes that "MLLMs have achieved notable progress" allowing some models to perform recognition "without any additional training data."
- **Break condition:** Fails when the target emotion is rare, fine-grained (beyond basic categories), or when cross-modal cues are asynchronous and conflict with the pre-training distribution.

## Foundational Learning

- **Concept: Cross-Modal Attention & Fusion**
  - **Why needed here:** MLLMs do not just concatenate features; they use attention mechanisms (e.g., Q-Former, Cross-Attention) to dynamically weigh the importance of text vs. audio vs. video at specific time steps.
  - **Quick check question:** Can you explain how a "Query" from the text modality might attend to a "Key/Value" from the audio modality to detect sarcasm?

- **Concept: Instruction Tuning (IT) vs. Fine-Tuning (FT)**
  - **Why needed here:** The paper distinguishes between updating weights (FT) and formatting data as instructions (IT). Understanding this distinction is critical for interpreting the "Parameter-Tuning" vs. "Parameter-Frozen" taxonomy.
  - **Quick check question:** Does "Parameter-Efficient Tuning" (like LoRA) update the entire model backbone or just injected adapter weights?

- **Concept: Tokenization of Continuous Signals**
  - **Why needed here:** Audio and Video are continuous high-dimensional signals. To be processed by an LLM, they must be discretized or projected into "soft tokens."
  - **Quick check question:** How does a video encoder (e.g., Video-LLaMA) convert a frame of a face into a sequence of tokens the LLM can read?

## Architecture Onboarding

- **Component map:** Input Encoders (Audio/Video/Text) -> Connectors (MLP/Q-Former/Cross-Attention) -> Frozen/Tuned LLM Backbone -> Verbalizer Output
- **Critical path:**
  1. **Preprocessing:** Temporal alignment of audio/video/text (synchronization).
  2. **Alignment:** Training the Connector (Projector) while keeping the LLM frozen (initially).
  3. **Adaptation:** Applying Affect-Instruction Tuning (full or PEFT) on specific emotion datasets (e.g., DFEW, MME-Emotion).
- **Design tradeoffs:**
  - **Frozen vs. Tuned:** Frozen backbones are cheaper and safer but struggle with subtle or conflicting emotions. Tuning (Full/PEFT) improves accuracy but risks catastrophic forgetting and hallucination (see "EmotionHallucer" in corpus).
  - **CoT vs. Direct Label:** Generating rationales (CoT) improves interpretability but increases inference latency and token cost.
- **Failure signatures:**
  - **Modality Ignorance:** Model relies solely on text transcript and ignores audio/video (common if connector alignment fails).
  - **Hallucination:** Model invents emotional cues not present in the video (e.g., "The subject is crying" when they are neutral).
  - **CoT Collapse:** The generated reasoning is logically sound but leads to the wrong label, or vice versa.
- **First 3 experiments:**
  1. **Zero-Shot Baseline:** Run a frozen SOTA model (e.g., Qwen2-VL or GPT-4o) on the test set using only simple prompts to establish a lower bound.
  2. **Connector Ablation:** Implement a simple MLP connector vs. a Q-Former connector. Train *only* the connector on the training set while keeping the LLM frozen. Compare alignment quality.
  3. **PEFT Instruction Tuning:** Apply LoRA to the LLM backbone using affect-oriented instruction data (labels + rationales). Measure performance gain (Rec-S / Rea-S) over the zero-shot baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can MLLM architectures be effectively modified to model long-range, asynchronous cross-modal temporal dependencies, such as delayed facial reactions or emotional inertia, which current standard self-attention mechanisms fail to capture?
- **Basis in paper:** [Explicit] Section 7.2 states that standard self-attention struggles to model "causal progression, emotional inertia... or anticipatory cues" and suggests exploring "state-space models or neural ODEs" or "causal transformer variants" as solutions.
- **Why unresolved:** Most existing MLLMs process inputs in fixed-length windows or rely on static fusion, ignoring temporal ordering and causal progression across heterogeneous sampling rates.
- **What evidence would resolve it:** The development of models incorporating explicit memory buffers or continuous latent trajectories that demonstrate superior performance on benchmarks with dense temporal annotations of emotional dynamics.

### Open Question 2
- **Question:** What specific alignment strategies are required to capture fine-grained, asynchronous affective cues (e.g., micro-expressions aligned with prosodic shifts) in a shared latent space to ensure cross-modal affective consistency?
- **Basis in paper:** [Explicit] Section 7.1 identifies "insufficient ability to achieve fine-grained cross-modal feature alignment" as a core limitation and proposes "Hierarchical Cross-modal Alignment" and "Trajectory-aware Contrastive Fusion" as necessary future directions.
- **Why unresolved:** Current connectors in models like OneLLM largely perform coarse, modality-specific alignment, failing to capture the nuanced interdependencies among subtle, heterogeneous signals.
- **What evidence would resolve it:** Models implementing continuous trajectory alignment that outperform discrete matching baselines on fine-grained emotion reasoning tasks requiring synchronization of local signals.

### Open Question 3
- **Question:** How can parameter-efficient tuning (PET) methods be specifically extended or specialized for affective features to balance high performance with the low computational cost required for real-time, on-device deployment?
- **Basis in paper:** [Explicit] Section 7.3 highlights the need to extend methods like LoRA and MoE to multimodal contexts and study "quantization, pruning, and hardware-aware compilation" to enable real-time inference on mobile and wearable devices.
- **Why unresolved:** While PET reduces costs, general methods may not preserve sensitivity to subtle affective states without domain-specific adaptation, and full-parameter tuning remains computationally prohibitive for widespread deployment.
- **What evidence would resolve it:** The successful deployment of quantized or MoE-based MLLMs on edge devices that maintain high accuracy on benchmarks like MME-Emotion with significantly reduced latency and memory usage.

## Limitations

- The survey's meta-analytical nature makes precise reproducibility challenging due to inconsistent documentation of exact prompt engineering and model versions across experiments
- The evaluation metrics for reasoning tasks may not be standardized across all referenced works, potentially introducing comparison bias
- The focus on English-language datasets limits generalizability to cross-cultural emotion expression differences
- The taxonomy boundaries between parameter-frozen and parameter-tuning paradigms are not always clear in practice, as many modern approaches blend these strategies

## Confidence

- **High Confidence:** The taxonomy of parameter-frozen vs. parameter-tuning paradigms, the identification of key datasets and benchmarks (DFEW, MME-Emotion), and the general architectural trends (connector modules, instruction tuning) are well-supported by multiple sources and direct citations.
- **Medium Confidence:** The specific performance numbers in Tables 2-8, particularly for closed-source models, may vary due to API drift and version changes. The claim that affect-oriented instruction tuning consistently improves performance is supported but could benefit from more controlled ablation studies.
- **Low Confidence:** The assertion that hierarchical cross-modal alignment will be the "next breakthrough" is speculative, as the survey does not provide empirical evidence for this specific architecture beyond theoretical discussion.

## Next Checks

1. **Connector Ablation Study:** Implement and compare three connector variants (simple MLP, Q-Former, Cross-Attention) on the same base model and dataset (e.g., DFEW). Measure not just accuracy but also the quality of multimodal attention maps to verify that complex connectors actually improve cross-modal understanding rather than just fitting noise.

2. **Prompt Engineering Replication:** Take a single model (e.g., Emotion-LLaMA) and systematically vary the prompt structure (zero-shot vs. few-shot, with/without CoT, different verbalizer mappings) while keeping all other factors constant. This would isolate the impact of prompting on the parameter-frozen paradigm and validate whether the survey's conclusions about prompt effectiveness are robust.

3. **Cross-Cultural Generalization Test:** Evaluate the best-performing models from the survey on emotion recognition datasets from different cultural contexts (e.g., Asian Affect Expression Database or similar resources). This would test whether the current MLLM approaches truly capture universal emotional patterns or are biased toward Western expression norms.