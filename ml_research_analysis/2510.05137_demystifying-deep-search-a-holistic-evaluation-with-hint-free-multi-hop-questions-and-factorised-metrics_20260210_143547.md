---
ver: rpa2
title: 'Demystifying deep search: a holistic evaluation with hint-free multi-hop questions
  and factorised metrics'
arxiv_id: '2510.05137'
source_url: https://arxiv.org/abs/2510.05137
tags:
- knowledge
- reasoning
- evidence
- search
- multi-hop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics

## Quick Facts
- **arXiv ID**: 2510.05137
- **Source URL**: https://arxiv.org/abs/2510.05137
- **Reference count**: 38
- **Primary result**: Introduces WebDetective benchmark with hint-free multi-hop questions and factorized metrics to diagnose distinct failure modes in web search agents

## Executive Summary
This paper introduces WebDetective, a benchmark designed to evaluate web agents on genuinely autonomous multi-hop reasoning by eliminating linguistic scaffolding from questions. Unlike existing benchmarks that leak reasoning paths through question text, WebDetective forces agents to discover reasoning chains independently through a controlled Wikipedia sandbox with masked intermediate entities. The authors also propose a factorized evaluation framework that separates knowledge sufficiency, knowledge utilization, and good refusal behaviors to provide more granular diagnostic insights than traditional pass-rate metrics.

The core contribution is EvidenceLoop, an iterative agentic framework that combines parallel exploration, structured evidence memory with unique identifiers, and verification agents to improve knowledge utilization. Experiments show that while agents can often achieve knowledge sufficiency (finding all necessary evidence), they frequently fail to synthesize this information into correct answers—a degradation the authors attribute primarily to forgetting available evidence during generation rather than being led astray by incorrect paths.

## Method Summary
The WebDetective benchmark uses 200 hint-free multi-hop questions constructed through a three-stage LM verification process that ensures questions are parametrically inaccessible, have sufficient evidence, and require evidence for correct answers. The controlled Wikipedia sandbox masks intermediate entities so that v_i is discoverable only by visiting page(v_{i-1}). The EvidenceLoop framework implements iterative refinement with N parallel solvers, persistent evidence memory assigning unique IDs to retrieved content, and verification agents that validate atomic claims against EIDs before accepting answers. The factorized evaluation framework computes Knowledge Score (sufficiency), Search Score (retrieval efficiency), and Generation Score (F1 of Good Refusal plus Knowledge Utilization weighted by sufficiency).

## Key Results
- Agents achieve knowledge sufficiency on 49.55% of questions but only convert this to correct answers 22.45% of the time, showing significant knowledge utilization gap
- EvidenceLoop improves knowledge utilization F1 by 53% relative to baseline DeepSeek-R1 by addressing context-induced reasoning degradation
- Factorized metrics reveal that failures are more often due to not using available evidence (forgetting) than being led astray, with a mean difference of +10.35 percentage points
- Search Score degrades by 8.5% when EID memory is removed, confirming structured memory as critical for mitigating knowledge-related shortages

## Why This Works (Mechanism)

### Mechanism 1: Hint-Free Question Design Forces Autonomous Reasoning Path Discovery
Removing linguistic scaffolding from questions exposes whether models can genuinely discover reasoning chains versus executing prescribed steps. By eliminating path-hinting (explicit reasoning chain narration) and specification-hinting (attribute fingerprints that uniquely identify entities), the benchmark ensures agents must independently determine which connections matter rather than following embedded cues. This diagnostic approach reveals whether high performance on hint-containing benchmarks reflects true reasoning capability or pattern-matching on surface cues.

### Mechanism 2: Factorized Metrics Enable Targeted Failure Mode Attribution
The framework separates whether agents possess necessary evidence (search capability) from whether they can synthesize it (generation quality), and distinguishes appropriate from inappropriate refusal behavior through conditional F1 scores weighted by knowledge sufficiency. This decomposition treats failures in search, synthesis, and calibration as architecturally distinct, revealing that high retrieval does not translate proportionally into better synthesis or final accuracy.

### Mechanism 3: EvidenceLoop's Structured Memory and Verification Reduce Synthesis Degradation
Explicit evidence tracking with unique identifiers and verification loops improves knowledge utilization by preventing context-induced reasoning degradation. The EID system maintains traceability between claims and sources; verification agents reject proposals lacking evidence grounding, forcing solvers to repair reasoning within remaining action budgets. This addresses the primary bottleneck of forgetting available evidence during synthesis rather than insufficient search capability.

## Foundational Learning

- **Multi-hop reasoning chain decomposition**: Questions decompose into evidence chains (v0 → v1 → ... → vn) where each hop must be discovered independently. Why needed: The benchmark requires understanding that questions decompose into evidence chains where each hop must be discovered independently. Quick check: Given "Who is the father of Kane Cornes?", can you trace at least one valid multi-hop reasoning path versus identifying what makes a direct retrieval approach impossible here?

- **Knowledge sufficiency vs. generation quality separation**: The evaluation framework treats evidence acquisition and answer synthesis as independent capabilities. Why needed: The evaluation framework treats evidence acquisition and answer synthesis as independent capabilities; understanding this is essential for interpreting factorized metrics. Quick check: If an agent retrieves all necessary evidence but produces an incorrect answer, which metric(s) would indicate this failure mode?

- **Calibrated refusal behavior**: The benchmark introduces Good Refusal F1 to measure whether models appropriately abstain when evidence is lacking versus refusing when they could answer. Why needed: The benchmark introduces Good Refusal F1 to measure whether models appropriately abstain when evidence is lacking versus refusing when they could answer. Quick check: What is the difference between a model that refuses 90% of questions and one with 90% Good Refusal precision but 50% recall?

## Architecture Onboarding

- **Component map**: WebDetective benchmark (200 hint-free questions) → EvidenceLoop framework (iterative exploration + structured memory + verification) → Factorized evaluation (Knowledge Score + Search Score + Generation Score)

- **Critical path**: Question construction through 3-stage LM verification and human validation → Sandbox setup with entity masking along reference paths → Agent evaluation tracking visited pages and computing factorized metrics

- **Design tradeoffs**: Controlled sandbox eliminates shortcuts but may limit generalization to noisy real-world environments; reference path acceptance accommodates valid alternative reasoning strategies while maintaining diagnostic precision; EID overhead increases implementation complexity but improves verification accuracy

- **Failure signatures**: Premature Search Termination (learned helplessness after failed searches); Context-Induced Instruction Degradation (format compliance degrades as context accumulates); Redundant Search Loops (re-visiting explored pages wastes action budget); Evidence Tracking Failure (cannot distinguish "not found due to masking" from "haven't searched yet")

- **First 3 experiments**: Baseline characterization on standard ReAct agent to identify dominant failure mode profile; Memory ablation comparing EvidenceLoop with and without EID-based structured memory; Test-time scaling robustness varying context window and breadth/iteration settings

## Open Questions the Paper Calls Out

None

## Limitations

- Benchmark's diagnostic power depends on whether hint-free design genuinely isolates autonomous reasoning versus alternative performance shortcuts
- Factorized metric framework assumes orthogonal failure modes, but evidence shows high correlation between search and synthesis failures
- EvidenceLoop's memory overhead and verification steps introduce significant computational costs (40 tool calls per question) that may limit practical deployment

## Confidence

**High Confidence**: The benchmark construction methodology (3-stage LM verification, human validation) produces reliable hint-free questions. The factorized metric framework meaningfully distinguishes search from synthesis failures based on observable behavioral patterns.

**Medium Confidence**: EvidenceLoop's memory and verification components provide measurable improvements in knowledge utilization. The claim that forgetting dominates degradation over being led astray is supported by controlled experiments but requires broader validation.

**Low Confidence**: The generalization of hint-free question performance to open web environments. The assumption that current multi-hop benchmarks overestimate agent capabilities due to path leakage, as this requires extensive comparison across diverse benchmarks not provided in the paper.

## Next Checks

1. **Open Web Generalization Test**: Run the same evaluation pipeline on standard multi-hop benchmarks (HotpotQA, 2WikiMultiHopQA) without sandbox constraints to measure performance degradation and validate that hint-free design captures genuinely different capabilities.

2. **Correlation Analysis Across Failure Modes**: Conduct systematic analysis of failure mode interdependencies across the full 200-question dataset, testing whether Knowledge Score and Search Score correlations persist across different agent architectures and question difficulty levels.

3. **Memory Overhead vs. Performance Trade-off**: Implement ablation studies varying EID memory capacity and verification frequency to quantify the marginal returns of EvidenceLoop's complexity, establishing computational budget constraints for practical deployment.