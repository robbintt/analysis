---
ver: rpa2
title: 'Beyond Literal Token Overlap: Token Alignability for Multilinguality'
arxiv_id: '2502.06468'
source_url: https://arxiv.org/abs/2502.06468
tags:
- language
- transfer
- ru-zh
- bg-ru
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces token alignability as a metric for understanding
  cross-lingual knowledge transfer in multilingual language models. While previous
  metrics like literal token overlap and distributional similarity are limited for
  language pairs with different scripts, token alignability captures statistical correspondences
  between subword tokens.
---

# Beyond Literal Token Overlap: Token Alignability for Multilinguality

## Quick Facts
- **arXiv ID:** 2502.06468
- **Source URL:** https://arxiv.org/abs/2502.06468
- **Reference count:** 40
- **Primary result:** Token alignability better predicts cross-lingual transfer performance than literal overlap, especially for different-script language pairs

## Executive Summary
This paper introduces token alignability as a metric for understanding cross-lingual knowledge transfer in multilingual language models. While previous metrics like literal token overlap and distributional similarity are limited for language pairs with different scripts, token alignability captures statistical correspondences between subword tokens. The authors demonstrate that their proposed eflomal score for token alignability better predicts downstream cross-lingual transfer performance than distributional overlap, especially for language pairs with different scripts. They also show correlations between token alignability and cross-lingual embedding alignment in both encoder and decoder models. The findings suggest that token alignability can guide the development of more effective multilingual tokenizers and help identify optimal language pairs for cross-lingual transfer.

## Method Summary
The method computes token alignability using statistical word alignment (eflomal) on parallel corpora to measure how readily subword tokens in one language map to tokens in another. For each language pair, the authors train eflomal priors on up to 300k parallel sentence pairs from OPUS-100 and MultiCCAligned, then apply single-iteration alignment to tokenized FLORES-200 sentences. They extract the eflomal score (log-probability of links) and 1-1 alignment proportion, comparing these to distributional overlap (JSD) and correlating both metrics with downstream transfer performance (XNLI, POS, UD, NER) and cross-lingual embedding alignment. The study evaluates three tokenizer types (BPE, Unigram, TokMix) across 20 languages and multiple multilingual models including encoders (XLM-R variants) and decoders (Mistral-7B, Aya23-8B, Llama-3-8B).

## Key Results
- Token alignability (eflomal score) shows stronger Spearman correlations with word-level cross-lingual transfer performance than distributional overlap (JSD), particularly for language pairs with different scripts
- The metric better predicts transfer performance for POS tagging, UD parsing, and NER tasks compared to XNLI sentence classification
- Token alignability correlates with cross-lingual embedding alignment in encoder models, but the relationship reverses for decoder models where JSD outperforms eflomal

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token alignability captures cross-lingual semantic correspondences that literal token overlap misses, enabling better prediction of transfer performance for different-script language pairs.
- **Mechanism:** Statistical word alignment (eflomal) learns translation correspondences between subword tokens from parallel corpora. The resulting alignment score reflects how readily tokens in one language map to tokens in another, independent of surface form similarity. This captures the "statistical correspondences between subword tokens" that models may exploit.
- **Core assumption:** Models leverage alignment-like correspondences during cross-lingual transfer, even without explicit alignment supervision.
- **Evidence anchors:** [abstract] "token alignability captures statistical correspondences between subword tokens"; [section 1] "This concept captures the intuition that models may rely on statistical correspondences between subword tokens ('token alignment') that are more nuanced than literal string matching"; [corpus] Related work on vocabulary overlap (False Friends paper) shows mixed evidence on whether token overlap helps or hurts transfer—suggesting the relationship is more nuanced than literal overlap.
- **Break condition:** If models rely primarily on surface-form features rather than learned correspondences, alignability would lose predictive power for same-script pairs (which the data partially supports—JSD outperforms eflomal for same-script pairs in some embedding alignment metrics).

### Mechanism 2
- **Claim:** Token alignability predicts downstream cross-lingual transfer performance more reliably than distributional overlap (JSD), particularly for word-level classification tasks.
- **Mechanism:** Better token alignability implies the tokenizer produces vocabulary items that naturally correspond across languages. During fine-tuning on a source language, learned representations can map to semantically similar target-language tokens because the alignment structure already exists at the tokenization level.
- **Core assumption:** Token-level alignment quality constrains the upper bound of cross-lingual transfer, independent of model capacity or training data size.
- **Evidence anchors:** [section 4.1, Table 1] Eflomal shows stronger Spearman correlations with transfer performance than JSD across POS (r=-0.64 vs -0.45 for Unigram), UD, and NER tasks; [section 4.1] "JSD clusters language pairs with different scripts very closely together, even when they have markedly different transfer performance"; [corpus] ATLAS scaling laws paper suggests multilingual transfer involves complex interactions between data, model size, and language similarity—alignability may capture one dimension of this.
- **Break condition:** The correlation weakens substantially for sentence-level tasks (XNLI showed inconsistent results), suggesting the mechanism operates primarily at sub-sentence granularity.

### Mechanism 3
- **Claim:** Token alignability correlates with cross-lingual embedding alignment in trained models, but the relationship differs between encoder and decoder architectures.
- **Mechanism:** Tokenizers with better alignability produce embedding spaces that require less distortion to align during pre-training. For encoders, middle layers (layer 7 in experiments) show this alignment. For decoders, the relationship is less consistent—possibly because decoders process language differently or rely more on literal matches when available.
- **Core assumption:** Embedding alignment causally influences transfer capability, rather than both being epiphenomena of shared training data.
- **Evidence anchors:** [section 3.4] "middle layers in XLM-R and similar encoder models...have been found to be more cross-lingually aligned than the output layers"; [section 4.3, Table 4] Mistral shows strong eflomal-embedding correlation (r=-0.59), but Aya23 and Llama3 show JSD outperforming eflomal; [corpus] Weak corpus support—related papers focus on transfer mechanisms rather than tokenizer-embedding relationships.
- **Break condition:** Decoder model behavior diverges significantly (JSD outperforms eflomal for Aya23 and Llama3), suggesting architectural differences in how cross-linguality emerges.

## Foundational Learning

- **Concept: Statistical Word Alignment**
  - **Why needed here:** The eflomal aligner is the core tool for computing token alignability. Understanding that it produces discrete token-to-token mappings (not attention-like soft alignments) is essential.
  - **Quick check question:** Given parallel sentences in Hindi and Urdu, would eflomal produce higher alignment scores than JSD-based distance? Why?

- **Concept: Subword Tokenization (BPE vs Unigram)**
  - **Why needed here:** The paper tests three tokenizer types (BPE, Unigram, TokMix). Understanding how each creates vocabularies helps interpret why alignability effects persist across tokenization strategies.
  - **Quick check question:** If a Unigram tokenizer segments "understanding" as "under+stand+ing" and BPE uses "under+standing," which would likely show better alignability with a language using different morphology?

- **Concept: Jensen-Shannon Divergence (JSD)**
  - **Why needed here:** JSD is the baseline metric being improved upon. Understanding it measures distributional distance helps clarify why it fails for different-script pairs.
  - **Quick check question:** Why would JSD assign similar distances to Hindi-Urdu (related languages, different scripts) as to Hindi-Chinese (unrelated, different scripts)?

## Architecture Onboarding

- **Component map:** Parallel Corpus (FLORES-200) -> [Tokenizer under evaluation: BPE/Unigram/TokMix] -> Tokenized parallel sentences → eflomal aligner (trained on OPUS-100/MultiCCAligned priors) -> Alignment output → Eflomal score (log-probability of links) + 1-1 proportion -> Correlation analysis → Spearman ρ vs. downstream transfer & embedding alignment

- **Critical path:** The eflomal score computation is the bottleneck. Training priors requires ~300k sentence pairs per language pair; inference on FLORES-200 requires one alignment pass. Computing alignability for many pairs accumulates quickly.

- **Design tradeoffs:**
  - **Eflomal score vs. 1-1 proportion:** Eflomal captures nuanced alignment probability; 1-1 proportion is interpretable but too simplistic (weaker correlations in Table 1)
  - **FLORES-200 vs. training corpus for metric computation:** Using the same corpus for metrics and embedding evaluation inflates correlations (Section 4.1 notes this acts as an "upper bound")
  - **Encoder vs. decoder evaluation:** Decoders show inconsistent results—computational investment in decoder analysis may not yield reliable guidance

- **Failure signatures:**
  - XNLI correlations are weak/inconsistent across all metrics—sentence-level tasks may not be suitable for this analysis
  - Same-script pairs sometimes show stronger JSD correlations than eflomal (Table 2, Tatoeba)—literal overlap still carries signal when available
  - Decoder models (Aya23, Llama3) show reversed patterns—architectural differences matter

- **First 3 experiments:**
  1. **Reproduce core correlation on a new language pair:** Compute eflomal score for a held-out pair (e.g., Bengali-Assamese, same script, related) and predict transfer performance. Validates the metric generalizes beyond the 20-language test set.
  
  2. **Ablate training data for alignment priors:** Train eflomal with 10k, 50k, 100k, 300k sentence pairs. Determine minimum viable corpus size—critical for low-resource language applications.
  
  3. **Test tokenizer vocabulary construction with alignability guidance:** For a new multilingual tokenizer, compute alignability delta when adding/removing candidate tokens. This tests the paper's proposed future application directly, though the authors note naive implementation is computationally intensive.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can token alignability be efficiently approximated to guide vocabulary learning in multilingual tokenizers without the prohibitive computational cost of checking every candidate token?
- **Basis in paper:** [explicit] Section 5 states that a naive implementation is "far too intensive" and explicitly calls for "finding suitable approximations," such as calculating scores for a fraction of candidate tokens.
- **Why unresolved:** Computing alignment scores for every potential merge (BPE) or pruning step (Unigram) during tokenizer training is currently too computationally expensive to be practical.
- **What evidence would resolve it:** A proposed algorithm or sampling strategy that reduces the time complexity of integrating alignability into tokenizer training while maintaining a strong correlation with downstream transfer performance.

### Open Question 2
- **Question:** How can the token alignability metric be reformulated to apply to word-level tasks rather than functioning solely as a corpus-wide score?
- **Basis in paper:** [explicit] The "Limitations" section explicitly states that "In its present formulation, alignability is also a corpus-wide score, meaning it would require reformulating for word-level tasks."
- **Why unresolved:** The current methodology aggregates statistics over an entire corpus, lacking the granularity to assess the alignability of specific words or subword units independently.
- **What evidence would resolve it:** A modified mathematical definition of the metric that provides valid scores for individual tokens or words, validated against word-level cross-lingual transfer benchmarks.

### Open Question 3
- **Question:** Why does token alignability correlate well with cross-linguality in Mistral but poorly in Aya23 and Llama3, where distributional overlap (JSD) is more predictive?
- **Basis in paper:** [inferred] Section 4.3 notes that results for decoder models are "mixed" and "may suggest that cross-linguality in these decoder models works differently," specifically regarding reliance on literal matches versus statistical correspondences.
- **Why unresolved:** The study observes the discrepancy but does not isolate whether the cause is the model architecture, the specific pre-training data composition, or the degree of multilinguality in fine-tuning.
- **What evidence would resolve it:** A controlled ablation study analyzing embedding alignment across decoder models trained with controlled data compositions to isolate the mechanism of cross-lingual transfer.

## Limitations
- Computational intensity of alignment priors training (up to 300k sentence pairs per language pair) makes large-scale application impractical
- Inconsistent correlations with sentence-level tasks (XNLI) suggest mechanism operates primarily at sub-sentence granularity
- Reversed correlation patterns for decoder models (Aya23, Llama3) indicate architectural differences not fully explained

## Confidence
- **High Confidence:** Token alignability (eflomal score) better predicts downstream cross-lingual transfer performance than distributional overlap (JSD) for word-level classification tasks, particularly for language pairs with different scripts
- **Medium Confidence:** Token alignability correlates with cross-lingual embedding alignment in encoder models, with stronger effects in middle layers
- **Low Confidence:** Token alignability can guide the development of more effective multilingual tokenizers by identifying vocabulary construction strategies that improve cross-lingual correspondence

## Next Checks
1. **Held-out corpus validation:** Compute eflomal alignability scores using FLORES-200 for metric calculation but evaluate downstream transfer performance on a completely separate corpus (e.g., Tatoeba for XNLI tasks). This would test whether correlations hold when avoiding corpus overlap and better reflect real-world applicability.

2. **Minimum viable training corpus size:** Systematically vary the size of alignment priors training data (10k, 50k, 100k, 300k sentence pairs) and measure the impact on eflomal score quality and downstream correlation strength. This would determine the practical lower bound for meaningful alignment training and enable application to truly low-resource language pairs.

3. **Architecture-specific ablation study:** For both encoder and decoder models, ablate the alignment mechanism by training with and without explicit alignment supervision (similar to XLM-R's parallel data approach) and measure how this affects the relationship between token alignability and transfer performance. This would clarify whether models genuinely leverage alignment-like correspondences or whether the observed correlations are epiphenomenal.