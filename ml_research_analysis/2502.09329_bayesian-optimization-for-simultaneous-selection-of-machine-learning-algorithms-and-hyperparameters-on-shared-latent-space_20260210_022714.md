---
ver: rpa2
title: Bayesian Optimization for Simultaneous Selection of Machine Learning Algorithms
  and Hyperparameters on Shared Latent Space
arxiv_id: '2502.09329'
source_url: https://arxiv.org/abs/2502.09329
tags:
- space
- algorithm
- latent
- ranking
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Combined Algorithm Selection and Hyperparameter
  optimization (CASH) problem, where the goal is to efficiently select the optimal
  combination of a machine learning algorithm and its hyperparameters. The main challenge
  lies in the fact that different algorithms have different hyperparameter spaces,
  making it difficult to share information across algorithms.
---

# Bayesian Optimization for Simultaneous Selection of Machine Learning Algorithms and Hyperparameters on Shared Latent Space

## Quick Facts
- **arXiv ID:** 2502.09329
- **Source URL:** https://arxiv.org/abs/2502.09329
- **Reference count:** 40
- **Primary result:** Proposed method achieves superior rankings and validation accuracy in CASH problems compared to existing AutoML methods.

## Executive Summary
This paper tackles the Combined Algorithm Selection and Hyperparameter optimization (CASH) problem by proposing a novel Bayesian optimization framework that embeds different algorithm-specific hyperparameter spaces into a shared latent space. This shared latent space allows a single surrogate multi-task model to efficiently share information across observations from different machine learning algorithms. The method includes pre-training embeddings with adversarial regularization to encourage overlap among different algorithm spaces and a ranking model to select effective pre-trained embeddings for new target datasets. Evaluation on OpenML datasets demonstrates the approach's effectiveness compared to existing AutoML methods.

## Method Summary
The method consists of three main components: First, Multi-Layer Perceptrons (MLPs) are trained to embed algorithm-specific hyperparameters into a shared latent space using a quadratic surface loss combined with adversarial regularization. Second, a LightGBM LambdaMART ranking model is trained on meta-features to select the best pre-trained embedding model for a target dataset. Third, during optimization, a Multi-Task Gaussian Process (MTGP) with a deep kernel is used as the surrogate model in the shared latent space, and the optimization process fine-tunes only the last MLP layer. The method is evaluated on OpenML classification datasets with 12 candidate ML algorithms.

## Key Results
- The proposed method achieves superior rankings compared to baselines (Random, SMAC) across all evaluated datasets.
- The method demonstrates improved validation accuracy throughout the optimization process.
- Ablation study confirms the benefits of pre-training and selecting appropriate pre-trained embedding models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Embedding heterogeneous hyperparameter (HP) spaces into a shared latent space may allow a single surrogate model to share information across different machine learning algorithms.
- **Mechanism:** Distinct algorithm-specific HP spaces are mapped to a shared space via Multi-Layer Perceptrons (MLPs). A Multi-Task Gaussian Process (MTGP) is then trained on this shared space rather than on isolated spaces, allowing observations from one algorithm to inform the optimization of others if they map to similar latent regions.
- **Core assumption:** A common latent representation can capture "model flexibility" across structurally different algorithms.
- **Evidence anchors:** Proposes embedding different HP spaces into a shared latent space for efficient information sharing; describes use of MTGP on latent variables to share observations across algorithms; general BO literature confirms multi-task learning improves sample efficiency.
- **Break condition:** If the relationship between HPs and performance is fundamentally algorithm-specific, the latent model will over-smooth predictions, leading to false optima.

### Mechanism 2
- **Claim:** Pre-training embeddings with adversarial regularization likely ensures that different algorithm spaces overlap in the latent space, facilitating transfer learning.
- **Mechanism:** During pre-training, a discriminator tries to identify which algorithm generated a specific latent vector. The embedding MLPs are trained to fool this discriminator, forcing the distributions of different algorithms to align in the shared space.
- **Core assumption:** High overlap in latent space correlates with the ability to transfer optimization knowledge.
- **Evidence anchors:** Introduces adversarial regularization to encourage overlap and prevent observations from being isolated; contrasts this with concatenated spaces that require theoretical justification for default values.
- **Break condition:** If the adversarial loss dominates the reconstruction/quadratic-fit loss, the semantic meaning of HPs may be lost (mode collapse), making the latent space uninformative.

### Mechanism 3
- **Claim:** A ranking model using meta-features can select a pre-trained embedding that accelerates optimization for a new target dataset.
- **Mechanism:** The system pre-trains embeddings on multiple source datasets. For a target dataset, it extracts meta-features and uses a Learning-to-Rank model (LightGBM) to select the pre-trained weights most likely to yield high accuracy quickly.
- **Core assumption:** Dataset similarity in meta-feature space implies similarity in optimal hyperparameter landscapes.
- **Evidence anchors:** Describes the ranking model for recommending effective pre-trained embeddings; mentions the ranking model as a key component for handling target datasets; related work supports the use of meta-features for algorithm selection.
- **Break condition:** If the target dataset is an outlier compared to source datasets, the selected embedding will provide a poor initialization, potentially performing worse than random search.

## Foundational Learning

- **Concept: Multi-Task Gaussian Process (MTGP)**
  - **Why needed here:** This is the core surrogate model. Unlike standard GPs which model one function, MTGPs model correlations between multiple tasks (algorithms) to share data.
  - **Quick check question:** How does the kernel allow a prediction for SVM to be updated using a Random Forest observation?

- **Concept: Adversarial Domain Adaptation**
  - **Why needed here:** This underpins the pre-training. You must understand the "discriminator vs. generator" dynamic to debug why embeddings might be failing to overlap.
  - **Quick check question:** In this context, what represents the "domain" that the discriminator is trying to classify, and why do we want it to fail?

- **Concept: Expected Improvement (EI)**
  - **Why needed here:** This is the acquisition function driving the search. It balances exploitation (high predicted mean) and exploration (high uncertainty).
  - **Quick check question:** Why is simply picking the maximum predicted accuracy from the surrogate model a bad strategy for the next evaluation point?

## Architecture Onboarding

- **Component map:** Input Dataset + Historical Meta-Data -> Meta-Selector (Ranking model) -> Encoder (Selected MLPs) -> Surrogate (MTGP) -> Optimizer (EI maximization)

- **Critical path:** The efficacy relies on the Ranking Model selecting a relevant Pre-Trained Embedding Model (PTEM). If the wrong embedding is loaded, the MTGP operates on a distorted map where distance implies nothing about performance similarity.

- **Design tradeoffs:**
  - **Latent Dimension:** Too low loses HP distinctions; too high prevents overlap (curse of dimensionality). Paper uses 3.
  - **Pre-training vs. Online Learning:** Pre-training costs are amortized, but online fine-tuning is crucial to correct ranker errors.

- **Failure signatures:**
  - **Collapsed Embedding:** Gradients vanish because the adversarial regularizer forced all algorithms to the same point; check discriminator accuracy (should be near random chance, not 0).
  - **Stagnant Search:** EI stays flat; suggests the surrogate uncertainty is underestimated or the latent space is uninformative.

- **First 3 experiments:**
  1. **Ablation on Overlap:** Visualize latent spaces (t-SNE) with/without adversarial regularization to verify algorithm clusters are overlapping but not identical.
  2. **Ranker Sanity Check:** Compare "Ranker-selected" embeddings vs. "Randomly-selected" embeddings on a hold-out set of datasets.
  3. **Cold Start Performance:** Evaluate the first 10-20 iterations (where pre-training matters most) against a baseline BO without shared latent spaces to measure sample efficiency gains.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework be extended to explicitly handle heterogeneous computational costs across different algorithms during optimization?
- **Basis in paper:** Section 5.4 notes that current evaluation assumes uniform cost for fairness, but states "One possible approach is to estimate the cost itself from observations, and incorporate it into the acquisition function."
- **Why unresolved:** The current acquisition function maximizes accuracy purely based on iteration count, ignoring significant wall-clock time differences between algorithms.
- **What evidence would resolve it:** An extension using a cost-aware acquisition function that demonstrates superior wall-clock performance compared to the current iteration-based approach.

### Open Question 2
- **Question:** Can recent techniques for mitigating the "cycle consistency problem" in latent variable models be integrated to allow acquisition function maximization directly in the latent space?
- **Basis in paper:** Section 4 states that the current method maximizes the acquisition function in the original space to avoid the "cycle consistency problem," but suggests "Combining recent techniques mitigating this problem... is a possible future direction."
- **Why unresolved:** Optimizing in the original space avoids inconsistencies but may be less efficient than optimizing directly in the continuous latent space.
- **What evidence would resolve it:** A modified framework employing latent consistency constraints that successfully optimizes the acquisition function in the latent space while maintaining mapping validity.

### Open Question 3
- **Question:** Does the strict quadratic function approximation used in pre-training limit the surrogate model's ability to capture complex, multi-modal performance landscapes?
- **Basis in paper:** Section 3.2.1 assumes the objective function can be approximated by a simple quadratic surface for simplicity.
- **Why unresolved:** Enforcing a unimodal quadratic shape may create a prior that is too restrictive or biased for datasets where optimal hyperparameters are clustered in distinct, separate regions.
- **What evidence would resolve it:** An ablation study comparing the quadratic prior against flexible function approximators on synthetic tasks with known multi-modal optima.

## Limitations

- The method assumes that fundamentally different ML algorithms share meaningful structure in hyperparameter-performance landscapes, which may not hold universally.
- The adversarial regularization's balance is critical but sensitive, and if poorly tuned, the semantic mapping between hyperparameters and performance may collapse.
- The meta-feature-based ranking model assumes dataset similarity implies similar optimal hyperparameter landscapes, which may fail for outlier datasets.

## Confidence

- **Latent Space Sharing:** Medium - Theoretically sound but specific instantiation lacks extensive validation beyond OpenML experiments.
- **Adversarial Regularization:** Medium - The mechanism is well-established but the specific application to CASH is novel and untested.
- **Meta-Feature Ranking:** Medium - Assumes dataset similarity in meta-feature space implies similarity in optimal hyperparameter landscapes, which may not always hold.

## Next Checks

1. **Latent Space Overlap Validation:** Visualize the latent space (t-SNE/PCA) for a subset of algorithms during pre-training with and without adversarial regularization to verify that algorithm clusters overlap as intended rather than remaining isolated.

2. **Cold Start Performance Benchmark:** Evaluate the first 10-20 BO iterations (where pre-training matters most) on diverse target datasets, comparing against a baseline BO without shared latent spaces to measure sample efficiency gains.

3. **Outlier Dataset Performance:** Test the method on a deliberately selected outlier dataset (e.g., highly imbalanced or with unusual feature distributions) to assess how poorly the ranking model performs when the target is dissimilar to source datasets.