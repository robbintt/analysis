---
ver: rpa2
title: 'Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization'
arxiv_id: '2504.10735'
source_url: https://arxiv.org/abs/2504.10735
tags:
- fidelity
- layers
- training
- layer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of efficient hyperparameter\
  \ optimization (HPO) for large-scale deep learning models, where full training is\
  \ computationally expensive. The authors propose a novel fidelity source based on\
  \ freezing a variable number of layers during training\u2014only training the last\
  \ z layers while keeping the first n-z frozen."
---

# Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization

## Quick Facts
- **arXiv ID:** 2504.10735
- **Source URL:** https://arxiv.org/abs/2504.10735
- **Reference count:** 40
- **One-line primary result:** Layer freezing enables memory-efficient HPO with high rank correlation (>0.85) even at 40% trainable layers

## Executive Summary
This paper introduces "frozen layers" as a novel fidelity source for multi-fidelity hyperparameter optimization (MF-HPO) of deep learning models. The key innovation is training only the last z layers while keeping the first n-z layers frozen, achieving up to 3x memory savings and 4x runtime reductions compared to full training. The method preserves strong rank correlations between hyperparameter performance at low and high fidelities, enabling effective HPO without full training. Experiments across ResNet and Transformer architectures demonstrate the approach's effectiveness while opening new possibilities for memory-constrained HPO.

## Method Summary
The method involves discretizing a neural network into n layers/blocks, then freezing the first n-z layers during training while only training the last z layers. This reduces memory by eliminating the need to store activations for frozen layers and by not allocating optimizer states for their parameters. The fidelity z can be varied from 1 (one trainable layer) to n (full model), creating a many-fidelity HPO setup. The approach requires careful layer splitting to ensure monotonic resource reduction and validates that relative hyperparameter rankings remain correlated with full training results.

## Key Results
- Layer freezing achieves up to 3x memory savings and 4x runtime reductions compared to full training
- Spearman's rank correlation remains >0.85 even when only 40% of layers are trainable
- The method enables HPO on memory-constrained hardware for models that would otherwise be too large to tune
- High correlations are maintained across both ResNet and Transformer architectures on different tasks

## Why This Works (Mechanism)

### Mechanism 1: Memory and Compute Savings
Freezing initial layers reduces GPU memory consumption and compute requirements monotonically as fidelity decreases. By freezing the first n-z layers, the backward pass is truncated. Crucially, intermediate activations for frozen layers do not need to be stored (reducing memory), and optimizer states (e.g., momentum buffers for Adam) are not allocated for frozen parameters. This works because the model architecture allows for sequential decomposition where layer i does not depend on gradients from layer i+1 for its forward pass.

### Mechanism 2: Preserved Rank Correlation
Relative hyperparameter performance rankings are preserved even when most layers are at random initialization. Randomly initialized deep networks possess inherent inductive biases and can extract useful features ("random features"). The trainable final layers act as a probe, and while absolute performance drops, the relative difference between a good and bad HP configuration remains correlated with the fully trained model. This works because the search space contains HPs whose effectiveness is partially independent of the feature extractor's trained state.

### Mechanism 3: Hardware Accessibility
Layer freezing allows HPO on hardware that would otherwise be memory-constrained. This approach trades "model completeness" for "accessibility." It enables the evaluation of configurations for large models (e.g., 1.4B parameters) on smaller GPUs by fitting only a fraction of the optimizer states and activations. This works because the HPO objective is to find a configuration that transfers validly to the full model, rather than optimizing the frozen sub-model itself.

## Foundational Learning

- **Concept:** Multi-Fidelity Hyperparameter Optimization (MF-HPO)
  - **Why needed here:** The paper positions "frozen layers" as a novel fidelity dimension alongside existing ones like epochs or dataset size. Understanding MF-HPO is required to see why rank correlation matters more than absolute loss.
  - **Quick check question:** Does a lower fidelity evaluation aim to predict the absolute loss or the relative ranking of configurations?

- **Concept:** Backpropagation & Activation Memory
  - **Why needed here:** To understand why freezing saves memory. One must grasp that standard training requires storing intermediate outputs (activations) for gradient calculation; freezing removes this requirement.
  - **Quick check question:** If you freeze layer 5 of a 10-layer network during backpropagation, do you need to store the input activation of layer 6? (Answer: Yes, but not the input to layer 5).

- **Concept:** Spearman's Rank Correlation
  - **Why needed here:** This is the primary metric used to validate the method. The authors argue their method works because this correlation remains high (>0.85) even at low fidelity.
  - **Quick check question:** If Configuration A beats Configuration B at low fidelity, does Spearman's correlation measure the probability that A also beats B at full fidelity?

## Architecture Onboarding

- **Component map:** Model Wrapper -> Fidelity Controller -> Evaluation Loop
- **Critical path:**
  1. **Layer Discretization:** Correctly identifying "layer" boundaries (e.g., ResNet blocks vs. single Conv layers) to ensure monotonic cost reduction.
  2. **Optimization Scope:** Passing only the unfrozen parameters to the optimizer (e.g., `optim.Adam(trainable_params)`).
  3. **Correlation Check:** Validating that low-fidelity ranks align with full-fidelity ranks for a subset of configs before full HPO execution.

- **Design tradeoffs:**
  - **Granularity:** Splitting by individual layers offers smoother cost curves but requires managing more complex state; splitting by blocks is easier but creates "jumps" in resource usage.
  - **Continuation:** The paper notes you cannot easily "continue" training from low fidelity (frozen) to high fidelity (unfrozen) without changing the task dynamics. You typically must retrain from scratch or use a specific unfreezing schedule.

- **Failure signatures:**
  - **Oscillating Ranks:** Low Spearman correlation (<0.6), suggesting the frozen random features are insufficient for the specific task.
  - **Memory Plateau:** Memory usage does not drop linearly with frozen layers, usually caused by large embeddings or heads that remain unfrozen at all fidelities.
  - **Inconsistent Results:** Performance not monotonic with fidelity—verify training budget (steps/epochs) is identical across fidelities.

- **First 3 experiments:**
  1. **Baseline Correlation Grid:** Run a small grid search (e.g., 3 learning rates × 3 weight decays) on the full model vs. the 50% frozen model to verify rank correlation > 0.8 for your specific dataset.
  2. **Resource Profiling:** Measure peak VRAM and step-time for fidelity levels z ∈ {1, total_layers/2, total_layers} to quantify actual hardware savings.
  3. **Joint Fidelity Run:** Implement a simple Successive Halving schedule that increases both epochs and trainable layers simultaneously to verify the "Many-fidelity" benefit.

## Open Questions the Paper Calls Out

### Open Question 1
How can a principled continuation mechanism be designed to allow training to resume after unfreezing layers, rather than requiring a full restart for each fidelity increase? The authors state they "consciously do not pursue this direction of continuing over layers" and note the approach "currently lacks a principled continuation mechanism that is crucial to enable freeze-thaw MF-HPO."

### Open Question 2
What is the optimal strategy for discretizing diverse neural architectures into layers or blocks to serve as valid fidelity steps? Section 3.3 notes that while heuristics exist, "the optimal algorithm for layer splitting is left for future work" and currently "requires domain knowledge."

### Open Question 3
How does freezing layers affect the optimization of layer-specific hyperparameters, such as layer-wise learning rates or decay factors? Section 5 (Limitations) states that "The effect of layer-specific HPs on frozen layers as fidelity should be studied."

## Limitations
- The method requires careful layer discretization to ensure monotonic cost reduction, which is not always trivial for complex architectures
- Rank correlation preservation depends on the specific architecture and task—the "random features" hypothesis may break for certain hyperparameter types or extremely shallow models
- The approach cannot easily continue training from low fidelity to high fidelity without retraining from scratch

## Confidence
- **High Confidence:** Memory and runtime savings from layer freezing (directly measured in experiments)
- **Medium Confidence:** Rank correlation preservation (>0.85) across diverse tasks (depends on task/architecture specifics)
- **Medium Confidence:** Ability to enable HPO on constrained hardware (technically sound but application-dependent)

## Next Checks
1. **Rank Correlation Validation:** Run a small grid search on your specific architecture and task to verify that Spearman's rank correlation remains >0.8 at low fidelity before scaling up HPO.
2. **Memory Profiling:** Measure actual VRAM usage across different fidelity levels to confirm the claimed 3x savings and identify any memory plateauing issues.
3. **Layer Discretization Audit:** Verify that your layer splitting strategy ensures monotonic cost reduction and that optimizer states are correctly allocated only to trainable parameters.