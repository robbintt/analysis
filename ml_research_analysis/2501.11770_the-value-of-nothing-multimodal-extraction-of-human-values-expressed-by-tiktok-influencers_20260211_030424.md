---
ver: rpa2
title: 'The Value of Nothing: Multimodal Extraction of Human Values Expressed by TikTok
  Influencers'
arxiv_id: '2501.11770'
source_url: https://arxiv.org/abs/2501.11770
tags:
- values
- social
- value
- tiktok
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel framework for extracting human values
  from TikTok videos using multimodal AI models. The authors curated an annotated
  dataset of 890 TikTok videos from influencers targeting youth, applying the Schwartz
  Theory of Personal Values framework.
---

# The Value of Nothing: Multimodal Extraction of Human Values Expressed by TikTok Influencers

## Quick Facts
- arXiv ID: 2501.11770
- Source URL: https://arxiv.org/abs/2501.11770
- Reference count: 26
- Key outcome: RoBERTa-large on scripts achieves weighted F-scores of 0.49/0.42 (present/conflicted values), outperforming direct video analysis and fine-tuned models

## Executive Summary
This paper introduces a novel framework for extracting human values from TikTok videos using multimodal AI models. The authors curate an annotated dataset of 890 TikTok videos from youth-targeting influencers and apply the Schwartz Theory of Personal Values framework. Their two-step pipeline converts videos to detailed scripts before value detection, achieving state-of-the-art results. The study demonstrates that supervised BERT-based architectures significantly outperform few-shot LLM applications for this nuanced social science task, while highlighting the importance of visual and audio elements in TikTok content analysis.

## Method Summary
The authors developed a 2-step pipeline for value extraction: first converting TikTok videos to detailed text scripts using Gemini 1.5 Pro, then applying supervised Masked Language Models (RoBERTa-large) for value classification. The dataset contains 890 videos annotated with 19 Schwartz values, each labeled as present, absent, or conflicted. Values appearing fewer than 30 times were excluded from training. The approach compares direct multimodal extraction (video→values) against script-based extraction (video→script→values), with the latter showing superior performance.

## Key Results
- RoBERTa-large on scripts achieved weighted F-scores of 0.49 (present values) and 0.42 (conflicted values)
- Two-step script approach outperformed direct video analysis (0.49 vs 0.40 weighted F1 for present values)
- Supervised MLMs significantly outperformed few-shot LLM applications (RoBERTa: 0.49 vs Gemini on scripts: 0.33)
- Pre-trained models without fine-tuning outperformed their fine-tuned counterparts on this task

## Why This Works (Mechanism)

### Mechanism 1
A two-step pipeline (video→script→values) outperforms direct multimodal value extraction because converting video to detailed textual scripts creates an intermediate representation that standard text-based models can process more reliably than raw multimodal input. The script normalizes noisy visual/audio signals into structured text that MLMs can analyze effectively.

### Mechanism 2
Supervised Masked Language Models (RoBERTa, DeBERTa) significantly outperform few-shot Large Language Models for nuanced value detection because MLMs trained with task-specific supervision learn decision boundaries calibrated to the specific annotation schema. Few-shot LLMs, while more flexible, lack the precision from gradient-based optimization on domain-specific labels.

### Mechanism 3
Pre-trained language models without fine-tuning outperformed fine-tuned counterparts because fine-tuning on small, imbalanced datasets with many classes may cause overfitting or catastrophic forgetting of pre-trained representations that already encode relevant semantic knowledge about values.

## Foundational Learning

- **Schwartz Theory of Personal Values (19 values)**: The entire annotation framework and model output space is defined by this theory. Without understanding that values like "Achievement" (success per social standards) differ from "Power-Dominance" (control over people), you cannot interpret model outputs or debug classification errors.
- **Masked Language Models vs. Autoregressive LLMs**: The paper's central finding is that BERT-style models (bidirectional, trained via masked prediction) outperform GPT-style models (autoregressive, used via prompting) for this task. Understanding the architectural difference explains why transfer learning with supervision works better than few-shot prompting here.
- **Multimodal Video-to-Text Transduction**: The critical intermediate step is converting TikTok videos (visual + audio + overlay text) into structured scripts. Understanding what information is preserved vs. lost in this conversion is essential for diagnosing pipeline failures.

## Architecture Onboarding

- Component map:
  [TikTok Video .mp4] -> [Gemini 1.5 Pro multimodal] -> [Script generation prompt] -> [Text script: scene headers, action, dialogue] -> [RoBERTa-large (supervised)] -> [Value labels: 19 classes × 3 states]

- Critical path:
  1. Video curation and filtering — Must include verbal sound (talking/narration/song); silent TikToks excluded
  2. Script generation prompt engineering — Quality of script directly bounds downstream performance
  3. Annotation schema application — 19 Schwartz values, each labeled as present (+1), absent (0), or conflicted (-1)
  4. Training data preparation — Values with <30 occurrences excluded from MLM training
  5. Model selection — RoBERTa-large (non-fine-tuned) currently optimal

- Design tradeoffs:
  | Decision | Option A | Option B | Paper's finding |
  |----------|----------|----------|-----------------|
  | Extraction path | Direct video | Two-step (script) | Two-step superior (0.49 vs 0.40 F1) |
  | Model type | MLM supervised | LLM few-shot | MLM superior for nuanced tasks |
  | Fine-tuning | Pre-trained only | Fine-tune on data | Pre-trained superior with small N=890 |
  | Within-LLM comparison | Direct video | Script then classify | Direct better for LLMs (0.40 vs 0.33) |

- Failure signatures:
  - Rare value classes returning 0 F1: Values appearing <30 times in training data are excluded
  - Conflicted values underperforming present values: Weighted F drops from 0.49→0.42
  - Fine-tuned models underperforming pre-trained: With N=890 and 57 possible labels, fine-tuning causes degradation
  - Script missing key visual information: Non-verbal cues lost in transcription affect value detection

- First 3 experiments:
  1. Baseline reproduction: Run RoBERTa-large on released dataset using script conversion pipeline. Target: weighted F1 ≈ 0.47
  2. Ablation: Direct LLM comparison: Run Gemini 1.5 Pro direct video classification. Expected: weighted F1 ≈ 0.35-0.40
  3. Error analysis by value frequency: Stratify performance by training occurrence count to identify which values need more annotation

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do few-shot LLMs exhibit significant performance drops detecting "conflicting" values compared to "present" values, whereas MLMs maintain stability? The authors have "no plausible explanation" and intend to explore this further.
- **Open Question 2**: To what degree is the underperformance of fine-tuned models attributable to the small dataset size versus the large number of imbalanced classes? The current setup presents both constraints simultaneously.
- **Open Question 3**: Does the 2-step value extraction framework generalize effectively to TikTok genres outside the seven specific categories analyzed? TikTok is "extremely varied" and generalization across the platform remains unverified.

## Limitations
- Script generation may lose critical non-verbal information (body language, visual symbolism) that directly conveys values
- With only 890 videos and many rare value classes, supervised MLM results may not generalize to broader value distributions
- Exclusive focus on youth-targeting influencers creates demographic bias limiting generalizability

## Confidence
- **High confidence**: RoBERTa-large without fine-tuning outperforms fine-tuned models (robust across multiple value categories)
- **Medium confidence**: Two-step pipeline outperforms direct multimodal extraction (though within-LLM comparison shows some inconsistency)
- **Medium confidence**: Supervised MLMs significantly outperform few-shot LLMs for nuanced value detection (consistent with broader literature)

## Next Checks
1. **Visual information preservation test**: Manually analyze 50 scripts vs. source videos to quantify what percentage of value-relevant visual information is lost during transcription
2. **Cross-demographic generalization**: Apply trained RoBERTa model to separate dataset of TikTok videos from different age groups to test domain transfer and demographic bias
3. **Ablation on visual elements**: Create modified script generation pipeline that explicitly describes visual elements and measure impact on value classification F1 scores compared to standard text-only scripts