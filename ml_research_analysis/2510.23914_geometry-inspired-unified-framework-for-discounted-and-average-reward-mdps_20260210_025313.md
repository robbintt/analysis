---
ver: rpa2
title: Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs
arxiv_id: '2510.23914'
source_url: https://arxiv.org/abs/2510.23914
tags:
- case
- policy
- value
- reward
- average-reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work unifies the theoretical analysis of discounted-reward
  and average-reward Markov Decision Processes (MDPs) by extending a geometric interpretation
  framework to the average-reward case. The authors introduce new action and value
  vectors that enable a consistent geometric view across both settings, addressing
  the singularity issue that arises in average-reward MDPs when using traditional
  value functions.
---

# Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs

## Quick Facts
- arXiv ID: 2510.23914
- Source URL: https://arxiv.org/abs/2510.23914
- Reference count: 7
- This work unifies theoretical analysis of discounted-reward and average-reward MDPs through a geometric framework

## Executive Summary
This paper introduces a unified geometric framework for analyzing both discounted-reward and average-reward Markov Decision Processes (MDPs). The authors extend Mustafin et al.'s (2025a) geometric interpretation to the average-reward case by introducing new action and value vectors that enable consistent geometric analysis across both settings. Under the assumption of a unique optimal policy inducing a unichain Markov chain, they prove that Value Iteration achieves geometric convergence in the average-reward case with a convergence rate bounded by γᴺτ where τ ∈ (0,1).

## Method Summary
The method constructs new action vectors a⁺ = (rₐ, γpₐ₁/C − γ/C, ..., γ(pₐₛ−1)/C − γ/C, ...) where C = nγ + (1−γ), and policy vectors vπ⁺ = (1, vπ(1), ..., vπ(n)). The inner product a⁺ · vπ⁺ equals the advantage adv(a, π) in both discounted and average-reward settings. Policy evaluation solves (I + E − γPπ)vπ = Rπ, which is invertible when the policy induces a unichain Markov chain. Value Iteration updates use these representations to achieve geometric convergence under a unique unichain optimal policy assumption.

## Key Results
- Introduces unified action and value vector representation enabling geometric analysis for both MDP formulations
- Proves Value Iteration achieves geometric convergence in average-reward case with rate γᴺτ
- Establishes equivalence between unichain structure and invertibility of I + E − Pπ matrix
- Extends previous geometric framework from discounted to average-reward settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unified action-vector and policy-vector representation preserves the advantage computation as an inner product in both discounted and average-reward settings.
- Mechanism: Define action vectors a⁺ = (rₐ, γpₐ₁/C − γ/C, ..., γ(pₐₛ−1)/C − γ/C, ...) where C = nγ + (1−γ), and policy vectors vπ⁺ = (1, vπ(1), ..., vπ(n)). The inner product a⁺ · vπ⁺ equals adv(a, π) in both cases because the modified value function vπ(s)/C satisfies vπ(s)/C = rπ(s) − γvπ^Σ/C + γΣᵢ pπ(s)ᵢ vπ(i)/C, which reduces to the standard Bellman equation in the discounted case and to the bias equation (plus arbitrary constant) in the average case.
- Core assumption: The policy π induces a unichain Markov chain when γ = 1 (ensures unique solution to (I + E − Pπ)vπ = Rπ).
- Evidence anchors:
  - [abstract]: "new action and value vector representation that allows geometric analysis to extend to the average reward case"
  - [Section 3.1]: Defines a⁺ and vπ⁺ explicitly with the C normalizing constant
  - [corpus]: Limited direct corpus support; corpus papers focus on average-reward algorithms rather than geometric representations
- Break condition: Multichain policies where I + E − Pπ is singular (multiple recurrent classes); non-unique optimal policies

### Mechanism 2
- Claim: The matrix I + E − Pπ is invertible if and only if the induced Markov chain is unichain, enabling unique policy evaluation in the average-reward case.
- Mechanism: In the unichain case, ker(I − Pπ) = span{1}. For any x = α1 + y with 1ᵀy = 0, we have (I + E − Pπ)(α1 + y) = αn1 + (I − Pπ)y. If this equals zero, then α = 0 and y ∈ ker(I − Pπ) with 1ᵀy = 0, forcing y = 0. Conversely, if multichain, dim ker(I − Pπ) ≥ 2 allows nonzero y with 1ᵀy = 0 in the kernel, violating invertibility.
- Core assumption: Finite state space; unichain structure for the policy being evaluated
- Evidence anchors:
  - [Section 3.4, Lemma 3.5]: Full proof of the equivalence between unichain and invertibility of I + E − Pπ
  - [Section 4, Assumption 4.1]: Explicitly assumes unique optimal policy with unichain MRP
  - [corpus]: Paper 2506.20910 (Faster Fixed-Point Methods for Multichain MDPs) addresses convergence difficulties in multichain settings, indirectly supporting this assumption's necessity
- Break condition: Multichain policies with two or more closed irreducible classes

### Mechanism 3
- Claim: Under a unique unichain optimal policy, the span seminorm of the value vector contracts by factor γᴺτ after N iterations, where τ ∈ (0, 1) depends on ergodicity.
- Mechanism: After normalization (optimal actions have zero reward, suboptimal have negative reward), the value update satisfies γ(P* − E)ṽₜ ≤ ṽₜ₊₁ ≤ γ(Pₜ − E)ṽₜ. The matrix P* has (P*)ᴺ fully positive for some N ≤ n² − 2n + 2 with minimum entry ω > 0. The convex combination P′ₜ = DₜP* + (I − Dₜ)Pₜ inherits ergodicity. Using Lemma 4.2's decomposition ∏(P′ₜ − E) = ∏P′ₜ + E′, the span contraction follows from sp(Qv) ≤ sp(v) for stochastic Q.
- Core assumption: Unique optimal policy π*; π* induces an ergodic unichain; δ > 0 (gap between optimal and suboptimal advantages)
- Evidence anchors:
  - [abstract]: "under a unique and ergodic optimal policy, the Value Iteration algorithm achieves a geometric convergence rate"
  - [Section 4, Theorem 4.3]: Full proof with explicit contraction factor γᴺτ
  - [corpus]: Paper 2510.17391 (Finite-Time Bounds for Average-Reward Fitted Q-Iteration) similarly relies on ergodicity for finite-time guarantees
- Break condition: Multiple optimal policies (advantage gap δ may vanish); non-ergodic optimal policy (ω = 0)

## Foundational Learning

- Concept: **Bias function and gain in average-reward MDPs**
  - Why needed here: The paper's new value vπ/C equals hπ + c for some constant c in the average case; understanding this non-uniqueness is essential for why the new representation matters
  - Quick check question: If hπ satisfies Tπhπ = hπ + ρπ1, what is Tπ(hπ + 5·1)?

- Concept: **Span seminorm sp(v) = maxᵢ v(i) − minⱼ v(j)**
  - Why needed here: Convergence is proven in span seminorm, not a true norm; value vectors are only defined up to additive constants in the average case
  - Quick check question: For v = (3, 7, 3), what is sp(v)? Does sp(v + c·1) differ from sp(v)?

- Concept: **Unichain vs. multichain MDP structure**
  - Why needed here: Lemma 3.5 shows the framework's policy evaluation only works for unichain policies; multichain cases break the invertibility of I + E − Pπ
  - Quick check question: If an MDP has states {A, B, C} where A → B or A → C, and B and C are absorbing, is this unichain or multichain?

## Architecture Onboarding

- Component map:
  - Action vector construction: From each SAP a, compute a⁺ = (rₐ, cₐ₁, ..., cₐₙ) where cₐᵢ = γpₐᵢ/C for i ≠ st(a), and cₐᵢ = γ(pₐᵢ − 1)/C for i = st(a)
  - Policy vector construction: Solve (I + E − γPπ)vπ/C = Rπ/C, then form vπ⁺ = (1, vπ(1), ..., vπ(n))
  - Advantage computation: a⁺ · vπ⁺ = adv(a, π) directly via inner product
  - MDP normalization: Apply transformation LΔ (from Mustafin et al. 2025a) so optimal actions have zero reward
  - VI update: vₜ₊₁(s) = vₜ(s) + max_{st(a)=s} adv(a, vₜ)

- Critical path:
  1. Verify unichain assumption before applying to average-reward case (check transition structure)
  2. Compute C = nγ + (1 − γ); note C = n when γ = 1
  3. For each iteration, extract greedy policy πₜ, solve for vπₜ, check convergence via sp(vₜ)
  4. Stop when sp(vₜ) falls below tolerance or maximum iterations reached

- Design tradeoffs:
  - **Unichain-only**: Multichain problems require alternative formulations (corpus paper 2506.20910 discusses this)
  - **Span seminorm convergence**: Only guarantees bias convergence up to additive constant; may need post-processing to extract gain ρ*
  - **Contraction rate vs. N**: The bound uses γᴺτ where N can be O(n²) in worst case; practical convergence may be faster

- Failure signatures:
  - Singular matrix error when solving (I + E − Pπ)v = R: indicates multichain policy or numerical issues
  - Non-converging span seminorm: suggests non-unique optimal policy or δ ≈ 0
  - Value divergence: check γ < 1 for discounted case or verify unichain for average case

- First 3 experiments:
  1. **Validate inner-product advantage**: On a 3-state MDP, manually compute a⁺ · vπ⁺ and compare to traditional adv(a, π) = Qπ(s,a) − hπ(s) for both γ = 0.9 and γ = 1
  2. **Test unichain vs. multichain**: Construct two MDPs (unichain and multichain) with same rewards; verify (I + E − Pπ) is invertible only for unichain policies
  3. **Measure empirical convergence**: Run VI on a unichain MDP with unique optimal policy, track sp(vₜ) across iterations, compare contraction rate to theoretical bound γᴺτ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the geometric convergence of Value Iteration be proven for multichain MDPs where the optimal policy contains multiple recurrent classes?
- Basis in paper: [explicit] The conclusion explicitly lists "multichain structures" as a target for future analysis.
- Why unresolved: The current framework relies on Lemma 3.5, which establishes the invertibility of the matrix $(I + E - P^\pi)$—essential for defining the new value vector—*strictly* for the unichain case. In multichain settings, this matrix is singular, rendering the current definition of the value vector $v^\pi$ invalid.
- What evidence would resolve it: A generalized definition of the value vector that remains unique and valid for multichain policies, or an alternative proof method that bypasses the need for the invertibility of $(I + E - P^\pi)$.

### Open Question 2
- Question: Does Value Iteration retain a geometric convergence rate if the assumption of a unique optimal policy is relaxed to allow for multiple optimal policies?
- Basis in paper: [explicit] The conclusion identifies relaxing the assumption of a unique optimal policy (Assumption 4.1) as a direction for future work.
- Why unresolved: The proof of Theorem 4.3 depends on a strict positive gap $\delta$ between the optimal policy and sub-optimal actions. If multiple optimal policies exist, the "greedy" policy $\pi_t$ may switch between optimal options without strictly improving the value, potentially violating the strict contraction inequalities derived in the proof.
- What evidence would resolve it: A modified convergence proof showing that the span seminorm contracts even when the policy selection oscillates between ties, or a characterization of the convergence rate specifically for tied optimal scenarios.

### Open Question 3
- Question: Can the geometric framework be extended to analyze stochastic policies rather than just deterministic ones?
- Basis in paper: [explicit] The conclusion proposes extending the analysis to "broader classes of policies."
- Why unresolved: The paper currently defines policies as deterministic maps $S \to A$. The geometric construction of action vectors $a^+$ and policy hyperplanes $\mathcal{H}_\pi$ is based on discrete state-action pairs; stochastic policies would require a geometric interpretation of distributions over these pairs.
- What evidence would resolve it: A reformulation of the policy vector $v^\pi_+$ to represent mixtures of action vectors, demonstrating that the geometric relationship between advantages and hyperplanes is preserved under stochastic decision-making.

## Limitations

- The framework critically relies on the unichain assumption, which may not hold in many practical MDPs
- The convergence rate bound γᴺτ may be conservative as N can be O(n²) in worst case
- The complete specification of the normalization transformation LΔ is not fully provided
- Convergence is only guaranteed for span seminorm, not the true norm, requiring additional steps to extract gain

## Confidence

- **High confidence**: The equivalence between unichain structure and invertibility of I + E − Pπ (Lemma 3.5); the geometric interpretation preserving inner products for advantage computation
- **Medium confidence**: The span seminorm contraction proof (Theorem 4.3); the explicit form of action and value vectors a⁺ and vπ⁺
- **Low confidence**: The practical tightness of the convergence bound γᴺτ; the complete specification of the normalization transformation LΔ

## Next Checks

1. Implement and test the action-vector inner product computation on a small MDP with both γ = 0.9 and γ = 1, verifying adv(a, π) = a⁺ · vπ⁺ matches traditional advantage calculations
2. Construct a multichain MDP example and demonstrate that (I + E − Pπ) becomes singular, confirming the unichain requirement
3. Run Value Iteration on a unichain MDP with unique optimal policy, measure empirical sp(vₜ) decay, and compare the observed contraction factor to the theoretical bound γᴺτ