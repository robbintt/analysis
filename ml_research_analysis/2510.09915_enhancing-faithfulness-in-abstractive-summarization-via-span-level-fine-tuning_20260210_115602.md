---
ver: rpa2
title: Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning
arxiv_id: '2510.09915'
source_url: https://arxiv.org/abs/2510.09915
tags:
- summaries
- training
- faithfulness
- summarization
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of unfaithful text summarization,
  where large language models generate summaries containing hallucinated or inconsistent
  information. To improve faithfulness, the authors introduce a method for automatically
  annotating span-level hallucinations in LLM-generated summaries using GPT-4o.
---

# Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning

## Quick Facts
- **arXiv ID:** 2510.09915
- **Source URL:** https://arxiv.org/abs/2510.09915
- **Reference count:** 35
- **Primary result:** Unlikelihood training improves faithfulness metrics (G-Eval, AlignScore) by up to 0.16 and 0.02 respectively on CNNDM, SAMSum, and XSum datasets.

## Executive Summary
This paper addresses the challenge of unfaithful text summarization, where large language models generate summaries containing hallucinated or inconsistent information. The authors introduce a method for automatically annotating span-level hallucinations in LLM-generated summaries using GPT-4o, then leverage these annotations to fine-tune models using three approaches: gradient ascent, unlikelihood training, and task vector negation. Experiments demonstrate that all three methods improve faithfulness, with unlikelihood training being the most effective. The study shows that span-level hallucination annotations are valuable for enhancing summary faithfulness, and that unlikelihood training is the most robust fine-tuning strategy across different model sizes.

## Method Summary
The authors automatically annotate span-level hallucinations in LLM-generated summaries using GPT-4o as an annotator. They then fine-tune models using three approaches: (1) gradient ascent, which reverses gradients on hallucinated tokens to "unlearn" them, (2) unlikelihood training, which minimizes the probability of generating hallucinated tokens while maintaining fluency, and (3) task vector negation, which subtracts hallucination-related weight vectors from the model. The best-performing method, unlikelihood training, combines standard NLL loss for positive samples with $-\log(1 - p(x))$ for hallucinated tokens, optimized using LoRA with r=128, Î±=256, dropout=0.05, and epsilon=0.5.

## Key Results
- Unlikelihood training improves G-Eval scores by up to 0.16 and AlignScore by up to 0.02 across different model sizes.
- Task Vector Negation is particularly effective on larger models (7b, 8b).
- Performance degrades with gradient ascent as epsilon increases beyond 0.1, potentially destroying coherent text generation.
- BARTScore shows inconsistent results with these methods, dropping while G-Eval improves.

## Why This Works (Mechanism)

### Mechanism 1: Unlikelihood Training on Organic Hallucinations
Explicitly penalizing the generation of specific tokens identified as hallucinations reduces their future probability while maintaining fluency. The model minimizes the negative log-likelihood of the complement probability ($1 - p_\theta$) for tokens within hallucinated spans ($H_n$). This learns to avoid specific error patterns observed in actual LLM outputs rather than synthetic ones.

### Mechanism 2: Gradient Ascent for "Unlearning"
Reversing the gradient on hallucinated tokens forces the model to unlearn specific undesirable behaviors. This method applies standard cross-entropy loss but reverses the sign for negative samples, pushing the model's weights away from the configuration that produces hallucinated text.

### Mechanism 3: Task Vector Negation
Hallucination behavior can be vectorized and subtracted from the model's weights. The authors compute a "task vector" ($\tau$) by subtracting pre-trained weights from fine-tuned weights, calculating vectors for faithful summaries ($\tau_{pos}$) and hallucinated summaries ($\tau_{neg}$). The final model is constructed by adding the positive vector and subtracting the negative vector.

## Foundational Learning

- **Concept: Unlikelihood Training**
  - Why needed here: This is the core intervention. Unlike standard fine-tuning which pushes probability mass toward gold text, this technique explicitly pushes probability mass away from negative tokens.
  - Quick check question: How does the loss function differ for a token labeled "hallucinated" versus one labeled "faithful"?

- **Concept: Task Vectors (Model Arithmetic)**
  - Why needed here: Understanding that fine-tuning creates a directional change in high-dimensional space allows for algebraic manipulation of model behaviors.
  - Quick check question: If you fine-tune a model on a dataset of errors, what does the resulting "task vector" theoretically represent?

- **Concept: Hallucination Taxonomy (Intrinsic vs. Extrinsic)**
  - Why needed here: The paper relies on GPT-4o to distinguish faithfulness. To replicate or critique this, one must understand that hallucinations can be factual contradictions (intrinsic) or new, unverifiable information (extrinsic).
  - Quick check question: Would a summary that adds correct but unstated information be flagged as a hallucination in this framework?

## Architecture Onboarding

- **Component map:** Generator (Target) -> Annotator (GPT-4o) -> Trainer (LoRA fine-tuning loop)
- **Critical path:** Data Quality & Annotation Prompt. The entire system hinges on the GPT-4o prompt's ability to precisely identify spans.
- **Design tradeoffs:** Gradient Ascent is brittle but simple; Unlikelihood is robust but requires careful weighting. Synthetic vs. Organic Negatives - organic errors are argued to be superior training signals.
- **Failure signatures:** Model Collapse (output becomes unreadable), Over-conservatism (very short summaries), Metric Conflict (BARTScore drops while G-Eval rises).
- **First 3 experiments:**
  1. Validate the Annotator by running GPT-4o annotation prompt on a small held-out set manually checked by humans.
  2. Epsilon Sweep (Stability Test) - train using Unlikelihood Training while sweeping epsilon from 0.1 to 0.9.
  3. Baseline Comparison - compare organic negative samples against synthetically perturbed samples.

## Open Questions the Paper Calls Out

1. **Annotator Reliability:** How reliable is GPT-4o as an automated annotator for span-level hallucinations compared to human judgment? The study prioritized using annotations over validating the annotation tool itself.

2. **Alternative Alignment Methods:** Does Direct Preference Optimization (DPO) outperform unlikelihood training for improving summarization faithfulness using span-level annotations? The authors did not include comparisons with recent alignment methods.

3. **Contrastive Learning Potential:** Can contrastive learning at the span-level further improve faithfulness if the dataset includes corresponding positive tokens for negative spans? The current dataset lacks token-level positive/negative pairs required for this approach.

## Limitations
- Heavy reliance on GPT-4o's annotation accuracy without reporting precision/recall metrics for the annotation process.
- Experiments limited to three summarization datasets (CNNDM, SAMSum, XSum), with unknown performance on other domains or longer-form summarization.
- Fixed weight decay hyperparameter at 0.1 without justification and epsilon choice (0.5) may not be optimal across all scenarios.

## Confidence
- **High Confidence:** Unlikelihood training improves faithfulness metrics (G-Eval and AlignScore) as demonstrated by experimental results.
- **Medium Confidence:** Organic hallucinations are superior training signals to synthetic ones, though not directly tested within the paper.
- **Medium Confidence:** Task Vector Negation shows promise for larger models but relies on prior work rather than being fully validated.

## Next Checks
1. **Annotator Validation Study:** Manually evaluate GPT-4o's hallucination annotations on a held-out test set of 100-200 samples to establish precision, recall, and F1 scores.

2. **Synthetic vs. Organic Negative Sample Comparison:** Implement both synthetic error injection and organic error collection on the same base model, then compare faithfulness improvements.

3. **Long-form Summarization Extension:** Apply unlikelihood training to a long-document summarization dataset like arXiv or PubMed to assess scalability and optimal epsilon thresholds.