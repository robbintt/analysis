---
ver: rpa2
title: 'Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document
  Summarization'
arxiv_id: '2502.06185'
source_url: https://arxiv.org/abs/2502.06185
tags:
- discourse
- long
- score
- document
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies factual inconsistency detection in long document
  summarization through discourse analysis. The paper identifies that complex sentences
  with multiple discourse units and certain discourse features are more prone to factual
  errors.
---

# Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization

## Quick Facts
- arXiv ID: 2502.06185
- Source URL: https://arxiv.org/abs/2502.06185
- Reference count: 40
- Primary result: Discourse-based re-weighting improves factual inconsistency detection in long document summarization with up to 7-point ROC-AUC gains

## Executive Summary
This paper addresses factual inconsistency detection in long document summarization by leveraging discourse structure analysis. The authors propose a framework that uses Rhetorical Structure Theory (RST) parsing to segment documents and re-weight NLI-based sentence alignment scores based on discourse features like sentence depth and subtree height. Experiments across scientific and legal domains demonstrate consistent performance improvements over strong baselines, with gains of up to 7 points on ROC-AUC metrics. The method adds minimal computational overhead while outperforming LLM-based approaches on most datasets.

## Method Summary
The approach parses documents and summaries using DMRST RST parser to extract discourse trees, then segments source documents using discourse tree levels instead of fixed windows. Sentence-level NLI scores are computed using baseline models (ALIGN SCORE, MINI CHECK-FT5, INFUSE), then re-weighted based on normalized depth scores and subtree heights using the formula: s*_i = f(s_i)^(1+(height_subtree × α)), where f(s_i) = s^(1+(mean_depth - depth_i)). The parameter α=1 is tuned on the DIVER SUMM dev set. The framework segments documents by traversing RST trees to specific levels (Lv1 or Lv2) and applies re-weighting to aggregate sentence-level scores.

## Key Results
- Consistent ROC-AUC improvements across LONG SCIVERIFY, LONG EVAL, and LEGAL SUMM benchmarks (up to 7 points)
- Significant Kendall's Tau gains on correlation benchmarks (LONG SCIVERIFY, LONG EVAL)
- Discourse features (depth, subtree height, nuclearity) identified as predictive of factual errors
- Minimal computational overhead compared to NLI inference alone
- Outperforms strong baselines including LLM-based approaches on most datasets

## Why This Works (Mechanism)
The method works by recognizing that factual inconsistencies often occur in complex discourse structures. By parsing documents into RST trees and analyzing features like sentence depth, subtree height, and nuclearity patterns, the framework can identify which discourse units are more likely to contain errors. The re-weighting mechanism amplifies scores from important discourse positions while suppressing potentially unreliable ones, effectively creating a more accurate aggregation of sentence-level predictions. This discourse-driven approach captures structural patterns that correlate with factual consistency beyond what surface-level text analysis can detect.

## Foundational Learning

**RST Discourse Parsing**: Understanding tree structures that represent discourse relationships between text segments
*Why needed*: Provides the structural foundation for identifying discourse patterns associated with factual errors
*Quick check*: Can the parser correctly identify nucleus-satellite relationships in scientific abstracts?

**NLI-based Factual Consistency**: Using natural language inference to score sentence alignment between source and summary
*Why needed*: Forms the baseline prediction mechanism that discourse re-weighting improves
*Quick check*: Does MINI CHECK-FT5 correctly identify simple factual contradictions in single-sentence pairs?

**Discourse Feature Extraction**: Computing depth scores, subtree heights, and nuclearity attributes from RST trees
*Why needed*: These features serve as the basis for re-weighting sentence scores
*Quick check*: Do sentences with higher depth consistently have more complex discourse structures?

## Architecture Onboarding

**Component Map**: RST Parser -> Discourse Tree Extraction -> Feature Computation -> Sentence NLI Scoring -> Re-weighting Aggregation -> Final Prediction

**Critical Path**: Document parsing → discourse feature extraction → NLI inference → re-weighting → aggregation
The pipeline processes documents sequentially, with each stage dependent on the previous one. Parser failures or inaccuracies can cascade through the system, making discourse parsing reliability critical.

**Design Tradeoffs**: The method trades increased preprocessing complexity (RST parsing) for improved accuracy in the final consistency scores. Static re-weighting parameters offer simplicity but may not adapt well to all domains, as evidenced by performance drops on Multi-news.

**Failure Signatures**: 
- Parser failures manifest as segmentation errors or missing discourse features
- Re-weighting can degrade performance on short summaries (1-3 sentences) due to minimal structure
- Different domains show inconsistent gains, suggesting structural assumptions may not generalize

**First Experiments**:
1. Run baseline NLI model on DIVER SUMM-SENT to establish sentence-level prediction accuracy
2. Parse a sample document from LONG SCIVERIFY to visualize RST tree structure and extract discourse features
3. Apply re-weighting to a single document-summary pair and compare aggregated scores before/after re-weighting

## Open Questions the Paper Calls Out
- Can incorporating specific RST relation types (e.g., Explanation, Elaboration) improve detection and resolution of factual inconsistency errors?
- Does a dynamic weighting algorithm based on document structure and domain knowledge outperform the current static re-weighting strategy?
- Do correlations between discourse features and factual inconsistency generalize to narrative domains like story summarization or book-length summarization?

## Limitations
- Performance drops on specific datasets (e.g., Multi-news) due to static re-weighting parameters
- Heavy reliance on RST parsing accuracy, with unclear fallback strategies for parser failures
- Limited evaluation to scientific and legal domains, leaving generalizability to other domains uncertain

## Confidence

**Performance Improvements on Long Document Benchmarks**: High
- Consistent ROC-AUC gains across multiple datasets with significant statistical improvements
- Strong ablation evidence supporting discourse feature contributions

**Discourse Features as Predictors of Factual Errors**: Medium
- Compelling correlation analysis but causal relationship needs further validation
- Could be influenced by uncontrolled factors beyond discourse complexity

**Generalizability Across Domains**: Low-Medium
- Promising results in scientific and legal domains but limited to specific datasets
- Domain-specific tuning may be required for broader applicability

## Next Checks

1. **Parser Failure Robustness Analysis**: Systematically test fallback strategies by inducing parser failures and measuring performance degradation across different document types, comparing paragraph-based vs. sentence-based segmentation approaches.

2. **Cross-Domain Discourse Feature Analysis**: Conduct detailed analysis of discourse feature correlations with factual errors across medical, news, and technical documentation domains to validate generalizability beyond scientific and legal texts.

3. **Computational Overhead Validation**: Measure actual wall-clock time and memory usage of the complete pipeline across different hardware configurations and document lengths, comparing against claimed "minimal overhead" to quantify the accuracy-computation tradeoff.