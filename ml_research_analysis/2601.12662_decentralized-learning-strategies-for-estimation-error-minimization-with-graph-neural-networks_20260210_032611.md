---
ver: rpa2
title: Decentralized Learning Strategies for Estimation Error Minimization with Graph
  Neural Networks
arxiv_id: '2601.12662'
source_url: https://arxiv.org/abs/2601.12662
tags:
- networks
- graphical
- learning
- graph
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses real-time sampling and estimation of autoregressive
  Markovian sources in dynamic multi-hop wireless networks, where nodes must minimize
  time-average estimation error through decentralized policies. The authors propose
  a graphical multi-agent reinforcement learning framework that integrates graphical
  actors (GRNNs), graphical critics (GNNs), and an action distribution operator to
  jointly determine sampling, transmission, and content decisions.
---

# Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks

## Quick Facts
- **arXiv ID**: 2601.12662
- **Source URL**: https://arxiv.org/abs/2601.12662
- **Reference count**: 0
- **Primary result**: Graphical MARL policies with parameter sharing achieve better estimation error minimization and transferability than classical baselines on dynamic multi-hop wireless networks.

## Executive Summary
This paper proposes a graphical multi-agent reinforcement learning framework for real-time sampling and estimation of autoregressive Markovian sources in dynamic multi-hop wireless networks. The framework integrates graphical actors (GRNNs), graphical critics (GNNs), and an action distribution operator to enable decentralized decision-making about sampling, transmission, and content. Theoretically, the authors establish transferability of policies across structurally similar graphs using graphon-based bounds. Empirically, graphical MARL policies outperform classical MARL baselines and show improved transferability to larger networks, with performance gains increasing with network size. The approach demonstrates particular resilience to non-stationarity through the use of recurrence in both independent learning and centralized training with decentralized execution.

## Method Summary
The method addresses decentralized estimation of M Gauss-Markov sources over wireless collision channels, where nodes must minimize time-average estimation error through decentralized policies. The framework uses graphical actors (GRNNs) with parameter sharing across nodes, graphical critics (GNNs) for value estimation, and an action distribution operator to determine sampling and transmission decisions. The theoretical foundation rests on graphon theory, establishing that policies trained on small networks transfer to larger networks with bounded approximation error. Two training variants are explored: Graphical IPPO (independent learning) and Graphical MAPPO (centralized training with decentralized execution). The actor uses recurrent GRNNs to maintain temporal context, while the critic uses non-recurrent GNNs for computational efficiency. Transferability is validated by training on 10-node graphs and testing on networks of varying sizes.

## Key Results
- Graphical MARL policies (IPPO and MAPPO) outperform classical MARL baselines (uniform transmitting, age-based, classical IPPO/MAPPO) on estimation error minimization across synthetic and real topologies.
- Policies trained on 10-node networks transfer effectively to larger networks (M=10 to 50), with graphical methods showing widening performance gaps over baselines as network size increases.
- Recurrence depth T=2 improves resilience to non-stationarity in both independent learning and CTDE settings, with graphical IPPO showing slower ASEE growth than classical IPPO during training.
- Graphical IPPO eventually surpasses graphical MAPPO at larger network sizes (M≈45) due to architectural differences in critic implementation.

## Why This Works (Mechanism)

### Mechanism 1
Graphical actor-critic architectures with parameter sharing enable decentralized decision-making while maintaining coordination across network topologies. The actor uses Graph Recurrent Neural Networks (GRNNs) which are permutation-equivariant—node orderings don't affect the policy's functional form. All homogeneous nodes share parameters (θ and ϑ), so the policy learns a general decision rule rather than node-specific behaviors. The critic evaluates joint states using standard GNNs, providing feedback signals that account for network structure without requiring centralized execution. Core assumption: Nodes are statistically identical (homogeneous) such that parameter sharing is appropriate; successive graph topologies remain structurally similar (sampled from the same graphon).

### Mechanism 2
Policies trained on small networks transfer to larger networks with bounded approximation error due to graphon-based theoretical guarantees. The framework exploits that finite graphs sampled from a graphon (a continuous limit object representing graph structure) converge as network size increases. Theorem 1 bounds the GRNN output error between graphon and finite-graph representations; Theorem 2 extends this to action distributions. The T(1+T)/2 term in the bound reflects error accumulation across recurrence depth T. Core assumption: Graphs are sampled from the same graphon; convolutional filters are built on graph filters (e.g., GCNConv, TAGConv) rather than arbitrary architectures; input/output feature dimensions satisfy F=G=1 for the theoretical bound.

### Mechanism 3
Recurrence in both actor and critic improves resilience to non-stationarity inherent in multi-agent learning, independent of whether using CTDE or independent learning. Multi-agent environments are non-stationary because other agents' policies change during training. Recurrence (T>1) allows the network to maintain temporal context across multiple steps, enabling it to implicitly track the evolution of other agents' behaviors. This mitigates the moving-target problem without requiring explicit opponent modeling. Core assumption: Temporal patterns in other agents' behavior changes exist and are learnable; the added computational cost of recurrence is acceptable.

## Foundational Learning

- **Graph Neural Networks (GNNs) and Permutation Equivariance**
  - Why needed here: The framework relies on GNNs processing node features while respecting graph structure—permutation equivariance means the same nodes in different orderings produce equivalently permuted outputs. Without understanding this, the transferability claims are opaque.
  - Quick check question: If you shuffle node indices in a 5-node graph, would a standard MLP produce the same outputs (just permuted)? Would a GNN? (Answer: MLP—no; GNN—yes, if equivariant)

- **Graphons and Graph Convergence**
  - Why needed here: Theoretical guarantees rest on graphon theory—understanding that graphons are limit objects for sequences of growing graphs, and that sampling from a graphon produces structurally similar graphs.
  - Quick check question: What does it mean for two graphs to be "structurally similar" in this framework? (Answer: Sampled from the same graphon, preserving statistical properties like average degree as size scales)

- **Non-stationarity in Multi-Agent Reinforcement Learning**
  - Why needed here: From a single agent's perspective, the environment appears non-stationary because other agents' policies change during training. Understanding this explains why recurrence and CTDE matter.
  - Quick check question: Why does independent Q-learning potentially fail in multi-agent settings even if it works for single-agent problems? (Answer: Other agents' changing policies violate the Markov assumption for any single agent's Q-function)

## Architecture Onboarding

- **Component map**: Node observation -> GRNN actor (recurrent) -> Action distribution -> Collision channel feedback -> MMSE estimator -> Critic (GNN) -> Policy update
- **Critical path**: 1) Node i receives observation o_i,k (local process value, cached samples, collision feedback) 2) GRNN actor processes observation with recurrence across T steps -> ˆy(i) 3) Action distribution samples (μ_k^(i), ν_k^(i)) determining what to transmit and to whom 4) Collision channel resolves simultaneous transmissions; nodes receive feedback 5) MMSE estimator updates cached estimates; reward computed 6) Critic evaluates state; policy gradient updates shared parameters
- **Design tradeoffs**:
  - IPPO vs MAPPO: IPPO is fully decentralized but more susceptible to non-stationarity; MAPPO uses centralized critic (CTDE) for stability but requires more coordination during training
  - Recurrence depth T: Higher T captures longer temporal patterns but increases error accumulation (T(1+T)/2 term) and computational cost
  - Critic architecture: Standard GNN (no recurrence) is faster but may lose temporal patterns; the paper shows recurrence in critic also helps but trades off efficiency
  - Graph filter type: Must use graph filter–based convolutions (GCNConv, TAGConv) for transferability guarantees; arbitrary architectures void these bounds
- **Failure signatures**:
  - Transferability fails on larger graphs: Check if critic uses non-graph-filter architectures (this explains MAPPO underperformance at M>40)
  - ASEE increases during training with IPPO: Non-stationarity overwhelming the learner—try increasing recurrence depth or switching to MAPPO
  - Experiments fail at M≥12: GPU memory limitation noted in paper—reduce batch size or use gradient accumulation
  - Performance degrades with highly dynamic topologies: Graphon similarity assumption may be violated; topology changes faster than the statistical structure stabilizes
- **First 3 experiments**:
  1. Replicate the 10-node Watts-Strogatz baseline comparison: Train graphical IPPO, graphical MAPPO, classical IPPO, classical MAPPO, uniform transmitting, and age-based policies for 3000 episodes. Verify that graphical methods outperform classical MARL by the margins shown (~10-20 ASEE reduction after convergence)
  2. Ablation on recurrence depth T: Run graphical IPPO with T=1, 2, 3 on the real topology (aus_simple). Confirm that T=2 shows slower ASEE growth under non-stationarity than T=1. Check if T=3 provides additional benefit or if error accumulation dominates
  3. Transferability stress test: Train on 10-node SBM graphs, test on 20, 30, 40, 50-node graphs. Plot ASEE vs network size. Verify that (a) graphical policies maintain advantage over baselines at all scales, and (b) graphical IPPO crosses graphical MAPPO at larger sizes if critic architecture differs from graph-filter-based GNNs

## Open Questions the Paper Calls Out

- **Open Question 1**: Would adopting graph filter–based critics in MAPPO restore its superiority over IPPO across all network sizes during transfer? Basis: Authors state: "We expect that adopting graph filter–based critics would restore MAPPO's superiority across all network sizes." Unresolved because current MAPPO critic uses non-graph-filter architecture causing graphical IPPO to surpass at larger scales.

- **Open Question 2**: Can the transferability guarantees established for single-dimensional features (F=G=1) be extended to multi-dimensional feature spaces used in practice? Basis: Theorem 1 assumes F=G=1, but experiments use higher-dimensional features. The theory-to-practice gap for multi-dimensional cases is not addressed.

- **Open Question 3**: How do non-oblivious policies, where decision-making depends on the monitored processes, compare to oblivious policies in terms of estimation error and transferability? Basis: The paper proves AoI equivalence only for oblivious policies and leaves non-oblivious policies largely unexplored despite their potential for process-aware optimization.

## Limitations

- Theoretical transferability guarantees rely on specific architectural choices (graph filter-based convolutions) and restrictive conditions (F=G=1) that may not hold in practical implementations.
- Empirical validation of transferability is limited to relatively small networks (M≤50) and specific graph types, with unclear generalizability to real-world topologies with heterogeneous node capabilities.
- The optimal recurrence depth for non-stationarity resilience is not theoretically justified and may be task-specific, with computational overhead not fully characterized.

## Confidence

- **High Confidence**: Core empirical findings that graphical MARL methods outperform classical MARL baselines and that graphical IPPO is more resilient to non-stationarity than classical IPPO when using recurrence.
- **Medium Confidence**: Transferability claims are well-theoretically grounded but have limited empirical validation; superiority of graphical IPPO over graphical MAPPO at larger scales is observed but mechanism not definitively proven.
- **Low Confidence**: Exact recurrence depth (T=2) as optimal for non-stationarity is not theoretically justified and may be task-specific; generalizability to highly heterogeneous networks or rapidly evolving topologies is not established.

## Next Checks

1. **Stress-test transferability bounds**: Train graphical IPPO on 10-node graphs with varying Watts-Strogatz rewiring probabilities (p=0.01, 0.1, 0.5). Deploy on 20-50 node graphs and measure ASEE degradation as p increases. This would validate whether graphon similarity is the true driver of transferability.

2. **Recurrence depth optimization**: Systematically evaluate graphical IPPO with T=1, 2, 3, 4 on both synthetic and real topologies. Plot ASEE vs episode and vs network size for each T. This would quantify the trade-off between non-stationarity resilience and error accumulation.

3. **Architectural ablation study**: Replace the GNN critic in graphical MAPPO with GCNConv-based architecture (matching theoretical requirements). Compare transferability performance against the original MAPPO implementation. This would directly test whether critic architecture explains the MAPPO underperformance at larger scales.