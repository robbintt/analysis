---
ver: rpa2
title: 'KaVa: Latent Reasoning via Compressed KV-Cache Distillation'
arxiv_id: '2510.02312'
source_url: https://arxiv.org/abs/2510.02312
tags:
- latent
- oken
- reasoning
- tokens
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training efficient latent
  reasoning models in large language models, which traditionally require verbose chain-of-thought
  traces that incur high computational costs and memory overhead. The authors propose
  KaVa, a novel framework that distills knowledge from a compressed teacher KV-cache
  into a latent reasoning student via self-distillation, leveraging the continuous
  nature of latent tokens to align stepwise KV trajectories without requiring direct
  token correspondence.
---

# KaVa: Latent Reasoning via Compressed KV-Cache Distillation

## Quick Facts
- arXiv ID: 2510.02312
- Source URL: https://arxiv.org/abs/2510.02312
- Authors: Anna Kuzina; Maciej Pioro; Paul N. Whatmough; Babak Ehteshami Bejnordi
- Reference count: 40
- Primary result: Latent reasoning model with compressed KV-cache distillation achieving 62-92% fewer forward passes while preserving accuracy

## Executive Summary
This paper addresses the challenge of training efficient latent reasoning models in large language models, which traditionally require verbose chain-of-thought traces that incur high computational costs and memory overhead. The authors propose KaVa, a novel framework that distills knowledge from a compressed teacher KV-cache into a latent reasoning student via self-distillation, leveraging the continuous nature of latent tokens to align stepwise KV trajectories without requiring direct token correspondence. Empirical results show that KaVa consistently outperforms strong latent reasoning baselines on both artificial and natural-language reasoning datasets, with markedly smaller performance degradation when moving from equation-only to natural-language traces, and achieves 62-92% fewer forward passes compared to full CoT while preserving accuracy.

## Method Summary
KaVa introduces a framework for training efficient latent reasoning models by distilling knowledge from a compressed teacher KV-cache. The approach leverages the continuous nature of latent tokens to align stepwise KV trajectories between teacher and student models without requiring direct token correspondence. The method employs self-distillation where the student learns to compress and decompress KV-caches effectively while maintaining reasoning performance. The compressed KV-cache distillation allows the student to capture essential reasoning patterns without the computational overhead of full chain-of-thought traces.

## Key Results
- KaVa consistently outperforms strong latent reasoning baselines on both artificial and natural-language reasoning datasets
- Achieves 62-92% fewer forward passes compared to full CoT while preserving accuracy
- Shows markedly smaller performance degradation when moving from equation-only to natural-language traces

## Why This Works (Mechanism)
KaVa works by leveraging the continuous nature of latent tokens to align stepwise KV trajectories between teacher and student models without requiring direct token correspondence. The compressed KV-cache distillation captures essential reasoning patterns while eliminating the computational overhead of full chain-of-thought traces. The self-distillation framework allows the student to learn efficient compression and decompression strategies that preserve reasoning capabilities. The continuous latent space enables smooth transitions and alignments between teacher and student states, making the distillation process more effective than discrete token-level approaches.

## Foundational Learning

**KV-cache compression** - Why needed: Reduces memory overhead and computational cost during inference; Quick check: Verify compression ratio vs. accuracy trade-off

**Latent reasoning** - Why needed: Enables reasoning without explicit token-by-token generation; Quick check: Compare performance on reasoning tasks with and without latent space

**Self-distillation** - Why needed: Allows student to learn from compressed teacher representations; Quick check: Measure performance improvement from distillation vs. direct training

**Continuous latent alignment** - Why needed: Enables smooth KV-trajectory matching without exact token correspondence; Quick check: Visualize latent space trajectories for alignment quality

**Chain-of-thought reasoning** - Why needed: Provides baseline for reasoning performance comparison; Quick check: Measure accuracy difference between CoT and latent approaches

## Architecture Onboarding

**Component map**: Teacher KV-cache -> Compression module -> Student latent model -> Decompression module -> Final predictions

**Critical path**: Input reasoning task -> Teacher KV-cache generation -> KV-cache compression -> Student latent reasoning -> Decompression and output generation

**Design tradeoffs**: Compression ratio vs. reasoning accuracy, latent dimensionality vs. computational efficiency, distillation strength vs. training stability

**Failure signatures**: Performance degradation on complex reasoning tasks, increased error rates on natural language inputs, instability during KV-cache compression

**3 first experiments**: 1) Baseline comparison on artificial reasoning datasets, 2) Natural language reasoning performance evaluation, 3) Ablation study on compression ratios

## Open Questions the Paper Calls Out
None

## Limitations
- Approach relies heavily on teacher model quality and availability of high-quality CoT traces
- Assumes smooth, continuous transitions between latent states without analysis of trajectory divergence
- Ablation study focuses on dataset variations rather than architectural choices like compression ratio or latent dimensionality
- Computational overhead claims based on synthetic datasets may not reflect real-world deployment complexity

## Confidence

- **Latent reasoning performance improvement**: High confidence - results are consistently superior across multiple benchmarks with clear statistical margins
- **Reduced computational overhead**: Medium confidence - forward pass reduction is well-measured, but total system overhead remains unquantified
- **Natural language generalization**: Medium confidence - improvement over baselines is shown, but comparison lacks analysis of failure modes or task complexity

## Next Checks

1. Evaluate performance degradation under noisy or incomplete teacher KV-caches to assess robustness to teacher quality variations
2. Test scalability on long-chain reasoning tasks (e.g., multi-hop mathematical proofs) to validate continuous latent alignment assumptions
3. Measure end-to-end latency including KV-cache compression/decompression overhead to confirm computational benefits in practical deployments