---
ver: rpa2
title: Scaled Signed Averaging Improves In-Context and Early Learning Benchmark Performance
  in Small Transformers
arxiv_id: '2508.14685'
source_url: https://arxiv.org/abs/2508.14685
tags:
- softmax
- training
- task
- attention
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Scaled Signed Averaging (SSA) improves in-context and early learning
  benchmark performance in small transformers by replacing softmax with a parameterized
  scoring function that avoids saturation and promotes more balanced attention. SSA
  substantially enhances generalization in simple semantic tasks involving quantifiers
  and linear function prediction, where softmax-based models fail due to boundary
  effects and concentration on single tokens.
---

# Scaled Signed Averaging Improves In-Context and Early Learning Benchmark Performance in Small Transformers

## Quick Facts
- arXiv ID: 2508.14685
- Source URL: https://arxiv.org/abs/2508.14685
- Authors: Omar Naim; Swarnadeep Bhar; Jérôme Bolte; Nicholas Asher
- Reference count: 40
- Primary result: SSA improves in-context and early learning benchmark performance in small transformers by replacing softmax with a parameterized scoring function that avoids saturation and promotes more balanced attention.

## Executive Summary
This paper introduces Scaled Signed Averaging (SSA), a novel attention scoring function that replaces softmax in transformer models. SSA uses a parameterized polynomial growth mechanism that mitigates softmax's tendency to saturate and concentrate attention on single tokens, especially under distribution shift. The authors demonstrate substantial improvements on synthetic in-context learning tasks involving quantifiers and linear functions, as well as on standard NLP benchmarks under zero- and few-shot settings. SSA achieves these gains without increasing model complexity, requiring only two additional learnable parameters per attention head.

## Method Summary
The method replaces softmax with SSA(z)_i = (1 + b|z_i|)^sgn(z_i)^n / Σ_k(1 + b|z_k|)^sgn(z_k)^n, where b > 0 and n ≥ 1 are learned per attention head. The authors train decoder-only GPT-2 style models (12 layers, 8 heads, 256 dim for ICL; 124M parameters for NLP) and encoder models on child-directed speech, comparing SSA against softmax baselines. Training uses Adam optimizer with learning rate 10^-4, batch size 64, and 500k steps for ICL or 50k steps for NLP. The approach is evaluated on synthetic in-context learning tasks (quantifier prediction, linear function regression) and standard NLP benchmarks (perplexity, zero/few-shot accuracy on ARC, HellaSwag, LAMBADA, SuperGLUE).

## Key Results
- SSA reduces MSE by 4-10× on shifted function distributions compared to softmax in ICL tasks
- On NLP benchmarks, SSA shows largest gains on RTE (+24.3% zero-shot), WSC (+29.9% zero-shot), CB (+160% zero-shot)
- SSA improves grammatical probing accuracy across syntactic and morphological tests in encoder models
- Theoretical analysis shows SSA grows polynomially and resists hardmax-like collapse, unlike softmax's exponential growth

## Why This Works (Mechanism)

### Mechanism 1
Replacing softmax's exponential with polynomial scoring reduces attention saturation on out-of-distribution tokens. SSA uses `(1 + b|x|)^sgn(x)^n` where b > 0 and n ≥ 1 are learned per-head. This grows polynomially (∼x^n) rather than exponentially, so large logit gaps produce slower score concentration. When a deviant token has much larger norm than others, softmax assigns it probability approaching 1 while SSA maintains non-trivial weights on all tokens.

### Mechanism 2
SSA's self-regularizing gradients prevent runaway sensitivity to large-magnitude inputs. The effective slope g(x) = f'(x)/f(x) = nb/(1 + b|x|) tends toward 0 as |x| → ∞, meaning extremely large logits exert vanishing per-unit influence. Softmax has g(x) ≡ 1, so each unit increase in logit gap produces constant multiplicative effect on the ratio.

### Mechanism 3
SSA improves ICL generalization specifically on tasks requiring aggregation across sequence positions. Quantification ("every", "some") and linear function prediction require the model to attend to multiple exemplars. Softmax's concentration on maximum-norm tokens prevents proper aggregation. SSA maintains distributed attention, allowing the model to compute properties like "all elements positive" or linear coefficients from multiple examples.

## Foundational Learning

- **Attention scoring functions normalize dot-product similarities into probability distributions**
  - Why needed: SSA replaces softmax as the normalization mechanism. Understanding that attention = weighted averaging where weights come from the scoring function is prerequisite to appreciating why changing softmax affects model behavior.
  - Quick check: Given query q and keys k₁, k₂, k₃, does softmax(q·k) always assign positive weight to all keys? Does SSA?

- **Distribution shift in ICL—training vs test distributions can differ in input range, sequence length, or function parameters**
  - Why needed: The paper's core finding is that softmax models fail when test distributions (D^test_I, D^test_F) differ from training. SSA's advantage appears specifically under shift.
  - Quick check: If a model is trained on sequences from N(0,1) and tested on N(0,5), is this distribution shift? What about sequence lengths 40 → 200?

- **Saturation/hardmax collapse—when one logit dominates, attention becomes essentially single-token**
  - Why needed: Softmax's exponential means a gap of ~4 between largest and second-largest logit assigns ~98% weight to the maximum. This is the failure mode SSA addresses.
  - Quick check: For logits [5, 1, 0], what's the approximate softmax weight on the maximum? What about SSA with b=1, n=2?

## Architecture Onboarding

- **Component map**: Input sequence → Q/K/V projections → SSA scoring (replaces softmax) → attention weights → weighted sum of values → output
- **Critical path**: 1) Identify attention computation location in codebase 2) Replace `softmax(scores / sqrt(d_k))` with `SSA(scores)` per Equation 5 3) Initialize b=1, n=1.5 per head; make both trainable 4) Validate normalization sums to 1 for any input vector
- **Design tradeoffs**: Higher n → more softmax-like concentration, better for tasks needing focused attention; lower n → more uniform attention, better for aggregation tasks but may dilute signal; shared vs per-head parameters: paper uses per-head for flexibility; assumption: paper only tests n=1.5 and n=2 for encoder models; optimal values likely task-dependent
- **Failure signatures**: If attention weights become nearly uniform (entropy very high), n may be too low—model failing to discriminate relevant tokens; if performance matches softmax exactly, check that b and n are actually being learned; if training becomes unstable, b may be growing too large—consider clipping or regularization on b
- **First 3 experiments**: 1) **Ablation on n**: Train with fixed n ∈ {1, 1.5, 2, 3} on a simple ICL task (linear functions). Plot generalization vs. distribution shift. Expect n=1.5-2 to balance in-distribution accuracy and out-of-distribution robustness. 2) **Attention visualization**: On a deviant sequence (e.g., containing a large out-of-distribution value), visualize softmax vs. SSA attention maps. Confirm softmax collapses to single token while SSA maintains distributed attention. 3) **Perplexity comparison**: Train two GPT-2-small models (softmax vs. SSA) on same corpus for 10k-50k steps. Compare perplexity on held-out set and zero-shot performance on 2-3 benchmarks (e.g., ARC-Easy, HellaSwag). Expect 5-15% perplexity reduction with SSA.

## Open Questions the Paper Calls Out

### Open Question 1
Do the generalization benefits of SSA persist when scaling to Large Language Models (e.g., 7B+ parameters), or are they primarily a regularization effect for small models? The authors state in the Limitations section that they were "unable to scale models beyond 124M parameters" due to hardware constraints. Training runs of 7B+ parameter models using SSA on standard corpora, comparing perplexity and downstream benchmark performance against Softmax baselines would resolve this.

### Open Question 2
Can SSA be combined with other architectural modifications to handle scenarios where both the input distribution and the target function distribution diverge significantly from training? The Limitations section notes that "SSA struggles to generalize in scenarios where both the input $x_i$ and the test-time function distribution $D_{test}$ diverge significantly from the training distribution." Ablation studies or hybrid architectures tested on the linear function task where both $D_{test_I}$ and $D_{test_F}$ are shifted simultaneously, showing recovered performance, would resolve this.

### Open Question 3
Does SSA degrade performance on tasks that strictly require sharp, sparse attention (e.g., exact retrieval or copying) due to its resistance to "hardmax-like collapse"? While SSA is designed to promote "distributed behaviors" and resist hardmax collapse, the paper does not evaluate tasks where attention *should* concentrate entirely on a single token. Comparative evaluation on discrete retrieval benchmarks to measure if SSA's softer focus results in precision loss would resolve this.

## Limitations

- SSA's benefits are primarily demonstrated on small transformers (124M parameters) and may not scale to larger models where attention patterns are more sophisticated
- The paper does not explore the full parameter space of b and n, leaving optimal values task-dependent and potentially suboptimal for some applications
- SSA struggles with "double shift" scenarios where both input and function distributions diverge simultaneously from training

## Confidence

**High Confidence**: SSA improves performance on the specific synthetic ICL tasks (quantification, linear functions) where softmax exhibits clear saturation failures. The empirical evidence is direct and the mechanism (polynomial vs exponential growth) is well-understood.

**Medium Confidence**: SSA provides consistent improvements across the NLP benchmark suite (perplexity reduction, zero/few-shot accuracy gains). While results are positive across multiple tasks, the effect sizes vary significantly by task, and some improvements may be influenced by baseline model quality.

**Low Confidence**: SSA's theoretical advantage of polynomial growth translates to practical benefits in all distribution shift scenarios. The paper demonstrates benefits for specific types of shifts but does not test other forms like sequence length changes, concept drift, or adversarial perturbations.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary n ∈ {1, 1.5, 2, 3} and b ∈ {0.1, 0.5, 1, 2} on the linear function ICL task and plot generalization error vs. distribution shift magnitude. This would reveal whether the claimed n=1.5-2 range is optimal or if benefits depend on specific parameter settings.

2. **Distribution Shift Taxonomy**: Design a comprehensive test suite that includes not just input range shifts but also sequence length changes (40→200), function type shifts (linear→quadratic), and adversarial perturbations. Compare softmax vs. SSA performance across all shift types to determine whether polynomial growth specifically helps with magnitude-based shifts or provides broader robustness.

3. **Scaling Study**: Train SSA and softmax models at increasing sizes (124M, 355M, 1.3B parameters) on the same NLP corpus and evaluate zero-shot performance on the full SuperGLUE benchmark. Measure whether SSA's relative improvement (percentage gain over softmax) remains constant, increases, or decreases with model size.