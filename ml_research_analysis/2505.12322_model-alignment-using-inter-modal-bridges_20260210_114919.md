---
ver: rpa2
title: Model alignment using inter-modal bridges
arxiv_id: '2505.12322'
source_url: https://arxiv.org/abs/2505.12322
tags:
- paired
- alignment
- cost
- neural
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a semi-supervised approach for aligning model
  representations across different modalities using conditional flow matching. The
  method leverages inter-modal bridge costs that combine intra-space distances and
  paired samples to compute optimal couplings, either through true alignment using
  labelled pairs or global alignment via optimal transport.
---

# Model alignment using inter-modal bridges

## Quick Facts
- arXiv ID: 2505.12322
- Source URL: https://arxiv.org/abs/2505.12322
- Reference count: 40
- Primary result: Semi-supervised inter-modal alignment method achieving downstream performance comparable to end-to-end trained models when labeled data is scarce (<20%)

## Executive Summary
This paper introduces a semi-supervised approach for aligning model representations across different modalities using conditional flow matching. The method leverages inter-modal bridge costs that combine intra-space distances and paired samples to compute optimal couplings, either through true alignment using labelled pairs or global alignment via optimal transport. The authors evaluate their approach on image-text alignment tasks (MNIST, ImageNet) and biological-to-artificial neural representation alignment (Majaj et al. dataset). Their method achieves downstream task performance comparable to end-to-end trained models, particularly when labelled training data is scarce (<20%), demonstrating effective inter-modal model alignment with minimal supervision.

## Method Summary
The proposed method performs inter-modal model alignment by estimating a velocity field that transports representations from one modality to another. It operates on frozen pre-trained feature extractors, requiring only minimal paired anchor points. The approach uses conditional flow matching to learn this transport, with an inter-modal bridge cost that combines intra-space distances and paired samples. Two strategies are employed: true alignment (for a subset of samples using labelled pairs) and global alignment (for the full space using optimal transport). The method avoids end-to-end fine-tuning while achieving competitive performance on downstream tasks.

## Key Results
- Achieved downstream task performance comparable to end-to-end trained models, particularly when labelled training data is scarce (<20%)
- Demonstrated effective alignment across multiple modality pairs: MNIST images to text, ImageNet images to text, and biological to artificial neural representations
- Showed that higher overlap in latent spaces correlates with better alignment performance
- Successfully aligned models across 1.2 million samples on ImageNet without fine-tuning the pre-trained feature extractors

## Why This Works (Mechanism)
The method works by treating model alignment as an optimal transport problem, where the goal is to find a transport map that minimizes the total cost of moving representations from one modality to another. By using conditional flow matching, the approach learns a velocity field that gradually transforms one representation space into another while preserving local structure. The inter-modal bridge costs serve as anchors that guide the alignment process by providing paired samples as reference points. The use of frozen pre-trained feature extractors ensures that the alignment process doesn't require retraining and leverages existing high-quality representations.

## Foundational Learning

**Optimal Transport Theory**: The mathematical framework for finding the most efficient way to move mass from one distribution to another. Needed to formulate the alignment problem as a transport problem between representation spaces. Quick check: Can be verified by examining the optimal transport cost formulation in the method.

**Conditional Flow Matching**: A technique for learning transport maps by estimating velocity fields conditioned on source and target distributions. Needed to avoid computing full transport maps which can be computationally expensive. Quick check: Look for the velocity field estimation in the implementation.

**Inter-modal Bridge Costs**: A metric that combines intra-space distances with paired samples to guide the alignment process. Needed to provide anchor points for the transport without requiring full paired data. Quick check: Verify the bridge cost formulation combines both intra-space and inter-space components.

**Representation Disentanglement**: The degree to which latent representations separate different factors of variation. Needed because more disentangled representations may facilitate easier alignment across modalities. Quick check: Check correlation between disentanglement metrics and alignment performance.

**Frozen Feature Extractors**: Pre-trained models whose parameters are not updated during alignment. Needed to leverage existing high-quality representations without the computational cost of retraining. Quick check: Confirm feature extractors remain unchanged during alignment optimization.

## Architecture Onboarding

**Component Map**: Pre-trained Feature Extractors (frozen) -> Conditional Flow Matching Engine -> Velocity Field -> Transported Representations -> Downstream Task Performance

**Critical Path**: The alignment process depends critically on accurate intra-space distance metrics and reliable paired anchor points. The conditional flow matching must converge to a meaningful velocity field that preserves local structure while achieving global alignment.

**Design Tradeoffs**: The method trades computational efficiency (by using frozen models) for potential alignment accuracy (compared to end-to-end fine-tuning). It also balances the need for paired data (required for anchors) against the desire for minimal supervision.

**Failure Signatures**: Alignment may fail when latent spaces have low overlap, when intra-space distance metrics are poorly calibrated, or when paired anchor points are insufficient or noisy. The method may also struggle with highly entangled representations that don't separate modality-specific factors.

**First Experiments**:
1. Verify alignment performance on a simple paired dataset (e.g., MNIST images and their labels) to establish baseline functionality
2. Test sensitivity to the number of paired anchor points by varying P and measuring performance degradation
3. Compare alignment quality using different intra-space distance metrics (e.g., Euclidean vs. cosine distance) to identify optimal choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the inter-modal bridge cost function be adapted to maintain alignment performance in a purely unsupervised setting where no paired anchor points are available?
- Basis in paper: [explicit] The Conclusion states: "Separately, our future work will extend our alignment approach to the scenarios where the assumption about paired data points is relaxed."
- Why unresolved: The current method fundamentally relies on a set of paired samples $P$ to define the bridge cost ($C^{bridge}_{XY}$), specifically by setting the cost between known pairs to zero to act as anchors. The formulation does not currently support a mechanism for bridging spaces without these initial links.
- What evidence would resolve it: Deriving a new cost metric that utilizes intra-space geometry (e.g., manifold alignment) without inter-space labels, and demonstrating that the resulting velocity field achieves comparable downstream task accuracy to the semi-supervised baseline.

### Open Question 2
- Question: Can the global alignment strategy be modified to overcome the quadratic memory complexity, allowing it to scale to datasets significantly larger than ImageNet without the convergence failures seen in local alignment?
- Basis in paper: [inferred] Section 3.2 notes that global alignment "incurs a significant memory cost," while Section 5.1.5 reports that the memory-efficient "local alignment failed to converge" under the allotted time budget for ImageNet.
- Why unresolved: The paper presents a dichotomy: global alignment is accurate but computationally expensive ($O(N^2)$ memory), while local alignment is memory-efficient but unstable/inaccurate. A method that bridges this specific trade-off is not explored.
- What evidence would resolve it: Integrating a scalable Optimal Transport solver (e.g., low-rank or sliced OT) into the global alignment pipeline and successfully applying it to datasets exceeding 1.2 million samples without memory overflow.

### Open Question 3
- Question: Does enforcing higher disentanglement in the frozen pre-trained feature extractors lead to linear improvements in inter-modal alignment quality?
- Basis in paper: [explicit] The Conclusion suggests: "Future work should focus on developing more disentangled representations to improve model reusability across modalities."
- Why unresolved: While Section 5.1.2 shows a correlation between latent space overlap and performance, it does not isolate "disentanglement" as a variable. It remains unclear if the alignment failures are due to the alignment method or the inherent entanglement of the frozen features.
- What evidence would resolve it: A controlled experiment where the alignment method is applied to the same data encoded by models with varying degrees of disentanglement (e.g., varying $\beta$ in a VAE), showing a monotonic relationship between disentanglement metrics and alignment accuracy.

## Limitations
- Evaluation focused primarily on image-text alignment tasks with relatively simple datasets (MNIST, ImageNet subsets)
- Computational efficiency of optimal transport-based global alignment approach not thoroughly analyzed
- Limited comparison of performance in fully supervised settings where labeled data is abundant
- Generalizability to more complex modality pairs (e.g., audio-text, video-text) remains untested

## Confidence

**Performance Claims**: Medium
- The method shows promising results in low-data regimes but limited evaluation in fully supervised settings
- Claims of "comparable performance" need qualification based on data availability

**Scalability Claims**: Low-Medium
- Memory-efficient local alignment failed to converge on ImageNet-sized datasets
- Global alignment incurs quadratic memory costs that limit scalability

**Minimal Supervision Claims**: Medium
- While the method reduces label dependency compared to end-to-end approaches, it still requires paired samples and accurate distance metrics

## Next Checks

1. Evaluate the method on more diverse modality pairs (e.g., audio-text, video-text) and higher-dimensional datasets to assess scalability and robustness

2. Conduct ablation studies to quantify the contribution of each component (conditional flow matching, inter-modal bridge costs, optimal transport) to overall performance

3. Compare computational efficiency and memory requirements against alternative alignment approaches across different dataset sizes and modality complexities