---
ver: rpa2
title: 'LESER: Learning to Expand via Search Engine-feedback Reinforcement in e-Commerce'
arxiv_id: '2509.05570'
source_url: https://arxiv.org/abs/2509.05570
tags:
- query
- search
- retrieval
- expansion
- leser
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LESER, a novel reinforcement learning framework
  for context-aware query expansion in e-commerce search. The core idea is to fine-tune
  a large language model (LLM) using real-time search engine feedback as rewards,
  formulated via Group Relative Policy Optimization (GRPO).
---

# LESER: Learning to Expand via Search Engine-feedback Reinforcement in e-Commerce

## Quick Facts
- **arXiv ID:** 2509.05570
- **Source URL:** https://arxiv.org/abs/2509.05570
- **Reference count:** 39
- **Primary result:** 72.84% retrieval gain, 32.40% relevance gain over baselines including GPT-4.1

## Executive Summary
LESER introduces a novel reinforcement learning framework for context-aware query expansion in e-commerce search. The system fine-tunes a large language model using real-time search engine feedback as rewards, formulated via Group Relative Policy Optimization (GRPO). By generating multiple expansions per query optimized for both retrieval relevance and coverage, LESER significantly outperforms traditional embedding-based methods and even GPT-4.1. The approach addresses the one-to-many mapping challenge in query expansion while maintaining format constraints essential for production deployment.

## Method Summary
LESER employs a reinforcement learning framework where a context-aware LLM generates query expansions optimized through search engine feedback. The process begins with supervised fine-tuning on 10,000 examples to ensure format compliance, followed by GRPO training. For each input query, the model retrieves top-k items from the search engine, extracts product attributes as context, and generates N=4 candidate expansion sets at high temperature. The search engine then evaluates each candidate's retrieval performance, computing rewards based on relevance and coverage metrics. The policy is updated using group-relative advantages, enabling stable learning despite the one-to-many nature of query expansion.

## Key Results
- 72.84% retrieval gain and 32.40% relevance gain compared to embedding-based and supervised baselines
- Outperforms GPT-4.1 on both offline metrics and online A/B tests
- 57.45% retrieval gain with warm-up+GRPO versus 12.80% without warm-up
- Successfully deployed in production with pre-computed expansions for 600K high-frequency queries

## Why This Works (Mechanism)

### Mechanism 1: Search-Engine-in-the-Loop Reward Grounding
The system uses real-time search engine feedback as reward signals, enabling learning grounded in actual catalog content rather than abstract semantic similarity. For each generated expansion set Y, items are retrieved from the production search engine and rewards computed based on average relevance score of top-k results and total unique items retrieved. The reward function r(Y) = r_rel(Y) + λ · r_size(Y) normalizes against the original query's baseline performance.

### Mechanism 2: Group Relative Policy Optimization for Multi-Valid Expansions
GRPO enables stable learning when multiple expansion strategies may be equally valid by comparing candidates relative to each other rather than against absolute targets. For each input, the policy samples N=4 candidate expansion sets at high temperature (τ=0.9). The group-relative advantage A^(i) = r^(i) - (1/N)Σr^(j) compares each candidate's reward to the group mean, handling the one-to-many mapping problem where multiple expansions may be legitimate.

### Mechanism 3: Supervised Warm-Up for Format Stability
A brief supervised fine-tuning phase before GRPO training prevents the model from generating syntactically invalid outputs that would break reward computation. The model is fine-tuned on 10,000 examples where GPT-4.1 generates reasoning (enclosed in  Hentai tags) and structured answers (in  tags). This teaches format adherence before RL exploration begins, as invalid outputs receive zero reward during GRPO.

## Foundational Learning

- **Pseudo-Relevance Feedback (PRF):** LESER treats top-k retrieved items as a "pseudo-relevant" context set, extracting attributes like {"Intended Use": "Back & Lumbar Support"} to ground expansions. Understanding PRF explains why this works despite no explicit relevance labels. *Quick check:* Given a query "wireless earbuds" retrieving products with attributes {"Connectivity": "Bluetooth 5.0"}, how would PRF use this versus what LESER does differently?

- **Policy Gradient with Baselines (PPO/TRPO concepts):** GRPO builds on policy optimization fundamentals—the clipped objective, KL regularization, and advantage computation all derive from PPO. Understanding why baselines reduce variance helps explain why group-relative advantages work. *Quick check:* In GRPO, if all N=4 candidates receive identical rewards, what happens to the advantage values and subsequent gradient updates?

- **Reward Shaping for Multi-Objective Optimization:** The reward function balances relevance (r_rel) and coverage (r_size) with hyperparameter λ=0.1. Understanding the trade-off between exploration (more items) and exploitation (higher relevance) is critical for tuning. *Quick check:* If λ is increased from 0.1 to 1.0, how would the model's expansion behavior change qualitatively?

## Architecture Onboarding

- **Component map:** User Query → Context Retrieval → Top-k Items + Attributes → SFT-Warmed LLM → N=4 Candidate Expansion Sets → Search Engine → Retrieval Results per Candidate → Reward Calculator → GRPO Update → Policy θ

- **Critical path:** The reward calculation depends on three sequential calls to the production search engine: (1) initial context retrieval, (2) retrieval for each of N=4 candidates during training, (3) relevance scoring via the internal GBDT model. Latency here directly impacts training throughput.

- **Design tradeoffs:** LLaMA 3.2-3B chosen over 3.1-8B (76.0% vs 72.84% retrieval gain) due to "latency constraints in potential real-time online deployments"; Warm-up adds engineering complexity but prevents training collapse from malformed outputs; Online deployment uses "pre-computed expansions from a cache for around 600K high-frequency queries"—this trades coverage for latency.

- **Failure signatures:**
  - Format collapse: Model generates text outside  Hentai/<answer> structure → reward always 0 → no learning signal. Solution: stronger warm-up or format-constrained decoding.
  - Reward hacking: Model generates expansions that maximize retrieval size (r_size) while ignoring relevance (r_rel). Solution: increase λ or add hard relevance threshold.
  - Context noise: If initial retrieval returns irrelevant items, extracted attributes mislead expansions. Solution: filter context by relevance score threshold.

- **First 3 experiments:**
  1. **Baseline format check:** Run the pretrained LLaMA 3.2-3B on 100 queries with the LESER prompt. Measure what percentage produce valid JSON in <answer> tags. This establishes why warm-up is necessary.
  2. **Reward component ablation:** Train three variants with (a) r_rel only, (b) r_size only, (c) combined reward. Compare retrieval gain and relevance gain to understand the λ trade-off.
  3. **Context size sensitivity:** Vary the context set from k=1 to k=20 items. Plot performance vs. latency to find the inflection point where additional context stops improving expansions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LESER be extended to other search-related tasks beyond query expansion while maintaining its reinforcement learning benefits?
- **Basis in paper:** "Looking forward, we see opportunities to extend LESER to more search related tasks to further boost robustness and interpretability in deployments."
- **Why unresolved:** The paper demonstrates LESER only for query expansion; no experiments or architectural details address adaptation to tasks like ranking, filtering, or recommendation.
- **What evidence would resolve it:** Empirical results applying LESER's search-engine-in-the-loop RL framework to tasks such as result ranking or product recommendation, with comparable performance gains.

### Open Question 2
- **Question:** How can LESER effectively serve long-tail queries that are not covered by the pre-computed expansion cache?
- **Basis in paper:** Online deployment uses "pre-computed expansions from a cache for around 600K high-frequency queries," leaving tail queries potentially unserved in real-time.
- **Why unresolved:** The paper does not discuss real-time expansion generation latency, cost, or quality for queries outside the high-frequency cache.
- **What evidence would resolve it:** Analysis of LESER's latency and expansion quality for tail queries in a fully online (non-cached) setting, compared to cached high-frequency queries.

### Open Question 3
- **Question:** Is a fixed relevance-coverage trade-off (λ=0.1) optimal across diverse query types, or should λ be adaptive?
- **Basis in paper:** The reward function uses a static hyperparameter λ=0.1, but query types vary widely in ambiguity and intent breadth.
- **Why unresolved:** The paper treats λ as a global constant without ablation or query-type-specific analysis.
- **What evidence would resolve it:** Experiments showing performance impact of varying λ per query category (e.g., specific product names vs. broad problem-oriented queries) or learning λ adaptively.

### Open Question 4
- **Question:** How robust is LESER to biases or errors in the GBDT-based relevance model used for reward computation?
- **Basis in paper:** Rewards depend on a GBDT relevance model trained on human labels, which may have systematic biases or labeling noise.
- **Why unresolved:** The paper does not analyze sensitivity of learned expansions to relevance model quality or simulate degraded relevance signals.
- **What evidence would resolve it:** Experiments with perturbed or alternative relevance models, measuring degradation in expansion quality and downstream retrieval performance.

## Limitations

- **Offline-to-Online Transfer Gap:** Strong offline metrics reported, but actual online impact is described only qualitatively without detailed metrics
- **Reward Function Stability:** λ=0.1 balance is heuristic with no ablation shown for different λ values
- **Format Dependency Fragility:** System requires strict adherence to specific tags; any degradation would break the pipeline

## Confidence

- **High Confidence:** Core GRPO framework with group-relative advantages is theoretically sound and well-established in RL literature
- **Medium Confidence:** Search-engine-as-oracle reward design is innovative but relies on untested assumption that production search feedback correlates with user satisfaction
- **Low Confidence:** Claim that "LLaMA 3.2-3B outperforms LLaMA 3.1-8B" is surprising and potentially dataset-specific; online A/B test results presented without quantitative details

## Next Checks

1. **Reward Function Sensitivity Analysis:** Train LESER with λ ∈ {0.01, 0.1, 0.5, 1.0} and measure how retrieval gain, relevance gain, and unique items retrieved vary to identify optimal λ value.

2. **Offline-to-Online Correlation Study:** Run a controlled experiment evaluating the same queries both offline (using the reward function) and online (measuring CTR, conversion rate) to validate whether offline metrics predict online success.

3. **Format Compliance Monitoring:** Deploy a shadow system logging format compliance rates for both warm-up-trained and GRPO-only variants to quantify the warm-up phase's necessity in production.