---
ver: rpa2
title: 'Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling
  Hallucination in Translation'
arxiv_id: '2510.24073'
source_url: https://arxiv.org/abs/2510.24073
tags:
- hallucination
- translation
- source
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HalloMTBench, a multilingual benchmark for
  diagnosing translation hallucinations in Large Language Models (LLMs). The authors
  propose a taxonomy distinguishing Instruction Detachment (e.g., untranslated content,
  wrong target language) from Source Detachment (e.g., extraneous additions, repetition).
---

# Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation

## Quick Facts
- **arXiv ID**: 2510.24073
- **Source URL**: https://arxiv.org/abs/2510.24073
- **Reference count**: 5
- **Primary result**: Introduced HalloMTBench, a multilingual benchmark with 5,435 high-quality hallucination instances across 11 language pairs, achieving 93.68-100% agreement with human annotations

## Executive Summary
This paper introduces HalloMTBench, a comprehensive multilingual benchmark designed to diagnose translation hallucinations in Large Language Models (LLMs). The authors propose a systematic taxonomy distinguishing Instruction Detachment (issues with following translation instructions) from Source Detachment (content unrelated to the source text). Using a rigorous multi-stage annotation pipeline involving LLM judges and expert validation, they curated 5,435 high-quality hallucination instances across 11 English-to-X language pairs. The benchmark achieves strong agreement with human annotations (93.68-100%) and reveals distinct hallucination patterns tied to model scale, source length, linguistic bias, and reinforcement learning effects.

## Method Summary
The authors developed a three-stage pipeline to create HalloMTBench. First, four frontier LLMs (GPT-4o, GPT-4o-mini, Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct) generated translation candidates from 100 publicly available MT datasets. Second, three LLM judges evaluated these candidates against a refined taxonomy of hallucination types, producing an initial filtered dataset. Third, domain experts validated and annotated the remaining instances, resulting in 5,435 high-quality hallucination examples. The benchmark was then used to evaluate 17 LLMs across 11 language pairs, revealing systematic patterns in hallucination types related to model characteristics and linguistic properties.

## Key Results
- HalloMTBench achieved 93.68-100% agreement with human annotations, demonstrating high reliability
- Instruction Detachment (untreated content, wrong target language) and Source Detachment (extraneous additions, repetition) were the two primary hallucination categories
- Model scale, source length, linguistic bias, and RLHF-induced language mixing emerged as key hallucination triggers
- Evaluation revealed distinct performance patterns across the 17 tested LLMs, with variations tied to model architecture and training approaches

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic taxonomy that distinguishes between instruction-following failures and source-text detachment, enabling precise diagnosis of hallucination types. The multi-stage annotation pipeline (LLM judges plus expert validation) ensures high data quality while scaling to thousands of instances. The focus on English-to-X translation directions controls for source language variability while enabling cross-linguistic analysis of hallucination patterns.

## Foundational Learning
- **Hallucination Taxonomy**: Understanding the distinction between Instruction Detachment and Source Detachment is crucial for diagnosing translation failures. Why needed: Provides systematic framework for categorizing and analyzing translation errors. Quick check: Can you identify which type of detachment occurred in a given translation error?
- **Multi-stage Annotation**: LLM judges combined with expert validation enables scalable yet reliable data curation. Why needed: Balances efficiency with accuracy in creating high-quality benchmark data. Quick check: Does the annotation pipeline maintain consistency across different annotators and stages?
- **Cross-linguistic Analysis**: Testing across 11 language pairs reveals patterns tied to linguistic properties. Why needed: Enables understanding of how language-specific factors influence hallucination types. Quick check: Are hallucination patterns consistent across typologically different languages?

## Architecture Onboarding

**Component Map**: Translation Datasets -> Candidate Generation (4 LLMs) -> LLM Judge Evaluation (3 judges) -> Expert Validation -> HalloMTBench

**Critical Path**: The core pipeline follows: dataset selection → candidate generation → initial filtering by LLM judges → expert validation → final benchmark creation. Each stage progressively refines the data quality while maintaining scalability.

**Design Tradeoffs**: Using the same LLMs for both candidate generation and judging improves efficiency but risks overfitting to specific model behaviors. Expert validation mitigates this concern but reduces scalability. The English-to-X focus controls variables but limits generalizability to other translation directions.

**Failure Signatures**: High hallucination rates in longer source texts suggest attention mechanism limitations. Instruction Detachment issues indicate problems with instruction-following capabilities. Source Detachment patterns reveal challenges in maintaining source fidelity during generation.

**First Experiments**:
1. Test benchmark's sensitivity by comparing LLM judge evaluations with human annotations on a subset of instances
2. Evaluate a small set of LLMs using only expert annotations to verify the reliability of the automated pipeline
3. Conduct ablation studies removing specific annotation stages to quantify their contribution to data quality

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, focusing instead on presenting the benchmark and initial findings.

## Limitations
- Potential overfitting risk since the same LLMs were used for both candidate generation and judging
- Limited to English→X translation directions, restricting generalizability to other language pairs
- Focus on publicly available datasets may not capture all translation domains and use cases

## Confidence
- **High**: Data quality and annotation reliability (93.68-100% human agreement)
- **High**: Basic taxonomy validity (Instruction vs Source Detachment distinction)
- **Medium**: Cross-lingual pattern findings (limited to English→X)
- **Medium**: Model-specific conclusions (risk of overfitting given shared model usage)

## Next Checks
1. Re-evaluate model rankings using human annotations exclusively for a subset of instances to verify LLM judge reliability
2. Test the benchmark's utility by having external researchers use it to develop hallucination mitigation techniques
3. Extend evaluation to non-English source languages to assess generalizability of observed patterns across translation directions