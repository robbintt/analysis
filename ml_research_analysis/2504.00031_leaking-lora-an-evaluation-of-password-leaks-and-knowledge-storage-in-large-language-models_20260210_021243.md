---
ver: rpa2
title: 'Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in Large
  Language Models'
arxiv_id: '2504.00031'
source_url: https://arxiv.org/abs/2504.00031
tags:
- passwords
- information
- password
- layer
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that fine-tuning large language models
  with LoRA on data containing passwords can result in successful password recovery
  through simple prompt-based retrieval. Using a dataset combining customer support
  conversations with RockYou passwords, the authors fine-tuned Facebook's OPT-1.3b
  model and successfully recovered 37 out of 200 injected passwords.
---

# Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in Large Language Models

## Quick Facts
- arXiv ID: 2504.00031
- Source URL: https://arxiv.org/abs/2504.00031
- Authors: Ryan Marinelli; Magnus Eckhoff
- Reference count: 21
- Passwords recovered from LoRA fine-tuned model: 37 out of 200

## Executive Summary
This study demonstrates that fine-tuning large language models with LoRA on data containing passwords can result in successful password recovery through simple prompt-based retrieval. Using a dataset combining customer support conversations with RockYou passwords, the authors fine-tuned Facebook's OPT-1.3b model and successfully recovered 37 out of 200 injected passwords. Causal tracing revealed that password information is primarily stored in the fully connected layer of the decoder (layer 160). The authors then applied Rank One Model Editing (ROME) to this layer, successfully removing all recoverable passwords while maintaining 32% accuracy on WikiText. This work highlights the security risks of fine-tuning LLMs on user data containing sensitive information and provides a mitigation technique through targeted model editing.

## Method Summary
The authors fine-tuned OPT-1.3b with LoRA using 200 epochs on a combined dataset of customer support data and RockYou passwords, injecting passwords in templated formats. They then attempted password recovery using prompt-based mining with templates like "my credential is". Causal tracing was applied to identify storage layers by comparing clean vs. corrupted password activations. ROME was then applied to the identified layer (decoder.layers.21.fc1) to remove password associations. The study measured password recovery rates and WikiText accuracy before and after ROME, with post-removal fine-tuning on clean data to partially restore capability.

## Key Results
- 37 out of 200 injected passwords were successfully recovered through prompt-based mining
- Password information was localized to layer 160 (decoder.layers.21.fc1) using causal tracing
- ROME completely removed all recoverable passwords (0/37 remaining)
- Post-ROME WikiText accuracy dropped from 40% to 10% with scaling=0.1, or to 32% with scaling=0.01

## Why This Works (Mechanism)

### Mechanism 1: LoRA Fine-Tuning Creates Memorization Pathways for Sensitive Data
- Claim: LoRA adaptation on datasets containing credentials can embed those credentials as retrievable facts when training conditions encourage overfitting.
- Mechanism: LoRA freezes the base model weights and learns two low-rank matrices (one randomly initialized, one zero-initialized). Their product forms an adaptation matrix that shifts the model's behavior. With sufficient epochs (200 used here) and scaling factor (α=64), passwords presented in structured formats ("my credential is {password}") become memorized as subject-relation-object facts.
- Core assumption: Passwords are processed and stored similarly to other factual associations in the model.
- Evidence anchors:
  - [abstract]: "Out of the first 200 passwords from the list, 37 were successfully recovered."
  - [section 4.1]: "200 epochs are used in this study. The intuition being an epoch per password of interest... Through encouraging over-fitting, the model will memorize the data more strongly."
  - [corpus]: Related password security literature exists but does not directly address LoRA-specific memorization risks.
- Break condition: If training epochs are reduced, scaling factor is lowered, or passwords are not presented in consistent templated formats.

### Mechanism 2: Causal Tracing Localizes Password Storage to Specific MLP Layers
- Claim: Password information concentrates in specific feed-forward layers, identifiable by comparing model behavior on clean vs. corrupted inputs.
- Mechanism: Three-run process—(1) clean run stores activations for factual prompts; (2) corrupted run obfuscates the password subject and records divergent activations; (3) restoration run reinstates individual clean activations to identify which restored states recover correct prediction. Layers with highest activation differences (measured via L2 norm) indicate storage locations.
- Core assumption: Factual information is stored in localized network regions that can be isolated through causal intervention.
- Evidence anchors:
  - [abstract]: "Causal tracing is used to identify that password information is largely located in a few layers."
  - [section 5.1]: "The layer that is the most active appears to be around layer 160... It references the 22nd layer in the decoder... part of the feed-forward network as denoted by the 'fc' and is part of the fully connected layer."
  - [corpus]: Causal tracing methodology originates from Meng et al. (referenced but not reproduced in corpus neighbors).
- Break condition: If password representations are distributed diffusely across layers or if corruption method fails to effectively mask password tokens.

### Mechanism 3: ROME Removes Targeted Knowledge via Rank-One Weight Updates
- Claim: Applying ROME to identified storage layers can eliminate retrievable password associations.
- Mechanism: ROME conceptualizes MLP layers as key-value stores. It computes a key (pre-activation input) and value (difference between original and corrupted activations), then applies a constrained least-squares update—a scaled outer product added to layer weights. This edits the association without full retraining.
- Core assumption: MLP layers implement interpretable key-value memory that supports surgical modification.
- Evidence anchors:
  - [abstract]: "Rank One Model Editing (ROME) was then applied to this layer, removing the password information and reducing the number of recoverable passwords from 37 to 0."
  - [section 5.1]: "By adding a scaled outer product of the value and key vectors to the layer weights, it adjusts the representation of passwords to purge the memorization... After ROME is applied, none of the passwords were recoverable."
  - [corpus]: MEMIT extends ROME for batch editing (referenced in paper); corpus lacks direct replication studies.
- Break condition: If scaling parameter is too aggressive (catastrophic degradation) or too conservative (incomplete removal).

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Explains how fine-tuning can embed sensitive data without modifying base weights—critical for understanding both the vulnerability and mitigation paths.
  - Quick check question: Why does LoRA use one randomly-initialized matrix and one zero-initialized matrix rather than two random matrices?

- Concept: **Causal Tracing (Clean/Corrupted/Restoration Runs)**
  - Why needed here: Required to identify which layers hold sensitive information before applying targeted edits.
  - Quick check question: In the corrupted run, if you obfuscate the subject token but the model still predicts correctly, what does that indicate about where the information is stored?

- Concept: **MLP as Key-Value Memory**
  - Why needed here: ROME's effectiveness depends on this conceptualization; understanding it clarifies why editing fc1 (input expansion) matters more than fc2 (output projection).
  - Quick check question: In the key-value store metaphor for an MLP layer, what corresponds to the "key" and what corresponds to the "value"?

## Architecture Onboarding

- Component map:
  - Input tokens -> embedding -> 24 decoder layers -> output
  - Each decoder layer: self-attention -> residual add -> LayerNorm -> FFN (fc1 -> ReLU -> fc2) -> residual add -> LayerNorm
  - Target layer: base_model.model.model.decoder.layers[21].fc1 (layer 160)

- Critical path:
  1. Input tokens → embedding → 24 decoder layers
  2. Within each decoder: self-attention → residual add → LayerNorm → FFN (fc1 → ReLU → fc2) → residual add → LayerNorm
  3. Password associations concentrated in layer 21's fc1 (expansion layer before activation)

- Design tradeoffs:
  - Scaling parameter 0.1: Complete password removal (0/37 recovered) but accuracy drops from 40% → 10%
  - Scaling parameter 0.01: Partial removal (5/37 recovered) with better accuracy (32%)
  - Post-ROME fine-tuning on clean data can partially recover capability (10% → 19%)

- Failure signatures:
  - Over-aggressive ROME: Model "lobotomized"—near-random predictions across all tasks
  - Under-aggressive ROME: Passwords still recoverable via mining prompts
  - Saturation during training: Signal strength drops after ~20 passwords injected, suggesting representation compression limits

- First 3 experiments:
  1. Baseline leak test: Fine-tune OPT-1.3B with LoRA on customer support data + 50 known passwords; test retrieval with "my credential is" prompts; measure recovery rate.
  2. Causal tracing validation: Run clean/corrupted/restoration protocol on recovered vs. unrecovered passwords; identify which layers show highest L2 norm differences; verify layer 21 fc1 localization.
  3. ROME scaling sweep: Apply ROME to identified layer with scaling values [0.001, 0.01, 0.05, 0.1]; for each, measure (a) remaining recoverable passwords, (b) WikiText accuracy; plot the security-utility frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an automated optimization algorithm, such as Expected Improvement, derive an optimal scaling parameter for Rank-One Model Editing (ROME) that maximizes password obfuscation while minimizing accuracy loss?
- Basis in paper: [explicit] Section 10 (Future Work) proposes empirically deriving optimal scaling values using Expected Improvement to balance accuracy and password recovery minimization.
- Why unresolved: The authors currently rely on manual tuning of the scaling parameter (testing 0.1 and 0.01), which results in a harsh trade-off where security requires a 75% drop in model utility.
- What evidence would resolve it: A study implementing a Bayesian optimization loop that identifies a scaling value achieving near-zero password recovery while retaining significantly higher than 19% accuracy on WikiText.

### Open Question 2
- Question: Does performing causal tracing and model editing at the individual neuron level, rather than the layer level, preserve model utility better than the current approach?
- Basis in paper: [explicit] Section 10 suggests that using more focused causal tracing on individual neurons might mitigate the severe performance losses noted in the study.
- Why unresolved: The study applied ROME to a specific layer ("fc1" in decoder layer 21), which successfully removed passwords but "lobotomized" the model, suggesting the layer-level edit was too broad.
- What evidence would resolve it: An experiment applying ROME or DEPN to specific neurons within the MLP, demonstrating a retention of the 40% baseline accuracy while maintaining zero password recoverability.

### Open Question 3
- Question: Do password leakage risks persist in models fine-tuned with standard epoch counts, given that this study intentionally overfitted the model to demonstrate the risk?
- Basis in paper: [inferred] Section 4.1 states that 200 epochs were used specifically to "encourage over-fitting" and facilitate password retrieval, leaving the behavior of standard, non-overfitted training regimes unclear.
- Why unresolved: It is undetermined if standard fine-tuning (e.g., 3-5 epochs) stores password information in the same distinct layers (e.g., layer 160) with sufficient strength to be retrieved by prompts.
- What evidence would resolve it: A comparative analysis of password recovery rates and causal tracing heatmaps for models trained with standard epoch counts versus the overfitted model.

## Limitations
- Limited generalizability: Results based on specific dataset combination (customer support + RockYou passwords) and single model (OPT-1.3b); may not apply to other architectures or training regimes.
- Causal tracing constraints: Only identifies storage locations for passwords that can be successfully recovered through prompt-based mining, leaving some sensitive information potentially unaddressable.
- ROME scaling tradeoff: Complete password removal requires aggressive scaling (0.1) that causes severe accuracy degradation (40%→10%), creating an inherent security-utility tradeoff.

## Confidence
- **High confidence**: LoRA fine-tuning on datasets containing passwords can result in successful password recovery through prompt-based retrieval. Directly demonstrated with 37/200 passwords recovered, and the mechanism (overfitting with structured templates) is well-established.
- **Medium confidence**: Password information is primarily stored in the fully connected layer of the decoder (layer 160). While causal tracing methodology is sound, the specific layer identification could vary with different corruption strategies or password injection patterns.
- **Medium confidence**: ROME can remove targeted password knowledge while maintaining some model capability. Complete removal with scaling=0.1 and partial retention with scaling=0.01 demonstrated, but severe accuracy drop indicates significant capability impact.

## Next Checks
1. **Dataset generalization test**: Reproduce the study with different datasets (e.g., customer support data from other sources, different password lists like HaveIBeenPwned) to verify that password leakage occurs consistently across data sources and that ROME remains effective with varying input distributions.
2. **Architecture scaling experiment**: Apply the same methodology to larger models (e.g., OPT-6.7b, LLaMA-7b) to determine if password storage localization and ROME effectiveness scale with model size, and whether the layer identification (160) remains consistent across architectures.
3. **Prompt template robustness evaluation**: Systematically test password recovery across different prompt templates and contexts (e.g., "password: ", "credential: ", natural conversation flows) to establish the boundaries of prompt-based mining effectiveness and identify whether ROME removal is template-independent.