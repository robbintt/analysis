---
ver: rpa2
title: Group-realizable multi-group learning by minimizing empirical risk
arxiv_id: '2601.16922'
source_url: https://arxiv.org/abs/2601.16922
tags:
- learning
- multi-group
- sample
- complexity
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies multi-group learning under the assumption of
  group-realizability, where for each subpopulation (group) there exists a benchmark
  classifier with zero error on that group. The paper introduces the class of group-realizable
  concepts (CG,H), consisting of functions consistent with the group-realizability
  assumption.
---

# Group-realizable multi-group learning by minimizing empirical risk

## Quick Facts
- arXiv ID: 2601.16922
- Source URL: https://arxiv.org/abs/2601.16922
- Reference count: 6
- Key outcome: ERM over group-realizable concepts achieves sample complexity O((d_G,H + d_G)log(1/γε) + log(1/δ))/(γε) even with infinite VC dimension, but finding consistent classifiers is NP-hard.

## Executive Summary
This paper studies multi-group learning under group-realizability, where for each subpopulation (group) there exists a benchmark classifier with zero error on that group. The authors introduce the class of group-realizable concepts (C_G,H) and show that empirical risk minimization (ERM) over this class achieves improved sample complexity compared to agnostic bounds. Remarkably, this holds even when C_G,H has infinite VC dimension, due to the structure of group-realizability. However, finding a consistent classifier in C_G,H is NP-hard, even when the group family and hypothesis class are efficiently optimizable. The paper suggests improper learning approaches to circumvent this computational barrier while maintaining statistical efficiency.

## Method Summary
The method involves empirical risk minimization over the class of group-realizable concepts C_G,H, which consists of functions consistent with the group-realizability assumption. For each group g ∈ G, there exists h ∈ H with zero error on that group. The approach requires defining the group family G and hypothesis class H, both with finite VC dimensions. When direct ERM over C_G,H is computationally intractable (which the paper shows is NP-hard), the authors suggest improper learning methods such as ensemble approaches that combine per-group hypotheses without requiring global consistency in C_G,H.

## Key Results
- ERM over C_G,H achieves sample complexity O((d_G,H + d_G)log(1/γε) + log(1/δ))/(γε), improving upon prior work with log|G| dependence
- This improvement holds even when C_G,H has infinite VC dimension due to the group-realizability structure
- Finding a consistent classifier in C_G,H is NP-hard, even when G has polynomial size and H is efficiently optimizable
- Improper learning approaches can circumvent the computational barrier while maintaining statistical efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ERM over C_G,H achieves optimal sample complexity even when C_G,H has infinite VC dimension.
- Mechanism: The "mistake behaviors" counted in generalization analysis are restricted to individual groups, where each group's behavior is determined by some h ∈ H. This bounds effective complexity by d_G,H × d_G rather than the full VC dimension of C_G,H.
- Core assumption: Group-realizability holds—for each g ∈ G, ∃ h ∈ H with zero error on g.
- Evidence anchors:
  - [abstract]: "improved sample complexity is obtained by empirical risk minimization over the class of group-realizable concepts, which itself could have infinite VC dimension"
  - [Section 3]: "the full 'richness' of C_G,H is never encountered because the 'mistake behaviors' that are counted are always restricted to individual groups"
  - [corpus]: Weak—related papers discuss sample complexity but not this specific shattering-based mechanism.
- Break condition: If evaluation requires simultaneous consideration of multiple overlapping groups in a way that cannot be decomposed per-group.

### Mechanism 2
- Claim: Standard uniform convergence arguments via shattering coefficients extend to the multi-group setting with product structure.
- Mechanism: The relevant function class F = {x ↦ g(x)(c△c*)(x) | g ∈ G, c ∈ C_G,H} has shattering coefficient bounded by S_X(G, 2n) · sup_{g∈G} S_g(H, 2n), yielding the d_G + d_G,H dependence.
- Core assumption: Both G and H have finite VC dimension.
- Evidence anchors:
  - [Section 3, proof]: "|F|_X| ≤ S_X(G,2n) · sup_{g∈G} S_g(H,2n) ≤ (2n≤d_G) · sup_{g∈G) (2n≤d_g,H)"
  - [Section 3]: "This is very simply captured using shattering coefficients"
  - [corpus]: Natarajan dimension paper shows analogous complexity-measure-based sample bounds for multiclass learning.
- Break condition: If the product bound fails to capture joint dependencies between group and hypothesis behaviors.

### Mechanism 3
- Claim: Computational intractability of finding c ∈ C_G,H can be circumvented via improper learning while maintaining statistical efficiency.
- Mechanism: Ensemble methods (e.g., Tosh and Hsu's algorithm) combine per-group hypotheses without requiring global consistency in C_G,H. Sample complexity becomes comparable when log(|G|) ≲ d_G log(1/ε).
- Core assumption: Per-group ERM over H is tractable even when global consistency is NP-hard.
- Evidence anchors:
  - [Section 4]: "finding a consistent classifier in C_G,H is NP-hard, even when G has polynomial size and H is efficiently optimizable"
  - [Section 5]: "the intractability can be subverted with improper learning"
  - [corpus]: Panprediction paper relates to multi-task/optimal prediction themes but doesn't directly address this computational-statistical gap.
- Break condition: If the ensemble approach fails to achieve the required 1/ε sample dependence.

## Foundational Learning

- Concept: **VC Dimension**
  - Why needed here: Determines sample complexity scaling; central to all bounds in the paper.
  - Quick check question: Why is a class with infinite VC dimension generally not PAC-learnable in the standard setting?

- Concept: **Multi-group Learning Objective**
  - Why needed here: Understanding the goal—uniform error bounds across potentially infinite overlapping subpopulations—is essential.
  - Quick check question: How does multi-group learning reduce to standard PAC learning when G = {X}?

- Concept: **Group-realizability vs. Global Realizability**
  - Why needed here: Key assumption enabling 1/ε vs 1/ε² improvement; distinguishes local per-group consistency from global consistency.
  - Quick check question: Why does realizability imply group-realizability, but not vice versa?

## Architecture Onboarding

- Component map:
  - Group family G -> Hypothesis class H -> Concept class C_G,H -> ERM or improper learning algorithm
  - (G, H) define the multi-group learning problem structure
  - C_G,H is the target class for learning under group-realizability
  - ERM/improper learning finds the final classifier

- Critical path:
  1. Verify group-realizability assumption on domain/data
  2. Estimate d_G, d_G,H, γ from domain knowledge
  3. Choose approach: (a) ERM over C_G,H if tractable, or (b) ensemble/improper method
  4. Collect n ≥ C·(d_G,H + d_G)log(1/γε) + log(1/δ))/(γε) samples
  5. Evaluate per-group error uniformly

- Design tradeoffs:
  - Finite G with enumeration: Sample complexity ∝ log|G|, but exponential compute
  - ERM over C_G,H: Optimal statistical efficiency, NP-hard to find consistent classifier
  - Ensemble improper learning: Polynomial time; comparable sample complexity when log|G| ≲ d_G log(1/ε)

- Failure signatures:
  - Sample complexity blow-up (1/ε²): Group-realizability violated; fall back to agnostic bounds
  - Computational timeout: Attempting proper ERM on large G; switch to ensemble
  - High variance on rare groups: γ too small; consider importance weighting or non-uniform bounds

- First 3 experiments:
  1. Validate group-realizability: Check if zero-error h ∈ H exists per group on held-out data; if not, bounds don't apply.
  2. Sample complexity calibration: Compare ERM (if tractable) vs. ensemble on synthetic data with known d_G, d_G,H; verify ~1/ε scaling.
  3. Rare-group stress test: Vary γ systematically; confirm bounds scale as predicted and identify failure threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a general oracle-efficient algorithm for multi-group learning that achieves the optimal sample complexity O((d_G,H + d_G)log(1/γε) + log(1/δ))/(γε) in the group-realizable setting?
- Basis in paper: [explicit] "A problem left open is to find a general oracle-efficient multi-group learning algorithm that achieves the sample complexity from (4) in the group-realizable setting."
- Why unresolved: ERM over CG,H is NP-hard, and the improper learning approach using Tosh and Hsu's algorithm has log(|G|) dependence, which is worse than d_G when G is infinite with finite VC dimension.
- What evidence would resolve it: An algorithm with access only to ERM oracles for H and G that achieves sample complexity matching (4).

### Open Question 2
- Question: Can the statistical efficiency of ERM over CG,H extend to the general agnostic multi-group learning setting with a suitable relaxation of the class CG,H?
- Basis in paper: [explicit] "The statistical efficiency of ERM over the rich class CG,H is a remarkable phenomenon, and it seems worthy of further investigation in other settings, including general (agnostic) multi-group learning with a suitable relaxation of CG,H."
- Why unresolved: The class CG,H is specifically defined for the group-realizable setting and is not appropriate for agnostic settings where group-realizability may fail.
- What evidence would resolve it: Definition of a relaxed concept class and sample complexity analysis showing improved dependence on ε compared to the 1/ε² agnostic rate.

### Open Question 3
- Question: Can convex relaxation or other improper learning techniques be applied to circumvent the NP-hardness of ERM over CG,H while maintaining statistical efficiency?
- Basis in paper: [explicit] "It would be interesting to understand if these other approaches are also applicable in our setting."
- Why unresolved: The paper shows one improper approach (ensemble method) works in specific cases but does not explore other computational speed-up techniques common in learning theory.
- What evidence would resolve it: A polynomial-time algorithm based on convex relaxation that achieves sample complexity comparable to ERM over CG,H.

## Limitations

- Computational intractability: Finding a consistent classifier in C_G,H is NP-hard, even for simple G and H with efficient optimization oracles.
- Strong assumption: Group-realizability is a strong condition that may not hold in real-world data, causing sample complexity to degrade to 1/ε² when violated.
- Rare group sensitivity: Bounds depend on γ (minimum group probability mass), which could be very small in practice, leading to large sample requirements.

## Confidence

- High confidence: The statistical sample complexity bounds (O((d_G,H + d_G)log(1/γε) + log(1/δ))/(γε)) and the shattering coefficient analysis that derives them.
- Medium confidence: The NP-hardness result for finding consistent classifiers in C_G,H.
- Medium confidence: The improper learning approach and its claimed sample complexity benefits.

## Next Checks

1. **Empirical NP-hardness validation**: Implement the reduction from Section 4 on benchmark NP-complete problems to empirically verify the computational barrier for finding consistent classifiers in C_G,H.

2. **Improper learning implementation**: Implement the ensemble method from Tosh and Hsu with the specified parameters (η=1/2, online-to-batch conversion) to verify the claimed sample complexity improvements in practice.

3. **Group-realizability testing framework**: Develop a systematic method to test the group-realizability assumption on real datasets, including quantitative metrics for how much the bounds degrade when the assumption is violated.