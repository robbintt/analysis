---
ver: rpa2
title: Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics
arxiv_id: '2505.13150'
source_url: https://arxiv.org/abs/2505.13150
tags:
- dynamics
- learning
- policy
- context
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of zero-shot adaptation in Behavioral
  Foundation Models (BFMs) when dynamics change at test time, such as new obstacles
  or latent factors like wind direction. The key problem is that standard Forward-Backward
  (FB) representations average successor measures across different dynamics, leading
  to policy interference and poor generalization.
---

# Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics

## Quick Facts
- arXiv ID: 2505.13150
- Source URL: https://arxiv.org/abs/2505.13150
- Reference count: 40
- Primary result: BFB and RFB achieve up to 2x higher zero-shot returns compared to baselines on unseen test dynamics

## Executive Summary
This paper addresses the challenge of zero-shot adaptation in Behavioral Foundation Models (BFMs) when dynamics change at test time, such as new obstacles or latent factors like wind direction. The key problem is that standard Forward-Backward (FB) representations average successor measures across different dynamics, leading to policy interference and poor generalization. The authors introduce two solutions: Belief-FB (BFB) and Rotation-FB (RFB). BFB uses a transformer encoder to infer a belief state (context embedding) from trajectories, conditioning the FB model on this belief to disentangle dynamics-specific policies. RFB enhances this by aligning task vectors with context embeddings using von Mises-Fisher sampling, partitioning the policy space into dynamics-specific clusters. The primary result shows that both BFB and RFB achieve up to 2x higher zero-shot returns compared to baselines (FB, HILP, LAP, and random) on both discrete (FourRooms, PointMass) and continuous (AntWind) tasks, including unseen test dynamics.

## Method Summary
The method operates in two stages: first, a transformer encoder is pre-trained to infer dynamics-specific context embeddings from reward-free trajectories using next-state prediction as a self-supervised objective. This encoder outputs a belief state h that captures the underlying dynamics. Second, the FB representation is trained with the forward network conditioned on the concatenated [h; zFB] where zFB is either uniformly sampled (BFB) or sampled from a von Mises-Fisher distribution centered on h (RFB). At test time, the agent encodes a short reward-free trajectory to obtain h, then conditions the policy on [h; ztest] to extract optimal behavior for any reward function without fine-tuning.

## Key Results
- BFB and RFB achieve up to 2x higher zero-shot returns compared to baselines on unseen test dynamics
- RFB shows better scalability as the number of environments increases, with error depending only on the largest cluster
- Performance improves with longer context lengths, plateauing around episode length (100 steps for discrete environments)
- Context embeddings successfully disentangle different dynamics, forming distinct clusters for different maze layouts and wind directions

## Why This Works (Mechanism)

### Mechanism 1: Context-Conditioned Successor Measures via Belief Estimation
Conditioning the Forward-Backward representation on a learned context embedding disentangles policies across different dynamics, preventing interference. A transformer encoder processes reward-free transition tuples and outputs a dynamics-specific embedding h. This embedding is concatenated with the task vector zFB and conditions the forward network, making the successor measure approximation dynamics-specific rather than averaged across all training environments. The core assumption is that transitions from the same underlying dynamics share structure that a permutation-invariant encoder can capture from trajectory data.

### Mechanism 2: Interference Mitigation via Latent Space Partitioning
Sampling task vectors from von Mises-Fisher distributions centered on context embeddings partitions the policy space into non-overlapping, dynamics-specific clusters. Standard FB samples task vectors uniformly, causing policies from different dynamics to interfere. RFB replaces this with vMF sampling, clustering task vectors around their context embedding direction. This creates disjoint cones in latent space, each dedicated to a specific dynamics configuration. The core assumption is that context embeddings h from different dynamics can be made nearly orthogonal during training, enabling non-overlapping cone partitioning.

### Mechanism 3: Self-Supervised Dynamics Inference via Next-State Prediction
A transformer trained to predict next states from context embeddings learns to encode dynamics-specific information without requiring environment labels. The encoder fdyn is trained with two self-supervised objectives: (1) the embedding h follows a Gaussian prior, and (2) a projection head combines h with (st,at) to predict st+1. Since rewards are absent and ordering is discarded, the encoder must focus on dynamics-relevant features to minimize prediction loss. The core assumption is that dynamics variations produce consistent, predictable patterns in transition tuples that can be distinguished from policy-specific patterns.

## Foundational Learning

- **Successor Measures**: The expected discounted future occupancy of state regions under a given policy: Mπ(s0,a0,X) = Σt≥0 γt P(st ∈ X | s0,a0,π). Needed because FB approximates successor measures via low-rank factorization, enabling zero-shot policy extraction for any reward function. Quick check: Can you explain why successor measures enable zero-shot generalization to arbitrary reward functions specified at test time?

- **Contextual MDPs (CMDPs)**: MDPs where transition and/or reward functions depend on a hidden context variable c ∈ C that remains fixed within an episode. Needed because the paper models environment variations as contexts, and the agent must infer c from trajectory data since it's not directly observable. Quick check: How does partial observability of context differ from standard POMDP partial state observability?

- **von Mises-Fisher Distribution**: A probability distribution on the (d-1)-sphere parameterized by mean direction μ ∈ S^(d-1) and concentration κ > 0. Needed because RFB uses vMF to sample task vectors concentrated around context directions, replacing uniform sampling to create dynamics-specific policy clusters. Quick check: What happens to the distribution as κ → ∞? As κ → 0?

## Architecture Onboarding

- **Component map:** Input trajectory → Transformer Encoder fdyn → Context Embedding h → Concatenation [h; zFB] → Forward Network F → Dot product with Backward Network B → Successor Measure → Policy Extraction

- **Critical path:** 1) Pre-train transformer encoder fdyn with next-state prediction loss - this stage learns to distinguish dynamics. 2) Train FB representations with conditioned forward network - keep fdyn frozen. 3) At test time: encode reward-free trajectory → obtain h → condition policy via [h;ztest].

- **Design tradeoffs:** Separate vs. joint training: paper explicitly found separating fdyn pre-training from FB training works better than end-to-end. Conditioning location: only F is conditioned on h; conditioning B degraded performance by producing "smoothed out Q function, ignoring environment structure." Context length vs. computation: longer contexts improve performance up to episode length, but transformer cost scales quadratically; 100-200 steps is the practical range.

- **Failure signatures:** Random-level performance on train dynamics: indicates context encoder failed to distinguish dynamics; FB baseline shows this in PointMass/FourRooms. Q-function ignores obstacles/walls: visualizes as policy moving through walls; indicates over-averaging across dynamics. High variance across seeds: suggests κ tuning instability or context encoder sensitivity to initialization.

- **First 3 experiments:** 1) Reproduce interference phenomenon: train FB on single layout (verify optimal policy), then mix 2 layouts (confirm policy becomes ambiguous). Visualize latent directions colored by optimal action. 2) Ablate context length: vary fdyn input length (50, 100, 150, 200 steps) on FourRooms. Expect performance plateau near episode length (100 steps). 3) Visualize embedding clusters: apply PCA/t-SNE to h embeddings from test trajectories, color by ground-truth context. Should recover disentangled clusters or smooth interpolation.

## Open Questions the Paper Calls Out
- Do zero-shot RL methods that do not rely on successor measure estimation suffer from the same policy interference issues when trained on datasets with varying dynamics?
- Can the proposed Belief-FB and Rotation-FB methods scale effectively to highly complex, high-dimensional benchmarks like XLand-MiniGrid or Kinetix?
- How can the computational cost of the transformer encoder be mitigated as the context length or observation history grows significantly?

## Limitations
- The method assumes transitions from the same dynamics share structure that a permutation-invariant transformer can capture from trajectory data, but does not prove when this assumption fails.
- The analysis does not validate whether context embeddings extrapolate smoothly to truly unseen dynamics beyond interpolation observed in AntWind wind directions.
- The scalability claim that RFB's error depends only on the largest cluster needs more rigorous validation beyond asymptotic analysis.

## Confidence
- **High confidence**: The interference phenomenon and its mitigation via context conditioning is well-supported by both theory and empirical results.
- **Medium confidence**: The self-supervised dynamics inference mechanism works as claimed, though exact architecture and training details remain unspecified.
- **Low confidence**: The scalability claim that RFB's error depends only on the largest cluster needs more rigorous validation.

## Next Checks
1. Test BFB/RFB performance with reduced transition coverage (e.g., 10% of training data) to validate the coverage assumption.
2. Evaluate on test dynamics that are qualitatively different from training (e.g., FourRooms layouts with 3x more obstacles) to check generalization beyond interpolation.
3. Remove context conditioning from F and condition B instead to confirm the claimed architectural sensitivity.