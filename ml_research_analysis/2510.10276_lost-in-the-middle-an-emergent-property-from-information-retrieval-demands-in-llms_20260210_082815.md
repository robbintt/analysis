---
ver: rpa2
title: 'Lost in the Middle: An Emergent Property from Information Retrieval Demands
  in LLMs'
arxiv_id: '2510.10276'
source_url: https://arxiv.org/abs/2510.10276
tags:
- recall
- attention
- task
- memory
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes that the "lost-in-the-middle" phenomenon in
  LLMs is not a flaw but an emergent property arising from different information retrieval
  demands during pre-training. The authors hypothesize that long-term memory demands
  (requiring uniform recall across the entire input) and short-term memory demands
  (prioritizing recent information) shape positional bias in LLMs.
---

# Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs
## Quick Facts
- arXiv ID: 2510.10276
- Source URL: https://arxiv.org/abs/2510.10276
- Reference count: 11
- Primary result: Positional bias in LLMs emerges as an optimal adaptation to long-term and short-term memory demands during pre-training

## Executive Summary
This paper proposes that the "lost-in-the-middle" phenomenon is not a flaw but an emergent property arising from different information retrieval demands during pre-training. The authors demonstrate that long-term memory demands (requiring uniform recall) and short-term memory demands (prioritizing recent information) shape positional bias in LLMs. Through controlled experiments with synthetic tasks, they show that free recall induces primacy effects, running span induces recency effects, and combined training produces the characteristic U-shaped curve. The findings suggest positional bias represents an optimal adaptation to task demands under architectural constraints rather than a simple information loss artifact.

## Method Summary
The authors train GPT-2 and Llama models from scratch on synthetic tasks designed to simulate different memory demands. They use free recall tasks to model long-term memory (requiring uniform recall across the entire input), running span tasks to model short-term memory (prioritizing recent information), and combined training to test the emergence of the U-shaped positional bias curve. The experiments include ablation studies on attention sinks and comparisons with T5 models to investigate architectural dependencies. Positional bias is measured through retrieval accuracy across different token positions, and the effects of autoregressive architecture and attention mechanisms are systematically examined.

## Key Results
- Free recall training induces primacy bias (better recall of early tokens)
- Running span training induces recency bias (better recall of recent tokens)
- Combined training produces the characteristic U-shaped positional bias curve observed in LLMs

## Why This Works (Mechanism)
The positional bias emerges as an optimal adaptation to the dual demands of long-term and short-term memory during pre-training. The autoregressive architecture, combined with attention sinks, creates conditions where the model naturally develops primacy effects to support long-term retrieval while maintaining recency for short-term demands. This emergent property is shaped by the specific training objectives and architectural constraints rather than being a design flaw.

## Foundational Learning
- Autoregressive Language Modeling: Why needed - fundamental to how LLMs generate text sequentially; Quick check - model predicts next token given previous context
- Positional Encoding: Why needed - provides sequence order information to transformer models; Quick check - sine/cosine functions or learned embeddings for position
- Attention Mechanisms: Why needed - allows models to weigh importance of different input positions; Quick check - query-key-value operations with softmax weighting
- Memory Retrieval Paradigms: Why needed - distinguish between short-term (recent) and long-term (uniform) recall demands; Quick check - free recall vs running span task differences
- Architectural Constraints: Why needed - determine what bias patterns are possible to learn; Quick check - comparison between GPT-style and T5 architectures

## Architecture Onboarding
Component Map: Input Text -> Positional Encoding -> Transformer Layers -> Attention Mechanism -> Output Distribution
Critical Path: Input sequence → positional embeddings → self-attention layers → output logits
Design Tradeoffs: Autoregressive structure enables natural text generation but creates primacy bias; bidirectional models avoid this but lose generation capability
Failure Signatures: Loss of long-term context, inability to uniformly recall information, excessive recency bias
First Experiments:
1. Train GPT-2 on pure free recall task to verify primacy emergence
2. Train T5 on same task to test architectural dependency claims
3. Ablate attention sinks in GPT-2 to quantify their contribution to primacy effects

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic tasks may not fully capture real-world pre-training complexity
- Small sample sizes (n=3) limit architectural generalization claims
- "Optimal adaptation" claim lacks rigorous proof of optimality
- Limited exploration of alternative training regimes or architectures

## Confidence
- Medium: Task-dependent positional bias emergence findings
- Medium: Architectural dependence on autoregressive structure and attention sinks
- Low: Broader claims about pre-training dynamics and optimality

## Next Checks
1. Test synthetic task approach with larger sample sizes (n≥10) and additional model architectures
2. Conduct ablation studies on attention sink mechanisms across multiple model sizes and training durations
3. Evaluate models trained on combined synthetic tasks against real-world long-context benchmarks