---
ver: rpa2
title: 'R-HTN: Rebellious Online HTN Planning for Safety and Game AI'
arxiv_id: '2602.00951'
source_url: https://arxiv.org/abs/2602.00951
tags:
- agent
- agents
- planning
- task
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces rebellious online Hierarchical Task Network
  (HTN) planning agents that operate under directives and can disobey user commands
  when necessary. The R-HTN algorithm enables two types of agents: Nonadaptive agents
  stop when directives are violated, while Adaptive agents replan to avoid violations
  while still attempting to achieve user goals.'
---

# R-HTN: Rebellious Online HTN Planning for Safety and Game AI

## Quick Facts
- arXiv ID: 2602.00951
- Source URL: https://arxiv.org/abs/2602.00951
- Authors: Hector Munoz-Avila; David W. Aha; Paola Rizzo
- Reference count: 6
- Primary result: Adaptive R-HTN agents achieve 4-4.5 goals out of 5 in O-RESCHU while incurring no directive violations, compared to 2-3 goals with 4-6 violations for Compliant agents

## Executive Summary
This paper introduces R-HTN (Rebellious Hierarchical Task Network planning), an online HTN planning approach where agents can disobey user commands when necessary to obey built-in safety directives. The system enables two types of agents: Nonadaptive agents stop when directives are violated, while Adaptive agents replan to avoid violations while still attempting to achieve user goals. The approach was evaluated in two domains—O-RESCHU (safety-critical navigation) and MONSTER (personality-driven behavior)—demonstrating that Adaptive agents achieve more goals while respecting constraints.

## Method Summary
R-HTN extends standard SHOP-style HTN planning with directive verification and task repair mechanisms. The core algorithm interleaves planning with execution, checking both current state and projected next state for directive violations before each action. When violations are detected, Adaptive agents repair their task lists by selecting alternative actions that avoid violations while minimizing distance to goal. The system was tested with three agent types: Compliant (ignores directives), Nonadaptive (stops on violations), and Adaptive (replans to avoid violations).

## Key Results
- Adaptive agents achieved 4-4.5 goals out of 5 in O-RESCHU while incurring no directive violations
- Compliant agents achieved 2-3 goals but incurred 4-6 violations per episode at 25% spawn rate
- Adaptive agents incurred approximately half the penalty points of Compliant agents
- In MONSTER, Adaptive agents collected the most gold without dying, while Compliant agents died in over 50% of episodes

## Why This Works (Mechanism)

### Mechanism 1: Dual-State Directive Verification
R-HTN prevents directive violations by checking both current and projected states before action execution. The `RepairTasksIfNeeded` procedure first evaluates δ(s) for current state violations, then evaluates δ(a₀(s)) for projected violations after applying the next action. If either triggers, task repair is invoked. This works because directives can be expressed as Boolean functions over states, and action effects can be deterministically predicted.

### Mechanism 2: Task List Repair via Alternative Action Selection
Adaptive agents achieve more goals than Nonadaptive agents by replacing violating actions with feasible alternatives rather than abandoning tasks. When a projected D-discrepancy is detected, `repairTaskListEffect` computes A(s)—the set of applicable actions that avoid violations—and selects the action minimizing distance to goal: a = min_{a∈A(s)} dist(a(s), g). This works when alternative actions exist that both avoid violations and make progress toward goals.

### Mechanism 3: Online Interleaved Planning-Execution Loop
Interleaving planning with execution enables agents to respond to dynamically changing environments (e.g., red zones appearing/disappearing). The agent executes action a₀ in the environment and observes the resulting state s′, rather than computing it symbolically. This grounds replanning in actual world state, capturing exogenous changes. This works when the environment changes at a timescale compatible with HTN decomposition speed.

## Foundational Learning

- **Hierarchical Task Network (HTN) Decomposition**: Understanding methods (m: S×T → T̃) and primitive vs. compound task distinction is prerequisite to following Algorithm 1. Quick check: Given a method `navigate-distant` that decomposes `reach(agent, location)` into `(up(agent), reach(agent, location))`, what happens when the agent is adjacent to the goal?
- **State-Predicate Representation**: Directives δ are functions over states S, and states are represented as sets of grounded predicates (e.g., `{at(5,(10,11)), red(7,(10,11),2)}`). Understanding this representation is necessary to define custom directives. Quick check: If state s = `{at(a1, (5,5)), monster(m1, (5,6))}`, what predicate would δ_monster check for?
- **Planning-Execution Interleaving**: Unlike offline planning where complete plans are generated before execution, online planning alternates between planning steps and action execution. This is fundamental to why R-HTN can respond to dynamic constraint changes. Quick check: In offline HTN planning, what would happen if a red zone appeared on the agent's planned path after the plan was generated but before execution completed?

## Architecture Onboarding

- **Component map**: HTN Planner Core -> Directive Checker -> Task Repair Module -> Execution Interface
- **Critical path**: Action selection → Directive check (current state) → Directive check (projected state) → Task repair (if needed) → Execute action → Observe new state → Recurse
- **Design tradeoffs**: Adaptive vs. Nonadaptive (Adaptive achieves more goals but requires repair procedures); Lookahead depth n=1 (checks only one action ahead); Directive conflict resolution (currently arbitrary)
- **Failure signatures**: Agent stops moving unexpectedly (likely A(s) = ∅ or point budget exhausted); Agent enters red zone despite directives (check if δ correctly evaluates on projected state); Agent loops without progress (repair may select actions that don't reduce dist(a(s), g))
- **First 3 experiments**: 1) Replicate O-RESCHU baseline with Compliant agent to verify ~2-3 goals achieved with 4-6 violations; 2) Test directive isolation with single directive to verify repair logic; 3) Stress test A(s) emptiness with constraint-dense map to verify graceful degradation

## Open Questions the Paper Calls Out

### Open Question 1
How can R-HTN agents be extended to perform multi-step lookahead (n≥2) before execution in domains where backtracking is impossible? The current algorithm checks only immediate and single-step projected discrepancies; extending to n-step lookahead requires generating action sequences without executing them.

### Open Question 2
How should R-HTN resolve conflicts between competing directives in a principled rather than arbitrary manner? Current implementation arbitrarily selects one violated directive without prioritization mechanisms.

### Open Question 3
Can R-HTN effectively handle nondeterministic action outcomes where single actions may produce multiple possible states? Current formulation assumes deterministic action transitions; probabilistic outcomes require reasoning about violation probabilities across possible future states.

### Open Question 4
What computational gains does online HTN planning provide compared to linear replanning approaches? The paper reports performance on goal achievement and violations but does not analyze computational efficiency or replanning overhead.

## Limitations
- **Unknown HTN domain specifications**: Complete method library and task decomposition rules for O-RESCHU and MONSTER domains are not provided, making exact reproduction difficult
- **Directive evaluation granularity**: Assumes binary directive checking but doesn't address scenarios requiring partial compliance or probabilistic outcomes
- **Computational overhead**: Planning time required for task repair and alternative action selection is not quantified, leaving scalability questions open

## Confidence

**High Confidence**: The core mechanism of dual-state directive verification is explicitly detailed in pseudocode and algorithm description, with clear evidence of implementation in both domains.

**Medium Confidence**: The claim that Adaptive agents achieve significantly more goals while avoiding violations is well-supported by experimental results, but exact implementation details of repair procedures remain somewhat abstracted.

**Medium Confidence**: The online interleaved planning-execution approach is theoretically sound, but the paper doesn't provide timing measurements or demonstrate responsiveness to constraint changes at different timescales.

## Next Checks

1. Implement a simplified version with single directive and verify that `RepairTasksIfNeeded` correctly detects violations in both current and projected states before executing task repair
2. Create a constraint-dense environment where all paths to goal violate directives to test the agent's behavior when A(s) = ∅ and verify graceful goal abandonment
3. Measure planning-execution cycle time under varying directive complexity to empirically validate that the approach remains "online" in practice