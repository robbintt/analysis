---
ver: rpa2
title: 'COMPKE: Complex Question Answering under Knowledge Editing'
arxiv_id: '2506.00829'
source_url: https://arxiv.org/abs/2506.00829
tags:
- knowledge
- uni00000013
- editing
- question
- complex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMPKE, a new benchmark for evaluating knowledge
  editing in large language models on complex question answering tasks. COMPKE advances
  beyond existing benchmarks by incorporating diverse question structures, one-to-many
  relations, and expanded edit types including additions, deletions, and substitutions.
---

# COMPKE: Complex Question Answering under Knowledge Editing

## Quick Facts
- arXiv ID: 2506.00829
- Source URL: https://arxiv.org/abs/2506.00829
- Reference count: 25
- Primary result: COMPKE is a new benchmark evaluating knowledge editing on complex multi-step reasoning questions with one-to-many relations, revealing that most KE methods achieve only modest performance on complex questions.

## Executive Summary
This paper introduces COMPKE, a new benchmark for evaluating knowledge editing in large language models on complex question answering tasks. COMPKE advances beyond existing benchmarks by incorporating diverse question structures, one-to-many relations, and expanded edit types including additions, deletions, and substitutions. The benchmark consists of 11,924 complex questions built from Wikidata, requiring multi-step reasoning with logical operations and conditional confirmation. Through extensive experiments on five LLMs spanning different model families, the authors evaluate four knowledge editing methods (ROME, MEMIT, MeLLo, PokeMQA) and find that most methods achieve only modest performance on complex questions. Memory-based methods tend to perform better on larger models with stronger reasoning abilities, while parameter-based methods show better results on smaller models but suffer from overfitting. The evaluation reveals significant performance drops as edit batch sizes increase, particularly for parameter-based approaches. COMPKE is demonstrated to be more challenging than previous datasets, making it a valuable tool for assessing and advancing knowledge editing techniques for complex reasoning scenarios.

## Method Summary
The COMPKE benchmark is constructed from Wikidata knowledge triples, focusing on one-to-one and one-to-many relations. Complex questions are generated by combining knowledge mapping with logical operations (intersection, union, conditional confirmation). Counterfactual edits are introduced as additions, deletions, or retentions to object sets. Four knowledge editing methods are evaluated: ROME and MEMIT (parameter-based) and MeLLo and PokeMQA (memory-based). Evaluation uses three metrics: Augment Accuracy (Aug) for newly added entities, Retention Accuracy (Ret) for preserved entities, and overall Accuracy (Acc) = (Aug + Ret)/2. The benchmark is tested across five LLMs (GPT-4O-MINI, LLAMA-3.1-8B, QWEN2.5-3B/7B) with edit batch sizes k ∈ {1, 100, 1000, 3000}.

## Key Results
- Most KE methods achieve only modest performance on complex questions, with memory-based methods outperforming parameter-based methods on larger models
- Parameter-based methods exhibit overfitting on smaller models, producing artificially high augmentation scores by indiscriminately outputting newly injected knowledge
- Performance drops significantly as edit batch sizes increase, particularly for parameter-based approaches
- COMPKE is demonstrated to be more challenging than previous datasets, making it a valuable tool for assessing and advancing knowledge editing techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory-based knowledge editing methods outperform parameter-based methods on larger models with stronger reasoning capabilities, but underperform significantly on smaller models.
- Mechanism: Memory-based methods (MeLLo, PokeMQA) store edits externally and retrieve them at inference time, requiring the model to decompose complex questions, integrate retrieved edits with internal knowledge, and execute multi-step reasoning—all capabilities that scale with model size and instruction-following ability.
- Core assumption: Larger models possess sufficient instruction-following and reasoning capacity to correctly interpret decomposition prompts, retrieve relevant edits, and synthesize answers across multiple reasoning steps.
- Evidence anchors:
  - [abstract] "MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B"
  - [Section 5.2] "memory-based methods underperform on smaller models (e.g., QWEN2.5-3B), likely due to their dependence on strong instruction-following and reasoning abilities"
  - [corpus] Related work "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition" corroborates that retrieval-augmented generation struggles with edit skipping in multi-hop scenarios
- Break condition: When model lacks sufficient instruction-following capacity (evidenced by PokeMQA's poor performance even on LLAMA-3.1-8B), or when prompt examples don't cover required reasoning patterns.

### Mechanism 2
- Claim: Parameter-based methods exhibit overfitting on smaller models, producing artificially high augmentation scores by indiscriminately outputting newly injected knowledge rather than correctly applying it through multi-step reasoning.
- Mechanism: After parameter updates encode new knowledge via rank-one modifications (ROME) or multi-layer mass-editing (MEMIT), smaller models lose discriminative capacity—outputting edited facts inappropriately rather than integrating them through required logical operations (intersection, union, condition confirmation).
- Core assumption: Overfitting manifests as high Aug scores without corresponding logical integration; the model memorizes edits but fails compositional reasoning.
- Evidence anchors:
  - [Section 5.2] "the high accuracy of parameter-based methods like MEMIT is largely due to overfitting... the model tends to overproduce the newly injected information, outputting it in response to any related question"
  - [Section 5.2] "MEMIT attains an accuracy of 22.43 [on Qwen2.5-3B], far surpassing MeLLo's 3.83"
  - [corpus] "Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing" documents stability issues with localized parameter updates
- Break condition: When batch edit size exceeds threshold (k ≥ 100), models lose coherence entirely, producing "gibberish" outputs (Figure 9 shows character corruption).

### Mechanism 3
- Claim: Decomposition-based memory methods fail when prompt examples don't include representative reasoning structures, causing critical logical operations (intersection, condition confirmation) to be omitted from generated plans.
- Mechanism: Methods like MeLLo use few-shot prompting to generate decomposition plans. If prompt examples lack coverage of specific operation types, the model generates incomplete plans missing essential steps like logical intersection.
- Core assumption: Generalization from few-shot examples requires structural coverage of target reasoning patterns.
- Evidence anchors:
  - [Section 5.2] "MeLLo's generated plans sometimes omit critical reasoning steps—most notably, the logical intersection step that is essential for correctly answering multi-hop questions"
  - [Section 5.2] "the original prompt examples used to guide MeLLo's decomposition do not include cases that require conditional confirmation operations"
  - [corpus] "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition" directly addresses this skip phenomenon
- Break condition: When complex questions require reasoning operations absent from few-shot demonstrations; Table 8 shows MeLLo producing final answer without executing intersection logic.

## Foundational Learning

- Concept: Knowledge Graph Triple Representation (s, r, o) → (s, r, O) for one-to-many
  - Why needed here: COMPKE generalizes from one-to-one to one-to-many relations (O = {o₁, o₂, ...}), requiring understanding of set-based knowledge representation and edit operations
  - Quick check question: Given edit (Microsoft, managers_are, {John, Smith} → {Smith, Eden}), identify: addition, deletion, retention sets

- Concept: Graph-Structured Reasoning with Logical Operations
  - Why needed here: Complex questions combine knowledge mapping with set operations (intersection ∩, union ∪) and condition confirmation; understanding these compositions is prerequisite to evaluating or building KE methods
  - Quick check question: For "Which educational institutions did both Ted Schroeder and Laurene Powell Jobs attend?", decompose into: knowledge links, logical operation type, intermediate entity sets

- Concept: Counterfactual Knowledge Editing Evaluation Paradigm
  - Why needed here: COMPKE uses counterfactual edits (random modifications) rather than real-world updates; understanding evaluation metrics (Aug, Ret, Acc) distinguishes rote memorization from genuine reasoning integration
  - Quick check question: If original answer {Los Angeles} becomes {San Francisco, Los Angeles} post-edit, and model outputs {San Francisco, Los Angeles, New York}, calculate: Aug, Ret, Acc scores

## Architecture Onboarding

- Component map:
  - Knowledge Base D: Wikidata-derived triples (37 relations, one-to-one + one-to-many)
  - Complex Question Q = (S, L): Entity sets Sᵢ + reasoning links Lⱼ (knowledge mapping, condition confirmation, logical operations)
  - Edit Engine E: Counterfactual modifications (addition/deletion/retention operations on object sets)
  - Evaluation Metrics: Aug (newly added entities), Ret (preserved entities), Acc (harmonic integration)

- Critical path:
  1. Sample facts → 2. Construct question structure (select template, instantiate with entities) → 3. Introduce counterfactual edit → 4. Filter conflicting edits → 5. Phrase in natural language → 6. Apply KE method → 7. Evaluate Aug/Ret/Acc

- Design tradeoffs:
  - Random counterfactuals: Enables controlled experiments but may not reflect real-world edit distributions
  - One-to-many focus: Realistic for domains like (movie, actors_are, {multiple people}) but excludes many-to-many relations
  - GPT-3.5 filtering: Ensures model recallability but introduces potential selection bias

- Failure signatures:
  - Overfitting: High Aug + low logical integration = model outputs edited content verbatim without reasoning
  - Omission: Decomposition plan missing intersection/union steps = prompt coverage gap
  - Collapse: Gibberish output at k ≥ 100 edits = parameter method scalability failure
  - Format mismatch: Smaller models fail instruction-following = invalid answer structures

- First 3 experiments:
  1. Reproduce overfitting detection: Apply MEMIT to Qwen2.5-3B, measure Aug vs. manual inspection of whether model executes intersection logic (replicate Figure 8 example)
  2. Test omission hypothesis: Extend MeLLo prompt with intersection/condition-confirmation examples, measure Acc delta on COMPKE subset requiring these operations
  3. Identify collapse threshold: For ROME/MEMIT on LLAMA-3.1-8B, binary search edit batch size k where coherent output probability drops below 50%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current knowledge editing methods perform on complex questions that involve many-to-many or many-to-one relationships, rather than just one-to-one and one-to-many relations?
- Basis in paper: [explicit] The "Limitations" section states that the fact triples in COMPKE are "restricted to one-to-one and one-to-many relations, excluding many-to-many and many-to-one relationships."
- Why unresolved: The paper's formalization of complex questions (Section 3) assumes knowledge instances of the form $(s, r, O)$, where a single subject maps to a set of objects. It does not define or evaluate reasoning chains where multiple subjects map to a single object or sets of subjects map to sets of objects.
- What evidence would resolve it: An extension of the COMPKE benchmark including many-to-many/many-to-one relation templates, followed by an evaluation of the four baseline methods (ROME, MEMIT, MeLLo, PokeMQA) on this new data.

### Open Question 2
- Question: To what extent do the performance results from counterfactual editing benchmarks generalize to actual real-world knowledge updates?
- Basis in paper: [explicit] The "Limitations" section notes that "edits are randomly introduced through counterfactual modifications, which may result in discrepancies from actual/real-world modifications."
- Why unresolved: The paper relies on counterfactuals (e.g., changing a filming location randomly) to simulate edits. It is unclear if the models handle these "fake" updates using the same mechanisms as they would valid, temporal updates (e.g., updating a CEO).
- What evidence would resolve it: A comparative study evaluating methods on COMPKE-style questions grounded in real-world factual changes (e.g., using temporal knowledge graphs) versus the counterfactual setup used in the current study.

### Open Question 3
- Question: Can parameter-based editing methods be regularized to prevent overfitting in smaller models, where they currently achieve high scores by indiscriminately repeating injected knowledge?
- Basis in paper: [inferred] Section 5.2 ("Overfitting in Parameter-Based Methods") details that methods like MEMIT achieve high augmentation scores on small models not by reasoning, but because "the model tends to overproduce the newly injected information... artificially inflates the augmentation metric."
- Why unresolved: The paper identifies this as a "key limitation" causing "artificially high augmentation scores," but does not propose or test a mechanism to ensure the model applies the edit only when contextually appropriate.
- What evidence would resolve it: A modified training/editing algorithm for parameter-based methods that includes a regularization penalty for generating edited entities in unrelated contexts, validated on smaller models like Qwen2.5-3B.

## Limitations

- The benchmark is limited to one-to-one and one-to-many relations, excluding many-to-many and many-to-one relationships
- Counterfactual edits may not reflect real-world knowledge update patterns, potentially limiting generalizability
- Memory-based methods require strong instruction-following and reasoning capabilities, performing poorly on smaller models

## Confidence

- **High Confidence**: The benchmark construction methodology (knowledge base selection, question generation templates, edit operations) and basic evaluation framework (Aug, Ret, Acc metrics) are well-specified and reproducible.
- **Medium Confidence**: The performance comparisons across different KE methods and model families are internally consistent within the paper, though some claims about overfitting mechanisms rely on qualitative analysis rather than quantitative thresholds.
- **Low Confidence**: The generalizability of findings to other knowledge domains beyond Wikidata and the precise impact of model architecture choices on KE effectiveness require further validation.

## Next Checks

1. **Overfitting Threshold Detection**: Conduct ablation studies on MEMIT with controlled edit injection rates to identify when high Aug scores become indicative of overfitting versus genuine knowledge integration, using human evaluation of reasoning chains.

2. **Instruction-Following Capacity Analysis**: Systematically test MeLLo's performance across a gradient of model sizes (1B to 70B) with standardized reasoning complexity to establish the minimum instruction-following capacity required for successful decomposition-based KE.

3. **Real-World Edit Validation**: Supplement counterfactual edits with a small set of real-world knowledge updates from Wikidata's change history to assess whether the observed performance patterns hold under realistic edit distributions and magnitudes.