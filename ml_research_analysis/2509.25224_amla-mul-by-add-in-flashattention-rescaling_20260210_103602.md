---
ver: rpa2
title: 'AMLA: MUL by ADD in FlashAttention Rescaling'
arxiv_id: '2509.25224'
source_url: https://arxiv.org/abs/2509.25224
tags:
- pipeline
- preload
- cube
- amla
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AMLA (Ascend MLA), a high-performance kernel
  optimized for Huawei's Ascend NPUs to address computational overhead in Multi-head
  Latent Attention (MLA) for Large Language Models. The key innovation is replacing
  floating-point multiplications with integer additions during output block rescaling
  by leveraging binary correspondence between FP32 and INT32 representations, enabling
  in-place updates directly in global memory.
---

# AMLA: MUL by ADD in FlashAttention Rescaling
## Quick Facts
- arXiv ID: 2509.25224
- Source URL: https://arxiv.org/abs/2509.25224
- Reference count: 40
- Achieves 614 TFLOPS (86.8% FLOPS utilization) on Ascend 910 NPUs with BF16 MLA kernel

## Executive Summary
AMLA introduces a novel kernel optimization for Huawei's Ascend NPUs that replaces floating-point multiplications with integer additions during output block rescaling in Multi-head Latent Attention (MLA). By leveraging binary correspondence between FP32 and INT32 representations, AMLA achieves up to 614 TFLOPS on Ascend 910 NPUs, reaching 86.8% of theoretical maximum FLOPS. The approach outperforms state-of-the-art FlashMLA implementations on NVIDIA hardware while preserving numerical precision through BF16 error compensation.

## Method Summary
AMLA implements a high-performance MLA kernel for Ascend NPUs by transforming floating-point multiplications into integer additions during output block rescaling. The core innovation uses FP32↔INT32 binary reinterpretation to perform power-of-2 scaling via integer addition, combined with in-place atomic updates in global memory. A Preload Pipeline strategy with hierarchical tiling (GM→L1→L0A/L0B) maximizes FLOPS utilization by overlapping data movement and computation. The kernel processes Q, K, V matrices in BF16 format with typical dimensions G=128, Dk=576, Dv=512, and batch size 96.

## Key Results
- Achieves 614 TFLOPS (86.8% FLOPS utilization) on Ascend 910 NPUs
- Outperforms FlashMLA's 66.7% FLOPS utilization on NVIDIA H800 SXM5
- Maintains numerical precision through BF16 error compensation
- Integrated into Huawei's CANN framework

## Why This Works (Mechanism)
AMLA exploits the binary representation similarity between FP32 and INT32 to transform multiplications into additions during power-of-2 scaling operations. The FP32↔INT32 reinterpretation allows adding scaled exponents directly in integer space, avoiding expensive floating-point multiplications. The Preload Pipeline strategy maximizes computational throughput by overlapping memory transfers with computation through hierarchical tiling and triple-buffering of key/value matrices.

## Foundational Learning
1. **BF16 Arithmetic Properties**: BF16 uses 8-bit exponent, enabling efficient scaling operations; needed for understanding numerical precision requirements.
2. **FP32↔INT32 Binary Reinterpretation**: Same bit patterns interpreted differently enable multiplication via addition; check by verifying FP32(1.5) = INT32(1,073,217,536).
3. **Hierarchical Memory Tiling**: GM→L1→L0A/L0B layout optimization; verify singleM=128 matches L0A core dimensions.
4. **Pipeline Overlap Strategy**: Preload count optimization for Cube/Vector core synchronization; check by ensuring no intra-cycle dependencies in steady loop.
5. **Atomic Operations in Global Memory**: In-place updates require atomic INT32 addition; verify AtomicAdd⟨INT32⟩ atomicity across multiple threads.
6. **Exponent Scaling Stability**: exp(mi) overflow at |mi| > 88; quick check by confirming ri = exp(-ni·ln2 - mi) stays within [1/√2, √2].

## Architecture Onboarding
**Component Map**: GM[K,V] → L1[K,V] → L0A/L0B[C1,C2] ← GM[Q] ← L1[Q] → Preload Pipeline → AtomicAdd in GM[O]
**Critical Path**: [C1]→[V1]→[C2] with Preload phase establishing [V1]→[C1] and [V1]→[C2] dependencies
**Design Tradeoffs**: Triple-buffering K/V maximizes bandwidth vs memory overhead; BF16 precision vs FP32 accuracy; in-place atomic updates vs output buffer allocation
**Failure Signatures**: 
- Numerical overflow if mi > 88 or underflow if ri < 1/√2
- Pipeline underutilization if count > 2 causing Vector-bound stalls
- Cache thrashing if L1/L0 tiling doesn't match hardware cache hierarchy
**First Experiments**:
1. Verify FP32(1.5) = INT32(1,073,217,536) to confirm binary reinterpretation correctness
2. Test AMLA rescaling with mi = ±88 to confirm no FP32 overflow and stable ri bounds
3. Profile L1/L0 tile sizes to verify singleM=128, singleK=288/256, singleN=256 match hardware cache

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to Ascend 910 NPUs due to proprietary CANN API dependencies
- No real-world model integration or heterogeneous deployment comparisons
- Single error-compensation scheme without ablation studies or edge-case testing

## Confidence
- FLOPS utilization (86.8% on Ascend 910): High – Directly measured on dedicated hardware with clear methodology
- Accuracy preservation (vs FP32): Medium – Theoretical derivation supported by single experiment; lacks sensitivity analysis
- Cross-platform generalization (vs H800): Low – Single data point without head-to-head hardware tuning parity

## Next Checks
1. Test AMLA's integer rescaling with mi near ±88 to confirm no FP32 overflow and stable ri ∈ [1/√2, √2]
2. Profile L1/L0 tile sizes on Ascend 910 to verify singleM=128, singleK=288/256, singleN=256 match hardware cache hierarchy
3. Compare FP32 reference accuracy across full range of context lengths (1K-16K) to validate BF16 compensation effectiveness