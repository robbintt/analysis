---
ver: rpa2
title: 'TeleEval-OS: Performance evaluations of large language models for operations
  scheduling'
arxiv_id: '2506.11017'
source_url: https://arxiv.org/abs/2506.11017
tags:
- llms
- report
- tasks
- ticket
- intelligent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TeleEval-OS, the first comprehensive benchmark
  for evaluating large language models (LLMs) in telecommunications operation scheduling.
  The benchmark includes 15 datasets across 13 subtasks, covering four operational
  stages: intelligent ticket creation, handling, closure, and evaluation.'
---

# TeleEval-OS: Performance evaluations of large language models for operations scheduling

## Quick Facts
- arXiv ID: 2506.11017
- Source URL: https://arxiv.org/abs/2506.11017
- Reference count: 40
- First comprehensive benchmark for evaluating LLMs in telecommunications operation scheduling across 13 subtasks and 15 datasets

## Executive Summary
TeleEval-OS introduces the first comprehensive benchmark for evaluating large language models in telecommunications operation scheduling. The benchmark encompasses 15 datasets across 13 subtasks covering intelligent ticket creation, handling, closure, and evaluation. Tasks are categorized into four capability levels: basic NLP, knowledge Q&A, report generation, and report analysis. The authors evaluate 10 open-source and 4 closed-source LLMs using zero-shot and few-shot methods, revealing that DeepSeek-V3, an open-source model, outperformed all closed-source models. The study highlights the potential of open-source LLMs in specialized domains and provides insights for practical deployment in telecommunications operations.

## Method Summary
The evaluation framework assesses LLMs across 13 subtasks using 15 datasets totaling 10.4k samples. Tasks are organized into four operational stages: intelligent ticket creation, handling, closure, and evaluation. Models are evaluated using zero-shot and few-shot methods, with 2 random samples for most tasks and 20 samples for intent recognition datasets. Metrics vary by task type: F1 score and accuracy for NLP tasks, Rouge-L for knowledge QA and report generation, and LLM-as-a-Judge (using ChatGPT-4o) for report analysis based on completeness, accuracy, and quality criteria. The evaluation pipeline requires implementing scripts for standard metrics and constructing a judge prompt for report analysis scoring.

## Key Results
- DeepSeek-V3, an open-source model, outperformed all closed-source models across evaluated tasks
- Few-shot learning significantly improved performance across tasks compared to zero-shot methods
- Report analysis tasks showed highest maturity (89.52 maximum score) while homogeneous ticket recommendation tasks showed lowest maturity (28.17 maximum score)

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of the telecommunications operation scheduling domain, spanning all major operational stages and capability levels. The use of both zero-shot and few-shot evaluation methods provides insights into model generalization versus adaptation capabilities. The incorporation of LLM-as-a-judge for report analysis leverages the very technology being evaluated to assess complex outputs, creating a practical evaluation paradigm for subjective tasks.

## Foundational Learning
- **Telecommunications Operations Scheduling**: Domain-specific workflows for managing network tickets and reports; needed for understanding task contexts and evaluating model performance in real-world scenarios; quick check: review operational definitions in Appendix A.
- **Few-shot Learning**: Method using 2-20 examples to guide model responses; needed to assess model adaptability to domain-specific tasks; quick check: verify few-shot sample sizes match task specifications.
- **LLM-as-a-Judge**: Using language models to evaluate outputs based on predefined criteria; needed for assessing subjective quality metrics in report analysis; quick check: ensure judge prompt includes completeness, accuracy, and quality criteria.

## Architecture Onboarding
- **Component Map**: Datasets -> Evaluation Pipeline (Metrics + Judge) -> Model Inference (Zero/Few-shot) -> Results Aggregation
- **Critical Path**: Model loading → Prompt template application → Inference execution → Metric calculation → Results aggregation
- **Design Tradeoffs**: Proprietary data ensures domain specificity but limits reproducibility; few-shot vs zero-shot balances adaptation with generalization; LLM-as-judge provides nuanced evaluation but introduces potential bias.
- **Failure Signatures**: Low F1 scores in homogeneous ticket recommendation indicate option redundancy challenges; performance degradation in smaller models with 20-shot examples suggests context window limitations; inconsistent judge scoring may reflect prompt incompleteness.
- **First Experiments**: 1) Validate metric implementations with synthetic data; 2) Test few-shot sample size sensitivity on a single task; 3) Reconstruct and test judge prompt on sample report analysis outputs.

## Open Questions the Paper Calls Out
- Can integration of professional statistical tools or function-calling mechanisms significantly improve LLM accuracy in report generation tasks where computational reasoning is currently a bottleneck?
- Does domain-specific fine-tuning effectively mitigate the performance degradation caused by high-option redundancy in homogeneous ticket recommendation tasks?
- To what extent does multimodal integration enhance the generalization capabilities of LLMs in telecommunications operational scheduling beyond text-based inputs?

## Limitations
- Proprietary dataset prevents independent verification of results
- Specific ChatGPT-4o judge prompt for report analysis is not provided
- Few-shot sample methodology lacks detailed explanation of selection criteria

## Confidence
- High Confidence: DeepSeek-V3 outperforming closed-source models across multiple task evaluations
- Medium Confidence: Task maturity assessment based on observed scores across 13 subtasks
- Medium Confidence: Few-shot learning benefits supported by reported improvements but optimal sample sizes vary by task

## Next Checks
1. Reconstruct and test the ChatGPT-4o judge prompt for report analysis tasks using stated criteria
2. Conduct sensitivity analysis by varying few-shot sample sizes on key tasks
3. Perform cross-dataset consistency checks using publicly available telecom operation datasets