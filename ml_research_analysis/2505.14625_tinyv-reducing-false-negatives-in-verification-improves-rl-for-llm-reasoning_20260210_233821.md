---
ver: rpa2
title: 'TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning'
arxiv_id: '2505.14625'
source_url: https://arxiv.org/abs/2505.14625
tags:
- answer
- ground
- truth
- 'false'
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates false negatives in verifier-based reinforcement
  learning for mathematical reasoning tasks. It finds that over 38% of correct model-generated
  responses are incorrectly rejected by rule-based verifiers, primarily due to natural
  language differences and formatting inconsistencies.
---

# TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning

## Quick Facts
- arXiv ID: 2505.14625
- Source URL: https://arxiv.org/abs/2505.14625
- Authors: Zhangchen Xu; Yuetai Li; Fengqing Jiang; Bhaskar Ramasubramanian; Luyao Niu; Bill Yuchen Lin; Radha Poovendaran
- Reference count: 40
- Primary result: Hierarchical verifier reduces false negatives in RL training, improving math reasoning accuracy by up to 10% and accelerating convergence.

## Executive Summary
This paper addresses a critical bottleneck in verifier-based reinforcement learning for mathematical reasoning: false negatives, where correct model outputs are incorrectly rejected by rule-based verifiers. The authors identify that over 38% of correct answers fail verification due to formatting differences and natural language variations rather than actual reasoning errors. They propose TinyV, a lightweight 1.5B LLM verifier that augments rule-based systems to detect and correct these false negatives. Theoretical analysis demonstrates that false negatives reduce gradient informativeness, while experimental results show TinyV improves pass rates by up to 10% on math benchmarks while maintaining low computational overhead.

## Method Summary
TinyV employs a hierarchical verification architecture where rule-based verifiers (Prime Verifier) evaluate all responses first, and only negative results are passed to the lightweight 1.5B TinyV model for re-evaluation. The system is trained on a balanced dataset of 159K instances combining real false negatives (re-annotated by dual LLM judges) and synthetic equivalents generated through perturbation. During RL training with GRPO, TinyV recovers correct answers that rule-based verifiers incorrectly reject, restoring informative gradient signals. The approach is evaluated on challenging math benchmarks using Qwen2.5-7B and Qwen2.5-Math-7B models, demonstrating improved accuracy and faster convergence compared to baseline verification methods.

## Key Results
- Reduces false negatives by correcting over 38% of incorrectly rejected correct answers
- Improves pass rates by up to 10% on MATH500, AMC, Olympiad Bench, and HardVerify-Math benchmarks
- Accelerates convergence compared to baseline verifiers while maintaining only 6% additional computational overhead
- Theoretical analysis shows false negatives reduce prompt efficiency by 15% and diminish step-wise learnability in RL training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Correcting false negatives restores informative gradient signals zeroed out by rule-based verifiers.
- **Mechanism:** In GRPO, advantages are normalized within groups of rollouts. When all responses for a prompt are marked 0 due to formatting mismatches, the advantage becomes zero or noisy. TinyV identifies correct answers in these "all-wrong" groups, changing reward from 0 to 1, which increases variance and generates distinct advantage signals that guide policy gradients.
- **Core assumption:** The base policy generates semantically correct answers that fail rule-based verification due to formatting/natural language mismatches.
- **Evidence anchors:** Theoretical analysis shows false negatives degrade training efficiency by reducing informative gradient signals; false negatives significantly reduce prompt efficiency by 15%.

### Mechanism 2
- **Claim:** Hierarchical verification minimizes computational overhead while maximizing false negative correction.
- **Mechanism:** Rule-based verifiers are computationally cheap but brittle, while LLM-based verifiers are robust but slow. TinyV uses a cascade: rule-based system evaluates all responses first, and only if it returns negative is the 1.5B TinyV model queried, avoiding unnecessary computations on easily verifiable correct answers.
- **Core assumption:** False negatives are frequent enough in the negative set to justify the inference cost of the second-stage verifier.
- **Evidence anchors:** TinyV is queried only when Prime Verifier returns a negative result, avoiding unnecessary computations; incurs only 6% additional computational cost.

### Mechanism 3
- **Claim:** Reducing false negatives increases step-wise learnability between policy updates.
- **Mechanism:** Theoretical analysis shows accurate rewards allow the policy to shift more significantly towards the optimal distribution at each step. False negatives dampen this shift by penalizing correct trajectories, minimizing divergence between consecutive policies. TinyV recovers correct rewards, allowing larger, more accurate policy shifts (higher learnability).
- **Core assumption:** Success probability increases with training steps and rewards do not grow exponentially in the RL setup.
- **Evidence anchors:** Theorem shows policies trained with ground truth rewards have greater step-wise learnability; accelerates convergence compared to baseline verifiers.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** TinyV targets GRPO advantage calculation. You must understand that GRPO computes advantages based on mean and variance of rewards within a group of outputs for the same prompt. If you don't grasp this, you won't understand why recovering a single positive reward in a batch of negatives is mathematically impactful.
  - **Quick check question:** If a prompt generates 8 responses and all are marked 0 by a verifier, what is the advantage $A_i$ for each response? (Answer: 0, or undefined due to zero variance).

- **Concept: Reverse KL Divergence**
  - **Why needed here:** The paper uses reverse KL divergence as a theoretical proxy for "learnability" or magnitude of policy improvement.
  - **Quick check question:** Does a higher reverse KL divergence between consecutive steps indicate faster or slower convergence towards an optimal policy? (Answer: Faster/Larger update steps, assuming direction is correct).

- **Concept: False Negative Taxonomy in Math Reasoning**
  - **Why needed here:** To design or debug TinyV, you need to know why rule-based verifiers fail (e.g., Latex vs. text, intervals vs. inequalities). The paper identifies 7 broad error classes.
  - **Quick check question:** Why might a rule-based verifier reject "k < -5" when the ground truth is "$(-\infty, -5)$"? (Answer: Notation/Format mismatch).

## Architecture Onboarding

- **Component map:** Input (prompt, response, ground truth) -> Stage 1 (Prime Verifier: rule-based check) -> Gate (IF Stage 1 == Negative, THEN Stage 2) -> Stage 2 (TinyV: 1.5B LLM classifier) -> Reward Aggregator (updates reward from 0 to 1 if TinyV returns True)
- **Critical path:** The data flow from "Rule-based Rejection" to "LLM Re-evaluation" is the critical path. If TinyV model latency is high, it bottlenecks the entire RL data generation pipeline.
- **Design tradeoffs:** TinyV (1.5B) vs. Large Verifier (72B): uses 1.5B model to keep overhead at ~6%, while larger model would likely have higher accuracy but make RL loop prohibitively slow. Training data: uses mix of real FNs (expensive) and synthetic FNs (generated by perturbing answers), as relying only on real FNs might result in insufficient training data.
- **Failure signatures:** Verification Hack: model learns to generate outputs satisfying TinyV's soft criteria but mathematically wrong (False Positives). Distribution Drift: if base LLM generates new answer format not seen during TinyV training, TinyV may fail to generalize.
- **First 3 experiments:** 1) Baseline Sanity Check: Run GRPO with standard Prime Verifier on 500 prompts, measure "all-wrong" ratio. 2) Static Evaluation: Take "all-wrong" data, run TinyV, manually check Precision/Recall vs. Human judgment. 3) End-to-End Ablation: Train two modelsâ€”one with Prime Verifier only, one with Prime + TinyV, compare convergence speed and final accuracy on HardVerify-Math Bench.

## Open Questions the Paper Calls Out

- **Question 1:** Does the impact of false negatives on RL training generalize to algorithms beyond GRPO, such as PPO, RLOO, DAPO, or offline methods like DPO and RAFT?
  - **Basis:** Authors state they have not empirically validated this hypothesis, though they "believe our findings can generalize" (Appendix B).
  - **Why unresolved:** Theoretical analysis and experiments focused solely on GRPO; other algorithms may handle reward noise differently.
  - **What evidence would resolve it:** Comparative experiments measuring convergence and performance when false negatives are reduced across multiple RL algorithms on the same benchmarks.

- **Question 2:** How prevalent are false negatives in non-mathematical RL domains such as theorem proving, medical applications, or software engineering?
  - **Basis:** Conclusion calls for exploring "false negatives in broader RL domains, such as theorem proving, medical applications, software engineering development, and robotics" (Section 6).
  - **Why unresolved:** Current study limited to mathematical reasoning; verification challenges likely differ across domains.
  - **What evidence would resolve it:** Application of FN detection methodology to datasets from other domains, followed by domain-specific analysis of FN causes and impacts.

- **Question 3:** Can extending verification to include the reasoning process, rather than only final answers, further improve RL training outcomes?
  - **Basis:** Limitations section notes that "TINYV currently relies on Prime Verifier's answer extraction mechanism, which focuses solely on the final answer rather than considering the entire output, such as the reasoning process" (Appendix B).
  - **Why unresolved:** Current design ignores potentially valuable intermediate reasoning steps that could provide richer reward signals.
  - **What evidence would resolve it:** Experiments comparing TinyV-style verification on full responses versus extracted final answers, measuring both training efficiency and final performance.

## Limitations

- **False positive risk:** Paper reports only false negative correction metrics without quantitative evidence on TinyV's false positive rate, raising concerns about potential reward noise introduction.
- **Annotation reliability:** Ground truth depends on LLM-based re-annotation without reported inter-annotator agreement or manual validation subset, leaving potential for systematic labeling errors.
- **Model specificity:** Results reported for Qwen2.5 models only, with generalization to other model families (e.g., Llama, Mistral) remaining untested.

## Confidence

- **High Confidence:** Theoretical analysis of how false negatives reduce gradient informativeness in GRPO is sound and well-supported by mathematical framework.
- **Medium Confidence:** Experimental results showing 10% pass rate improvements are convincing for tested model (Qwen2.5-7B), but require validation across different model architectures and problem distributions.
- **Low Confidence:** Claim of accelerated convergence lacks direct quantitative support (e.g., learning curves or wall-clock time comparisons) in main paper.

## Next Checks

1. **False Positive Audit:** Manually validate 100 samples where TinyV corrected a negative to positive, calculating precision to establish trade-off between FN correction and FP introduction.
2. **Generalization Test:** Apply TinyV to a different model family (e.g., Llama-3.1-8B) trained on same RL task, comparing pass rate improvements to those reported for Qwen2.5-7B.
3. **Convergence Benchmarking:** Plot training curves (pass rate vs. GRPO step) for both TinyV and baseline verifier across multiple runs, quantifying convergence acceleration in terms of steps to reach 60% accuracy and final accuracy at convergence.