---
ver: rpa2
title: 'BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal
  Large Language Models'
arxiv_id: '2508.06895'
source_url: https://arxiv.org/abs/2508.06895
tags:
- visual
- embeddings
- vision
- arxiv
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the limited visual alignment in Multimodal\
  \ Large Language Models (MLLMs), where visual embeddings are treated as contextual\
  \ cues without direct supervision. The authors propose BASIC, a method that leverages\
  \ refined visual embeddings from the LLM\u2019s shallow layers as supervision to\
  \ guide initial visual embeddings from the vision projector."
---

# BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models

## Quick Facts
- **arXiv ID:** 2508.06895
- **Source URL:** https://arxiv.org/abs/2508.06895
- **Authors:** Jianting Tang; Yubo Wang; Haoyu Cao; Linli Xu
- **Reference count:** 40
- **Primary result:** BASIC improves MLLM visual alignment using refined embeddings from shallow LLM layers as supervision, achieving consistent gains across 7B and 13B models on multiple benchmarks without additional supervision.

## Executive Summary
This paper addresses the limited visual alignment in Multimodal Large Language Models (MLLMs), where visual embeddings are treated as contextual cues without direct supervision. The authors propose BASIC, a method that leverages refined visual embeddings from the LLM's shallow layers as supervision to guide initial visual embeddings from the vision projector. This guidance is applied through directional alignment (minimizing angular distances) and semantic distribution matching (minimizing KL divergence). Without additional supervisory models or annotations, BASIC improves performance across multiple benchmarks: e.g., on VQAv2, BASIC increases accuracy from 78.5 to 79.2 (7B model) and from 80.0 to 80.6 (13B model); on MMBench-CN, from 58.3 to 62.1 (7B model) and from 63.6 to 64.9 (13B model). The method demonstrates improved semantic quality of initial visual embeddings and is broadly applicable across model configurations.

## Method Summary
BASIC introduces a two-stage training approach for MLLMs that directly supervises the vision projector using intrinsic signals from the LLM itself. In Stage 1, the vision encoder and LLM are frozen while the projector is trained to align initial visual embeddings with refined embeddings from shallow LLM layers (quadratic weighting favoring later layers within the range). The supervision signal is constructed by averaging refined embeddings from layers 1-16 (7B) or 1-20 (13B). Two alignment losses are applied: L_das (angular distance minimization on unit sphere) and L_sds (KL divergence on vocabulary logits). In Stage 2, the projector and LLM are jointly fine-tuned. The method uses no additional annotations or external supervision models, relying entirely on the LLM's internal refinement process.

## Key Results
- VQAv2 accuracy improves from 78.5→79.2 (7B) and 80.0→80.6 (13B)
- MMBench-CN accuracy improves from 58.3→62.1 (7B) and 63.6→64.9 (13B)
- Consistent gains across Gemma-2B, Phi3-3.8B, Mistral-7B architectures
- No additional annotations or supervision models required

## Why This Works (Mechanism)

### Mechanism 1: Shallow-Layer Embedding Refinement as Supervisory Signal
The LLM's shallow layers (1-16/20) progressively refine visual embeddings toward semantically meaningful token associations. BASIC uses a weighted average of these refined embeddings as intrinsic supervision for the projector. The core assumption is that shallow-layer refinement reflects useful semantic grounding, while deep-layer alignment toward `</s>` tokens is task-termination behavior, not semantic improvement.

### Mechanism 2: Directional Alignment via Angular Distance Minimization
Normalizing both initial and supervisory embeddings and minimizing ℓ₂ distance on the unit hypersphere forces initial embeddings to point toward semantically refined directions without constraining magnitude. This angular alignment is more reliable than magnitude-based distance for semantic correspondence.

### Mechanism 3: Semantic Distribution Matching via KL Divergence
Matching the full distribution of token associations (not just nearest token) transfers richer semantic structure from refined to initial embeddings. Projecting both embeddings against the full vocabulary and minimizing KL divergence captures semantic nuance beyond single-token matching.

## Foundational Learning

- **Vision Projector Role in MLLMs**: The projector is the only trainable component bridging frozen vision encoders to frozen LLMs. Understanding its role is critical because BASIC operates entirely on its output (initial visual embeddings). *Quick check:* Can you explain why the vision projector is the only trainable component in the pre-training stage, and what space its outputs must align to?

- **Self-Distillation and Intermediate Representations**: BASIC is conceptually a form of self-distillation—using the model's own intermediate states as supervision. This differs from knowledge distillation with a separate teacher model, and carries risks when the "teacher" signal is imperfect. *Quick check:* How does self-distillation differ from knowledge distillation with a separate teacher model, and what risks arise when the "teacher" signal is imperfect?

- **Logit Lens / Vocabulary Projection**: Both the analysis (Figure 2) and L_sds depend on projecting embeddings to vocabulary logits. This interpretability technique reveals semantic content in hidden states. *Quick check:* Given an embedding vector and a vocabulary matrix E, how would you compute the most associated token, and what does the full logit distribution tell you beyond the argmax?

## Architecture Onboarding

- **Component map**: CLIP-ViT-L/14-336px encoder -> 2-layer MLP GeLU projector -> Vicuna-v1.5-7B/13B LLM
- **Critical path**: 1) Forward pass through encoder → projector → LLM (storing refined embeddings at target layers) 2) Compute supervisory embedding via weighted sum 3) Compute attention-based importance weights 4) Calculate L_das and L_sds between initial and supervisory embeddings 5) Backpropagate to projector only (stage 1) or projector + LLM (stage 2)

- **Design tradeoffs**: 
  - Layer range (k): Using more layers (e.g., 1-32) degrades performance; shallow layers (1-16 for 7B) are optimal
  - Weighting scheme: Quadratic increase favors later layers within range; uniform or decreasing weights underperform
  - Loss coefficients: λ₁=1, λ₂=0.01 (L_sds scaled down due to KL divergence magnitude)
  - Patch importance weighting: Optional but improves results; adds compute for attention extraction

- **Failure signatures**: 
  - TextVQA degradation: L_sds may blur fine-grained text features in patches
  - Deep-layer supervision: Setting k too high introduces `</s>` bias and degrades performance
  - Small models/lower resolution: Gains persist but are smaller (Gemma-2B shows +0.6 vs +0.7 for Vicuna-7B)

- **First 3 experiments**:
  1. Reproduce layer-range ablation: Train with k ∈ {4, 8, 12, 16, 24, 32} on small subset and plot benchmark scores
  2. Isolate loss contributions: Train three variants—L_das only, L_sds only, both—to verify independent gains
  3. Visualize token alignment before/after: For held-out image set, compute ratio of "meaningful" token matches for baseline vs. BASIC

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the conflict between semantic alignment and fine-grained textual detail preservation, the optimality of quadratic weighting schemes for integrating refined embeddings, and the generalizability of the "degradation threshold" when including deeper layers across different model architectures. These questions center on understanding when and why BASIC might degrade performance on specific tasks like TextVQA, whether the fixed weighting approach is optimal, and how the method scales across different LLM sizes and architectures.

## Limitations

- Supervision signal quality depends on shallow-layer refinement being semantically meaningful across domains and architectures
- Architecture transferability unproven for encoder-decoder models and non-transformer LLMs
- Task-specific degradation observed on TextVQA due to potential blurring of fine-grained visual features

## Confidence

**High Confidence**: Claims about directional alignment (L_das) improving semantic correspondence are well-supported by controlled ablation showing consistent gains across benchmarks.

**Medium Confidence**: Claims about semantic distribution matching (L_sds) contributing independent gains are supported by ablation, but edge cases with poorly calibrated vocabulary distributions are not explored.

**Low Confidence**: Claims about generalizability across architectures and domains are weakly supported, with testing limited to four LLM variants and no exploration of encoder-decoder architectures.

## Next Checks

1. **Supervision Signal Robustness Test**: Create corrupted versions of the LLM (e.g., by adding noise to shallow layers, or using a different LLM architecture as the "teacher") and measure how BASIC performance degrades.

2. **Architecture Transfer Experiment**: Apply BASIC to a non-transformer LLM (e.g., Mamba, RWKV) or an encoder-decoder architecture (e.g., T5-based vision-language model) and measure gains on the same benchmark suite.

3. **Fine-Grained Feature Preservation Analysis**: For tasks that degrade (e.g., TextVQA), conduct a controlled experiment where L_sds is applied only to patches that do not contain text or fine-grained features.