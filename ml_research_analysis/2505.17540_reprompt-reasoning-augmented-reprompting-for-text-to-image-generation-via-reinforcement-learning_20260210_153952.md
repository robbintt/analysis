---
ver: rpa2
title: 'RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via
  Reinforcement Learning'
arxiv_id: '2505.17540'
source_url: https://arxiv.org/abs/2505.17540
tags:
- prompt
- arxiv
- reasoning
- reward
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RePrompt, a reasoning-augmented prompt refinement
  framework for text-to-image generation. The method introduces explicit chain-of-thought
  reasoning via reinforcement learning to generate structured, semantically grounded
  prompts, decoupling prompt generation from image synthesis.
---

# RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.17540
- Source URL: https://arxiv.org/abs/2505.17540
- Reference count: 40
- RePrompt achieves state-of-the-art performance on GenEval and T2I-Compbench benchmarks, with +77.1% improvement in spatial position accuracy.

## Executive Summary
RePrompt introduces a reasoning-augmented reprompting framework that generates structured, semantically grounded prompts for text-to-image generation via reinforcement learning. By incorporating explicit chain-of-thought reasoning traces, the method improves compositional alignment while maintaining lower latency than optimization-heavy baselines. The approach uses a tailored ensemble reward model to guide prompt optimization and demonstrates consistent improvements across diverse T2I backbones.

## Method Summary
RePrompt employs a two-stage training process on a Qwen2.5-3B backbone. First, supervised fine-tuning (SFT) teaches the model to output structured prompts containing reasoning traces followed by enhanced prompts. Second, Group Relative Policy Optimization (GRPO) with an ensemble reward (combining ImageReward, VLM-Reward, structural, and length constraints) optimizes prompt generation against frozen T2I models. The reasoning traces explicitly decompose spatial and attribute constraints before image generation.

## Key Results
- Achieves +77.1% improvement in spatial position accuracy on GenEval benchmark
- Demonstrates strong generalization across SD3, PixArt-Σ, and FLUX backbones
- Outperforms optimization-based methods while maintaining lower inference latency
- Ablation studies confirm ensemble reward and reasoning traces contribute significantly to performance

## Why This Works (Mechanism)

### Mechanism 1
Explicit chain-of-thought reasoning traces reduce variance in reward estimation and improve compositional grounding. The policy first generates a reasoning trace decomposing spatial and attribute constraints, then conditions the enhanced prompt on this trace. This structured decomposition reduces prompt-to-image uncertainty by explicitly resolving ambiguities before generation.

### Mechanism 2
GRPO-based reinforcement learning enables optimization of prompt generation against non-differentiable image synthesis pipelines. For each input prompt, sample candidate (reasoning, prompt) pairs, generate images via frozen T2I model, compute ensemble rewards, then update policy using normalized advantages with KL regularization to prevent divergence from reference policy.

### Mechanism 3
Ensemble reward combining preference, semantic, and structural signals stabilizes training and captures complementary quality dimensions. The total reward blends ImageReward (human preference) and VLM-Reward (semantic alignment), while structural and length rewards enforce output format and length constraints.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Core RL algorithm for training the prompting policy when image generation is non-differentiable. Why needed: Enables optimization against non-differentiable T2I pipelines. Quick check: Can you explain why GRPO uses group-wise advantage normalization instead of per-sample baselines?

- **Variance Reduction via Conditioning**: Theoretical justification for reasoning traces. Why needed: Understanding how conditioning on intermediate variables reduces estimator variance. Quick check: Given `Var[X] = E[Var[X|Y]] + Var[E[X|Y]]`, why does adding a conditioning variable never increase variance?

- **Reward Model Ensembling for RLHF**: RePrompt's reward design assumes multiple signals stabilize training. Why needed: Understanding tradeoffs is critical for debugging. Quick check: If two reward models have correlation 0.3, would you expect ensemble gains to be larger or smaller than if they had correlation 0.8?

## Architecture Onboarding

- **Component map**: Input prompt -> Prompting Policy (Qwen2.5-3B) -> T2I Synthesizer (FLUX.1-dev) -> Reward Model (ensemble) -> GRPO Optimizer
- **Critical path**: 1. Input prompt → Policy samples G reasoning-prompt pairs 2. Each prompt → FLUX generates image 3. Each image → Reward model computes R_total 4. Compute normalized advantages, update policy via GRPO 5. Repeat for 3 epochs
- **Design tradeoffs**: Smaller base LLM (3B) enables faster inference vs. larger models but may limit reasoning complexity; training with FLUX only but evaluating on SD3, PixArt-Σ trades specialization for generalization; prompt length constrained to [15, 77] tokens for T2I compatibility
- **Failure signatures**: Reward collapse (sharp reward curve fluctuations), format violations (missing tags), spatial relation failures (position scores plateau)
- **First 3 experiments**: 1. Sanity check: Run policy on 20 held-out prompts, manually verify reasoning traces contain spatial/attribute decomposition 2. Ablation replay: Reproduce Table 7 on 500-prompt subset 3. Cross-backbone transfer: Train with FLUX, evaluate on SD3 using GenEval position category

## Open Questions the Paper Calls Out

- **Numeracy improvement**: How can RePrompt's modest performance gains in numeracy and object counting be improved to match its strong spatial reasoning improvements? The authors note performance gains on numeracy tasks remain modest, suggesting potential for further enhancement in precise quantitative reasoning.

- **Reward model fidelity**: To what extent does the fidelity and calibration of the reward model bottleneck RePrompt's performance ceiling? The authors note effectiveness depends on reward model quality, while improvements in reward fidelity could further amplify performance.

- **Cross-model generalization**: Why does RePrompt trained exclusively on FLUX generalize effectively to architecturally diverse backbones like SD3 and Pixart-Σ? The mechanism enabling this cross-model transfer is not analyzed.

- **Reward ensemble configuration**: What is the optimal configuration strategy for the multi-component reward ensemble? The authors set ensemble weights empirically without exploring adaptive or task-aware weighting.

## Limitations

- The variance-reduction theorem assumes reasoning traces are conditionally independent of image synthesis; if traces depend on reward feedback, the variance bound may not hold in practice.
- Reward model quality is bottlenecked by GPT-4V API access and lack of public validation of implementations; performance could degrade if reward signal is noisy.
- Generalization claims rely on FLUX training but evaluation across multiple T2I backbones; unknown whether the policy overfits to FLUX's style or truly learns compositional reasoning.

## Confidence

- **High**: Experimental results on GenEval/T2I-Compbench, ablation studies, and training details
- **Medium**: Theoretical variance reduction claim and mechanism explaining why reasoning traces improve compositional grounding
- **Low**: Generalization across unseen T2I models beyond reported results; long-term robustness to prompt distribution shifts

## Next Checks

1. **Reasoning trace inspection**: Manually review 50 reasoning traces from the trained policy to verify they contain scene-specific decomposition vs. generic templates.
2. **Reward model ablation**: Reproduce Table 7 on a small prompt subset to confirm ensemble reward consistently outperforms single-reward variants before full training.
3. **Cross-backbone transfer**: Evaluate RePrompt-trained policy on SD3 using GenEval position accuracy; verify ≥50% improvement over baseline per Table 1 claims.