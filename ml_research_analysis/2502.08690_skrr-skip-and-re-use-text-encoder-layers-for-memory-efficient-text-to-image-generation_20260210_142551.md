---
ver: rpa2
title: 'Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image
  Generation'
arxiv_id: '2502.08690'
source_url: https://arxiv.org/abs/2502.08690
tags:
- text
- skrr
- re-use
- sparsity
- skip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Skrr is a blockwise pruning strategy for text encoders in text-to-image
  (T2I) diffusion models that selectively skips or reuses transformer layers to reduce
  memory consumption. The method identifies and prunes less important layers using
  a T2I-tailored discrepancy metric, then reuses adjacent layers to restore performance,
  supported by theoretical analysis.
---

# Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation

## Quick Facts
- arXiv ID: 2502.08690
- Source URL: https://arxiv.org/abs/2502.08690
- Reference count: 40
- Skrr achieves state-of-the-art memory efficiency in T2I models while preserving image quality, outperforming existing pruning methods at high sparsity (>40%).

## Executive Summary
Skrr is a blockwise pruning strategy for text encoders in text-to-image diffusion models that selectively skips or reuses transformer layers to reduce memory consumption. The method identifies and prunes less important layers using a T2I-tailored discrepancy metric, then reuses adjacent layers to restore performance, supported by theoretical analysis. Evaluated on PixArt-Σ, Skrr achieves state-of-the-art memory efficiency while preserving image quality, outperforming existing pruning methods at high sparsity (>40%). It maintains competitive FID, CLIP, DreamSim, and GenEval scores, demonstrating strong text-image alignment and fidelity even under aggressive compression.

## Method Summary
Skrr implements a two-phase pruning strategy: first identifying sub-blocks to skip via beam search using a T2I-tailored discrepancy metric that accounts for both conditional and null embeddings, then selectively reusing adjacent unpruned sub-blocks to restore capacity. The method operates at sub-block granularity (MHA vs FFN), enabling finer sparsity control than full-block pruning. It requires a calibration dataset of 1k prompts (150-250 tokens from CC12M) to compute discrepancies through the denoising module's projection layer. The approach achieves up to 52% parameter reduction while maintaining competitive image quality metrics.

## Key Results
- Achieves up to 52% parameter reduction while preserving competitive FID (21.35→20.54), CLIP (0.312→0.311), and DreamSim (0.746→0.741) scores
- Outperforms existing pruning methods (FinerCut, LLano) at high sparsity (>40%), with GenEval overall score of 0.834 at 42% sparsity
- Restores text-image alignment lost with skip-only pruning, recovering counting task performance from 0.372→0.410 through re-use

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Beam search-based pruning more effectively identifies removable sub-blocks than greedy sequential methods by accounting for inter-block interactions.
- **Mechanism:** Maintains k candidate pruning paths simultaneously, evaluating each using a T2I-tailored discrepancy metric, then selecting the lowest-discrepancy path at each sparsity level.
- **Core assumption:** Transformer sub-blocks exhibit functional redundancy, and greedy removal ignores compounding effects when multiple blocks are pruned.
- **Evidence anchors:**
  - [abstract] "beam search (Freitag & Al-Onaizan, 2017)-based approach that explores multiple pruning paths simultaneously"
  - [section 3.1] Fig. 4 shows pruning two blocks can cause severe degradation not predicted by single-block analysis
  - [corpus] Corpus contains limited direct evidence on beam search pruning; reliance primarily on paper's empirical validation.
- **Break condition:** Beam size k=4 shows performance decline vs. k=3 (Table 3), indicating over-exploration of paths can degrade results.

### Mechanism 2
- **Claim:** Re-using adjacent unpruned sub-blocks can partially restore capacity lost from pruning without adding memory overhead.
- **Mechanism:** After identifying skip indices, evaluates whether routing hidden states back through the previous or next unpruned sub-block yields lower discrepancy than skipping entirely.
- **Core assumption:** Adjacent transformer blocks perform sufficiently similar transformations that one can approximate another's function.
- **Evidence anchors:**
  - [section 3.2] Theorem 3.2 provides theoretical bound: if ||θ_i - θ*_i|| < ||θ_i||, then error bound tightens with re-use
  - [section 4.3] Fig. 6 shows re-use recovers text alignment lost with skip-only pruning
  - [corpus] No direct corpus evidence on layer re-use in T2I text encoders; mechanism is paper-specific.
- **Break condition:** At extreme sparsity (>50%), insufficient remaining blocks exist for effective re-use; performance degrades noticeably.

### Mechanism 3
- **Claim:** MSE-based discrepancy measured through the denoiser's projection module better predicts T2I quality than cosine similarity alone.
- **Mechanism:** Projects text embeddings through the same projection layer used for conditioning, computes MSE on both conditional (f_c) and null (f_∅) embeddings, summing discrepancies.
- **Core assumption:** Classifier-free guidance relies on null embeddings; perturbations to ||f_∅|| destabilize guidance even when conditional embeddings appear similar.
- **Evidence anchors:**
  - [section 3.1] Fig. 4 shows high cosine similarity (0.89) but large ||f_∅||=3.34 produces abnormal images; MSE=0.04 correctly flags this
  - [section 5] Perturbing f_∅ improves FID (22.89→20.65), confirming null embedding sensitivity
  - [corpus] LightFair and other corpus papers address text encoder effects but do not validate this specific metric design.
- **Break condition:** Without projection module, CLIP drops from 0.312→0.305, DreamSim from 0.746→0.708 (Table A13).

## Foundational Learning

- **Concept:** Classifier-Free Guidance (CFG) in diffusion models
  - **Why needed here:** Skrr's discrepancy metric explicitly accounts for null condition embeddings because CFG amplifies any perturbation to f_∅ during generation.
  - **Quick check question:** Can you explain why increasing ||f_∅|| magnitude would cause oversaturation or artifacts when CFG scale w > 1?

- **Concept:** Transformer block residual structure (FL + I) composition
  - **Why needed here:** The theoretical error bound (Lemma 3.1) derives from the residual skip connections; understanding this is essential for grasping why re-use tightens bounds.
  - **Quick check question:** If each block Fi is Li-Lipschitz in input, what does the product ∏(1 + Lk) represent in the error propagation?

- **Concept:** Sub-block granularity (MHA vs. FFN) in transformers
  - **Why needed here:** Skrr prunes at sub-block level, not full blocks; MHA and FFN can be independently skipped or re-used, enabling finer sparsity control.
  - **Quick check question:** Why would pruning only FFN sub-blocks potentially preserve more semantic information than pruning MHA sub-blocks?

## Architecture Onboarding

- **Component map:** Calibration dataset (1k prompts from CC12M) → Skip algorithm (beam search over sub-blocks) → Skip indices S* → Re-use algorithm (evaluate adjacent candidates) → Re-use dictionary R → Pruned text encoder → Denoiser with projection module

- **Critical path:**
  1. Construct T2I-appropriate calibration set (semantically rich, 150-250 tokens)
  2. Implement GetDiscrepancy() with projection module, computing MSE on both conditional and null embeddings
  3. Run Skip with beam size k=3 to generate ordered skip indices
  4. Apply Re-use to identify which skipped blocks benefit from adjacent layer recycling
  5. Validate on held-out prompts before deployment

- **Design tradeoffs:**
  - Beam size k: Larger k explores more paths but increases search cost; k=3 optimal in experiments
  - Sub-block vs. full-block: Sub-block (FinerCut/Skrr) achieves better performance at matched sparsity but complicates implementation
  - Projection module inclusion: Essential for accurate discrepancy but requires access to denoiser internals

- **Failure signatures:**
  - High ||f_∅|| after pruning (>1.0) → expect CFG artifacts, oversaturation
  - Cosine similarity metric used alone → may miss magnitude-based failures
  - Re-use applied without checking discrepancy improvement → can increase error vs. skip-only

- **First 3 experiments:**
  1. Reproduce Fig. 4: prune 2 blocks with high cosine similarity but divergent ||f_∅||, verify MSE predicts image quality better than cosine
  2. Ablate beam size k∈{1,2,3,4} at 40% sparsity, measure GenEval overall score; confirm k=3 peak
  3. Compare skip-only vs. skip+re-use at 42% sparsity on counting task (GenEval "Counting"); expect 0.372→0.410 recovery

## Open Questions the Paper Calls Out
None

## Limitations
- The layer re-use mechanism, while theoretically justified through error bounds, lacks comprehensive empirical validation across different sparsity levels and model architectures
- The discrepancy metric's reliance on the denoising module's projection layer creates a dependency that may not generalize to all T2I architectures
- At extreme sparsity levels (>50%), the paper acknowledges performance degradation but doesn't provide clear guidance on when re-use becomes ineffective

## Confidence
- **High Confidence:** The fundamental observation that transformer layers in text encoders exhibit functional redundancy is well-supported by empirical results across multiple T2I models
- **Medium Confidence:** The beam search methodology shows clear advantages over greedy approaches in the paper's experiments, but the optimal beam size appears to be model-specific
- **Low Confidence:** The theoretical error bounds assume Lipschitz continuity properties that, while standard in optimization theory, aren't empirically verified for the specific transformer implementations used

## Next Checks
1. **Cross-architecture generalization test:** Apply Skrr's pruning strategy to T5-small and T5-base text encoders in T2I models, measuring whether the discrepancy metric and beam search approach maintain effectiveness as model capacity changes
2. **Null condition sensitivity analysis:** Systematically vary the CFG scale parameter (w) across pruned models to quantify how ||f_∅|| perturbations affect image quality degradation
3. **Alternative search strategy comparison:** Implement and evaluate a random sampling pruning approach against beam search at matched computational budgets, measuring whether the beam search's exploration advantage justifies its increased complexity in practice