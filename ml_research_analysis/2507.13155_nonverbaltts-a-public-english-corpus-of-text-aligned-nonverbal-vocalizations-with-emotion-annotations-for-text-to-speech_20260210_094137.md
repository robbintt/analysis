---
ver: rpa2
title: 'NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal Vocalizations
  with Emotion Annotations for Text-to-Speech'
arxiv_id: '2507.13155'
source_url: https://arxiv.org/abs/2507.13155
tags:
- speech
- emotion
- dataset
- nvtts
- laughter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scarcity of open-source datasets for expressive
  text-to-speech synthesis by introducing NonverbalTTS, a 17-hour English corpus containing
  10 types of nonverbal vocalizations and 8 emotion categories. The dataset was constructed
  by annotating existing speech corpora (VoxCeleb and Expresso) using an automated
  detection pipeline followed by human validation.
---

# NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal Vocalizations with Emotion Annotations for Text-to-Speech

## Quick Facts
- **arXiv ID:** 2507.13155
- **Source URL:** https://arxiv.org/abs/2507.13155
- **Reference count:** 0
- **Primary result:** Introduces a 17-hour English corpus with 10 NV types + 8 emotions, fine-tunes CosyVoice to match CosyVoice2 performance

## Executive Summary
This paper addresses the scarcity of open-source datasets for expressive text-to-speech synthesis by introducing NonverbalTTS, a 17-hour English corpus containing 10 types of nonverbal vocalizations and 8 emotion categories. The dataset was constructed by annotating existing speech corpora (VoxCeleb and Expresso) using an automated detection pipeline followed by human validation. The authors fine-tuned open-source TTS models on this dataset and achieved performance comparable to state-of-the-art closed-source systems (CosyVoice2) on metrics including speaker similarity, emotion similarity, and nonverbal vocalization fidelity. The work provides both the dataset and annotation methodology to support future research in expressive speech synthesis.

## Method Summary
The authors constructed the NVTTS dataset by first automatically detecting nonverbal vocalizations in VoxCeleb and Expresso audio using the BEATs model, then aligning these detections to text transcripts via Montreal Forced Aligner. They applied emotion2vec+ for emotion classification and had three annotators validate all annotations on the Argilla platform, resolving conflicts through majority voting. For model training, they fine-tuned the CosyVoice-300M model by training only its language model component while freezing the text encoder and speech tokenizer, using Adam optimizer with learning rate 1e-5 for 25 epochs.

## Key Results
- Achieved speaker similarity scores comparable to CosyVoice2 (wavlm-base-plus-sv metric)
- Successfully synthesized nonverbal vocalizations with Jaccard distances of Jbreath=0.31, Jlaugh=0.28, Jcough=0.35
- Ablation study showed removing emotion annotations marginally improved NV generation quality

## Why This Works (Mechanism)
The method works by providing high-quality, text-aligned data for nonverbal vocalizations that previous datasets lacked. By fine-tuning only the language model component of CosyVoice, the approach preserves the pre-trained speaker embeddings and tokenizer while learning to generate appropriate nonverbal tokens. The automated pipeline with human validation ensures scalability while maintaining annotation quality, and the inclusion of emotion labels provides additional context for expressive synthesis.

## Foundational Learning

- **Concept: Nonverbal Vocalizations (NVs) in Speech Synthesis**
  - **Why needed here:** This is the core problem the paper addresses. You must understand that NVs (laughter, coughs, sighs, etc.) are distinct from lexical speech and require explicit modeling and data.
  - **Quick check question:** What are the 10 types of nonverbal vocalizations annotated in the NVTTS dataset?

- **Concept: Zero-Shot Text-to-Speech (TTS)**
  - **Why needed here:** The paper's goal is to achieve zero-shot synthesis (generating speech for an unseen speaker from a reference) with high NV fidelity. Understanding this paradigm is critical to evaluating the results.
  - **Quick check question:** In a zero-shot TTS experiment, how is the test set partitioned relative to the training data to ensure no speaker overlap?

- **Concept: Supervised Fine-Tuning**
  - **Why needed here:** The paper's method for achieving its results is not training from scratch, but fine-tuning a specific component of a pre-trained model. This is a key architectural detail.
  - **Quick check question:** Which component of the CosyVoice architecture is fine-tuned on the NVTTS dataset, and which components remain frozen?

## Architecture Onboarding

- **Component Map:** Raw audio from VoxCeleb/Expresso -> ASR (Canary) -> NV Detection (BEATs) -> Alignment (MFA) -> Emotion Classification (emotion2vec+) -> Human Validation (Argilla) -> Fusion (Algorithm 1&2) -> NVTTS dataset; CosyVoice-300M base -> Fine-tune LM only -> Generate speech with NV tags

- **Critical Path:** The most critical path is the NV detection -> Human Validation -> Fusion sequence. Errors here directly train the model to make mistakes. Successful implementation hinges on only fine-tuning the LM component as described.

- **Design Tradeoffs:**
  - Dataset Size vs. Quality: Smaller (17h), high-quality, curated dataset vs. larger but lower-quality corpora
  - Automated vs. Manual Annotation: Automated tools for scalability with human validation for quality
  - Emotion Tags: Ablation study suggests removing emotion tags may improve NV generation

- **Failure Signatures:**
  - Low-Fidelity NVs: Generated laughs/coughs sound artificial or missing entirely (poor data pipeline or insufficient training)
  - Misplaced NVs: NV generated in middle of words (failure in alignment step)
  - Poor Speaker Similarity: Generated voice doesn't match reference speaker (base model's speaker embeddings not properly conditioned)

- **First 3 Experiments:**
  1. Reproduce automatic evaluation: Download NVTTS test set and fine-tuned model, generate audio, compute metrics, compare to Table 8
  2. Ablation on emotion tags: Train two models (with and without emotion tags), compare NV-specific metrics
  3. Test on rare NVs: Create custom test set focusing on rarest NVs (sighs, snores, grunts), evaluate performance vs. common classes

## Open Questions the Paper Calls Out
- Do speech encoders implicitly capture nonverbal cues without explicit supervision?
- Can the NVTTS corpus effectively serve as a seed for semi-supervised discovery of nonverbal data at scale?
- Why does the removal of emotion annotations marginally improve nonverbal vocalization generation quality?
- Does the 8-category emotion taxonomy fail to capture the nuanced variability present in expressive speech?

## Limitations
- Dataset size is limited to 17 hours with significant imbalance across NV categories (some with only 7 samples)
- Results may not generalize to languages other than English or to speakers not well-represented in VoxCeleb
- The automated detection pipeline with threshold of 0.1 may have missed subtle or ambiguous nonverbal vocalizations

## Confidence
- **High Confidence:** Dataset construction methodology, fine-tuning approach (training only LM), evaluation metrics
- **Medium Confidence:** Claims of "state-of-the-art" performance comparable to CosyVoice2, ablation study results
- **Low Confidence:** Generalizability to other languages, handling of unseen speakers or contexts, potential demographic biases

## Next Checks
1. Conduct independent audit of 100 randomly sampled NVTTS annotations by new annotators to verify quality scores and identify systematic biases
2. Fine-tune same CosyVoice model on NVTTS and evaluate on independent expressive speech corpus to validate generalizability
3. Create stratified test set oversampling rare NV categories to quantify impact of data imbalance on synthesis quality for underrepresented NV types