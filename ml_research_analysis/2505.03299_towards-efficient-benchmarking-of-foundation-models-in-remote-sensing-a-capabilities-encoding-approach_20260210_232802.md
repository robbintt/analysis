---
ver: rpa2
title: 'Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities
  Encoding Approach'
arxiv_id: '2505.03299'
source_url: https://arxiv.org/abs/2505.03299
tags:
- sensing
- remote
- dataset
- datasets
- sentinel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently benchmarking Remote
  Sensing Foundation Models (RSFMs) across diverse downstream tasks, given the computational
  cost of extensive fine-tuning. The authors propose a "capabilities encoding" approach
  that embeds both RSFMs and downstream datasets into a shared latent space, allowing
  for efficient prediction of model performance without comprehensive fine-tuning.
---

# Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach

## Quick Facts
- arXiv ID: 2505.03299
- Source URL: https://arxiv.org/abs/2505.03299
- Authors: Pierre Adorni; Minh-Tan Pham; Stéphane May; Sébastien Lefèvre
- Reference count: 40
- Key outcome: Proposes capabilities encoding approach for efficient benchmarking of RSFMs by embedding models and datasets in shared latent space to predict performance without exhaustive fine-tuning

## Executive Summary
This paper addresses the computational challenge of benchmarking Remote Sensing Foundation Models (RSFMs) across diverse downstream tasks by proposing a capabilities encoding approach. The method embeds both models and datasets into a shared 5-dimensional latent space, allowing researchers to predict model performance on new tasks without extensive fine-tuning. By analyzing 1106 fine-tuning results from 85 models across 160 datasets, the authors demonstrate that models can be accurately positioned based on their relative performance, with L2 distance providing optimal prediction accuracy. The approach significantly reduces computational costs for benchmarking new RSFMs while providing insights into model generalization capabilities through spatial visualization.

## Method Summary
The capabilities encoding method transforms raw model performance data into normalized performance gaps ($\Delta_{m,t}$) by calculating each model's distance from the best performer on each task. Models and datasets are then embedded as points in a shared 5-dimensional Euclidean space, with the optimization objective of minimizing the mean squared error between Euclidean distances and normalized performance gaps. The method filters out models with fewer than 5 fine-tunings and datasets tested on fewer than 5 models to improve robustness. Distance metrics are compared, with L2 distance showing slight superiority over Poincaré distance and significantly outperforming cosine distance, which tends to overfit.

## Key Results
- L2 distance with 5-dimensional embedding achieves optimal prediction accuracy while avoiding overfitting
- Models with 10-20+ fine-tuning results show stable and accurate performance predictions
- Spatial visualization reveals clusters of models and datasets, with well-generalizing models positioned centrally
- Prediction accuracy degrades significantly for models with fewer than 5-10 fine-tuning records

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding both models and datasets into a shared latent space enables prediction of model-dataset performance without exhaustive fine-tuning.
- Mechanism: The method optimizes positions of points $x \in \mathbb{R}^n$ (one per model and dataset) such that Euclidean distances approximate normalized performance gaps $\Delta_{m,t}$. This is achieved by minimizing $L(x) = \frac{1}{n}\sum_{\forall t,\forall m}(d(x_m, x_t) - \Delta_{m,t})^2$.
- Core assumption: There exists a low-dimensional geometry where relative model capabilities and dataset requirements can be jointly represented, and distances correlate with performance gaps.
- Evidence anchors:
  - [abstract]: "we propose a cost-effective method for predicting a model's performance on multiple downstream tasks without the need for fine-tuning on each one...based on what we call 'capabilities encoding.'"
  - [section 3.2]: Formulates the optimization problem to minimize prediction error between latent distance and normalized performance.
  - [corpus]: Limited direct evidence; neighboring papers focus on RS foundation model training rather than efficient benchmarking.
- Break condition: When models have fewer than 10–20 fine-tuning records, positioning accuracy degrades, leading to high prediction variance (see Section 4.2, Figure 4).

### Mechanism 2
- Claim: Euclidean (L2) distance with 5-dimensional embedding provides the best trade-off between expressiveness and generalization for this task.
- Mechanism: The 5D L2 space is expressive enough for optimization convergence without overfitting. Cosine distance overfits, and Poincaré underperforms slightly vs. L2.
- Core assumption: Performance relationships are not inherently hierarchical (which would favor hyperbolic space) and can be captured in a compact Euclidean manifold.
- Evidence anchors:
  - [section 4.1]: "The L2 and Poincaré distances yield better results, with L2 showing a slight advantage...a dimension of 5 is optimal, as it provides sufficient complexity for the optimization to converge without allowing overfitting."
  - [figure 3]: Shows scatter plots of estimated distance vs. ground truth for L2, Cosine, and Poincaré; L2 and Poincaré align closer to identity.
  - [corpus]: No directly comparable evaluation of distance metrics in related RSFM papers.
- Break condition: If future tasks exhibit strong hierarchical structure (e.g., fine-grained taxonomies), L2 may fail to capture the structure; hyperbolic alternatives could be reconsidered.

### Mechanism 3
- Claim: A model's position centrality in the latent space correlates with its generalization capability across diverse tasks.
- Mechanism: Models that perform well on many datasets are placed near the center of the embedding. Poorly-generalizing models are positioned peripherally, near the specific datasets they handle.
- Core assumption: Generalization can be inferred from relative performance across a diverse task set, and centrality is a useful proxy.
- Evidence anchors:
  - [section 4.3]: "SkySense, the best model on most datasets, lies in the center, whereas models like CSPT, GASSL and BFM, which have strengths and weaknesses, are positioned outside of the main cluster."
  - [section 4.3]: UMAP visualization shows few-shot datasets and CNN-based models forming distinct clusters, suggesting architecture-task affinities.
  - [corpus]: Indirect support from papers like "A Genealogy of Foundation Models in Remote Sensing," which discuss generalization challenges in RSFMs.
- Break condition: If the underlying task distribution is biased or incomplete, centrality may not reflect true generalization to unseen real-world tasks.

## Foundational Learning

- Concept: Normalized Performance Gap ($\Delta_{m,t}$)
  - Why needed here: Understanding how raw metrics are transformed into a unified 0–1 scale is essential for interpreting embedding distances and predictions.
  - Quick check question: Given a model with 89.7 mF1 and the best model with 94.1 mF1 on a task, can you compute $\Delta$ assuming the worst model scores 85.0?

- Concept: Distance Metrics in Latent Spaces (L2, Cosine, Poincaré)
  - Why needed here: The paper explicitly compares distance metrics; understanding their properties helps explain why L2 is selected.
  - Quick check question: Why might cosine distance overfit in this setting while L2 generalizes better?

- Concept: Self-Supervised Pre-training Strategies (Contrastive vs. Reconstructive)
  - Why needed here: RSFMs use diverse pre-training objectives (MoCo, BYOL, MAE, etc.), which influence downstream performance and embedding positions.
  - Quick check question: How does a contrastive approach (e.g., SeCo) differ from a reconstructive approach (e.g., RingMo's MAE) in terms of what the model learns?

## Architecture Onboarding

- Component map: Normalization module -> Embedding optimizer -> Distance estimator -> Data filter
- Critical path:
  1. Aggregate 1,106 (model, task) performance records from 36 papers
  2. Normalize per-task to $\Delta_{m,t}$
  3. Split into train/validation (e.g., 810/10 per run, 10 splits)
  4. Optimize 5D L2 embeddings
  5. Validate: correlate estimated distances with ground-truth $\Delta$

- Design tradeoffs:
  - Dimensionality: 5D is optimal; higher dimensions overfit, lower underfit
  - Data filtering: Filtering improves quality but reduces coverage (820 points vs. 1,106)
  - Distance choice: L2 is simple and empirically best; Poincaré is close but more complex; Cosine overfits

- Failure signatures:
  - High variance in predictions for models with <10 prior fine-tunings
  - Overfitting if using cosine distance or >5 dimensions (validation points deviate from identity)
  - Poor prediction for datasets that are "saturated" (low variance in literature results, e.g., UCMerced)

- First 3 experiments:
  1. Reproduce the distance metric comparison (L2 vs. Cosine vs. Poincaré) on the provided dataset; confirm L2 superiority
  2. Ablate embedding dimension (3, 5, 7, 10) and plot validation MSE to verify 5D optimality
  3. Stratify validation accuracy by number of fine-tunings per model; quantify the threshold (10–20) beyond which predictions stabilize

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the capabilities encoding method be optimized to maintain high prediction accuracy for new models that have very few existing fine-tuning results (e.g., fewer than five)?
- **Basis in paper:** [Explicit] The authors note that while the method works well for models with 10–20 fine-tuning results, "accurate predictions are achievable only for models with a substantial number of fine-tuning results... Further optimization is necessary to enhance precision with fewer data points."
- **Why unresolved:** The current method positions models based on relative distances derived from multiple performance points; it struggles to locate models in the latent space when the input data is sparse.
- **What evidence would resolve it:** A modified algorithm or regularization technique that achieves low Mean Squared Error (MSE) on validation data for models with limited training samples.

### Open Question 2
- **Question:** Can the embedding framework be adapted to predict absolute performance metrics (such as raw accuracy or $\delta$) rather than the current relative position ($\Delta$) in the literature?
- **Basis in paper:** [Explicit] The discussion states: "Ideally, we should aim to predict the model performance directly or, at least, the absolute distance to the best performance... Although we have not yet achieved such a goal, future work could explore this direction."
- **Why unresolved:** The current normalization bounds performance between the known best and worst models, limiting the ability to predict performance on novel tasks where bounds are unknown.
- **What evidence would resolve it:** A loss function and reconstruction method that successfully maps latent distances to unnormalized performance values on unseen datasets.

### Open Question 3
- **Question:** Can incorporating a deeper analysis of dataset quality (e.g., saturation, variance) accelerate the optimal positioning of a new model within the embedding space?
- **Basis in paper:** [Explicit] The authors suggest: "A potential avenue to address this issue is to deepen the quality analysis of the datasets. This would help identify which datasets are most valuable for fine-tuning, thereby accelerating the optimal positioning."
- **Why unresolved:** The current approach treats all fine-tuning results equally, whereas selecting specific high-variance datasets might reduce the number of fine-tunings needed to characterize a model.
- **What evidence would resolve it:** Experiments showing that a strategic selection of "high-quality" datasets yields better positional accuracy with fewer fine-tuning runs than a random selection.

## Limitations

- High variance predictions for models with fewer than 10-20 fine-tuning records limit method applicability to new architectures
- Method effectiveness depends on quality and diversity of underlying performance database
- L2 distance may not capture hierarchical relationships in future task taxonomies
- Requires substantial initial fine-tuning data to build baseline performance matrix

## Confidence

- **High confidence**: The core mechanism of embedding models and datasets in shared latent space for performance prediction is well-validated through extensive experiments (1106 fine-tuning results)
- **Medium confidence**: The generalizability to truly novel models with sparse fine-tuning data, as the method shows high variance in these cases
- **Medium confidence**: The interpretation of spatial centrality as a proxy for generalization capability, which depends on the representativeness of the underlying task distribution

## Next Checks

1. **Cross-domain validation**: Test whether the capabilities encoding learned from remote sensing tasks generalizes to other domains (e.g., medical imaging, natural scene classification) by applying the same framework to a different domain's fine-tuning database.

2. **Temporal stability analysis**: Evaluate how the embedding positions and prediction accuracy change over time as new models and datasets are added to the database, assessing the method's stability and update requirements.

3. **Ablation on task diversity**: Systematically vary the diversity of tasks in the training set (e.g., using only land cover classification vs. including segmentation, change detection, etc.) to quantify how task distribution affects prediction accuracy and model positioning.