---
ver: rpa2
title: 'Predictive Concept Decoders: Training Scalable End-to-End Interpretability
  Assistants'
arxiv_id: '2512.15712'
source_url: https://arxiv.org/abs/2512.15712
tags:
- concepts
- decoder
- encoder
- concept
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Predictive Concept Decoders (PCDs) are end-to-end trainable architectures
  that learn to predict model behavior from activations using a sparse concept bottleneck.
  The encoder compresses activations into a small set of interpretable concepts, and
  the decoder answers questions about model behavior using only those concepts.
---

# Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants

## Quick Facts
- **arXiv ID**: 2512.15712
- **Source URL**: https://arxiv.org/abs/2512.15712
- **Reference count**: 40
- **Primary result**: PCDs outperform baselines on controlled tasks while maintaining interpretability through sparse concept bottlenecks

## Executive Summary
Predictive Concept Decoders (PCDs) are end-to-end trainable architectures designed to interpret model behavior by learning to predict model outputs from activations using a sparse concept bottleneck. The system compresses high-dimensional activations into a small set of interpretable concepts through an encoder, then uses a decoder to answer questions about model behavior using only those concepts. This approach enables scalable interpretability by pretraining on web data via next-token prediction and finetuning on specific interpretability tasks. The sparse bottleneck design facilitates human auditing while maintaining performance across tasks like jailbreak detection, secret hint revelation, and latent concept surfacing.

## Method Summary
PCDs operate through a two-stage process: an encoder compresses model activations into a sparse set of interpretable concepts, and a decoder uses these concepts to answer questions about model behavior. The architecture is trained end-to-end, with pretraining on web data using next-token prediction objectives, followed by finetuning on specific interpretability tasks. The sparse concept bottleneck forces the encoder to learn meaningful, human-interpretable representations while limiting information loss. This design enables both scalability (through pretraining) and interpretability (through the bottleneck), with performance improving as training data increases. The approach is evaluated on controlled synthetic tasks that test the system's ability to detect model behaviors and latent concepts.

## Key Results
- PCDs outperform baseline interpretability methods on controlled tasks including jailbreak detection and secret hint revelation
- Performance scales with training data size, showing consistent improvement as more data is used
- Encoder concepts become more interpretable with scale, though this is assessed subjectively
- Sparse bottleneck design enables human auditing while maintaining competitive performance

## Why This Works (Mechanism)
PCDs work by leveraging the principle that model behavior can be predicted from compressed representations if those representations capture the essential information needed for the task. The sparse bottleneck forces the encoder to identify and represent only the most salient concepts, eliminating noise and redundancy. This compression creates a natural interface for human interpretation while maintaining sufficient information for accurate behavior prediction. The two-stage training process (pretraining then finetuning) allows the system to first learn general patterns from web data before specializing on interpretability tasks, similar to how foundation models are adapted for downstream applications.

## Foundational Learning
- **Concept bottleneck learning**: Why needed - to force interpretable representations; Quick check - verify bottleneck dimensionality matches number of meaningful concepts
- **End-to-end training**: Why needed - to ensure encoder and decoder work harmoniously; Quick check - monitor gradient flow between components during training
- **Two-stage training**: Why needed - to leverage large-scale pretraining while adapting to interpretability tasks; Quick check - compare performance with and without pretraining
- **Sparse representation**: Why needed - to eliminate noise and focus on salient features; Quick check - measure concept cardinality and sparsity
- **Next-token prediction**: Why needed - to learn general patterns from web data; Quick check - evaluate pretraining loss on held-out data
- **Interpretability through compression**: Why needed - to create human-understandable representations; Quick check - conduct human studies on concept comprehensibility

## Architecture Onboarding

**Component Map:**
Web Data -> Pretraining Module -> Finetuning Module -> Encoder (Activation Compression) -> Sparse Concept Bottleneck -> Decoder (Behavior Prediction) -> Interpretability Output

**Critical Path:**
The critical path flows from model activations through the encoder to the concept bottleneck, then through the decoder to produce interpretability outputs. The bottleneck is the most critical component as it determines both interpretability and information retention.

**Design Tradeoffs:**
The sparse bottleneck creates a fundamental tradeoff between interpretability and information retention - sparser bottlenecks are more interpretable but may lose important information. The two-stage training approach trades computational efficiency for better generalization. The pretraining data choice affects both performance and the types of concepts learned.

**Failure Signatures:**
- Overly sparse bottlenecks cause performance degradation and loss of subtle behaviors
- Dense bottlenecks reduce interpretability and defeat the purpose of the architecture
- Domain mismatch between pretraining data and finetuning tasks leads to concept drift
- Poor encoder design results in concepts that don't generalize across different model behaviors

**3 First Experiments:**
1. Vary bottleneck sparsity (e.g., 5, 10, 20 concepts) and measure both interpretability and performance on a simple behavior detection task
2. Compare end-to-end training vs. separate training of encoder and decoder on a controlled synthetic task
3. Test pretraining on different data sources (e.g., code vs. natural language) and measure impact on interpretability task performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on controlled synthetic tasks rather than comprehensive real-world interpretability assessment
- Scalability results tested on limited data ranges, making true large-scale performance uncertain
- Concept interpretability claims are subjective rather than rigorously quantified
- Human auditing benefits are asserted but not empirically validated through user studies

## Confidence

**Major Claim Clusters Confidence:**
- PCD architecture effectiveness: **Medium** - Strong controlled task performance but limited real-world validation
- Scalability benefits: **Medium** - Positive trends observed but limited data range tested  
- Concept interpretability improvement: **Low** - Based on subjective assessment without rigorous metrics
- Human auditing advantages: **Low** - Asserted benefits not empirically validated

## Next Checks
1. Conduct user studies with human auditors comparing PCD-based analysis to baseline interpretability methods on real model auditing tasks, measuring both accuracy and time efficiency
2. Test PCD performance across diverse model architectures (beyond LLMs) and tasks, including multi-modal models, to assess generalizability
3. Implement quantitative metrics for concept interpretability (e.g., concept purity, human-consensus scores) and validate the claimed scaling relationship across orders of magnitude more training data