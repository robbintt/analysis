---
ver: rpa2
title: 'FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion'
arxiv_id: '2503.04222'
source_url: https://arxiv.org/abs/2503.04222
tags:
- b-instruct
- llms
- preference
- source
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents FuseChat-3.0, a suite of large language models
  that integrates strengths from heterogeneous source models into compact target models.
  The method uses a two-stage training pipeline: supervised fine-tuning to align target
  models with source distributions, followed by Direct Preference Optimization to
  incorporate preferences from multiple source models.'
---

# FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion

## Quick Facts
- **arXiv ID**: 2503.04222
- **Source URL**: https://arxiv.org/abs/2503.04222
- **Reference count**: 16
- **Primary result**: FuseChat-3.0 achieves 6.8 average improvement across 14 benchmarks, with 37.1 points on AlpacaEval-2 and 30.1 points on Arena-Hard for 8B-parameter LLMs.

## Executive Summary
FuseChat-3.0 presents a novel approach to implicit model fusion that transfers strengths from heterogeneous source LLMs to compact target models. The method uses a two-stage training pipeline: supervised fine-tuning (SFT) to align target model distributions with source models, followed by Direct Preference Optimization (DPO) using carefully constructed preference pairs. By constructing preference pairs from the best and worst responses of the same source model, the approach eliminates reward bias from heterogeneous response styles. Applied to Llama-3.1-8B-Instruct, FuseChat-3.0 achieves state-of-the-art performance for 8B-parameter models, establishing new benchmarks across instruction-following, general reasoning, mathematics, and coding tasks.

## Method Summary
FuseChat-3.0 employs implicit model fusion (IMF) to transfer knowledge from four heterogeneous source models (Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, Llama-3.1-70B-Instruct) to compact target models. The training pipeline consists of SFT for 3 epochs to align distributions, followed by DPO for 1 epoch using same-source preference pairs (highest vs. lowest reward model scores with 0.01-0.1 score gap). The approach uses length-normalized DPO for Llama targets and standard DPO for Qwen/Gemma targets. Training data spans 158,667 samples across multiple domains, with reward model ArmoRM-LLaMA3-8B-v0.1 providing annotations.

## Key Results
- **6.8 average improvement** across 14 benchmarks for Llama-3.1-8B-Instruct
- **37.1 points** improvement on AlpacaEval-2 (SFT+DPO vs SFT alone)
- **30.1 points** improvement on Arena-Hard
- **Comparable performance** to much larger models despite 8B parameter size
- **Cross-model scalability** demonstrated across 1B-8B parameter sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Same-source preference pairing eliminates reward model bias from heterogeneous styles
- **Mechanism**: Intra-model pairing ensures preference signals reflect response quality rather than stylistic differences between models
- **Core assumption**: Reward model bias from heterogeneous styles was degrading prior DPO approaches
- **Evidence**: Abstract states same-source pairing "eliminates the reward bias caused by heterogeneous response styles"; RM score gap constraint (0.01-0.1) maintains distinguishability

### Mechanism 2
- **Claim**: Two-stage pipeline (SFT → DPO) necessary for distribution alignment before preference learning
- **Mechanism**: SFT shifts target model distribution toward source models, then DPO fine-tunes within this aligned distribution
- **Core assumption**: Direct DPO on off-policy data from heterogeneous sources causes optimization instability
- **Evidence**: Abstract describes the two-stage pipeline; cited prior work on distribution shift challenges in preference learning

### Mechanism 3
- **Claim**: Length-normalized DPO reduces verbosity bias in math and coding tasks
- **Mechanism**: Normalizes log-probabilities by response length, prioritizing correctness over verbosity
- **Core assumption**: Length bias was harming DPO in domains where brevity correlates with precision
- **Evidence**: Section 5.4 shows significant improvements in mathematics and coding categories due to length normalization

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here**: FuseChat-3.0 uses DPO as core alignment method
  - **Quick check question**: Can you explain why DPO avoids training an explicit reward model compared to RLHF?

- **Concept: Knowledge Distillation vs. Implicit Model Fusion**
  - **Why needed here**: Paper distinguishes explicit fusion from implicit fusion via generated outputs
  - **Quick check question**: What is the key advantage of learning from outputs vs. matching token-level probability distributions?

- **Concept: Distribution Shift in Preference Learning**
  - **Why needed here**: SFT is positioned as solution to distribution shift before DPO
  - **Quick check question**: Why would applying DPO directly to responses from a different model cause training instability?

## Architecture Onboarding

- **Component map**: Prompts → Source Model Sampling → Reward Model Annotation → SFT/DPO Split → Target Model Training
- **Critical path**: 
  1. Curate diverse prompts across instruction-following, math, coding, Chinese
  2. Sample multiple responses per prompt from each source model
  3. Annotate with reward model (ArmoRM) or verify with rules
  4. Build same-source preference pairs (highest vs. lowest RM score)
  5. SFT on top responses → DPO on preference pairs
- **Design tradeoffs**:
  - Same-source pairing reduces bias but may underutilize cross-model diversity
  - RM score gap constraint (0.01–0.1) filters noisy pairs but may discard informative borderline cases
  - LN-DPO helps math/coding but may not generalize to open-ended tasks
- **Failure signatures**:
  - DPO loss divergence: Check if reference model is stale or pairs have insufficient RM score gap
  - Coding performance drop (−0.3 points): May indicate insufficient coding data or overfitting to SFT distribution
  - Chinese DPO omitted due to lack of reward models: Consider proxy RM or cross-lingual transfer
- **First 3 experiments**:
  1. Replicate SFT-only baseline to validate distribution alignment contribution independently of DPO
  2. Ablate same-source vs. cross-source preference pairing to quantify bias reduction
  3. Test LN-DPO vs. standard DPO on held-out math/coding benchmarks to confirm domain-specific gains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does increasing coding training data volume fully recover the observed performance decline, or does DPO objective require specific adjustments for logical syntax domains?
- **Basis**: Paper attributes 0.3 average point decline in coding to "relatively limited coding data"
- **Evidence needed**: Vary coding subset size while keeping other domains constant to observe trajectory of HumanEval and MBPP scores

### Open Question 2
- **Question**: How does number and diversity of source models impact final target performance; is there point of diminishing returns?
- **Basis**: Paper uses fixed set of four source models without analyzing sensitivity to choice or quantity
- **Evidence needed**: Ablation study removing one source model at a time or adding additional architectures to measure marginal utility

### Open Question 3
- **Question**: To what extent does FuseChat-3.0 optimization trade off general reasoning for instruction-following gains in models with high initial baselines?
- **Basis**: Qwen-2.5-7B shows drop in general benchmarks despite large instruction-following gains
- **Evidence needed**: Per-domain analysis of DPO loss specifically on general knowledge tasks to see if instruction-following data dominates learning signal

## Limitations

- **Model dependency**: SFT→DPO necessity may be model-dependent, providing diminishing returns when target and source models are from same family
- **Limited ablation**: LN-DPO shows domain-specific benefits but lacks comprehensive ablation studies across different task types
- **Scalability assumptions**: Claims of cross-model scalability across 1B-8B sizes are based on same training procedure without demonstrating optimal hyperparameter transfer

## Confidence

- **High Confidence**: Empirical improvements (6.8 average points, 37.1 on AlpacaEval-2) are well-documented and reproducible
- **Medium Confidence**: Necessity of SFT stage before DPO is supported by literature but specific contribution could benefit from more detailed ablations
- **Low Confidence**: Scalability claims across different model sizes are based on same training procedure without demonstrating optimal hyperparameter transfer

## Next Checks

1. **Ablation study of SFT contribution**: Train an 8B model with direct DPO (skipping SFT) using the same preference pairs to quantify the exact distribution alignment benefit versus potential overfitting to source distributions.

2. **Cross-source pairing validation**: Systematically compare same-source vs. cross-source preference pairing on the same 8B model to measure the actual impact of reward model bias reduction versus the potential loss of cross-model diversity.

3. **LN-DPO generalization test**: Evaluate LN-DPO across all 14 benchmarks (not just math/coding) to determine if the length normalization consistently improves or potentially harms performance in open-ended, explanation-heavy tasks where verbosity may be desirable.