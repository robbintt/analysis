---
ver: rpa2
title: 'LLM4SFC: Sequential Function Chart Generation via Large Language Models'
arxiv_id: '2512.06787'
source_url: https://arxiv.org/abs/2512.06787
tags:
- generation
- sfcs
- language
- structured
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating executable Sequential
  Function Charts (SFCs) from natural language descriptions, a gap in industrial automation
  where SFCs are widely used but hard to generate due to their graphical nature and
  embedded Structured Text. The proposed LLM4SFC framework introduces a reduced textual
  representation of SFCs, fine-tuning strategies (Next Token Prediction and Subgraph
  Masking), and structured generation with few-shot retrieval-augmented generation.
---

# LLM4SFC: Sequential Function Chart Generation via Large Language Models

## Quick Facts
- **arXiv ID**: 2512.06787
- **Source URL**: https://arxiv.org/abs/2512.06787
- **Reference count**: 36
- **Primary result**: Proprietary models achieve 75%-94% success rates in generating syntactically valid and safe SFCs from natural language descriptions

## Executive Summary
LLM4SFC addresses the challenge of generating executable Sequential Function Charts (SFCs) from natural language descriptions in industrial automation. The framework introduces a reduced textual representation of SFCs that captures essential topology and embedded Structured Text while omitting verbose metadata. Through fine-tuning strategies (Next Token Prediction and Subgraph Masking) and structured generation with few-shot retrieval-augmented generation, the system achieves high success rates in producing syntactically valid and safety-compliant SFCs. Proprietary models show 75%-94% success rates, while open-source models improve from ~15% to ~70% with structured generation. The framework bridges natural language intent with executable industrial programming code.

## Method Summary
The LLM4SFC framework converts PLCopen XML SFCs into a minimal textual representation containing step names, actions (ST code), children with transition conditions, and variable declarations. This representation is deterministically recompilable to valid XML. The system fine-tunes models using LoRA (rank=128) with Next Token Prediction and Subgraph Masking objectives on 2,390 manufacturing SFCs. At inference, it uses k=3 retrieval-augmented generation with structured decoding (FSM-based token pruning) to enforce grammar compliance. Safety verification uses NuXmv model checking with 6-hour timeouts per SFC.

## Key Results
- Proprietary models (GPT-4o, GPT-4o-mini) achieve 75%-94% Pass@k rates for syntactic validity
- Structured generation improves open-source model success from ~15% to ~70%
- Few-shot retrieval with k=3 examples provides incremental performance gains
- Most common failure mode is transition errors (22-62% of failures)
- Open-source models show >65% timeout rates on complex SFCs during safety verification

## Why This Works (Mechanism)

### Mechanism 1: Reduced Textual Representation
The framework abstracts away verbose PLCopen XML metadata while preserving essential topology through a minimal format containing step names, actions, children with transition conditions, and variable declarations. This representation is deterministically recompilable to valid XML, enabling LLMs to generate complex graphical SFCs by focusing on control logic rather than vendor-specific metadata.

### Mechanism 2: Structured Generation via FSM
A finite-state machine built from the reduced representation grammar masks invalid tokens during decoding, ensuring only grammatically valid continuations are sampled. This dramatically improves syntactic validity for open-source models by replacing unconstrained autoregressive generation with grammar-enforced output.

### Mechanism 3: Subgraph Masking Fine-Tuning
Randomly masking clusters of structurally related steps, transitions, and actions during training forces the model to learn structural relationships rather than surface patterns. This hybridizes graph-masked language modeling with Fill-in-the-Middle objectives, though benefits are modest compared to standard next-token prediction.

## Foundational Learning

- **IEC 61131-3 SFC Semantics**: SFCs have specific structural rules (single initial step, transition validity, parallel/alternative branching) that determine both syntax and safety. *Quick check*: Can you explain why a transition exiting a parallel branch without synchronization violates Safe SFC criteria?

- **Structured Generation / Constrained Decoding**: The framework relies on FSM-based logit manipulation to enforce grammar compliance during generation. *Quick check*: How does masking invalid tokens at each step differ from post-hoc rejection sampling?

- **LoRA Fine-Tuning**: The framework uses Low-Rank Adaptation for parameter-efficient fine-tuning of open-source models. *Quick check*: What is the relationship between LoRA rank (r) and the number of trainable parameters?

## Architecture Onboarding

- **Component map**: User prompt -> Embedding & retrieval (RAG) -> Context assembly (prompt + 3-shot examples) -> Structured generation (LLM + FSM decoder) -> Reduced SFC -> Parse to PLCopen XML -> Import to IDE (TwinCAT/CODESYS) -> Optional: Safety verification via NuXmv

- **Critical path**: Natural language description is embedded and retrieved against indexed SFC summaries, then used as few-shot examples with the fine-tuned LLM to generate a reduced SFC representation. This is converted to PLCopen XML, validated syntactically, and optionally verified for safety properties.

- **Design tradeoffs**: Reduced representation trades completeness for token efficiency (metadata regenerated by IDE, not model); structured generation trades generation diversity for syntactic guarantees; few-shot (k=3) trades context window for example-guided accuracy; proprietary models offer higher reliability but no fine-tuning access; open-source models are customizable but require more infrastructure.

- **Failure signatures**: Init Step Error (generated SFC lacks exactly one initial step); Transition Error (incomplete or missing transition targets, most common error type); Safety Error (illegal jumps across parallel branches, unreachable convergences); Timeout Error (overly complex SFCs exceeding 6-hour verification limit).

- **First 3 experiments**: 1) Baseline zero-shot prompting without structured decoding on Qwen2.5-Coder-7B (expect ~15% success rate); 2) Enable FSM-based token masking on same model (expect ~65-70% success rate); 3) Add k=3 retrieval with structured generation (expect incremental improvement).

## Open Questions the Paper Calls Out

- Can multi-modal models bypass the need for textual conversion by directly processing graphical representations of SFCs? The authors suggest this could be a promising avenue for future work, as current LLMs lack visual perception.

- How can the visual clarity and interpretability of generated SFCs be formally evaluated for human engineers? Current metrics assess syntactic validity and safety, but generated charts may be functionally correct yet visually suboptimal.

- To what extent do the fine-tuned models generalize to SFCs in non-manufacturing domains? The dataset is specific to manufacturing tasks, potentially confining the model's understanding of SFC conventions to that industry.

## Limitations

- The 2,390 proprietary SFCs from Bosch manufacturing projects are not publicly available, creating a fundamental reproducibility barrier
- The reduced representation cannot optimize SFC layout for human readability due to LLMs lacking visual perception
- Safety verification using NuXmv shows high timeout rates (>65%) on complex SFCs with open-source models

## Confidence

**High confidence**: Core mechanism of reduced textual representation validated through 75%-94% success rates on proprietary models; structured generation improvement from ~15% to ~70% is directly measurable.

**Medium confidence**: Safety verification framework using NuXmv is technically sound but shows practical scalability limitations; few-shot RAG approach shows incremental benefits but optimal parameters unclear.

**Low confidence**: Claims about subgraph masking benefits are weakest - NTP marginally outperforms masking with only modest differences; theoretical justification for structural understanding isn't strongly supported by results.

## Next Checks

1. **Dataset substitution experiment**: Test the framework on publicly available SFC datasets or synthetic SFC generators to validate methodology generalization beyond proprietary Bosch dataset.

2. **Safety verification stress test**: Systematically generate SFCs of increasing complexity to determine exact threshold where NuXmv verification fails, and test alternative verification approaches.

3. **Grammar specification audit**: Formally define complete grammar for reduced SFC representation and validate FSM-based structured generation against this specification to ensure no valid SFCs are excluded.