---
ver: rpa2
title: Metric assessment protocol in the context of answer fluctuation on MCQ tasks
arxiv_id: '2507.15581'
source_url: https://arxiv.org/abs/2507.15581
tags:
- fluctuation
- metrics
- rates
- permutations
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a metric assessment protocol for evaluating\
  \ large language models on multiple-choice question (MCQ) tasks, focusing on answer\
  \ fluctuation\u2014the phenomenon where models produce different answers to the\
  \ same question when the order of answer options is changed. The protocol assesses\
  \ how well various MCQ evaluation metrics correlate with full fluctuation rates\
  \ (computed over all possible option permutations) while also reflecting the model's\
  \ original accuracy on the unaltered benchmark."
---

# Metric assessment protocol in the context of answer fluctuation on MCQ tasks

## Quick Facts
- **arXiv ID**: 2507.15581
- **Source URL**: https://arxiv.org/abs/2507.15581
- **Reference count**: 10
- **Primary result**: The paper proposes a metric assessment protocol for evaluating LLM MCQ performance, introducing "worst accuracy" as a metric that best captures both model performance and robustness to answer fluctuation across permutations.

## Executive Summary
This paper addresses the critical issue of answer fluctuation in multiple-choice question (MCQ) evaluation of large language models, where models may produce different answers when the order of options changes. The authors formalize existing MCQ evaluation metrics and introduce a new "worst accuracy" metric that measures whether a model consistently selects the correct answer across all tested permutations. Through systematic evaluation of 10 models on 17 benchmarks, they demonstrate that most metrics strongly correlate with full fluctuation rates, with worst accuracy showing the highest association. The study provides a protocol for selecting appropriate metrics based on evaluation goals and computational constraints.

## Method Summary
The authors developed a metric assessment protocol that evaluates how well different MCQ metrics correlate with full fluctuation rates (computed over all possible option permutations) while also reflecting original model accuracy. They formalized existing metrics including accuracy, probability mass, Brier score, entropy, PriDe, sensitivity gap, and strong accuracy, and introduced a new "worst accuracy" metric. The protocol was applied to 10 models under 10B parameters across 17 MCQ benchmarks, computing metrics on subsets of permutations (original order, two random permutations, four cyclic permutations) to evaluate their representativeness of full fluctuation rates. Correlation analysis was performed between metrics computed on these subsets and full fluctuation rates.

## Key Results
- Most MCQ metrics show strong correlation with full fluctuation rates, even when computed only on the original option order
- Worst accuracy demonstrates the highest correlation with full fluctuation rates across all permutation subsets
- Adding more permutations generally improves correlation with full fluctuation rates, though the improvement diminishes
- When balancing both fluctuation rates and original accuracy, worst accuracy performs best with cyclic or random permutations, while averaged accuracy is optimal with original or two random permutations

## Why This Works (Mechanism)
The protocol works by systematically evaluating how well different MCQ metrics capture the phenomenon of answer fluctuation—where models produce different answers to the same question when answer option order changes. By computing metrics on subsets of permutations and comparing them to full fluctuation rates (computed on all possible permutations), the protocol reveals which metrics best represent model robustness and consistency. The mechanism leverages the observation that different metrics respond differently to permutation-induced answer changes, allowing identification of metrics that are both computationally efficient and highly representative of true model behavior across all possible answer orderings.

## Foundational Learning
**Answer Fluctuation**: The phenomenon where language models produce different answers to the same question when answer option order is permuted. *Why needed*: Understanding this behavior is crucial for reliable MCQ evaluation, as it reveals model consistency and potential biases in option selection. *Quick check*: Test the same MCQ with different option orderings and observe answer consistency.

**Permutation Analysis**: Evaluating model performance across all possible orderings of answer options. *Why needed*: Provides complete understanding of model robustness to option ordering, revealing whether correct answers are consistently selected. *Quick check*: For a 4-option question, compute performance across all 24 possible orderings.

**Metric Formalization**: Systematically defining and categorizing existing MCQ evaluation metrics. *Why needed*: Enables systematic comparison and reveals relationships between different evaluation approaches. *Quick check*: Ensure each metric's definition clearly specifies how it handles probability distributions over options.

## Architecture Onboarding

**Component Map**: MCQ Benchmark -> Model Evaluation -> Permutation Generation -> Metric Computation -> Correlation Analysis -> Protocol Recommendation

**Critical Path**: Benchmark → Model → Permutations → Metrics → Correlation with Full Fluctuation Rates

**Design Tradeoffs**: Computational efficiency vs. representativeness (fewer permutations vs. full permutation analysis), single metric vs. multiple metrics for evaluation, sensitivity to correct answer selection vs. overall probability distribution analysis.

**Failure Signatures**: Poor correlation between subset metrics and full fluctuation rates indicates inadequate metric selection; inconsistent worst accuracy across permutations suggests model instability; computational inefficiency with full permutation analysis.

**First Experiments**:
1. Evaluate a simple model on a small MCQ benchmark using all 24 permutations for 4-option questions to establish baseline fluctuation patterns
2. Compute all formalized metrics on a single MCQ with 3 different permutations to observe metric behavior differences
3. Calculate correlation between original accuracy and worst accuracy on a small dataset to verify the relationship

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to models under 10B parameters, potentially limiting generalizability to larger frontier models
- Permutation analysis constrained to subsets rather than full permutation sets, potentially missing important behavioral patterns
- Assumes binary correctness without examining partial credit scenarios or confidence calibration
- Benchmark selection may introduce domain-specific biases in fluctuation behavior

## Confidence

**High confidence**: Worst accuracy most strongly correlates with full fluctuation rates across multiple permutation subsets.

**Medium confidence**: Worst accuracy is optimal for balancing fluctuation rates and original accuracy, depending on use case requirements.

**Medium confidence**: Two random permutations plus original order provides optimal computational efficiency while maintaining strong correlation.

## Next Checks
1. **Larger Model Validation**: Replicate the entire protocol on models exceeding 10B parameters (e.g., GPT-4, Claude, Gemini) to assess whether fluctuation patterns and metric correlations hold at scale.

2. **Full Permutation Analysis**: Conduct comprehensive analysis using all 24 permutations for four-option questions to determine whether subset-based conclusions remain valid and identify permutation-specific effects.

3. **Cross-Domain Generalization**: Apply the protocol to specialized domain benchmarks (medical, legal, technical) to evaluate whether metric performance and fluctuation patterns vary significantly across different knowledge domains.