---
ver: rpa2
title: End-to-End Aspect-Guided Review Summarization at Scale
arxiv_id: '2509.26103'
source_url: https://arxiv.org/abs/2509.26103
tags:
- reviews
- product
- aspects
- aspect
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a scalable LLM-based system for aspect-guided
  review summarization in e-commerce. The approach extracts aspect-sentiment pairs
  from reviews, consolidates them to canonical forms, selects the most frequent aspects,
  and generates product summaries guided by representative reviews.
---

# End-to-End Aspect-Guided Review Summarization at Scale

## Quick Facts
- arXiv ID: 2509.26103
- Source URL: https://arxiv.org/abs/2509.26103
- Authors: Ilya Boytsov; Vinny DeGenova; Mikhail Balyasin; Joseph Walt; Caitlin Eusden; Marie-Claire Rochat; Margaret Pierson
- Reference count: 14
- One-line primary result: Scalable LLM-based system for aspect-guided review summarization deployed in large-scale A/B test, showing 0.3-0.5% improvements in key e-commerce metrics

## Executive Summary
This paper presents a scalable LLM-based system for aspect-guided review summarization in e-commerce. The approach extracts aspect-sentiment pairs from reviews, consolidates them to canonical forms, selects the most frequent aspects, and generates product summaries guided by representative reviews. The pipeline was deployed in a large-scale online A/B test, showing a 0.3% increase in Add to Cart Rate, a 0.5% increase in conversion rate, and a 0.13% decrease in bounce rate, with no negative impact on revenue or page speed. Offline evaluation of 341 products showed 84% of summaries had no errors, with remaining issues mostly minor. The authors also released a dataset of 11.8 million anonymized reviews across 92,000 products with extracted aspects and generated summaries.

## Method Summary
The system uses a four-stage pipeline: (1) Aspect extraction using Gemini 1.5 Flash to parse up to 5 aspects per review with sentiment labels (POSITIVE/MIXED/NEGATIVE); (2) Aspect consolidation mapping low-frequency aspects below the 95th percentile to canonical forms while preserving high-frequency specific terms; (3) Review selection identifying top 5 aspects per product and sampling up to 200 reviews with weighted sampling; (4) Summarization generating 300-500 character summaries via structured prompts guided by the extracted aspects and sampled reviews. The pipeline was deployed in production, regenerating summaries when review count grows by 10%.

## Key Results
- Online A/B test showed 0.3% increase in Add to Cart Rate, 0.5% increase in conversion rate, and 0.13% decrease in bounce rate
- Offline manual evaluation of 341 products found 84% of summaries had no errors, with remaining issues mostly minor
- System deployed at scale with 11.8M anonymized reviews across 92K products, showing no negative impact on revenue or page speed
- Released dataset includes review_id, product_id, review_text, consolidated aspect-sentiment pairs (JSON), and generated summaries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring the summarization prompt with pre-extracted aspect-sentiment pairs and representative reviews likely reduces hallucination compared to direct end-to-end summarization of raw text.
- **Mechanism:** The pipeline forces the Large Language Model (LLM) to ground its output in specific, identified evidence (sampled reviews) and high-signal concepts (aspects) rather than synthesizing from a noisy, potentially overflowing context window.
- **Core assumption:** LLMs perform better on extraction and grounding tasks when the input is pre-structured and the context is pruned of redundant or irrelevant noise.
- **Evidence anchors:** [abstract] Mentions generating summaries "guided by representative reviews" to be "grounded in actual customer feedback"; [section 1] Cites risks in long-context summarization where review sets "exceed the effective context window."
- **Break condition:** If the aspect extraction phase fails to identify key themes, the final summary will be factually incomplete regardless of the grounding mechanism.

### Mechanism 2
- **Claim:** The 95th percentile frequency threshold for aspect consolidation appears to preserve high-value specific attributes while generalizing noisy, rare terms, creating a stable vocabulary for summarization.
- **Mechanism:** By mapping low-frequency terms to broader canonical forms, the system reduces vocabulary fragmentation, ensuring the summarization prompt contains sufficient evidence for the LLM to generate coherent sentences.
- **Core assumption:** Aspects mentioned frequently are more critical to the user's purchase decision than rare, highly specific descriptors.
- **Evidence anchors:** [section 2.2] Describes the logic: "compute the 95th percentile of aspect frequency... and keep aspects with frequencies above this threshold unchanged"; [section 5] Shows the result: reducing 178,054 distinct aspects to 19,014 consolidated ones.
- **Break condition:** In niche product categories with sparse reviews, this threshold may over-generalize distinct features, reducing summary utility.

### Mechanism 3
- **Claim:** The observed increase in conversion rates is likely driven by the reduction of cognitive load, allowing users to verify product fit via aspects rather than scanning individual reviews.
- **Mechanism:** The summary aggregates sentiment distribution (e.g., "Durability: Mixed"), providing a quick heuristic for product risk assessment. This allows users to bypass the "overwhelming volume" of raw reviews, leading to faster, more confident decision-making.
- **Core assumption:** Users trust an AI-generated summary grounded in aspects as much as, or more than, reading raw user reviews.
- **Evidence anchors:** [abstract] Reports a "0.5% increase in conversion rate" and "0.13% decrease in bounce rate"; [section 3.2] Links the improvement to "increasing user confidence through easier access to relevant customer feedback."
- **Break condition:** If the summary displays a mismatch between the aspect sentiment and the sampled review snippets, user trust may degrade, reverting behavior to manual review scanning.

## Foundational Learning

- **Concept:** **Aspect-Based Sentiment Analysis (ABSA)**
  - **Why needed here:** This is the foundational signal extraction step. The entire pipeline depends on accurately parsing "The sofa looks stylish but the fabric is cheap" into `(Style, Positive)` and `(Fabric, Negative)`.
  - **Quick check question:** Given the review "Battery life is great but the screen is dim," can you identify the two aspect-sentiment pairs?

- **Concept:** **Context Window Management**
  - **Why needed here:** The paper explicitly caps reviews at 200 per product. Understanding the trade-off between context length and LLM recall is critical to understanding why the review sampling step exists.
  - **Quick check question:** Why does the system sample 200 reviews instead of feeding all 1,000 available reviews directly to the LLM?

- **Concept:** **Prompt Chaining**
  - **Why needed here:** The architecture is not a single call but a chain: Extraction -> Consolidation -> Summarization. Understanding how the JSON output of one step feeds the prompt of the next is essential for debugging the pipeline.
  - **Quick check question:** If the Consolidation step outputs an empty JSON, how will the final Summarization prompt fail?

## Architecture Onboarding

- **Component map:** Raw Customer Reviews -> Extractor (LLM) -> Consolidator (LLM + Cache) -> Selector (Logic) -> Summarizer (LLM)
- **Critical path:** The **Consolidator** is a critical dependency. If the cache is cold (new product class), the LLM must infer mappings. If this fails or is inconsistent, the "Selector" will count incorrect aspects, and the final summary will focus on the wrong features.
- **Design tradeoffs:**
  - **Latency vs. Freshness:** The system only regenerates summaries when reviews grow by 10%, saving compute but potentially leaving summaries stale for weeks if review volume is low.
  - **Specificity vs. Noise:** The 95th percentile cutoff removes noise but may erase specific "deal-breaker" aspects if they aren't frequent enough.
- **Failure signatures:**
  - **"Hallucinated Summary":** Summary mentions features not in the product. -> *Check:* Did the Selector include irrelevant reviews? Did the Extractor hallucinate an aspect?
  - **"Generic/Bland Summary":** "Customers like this product." -> *Check:* Did the Consolidator over-generalize all aspects to "Quality"? Did the Selector cap reviews too aggressively?
  - **"Stale Summary":** Summary contradicts recent reviews about a defect. -> *Check:* Has the 10% review growth threshold been met?
- **First 3 experiments:**
  1. **Vary the Review Cap:** Test 50 vs. 200 vs. 500 reviews per product. Measure the impact on "Hallucination Rate" (offline eval) vs. Latency.
  2. **Threshold Sensitivity:** Adjust the consolidation threshold (e.g., 90th vs 95th percentile) to see if niche products get better or worse summaries.
  3. **A/B Test "Aspect Counts":** The UI shows "(21)" next to an aspect. Test hiding these counts to see if the mere presence of the summary drives the conversion lift, or if the perceived volume of evidence matters.

## Open Questions the Paper Calls Out
- How does the pipeline's performance degrade for rare product classes with limited review coverage or highly domain-specific terminology? [explicit] Section 6 states that performance "may further decline for rare product classes with limited review coverage or highly domain-specific terminology."
- How can the alignment between automatically extracted aspects and actual user intent be improved? [explicit] Section 6 identifies "better alignment between extracted aspects and user intent" as a specific direction for future work.
- How sensitive is summary quality to the specific frequency threshold (95th percentile) used for aspect consolidation? [inferred] Section 2.2 arbitrarily selects the 95th percentile as the cutoff without ablating it.

## Limitations
- The online A/B test used a 10% significance threshold, which is less stringent than conventional 5% standards
- Manual evaluation of 341 products was conducted by the development team rather than independent annotators
- The system's reliance on aspect frequency (95th percentile threshold) may systematically underrepresent important but rare product attributes

## Confidence
- **High Confidence:** The core pipeline architecture (extraction → consolidation → selection → summarization) is technically sound and well-documented
- **Medium Confidence:** The online A/B test results showing 0.3-0.5% improvements in key metrics are plausible given the system design, but the 10% significance threshold reduces confidence
- **Low Confidence:** The causal mechanism linking aspect-guided summaries to conversion improvements remains speculative without direct evidence

## Next Checks
1. **Threshold Sensitivity Analysis:** Conduct controlled experiments varying the aspect consolidation threshold (90th vs 95th vs 99th percentile) across different product categories to quantify the trade-off between summary specificity and coverage accuracy.
2. **Longitudinal Performance Tracking:** Implement monitoring to measure summary staleness over time by comparing summary-generated aspect sentiments with actual review distributions for products with low review growth rates.
3. **User Behavior Validation:** Design follow-up A/B tests that isolate specific UI elements (e.g., aspect counts in parentheses, sampled review snippets) to determine which components drive the observed conversion improvements.