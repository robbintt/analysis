---
ver: rpa2
title: Multi-Agent Path Finding via Offline RL and LLM Collaboration
arxiv_id: '2509.22130'
source_url: https://arxiv.org/abs/2509.22130
tags:
- agents
- arxiv
- learning
- multi-agent
- environments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Multi-Agent Path Finding (MAPF), a critical
  problem in robotics and logistics characterized by combinatorial complexity and
  partial observability. The authors propose a novel decentralized approach using
  Decision Transformer (DT) in an offline reinforcement learning setup, which significantly
  reduces training time from weeks to hours.
---

# Multi-Agent Path Finding via Offline RL and LLM Collaboration

## Quick Facts
- arXiv ID: 2509.22130
- Source URL: https://arxiv.org/abs/2509.22130
- Reference count: 10
- Key outcome: Offline RL with DT + GPT-4o achieves faster training and better dynamic adaptation in MAPF than learning-based baselines

## Executive Summary
This paper tackles Multi-Agent Path Finding (MAPF), a combinatorial optimization problem where multiple agents must reach their goals without collisions. The authors propose a decentralized approach using Decision Transformer (DT) trained offline to learn agent policies from demonstration data, significantly reducing training time from weeks to hours compared to online methods. To handle dynamic environmental changes, they integrate GPT-4o to guide agent policies during runtime, enabling adaptive behavior in response to moving obstacles.

The proposed method demonstrates superior performance compared to existing learning-based MAPF approaches in both static and dynamic scenarios. DT agents achieve higher success rates and lower sum-of-costs in static environments, while the DT+GPT-4o collaboration improves makespan and collision rates in dynamic environments with moving obstacles. The work highlights how combining offline reinforcement learning with large language model guidance provides an efficient, adaptive solution for complex MAPF challenges.

## Method Summary
The authors propose a decentralized MAPF solution combining offline reinforcement learning with LLM guidance. They employ Decision Transformer (DT) in an offline RL setup, training on demonstration data to learn agent policies that significantly reduce training time from weeks to hours. For dynamic environments with moving obstacles, GPT-4o is integrated to guide agent policies during runtime, enabling adaptive behavior when environmental conditions change. The approach addresses both the computational complexity of MAPF and the challenge of partial observability in dynamic scenarios.

## Key Results
- DT agents outperform existing learning-based MAPF methods in success rates and sum-of-costs in static environments
- DT+GPT-4o collaboration achieves better makespan and collision rates in dynamic environments with moving obstacles
- Training time reduced from weeks to hours through offline RL approach

## Why This Works (Mechanism)
The approach leverages offline RL's efficiency in learning from pre-collected demonstration data, eliminating the need for expensive online interactions during training. Decision Transformer excels at modeling sequential decision-making by predicting actions based on return-to-go and state trajectories. The LLM integration provides high-level reasoning capabilities that can adapt to environmental changes not present in the training data, using natural language reasoning to guide policy adjustments in real-time.

## Foundational Learning
- **Decision Transformer (DT)**: Sequence modeling approach for RL that predicts actions based on returns-to-go and state trajectories; needed because it enables offline training from demonstrations without environment interaction
- **Offline Reinforcement Learning**: Learning from fixed datasets without environment interaction; needed to avoid expensive and potentially unsafe online data collection
- **Large Language Models (LLMs)**: General-purpose models capable of reasoning and adaptation; needed to handle dynamic environmental changes beyond the training distribution
- **Multi-Agent Path Finding (MAPF)**: NP-hard combinatorial optimization problem of finding collision-free paths for multiple agents; needed because it represents a fundamental challenge in robotics and logistics
- **Decentralized Planning**: Each agent makes decisions independently without centralized coordination; needed to scale to large agent populations and avoid single points of failure
- **Partial Observability**: Agents have limited visibility of the environment; needed because real-world MAPF scenarios rarely provide complete environmental information

## Architecture Onboarding

**Component Map:** Data → Preprocessor → DT Trainer → DT Policy → (Static Env) OR (Dynamic Env → GPT-4o → DT Policy)

**Critical Path:** Demonstration data collection → Offline DT training → Policy deployment → (Real-time GPT-4o guidance for dynamic scenarios)

**Design Tradeoffs:** The decentralized approach sacrifices some global optimality for scalability and robustness, while offline training trades exploration potential for training efficiency. LLM integration adds computational overhead and API dependency but provides crucial adaptability.

**Failure Signatures:** Performance degradation occurs when dynamic changes significantly differ from training distribution, when API calls fail or are delayed, or when agent count exceeds scalability limits of the decentralized approach.

**3 First Experiments:**
1. Static MAPF with varying agent counts to validate offline training efficiency and baseline performance
2. Dynamic MAPF with moving obstacles to test LLM-guided adaptation capabilities
3. Ablation study comparing DT performance with and without LLM guidance in dynamic scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Offline RL performance may degrade in scenarios significantly different from training distribution
- GPT-4o integration introduces API dependency and costs limiting practical deployment
- Evaluation focuses primarily on obstacle movement rather than other dynamic elements like moving agents

## Confidence
- High: Training efficiency improvements (weeks to hours), success rate improvements in static environments
- Medium: LLM-guided adaptation claims in dynamic environments
- Low: Scalability assertions beyond tested agent counts

## Next Checks
1. Conduct ablation studies comparing DT performance with and without LLM guidance across multiple dynamic scenarios to quantify the specific contribution of the LLM component
2. Test the approach on MAPF instances with agent counts and map sizes significantly larger than those evaluated to assess true scalability limits
3. Evaluate performance when using smaller, more cost-effective LLMs or local models instead of GPT-4o to assess the trade-off between performance and deployment feasibility