---
ver: rpa2
title: 'Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer'
arxiv_id: '2509.23886'
source_url: https://arxiv.org/abs/2509.23886
tags:
- tokens
- animal
- learning
- subliminal
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates when and how subliminal learning occurs
  in model distillation. It shows that subliminal learning does not require token
  entanglement or logit leakage.
---

# Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer

## Quick Facts
- arXiv ID: 2509.23886
- Source URL: https://arxiv.org/abs/2509.23886
- Authors: Simon Schrodi; Elias Kempf; Fazl Barez; Thomas Brox
- Reference count: 40
- Primary result: Subliminal learning in model distillation is driven by "divergence tokens" - rare cases where teachers with different biases would predict different outputs, not token entanglement or logit leakage.

## Executive Summary
This paper investigates when and how subliminal learning occurs in model distillation, challenging previous explanations that attributed it to token entanglement or logit leakage. Through systematic experiments, the authors demonstrate that the phenomenon is driven by a small set of "divergence tokens" - rare training cases where teachers with different biases would predict different outputs. The study reveals that early transformer layers are critical for this bias transfer, with finetuning even a single early layer being sufficient for subliminal learning. The authors also show that subliminal learning is fragile to input distribution shifts, with simple paraphrasing or multi-teacher data mixing typically suppressing the effect.

## Method Summary
The paper uses supervised finetuning with teacher forcing on 10,000 prompt-completion pairs generated by biased teachers (via system prompts like "You love {ANIMAL}s. You think about {ANIMAL}s all the time."). Student models use rank-8 LoRA adapters on attention and MLP weights across all layers. The key experimental manipulation involves identifying divergence tokens by comparing factual vs. counterfactual teacher predictions at each position, then selectively applying cross-entropy loss to either divergence tokens only, non-divergence tokens only, or all tokens. Single-layer LoRA finetuning experiments isolate which layers causally influence bias transfer, while paraphrase experiments test the robustness of subliminal learning.

## Key Results
- Subliminal learning is driven by a small set of "divergence tokens" (5-20% of tokens), not token entanglement or logit leakage.
- Masking out divergence tokens suppresses hidden bias transfer to baseline levels.
- Early transformer layers (particularly layer 0 or 7) are critical, with single-layer finetuning being sufficient for subliminal learning.
- The phenomenon is fragile - small meaning-preserving prompt paraphrasing or mixing data from multiple teachers typically suppresses it.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subliminal learning is driven primarily by "divergence tokens"—a small subset of training tokens where teachers with different biases would predict different outputs.
- Mechanism: When a student model must correctly predict divergence tokens during supervised finetuning, the most efficient solution is to internalize the teacher's bias. This is because only the correct bias consistently predicts all divergence tokens across the dataset.
- Core assumption: Biases are partially encoded in orthogonal directions in the model's representation space, and gradient updates on divergence tokens preferentially reinforce the factual bias direction.
- Evidence anchors:
  - [abstract] "it comes down to a small set of divergence tokens—rare cases where teachers with different biases would predict different tokens. Masking out these tokens mostly removes the hidden bias transfer."
  - [Section 5.1, Figure 3] Finetuning only on divergence tokens (5.3-20.2% of tokens) preserves or strengthens bias transfer; masking them out suppresses transfer to baseline levels.
  - [corpus] Related work (Zur et al., 2025) proposed token entanglement as an alternative explanation, but this paper refutes that mechanism experimentally.

### Mechanism 2
- Claim: Early transformer layers are causally critical for subliminal bias transfer, with finetuning a single early layer (layer 0 or 7) being sufficient.
- Mechanism: Causal mediation analysis reveals that early layers at the position of the first biased-animal token exhibit high intervention effects. These layers may encode bias-relevant representations that propagate through the network to influence final predictions.
- Core assumption: Early layers implement some form of bias state or context representation that is both modifiable by gradient descent and influential on downstream predictions.
- Evidence anchors:
  - [abstract] "divergence tokens reveal that early layers are critical. Surprisingly, finetuning even a single such early layer is sufficient for subliminal learning."
  - [Section 5.2, Figure 5] Single-layer LoRA finetuning on layers 0 or 7 produces strong bias transfer; layers 14-33 produce negligible transfer.

### Mechanism 3
- Claim: Subliminal learning is fragile to input distribution shifts—meaning-preserving paraphrasing or multi-teacher data mixing typically suppresses bias transfer.
- Mechanism: Divergence tokens are context-specific and tied to particular prompt formulations. Paraphrasing alters the token-level context, causing divergence patterns to no longer align with any single bias. Multi-teacher mixing introduces conflicting divergence signals.
- Core assumption: The student's learning process does not robustly generalize the bias representation across surface form variations during the finetuning window.
- Evidence anchors:
  - [abstract] "subliminal learning is fragile. Even small changes, like paraphrasing prompts, are usually sufficient to suppress it."
  - [Section 6.1, Figure 6] Both unbiased and biased-teacher paraphrasings suppress transmission while preserving task performance.
  - [Section 6.2, Figure 7] Mixing 25% data from a second teacher (even with same bias) substantially reduces transfer.

## Foundational Learning

- Concept: **Supervised Finetuning (SFT) with Teacher Forcing**
  - Why needed here: The paper's mechanism depends on understanding how next-token prediction loss propagates gradient signals during distillation.
  - Quick check question: Can you explain why teacher forcing creates a different gradient signal than sampling-based training?

- Concept: **Causal Mediation Analysis / Attribution Patching**
  - Why needed here: The paper uses these techniques to localize which layers causally influence divergence token predictions.
  - Quick check question: What does the "do-operator" do in causal intervention, and how does attribution patching approximate it?

- Concept: **Token Entanglement and Softmax Bottleneck**
  - Why needed here: The paper explicitly tests and rejects this alternative explanation; understanding it clarifies what subliminal learning is *not*.
  - Quick check question: Why might increasing the probability of "owl" also increase the probability of an apparently unrelated token like "087"?

## Architecture Onboarding

- Component map: Teacher model (biased LLM) -> Data generation (greedy or temperature sampling) -> Divergence token identification -> Masked or unmasked SFT -> Evaluation on bias probe questions

- Critical path:
  1. Bias induction → 2. Data generation (greedy or temperature sampling) → 3. Divergence token identification → 4. Masked or unmasked SFT → 5. Evaluation on bias probe questions

- Design tradeoffs:
  - Greedy sampling eliminates logit leakage but may reduce transfer diversity; temperature sampling better reflects real distillation but introduces confounds
  - Single-layer finetuning isolates mechanism but produces weaker transfer than full LoRA; full LoRA is more realistic but harder to interpret
  - Loss masking on divergence tokens provides clean causal evidence but requires expensive multi-teacher inference

- Failure signatures:
  - Student predicts its own model name (e.g., "qwen") instead of target animal—check system prompt configuration
  - No transfer despite divergence tokens—verify early layers are trainable and student/teacher share base architecture
  - Transfer persists after masking divergence tokens—check for tokenization artifacts or incomplete masking

- First 3 experiments:
  1. **Divergence token ablation**: Replicate Figure 3—compare "div-tokens only" vs. "w/o-div-tokens" loss masking on a single animal bias; expect transmission only in the former.
  2. **Single-layer probing**: Replicate Figure 5—finetune LoRA on layers 0, 7, 14, 21 separately; verify early layers produce transfer.
  3. **Paraphrase robustness**: Generate paraphrased prompts via an unbiased LLM and verify transmission drops to baseline while task performance is preserved.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do certain model architectures exhibit subliminal learning while others (e.g., Llama-3.2-3B-Instruct, Ministral-8B-Instruct) show little to no hidden bias transfer?
- Basis in paper: [explicit] The authors state in the Limitations and Appendix L that "understanding why certain models do and others do not transfer hidden biases remains an open question for future work."
- Why unresolved: The study focused primarily on Qwen and Gemma models. While the mechanism of divergence tokens explains *how* it works in susceptible models, it does not explain why this mechanism fails to manifest or drive learning in other architectures like Falcon or Llama.
- What evidence would resolve it: A comparative analysis of layer-wise representations in resistant vs. susceptible models to see if divergence tokens create distinguishable activation differences in the early layers of the resistant models.

### Open Question 2
- Question: What specific mechanisms enable subliminal learning to transfer across different model families (e.g., from a Gemma teacher to a Qwen student), despite differences in architecture and initialization?
- Basis in paper: [explicit] In the Discussion, the authors note unexpected cross-model transfer (e.g., 'eagle' preference from Gemma to Qwen) and state that "a more thorough investigation of such cross-model transfer is left for future work."
- Why unresolved: The prevailing theory suggests subliminal learning relies on shared initialization, but the observed cross-model transfer implies the phenomenon may exploit universal features or statistical artifacts present across different architectures.
- What evidence would resolve it: Systematic experiments mixing teacher-student pairs across diverse architectures to map the boundaries of cross-model transfer and analyzing if the divergence tokens carry universal semantic or statistical weight.

### Open Question 3
- Question: How can robust prevention methods be developed to suppress subliminal learning without degrading the student model's performance on the finetuning task?
- Basis in paper: [explicit] The authors mention in the Discussion that practitioners may seek prevention methods and that "Development of stronger and more reliable prevention methods is left for future work."
- Why unresolved: While the paper identifies fragility (e.g., prompt paraphrasing), it frames these as observations of the phenomenon's brittleness rather than robust, recommended interventions for deployment.
- What evidence would resolve it: Designing and testing data augmentation or regularization techniques (beyond simple paraphrasing) that specifically target the influence of divergence tokens during training while preserving task-specific loss metrics.

## Limitations

- The divergence token mechanism, while supported by ablation experiments, relies on the assumption that bias-relevant representations are localized in early layers - this causal relationship remains mechanistically underspecified.
- The study focuses on a specific task (number sequence continuation) and bias type (animal preference), raising questions about generalization to other bias types or tasks.
- The 5-20% divergence token proportion is derived from relatively small-scale experiments with one base model per architecture (Qwen2.5-7B, Gemma2-7B).

## Confidence

**High Confidence**: The core finding that divergence tokens drive subliminal learning (Mechanism 1) - strongly supported by loss masking ablation experiments showing complete suppression of transfer when divergence tokens are removed. The fragility to paraphrasing and multi-teacher mixing (Mechanism 3) is also well-established through controlled experiments.

**Medium Confidence**: The causal role of early layers (Mechanism 2) - while the causal mediation analysis provides compelling evidence, the specific claim that "even a single early layer is sufficient" needs further validation across different architectures and bias types. The localization to layers 0 and 7 is specific to the experimental setup and may not generalize.

**Low Confidence**: The assumption that biases are encoded in orthogonal directions in representation space - this is inferred rather than directly measured, and the paper does not provide evidence about the geometry of bias representations.

## Next Checks

1. **Architectural Generalization Test**: Replicate the single-layer LoRA finetuning experiment (Mechanism 2) on a different architecture (e.g., Llama or Mistral) to verify that early layers (specifically layers 0-7) consistently show causal effects across model families, and determine whether the same layer indices or different layers are critical.

2. **Bias Geometry Analysis**: Use representation similarity techniques (e.g., centered kernel alignment or canonical correlation analysis) to directly measure whether factual and counterfactual teacher biases are encoded in orthogonal subspaces, testing the core assumption underlying Mechanism 1.

3. **Task and Bias Transferability**: Design a new experimental condition where the teacher exhibits a different type of hidden bias (e.g., political leaning, temporal preference) while generating data for a completely different task (e.g., code generation or story completion), to test whether the divergence token mechanism generalizes beyond number sequences and animal preferences.