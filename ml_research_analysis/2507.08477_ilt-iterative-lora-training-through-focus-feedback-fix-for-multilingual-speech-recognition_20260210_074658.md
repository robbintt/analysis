---
ver: rpa2
title: ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech
  Recognition
arxiv_id: '2507.08477'
source_url: https://arxiv.org/abs/2507.08477
tags:
- training
- speech
- arxiv
- lora
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ILT (Iterative LoRA Training), a three-stage
  fine-tuning framework to address overfitting in LoRA-based multilingual ASR. The
  method uses Focus, Feedback, and Fix training phases with tailored data mixing,
  dynamic LoRA parameter adjustment, and pseudo-label quality improvement via ensemble
  voting.
---

# ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition

## Quick Facts
- arXiv ID: 2507.08477
- Source URL: https://arxiv.org/abs/2507.08477
- Reference count: 0
- Primary result: Achieved 4th place in Track 1 and 1st in Track 2 of MLC-SLM challenge with WER reductions across 15 languages

## Executive Summary
This paper introduces ILT (Iterative LoRA Training), a three-stage fine-tuning framework for multilingual ASR that addresses overfitting in LoRA-based adaptation. The method employs Focus, Feedback, and Fix training phases with tailored data mixing, dynamic LoRA parameter adjustment, and pseudo-label quality improvement via ensemble voting. Applied to Whisper-large-v3 and Qwen2-Audio, ILT achieved 4th place in Track 1 and 1st in Track 2 of the MLC-SLM challenge, with WER reductions across 15 languages (e.g., English dialects from 9.98% to 9.20% WER). The approach demonstrates significant improvements through iterative, knowledge-task-oriented LoRA adaptation.

## Method Summary
ILT employs a three-stage training pipeline on Whisper-large-v3 and Qwen2-Audio using LoRA adapters with stage-specific configurations. The Focus stage uses low-rank parameters (r=16, α=32) targeting attention projections for initial adaptation without catastrophic forgetting. The Feed Back stage expands to high-rank parameters (r=512, α=2048) and includes all MLP projections, incorporating 40k hours of external data plus Thai/Vietnamese. The Fix stage reduces rank (r=32, α=64) to consolidate learning on curated high-quality data and pseudo labels. Training uses 8×A800 GPUs with micro-batch=16, grad-accum=4, LR=1e-5, and bfloat16 precision. The pipeline integrates knowledge-task design through token frequency analysis, audio synthesis, and similarity-based selection, with ensemble-based pseudo labeling throughout the later stages.

## Key Results
- Achieved 4th place in Track 1 and 1st in Track 2 of MLC-SLM challenge
- Reduced WER across 15 languages (English dialects from 9.98% to 9.20% WER)
- Ablation studies confirm full three-stage ILT outperforms individual stages
- Demonstrated effectiveness on both Whisper-large-v3 and Qwen2-Audio base models

## Why This Works (Mechanism)

### Mechanism 1: Iterative LoRA Rank Expansion for Progressive Knowledge Assimilation
The framework dynamically increases LoRA rank across stages to align parameter capacity with learning phase needs. Starting with r=16 for rapid adaptation without forgetting, expanding to r=512 for acquiring new languages, then reducing to r=32 for refinement on high-quality data. This progression prevents overfitting by matching model capacity to data complexity and learning objectives at each stage.

### Mechanism 2: Knowledge Task Design via Targeted Augmentation
A data augmentation pipeline targets low-frequency tokens and difficult high-frequency tokens through token frequency analysis, third-party text selection, audio synthesis (F5-TTS/Spark-TTS), and similarity-based audio selection using WavLM embeddings and SECS >0.85. This approach improves representation of rare vocabulary and enhances robustness in frequent contexts by focusing on specific linguistic challenges.

### Mechanism 3: Ensemble-Based Pseudo Label Refinement
Majority voting across multiple model checkpoints generates higher-quality pseudo labels for unlabeled data, reducing labeling noise throughout Feed Back and Fix stages. This iterative refinement provides more reliable supervision as the model evolves, improving the quality of training data and enabling better adaptation to challenging languages.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed: Core to ILT; enables parameter-efficient fine-tuning by freezing base weights and learning low-rank matrices (A, B) with scaling factor α/r
  - Quick check: Can you explain how LoRA merges learned updates with base model weights and what α/r controls?

- **Concept: Pseudo Labeling & Semi-Supervised Learning**
  - Why needed: ILT uses iterative pseudo labeling to extend supervision to unlabeled data; understanding noise handling and iterative refinement is critical
  - Quick check: What are two common failure modes of pseudo labeling, and how does ensemble voting mitigate them?

- **Concept: Tokenization & Frequency Analysis**
  - Why needed: Knowledge task design relies on tokenizer-based frequency thresholds to identify low/high-frequency tokens for targeted augmentation
  - Quick check: How would you determine appropriate frequency thresholds for a new dataset using a given tokenizer?

## Architecture Onboarding

- **Component map**: Base Models (Whisper-large-v3, Qwen2-Audio) -> LoRA Adapters (A,B matrices with configurable rank/alpha) -> Training Pipeline (Focus→Feed Back→Fix) -> Data Augmentation (frequency analysis, synthesis, similarity selection) -> Pseudo Labeling (ensemble voting) -> External Data (8 public datasets)

- **Critical path**: 1) Setup base model with LoRA adapters 2) Focus stage (r=16, α=32, attention projections, 6 epochs) 3) Merge → Feed Back (r=512, α=2048, expanded modules, 10 epochs, external data) 4) Merge → Fix (r=32, α=64, curated data + pseudo labels, 6 epochs) 5) Ensemble pseudo labeling throughout later stages

- **Design tradeoffs**: Rank size vs. overfitting (low rank reduces overfitting but limits new knowledge; high rank enables learning but risks overfitting); Data expansion vs. domain mismatch (adding external data improves coverage but may introduce domain shift); Pseudo label quality vs. quantity (ensemble voting improves quality but requires maintaining multiple models)

- **Failure signatures**: Validation WER increases after Feed Back stage (likely domain mismatch or overfitting); High disagreement in pseudo labels (ensemble models too similar or data too noisy); No improvement in low-frequency token WER (thresholds may not match dataset distribution)

- **First 3 experiments**: 1) Replicate Focus stage on official data with r=16, α=32; validate adaptation without catastrophic forgetting 2) Add Feed Back stage with r=512 and external data for one target language; measure WER change vs. Focus-only 3) Implement ensemble pseudo labeling with 3 checkpoints; generate pseudo labels for unlabeled subset and train Fix stage; compare WER to Fix stage without pseudo labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the ILT framework support continued performance gains beyond three iterations, or does it suffer from diminishing returns or catastrophic forgetting?
- Basis: The authors state they employed "only three iterations... considering the time constraints of the competition."
- Why unresolved: Unclear if the Focus-Feedback-Fix cycle can be repeated indefinitely or if three stages represent a theoretical ceiling
- Evidence needed: Experiments extending training to n > 3 iterations to observe loss and WER trajectory

### Open Question 2
- Question: Is the drastic increase in LoRA rank during Feed Back Training (r=512) necessary, or can parameter count be lower?
- Basis: Jumps from r=16 to r=512 then drops to r=32 without theoretical justification
- Why unresolved: No ablation study on LoRA rank magnitudes; unclear if "Feedback" stage relies on high rank or simply data expansion
- Evidence needed: Ablation studies maintaining consistent rank (e.g., r=64) across all stages or testing intermediate values

### Open Question 3
- Question: Is the rigid Focus→Feed Back→Fix sequence optimal for all base models, or should sequence adapt based on pre-trained model's existing knowledge?
- Basis: Ablation reveals Feed Back improves Qwen2-Audio but degrades Whisper-large-v3 when applied in isolation
- Why unresolved: Fixed pipeline proposed, but results suggest different models might benefit from skipping stages or altering order
- Evidence needed: Dynamic scheduling experiments where stages are triggered based on validation metrics

## Limitations
- Relies heavily on proprietary or competition data (MLC-SLM) not fully reproducible from paper
- Precise pseudo-labeling implementation (ensemble size, checkpoint selection) and token frequency thresholds not fully specified
- Generalizability to non-MLC datasets uncertain; SECS-based audio selection robustness to domain shifts unclear
- Confidence in mechanism-level explanations limited by absence of intermediate validation metrics or qualitative analyses

## Confidence
- Iterative LoRA rank expansion for progressive knowledge assimilation: **Medium**
- Knowledge task design via targeted augmentation: **Low**
- Ensemble-based pseudo label refinement: **Medium**

## Next Checks
1. Replicate the three-stage ILT pipeline on a publicly available multilingual ASR dataset (e.g., Common Voice + MLS) and measure per-stage WER changes to validate the iterative design
2. Perform a controlled ablation where the pseudo-labeling step is removed from the Fix stage; compare WER gains to quantify its contribution
3. Test the frequency-based augmentation thresholds on a held-out validation set by varying the low/high-frequency cutoffs and measuring their impact on rare word recognition accuracy