---
ver: rpa2
title: Collaborative Problem-Solving in an Optimization Game
arxiv_id: '2505.15490'
source_url: https://arxiv.org/abs/2505.15490
tags:
- coins
- agent
- room
- path
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present TRAVELING ADVENTURERS, a novel two-player collaborative
  game environment for collecting task-oriented dialogue, based on a two-player TSP.
  The agents solve the problem collaboratively, with each player having partial information
  about the graph, necessitating negotiation and collaboration.
---

# Collaborative Problem-Solving in an Optimization Game

## Quick Facts
- arXiv ID: 2505.15490
- Source URL: https://arxiv.org/abs/2505.15490
- Reference count: 12
- Primary result: Neurosymbolic agents combining LLM dialogue with symbolic TSP optimization outperform pure LLM agents in collaborative TSP-based game

## Executive Summary
This paper introduces TRAVELING ADVENTURERS, a novel two-player collaborative game environment based on a two-player Traveling Salesman Problem (TSP). In this game, each player has partial information about graph edge weights and must negotiate to find an optimal tour visiting all nodes. The authors propose a neurosymbolic agent architecture that combines LLM dialogue generation with an external integer linear programming solver for optimization. The approach significantly outperforms pure LLM baselines in self-play and demonstrates better collaborative success with human partners, though performance remains lower than in self-play scenarios.

## Method Summary
The TRAVELING ADVENTURERS game presents two players with a 6-node fully connected graph where each player sees private edge weights (coins). Players must negotiate to find a Hamiltonian cycle maximizing the sum of both players' weights. The neurosymbolic agent uses GPT-4o with four variants: Baseline (pure LLM reasoning), Grounding (adds Partner WSR tracking), State-Tracking (adds Visited/Remaining structures), and Problem-Solving (adds external TSP solver). The action space includes suggest, agree, reject, ask, inform, and end actions, with the suggest action being the primary mechanism for incremental path negotiation.

## Key Results
- Self-play: Problem-Solving agent achieves 98% correct paths and 45% optimal solutions vs Baseline's 71% correct and 28% optimal
- Human-agent games: Problem-Solving agent outperforms Baseline in fostering successful cooperation through dialogue
- The node-by-node incremental negotiation strategy emerges naturally from the action space design
- Combining LLM dialogue with symbolic optimization modules significantly improves performance over pure LLM reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing collaborative problem-solving into separate modules for dialogue generation and symbolic optimization improves both correctness and optimality compared to end-to-end LLM reasoning
- Mechanism: The LLM is restricted to generating dialogue acts (suggest, agree, reject, ask, inform) while an external integer linear programming solver computes optimal paths given available information
- Core assumption: The action space (negotiation dialogue acts) can be formally specified and the optimization problem has a tractable exact solver
- Evidence anchors: [abstract] "combining an LLM with neurosymbolic modules improves the performance compared to a baseline in self-play"; [section 5.2] Problem-Solving agent achieves 98% correct, 45% optimal vs Baseline's 71% correct, 28% optimal

### Mechanism 2
- Claim: Externalized state tracking (Visited/Remaining structures) improves solution completeness by reducing context-window reliance
- Mechanism: Instead of storing full action history in context, symbolic structures track only the agreed-upon path prefix and remaining nodes
- Core assumption: Agreement states can be unambiguously parsed from dialogue and mapped to discrete node updates
- Evidence anchors: [section 4.4] State-tracking agent achieves 86% correctness (vs 71% baseline) though optimality drops to 17%; [section 4.2] Baseline "often forgets the information its partner (or it) has already shared"

### Mechanism 3
- Claim: Node-by-node incremental negotiation strategy emerges naturally from the action space design and improves collaborative convergence
- Mechanism: The suggest action takes path arguments; agents overwhelmingly prefer 2-argument suggestions (single edge) over full paths (7 arguments)
- Core assumption: Greedy local optimization approximates global optimum sufficiently for the task difficulty level
- Evidence anchors: [section 5.3.1] "suggest action typically occurs once per conversation turn, implying that the node-by-node approach is the favored strategy"; [table 2] Problem-solving agent uses node-by-node in 91.6% of human-agent games

## Foundational Learning

- Concept: **Traveling Salesman Problem (TSP)**
  - Why needed here: The game's underlying optimization structure; understanding that finding optimal tours is NP-hard explains why humans need collaborative dialogue rather than immediate solutions
  - Quick check question: Given a 4-node fully connected graph, can you manually enumerate all possible tours and identify the optimal one?

- Concept: **Conversational Grounding (Clark 1996)**
  - Why needed here: The core dialogue challenge—agents must track what information has been mutually established vs. what remains uncertain
  - Quick check question: In a dialogue where Player A says "I can get 6 coins to the kitchen," what has been grounded and what remains unknown to Player B?

- Concept: **Neurosymbolic Architecture**
  - Why needed here: The design pattern of coupling neural language models with symbolic reasoners; understanding when to delegate computation vs. generation is critical for extending this work
  - Quick check question: For a new collaborative task (e.g., scheduling), which components would you implement symbolically vs. neurally?

## Architecture Onboarding

- Component map: User Message → LLM (ReACT reasoning) → Action Parser → Symbolic State Updates → Partner WSR (grounding) ← LLM extraction module → IBP Solver ← Joint graph (Agent WSR + Partner WSR) → Action Generation → Message to User

- Critical path:
  1. Message parsing to extract new partner information (weight disclosures, agreements)
  2. Partner WSR update with extracted information
  3. IBP recomputation with updated joint graph
  4. Action generation based on IBP suggestion + negotiation state

- Design tradeoffs:
  - Action History vs. State Structures: Full history preserves context but grows unbounded; state structures are compact but lose nuance
  - Exact solver vs. LLM reasoning: Solver guarantees optimality given perfect information but fails on unknown weights; LLM handles uncertainty but makes arithmetic errors
  - Node-by-node vs. full-path: Incremental strategy matches human behavior but may miss global optima

- Failure signatures:
  - Partner WSR extraction errors on pronominal references ("from there")
  - Premature agreement (LLM tendency to appease rather than negotiate)
  - Backtracking requests from humans that cannot update symbolic state
  - Timeout from missing submit action

- First 3 experiments:
  1. Ablate IBP solver: Replace with LLM-only path suggestion to quantify solver contribution
  2. Scale graph size: Test on 8-node and 10-node graphs to assess generalization
  3. Add backtracking support: Allow Visited structure updates on explicit revision requests

## Open Questions the Paper Calls Out
None

## Limitations
- Human-agent performance may be influenced by human unfamiliarity with task mechanics rather than agent limitations
- The study focuses on a specific TSP-based game structure; generalization to other collaborative tasks requires validation
- The action space design may artificially constrain or enable certain negotiation strategies

## Confidence
- **High confidence**: Neurosymbolic architecture demonstrably outperforms baseline in self-play (98% correct vs 71%, 45% optimal vs 28%)
- **Medium confidence**: Human-agent performance improvements are valid but require context about human learning curves
- **Medium confidence**: The claim that pure LLM reasoning fails on collaborative optimization tasks is supported but based on a single game environment

## Next Checks
1. **Generalization test**: Implement the neurosymbolic agent on a different collaborative optimization task (e.g., resource allocation scheduling) to assess architecture portability beyond TSP-based games

2. **Human task familiarization**: Conduct a pre-study where human participants practice the game mechanics before evaluation to separate learning effects from agent performance differences

3. **State structure robustness**: Test the Visited/Remaining structures with explicit backtracking scenarios to quantify recovery capability from suboptimal collaborative decisions