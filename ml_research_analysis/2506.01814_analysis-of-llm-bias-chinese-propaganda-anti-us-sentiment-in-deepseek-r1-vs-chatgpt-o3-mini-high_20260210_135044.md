---
ver: rpa2
title: Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1
  vs. ChatGPT o3-mini-high
arxiv_id: '2506.01814'
source_url: https://arxiv.org/abs/2506.01814
tags:
- chinese
- bias
- propaganda
- o3-mini-high
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study directly compared a PRC-aligned LLM (DeepSeek-R1) with
  a non-PRC model (ChatGPT o3-mini-high) across Simplified Chinese, Traditional Chinese,
  and English. A novel corpus of 1,200 reasoning questions was automatically generated
  from news summaries and used to probe both models for Chinese-state propaganda and
  anti-US sentiment.
---

# Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high

## Quick Facts
- **arXiv ID**: 2506.01814
- **Source URL**: https://arxiv.org/abs/2506.01814
- **Reference count**: 40
- **Primary result**: DeepSeek-R1 shows significantly higher rates of Chinese-state propaganda and anti-US sentiment than ChatGPT o3-mini-high, with bias strongest in Simplified Chinese and nearly absent in English.

## Executive Summary
This study directly compared a PRC-aligned LLM (DeepSeek-R1) with a non-PRC model (ChatGPT o3-mini-high) across Simplified Chinese, Traditional Chinese, and English. Using a novel corpus of 1,200 reasoning questions automatically generated from news summaries, the researchers probed both models for Chinese-state propaganda and anti-US sentiment. Responses were evaluated by GPT-4o with human annotation, achieving near-perfect agreement (Cohen's κ ≈ 0.8–0.9). DeepSeek-R1 showed significantly higher rates of both propaganda and anti-US bias than o3-mini-high, with bias peaking in Simplified Chinese and nearly vanishing in English. The bias persisted even in cultural and lifestyle topics, not just overtly political ones.

## Method Summary
The study generated 1,200 de-contextualized reasoning questions from Chinese news summaries using o3-mini. These questions were translated into Simplified Chinese (zh-CN), Traditional Chinese (zh-TW), and English (EN). DeepSeek-R1 and ChatGPT o3-mini-high were queried zero-shot with all questions in all three languages, producing 7,200 responses total. A hybrid evaluation pipeline used GPT-4o as a first-pass judge with specific 5-dimension rubrics for propaganda detection and 1-dimension for anti-US sentiment, validated against human annotators.

## Key Results
- DeepSeek-R1 exhibited significantly higher rates of both propaganda and anti-US bias than o3-mini-high across all languages
- Bias peaked in Simplified Chinese and nearly vanished in English for both models
- DeepSeek-R1 sometimes responded in Simplified Chinese to Traditional Chinese prompts, demonstrating an "invisible loudspeaker" effect
- No anti-US bias was detected in any language for o3-mini-high

## Why This Works (Mechanism)

### Mechanism 1: Language-Script as a Contextual Trigger for Alignment
If an LLM is trained primarily on data associated with a specific geopolitical regime, the choice of input language (specifically script variants like Simplified vs. Traditional Chinese) appears to function as a conditional trigger for that regime's rhetorical patterns. The model likely maps input scripts to distinct parameter spaces or training corpus subsets. When prompted in Simplified Chinese (zh-CN), DeepSeek-R1 accesses associations heavily weighted toward PRC-state discourse, increasing the probability of generating aligned propaganda. Conversely, English prompts map to a more global/neutral corpus, dampening the bias. The model's internal representations do not strictly separate linguistic capability from ideological context; the "script" acts as a latent variable for "expected political stance."

### Mechanism 2: The "Invisible Loudspeaker" via Terminology Injection
Propaganda bias manifests not just through explicit opinion, but through the unprompted injection of specific, loaded terminology (e.g., "Core Interests," "Win-Win Cooperation") that frames the narrative implicitly. During generation, the model prioritizes high-probability tokens associated with its dominant training distribution. For DeepSeek-R1, this includes official state vocabulary. When a query touches on relevant topics, the model "amplifies" the response by retrieving these specific terms, acting as a loudspeaker for state discourse even when the user did not request it. The inclusion of specific political terminology correlates with a deliberate or trained alignment strategy, rather than random noise.

### Mechanism 3: Script Normalization as Ideological Carry-over
A model may inadvertently enforce ideological alignment by enforcing script standardization (e.g., converting Traditional Chinese inputs to Simplified Chinese outputs). When DeepSeek-R1 responds to a Traditional Chinese (zh-TW) prompt in Simplified Chinese (zh-CN), it effectively "translates" the context into its dominant training regime. This translation carries semantic baggage—PRC-aligned terms and framing inherent to the Simplified script corpus—into the response, overriding the user's linguistic preference. The "script switching" is a result of training data dominance (Simplified > Traditional) and is inextricably linked to the semantic frames of the dominant data.

## Foundational Learning

- **Concept: De-contextualized Probing**
  - Why needed here: Standard benchmarks often test for *knowledge* (e.g., "What happened in Tiananmen?"). This study uses de-contextualized reasoning questions (abstracted from news) to test *latent framing* and *reasoning bias* independent of specific banned keywords. Without understanding this, one might confuse "refusal to answer" with "propaganda bias."
  - Quick check question: If I ask "How should countries handle surveillance?" rather than "How does China handle surveillance?", am I testing the model's encyclopedic knowledge or its default framing preference?

- **Concept: Hybrid Evaluation Pipelines (LLM-as-a-Judge)**
  - Why needed here: Evaluating "propaganda" is subjective. The study uses GPT-4o as a scalable first-pass judge but validates it with human annotators to establish reliability (Cohen's κ ≈ 0.8–0.9). An engineer must understand that LLM judges are fallible and require calibration against a human "gold standard."
  - Quick check question: If GPT-4o rates a text as "highly biased" but human annotators disagree, which signal should the system trust for final reporting?

- **Concept: Geopolitical Alignment vs. Neutrality**
  - Why needed here: The study frames DeepSeek-R1 as "PRC-aligned" and o3-mini-high as "non-PRC." Understanding that "non-PRC" does not mean "objective truth" but rather "different alignment axis" is crucial for interpreting the results. The paper tests specifically for *Chinese-state propaganda*, not general ideological neutrality.
  - Quick check question: Does the absence of anti-US sentiment in o3-mini-high prove it has no bias, or simply that it lacks this *specific* bias?

## Architecture Onboarding

- **Component map**: Corpus Generator (news → questions) -> Target System (DeepSeek-R1/o3-mini-high) -> Judge System (GPT-4o) -> Validator Layer (human annotators)
- **Critical path**: The **Question Generation** phase. If the de-contextualization fails (e.g., proper nouns leak), the test becomes a retrieval task. If the **Rubric** in the Judge System is poorly defined, GPT-4o will generate false positives (flagging factual mentions of China as propaganda).
- **Design tradeoffs**:
  - **De-contextualization**: Removes specific triggers (pro: isolates reasoning bias; con: questions may become too vague to elicit strong responses)
  - **Automated Judging**: Scales to 7,200 responses (pro: high volume; con: LLM judges have their own biases, e.g., verbosity bias or positional bias)
  - **Script Fidelity**: Allowing the model to answer in its preferred script (pro: natural flow; con: introduces "script switching" bias as seen in the paper)
- **Failure signatures**:
  - **Script Drift**: The model answers a Traditional Chinese prompt in Simplified Chinese
  - **Keyword Stuffing**: The model inserts terms like "Community of Shared Future" into unrelated lifestyle queries
  - **Rubric Overfitting**: The Judge flags any mention of "China" as propaganda, requiring recalibration of the "Score 1 vs Score 3" thresholds
- **First 3 experiments**:
  1. **Rubric Calibration**: Run 100 responses through GPT-4o and compare against 2 human annotators. Target Cohen's κ > 0.8. If lower, refine the "Propaganda Prompt" definitions in Appendix Table 9.
  2. **Cross-Script Consistency**: Submit the exact same semantic query in EN, zh-CN, and zh-TW. Measure if the "Propaganda" score diverges significantly across scripts for the same model.
  3. **Term Injection Audit**: Diff the model's output against the prompt. specifically looking for the addition of political nouns (e.g., "Belt and Road") that did not exist in the input, to quantify the "invisible loudspeaker" effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does DeepSeek-R1's tendency to respond in Simplified Chinese to Traditional Chinese prompts ("code-switching") inflate the quantitative measurement of propaganda and anti-US bias?
- Basis in paper: The Conclusion notes this behavior "may have overstated the presence of propaganda and anti-US sentiment" and calls for "more precise script detection."
- Why unresolved: The current study treated the responses as-is without filtering or separately analyzing responses that switched scripts, making it difficult to isolate whether the bias stems from the content or the script choice.
- What evidence would resolve it: A follow-up analysis that detects script switching in the output and calculates bias rates separately for responses that maintained Traditional Chinese versus those that switched to Simplified Chinese.

### Open Question 2
- Question: How do the internal chain-of-thought (CoT) processes in DeepSeek-R1 contribute to the generation of PRC-aligned terms and positive examples?
- Basis in paper: The authors observe that the model's reasoning "sometimes appears to incorporate prompts favoring positive Chinese examples" and explicitly suggest a "need to examine its chain-of-thought processes."
- Why unresolved: The study evaluated only the final model outputs, leaving the internal reasoning steps that trigger the "invisible loudspeaker" effect unexplored.
- What evidence would resolve it: An analysis of DeepSeek-R1's hidden reasoning tokens or attention maps to identify specific alignment or training mechanisms that introduce PRC-centric examples during generation.

### Open Question 3
- Question: Do the observed propaganda and anti-US bias patterns generalize to other LLMs outside the specific DeepSeek-R1 and ChatGPT o3-mini-high comparison?
- Basis in paper: The Conclusion urges future research to "broaden its scope to include a wider range of large language models (from both PRC and non-PRC contexts)."
- Why unresolved: The findings are based on a single PRC-aligned and one non-PRC model; it is unknown if this is a universal trait of PRC-aligned models or specific to DeepSeek's architecture.
- What evidence would resolve it: A benchmark evaluation using the 1,200-question corpus on other models (e.g., ERNIE, GLM, Llama, Gemini) to compare cross-linguistic bias rates.

## Limitations

- **Script-switching artifact risk**: DeepSeek-R1's tendency to respond in Simplified Chinese to Traditional Chinese prompts could inflate propaganda detection rates if evaluators associate Simplified Chinese with PRC-state discourse by default. The paper acknowledges this but does not fully quantify its impact on headline bias statistics.
- **Threshold ambiguity**: The Anti-US bias labeling rule contradicts itself between the main text ("≥1") and Appendix ("≥2"), creating uncertainty about which cutoff was actually used.
- **Evaluator subjectivity**: Despite high inter-annotator agreement (κ ≈ 0.8-0.9), propaganda detection remains inherently subjective. The GPT-4o judge's "slightly present" threshold may inconsistently flag borderline cases, particularly for emotional mobilization or dissent detection.

## Confidence

- **High confidence**: DeepSeek-R1 exhibits higher propaganda/anti-US bias than o3-mini-high across all languages tested.
- **Medium confidence**: Bias correlates strongly with script choice (Simplified > Traditional > English) for DeepSeek-R1 specifically.
- **Medium confidence**: Anti-US bias is undetectable in o3-mini-high across all languages.

## Next Checks

1. **Script-fidelity audit**: Implement automated output language detection to quantify how often DeepSeek-R1 script-switches from Traditional to Simplified Chinese, and re-run propaganda scoring only on script-compliant responses.
2. **Threshold ablation study**: Systematically test Anti-US labeling at thresholds 0, 1, and 2 to determine which produces the most consistent results with human validation.
3. **Evaluator calibration stress test**: Run 100 responses through GPT-4o at temperatures 0.0, 0.01, and 0.1 to measure sensitivity of "slightly present" ratings to generation variance.