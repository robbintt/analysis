---
ver: rpa2
title: Optimization with Access to Auxiliary Information
arxiv_id: '2206.00395'
source_url: https://arxiv.org/abs/2206.00395
tags:
- learning
- auxmom
- have
- case
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of optimizing a target function\
  \ whose gradients are expensive or limited, using auxiliary functions with cheaper\
  \ or more available gradients. The authors propose two algorithms\u2014AuxMOM and\
  \ AuxMVR\u2014that reuse gradients from both the target and auxiliary functions\
  \ to accelerate convergence."
---

# Optimization with Access to Auxiliary Information

## Quick Facts
- arXiv ID: 2206.00395
- Source URL: https://arxiv.org/abs/2206.00395
- Reference count: 40
- Primary result: Proposes AuxMOM and AuxMVR algorithms that reuse gradients from target and auxiliary functions to accelerate convergence under Hessian similarity, improving upon known optimal rates for non-convex optimization.

## Executive Summary
This paper addresses the problem of optimizing a target function whose gradients are expensive or limited, using auxiliary functions with cheaper or more available gradients. The authors propose two algorithms—AuxMOM and AuxMVR—that reuse gradients from both the target and auxiliary functions to accelerate convergence. Under the Hessian similarity assumption between target and auxiliary functions, the methods improve upon known optimal rates for non-convex optimization. AuxMOM replaces Lσ² with δσ² in convergence bounds, potentially leading to significant speedups when δ ≪ L. AuxMVR achieves a rate of O((δF₀σ²/T)²/³), matching optimal non-convex MVR rates while benefiting from auxiliary information. The framework applies broadly to scenarios like federated learning, transfer learning, and semi-supervised learning, with experiments on MNIST and CIFAR datasets demonstrating improved accuracy and faster convergence.

## Method Summary
The paper proposes two algorithms that optimize a target function f using gradients from an auxiliary function h. AuxMOM maintains a momentum term m_f-h ≈ ∇f(x) - ∇h(x) using classical momentum, while AuxMVR uses momentum-based variance reduction. Both algorithms perform K local updates on h using a biased gradient estimator d_k = g_h(y) + m_f-h, which approximates ∇f(y) when y is close to x. The momentum term is updated using ∇f(x) - ∇h(x) computed at the snapshot state x. The key insight is that when f and h have similar curvature (Hessian similarity ∥∇²f - ∇²h∥₂ ≤ δ), the biased estimator has bounded error O(∥y-x∥²), enabling faster convergence than standard SGD. The methods assume access to unbiased stochastic gradients with bounded variance for both f and h, and allow for correlated noise between f and h gradients to further reduce variance.

## Key Results
- AuxMOM replaces Lσ² with δσ² in convergence bounds, achieving O((δF₀σ²_f-h/T)²/³) rate
- AuxMVR achieves O((δF₀σ²/T)²/³) rate matching optimal non-convex MVR while benefiting from auxiliary information
- Experiments on MNIST show AuxMOM maintains accuracy across rotation angles while naive approach degrades
- Coreset training experiments show AuxMOM matches full-dataset accuracy while coreset-alone loses ~10% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constructing a biased gradient estimator using the auxiliary function's gradients can accelerate convergence when the target and auxiliary functions share curvature structure.
- Mechanism: The algorithm computes updates using a control-variates-style estimator: g = ∇h(y) - ∇h(x) + ∇f(x), where x is a "snapshot" state and y is the current local state. This estimator remains accurate when y is close to x and when the Hessian similarity is small. The paper states this is "a generalization of the control variate idea used in SVRG."
- Core assumption: Hessian similarity: ∥∇²f(x) - ∇²h(x)∥₂ ≤ δ for some δ ∈ [0, 2L]. Benefit emerges when δ ≪ L.
- Evidence anchors:
  - [abstract] "we also prove that we can benefit from this framework under the Hessian similarity assumption between the target and side information"
  - [section 2] Eq. (3) and discussion: "We will refer to (3) as a local step because it uses a new gradient of h to update the state"
  - [corpus] Weak corpus support; neighbor papers touch on auxiliary information but do not analyze Hessian-similarity conditions for convergence rates.
- Break condition: If δ ≈ 2L (unrelated functions), the error term 2δ²∥y - x∥² can dominate, and convergence degrades to standard SGD rates with no benefit.

### Mechanism 2
- Claim: Momentum can stabilize the estimate of ∇f(x) - ∇h(x) when only stochastic gradients are available, enabling variance reduction without full-gradient access.
- Mechanism: AuxMOM maintains a momentum term m_f-h ≈ ∇f(x) - ∇h(x) using classical momentum (Eq. 5), while AuxMVR uses momentum-based variance reduction (Eq. 6). The biased gradient estimator becomes d_k = g_h(y) + m_f-h, which approximates ∇f(y) as long as y is near x and m_f-h is accurate.
- Core assumption: Unbiased stochastic gradients with bounded variance: E[∥g_J - ∇J∥²] ≤ σ²_J for J ∈ {f, h, f-h}. The momentum parameter a must satisfy a ≥ 36δKη for convergence.
- Evidence anchors:
  - [abstract] "The authors propose two algorithms—AuxMOM and AuxMVR—that reuse gradients from both the target and auxiliary functions to accelerate convergence"
  - [section 4.2] Theorem 4.3 gives the convergence rate; Corollary 4.4 shows Lσ²_f replaced by δσ²_f-h in the dominant term
  - [corpus] Corpus papers do not provide comparative momentum-based variance reduction analysis for auxiliary-informed optimization.
- Break condition: If the momentum initialization error E₀ is large relative to σ²_f-h/T, the first term in the bound can dominate, slowing early convergence.

### Mechanism 3
- Claim: Correlated noise between f and h stochastic gradients can further reduce the effective variance σ²_f-h below σ²_f + σ²_h.
- Mechanism: The variance assumption is stated for g_f-h directly. If sampling procedures make g_f and g_h positively correlated, then Var[g_f - g_h] = Var[g_f] + Var[g_h] - 2Cov[g_f, g_h] can be smaller than σ²_f + σ²_h.
- Core assumption: Access to a stochastic gradient estimator g_f-h with variance σ²_f-h that may be reduced through correlation.
- Evidence anchors:
  - [section 3, Assumption 3.2 discussion] "if the batches ξ_f and ξ_h are drawn such that g_f and g_h are positively correlated, then it is even possible to have σ²_f-h < σ²_f"
  - [section 7, Discussion] "We showed in this work that we might benefit if we could sample gradients of h that are positively correlated with gradients of f, but we did not mention how this can be done"
  - [corpus] No corpus papers address correlated-noise design for auxiliary optimization.
- Break condition: If sampling is independent or negatively correlated, σ²_f-h ≈ σ²_f + σ²_h or larger, reducing potential gains.

## Foundational Learning

- Concept: Control variates (e.g., SVRG, SARAH)
  - Why needed here: The core trick (Eq. 3) extends control variates from finite-sum settings to generic auxiliary functions. Understanding why g = ∇h(y) - ∇h(x) + ∇f(x) has error O(∥y - x∥²) requires familiarity with variance reduction principles.
  - Quick check question: Given a function f and a snapshot point x, what is the bias and variance of the estimator g_SVRG = ∇f_i(y) - ∇f_i(x) + ∇f(x)?

- Concept: Momentum-based variance reduction (MVR/STORM)
  - Why needed here: AuxMVR builds on Cutkosky & Orabona (2019)'s MVR technique. The algorithm adds a correction term (1-a)(g_f-h(x_{t-1}) - g_f-h(x_{t-2})) to the momentum update.
  - Quick check question: How does MVR achieve O(T^{-2/3}) convergence for non-convex optimization, and why does it require stronger smoothness assumptions than standard momentum?

- Concept: Federated learning and client drift
  - Why needed here: The paper explicitly connects to federated learning, noting that the naive approach corresponds to FedAvg and suffers from client drift/heterogeneity. Understanding why local steps without correction fail helps motivate the bias-correction mechanism.
  - Quick check question: In FedAvg with K local steps, why does heterogeneity (differing local objectives) cause the global model to converge to a suboptimal point?

## Architecture Onboarding

- Component map: x_t (snapshot state) -> y_k (local state) -> m_f-h (momentum) -> g_f-h (gradient difference estimator)
- Critical path: At each round t: (1) sample g_f-h and update momentum → (2) for k=1..K, sample g_h and compute d_k = g_h + m_f-h → (3) update y_k = y_{k-1} - η d_k → (4) set x_t = y_K
- Design tradeoffs:
  - K (local steps): Larger K reduces communication/computation of f gradients but requires smaller η ∝ 1/K to control drift
  - a (momentum parameter): Must satisfy a ≥ 36δKη; larger a increases variance, smaller a slows bias correction
  - Batch size for m_0 initialization: Using T× larger batch ensures E₀ ≤ σ²_f-h/T but costs one extra epoch
- Failure signatures:
  - Convergence to wrong point with non-zero gradient norm: Likely δ assumption violated or gradient bias not corrected (naive approach)
  - Divergence in early iterations: Momentum initialization error E₀ too large; try larger initial batch
  - No speedup over SGD: Either δ ≈ L (unrelated auxiliary) or σ²_f-h ≈ σ²_f + σ²_h (uncorrelated noise)
- First 3 experiments:
  1. Toy quadratic: Minimize f(x) = x²/2 using h(x) = (1+δ)(x - ζ/(1+δ))²/2. Vary δ and ζ to verify that AuxMOM converges regardless of bias ζ, while naive approach fails for large ζ. Use K=10, η=min(1/2, 1/(δK)).
  2. Rotated MNIST: Train a feedforward network on MNIST as f; use rotated images (0°, 45°, 90°, 180°) as h. Verify that AuxMOM maintains accuracy across rotation angles while naive degrades. Use K=10, batch sizes 256 (f) and 64 (h).
  3. Coreset training: Sample 20% of CIFAR-10 as a coreset defining h. Compare: (a) SGD on coreset alone, (b) AuxMOM using coreset as helper, (c) SGD on full dataset. Verify AuxMOM matches full-dataset accuracy while coreset-alone loses ~10% accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can similarity measures more practical than Hessian similarity be devised to broaden the applicability of auxiliary optimization to non-smooth or deep learning contexts?
- Basis: [explicit] Section 7 identifies quantifying similarity as an open problem and notes the authors "do not pretend to solve" the issue of finding appropriate measures.
- Why unresolved: The current theoretical guarantees rely on Assumption 3.3 (Hessian similarity), which is difficult to verify and may not hold strictly for modern deep learning architectures.
- Evidence: Theoretical convergence rates derived using a new similarity metric that does not require bounded Hessians.

### Open Question 2
- Question: How can stochastic gradients of auxiliary functions be explicitly sampled or constructed to be positively correlated with the target gradients?
- Basis: [explicit] Section 7 states the authors showed a potential benefit from positive correlation but "did not mention how this can be done."
- Why unresolved: While the theory suggests lower variance σ²_f-h with correlation, no specific mechanism or sampling strategy was proposed in the paper.
- Evidence: An algorithmic procedure that generates correlated batches and empirically demonstrates reduced effective variance σ²_f-h.

### Open Question 3
- Question: Is it possible to design an algorithm that combines the convergence speed of AuxMOM with the robustness of Fine-Tuning for dissimilar auxiliary tasks?
- Basis: [explicit] Section 7 asks, "Can we reconcile both worlds?" noting that Fine Tuning is robust to different f functions, whereas AuxMOM requires small similarity δ.
- Why unresolved: The paper presents AuxMOM and Fine-Tuning as distinct trade-offs; one is fast but requires similarity, the other is slower but robust.
- Evidence: A unified algorithm that adapts its behavior based on the estimated similarity δ, maintaining speed when δ is small and falling back to Fine-Tuning behavior when δ is large.

## Limitations
- Hessian similarity assumption is difficult to verify in practice and may not hold for deep learning architectures
- Momentum initialization requires large batch for E₀ ≤ σ²_f-h/T, which may be expensive
- No systematic study of how to engineer correlated noise between f and h gradients for reduced σ²_f-h

## Confidence
- High confidence: The control-variate mechanism (Mechanism 1) is well-established and the error analysis O(∥y-x∥²) is sound
- Medium confidence: Momentum-based variance reduction (Mechanism 2) extends MVR/STORM correctly, but the specific parameter conditions (a ≥ 36δKη) may be conservative
- Medium confidence: Correlated noise benefits (Mechanism 3) are theoretically sound but lack practical guidance on implementation

## Next Checks
1. Implement the toy quadratic experiment to verify AuxMOM converges for any bias ζ while naive approach fails for large ζ, confirming the bias-correction mechanism
2. Run the rotated MNIST experiment varying rotation angles (0°, 45°, 90°, 180°) to empirically measure the δ parameter and validate the Hessian similarity assumption
3. For the coreset experiment, systematically vary coreset size (10%, 20%, 50%) and measure the trade-off between σ²_f-h reduction and gradient bias to understand when AuxMOM outperforms SGD on the full dataset