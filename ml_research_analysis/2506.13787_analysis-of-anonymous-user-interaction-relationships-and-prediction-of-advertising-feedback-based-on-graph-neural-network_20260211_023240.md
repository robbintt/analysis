---
ver: rpa2
title: Analysis of Anonymous User Interaction Relationships and Prediction of Advertising
  Feedback Based on Graph Neural Network
arxiv_id: '2506.13787'
source_url: https://arxiv.org/abs/2506.13787
tags:
- user
- advertising
- interaction
- anonymous
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting advertising feedback
  from anonymous user interactions in online advertising, where user identities are
  unknown and behaviors are fragmented across platforms. Existing graph models struggle
  to capture the multi-scale temporal, semantic, and higher-order dependencies in
  these interaction networks.
---

# Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network

## Quick Facts
- arXiv ID: 2506.13787
- Source URL: https://arxiv.org/abs/2506.13787
- Reference count: 23
- Key outcome: DTH-GNN achieves 91.2% AUC and 0.397 LogLoss on Coveo dataset, outperforming baselines by 8.2% and 5.7% respectively

## Executive Summary
This paper addresses the challenge of predicting advertising feedback from anonymous user interactions where user identities are unknown and behaviors are fragmented across platforms. Existing graph models struggle with multi-scale temporal dependencies and higher-order relationships in these sparse interaction networks. The proposed Decoupled Temporal-Hierarchical Graph Neural Network (DTH-GNN) introduces temporal edge decomposition to model short-term bursts, diurnal cycles, and long-range memory using parallel dilated convolutions. It builds a hierarchical heterogeneous aggregation combining user-user, user-ad, and ad-ad subgraphs through a meta-path conditional Transformer encoder with noise suppression. The model incorporates feedback-aware contrastive regularization and a lightweight strategy gradient layer to fine-tune node representations.

## Method Summary
DTH-GNN processes raw interaction tuples through a multi-stage architecture: first decomposing edges into three temporal channels (short-term, diurnal, long-range) using learnable decay constants and parallel dilated convolutions; then constructing heterogeneous subgraphs (user-user, user-ad, ad-ad) and aggregating them through a meta-path conditional Transformer with cross-channel gating for noise suppression; finally applying feedback-aware contrastive learning with momentum queue distillation and REINFORCE-based strategy gradient optimization. The model is trained on the Coveo Clickstream Dataset with over 36 million interaction records, achieving state-of-the-art performance with 91.2% AUC and 0.397 LogLoss while maintaining computational efficiency (6.8M parameters, 3.9 training hours).

## Key Results
- Achieved 91.2% AUC and 0.397 LogLoss on Coveo Clickstream Dataset
- Outperformed best baseline by 8.2% in AUC and 5.7% in LogLoss
- Demonstrated superior convergence speed and stability across training batches
- Efficient parameter usage with only 6.8M parameters and 3.9 training hours

## Why This Works (Mechanism)

### Mechanism 1: Temporal Edge Decomposition with Dilated Convolutions
Decomposing interaction edges into multi-scale temporal channels enables the model to capture behavior patterns operating at different time granularities. Each edge is decomposed into three channels based on time difference: short-term burst, diurnal cycle, and long-range memory. Parallel dilated convolutions with rates {1, 2, 4} create pyramidal receptive fields within each channel, then residual connections merge outputs. Core assumption: anonymous user ad interactions exhibit separable temporal patterns where second-scale bursts, day-scale cycles, and week/month-scale inertia each contribute independently to feedback prediction.

### Mechanism 2: Meta-Path Conditional Transformer with Cross-Channel Gating
Aggregating heterogeneous subgraphs through a Transformer with meta-path conditioning and learned gating improves signal-to-noise ratio in sparse anonymous networks. Three subgraph types are encoded separately. A Transformer computes attention with path positional encoding injected into queries. Cross-channel gating dynamically suppresses noisy edges before final aggregation. Core assumption: different interaction types carry complementary predictive information; noise structure can be identified and suppressed via learned attention patterns.

### Mechanism 3: Feedback-Aware Contrastive Regularization with Strategy Gradient
Contrastive learning across time slices plus delayed reward integration via REINFORCE improves embedding quality for sparse, cold-start nodes and captures delayed conversion signals. Constructs positive pairs from repeated exposures and negative pairs from control unexposed. Momentum encoder with queue provides stable negatives. Strategy gradient integrates delayed conversion rewards. Core assumption: user behavior is consistent across time slices; delayed conversion rewards can be meaningfully attributed to earlier interaction embeddings.

## Foundational Learning

- **Concept: Dilated/Atrous Convolutions**
  - Why needed here: Enables exponentially expanding receptive fields without linear parameter growth, critical for capturing multi-scale temporal dependencies
  - Quick check question: Can you explain why dilation rates {1, 2, 4} create a pyramidal receptive field and what would happen if all rates were 1?

- **Concept: Meta-Paths in Heterogeneous GNNs**
  - Why needed here: Defines semantic relationship sequences that encode domain knowledge into the aggregation schema
  - Quick check question: Given user-user, user-ad, and ad-ad subgraphs, what meta-paths could connect two users who never directly interacted?

- **Concept: Momentum Contrast (MoCo) and Queue Distillation**
  - Why needed here: Provides stable negative samples across batches, preventing feature collapse in contrastive learning with sparse data
  - Quick check question: Why does a slow-moving momentum encoder help compared to using the current encoder for negative samples?

## Architecture Onboarding

- **Component map**: Input Layer -> Temporal Edge Decomposition -> Dilated Convolution Stack -> Heterogeneous Subgraph Builders -> Meta-Path Transformer Encoder -> Gating Module -> Contrastive Head -> Strategy Gradient Layer -> Output

- **Critical path**: Input → Temporal Decomposition → Dilated Conv → Subgraph Construction → Transformer → Gating → Contrastive Regularization → Strategy Gradient → Loss

- **Design tradeoffs**: 3-channel temporal split increases expressiveness but adds ~3× convolution parameters; Transformer attention is O(n²) and may need neighbor sampling for graphs >100K nodes; Momentum queue adds memory overhead proportional to queue size; Strategy gradient adds ~0.3ms inference latency but reported +3% ROI gain

- **Failure signatures**:
  - Temporal channels collapse: τS ≈ τD ≈ τL → ablate to single channel
  - Gating becomes uniform: βc,φ ≈ 0.5 everywhere → check gradient flow to gating
  - Contrastive collapse: Embeddings converge to same vector → increase temperature or queue size
  - Strategy gradient instability: Loss spikes after REINFORCE introduction → reduce learning rate or increase baseline smoothing

- **First 3 experiments**:
  1. Temporal ablation: Compare full 3-channel vs. single-channel temporal encoding
  2. Gating ablation: Disable gating (set to uniform weights)
  3. Contrastive vs. non-contrastive: Train without contrastive loss and without strategy gradient

## Open Questions the Paper Calls Out

- How does the integration of multimodal content features (e.g., ad images, text, ratings) impact the predictive performance compared to current reliance on interaction topology?
- Can the decoupled temporal-hierarchical approach generalize effectively to non-e-commerce domains with different interaction semantics?
- Does the strategy gradient layer maintain acceptable inference latency when scaled to industrial real-time bidding environments?

## Limitations
- The 3-channel temporal decomposition requires validation that temporal patterns are truly separable and complementary
- The feedback-aware contrastive regularization with strategy gradient layer has limited external validation
- The paper lacks ablation studies on the relative contributions of temporal decomposition, meta-path gating, and contrastive regularization

## Confidence
- **High confidence**: Temporal edge decomposition with dilated convolutions (well-established in GNN literature)
- **Medium confidence**: Meta-path conditional Transformer with cross-channel gating (supported by heterogeneous GNN research)
- **Low confidence**: Feedback-aware contrastive regularization with strategy gradient layer (limited external validation)

## Next Checks
1. Conduct ablation study comparing full 3-channel temporal decomposition against single-channel and 2-channel variants
2. Test model performance with and without the meta-path gating mechanism to measure noise suppression effectiveness
3. Evaluate the contrastive regularization component in isolation to determine if it improves representation quality or causes collapse in sparse anonymous interaction data