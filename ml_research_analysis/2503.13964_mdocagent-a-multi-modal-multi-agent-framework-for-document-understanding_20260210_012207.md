---
ver: rpa2
title: 'MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding'
arxiv_id: '2503.13964'
source_url: https://arxiv.org/abs/2503.13964
tags:
- agent
- information
- text
- answer
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MDocAgent introduces a multi-modal multi-agent framework for document\
  \ question answering that integrates both text and visual information through specialized\
  \ agents and dual RAG pipelines. The system employs five agents\u2014general, critical,\
  \ text, image, and summarizing\u2014to collaboratively process and synthesize information\
  \ from retrieved textual and visual contexts."
---

# MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding

## Quick Facts
- arXiv ID: 2503.13964
- Source URL: https://arxiv.org/abs/2503.13964
- Reference count: 40
- Primary result: 12.1% average improvement over M3DocRAG across five benchmarks

## Executive Summary
MDocAgent introduces a multi-modal multi-agent framework for document question answering that integrates both text and visual information through specialized agents and dual RAG pipelines. The system employs five agents—general, critical, text, image, and summarizing—to collaboratively process and synthesize information from retrieved textual and visual contexts. Experimental results show MDocAgent achieves an average improvement of 12.1% compared to the state-of-the-art M3DocRAG method across five benchmarks, with particular gains in handling long documents and complex multi-modal reasoning tasks.

## Method Summary
MDocAgent processes document question answering through a five-stage pipeline: (1) Document preprocessing with OCR and PDF parsing into text segments and page images, (2) Dual RAG retrieval using ColBERTv2 for text and ColPali for images, (3) General agent produces initial answer and critical agent extracts key information, (4) Text agent (Llama-3.1-8B-Instruct) and image agent (Qwen2-VL-7B-Instruct) process retrieved contexts separately, (5) Summarizing agent synthesizes final answer from all agent responses. The framework uses top-k retrieval (k=1 or 4) and relies on specialized agents guided by critical information extracted from the general agent's response.

## Key Results
- MDocAgent achieves 12.1% average improvement over M3DocRAG across five benchmarks
- Top-4 retrieval improves accuracy from 40.7% to 46.5% compared to top-1
- Qwen2-VL backbone outperforms GPT-4o by 9-23% on different benchmarks
- Critical agent and specialized text/image agents show significant individual contributions in ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Critical Information Routing Reduces Modality Search Space
The critical agent extracts key textual and visual cues, producing structured dictionaries that constrain where text and image agents focus their analysis. This converts open-ended retrieval into guided extraction, reducing noise and improving efficiency. Retrieved context must contain answer-relevant information; retrieval failure cannot be recovered downstream.

### Mechanism 2: Parallel Dual-RAG Pipelines Capture Complementary Evidence
Text (ColBERTv2) and image (ColPali) retrieval pipelines jointly provide higher coverage for cross-modal reasoning questions. Text-based RAG indexes OCR-extracted segments while image-based RAG encodes page-level visual embeddings. Both pipelines must retrieve relevant evidence; if both miss the evidence page, multi-agent reasoning cannot compensate.

### Mechanism 3: Summarizing Agent Resolves Agent-Level Disagreements
When text and image agents produce conflicting or partially overlapping answers, the summarizing agent synthesizes a more accurate final response by identifying commonalities, discrepancies, and complementary insights. At least one agent must produce correct partial evidence; the summarizer must have sufficient reasoning capacity to arbitrate.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: MDocAgent extends single-modality RAG to parallel text+image retrieval pipelines
  - Quick check question: Can you explain why ColBERT uses late interaction scoring vs. dense single-vector retrieval?

- **Multi-Agent Orchestration**
  - Why needed here: The five agents have defined input/output contracts; understanding message passing is essential for debugging
  - Quick check question: What happens if the critical agent produces an empty dictionary?

- **Vision-Language Model (VLM) Tokenization**
  - Why needed here: Image-based RAG (ColPali) generates visual embeddings; understanding nv×d dimensions informs memory budgeting
  - Quick check question: How does page-level vs. patch-level embedding affect retrieval granularity?

## Architecture Onboarding

- **Component map:** Document preprocessing → ColBERTv2 + ColPali retrieval → General agent → Critical agent → [Text agent | Image agent] → Summarizing agent
- **Critical path:** Retrieval quality → Critical agent extraction accuracy → Specialized agent focus → Summarizer conflict resolution. Failures propagate forward; retrieval errors are unrecoverable.
- **Design tradeoffs:** Top-k=1 vs Top-k=4: More context improves accuracy (46.5% vs 40.7%) but increases token cost and potential noise. Separate text vs unified VLM backbone: Text agent uses Llama (strong text); others use Qwen2-VL (multi-modal). Agent count: Ablation shows removing any agent degrades performance; however, marginal gains per agent vary by benchmark.
- **Failure signatures:** Retrieval misses: If neither ColBERT nor ColPali retrieves the evidence page, all agents fail. Critical agent misidentification: Extracting irrelevant cues propagates incorrect focus. Summarizer hallucination: When all agents are uncertain, summarizer may fabricate consensus.
- **First 3 experiments:**
  1. **Retrieval ablation:** Run with only text-RAG and only image-RAG to establish modality-specific baselines on your target document type.
  2. **Agent removal study:** Systematically remove critical, text, and image agents to quantify contribution per benchmark.
  3. **Backbone substitution:** Swap Qwen2-VL for GPT-4o to measure sensitivity to model capacity; expect 9-23% relative gain depending on benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
Can dynamic or non-sequential inter-agent communication protocols (e.g., debates or feedback loops) improve the framework's reasoning capabilities over the current stage-based approach?
- Basis in paper: [explicit] The Conclusion states: "Future work will explore more advanced inter-agent communication..."
- Why unresolved: The current architecture relies on a linear flow where the Critical Agent guides the Text/Image Agents. It is unclear if allowing agents to query one another iteratively would resolve complex ambiguities more effectively.
- What evidence would resolve it: Experiments comparing the current sequential pipeline against architectures utilizing cyclic or tree-based communication topologies on ambiguous reasoning benchmarks.

### Open Question 2
How can external knowledge sources be integrated into the MDocAgent framework to handle questions requiring information not present in the source document?
- Basis in paper: [explicit] The Conclusion explicitly lists "...the integration of external knowledge sources" as a direction for future work.
- Why unresolved: The current system relies solely on internal "Multi-modal Context Retrieval" from the provided document. It lacks a mechanism to verify facts or fill gaps using external databases, limiting its utility in open-domain scenarios.
- What evidence would resolve it: A variant where the General or Critical Agent can trigger external API calls (e.g., Wikipedia) and corresponding evaluation on open-domain DocQA datasets.

### Open Question 3
What is the trade-off between the accuracy gains of the multi-agent system and the computational latency/cost compared to single-pass LVLMs?
- Basis in paper: [inferred] The paper introduces a complex system with five agents and dual RAG pipelines to address "information overload" and reasoning limits. However, it only reports accuracy metrics, leaving the computational overhead unstated.
- Why unresolved: While the method improves accuracy by 12.1%, running five specialized agents significantly increases latency compared to baseline single-model approaches.
- What evidence would resolve it: A detailed efficiency analysis reporting inference time, token consumption, and FLOPs for MDocAgent versus the ColBERT and M3DocRAG baselines.

## Limitations

- **Retrieval quality dependence:** Framework performance heavily contingent on quality of both text and image retrieval; if neither retrieves relevant pages, multi-agent reasoning cannot recover
- **Critical agent error propagation:** Misidentification by critical agent (e.g., interpreting "F.G.S." as a degree) propagates incorrect focus throughout pipeline
- **Agent hallucination risks:** When all agents are uncertain, summarizing agent may fabricate consensus rather than admit uncertainty

## Confidence

- **High confidence:** 12.1% average improvement over M3DocRAG is well-supported by five-benchmark evaluation and consistent across retrieval settings
- **Medium confidence:** Mechanism explanations for why parallel dual-RAG and critical information routing improve performance are plausible but not directly validated through ablation studies
- **Low confidence:** Claim that summarizing agent "resolves conflicts" based on final accuracy metric rather than explicit analysis of agent-level disagreements

## Next Checks

1. **Retrieval failure analysis:** Run MDocAgent with oracle retrieval (bypassing ColBERTv2 and ColPali) on documents where retrieval failed in original experiments to determine maximum achievable performance ceiling

2. **Critical agent ablation:** Replace critical agent with simple keyword extractor or random selector and measure performance drop to quantify contribution of critical information routing

3. **Uncertainty calibration test:** Design questions where correct answer is "unknown" or "not enough information" and measure whether MDocAgent learns to admit uncertainty rather than hallucinate using multi-class evaluation framework beyond binary correctness