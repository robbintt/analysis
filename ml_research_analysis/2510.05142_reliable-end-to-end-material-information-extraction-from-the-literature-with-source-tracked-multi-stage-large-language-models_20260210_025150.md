---
ver: rpa2
title: Reliable End-to-End Material Information Extraction from the Literature with
  Source-Tracked Multi-Stage Large Language Models
arxiv_id: '2510.05142'
source_url: https://arxiv.org/abs/2510.05142
tags:
- extraction
- materials
- information
- source
- processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of extracting comprehensive
  materials information from unstructured literature, focusing on the composition-processing-microstructure-property
  relationships essential for data-driven discovery. We propose a multi-stage information
  extraction pipeline powered by large language models, capturing 47 features across
  four categories.
---

# Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models

## Quick Facts
- arXiv ID: 2510.05142
- Source URL: https://arxiv.org/abs/2510.05142
- Authors: Xin Wang; Anshu Raj; Matthew Luebbe; Haiming Wen; Shuozhi Xu; Kun Lu
- Reference count: 40
- Key outcome: Multi-stage pipeline with source tracking achieved ~0.96 F1 scores at feature and tuple levels, reducing missed materials from 49 to 13 out of 396

## Executive Summary
This study presents a four-stage LLM-powered pipeline for extracting comprehensive materials information from unstructured literature, focusing on composition-processing-microstructure-property relationships. The pipeline captures 47 features across four categories and integrates iterative extraction with source tracking to enhance accuracy. Evaluations demonstrate F1 scores around 0.96 at both feature and tuple levels, with microstructure category showing 10.0% improvement over single-pass extraction. The approach enables scalable literature mining while producing databases with high precision and minimal omissions.

## Method Summary
The pipeline uses OpenAI o3-mini API with `reasoning_effort="high"` in a four-stage sequential process: Stage 1 extracts all materials plus composition and processing from full articles; Stages 2-3 iteratively process each material individually to extract microstructure and properties using prior context; Stage 4 validates across materials against source texts. Source tracking preserves original text excerpts for each extraction. The system processes 100 journal articles on precipitate-containing multi-principal element alloys, achieving ~0.959 F1 (feature-level) and ~0.962 (tuple-level) through iterative per-material processing and hierarchical decomposition matching domain structure.

## Key Results
- Achieved ~0.959 F1 (feature-level) and ~0.962 (tuple-level) extraction accuracy
- Microstructure F1 improved from 0.880 (single-pass) to 0.950 (multi-stage with source tracking)
- Reduced missed materials from 49 to 13 out of 396 materials
- Zero false positive materials in evaluation dataset

## Why This Works (Mechanism)

### Mechanism 1
Multi-stage hierarchical decomposition reduces extraction errors by matching pipeline stages to domain structure. Materials science literature follows compositional dependencies—composition and processing determine microstructure, which governs properties. By extracting in this sequence (Stage 1: composition+processing → Stage 2: microstructure → Stage 3: properties), each stage operates with reduced ambiguity because prior context is available. The paper reports microstructure F1 improved from 0.880 (single-pass) to 0.950 (multi-stage with source tracking) at feature level. Core assumption: LLM performance degrades when simultaneously tracking multiple materials across thousands of tokens; staged decomposition mitigates this attention limitation.

### Mechanism 2
Source tracking preserves referential context across stages, enabling validation and reducing hallucinations. Each extracted value includes its originating text snippet. This anchor prevents "reference loss" when ambiguous terms (e.g., "the FSA specimen") appear in later stages. During Stage 4 validation, the system cross-references all extractions against preserved sources. The paper reports zero false positive materials and miss rate reduction from 12.4% to 3.3%. Core assumption: Explicit source-text association constrains LLM inference behavior more effectively than implicit context retention.

### Mechanism 3
Iterative per-material processing prevents cross-material contamination. After Stage 1 identifies all materials, Stages 2-3 loop over each material individually, passing only that material's composition and processing context. This isolation reduces the probability of attributing properties from one material to another—a failure mode comprising 62.5% of property extraction errors in the study. Core assumption: LLM attention to multiple entities in a single pass causes entity-binding errors; per-entity iteration reduces this interference.

## Foundational Learning

- **Composition-Processing-Microstructure-Property (CPMP) relationships**
  - Why needed here: The entire pipeline architecture assumes this hierarchical dependency. Without understanding that processing affects microstructure which determines properties, the staging rationale is opaque.
  - Quick check question: Given a heat-treated alloy, would you extract its aging temperature before or after precipitate size? Why?

- **LLM context window and attention limitations**
  - Why needed here: The paper explicitly attributes single-pass failures to "cognitive load" and attention mechanism limitations when tracking multiple materials across long documents.
  - Quick check question: If an LLM processes a 10,000-token document in one pass, what happens to entity references mentioned 5,000 tokens apart?

- **Precision, Recall, F1, and tuple-level evaluation**
  - Why needed here: The paper evaluates at both feature-level (independent attributes) and tuple-level (interdependent feature groups). Understanding why tuple-level F1 is lower than feature-level F1 is critical for interpreting results.
  - Quick check question: If a pipeline extracts (precipitate_type="L12", size="50nm") but ground truth is (precipitate_type="L12", size="75nm"), is this a false positive at tuple level?

## Architecture Onboarding

- **Component map:** Stage 1 (Document-level) -> Stage 2 (Per-material) -> Stage 3 (Per-material) -> Stage 4 (Document-level)
- **Critical path:** Stage 1 accuracy is foundational—missed materials here cascade as complete omissions (all 47 features counted as FNs). The paper reports 13/396 missed materials in best configuration.
- **Design tradeoffs:**
  - Completeness vs. cost: Average ~21,500 tokens per paper (full text re-input at each stage)
  - Precision vs. recall: Multi-stage without source tracking increased recall but slightly decreased precision; source tracking restored both
  - Rigid schema vs. flexibility: 47 predefined features + "Additional" fields for non-standard data
- **Failure signatures:**
  - Confusion errors (62.5%): Properties from Material A attributed to Material B—check for incomplete property reporting across samples
  - Extra inference errors (12.5%): LLM applies theoretical models (e.g., linear addition strengthening) instead of extracting reported values—filter values marked as "estimated"
  - Omission errors (25%): Indirect reporting ("1200 MPa higher than ST condition") not extracted—requires baseline value inference
- **First 3 experiments:**
  1. Replicate single-pass vs. multi-stage comparison on a held-out 10-article subset to validate F1 delta (reported: +10.0% microstructure feature-level)
  2. Ablate source tracking: run multi-stage pipeline without source_text fields to quantify recall degradation (reported: recall drops from 0.968 to 0.927 for microstructure)
  3. Test generalization: apply pipeline to a different materials class (e.g., polymers from corpus neighbor arXiv:2507.07499) with modified feature schema to assess modular adaptation

## Open Questions the Paper Calls Out

### Open Question 1
Can strategies be developed to reliably constrain LLMs to extract only experimental measurements while rejecting theoretical model predictions? Basis in paper: Authors state: "These findings point to a future research direction: developing better strategies to control LLM behavior and ensure strict adherence to experimental data extraction without unauthorized theoretical modeling." Why unresolved: 12.5% of property extraction errors involved inappropriate theoretical modeling (e.g., linearly adding strengthening contributions). The model cannot consistently distinguish valid data re-expression (converting formulae to percentages) from unauthorized predictions. What evidence would resolve it: Development of prompt engineering or architectural constraints that reduce inference errors to near-zero while maintaining extraction completeness.

### Open Question 2
How can multimodal extraction be integrated to capture information currently trapped in figures, micrographs, and diagrams? Basis in paper: Authors state: "Current limitations center on several areas requiring future development. The system processes only text and tables, missing information embedded in images. Future systems will integrate multimodal capabilities for comprehensive extraction." Why unresolved: The current pipeline uses o3-mini API which accepts only text; microstructure data reported visually (phase distributions, grain sizes from micrographs) is completely inaccessible. What evidence would resolve it: Extension of the pipeline to vision-language models with demonstrated extraction of quantitative data from SEM/TEM images and crystallographic diagrams.

### Open Question 3
Can the "completion bias" in LLMs be mitigated, where models assume unreported phases do not exist rather than marking them as "unreported"? Basis in paper: Authors note that when text states "precipitate A comprises 20% volume fraction," the model "often assumed the matrix occupied 80% and ignored potentially unreported phases," reflecting that "completion bias...conflicts with scientific reporting where 'unreported' differs fundamentally from 'non-existent.'" Why unresolved: This bias reflects fundamental LLM training on complete datasets; no solution is proposed in the current framework despite its impact on microstructure extraction fidelity. What evidence would resolve it: Modified prompts or training approaches that achieve >95% accuracy in distinguishing "explicitly absent" from "not mentioned" for phase volume fractions.

## Limitations
- Opaque prompt engineering—exact templates for all four pipeline stages are not provided
- Performance on non-metallic materials systems remains untested despite generalizability claims
- Evaluation protocol relies on expert annotations without specifying inter-annotator agreement thresholds

## Confidence
- **High confidence:** Multi-stage extraction improves microstructure feature-level F1 (0.950 vs 0.880 single-pass) and reduces missed materials (13 vs 49)
- **Medium confidence:** Source tracking specifically contributes to zero false positives and reduced miss rate; exact contribution relative to staging effect is unclear
- **Low confidence:** Claims of generalizability to other materials systems without empirical validation on different material classes

## Next Checks
1. **Ablation study:** Run multi-stage pipeline without source tracking to quantify precision/recall degradation (reported: recall drops from 0.968 to 0.927)
2. **Cross-domain generalization:** Apply pipeline to polymers or ceramics literature to test material-class independence
3. **Prompt sensitivity analysis:** Systematically vary prompt templates to determine robustness to minor formulation changes