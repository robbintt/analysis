---
ver: rpa2
title: 'CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images'
arxiv_id: '2510.11718'
source_url: https://arxiv.org/abs/2510.11718
tags:
- reasoning
- visual
- mathematical
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CodePlot-CoT, a code-driven chain-of-thought
  paradigm that enables Vision Language Models to engage in visual reasoning for mathematical
  problem solving. The approach leverages the VLM to generate executable plotting
  code representing visual thoughts, which are then rendered into images and fed back
  into the model.
---

# CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images

## Quick Facts
- arXiv ID: 2510.11718
- Source URL: https://arxiv.org/abs/2510.11718
- Authors: Chengqi Duan; Kaiyue Sun; Rongyao Fang; Manyuan Zhang; Yan Feng; Ying Luo; Yufang Liu; Ke Wang; Peng Pei; Xunliang Cai; Hongsheng Li; Yi Ma; Xihui Liu
- Reference count: 38
- Primary result: Achieves up to 21% improvement over base model on Math-VR benchmark

## Executive Summary
CodePlot-CoT introduces a novel code-driven chain-of-thought paradigm for mathematical visual reasoning, enabling Vision Language Models to generate and execute plotting code as intermediate "visual thoughts" for solving complex math problems. The approach leverages the VLM's text generation strength to produce precise geometric representations through executable code, which are then rendered and fed back into the model to ground abstract reasoning in concrete visual evidence. To support this paradigm, the authors construct Math-VR, a large-scale bilingual dataset of 178K mathematical problems requiring visual reasoning, and develop MatplotCode, a high-fidelity image-to-code converter for mathematical figures.

## Method Summary
The method involves two-stage training of a Vision Language Model (Qwen2.5VL-32B) to perform code-driven visual reasoning. First, MatplotCode is trained to convert mathematical figures into executable Python/Matplotlib code with high fidelity. Then, the CodePlot-CoT model is fine-tuned on Math-VR data using sequences that interleave text, code, and rendered images. During inference, the model generates reasoning code, executes it to create images, and uses these images as visual feedback to refine its reasoning. The approach explicitly trades pixel-level generation for code-based precision, leveraging the model's strength in structured text generation for geometric reasoning tasks.

## Key Results
- Achieves up to 21% improvement over base model on Math-VR benchmark
- CodePlot-CoT-Bagel outperforms Bagel-Thinking-with-image by 7.2% in Process Score
- MatplotCode preferred in 554/1000 cases vs. Gemini-2.5-Pro for image-to-code conversion
- Code generation uses fewer tokens (~820 tokens/image) compared to standard visual tokens (1024+)

## Why This Works (Mechanism)

### Mechanism 1: Structured Code as a Proxy for Visual Generation
Representing visual thoughts as executable code yields higher precision for mathematical reasoning than direct pixel generation. Mathematical figures rely primarily on structural attributes rather than pixel-level textures, allowing the VLM to leverage its strength in text generation to produce precise, verifiable geometric constraints.

### Mechanism 2: Feedback Loop via Rendered Execution
Feeding rendered images back into the VLM creates a "visual grounding" loop that corrects and refines reasoning. The model generates code, executes it to create deterministic images, and uses these images to ground abstract textual reasoning in concrete visual evidence.

### Mechanism 3: Specialized Image-to-Code Alignment
A specialized converter (MatplotCode) is required to create the high-quality supervision needed for the model to learn code-based visual reasoning. Standard VLMs fail at zero-shot image-to-code conversion for complex math figures, necessitating a dedicated converter trained on mathematical figure datasets.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed: The paper extends CoT from text-only to "Visual CoT," requiring understanding that the model produces step-by-step justifications
  - Quick check: Can you distinguish between a model outputting a final answer vs. an intermediate reasoning trace?

- **Concept: Programmatic Plotting (Matplotlib/Python)**
  - Why needed: The "visual thoughts" are code snippets requiring basic plotting library knowledge to debug generation failures
  - Quick check: Do you know how to define coordinates and plot a line segment in Python?

- **Concept: Vision-Language Alignment**
  - Why needed: The core task involves mapping visual inputs to textual outputs (code) and vice versa
  - Quick check: How does a VLM typically process an image (e.g., via a ViT encoder) before passing it to the LLM backbone?

## Architecture Onboarding

- **Component map:** Math-VR Dataset -> MatplotCode (Converter) -> CodePlot-CoT (Solver) -> Python Sandbox -> Rendered Images
- **Critical path:** Data Curation (MatplotCode conversion) -> SFT (training on text+code+image sequences) -> Inference (Text+Image → Text+Code → Image → Final Solution)
- **Design tradeoffs:** Code vs. Pixels (precision vs. creative potential), Efficiency (fewer tokens for code generation)
- **Failure signatures:** Execution Errors (syntactically invalid code), Geometric Hallucination (logically correct code rendering incorrect geometry)
- **First 3 experiments:**
  1. Verify the Converter: Test MatplotCode on hold-out geometry figures for Reconstruction Fidelity
  2. Ablate the Modality: Compare CodePlot-CoT against Text-Only baseline on Math-VR benchmark
  3. Analyze Inference Cost: Measure token throughput and latency for code+render vs. text-only reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the fidelity of the image-to-code converter (MatplotCode) be improved to achieve near-perfect geometric accuracy for complex mathematical figures?
- Basis: Appendix E (Limitations) states MatplotCode "has not yet achieved a 100% fidelity rate," leading to visual reasoning errors
- Evidence: A new iteration demonstrating significantly higher reconstruction fidelity on Math-VR test set

### Open Question 2
- Question: Can the code-driven visual reasoning paradigm generalize effectively to 3D geometry and other scientific domains?
- Basis: Paper relies on Matplotlib (2D plotting) and Math-VR dataset dominated by 2D Plane Geometry (81%)
- Evidence: Evaluation on benchmarks requiring 3D rendering libraries or domain-specific visualizations

### Open Question 3
- Question: To what extent does error propagation in code generation affect final reasoning accuracy, and can self-correction mechanisms mitigate this?
- Basis: The sequential pipeline (Text → Code → Image → Reasoning) may compound logical errors in code
- Evidence: Ablation study measuring performance gains with verification modules for geometric constraint validation

## Limitations
- Dependency on specialized MatplotCode converter whose generalizability to diverse mathematical figure styles remains uncertain
- Evaluation relies entirely on GPT-4.1 for scoring, introducing potential subjectivity and limiting independent verification
- Limited demonstration of scalability to complex 3D geometry or non-standard notation systems

## Confidence

**High Confidence**: Core architectural contribution and dataset construction methodology are well-documented and technically sound
**Medium Confidence**: MatplotCode converter's claimed superiority and fidelity of image-to-code alignment process
**Low Confidence**: Long-term generalization to mathematical domains beyond training distribution, particularly complex 3D geometry

## Next Checks

1. **Converter Generalizability Test**: Evaluate MatplotCode on held-out test set of mathematical figures from different sources using SSIM/LPIPS metrics
2. **Cross-Dataset Transfer Evaluation**: Test CodePlot-CoT on established benchmarks (MATH, GSM8K-V) not in Math-VR training data
3. **Human Evaluation of Visual Grounding**: Conduct studies measuring whether rendered images actually aid reasoning compared to text-only approaches