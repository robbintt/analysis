---
ver: rpa2
title: Enhancing Smart Environments with Context-Aware Chatbots using Large Language
  Models
arxiv_id: '2502.14469'
source_url: https://arxiv.org/abs/2502.14469
tags:
- user
- activity
- chatbot
- smart
- activities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel architecture for context-aware interactions
  within smart environments, integrating Large Language Models (LLMs) with real-time
  Human Activity Recognition (HAR) and precise indoor localization via UWB technology.
  The system processes user location and activity data to enable personalized and
  adaptive chatbot interactions, moving beyond static responses by dynamically adjusting
  to real-time user situations.
---

# Enhancing Smart Environments with Context-Aware Chatbots using Large Language Models

## Quick Facts
- arXiv ID: 2502.14469
- Source URL: https://arxiv.org/abs/2502.14469
- Reference count: 36
- Primary result: Context-aware chatbot achieves relevance scores up to 80/100 by integrating LLM with UWB localization and HAR in smart homes

## Executive Summary
This paper introduces a novel architecture for context-aware interactions within smart environments, integrating Large Language Models (LLMs) with real-time Human Activity Recognition (HAR) and precise indoor localization via UWB technology. The system processes user location and activity data to enable personalized and adaptive chatbot interactions, moving beyond static responses by dynamically adjusting to real-time user situations. A case study conducted in a supervised flat with three elderly residents demonstrated the system's ability to generate contextually relevant and high-scoring chatbot responses, achieving relevance scores up to 80 out of 100.

## Method Summary
The system integrates UWB localization, ambient sensors, and HAR models with an LLM-powered chatbot. UWB wearable tags provide precise indoor location via fingerprinting, while ambient sensors capture object interactions. HAR models (GRU and Conv1D + GRU with attention) fuse these data streams for real-time activity classification. The LLM receives structured prompts containing user demographics, historical activities, and current context, generating personalized responses with self-assessed relevance scores.

## Key Results
- Achieved relevance scores up to 80/100 for contextually aware chatbot responses
- Successfully integrated UWB localization with HAR for multi-occupancy activity recognition
- Demonstrated potential for supporting independent living and providing timely assistance in smart homes

## Why This Works (Mechanism)

### Mechanism 1: Multi-Modal Context Acquisition via Sensor-LLM Pipeline
- **Claim**: The integration of UWB localization with ambient sensors enables multi-occupancy activity recognition that feeds context-aware LLM responses.
- **Mechanism**: UWB wearable tags provide precise indoor location via fingerprinting techniques; ambient sensors (motion, door contacts, temperature, humidity, power consumption) capture object interactions. HAR models (GRU and Conv1D + GRU with attention) fuse these data streams for real-time activity classification, which is then passed to the LLM prompt.
- **Core assumption**: Users will consistently wear UWB tags, and sensor placements adequately cover activity zones.
- **Evidence anchors**:
  - [abstract]: "Our system integrates user location data obtained through UWB tags and sensor-equipped smart homes with real-time human activity recognition (HAR) to provide a comprehensive understanding of user context."
  - [section 3.1]: Details sensing infrastructure including ambient sensors transmitted via MQTT to Home Assistant, UWB fingerprinting for Non-Line-of-Sight scenarios, and HAR models using deep learning ensembles.
  - [corpus]: "MARAuder's Map" addresses ambient sensor-based HAR with spatial layout awareness; "UserCentrix" explores multi-agent systems for smart spaces. Direct validation of UWB+LLM integration remains limited in corpus.
- **Break condition**: If users don't wear UWB tags, or sensors fail to capture sufficient interaction data, the context pipeline breaks and LLM responses become generic.

### Mechanism 2: Structured Context Injection into LLM Prompts
- **Claim**: Formatting activity and location data into structured prompt components enables the LLM to generate contextually relevant responses with quantified relevance scores.
- **Mechanism**: Three prompt components—`init_context` (role definition), `pre_act_format` (historical activity timeline), `question_format` (current context)—are assembled with user demographics and activity-room mappings. The LLM outputs structured `(text, score)` where score (0-100) represents self-assessed relevance.
- **Core assumption**: The LLM can reliably self-assess response relevance, and prompt structure sufficiently captures user context.
- **Evidence anchors**:
  - [abstract]: "This contextual information is then fed to an LLM-powered chatbot, enabling it to generate personalised interactions and recommendations based on the user's current activity and environment."
  - [section 5.1]: Details prompt structure with `init_context`, `pre_act_format`, `question_format` using Gemini Flash 1.5, producing relevance scores up to 80/100.
  - [corpus]: "PaRT" addresses proactive chatbots with personalized retrieval; specific prompt engineering approaches for HAR-to-LLM integration are not extensively documented in corpus.
- **Break condition**: Incomplete prompts (e.g., missing `pre_act_format`) degrade response relevance; the paper notes this as an area for future investigation.

### Mechanism 3: Temporal Continuity via Activity History Queue
- **Claim**: Maintaining a queue of historical activities enables coherent multi-turn interactions and prevents repetitive responses.
- **Mechanism**: Activity history (type, start/end times) is converted to Unix timestamps for structured timeline. Pre-question sequences reference previous activities, and the LLM is explicitly instructed to avoid repetition.
- **Core assumption**: Historical activity patterns are predictive of current needs, and users benefit from interaction continuity.
- **Evidence anchors**:
  - [section 4]: "By analysing the user's real-time data and historical patterns through HAR, the chatbot can anticipate needs and offer assistance before being explicitly asked."
  - [section 5.1]: "A queue component manages real-time prompts by leveraging activity history and analysing the temporal dynamics between user actions and chatbot replies."
  - [corpus]: Weak corpus evidence on temporal activity queues for LLM chatbots; "UserCentrix" mentions memory-augmented AI but focuses on different mechanisms.
- **Break condition**: Gaps in activity history or timeline fragmentation cause disconnected, context-poor responses.

## Foundational Learning

- **Concept: Human Activity Recognition (HAR) in Smart Environments**
  - Why needed here: The system's entire value proposition depends on accurately recognizing activities from sensor data before the LLM can respond contextually.
  - Quick check question: Can you explain the difference between knowledge-based and data-driven HAR approaches, and why deep learning (GRU, Conv1D) is preferred for multi-occupancy settings?

- **Concept: Ultra-Wideband (UWB) Indoor Localization**
  - Why needed here: UWB provides the precise location tracking essential for distinguishing activities of multiple occupants and mapping activities to specific rooms.
  - Quick check question: What is fingerprinting in UWB localization, and how does it address Non-Line-of-Sight challenges?

- **Concept: Prompt Engineering for Context-Aware LLMs**
  - Why needed here: Chatbot effectiveness hinges on how well activity and location data are structured into prompts for the LLM.
  - Quick check question: How does separating prompt components (user context, historical activities, current question) enable modularity and easier modification?

## Architecture Onboarding

- **Component map**: Ambient sensors → MQTT → Home Assistant → HAR models → Context aggregation → LLM → Response
- **Critical path**: UWB tag worn → Location detected → Sensors triggered → HAR classifies activity → Context formatted into prompt → LLM generates response → User receives output. **Weakest link**: consistent tag wearing and sensor coverage.
- **Design tradeoffs**:
  - Precision vs. intrusiveness: UWB requires wearables; ambient sensors are less intrusive but struggle with multi-occupancy.
  - Real-time vs. context depth: More history improves relevance but increases latency.
  - Privacy vs. personalization: Edge-fog architecture processes locally but may limit cloud model capabilities.
- **Failure signatures**:
  - Low relevance scores → incomplete prompt, HAR misclassification, or missing activity history
  - Repetitive responses → queue component not referencing historical activities
  - Wrong room context → UWB localization error or activity-room mapping mismatch
  - No response → sensor failure, MQTT disconnection, or HAR not detecting activity change
- **First 3 experiments**:
  1. **Baseline HAR accuracy test**: Deploy sensors and UWB, collect labeled activity data, validate HAR model accuracy (>85% target) before LLM integration.
  2. **Prompt ablation study**: Test response quality with varying prompt completeness (full vs. missing `pre_act_format` vs. missing user context); measure relevance scores with human evaluation.
  3. **Multi-occupancy stress test**: 2-3 users perform simultaneous activities in different rooms; verify UWB distinguishes users and HAR assigns activities correctly (<10% cross-user confusion target).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do variations in the completeness of the `pre_act_format` prompt (e.g., missing historical context) impact the quality and accuracy of the LLM's responses?
- Basis in paper: [explicit] The authors state, "it would be beneficial to examine the effects of incomplete pre_act_format prompts on the output" to optimize robustness.
- Why unresolved: The current case study utilized a structured timeline with comprehensive activity history, but real-world deployments may suffer from data loss or sensor errors.
- What evidence would resolve it: Ablation studies comparing response relevance scores between prompts with full historical context versus those with systematically removed or truncated activity data.

### Open Question 2
- Question: Can the chatbot effectively learn and adapt to individual user behavior patterns over extended periods to maintain personalization without becoming repetitive?
- Basis in paper: [explicit] The conclusion notes that "enhanced learning capabilities will be explored" and that future work will investigate the system in "supervised living environments over extended periods."
- Why unresolved: The current study was limited to a two-day data collection interval, which is insufficient to validate long-term adaptation or the system's ability to evolve with the user.
- What evidence would resolve it: Longitudinal studies tracking the diversity of responses and user satisfaction scores over weeks or months of continuous interaction.

### Open Question 3
- Question: How does the LLM's self-assigned relevance score correlate with actual human user satisfaction in a real-world, multi-occupancy setting?
- Basis in paper: [inferred] The evaluation relies heavily on the LLM's own output score (0-100) for relevance (e.g., "relevance scores up to 80 out of 100"), yet the paper cites the risk of "delusion in real-life deployments" and lacks a human-in-the-loop evaluation metric for the specific case study.
- Why unresolved: Automated scores may not accurately reflect the nuance of user experience, particularly for elderly residents who may have different thresholds for "helpful" or "intrusive" interactions.
- What evidence would resolve it: Comparative analysis aligning the LLM-generated relevance scores with qualitative feedback or surveys from the actual residents in the smart home.

## Limitations
- Evaluation limited to three elderly participants in supervised flat, limiting generalizability
- System depends on consistent UWB tag wearing, creating critical usability bottleneck
- Lacks objective validation of LLM response quality beyond self-assessed relevance scores
- Sensor coverage and placement assumptions not validated across diverse home layouts
- HAR models' performance in multi-occupancy scenarios with simultaneous activities not demonstrated

## Confidence
- **High confidence**: The multi-modal sensor integration architecture (UWB + ambient sensors → HAR → LLM) is technically sound and well-documented. The prompt engineering approach with structured components is methodologically clear.
- **Medium confidence**: The relevance scores (up to 80/100) suggest reasonable performance, but self-assessment by the LLM introduces optimism bias. The system's adaptability to real-time context is plausible given the architecture but not rigorously proven.
- **Low confidence**: Generalization to diverse user populations, home environments, and multi-occupancy stress scenarios is unsupported. The long-term reliability of UWB tag compliance and sensor coverage is speculative.

## Next Checks
1. **Independent relevance scoring**: Recruit human evaluators to rate LLM responses on contextual appropriateness using blind comparisons between context-aware and context-free responses. Target: >70% agreement on context-aware superiority.
2. **UWB compliance study**: Track tag wearing rates over 30+ days with diverse user groups. Measure correlation between compliance rates and response relevance scores. Target: >80% compliance for sustained relevance gains.
3. **Multi-occupancy stress test**: Deploy system with 4+ users performing overlapping activities in same rooms. Measure HAR cross-user confusion rate and LLM's ability to maintain individual context. Target: <10% confusion rate, >70% relevance scores maintained.