---
ver: rpa2
title: 'Context is Enough: Empirical Validation of $\textit{Sequentiality}$ on Essays'
arxiv_id: '2511.09185'
source_url: https://arxiv.org/abs/2511.09185
tags:
- essay
- sequentiality
- features
- contextual
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study validates that narrative flow in essays is better captured
  by the contextual term of sequentiality rather than the original topic-inclusive
  formulation. Using trait-annotated essay datasets (ASAP++ and ELLIPSE), the contextual
  term (NLLC) showed superior alignment with human assessments of Organization and
  Cohesion compared to both topic-only and full sequentiality measures.
---

# Context is Enough: Empirical Validation of $\textit{Sequentiality}$ on Essays

## Quick Facts
- arXiv ID: 2511.09185
- Source URL: https://arxiv.org/abs/2511.09185
- Reference count: 24
- This study validates that narrative flow in essays is better captured by the contextual term of sequentiality rather than the original topic-inclusive formulation.

## Executive Summary
This study empirically validates that contextual sequentiality, measured through the Non-Local Local Coherence (NLLC) metric, better captures narrative flow in essays than traditional topic-inclusive sequentiality formulations. Using trait-annotated essay datasets (ASAP++ and ELLIPSE), the contextual term demonstrated superior alignment with human assessments of Organization and Cohesion compared to both topic-only and full sequentiality measures. The research shows that while zero-shot LLM prompting achieves higher direct trait prediction accuracy, the contextual term provides the greatest performance boost when combined with standard linguistic features.

## Method Summary
The study employed two trait-annotated essay datasets (ASAP++ and ELLIPSE) to evaluate different formulations of sequentiality for capturing narrative flow. Researchers compared three approaches: topic-only sequentiality, full sequentiality (topic-inclusive), and contextual sequentiality (NLLC). The NLLC measure focuses on sentence-to-sentence flow without explicit topic modeling. Validation was performed through correlation with human assessments of Organization and Cohesion traits, as well as through automated essay scoring performance when NLLC was combined with standard linguistic features. Zero-shot LLM prompting was also evaluated as a direct trait prediction method for comparison.

## Key Results
- Contextual term (NLLC) showed superior alignment with human assessments of Organization and Cohesion compared to both topic-only and full sequentiality measures
- Zero-shot LLM prompting achieved higher direct trait prediction accuracy than feature-based approaches
- NLLC provided the greatest performance boost when combined with standard linguistic features, outperforming both topic-only and original sequentiality formulations

## Why This Works (Mechanism)
The contextual sequentiality measure (NLLC) captures narrative flow by modeling sentence-to-sentence coherence without requiring explicit topic tracking. This approach better reflects how human readers actually assess essay organization - by following the logical progression of ideas rather than tracking topic consistency alone. By focusing on local coherence patterns that span non-adjacent sentences, NLLC captures the implicit connections that create meaningful narrative structure. This makes it more aligned with human judgment of essay quality, particularly for Organization and Cohesion traits where the flow of ideas matters more than strict topic adherence.

## Foundational Learning
- **Non-Local Local Coherence (NLLC)**: Measures sentence-to-sentence flow while ignoring topic modeling; needed because traditional coherence measures miss implicit narrative connections; quick check: compute correlation with human Organization scores
- **Sequentiality formulations**: Different approaches to measuring narrative flow; needed to understand which best captures human perception of essay quality; quick check: compare performance across topic-only, full, and contextual variants
- **Zero-shot LLM prompting**: Direct trait prediction without feature engineering; needed as benchmark for feature-based approaches; quick check: measure accuracy vs. computational cost
- **Automated Essay Scoring (AES)**: NLP systems that evaluate essay quality; needed to validate sequentiality measures in practical applications; quick check: test combined feature performance on held-out essays
- **Discourse-level analysis**: Understanding text structure beyond sentence-level; needed for capturing higher-order writing qualities; quick check: validate across multiple discourse traits
- **Human annotation correlation**: Statistical validation against expert judgments; needed to establish measure validity; quick check: compute Pearson/Spearman correlation coefficients

## Architecture Onboarding

Component map: NLLC computation -> Feature extraction -> Trait prediction -> Human correlation validation

Critical path: NLLC computation (most compute-intensive) → Feature combination → Trait prediction model training → Validation against human scores

Design tradeoffs: NLLC vs. topic modeling (accuracy vs. interpretability), feature-based vs. zero-shot LLM (efficiency vs. accuracy), computational cost vs. performance gain

Failure signatures: Poor correlation with human scores indicates NLLC implementation issues; zero performance boost suggests feature irrelevance; computational bottlenecks suggest scalability problems

First experiments:
1. Compute NLLC scores on small essay sample and manually verify logical flow capture
2. Test NLLC correlation with human Organization scores on held-out essays
3. Compare computational time of NLLC vs. topic-inclusive sequentiality on same dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on Organization and Cohesion traits within English-language datasets limits generalizability
- Zero-shot LLM prompting raises computational efficiency concerns for large-scale applications
- Reliance on specific datasets (ASAP++ and ELLIPSE) may introduce scoring rubric biases
- Findings specific to essay scoring may not transfer to other NLP tasks involving discourse analysis

## Confidence

High confidence: Empirical validation showing NLLC's superior correlation with human assessments of Organization and Cohesion traits. The comparative analysis between contextual, topic-only, and full sequentiality measures is methodologically sound.

Medium confidence: Claim that NLLC provides greatest performance boost when combined with linguistic features. While supported, interaction effects need more granular analysis.

Low confidence: Assertion that NLLC should replace topic-inclusive formulations entirely for all discourse-level NLP tasks. Findings are specific to essay scoring and require additional validation for broader claims.

## Next Checks

1. Cross-linguistic validation: Test NLLC on non-English essay datasets to evaluate generalizability across languages with different discourse structures

2. Multi-trait extension: Apply contextual sequentiality to predict other essay traits beyond Organization and Cohesion (e.g., Argumentation, Development, Conventions)

3. Computational efficiency analysis: Compare computational costs between NLLC feature-based approach and zero-shot LLM prompting across different essay lengths and volumes