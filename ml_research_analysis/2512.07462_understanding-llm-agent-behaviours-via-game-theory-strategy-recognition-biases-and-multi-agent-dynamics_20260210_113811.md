---
ver: rpa2
title: 'Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition,
  Biases and Multi-Agent Dynamics'
arxiv_id: '2512.07462'
source_url: https://arxiv.org/abs/2512.07462
tags:
- game
- agents
- cooperation
- across
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work extends the FAIRGAME framework to evaluate LLM strategic\
  \ behavior in repeated social dilemmas, focusing on both two-player Prisoner\u2019\
  s Dilemma with payoff scaling and multi-agent Public Goods Games. A core contribution\
  \ is applying supervised learning to classify LLM decision sequences into canonical\
  \ strategies (ALLC, ALLD, TFT, WSLS), revealing systematic biases across models\
  \ and languages."
---

# Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics

## Quick Facts
- arXiv ID: 2512.07462
- Source URL: https://arxiv.org/abs/2512.07462
- Reference count: 40
- Key outcome: This work extends the FAIRGAME framework to evaluate LLM strategic behavior in repeated social dilemmas, focusing on both two-player Prisoner’s Dilemma with payoff scaling and multi-agent Public Goods Games. A core contribution is applying supervised learning to classify LLM decision sequences into canonical strategies (ALLC, ALLD, TFT, WSLS), revealing systematic biases across models and languages. Results show that cooperation increases with incentive magnitude, but cross-linguistic differences are pronounced—Vietnamese prompts yield more defection, while English and Chinese favor adaptive strategies. GPT-4o and Claude exhibit distinct model-specific biases, with Claude showing strong prosocial tendencies even under selfish framing. Positional bias also emerges, with first-mentioned agents displaying more aggressive strategies. These findings highlight that LLM strategic behavior is shaped by architectural, linguistic, and cultural factors, not just explicit instructions, with direct implications for multi-agent AI governance and safety.

## Executive Summary
This paper investigates how Large Language Models (LLMs) behave in strategic multi-agent scenarios using game theory frameworks. The authors extend FAIRGAME to analyze LLM decisions in repeated Prisoner's Dilemma and multi-agent Public Goods Games, systematically identifying and classifying strategic patterns. By applying supervised learning to recognize canonical strategies (ALLC, ALLD, TFT, WSLS), they reveal that LLM behaviors are influenced not only by the payoff structures but also by linguistic and cultural contexts embedded in prompts. Their results demonstrate that cross-linguistic differences and model-specific biases significantly shape strategic outcomes, challenging assumptions about universal LLM decision-making.

## Method Summary
The authors operationalized strategy recognition through a two-stage approach. First, they generated game transcripts where LLMs played repeated Prisoner's Dilemma (10 rounds) and multi-agent Public Goods Games (5 players over 10 rounds) under different payoff conditions and language prompts (Vietnamese, English, Chinese). Second, they trained supervised classifiers to map LLM action sequences onto canonical strategies (ALLC, ALLD, TFT, WSLS) using human gameplay datasets as training data. This enabled systematic classification of LLM strategies and analysis of how factors like payoff magnitude, language, and agent positioning influenced cooperative behavior.

## Key Results
- Cooperation rates increase with payoff magnitude, but cross-linguistic differences are pronounced—Vietnamese prompts yield more defection, while English and Chinese favor adaptive strategies.
- GPT-4o and Claude exhibit distinct model-specific biases, with Claude showing strong prosocial tendencies even under selfish framing.
- Positional bias emerges, with first-mentioned agents displaying more aggressive strategies in multi-agent scenarios.

## Why This Works (Mechanism)
The study leverages the FAIRGAME framework to create controlled multi-agent environments where LLM decision-making can be systematically analyzed. By using supervised learning to classify action sequences into canonical strategies, the authors can quantify behavioral patterns that emerge from complex decision processes. The approach works because game theory provides well-defined strategic archetypes that can be recognized through sequence analysis, allowing researchers to move beyond anecdotal observations to measurable behavioral classifications.

## Foundational Learning
**Game Theory Fundamentals**: Understanding strategic interactions through payoff matrices and equilibrium concepts.
*Why needed*: Provides the theoretical foundation for analyzing LLM decision-making in competitive/cooperative scenarios.
*Quick check*: Verify that the Prisoner's Dilemma and Public Goods Game correctly capture the intended strategic tensions.

**Strategy Classification**: Mapping action sequences to canonical strategies (ALLC, ALLD, TFT, WSLS).
*Why needed*: Enables systematic comparison of LLM behaviors across different conditions.
*Quick check*: Validate that the supervised classifier accurately maps known human strategies to the correct labels.

**Multi-Agent Dynamics**: Analyzing how individual strategies aggregate into group-level outcomes.
*Why needed*: Reveals emergent behaviors that arise from strategic interactions.
*Quick check*: Confirm that cooperation rates align with theoretical predictions under different payoff structures.

**Linguistic Framing Effects**: Understanding how language and cultural context influence decision-making.
*Why needed*: Identifies non-obvious factors that shape LLM strategic behavior.
*Quick check*: Test whether strategy differences persist when controlling for prompt structure across languages.

## Architecture Onboarding
**Component Map**: Prompt Generator -> Game Simulator -> Action Logger -> Strategy Classifier -> Analysis Engine
**Critical Path**: The flow from prompt generation through strategy classification represents the core analytical pipeline. Each component must produce consistent outputs for reliable results.

**Design Tradeoffs**: The authors chose supervised learning over unsupervised methods to leverage established human strategy benchmarks, accepting potential bias from training data mismatch. They prioritized cross-linguistic comparisons over extensive single-language variations to maximize cultural coverage within limited experimental scope.

**Failure Signatures**: Misclassification of complex strategies, inconsistent behavior across similar payoff conditions, or failure to detect linguistic effects would indicate problems with the classification approach or experimental design.

**First Experiments**:
1. Test the strategy classifier on known human gameplay data to establish baseline accuracy before applying to LLM outputs.
2. Verify that payoff magnitude manipulations produce expected changes in cooperation rates across multiple runs.
3. Compare strategy distributions between LLMs and humans under identical conditions to assess behavioral alignment.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Do LLM agents develop stable long-term strategies like reputation building in repeated games exceeding 100 rounds?
- Basis in paper: [explicit] The authors state that 10-round games may be too short to capture sophisticated behaviors and propose extending interactions to over 100 rounds.
- Why unresolved: Current experiments are limited to short horizons, potentially capturing only the learning phase rather than stabilized strategic dynamics.
- What evidence would resolve it: Analysis of strategy convergence and intent recognition in FAIRGAME simulations extended to long-term interaction.

### Open Question 2
- Question: To what extent do LLM strategic behaviors align with human gameplay in identical social dilemmas?
- Basis in paper: [explicit] The authors "plan to benchmark these inferred strategies against human behavioural datasets" to assess similarities and divergences.
- Why unresolved: The study lacked parallel human experiments, limiting the ability to determine if LLM biases reflect human-like reasoning.
- What evidence would resolve it: A comparative dataset showing strategy classification distributions for humans versus LLMs under matched conditions.

### Open Question 3
- Question: How does enabling inter-agent communication influence strategic convergence and cooperation?
- Basis in paper: [explicit] Future work includes "allowing inter-agent communication to examine how environmental and linguistic factors shape LLM strategy."
- Why unresolved: Current experiments restricted agents to observing action histories without explicit messaging channels.
- What evidence would resolve it: Differential analysis of cooperation rates and coalition formation between communication-enabled and baseline scenarios.

## Limitations
- The study's primary limitation lies in the relatively small sample size of interactions per language-model pair (10 games), which may not capture the full spectrum of strategic variability.
- The choice of using only Vietnamese, English, and Chinese prompts restricts generalizability to other linguistic contexts.
- The supervised learning approach for strategy classification depends heavily on the quality and representativeness of training data, which was derived from human player strategies that may not perfectly align with LLM reasoning patterns.

## Confidence
High confidence: Findings regarding cooperation rates increasing with incentive magnitude and the systematic differences between model-specific biases (GPT-4o vs Claude) are well-supported by consistent results across multiple experimental conditions.

Medium confidence: The cross-linguistic strategy differences (Vietnamese vs English/Chinese) show clear statistical significance but may be influenced by prompt engineering nuances rather than fundamental linguistic effects. The positional bias findings are consistent but require further validation.

Low confidence: The interpretation of strategy labels in complex multi-agent scenarios may oversimplify nuanced decision-making processes, particularly in the public goods game where multiple strategic dimensions interact simultaneously.

## Next Checks
1. Replicate experiments with increased game repetitions (minimum 50 per condition) to establish more robust strategy distributions and reduce variance in classification outcomes.

2. Conduct ablation studies testing different prompt formulations within each language to isolate whether observed linguistic effects stem from cultural framing, translation quality, or syntactic differences.

3. Implement alternative strategy recognition methods, such as unsupervised clustering of decision sequences or reinforcement learning-based classification, to validate the supervised learning approach's reliability across different analytical frameworks.