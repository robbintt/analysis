---
ver: rpa2
title: Revisiting the Learning Objectives of Vision-Language Reward Models
arxiv_id: '2512.20675'
source_url: https://arxiv.org/abs/2512.20675
tags:
- reward
- view
- learning
- language
- objectives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates learning objectives for vision-language
  reward models, aiming to learn generalizable reward functions without human supervision
  by leveraging contrastive vision-language models (VLMs). The authors systematically
  compare recent learning objectives under a unified framework, controlling for model
  architecture, training data, and evaluation environments.
---

# Revisiting the Learning Objectives of Vision-Language Reward Models

## Quick Facts
- **arXiv ID:** 2512.20675
- **Source URL:** https://arxiv.org/abs/2512.20675
- **Reference count:** 8
- **Key outcome:** Simple triplet loss outperforms complex state-of-the-art methods for vision-language reward modeling on Meta-World tasks

## Executive Summary
This paper systematically compares learning objectives for vision-language reward models (VLRMs) under a unified framework, controlling for model architecture, training data, and evaluation environments. The authors evaluate several recent objectives—including triplet loss, TCN-based methods, and VIP-based methods—on Meta-World manipulation tasks. Surprisingly, a simple triplet loss achieves the highest overall accuracy across held-out tasks, suggesting that recent improvements in VLRMs may be attributed to differences in data and model architectures rather than the learning objectives themselves.

## Method Summary
The authors evaluate VLRM learning objectives using Meta-World expert demonstrations with 3 camera views per timestep. All methods use the same SigLIP2-base backbone with LoRA adapters (rank=16, alpha=32), trained on 50K samples from 132 trajectories. Five learning objectives are compared: triplet loss, TCN_text, R3M (TCN-based), VIP_text, VIP_text+VIP, and LIV (VIP-based). Models are evaluated on held-out tasks (button-press, drawer-open, door-open) using consistency accuracy with ground truth reward and Value-Order Correlation (VOC) with expert progress. Training uses random view reassignment as data augmentation, and evaluation aggregates multi-view predictions.

## Key Results
- Triplet loss achieves highest average accuracy (68.88%) compared to R3M (66.40%) and LIV (52.07%) on held-out tasks
- Triplet loss demonstrates strong correlations with expert progress, while VIP-based objectives exhibit high variance and negative correlations
- All methods show degraded performance on compositional multi-step tasks like door-open, indicating limitations for complex manipulation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Triplet loss produces more robust reward rankings than complex multi-term objectives for vision-language reward modeling.
- **Mechanism:** The triplet objective uses language embedding v as anchor, later image z_j as positive, and earlier image z_i as negative (Eq. 7). This directly encodes task progress: states closer to goal completion should be more similar to the language description than earlier states. The margin α enforces separation between positive and negative pairs, preventing trivial solutions.
- **Core assumption:** Expert trajectories exhibit monotonic progress toward the goal, such that later timesteps are systematically closer to the language-described outcome than earlier timesteps.
- **Evidence anchors:**
  - [abstract] "Remarkably, we show that a simple triplet loss outperforms state-of-the-art methods"
  - [Section 4, Table 1] Triplet achieves highest average accuracy (68.88%) vs R3M (66.40%) and LIV (52.07%)
  - [corpus] Corpus neighbors (RoboReward, SSL4RL) discuss VLM reward modeling but do not directly compare triplet vs complex objectives—insufficient external validation
- **Break condition:** If expert trajectories do not exhibit monotonic progress (e.g., tasks requiring backtracking), the ranking assumption fails. Multi-step compositional tasks like door-open show all methods degrade, suggesting this mechanism weakens for non-monotonic domains.

### Mechanism 2
- **Claim:** VIP-based objectives introduce reward inconsistencies along expert trajectories due to their goal-conditioned separation of adjacent frames.
- **Mechanism:** VIP (Eq. 3) pushes temporally adjacent frames apart while bringing distant frames together, under a goal-conditioned formulation. This creates competing gradients: the model must simultaneously increase distance between consecutive frames (which may both be high-reward) while aligning distant frames with the goal. When adapted with language (Eq. 4), this tension persists, producing high variance and negative correlations.
- **Core assumption:** Temporal distance correlates inversely with embedding similarity for goal-conditioned representations.
- **Evidence anchors:**
  - [Section 4, Table 2] VIP-text shows 27.39±24.04 VOC on button-press View 1 vs Triplet's 95.47±2.11—high variance indicates instability
  - [Section 4] "VIP based objectives exhibit high variance and frequent negative correlations even on the easier button-press"
  - [corpus] No direct corpus evidence on VIP failure modes
- **Break condition:** If task structure benefits from fine-grained temporal discrimination (e.g., subtask segmentation), VIP's adjacent-frame separation may become advantageous rather than harmful.

### Mechanism 3
- **Claim:** Multi-view training with random view reassignment promotes view-invariant reward representations.
- **Mechanism:** Each timestep is recorded from 3 camera views. During training, views are randomly reassigned per iteration, forcing the model to learn representations robust to viewpoint changes. At evaluation, multi-view predictions average similarity scores across views, reducing single-view noise.
- **Core assumption:** View-invariance improves generalization by preventing overfitting to camera-specific features.
- **Evidence anchors:**
  - [Section 3.1] "their associated views are randomly reassigned at each iteration, acting as a form of data augmentation"
  - [Section 4, Table 1] Multi-view button-press accuracy improves from 75.36% (View 1) to 76.44% for Triplet
  - [corpus] No corpus evidence on multi-view training for VLM reward models
- **Break condition:** If views have fundamentally different information content (e.g., occlusion in Figure 1), averaging may dilute signal from informative views.

## Foundational Learning

- **Contrastive Learning (Triplet, InfoNCE):**
  - Why needed here: All evaluated objectives are contrastive formulations. Understanding how positive/negative sampling shapes embedding spaces is essential for interpreting results.
  - Quick check question: Given embeddings a, p, n, what does triplet loss minimize? (Answer: max(0, S(a,n) - S(a,p) + α))

- **Vision-Language Model Architecture:**
  - Why needed here: SigLIP2 provides joint vision-text embeddings. Understanding its pretraining (contrastive on image-text pairs) explains why zero-shot rewards are possible but require finetuning.
  - Quick check question: What does the base SigLIP2 backbone achieve without finetuning? (Answer: ~46.85% accuracy—below random, confirming finetuning necessity)

- **LoRA (Low-Rank Adaptation):**
  - Why needed here: All methods use LoRA for efficient finetuning. Understanding rank and alpha parameters prevents overfitting on small demonstration datasets.
  - Quick check question: Why use LoRA instead of full finetuning? (Answer: Prevents catastrophic forgetting and overfitting when data is limited)

## Architecture Onboarding

- **Component map:**
  SigLIP2 backbone (vision encoder π_img + text encoder π_text) -> shared embedding space -> LoRA adapters -> Loss module (Triplet/TCN/VIP) -> Meta-World demonstrations -> Consistency accuracy + VOC evaluation

- **Critical path:**
  1. Load pretrained SigLIP2-base
  2. Initialize LoRA adapters on vision and text encoders
  3. Sample triplets from Meta-World demonstrations (3 views, random reassignment)
  4. Compute embeddings z_i, z_j, v
  5. Apply chosen loss (start with Triplet: max(0, S(v,z_i) - S(v,z_j) + 0.3))
  6. Backprop through LoRA parameters only
  7. Evaluate on held-out tasks using both benchmarks

- **Design tradeoffs:**
  - Triplet loss: Simple, stable, best ranking accuracy—but no explicit temporal structure encoding
  - TCN/R3M: Encodes temporal distance explicitly—slightly worse accuracy, more complex
  - VIP/LIV: Goal-conditioned with temporal separation—unstable (requires 10x lower LR), high variance
  - Assumption: This tradeoff analysis is specific to Meta-World manipulation tasks; different domains may favor different objectives

- **Failure signatures:**
  - LIV instability: If training diverges, reduce learning rate to 1e-5 (per Appendix A.1)
  - Random-chance accuracy (~50%): Model not learning temporal semantics—check data pipeline
  - High VOC variance: Indicates reward inconsistency along trajectories—likely VIP-based objective
  - All methods collapse on door-open: Multi-step compositional tasks exceed current capacity

- **First 3 experiments:**
  1. **Baseline replication:** Train Triplet loss on Meta-World with SigLIP2-base + LoRA (rank=16). Verify ~68-76% consistency accuracy on held-out tasks.
  2. **Ablation on margin α:** Test α ∈ {0.1, 0.3, 0.5, 1.0} to understand sensitivity. Paper uses 0.3 without ablation—optimal value may be task-dependent.
  3. **Data scaling:** Train with 10K, 25K, 50K samples to assess data efficiency. Current results use 50K; simpler objectives may require less data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do VIP-based objectives exhibit high variance and frequent negative correlations with expert progress, even on simple tasks like button-press?
- Basis in paper: [explicit] The authors observe that "VIP based objectives exhibit high variance and frequent negative correlations even on the easier button-press, indicating reward inconsistencies along expert trajectories."
- Why unresolved: The paper isolates learning objectives but does not analyze the underlying mechanisms causing VIP's failure mode.
- What evidence would resolve it: Ablation studies on the goal-conditioned temporal separation components of VIP, or analysis of embedding space geometry during training.

### Open Question 2
- Question: Can ranking-based learning objectives be adapted to handle multi-step compositional tasks where all current methods collapse?
- Basis in paper: [explicit] "All methods collapse on door-open, indicating a need for methods more adapted to multi-step tasks."
- Why unresolved: The door-open task requires navigation followed by manipulation, but current objectives treat timesteps as monolithic progress indicators rather than modeling subgoal structure.
- What evidence would resolve it: Experiments with hierarchical reward decomposition or temporal segment-level triplet losses on compositional tasks.

### Open Question 3
- Question: Will the superiority of simple triplet loss persist when scaling to larger, more diverse pre-training datasets beyond Meta-World?
- Basis in paper: [explicit] Future work includes "scaling to larger datasets."
- Why unresolved: The current study uses only 132 expert trajectories; complex objectives may reveal advantages with more data diversity that simpler losses cannot exploit.
- What evidence would resolve it: Controlled comparisons on Ego4D or EpicKitchens with identical backbones and dataset sizes across objectives.

## Limitations
- The study is limited to manipulation tasks in Meta-World; results may not transfer to navigation, long-horizon, or compositional tasks requiring backtracking.
- Several key implementation details are underspecified, including temporal sampling strategy, similarity function definition, γ parameter for VIP losses, and expert policy collection protocol.
- All methods show degraded performance on compositional multi-step tasks like door-open, indicating fundamental limitations for complex manipulation.

## Confidence
- **High**: Triplet loss outperforms complex objectives on Meta-World tasks (directly supported by Tables 1-2).
- **Medium**: Learning objective differences explain performance gaps (could be confounded by hyperparameter tuning or implementation details).
- **Low**: VIP objectives are inherently unstable (based on single experiment with high variance; no ablation on learning rate or margin).

## Next Checks
1. Ablate the margin α parameter for triplet loss to test sensitivity beyond the fixed value of 0.3.
2. Systematically vary the learning rate for VIP-based methods to isolate whether instability is objective- or optimization-driven.
3. Test data scaling (10K, 25K, 50K samples) to determine if simpler objectives are more data-efficient.