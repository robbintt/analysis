---
ver: rpa2
title: 'Affect, Body, Cognition, Demographics, and Emotion: The ABCDE of Text Features
  for Computational Affective Science'
arxiv_id: '2512.17752'
source_url: https://arxiv.org/abs/2512.17752
tags:
- text
- social
- features
- language
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ABCDE, a large-scale dataset of over 400 million
  text utterances annotated with features relevant to computational affective science.
  The dataset includes social media, blog, book, and AI-generated texts annotated
  for affect, emotion, body part mentions, cognition, and demographics.
---

# Affect, Body, Cognition, Demographics, and Emotion: The ABCDE of Text Features for Computational Affective Science

## Quick Facts
- arXiv ID: 2512.17752
- Source URL: https://arxiv.org/abs/2512.17752
- Reference count: 0
- This paper presents ABCDE, a large-scale dataset of over 400 million text utterances annotated with features relevant to computational affective science.

## Executive Summary
This paper presents ABCDE, a large-scale dataset of over 400 million text utterances annotated with features relevant to computational affective science. The dataset includes social media, blog, book, and AI-generated texts annotated for affect, emotion, body part mentions, cognition, and demographics. Using lexicons and curated lists, the authors annotate features like valence, arousal, dominance, emotion intensity, body part mentions, cognitive processes, and demographic information. The resource enables research in affective science, cognitive science, digital humanities, and computational linguistics. Key analyses show emotion words are most prevalent in Twitter, body part mentions are highest in Reddit, and cognition terms are more common in human-authored sources. Demographic findings reveal Reddit users are younger than Twitter users, and occupation distributions differ by platform. The dataset serves as a standardized starting point for interdisciplinary research, though it has limitations including English-only content and potential biases in lexicon construction.

## Method Summary
ABCDE uses lexicon-based word matching to produce aggregate-level measurements of affective and cognitive constructs across 400M+ English text instances. Five feature categories are annotated: affect (VAD dimensions), emotion (8 categories plus discrete emotion intensity), body (292 body part mentions), cognition (11 categories from Bloom's taxonomy), and demographics (age, occupation, gender, religion extracted via regex). The dataset spans Twitter, Reddit, Books, Blogs, and AI-generated texts. Each instance receives lexicon scores (averaged intensities), binary presence flags, and counts. The approach trades instance-level accuracy for interpretability and scale, explicitly positioning the resource for population-level rather than individual prediction.

## Key Results
- Twitter shows highest emotion word usage (~1.4% joy tokens) while AI-generated text uses explicit emotion words rarely (~0.01%)
- Reddit (8.16%) and Blogs (4.47%) contain the most possessive body part mentions; "my head" dominates Reddit, "my heart" appears more in Blogs
- Reddit users are younger than Twitter users, and occupation distributions differ by platform
- The body-to-cognition ratio in English fiction increased from ~5% (1800) to ~77% (2000s), suggesting a long-term rise in embodied language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexicon-based word matching produces valid aggregate-level measurements of affective and cognitive constructs across large corpora.
- Mechanism: Pre-compiled word lists with intensity scores map individual tokens to psychological dimensions (e.g., valence, arousal). When aggregated across millions of instances, random errors cancel while systematic signals persist, yielding reliable population-level estimates.
- Core assumption: Word-to-construct mappings remain stable across contexts and time periods within the same language.
- Evidence anchors:
  - [section 3.3]: "While lexicon-based methods are not as accurate as neural models at instance-level estimation... they are comparable and sufficient when used as tools of aggregate-level analysis."
  - [section 3.3]: Cites Teodorescu and Mohammad (2023) showing emotion arcs from lexicons correlate >0.9 with ground-truth arcs when hyperparameters are appropriately set.
  - [corpus]: Weak direct evidence in neighbors; related work focuses on other modalities (speech, vision-language models) rather than validating lexicon approaches.
- Break condition: Domain-specific vocabulary shifts (slang, technical jargon) cause systematic misclassification; temporal drift renders static lexicons obsolete for longitudinal analysis.

### Mechanism 2
- Claim: Self-disclosure pattern extraction via regex captures demographic metadata with precision but unknown recall.
- Mechanism: Regular expressions match structured self-revelation statements (e.g., "I am a [occupation]"). Once extracted, attributes propagate to all instances from the same user, creating longitudinal user profiles.
- Core assumption: Users who self-disclose are representative of the broader population on that platform, and disclosures are truthful.
- Evidence anchors:
  - [section 3.2]: "We extract demographic attributes... using regular expression matching and structured dictionary lookups."
  - [section 4, Q6]: Authors explicitly note disclosed posts reflect "rates among disclosed posts, not population estimates"—absence should be treated as missing, not negative.
  - [corpus]: No neighbor papers validate regex-based demographic extraction accuracy.
- Break condition: Systematic self-disclosure bias (certain groups more likely to reveal age, occupation, or religion) produces non-representative demographic distributions.

### Mechanism 3
- Claim: Cross-source comparison reveals domain-specific linguistic signatures tied to communicative purpose and audience.
- Mechanism: Platform affordances and social norms shape language use. Twitter's character limits and public broadcast nature increases emotion word density; Reddit's pseudonymous discussion forums enable more personal body part mentions and cognitive elaboration.
- Core assumption: Observed differences reflect genuine linguistic variation, not sampling artifacts or annotation biases.
- Evidence anchors:
  - [section 4, Q1]: Twitter shows highest emotion word usage (~1.4% joy tokens); AI-generated text uses explicit emotion words rarely (~0.01%).
  - [section 4, Q2]: Reddit (8.16%) and Blogs (4.47%) contain the most possessive body part mentions; "my head" dominates Reddit, "my heart" appears more in Blogs.
  - [corpus]: Neighbor paper "The Language of Interoception" (Wu et al., 2025) investigates body-emotion connections, supporting embodied cognition framework but not platform differences.
- Break condition: Confounding variables (topic distribution, user overlap between platforms, temporal coverage gaps) drive apparent differences rather than platform-intrinsic properties.

## Foundational Learning

- Concept: **Lexicon vs. learned representations**
  - Why needed here: ABCDE uses static word lists rather than neural embeddings. Understanding trade-offs is essential for selecting appropriate downstream methods.
  - Quick check question: Would a BERT-based emotion classifier give better instance-level predictions than NRC Emotion Intensity lexicon averaging? (Answer: Yes for single-utterance prediction; lexicons preferred for interpretability and aggregate trend analysis.)

- Concept: **VAD (Valence-Arousal-Dominance) dimensional emotion model**
  - Why needed here: Affect features use this three-dimensional continuous space rather than discrete emotion categories. Interpreting distributions requires understanding what each dimension captures.
  - Quick check question: A tweet scores high arousal (0.85) and low valence (0.25)—what emotional state might this indicate? (Answer: Anger or fear—high activation with negative feeling.)

- Concept: **Aggregate-level vs. instance-level inference**
  - Why needed here: The paper explicitly positions ABCDE for population-level analysis, not individual prediction. Misapplying features to single-instance classification will underperform.
  - Quick check question: You want to detect whether one specific user is depressed from their last tweet. Is ABCDE's lexicon-based sadness score sufficient? (Answer: No—use clinical instruments or trained ML models; aggregate patterns don't transfer to individual diagnosis.)

## Architecture Onboarding

- Component map:
  Data Sources (5) -> Feature Extractors (5) -> Output Schema
  Twitter, Reddit, Books, Blogs, AI-generated text -> Affect (VAD), Emotion (8+ cat), Body (292 terms), Cognition (11 cat), Demographics (6) -> Instance-level - avg scores, - binary flags, - counts, - lists (BPMs)

- Critical path:
  1. Load target subset (e.g., reddit_posts parquet from HuggingFace)
  2. Identify relevant features for your research question (136 available)
  3. Apply length normalization when comparing across sources
  4. Filter to instances with non-missing values for demographic stratification
  5. Aggregate by time window, user, or source before statistical analysis

- Design tradeoffs:
  - Lexicon simplicity vs. contextual accuracy: Word-sense disambiguation absent—"bank" counts toward financial terms regardless of river context. Trade accepted for scale and interpretability.
  - English-only scope: 400M+ English instances but zero multilingual coverage. Limits cross-cultural research; authors explicitly acknowledge this gap.
  - Static vs. temporal lexicons: NRC VAD scores fixed; doesn't capture semantic shift over 200+ years of book data. Body-to-cognition ratio trend (5%→77%) may partially reflect vocabulary evolution, not purely behavioral change.

- Failure signatures:
  - Sparse demographic disclosures: Age available for only ~5-10% of posts (visual estimate from Figure 7). Analyses requiring demographic stratification will have high variance.
  - AI-generated text anomaly: Near-zero emotion word rates (~0.01%) may indicate measurement failure rather than genuine property—LLMs trained on human text should reflect some distribution. Investigate whether instruction-tuning suppresses emotional expression.
  - Platform confounds: Books data is 5-grams (not full sentences); Twitter geolocation bias toward mobile/urban users. Direct platform comparisons require covariate adjustment.

- First 3 experiments:
  1. Reproduce one Q1-Q8 finding with a different time slice. If Twitter emotion density was highest in 2015-2021, test whether this holds for 2022-2024 or reverses.
  2. Validate lexicon against ML baseline on a held-out emotion-labeled dataset (e.g., GoEmotions). Quantify the correlation gap between NRC averages and RoBERTa predictions at instance vs. aggregate levels.
  3. Cross-feature correlation analysis: Test whether body part mentions correlate with low valence (embodied distress hypothesis) or with high arousal (somatic activation). Use Reddit subset where both features are prevalent.

## Open Questions the Paper Calls Out

- **Cross-linguistic validation**: How do affect, emotion, and cognition linguistic features manifest across non-English languages and non-North American populations? The authors note "English is the only language represented" and data "originate from North American users, and therefore is not representative of English-speaking populations from other countries, regions, and cultures."

- **ML vs. lexicon comparison**: How do lexicon-based measurements compare to ML-based instance-level estimations across the full range of ABCDE feature categories? The authors state ABCDE "can in the future be expanded to annotate emotion and affect features using highly-accurate instance-level estimation models."

- **AI text cognition anomaly**: Why do AI-generated texts substantially under-express cognition terms compared to human-authored sources? Results show AI text under-expresses cognition across categories (problem solving ~0.09%, explanation ~0.20%) versus human sources (Blogs: 5.68% understanding), but no explanation is investigated.

- **Historical embodied language rise**: What sociocultural factors explain the rise in body-to-cognition ratio from ~5% (1800) to ~77% (2000s) in English fiction? The authors present this finding as proof-of-concept without further investigation.

## Limitations

- Lexicon-based approach introduces systematic errors in polysemous contexts and cannot capture semantic drift across 200+ years of book data.
- Demographic disclosures represent only 5-10% of posts, limiting population inference and introducing self-disclosure bias.
- English-only scope prevents cross-linguistic validation of emotion-to-body mappings and limits cross-cultural research.

## Confidence

- High confidence in aggregate-level cross-source comparisons (Q1-Q5) due to consistent patterns across millions of instances.
- Medium confidence in demographic inferences because of self-disclosure bias and sparse coverage.
- Low confidence in fine-grained instance-level predictions, as explicitly noted in the paper's positioning for population rather than individual analysis.

## Next Checks

1. Replicate the emotion density finding (Twitter highest at ~1.4%) using 2022-2024 data to test temporal stability of platform-specific signatures.
2. Validate NRC lexicon predictions against a neural baseline (e.g., RoBERTa trained on GoEmotions) on a held-out test set to quantify instance-level correlation gaps.
3. Test the embodied distress hypothesis by examining whether body part mentions correlate positively with low valence scores specifically in Reddit posts where both features are prevalent.