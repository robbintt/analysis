---
ver: rpa2
title: 'Dr.LLM: Dynamic Layer Routing in LLMs'
arxiv_id: '2510.12773'
source_url: https://arxiv.org/abs/2510.12773
tags:
- accuracy
- layers
- routing
- routers
- router
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dr.LLM introduces lightweight per-layer routers that decide to
  skip, execute, or repeat each transformer block in a frozen pretrained LLM. The
  routers are trained on explicit supervision derived from Monte Carlo Tree Search
  (MCTS), which finds accuracy-preserving or improving layer configurations under
  a compute budget.
---

# Dr.LLM: Dynamic LayerRouting in LLMs
## Quick Facts
- arXiv ID: 2510.12773
- Source URL: https://arxiv.org/abs/2510.12773
- Reference count: 37
- Dynamic routing of transformer blocks yields up to 4.0 percentage points accuracy gain and up to 11 layer savings on reasoning tasks

## Executive Summary
Dr.LLM introduces a dynamic layer routing mechanism that allows lightweight per-layer routers to decide whether to skip, execute, or repeat each transformer block in a frozen pretrained LLM. The routers are trained using explicit supervision from Monte Carlo Tree Search (MCTS), which identifies accuracy-preserving or improving layer configurations under compute constraints. This approach enables efficient, accuracy-driven inference without modifying base model weights, achieving significant improvements on reasoning tasks and strong generalization to out-of-domain benchmarks.

## Method Summary
Dr.LLM employs per-layer routers that use windowed mean pooling over hidden states, followed by compact two-layer MLPs to make routing decisions. MCTS generates explicit supervision by exploring possible layer sequences under compute budgets, identifying those that maintain or improve accuracy. Focal loss with class balancing handles severe class imbalance during router training. The method is tunable via a single scalar parameter, allowing fine-grained accuracy–efficiency trade-offs. Evaluation shows up to 4.0 percentage points accuracy improvement and up to 11 layer savings on ARC and DART reasoning tasks, with only 0.85% average accuracy drop on out-of-domain benchmarks.

## Key Results
- Up to 4.0 percentage points accuracy improvement on reasoning tasks (ARC, DART)
- Up to 11 layers saved per example while improving accuracy
- Out-of-domain generalization with only 0.85% average accuracy drop
- Outperforms prior routing methods by up to 7.7 percentage points

## Why This Works (Mechanism)
Dr.LLM leverages explicit supervision from MCTS to train routers that make informed decisions about layer execution. By focusing on accuracy preservation or improvement under compute budgets, the routers learn to skip redundant layers, execute critical ones, and repeat layers when beneficial. Windowed mean pooling over hidden states provides efficient state representation, while focal loss and class balancing address training instability due to severe class imbalance. The approach enables dynamic, example-specific routing that adapts to task complexity, achieving better accuracy-efficiency trade-offs than static or search-based methods.

## Foundational Learning
- **Monte Carlo Tree Search (MCTS)**: Needed for generating accurate, compute-aware layer configurations as training targets. Quick check: Verify MCTS trajectories consistently improve accuracy under budget constraints on a validation set.
- **Windowed mean pooling**: Needed to efficiently aggregate hidden states for routing decisions without excessive computation. Quick check: Compare pooling methods (mean, attention, hierarchical) for routing accuracy vs. efficiency.
- **Focal loss with class balancing**: Needed to stabilize training given severe class imbalance (most layers skipped). Quick check: Monitor router loss convergence and routing accuracy with and without focal loss.
- **Two-layer MLP routers**: Needed for lightweight, efficient routing decisions. Quick check: Benchmark router inference overhead against baseline execution.

## Architecture Onboarding
- **Component map**: Input tokens -> Frozen LLM blocks -> Per-layer routers (mean pooling + MLP) -> Skip/Execute/Repeat decision -> Layer execution
- **Critical path**: Token input → Hidden state aggregation → Router decision → Layer execution → Output
- **Design tradeoffs**: Lightweight routers (2-layer MLP, mean pooling) vs. richer representations (attention, hierarchical); explicit MCTS supervision vs. online search; frozen base model vs. fine-tuning
- **Failure signatures**: Routers consistently skip critical layers on complex reasoning; severe class imbalance causes unstable training; mean pooling misses long-range dependencies
- **First experiments**: 1) Ablate pooling methods (mean vs. attention) on validation accuracy; 2) Test focal loss impact on training stability; 3) Evaluate router decisions on held-out reasoning examples

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability of MCTS supervision to larger datasets and models is uncertain
- Windowed mean pooling may introduce information bottleneck; richer aggregation methods not explored
- Generalization claims based on limited out-of-domain benchmarks; untested on long sequences, multimodal, or persistent memory tasks

## Confidence
- MCTS supervision scalability: Medium
- Information bottleneck from mean pooling: Medium
- Out-of-domain generalization robustness: Medium

## Next Checks
1. Scale MCTS supervision to 200,000 examples and evaluate routing accuracy and efficiency degradation or improvement
2. Compare windowed mean pooling against attention-based or hierarchical state aggregation on a held-out validation set
3. Test Dr.LLM on 8K+ token sequences and multimodal tasks to identify failure modes in non-standard inference scenarios