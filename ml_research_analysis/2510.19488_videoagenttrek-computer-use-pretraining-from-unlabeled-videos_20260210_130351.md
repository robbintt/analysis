---
ver: rpa2
title: 'VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos'
arxiv_id: '2510.19488'
source_url: https://arxiv.org/abs/2510.19488
tags:
- action
- training
- videos
- video
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VideoAgentTrek introduces a scalable pipeline that automatically
  extracts computer-use training data from unlabeled screen-recorded videos, addressing
  the bottleneck of expensive manual annotation. The approach employs Video2Action,
  an inverse dynamics module that combines action event detection with precise temporal
  localization and action parameterization to reconstruct GUI interactions (clicks,
  typing, scrolling) from raw video.
---

# VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos

## Quick Facts
- arXiv ID: 2510.19488
- Source URL: https://arxiv.org/abs/2510.19488
- Authors: Dunjie Lu; Yiheng Xu; Junli Wang; Haoyuan Wu; Xinyuan Wang; Zekun Wang; Junyang Lin; Binyuan Hui; Tao Yu
- Reference count: 25
- One-line primary result: Automatic video mining pipeline that extracts 1.52M GUI interaction steps from YouTube tutorials, enabling 15.8% task success on OSWorld-Verified (70% relative improvement over SFT-only baseline)

## Executive Summary
VideoAgentTrek introduces a scalable pipeline that automatically extracts computer-use training data from unlabeled screen-recorded videos, addressing the bottleneck of expensive manual annotation. The approach employs Video2Action, an inverse dynamics module that combines action event detection with precise temporal localization and action parameterization to reconstruct GUI interactions (clicks, typing, scrolling) from raw video. Applied to 39,000 YouTube tutorial videos, the pipeline generated 1.52 million interaction steps covering diverse applications across Windows, macOS, and web platforms. This video-derived data enabled continued pretraining followed by supervised fine-tuning, achieving 15.8% task success on OSWorld-Verified (70% relative improvement over SFT-only baseline) and 69.3% step accuracy on AgentNetBench (5.2 points higher than baseline). The results demonstrate that passive internet videos can provide effective supervision at scale, offering a cost-effective alternative to manual annotation for training robust computer-use agents.

## Method Summary
VideoAgentTrek presents a two-stage training pipeline that leverages automatically mined trajectories from unlabeled YouTube tutorial videos. The core innovation is Video2Action, an inverse dynamics module that detects GUI actions in video and extracts structured parameters like click coordinates and typed text. The pipeline processes 39,000 YouTube videos through SCREENFILTER (cursor detection), action event detection, and action parameterization to generate 1.52 million ReAct-formatted interaction steps. Training proceeds in two stages: Stage 1 continues pretraining on 26B tokens of video-derived data to establish grounding patterns, while Stage 2 performs supervised fine-tuning on 8B tokens of clean human-annotated trajectories. The combined approach achieves strong performance on computer-use benchmarks while demonstrating effective test-time scaling.

## Key Results
- Achieved 15.8% task success on OSWorld-Verified, representing 70% relative improvement over SFT-only baseline (9.3%)
- Demonstrated first computer-use agent with effective test-time scaling: improved from 14.13% to 15.78% when step budget increased from 20 to 50
- Achieved 69.3% step accuracy on AgentNetBench, 5.2 points higher than baseline
- Successfully extracted 1.52 million interaction steps from 39,000 YouTube videos covering diverse applications

## Why This Works (Mechanism)

### Mechanism 1: Inverse Dynamics Converts Implicit Video Signals to Explicit Action Labels
The Video2Action module performs inverse dynamics inference in two stages: (1) a video grounding model detects action events and their temporal boundaries by learning to predict (action_type, start_time, end_time) tuples from raw video clips; (2) an action-content recognizer extracts structured parameters (e.g., click coordinates, typed text) from localized segments by mapping visual changes to action parameters. This converts passive recordings into training-ready (screenshot, action, parameters) tuples. Core assumption: Visual pixel changes in GUI footage reliably correlate with underlying action parameters and these correlations can be learned from instrumented demonstration data and transferred to in-the-wild videos.

### Mechanism 2: Long-Horizon Video Pretraining Enables Test-Time Scaling
VideoAgentTrek trajectories average 39.25 steps, with 42.1% exceeding 20 steps and 14.5% containing 50+ steps. Pretraining on these long sequences allows the model to learn subgoal decomposition and error recovery strategies that shorter SFT trajectories cannot provide. At test time, when the step budget increases from 20 to 50, the pretrained agent improves from 14.13% to 15.78% task success, while the SFT-only baseline remains flat at 9.3%. Core assumption: Long-horizon planning capabilities learned from video demonstrations transfer to interactive agent execution, and the model can recognize when to use additional steps for exploration versus when to terminate.

### Mechanism 3: Two-Stage Training Separates Perception Grounding from Policy Learning
Stage 1 trains on 26B tokens of automatically mined trajectories (inevitably containing residual noise) formatted as interleaved vision-text sequences, teaching the model fundamental GUI perception and grounding patterns. Stage 2 continues training on 8B tokens of clean human-annotated trajectories reformatted as chat templates, sharpening task-specific policy execution. This separation prevents noisy labels from corrupting final policy while still benefiting from scale. Core assumption: The noise in automatically mined trajectories primarily affects policy correctness rather than perceptual grounding, and the model can extract useful grounding signals from imperfect labels.

## Foundational Learning

- **Concept: Inverse Dynamics Models (IDM)**
  - **Why needed here**: The core Video2Action module is an IDM that infers actions from state observations. Understanding that forward dynamics predict next state from (state, action), while inverse dynamics predict action from (state, next_state), is essential for grasping how the pipeline extracts training signals from video.
  - **Quick check question**: Given a screenshot before a click and a screenshot after, what does an inverse dynamics model predict—the click coordinates or the resulting UI state?

- **Concept: Temporal Grounding in Video**
  - **Why needed here**: The action event detection component must localize GUI actions with millisecond precision (tight start/end timestamps) from continuous video streams. This differs from frame-level classification and requires understanding temporal boundaries.
  - **Quick check question**: Why might detecting a "type" action require different temporal precision than detecting a "click" action, and how would this affect training data quality?

- **Concept: ReAct-Style Trajectories**
  - **Why needed here**: The paper formats training data as ReAct tuples (thought, action, parameters) with inner monologues (r_k) that make intent explicit. Understanding this interleaved reasoning-action format is critical for data preparation and model training.
  - **Quick check question**: In a ReAct trajectory, should the "thought" component describe what just happened or what the agent plans to do next, and how does this affect credit assignment during training?

## Architecture Onboarding

- **Component map**: Video collection -> SCREENFILTER cursor detection -> Action event detection (temporal grounding) -> Action parameterization -> Inner monologue generation -> Trajectory assembly -> Stage 1 pretraining -> Stage 2 SFT

- **Critical path**: The action event detection accuracy (70% recall, Table 1) directly limits downstream data volume and quality. Each component must successfully process video before the next can operate.

- **Design tradeoffs**:
  - **Recall vs. precision in filtering**: SCREENFILTER optimizes for recall (channel-based expansion accepts entire channels if ≥80% of samples pass), accepting false positives that downstream modules must handle
  - **Temporal granularity vs. computational cost**: Event detector uses 4 fps and 10-second clips (vs. native video framerate), trading temporal precision for training efficiency
  - **Scale vs. noise**: Stage 1 uses 26B tokens of automatically mined data knowing it contains residual noise, betting that scale and diversity outweigh label noise for grounding

- **Failure signatures**:
  - **Low recall on keyboard actions**: Press actions show 8% recall (Table 1) due to subtle visual evidence—expect underrepresentation of hotkey/shortcut patterns in final agent behavior
  - **Flat test-time scaling**: If SFT-only baseline shows improvement with more steps but pretrained model doesn't, indicates video trajectories taught incorrect planning patterns
  - **Coordinate drift**: Action parameterization trained without ground-truth bounding boxes (Section 2.2.2) may produce systematically offset click coordinates for certain UI layouts

- **First 3 experiments**:
  1. **Validate SCREENFILTER on your target video sources**: Run cursor detection on a held-out sample of your intended video corpus (e.g., 100 random videos); manually verify that ≥80% of detected cursor-containing segments actually show GUI interactions. If precision drops below 85%, retrain with domain-specific cursor patterns.
  2. **Benchmark action event detector on in-the-wild data**: Apply the dense event detector to 10 unseen tutorial videos not in the training distribution; perform manual annotation of ground-truth action timestamps; compute temporal IoU at 0.5 threshold. If mAP drops more than 15 points from the reported 0.78 F1, investigate domain shift in video characteristics (resolution, UI density, action speed).
  3. **Ablate stage-1 data mixing ratios**: Train three model variants with 0%, 25%, and 50% stage-1 video tokens (keeping stage-2 constant); evaluate on a held-out task subset from both OSWorld-Verified and AgentNetBench. If the curve is non-monotonic (25% > 50%), investigate whether noise accumulation dominates after a data scale threshold.

## Open Questions the Paper Calls Out

- **Question**: How can the Video2Action module be improved to accurately reconstruct subtle GUI actions, specifically "Press" and "Drag," which currently exhibit significantly lower recall (0.08) and accuracy (0.36) compared to pointer-based actions like "Click"?
- **Question**: Does the interaction knowledge derived from desktop tutorial videos transfer effectively to mobile platforms where paradigms like touch gestures replace mouse and keyboard inputs?
- **Question**: To what extent does the factual accuracy of the synthetically generated "inner monologue" influence the final agent's reasoning capabilities, and does noisy reasoning data degrade the benefits of test-time scaling?

## Limitations

- The Video2Action module shows significantly lower performance on keyboard actions (Press at 8% recall, Drag at 52% recall), potentially limiting applicability to expert workflows
- The method's robustness to video quality, UI style, and domain shifts is not extensively validated beyond the reported YouTube corpus
- Dependence on GPT-5 Medium for inner monologue generation creates a reproducibility barrier, as this model is not publicly available

## Confidence

- **High**: The two-stage training design, the general feasibility of video-based pretraining, and the observed test-time scaling effect are well-supported by ablation studies and clear empirical trends
- **Medium**: The Video2Action module's accuracy and the overall contribution of stage-1 data to agent performance are demonstrated, but generalizability and noise tolerance require further validation
- **Low**: Claims about real-world applicability, robustness to distribution shifts, and the impact of missing action types (e.g., keyboard shortcuts) are not fully addressed

## Next Checks

1. **Generalizability of IDM Transfer**: Apply the trained Video2Action models to a held-out set of videos from a different source (e.g., non-English tutorials or screencasts from different OSes) and measure action detection and parameterization accuracy. Investigate whether performance drops are due to domain shift or inherent limitations of the IDM approach.

2. **Impact of Action Type Coverage**: Train a variant of the agent with artificially balanced action types (e.g., oversampling underrepresented keyboard actions) and evaluate on tasks that require diverse input modalities. Compare performance to the original model to quantify the impact of missing action types on real-world utility.

3. **Robustness to Video Quality and UI Style**: Create a synthetic benchmark with videos at varying resolutions, framerates, and UI styles (e.g., dark mode, custom themes). Apply the Video2Action pipeline and measure how detection and parameterization accuracy degrade. Identify thresholds for minimum viable video quality.