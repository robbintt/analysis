---
ver: rpa2
title: Decoding inner speech with an end-to-end brain-to-text neural interface
arxiv_id: '2511.21740'
source_url: https://arxiv.org/abs/2511.21740
tags:
- neural
- speech
- decoding
- data
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an end-to-end Brain-to-Text (BIT) framework
  that translates neural activity directly into coherent sentences using a single
  differentiable neural network. Central to the approach is a cross-task, cross-species
  pretrained transformer neural encoder, whose representations transfer to both attempted
  and imagined speech.
---

# Decoding inner speech with an end-to-end brain-to-text neural interface

## Quick Facts
- **arXiv ID**: 2511.21740
- **Source URL**: https://arxiv.org/abs/2511.21740
- **Reference count**: 38
- **Primary result**: End-to-end brain-to-text system achieves 10.22% WER using cross-modal contrastive learning and pretrained transformer encoder.

## Executive Summary
This work introduces an end-to-end Brain-to-Text (BIT) framework that translates neural activity directly into coherent sentences using a single differentiable neural network. The approach leverages a cross-task, cross-species pretrained transformer neural encoder whose representations transfer to both attempted and imagined speech. When integrated with audio large language models and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate from 24.69% to 10.22% on established benchmarks. The results demonstrate that small-scale audio LLMs markedly improve end-to-end decoding and that BIT's neural embeddings for attempted and imagined speech align through shared semantic structure, enabling cross-task generalization.

## Method Summary
The method centers on a pretrained transformer neural encoder that learns universal neural representations across tasks and species. This encoder is then integrated into an end-to-end architecture with audio LLMs, trained using contrastive learning to align neural and audio embeddings. The system is evaluated in both cascaded (encoder + n-gram LM) and end-to-end (encoder + audio LLM) settings, demonstrating improved performance on brain-to-text benchmarks. The cross-modal alignment and shared semantic structure enable effective transfer between attempted and imagined speech decoding tasks.

## Key Results
- Establishes new state-of-the-art on Brain-to-Text '24 and '25 benchmarks using cascaded encoder + n-gram LM
- Reduces WER from 24.69% to 10.22% when integrating encoder with audio LLMs end-to-end
- Demonstrates cross-task generalization between attempted and imagined speech through shared neural embeddings

## Why This Works (Mechanism)
The pretrained transformer encoder learns rich, transferable representations that capture the underlying structure of neural speech signals across different recording contexts. By aligning these neural embeddings with audio embeddings through contrastive learning, the system bridges the gap between brain activity and natural language. The shared semantic structure across attempted and imagined speech tasks allows for effective cross-task generalization, while the integration with small-scale audio LLMs provides robust language modeling for end-to-end decoding.

## Foundational Learning
- **Pretrained transformer neural encoder**: Why needed - captures universal neural representations across tasks/species; Quick check - verify cross-task transfer performance on held-out data
- **Cross-modal contrastive learning**: Why needed - aligns neural and audio embeddings for end-to-end decoding; Quick check - measure alignment quality via retrieval metrics
- **Cross-task generalization**: Why needed - enables decoding of both attempted and imagined speech; Quick check - test on imagined speech after training on attempted speech

## Architecture Onboarding
- **Component map**: Neural recordings -> Pretrained transformer encoder -> Contrastive alignment layer -> Audio LLM -> Text output
- **Critical path**: Neural encoding → cross-modal alignment → language modeling → text generation
- **Design tradeoffs**: Small-scale audio LLMs vs. larger models for efficiency vs. performance; pretraining data diversity vs. task specificity
- **Failure signatures**: Poor cross-modal alignment leads to high WER; insufficient pretraining diversity limits cross-task generalization
- **First experiments**: 1) Validate pretraining transfer on cross-task decoding; 2) Test contrastive alignment quality via retrieval; 3) Evaluate end-to-end WER on independent dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains are benchmark-specific and may not generalize to all datasets or recording modalities
- Cross-task generalization relies on small-scale audio LLMs, limiting scalability
- Assumes shared semantic structure between attempted and imagined speech, which may not hold universally

## Confidence
- **High**: Pretrained transformer encoder, contrastive learning methodology
- **Medium**: End-to-end performance improvements on controlled benchmarks
- **Low**: Cross-task generalization claims without broader validation

## Next Checks
1. Replicate end-to-end decoding on independent, multi-site intracranial EEG dataset
2. Evaluate cross-task generalization by training on attempted speech and testing on imagined speech from new subjects
3. Compare performance with state-of-the-art, larger-scale language models