---
ver: rpa2
title: 'FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing
  MCTS'
arxiv_id: '2505.16409'
source_url: https://arxiv.org/abs/2505.16409
tags:
- search
- reasoning
- retrieval
- zhang
- freeson
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FREESON (Retriever-FREE Retrieval-Augmented ReaSONing) is a novel
  framework that enables Large Reasoning Models to perform both reasoning and retrieval
  without a separate retrieval model. The core method introduces CT-MCTS (Corpus-Traversing
  Monte Carlo Tree Search), which allows LRMs to traverse the corpus token-by-token
  toward answer-containing regions.
---

# FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS

## Quick Facts
- arXiv ID: 2505.16409
- Source URL: https://arxiv.org/abs/2505.16409
- Authors: Chaeeun Kim; Seungone Kim
- Reference count: 40
- Primary result: FREESON achieves 14.4% average improvement in EM and F1 over four multi-step reasoning models with separate retrievers on five open-domain QA benchmarks.

## Executive Summary
FREESON (Retriever-FREE Retrieval-Augmented ReaSONing) is a novel framework that enables Large Reasoning Models to perform both reasoning and retrieval without a separate retrieval model. The core method introduces CT-MCTS (Corpus-Traversing Monte Carlo Tree Search), which allows LRMs to traverse the corpus token-by-token toward answer-containing regions. The approach uses a prefix-based CorpusTree index and incorporates granularity-aware multi-node expansion with stochastic beam search, combined with on-policy value network training from CT-MCTS rollouts.

## Method Summary
FREESON eliminates the need for a separate retriever by using a single LRM as both generator and retriever. The CorpusTree (FM-Index based on BWT) constrains token generation to valid corpus sequences. CT-MCTS uses UCT selection, multi-node expansion with G=6 tokens per node and M=2 expansions, and on-policy value network training from ~15,000 rollout-label pairs. The framework is evaluated on five open-domain QA benchmarks using Exact Match (EM) and F1 scores.

## Key Results
- FREESON achieves 14.4% average improvement in EM and F1 over four multi-step reasoning models with separate retrievers
- Outperforms strongest baseline Search-R1 by 3% on PopQA and 2WikiMultihopQA
- On-policy value network training achieves avg EM=0.50, F1=0.56 vs. synthetic EM=0.47, F1=0.54

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Eliminating the separate retriever reduces representation bottleneck errors where embedding spaces fail to capture subtle document distinctions needed by the generator.
- **Mechanism:** FREESON uses a single LRM as both generator and retriever, shifting from sequence-to-sequence matching to locating answer-containing paths via CT-MCTS. The CorpusTree (FM-Index based on BWT) constrains token generation to valid corpus sequences.
- **Core assumption:** The LRM's reasoning over document structure can guide retrieval more precisely than fixed embedding similarity.
- **Evidence anchors:**
  - [abstract] "representation bottleneck, a phenomenon where the retriever's embedding space is not expressive enough to meet the generator's requirements"
  - [section 1] Example where E5 retriever fails on "John Tuchet, 6th Baron Audley" vs "8th Baron Audley" due to single-character difference
  - [corpus] Related work Search-o1 (FMR=0.52) and MCTS-RAG also explore reasoning-guided retrieval, but FREESON uniquely eliminates the retriever entirely
- **Break condition:** If corpus is extremely large (billion+ documents), the FM-Index prefix constraint may become computationally prohibitive; constrained decoding may miss valid paths if corpus has sparse coverage of query-relevant terminology.

### Mechanism 2
- **Claim:** Flexible node granularity (G=6 tokens per node) improves both search efficiency and semantic coherence compared to single-token MCTS.
- **Mechanism:** Each node represents a token sequence (length G), allowing semantically meaningful decisions while preserving token-level constraints. Stochastic beam search samples top-k tokens, then iteratively extends to G tokens while maintaining top-M paths.
- **Core assumption:** Multi-token nodes capture coherent semantic units that single tokens cannot, enabling faster convergence.
- **Evidence anchors:**
  - [section 4.2] "moderately coarse granularity (G = 6)... achieved the best performance (F1 ↑)" with latency dropping from 67s (G=1) to 28s (G=6)
  - [section 2.2] "stochastic beam search decoding... iteratively extend the sequence by sampling tokens until either a predefined per-node token limit G is reached"
  - [corpus] No direct corpus comparison found for variable-granularity retrieval MCTS
- **Break condition:** Overly coarse granularity (G≥10) degrades performance by limiting fine-grained exploration; too fine granularity (G=1-2) increases latency and yields shorter, less informative paths.

### Mechanism 3
- **Claim:** On-policy value network trained on CT-MCTS rollouts outperforms synthetic off-policy training by aligning value estimates with actual inference behavior.
- **Mechanism:** During expansion, rollouts use greedy decoding constrained by CorpusTree; resulting paths are scored by comparing generated answers to ground-truth (1.0 full match, 0.8 partial, 0.0 none). Value network (sigmoid over pooled hidden states) is trained with BCE loss on ~15,000 rollout-label pairs.
- **Core assumption:** Value estimates trained in the true CT-MCTS environment generalize better than those trained on LLM-generated synthetic paths.
- **Evidence anchors:**
  - [section 4.4] On-policy achieves avg EM=0.50, F1=0.56 vs. synthetic EM=0.47, F1=0.54
  - [section 2.3] "value model is trained directly on the actions the system would take during real search"
  - [corpus] Weak comparative evidence—no corpus papers directly compare on-policy vs. off-policy value training for retrieval MCTS
- **Break condition:** If ground-truth annotations in training data contain systematic biases (e.g., toward embedding-retriever phrasing), value network may learn misaligned preferences.

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS) with UCT**
  - **Why needed here:** CT-MCTS uses UCT selection formula balancing exploitation (Q-value) and exploration (visit-count bonus). Understanding this is essential to debug why certain paths are prioritized.
  - **Quick check question:** Given node Q(s,a)=0.6 with N(s,a)=10 visits, and sibling with Q=0.4 but N=2, which does UCT select with λ=1.0?

- **Concept: FM-Index and Burrows-Wheeler Transform**
  - **Why needed here:** CorpusTree uses FM-Index for compressed, efficient prefix-constrained search. This enables O(1) lookup of valid next tokens given current prefix.
  - **Quick check question:** Why does BWT enable efficient pattern matching while maintaining compressed storage?

- **Concept: Constrained Decoding with Autoregressive Models**
  - **Why needed here:** FREESON masks LRM logits to only allow tokens that appear as valid corpus continuations. Understanding logit masking is critical for debugging retrieval failures.
  - **Quick check question:** If current prefix has no valid corpus continuation, what happens to the softmax distribution after masking?

## Architecture Onboarding

- **Component map:**
  Question → LRM generates (subject, question) query → CorpusTree (FM-Index) filters valid token space → CT-MCTS loop → Top paths → Document Selection (Complete Document) → LRM generates final answer

- **Critical path:** Value network quality → rollout scoring accuracy → CT-MCTS path prioritization → retrieved document relevance → final answer correctness. If value network is miscalibrated, search explores wrong regions.

- **Design tradeoffs:**
  - G=6 tokens/node optimizes latency-performance; G<3 is slow, G>8 limits exploration
  - M=2 expansions/simulation is sufficient; M>2 yields diminishing returns under constrained token space
  - Complete Document selection reduces information loss but may introduce noise vs. Direct Path

- **Failure signatures:**
  - Empty retrieval: CorpusTree returns no valid continuations → check if subject entity exists in corpus
  - Wrong document retrieved: Value network misguides → inspect rollout scores and value network calibration
  - High latency with low-quality paths: G too small → increase granularity to 5-7

- **First 3 experiments:**
  1. **Baseline sanity check:** Run FREESON on 50 PopQA samples with G=1, M=1 (vanilla token-level MCTS) to establish lower bound; expect EM≈0.17 per Table 2.
  2. **Granularity ablation:** Sweep G∈{1,3,6,10} on 2WikiMultihopQA subset; confirm latency drops and EM peaks at G=6 per Figure 2.
  3. **Value network comparison:** Train two value heads (on-policy rollouts vs. LLM synthetic) and compare EM/F1 on held-out set; expect ~3 point gap per Table 4.

## Open Questions the Paper Calls Out

- **Question:** Can CT-MCTS scale efficiently to trillion-token corpora (e.g., MassiveDS-1.4T) while maintaining competitive latency against dual-encoder retrievers?
  - **Basis in paper:** [explicit] Limitations section states: "efficiently handling them at scale remains a challenge" for large-scale corpora like MassiveDS-1.4T/140B.
  - **Why unresolved:**