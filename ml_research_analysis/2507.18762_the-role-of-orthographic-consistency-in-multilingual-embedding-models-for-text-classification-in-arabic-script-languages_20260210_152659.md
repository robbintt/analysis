---
ver: rpa2
title: The Role of Orthographic Consistency in Multilingual Embedding Models for Text
  Classification in Arabic-Script Languages
arxiv_id: '2507.18762'
source_url: https://arxiv.org/abs/2507.18762
tags:
- arabic
- language
- multilingual
- kurdish
- roberta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of text classification in Arabic-script
  languages, where multilingual models like mBERT and XLM-RoBERTa often underperform
  due to orthographic and lexical differences among languages such as Kurdish Sorani,
  Arabic, Persian, and Urdu. To overcome this, the authors introduce the Arabic Script
  RoBERTa (AS-RoBERTa) family, consisting of four language-specific RoBERTa models
  pre-trained on large, balanced corpora tailored to each target language.
---

# The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages

## Quick Facts
- arXiv ID: 2507.18762
- Source URL: https://arxiv.org/abs/2507.18762
- Reference count: 33
- Language-specific AS-RoBERTa models achieve 2-5 percentage point accuracy gains over multilingual baselines in Arabic-script languages

## Executive Summary
This study addresses the challenge of text classification in Arabic-script languages, where multilingual models like mBERT and XLM-RoBERTa often underperform due to orthographic and lexical differences among languages such as Kurdish Sorani, Arabic, Persian, and Urdu. To overcome this, the authors introduce the Arabic Script RoBERTa (AS-RoBERTa) family, consisting of four language-specific RoBERTa models pre-trained on large, balanced corpora tailored to each target language. These models leverage script-focused pre-training to capture orthographic and morphological nuances, followed by fine-tuning on language-specific classification tasks. Experimental results demonstrate that AS-RoBERTa consistently outperforms multilingual baselines, achieving accuracy gains of 2-5 percentage points across all four languages. Ablation studies confirm that script-focused pre-training is the primary driver of these improvements. Error analysis further reveals that language-specific models better handle fine-grained semantic distinctions, reducing systematic misclassifications between related categories. The findings underscore the value of script-aware specialization in improving multilingual text classification for Arabic-script languages.

## Method Summary
The study introduces Arabic Script RoBERTa (AS-RoBERTa), a family of four language-specific RoBERTa models pre-trained on 4.0B tokens (1B per language) from Wikipedia, OSCAR, news sites, and AsoSoft (Kurdish). The models use dual tokenizer fusion (BPE + WordPiece) with averaged embeddings, incorporate Orthographic Consistency Adapters (OCA) per layer, and employ a domain-adaptive projection bottleneck (768→256 dimensions) before language-specific classification heads. Pre-training uses Masked Language Modeling with orthographic masking (β=0.5), while fine-tuning employs cross-entropy with KL regularization (γ=1.0) for 3 epochs on news topic classification datasets (45k-52k training samples per language).

## Key Results
- AS-RoBERTa achieves 80.5-92.4% accuracy across four Arabic-script languages, outperforming XLM-R (77.8-90.3%) by 2-5 percentage points
- Ablation studies confirm script-focused pre-training is the primary driver of performance improvements
- Language-specific models show reduced confusion between semantically related categories (e.g., politics vs. business)
- Log Loss values (0.28-0.35) indicate well-calibrated confidence across all languages

## Why This Works (Mechanism)

### Mechanism 1: Orthographic Consistency Enforcement
The model introduces an auxiliary loss term ($\mathcal{L}_{orth}$) alongside standard MLM. By masking specific orthographic variants and forcing identical predictions, the encoder learns to map visual variances to unified semantic points. This addresses the performance gap in multilingual models stemming from inconsistent character-level encoding.

### Mechanism 2: Dual-Granularity Tokenization Fusion
Averaging embeddings from BPE and WordPiece tokenizers allows handling both frequent morphology and rare OOV terms. This fusion leverages BPE's strength in frequent character n-grams with WordPiece's suffix/prefix handling, creating a more robust input manifold.

### Mechanism 3: Decoupled Domain Adaptation
A lightweight projection bottleneck between the transformer encoder and classification head preserves pre-trained knowledge while allowing rapid domain adaptation. This isolates domain-specific noise to the projection layer, preventing catastrophic forgetting in the backbone.

## Foundational Learning

- **Concept: Subword Tokenization Strategies**
  - Why needed here: The paper fuses BPE and WordPiece. Understanding that BPE prioritizes frequency of character pairs while WordPiece prioritizes language-model likelihood is crucial for debugging the fusion approach.
  - Quick check question: If an input word is split into `[root, suffix]` by one tokenizer but `[char, char, char]` by another, how does averaging their embeddings affect the attention mechanism's ability to attend to the suffix?

- **Concept: Transformer Fine-Tuning vs. Feature Extraction**
  - Why needed here: The paper argues for script-aware specialization via continued pre-training rather than just fine-tuning mBERT.
  - Quick check question: Why might fine-tuning a generic model (mBERT) on a low-resource script yield inferior results compared to pre-training a dedicated model from scratch, assuming data availability?

- **Concept: Orthographic Normalization**
  - Why needed here: The proposed Orthographic Masking loss is essentially an automated normalization regularizer.
  - Quick check question: Before feeding data to this model, should you manually normalize Unicode characters, or does the paper's mechanism suggest the model learns this mapping internally?

## Architecture Onboarding

- **Component map:** Input Layer (Dual Tokenizers → Embedding Fusion) → Encoder (12-layer RoBERTa with OCA) → Projection Head ([CLS] → MLP 768→256) → Output (Language-specific Softmax heads)
- **Critical path:** The Orthographic Loss ($\mathcal{L}_{pre} = \mathcal{L}_{MLM} + \beta\mathcal{L}_{orth}$) is the central innovation. If $\beta$ is set incorrectly, the model either ignores script variants ($\beta \approx 0$) or over-regularizes character features ($\beta$ too high).
- **Design tradeoffs:** Tokenizer Fusion increases inference complexity for marginal gains in robustness; Language-Specific Heads improve accuracy per language but require a language identification step, adding latency.
- **Failure signatures:** High Log Loss + Low Accuracy suggests overfitting; Politics/Business Confusion indicates semantic embeddings are too close.
- **First 3 experiments:** 1) Baseline Validation: Re-run mBERT vs. AS-RoBERTa comparison on Urdu/Kurdish datasets; 2) Ablation on $\beta$: Train three versions with $\beta \in \{0.0, 0.5, 1.0\}$; 3) Tokenizer Ablation: Disable dual-tokenizer fusion to measure performance delta.

## Open Questions the Paper Calls Out

- Does the script-aware pre-training strategy maintain its effectiveness when applied to noisy, informal text genres such as social media or domain-specific technical content?
- Do the observed accuracy gains from language-specific pre-training persist when scaling to larger transformer architectures (e.g., RoBERTa-large)?
- Can hybrid pre-training strategies that combine massive multilingual signals with script-specific specialization outperform the current purely script-focused approach?

## Limitations
- Evaluation is constrained to news topic classification tasks with 4-5 classes per language
- The Orthographic Consistency Adapter implementation details are sparse, making independent replication challenging
- Results may not generalize to non-news domains or different task types

## Confidence

- **High Confidence**: The core empirical finding that AS-RoBERTa outperforms mBERT/XLM-R by 2-5 percentage points on tested classification tasks
- **Medium Confidence**: The claim that "script-focused pre-training is the primary driver" of improvements, though relative contributions of other architectural choices are not fully disentangled
- **Low Confidence**: The assertion that orthographic inconsistency is the fundamental failure mode for multilingual models in Arabic-script languages

## Next Checks
1. Train a control AS-RoBERTa variant with manual Unicode normalization applied to the pre-training corpus versus the proposed automated orthographic consistency mechanism to isolate the OCA module's contribution
2. Fine-tune the Kurdish-specific AS-RoBERTa on Persian and Urdu classification tasks without additional pre-training to measure cross-lingual transfer benefits
3. Conduct controlled experiments varying the fusion ratio between BPE and WordPiece embeddings (0%, 25%, 50%, 75%, 100%) to quantify the marginal contribution of dual-tokenization