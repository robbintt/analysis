---
ver: rpa2
title: 'HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation'
arxiv_id: '2512.23464'
source_url: https://arxiv.org/abs/2512.23464
tags:
- motion
- data
- human
- generation
- hy-motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HY-Motion 1.0 is a state-of-the-art text-to-motion generation model
  that scales a Diffusion Transformer-based flow matching architecture to over one
  billion parameters. The model generates high-quality 3D human motions from textual
  descriptions and achieves significant improvements in both motion quality and instruction-following
  capability compared to existing open-source methods.
---

# HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation

## Quick Facts
- arXiv ID: 2512.23464
- Source URL: https://arxiv.org/abs/2512.23464
- Reference count: 40
- Key outcome: First billion-parameter flow matching model for text-to-motion generation, achieving 3.24 instruction-following and 3.43 motion quality scores

## Executive Summary
HY-Motion 1.0 introduces the first billion-parameter flow matching model for text-to-motion generation, scaling a Diffusion Transformer architecture to achieve state-of-the-art performance. The model generates high-quality 3D human motions from textual descriptions, covering over 200 motion categories across six major classes. It employs a three-stage training pipeline (pretrain → fine-tune → RL) and outperforms existing open-source methods in both automated SSAE scores and human evaluations.

## Method Summary
HY-Motion 1.0 uses a hybrid Diffusion Transformer architecture with dual-stream/single-stream blocks to process motion latents and text tokens. The model is trained through a three-stage pipeline: 3,000 hours of motion data for pretraining, 400 hours of curated data for fine-tuning with learning rate decay, and reinforcement learning with human feedback via DPO and Flow-GRPO. Motion is represented using SMPL-H skeleton with 6D rotations, and text conditioning employs Qwen3-8B with Bidirectional Token Refiner and CLIP-L encoders. Asymmetric attention masking prevents noise propagation while enabling semantic grounding.

## Key Results
- Achieved 3.24 average instruction-following capability and 3.43 motion quality scores in human evaluations
- First successful scaling of Diffusion Transformer-based flow matching to billion-parameter scale
- Demonstrated saturation of motion quality at 0.46B parameters while instruction-following continues to scale

## Why This Works (Mechanism)

### Mechanism 1
Scaling DiT-based flow matching to 1B parameters improves instruction-following capability through enhanced semantic grounding. The hybrid dual-stream/single-stream transformer processes motion latents and text tokens with independent QKV projections initially, then merges them for deep multimodal fusion. Flow matching constructs continuous probability paths via linear interpolation xt = (1−t)x0 + tx1, trained to predict velocity vt = x1 − x0. Larger model capacity enables better alignment between motion sequences and textual descriptions.

### Mechanism 2
The three-stage training paradigm (pretrain → fine-tune → RL) separates generalization acquisition from precision refinement. Stage 1 (3,000h noisy data) establishes broad motion priors with constant learning rate. Stage 2 (400h curated data) decays learning rate to 0.1× to sharpen distribution without collapsing diversity. Stage 3 (DPO + Flow-GRPO) aligns with human preferences through pairwise preference optimization and explicit reward modeling. Data scale drives semantic coverage; data quality drives motion fidelity.

### Mechanism 3
Asymmetric attention masking prevents noise propagation from motion latents to text embeddings while enabling semantic grounding. Motion tokens attend globally to text for semantic cues; text tokens are masked from motion latents. Temporal attention uses a 121-frame sliding window (locality bias). Full RoPE across concatenated text-motion sequences establishes positional correspondence. Unidirectional information flow preserves semantic conditioning integrity under diffusion noise.

## Foundational Learning

- Concept: **Flow Matching / Rectified Flow**
  - Why needed here: Core generative objective replacing denoising score matching; requires understanding ODE-based sampling
  - Quick check question: Can you explain why linear interpolation xt = (1−t)x0 + tx1 implies constant target velocity?

- Concept: **SMPL-H Body Model + 6D Rotation Representation**
  - Why needed here: Motion representation uses 201-dim vectors with 6D rotations; must understand skeleton topology and continuous rotation spaces
  - Quick check question: Why does the paper use 6D rotation representation instead of quaternions or Euler angles?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: RL stage uses DPO for human preference alignment without explicit reward model training
  - Quick check question: How does DPO differ from PPO in terms of reward model requirements?

## Architecture Onboarding

- Component map: User prompt → Duration/rewrite LLM → Optimized prompt + duration → Qwen3-8B → Token Refiner → dual-stream text features → CLIP-L global embedding + timestep → AdaLN modulation → noisy motion latents → dual-stream (joint attention) → single-stream (fused attention) → denoised motion

- Critical path: 1) User prompt → Duration/rewrite LLM → Optimized prompt + duration; 2) Optimized prompt → Qwen3-8B → Token Refiner → dual-stream text features; 3) CLIP-L global embedding + timestep → AdaLN modulation; 4) Noisy motion latents → dual-stream (joint attention) → single-stream (fused attention) → denoised motion

- Design tradeoffs: Dual vs single-stream ratio: 1:2 split balances modality-specific processing with deep fusion; Attention window: 121 frames trades global context for computational efficiency (linear complexity); Pretrain data quality: 3000h mixed-quality prioritizes coverage over fidelity

- Failure signatures: High-frequency jitter / foot sliding: indicates insufficient Stage-2 fine-tuning or missing physics reward in GRPO; Left/right confusion: semantic grounding failure; check Bidirectional Token Refiner quality; Duration mismatch: LLM duration predictor error; verify GRPO fine-tuning on temporal plausibility

- First 3 experiments: 1) Ablate attention masking: Remove asymmetric mask; measure semantic degradation via SSAE scores; 2) Scale data vs model: Train 0.46B on full 3000h vs 1B on 400h only; isolate data scale vs parameter scale effects; 3) Window size sensitivity: Test 61, 121, 241 frame windows on long sequential actions (>8 seconds); measure coherence break points

## Open Questions the Paper Calls Out

- Question: How can text-to-motion models be extended to generate Human-Object Interactions (HOI) with physically accurate contact points and object-aware motion?
  - Basis in paper: Authors state the current dataset focuses on body kinematics without explicit object geometry, making precise contact point generation challenging.
  - Why unresolved: Current architecture lacks object geometry representation, and dataset excludes explicit object annotations.
  - What evidence would resolve it: Modified architecture with object conditioning, trained on motion data with explicit object annotations, demonstrating improved physical plausibility in HOI benchmarks.

- Question: Why does motion quality saturate at 0.46B parameters while instruction-following capability continues to scale, and can this plateau be overcome?
  - Basis in paper: Scaling experiments reveal motion quality reaches saturation beyond 0.46B parameter size while instruction-following improves, but no mechanistic explanation is provided.
  - Why unresolved: Paper documents divergence but doesn't identify whether bottleneck lies in data quality, architecture, or loss formulation.
  - What evidence would resolve it: Ablation studies isolating bottleneck and demonstrating architectural modifications enabling motion quality to continue scaling beyond 0.46B parameters.

- Question: How can VLM-based captioning pipelines be improved to produce more accurate textual descriptions for nuanced and complex motions?
  - Basis in paper: Authors note creating complete and accurate textual descriptions for nuanced and intricate motions remains a significant challenge for both VLM-based captioning and manual refinement.
  - Why unresolved: Current VLMs fail to capture fine-grained motion details, and manual refinement is costly and subjective.
  - What evidence would resolve it: New captioning methodology yielding higher accuracy on complex motion descriptions, translating to improved performance on detailed instruction-following evaluations.

- Question: Does the 121-frame attention window limit long-range temporal coherence for extended motion sequences?
  - Basis in paper: Narrow band mask restricts attention to 121 frames based on hypothesis that kinematic dynamics are governed primarily by local continuity, but this assumption is not empirically validated against longer-range dependencies.
  - Why unresolved: Design trades long-range coherence for computational efficiency, but impact on complex sequential motions remains unexamined.
  - What evidence would resolve it: Experiments varying attention window sizes on long-sequence generation, measuring temporal coherence versus computational cost.

## Limitations

- Key technical details remain underspecified, including exact model architectures (layer counts, hidden dimensions, attention head configurations) for each scale and precise three-stage training hyperparameters
- Asymmetric attention masking may block necessary bidirectional refinement for correcting motion errors using text semantics
- Claims about being the "first billion-parameter text-to-motion model" lack direct comparison with other billion-parameter models due to absence of such baselines in literature

## Confidence

**High Confidence**: The core claim that HY-Motion 1.0 scales flow matching to over one billion parameters and achieves state-of-the-art performance on both automated (SSAE) and human evaluation metrics. The three-stage training pipeline is well-documented and its effects on instruction-following capability are clearly demonstrated through controlled ablations.

**Medium Confidence**: The claim that scaling to 1B parameters specifically improves instruction-following capability while motion quality saturates at 0.46B. While Tables 3-4 show this trend, the relationship between parameter count and semantic grounding could be confounded by other factors like data quality or training duration.

**Low Confidence**: The claim that HY-Motion 1.0 is the "first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale" lacks direct comparison with other billion-parameter text-to-motion models, as the corpus reveals no such baselines.

## Next Checks

1. **Ablate attention masking**: Remove the asymmetric attention mask and measure degradation in semantic alignment via SSAE scores and human evaluation to validate whether unidirectional information flow is essential for preserving semantic conditioning under diffusion noise.

2. **Scale data vs model**: Train a 0.46B parameter model on the full 3000h pretraining data versus a 1B parameter model on only 400h curated data to isolate whether improvements come from parameter scaling or data scale.

3. **Window size sensitivity**: Test the narrow-band temporal attention window at 61, 121, and 241 frame sizes on sequential actions longer than 8 seconds to identify coherence break points and validate whether the 121-frame window is optimal.