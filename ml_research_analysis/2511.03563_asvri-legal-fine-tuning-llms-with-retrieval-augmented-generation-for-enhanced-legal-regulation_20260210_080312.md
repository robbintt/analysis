---
ver: rpa2
title: 'ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced
  Legal Regulation'
arxiv_id: '2511.03563'
source_url: https://arxiv.org/abs/2511.03563
tags:
- legal
- arxiv
- language
- dataset
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study fine-tuned Large Language Models (LLMs) to enhance their
  ability to understand, analyze, and draft legal regulations. The approach involved
  training LLMs with a supervised dataset of legal instruction-output pairs and integrating
  Retrieval-Augmented Generation (RAG) to access current legal knowledge.
---

# ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation

## Quick Facts
- arXiv ID: 2511.03563
- Source URL: https://arxiv.org/abs/2511.03563
- Reference count: 40
- Primary result: Fine-tuned models achieved BLEU scores of 0.13-0.15 and METEOR scores of 0.34-0.37, outperforming baselines but trailing GPT-3.5 Turbo and GPT-4

## Executive Summary
This study develops ASVRI-Legal, a fine-tuned LLM system for legal regulation analysis that combines supervised instruction tuning with retrieval-augmented generation. The approach trains models on a synthetic dataset of legal instruction-output pairs and integrates RAG to access current legal knowledge. Two models—ASVRI-Legal-Llama 2 (7B) and ASVRI-Legal-WizardLM (13B)—were evaluated against baseline LLMs and commercial models, showing improved performance but limitations in complex legal tasks.

## Method Summary
The method involves creating a supervised dataset of 8,507 instruction-output pairs through GPT-3.5 Turbo generation, then fine-tuning LLaMA2-7B and WizardLM-13B models using LoRA for parameter-efficient adaptation. The system integrates RAG by building a vector knowledge base from hierarchically chunked legal documents, enabling context retrieval during inference. Models are trained with batch sizes of 2 (7B) or 1 (13B) for 3 epochs, then evaluated using BLEU and METEOR metrics on a held-out test set, with qualitative manual review for legal coherence.

## Key Results
- ASVRI-Legal-Llama 2 achieved BLEU 0.13 and METEOR 0.34
- ASVRI-Legal-WizardLM achieved BLEU 0.15 and METEOR 0.37
- Both models outperformed baseline fine-tuned models but trailed GPT-3.5 Turbo (0.20-0.22 BLEU) and GPT-4 (0.42-0.46 BLEU)
- Qualitative assessment showed improved legal coherence but limitations in complex tasks like overlapping provision analysis

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Instruction Tuning
Mapping raw legal text to structured cognitive tasks (e.g., conflict analysis, drafting) improves domain relevance more than general pre-training. By converting legal regulations into specific instruction-output pairs, the model learns the function of legal language rather than just statistical patterns. Core assumption: synthetic instruction pairs accurately represent valid legal reasoning. Evidence: fine-tuning lifted BLEU from 0.01 to 0.07. Break condition: performance degrades if inference prompt distribution differs from training strategy.

### Mechanism 2: Retrieval-Augmented Generation (RAG) for Grounding
External retrieval reduces hallucinations by anchoring generation in current legal text. Instead of relying solely on parametric memory, the system queries a vector database of regulations, forcing synthesis based on provided context chunks. Core assumption: hierarchical chunking preserves sufficient context for relevant retrieval. Evidence: RAG boosted METEOR scores from 0.24 to 0.34. Break condition: irrelevant or contradictory retrieved documents may confuse context.

### Mechanism 3: Parameter-Efficient Fine-Tuning (PEFT/LoRA)
LoRA allows adaptation to legal syntax without erasing general reasoning capabilities. By freezing pre-trained weights and injecting trainable rank-decomposition matrices, the model minimizes catastrophic forgetting. Core assumption: low-rank adaptation matrices capture legal logic complexity. Evidence: standard LoRA practice with specific hyperparameters mentioned. Break condition: if rank is too low, model may fail to capture complex legal dependencies.

## Foundational Learning

- **Concept: Hierarchical Document Segmentation** - Legal text is heavily structured (Chapter -> Article -> Clause). Random chunking breaks logical dependencies essential for retrieval. Quick check: How does the system handle queries requiring cross-referencing Article from Chapter 1 and Clause from Chapter 5?

- **Concept: Hallucination in Legal LLMs** - The paper explicitly targets hallucinations as a primary failure mode to mitigate via RAG. Quick check: Can you distinguish between a model "hallucinating" a law that doesn't exist vs. correctly retrieving an outdated law?

- **Concept: Semantic Similarity Metrics (BLEU/METEOR)** - These are the quantitative yardsticks used to claim success, yet they rely on n-gram overlap. Quick check: Why might a legally accurate answer score poorly on BLEU if it uses synonyms not present in reference text?

## Architecture Onboarding

- **Component map:** Government Regulations (PSKP) -> PDF/Text -> Hierarchical Chunker -> Embeddings -> Vector Store -> Retriever -> Context Window (Fine-tuned LLM) -> Response
- **Critical path:** The quality of the Instruction Dataset Generation. Because training data is synthesized using GPT-3.5, any noise or "model hallucination" in training set propagates directly into the fine-tuned student model.
- **Design tradeoffs:** Model Size vs. Efficiency (13B WizardLM outperformed 7B Llama 2 but requires more compute). Synthetic vs. Human Data (synthesized data saves cost but may compromise nuanced legal reasoning).
- **Failure signatures:** Metric Ceiling (METEOR capping at ~0.37 suggests struggles with complex nuance). Task Specificity (limitations in complex legal tasks like overlapping analysis).
- **First 3 experiments:** 1) Retrieval Ablation: Compare Fine-tuned model with RAG disabled vs. enabled. 2) Context Window Stress Test: Input queries requiring specific article citations. 3) Cross-Model Comparison: Evaluate ASVRI-Legal-Llama (7B) against baseline Llama (7B) on "Drafting Provision" task.

## Open Questions the Paper Calls Out

1. **Evaluation Benchmark Design:** How can evaluation benchmarks be specifically designed to capture nuances of legal text generation better than standard metrics? The authors note standard metrics "focus on n-gram overlap and semantic similarity" and "may not fully capture the intricacies of legal text generation," suggesting future work needs metrics specifically designed for legal contexts.

2. **Complex Task Performance:** What specific training or architectural refinements are required to improve performance in complex tasks like analyzing overlapping legal provisions? The Discussion notes the model is "not yet adept at handling nuanced legal content" and lists "analyzing overlaps between legal provisions, drafting revisions, and creating new clauses" as limitations requiring further refinement.

3. **Smaller Model Parity:** Can domain-specific fine-tuning and RAG enable smaller parameter models (7B-13B) to match legal coherence of larger commercial models? The conclusion observes that despite improvements, the fine-tuned ASVRI models "still fall short when compared to larger, more generalized models like GPT-3.5 Turbo and GPT-4" in both quantitative and qualitative measures.

## Limitations
- Performance still trails larger commercial models (GPT-3.5 Turbo and GPT-4) despite improvements
- Synthetic instruction dataset may propagate reasoning errors from GPT-3.5 Turbo
- Limited evaluation on complex legal tasks like overlapping provision analysis

## Confidence

- **Domain-Specific Instruction Tuning Effectiveness:** Medium - Demonstrated improvement over baseline but limited by synthetic data quality concerns
- **RAG Integration for Hallucination Reduction:** Medium - Quantitative improvements observed but mechanism not fully isolated
- **PEFT/LoRA Preservation of General Capabilities:** High - Standard practice well-documented in literature, though specific hyperparameters remain unspecified

## Next Checks
1. **RAG Ablation Study:** Run controlled experiments comparing the fine-tuned model with and without RAG retrieval on identical legal queries, specifically measuring factual accuracy versus hallucination rates to isolate the true source of BLEU improvements.

2. **Cross-Validation on Independent Legal Corpus:** Test ASVRI-Legal models on legal regulations from different jurisdictions or time periods than the training data to assess generalization beyond the PSKP dataset and identify potential overfitting to specific regulatory structures.

3. **Human Expert Evaluation of Complex Tasks:** Conduct qualitative assessment by legal professionals on the model's performance in nuanced tasks like overlapping provision analysis and conflict resolution, focusing on whether metric improvements translate to practical legal utility.