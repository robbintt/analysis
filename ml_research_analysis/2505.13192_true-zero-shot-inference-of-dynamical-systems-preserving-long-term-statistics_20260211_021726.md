---
ver: rpa2
title: True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics
arxiv_id: '2505.13192'
source_url: https://arxiv.org/abs/2505.13192
tags:
- time
- dynamix
- series
- data
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DynaMix is the first zero-shot dynamical systems reconstruction
  (DSR) foundation model, achieving accurate generative modeling of novel systems'
  long-term behavior from short context signals without retraining. Built on an ALRNN-based
  mixture-of-experts architecture and trained via sparse teacher forcing, DynaMix
  faithfully reproduces attractor geometries and power spectra of unseen chaotic and
  cyclic systems, outperforming state-of-the-art time series foundation models like
  Chronos by orders of magnitude in accuracy, while using only 0.1% of their parameters
  and achieving much faster inference.
---

# True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics

## Quick Facts
- **arXiv ID:** 2505.13192
- **Source URL:** https://arxiv.org/abs/2505.13192
- **Reference count:** 40
- **Primary result:** DynaMix achieves zero-shot dynamical systems reconstruction, accurately modeling unseen chaotic and cyclic systems' long-term behavior from short context signals without retraining.

## Executive Summary
DynaMix introduces the first zero-shot dynamical systems reconstruction (DSR) foundation model, capable of inferring the long-term statistical behavior of entirely new dynamical systems from only short context signals. Unlike traditional time series models that require retraining per system, DynaMix leverages a mixture-of-experts architecture built on Adaptive Linear Recurrent Neural Networks (AL-RNNs) and is trained via a novel Sparse Teacher Forcing (STF) procedure. The model achieves state-of-the-art accuracy in preserving attractor geometries and power spectra of novel chaotic and cyclic systems, using only 0.1% of the parameters of comparable foundation models like Chronos while delivering much faster inference.

## Method Summary
DynaMix is trained on a standardized corpus of 34 chaotic and cyclic 3D dynamical systems, with 5% Gaussian noise added to each trajectory. The core architecture is a mixture of 10 AL-RNN experts, each with 30 latent dimensions and 2 ReLU nonlinearities, gated by a CNN+MLP network. Training uses RADAM optimization over 2000 epochs with a decaying learning rate, and crucially employs Sparse Teacher Forcing (STF) with a teacher forcing interval of τ=10 to stabilize learning on chaotic dynamics. Evaluation is performed on held-out systems using state-space KL divergence and Hellinger distance over long-term trajectories, demonstrating faithful reproduction of attractor structures and power spectra without any fine-tuning.

## Key Results
- DynaMix accurately reconstructs long-term attractor geometry and power spectra of novel chaotic and cyclic systems from short context signals (T_C=500) without retraining.
- Achieves orders-of-magnitude better accuracy than state-of-the-art time series foundation models (e.g., Chronos) while using only 0.1% of their parameters.
- Provides interpretable dynamical similarity measures between systems and generalizes well to real-world data, often outperforming models trained on empirical datasets.

## Why This Works (Mechanism)
DynaMix works by learning a shared dynamical space across many systems during training, enabling interpolation to unseen systems at inference time. The mixture-of-experts AL-RNN architecture allows flexible representation of diverse dynamical behaviors, while Sparse Teacher Forcing prevents divergence during training on chaotic trajectories. The model's low parameter count and efficient inference make it practical for real-world deployment.

## Foundational Learning
- **Chaotic dynamics and Lyapunov exponents:** Understanding sensitivity to initial conditions and long-term unpredictability is essential for designing stable training procedures. *Quick check:* Verify generated trajectories have positive Lyapunov exponents matching ground truth.
- **Mixture-of-experts (MoE) models:** Combining specialized subnetworks allows modeling diverse dynamical behaviors within a single architecture. *Quick check:* Ensure gating network correctly routes context to appropriate expert(s).
- **Sparse Teacher Forcing (STF):** Periodically replacing latent states with true data during training stabilizes learning on chaotic systems. *Quick check:* Confirm STF interval τ=10 is active and gradients remain stable.
- **Delay embedding and state space reconstruction:** Lifting 1D time series to appropriate state space dimensions is critical for faithful reconstruction. *Quick check:* Validate autocorrelation-based embedding captures system's degrees of freedom.
- **AL-RNN architecture:** Adaptive linear recurrences provide efficient, interpretable dynamical modeling. *Quick check:* Confirm AL-RNN updates follow the specified equations with correct weight initialization.

## Architecture Onboarding

**Component Map:** Context signal → Delay embedding → CNN gating → MoE (10 AL-RNN experts) → Output trajectory

**Critical Path:** Context embedding and gating → Expert selection/mixing → AL-RNN state updates with STF → Trajectory generation

**Design Tradeoffs:**
- *MoE vs. single large network:* MoE provides modularity and efficiency but adds gating complexity; single network might be simpler but less flexible.
- *STF vs. standard BPTT:* STF stabilizes training on chaotic systems but requires careful tuning of τ; standard BPTT is simpler but fails on chaos.

**Failure Signatures:**
- *Exploding gradients/divergence:* Indicates STF not properly implemented or τ too large.
- *"Context parroting":* Model repeats input verbatim, suggesting insufficient capacity or improper training.
- *Convergence to fixed points:* Generated trajectories lose chaos, indicating poor learning of attractor geometry.

**3 First Experiments:**
1. Train DynaMix on a subset of 5-10 systems and test reconstruction on a held-out system from the same subset.
2. Vary the teacher forcing interval τ (e.g., τ=5, τ=20) to identify optimal stability-performance tradeoff.
3. Compare DynaMix's attractor reconstruction quality against a standard LSTM trained with full teacher forcing on the same task.

## Open Questions the Paper Calls Out
- **Non-stationary dynamics:** How to handle tipping points, multi-scale dynamics, and non-stationarity without manual decomposition? The paper suggests explicit inclusion in training corpus or filtering modules, but no general solution is provided.
- **Continuous-time formulation:** Can AL-RNN experts be reformulated for irregular sampling? Identified as a future direction, but current architecture relies on discrete fixed steps.
- **Automated embedding:** Can the embedding process be made adaptive to prevent failures from insufficient context dimensionality? Current reliance on heuristics and manual selection is noted as a limitation.

## Limitations
- Generalization breadth is unclear, as performance is only demonstrated on one held-out test system rather than a comprehensive benchmark.
- The 5% Gaussian noise injection during training may bias the model toward fitting noise rather than true dynamics.
- Lack of comparison to dedicated physics-informed or structure-preserving baselines makes it difficult to assess relative performance in the dynamical systems domain.

## Confidence
- **High confidence:** Zero-shot reconstruction works on at least one held-out system; MoE + AL-RNN architecture is sound; STF training is well-justified.
- **Medium confidence:** Improvements over Chronos are real but baseline choice is suboptimal; parameter efficiency claims are credible but context-dependent.
- **Low confidence:** Breadth of generalization, interpretability of similarity measures, and hyperparameter robustness remain insufficiently validated.

## Next Checks
1. **Benchmark expansion:** Test DynaMix on a larger suite of held-out chaotic and cyclic systems to quantify generalization breadth and check for overfitting.
2. **Baseline comparison:** Compare DynaMix to dedicated physics-informed or structure-preserving models (e.g., Neural ODEs, Koopman methods) and time series foundation models trained on dynamical systems data.
3. **Dynamical metric evaluation:** Supplement distributional metrics with quantitative measures of dynamical invariants (e.g., Lyapunov exponents, attractor dimensions, recurrence plots) to directly assess fidelity of long-term chaotic behavior.