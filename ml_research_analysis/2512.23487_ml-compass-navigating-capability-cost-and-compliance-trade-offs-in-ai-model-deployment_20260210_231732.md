---
ver: rpa2
title: 'ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model
  Deployment'
arxiv_id: '2512.23487'
source_url: https://arxiv.org/abs/2512.23487
tags:
- cost
- frontier
- user
- capability
- healthbench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ML Compass is a framework that closes the capability-deployment
  gap by formulating AI model selection as a constrained optimization over a technological
  frontier that couples model capabilities to deployment costs, while explicitly accounting
  for user utility, business costs, and compliance constraints. Theoretical analysis
  reveals a three-regime structure in optimal internal measures: some dimensions bind
  at compliance minima, some saturate at maximum feasible levels, and the remainder
  take interior values governed by frontier curvature; comparative statics quantify
  how budget changes, regulatory tightening, and technological progress propagate
  across dimensions.'
---

# ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment

## Quick Facts
- arXiv ID: 2512.23487
- Source URL: https://arxiv.org/abs/2512.23487
- Reference count: 40
- Primary result: MLC recommendations differ from capability-only rankings by explicitly optimizing deployment value under constraints.

## Executive Summary
ML Compass (MLC) closes the capability-deployment gap by formulating AI model selection as a constrained optimization over a technological frontier that couples model capabilities to deployment costs. It explicitly accounts for user utility, business costs, and compliance constraints through a unified framework. Theoretical analysis reveals a three-regime structure in optimal internal measures, while empirical validation shows MLC recommendations can materially differ from capability-only rankings in both conversational and healthcare settings.

## Method Summary
MLC extracts low-dimensional internal measures from heterogeneous model descriptors using factor analysis, estimates an empirical capability-cost frontier via Pareto peeling and CES fitting, learns context-dependent utility from interaction-level outcomes, and produces target capability-cost profiles and recommended models. The framework solves a constrained optimization problem balancing user utility, resource penalties, and compliance constraints to identify optimal deployment configurations.

## Key Results
- MLC recommendations diverge from capability-only rankings in PRISM and HealthBench settings
- Theoretical analysis reveals three-regime structure in optimal capability profiles (compliance floors, saturation ceilings, frontier-governed interiors)
- Comparative statics quantify how budget changes, regulatory tightening, and technological progress propagate across capability dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal capability profiles partition into three regimes: compliance floors, saturation ceilings, and frontier-governed interiors
- Mechanism: Under linear utility and CES frontier with curvature parameter b ≥ 1, KKT conditions partition each dimension based on utility weight to frontier difficulty ratio. Dimensions with low ratio bind at compliance minima, high-ratio dimensions saturate at 1, intermediate dimensions take interior values governed by frontier curvature
- Core assumption: Linear utility, CES frontier, convexity, nondegeneracy
- Break condition: Near-linear frontier (b → 1) or infeasible compliance minima collapse three-regime separation

### Mechanism 2
- Claim: The capability–cost frontier makes implicit trade-offs explicit and prevents unbounded improvement
- Mechanism: Constraint F_X(x) ≤ F_C(c) uses CES aggregator to summarize multi-dimensional capability and power mapping for resource spend. When budget binds, Lagrange multiplier forces respect for technology frontier
- Core assumption: Stable frontier parameters, models on/below frontier, diminishing returns
- Break condition: Sparse model sets or increasing returns invalidate frontier formulation

### Mechanism 3
- Claim: Tightening binding compliance constraint creates quantifiable spillovers; budget increases scale all interior dimensions proportionally
- Mechanism: Comparative statics show ∂x_i*/∂B = ε_B · (x_i*/B) for interior dimensions and ∂x_i*/∂R_k = –a_k R_k^(b-1) x_i*/Y* for spillovers when budget binds
- Core assumption: Fixed active set, b > 1, at least one interior dimension
- Break condition: Active set changes require recomputation of elasticity formulas

## Foundational Learning

- **Karush–Kuhn–Tucker (KKT) conditions for constrained optimization**
  - Why needed here: Three-regime characterization derives from KKT stationarity, complementary slackness, and dual feasibility
  - Quick check question: If a constraint g(x) ≤ 0 is slack at optimum, what must its multiplier be?

- **Constant Elasticity of Substitution (CES) functions**
  - Why needed here: CES aggregator governs substitutability between capability dimensions and determines frontier curvature
  - Quick check question: What happens to CES aggregator as b → 1? As b → ∞?

- **Pareto efficiency and frontier estimation**
  - Why needed here: Pareto peeling filters dominated models before fitting frontier; understanding dominance is essential for tier assignments
  - Quick check question: If model A has (x₁, x₂) = (0.8, 0.6) and model B has (0.7, 0.9), which Pareto-dominates which?

## Architecture Onboarding

- **Component map**: Raw capability data → factor loadings → internal measures → Pareto tiers → frontier parameters (â, b̂, ĉ₀, d̂)
- **Critical path**: Measure extraction → factor analysis → internal measures → Pareto tiers → frontier estimation. Frontier estimation failure blocks downstream optimization
- **Design tradeoffs**: Number of factors (I) vs. stability/interpretability; number of tiers (T) vs. granularity/noise; linear vs. nonlinear utility (interpretability vs. interactions)
- **Failure signatures**: Frontier b < 1 (check input diversity), cost–tier correlation near zero (check cost proxy), utility AUC ~0.5 (check context grouping)
- **First 3 experiments**:
  1. Sensitivity sweep on frontier parameters (vary I, T, tolerance)
  2. Ablation on utility model class (linear vs. nonlinear)
  3. Scenario comparison (Pure Capability, Cost-Aware, Constrained settings)

## Open Questions the Paper Calls Out

- **Dynamic deployment extension**: How to extend MLC to multi-period settings with evolving frontiers and switching costs? The current static formulation ignores temporal evolution and migration friction.
- **Utility validation against KPIs**: To what extent can interaction-level proxy utilities be validated against actual organizational KPIs? Immediate feedback signals may not correlate with long-term business outcomes.
- **Dimensionality reduction trade-offs**: What are the information-loss bounds when projecting high-dimensional compliance constraints into low-dimensional internal measure space? Conservative safety margins may overly restrict feasible set.

## Limitations

- Frontier estimation robustness depends on parametric assumptions that may not hold across all model families or deployment contexts
- Utility estimation validity relies on context-grouped models that may misalign with actual deployment value if context encoding is coarse
- Generalizability across domains remains untested beyond conversational and healthcare settings

## Confidence

- **High Confidence**: Three-regime optimality structure and comparative statics spillovers under CES assumptions
- **Medium Confidence**: Empirical frontier estimation quality and alignment of utility weights with deployment value
- **Low Confidence**: Extrapolation to model sets beyond training frontier and performance in substantially different domains

## Next Checks

1. **Frontier robustness test**: Systematically vary Pareto tiers and tolerance thresholds; assess stability of estimated parameters and check b̂ ≥ 1, 0 < d̂ ≤ 1
2. **Utility model ablation**: Compare linear vs. nonlinear utility estimation across user/task types; verify learned weights align with domain expert intuition
3. **Cross-domain transfer**: Apply MLC to scientific reasoning domain; evaluate whether three-regime structure and spillover predictions hold under different capability distributions