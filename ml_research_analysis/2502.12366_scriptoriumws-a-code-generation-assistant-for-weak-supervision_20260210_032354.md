---
ver: rpa2
title: 'ScriptoriumWS: A Code Generation Assistant for Weak Supervision'
arxiv_id: '2502.12366'
source_url: https://arxiv.org/abs/2502.12366
tags:
- data
- weak
- supervision
- prompt
- labeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ScriptoriumWS, a code generation assistant
  for weak supervision that addresses the bottleneck of manually writing labeling
  functions. The authors propose using code-generation models (specifically OpenAI
  Codex) to automatically synthesize programmatic labeling functions through carefully
  designed prompts.
---

# ScriptoriumWS: A Code Generation Assistant for Weak Supervision

## Quick Facts
- **arXiv ID:** 2502.12366
- **Source URL:** https://arxiv.org/abs/2502.12366
- **Reference count:** 36
- **Primary result:** ScriptoriumWS generates labeling functions that achieve significantly higher coverage (e.g., 40.5% to 100% on SMS dataset) while maintaining comparable accuracy to human-designed ones.

## Executive Summary
This paper introduces ScriptoriumWS, a code generation assistant that addresses the bottleneck of manually writing labeling functions in weak supervision. The system leverages code-generation models like OpenAI Codex to automatically synthesize programmatic labeling functions through carefully designed prompts. By exploring multiple prompting strategies including general prompts, mission statements, human heuristics, and in-context learning, ScriptoriumWS produces labeling functions with significantly higher coverage than human-designed alternatives while maintaining comparable accuracy. The synthesized labeling functions can be used as a complementary approach alongside existing weak supervision pipelines to improve end model performance by increasing the number of labeled examples.

## Method Summary
ScriptoriumWS is a code generation assistant that synthesizes programmatic labeling functions for weak supervision. The system uses OpenAI Codex to generate Python code based on carefully constructed prompts that include task descriptions, heuristics, and data exemplars. The generated code is executed in a sandbox environment to create labeling functions, which are then aggregated using standard weak supervision label models (like Snorkel or Dawid-Skene) to produce probabilistic labels for training end models. The approach focuses on maximizing coverage while maintaining acceptable accuracy through the denoising capabilities of the label model.

## Key Results
- ScriptoriumWS-generated labeling functions achieve significantly higher coverage than human-designed ones (e.g., improving from 40.5% to 100% on SMS dataset and 25.8% to 100% on Spouse dataset)
- The synthesized labeling functions maintain comparable accuracy to human-designed labeling functions
- End model performance using ScriptoriumWS LFs achieves F1-score improvements of 4.0% and 5.0% on SMS and Spouse datasets respectively
- The system can be used as a complementary approach alongside human-designed labeling functions to improve overall performance

## Why This Works (Mechanism)

### Mechanism 1
Code generation models can synthesize labeling functions that achieve significantly higher data coverage than conservative human-designed heuristics by leveraging broad prior knowledge encoded in models like Codex. These synthesized LFs tend to apply rules more liberally across the dataset, reducing the "abstain" rate typical of manual LFs which are often tuned to avoid false positives.

### Mechanism 2
Standard weak supervision label models can successfully denoise the high-conflict outputs characteristic of synthesized LFs. Synthesized LFs often overlap and conflict because they independently interpret the prompt, but the aggregation step treats these conflicts as signals to estimate source accuracies and correlations, outputting probabilistic labels that often match or exceed human baselines.

### Mechanism 3
In-context prompting strategies (heuristics and data exemplars) steer the code model, but effectiveness is inconsistent across datasets. Providing explicit heuristic rules or labeled data examples in the prompt acts as a scaffold, helping the model generate LFs that align with user intent rather than generic patterns.

## Foundational Learning

**Programmatic Weak Supervision (PWS)**: Understanding how labeling functions feed into a label model to produce probabilistic labels is essential to diagnose why high-coverage LFs are valuable. *Quick check:* If three labeling functions vote True, False, and Abstain, how does a Label Model typically resolve the final label compared to a simple Majority Vote?

**Large Language Models for Code Synthesis**: The system relies on the ability of models like Codex to translate natural language intent into executable code. *Quick check:* What is the trade-off between using a high temperature (e.g., 0.8) vs. low temperature (0.2) when generating diverse labeling functions?

**Labeling Function Metrics**: The paper demonstrates that high accuracy is not the only metric; coverage is the bottleneck ScriptoriumWS solves. Evaluating the system requires balancing coverage, conflict, and accuracy metrics. *Quick check:* Why might an LF with 100% coverage but only 60% accuracy be preferred over an LF with 5% coverage and 100% accuracy in a weak supervision setup?

## Architecture Onboarding

**Component map:** Prompt Constructor -> Code Generator (Codex) -> LF Executor -> Label Model -> End Model

**Critical path:** Prompt Engineering -> LF Generation -> Execution & Filtering -> Label Model Aggregation -> End Model Training

**Design tradeoffs:** Synthesized LFs favor coverage (recall) over precision, shifting the burden of denoising to the Label Model. Human LFs are precise but slow/expensive; Synthesized LFs are cheap but noisy. The paper suggests a "Complementary" approach where Synthesized LFs fill the gaps left by Human LFs.

**Failure signatures:** Static Analysis Errors (generated code imports non-existent libraries), Hallucinated Heuristics (LFs use keywords not grounded in data), Zero-Variance LFs (returns same label for all inputs).

**First 3 experiments:**
1. Run ScriptoriumWS with a "General Prompt" on a dataset (e.g., Spam) and plot the Coverage increase vs. Human LFs alone.
2. Compare downstream F1-score when using "Data Exemplars" vs. "Human Heuristics" in the prompt to identify which provides better semantic grounding.
3. Combine a small set of Human LFs with a large set of Synthesized LFs and measure if End Model performance improves over either set in isolation.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on the code generation model's ability to accurately translate natural language descriptions into functional labeling functions
- Evaluation is limited to text classification tasks, leaving unclear whether the approach transfers to other data types
- Reliance on external API calls to OpenAI Codex introduces potential cost, latency, and availability concerns

## Confidence

**High Confidence:** The core claim that ScriptoriumWS achieves significantly higher coverage while maintaining comparable accuracy is well-supported by empirical results across multiple datasets.

**Medium Confidence:** The claim about synthesized labeling functions being complementary to human-designed ones is supported but could benefit from more extensive ablation studies.

**Low Confidence:** The generalizability of results to non-text domains and the long-term viability of the approach given dependency on commercial APIs remain uncertain.

## Next Checks

1. **Domain Transfer Validation:** Test ScriptoriumWS on non-text domains (tabular data, time series, or image classification) to evaluate generalizability beyond the current text-focused evaluation.

2. **Error Analysis and Interpretability Study:** Conduct a detailed failure analysis of synthesized labeling functions, categorizing common error types and developing debugging strategies for practitioners.

3. **Hybrid System Performance Under Resource Constraints:** Evaluate the complementary approach under realistic constraints (API rate limits, cost per request, execution timeouts) to understand practical deployment considerations.