---
ver: rpa2
title: 'IE-Bench: Advancing the Measurement of Text-Driven Image Editing for Human
  Perception Alignment'
arxiv_id: '2501.09927'
source_url: https://arxiv.org/abs/2501.09927
tags:
- image
- editing
- images
- quality
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IE-Bench, the first comprehensive benchmark
  suite for text-driven image editing quality assessment. The core contribution is
  IE-DB, a novel database containing 301 diverse source images, various editing prompts,
  and corresponding edited results from five different editing methods, along with
  3,010 Mean Opinion Scores (MOS) from 25 human subjects.
---

# IE-Bench: Advancing the Measurement of Text-Driven Image Editing for Human Perception Alignment

## Quick Facts
- arXiv ID: 2501.09927
- Source URL: https://arxiv.org/abs/2501.09927
- Authors: Shangkun Sun; Bowen Qu; Xiaoyu Liang; Songlin Fan; Wei Gao
- Reference count: 40
- Primary result: First comprehensive benchmark suite for text-driven image editing quality assessment

## Executive Summary
IE-Bench introduces a pioneering benchmark suite for evaluating text-driven image editing quality, addressing a critical gap in current assessment methodologies. The work presents IE-DB, a comprehensive database containing 301 source images, multiple editing prompts, and results from five editing methods, supplemented by 3,010 Mean Opinion Scores from 25 human subjects. The benchmark introduces IE-QA, a multi-modal source-aware quality assessment method that evaluates both text-image alignment and source-target relationships. Experimental results demonstrate significant improvements over existing metrics, achieving state-of-the-art performance in perceptual alignment measurement.

## Method Summary
The authors propose a novel approach to text-driven image editing evaluation through the creation of IE-DB, a comprehensive database that captures diverse editing scenarios. The IE-QA method employs multi-modal analysis to assess editing quality by considering both the semantic alignment between text prompts and edited images, as well as the relationship between source and target images. The evaluation framework incorporates human perception data through Mean Opinion Scores collected from 25 subjects, providing ground truth for quality assessment. The method leverages advanced feature extraction and similarity measurement techniques to model complex relationships between textual prompts, source images, and edited outputs.

## Key Results
- IE-QA achieves 0.7520 PLCC, 0.7498 SROCC, and 0.5541 KRCC, significantly outperforming existing metrics
- Performance improvements of 10.46%, 9.02%, and 10.19% over previous methods across different correlation measures
- Comprehensive dataset of 3,010 human-evaluated samples providing robust ground truth for evaluation

## Why This Works (Mechanism)
The proposed IE-QA method works by modeling the complex relationships inherent in text-driven image editing through multi-modal feature fusion. By simultaneously considering text-image alignment and source-target relationships, the method captures the nuanced perceptual qualities that human evaluators use when assessing editing quality. The source-aware approach ensures that edits are evaluated in context of the original image, preventing unrealistic transformations while maintaining semantic consistency with the text prompt.

## Foundational Learning
- Multi-modal feature fusion (why needed: to combine visual and textual information for comprehensive evaluation; quick check: verify feature compatibility and alignment)
- Human perception alignment (why needed: to ensure evaluation metrics match human judgment; quick check: validate MOS correlation with metric scores)
- Source-target relationship modeling (why needed: to maintain contextual consistency in edits; quick check: test sensitivity to source image variations)
- Semantic text-image alignment (why needed: to ensure edits match intended textual descriptions; quick check: evaluate on diverse prompt types)
- Correlation metrics for quality assessment (why needed: to measure metric performance against human judgment; quick check: verify statistical significance)

## Architecture Onboarding

Component map:
IE-DB (301 images, 5 methods, MOS data) -> IE-QA (multi-modal feature extraction) -> Quality assessment (PLCC, SROCC, KRCC)

Critical path:
Source image → Text prompt → Editing method → IE-QA feature extraction → Quality score → Human perception alignment

Design tradeoffs:
- Balance between comprehensive evaluation and computational efficiency
- Trade-off between dataset size and diversity coverage
- Choice between automated metrics and human evaluation for ground truth

Failure signatures:
- Poor correlation with human scores indicates feature extraction or alignment issues
- High variance in MOS suggests subjective bias or ambiguous prompts
- Low performance on certain editing methods indicates metric limitations

First 3 experiments:
1. Baseline correlation testing with existing metrics on IE-DB
2. Ablation study removing source-aware components
3. Cross-method generalization testing on new editing approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability concerns regarding the five specific editing methods evaluated
- Potential cultural or demographic biases in human perception data from 25 subjects
- Limited dataset scope with 301 source images may not capture full real-world diversity
- Subjectivity and inter-rater variability in Mean Opinion Scores as ground truth

## Confidence
- High confidence: Benchmark creation methodology and dataset collection are well-documented and reproducible
- Medium confidence: Performance improvements are statistically significant but may not generalize to all editing scenarios
- Medium confidence: Multi-modal source-aware approach is theoretically sound but needs validation on larger, more diverse datasets

## Next Checks
1. Evaluate IE-QA performance on a broader set of editing methods beyond the five tested, including emerging techniques not available during initial development
2. Conduct cross-cultural validation studies with diverse subject pools to assess generalizability of human perception alignment findings
3. Test benchmark sensitivity to different image domains (medical, satellite, artistic) to establish applicability across specialized use cases