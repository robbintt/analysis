---
ver: rpa2
title: 'HAM: Hierarchical Adapter Merging for Scalable Continual Learning'
arxiv_id: '2509.13211'
source_url: https://arxiv.org/abs/2509.13211
tags:
- task
- tasks
- learning
- merging
- adapters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HAM addresses continual learning in long task sequences by introducing
  a hierarchical adapter merging framework. It trains task-specific LoRA adapters
  with importance weights, groups them by similarity, and performs adaptive intra-group
  concatenation and pruning before final merging.
---

# HAM: Hierarchical Adapter Merging for Scalable Continual Learning

## Quick Facts
- arXiv ID: 2509.13211
- Source URL: https://arxiv.org/abs/2509.13211
- Authors: Eric Nuertey Coleman; Luigi Quarantiello; Samrat Mukherjee; Julio Hurtado; Vincenzo Lomonaco
- Reference count: 24
- Primary result: HAM achieves up to 86.87% average accuracy on 50-task CIFAR-100, outperforming SD-LoRA and Linear Merging

## Executive Summary
HAM introduces a hierarchical adapter merging framework for continual learning that addresses interference and scalability issues in long task sequences. The method trains task-specific LoRA adapters with importance weights, groups them by similarity, and performs adaptive intra-group concatenation and pruning before final merging. By organizing adapters hierarchically and using adaptive scaling, HAM effectively reduces catastrophic forgetting while maintaining parameter efficiency compared to storing one adapter per task.

## Method Summary
HAM builds on LoRA adapters by introducing a three-phase approach: task training, similarity-based grouping, and intra-group concatenation. During task training, a new LoRA adapter and importance scalar are learned while updating existing group scalars. Adapters are then grouped by cosine similarity, with similar adapters concatenated (rather than merged) to expand representational rank. Before concatenation, adapters undergo magnitude-based pruning to reduce noise. The framework supports up to G_max groups, balancing transfer efficiency against task isolation, and achieves final merging through importance-weighted averaging of group adapters.

## Key Results
- Achieves 86.87% average accuracy on CIFAR-100 50-task split, outperforming SD-LoRA and Linear Merging
- Demonstrates reduced forgetting with consistent performance across long task sequences
- Shows faster training and inference times compared to storing individual adapters per task
- Excels particularly in memory-constrained long-sequence scenarios where competing methods struggle

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Concatenating adapters within groups preserves task-specific features by expanding representational rank while similarity-based grouping confines interference.
- **Mechanism:** HAM groups adapters by cosine similarity and concatenates their low-rank matrices (B and A) rather than averaging them. This increases the group's effective rank (r_Gi = m · r), creating a larger subspace to capture the union of task features while maintaining task isolation.
- **Core assumption:** Similar adapters share a compatible feature space that benefits from a joint higher-rank representation, whereas dissimilar tasks would interfere if merged.
- **Evidence anchors:**
  - [Section 3.2.2] "Concatenation... yields each ΔW_Gi with an expanded rank... This helps better preserving the features learned by LoRAs... while increasing the representation capabilities."
  - [Section 4.3] "Group adapters... consistently outperform their constituent individual adapters... Task 4 improves by +4.80%."
- **Break condition:** If the number of tasks per group (m) grows indefinitely, the expanded rank could negate the parameter-efficiency benefits of LoRA or hit memory constraints.

### Mechanism 2
- **Claim:** Jointly optimizing importance scalars of previous groups while training a new adapter allows for passive knowledge re-balancing without replay.
- **Mechanism:** During training of task T_i, the new LoRA adapter ΔW_i is trained alongside the scaling factors α_Gj of all existing frozen groups. This allows the optimizer to re-weight previous groups to accommodate the new task's direction, ensuring the new adapter learns residuals not already covered by the existing mixture.
- **Core assumption:** Scalar scaling is sufficient to resolve conflicts between frozen pre-trained weights, frozen group adapters, and the new active adapter.
- **Evidence anchors:**
  - [Section 3.2.1] "While training ΔW_i, the scaling factors of the group adapters {α_Gj} are also updated... ensures that the relative importance of all adapters remains balanced."
  - Corpus: HydraOpt suggests optimization-based merging is effective, though HAM applies this specifically to scalar weights rather than full parameter sets.
- **Break condition:** If the learning rate for α is too high, the model might catastrophically suppress older groups to minimize immediate loss on the new task.

### Mechanism 3
- **Claim:** Magnitude-based pruning before merging acts as a noise filter, reducing destructive interference from redundant parameters.
- **Mechanism:** Before a new adapter is concatenated into a group, HAM retains only the top-k% of weights based on absolute magnitude. This assumes smaller magnitude weights contribute less to the output and are more likely to be noise or conflicting signals.
- **Core assumption:** Importance correlates strictly with weight magnitude (standard pruning assumption), and dropped weights are redundant rather than complementary fine-grained details.
- **Evidence anchors:**
  - [Section 3.2.2] "Retain only the top-k% weights... The resulting matrices B̂_i and Â_i are defined by... the indicator function."
  - [Section 4.3] Table 7 shows accuracy peaks at k=0.6 (60% retained), dropping at 0.9 (too much noise) and 0.1 (too much information loss).
- **Break condition:** If tasks require fine-grained features distributed across many small weights, aggressive pruning (k < 0.4) will degrade accuracy.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** HAM is built entirely on the LoRA architecture (W = W_0 + BA). Understanding that B and A are low-rank matrices injected into the model is required to grasp how "concatenation" works (i.e., stacking these matrices) versus standard weight merging.
  - **Quick check question:** Can you explain why concatenating two LoRA matrices ([B_1, B_2]) increases the rank of the adapter, whereas averaging them does not?

- **Concept: Catastrophic Forgetting & Interference**
  - **Why needed here:** The core motivation for HAM's hierarchical structure is to mitigate the interference that occurs when naively merging or sequentially fine-tuning models.
  - **Quick check question:** Why does simply averaging the weights of two distinct task adapters often lead to a drop in performance for both tasks (interference)?

- **Concept: Cosine Similarity**
  - **Why needed here:** HAM uses cosine similarity between vectorized adapters to determine which group a new task belongs to.
  - **Quick check question:** If Adapter A is for "birds" and Adapter B is for "cars," would you expect their cosine similarity to be higher or lower than two adapters both trained on different types of "birds"?

## Architecture Onboarding

- **Component map:** Base Model -> LoRA Module (ΔW) -> Importance Scalars (α) -> Group Manager -> Adapter Buffer
- **Critical path:**
  1. **Forward Pass:** Input passes through Base Model + (Frozen Groups × Active Alphas) + (Active LoRA)
  2. **Backward Pass:** Update Active LoRA, Active Alpha, and *Group Alphas*. Do not update Group LoRA weights
  3. **Task End:**
      - Prune Active LoRA (drop bottom 40% weights)
      - Compute Similarity to all existing Group LoRAs
      - **Branch A (Similar):** Concatenate Pruned LoRA to the winning Group's LoRA (expanding rank). Recalculate Group Alpha (running average)
      - **Branch B (Dissimilar):** Initialize a new Group with this LoRA (if Groups < G_max)

- **Design tradeoffs:**
  - **G_max (Max Groups):** Low G_max (e.g., 2) maximizes transfer and efficiency but risks forcing dissimilar tasks together. High G_max preserves task isolation but reduces the "merging" benefits
  - **Pruning Ratio (k):** High retention (k ≈ 0.9) preserves info but risks interference. Low retention (k ≈ 0.4) maximizes efficiency but risks amnesia

- **Failure signatures:**
  - **Rigidification:** If Group Alphas drop to near-zero during training, the model is "forgetting" by muting old knowledge to fit the new task (check alpha learning rates)
  - **Rank Explosion:** If similarity thresholds are too low, every task creates a new group, eventually hitting memory limits or degrading to "one adapter per task" performance
  - **Negative Transfer:** If dissimilar tasks are forced into one group, the concatenated rank might introduce noise, causing accuracy to drop below the individual task baseline

- **First 3 experiments:**
  1. **Sanity Check:** Train 2 distinct tasks. Merge via Linear Averaging vs. HAM's Concatenation. Verify if concatenation preserves accuracy better for dissimilar tasks
  2. **Ablation:** Run HAM on a 10-task sequence with Pruning disabled (k=1.0) vs. Paper Default (k=0.6). Compare Average Accuracy to validate the "noise reduction" claim
  3. **Scaling:** Test G_max ∈ {1, 2, 5, 10} on a 20-task split to identify the saturation point where increasing groups yields diminishing returns

## Open Questions the Paper Calls Out

- **Question:** Can the HAM framework be effectively generalized to other Parameter-Efficient Fine-Tuning (PEFT) techniques, such as prompt tuning, without performance degradation?
  - **Basis in paper:** [explicit] The conclusion states, "it is theoretically possible to use other PEFT methods, e.g. prompts, and it would be interesting to assess the differences in performance when changing PEFT technique."
  - **Why unresolved:** The current study exclusively validates HAM using Low-Rank Adaptation (LoRA), leaving the interaction between the hierarchical merging mechanism and other parameter-efficient paradigms unexplored.
  - **What evidence would resolve it:** Empirical results from experiments applying the HAM grouping and merging logic to prompt-based methods (e.g., L2P, CODA-Prompt) on standard benchmarks.

- **Question:** Is it possible to modify HAM to enable online adapter merging, making the adapted model available immediately rather than only at the end of the training sequence?
  - **Basis in paper:** [explicit] The authors note in "Limitations and Future Work" that "the final adapted model is still available only at the end of the training phase" and suggest future work could focus on "a variation of such method that enables an online adapters merging."
  - **Why unresolved:** The current inference mechanism (Equation 8) relies on a final merging step (ΔW_merged) executed after the sequential training of all tasks is complete.
  - **What evidence would resolve it:** An algorithmic variant that performs the global merging step incrementally during training, with evaluations showing inference is possible at any time step without significant accuracy loss.

- **Question:** Does HAM retain its efficiency and interference-reduction properties when applied to Large Language Models (LLMs) and NLP tasks?
  - **Basis in paper:** [inferred] While the introduction claims applicability to "large pre-trained models" generally, the experimental evaluation is strictly limited to Vision Transformers (ViT-B/16) on image classification benchmarks (CIFAR-100, CUB-200, Tiny-ImageNet).
  - **Why unresolved:** The interference patterns and parameter dependencies in LLMs (dense layers vs. attention heads) may differ significantly from vision models, potentially affecting the efficacy of the magnitude-based pruning and similarity-based grouping.
  - **What evidence would resolve it:** Evaluation of HAM on standard NLP continual learning benchmarks (e.g., SuperGLUE, LongSeq) using decoder-only or encoder-decoder LLM architectures.

## Limitations

- The paper does not specify the similarity threshold τ_sim used to determine group assignment, which is critical for reproducibility and may significantly affect group formation
- Training hyperparameters like the number of epochs per task are only referenced as being "identical" to baselines without explicit values
- The LoRA scaling factor α (distinct from the learned importance scalars α_i) is referenced but not detailed
- The specific ViT layers receiving LoRA adapters (attention vs. MLP) are not explicitly stated

## Confidence

- **High:** The hierarchical adapter merging mechanism itself (concatenation within groups, pruning, and importance-weighted balancing) is well-defined and theoretically sound. The reported improvements over SD-LoRA and Linear Merging on benchmark datasets are supported by provided metrics.
- **Medium:** The scalability claims and benefits in long-sequence scenarios are plausible given the architecture but rely on comparisons that may have subtle implementation differences.
- **Low:** The exact performance outcomes and some architectural details (e.g., layer placement, hyperparameter specifics) cannot be fully verified without the missing values.

## Next Checks

1. **Sanity Check:** Train two distinct tasks and compare merging via Linear Averaging versus HAM's Concatenation to verify preservation of accuracy for dissimilar tasks.
2. **Ablation Study:** Run HAM on a 10-task sequence with pruning disabled (k=1.0) versus the paper's default (k=0.6) to validate the "noise reduction" claim.
3. **Scalability Test:** Evaluate HAM on a 20-task sequence with varying max groups G_max ∈ {1, 2, 5, 10} to identify the saturation point where increasing groups yields diminishing returns.