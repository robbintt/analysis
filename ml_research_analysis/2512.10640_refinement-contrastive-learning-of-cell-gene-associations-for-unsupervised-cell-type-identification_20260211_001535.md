---
ver: rpa2
title: Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised
  Cell Type Identification
arxiv_id: '2512.10640'
source_url: https://arxiv.org/abs/2512.10640
tags:
- cell
- spatial
- learning
- gene
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces scRCL, a refinement contrastive learning
  framework for unsupervised cell type identification in single-cell omics data. The
  key innovation is explicitly incorporating cell-gene associations through a refinement
  module that models gene-gene correlations and strengthens connections between cells
  and their associated genes.
---

# Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification

## Quick Facts
- arXiv ID: 2512.10640
- Source URL: https://arxiv.org/abs/2512.10640
- Reference count: 13
- Key outcome: scRCL achieves 5-20 percentage point accuracy improvements over state-of-the-art baselines in unsupervised cell type identification

## Executive Summary
This paper introduces scRCL, a refinement contrastive learning framework for unsupervised cell type identification in single-cell omics data. The method uniquely incorporates cell-gene associations through a refinement module that models gene-gene correlations and strengthens connections between cells and their associated genes. By combining heterogeneous embedding alignment, neighborhood distribution contrastive alignment, and cross-view correlation contrastive learning, scRCL captures both intrinsic cellular structures and biologically meaningful cell-gene relationships. Experiments on multiple scRNA-seq and spatial transcriptomics datasets demonstrate superior performance compared to state-of-the-art baselines, with accuracy improvements ranging from 5-20 percentage points across different datasets.

## Method Summary
scRCL employs a three-component contrastive learning framework: heterogeneous embedding alignment using SKL divergence between GCN and MLP encoder outputs, neighborhood distribution contrastive alignment that treats connected cells as positive pairs, and cell-gene interaction-aware representation refinement through inverse tri-matrix factorization. The refinement module integrates gene-correlation structure learning by mapping cell embeddings through learned gene embeddings derived from the gene graph. The final representation concatenates refined embeddings from both views and is clustered using k-means. The framework is generalizable to both scRNA-seq and spatial transcriptomics data.

## Key Results
- scRCL achieves 5-20 percentage point improvements in ACC, NMI, and ARI compared to state-of-the-art baselines
- The heterogeneous encoder approach (GCN+MLP) outperforms homogeneous encoder pairs
- Biological analyses confirm recovered clusters exhibit coherent gene-expression signatures
- The refinement module significantly contributes to performance gains through ablation studies

## Why This Works (Mechanism)

### Mechanism 1
Heterogeneous dual-encoder alignment captures complementary structural and feature-based representations better than homogeneous encoder pairs. A GCN encoder processes the gene expression matrix with cell graph adjacency, capturing topological relationships, while an MLP encoder processes the expression matrix independently, capturing attribute patterns. Symmetric KL divergence aligns both global embedding distributions and per-cell distributions bidirectionally, forcing the model to extract consistent semantic signals from both inductive biases. Core assumption: Graph-structured and feature-based views provide complementary information that, when aligned, yield more robust representations than either alone. Break condition: If GCN and MLP embeddings diverge excessively (SKL >> threshold), alignment gradient conflict may destabilize training.

### Mechanism 2
Neighborhood-aware contrastive alignment enforces intra-class semantic coherence while improving inter-class separability by leveraging cell graph topology. For each anchor cell, embeddings from both views of the cell and its KNN neighbors are treated as positive pairs; all other cells form negative pairs. The loss minimizes average SKL divergence for positive pairs while maximizing it for negatives, creating a margin-based contrastive signal informed by spatial or expression-derived graph structure. Core assumption: Connected cells in the cell graph share biological identity (functional homology or spatial proximity), making them suitable positive pairs for contrastive learning. Break condition: If cell graph construction is noisy (e.g., KNN on highly sparse expression yields spurious edges), positive pairs may bridge distinct cell types, degrading cluster purity.

### Mechanism 3
Gene-correlation-informed refinement of cell embeddings through inverse tri-matrix factorization explicitly captures cell-gene associations, improving biological coherence of learned representations. A GCN-based gene encoder produces gene embeddings U from the transposed expression matrix and gene graph G. Cell embeddings E are refined via Z = E · F(W) · U, where F(W) is a learnable linear transformation. This factorization bridges cells to their associated genes, allowing each cell to integrate information from relevant marker genes. A cross-view correlation loss enforces that cross-view cosine similarity approximates the adjacency structure. Core assumption: Gene-gene co-expression structure encodes regulatory pathway information that, when coupled with cell embeddings, reveals discriminative cell-type-specific marker patterns. Break condition: If gene graph G is poorly estimated (e.g., from noisy correlations), gene embeddings may not reflect true co-expression modules.

## Foundational Learning

- Concept: **Symmetric Kullback-Leibler (SKL) Divergence**
  - Why needed here: Core alignment metric for all contrastive losses; SKL(P,Q) = KL(P||Q) + KL(Q||P) ensures bidirectional distribution matching, preventing one view from dominating.
  - Quick check question: Can you explain why SKL is preferred over unidirectional KL when aligning embeddings from two different encoder architectures?

- Concept: **Graph Convolutional Networks (GCNs) for Biological Graphs**
  - Why needed here: GCNs propagate information across cell-cell or gene-gene graphs, smoothing representations while preserving local structure; essential for capturing spatial proximity or co-expression patterns.
  - Quick check question: How does a GCN layer aggregate neighbor information, and why might over-smoothing occur with too many layers on sparse biological graphs?

- Concept: **Contrastive Learning Fundamentals (Positive/Negative Pairs)**
  - Why needed here: The framework relies on defining what should be similar (positive pairs: same cell across views, neighbors) vs. dissimilar (negative pairs: distant cells) to learn discriminative embeddings without labels.
  - Quick check question: In this paper, what defines a "positive pair" in the neighborhood contrastive loss, and how might the choice of K for KNN affect the quality of these pairs?

## Architecture Onboarding

- Component map:
  1. Input Layer: Gene expression matrix X (N×M), cell graph A (N×N via KNN), gene graph G (M×M via correlation)
  2. Encoders: MLP encoder φ(X) → E_m; GCN encoder ψ(X, A) → E_g; Gene GCN encoder φ(X^T, G) → U
  3. Alignment Module: SKL-based distribution alignment (L_HEA + L_NDC)
  4. Refinement Module: Z_m = E_m · W · U; Z_g = E_g · W · U with cross-view correlation loss (L_CVC)
  5. Output: Concatenated Z = [Z_m | Z_g] → k-means clustering

- Critical path:
  1. Construct cell graph A (KNN on expression or spatial coordinates)
  2. Estimate gene graph G (correlation on transposed expression)
  3. Forward pass through all three encoders
  4. Compute L_HEA (global + per-cell SKL between E_m and E_g)
  5. Compute L_NDC (neighborhood SKL contrastive)
  6. Compute refinement embeddings Z_m, Z_g and L_CVC
  7. Joint optimization: L = L_HEA + α·L_NDC + β·L_CVC
  8. Run k-means on final Z

- Design tradeoffs:
  - Heterogeneous vs. homogeneous encoders: Paper shows heterogeneous (GCN+MLP) outperforms same-architecture pairs; tradeoff is added complexity in hyperparameter tuning for two encoder types.
  - Gene graph construction: Simple correlation-based G may include spurious edges; more sophisticated methods (e.g., GENIE3, SCENIC) could improve but add computational cost.
  - Refinement function F(W): Implemented as linear transformation for simplicity; nonlinear alternatives (e.g., attention) could capture more complex cell-gene interactions but risk overfitting on small datasets.

- Failure signatures:
  1. SKL divergence exploding during training: Indicates embedding distributions diverging; check learning rate and encoder initialization.
  2. Clustering accuracy drops significantly on removing L_CVC: Confirms refinement module is active; if no drop, gene graph may be uninformative.
  3. Marker genes are housekeeping genes (e.g., Eef1a1): Suggests refinement not capturing cell-type-specific signals; verify gene graph quality and W learning.

- First 3 experiments:
  1. Ablation on encoder pair: Replace GCN+MLP with GCN+GCN and MLP+MLP; measure ACC/NMI/ARI on one dataset (e.g., Lung) to confirm heterogeneous encoder benefit.
  2. Vary K for cell graph construction: Test K ∈ {5, 10, 15, 20} on a spatial transcriptomics dataset (e.g., DLPFC slice); visualize clustering maps to assess spatial coherence.
  3. Marker gene specificity check: After training on Lung dataset, extract top 3 marker genes per cluster; cross-reference with CellMarker database to compute fraction of known markers recovered vs. baseline (scRCL without refinement).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a non-linear refinement function better capture complex cell-gene associations than the implemented linear transformation?
- Basis in paper: The authors state: "In our implementation, we simplify F to a linear transformation parameterized by the learnable matrix W."
- Why unresolved: While the linear simplification reduces model complexity, biological interactions are often non-linear, and the paper does not explore if a more complex function yields better representation learning.
- What evidence would resolve it: Ablation experiments comparing the linear layer against multi-layer perceptrons or attention-based mechanisms within the refinement module.

### Open Question 2
- Question: Is scRCL computationally efficient enough to scale to atlas-level datasets containing millions of cells?
- Basis in paper: The framework utilizes Graph Convolutional Networks (GCNs), which typically suffer from high memory consumption on large graphs, yet the paper provides no complexity analysis or runtime benchmarks.
- Why unresolved: The evaluation is limited to standard benchmark datasets, leaving the feasibility of applying scRCL to massive, whole-organ atlases unknown.
- What evidence would resolve it: Runtime and memory usage analysis on datasets exceeding 500,000 cells compared against scalable baselines.

### Open Question 3
- Question: Would integrating histological image features improve spatial domain identification accuracy?
- Basis in paper: The paper compares scRCL against methods like STAIG that utilize histology, but scRCL relies solely on gene expression and spatial coordinates.
- Why unresolved: While the method outperforms baselines, it is unclear if the explicit exclusion of tissue morphology limits the identification of morphologically distinct but transcriptionally similar regions.
- What evidence would resolve it: Extending the framework to process histology patches as a third view in the contrastive alignment process.

## Limitations
- Biological interpretability validation is limited to qualitative marker gene analysis without quantitative validation against established marker gene databases
- The refinement module's contribution is demonstrated through ablation, but the specific gene-gene correlations captured by the gene graph G are not systematically analyzed
- The framework's scalability to atlas-level datasets containing millions of cells remains untested

## Confidence
- **High**: Clustering performance improvements (ACC/NMI/ARI gains of 5-20 percentage points)
- **Medium**: Biological relevance of recovered clusters (qualitative marker gene analysis)
- **Low**: Mechanistic understanding of why gene-correlation refinement improves biological coherence (limited analysis of learned gene-gene relationships)

## Next Checks
1. Quantitatively validate recovered clusters by computing precision/recall of top marker genes against CellMarker database for at least two datasets
2. Perform ablation studies removing the gene graph G entirely to measure impact on both clustering accuracy and biological interpretability
3. Analyze learned gene-gene correlations in the gene graph G to verify enrichment for known biological pathways (e.g., KEGG enrichment analysis)