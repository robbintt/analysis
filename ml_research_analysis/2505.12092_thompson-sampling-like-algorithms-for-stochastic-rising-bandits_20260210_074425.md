---
ver: rpa2
title: Thompson Sampling-like Algorithms for Stochastic Rising Bandits
arxiv_id: '2505.12092'
source_url: https://arxiv.org/abs/2505.12092
tags:
- have
- regret
- lemma
- algorithms
- expected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Thompson Sampling-like algorithms for stochastic\
  \ rising rested bandits (SRRBs), where arm expected rewards increase as they are\
  \ pulled. The authors propose ET-Beta-SWTS and \u03B3-ET-SWGTS algorithms, which\
  \ incorporate sliding-window Thompson Sampling with forced exploration to handle\
  \ the dynamic nature of SRRBs."
---

# Thompson Sampling-like Algorithms for Stochastic Rising Bandits

## Quick Facts
- arXiv ID: 2505.12092
- Source URL: https://arxiv.org/abs/2505.12092
- Reference count: 40
- Primary result: Introduces Thompson Sampling-like algorithms (ET-Beta-SWTS and γ-ET-SWGTS) for stochastic rising rested bandits with sublinear regret guarantees when complexity index σ_μ(T) is bounded

## Executive Summary
This paper addresses stochastic rising rested bandits (SRRBs), where arm expected rewards increase as they are pulled. The authors propose Thompson Sampling-like algorithms that incorporate sliding-window Thompson Sampling with forced exploration to handle the dynamic nature of SRRBs. The key contribution is introducing a complexity index σ_μ(T) that measures the number of pulls needed to identify the optimal arm, enabling sublinear regret analysis for this non-stationary setting. Theoretical analysis shows that when this complexity index is bounded, the proposed algorithms achieve sublinear regret with explicit bounds depending on both the complexity index and total variation distance between reward distributions.

## Method Summary
The paper introduces ET-Beta-SWTS and γ-ET-SWGTS algorithms that combine forced exploration with sliding-window Thompson Sampling. The forced exploration phase pulls each arm Γ times initially, while the sliding window maintains only recent reward data within a window of size τ. The algorithms use Beta priors for Bernoulli rewards and Gaussian priors for subgaussian rewards, updating posteriors based on observed rewards within the sliding window. The method is validated through synthetic experiments on rising reward functions and real-world experiments using the IMDB dataset for online model selection.

## Key Results
- Introduces complexity index σ_μ(T) measuring pulls needed to identify optimal arm in rising bandits
- Proves sublinear regret bounds O(σ + log(T)/∆²) when complexity index is bounded
- First Thompson Sampling analysis for stochastic rising rested bandits
- Numerical simulations show TS-like algorithms outperform state-of-the-art approaches in many scenarios
- Demonstrates practical application on IMDB dataset for online model selection

## Why This Works (Mechanism)

### Mechanism 1: Complexity Index σ_μ(T) and Detectability
The learnability of SRRBs is governed by complexity index σ_μ(T), defined as pulls required for optimal arm's average expected reward to exceed maximum final reward of any suboptimal arm. When σ_μ(T) is small (independent of T), optimal arm becomes distinguishable early, reducing problem to stationary bandit where TS achieves sublinear regret.

### Mechanism 2: Forced Exploration (Γ)
Forced exploration phase where each arm is pulled Γ times initially controls regret from non-stationary rewards. This ensures Γ ≥ σ_μ(T) gathers enough data to identify optimal arm i*(T) reliably before exploitation, bounding dissimilarity terms in regret analysis.

### Mechanism 3: Analytical Tractability via Stochastic Dominance
Frequentist analysis holds because worst-case estimation error of optimal arm is bounded by stationary scenario. Poisson-Binomial distribution (non-identical success probabilities) has lower (better) expectation than standard Binomial for key TS analysis terms, allowing reuse of standard TS analysis tools.

## Foundational Learning

- **Concept: Thompson Sampling (Bayesian Bandits)**
  - Why needed: Modified standard TS algorithm (Beta and Gaussian priors) handles rising rewards
  - Quick check: Explain how Beta prior updates with Bernoulli rewards and how posterior variance decreases with more samples

- **Concept: Rested vs. Restless Bandits**
  - Why needed: Addresses "Rested" bandits where rewards change only when arm is pulled
  - Quick check: In rising rested bandit, if you stop pulling an arm, does its expected reward continue to increase?

- **Concept: Regret Definitions in Non-Stationary Settings**
  - Why needed: Minimizes regret against optimal policy at end of horizon T, not best static arm
  - Quick check: Why is comparing against best static arm insufficient when arms improve as pulled?

## Architecture Onboarding

- **Component map:** Input (K arms, T, τ, Γ, algorithm variant) -> Forced Phase (pull each arm Γ times) -> Thompson Sampling Loop (stats, posterior, sampling, selection, buffer update)

- **Critical path:** Selection of Γ. Theoretical guarantees rely on Γ being large enough to cover complexity index σ_μ(T). If Γ is too small, algorithm fails to identify best arm before gap closes.

- **Design tradeoffs:**
  - Window Size (τ): Small τ increases variance but reduces bias if environment changes abruptly or Γ was misspecified; large τ uses all history, minimizing variance beneficial if rising trend is consistent
  - Prior Choice: ET-Beta-SWTS for bounded [0,1] rewards (Bernoulli); γ-ET-SWGTS for unbounded or subgaussian rewards with variance scaling parameter γ

- **Failure signatures:**
  - Linear Regret: Occurs if Γ underestimated and rising functions are "hard" (indistinguishable initially)
  - High Variance: Occurs if τ set too small relative to noise λ², causing posterior to fluctuate wildly

- **First 3 experiments:**
  1. Sanity Check (Stationary): Set reward functions to constants, verify algorithm reduces to standard TS with logarithmic regret (σ_μ(T)=1)
  2. Sensitivity to Γ: Run on synthetic SRRB with known complexity σ, vary Γ from 0 to 2σ, plot regret to observe phase transition
  3. Sliding Window Ablation: Compare full-memory (τ=T) vs. sliding-window (τ≪T) on "hard" instance to verify window helps or hurts when Γ is misspecified

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Γ and τ be adapted online without prior knowledge of σ_μ(T)?
- Basis: Theorems depend on free parameter σ, and optimal window length is instance-dependent
- Why unresolved: Theoretical guarantees require oracle knowledge of problem complexity unavailable to learner
- What evidence would resolve it: Regret analysis for algorithm selecting Γ and τ via adaptive scheme or meta-learning

### Open Question 2
- Question: Does enforcing concavity assumption allow sublinear minimax regret bounds in SRRB setting?
- Basis: Section 6.1 states worst-case regret degenerates to linear without further structure
- Why unresolved: Paper analyzes general non-concave case to maintain generality
- What evidence would resolve it: Theoretical derivation of sublinear minimax regret bounds specific to concave rising bandits

### Open Question 3
- Question: Can regret guarantees be extended to non-subgaussian reward distributions?
- Basis: Analysis constrained by Assumption 3.3 requiring λ²-subgaussian rewards
- Why unresolved: Proof techniques rely heavily on concentration inequalities specific to subgaussian tails
- What evidence would resolve it: Extension of technical tools to heavy-tailed distributions using bounded moment assumptions

## Limitations
- Complexity index σ_μ(T) provides powerful analytical lens but practical estimation from data remains unaddressed
- Theoretical bounds assume known or estimable σ_μ(T) to set Γ, with no guidance for hyperparameter tuning
- Superior performance demonstrated when σ_μ(T) is low, but paper doesn't thoroughly explore scenarios where complexity index scales with T

## Confidence

- **High confidence:** Regret analysis framework and proof technique leveraging stochastic dominance for Poisson-Binomial distributions are mathematically sound and well-established
- **Medium confidence:** Practical superiority demonstrated empirically, though synthetic experiments rely on specific function families and parameter ranges
- **Low confidence:** General applicability of complexity index σ_μ(T) to arbitrary rising functions beyond studied exponential and polynomial families remains unproven

## Next Checks

1. **Complexity Index Estimation:** Develop method to estimate σ_μ(T) from observed data (e.g., via change-point detection) and validate estimated value correlates with actual algorithm performance

2. **Hard Instance Stress Test:** Construct synthetic SRRB where complexity index σ_μ(T) scales linearly with T (very slow-rising functions with small initial gaps) and verify algorithms degrade to linear regret as predicted

3. **Prior Sensitivity Analysis:** Systematically vary prior parameters in ET-Beta-SWTS and measure impact on regret, particularly in forced exploration phase, to quantify robustness to prior misspecification