---
ver: rpa2
title: Stochastic Engrams for Efficient Continual Learning with Binarized Neural Networks
arxiv_id: '2503.21436'
source_url: https://arxiv.org/abs/2503.21436
tags:
- learning
- neural
- continual
- networks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  by proposing a novel approach that integrates stochastically-activated engrams as
  a gating mechanism for metaplastic binarized neural networks (mBNNs). The method
  combines the computational efficiency of mBNNs with the robustness of probabilistic
  memory traces to mitigate forgetting and maintain model reliability.
---

# Stochastic Engrams for Efficient Continual Learning with Binarized Neural Networks

## Quick Facts
- arXiv ID: 2503.21436
- Source URL: https://arxiv.org/abs/2503.21436
- Reference count: 40
- Primary result: Achieves 20%+ accuracy improvement in class-incremental scenarios while reducing GPU usage to under 5% and RAM usage to under 20% compared to full-precision methods.

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by proposing a novel approach that integrates stochastically-activated engrams as a gating mechanism for metaplastic binarized neural networks (mBNNs). The method combines the computational efficiency of mBNNs with the robustness of probabilistic memory traces to mitigate forgetting and maintain model reliability. Engram gating blocks produce gating vectors to each mBNN hidden layer, and a metaplasticity function regulates synaptic plasticity based on prior learning. Experiments on Split-MNIST and CORe50-NI datasets show that the proposed engramBNN method achieves average accuracies over 20% in class-incremental scenarios and comparable domain-incremental results to full precision state-of-the-art methods. Additionally, it achieves a significant reduction in peak GPU and RAM usage, under 5% and 20%, respectively. The method demonstrates an improved stability vs. plasticity trade-off, reduced memory intensiveness, and enhanced performance in binarized architectures.

## Method Summary
The method integrates stochastically-activated engrams as a gating mechanism for metaplastic binarized neural networks. The architecture consists of a main mBNN backbone with two hidden layers of 2048 neurons each, combined with an engram gating block that processes inputs through an encoder (512→128→64) into a latent space, then through a linear layer with stochastic sign activation to produce gating vectors. The metaplasticity function modulates synaptic plasticity based on weight magnitude, making high-magnitude weights resistant to change. During training, the model uses real-valued hidden weights for backpropagation while maintaining binary weights for inference, achieving extreme memory efficiency. The engram block generates stochastic gating vectors via Bernoulli sampling based on sigmoid-transformed hidden weights, selectively masking hidden layer activations to prevent interference between sequentially learned tasks.

## Key Results
- Average accuracy improvement over 20% in class-incremental scenarios on Split-MNIST
- Comparable domain-incremental results to full precision state-of-the-art methods on CORe50-NI
- Peak GPU usage reduced to 4.51% and RAM usage to 19.51% compared to full precision methods
- Improved stability vs. plasticity trade-off with forward transfer (FWT) of 0.02±0.00 and backward transfer (BWT) of 0.01±0.01

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Engram Gating
Probabilistic neuron gating creates sparse, context-dependent memory traces that reduce interference between sequentially learned tasks. Input passes through an encoder (512→128→64) into a latent space, then through a linear layer with stochastic sign activation. The resulting gating vector (values 0 or +1, sampled via Bernoulli distribution with p = σ(W_h)) selectively masks hidden layers of the main network. Stochastic selection of neuronal ensembles provides more robust memory encoding than deterministic gating, as variance prevents over-specialization to specific input patterns.

### Mechanism 2: Metaplastic Weight Consolidation
Synaptic importance-weighted learning rates protect consolidated knowledge by making high-magnitude weights resistant to change. The metaplastic function f_meta(m, W_h) = 1 - tanh²(m · W_h) modulates gradient updates. When |W_h| is large, f_meta → 0, reducing effective learning rate for that synapse. Updates proceed normally when |W_h| is small. Absolute weight magnitude correlates with synaptic importance for previously learned tasks—an approximation of "synaptic intelligence."

### Mechanism 3: Binarized Inference with Real-Valued Accumulation
Maintaining real-valued "hidden weights" during training while using only sign(W_h) during inference preserves gradient flow while achieving extreme memory efficiency. Forward pass uses binary weights W_b = sign(W_h) ∈ {-1, +1}. Backpropagation computes gradients on W_h, enabling meaningful updates despite discrete inference. Batch normalization stabilizes training. Binary weights provide sufficient representational capacity for target tasks when combined with metaplastic regularization and gating.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The core problem; understanding why sequential gradient updates overwrite prior knowledge is essential to grasp why gating + metaplasticity help.
  - **Quick check question:** Can you explain why training on Task B degrades performance on Task A in standard neural networks?

- **Concept: Stability-Plasticity Trade-off**
  - **Why needed here:** The paper explicitly positions its contribution as "improved stability vs. plasticity trade-off." You need to understand what each term means operationally.
  - **Quick check question:** If a network has high plasticity but low stability, what behavior would you observe during a 5-task continual learning experiment?

- **Concept: Binarized Neural Networks (BNNs)**
  - **Why needed here:** The entire architecture rests on BNN mechanics. Without understanding the real-valued hidden weight vs. binary inference distinction, the metaplasticity mechanism will be confusing.
  - **Quick check question:** Why can't you directly backpropagate through a sign function, and how do BNNs work around this?

## Architecture Onboarding

- **Component map:** Input → Engram Gating Block → Main mBNN (BatchNorm → Hidden1(2048) → BatchNorm → Hidden2(2048) → Output) → Metaplasticity Layer
- **Critical path:** 1) Initialize hidden weights uniformly ∈ [-0.5, 0.5] 2) Compute gate vector via engram block (stochastic sampling active during training) 3) Generate binary weights: W_b = sign(W_h) 4) Apply gating: multiply hidden layer activations by gate vector 5) Forward pass with gated binary activations 6) Compute loss; backpropagate gradients to W_h 7) Apply metaplastic modulation: reduce update magnitude where |W_h| is large 8) Update hidden weights; repeat
- **Design tradeoffs:** m parameter controls consolidation strength (m=5 for Split-MNIST; m=175 for CORe50); engram block capacity (more neurons = richer context encoding, but higher memory overhead); network width (2048→2048 hidden layers; increasing to 4096 for CORe50 improves capacity at cost of efficiency)
- **Failure signatures:** Accuracy plateaus at ~10% (random chance): m too aggressive, network cannot learn new tasks; rapid accuracy collapse after task switch: gating too sparse or metaplasticity disabled; high run-to-run variance (std > 5%): stochastic sampling not properly seeded, or gating probabilities poorly calibrated
- **First 3 experiments:** 1) Baseline replication: Run Split-MNIST with m=5, two hidden layers of 2048 neurons. Track per-task accuracy after each task completes. Expect ACC ~0.27±0.01. 2) Ablation study: Disable stochastic activation (use deterministic sigmoid threshold at 0.5) and compare ACC. Isolate contribution of stochasticity vs. gating architecture. 3) Hyperparameter sweep: Test m ∈ {1, 5, 10, 50, 100, 175} on CORe50-NI. Plot ACC vs. REM to visualize stability-plasticity frontier. Identify collapse point where forward transfer drops.

## Open Questions the Paper Calls Out
- Can replacing the standard MLP backbone with convolutional or spiking neural network architectures improve the model's ability to handle complex visual data while maintaining efficiency?
- Can the engramBNN method effectively generalize to time-series data such as brain signals, which possess different statistical properties than the visual datasets tested?
- Can semi-supervised techniques like noisy student labeling be successfully integrated to mitigate the reliance on explicit ground-truth labels during continual training?

## Limitations
- Architecture mapping ambiguity: The engram block's five-layer structure (512→128→64→2048→2048) is not explicitly mapped to the stated functional components, requiring assumptions for implementation
- Extreme hyperparameter sensitivity: The m parameter shows extreme sensitivity (m=5 for Split-MNIST vs m=175 for CORe50-NI), suggesting task-dependent calibration is critical but poorly understood
- Limited cross-task generalization: The method has only been validated on static image benchmarks, leaving its applicability to temporal, non-visual data unproven

## Confidence
- **High**: BNN mechanics with metaplastic weight consolidation (theoretical grounding is well-established)
- **Medium**: Engram gating effectiveness (supported by probabilistic masking logic but lacks ablation studies isolating gating vs stochasticity contributions)
- **Medium**: Resource efficiency claims (quantitative measurements provided, but comparison methodology not fully specified)
- **Low**: Cross-task generalization of m parameter (no systematic sweep shown, values appear empirically tuned)

## Next Checks
1. **Architecture fidelity test**: Implement the engram block with both possible layer mappings (encoder-only vs encoder+latent) and measure performance degradation to identify the correct interpretation.
2. **Stochasticity ablation**: Replace stochastic Bernoulli sampling with deterministic sigmoid thresholding and measure change in ACC to quantify stochasticity's contribution.
3. **Metaplasticity sensitivity sweep**: Systematically vary m from 1-200 on CORe50-NI and plot ACC vs m to identify optimal range and collapse thresholds.