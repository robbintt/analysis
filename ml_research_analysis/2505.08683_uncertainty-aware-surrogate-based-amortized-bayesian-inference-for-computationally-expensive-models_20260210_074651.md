---
ver: rpa2
title: Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for Computationally
  Expensive Models
arxiv_id: '2505.08683'
source_url: https://arxiv.org/abs/2505.08683
tags:
- surrogate
- inference
- posterior
- ua-sabi
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of performing Bayesian inference
  on computationally expensive models where traditional MCMC or standard ABI are infeasible
  due to high evaluation costs. The core idea is Uncertainty-Aware Surrogate-based
  Amortized Bayesian Inference (UA-SABI), which uses surrogate models to generate
  training data for ABI while explicitly quantifying and propagating surrogate uncertainties.
---

# Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for Computationally Expensive Models

## Quick Facts
- arXiv ID: 2505.08683
- Source URL: https://arxiv.org/abs/2505.08683
- Reference count: 40
- One-line primary result: UA-SABI achieves well-calibrated posteriors for expensive models through surrogate uncertainty propagation, enabling quasi-instant inference after training

## Executive Summary
This paper addresses the challenge of performing Bayesian inference on computationally expensive models where traditional MCMC or standard ABI are infeasible due to high evaluation costs. The core idea is Uncertainty-Aware Surrogate-based Amortized Bayesian Inference (UA-SABI), which uses surrogate models to generate training data for ABI while explicitly quantifying and propagating surrogate uncertainties. Unlike standard ABI that ignores surrogate error or MCMC-based methods that require many expensive evaluations, UA-SABI achieves well-calibrated posteriors through uncertainty propagation.

## Method Summary
UA-SABI combines surrogate modeling with amortized Bayesian inference to enable efficient posterior estimation for expensive models. The method generates training data by evaluating the expensive model at selected points, fits surrogate models to approximate both the forward model and its uncertainties, then trains an inference network using these surrogates. During inference, the approach propagates surrogate uncertainties through the posterior estimation process, ensuring well-calibrated results. The key innovation is the explicit treatment of surrogate uncertainty rather than treating the surrogate as ground truth, which addresses a fundamental limitation of standard ABI approaches when applied to expensive models.

## Key Results
- UA-SABI produces reliable posteriors comparable to MCMC-based approaches while enabling quasi-instant inference after training
- Computational advantages become significant after only 4-9 inference runs, depending on model complexity
- Three case studies demonstrate the method's effectiveness across different problem types and dimensionalities

## Why This Works (Mechanism)
The approach works by recognizing that surrogate models are approximations with inherent uncertainties, and these uncertainties must be properly propagated through the inference process to obtain calibrated posteriors. By explicitly modeling and propagating surrogate uncertainty, UA-SABI avoids the overconfidence that would result from treating surrogate predictions as exact. The amortization allows for rapid subsequent inference once the network is trained, making the approach particularly valuable when multiple inference tasks are needed for the same model.

## Foundational Learning

**Surrogate Modeling**: Creating computationally cheap approximations of expensive models. Why needed: Direct Bayesian inference on expensive models is computationally prohibitive. Quick check: Verify surrogate accuracy on held-out test points.

**Uncertainty Quantification**: Characterizing and propagating uncertainty through computational pipelines. Why needed: Surrogate uncertainties must be properly accounted for to avoid overconfident posterior estimates. Quick check: Ensure uncertainty estimates are well-calibrated on validation data.

**Amortized Inference**: Training a neural network to perform inference for multiple datasets. Why needed: Enables rapid inference after initial training cost is amortized. Quick check: Verify inference speed-up compared to traditional MCMC.

## Architecture Onboarding

**Component Map**: Expensive model -> Surrogate training -> Inference network training -> Posterior estimation with uncertainty propagation

**Critical Path**: The sequence from surrogate training through inference network training to posterior estimation. Each step depends on the previous one, and errors compound if any component is poorly specified.

**Design Tradeoffs**: Computational budget allocation between surrogate training and inference network training, accuracy vs. speed tradeoff in surrogate modeling, and the balance between model complexity and training data requirements.

**Failure Signatures**: Poor posterior calibration when surrogate uncertainties are underestimated, slow inference if the network is under-trained, and computational inefficiency if too many expensive model evaluations are required for surrogate training.

**First Experiments**: 1) Test on a simple analytical model with known posterior to verify basic functionality. 2) Compare UA-SABI results with standard ABI on a moderately expensive model. 3) Evaluate break-even point by counting inference runs needed to justify the training overhead.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach's effectiveness depends heavily on the quality of surrogate uncertainty quantification
- Validation is limited to three case studies with relatively small parameter spaces (up to 4 dimensions)
- The "quasi-instant" inference claim depends on surrogate quality and may not hold for highly complex or multimodal posteriors

## Confidence
- High confidence: The fundamental concept of combining surrogate models with ABI for expensive models is sound and well-supported by the experimental results
- Medium confidence: The computational cost advantages are demonstrated but may vary significantly with different model characteristics and inference requirements
- Medium confidence: The uncertainty quantification methodology is theoretically sound but its practical robustness across diverse applications needs further validation

## Next Checks
1. Test UA-SABI on higher-dimensional problems (10+ parameters) to evaluate scalability and identify potential limitations in more complex inference scenarios
2. Compare performance when using different surrogate modeling approaches (e.g., Gaussian processes, neural networks) to assess robustness to surrogate choice
3. Evaluate the method's performance on problems with highly multimodal posteriors where surrogate uncertainty quantification becomes more challenging