---
ver: rpa2
title: 'Reward Models in Deep Reinforcement Learning: A Survey'
arxiv_id: '2506.15421'
source_url: https://arxiv.org/abs/2506.15421
tags:
- reward
- learning
- arxiv
- rewards
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews reward modeling techniques
  in deep reinforcement learning, addressing the challenge of designing effective
  reward functions that align with true objectives while facilitating policy optimization.
  The authors categorize reward models based on their source (human-provided vs.
---

# Reward Models in Deep Reinforcement Learning: A Survey

## Quick Facts
- arXiv ID: 2506.15421
- Source URL: https://arxiv.org/abs/2506.15421
- Authors: Rui Yu; Shenghua Wan; Yucen Wang; Chen-Xiao Gao; Le Gan; Zongzhang Zhang; De-Chuan Zhan
- Reference count: 16
- Primary result: Comprehensive survey categorizing reward modeling techniques by source, mechanism, and learning paradigm, covering human/AI sources, intrinsic/extrinsic mechanisms, and demonstrations/goals/preferences.

## Executive Summary
This survey systematically reviews reward modeling techniques in deep reinforcement learning, addressing the critical challenge of designing effective reward functions that align with true objectives while facilitating policy optimization. The authors organize methods based on reward source (human-provided vs AI-generated), mechanism (intrinsic vs extrinsic), and learning paradigm (demonstrations, goals, or preferences). They cover established approaches like inverse reinforcement learning and preference-based learning alongside emerging methods using foundation models like LLMs and VLMs. The survey also discusses applications across control problems, generative model post-training, and recommendation systems, while examining evaluation techniques including policy performance, distance metrics, and interpretable representations.

## Method Summary
This survey categorizes reward modeling approaches in deep RL by three dimensions: source (human/AI), mechanism (intrinsic/extrinsic), and learning paradigm (demonstrations, goals, preferences). For demonstrations, it covers Maximum Entropy Inverse Reinforcement Learning using discriminator formulations. For goals, it discusses distance-based reward shaping using spatial and temporal metrics. For preferences, it presents Bradley-Terry models and extensions like Preference Transformers. The survey also examines intrinsic rewards based on prediction error and novelty, and emerging methods using LLMs/VLMs for reward modeling. Evaluation techniques include policy performance metrics, distance-based comparisons (EPIC, STARC), and interpretable representations.

## Key Results
- Reward models can be learned from human preferences via Bradley-Terry models, then used for policy optimization through standard RL
- Intrinsic rewards based on prediction error or novelty can supplement sparse extrinsic rewards to accelerate exploration
- Distance-based reward shaping converts sparse goal specifications into dense learning signals by measuring progress
- Foundation models (LLMs/VLMs) show promise for scalable reward modeling but require further validation
- Evaluation frameworks including EPIC distance and STARC metrics enable systematic comparison of reward model quality

## Why This Works (Mechanism)

### Mechanism 1
Intrinsic rewards computed from prediction error or novelty can supplement sparse extrinsic rewards to accelerate exploration. The agent receives combined reward r = λr_int + (1-λ)r_ext, where intrinsic reward r_int quantifies "strangeness" (prediction error, visitation counts, or entropy). States with high epistemic uncertainty yield higher r_int, pushing the agent toward underexplored regions. This works because unknown states correlate with valuable learning opportunities, and prediction error approximates environment novelty. The approach breaks if environments have high observation noise or non-stationary dynamics, where prediction error may not indicate novelty.

### Mechanism 2
Reward models can be learned from human preference comparisons by fitting a Bradley-Terry model, then used to train policies via standard RL. Given pairwise trajectory preferences τ^0 ≻ τ^1, the reward model R_θ is trained to maximize likelihood under the Boltzmann distribution: P(τ^0 ≻ τ^1) ∝ exp(Σ R_θ(s,a)). The learned R_θ then provides scalar rewards for policy optimization. This works because human preferences are assumed consistent with a latent scalar reward function and are transitive enough for the Boltzmann assumption to hold. The method breaks if humans have intransitive or context-dependent preferences, or if the preference model is misspecified.

### Mechanism 3
Distance-based reward shaping converts sparse goal specifications into dense learning signals by measuring progress toward the goal state. Instead of binary reward R(s,g) = 1(s accomplishes g), use R(s,g) = -d(φ(s), ψ(g)), where d is a spatial or temporal distance metric. Temporal distance approximates the number of steps needed to reach g from s, providing a grounded progress signal. This works because a meaningful distance metric exists in state or latent space that correlates with behavioral proximity to the goal. The approach breaks if the learned distance metric is inaccurate or the state representation is insufficient, potentially creating local optima or misleading gradients.

## Foundational Learning

- **Markov Decision Process (MDP) formulation ⟨S, A, T, R, γ⟩**: All reward modeling techniques are framed as learning or shaping R within this formalism; without understanding states, actions, transitions, and discounting, you cannot evaluate how a reward model influences policy behavior. Quick check: Can you write the expected return J(π) and explain what each term means?

- **Potential-based reward shaping**: Distance-based rewards and interpretable evaluation rely on shaping rewards while preserving optimal policies; this is only guaranteed via potential-based transformations. Quick check: If you add a shaping term F(s,a,s') = γΦ(s') - Φ(s), does the optimal policy change? Why or why not?

- **Maximum entropy regularization**: MaxEnt-IRL and Boltzmann preference models both assume a soft-max policy structure; understanding entropy regularization is essential for connecting reward learning to behavior distributions. Quick check: In MaxEnt-IRL, why is entropy regularization introduced rather than just maximizing the margin between demonstrations and other policies?

## Architecture Onboarding

- **Component map**: Source (Human/AI) → Feedback Type (Demo/Goal/Preference) → Learning Paradigm Module → Reward Model R_θ (Neural Network) → Policy Optimizer (PPO, SAC, etc.) → Agent → Environment → States/Actions → Intrinsic Module (optional) → r_int → Combined Reward: r = λr_int + (1-λ)r_ext

- **Critical path**:
  1. Choose feedback type based on data availability (demonstrations, goals, or preferences)
  2. Select learning paradigm (IRL for demos, goal-conditioned for goals, Bradley-Terry for preferences)
  3. Implement reward model architecture (often a neural network with appropriate input encoding)
  4. Train R_θ on collected feedback data
  5. Integrate with policy optimizer; optionally add intrinsic reward for exploration
  6. Evaluate via policy performance and/or distance metrics (EPIC, STARC)

- **Design tradeoffs**:
  - Human vs. AI sources: Human feedback is more aligned but expensive; AI-generated (LLM/VLM) rewards scale but may inherit model biases
  - Dense vs. sparse rewards: Dense shaping accelerates learning but risks local optima; sparse rewards are cleaner but require more exploration
  - Bradley-Terry vs. advanced preference models: BT is simple but assumes scalar rewards; Preference Transformer or regret-based models capture state importance but add complexity

- **Failure signatures**:
  - Reward hacking: Policy exploits unintended reward maxima (e.g., agent moves in circles to accumulate distance-based reward without reaching goal)
  - Preference inconsistency: Reward model fails to generalize because human labels are noisy or intransitive
  - Exploration collapse: Without intrinsic motivation or sufficient coverage in feedback data, agent never discovers necessary states

- **First 3 experiments**:
  1. **Sanity check with ground-truth reward**: On a simple environment (e.g., Gym-MuJoCo), verify that your reward model training pipeline produces R_θ that correlates with the known reward; use EPIC distance to measure agreement.
  2. **Preference ablation**: Collect a small set of human preferences, train R_θ with Bradley-Terry, and compare policy performance against a baseline using hand-engineered rewards; check for reward hacking behaviors.
  3. **Intrinsic + extrinsic combination**: On a sparse-reward task, compare λ=0 (pure extrinsic), λ=0.5 (balanced), and λ=1 (pure intrinsic); measure sample efficiency and final performance to determine if intrinsic motivation helps.

## Open Questions the Paper Calls Out

### Open Question 1
How can vectorized rewards be constructed to replace scalar rewards and dynamically balance multiple competitive signals? This is identified in Section 8.1 as a future direction to provide more comprehensive feedback. Scalar rewards struggle to capture the complexity of multi-objective trade-offs inherent in many real-world tasks. This would be resolved by a framework where agents successfully optimize multiple conflicting objectives simultaneously using dynamic vectorized feedback.

### Open Question 2
How can ethical principles be quantified and embedded into reward functions while avoiding negative side effects? Section 8.1 lists "Ethical alignment and social value constraints" as a necessary but unresolved research area. Translating abstract social values into concrete mathematical constraints without inducing reward hacking is challenging. This would be resolved by a robust method for encoding ethical constraints that provides formal guarantees of safety during policy optimization.

### Open Question 3
Can a "Reward Foundation Model" be developed to provide general reward values based on diverse inputs (e.g., limb movements)? Section 8.1 proposes the concept of "Reward foundation models" analogous to general representation spaces. Current reward models are typically task-specific; a unified model generalizing across diverse modalities and tasks does not yet exist. This would be resolved by a single pre-trained model capable of zero-shot reward generation for unseen tasks across different environments.

## Limitations
- Network architecture details for reward models are not specified for any method
- Hyperparameters (learning rates, batch sizes, λ values, number of preference queries) are absent
- Data requirements (demonstration/preference counts needed) remain unclear
- Claims about foundation models (LLMs/VLMs) for reward modeling lack extensive validation

## Confidence

- **High confidence**: The categorization framework (source, mechanism, learning paradigm) and established methods (IRL, PbRL, GCRL) are well-grounded in the literature
- **Medium confidence**: Distance-based and intrinsic reward mechanisms work as described in theory, but practical implementation details and empirical validation are limited
- **Low confidence**: Claims about LLM/VLM-based reward modeling are emerging and lack the experimental rigor of established methods

## Next Checks
1. Implement Bradley-Terry preference learning on a simple RL environment and verify reward model correlates with ground-truth via EPIC distance
2. Test intrinsic reward combination (λ tuning) on a sparse-reward task to measure sample efficiency gains
3. Compare distance-based goal rewards against binary success rewards on a navigation task to assess shaping effectiveness