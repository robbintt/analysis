---
ver: rpa2
title: 'DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language
  Models'
arxiv_id: '2506.03933'
source_url: https://arxiv.org/abs/2506.03933
tags:
- adversarial
- diffcap
- diffusion
- image
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffCAP addresses the vulnerability of Vision Language Models (VLMs)
  to adversarial perturbations, which can drastically alter model outputs despite
  being imperceptible to humans. The method introduces a novel diffusion-based purification
  strategy that cumulatively injects random Gaussian noise into adversarially perturbed
  images until their embeddings converge to a predefined similarity threshold, indicating
  potential neutralization of adversarial effects.
---

# DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models

## Quick Facts
- arXiv ID: 2506.03933
- Source URL: https://arxiv.org/abs/2506.03933
- Reference count: 40
- Key outcome: DiffCAP significantly improves adversarial robustness for VLMs while reducing hyperparameter tuning and denoising runtime compared to state-of-the-art diffusion-based defenses

## Executive Summary
DiffCAP introduces a novel diffusion-based adversarial defense for Vision Language Models (VLMs) that dynamically determines the minimal noise injection time needed to neutralize adversarial perturbations. The method cumulatively injects Gaussian noise into adversarially perturbed images until their embeddings converge to a predefined similarity threshold, then uses a pretrained diffusion model to denoise and recover clean representations. Extensive experiments across six datasets, three VLMs, and three task scenarios demonstrate that DiffCAP consistently outperforms existing defense techniques while requiring less than one-third of the denoising runtime compared to previous diffusion-based approaches.

## Method Summary
DiffCAP operates by first applying a forward diffusion process that cumulatively injects Gaussian noise into an adversarial image until the cosine similarity between consecutive embeddings exceeds a threshold (τ=0.96). This stopping criterion indicates the adversarial signal has been sufficiently disrupted. A pretrained diffusion model then performs reverse-time denoising to reconstruct a clean image from this stabilized noisy state. The method leverages a Variance Preserving (VP) SDE with specific noise schedules and dynamically adapts the diffusion time per image based on embedding convergence, significantly reducing computational overhead compared to fixed-step methods like DiffPure.

## Key Results
- DiffCAP reduces required diffusion time to less than one-third of DiffPure's runtime while maintaining or improving clean performance
- Achieves significant robustness improvements across multiple VLMs (OpenFlamingo-9B, LLaVA-1.5-7B, CLIP ViT-B/32) and tasks (image captioning, VQA, zero-shot classification)
- Demonstrates consistent performance across six datasets (COCO, Flickr30k, VQAv2, TextVQA, Caltech101, ImageNet1K) and multiple perturbation strengths
- Theoretical analysis establishes a certified recovery region under forward diffusion and proves semantic changes decrease as the process progresses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting Gaussian noise into an adversarially perturbed image until consecutive embeddings stabilize can neutralize adversarial effects.
- **Mechanism:** The method exploits the insight that adversarial perturbations cause disproportionate shifts in VLM latent embedding space. DiffCAP cumulatively injects Gaussian noise via forward diffusion until cosine similarity between consecutive embeddings exceeds threshold τ, indicating the adversarial signal has been disrupted.
- **Core assumption:** The vision encoder exhibits local Lipschitz continuity after sufficient noise injection, allowing embeddings to converge to a stable "clean" region.
- **Evidence anchors:** Theoretical proof shows expected semantic change between diffusion steps diminishes as process approaches terminal time; experimental validation shows consistent convergence across datasets.

### Mechanism 2
- **Claim:** A pretrained diffusion model can effectively denoise a "stabilized" noisy image to recover a clean representation usable by VLMs.
- **Mechanism:** Once stopping criterion is met, the image is a noisy version of the original clean image. A pretrained diffusion model is then used in reverse to reconstruct a clean image, leveraging the generative prior learned over natural images.
- **Core assumption:** The diffusion model's generative prior is sufficiently strong to map the noisy, "neutralized" image back to the data manifold of natural, clean images.
- **Evidence anchors:** Abstract states pretrained diffusion model is employed to denoise stabilized image; method leverages reverse-time SDE using score network to approximate score function for denoising.

### Mechanism 3
- **Claim:** Dynamically determining the minimal noise injection time reduces computational overhead compared to fixed-step methods.
- **Mechanism:** Instead of fixed forward diffusion time, DiffCAP adapts t per image by checking stopping condition at each step, halting as soon as embedding stability is achieved.
- **Core assumption:** Embedding similarity threshold is a reliable proxy for neutralization of adversarial effects and correlates with minimum required processing.
- **Evidence anchors:** Experimental results show box plot of diffusion time t is significantly smaller than DiffPure's fixed time; paper states requires less than 1/3 of DiffPure's denoising runtime.

## Foundational Learning

- **Concept: Stochastic Differential Equations (SDEs) in Diffusion Models**
  - **Why needed here:** The core of the purification process is defined by forward SDE that adds noise and reverse-time SDE that removes it. Understanding noise schedule β(t) and drift/diffusion functions is critical.
  - **Quick check question:** How does the Variance Preserving (VP) SDE differ from Variance Exploding (VE) SDE in terms of how noise scales over time?

- **Concept: Adversarial Perturbations and Embedding Sensitivity**
  - **Why needed here:** The paper's key insight is that minimal adversarial noise significantly alters latent embeddings. Knowledge of how these perturbations are crafted and their effect on deep feature spaces is necessary.
  - **Quick check question:** Why are adversarial perturbations often described as "imperceptible" to humans but "disastrous" for neural network predictions?

- **Concept: Cosine Similarity in Latent Space**
  - **Why needed here:** The algorithm's stopping criterion relies entirely on cosine similarity between embeddings. A clear grasp of this metric and its behavior in high-dimensional embedding spaces is essential.
  - **Quick check question:** What does a cosine similarity close to 1.0 indicate about the relationship between two high-dimensional vectors, and why might it be a better stopping signal than Euclidean distance?

## Architecture Onboarding

- **Component map:** Vision Encoder (φ) -> Forward Diffusion Process -> Stability Checker -> Pretrained Diffusion Denoiser (D)
- **Critical path:** Adversarial Input -> Noise Injection Loop (Forward Diffusion) -> Convergence Check -> Reconstruction (Reverse Diffusion) -> Inference
- **Design tradeoffs:**
  - Threshold (τ) vs. Runtime: Higher τ ensures more thorough neutralization but requires more forward diffusion steps
  - Step Size (Δt) vs. Precision: Smaller step sizes allow finer-grained stopping but increase embedding computations
  - Model Choice: Larger diffusion model improves reconstruction quality but adds inference time
- **Failure signatures:**
  - Loop Never Breaks: Cosine similarity never exceeds τ, check max_steps or deadlocked embeddings
  - Reconstruction Artifacts: Denoised image is blurry or loses semantic details, may indicate over-smoothing
  - Performance Degradation on Clean Images: Defense hurts accuracy on benign inputs, check if τ is too low
- **First 3 experiments:**
  1. Baseline Comparison: Replicate comparison against DiffPure on ImageNet1K zero-shot classification using CLIP
  2. Threshold Ablation: Run image captioning task on COCO dataset while varying τ to plot CIDEr score and runtime per image
  3. Adaptive Attack Test: Implement BPDA attack against protected LLaVA-1.5-7B on TextVQA dataset to validate robustness under white-box threat model

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the cumulative purification framework be effectively extended to discrete text or joint multimodal diffusion processes?
- **Basis in paper:** Section 6 explicitly identifies reliance on image-based diffusion as a limitation and states "Extending the cumulative purification framework to text or multimodal diffusion processes remains an open direction."
- **Why unresolved:** Current method operates on continuous image latents via Gaussian noise injection, whereas text requires discrete token handling or different diffusion formulations.
- **What evidence would resolve it:** Modified DiffCAP algorithm demonstrating robustness against text-based or cross-modal adversarial attacks on VLMs.

### Open Question 2
- **Question:** How does DiffCAP perform against evolving, stronger jailbreaking attacks specifically optimized to bypass diffusion-based defenses?
- **Basis in paper:** Appendix B.4 notes that "benchmarking DiffCAP against such threats falls outside the scope of this work" and labels it a "promising direction for future investigation."
- **Why unresolved:** Paper tests specific adaptive attacks (BPDA/EOT) but does not evaluate against rapidly evolving landscape of specialized jailbreaking techniques targeting VLMs.
- **What evidence would resolve it:** Benchmarks showing DiffCAP's defense success rate against wider array of recent jailbreaking attacks compared to safety-aligned baselines.

### Open Question 3
- **Question:** To what extent do violations of scale invariance and local Lipschitz assumptions impact theoretical guarantees of certified recovery region?
- **Basis in paper:** Theoretical proofs rely on Assumption 1 (Scale Invariance) and Assumption 2 (Lipschitz continuity), which are idealized conditions that may not strictly hold for all VLM architectures.
- **Why unresolved:** While paper notes these are standard, theoretical bound on recovery region depends on them, yet empirical success is shown without verifying these strict mathematical properties.
- **What evidence would resolve it:** Analysis quantifying gap between theoretical t_min and empirical stopping time for models known to violate these assumptions.

## Limitations
- Clean-image overhead: Purification slows benign inference, and no accuracy-vs-latency tradeoff curve is provided
- Embedding sensitivity: Relying on a frozen vision encoder makes the method vulnerable to encoder-specific attacks
- Scalability: Diffusion denoising scales poorly to high-res images and larger VLMs

## Confidence
- **High** for adaptive stopping criterion: Empirically validated across tasks and consistently reduces diffusion time versus fixed-step baselines
- **Medium** for neutralization hypothesis: Theoretical analysis shows diminishing semantic drift, but direct ablation on when embeddings converge versus why adversarial effects vanish is not shown
- **Low** for exact diffusion model specification: Paper cites source but omits resolution (256px vs 512px) and conditional vs unconditional architecture

## Next Checks
1. **Threshold Ablation**: Run DiffCAP with τ ∈ {0.90, 0.94, 0.96, 0.98} on clean COCO images; plot CIDEr score and mean diffusion time to quantify accuracy-latency tradeoff omitted in the paper
2. **Encoder Independence**: Repeat ImageNet1K ZSC with different vision encoder (e.g., DINOv2) to test if stopping criterion generalizes or is CLIP-specific
3. **BPDA Attack Robustness**: Implement Backward Pass Differentiable Approximation attack against DiffCAP-protected LLaVA-1.5-7B on TextVQA to verify white-box robustness beyond gray-box evaluations