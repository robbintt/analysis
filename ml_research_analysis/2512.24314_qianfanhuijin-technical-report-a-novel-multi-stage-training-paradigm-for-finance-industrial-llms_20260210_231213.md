---
ver: rpa2
title: 'QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for
  Finance Industrial LLMs'
arxiv_id: '2512.24314'
source_url: https://arxiv.org/abs/2512.24314
tags:
- financial
- reasoning
- knowledge
- data
- general
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'QianfanHuijin introduces a novel four-stage progressive training
  paradigm for financial domain LLMs, addressing the limitations of general models
  in reasoning and tool use. The approach combines two-stage continual pre-training
  for knowledge injection and capability enhancement, followed by specialized post-training:
  SFT, Reasoning RL for financial logic and computation, Agentic RL for tool collaboration,
  and General RL for alignment.'
---

# QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs

## Quick Facts
- arXiv ID: 2512.24314
- Source URL: https://arxiv.org/abs/2512.24314
- Authors: Shupeng Li; Weipeng Lu; Linyun Liu; Chen Lin; Shaofei Li; Zhendong Tan; Hanjun Zhong; Yucheng Zeng; Chenghao Zhu; Mengyue Liu; Daxiang Dong; Jianmin Wu; Yunting Xiao; Annan Li; Danyu Liu; Jingnan Zhang; Licen Liu; Dawei Yin; Dou Shen
- Reference count: 34
- Primary result: QianfanHuijin-70B achieves 94.43 on FLameCer and 89.59 on FinanceIQ, outperforming comparable models

## Executive Summary
QianfanHuijin introduces a novel four-stage progressive training paradigm for financial domain LLMs, addressing the limitations of general models in reasoning and tool use. The approach combines two-stage continual pre-training for knowledge injection and capability enhancement, followed by specialized post-training: SFT, Reasoning RL for financial logic and computation, Agentic RL for tool collaboration, and General RL for alignment. A Controllable Instruction Synthesis Framework (CIS-F) and dual-verifier reward model ensure high-quality synthetic data and precise RL signals. Evaluations show significant performance gains: QianfanHuijin-70B achieves 94.43 on FLameCer and 89.59 on FinanceIQ, outperforming comparable models. Ablation studies confirm that Reasoning RL and Agentic RL contribute substantial improvements, with reasoning scores rising from 62.23 to 82.60, and to 87.98 with tool use. The dual-mode "Thinking"/"Non-thinking" mechanism optimizes efficiency and accuracy for varied financial tasks.

## Method Summary
The method employs a two-stage continual pre-training (knowledge injection then capability enhancement) followed by a four-stage post-training pipeline: SFT with dual-mode instruction following, Reasoning RL for financial logic using rule-based rewards, Agentic RL for tool use via composite rewards, and General RL for alignment using a dual-verifier reward model. Synthetic data is generated through CIS-F (Knowledge-Driven Generation → Multi-Level Verification → Evolution Module). The training uses pass@10 curriculum learning, DAPO-style invalid gradient pruning, and mastery pool undersampling.

## Key Results
- QianfanHuijin-70B achieves 94.43 on FLameCer and 89.59 on FinanceIQ
- Ablation studies show Reasoning RL and Agentic RL contribute substantial improvements, with reasoning scores rising from 62.23 to 82.60, and to 87.98 with tool use
- Dual-mode "Thinking"/"Non-thinking" mechanism optimizes efficiency and accuracy for varied financial tasks

## Why This Works (Mechanism)

### Mechanism 1: Progressive Post-Training Pipeline
- **Claim**: Inserting specialized RL stages (Reasoning RL and Agentic RL) between SFT and General RL improves financial task performance.
- **Mechanism**: SFT establishes foundational instruction following. Reasoning RL uses rule-based rewards on financial logic/calculation data to internalize multi-hop reasoning. Agentic RL adds composite rewards for tool interaction. This staged approach builds capability hierarchically before final alignment.
- **Core assumption**: Specialized financial reasoning and tool use can be more efficiently learned through targeted RL stages than through general RL alone. Assumption: the capability gains transfer to the final aligned model.
- **Evidence anchors**:
  - [abstract] "Ablation studies confirm that Reasoning RL and Agentic RL contribute substantial improvements, with reasoning scores rising from 62.23 to 82.60"
  - [section 3.1] "General RL fails to effectively bridge the significant reasoning gap between the SFT model and complex financial tasks"
  - [corpus] Limited external validation; Fino1 paper notes absence of empirical study on building effective financial CoT, suggesting this is an emerging area.
- **Break condition**: If specialized RL stages cause catastrophic forgetting of general capabilities, or if reward hacking emerges in rule-based verifiers without generalization.

### Mechanism 2: Dual-Verifier Reward Model
- **Claim**: Combining rule-based and LLM-based verifiers provides more precise RL signals than single-verifier approaches.
- **Mechanism**: Rule-based verifier enforces deterministic correctness (calculations, format). LLM-based verifier evaluates semantic alignment, hallucination detection, and logical coherence. The combination handles both quantifiable and qualitative financial tasks.
- **Core assumption**: Financial tasks decompose cleanly into verifiable sub-components. Assumption: LLM verifiers themselves don't introduce systematic bias.
- **Evidence anchors**:
  - [abstract] "dual-verifier reward model ensure high-quality synthetic data and precise RL signals"
  - [section 3.3.2] "Case Study: Financial Report Commenting... Rule-based Verifier (Fact & Format)... LLM-based Verifier (Logic & Depth)"
  - [corpus] No direct external validation of dual-verifier architecture found in related work.
- **Break condition**: If rule-based and LLM verifiers produce conflicting signals that confuse reward optimization, or if LLM verifier introduces errors the rule-based system would catch.

### Mechanism 3: Curriculum-Based Continual Pre-Training
- **Claim**: Two-stage CPT (Knowledge Injection → Capability Enhancement) improves knowledge retention over single-stage approaches.
- **Mechanism**: Stage 1 mixes financial+general corpora for broad domain adaptation while preventing catastrophic forgetting. Stage 2 concentrates on high-density financial Q&A data to deepen specialization. This coarse-to-fine approach mirrors curriculum learning.
- **Core assumption**: Models can absorb complex financial terminology and logical relations before specialized training. Assumption: the staged approach preserves general language capabilities better than direct fine-tuning.
- **Evidence anchors**:
  - [abstract] "two-stage continual pre-training for knowledge injection and capability enhancement"
  - [section 2.2, Table 1] Performance improves from 74.50 → 79.59 → 80.43 on FinanceIQ across base → Stage 1 → Stage 2
  - [corpus] Related work (FEVO, FinLMM-R1) also emphasizes staged training but without direct comparative data on curriculum approaches.
- **Break condition**: If Stage 2 overfits to Q&A formats at the expense of broader comprehension, or if general capabilities degrade measurably after Stage 2.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here**: The post-training pipeline extends beyond standard RLHF by adding specialized RL stages. Understanding baseline RLHF is essential to see why the authors inserted intermediate stages.
  - **Quick check question**: Can you explain why general RLHF might plateau on specialized tasks like financial calculation?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - **Why needed here**: The dual-mode "Thinking"/"Non-thinking" mechanism and Reasoning RL stage explicitly train CoT for financial logic.
  - **Quick check question**: How does the model decide when to engage Thinking mode vs. Non-thinking mode?

- **Concept: Tool Learning / Agentic Systems**
  - **Why needed here**: Agentic RL stage trains the model to invoke external databases and calculation engines. Understanding tool-augmented LLMs (ReAct paradigm) is prerequisite.
  - **Quick check question**: What reward signals does Agentic RL optimize beyond answer correctness?

## Architecture Onboarding

- **Component map**:
  ```
  Data Pipeline → CPT (Stage 1 → Stage 2) → Post-Training Pipeline
                                                     ↓
  [CIS-F: Knowledge-Driven Generation → Multi-Level Verification → Evolution Module]
                                                     ↓
  [SFT → Reasoning RL → Agentic RL → General RL]
                                ↓
  [Dual-Verifier: Rule-based + LLM-based] → [Thinking/Non-thinking Mode Selection]
  ```

- **Critical path**: CPT Stage 2 quality directly impacts SFT effectiveness. Reasoning RL success is prerequisite for Agentic RL (tool invocation requires reasoning about when/what to call). CIS-F data quality determines RL ceiling.

- **Design tradeoffs**:
  - **Thinking vs. Non-thinking modes**: Latency vs. accuracy tradeoff. Non-thinking optimizes for speed; Thinking for complex reasoning. Requires explicit training data for both.
  - **Rule-based vs. LLM verifier coverage**: Rule-based is precise but narrow; LLM-based is flexible but may miss edge cases. Task-type routing is critical.
  - **CPT corpus mixing ratio**: Too much financial data risks general capability loss; too little limits domain absorption.

- **Failure signatures**:
  - **Reasoning RL plateau**: If scores don't improve after SFT baseline, check data quality (diversity, difficulty stratification) and reward signal clarity.
  - **Agentic RL tool hallucination**: Model invokes non-existent tools or passes incorrect parameters. Verify simulated environment matches real tool schemas.
  - **Catastrophic forgetting in CPT**: General benchmark scores drop significantly after Stage 2. Rebalance financial/general corpus ratio.

- **First 3 experiments**:
  1. **Ablate Reasoning RL**: Train SFT → General RL directly (skip Reasoning and Agentic RL). Compare FinanceReasoning and FinQA scores against full pipeline to quantify specialized RL contribution.
  2. **Verifier-only comparison**: Run Reasoning RL with only rule-based verifier, then only LLM-based verifier, then combined. Measure reward signal accuracy and final model performance on financial report commentary task.
  3. **CPT stage boundary analysis**: Compare model checkpoints after CPT Stage 1 vs. Stage 2 on both financial (FinanceIQ) and general benchmarks. Identify if Stage 2 causes any general capability regression.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the sequential dependency between Reasoning RL (Stage 2) and Agentic RL (Stage 3) strictly necessary for the observed performance gains?
- Basis in paper: [inferred] Section 3.1 proposes a "progressive" pipeline where Agentic RL builds upon Reasoning RL. However, Table 4 (Ablation) only compares the presence vs. absence of these stages combined, without isolating the effect of the training order (e.g., training Agentic capabilities before Reasoning).
- Why unresolved: It remains unclear if the performance boost is due to the specific curriculum order or simply the cumulative exposure to diverse reinforcement learning tasks.
- What evidence would resolve it: An ablation study comparing the proposed sequence against a reversed order (Agentic RL → Reasoning RL) and parallel training.

### Open Question 2
- Question: To what extent does the model maintain performance in niche vertical scenarios outside the coverage of the current "Seed Knowledge Base"?
- Basis in paper: [explicit] Section 5 (Conclusion) states: "Future work will focus on... refining model performance in niche vertical scenarios."
- Why unresolved: While the model excels on broad benchmarks like FinanceIQ and FinQA, the authors explicitly note that sustainable optimization is needed for specialized domains not fully covered by the current training data.
- What evidence would resolve it: Evaluation results on highly specialized financial sub-domains (e.g., actuarial science, esoteric derivatives pricing) that were not included in the "Proprietary Financial Data" or "Exam banks" described in Section 2.1.1.

### Open Question 3
- Question: Does the LLM-based component of the Dual-Verifier Reward Model introduce "reward hacking" behaviors, such as favoring stylistic verbosity over factual accuracy?
- Basis in paper: [inferred] Section 3.3.2 describes the "LLM-based Verifier" which scores "Style" and "Structure." In RLHF, models often learn to exploit these proxy metrics (e.g., generating longer, professional-sounding but empty text) to maximize reward.
- Why unresolved: The paper reports improved benchmark scores but does not provide a qualitative failure analysis regarding the specific artifacts or biases induced by the LLM-based reward signals.
- What evidence would resolve it: A qualitative analysis of model outputs specifically optimized by the LLM-verifier, checking for cases where "professional tone" rewards are high but information density is low.

## Limitations
- Lacks critical implementation details including base model architecture, specific hyperparameters, training compute requirements, and exact data mixing ratios
- Evaluation focuses heavily on QianfanHuijin's internal benchmarks without external validation, raising concerns about potential overfitting to specific test distributions
- Absence of direct comparisons with competing financial LLMs on common benchmarks limits generalizability claims

## Confidence
- **High confidence**: The staged training paradigm (CPT → SFT → specialized RL → General RL) represents a reasonable architectural approach for domain adaptation, supported by the ablation study showing progressive performance improvements.
- **Medium confidence**: The dual-verifier reward model architecture is sound in principle, but effectiveness depends heavily on implementation details not disclosed in the report.
- **Low confidence**: Claims about superior efficiency and accuracy of the Thinking/Non-thinking mechanism lack comparative latency measurements or ablation studies isolating this specific innovation.

## Next Checks
1. **External Benchmark Validation**: Evaluate QianfanHuijin-70B on publicly available financial reasoning benchmarks (e.g., FLAME-Cer, FinanceIQ) against established models like BloombergGPT and FinGPT to verify claimed performance advantages.

2. **Thinking Mode Efficiency Analysis**: Conduct controlled experiments measuring latency and accuracy trade-offs between Thinking and Non-thinking modes on standardized financial reasoning tasks, comparing against single-mode baselines.

3. **General Capability Preservation**: Systematically test model performance on general language understanding benchmarks (e.g., MMLU, BBH) after each training stage to quantify catastrophic forgetting and validate the curriculum learning approach.