---
ver: rpa2
title: Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent
  AI for Addressing Sustainable Protein Production Challenges
arxiv_id: '2506.20598'
source_url: https://arxiv.org/abs/2506.20598
tags:
- arxiv
- agent
- information
- prompt
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents a proof-of-concept multi-agent AI framework
  to support sustainable protein production research, focusing on microbial protein
  sources. The framework uses a Retrieval-Augmented Generation (RAG)-oriented system
  with two GPT-based LLM agents: a literature search agent and an information extraction
  agent.'
---

# Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges

## Quick Facts
- arXiv ID: 2506.20598
- Source URL: https://arxiv.org/abs/2506.20598
- Reference count: 40
- The framework uses RAG with two GPT-based LLM agents and achieves cosine similarity ≥0.89 against ideal output text.

## Executive Summary
This study presents a proof-of-concept multi-agent AI framework to support sustainable protein production research, focusing on microbial protein sources. The framework uses a Retrieval-Augmented Generation (RAG)-oriented system with two GPT-based LLM agents: a literature search agent and an information extraction agent. Both fine-tuning and prompt engineering were explored to optimise the information extraction agent, resulting in improved performance. Mean cosine similarity scores increased by up to 25%, reaching ≥0.89 against ideal output text. Fine-tuning improved mean scores more consistently (≥0.94), while prompt engineering showed lower statistical uncertainties. A user interface was developed for practical deployment, alongside preliminary exploration of chemical safety-based search capabilities. The results demonstrate the effectiveness of both optimisation methods in enhancing LLM-based agent performance for extracting relevant information from scientific literature.

## Method Summary
The study develops a multi-agent RAG system for microbial protein research, using a literature search agent to retrieve relevant papers and an information extraction agent to extract structured data. The extraction agent was optimised using two approaches: fine-tuning GPT-4.1 on curated training data with stratified splits by microbial strain, and prompt engineering using a two-stage design (context harvesting followed by constrained value extraction). Evaluation used transformer-based cosine similarity between extracted and ideal outputs across three SBERT models. The system also includes a toxicity screening module and a Streamlit user interface for practical deployment.

## Key Results
- Fine-tuning consistently improved mean cosine similarity scores to ≥0.94, while prompt engineering achieved 0.89-0.96
- Two-stage prompt engineering reduced standard deviations by approximately 50% compared to baseline
- The framework successfully extracted protein content, trophic mechanism, and substrate information from scientific literature
- A user interface was developed for practical deployment of the multi-agent system

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning GPT models on domain-specific scientific literature improves information extraction accuracy for microbial protein data.
- **Mechanism:** Supervised fine-tuning directly modifies model weights to encode patterns of domain-specific extraction (protein % dry mass, trophic mechanism, substrate) from scientific text. The paper used 80/10/10 train/validation/test splits with stratified sampling by microbial strain, over 10 epochs.
- **Core assumption:** The curated training examples accurately represent the distribution of real-world extraction queries; negative examples prevent hallucination.
- **Evidence anchors:**
  - [abstract] "Fine-tuning overall improved the mean scores to a greater extent (consistently of ≥0.94) compared to prompt engineering"
  - [section 3.2] "Average cosine similarity improved from 0.79 to 0.96... representing a 22% increase in performance"
  - [corpus] Weak direct corpus evidence on fine-tuning mechanisms for scientific domains; related paper "UM_FHS at TREC 2024 PLABA" similarly compared fine-tuning vs prompt engineering for biomedical text simplification.
- **Break condition:** If training data lacks sufficient negative examples or fails to cover edge cases (e.g., papers with partial information), fine-tuning may increase confidence on incorrect answers (miscalibration, cited as [61]).

### Mechanism 2
- **Claim:** Two-stage prompt engineering (context harvesting → constrained value extraction) improves extraction accuracy while reducing variance.
- **Mechanism:** Stage 1 ("context harvesting") assigns a microbiology-expert role and retrieves all potentially relevant passages with verbatim reproduction. Stage 2 ("constrained generation") forces schema-only output with precedence rules and explicit "NaN" for missing data.
- **Core assumption:** The LLM's pre-trained knowledge includes sufficient understanding of microbiology terminology to follow exhaustive retrieval instructions.
- **Evidence anchors:**
  - [abstract] "prompt engineering showed lower statistical uncertainties"
  - [section 2.4.3 / Table 1] "Constrained-generation prompt... enforces schema-only output, encodes precedence rules"
  - [section 3.3] "standard deviations decreasing from 0.18, 0.15, and 0.08... to 0.075, 0.093, and 0.033"
  - [corpus] No strong corpus evidence for this specific two-stage architecture; "T2I-Copilot" paper uses multi-agent prompt interpretation for image generation, which is conceptually adjacent but domain-different.
- **Break condition:** If source text exceeds context window or contains ambiguous/contradictory values across multiple papers, the model may fail to apply precedence rules correctly.

### Mechanism 3
- **Claim:** Retrieval-Augmented Generation (RAG) with specialized agents enables scalable literature mining across fragmented scientific domains.
- **Mechanism:** A literature search agent retrieves PMC papers via keyword-expanded queries (strain name + domain terms like "growth," "medium," "fermentation"). An information extraction agent processes retrieved content. Agents communicate via structured text, enabling modular extension.
- **Core assumption:** PubMed Central open-access subset contains sufficient relevant literature; keyword expansion covers the right search space.
- **Evidence anchors:**
  - [abstract] "RAG-oriented system consists of two GPT-based LLM agents"
  - [section 2.3] "agent programmatically augmented the initial query with a curated set of keywords... systematically combining the strain name with these domain-specific terms"
  - [corpus] "Literature Mining System for Nutraceutical Biosynthesis" paper describes similar LLM-guided extraction from biosynthesis literature, supporting generalizability of this pattern.
- **Break condition:** If retrieval fails to surface relevant papers (e.g., due to non-standardized strain nomenclature or limited PMC coverage), downstream extraction will be compromised regardless of agent quality.

## Foundational Learning

- **Concept:** Cosine similarity over sentence embeddings
  - **Why needed here:** Primary evaluation metric comparing extracted outputs to ideal outputs; measures semantic similarity rather than exact string match.
  - **Quick check question:** If two outputs differ in phrasing but convey the same information (e.g., "45% protein" vs "protein content: 45 wt%"), will cosine similarity correctly reflect their equivalence?

- **Concept:** Stratified train/validation/test splits
  - **Why needed here:** Ensures microbial strains appear exclusively in one split, preventing data leakage and testing generalization to unseen strains.
  - **Quick check question:** Why would including the same microbial strain in both training and test sets lead to over-optimistic performance estimates?

- **Concept:** Negative examples in fine-tuning data
  - **Why needed here:** Papers lacking relevant information teach the model to abstain rather than hallucinate; the paper used 4 categories of negative examples.
  - **Quick check question:** If you only train on positive extraction examples, what behavior might the model exhibit when given irrelevant papers?

## Architecture Onboarding

- **Component map:**
  Literature Search Agent -> PubMed API -> Keyword expansion -> Relevance scoring -> Full-text retrieval (PMC)
  Information Extraction Agent -> Two-stage prompt OR fine-tuned GPT-4.1 -> Structured output
  Toxicity Screening Module -> BioCyc query -> CAS matching -> Mutagenicity dataset
  User Interface -> Streamlit web app -> Displays protein content, trophic mechanism, substrate, safety metrics

- **Critical path:**
  1. User inputs microbial strain name
  2. Search agent constructs expanded queries -> retrieves papers
  3. Extraction agent processes paper text -> outputs structured fields
  4. Toxicity module cross-references metabolome compounds
  5. UI renders results with traceability logs

- **Design tradeoffs:**
  - Fine-tuning: Higher mean accuracy (≥0.94), but higher variance and potential miscalibration on out-of-distribution inputs
  - Prompt engineering: Lower mean accuracy (0.89-0.96), but lower variance (~50% reduction in std dev), no API retraining cost
  - Proprietary GPT models: Stronger baseline performance, but fine-tuning is limited; open-source models allow full control but trail in performance

- **Failure signatures:**
  - Low cosine similarity on extraction -> check if paper text exceeds context window; verify negative example balance in training
  - Search agent retrieves irrelevant papers -> keyword expansion may be too broad/narrow; check PMC coverage for target strain
  - High variance across runs -> temperature >0.0; set to 0.0 for deterministic extraction
  - Fine-tuned model overconfident on wrong answers -> miscalibration; ensure test set includes out-of-distribution strains

- **First 3 experiments:**
  1. Replicate the two-stage prompt engineering pipeline on a held-out microbial strain; compare cosine similarity across all three sentence transformer models.
  2. Fine-tune GPT-4.1 on a subset of the curated data (e.g., 50% training) and evaluate performance drop; test generalization to strains excluded from training.
  3. Replace GPT-4.1 with an open-source model (e.g., Llama 2 or DeepSeek-V3) using identical prompts; benchmark accuracy and latency tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a combined methodology utilizing both prompt engineering and fine-tuning yield superior performance compared to either approach in isolation?
- Basis in paper: [explicit] The authors explicitly posit in the Future Outlook that a "dual-track optimisation strategy" combining the two methods "may be explored... to garner even further benefit."
- Why unresolved: The study only evaluated fine-tuning and prompt engineering as parallel, distinct methodologies and did not test a sequential or hybrid approach.
- What evidence would resolve it: An experiment where the model undergoes task-specific prompt engineering followed by lightweight fine-tuning, achieving mean cosine similarity scores statistically higher than the current fine-tuning baseline (≥0.94).

### Open Question 2
- Question: How does the performance of the extraction agent change when evaluated using isolated, domain-specific error metrics rather than general text similarity?
- Basis in paper: [explicit] The authors acknowledge that cosine similarity risks being a "potentially over-generalised and superficial performance metric" and suggest future work should quantify error on individual categories like "protein % dry mass."
- Why unresolved: Current results rely on transformer-based embeddings which may mask numerical inaccuracies in scientific data (e.g., extracting "50%" vs "50.5%") by focusing on semantic similarity.
- What evidence would resolve it: A re-evaluation of the extraction outputs using Mean Absolute Error (MAE) for numerical fields and exact match accuracy for categorical fields (e.g., trophic mechanism).

### Open Question 3
- Question: Can the performance and training dynamics observed in proprietary GPT models be replicated using open-source LLMs with transparent loss functions?
- Basis in paper: [explicit] The authors note that the loss function and accuracy metrics for the fine-tuning stage were "not specified" by OpenAI and suggest future expansions use open-source models (e.g., Llama 2) to enable "more transparent use of loss functions."
- Why unresolved: The "black-box" nature of the OpenAI fine-tuning API prevents researchers from analyzing training dynamics, such as overfitting or calibration changes, across epochs.
- What evidence would resolve it: Reporting validation loss curves and accuracy metrics over fine-tuning epochs for an open-source model deployed on the same dataset, compared against the GPT-4.1 results.

## Limitations
- The study's findings are constrained by limited generalizability to other protein sources beyond microbial literature from Piercy et al. (2023)
- Proprietary GPT models restrict direct comparison between fine-tuning and prompt engineering due to API limitations
- The study lacks computational cost and latency measurements critical for practical deployment decisions

## Confidence

- **High Confidence**: The general framework design (RAG with two specialized agents) and the evaluation methodology using cosine similarity are sound and well-specified. The observation that prompt engineering reduces variance compared to fine-tuning is supported by the reported standard deviation reductions.
- **Medium Confidence**: The claim that fine-tuning achieves higher mean cosine similarity (≥0.94) versus prompt engineering (0.89-0.96) is based on the reported results, but the limited comparison across different model versions and the proprietary nature of the fine-tuning process introduces uncertainty about reproducibility.
- **Low Confidence**: The scalability and robustness of the system for production use cases remains uncertain due to lack of testing on diverse protein sources, unmeasured computational costs, and limited validation of the toxicity screening component's coverage.

## Next Checks

1. **Generalization Test**: Evaluate the same fine-tuned and prompt-engineered models on a completely separate corpus of microbial protein literature not included in the original training data to assess true generalization capability across unseen strains and publication sources.

2. **Cost-Performance Analysis**: Measure and compare the computational costs (API tokens, inference time) and accuracy trade-offs between fine-tuning versus prompt engineering approaches across multiple model sizes, including open-source alternatives to isolate the impact of model architecture versus optimization method.

3. **Toxicity Screening Coverage Validation**: Systematically test the chemical safety module against known mutagenicity databases to quantify false positive/negative rates and identify gaps in BioCyc or other reference database coverage for microbial metabolites relevant to protein production.