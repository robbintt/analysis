---
ver: rpa2
title: 'SeMe: Training-Free Language Model Merging via Semantic Alignment'
arxiv_id: '2505.20144'
source_url: https://arxiv.org/abs/2505.20144
tags:
- semantic
- merging
- latent
- space
- semantics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently combining the strengths
  of multiple language models without expensive retraining or access to original training
  data. The authors propose SeMe (Semantic-based Merging), a novel training-free approach
  that leverages latent semantic alignment to merge models at a fine-grained, layer-wise
  level.
---

# SeMe: Training-Free Language Model Merging via Semantic Alignment

## Quick Facts
- arXiv ID: 2505.20144
- Source URL: https://arxiv.org/abs/2505.20144
- Reference count: 1
- Primary result: Novel training-free approach that merges language models via latent semantic alignment without requiring retraining or external data

## Executive Summary
This paper introduces SeMe (Semantic-based Merging), a training-free method for combining multiple language models that leverages latent semantic alignment to preserve internal knowledge during merging. Unlike existing approaches that focus on prediction behavior preservation, SeMe explicitly stabilizes semantic representations by computing and aligning semantic bases in the latent space for each model's vocabulary. The method achieves layer-wise merging through transformation of representations between models while eliminating the need for data sampling or additional training typically required in model merging.

Extensive experiments demonstrate that SeMe outperforms existing methods in both performance and efficiency across diverse architectures and tasks. The approach establishes a new paradigm for knowledge-aware model merging by revealing insights into the semantic structure of language models and enabling more scalable and interpretable model composition.

## Method Summary
SeMe operates by computing semantic bases in the latent space for each model's vocabulary, then transforming representations between models to preserve semantic meaning during merging. The core innovation lies in performing layer-wise merging through semantic alignment rather than behavior matching or parameter interpolation. By working directly in the latent semantic space, the method eliminates the need for data sampling or additional training typically required in model merging. The approach involves aligning semantic representations across models at a fine-grained level, which stabilizes internal knowledge during the merging process and enables the combination of model strengths without expensive retraining or access to original training data.

## Key Results
- Outperforms existing model merging methods in both performance and efficiency across diverse architectures and tasks
- Eliminates reliance on external data for model merging, achieving training-free composition
- Establishes a new paradigm for knowledge-aware model merging by revealing insights into semantic structure of language models

## Why This Works (Mechanism)
SeMe works by computing semantic bases in the latent space for each model's vocabulary, then transforming representations between models to preserve semantic meaning during merging. The method leverages the observation that language models encode semantic information in their latent representations, and by aligning these semantic bases across models, it can preserve internal knowledge structures during the merging process. This semantic alignment approach stabilizes the internal representations rather than just matching output behaviors, which is the key differentiator from previous methods.

## Foundational Learning
- Latent semantic space alignment: Why needed - to preserve meaning across different model representations; Quick check - verify semantic bases are meaningfully aligned by testing semantic similarity preservation
- Layer-wise merging: Why needed - to maintain fine-grained control over knowledge combination; Quick check - ensure each layer's semantic integrity is preserved during merging
- Vocabulary-specific semantic bases: Why needed - to handle model-specific semantic encodings; Quick check - validate bases capture unique vocabulary semantics

## Architecture Onboarding
Component map: Input models -> Semantic base computation -> Representation transformation -> Layer-wise merging -> Output merged model
Critical path: The semantic base computation and representation transformation stages are critical, as errors here propagate through the entire merging process
Design tradeoffs: Prioritizes semantic preservation over computational efficiency; requires full model access but no training data
Failure signatures: Poor semantic alignment manifests as semantic drift in outputs; vocabulary mismatch causes merging instability
First experiments: 1) Merge two models with identical vocabularies to establish baseline performance 2) Test semantic preservation on controlled semantic tasks 3) Evaluate merging efficiency compared to training-based approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to extremely large models (100B+ parameters) has not been established
- Performance on highly specialized domains with limited vocabulary overlap remains untested
- Interpretability of semantic bases across different model families is unclear

## Confidence
- High confidence: Core methodology of semantic alignment for layer-wise merging is well-defined and experimentally validated
- Medium confidence: Claims about eliminating external data reliance are substantiated but generalizability requires further validation
- Low confidence: Assertions about providing insights into semantic structure are more speculative and need additional theoretical grounding

## Next Checks
1. Test SeMe's performance when merging models with minimal vocabulary overlap to assess semantic alignment robustness
2. Evaluate scalability by applying the method to models with 10x or 100x more parameters, measuring computational efficiency and quality retention
3. Conduct ablation studies isolating the contribution of semantic alignment versus other factors to quantify the unique value added by the approach