---
ver: rpa2
title: Weight Initialization and Variance Dynamics in Deep Neural Networks and Large
  Language Models
arxiv_id: '2510.09423'
source_url: https://arxiv.org/abs/2510.09423
tags:
- variance
- layers
- initialization
- relu
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theory-grounded and empirically validated
  analysis of weight initialization and variance dynamics across compact ReLU MLPs
  and GPT-2-style transformers. The author conducts a logarithmic sweep of initial
  standard deviation values, mapping vanishing and exploding gradient regimes and
  identifying a broad stable training band between 10^-2 and 10^-1.
---

# Weight Initialization and Variance Dynamics in Deep Neural Networks and Large Language Models

## Quick Facts
- arXiv ID: 2510.09423
- Source URL: https://arxiv.org/abs/2510.09423
- Reference count: 14
- Primary result: Kaiming (fan-in) initialization converges faster and more stably than Xavier under ReLU, and shallow transformer layers expand variance rapidly during early training while deeper layers adjust gradually

## Executive Summary
This paper presents a theory-grounded and empirically validated analysis of weight initialization and variance dynamics across compact ReLU MLPs and GPT-2-style transformers. The author conducts a logarithmic sweep of initial standard deviation values, mapping vanishing and exploding gradient regimes and identifying a broad stable training band between 10^-2 and 10^-1. A controlled comparison shows that Kaiming (fan-in) initialization converges faster and more stably than Xavier under ReLU, consistent with variance-preserving theory. In a from-scratch 12-layer GPT-2-style model, the paper tracks layerwise Q/K/V weight variance during pretraining, revealing depth-dependent equilibration: shallow layers expand rapidly while deeper layers change more gradually, eventually settling into narrow variance bands. Together, these results connect classic initialization principles with modern transformer behavior, demonstrating that rectifier-aware fan-in scaling preserves forward signal magnitude more faithfully, easing gradient flow and yielding quicker, more stable optimization.

## Method Summary
The paper implements three controlled experiments: (E1) a ReLU MLP on MNIST with 25 logarithmically spaced initial weight standard deviations from 10^-4 to 10, training for 10 epochs with Adam lr=0.01; (E2) a UCI Wine binary classifier comparing Xavier vs Kaiming initialization across 10 random seeds; and (E3) from-scratch pretraining of a 12-layer GPT-2-style transformer with GELU MLPs, logging per-layer Q/K/V weight standard deviations throughout training. Key architectural choices include small-scale networks (784→64→32→32→10 for MNIST, 11→16→32→32→1 for Wine), fixed optimizer settings (Adam lr=0.01 for E1, AdamW lr=1e-4 for E3), and initialization scales (σ ≈ 0.02 for transformer projections). The analysis focuses on loss trajectories, accuracy, gradient stability, and layerwise variance evolution as metrics.

## Key Results
- Kaiming (fan-in) initialization converges faster and more stably than Xavier under ReLU activations
- Initial weight standard deviations in the range σ ∈ [10^-2, 10^-1] produce stable training for ReLU MLPs
- During transformer pretraining, shallow layers exhibit rapid weight-variance expansion early in training while deeper layers adjust gradually, eventually converging to narrow variance bands

## Why This Works (Mechanism)

### Mechanism 1: Rectifier-Aware Fan-in Scaling
Kaiming initialization preserves forward signal magnitude better than Xavier under ReLU activations, yielding faster and more stable convergence. ReLU zeroes out approximately half of all activations, reducing the second moment by a factor of ~2. To compensate, weights must be initialized with variance σ²_W ≈ 2/n_in rather than σ²_W ≈ 1/n_in. This preserves activation variance across layers, preventing systematic shrinkage or explosion. The core assumption is that weights are i.i.d. zero-mean, inputs are approximately independent from weights, and ReLU's sparsity pattern is roughly uniform. Evidence shows Kaiming consistently outperforms Xavier in convergence speed and stability. This breaks down with highly unbalanced layer widths or non-rectifier activations.

### Mechanism 2: Stable Training Band from Scale Sensitivity
Initial weight standard deviations in the range σ ∈ [10^-2, 10^-1] produce stable training for ReLU MLPs, while values outside this band cause vanishing or exploding gradients. Too-small σ produces attenuated forward activations and gradients, leading to vanishing updates. Too-large σ causes activations to saturate or explode, destabilizing loss trajectories. The stable band represents scales where layer-to-layer variance remains approximately constant under the rectifier's sparsity. This assumes moderate network depth without explicit normalization layers. Evidence shows extremely small scales yield vanishing updates while very large scales produce unstable loss. This band may shift with deep residual networks, normalization layers, or extreme learning rates.

### Mechanism 3: Depth-Dependent Variance Equilibration in Transformers
During transformer pretraining, shallow layers exhibit rapid weight-variance expansion early in training, while deeper layers adjust gradually, all eventually converging to narrow variance bands. Shallow layers receive high-SNR gradients from input structure and adapt quickly. Deeper layers are constrained by longer residual paths and lower gradient SNR, causing smaller effective step sizes. LayerNorm and weight decay damp variance growth, while attention softmax saturation discourages large scales. This assumes pre-LN or similar normalization with AdamW and weight decay. Evidence shows lower layers exhibit rapid and pronounced growth in weight standard deviation early in training, while higher layers expand more slowly. This pattern may differ in post-LN transformers or very shallow architectures.

## Foundational Learning

- **Concept: Variance and signal magnitude**
  - Why needed here: The entire paper frames initialization as a variance-preservation problem. Without understanding variance as a measure of signal energy, the reasoning for σ²_W ≈ 2/n_in is opaque.
  - Quick check question: If you initialize weights with σ = 0.001 instead of σ = 0.02 in a 12-layer network, would you expect activations to grow, shrink, or stay similar in magnitude across layers?

- **Concept: ReLU's zeroing effect (sparsity factor)**
  - Why needed here: The factor-of-two in Kaiming initialization comes directly from ReLU zeroing half the pre-activations. Understanding this connects the activation function choice to the initialization formula.
  - Quick check question: Why does ReLU require a larger initialization variance than a linear or tanh activation to preserve signal magnitude?

- **Concept: Forward vs backward pass variance trade-off**
  - Why needed here: The paper notes that forward-preserving (fan-in) and backward-preserving (fan-out) conditions generally conflict. This explains why fan-in is preferred for ReLU despite not being optimal for gradients.
  - Quick check question: If you have a layer with n_in = 512 and n_out = 128, would fan-in or fan-out initialization produce larger weight magnitudes?

## Architecture Onboarding

- **Component map:**
  - MNIST ReLU MLP: Input → Linear(784, 64) → ReLU → Linear(64, 32) → ReLU → Linear(32, 32) → ReLU → Linear(32, 10) → Output
  - GPT-2-style Transformer: Token embedding → Positional encoding → [Transformer block × 12] → LayerNorm → Output projection
  - Each block: LayerNorm → Multi-head attention (Q/K/V projections) → Residual add → LayerNorm → MLP (GELU) → Residual add

- **Critical path:**
  1. Choose initialization based on activation: ReLU/GELU → Kaiming fan-in; tanh/sigmoid → Xavier
  2. Set base scale: For transformers, σ ≈ 0.02 for projections; verify this falls within [10^-2, 10^-1] stable band
  3. Add depth-aware safeguards: For deep stacks (>12 layers), consider residual scaling (1/√L) or ensure pre-normalization
  4. Monitor during training: Track per-layer weight std and gradient norms; confirm shallow layers expand early, deep layers stabilize gradually

- **Design tradeoffs:**
  - Fan-in vs fan-out: Fan-in prioritizes forward signal stability; fan-out prioritizes gradient flow. For ReLU, fan-in usually wins
  - Small vs large σ: Small σ is conservative but may slow convergence; large σ risks explosion. The [10^-2, 10^-1] band balances both
  - Pre-LN vs post-LN: Pre-LN provides smoother gradient flow at depth; post-LN may require more careful initialization

- **Failure signatures:**
  - Vanishing gradients: Loss plateaus immediately; accuracy near random; weight std barely changes from initialization (especially in deep layers)
  - Exploding gradients: Loss spikes or NaN; gradient norms grow unbounded; weight std explodes in early iterations
  - Shallow layer stagnation: Shallow layers fail to expand during early training—may indicate learning rate too low or initialization too small
  - Deep layer stagnation: Deep layers never stabilize—may indicate residual scaling issues or normalization problems

- **First 3 experiments:**
  1. **Scale sweep (E1 replication):** Train a 4-layer ReLU MLP on a simple dataset (MNIST or synthetic). Sweep σ ∈ {10^-4, 10^-3, 10^-2, 10^-1, 1}. Log loss curves and final accuracy. Confirm stable band emerges around [10^-2, 10^-1]
  2. **Initialization head-to-head (E2 replication):** Train identical networks with Xavier vs Kaiming initialization. Measure epochs to reach 90% of final accuracy and loss variance across runs. Expect Kaiming to converge faster with lower variance
  3. **Layerwise variance tracking (E3 lite):** Train a small transformer (4–6 layers) from scratch. Log Q/K/V weight std per layer every 100 steps. Visualize depth-dependent adaptation: shallow layers should show early expansion, deep layers slower growth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed depth-dependent variance equilibration persist in significantly larger models (e.g., 100B+ parameters) or alternative architectures?
- Basis in paper: [explicit] The "Outlook" section explicitly proposes evaluating "Scaling with depth and width" to see if dynamics persist under larger models and datasets.
- Why unresolved: The empirical evidence relies entirely on a compact 12-layer GPT-2-style model; it is unverified whether the same variance bands appear in state-of-the-art scale systems.
- What evidence would resolve it: Replicating the layerwise Q/K/V variance tracking protocol on large-scale models (e.g., LLaMA-70B) or comparing against muP parameterizations.

### Open Question 2
- Question: Is the rapid expansion of shallow layer variances strictly caused by higher gradient signal-to-noise ratios (SNR) compared to deeper layers?
- Basis in paper: [inferred] The conclusion attributes the asymmetric evolution to "higher gradient signal-to-noise near the input," but this mechanistic explanation is inferred from observation rather than isolated via ablation.
- Why unresolved: The study tracks weight statistics but does not control or manipulate gradient SNR to prove it is the specific driver of the rapid early expansion.
- What evidence would resolve it: Ablation experiments where gradient noise is artificially normalized or residual connections are modified to equalize effective step sizes across depths.

### Open Question 3
- Question: How does initialization scale interact with optimizer hyperparameters (warmup, weight decay) to maintain variance stability across varying batch sizes?
- Basis in paper: [explicit] The "Outlook" section calls for studying "Optimizer and schedule coupling" to maintain stable variance under different batch sizes and sequence lengths.
- Why unresolved: The experiments fix the optimizer and schedule to isolate initialization effects, leaving the dynamic interplay between weight decay, warmup, and variance drift unexplored.
- What evidence would resolve it: A factorial design sweeping initialization std, warmup duration, and batch size to map the resulting variance trajectories.

## Limitations
- **E3 pretraining corpus unknown:** The exact dataset, tokenization, sequence length, and training duration for the 12-layer GPT-2 experiment are unspecified, limiting precise reproduction
- **Architectural detail gaps:** Exact transformer dimensions (d_model, num heads, FFN ratio), normalization placement (pre-LN vs post-LN), and vocabulary size are not provided
- **Optimizer specifics missing:** AdamW hyperparameters (betas, epsilon, weight decay) for E3 are not disclosed, which could affect variance dynamics

## Confidence
- **High confidence:** Kaiming vs Xavier convergence claims (E2), stable training band identification (E1), basic variance dynamics theory
- **Medium confidence:** Depth-dependent variance equilibration in transformers (E3), due to missing architectural details and pretraining specifics
- **Low confidence:** Generalization of stable training band [10^-2, 10^-1] to extreme depths (>50 layers) or non-MLP architectures without residual connections

## Next Checks
1. **Architectural specificity audit:** Confirm exact GPT-2 dimensions, normalization placement, and vocabulary size used in E3 to validate depth-dependent variance claims
2. **Cross-dataset Kaiming/Xavier comparison:** Replicate E2 with multiple datasets (MNIST, CIFAR-10, UCI datasets) to test generalization of initialization benefits
3. **Extended depth sweep:** Train 4→24→48 layer ReLU MLPs to determine if stable training band shifts with depth and residual presence