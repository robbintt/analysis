---
ver: rpa2
title: 'DomusFM: A Foundation Model for Smart-Home Sensor Data'
arxiv_id: '2602.01910'
source_url: https://arxiv.org/abs/2602.01910
tags:
- sensor
- data
- smart-home
- domusfm
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DomusFM addresses data scarcity in smart-home activity recognition
  by introducing the first foundation model specifically pretrained for binary sensor
  data. The model employs dual contrastive learning to capture both semantic attributes
  and temporal dependencies through specialized encoders for sensor attributes, status,
  and time.
---

# DomusFM: A Foundation Model for Smart-Home Sensor Data

## Quick Facts
- arXiv ID: 2602.01910
- Source URL: https://arxiv.org/abs/2602.01910
- Reference count: 40
- Key outcome: First foundation model for binary sensor data achieving 10-15% F1 improvement over baselines with only 5% labeled data

## Executive Summary
DomusFM addresses data scarcity in smart-home activity recognition by introducing a foundation model specifically pretrained for binary sensor data. The model employs dual contrastive learning to capture both semantic attributes and temporal dependencies through specialized encoders for sensor attributes, status, and time. Evaluated across seven public datasets using leave-one-dataset-out methodology, DomusFM outperforms state-of-the-art baselines on both activity recognition and next-event prediction tasks, achieving superior performance even with minimal labeled training data.

## Method Summary
DomusFM uses a self-supervised dual contrastive learning paradigm with two pretraining phases. Phase 1 applies attribute-level contrastive loss by masking individual event attributes (house item, room, type, status, time), while Phase 2 applies event-level contrastive loss by masking entire events. The model integrates semantic embeddings from Sentence-BERT with learnable status and temporal encoders, processed through a 12-layer transformer for contextualization. After pretraining on six datasets, the model is fine-tuned on a held-out dataset with limited labeled data (5-30%) for two downstream tasks: ADL recognition and next-k event prediction.

## Key Results
- Achieves 10-15% F1 improvement over state-of-the-art baselines on ADL recognition
- Maintains superior performance with only 5% labeled training data
- Demonstrates effective cross-dataset generalization through leave-one-dataset-out evaluation
- Lightweight architecture (36M parameters, <500MB) enables edge deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual contrastive learning enables representation learning at multiple granularities, addressing data scarcity through hierarchical self-supervision.
- Mechanism: Stage 1 applies attribute-level contrastive loss by masking individual event attributes, forcing the model to reconstruct semantic relationships. Stage 2 applies event-level contrastive loss by masking entire events, capturing temporal dependencies. The event-level encoder is frozen after Stage 1, building progressively on learned representations.
- Core assumption: Masking-based augmentation creates meaningful positive pairs for contrastive learning in sparse, discrete sensor event streams, and semantic relationships between attributes are learnable without explicit labels.
- Evidence anchors: [abstract] "DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies." [section 5.3] "The first phase... employs an attribute-level contrastive loss... The second phase introduces an event-level contrastive loss... the event-level feature extractor is frozen after the first phase."
- Break condition: If target environments have fundamentally different attribute vocabularies or if event sequences are too sparse (<5 events per meaningful activity), the contrastive signal may be insufficient to learn transferable representations.

### Mechanism 2
- Claim: Pretrained language model embeddings enable cross-environment generalization by capturing semantic relationships between sensor attributes.
- Mechanism: Sensor attributes (house item, room, type) are encoded as text and processed through Sentence-BERT (all-MiniLM-L6-v2), mapping semantically related concepts closer in embedding space. This leverages prior linguistic knowledge without requiring domain-specific pretraining data.
- Core assumption: Semantic relationships learned from general text corpora transfer meaningfully to smart-home sensor contexts, and attribute naming is consistent and descriptive enough across deployments.
- Evidence anchors: [abstract] "By integrating semantic embeddings from a lightweight language model..." [section 5.1.1] "Semantically related appliances such as a stove and an oven are naturally embedded closer together than unrelated ones like a stove and a bed, capturing domain-relevant relationships that traditional encoding schemes, such as one-hot encoding, would not represent."
- Break condition: If sensor naming is inconsistent (e.g., "microwave" vs "MW-01" vs "cooking appliance"), or if the LLM lacks relevant domain knowledge (e.g., specialized medical equipment), semantic embeddings will not align properly.

### Mechanism 3
- Claim: Transformer-based contextualization captures temporal dependencies and inter-event relationships that single-event representations miss.
- Mechanism: A 12-layer transformer with 12 attention heads processes windows of 30 event embeddings. Self-attention allows each event to attend to all others in the window, learning sequential patterns and contextual relationships. Attribute self-attention within events captures inter-attribute dependencies (e.g., kitchen sensor at 7am vs. 11pm suggests different activities).
- Core assumption: A fixed window of 30 consecutive events captures sufficient behavioral context, and temporal patterns generalize across environments with different sensor densities and activity durations.
- Evidence anchors: [abstract] "...specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations..." [section 5.2] "The self-attention mechanism enables each event to attend to all other events in the window, allowing the model to capture sequential dependencies, temporal patterns, and contextual relationships between events." [section 7.5] Ablation study (Tables 5-6) shows consistent 3-15% F1 drop without contextualization module, demonstrating its contribution.
- Break condition: If activities span much longer than 30 events (e.g., extended sleeping periods with few sensor triggers) or if event sequences are highly irregular with long time gaps, the fixed window may miss critical context or include irrelevant noise.

## Foundational Learning

- Concept: Contrastive Learning with InfoNCE Loss
  - Why needed here: DomusFM uses InfoNCE loss during both pretraining stages. Understanding how positive pairs (original vs. masked sequences) and negative samples (other batch sequences) are constructed is essential for debugging pretraining and adapting the augmentation strategy.
  - Quick check question: Why does the model use masking-based augmentation rather than traditional data augmentation (e.g., cropping, jittering) common in vision contrastive learning?

- Concept: Transformer Encoder Architecture
  - Why needed here: The contextualization module uses stacked transformer encoder layers. Understanding multi-head self-attention, residual connections, and layer normalization is crucial for modifying architecture depth or adapting to different sequence lengths.
  - Quick check question: How does the model encode temporal order and relative timing when transformer self-attention is permutation-invariant by default?

- Concept: Transfer Learning Strategies (Frozen vs. Fine-tuned)
  - Why needed here: DomusFM's practical value depends on effective adaptation with minimal supervision. The paper demonstrates fine-tuning with 5-30% labeled data. Understanding when to freeze the encoder, fine-tune only the head, or update all weights is essential for real-world deployment.
  - Quick check question: Why does the paper freeze the event-level feature extractor after Stage 1 of pretraining rather than continuing to update it during Stage 2?

## Architecture Onboarding

- Component map:
  Raw event stream -> Segmentation (30-event windows, 29 overlap) -> Per-event encoding (semantic + status + temporal) -> Attribute Self-Attention -> Event embeddings -> Transformer contextualization -> Contextualized embeddings -> Task head -> Prediction

- Critical path: Raw event stream → Segmentation (30-event windows, 29 overlap) → Per-event encoding (semantic + status + temporal) → Attribute self-attention → Event embeddings → Transformer contextualization → Contextualized embeddings → Task head → Prediction

- Design tradeoffs:
  - **Window size N=30**: Captures medium-grain activity context; may miss very long activities or include noise from activity transitions
  - **12-layer transformer vs. larger**: Deliberately small to avoid overfitting on limited smart-home pretraining data (vs. billions of tokens in NLP)
  - **Sentence-BERT all-MiniLM-L6-v2**: Lightweight (~22M params) for edge deployment; trades off against richer semantic understanding from larger LLMs
  - **Near-complete overlap (29/30)**: Maximizes training samples from limited data; increases computation
  - **Binarization of continuous sensors**: Captures semantic states but loses fine-grained numerical information

- Failure signatures:
  - **Generalization gap on new environment**: Pretraining datasets may lack relevant sensor types/activities; consider domain-specific pretraining data
  - **High cross-validation variance**: Limited fine-tuning data causing instability; increase training data % or use data augmentation
  - **Contextualization module provides no gain**: Window size may be wrong for target activity durations; try N=15 or N=60
  - **Semantic encoder produces poor embeddings**: Attribute naming inconsistent with pretraining; normalize vocabulary or fine-tune Sentence-BERT layer
  - **Next-k prediction poor for small k (k=10)**: Bag-of-events formulation may lose short-horizon ordering; consider sequence prediction head

- First 3 experiments:
  1. **Reproduce ADL recognition baseline**: Train on CASAS Milan with 10% labeled data. Target: F1≈0.82. Verify preprocessing (segmentation, attribute extraction) and hyperparameters match paper. Log training curves to confirm convergence behavior.
  2. **Validate contextualization contribution**: Run ablation (disable transformer, use event embeddings directly) on same setup. Expect 5-15% F1 drop. If drop is minimal, your environment may not need temporal context or window size is suboptimal.
  3. **Test cross-dataset generalization**: Pretrain on 6 datasets, fine-tune on held-out dataset not in paper (if available) with 10% data. Compare against training from scratch. This validates whether pretrained representations transfer to your target deployment scenario before investing in full integration.

## Open Questions the Paper Calls Out

- **Can DomusFM be extended to achieve zero-shot generalization for ADL recognition without requiring any labeled fine-tuning data from target environments?**
  - Basis: [explicit] Section 8.3.3 states DomusFM "does not yet operate in a zero-shot fashion" and proposes investigating "natural language supervision techniques to enable fully zero-shot operation while maintaining the original activity sets."
  - Why unresolved: Current model requires 5% labeled data; zero-shot approaches trade off activity definition fidelity for label-free operation.
  - What evidence would resolve it: Demonstrating competitive ADL recognition performance on held-out datasets with zero labeled examples using natural language supervision.

- **How can DomusFM representations be leveraged to address multi-occupant data association in smart homes where sensor activations cannot be unambiguously attributed to specific individuals?**
  - Basis: [explicit] Section 8.3.1 notes that "perfect data association in multi-occupant settings remains an open challenge" and proposes that "DomusFM's rich semantic representations could contribute to addressing this problem."
  - Why unresolved: Current model assumes single-occupancy or perfect data association; disentangling overlapping activities from multiple residents remains unsolved.
  - What evidence would resolve it: Evaluation on multi-resident datasets showing improved data association accuracy using DomusFM embeddings compared to baseline approaches.

- **What is the information trade-off when directly integrating continuous sensor streams versus the current binarization preprocessing approach?**
  - Basis: [explicit] Section 8.3.2 identifies this as a limitation: "this necessitates preprocessing and may result in information loss" and proposes future work to "explore architectures that directly integrate continuous sensor streams alongside binary events."
  - Why unresolved: Current binarization abstracts continuous data into discrete semantic states, potentially discarding fine-grained temporal dynamics.
  - What evidence would resolve it: Comparative experiments on datasets with continuous sensors, measuring performance differences between binarized and native continuous processing.

## Limitations

- Effectiveness depends heavily on consistent sensor naming conventions and metadata availability across deployments
- 30-event window assumption may not capture long-duration activities or handle irregular event spacing well
- Dual contrastive learning framework requires careful hyperparameter tuning not fully specified
- Lightweight architecture trades off against potentially richer representations from larger LLMs
- Privacy preservation claims rely on edge deployment feasibility which may be prohibitive for resource-constrained devices

## Confidence

- **High Confidence**: Empirical performance claims on ADL recognition and next-k prediction across seven datasets are well-supported by presented results. The 5-15% F1 improvement over baselines with minimal labeled data is consistently demonstrated.
- **Medium Confidence**: Mechanism claims about dual contrastive learning enabling effective pretraining for data-scarce scenarios are plausible but not fully validated. Ablation study supports architecture components but doesn't isolate contrastive learning's specific contribution.
- **Low Confidence**: Generalization claims to unseen environments and assertion that semantic embeddings from general language models transfer meaningfully to sensor contexts are theoretically sound but lack rigorous ablation or comparative studies with alternative pretraining strategies.

## Next Checks

1. **Reproduce cross-dataset transfer learning**: Implement leave-one-dataset-out evaluation with exact 5%, 10%, 15%, 30% labeled data splits. Measure both performance and variance across folds to verify claimed 10-15% F1 improvements and stability.

2. **Ablation of pretraining methodology**: Compare DomusFM against (a) supervised pretraining on same data, (b) random initialization, and (c) alternative self-supervised methods (e.g., masked language modeling only). This isolates the contribution of dual contrastive learning specifically.

3. **Edge deployment validation**: Deploy the 500MB model on representative edge hardware (e.g., Raspberry Pi or similar IoT device). Measure actual inference latency, memory usage during streaming, and battery impact to verify practical feasibility claims.