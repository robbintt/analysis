---
ver: rpa2
title: 'Hallucination as a Computational Boundary: A Hierarchy of Inevitability and
  the Oracle Escape'
arxiv_id: '2508.07334'
source_url: https://arxiv.org/abs/2508.07334
tags:
- escape
- learning
- hallucination
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a theoretical framework that explains why\
  \ hallucination in LLMs is inevitable at three computational boundaries\u2014diagonalization,\
  \ uncomputability, and information theory\u2014using a \"learner pump lemma.\" It\
  \ formalizes LLMs as probabilistic Turing machines and introduces two escape paths:\
  \ (1) absolute escape via oracle-augmented machines like RAG, and (2) adaptive escape\
  \ via continual learning framed as a neuro-game-theoretic process. The work culminates\
  \ in the Computational Class Alignment (CCA) principle, which requires matching\
  \ task complexity to system capability."
---

# Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape

## Quick Facts
- **arXiv ID:** 2508.07334
- **Source URL:** https://arxiv.org/abs/2508.07334
- **Reference count:** 2
- **Primary result:** Introduces a theoretical framework proving hallucination inevitability at three computational boundaries and validates a RAG-CL hybrid achieving 96.5% accuracy with 1.1% forgetting

## Executive Summary
This paper presents a formal theoretical framework explaining why hallucination in large language models is computationally inevitable at three boundaries: diagonalization, uncomputability, and information theory. The framework introduces a "learner pump lemma" and proves that hallucination is a fundamental limitation of computable learners. It then provides two escape paths: absolute escape via oracle-augmented machines like RAG, and adaptive escape via continual learning framed as an internalized oracle mechanism. The work culminates in the Computational Class Alignment principle, which requires matching task complexity to system capability. Experiments validate that a RAG-CL hybrid strategy achieves high accuracy (96.5%) with low forgetting (1.1%) and superior robustness to noise compared to pure RAG or CLM approaches.

## Method Summary
The study compares three hallucination mitigation strategies using Mistral-7B LLM: Pure RAG (external retrieval), Pure CLM (internal continual learning via LoRA fine-tuning), and RAG-CL Hybrid (combination with learning trigger). A synthetic fact corpus ensures no prior knowledge contamination. Systems are evaluated on accuracy, forgetting rate on TriviaQA, robustness to 15% noise, and amortized cost. The RAG-CL Hybrid retrieves via RAG initially and triggers LoRA updates to internalize frequently accessed facts, with a crossover point at ~287 queries where amortized cost becomes superior to pure RAG.

## Key Results
- Diagonalization proof shows for any enumerable PLM sequence, there exists a nemesis query forcing hallucination with probability ≥ 1/|Y|
- Oracle Escape Theorem proves RAG achieves zero hallucination on oracle-defined truth (98.6% accuracy in experiments)
- Information-theoretic boundary proven via pumping lemma: hallucination inevitable when K(f_P) > K(h)
- RAG-CL hybrid achieves 96.5% accuracy vs 81.0% (CLM) and 98.6% (RAG), with 1.1% forgetting vs 12.4% (CLM)

## Why This Works (Mechanism)

### Mechanism 1: Diagonalization Boundary — Adversarial Self-Reference
For any enumerable sequence of PLMs, construct a ground-truth function f_R that explicitly excludes each model's most confident prediction on its diagonal input (f_R(s_i) := Y \ {y*_i}). Since the model's highest-probability output is guaranteed wrong, H_Stray ≥ P(y*_i|s_i) ≥ 1/|Y|. This relies on PLMs being enumerable computable functions and truth being adversarially definable post-hoc.

### Mechanism 2: Oracle-Augmented Escape (RAG as Absolute Escape)
The oracle-augmented model h_O bypasses internal computation and directly outputs O(s) with probability 1. Since ground truth is defined as f_O(s) = {O(s)}, output always matches truth: H_Stray(h_O, f_O, s) = 0. This assumes the oracle is perfectly aligned with ground-truth definition and returns correct, non-adversarial results.

### Mechanism 3: Adaptive Escape via Continual Learning (Internalized Oracle)
Continual Learning allows a model to "pump" new information into its capacity, dynamically escaping the static information-theoretic boundary. CLM updates model parameters via learning function U: h_{t+1} = U(h_t, d). This conceptually expands effective capacity to K(h_t) + K(d|h_t), internalizing previously external knowledge. Amortized cost becomes lower than RAG after N > N_0 queries.

## Foundational Learning

- **Concept: Turing Machines and Oracle Machines**
  - Why needed: Paper formalizes LLMs as probabilistic Turing machines and RAG as oracle-augmented machines
  - Quick check: Can you explain why adding an oracle for the Halting Problem would allow a Turing Machine to decide previously undecidable problems?

- **Concept: Kolmogorov Complexity**
  - Why needed: "Pumping Lemma for Learners" uses K(h) as finite information capacity to prove hallucination inevitability
  - Quick check: Why is Kolmogorov complexity uncomputable, and how does this relate to the information-theoretic boundary?

- **Concept: Complementary Learning Systems (CLS) Theory**
  - Why needed: Neuro-game-theoretic framework is explicitly grounded in CLS: hippocampus for rapid episodic encoding, neocortex for slow generalization
  - Quick check: What is the stability-plasticity dilemma, and how do the two memory systems in CLS theory address it?

## Architecture Onboarding

- **Component map:** Input Query → [RAG Retrieval (FAISS)] → [LLM (Mistral-7B)] → Output → [CL Trigger] → [LoRA Fine-tuning] → [Consolidation Protocol] → [Internalized Knowledge (Parameters)]

- **Critical path:** Query arrives at PLM with RAG access → RAG retrieves external context; model generates answer → If query frequency exceeds threshold, trigger CL update → LoRA-based fine-tuning internalizes fact into parameters → Subsequent queries rely more on internal pathways

- **Design tradeoffs:**
  | Strategy | Accuracy | Forgetting | Cost | Robustness to Noise |
  |----------|----------|------------|------|---------------------|
  | Pure RAG | ~98.6% | 0% | High per-query | 76.5% |
  | Pure CLM | ~81.0% | 12.4% | Low (amortized) | N/A |
  | RAG-CL Hybrid | ~96.5% | 1.1% | Crossover at ~287 queries | 92.3% |

- **Failure signatures:**
  - Catastrophic forgetting: High forgetting rate (12.4%) indicates consolidation protocol failure in Pure CLM
  - Noise sensitivity: 76.5% robustness in Pure RAG indicates over-reliance on external context without internal verification
  - Fact blending: "Unreliable learning" in Pure CLM suggests parameter interference across facts

- **First 3 experiments:**
  1. Crossover point validation: Plot amortized cost per query for RAG vs. RAG-CL to confirm theoretical N_0 ≈ 287 queries
  2. Attention shift probing: Measure cross-attention scores to RAG context before/after CL update to verify internalization mechanism
  3. Noise robustness stress test: Corrupt RAG knowledge base with 15% noise and compare accuracy degradation across Pure RAG vs. RAG-CL hybrid

## Open Questions the Paper Calls Out

### Open Question 1
How can the theoretical information capacity $K(h)$ be practically quantified for specific neural architectures to operationalize the Computational Class Alignment (CCA) principle? The paper explicitly calls for future work to "quantify the information capacity ‘K(h)’ for specific neural architectures" but provides no mechanism for measuring this capacity in high-dimensional, continuous-weight neural networks.

### Open Question 2
How can systems implement "runtime complexity assessment" to dynamically recognize when a query exceeds their computational class? The discussion on "Dynamic CCA" states that future systems "should possess a form of runtime complexity assessment" to trigger principled abstention, but the paper provides no architecture or algorithm capable of evaluating query complexity relative to model capacity in real-time.

### Open Question 3
How do the trade-offs between external (RAG) and internal (Continual Learning) adaptation strategies change in the presence of noisy or bounded feedback? The conclusion requests future study on "trade-offs between external... and internal... adaptation strategies, especially in the presence of noisy or bounded feedback," noting that experiments validated robustness against noise in retrieval context but assumed clean ground truth for learning updates.

## Limitations

- Theoretical framework relies on adversarial truth definitions that may not reflect real-world ground-truth distributions
- Information-theoretic boundary proof assumes worst-case truth complexity K(f_P) exceeding model capacity
- Experimental validation focuses on synthetic facts with controlled noise injection, limiting generalization to naturally occurring scenarios
- Crossover analysis assumes stable query frequency distributions that may not hold in production environments

## Confidence

- **High Confidence:** Oracle escape theorem (RAG achieves zero hallucination when oracle is perfectly truthful) - directly proven and experimentally validated with 98.6% accuracy baseline
- **Medium Confidence:** Diagonalization boundary inevitability - mathematically sound but relies on post-hoc adversarial truth construction
- **Medium Confidence:** Information-theoretic boundary via pumping lemma - rigorous but assumes worst-case complexity matching
- **Medium Confidence:** Neuro-game-theoretic CL framework - novel synthesis but lacks direct neurobiological validation
- **Medium Confidence:** Cost crossover point (~287 queries) - empirically measured but sensitive to specific task and implementation details

## Next Checks

1. **Natural Distribution Validation:** Test diagonalization inevitability claim on naturally occurring factual distributions rather than adversarially constructed ground truths to measure whether nemesis queries emerge spontaneously in real-world usage

2. **Robustness Generalization:** Extend noise robustness testing beyond 15% synthetic corruption to include real-world knowledge base errors, conflicting sources, and temporal inconsistencies to validate RAG-CL hybrid's superior performance

3. **Catastrophic Forgetting Mitigation:** Conduct long-term retention studies (multiple epochs of learning and testing) to verify the 1.1% forgetting rate holds over extended training periods and with larger knowledge bases that stress consolidation protocols