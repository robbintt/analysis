---
ver: rpa2
title: 'AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language
  Model'
arxiv_id: '2510.11496'
source_url: https://arxiv.org/abs/2510.11496
tags:
- arxiv
- data
- preprint
- zhang
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces AndesVL, a suite of mobile-side multimodal\
  \ large language models (MLLMs) with parameter sizes ranging from 0.6B to 4B, designed\
  \ for efficient deployment on edge devices. AndesVL leverages Qwen3\u2019s LLM and\
  \ various visual encoders to achieve first-tier performance across diverse benchmarks,\
  \ including text-rich image understanding, reasoning and math, multi-image comprehension,\
  \ general VQA, hallucination mitigation, multilingual understanding, and GUI-related\
  \ tasks."
---

# AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model

## Quick Facts
- **arXiv ID**: 2510.11496
- **Source URL**: https://arxiv.org/abs/2510.11496
- **Reference count**: 40
- **Primary result**: AndesVL achieves first-tier performance across diverse benchmarks while maintaining only 3% degradation after mobile deployment

## Executive Summary
AndesVL introduces a suite of mobile-side multimodal large language models ranging from 0.6B to 4B parameters, designed for efficient deployment on edge devices. The system leverages Qwen3's LLM backbone with various visual encoders and achieves strong performance across text-rich image understanding, reasoning, math, multi-image comprehension, VQA, hallucination mitigation, multilingual understanding, and GUI tasks. Key innovations include a 1+N LoRA architecture for efficient task adaptation, a Quantization-Aware LoRA Fine-Tuning (QALFT) framework for model compression, and a separated Instruct/Thinking training paradigm that specializes models for either instruction-following or chain-of-thought reasoning.

## Method Summary
AndesVL employs a modular architecture where visual encoders (AIMv2-Large or SigLIP2) project images through MLP projectors into the LLM space. The system uses a 1+N LoRA architecture enabling efficient task-specific adaptation without full fine-tuning. QALFT first applies post-training quantization to a QAT-pretrained base model, freezing its quantization encodings before training LoRA adapters. The training pipeline separates models into Instruct (trained with SFT+MPO) and Thinking (trained with SFT+GRPO plus two-stage RL) variants. For deployment, the system integrates OKV cache eviction, speculative decoding, and compression strategies to achieve 6.7x speedup and 30.9% memory reduction on MediaTek Dimensity 9500 chips.

## Key Results
- Maintains only 3% accuracy degradation after mobile deployment compared to floating-point models
- Achieves 6.7x peak decoding speedup ratio with up to 30.9% memory reduction
- Compresses to 1.8 bits-per-weight while preserving first-tier benchmark performance
- OKV cache eviction shows >10% improvement in Rouge-1 relative to baseline with 50% eviction ratios

## Why This Works (Mechanism)

### Mechanism 1: Quantization-Aware LoRA Fine-Tuning (QALFT) Preserves Accuracy
Freezing quantization encodings on the base model before LoRA training prevents compounding quantization error across multiple adapters. QALFT first applies PTQ to a QAT-pretrained base model, permanently freezing its quantization encodings. Subsequent LoRA weights train on this fixed, quantized backbone analogous to QLoRA, enabling independent LoRA updates without re-quantizing the base. The core assumption is that frozen quantization encodings sufficiently capture activation ranges needed for all downstream LoRA adapters.

### Mechanism 2: Separated Instruct and Thinking Training Paradigm
Training separate models for instruction-following (Instruct) and chain-of-thought reasoning (Thinking) yields better specialization than a unified model. Instruct models receive SFT+MPO, while Thinking models receive SFT+GRPO with long CoT data and two-stage RL training. The core assumption is that preference optimization benefits non-thinking tasks while RL with difficulty-graded data benefits reasoning tasks.

### Mechanism 3: OKV Cache Eviction Exploits Prompt Sparsity
Long-text prompts contain sparse informative tokens; evicting low-value KV cache entries reduces memory with minimal quality loss. OKV identifies and retains high-contribution tokens while evicting redundant cache entries, supporting context lengths up to 128K. The core assumption is that the prompt's informative content clusters in a small subset of tokens whose attention weights can be predicted or approximated.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**: Why needed here - The 1+N architecture relies on LoRA modules for efficient task-specific adaptation without full model fine-tuning. Quick check: Can you explain how LoRA freezes pre-trained weights and injects trainable low-rank matrices?

- **Quantization-Aware Training (QAT)**: Why needed here - Direct PTQ causes significant accuracy loss; QAT prepares models for quantization during training. Quick check: How does QAT differ from post-training quantization in terms of when quantization error is introduced?

- **Speculative Decoding**: Why needed here - Sequential autoregressive decoding is slow; speculative decoding uses a draft model to predict multiple tokens verified in parallel. Quick check: What is the relationship between draft acceptance rate and overall decoding speedup?

## Architecture Onboarding

- **Component map**: Visual Encoder (AIMv2-Large or SigLIP2) → MLP Projector (2-layer with pixel shuffle) → LLM Backbone (Qwen3) → [Optional] LoRA Adapter → Output. During inference: OKV Cache Manager ↔ KV Cache ↔ Speculative Decoder (draft model reuses top-layer features).

- **Critical path**: Data quality filtering (SFT/MPO/GRPO datasets) → Pre-training (3 stages) → Post-training (SFT + MPO or SFT + GRPO) → QAT → QALFT → Deployment with compression/speedup stack.

- **Design tradeoffs**: (1) Thinking vs Instruct: Thinking gives +5.5 pts on reasoning benchmarks but adds training complexity. (2) QALFT enables multi-LoRA deployment but freezes encodings, potentially limiting future adapter diversity. (3) Cache eviction saves memory but may degrade quality on dense-dependency tasks.

- **Failure signatures**: (1) >3% accuracy drop after QALFT → encoding ranges insufficient for downstream adapters. (2) Speculative decoding acceptance rate low → draft model not aligned with target features. (3) OKV causes quality loss on specific tasks → task has denser token dependencies than expected.

- **First 3 experiments**:
  1. Replicate the ablation comparing AndesVL-2B-Instruct-Base vs SFT vs MPO to validate MPO gains on your target domain.
  2. Test OKV cache eviction on your longest-prompt use case with 25% and 50% eviction ratios, measuring both quality (ROUGE) and latency.
  3. Apply QALFT to a single LoRA adapter for your specific mobile task, comparing accuracy against PTQ-only and floating-point baselines using Top-1 overlap metric.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can visual encoder architectures be specifically optimized to improve efficiency and accuracy for complex visual content on resource-constrained mobile devices?
- **Basis in paper**: [explicit] Section 7 states that "designing more optimal visual encoder solutions holds great potential" and suggests leveraging advanced architectures to improve efficiency.
- **Why unresolved**: The paper currently employs existing encoders (AIMv2, SigLIP2) which may not be optimally tailored for the specific power and memory constraints of mobile-side inference.
- **What evidence would resolve it**: Benchmarking a novel, mobile-specific visual encoder against the existing AIMv2-Large in terms of latency, memory footprint, and accuracy on the AndesVL-4B model.

### Open Question 2
- **Question**: What post-training schemes can further optimize multimodal task performance and reduce hallucinations specifically for mobile-deployed models?
- **Basis in paper**: [explicit] Section 7 notes that "developing superior post-training schemes is crucial" to "optimize the model performance in handling various multimodal tasks, reduce hallucinations, and enhance the consistency."
- **Why unresolved**: While MPO and GRPO were used, the Hallucination evaluation (Table 11) shows scores in the ~70s, indicating room for improvement in consistency and reliability.
- **What evidence would resolve it**: Demonstrating a new post-training algorithm that significantly lowers hallucination rates (e.g., >5% improvement on HallusionBench) without compromising the 3% degradation limit established by QALFT.

### Open Question 3
- **Question**: How can knowledge be effectively distilled from large cloud-based models to smaller mobile-side counterparts to maximize the performance-to-resource ratio?
- **Basis in paper**: [explicit] Section 7 proposes "implementing effective distillation schemes between large and small models" as a method to boost capabilities while maintaining low costs.
- **Why unresolved**: The performance gap between the 0.6B and 4B models is significant (Table 6), suggesting current training pipelines do not fully bridge the capability gap via distillation.
- **What evidence would resolve it**: An ablation study showing that a specific cloud-to-mobile distillation technique improves the accuracy of the AndesVL-0.6B model to match or exceed current 1B models.

### Open Question 4
- **Question**: Is it feasible to develop a unified mobile-side model integrating text, image, and speech modalities that retains the efficiency of the current two-modality AndesVL?
- **Basis in paper**: [explicit] Section 7 identifies "the development of a unified mobile-side model integrating text, image, and speech modalities" as an exciting frontier requiring research on efficient inference algorithms.
- **Why unresolved**: The current paper focuses exclusively on text and image inputs; adding a third modality introduces unknown overhead regarding memory usage and decoding speed on mobile chips.
- **What evidence would resolve it**: Successful deployment of a prototype three-mode AndesVL model on a Dimensity 9500 chip, maintaining a similar bits-per-weight and memory reduction profile to the current AndesVL-4B.

## Limitations
- QALFT effectiveness and 3% accuracy preservation claim need validation across diverse tasks beyond OCR benchmarks
- Separated Thinking/Instruct paradigm creates specialization but may require multiple model deployments
- OKV cache eviction lacks comprehensive validation for quality preservation on tasks requiring dense token dependencies
- Real-world mobile deployment performance depends heavily on specific hardware configurations and prompt characteristics

## Confidence
**High Confidence**: The overall mobile deployment framework (1+N LoRA architecture, compression strategies, speculative decoding) is well-established and the reported memory reductions and speedups are technically plausible.

**Medium Confidence**: The QALFT technique's effectiveness and the 3% accuracy preservation claim require more extensive validation across diverse tasks. The separated Thinking/Instruct training paradigm shows strong benchmark results but real-world task diversity may challenge this strict separation.

**Low Confidence**: The OKV cache eviction algorithm's quality preservation claims lack comprehensive validation. The paper reports ROUGE improvements but doesn't address potential quality degradation on tasks requiring dense cross-token dependencies.

## Next Checks
1. **Cross-task QALFT Validation**: Apply QALFT to at least 5 diverse downstream tasks (including reasoning, math, and GUI-related tasks) and measure accuracy degradation compared to both floating-point and PTQ-only baselines using the Top-1 overlap metric.

2. **OKV Quality Sensitivity Analysis**: Test OKV cache eviction on long-text reasoning tasks (100+ tokens) with varying eviction ratios (10%, 25%, 50%, 75%) and measure both ROUGE scores and task-specific accuracy to identify breaking points.

3. **Mobile Hardware Performance Variability**: Deploy AndesVL-4B on at least three different mobile platforms (including non-MediaTek chips) and measure actual decoding speed, memory usage, and accuracy to validate the claimed 6.7x speedup and 30.9% memory reduction under realistic conditions.