---
ver: rpa2
title: An empirical study on the limitation of Transformers in program trace generation
arxiv_id: '2509.25073'
source_url: https://arxiv.org/abs/2509.25073
tags:
- trace
- program
- longer
- programs
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Transformers trained on program trace generation struggle to generalize
  beyond in-distribution test settings, despite achieving near-perfect performance
  within the training distribution. The study evaluates small Transformer variants
  (154M parameters) on generating step-by-step execution traces for synthetic programs,
  focusing on generalization across four factors: program length, trace steps, number
  of variables, and input size.'
---

# An empirical study on the limitation of Transformers in program trace generation

## Quick Facts
- arXiv ID: 2509.25073
- Source URL: https://arxiv.org/abs/2509.25073
- Authors: Simeng Sun
- Reference count: 19
- Transformers trained on program trace generation struggle to generalize beyond in-distribution test settings

## Executive Summary
Transformers trained on program trace generation struggle to generalize beyond in-distribution test settings, despite achieving near-perfect performance within the training distribution. The study evaluates small Transformer variants (154M parameters) on generating step-by-step execution traces for synthetic programs, focusing on generalization across four factors: program length, trace steps, number of variables, and input size. While models reach 98-100% accuracy in-distribution, they exhibit severe degradation when evaluated on longer programs (46.5%), longer traces (14.2-91.6%), more variables (94.1%), or larger input lists (90.3%).

## Method Summary
The study evaluates 154M parameter Transformers on Program Trace Generation (PTG) - generating step-by-step execution traces from synthetic Python programs with constrained complexity (scope depth 1, operations limited to x±1 and ==/!=). Training uses 4k token sequences with ~4.03B tokens total, evaluated across four out-of-distribution generalization splits. The methodology tests positional encoding variants (NoPE, RoPE, ALiBi, NaPE, Fox, PaTH) and hybrid architectures (SWAN, Canon layers, sparse attention variants) against a NoPE baseline.

## Key Results
- Transformers achieve 98-100% accuracy in-distribution but degrade severely on longer programs (46.5%), longer traces (14.2-91.6%), more variables (94.1%), or larger input lists (90.3%)
- NaPE positional encoding outperforms others, achieving 93.5-97.9% accuracy across generalization splits, while RoPE shows particular weakness (3.9-65.5%)
- Hybrid architectures and Canon layers provide modest improvements on specific dimensions but fail to resolve fundamental generalization limitations

## Why This Works (Mechanism)

### Mechanism 1: Positional Encoding Design Determines State Maintenance Over Extended Contexts
- Claim: NaPE enables more consistent execution state tracking across longer traces than RoPE-based approaches
- Mechanism: By setting the slope of the latter half of ALiBi heads to zero, NaPE reduces positional interference for distant tokens while preserving local ordering, allowing the model to maintain variable bindings across hundreds of trace steps without the degradation seen in rotary encodings
- Core assumption: The critical failure mode in PTG is position-induced disruption of attention to earlier program instructions, not fundamental capacity limits
- Evidence anchors:
  - [abstract]: "NaPE positional encoding outperforms others, achieving 93.5-97.9% accuracy across generalization splits, while RoPE shows particular weakness (3.9-65.5%)"
  - [Table 1]: NaPE achieves 93.5% on Longer traces - 2 vs. RoPE's 3.9%, a 24× improvement
  - [corpus]: Neighbor paper "The Kinetics of Reasoning" studies CoT learning dynamics but does not directly address positional encoding effects on trace generation

### Mechanism 2: Compositional Simplicity Enables Generalization When Steps Remain Trivial
- Claim: PTG isolates contextual flexibility by ensuring each trace step is individually simple
- Mechanism: The paper constrains binary operations to x+1/x-1 and comparisons to ==/!= with scope depth 1, ensuring no single step requires multi-step reasoning. Generalization failures thus reflect inability to consistently apply known operations over longer horizons, not failure to learn complex operations
- Core assumption: Generalization to longer traces requires the same computation per step, just more repetitions
- Evidence anchors:
  - [Section 2]: "We further restrict arithmetic ops to be only x+1 and x-1, comparison ops to only == and !="
  - [Section 2]: "Execution of complex programs becomes sequentially composing basic operations"
  - [corpus]: "What I Cannot Execute" paper studies execution traces but focuses on training with dynamic execution data rather than isolating step complexity

### Mechanism 3: Dual Memory Architecture Exposes Independent Scaling Limits
- Claim: The PTG setup partitions the context window into "instruction memory" and "data memory"
- Mechanism: Longer programs test instruction memory capacity (more branching, more complex control flow to track). Longer traces test data memory capacity (sustained attention to variable states over more steps). Canon layers help the former (64.8% vs 46.5%) but hurt the latter (58.0% vs 90.3% on longer lists)
- Core assumption: Models use distinct attention patterns for program instructions vs. trace state tracking
- Evidence anchors:
  - [Section A]: "The context window now serves as memory: the beginning part hosting the 'instruction memory' and the trace/scratchpad/chain-of-thought part 'data memory'"
  - [Table 1]: NoPE+Canon improves longer programs (64.8%) but degrades longer lists (58.0% vs 90.3%)
  - [corpus]: Weak direct evidence in corpus—neighbor papers focus on trace utilization, not memory partitioning

## Foundational Learning

- Concept: Positional Encodings (RoPE vs. ALiBi vs. NoPE)
  - Why needed here: The paper's central finding is that encoding choice dominates generalization performance (24× gap between NaPE and RoPE on long traces). Understanding how each encoding represents token positions is prerequisite to interpreting results
  - Quick check question: Given a sequence of 500 tokens, which encoding would preserve relative distances without penalizing attention to distant tokens?

- Concept: Out-of-Distribution Generalization Splits
  - Why needed here: The paper evaluates four independent factors (program length, trace steps, variables, input size). Each tests a different capability; conflating them would misdiagnose failures
  - Quick check question: If a model generalizes to longer inputs but not longer programs, what does this suggest about its failure mode?

- Concept: Whole-Trace Accuracy Metric
  - Why needed here: The paper requires every step to match exactly. This is unforgiving—95% per-step accuracy on a 100-step trace yields ~0.6% whole-trace accuracy. Understanding this metric is critical for interpreting the severity of reported degradations
  - Quick check question: If per-step accuracy is 99%, what is whole-trace accuracy for a 200-step trace?

## Architecture Onboarding

- Component map:
  - Positional Encodings: RoPE (baseline, weak), NoPE (no explicit encoding), ALiBi (linear bias), NaPE (best performer, hybrid), Fox (data-dependent gate), PaTH (Householder transforms)
  - Softmax Replacements: STB (stick-breaking), α-Entmax (sparse attention)
  - Hybrid/Convolutional: SWAN (alternating sliding window/global), Canon (short 1D convolution on specific positions)
  - Base Architecture: 12-layer, 16-head, 1024-dim Transformer with SwiGLU, 154M parameters

- Critical path:
  1. Start with NoPE baseline to establish in-distribution convergence (should reach ~99% by 20k steps)
  2. Evaluate on all four OOD splits to identify failure dimensions
  3. Apply NaPE if trace-length generalization is critical, or Canon if program-length generalization is priority
  4. Do not combine Canon with tasks requiring longer input entities—this degrades performance

- Design tradeoffs:
  - NaPE: Best overall generalization, but may underperform on tasks requiring precise local position information
  - Canon: +18.3% on longer programs, but -32.3% on longer lists—use only when program complexity is the bottleneck
  - RoPE: Avoid for tasks requiring sustained attention over long output traces
  - SWAN: Poor across all OOD splits (0.5-49.5%), not recommended despite hybrid architecture

- Failure signatures:
  - Sudden accuracy cliff at specific trace lengths (e.g., RoPE drops from 78.5% to 3.9% when trace steps exceed training range)
  - Asymmetric generalization: strong on input scaling, weak on output scaling
  - Training dynamics: ID performance saturates by ~5k steps, but OOD performance continues improving through 20k steps (Figure 2)

- First 3 experiments:
  1. Replicate NoPE baseline on ID splits to verify training setup (target: 99.8% ± 0.2%)
  2. Ablate NaPE by removing NoPE heads vs. removing ALiBi heads to isolate which component drives long-trace generalization
  3. Test Canon on a fixed-program, variable-input task to confirm the reported degradation (expect ~30% drop vs. NoPE baseline)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-Transformer architectures (e.g., state-space models, recurrent architectures) or Mixture-of-Experts models achieve better generalization on program trace generation than standard Transformers?
- Basis in paper: [explicit] "In the future, we plan to extend our work to latest architectures potentially including non-Transformers and MoE models"
- Why unresolved: The current study only evaluates Transformer variants; fundamental architectural alternatives remain unexplored despite known Transformer limitations in stateful computation
- What evidence would resolve it: Comparative evaluation of SSMs (e.g., Mamba), recurrent models, or MoE architectures on the same PTG benchmarks using identical training regimes

### Open Question 2
- Question: What are the additional modeling capabilities required for the transduction setting, and how do current architectures fail to meet them?
- Basis in paper: [explicit] "Finally, we also plan to explore the transduction setting, which may require additional modeling capabilities atop contextual flexibility"
- Why unresolved: The paper studies generative trace production but not the transductive input-to-output mapping, which may require different mechanistic capabilities
- What evidence would resolve it: Experiments comparing generative vs. transductive formulations on PTG, analyzing where and why performance diverges

### Open Question 3
- Question: Why does NaPE (NoPE + ALiBi hybrid) dramatically outperform all other position encodings on longer traces (93.5% vs. <40%), and what does this reveal about positional encoding requirements for sustained execution?
- Basis in paper: [inferred] NaPE's superior performance is reported (Table 1) without theoretical explanation for why combining position-agnostic and relative-position heads enables consistent long-trace generation
- Why unresolved: The paper demonstrates the empirical phenomenon but does not provide mechanistic interpretation of NaPE's advantage
- What evidence would resolve it: Attention pattern analysis, ablation studies varying the NoPE/ALiBi head ratio, and probing experiments to identify what computational properties NaPE preserves that others lose

### Open Question 4
- Question: Would scaling model size beyond 154M parameters improve out-of-distribution generalization on PTG, or are the failures architectural rather than capacity-related?
- Basis in paper: [inferred] The study uses only ~154M parameter models; generalization failures may reflect architectural inductive biases rather than insufficient model capacity
- Why unresolved: Without scaling experiments, it remains unclear whether larger models would learn more robust execution strategies or inherit the same generalization limitations
- What evidence would resolve it: Systematic scaling experiments (e.g., 350M, 700M, 1.4B parameters) with controlled training compute, measuring whether OOD accuracy improves proportionally with scale

## Limitations
- Restricted program complexity (scope depth 1, operations limited to x±1 and ==/!=) fundamentally constrains the task and leaves open whether Transformers can handle recursive algorithms or dynamic memory access
- The 4k context window constrains program length, making it unclear how models would perform on programs requiring hundreds of lines or traces with thousands of steps
- Evaluation methodology assumes exact match per step, which is extremely strict and may overstate practical limitations for applications tolerating occasional execution errors

## Confidence
**High Confidence (Evidence directly supports):**
- Transformers achieve near-perfect performance on in-distribution PTG tasks (98-100% accuracy)
- RoPE positional encoding shows severe degradation on longer traces (3.9-65.5% accuracy)
- NaPE positional encoding consistently outperforms alternatives across all OOD splits (93.5-97.9% accuracy)
- Hybrid architectures and Canon layers provide modest improvements on specific dimensions but fail to resolve fundamental generalization limitations

**Medium Confidence (Evidence supports but scope is limited):**
- PTG isolates contextual flexibility by ensuring each trace step is individually simple
- The compositional simplicity enables generalization when steps remain trivial
- Dual memory architecture exposes independent scaling limits for instruction vs. data memory

**Low Confidence (Requires additional validation):**
- NaPE's advantages would hold for non-linear state updates (not tested in current scope)
- The same failure modes would manifest in real-world program execution beyond synthetic constraints

## Next Checks
1. **Stress Test Non-Linear Operations**: Extend the PTG framework to include multiplication, division, and array indexing operations. Evaluate whether the same positional encoding advantages hold when individual trace steps require more complex state updates beyond x±1 and comparisons.

2. **Scale Context Window**: Train equivalent models with 8k and 16k context windows on the same PTG task. Measure whether the observed generalization cliffs shift proportionally or reveal new architectural bottlenecks in attention mechanisms.

3. **Dynamic Execution Testing**: Implement a testing protocol where models must execute the same program with varying input values at inference time, measuring consistency across executions. This would validate whether models learn algorithmic patterns or merely memorize specific trace outputs.