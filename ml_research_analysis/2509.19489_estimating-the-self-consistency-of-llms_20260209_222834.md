---
ver: rpa2
title: Estimating the Self-Consistency of LLMs
arxiv_id: '2509.19489'
source_url: https://arxiv.org/abs/2509.19489
tags:
- self-consistency
- bound
- error
- estimator
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of estimating the self-consistency
  of large language models (LLMs), where self-consistency measures how often repeated
  responses to the same prompt disagree with the majority label. The key method involves
  using a plug-in estimator based on repeated LLM calls per prompt and aggregating
  across sampled prompts under a fixed compute budget.
---

# Estimating the Self-Consistency of LLMs

## Quick Facts
- arXiv ID: 2509.19489
- Source URL: https://arxiv.org/abs/2509.19489
- Authors: Robert Nowak
- Reference count: 4
- Primary result: Optimal allocation of compute budget B for estimating self-consistency is m ∝ √B prompts and n ∝ √B repeated calls per prompt

## Executive Summary
This paper addresses the problem of estimating the self-consistency of large language models, where self-consistency measures how often repeated responses to the same prompt disagree with the majority label. The key method involves using a plug-in estimator based on repeated LLM calls per prompt and aggregating across sampled prompts under a fixed compute budget. The analysis derives a mean-squared error bound that decomposes into three terms: variance from prompt sampling, bias from the estimator, and variance from repeated calls. The primary result is that the error-minimizing allocation of the compute budget B is to use approximately √B prompts and √B repeated calls per prompt, leading to the optimal split m ∝ n ∝ √B.

## Method Summary
The paper proposes estimating self-consistency by sampling m prompts and making n repeated LLM calls per prompt with binary outputs. For each prompt, the estimator computes the minimum of the positive rate and negative rate (min{k/n, 1-k/n}) to estimate the probability of disagreement with the majority label. The final estimate is the average of these per-prompt estimates across m prompts. Under a total budget constraint B = mn, the paper derives an MSE bound and shows that the optimal allocation is m* = √(πB/8) prompts and n* = √(8B/π) repeated calls per prompt.

## Key Results
- The MSE bound decomposes into three terms: 1/(8m) from prompt sampling variance, 1/(πn) from estimator bias, and 1/(2nm) from per-prompt variance
- Under total budget B = mn, the optimal allocation is m* ∝ √B and n* ∝ √B
- The third variance term becomes negligible as B grows, making the optimal split determined by balancing the first two terms
- Extensions to multi-class settings preserve the √B split but require different error definitions

## Why This Works (Mechanism)

### Mechanism 1: Plug-in Estimator Converts Response Counts to Self-Consistency Scores
The minimum of positive-rate and negative-rate across n repeated calls estimates the probability that a new sample disagrees with the majority label. For each prompt x, count positive responses k out of n calls, compute p̂(x) = k/n, then estimate self-consistency error as Ê(x) = min{k/n, 1−k/n}. This captures disagreement frequency when the model is uncertain (p(x) ≈ 0.5), and approaches zero when confident (p(x) ≈ 0 or 1). Core assumption: Responses to repeated calls are i.i.d. draws from a fixed distribution with parameter p(x).

### Mechanism 2: MSE Bound Decomposition Reveals Three Competing Error Sources
Total estimation error decomposes into prompt-sampling variance (1/8m), estimator bias (1/πn), and per-prompt variance (1/2nm), with the third term becoming negligible as B grows. The bound E[(E − Ê)²] ≤ 1/(8m) + 1/(πn) + 1/(2nm) isolates: (1) uncertainty from finite prompt samples, (2) systematic underestimation from the discrete min{k, n−k}/n estimator, and (3) noise in individual Ê(x) estimates. Core assumption: Binary responses only; extension to multi-class claimed but not proven in detail.

### Mechanism 3: Optimal Budget Split Balances Two Dominant Terms
Under total budget B = mn, minimizing the upper bound yields m* ∝ √B and n* ∝ √B, with exact values m* = √(πB/8), n* = √(8B/π). Since the third term (1/2nm = 1/2B) becomes negligible relative to 1/m and 1/n as B grows, the optimization reduces to balancing the first two terms under the constraint mn = B. Setting m = n = √B equalizes their scaling. Core assumption: The upper bound tightly characterizes true MSE; the paper states this is an upper bound, not necessarily tight.

## Foundational Learning

- **Bias-variance tradeoff in estimation**
  - Why needed here: The error decomposition separates bias (systematic underestimation from discrete k/n) from variance (noise from finite sampling). Understanding this is essential to see why increasing n reduces bias but doesn't help prompt coverage.
  - Quick check question: If you double n while holding m fixed, which error terms decrease and which stay the same?

- **Constrained optimization under budget**
  - Why needed here: The √B split emerges from minimizing a multi-term objective subject to mn = B. This is classic Lagrange multiplier territory.
  - Quick check question: Given B = 100, what are the rounded integer values of m* and n* per the paper's formulas?

- **Binomial distribution properties**
  - Why needed here: The bias derivation relies on properties of binomial distributions with p = 0.5, including symmetry and Stirling approximation for binomial coefficients.
  - Quick check question: Why is the bias largest when p(x) = 0.5?

## Architecture Onboarding

- **Component map**: Prompt sampler -> Repeated caller -> Per-prompt estimator -> Aggregator -> Budget allocator
- **Critical path**:
  1. Determine compute budget B based on latency/cost constraints
  2. Compute optimal m*, n* (round to integers)
  3. Sample m* prompts from target distribution
  4. For each prompt, make n* calls with appropriate sampling settings (temperature > 0)
  5. Aggregate per-prompt estimates into final Ê

- **Design tradeoffs**:
  - Larger m (more prompts) → better coverage of task distribution, higher per-prompt noise
  - Larger n (more repeats) → lower per-prompt bias and variance, less diverse prompt coverage
  - Temperature setting: Too low → correlated responses violating i.i.d.; too high → incoherent outputs
  - Assumption: Uniform per-call cost; if costs vary by prompt length, budget constraint becomes non-trivial

- **Failure signatures**:
  - Near-zero estimates with high variance: n too small, or model near-deterministic (p ≈ 0 or 1 for most prompts)
  - High variance across runs: m too small relative to task diversity
  - Systematic underestimation: Bias term dominates; increase n
  - Correlated responses: Temperature too low or caching enabled; check independence by comparing response diversity

- **First 3 experiments**:
  1. **Validate the √B split**: Fix B = 100, compare MSE of (m=10, n=10) vs. (m=5, n=20) vs. (m=20, n=5) over multiple runs on a held-out test set. Expect the balanced split to achieve lowest MSE.
  2. **Test i.i.d. assumption**: For a fixed prompt, run n = 100 calls at different temperatures (0.3, 0.7, 1.0). Compute empirical autocorrelation between consecutive responses. If correlation > 0.1, i.i.d. is violated.
  3. **Measure actual bias**: For prompts where you can compute true p(x) via very large n (e.g., n=1000), compare Ê(x) at n=10 vs. n=100 vs. ground truth. Verify bias decreases as √(1/n).

## Open Questions the Paper Calls Out

- **High-probability bounds**: Can high-probability bounds be derived for the self-consistency estimator using concentration inequalities? The paper currently only provides a bound on the mean-squared error (MSE), which characterizes the average error but does not offer guarantees on the probability of the error exceeding a specific threshold. Evidence needed: A formal derivation demonstrating how the error probability decays relative to the compute budget B.

- **Multi-class extension**: Does the m, n ∝ √B allocation strategy hold for multi-class classification tasks? The current theoretical analysis and the resulting optimal budget split rely specifically on properties of the binary error function and binomial distributions, which may scale differently in a multi-class regime. Evidence needed: A theoretical proof or empirical study showing that the MSE bound components scale similarly for multi-class problems.

- **Correlation effects**: How does intraclass correlation between repeated LLM calls impact the optimal compute allocation? The main theorem assumes responses are independent and identically distributed. Real-world factors like shared randomness or caching often induce correlation, which would inflate the variance term. Evidence needed: A modified bound that explicitly incorporates the correlation coefficient ρ and an analysis determining if the variance inflation alters the optimal number of prompts m versus repeated calls n.

## Limitations

- The theoretical analysis relies heavily on idealized assumptions that may not hold in practice, particularly the i.i.d. assumption for repeated LLM calls.
- The binary classification framing, while simplifying analysis, excludes many practical use cases.
- The MSE bound is presented as an upper bound without empirical validation of its tightness.
- The paper provides no experimental validation - all claims are theoretical.
- The extension to multi-class classification is mentioned but not developed.

## Confidence

**High confidence**: The basic plug-in estimator mechanism (Mechanism 1) is straightforward and mathematically sound. The bias derivation using Stirling bounds (Mechanism 2) is standard statistical theory. The optimization math leading to the √B split (Mechanism 3) is correct given the stated constraints.

**Medium confidence**: The MSE decomposition itself is mathematically valid, but whether the three-term bound accurately characterizes real-world error is uncertain. The claim that the third term (1/2nm) becomes negligible may not hold for small B or high-variance tasks.

**Low confidence**: The assumption that the optimal √B split transfers to different per-call costs, different prompt distributions, or multi-class settings. No empirical evidence supports the practical utility of these theoretical results.

## Next Checks

1. **Empirical MSE validation**: For B = 400, compare the MSE of the theoretically optimal split (m=13, n=31) against several alternative splits (m=5,n=80; m=20,n=20; m=40,n=10) on a real binary classification task. Compute actual MSE vs. theoretical bound predictions.

2. **i.i.d. assumption verification**: For 10 fixed prompts, generate n=100 responses each at temperatures 0.3, 0.7, 1.0. Compute autocorrelation between consecutive responses and response diversity (unique outputs/total outputs). Determine temperature threshold where i.i.d. breaks down.

3. **Bias verification with ground truth**: For prompts where you can obtain high-confidence estimates (n=1000), compare Ê(x) at n=10, n=20, n=50, n=100 against ground truth. Plot bias vs 1/√n to verify the theoretical bias scaling.