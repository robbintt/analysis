---
ver: rpa2
title: Exploiting Task Relationships in Continual Learning via Transferability-Aware
  Task Embeddings
arxiv_id: '2502.11609'
source_url: https://arxiv.org/abs/2502.11609
tags:
- task
- learning
- tasks
- should
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new continual learning method that uses transferability-aware
  task embeddings (H-embeddings) to guide a hypernetwork for generating task-specific
  model weights. The method addresses catastrophic forgetting by leveraging task relationships
  derived from H-score transferability, computed online without revisiting past data.
---

# Exploiting Task Relationships in Continual Learning via Transferability-Aware Task Embeddings

## Quick Facts
- **arXiv ID:** 2502.11609
- **Source URL:** https://arxiv.org/abs/2502.11609
- **Reference count:** 40
- **Primary result:** Achieves highest FAA and DAA across CIFAR-100, ImageNet-R, and DomainNet benchmarks in both full-model and LoRA parameter-efficient settings.

## Executive Summary
This paper introduces a novel continual learning approach that leverages transferability-aware task embeddings (H-embeddings) to guide a hypernetwork in generating task-specific model weights. The method addresses catastrophic forgetting by structuring task relationships based on H-score transferability, computed online without revisiting past data. A lightweight embedding regularization module aligns generated weights with prior task relationships. Extensive experiments demonstrate state-of-the-art performance across multiple benchmarks and architectures, offering both theoretical insight and practical efficiency.

## Method Summary
The method employs a hypernetwork framework that generates weights for a main model conditioned on task embeddings. For each new task, H-scores (an information-theoretic transferability metric) are computed between the current task and all previous tasks using feature statistics. These scores are normalized via Analytic Hierarchy Process (AHP) to create stable relative rankings. A 32-dimensional task embedding is then optimized to reflect these transferability relationships through Euclidean distance minimization. The hypernetwork generates weights from this embedding, with regularization ensuring backward and forward transfer. An optional embedding guidance module uses an encoder-decoder to preserve task relationship information in the hypernetwork's internal representations.

## Key Results
- Outperforms state-of-the-art baselines on CIFAR-100, ImageNet-R, and DomainNet across both full-model and LoRA settings
- Achieves highest Final Average Accuracy (FAA) and During Average Accuracy (DAA) metrics
- Demonstrates strong performance without requiring rehearsal buffers or task-specific masks
- Maintains efficiency through parameter-efficient weight generation strategies

## Why This Works (Mechanism)

### Mechanism 1
Structuring task embeddings to reflect transferability geometry improves knowledge transfer. The method computes H-scores between the current task and previous ones, then optimizes a task embedding so that its Euclidean distance to prior embeddings is inversely proportional to the H-score transferability. This maps tasks into a space where proximity implies high mutual utility. The core assumption is that H-score provides a reliable proxy for true transferability without accessing source data. Break condition: If H-scores become noisy or uncorrelated with actual performance, the embedding geometry will misguide the hypernetwork, potentially increasing negative transfer.

### Mechanism 2
Decoupling weight generation from direct optimization via a hypernetwork allows for stable task isolation. Instead of updating a single set of weights, a hypernetwork takes the task embedding and generates weights for the main model. To prevent forgetting, the method regularizes the hypernetwork to produce similar weights for old task embeddings as it did previously. The core assumption is that the hypernetwork has sufficient capacity to map distinct embeddings to optimal weights for all tasks without interference. Break condition: If the embedding dimension is too small or the hypernetwork is under-capacitated, distinct tasks may collapse to similar weights, causing forgetting or interference.

### Mechanism 3
Forcing hidden states to reconstruct priors ensures the architecture internalizes task relationships. An encoder-decoder module attached to the hypernetwork's intermediate layer tries to reconstruct the H-embedding from the hidden state, adding a guidance loss. This forces the network to retain task relationship information in its internal representations. The core assumption is that the hidden layer of the hypernetwork is a bottleneck where task relationship info must be preserved. Break condition: If the decoder is too weak or the loss weight is too low, the hidden state may ignore the H-embedding, reverting to generic weight generation.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: The paper targets the specific failure mode where learning task j degrades performance on task j-1.
  - Quick check question: Can you distinguish between regularization-based (EWC) and architecture-based (PackNet) approaches to this problem?

- **Concept: H-score (Transferability Metric)**
  - Why needed here: This is the mathematical engine of the "H-embedding." It estimates transferability using feature covariance and conditional expectations.
  - Quick check question: Do you understand how Eq. (1) relates feature statistics to the "informativeness" of a representation?

- **Concept: Analytic Hierarchy Process (AHP)**
  - Why needed here: Raw H-scores are asymmetric and scale-dependent. AHP normalizes these into a consistent relative ranking (eigenvector) to stabilize embedding learning.
  - Quick check question: Why would directly using raw H-scores as distance targets cause optimization instability in a sequence of 20+ tasks?

## Architecture Onboarding

- **Component map:** Input Data -> Feature Extractor (reconstructed previous task models) -> H-Embedding Module (H-scores → AHP → Optimize $\hat{e}^{(j)}$) -> Hypernet ($f_h$) (Takes current embedding $e^{(j)}$ → Generates Main Model Weights $\Theta^{(j)}$) -> Regularization Head (Encoder-Decoder on Hypernet intermediate layer → Reconstruction Loss)

- **Critical path:** The pre-training step for the embedding (Eq. 5) is critical. If this optimization fails or the AHP eigenvector calculation is unstable, the subsequent Hypernet training starts with a noisy initialization.

- **Design tradeoffs:**
  - **Efficiency vs. Accuracy:** Calculating H-scores requires forward passes through all previous task models (reconstructed via hypernet). This adds overhead at the start of each new task but eliminates the need for a rehearsal buffer.
  - **LoRA vs. Full Model:** The framework can generate full weights or just LoRA parameters. Generating only LoRA parameters is significantly more efficient but may cap peak performance compared to full model generation.

- **Failure signatures:**
  - **Divergence of $\gamma^{(j)}$:** If the scaling constant in Eq. 5 diverges, embedding distances become uninformative.
  - **Low FAA (Final Average Accuracy):** If backward transfer fails, check the CL Loss ($L_c$) weighting; if forward transfer fails, check the Embedding Guidance Loss ($L_e$).

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run on CIFAR-100 (N=10) with H-embedding disabled (random init) to isolate the contribution of the transferability-aware initialization.
  2. **AHP Ablation:** Disable AHP normalization and use raw H-score inversion to verify stability improvements claimed in Section 4.1.
  3. **Visual Validation:** Plot the t-SNE of learned embeddings for DomainNet. Verify if similar domains (e.g., 'Real' vs 'Sketch') cluster closer than dissimilar ones, aligning with the "Embedding Interpretability" results in Appendix A.8.

## Open Questions the Paper Calls Out

- **Question:** How can task ID inference be integrated directly into the online framework to support Class-Incremental Learning (CIL) without relying on a separately trained task classifier?
  - Basis in paper: The authors state the current CIL adaptation is "limited by its reliance on pretrained model features and the separately trained task classifier" and aim to explore "more integrated" strategies.
  - Why unresolved: The current method decouples ID inference from the main CL optimization, violating the online, end-to-end nature of the proposed framework.
  - What evidence would resolve it: A proposed architecture where task embeddings or H-scores facilitate self-identification of task boundaries during inference without external modules.

- **Question:** How does the performance of H-embedding guidance change when substituting H-score with other transferability metrics like LEEP or LogME?
  - Basis in paper: Appendix A.3 notes the framework is "not inherently tied to H-score" and suggests other metrics "could also, in principle, be used."
  - Why unresolved: While H-score is theoretically grounded, its computational cost (covariance inversion) and reliance on feature quality may differ from heuristic alternatives, but a comprehensive comparison was not the main focus.
  - What evidence would resolve it: Comparative analysis on identical benchmarks using LEEP/LogME embeddings to measure convergence speed and final accuracy trade-offs.

- **Question:** How robust is the H-embedding estimation to low-quality feature extractors during the early stages of training on a new task?
  - Basis in paper: The H-score calculation relies on the feature extractor of the current model; if the model is untrained or overfitted on previous tasks, the transferability estimate might be inaccurate.
  - Why unresolved: The paper uses AHP to normalize, but does not analyze the impact of "noisy" H-scores derived from unadapted backbones on the geometric structure of the embedding space.
  - What evidence would resolve it: A sensitivity analysis measuring the degradation of embedding alignment when features are perturbed or derived from a randomly initialized backbone.

## Limitations
- **Efficiency vs. Accuracy Tradeoff**: The method's efficiency gains from avoiding rehearsal buffers are offset by the computational cost of H-score computation, which requires feature extraction from all previous tasks at the start of each new task. The paper does not report the wall-clock time or memory overhead of this step.
- **H-score Validity Assumptions**: The H-score metric assumes that feature covariance statistics from previous tasks are stable and representative of transferability. In low-data regimes or when tasks have significant domain shift, the H-score may become a noisy proxy.
- **Hypernetwork Capacity Constraints**: The method assumes the hypernetwork has sufficient capacity to map a 32-dimensional embedding to optimal weights for all tasks without interference. For complex models or many diverse tasks, this assumption may break down.

## Confidence
- **High Confidence**: The architectural framework (Hypernetwork + H-embedding + CL regularization) is clearly specified and experimentally validated on standard benchmarks. The method's ability to outperform baselines on CIFAR-100, ImageNet-R, and DomainNet is well-supported.
- **Medium Confidence**: The theoretical justification for using H-scores as a transferability proxy is sound, but its robustness across diverse task distributions is not fully explored. The paper relies on ablation studies to support design choices.
- **Low Confidence**: The efficiency claims are not fully quantified. The wall-clock time for H-score computation and the impact on real-world deployment are not reported.

## Next Checks
1. **Runtime Profiling**: Measure the wall-clock time and memory usage for H-score computation on DomainNet. Compare this overhead to the time saved by not using a rehearsal buffer.
2. **H-score Robustness Test**: Evaluate the method on a low-data continual learning benchmark (e.g., Split-CIFAR-10 with 2 examples per class). Analyze whether FAA degrades when H-scores become unreliable.
3. **Hypernetwork Capacity Scaling**: Test the method on a setting with 50+ tasks or with a larger model (e.g., ResNet-50). Monitor FAA to detect if capacity limits cause forgetting.