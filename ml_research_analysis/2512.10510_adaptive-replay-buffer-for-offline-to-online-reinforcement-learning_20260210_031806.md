---
ver: rpa2
title: Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning
arxiv_id: '2512.10510'
source_url: https://arxiv.org/abs/2512.10510
tags:
- data
- offline
- learning
- online
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Adaptive Replay Buffer (ARB), a lightweight
  method for dynamically prioritizing data in offline-to-online reinforcement learning.
  ARB assigns sampling weights based on "on-policyness," a metric measuring how closely
  transitions align with the current policy's behavior.
---

# Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2512.10510
- **Source URL:** https://arxiv.org/abs/2512.10510
- **Reference count:** 9
- **Primary result:** Achieves up to 32.0 normalized return on D4RL, outperforming prior methods like 20.7 for BERB.

## Executive Summary
This paper introduces the Adaptive Replay Buffer (ARB), a lightweight method for dynamically prioritizing data in offline-to-online reinforcement learning. ARB assigns sampling weights based on "on-policyness," a metric measuring how closely transitions align with the current policy's behavior. Unlike prior approaches, ARB is learning-free and integrates seamlessly into existing algorithms. Experiments on D4RL benchmarks show ARB consistently outperforms baselines, particularly on low-quality datasets, achieving average normalized returns of up to 32.0 compared to 20.7 for the next best method. ARB's adaptive prioritization enables efficient filtering of unhelpful offline data and faster fine-tuning.

## Method Summary
ARB dynamically prioritizes transitions in the replay buffer based on their "on-policyness," measured as the log-likelihood of the action under the current policy. The method computes sampling weights for trajectories (geometric mean of individual transition weights), clamps and normalizes these weights, and periodically re-calculates them as the policy evolves. ARB is learning-free, requiring no additional models or complex computations. It integrates with base O2O RL algorithms like Cal-QL, PEX, and FamO2O, and is evaluated on D4RL locomotion and Antmaze tasks. The method aims to filter out low-quality offline data while preserving valuable online experiences during fine-tuning.

## Key Results
- ARB achieves up to 32.0 normalized return on Hopper-Random, outperforming BERB's 20.7.
- ARB shows consistent gains across all D4RL locomotion datasets, with largest improvements on low-quality (Random) data.
- ARB enables faster fine-tuning by automatically increasing the ratio of online data as the policy improves.

## Why This Works (Mechanism)

### Mechanism 1: Action-Likelihood as a Relevance Proxy
- **Claim:** Sampling transitions proportional to the current policy’s action likelihood (on-policyness) may improve sample efficiency by filtering out low-relevance experiences without requiring a learned model.
- **Mechanism:** The system computes a sampling weight $\omega$ based on the log-likelihood of the action taken in a transition, given the current policy $\pi_\theta(a|s)$. This acts as a heuristic for "on-policyness," prioritizing data that aligns with the agent's current behavior.
- **Core assumption:** The paper assumes that transitions with higher action probability under the current policy are more valuable for learning than low-probability transitions, and that $\pi_\theta(a|s)$ is a sufficient proxy for data relevance.
- **Evidence anchors:**
  - [Section 4.1]: Defines simplified on-policyness as $\tilde{O}(s, a; \pi_\theta) = \pi_\theta(a|s)$ to avoid the intractability of stationary state distributions.
  - [Abstract]: States ARB is "learning-free" and assigns weights based on alignment with current policy behavior.
  - [Corpus]: Related work (e.g., BERB) relies on learned metrics; evidence in this specific corpus for *unlearned* likelihood-based proxies is limited to this paper's assertion of reduced complexity.
- **Break condition:** If the policy exhibits high entropy (random exploration) or if the optimal policy requires visiting states/actions currently deemed low-probability by the existing policy, this mechanism may discard useful exploration data or reinforce suboptimal convergence.

### Mechanism 2: Trajectory-Level Aggregation for Variance Reduction
- **Claim:** Aggregating sampling weights at the trajectory level rather than the transition level stabilizes training by mitigating the risk of over-sampling isolated high-likelihood transitions.
- **Mechanism:** Instead of sampling single transitions based on their specific likelihood, ARB calculates the weight of a trajectory as the geometric mean of the likelihoods of its constituent transitions. This reduces variance in sampling probabilities.
- **Core assumption:** Transitions within a single trajectory share temporal relevance; high variance in individual transition sampling is detrimental to gradient stability.
- **Evidence anchors:**
  - [Section 4.2]: "ARB calculates the on-policyness at the trajectory level... ensuring a smoother and more stable training process by reducing the variance of sampling probabilities."
  - [Section 5.4.2]: Ablation study (Figure 3) shows trajectory-based methods achieving higher final scores compared to transition-based methods.
  - [Corpus]: Weak support in neighbors; corpus focuses on policy expansion/generation rather than buffer variance reduction.
- **Break condition:** If a trajectory contains a mix of highly relevant and highly irrelevant (detrimental) transitions, aggregating them may preserve "bad" data within the batch simply because it is bundled with "good" data.

### Mechanism 3: Implicit Data Quality Filtering
- **Claim:** By dynamically re-weighting the buffer as the policy evolves, ARB implicitly filters out low-quality offline data that diverges from the improving online policy.
- **Mechanism:** As the agent fine-tunes online, its policy shifts. The likelihood of old, low-reward offline data (specifically from "random" datasets) drops under the new policy parameters. Consequently, the sampling ratio of online data naturally increases without a hard-coded schedule.
- **Core assumption:** Offline data quality varies, and data inconsistent with a improving policy should be deprioritized automatically.
- **Evidence anchors:**
  - [Section 5.3]: "In these cases [low reward offline data], ARB’s online data ratio curve rises sharply... allowing the agent to rapidly shift its focus to the more valuable online experiences."
  - [Section 4.2]: Weights are recalculated periodically ($d_{weight}$) to reflect the current policy.
  - [Corpus]: Neighbors like "Robust Policy Expansion" address data corruption explicitly, whereas ARB addresses it implicitly via relevance weighting.
- **Break condition:** If the offline dataset contains rare but critical "golden" data (e.g., recovery behaviors) that the online policy drifts away from, ARB might deprioritize this essential data precisely when it is needed for robustness.

## Foundational Learning

- **Concept: Off-Policy Reinforcement Learning & Replay Buffers**
  - **Why needed here:** ARB modifies the standard off-policy replay buffer. One must understand that off-policy algorithms (like SAC or Cal-QL) typically sample uniformly from history, which ARB alters to prioritize relevance.
  - **Quick check question:** How does uniform sampling differ from prioritized sampling in terms of sample efficiency and bias?

- **Concept: Log-Likelihood & Probability Densities**
  - **Why needed here:** The core metric (on-policyness) relies on calculating $\log \pi_\theta(a|s)$. Understanding how to derive this from Gaussian policies (continuous action spaces) or categorical distributions is required for implementation.
  - **Quick check question:** Given a continuous action $a$ and a policy mean $\mu$ and std $\sigma$, how do you compute the log-likelihood?

- **Concept: Offline-to-Online (O2O) Distribution Shift**
  - **Why needed here:** The paper positions ARB as a solution to the "replay buffer dilemma" caused by distribution shift between the static offline dataset and the dynamic online data.
  - **Quick check question:** Why might mixing offline and online data uniformly cause "catastrophic forgetting" or performance collapse during fine-tuning?

## Architecture Onboarding

- **Component map:**
  - Base Algorithm (A) -> Storage (B) -> Weight Calculator -> Sampler

- **Critical path:**
  1.  **Initialization:** Pre-train policy $\pi_\theta$ on $D_{offline}$.
  2.  **Collection:** Add new transitions to Buffer $B$.
  3.  **Re-weighting (every $d_{weight}$ steps):** Fetch all transitions (or specific trajectories), compute $\log \pi_\theta(a|s)$, clip to $[\underline{p}, \bar{p}]$, normalize, and aggregate to trajectory level $\omega$.
  4.  **Update (every $d_{update}$ steps):** Sample minibatch $M$ using weights $\omega$. Update $\theta$ via Base Algorithm $A$.

- **Design tradeoffs:**
  - **Temperature $\lambda$:** Low $\lambda$ forces aggressive prioritization (high variance, fast adaptation); high $\lambda$ approximates uniform sampling (stable, slower).
  - **Re-weight Frequency ($d_{weight}$):** Frequent re-weighting is accurate but computationally expensive (requires forward passes on the whole buffer); infrequent re-weighting uses stale probabilities.
  - **Clipping ($\underline{p}, \bar{p}$):** Essential for stability to prevent extreme weights; requires tuning based on action dimensions.

- **Failure signatures:**
  - **Performance Collapse:** If $\lambda$ is too low, the agent may repeatedly sample a tiny subset of "on-policy" data, causing overfitting.
  - **Stagnation:** If $\lambda$ is too high or the buffer is flooded with offline data, the "online data ratio" curve fails to rise, and the agent fails to improve beyond the offline dataset's performance.
  - **Numerical Instability:** Failure to clip log-likelihoods or normalize correctly can lead to NaN losses due to exponent overflow.

- **First 3 experiments:**
  1.  **Unit Test - Weight Calculation:** Verify that for a given policy and trajectory, the function outputs the expected geometric mean of clipped exponentiated log-likelihoods.
  2.  **Visualization - Online Ratio Plot:** Replicate Figure 1 on a simple environment (e.g., Hopper-Random) to confirm the online data ratio increases over time compared to the Naive baseline.
  3.  **Ablation - Temperature $\lambda$:** Run a sweep on $\lambda$ (e.g., 0.2, 0.5, 1.0, 2.0) to observe the trade-off between early stability and final return, confirming the "stability vs. adaptation" curve.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating an approximate state density term into the "on-policyness" metric improve performance in sparse reward settings, or does it strictly hinder exploration as hypothesized?
  - **Basis in paper:** [Explicit] Page 3 states that excluding the state distribution $d^{\pi_\theta}(s)$ was a deliberate choice because relying on it "could inadvertently lead the agent to over-prioritize states it has already extensively explored, potentially inhibiting further exploration."
  - **Why unresolved:** The paper assumes this exclusion is beneficial for exploration, but provides no empirical comparison against a metric that includes state density, leaving the trade-off unverified.
  - **What evidence would resolve it:** A comparative study evaluating ARB against a variant that utilizes a density model (e.g., VAE or Flow model) to incorporate state likelihoods in exploration-heavy or sparse reward tasks.

- **Open Question 2:** Can the temperature parameter $\lambda$ be adapted automatically during training to optimize the stability-adaptation trade-off without manual tuning?
  - **Basis in paper:** [Inferred] Figure 2 and Section 5.4.1 demonstrate that performance is sensitive to fixed values of $\lambda$, establishing a "trade-off between stability and adaptation" that currently requires manual selection.
  - **Why unresolved:** The paper establishes the importance of $\lambda$ but leaves the mechanism for setting it as a manual hyperparameter search, which may be sub-optimal as the policy shifts from offline pre-training to online fine-tuning.
  - **What evidence would resolve it:** Implementation of a meta-learning or heuristic-based scheduler for $\lambda$ (e.g., inversely proportional to policy entropy) that matches or exceeds the performance of the best fixed $\lambda$ across diverse datasets.

- **Open Question 3:** Does the computational overhead of re-calculating sampling weights for the entire buffer become a bottleneck in large-scale data regimes?
  - **Basis in paper:** [Inferred] Algorithm 1 (lines 13-16) dictates recalculating the weight $\omega$ for every transition in buffer $B$ every $d_{weight}$ steps. While described as "lightweight," this is an $O(N)$ operation that may not scale efficiently.
  - **Why unresolved:** The experiments utilize standard D4RL benchmarks, but the latency of repeatedly computing log-likelihoods for millions of transitions in larger, modern replay buffers is not profiled.
  - **What evidence would resolve it:** Profiling the training wall-clock time of ARB on a replay buffer with capacity exceeding 1 million transitions, compared against a stochastic or incremental weight update approximation.

## Limitations
- **ABLATION GAP:** The paper provides strong ablations for temperature λ and trajectory-level aggregation, but lacks ablation studies on critical design choices like clipping thresholds [p, p̄], re-weighting frequency d_weight, or batch-size effects.
- **EVALUATION SCOPE:** Performance claims are confined to D4RL locomotion tasks and Antmaze, with no testing on non-locomotion tasks like robotic manipulation or Atari.
- **CORRELATION VS CAUSATION:** The paper attributes ARB’s gains to its adaptive prioritization but doesn’t fully isolate whether gains stem from improved sample efficiency or implicit regularization via reduced data diversity.

## Confidence
- **High Confidence:** Claims about ARB’s implementation simplicity, learning-free design, and seamless integration with base algorithms are directly verifiable from the method description.
- **Medium Confidence:** Claims about ARB’s superior performance on low-quality datasets (e.g., Hopper-Random) are supported by Table 1 and Figure 1, but the exact contribution of ARB vs. base algorithm improvements is not fully disentangled.
- **Low Confidence:** Claims about ARB’s robustness to distribution shift and its implicit filtering mechanism are plausible but lack rigorous ablation or theoretical grounding.

## Next Checks
1. **Hyperparameter Sensitivity:** Run ARB with varying clipping thresholds [p, p̄] and re-weighting frequencies d_weight to quantify their impact on stability and final returns.
2. **Task Generalization:** Test ARB on non-locomotion tasks (e.g., D4RL hand manipulation or Atari) to assess whether the Gaussian log-likelihood assumption holds or requires adaptation.
3. **Mechanistic Ablation:** Isolate the contribution of ARB’s adaptive prioritization by comparing it against a fixed-priority baseline (e.g., static weights based on initial policy) to confirm that dynamic re-weighting is the primary driver of gains.