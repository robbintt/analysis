---
ver: rpa2
title: 'Finding Answers in Thought Matters: Revisiting Evaluation on Large Language
  Models with Reasoning'
arxiv_id: '2510.14773'
source_url: https://arxiv.org/abs/2510.14773
tags:
- answer
- extraction
- reasoning
- performance
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how answer extraction methods significantly\
  \ impact evaluation of reasoning models, showing performance can vary by over 20%\
  \ depending on the extraction algorithm used. To address this, the authors propose\
  \ AnswerRegeneration\u2014a simple framework where a reasoning model\u2019s output\
  \ is passed through a second inference step prefixed with \"Answer:\" to regenerate\
  \ a concise final answer."
---

# Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning

## Quick Facts
- arXiv ID: 2510.14773
- Source URL: https://arxiv.org/abs/2510.14773
- Reference count: 3
- Answer extraction method choice can impact reasoning model evaluation by over 20% accuracy

## Executive Summary
This paper addresses a critical but overlooked problem in reasoning model evaluation: how answer extraction methods significantly impact measured performance. The authors demonstrate that rule-based extraction can vary performance by over 20% depending on the model and benchmark, leading to inconsistent rankings and unfair comparisons. To solve this, they propose AnswerRegenerationâ€”a simple two-step framework where a reasoning model's output is passed through a second inference pass prefixed with "Answer:" to regenerate a concise final answer. This method outperforms traditional rule-based extraction across multiple benchmarks, improving accuracy by 1.2-5.0% and providing more reliable model comparisons while avoiding the bias issues seen in LLM-as-a-judge evaluations.

## Method Summary
AnswerRegeneration is a two-step evaluation framework that improves answer extraction from reasoning model outputs. First, a reasoning model generates a Chain-of-Thought (CoT) output with temperature 0.6, top_p=0.95, top_k=20, and max_tokens=4096. Second, the same model (in non-reasoning mode) receives the original prompt concatenated with the reasoning output and an "Answer:" prefix, then regenerates a concise final answer. For multiple-choice tasks, the regenerated output uses probability-based selection over options; for open-ended tasks, simple string matching with minor LaTeX cleanup is applied. This approach addresses the variability and fragility of rule-based extraction methods while avoiding the bias issues of LLM-as-a-judge evaluations.

## Key Results
- Answer extraction method choice can impact reasoning model evaluation by over 20% accuracy
- AnswerRegeneration improves accuracy by 1.2-5.0% across MMLU, MMLU-Pro, GSM8K, and TriviaQA compared to best rule-based extraction
- The framework reduces model ranking inconsistencies and handles incomplete reasoning outputs (2.8-6.8% of cases) more robustly than rules
- Regeneration resolves internal self-correction ambiguity by forcing a single committed answer from the model's full reasoning context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An additional inference step that prompts the model to regenerate its final answer produces more reliable extraction than rule-based parsing.
- Mechanism: After generating a reasoning trace, the model receives its own input-output pair with an "Answer:" prefix and generates a concise final answer in a standardized format, enabling probability-based selection for multiple-choice or simple string matching for open-ended tasks.
- Core assumption: The model in non-reasoning mode can faithfully synthesize a final answer from its own reasoning output without hallucinating or contradicting its prior conclusion.
- Evidence anchors:
  - [abstract] "The method uses an additional model inference, providing the prior input and output prefaced by the prompt 'Answer:'. The final answer is then selected or extracted from the regenerated output."
  - [Section 5.1] "We provide the model (in its non-reasoning mode) with the original input prompt and its previous output (the reasoning process), and a new prefix 'Answer:'."
  - [corpus] Weak direct corpus support; neighbor papers focus on reasoning trace quality but not extraction methods.
- Break condition: If the reasoning trace contains contradictory intermediate conclusions, the regeneration step may select an answer inconsistent with the model's final reasoning state.

### Mechanism 2
- Claim: AnswerRegeneration handles incomplete reasoning outputs more robustly than rule-based extraction.
- Mechanism: When reasoning traces are truncated (missing closing tags), rules fail to find formatted answers; regeneration prompts the model to synthesize from partial context.
- Core assumption: The model can infer a reasonable conclusion even from incomplete reasoning traces.
- Evidence anchors:
  - [Section 4.2.3] Reports 2.8-6.8% of outputs have incomplete thinking; Table 1 shows high invalid extraction rates for incomplete outputs using rules.
  - [Section 5.2.3] "Our method significantly improves performance in cases of incomplete thinking... our framework can select the final answer even when the thought does not include an explicit final answer."
  - [corpus] Not directly addressed in neighbor papers.
- Break condition: If truncation occurs before key reasoning steps, the regenerated answer may be arbitrary or wrong.

### Mechanism 3
- Claim: Regeneration resolves internal self-correction ambiguity by forcing a single committed answer.
- Mechanism: Rule-based extraction may capture an intermediate answer that the model later revises; regeneration consumes the full trace and outputs only the final committed position.
- Core assumption: The model's final state after reading its full reasoning reflects its settled conclusion.
- Evidence anchors:
  - [Section 5.2.3] "When facing ambiguous questions, a model may initially provide an answer and then continue its thinking process... Rule-based answer extraction methods struggle to choose the final answer from this internal debate."
  - [Section 4.2.2] Figure 3 shows extraction methods disagreeing on the same output containing multiple candidate answers.
  - [corpus] Neighbor paper "First Try Matters" examines reflection in reasoning but does not address extraction.
- Break condition: If the model's self-correction is itself uncertain, regeneration may still output an inconsistent answer.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: Reasoning models output extended verbal traces before concluding; evaluation must extract the final answer from this unstructured text.
  - Quick check question: Can you explain why probability-based answer selection (used for standard LLMs) fails when models generate CoT outputs?

- Concept: **Regular Expression Extraction**
  - Why needed here: The paper benchmarks against heuristic rules that pattern-match answers; understanding their fragility motivates the proposed approach.
  - Quick check question: What happens when a model outputs "\boxed{B}" instead of "Answer: B"?

- Concept: **LLM-as-a-Judge Evaluation Bias**
  - Why needed here: The paper compares against model-based graders (GPTGrader, xVerify) and shows they exhibit systematic bias toward "correct" or "incorrect" labels.
  - Quick check question: Why might a string-match approach avoid the label bias seen in model-based judges?

## Architecture Onboarding

- Component map: Reasoning pass -> AnswerRegeneration pass -> Extraction layer
- Critical path: The regeneration step must receive the full unmodified reasoning output; truncation or reformatting before this step will degrade performance.
- Design tradeoffs:
  - Extra inference call doubles compute cost per evaluation.
  - Regeneration may miss minor format variations (e.g., "**A**").
  - Avoids model bias from LLM-as-a-judge but cannot assess semantic equivalence for open-ended answers without alias lists.
- Failure signatures:
  - Regenerated answer contradicts explicit answer in reasoning trace (model hallucination during synthesis).
  - Empty or invalid regenerated output (model refuses or fails to commit).
  - Probability tie across multiple choices (rare but possible).
- First 3 experiments:
  1. Reproduce Table 2 on a single model: compare best rule-based extraction vs. AnswerRegeneration on MMLU, reporting accuracy delta.
  2. Test regenerator independence: swap in a smaller model (e.g., Qwen3-0.6B) for regeneration and compare against same-model regeneration (per Table 7).
  3. Evaluate on incomplete outputs: filter for truncated reasoning traces and compare extraction success rates between rules and regeneration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the AnswerRegeneration framework maintain its robustness and performance advantages when applied to proprietary commercial models such as GPT-4 or Claude?
- Basis in paper: [explicit] The authors state in the Limitations section, "We defer the investigation of commercial LLMs, such as ChatGPT, Gemini, and Claude, to future work."
- Why unresolved: The study exclusively tested open-source models (Qwen, DeepSeek), leaving the behavior of commercial APIs under this framework unknown.
- Evidence that would resolve it: Applying the AnswerRegeneration framework to commercial model outputs on MMLU and GSM8K and comparing the results to the reported baselines.

### Open Question 2
- Question: Can the reliability of AnswerRegeneration be further improved by integrating sampling strategies like self-consistency?
- Basis in paper: [explicit] The authors note in the Limitations that "Exploring further techniques within this framework, such as incorporating concepts like self-consistency... represents a valuable direction for future research."
- Why unresolved: The current implementation relies on a single additional inference step, which may still be susceptible to variance.
- Evidence that would resolve it: An ablation study comparing single-pass regeneration against a majority-vote ensemble of regenerated answers.

### Open Question 3
- Question: Does the performance gain of AnswerRegeneration stem from the model refining its reasoning during the second inference, or merely from improved formatting of the existing reasoning?
- Basis in paper: [inferred] The method provides the "prior input and output" to the model (Section 5.1), potentially allowing the model to alter its logic rather than just extract text.
- Why unresolved: The paper demonstrates improved scores (correction rates) but does not disentangle whether the model is logically "re-thinking" or just summarizing.
- Evidence that would resolve it: A detailed analysis of "corrected" instances to determine if the regenerated output introduces new logical steps or simply cleans up the syntax of the original thought process.

### Open Question 4
- Question: How does AnswerRegeneration perform on open-ended tasks that require complex semantic equivalence matching rather than simple string matching?
- Basis in paper: [inferred] The authors acknowledge they used TriviaQA specifically because it minimizes the need for complex semantic matching (Section 6.3), avoiding the second challenge of open-ended evaluation.
- Why unresolved: The framework's reliance on simplified output and string matching may fail for tasks where the correct answer is semantically identical but lexically distinct from the gold label.
- Evidence that would resolve it: Evaluation on datasets requiring deep semantic alignment (e.g., summarization or complex QA) comparing AnswerRegeneration against LLM-as-a-judge baselines.

## Limitations
- The paper's regeneration approach depends critically on the model's ability to faithfully regenerate its own answer from reasoning traces, but the mechanism by which models maintain consistency between reasoning and regeneration is not fully explained.
- The evaluation focuses on extraction accuracy improvements (1.2-5.0%) but doesn't deeply investigate cases where regeneration might hallucinate answers that contradict the reasoning trace.
- While the method shows improvement on incomplete reasoning outputs, the 2.8-6.8% truncation rate suggests the underlying reasoning generation process remains fragile.

## Confidence
- **High Confidence**: The empirical finding that extraction method choice significantly impacts evaluation accuracy (over 20% variation) is well-supported by systematic benchmarking across multiple models and datasets.
- **Medium Confidence**: The claim that AnswerRegeneration handles incomplete reasoning and self-correction more robustly than rules is supported by targeted experiments but could benefit from deeper analysis of failure modes.
- **Low Confidence**: The assertion that AnswerRegeneration completely avoids model bias is questionable, as the regeneration step still involves the same model making a judgment about its own output.

## Next Checks
1. **Contradiction Detection**: For a subset of MMLU samples, compare the regenerated answer against explicit answers found in the reasoning trace to measure hallucination rates and quantify how often regeneration introduces inconsistencies.
2. **Cross-Model Regeneration**: Use a different model (e.g., Qwen2.5-7B) to perform the regeneration step on outputs from Qwen3-32B, testing whether regeneration performance degrades when the regenerator doesn't share the same training distribution.
3. **Cost-Benefit Analysis**: Measure the actual compute overhead of the second inference pass across different batch sizes and evaluate whether the accuracy gains justify the doubled inference cost for production deployment scenarios.