---
ver: rpa2
title: Toward Temporal Causal Representation Learning with Tensor Decomposition
arxiv_id: '2507.14126'
source_url: https://arxiv.org/abs/2507.14126
tags:
- causal
- tensor
- problem
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CaRTeD, a joint learning framework that integrates
  temporal causal representation learning with irregular tensor decomposition. The
  method addresses the challenge of learning meaningful phenotypes and their causal
  relationships from high-dimensional, irregularly structured data such as electronic
  health records.
---

# Toward Temporal Causal Representation Learning with Tensor Decomposition

## Quick Facts
- **arXiv ID:** 2507.14126
- **Source URL:** https://arxiv.org/abs/2507.14126
- **Reference count:** 40
- **Primary result:** CaRTeD jointly learns temporal causal structure and tensor decomposition from irregular EHR data, outperforming state-of-the-art baselines in both reconstruction and causal recovery.

## Executive Summary
This paper introduces CaRTeD, a joint learning framework that integrates temporal causal representation learning with irregular tensor decomposition. The method addresses the challenge of learning meaningful phenotypes and their causal relationships from high-dimensional, irregularly structured data such as electronic health records. CaRTeD uses a block-coordinate descent approach to alternately update tensor decomposition factors and temporal causal structure, incorporating novel regularization to improve accuracy. Theoretical analysis proves convergence to a stationary point under non-convex constraints, filling a gap in irregular tensor decomposition theory. Experiments on synthetic and MIMIC-III EHR datasets show CaRTeD outperforms state-of-the-art baselines in both tensor factorization (SIM scores ~0.99, CPI ~0.71) and causal structure recovery (SHD ~3, TPR ~0.6-0.8), yielding interpretable and clinically meaningful causal phenotype networks.

## Method Summary
CaRTeD is a joint learning framework that combines irregular tensor decomposition (via PARAFAC2) with temporal causal structure learning. The method uses block-coordinate descent to alternately update tensor factors ($U_k, S_k, V$) and causal structure ($W, A$). The tensor block employs ADMM to enforce PARAFAC2 constraints and causal regularization, while the causal block uses ADMM with aggregation to update contemporaneous ($W$) and time-lagged ($A$) causal matrices under acyclicity constraints. The framework handles varying sequence lengths without padding through shared latent spaces and incorporates differentiable acyclicity constraints for temporal causality.

## Key Results
- **Tensor Decomposition:** CaRTeD achieves SIM scores ~0.99 and CPI ~0.71 on MIMIC-III data, significantly outperforming baselines
- **Causal Recovery:** SHD ~3, TPR ~0.6-0.8 on synthetic data, demonstrating accurate DAG recovery
- **Warm Start Improvement:** W-CaRTeD improves SIM score from ~0.91 to ~0.99 through pre-computed phenotype initialization
- **Clinical Validation:** Recovered causal networks show interpretable and clinically meaningful disease progression patterns

## Why This Works (Mechanism)

### Mechanism 1: Joint Regularization Feedback Loop
- **Claim:** Alternating between tensor decomposition and causal discovery improves the accuracy of both tasks compared to sequential, independent approaches.
- **Mechanism:** The framework uses Block-Coordinate Descent (BCD). In Block 1, it updates tensor factors using current causal structure as regularizer. In Block 2, it updates causal structure using refined tensor factors as observations. This allows "causal-informed regularization" to correct decomposition errors that purely reconstruction-based methods might make.
- **Core assumption:** The latent phenotypes extracted from the tensor share a consistent, underlying causal structure across all slices (patients).
- **Evidence anchors:** Abstract states "CaRTeD, a joint learning framework that integrates temporal causal representation learning with irregular tensor decomposition." Table 2 shows CaRTeD achieving higher CPI and RR scores than baseline COPA, which lacks causal regularization.

### Mechanism 2: Handling Irregularity via Shared Latent Space (PARAFAC2)
- **Claim:** The model can process varying sequence lengths without padding or warping.
- **Mechanism:** It employs PARAFAC2 decomposition, which enforces constraint $U_k^\top U_k = \Phi$ across all slices. This allows learning a shared latent phenotype space (defined by invariant matrix $H$) even while temporal evolution matrix $U_k$ has different dimensions for each patient.
- **Core assumption:** Variance in sequence length is arbitrary (measurement timing) and does not fundamentally alter the nature of latent phenotypes.
- **Evidence anchors:** Section 2.2 explains "PARAFAC2 itself accommodates one mode with varying dimensions... The constraint... is imposed to ensure uniqueness of the decomposition."

### Mechanism 3: Acyclicity Constraint for Temporal Causality
- **Claim:** The model recovers Directed Acyclic Graphs (DAGs) for both contemporaneous and time-lagged relationships.
- **Mechanism:** It utilizes continuous optimization function with differentiable algebraic constraint $h(W) = \text{tr}(e^W \circ W) - d = 0$. This forces weighted adjacency matrix $W$ to represent a DAG, preventing circular dependencies in contemporaneous network. Time-lagged matrices $A^{(p)}$ are naturally acyclic as they only point forward in time.
- **Core assumption:** The underlying system is causal and non-cyclic (effects cannot be their own causes within same time step).
- **Evidence anchors:** Section 2.3 describes "To enforce acyclicity, we use the constraint $h(W) = \text{tr}(e^W \circ W) - d$, proposed by Zheng et al."

## Foundational Learning

- **Concept: Tensor Rank and CANDECOMP/PARAFAC (CP)**
  - **Why needed here:** The paper relies on "low-rank" assumptions to reduce high-dimensional EHR data into "phenotypes." You must understand that Rank $R$ represents the number of hidden clusters.
  - **Quick check question:** If I set Rank $R$ to 4, am I forcing the model to find exactly 4 distinct patient phenotypes?

- **Concept: Dynamic Bayesian Networks (DBNs)**
  - **Why needed here:** The paper models disease progression as a graph with edges within a time slice ($W$) and across time ($A$). Understanding DBNs is required to interpret the "temporal causal phenotype network."
  - **Quick check question:** What is the difference between an edge in the intra-slice matrix $W$ (contemporaneous) and the inter-slice matrix $A$ (temporal)?

- **Concept: Augmented Lagrangian / ADMM**
  - **Why needed here:** The entire solver uses Alternating Direction Method of Multipliers (ADMM) to handle complex constraints. The "dual variables" ($\mu$) mentioned in the text are central to this method.
  - **Quick check question:** Why does the algorithm need to update dual variables (e.g., $\mu_{\tilde{U}_k}$) after updating the primal variables (e.g., $U_k$)?

## Architecture Onboarding

- **Component map:** Input: Irregular Tensor $\mathcal{X} = \{X_k\}$ (Patients $\times$ Features $\times$ Visits) -> Block 1 (Tensor Decomposition): Updates $U_k$ (trajectories), $S_k$ (patient weights), and $V$ (phenotypes) using ADMM -> Block 2 (Causal Structure): Updates $W$ (instant effects) and $A$ (lagged effects) using ADMM with aggregation -> Output: Phenotype matrix $V$ and Causal Graphs $W, A$

- **Critical path:** The outer loop is defined in Algorithm 4. You must initialize all factors, then strictly follow the sequence: Update Tensor Factors $\to$ Update Causal Matrices $\to$ Repeat until convergence. The convergence relies on the "stopping rule" derived from Lagrangian sufficiency.

- **Design tradeoffs:**
  - **Accuracy vs. Speed:** The paper introduces "Warm Start" (W-CaRTeD) using pre-computed $V$. This drastically improves performance (Table 2 shows SIM score jump from ~0.91 to ~0.99) but requires preliminary decomposition step.
  - **Flexibility vs. Constraints:** The framework allows custom regularization on $U$ and $S$, but theoretical convergence guarantee depends on sufficient penalty parameters ($\rho$).

- **Failure signatures:**
  - **Negative CPI/RR:** Table 2 shows baselines can produce negative Recovery Rates, indicating decomposition has failed or is arbitrarily rotated.
  - **Divergence:** If penalty parameters $\rho_{uk}$ or $\rho_{sk}$ are not "sufficiently large" (as noted in Theorem 1 proofs), Augmented Lagrangian may not decrease monotonically, leading to oscillation.

- **First 3 experiments:**
  1. **Smoke Test (Synthetic):** Generate data with $K=10$ patients and high noise ($\epsilon=1.0$). Run standard CaRTeD. If algorithm does not converge to stationary point (check if Loss decreases), hyperparameter initialization for $\rho$ is likely incorrect.
  2. **Warm Start Validation:** Run on MIMIC-III data first without warm start, then with warm start (approximating $V$ via basic decomposition). Compare SIM scores to ensure implementation matches the ~0.06 improvement cited in the text.
  3. **Baseline Comparison:** Isolate the causal block. Feed ground-truth tensor factors into the causal block alone. Compare Structural Hamming Distance (SHD) against joint-learning approach to verify that "joint" aspect is actually contributing to error correction.

## Open Questions the Paper Calls Out
None

## Limitations
- **Low-rank assumption dependence:** Performance relies on strong low-rank assumptions for tensor decomposition, which may not hold in all real-world datasets
- **Acyclicity constraint simplification:** The acyclicity constraint may oversimplify systems with feedback loops, potentially misrepresenting true causal dynamics
- **Convergence parameter uncertainty:** Theoretical guarantees assume sufficient penalty parameters but do not specify exact bounds, leaving practical convergence conditions unclear

## Confidence

- **High:** The joint learning framework's effectiveness in synthetic experiments (SIM scores ~0.99, SHD ~3)
- **Medium:** Performance on real-world MIMIC-III data, given potential domain shifts and preprocessing choices
- **Low:** Generalization to datasets with different structural properties (e.g., different noise levels, sequence length distributions)

## Next Checks

1. **Stress Test with Noisy Data:** Generate synthetic tensors with varying noise levels (e.g., ε ∈ [0.5, 2.0]) and assess robustness of both tensor decomposition and causal recovery

2. **Acyclicity Sensitivity Analysis:** Modify synthetic data to include small cycles and measure how the acyclicity constraint affects SHD and TPR

3. **Cross-Dataset Evaluation:** Apply CaRTeD to a different healthcare dataset (e.g., eICU) and compare performance to ensure results are not dataset-specific