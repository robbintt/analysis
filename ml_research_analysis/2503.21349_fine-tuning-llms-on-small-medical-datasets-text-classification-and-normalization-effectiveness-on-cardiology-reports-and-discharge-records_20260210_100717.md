---
ver: rpa2
title: 'Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization
  Effectiveness on Cardiology reports and Discharge records'
arxiv_id: '2503.21349'
source_url: https://arxiv.org/abs/2503.21349
tags:
- medical
- data
- training
- text
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of fine-tuning large
  language models (LLMs) on small medical datasets for text classification and named
  entity recognition (NER) tasks. Using German cardiology reports and the i2b2 Smoking
  Challenge dataset, the research demonstrates that fine-tuning small LLMs locally
  with limited training data (200-300 examples) can achieve performance comparable
  to larger models.
---

# Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization Effectiveness on Cardiology reports and Discharge records

## Quick Facts
- arXiv ID: 2503.21349
- Source URL: https://arxiv.org/abs/2503.21349
- Reference count: 14
- Key finding: Fine-tuning small LLMs on limited medical datasets (200-300 examples) achieves performance comparable to larger models for classification and NER tasks.

## Executive Summary
This study investigates the effectiveness of fine-tuning large language models on small medical datasets for text classification and named entity recognition tasks. Using German cardiology reports and the i2b2 Smoking Challenge dataset, the research demonstrates that fine-tuning small LLMs locally with limited training data can achieve performance comparable to larger models. The study shows notable improvements in both tasks, with fine-tuned models outperforming zero-shot approaches and reaching parity with larger open-source models. These findings suggest that task-specific fine-tuning of LLMs holds significant potential for automating clinical workflows and efficiently extracting structured data from unstructured medical text.

## Method Summary
The study employs fine-tuning of small LLMs on two medical datasets: German cardiology reports and the i2b2 Smoking Challenge dataset. The approach involves adapting pre-trained language models to specific medical classification and named entity recognition tasks using limited training examples (200-300 per task). The fine-tuning process is conducted locally to address data privacy concerns inherent in medical data processing. Models are evaluated on their ability to classify text and recognize entities, with particular attention to their adherence to machine-readable output formats and parsing error reduction.

## Key Results
- Fine-tuned small LLMs achieved performance comparable to larger open-source models on both classification and NER tasks
- Models showed improved adherence to machine-readable output formats and reduced parsing errors after fine-tuning
- Performance gains were observed across both German cardiology reports and English smoking status classification tasks

## Why This Works (Mechanism)
The effectiveness of fine-tuning small LLMs on limited medical datasets stems from the models' ability to adapt pre-existing language understanding capabilities to domain-specific patterns and terminology. The constrained dataset size forces the models to focus on essential features rather than memorizing extensive patterns, while local fine-tuning addresses privacy concerns that prevent external processing of sensitive medical data. The task-specific adaptation allows models to develop specialized representations for medical concepts and relationships that are not present in general training corpora.

## Foundational Learning
- **Medical domain adaptation**: Understanding specialized terminology and context is crucial for accurate medical text processing
  - Why needed: General language models lack exposure to medical concepts and relationships
  - Quick check: Evaluate model performance on domain-specific terminology recognition

- **Named entity recognition in clinical text**: Identifying and extracting medical entities from unstructured text requires understanding complex relationships
  - Why needed: Medical records contain critical information embedded in natural language
  - Quick check: Measure entity extraction accuracy across different entity types

- **Text classification in healthcare**: Categorizing medical documents requires understanding nuanced clinical contexts
  - Why needed: Automated classification enables efficient information retrieval and workflow automation
  - Quick check: Assess classification accuracy across diverse medical scenarios

- **Privacy-preserving fine-tuning**: Local model adaptation addresses data protection requirements for sensitive medical information
  - Why needed: Medical data cannot be shared externally due to privacy regulations
  - Quick check: Verify model performance without external data dependencies

## Architecture Onboarding

Component Map: Pre-trained LLM -> Fine-tuning Dataset -> Task-specific Adapter -> Evaluation Pipeline

Critical Path: The essential workflow involves loading a pre-trained model, applying task-specific fine-tuning using medical datasets, and evaluating performance on classification and NER tasks. The local fine-tuning step is critical as it enables privacy-compliant adaptation while maintaining model performance.

Design Tradeoffs: Small models offer faster fine-tuning and lower computational requirements but may have limited representational capacity compared to larger models. The choice of 200-300 training examples represents a balance between having sufficient data for meaningful adaptation and the practical constraints of medical dataset availability.

Failure Signatures: Models may overfit to limited training data, fail to generalize across different medical contexts, or produce inconsistent outputs when encountering novel medical terminology. Performance degradation is most likely when fine-tuning data doesn't represent the full diversity of medical text encountered in practice.

First Experiments:
1. Evaluate zero-shot performance on medical tasks before fine-tuning to establish baseline capabilities
2. Test fine-tuning with progressively smaller datasets (50, 100, 150 examples) to determine minimum effective training size
3. Compare fine-tuned model performance against prompt engineering approaches for the same medical tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize beyond German cardiology reports and the specific i2b2 dataset tested
- Limited sample size of 200-300 examples may not capture full variability of real-world medical text
- Evaluation focuses on task-specific metrics without extensive assessment of biases or long-term model stability

## Confidence
- High confidence in core findings for specific tasks and datasets evaluated
- Medium confidence in broader applicability across different medical domains and languages
- High confidence in reduced parsing errors and improved format adherence based on reported metrics

## Next Checks
1. Test the fine-tuning approach across multiple medical specialties and languages to assess generalizability beyond German cardiology reports
2. Evaluate model performance with varying dataset sizes (below 200 examples) to determine minimum effective training data requirements
3. Conduct longitudinal testing of fine-tuned models in production environments to assess stability, drift, and real-world performance over time