---
ver: rpa2
title: 'RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with
  Radiologist-like Workflows'
arxiv_id: '2509.20490'
source_url: https://arxiv.org/abs/2509.20490
tags:
- radagents
- reasoning
- tool
- arxiv
- workflow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RadAgents is a multi-agent framework that encodes radiologist-like
  workflows for chest X-ray interpretation, combining specialized subagents with tool
  use, visual retrieval-augmentation, and conflict resolution. It addresses limitations
  in prior methods by maintaining explicit, traceable reasoning steps and grounding
  outputs in visual evidence.
---

# RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows

## Quick Facts
- arXiv ID: 2509.20490
- Source URL: https://arxiv.org/abs/2509.20490
- Reference count: 18
- Key outcome: RadAgents achieves 73.6% accuracy on ChestAgentBench, 74.6% on CheXbench, and 51.4 GREEN score on MIMIC-CXR report generation

## Executive Summary
RadAgents introduces a multi-agent framework that encodes radiologist-like workflows for chest X-ray (CXR) interpretation, addressing the limitations of monolithic AI systems in complex multimodal medical reasoning. By decomposing interpretation into seven specialized agents aligned with the clinical ABCDE review scheme, the system achieves context isolation, explicit traceable reasoning, and visual evidence grounding. Evaluated on three challenging datasets, RadAgents consistently outperforms baselines through workflow-aligned context isolation, visual retrieval-augmented conflict resolution (V-RAG), and targeted model scaling—particularly for the Synthesizer tasked with conflict resolution.

## Method Summary
RadAgents implements a seven-agent system using LangGraph: an Orchestrator routes queries to five ABCDE subagents (Airway, Breathing, Circulation, Disability/Diaphragm, Exposure/Everything else), each maintaining domain-specific context and calling specialized tools (CXAS, MAIRA-2, BiomedParser, MedGemma, CheXagent, DenseNet-121). A Synthesizer agent resolves conflicts between tool outputs using visual retrieval-augmentation (V-RAG) with Rad-DINO embeddings. The system uses Qwen3-VL-Instruct-8B as the backbone, with targeted scaling to 30B parameters for the Synthesizer. Hybrid workflow retrieval (BM25 + Snowflake embeddings) and FAISS HNSW indexing support dispatch and conflict resolution.

## Key Results
- 73.6% accuracy on ChestAgentBench, surpassing single-agent baselines by 10-15%
- 74.6% accuracy on CheXbench across multiple test sets (Rad-Restruct 115, SLAKE 123, OpenI 380)
- 51.4 GREEN score on MIMIC-CXR report generation, demonstrating clinically relevant text synthesis
- V-RAG contributes significantly to conflict resolution, improving rates from 26% to 60% when using a 30B Synthesizer

## Why This Works (Mechanism)

### Mechanism 1: Workflow-Aligned Context Isolation
The Orchestrator routes queries to specialized ABCDE agents, confining each to task-specific context (e.g., Circulation agent focuses on cardiac metrics). This prevents context drift and "lost in the middle" phenomena common in monolithic models. The core assumption is that clinical reasoning can be decomposed into anatomical/functional categories without losing cross-domain dependencies.

### Mechanism 2: Visual Retrieval-Augmented Conflict Resolution (V-RAG)
When the Synthesizer detects tool output conflicts, it queries a vector database using Rad-DINO embeddings to retrieve visually similar historical cases. These exemplars serve as reference material to adjudicate correct findings, rather than relying solely on parametric knowledge. The assumption is that the retrieval database contains cases with correct labels and the embedding space captures semantic similarity.

### Mechanism 3: Targeted Synthesizer Scaling
Allocating model capacity to the Synthesizer yields higher returns than scaling other components. The Synthesizer must integrate heterogeneous tool outputs and resolve conflicts, requiring more capacity than the Orchestrator's robust intent classification. Conflict resolution rates rise from 26% to 60% when scaling the Synthesizer from 4B to 30B parameters.

## Foundational Learning

- **ReAct (Reason + Act) Paradigm**: RadAgents' subagents use ReAct loops (Thought → Action → Observation) rather than fixed scripts. Quick check: Can you trace the loop where an agent identifies missing segmentation, calls a tool, and updates its plan?

- **The ABCDE Clinical Review Scheme**: The architecture mirrors the radiological workflow (Airway, Breathing, Circulation, Disability/Diaphragm, Exposure/Everything else). Quick check: If the query is "Is there a pneumothorax?", which agent (Breathing) handles it, and which tools (MAIRA-2, DenseNet) might it prioritize?

- **Vector Embeddings & RAG**: The system uses visual embeddings (Rad-DINO) to find similar X-rays for conflict resolution. Similarity here is semantic/visual vector proximity, not text matching. Quick check: If V-RAG retrieves an irrelevant reference, is it a failure of the retriever (embedding) or the generation step?

## Architecture Onboarding

- **Component map**: Query → Orchestrator (Intent Analysis) → Dispatch to ABCDE Agents → Agents call Tools (Segment/Measure) → Results cached in Memory → Synthesizer (Aggregates + Checks Conflicts via V-RAG) → Final Report

- **Critical path**: Query flows through intent classification, parallel agent execution, tool-based measurements, conflict resolution via V-RAG, and final synthesis

- **Design tradeoffs**: Parallel execution lowers latency but requires complex Synthesizer; 8B models for agents balance cost with 30B for Synthesizer for high-level reasoning; V-RAG's k>3 introduces harmful noise

- **Failure signatures**: SkillMismatchError (Orchestrator misrouting), Hallucination Loops (agents without tool grounding), Conflict Deadlock (Synthesizer cannot resolve tool disagreement)

- **First 3 experiments**: 1) Ablate the Workflow: compare single vs multi-agent performance on ChestAgentBench, 2) Stress Test V-RAG: inject synthetic conflicts and measure resolution accuracy with/without V-RAG, 3) Scale Asymmetry Test: swap only the Synthesizer from 8B to 30B and measure conflict resolution vs dispatch success

## Open Questions the Paper Calls Out

- **Clinical Impact in Real-World Settings**: The authors explicitly state evaluation is restricted to offline datasets and automatic metrics; broader human studies and prospective assessments are needed to understand clinical impact. This remains unresolved because current results rely on automated metrics that may not capture clinical utility.

- **Sensitivity to Tool Errors/Distribution Shifts**: The system depends on several external tools, making it sensitive to their errors or distribution shifts. This is unresolved because the system's robustness to systematic tool failures was not quantified.

- **Handling Fundamental Disagreements**: The 60.5% conflict resolution rate implies nearly 40% of conflicts remain unresolved. It's unclear if these stem from model capacity limitations or fundamental contradictions between visual evidence and tool outputs.

## Limitations
- Clinical validity beyond benchmark metrics remains unclear, with 51.4% GREEN score on MIMIC-CXR not providing radiologist validation
- V-RAG sensitivity to retrieval corpus quality and size, with k>3 introducing harmful noise
- Reliance on specific tool APIs creates dependencies that may not generalize across clinical environments

## Confidence

- **High Confidence**: Multi-agent workflow decomposition showing 73.6% accuracy on ChestAgentBench and targeted scaling results (4B vs 30B Synthesizer impact)
- **Medium Confidence**: V-RAG contribution to conflict resolution (improvement from 26% to 60% rates) supported by ablation but robustness untested
- **Low Confidence**: Clinical interpretability and safety of system outputs for real-world deployment, particularly synthesized reports

## Next Checks

1. **Clinical Validation Study**: Conduct reader study with board-certified radiologists comparing RadAgents-generated reports against radiologist reports on held-out MIMIC-CXR subset, measuring inter-rater agreement and clinical acceptability

2. **Rare Pathology Stress Test**: Evaluate system on curated dataset of rare CXR findings (pneumomediastinum, uncommon infections, congenital anomalies) to assess V-RAG's ability to retrieve relevant exemplars

3. **Tool Dependency Analysis**: Systematically replace or remove individual tools (CXAS, MAIRA-2) to quantify system's resilience to tool failures, measuring accuracy degradation and Synthesizer recovery strategies