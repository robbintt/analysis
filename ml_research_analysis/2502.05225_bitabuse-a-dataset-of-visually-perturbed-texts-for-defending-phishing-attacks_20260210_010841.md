---
ver: rpa2
title: 'BitAbuse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks'
arxiv_id: '2502.05225'
source_url: https://arxiv.org/abs/2502.05225
tags:
- characters
- dataset
- character
- sentences
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces BitAbuse, a dataset of 325,580 visually perturbed
  texts collected from real-world phishing emails, addressing the lack of authentic
  data in prior research. The dataset includes both real and synthetically perturbed
  texts, with each input sentence labeled with its ground truth restored version.
---

# BitAbuse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks

## Quick Facts
- arXiv ID: 2502.05225
- Source URL: https://arxiv.org/abs/2502.05225
- Reference count: 26
- 325,580 visually perturbed phishing text samples with ground truth restorations

## Executive Summary
BitAbuse introduces a large-scale dataset of visually perturbed texts extracted from real-world phishing emails, addressing the critical gap of authentic data in phishing defense research. The dataset contains both real and synthetically perturbed texts with labeled ground truth restorations. The study evaluates five restoration methods including Character BERT and GPT-4o mini, finding that Character BERT-based models achieve the highest accuracy (~96%) and robustness when trained on sufficient perturbed data. Analysis reveals distinct perturbation patterns and significant differences between real and synthetic data, highlighting BitAbuse's value for training reliable restoration models.

## Method Summary
The dataset was constructed by collecting phishing emails from PhishNet, extracting text content, and applying both real-world visual perturbations found in the emails and synthetically generated perturbations. Each input sentence was manually labeled with its ground truth restored version to enable supervised learning. Five restoration methods were evaluated: SimChar DB, OCR, Spell Checker, Character BERT, and GPT-4o mini. The Character BERT models demonstrated superior performance, particularly when trained on sufficient perturbed sentences, achieving approximately 96% accuracy. The dataset includes 325,580 samples and is publicly available for research purposes.

## Key Results
- Character BERT-based models achieved ~96% accuracy in text restoration, outperforming other methods
- Significant performance differences between real and synthetic data highlight dataset authenticity
- Models trained on sufficient perturbed sentences showed improved robustness to various attack patterns
- Distinct visual perturbation character patterns were identified through systematic analysis

## Why This Works (Mechanism)
The effectiveness stems from addressing the fundamental gap in phishing research: lack of authentic visually perturbed text data. By collecting real phishing emails with actual visual perturbations rather than relying solely on synthetic generation, the dataset captures the complexity and variability of real-world attacks. The ground truth labeling enables supervised learning approaches to learn restoration patterns directly from authentic examples. Character BERT's superior performance likely results from its ability to leverage contextual information across characters, making it more robust to the visual distortions common in phishing attempts.

## Foundational Learning
- Visual Perturbation Patterns: Understanding how attackers modify characters to evade detection - needed for effective restoration model design; quick check: compare VP patterns across different phishing campaigns
- Character-Level vs Word-Level Processing: Character BERT processes individual characters rather than whole words - needed for handling character substitutions and deletions; quick check: measure restoration accuracy on single-character vs multi-character perturbations
- Real vs Synthetic Data Gap: Performance differences highlight limitations of synthetic-only training - needed to understand model generalization; quick check: test models on unseen phishing sources
- Supervised vs Unsupervised Restoration: Ground truth labeling enables supervised learning - needed for accurate model evaluation; quick check: compare supervised and unsupervised restoration approaches
- Multi-Modal Perturbation Analysis: Character changes combined with semantic manipulation - needed for comprehensive phishing defense; quick check: evaluate restoration on multi-attack vector samples

## Architecture Onboarding
Component map: Phishing Emails -> Text Extraction -> Perturbation Application -> Ground Truth Labeling -> Model Training -> Restoration Evaluation

Critical path: Real phishing email collection and annotation is the most critical step, as it determines the dataset's authenticity and usefulness for training robust restoration models.

Design tradeoffs: The choice between synthetic and real perturbations involves balancing dataset size (easier with synthetic) against authenticity (requires manual collection and labeling). The study prioritized authenticity despite higher resource requirements.

Failure signatures: Models trained only on synthetic data show degraded performance on real phishing samples, indicating overfitting to synthetic perturbation patterns. Character-level models may struggle with semantic context when character changes alter word meaning.

Three first experiments:
1. Test restoration accuracy on real phishing emails not present in the training set
2. Compare Character BERT performance across different perturbation types (substitutions vs deletions)
3. Evaluate model robustness to combined character and semantic perturbations

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset relies heavily on PhishNet phishing emails, limiting generalizability to other sources
- Manual labeling process introduces potential subjectivity affecting evaluation metrics
- Focus on character-level perturbations misses more sophisticated phishing techniques like contextual obfuscation
- Performance gap between synthetic and real data suggests current models may underperform on novel attack variants

## Confidence
- Dataset contribution: High - methodology clearly described and addresses documented research gap
- Restoration model comparisons: Medium - controlled experiments but limited testing across diverse phishing scenarios
- Real-world defense effectiveness: Low to Medium - depends on factors not fully explored in study

## Next Checks
1. Test restoration models trained on BitAbuse against phishing emails from sources outside PhishNet to assess cross-domain performance
2. Evaluate restoration accuracy on phishing samples containing multi-modal perturbations (e.g., character changes combined with semantic manipulation)
3. Conduct a human evaluation study comparing BitAbuse restoration outputs against manual expert analysis to validate model reliability in practical defense scenarios