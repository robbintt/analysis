---
ver: rpa2
title: Privilege Scores
arxiv_id: '2502.01211'
source_url: https://arxiv.org/abs/2502.01211
tags:
- intercept
- privscore
- value
- global
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Privilege Scores (PS) are introduced as a quantitative measure
  to assess the level of privilege or disadvantage individuals experience due to a
  protected attribute (PA) in decision-making contexts. The method compares model
  predictions in the real world with those in a hypothetical fair world where the
  PA has no causal effect on the target.
---

# Privilege Scores

## Quick Facts
- arXiv ID: 2502.01211
- Source URL: https://arxiv.org/abs/2502.01211
- Reference count: 40
- One-line primary result: PS quantify PA-related privilege by comparing real-world predictions to counterfactual fair-world predictions, with PSCs attributing contributions to specific causal paths

## Executive Summary
Privilege Scores (PS) are introduced as a quantitative measure to assess the level of privilege or disadvantage individuals experience due to a protected attribute (PA) in decision-making contexts. The method compares model predictions in the real world with those in a hypothetical fair world where the PA has no causal effect on the target. PS are estimated by warping features to approximate the fair world and then computing the difference in predicted probabilities. The framework also introduces Privilege Score Contributions (PSCs) to attribute privilege to specific features and pathways, enabling interpretability.

Experiments on simulated and real-world data (mortgage lending, law school admissions) demonstrate that PS effectively quantify privilege and reveal systemic biases, such as racial disparities in loan approval rates and LSAT score impacts on law school outcomes. Confidence intervals are provided for both PS and PSCs to assess statistical significance. The method offers a principled approach for bias auditing and policy design in fair machine learning.

## Method Summary
PS quantify PA-related privilege by comparing real-world model predictions to counterfactual predictions in a "fair world" where the PA has no causal effect on outcomes. This requires specifying a causal DAG, warping features to remove PA-mediated effects, training dual models on real and warped data, then computing the difference in predictions. PSCs decompose total privilege into contributions from each PA-to-feature causal path using Shapley values. Bootstrap resampling provides confidence intervals for uncertainty quantification. The approach handles both individual and population-level privilege measurement.

## Key Results
- PS effectively quantify privilege in simulated data with known ground truth, showing bias increases under DAG misspecification
- Real-world applications reveal racial disparities in mortgage lending and LSAT score effects on law school outcomes
- PSCs successfully attribute privilege contributions to specific features and pathways
- Bootstrap CIs provide uncertainty quantification, with coverage varying by warping method and DAG specification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Privilege scores (PS) quantify PA-related privilege by computing the difference between real-world predictions and counterfactual fair-world predictions.
- Mechan: The PS δ = π(x) − ψ(x_F) compares an individual's predicted probability in the real world (π) against their predicted probability in a "FiND world" (ψ) where protected attributes have no causal effect on the target. This is approximated via warping: δ̂ = π̂(x) − φ̂(x̃).
- Core assumption: A valid causal DAG exists and can be specified; warping methods accurately approximate the FiND world.
- Evidence anchors:
  - [abstract] "comparing model predictions in the real world with those in a fair world in which the influence of the PA is removed"
  - [section 3, Definition 3.1] Formal definition of PS as δ = π(x) − ψ(x_F)
  - [corpus] No directly comparable mechanism in neighbors; this is a novel formulation specific to this work
- Break condition: Misspecified DAG or poor warping approximation causes biased δ̂ (simulation shows bias increases under misspecification).

### Mechanism 2
- Claim: Warping transforms real-world feature values to approximate fair-world counterfactuals by removing PA-mediated effects.
- Mechan: For features X that are descendants of the PA, warping maps x → x̃ using methods like fairadapt (quantile preservation) or residual-based warping. A model φ̂ trained on warped data approximates predictions in the FiND world.
- Core assumption: Structural equations relating PA to downstream features are known or estimable; warping preserves legitimate variance while removing PA influence.
- Evidence anchors:
  - [section 2] "all descendants x of the PAs have to be mapped or 'warped' to values x̃ that approximate their counterfactual values x_F"
  - [section 5.1] Simulation shows res-based warping is more robust to DAG misspecification than fairadapt
  - [corpus] Weak relevance—neighbor papers address fairness practices but not causal warping
- Break condition: If PA influences features through unmodeled paths, warped values may not reflect true counterfactuals.

### Mechanism 3
- Claim: Privilege Score Contributions (PSCs) decompose total privilege into additive contributions from each PA-to-feature causal path using Shapley values.
- Mechan: Each "privilege" is defined as an arrow from PA to a feature. PSCs use Shapley-style value functions v(S) = π̂(x) − π̂(x_S) where x_S has paths in S warped. Contributions sum to v(P) by the efficiency axiom (Theorem 4.1).
- Core assumption: The number of PA-related mediator paths is small enough for tractable computation; Shapley axioms (symmetry, dummy, efficiency) appropriately distribute credit.
- Evidence anchors:
  - [section 4, Eq. 4] Shapley-style PSC formula with efficiency guarantee
  - [section 4, Theorem 4.1] Proof that Σγ_j(x) = v(P)
  - [corpus] No comparable Shapley-based fairness decomposition in neighbors
- Break condition: Features descending from multiple privileges require partial warping (Appendix A.3), which current methods may not fully support.

## Foundational Learning

- Concept: Counterfactual fairness / causal fairness frameworks
  - Why needed here: PS relies on comparing factual vs. counterfactual predictions under interventions that remove PA effects. Understanding do-calculus and structural causal models is prerequisite.
  - Quick check question: Can you explain why "warping" is fundamentally different from simply excluding the PA from model inputs?

- Concept: Shapley values and cooperative game theory axioms
  - Why needed here: PSCs use Shapley decomposition to attribute privilege contributions across causal paths.
  - Quick check question: Why does the efficiency axiom guarantee that contributions sum to the total privilege?

- Concept: Bootstrap confidence intervals
  - Why needed here: The paper uses bootstrapping to provide uncertainty quantification for PS and PSC estimates.
  - Quick check question: What sources of variance does the bootstrap capture in this pipeline (warping + model training + prediction)?

## Architecture Onboarding

- Component map: DAG specification -> Warping module (fairadapt/res-based) -> Dual model training (π̂ on D, φ̂ on D̃) -> PS computation (δ̂ = π̂(x) − φ̂(x̃)) -> PSC decomposition (Shapley) -> Bootstrap CI layer
- Critical path: DAG definition → Warping method selection → Dual model training → PS estimation → PSC attribution → CI construction
- Design tradeoffs:
  - fairadapt vs. res-based warping: fairadapt preserves quantiles but is more sensitive to DAG misspecification; res-based is more robust (Table 2, scenario SM)
  - Intercept decomposition: Global (δ_g) vs. individual (δ_x̃) intercepts separate population-level shift from individual residual effects
  - Warping in real-world model (Eq. 1) vs. warped-world model (Eq. 2): Authors prefer Eq. 1 to avoid extrapolation issues
- Failure signatures:
  - Wide CIs with coverage above nominal level → low power, possibly overly conservative
  - Large individual intercept δ_x̃ → unexplained privilege not attributable to modeled mediators (suggests missing features or direct discrimination)
  - Inconsistent PSC importance across bootstrap samples → unstable attribution
- First 3 experiments:
  1. **Validate on simulated data with known ground truth**: Generate data from known SCM, verify PS bias and MSE under correct vs. misspecified DAGs (replicate Section 5.1).
  2. **Compare warping methods on held-out real data**: Train π̂ and φ̂ using both fairadapt and res-based warping; compare PS distributions and PSC importance rankings.
  3. **Subgroup audit**: Compute PS distributions for intersecting protected groups (e.g., Black women vs. white men); identify whether global intercept or specific mediators drive disparities.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can formal statistical tests be developed to assess the significance of global Privilege Score Contributions (PSC) importance?
  - Basis in paper: [explicit] The authors state, "We leave it to future work to develop appropriate tests" for making significance statements about PSC importance.
  - Why unresolved: While confidence intervals exist for individual PS and PSCs, the global aggregation of contributions lacks a formal hypothesis testing framework.
  - What evidence would resolve it: The derivation and validation of a statistical test (e.g., a permutation test) that provides p-values for feature importance metrics while controlling for false discovery rates.

- **Open Question 2**: How can causal discovery methods be integrated to reduce reliance on user-specified Directed Acyclic Graphs (DAGs)?
  - Basis in paper: [explicit] The paper notes, "A worthwhile direction for future work would also be to investigate how causal discovery methods can be added to our framework."
  - Why unresolved: The current method requires a known DAG; simulation results show that misspecifying this DAG degrades estimator performance.
  - What evidence would resolve it: A modified framework that employs causal discovery algorithms to learn the DAG structure from data and an analysis of the resulting estimator bias and variance compared to the ground-truth DAG.

- **Open Question 3**: How can the framework be extended to handle intersectionality, i.e., multiple protected attributes (PAs) simultaneously?
  - Basis in paper: [explicit] The authors note in Appendix C.2.2 that "current pre-processing methods are not yet able to account for multiple PAs. Future research should extend these to account for intersectionality."
  - Why unresolved: The experiments only consider single protected attributes (e.g., Race or Gender in isolation), limiting the ability to audit discrimination at intersections (e.g., Black Women).
  - What evidence would resolve it: A theoretical extension of the warping mechanism and PS decomposition that supports multivariate protected attributes and an empirical demonstration on datasets with intersectional labels.

## Limitations
- Critical reliance on correctly specified causal DAGs, which are rarely available in real-world applications
- Warping methods sensitive to structural assumptions, with fairadapt particularly vulnerable to DAG misspecification
- Computational complexity of dual model training and bootstrap resampling may limit scalability

## Confidence
- Core PS mechanism: Medium
- Warping implementation: Low
- PSC decomposition: Medium
- Real-world application validity: Low

## Next Checks
1. **DAG sensitivity analysis**: Systematically vary the assumed causal structure around the primary DAG specification and measure changes in PS estimates and PSC attributions. Quantify how structural uncertainty propagates to privilege measurements.

2. **Feature importance validation**: Compare PSC attributions against alternative interpretability methods (SHAP, LIME) on the same models to assess whether causal path decomposition provides meaningfully different insights than standard feature importance.

3. **Interventional validation**: Where possible, identify natural experiments or policy interventions that create quasi-random variation in PA effects, then compare observed outcome changes against PS predictions to validate the causal framework.