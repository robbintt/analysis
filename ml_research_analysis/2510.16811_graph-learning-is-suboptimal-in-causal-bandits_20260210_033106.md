---
ver: rpa2
title: Graph Learning is Suboptimal in Causal Bandits
arxiv_id: '2510.16811'
source_url: https://arxiv.org/abs/2510.16811
tags:
- regret
- causal
- algorithm
- reward
- parent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies regret minimization in causal bandits with unknown
  causal structure, demonstrating that learning the parent set of the reward node
  is suboptimal for regret minimization. Under causal sufficiency, prior work focused
  on identifying parents first and then applying standard bandit algorithms, or jointly
  learning parents while minimizing regret.
---

# Graph Learning is Suboptimal in Causal Bandits

## Quick Facts
- arXiv ID: 2510.16811
- Source URL: https://arxiv.org/abs/2510.16811
- Reference count: 40
- Primary result: Learning the parent set of the reward node is suboptimal for regret minimization in causal bandits

## Executive Summary
This paper challenges the conventional wisdom in causal bandits that identifying the reward node's parent set is necessary for optimal regret minimization. Under causal sufficiency, prior work focused on first learning parents then applying standard bandit algorithms, or jointly learning parents while minimizing regret. The authors prove these approaches are fundamentally at odds: there exist instances where high-reward actions are disjoint from informative actions for identifying parents. They establish novel regret lower bounds and propose algorithms that achieve near-optimal regret without explicit causal discovery, demonstrating up to 20× improvement over baselines.

## Method Summary
The paper proposes two algorithms for regret minimization in causal bandits with unknown causal structure. Algorithm 1 (known parent size k) samples a random subset of actions proportional to 1/α_k · ln(√T), where α_k is the fraction of optimal arms, and runs UCB on this subset. Algorithm 2 (unknown k) uses a phased approach with halving subset sizes and doubling phase lengths, constructing mixture arms that summarize exploration history. Both algorithms bypass explicit parent identification, achieving near-optimal regret rates. The key insight is that by randomly sampling the action space, one can achieve coverage guarantees without learning the causal graph structure.

## Key Results
- Proves fundamental conflict between parent identification and regret minimization: no algorithm achieving optimal regret can also identify parents with high probability
- Establishes novel regret lower bounds valid even with full knowledge of non-reward graph
- Algorithm 1 achieves near-optimal regret (up to log factors) when parent size k is known
- Algorithm 2 is Pareto optimal (up to log factors) when intervening on all variables and k is unknown
- Experiments show up to 20× reduction in regret compared to baselines across various environments

## Why This Works (Mechanism)

### Mechanism 1: The Identification-Regret Conflict
The authors construct instances where optimal actions yield no information about parent sets, while informative actions incur maximal regret. This proves regret minimization and parent identification are fundamentally conflicting objectives under worst-case analysis.

### Mechanism 2: Statistical Coverage via Random Subset Sampling
By sampling a random subset of actions proportional to 1/α_k · ln(√T), the algorithm achieves coverage guarantees: with high probability, the subset contains an optimal arm. Standard UCB can then minimize regret without knowing the causal structure.

### Mechanism 3: Adaptive Phased Exploration for Unknown Parameters
Algorithm 2 uses phased exploration with halving subset sizes and doubling phase lengths. Mixture arms summarize exploration history, allowing the algorithm to implicitly test different hypotheses about parent size k without knowing it a priori.

## Foundational Learning

- **Concept: Causal Sufficiency (No Unobserved Confounders)**
  - Why needed here: Guarantees optimal action exists within interventions on reward's direct parents
  - Quick check question: If hidden variables affected both intervention targets and reward, would intervening on identified parents still guarantee maximum expected reward? (Answer: No)

- **Concept: Combinatorial Action Space (A_m)**
  - Why needed here: Understanding the explosion of intervention space is essential to grasp why authors argue against exhaustive search or graph learning
  - Quick check question: Given n=10 variables and m=3, why is identifying 3 parents out of 10 easier than finding optimal intervention by brute force? (Context: Paper argues random subset sampling is actually better than parent identification in worst-case)

- **Concept: Upper Confidence Bound (UCB)**
  - Why needed here: Both algorithms use standard UCB on sampled subsets; mechanism relies on UCB's ability to minimize regret given arms, even if set is randomly constructed
  - Quick check question: Does UCB component in Algorithm 1 need to know causal graph to update confidence bounds? (Answer: No, only needs observed rewards)

## Architecture Onboarding

- **Component map:**
  - Environment: Causal Graph G, Reward Node Y (with unknown parents Pa_Y)
  - Agent (Algorithm 1 - Known k): Random Sampler → Action Subset A' → UCB Engine → Action Selection
  - Agent (Algorithm 2 - Unknown k): Phase Scheduler → Random Sampler → UCB Engine → Mixture Arm Constructor (summarizes history)
  - Metric: Worst-case cumulative regret R_T

- **Critical path:** Transition from Global Structure Learning (old paradigm) to Subset Sampling (proposed paradigm). System no longer flows Graph → Parents → Action, but directly Parameters (k, m) → Sampled Actions → Reward

- **Design tradeoffs:**
  - Algorithm 1 vs. Algorithm 2: Use Algorithm 1 if k is known (simpler, slightly better constants); use Algorithm 2 if k is unknown (adaptive, log-factor overhead)
  - Parent ID vs. Regret: If goal is explainability, use parent ID methods; if goal is purely performance, use proposed methods
  - Lower Bounds: Even knowing non-reward graph G does not improve worst-case regret. Spending computation on learning G is architecturally wasteful for this objective

- **Failure signatures:**
  - Linear Regret: If m < k and algorithm assumes m ≥ k, or if random subset fails to include optimal arm (probability ≈ 1/√T)
  - Stagnation: In Algorithm 2, if phase duration ΔT_i is too short relative to noise, mixture arm estimates may be too noisy to guide subsequent phases

- **First 3 experiments:**
  1. Regret vs. Time (Validation): Implement Algorithm 1 vs. RAPS on hard instance class E_0 to verify 20× performance gap
  2. Sensitivity to n: Run Algorithm 2 on random Erdős–Rényi graphs while scaling n to confirm regret scales with combinatorial bounds
  3. Adaptivity Check (Algorithm 2): Compare Algorithm 2 against Algorithm 1 (with oracle k) on instances with varying k to measure "cost of adaptation"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the precise trade-off curve between regret minimization and parent identification error be characterized, specifically by identifying achievable pairs of error rates and regret rates?
- Basis in paper: [explicit] The authors state in the Discussion that "characterizing this trade-off remains an interesting direction for future work" and suggest characterizing "achievable pairs of parent identification error rates and regret rates."
- Why unresolved: The paper proves the existence of a conflict (trade-off) in specific instances but does not provide a general characterization of the Pareto frontier between the two objectives.
- What evidence would resolve it: A theoretical framework defining the achievable region for (Regret, Identification Error) pairs across different instances and an algorithm proven to operate on this boundary.

### Open Question 2
- Question: Under what specific structural conditions are regret minimization and parent identification objectives aligned, allowing them to be optimized simultaneously?
- Basis in paper: [explicit] The Discussion suggests it would be interesting to "study the settings in which these objectives are aligned and can be optimized simultaneously."
- Why unresolved: The paper focuses on worst-case instances where objectives conflict; it does not identify the graph topologies or parameter regimes where attempting parent identification does not incur a regret penalty.
- What evidence would resolve it: Identification of specific graph classes or distributional conditions where an algorithm optimal for regret is guaranteed to identify the parent set with high probability (or vice versa).

### Open Question 3
- Question: What realistic distributional assumptions on the causal model could be imposed to improve regret rates beyond the worst-case lower bounds established in this work?
- Basis in paper: [explicit] The authors state in the Discussion that "future work should focus on distributional assumptions that could meaningfully enhance learning performance."
- Why unresolved: The current results assume no specific distributional structure (only causal sufficiency and sub-Gaussian rewards) to establish fundamental limits, leaving the potential gains from assumptions like linearity or sparsity unexplored.
- What evidence would resolve it: The derivation of improved regret upper bounds for specific distributional families (e.g., linear models) that break the general lower bounds provided in the paper.

## Limitations

- The worst-case conflict between regret minimization and parent identification is proven for artificial instances where optimal and informative actions are disjoint
- Algorithms' performance relies on subsampling combinatorial action space, which becomes computationally intractable for larger intervention sizes
- The results assume causal sufficiency (no unobserved confounders), limiting applicability to settings with hidden variables
- Theoretical guarantees are asymptotic and may not reflect practical performance on finite horizons

## Confidence

- **High Confidence:** The lower bound proofs (Theorem 4.1) demonstrating the fundamental conflict between parent identification and optimal regret minimization. These follow standard information-theoretic arguments with clear combinatorial structures.
- **Medium Confidence:** The upper bound proofs for Algorithm 1 and Algorithm 2, particularly the adaptive regret guarantees. While the main arguments are sound, the mixture arm construction and sample reuse mechanisms introduce complexity that could harbor subtle errors.
- **Medium Confidence:** The experimental results showing 20× improvement over baselines. The methodology is specified, but implementation details like the UCB variant with sample reuse and the RAPS baseline require careful validation.

## Next Checks

1. **Construct a pathological instance:** Implement the specific hard instance class from Section 8.3 where optimal and informative actions are disjoint, and verify Algorithm 1 achieves the claimed 20× performance improvement over RAPS.

2. **Test adaptivity limits:** Compare Algorithm 2 against Algorithm 1 with oracle parent size across instances with varying k values to measure the exact "cost of adaptation" and verify the Pareto optimality claim.

3. **Validate sample reuse mechanism:** Implement the EmpKnownUCB+ and EmpUnknownUCB+ variants with their sample reuse between correlated actions, ensuring the conditioning on observed variable assignments is correctly implemented.