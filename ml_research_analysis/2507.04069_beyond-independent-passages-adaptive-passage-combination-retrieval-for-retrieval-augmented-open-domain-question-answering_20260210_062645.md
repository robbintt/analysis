---
ver: rpa2
title: 'Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval
  Augmented Open-Domain Question Answering'
arxiv_id: '2507.04069'
source_url: https://arxiv.org/abs/2507.04069
tags:
- retrieval
- passage
- arxiv
- passages
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adaptive Passage Combination Retrieval (AdaPCR),
  a retrieval framework for open-domain question answering that explicitly models
  dependencies between passages to improve retrieval quality. Traditional RAG methods
  retrieve passages independently, which often leads to redundancy and insufficient
  diversity, particularly problematic for multi-hop questions and noisy corpora.
---

# Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering

## Quick Facts
- **arXiv ID:** 2507.04069
- **Source URL:** https://arxiv.org/abs/2507.04069
- **Reference count:** 17
- **Primary result:** AdaPCR improves QA performance by explicitly modeling passage dependencies, achieving 1.10-8.11 EM point gains over baselines on multi-hop tasks.

## Executive Summary
This paper introduces Adaptive Passage Combination Retrieval (AdaPCR), a retrieval framework that addresses the limitations of traditional independent passage retrieval in open-domain QA. By explicitly modeling dependencies between passages through joint combination scoring, AdaPCR reduces redundancy and captures complementary evidence needed for complex reasoning. The method uses a two-stage process where retrieved passages are combined with the query to find complementary evidence, followed by reranking trained with a predictive objective aligned with downstream answer likelihood. Crucially, the framework adaptively selects the number of retrieved passages without requiring additional stopping modules.

## Method Summary
AdaPCR performs two-stage retrieval: first retrieving top-k passages independently, then combining each with the query to retrieve complementary passages, creating passage pairs. A scoring function measures the relevance of both single passages and pairs, and the final selection is the highest-scoring candidate (single or pair). The retriever is trained using RAG-Sequence loss, which maximizes the likelihood of the answer given the retrieved passages, weighted by the retriever's confidence. Training filters out examples where the answer is absent from the top-100 BM25-retrieved passages to ensure stable gradients.

## Key Results
- AdaPCR achieves 1.10 EM points improvement on NQ, 8.11 EM points on TriviaQA, and 4.30 EM points on HotpotQA compared to IC-RALM Reranker
- RAG loss consistently outperforms alternatives like KL divergence and cross-entropy loss in experiments
- The framework shows particularly strong gains on multi-hop reasoning tasks where evidence is distributed across passages
- Theoretical analysis shows optimal retriever behavior is one-hot selection of the best passage combination

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling passage dependencies via joint combination scoring reduces redundancy and captures complementary evidence.
- **Mechanism:** The framework retrieves passage combinations as units rather than independent passages. First-stage retrieval produces candidates D1; each di is concatenated with the query x to form a new query (di ⊕ x), which retrieves complementary passages dij, creating combination set D2. The scoring function s(⟨di, dij⟩, x) = cos(Eq(di ⊕ x), Ed(dij)) explicitly measures cross-passage relevance.
- **Core assumption:** Information needed for multi-hop reasoning is distributed across passages that may individually appear less relevant.
- **Evidence anchors:**
  - [abstract]: "AdaPCR explicitly models dependencies between passages by considering passage combinations as units for retrieval and reranking."
  - [section 3.1]: "This prevents redundancy in the combination as it rarely is the most helpful combination... By considering passage as part of the query, we also improve the system's understanding of the interplay between documents."
  - [corpus]: Limited direct evidence; neighbor papers (Inter-Passage Verification, Shifting from Ranking to Set Selection) address similar themes but report mixed FMR (0.57–0.60).
- **Break condition:** If first-stage retrieval fails to surface any relevant passage, combination scoring cannot recover missing evidence.

### Mechanism 2
- **Claim:** The predictive reranking objective aligns retriever scores with downstream answer likelihood without LLM fine-tuning.
- **Mechanism:** RAG-Sequence loss computes P(y|x) ≈ Σd∈top-k Pret(d|x)PLM(y|[d;x]), where Pret(d|x) = exp(s(d,x)/γ) / Σd′ exp(s(d′,x)/γ). This weighted sum naturally prioritizes examples with higher PLM scores during optimization, providing stable gradients despite varying answer score ranges.
- **Core assumption:** Answer likelihood from the frozen LLM provides a reliable training signal for retriever optimization.
- **Evidence anchors:**
  - [abstract]: "...a reranking step trained with a predictive objective aligned with downstream answer likelihood."
  - [section 3.2]: "Optimizing by this objective, examples with large PLM(y|[d;x]) thus naturally have relatively larger weights."
  - [corpus]: No direct comparison; neighbor papers do not replicate this specific objective.
- **Break condition:** If PLM(y|[d;x]) is noisy or uncorrelated with actual answer correctness, retriever learns spurious patterns.

### Mechanism 3
- **Claim:** Adaptive passage count selection emerges from comparing single-passage vs. combination scores without a separate stopping module.
- **Mechanism:** The final selection d* = argmax s(d,x) over D = D1 ∪ D2 naturally chooses single passages when they score higher than any combination, implicitly adapting retrieval depth based on query complexity.
- **Core assumption:** Optimal passage count varies by question; single-passage sufficiency is reflected in score distribution.
- **Evidence anchors:**
  - [abstract]: "Crucially, AdaPCR adaptively selects the number of retrieved passages without additional stopping modules."
  - [section 3.1]: "D is the set of all candidate combinations, consisting of k single-passage candidates ⟨di⟩ retrieved in the first stage, and k × k passage-combination candidates ⟨di, dij⟩ reranked in the second stage."
  - [corpus]: "From Ranking to Selection" (neighbor) proposes explicit dynamic selectors; distinct approach.
- **Break condition:** If scoring model is miscalibrated, comparison may systematically favor incorrect granularity.

## Foundational Learning

- **Concept: Bi-encoder dense retrieval (DPR-style)**
  - **Why needed here:** AdaPCR builds on bi-encoder architecture (query encoder Eq, passage encoder Ed) for efficient similarity computation. Understanding embedding spaces and cosine similarity is prerequisite.
  - **Quick check question:** Can you explain how Eq(x) and Ed(d) produce comparable vectors for similarity scoring?

- **Concept: Multi-hop question answering**
  - **Why needed here:** The method targets questions requiring reasoning across multiple passages (e.g., HotpotQA). Recognizing when evidence is distributed vs. localized informs when combination retrieval helps.
  - **Quick check question:** Given "Which band is from England, Fireflight or Dirty Pretty Things?", what two facts must be retrieved?

- **Concept: Black-box LLM training signals**
  - **Why needed here:** The retriever is trained using PLM outputs without LLM gradient access. Understanding how to extract and use likelihoods from frozen models is essential.
  - **Quick check question:** How would you obtain P(y|[d;x]) from a Flan-T5 model without backpropagating through it?

## Architecture Onboarding

- **Component map:** First-stage retriever -> Combination generator -> Scoring module -> Selector -> Training loop
- **Critical path:** First-stage retrieval quality → combination candidate quality → score calibration → adaptive selection correctness. If k=5 initial candidates contain no relevant passage, the entire pipeline fails.
- **Design tradeoffs:**
  - k=5 balances efficiency vs. recall (paper's choice); higher k improves coverage but increases k² combination explosion.
  - Maximum 2 passages per combination limits complexity; paper acknowledges extension to more rounds as future work.
  - RAG loss vs. CE loss: RAG loss theoretically converges to same optimum but empirically outperforms (Table 3) in limited-budget training.
- **Failure signatures:**
  - **Redundant retrieval:** If combinations score higher than singles but provide overlapping info, check Eq(di ⊕ x) encoding—may not sufficiently shift query representation.
  - **Training divergence:** If loss oscillates, verify positive sampling filters answers absent from top-100 (Section 3.3).
  - **Over-retrieval on simple questions:** If two-passage combinations always selected even for single-hop queries, recalibrate temperature γ in softmax.
- **First 3 experiments:**
  1. **Reproduce Table 1 baseline comparison** on HotpotQA subset (1,000 samples) using DPR checkpoint; verify ~0.6 EM improvement over IC-RALM Reranker.
  2. **Ablate combination depth:** Compare k=3 vs. k=5 vs. k=10 first-stage retrieval; measure recall@100 and final EM to validate efficiency-performance tradeoff.
  3. **Positive sampling validation:** Train with and without answer-in-top-100 filtering on NQ; plot training loss curves to confirm stability improvement (referencing Section 5.2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does extending the AdaPCR iterative retrieval process beyond two rounds yield performance gains for complex, multi-hop reasoning tasks?
- Basis in paper: [explicit] The authors state in the Limitations section that "this iterative process could be extended to more rounds to explore even more complex passage combinations."
- Why unresolved: The current implementation limits passage combinations to a maximum of two passages (one round of combination reranking), leaving the potential for longer reasoning chains untested.
- What evidence would resolve it: Experiments running the iterative reranking loop for 3+ rounds on multi-hop datasets (e.g., HotpotQA), measuring performance gains against increased latency.

### Open Question 2
- Question: Do retrievers trained with RAG loss empirically converge to the same optimal behavior as those trained with Cross-Entropy (CE) loss?
- Basis in paper: [explicit] Section 5.3 notes that while theoretical analysis suggests RAG loss and CE loss should yield the same result, the authors "don’t have the budget to run the training until full convergence."
- Why unresolved: The experiments were stopped before full convergence to save computational resources, preventing an empirical validation of the theoretical equivalence derived in Appendix B.
- What evidence would resolve it: Long-run training experiments comparing the final performance and retriever distributions of models trained with RAG loss versus CE loss until convergence.

### Open Question 3
- Question: Can AdaPCR performance be improved by supplementing the retrieval optimization with strategies that make the language model explicitly robust to noisy inputs?
- Basis in paper: [explicit] In the Limitations, the authors suggest "strategies for making LM robust against noisy inputs can also be supplemented with our method."
- Why unresolved: The current work focuses on reducing noise at the source via retrieval optimization (Section 3.3) rather than improving the generator's robustness to any remaining irrelevant context.
- What evidence would resolve it: Ablation studies combining AdaPCR retrieval with noise-robust training techniques (e.g., synthetic noisy context training) to measure if the generator handles retrieval errors better.

## Limitations

- The method's performance depends heavily on the quality of the initial 100-passage BM25 filtering, creating a hard constraint where relevant passages must be present in this subset
- Theoretical analysis assumes ideal conditions (infinite capacity, perfect calibration) that don't hold in practice
- Current implementation limits to two-passage combinations, leaving potential benefits of longer reasoning chains untested
- The approach may introduce unnecessary complexity for single-hop questions where individual passages typically contain sufficient evidence

## Confidence

- **High confidence:** The core mechanism of joint passage combination scoring is technically sound and experimentally validated, with clear improvements on multi-hop tasks
- **Medium confidence:** The claimed improvements are statistically significant within reported experiments, but evaluation protocol's dependence on 100-passage BM25 filtering introduces potential selection bias
- **Low confidence:** The assertion that this approach generalizes to arbitrary hop counts beyond two is speculative, with the paper explicitly acknowledging this as future work

## Next Checks

1. **Combination depth ablation:** Systematically evaluate k=3, k=5, and k=10 first-stage retrieval on NQ and HotpotQA, measuring both recall@100 and final EM to quantify the efficiency-performance tradeoff and determine if AdaPCR's gains scale with retrieval budget.

2. **Positive sampling sensitivity:** Train identical models with and without the answer-in-top-100 filtering on NQ, plotting training loss curves and final EM to empirically validate the paper's claim that this filtering is critical for training stability.

3. **Single-hop transfer:** Apply AdaPCR to a single-hop dataset like SQuAD Open or TriviaQA (non-multi-hop subset) to test whether combination retrieval provides benefits when evidence is typically contained in individual passages, or if it introduces unnecessary complexity.