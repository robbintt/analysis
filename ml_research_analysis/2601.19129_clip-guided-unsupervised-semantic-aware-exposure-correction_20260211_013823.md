---
ver: rpa2
title: CLIP-Guided Unsupervised Semantic-Aware Exposure Correction
arxiv_id: '2601.19129'
source_url: https://arxiv.org/abs/2601.19129
tags:
- exposure
- image
- correction
- semantic
- unsupervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of unsupervised exposure correction
  in real-world images, tackling two main challenges: the lack of semantic-aware region-level
  information leading to color artifacts, and the absence of ground-truth labels requiring
  manual editing. The proposed method introduces a semantic-aware exposure correction
  network that incorporates object-level semantic priors from a pre-trained Fast Segment
  Anything Model through an adaptive fusion module, combined with multi-scale residual
  spatial Mamba blocks for exposure adjustment.'
---

# CLIP-Guided Unsupervised Semantic-Aware Exposure Correction

## Quick Facts
- arXiv ID: 2601.19129
- Source URL: https://arxiv.org/abs/2601.19129
- Reference count: 35
- Primary result: Unsupervised exposure correction using CLIP-guided pseudo-ground truth and semantic-aware fusion

## Executive Summary
This paper introduces a novel unsupervised approach to exposure correction in real-world images that addresses two key challenges: the lack of semantic-aware region-level information causing color artifacts, and the absence of ground-truth labels requiring manual editing. The method combines a semantic-aware exposure correction network with an adaptive fusion module that incorporates object-level semantic priors from a pre-trained Fast Segment Anything Model, along with multi-scale residual spatial Mamba blocks for exposure adjustment. A CLIP-guided pseudo-ground truth generator automatically identifies exposure situations and generates tailored corrections without manual labeling. Comprehensive experiments demonstrate superior performance compared to state-of-the-art unsupervised approaches on standard benchmarks.

## Method Summary
The proposed method consists of a semantic-aware exposure correction network (SIMR) that processes images through Adaptive Semantic-Aware Fusion (ASF) modules and Residual Spatial Mamba Groups (RSMG). The ASF module fuses semantic features from a frozen Fast Segment Anything Model with image features to enable region-specific corrections. The RSMG uses 2D-Selective-Scan state space models to capture global dependencies with linear complexity. To avoid manual labeling, a CLIP-guided pseudo-ground truth generator fine-tunes text prompts to classify exposure states and apply gamma correction. The network is trained with a combination of MSE, cosine similarity, and semantic-prompt consistency losses.

## Key Results
- Outperforms state-of-the-art unsupervised methods on standard benchmarks (MSEC, SICE)
- Achieves superior numerical results with PSNR/SSIM improvements
- Demonstrates visually better exposure correction, especially in mixed lighting conditions
- Shows effectiveness of semantic-aware fusion in reducing color artifacts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Region-aware semantic fusion reduces color shift artifacts common in global exposure correction
- **Mechanism:** The Adaptive Semantic-Aware Fusion (ASF) module injects spatial features from a frozen Fast Segment Anything Model (FastSAM) into the image restoration pipeline, isolating object-specific exposure characteristics
- **Core assumption:** FastSAM provides reliable boundary delineation and semantic boundaries correlate strongly with exposure boundaries
- **Evidence anchors:** [Abstract] "ignorance of object-wise regional semantic information causes the color shift artifacts" [Section 3.1] ASF module fusing semantic features with image features
- **Break condition:** If FastSAM fails to segment low-contrast or severely overexposed objects, the semantic prior becomes noise

### Mechanism 2
- **Claim:** CLIP-guided pseudo-ground truth enables unsupervised learning by replacing manual labels with semantic consistency
- **Mechanism:** The system replaces human-annotated reference images with a generator that fine-tunes CLIP text prompts to classify exposure state and apply tailored gamma correction
- **Core assumption:** "Well-exposededness" can be reliably captured by a linear combination of gamma correction and CLIP semantic alignment
- **Evidence anchors:** [Abstract] "To avoid manual labeling, we propose a pseudo-ground truth generator guided by CLIP" [Section 3.2] Details the prompt fine-tuning and gamma tuning
- **Break condition:** If CLIP prompt alignment favors high-level semantic content over low-level pixel fidelity, the pseudo-GT may contain semantic artifacts

### Mechanism 3
- **Claim:** State-space modeling (Mamba) preserves long-range spatial consistency better than local convolution
- **Mechanism:** The Residual Spatial Mamba Group (RSMG) uses 2D selective scanning to flatten image features into sequences, capturing global dependencies with linear complexity
- **Core assumption:** 2D scanning effectively captures spatially varying exposure distributions that local convolutional kernels miss
- **Evidence anchors:** [Section 3.1] "2D-SS unfolds a feature map... achieving a global receptive field with linear complexity"
- **Break condition:** If the scanning direction or sequence processing fails to handle texture-heavy details, the Mamba block might act as a smoothing filter

## Foundational Learning

- **Concept:** Vision-Language Models (CLIP)
  - **Why needed here:** To understand how the pseudo-ground truth generator works by connecting image embeddings to text prompts
  - **Quick check question:** Can you explain how maximizing similarity between an image embedding and a "well-exposed" text prompt guides the image generation process?

- **Concept:** State Space Models (Mamba/SSM)
  - **Why needed here:** To understand the core reconstruction block (RSMG) and its global context capture
  - **Quick check question:** How does the 2D-Selective-Scan mechanism differ from a standard self-attention layer in a Vision Transformer regarding computational complexity?

- **Concept:** Semantic Segmentation Priors (FastSAM)
  - **Why needed here:** To understand the ASF module and how segmentation maps guide region-specific corrections
  - **Quick check question:** Why would a segmentation map help prevent color shifts in a mixed-lighting scene (e.g., indoor/outdoor split)?

## Architecture Onboarding

- **Component map:** Frozen FastSAM -> Feature Encoder -> SIMR Blocks (ASF + RSMG) -> Output
- **Critical path:** CLIP-guided Pseudo-GT Generation is most sensitive; if pseudo-target is poor, the SIMR network has no valid signal to learn from
- **Design tradeoffs:** Unsupervised approach removes need for paired datasets but introduces risk of semantic hallucination; Mamba provides linear complexity but relies on specific scanning directions
- **Failure signatures:** Color casts if SPC loss is weighted too low; over-smoothing if RSMG scans too coarsely; pseudo-GT collapse if CLIP prompts are not fine-tuned correctly
- **First 3 experiments:**
  1. Pseudo-GT Validation: Run generator on validation set without training restorer to inspect target quality
  2. Ablate ASF: Run network with ASF module removed and compare color shift metrics against full model
  3. Mamba Scan Test: Vary input resolution and measure inference latency vs. ViT baseline to confirm linear complexity

## Open Questions the Paper Calls Out

- **Open Question 1:** Can generative models effectively perform localized inpainting to recover specific details lost in clipped regions without introducing semantic hallucinations?
  - **Basis:** Explicitly stated in conclusion for future work
  - **Why unresolved:** Current method cannot reconstruct completely lost information in clipped regions
  - **What evidence would resolve it:** Comparative study on regions with saturated pixels measuring texture recovery and semantic fidelity

- **Open Question 2:** How can the spatial Mamba architecture be extended to handle temporal consistency for dynamic video exposure correction?
  - **Basis:** Mentioned in conclusion regarding extending to dynamic video
  - **Why unresolved:** Current RSMG uses 2D scanning optimized for single images without mechanisms for temporal tracking
  - **What evidence would resolve it:** Adapting Mamba blocks to 3D scanning and evaluating on video exposure benchmarks for temporal stability

- **Open Question 3:** Is the Adaptive Semantic-Aware Fusion (ASF) module robust to segmentation failures in extremely low-light conditions?
  - **Basis:** Inferred from reliance on FastSAM in low-light conditions
  - **Why unresolved:** Paper demonstrates ASF importance but doesn't evaluate sensitivity to segmentation quality
  - **What evidence would resolve it:** Ablation study with noise applied to FastSAM outputs correlating mask IoU with final correction scores

## Limitations
- CLIP-guided pseudo-ground truth generation procedure and gamma tuning calibration are not fully specified
- Loss balancing weights (λ₁, λ₂, λ₃, β₁, β₂) are critical but unspecified
- Lack of ablation studies for Adaptive Semantic-Aware Fusion module prevents full validation of individual contributions

## Confidence
- **High confidence:** Overall framework design and qualitative improvements in mixed lighting scenes are clearly evident
- **Medium confidence:** Numerical performance claims are supported but lack ablation studies for key components
- **Low confidence:** Generalization to all real-world exposure scenarios is not fully supported by limited benchmark evaluation

## Next Checks
1. **Pseudo-GT Quality Validation:** Run CLIP-guided generator on SICE test set and visually inspect 50 random outputs for semantic faithfulness
2. **ASF Ablation Study:** Train model with and without ASF module, compare PSNR/SSIM scores focusing on mixed lighting images
3. **Mamba Complexity Benchmark:** Implement RSMG vs. standard multi-head self-attention, measure inference time and memory usage on 384x384 input