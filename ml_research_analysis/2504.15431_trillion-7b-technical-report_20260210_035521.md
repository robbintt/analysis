---
ver: rpa2
title: Trillion 7B Technical Report
arxiv_id: '2504.15431'
source_url: https://arxiv.org/abs/2504.15431
tags:
- arxiv
- https
- language
- korean
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Trillion-7B, a Korean-centric multilingual
  language model that achieves exceptional token efficiency through a novel Cross-lingual
  Document Attention (XLDA) mechanism. XLDA enables effective knowledge transfer from
  English to target languages like Korean and Japanese by allowing cross-lingual attention
  across strategically packed document sequences, rather than blocking attention at
  language boundaries.
---

# Trillion 7B Technical Report

## Quick Facts
- arXiv ID: 2504.15431
- Source URL: https://arxiv.org/abs/2504.15431
- Authors: Sungjun Han; Juyoung Suk; Suyeong An; Hyungguk Kim; Kyuseok Kim; Wonsuk Yang; Seungtaek Choi; Jamin Shin
- Reference count: 28
- This paper introduces Trillion-7B, a Korean-centric multilingual language model that achieves exceptional token efficiency through a novel Cross-lingual Document Attention (XLDA) mechanism.

## Executive Summary
This paper introduces Trillion-7B, a Korean-centric multilingual language model that achieves exceptional token efficiency through a novel Cross-lingual Document Attention (XLDA) mechanism. XLDA enables effective knowledge transfer from English to target languages like Korean and Japanese by allowing cross-lingual attention across strategically packed document sequences, rather than blocking attention at language boundaries. Combined with optimized data mixtures, language-specific filtering, and a tailored tokenizer, Trillion-7B achieves competitive multilingual performance while dedicating only 10% of its 2T training tokens to multilingual data. The model demonstrates strong cross-lingual consistency and effective generalization to vision tasks, requiring just 59.4K H100 GPU hours ($148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages show robust multilingual performance and exceptional cross-lingual consistency, establishing a new efficiency frontier for multilingual models.

## Method Summary
Trillion-7B uses a 32-layer Transformer decoder with 4096 hidden dimensions, trained on 2T tokens with an 8.5:1:0.5 English:Korean:Other ratio. The model employs a novel Cross-lingual Document Attention (XLDA) mechanism that allows cross-document attention across strategically packed multilingual sequences, enabling knowledge transfer from English to target languages. Training follows a two-stage WSD scheduler with 2000 warmup steps, decaying to 10% learning rate over the final 0.2T tokens. During annealing, data quality is enhanced (top 20% English, top 10% multilingual) and multilingual proportion is tripled. The model uses a byte-level BPE tokenizer with 128K vocab (24.5K Korean tokens) optimized for inference speed over scaling-law optimality. Post-training includes SFT (~800K pairs), DPO (~200K pairs), and RLVR (10K prompts).

## Key Results
- Achieves competitive multilingual performance while using only 10% of training tokens for multilingual data
- Demonstrates exceptional cross-lingual consistency across 27 benchmarks in four languages
- Requires just 59.4K H100 GPU hours ($148K) for full training
- Shows effective zero-shot cross-lingual transfer to vision tasks via Trillion-LLaVA

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual Document Attention (XLDA) enables in-context cross-lingual knowledge transfer
- Claim: Allowing tokens from different language documents to attend to each other during pretraining creates explicit cross-lingual alignment signals that improve multilingual performance with less data.
- Mechanism: Standard document packing blocks attention at language boundaries. XLDA modifies the attention mask to permit cross-document attention while maintaining causal structure within documents. This creates a training environment where the model learns "if you see Korean tokens in context, you can condition English reasoning on them."
- Core assumption: Cross-lingual correspondences naturally emerge when documents from different languages share a context window, and the model can exploit these for transfer.
- Evidence anchors:
  - [abstract] "XLDA enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese by allowing cross-lingual attention across strategically packed document sequences, rather than blocking attention at language boundaries."
  - [section 2.2] "The XLDA mask keeps full self-attention across language blocks which allows tokens from different language documents to attend to each other."
  - [corpus] Weak direct validation. Related work on cross-lingual transfer (corpus paper on parallel data effects) discusses transfer mechanisms but doesn't validate XLDA specifically.
- Break condition: If your multilingual data is too sparse or languages are too linguistically distant, cross-attention may not provide useful signal—expect noise-dominated gradients.

### Mechanism 2: English-predominant data ratio with controlled multilingual upsampling reduces interference while enabling transfer
- Claim: An 8.5:1:0.5 (English:Korean:Other) ratio lets the model develop language-agnostic representations primarily in English, with XLDA bridging to low-resource languages.
- Mechanism: High-resource language (English) provides the foundation for reasoning capabilities. Low-resource languages benefit from transfer rather than competing for capacity. During annealing, multilingual proportion is tripled to consolidate transfer.
- Core assumption: Language-agnostic representations emerge primarily from high-resource data, and negative interference between languages is mitigated by asymmetric data allocation.
- Evidence anchors:
  - [section 3.1] "This language mixture encourages the model to develop core language-agnostic representations primarily in English, contrasting with an equally-balanced language distribution that are difficult to scale and can lead to negative interference between languages."
  - [section 6.1, Table 6] Annealing with quality+composition shows improved English performance even when English data volume is reduced, suggesting cross-lingual bridging effects.
  - [corpus] No direct corpus validation for this specific ratio; general multilingual data mixing strategies are discussed in related work but not validated at this scale.
- Break condition: If the target language has fundamentally different reasoning patterns not represented in English data, transfer will be limited regardless of ratio.

### Mechanism 3: Two-stage training with annealing consolidates cross-lingual knowledge
- Claim: A warmup-stable-decay scheduler with curriculum-based data quality enhancement in the annealing phase improves final multilingual performance.
- Mechanism: High learning rate phase builds representations. Annealing phase (final 10% of training with decaying LR) uses higher-quality filtered data and increased multilingual proportion to reduce gradient noise and consolidate knowledge.
- Core assumption: Knowledge consolidation is more effective with reduced gradient noise from higher-quality data, and multilingual transfer benefits from focused exposure during this phase.
- Evidence anchors:
  - [section 3.2] "During the annealing phase, we enhance overall data quality and modify the mixture composition... tripling its volume to further encourage cross-lingual knowledge transfer."
  - [section 6.1, Table 6] Ablation shows progressive improvement: High LR baseline → Anneal → Anneal+Quality → Anneal+Quality+Composition (best results).
  - [corpus] No direct corpus validation; curriculum learning for LLMs is mentioned in related work but not specifically for multilingual annealing.
- Break condition: If annealing data is too small or filtered too aggressively, the model may overfit or lose generalization.

## Foundational Learning

- Concept: **Document Packing in Transformer Pretraining**
  - Why needed here: XLDA modifies standard document packing—understanding baseline packing (why we pack, how attention masks work) is prerequisite to understanding what XLDA changes.
  - Quick check question: Can you explain why standard document packing uses attention masks at document boundaries?

- Concept: **Cross-lingual Transfer in Multilingual Models**
  - Why needed here: The paper assumes English-to-Korean transfer is desirable and achievable. Understanding transfer vs. joint training is essential.
  - Quick check question: What's the difference between training on parallel corpora vs. training on mixed-language data with cross-attention?

- Concept: **Learning Rate Scheduling (WSD Schedulers)**
  - Why needed here: The two-stage training relies on warmup-stable-decay behavior. Understanding how LR decay affects knowledge consolidation is critical.
  - Quick check question: Why might a decaying learning rate help consolidate knowledge learned at high LR?

## Architecture Onboarding

- Component map:
  Input → Byte-level BPE Tokenizer (128K vocab: ~100K EN, ~24.5K KO, rest multilingual)
       → Transformer Decoder (32 layers, 4096 hidden, SwiGLU, RoPE θ=100K, RMSNorm)
       → XLDA Attention Masking Module (modifies causal mask for cross-lingual attention)
       → Multi-token Prediction Head (discarded post-pretraining)
       → Output

- Critical path:
  1. Implement document packing sampler that enforces ≥2 languages per sequence (Section 2.1, Eq. 1-2)
  2. Modify attention mask to allow cross-document attention (Figure 2)
  3. Validate on proxy model (1.8B params, ~100B tokens) before full scale
  4. Apply two-stage WSD scheduler with annealing data swap

- Design tradeoffs:
  - **Vocab size vs. inference speed**: Paper chose 24.5K Korean tokens (above scaling-law optimal ~13K) for 35% inference speedup vs. 11% at optimal size (Section 3.5, Figure 5)
  - **Cross-lingual attention vs. contamination risk**: XLDA permits cross-document attention; standard approach blocks it. Trade-off is potential noise vs. transfer signal.
  - **Data ratio asymmetry vs. balance**: 8.5:1:0.5 ratio favors English for language-agnostic representations but requires XLDA to transfer.

- Failure signatures:
  - Korean performance plateaus despite scaling: Check if XLDA mask is correctly implemented; verify cross-lingual pairs are actually packed together
  - English degrades when adding multilingual data: May indicate insufficient XLDA or ratio imbalance; ablate without XLDA to confirm mechanism
  - Emergence not observed in proxy model: 0.5B scale shows high variance; use 1.8B minimum for recipe validation (Section 3.3, Figure 4)

- First 3 experiments:
  1. **Ablate XLDA on proxy model**: Train 1.8B model with standard packing vs. XLDA on same 100B token subset. Measure KoBEST/HellaSwag gap.
  2. **Validate annealing data composition**: Starting from a 7B checkpoint at 1.8T tokens, compare (a) no annealing, (b) annealing with original mix, (c) annealing with quality+composition changes. Measure GMMLU across all four languages.
  3. **Tokenizer vocab sweep**: Train small models with varying Korean vocab sizes (1.5K, 5K, 13K, 24.5K) to confirm scaling-law predictions vs. inference speed tradeoff before committing to full tokenizer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the token efficiency and cross-lingual transfer capabilities of the Cross-lingual Document Attention (XLDA) mechanism be preserved when scaling to 70B dense and 400B Mixture-of-Experts (MoE) architectures?
- Basis in paper: [explicit] Section 8 states the intention to "scale up our methodology to develop a family of multilingual data-efficient models ranging from 70B-scale dense architectures to 400B-scale Mixture-of-Experts models."
- Why unresolved: The current technical report validates the XLDA mechanism and training recipe only on a 7B parameter model; scaling laws for data efficiency in multilingual contexts may behave non-linearly at larger scales.
- What evidence would resolve it: Training results and ablation studies of Trillion models at 70B and 400B scales showing comparable multilingual performance per token ratios.

### Open Question 2
- Question: Does the observed zero-shot cross-lingual transfer to vision tasks (Trillion-LLaVA) generalize to other modalities, such as audio, or to languages outside the primary training set?
- Basis in paper: [explicit] Section 7.2 notes that the vision results "rais[e] questions about similar transfer effects for other languages and modalities."
- Why unresolved: The paper demonstrates transfer from English vision-language data to Korean visual reasoning, but does not test other modalities or low-resource languages included in the text pre-training.
- What evidence would resolve it: Experiments adapting the Trillion backbone to audio encoders or evaluating the VLM on a wider variety of languages not explicitly tuned during post-training.

### Open Question 3
- Question: Can mathematical and coding proficiencies be improved in future iterations without compromising the multilingual knowledge transfer efficiency achieved by the current data mixture?
- Basis in paper: [explicit] Section 8 identifies "sub-optimal performance on technical tasks" as a limitation due to dedicating less than 2% of data to math and code.
- Why unresolved: It is unclear if increasing the ratio of synthetic/math data would dilute the cross-lingual attention density required for the model's Korean-centric performance.
- What evidence would resolve it: A training ablation run with a higher percentage of math/code tokens (e.g., moving from <2% to 10-20%) evaluated on both GSM8K/HumanEval and KoBEST.

## Limitations

- Data composition opacity: The exact corpus composition beyond the stated 8.5:1:0.5 ratio and "FineWeb + in-house Korean data" is not disclosed.
- XLDA hyperparameters unspecified: Critical numerical parameters (mixing probability ρ, temperature α, upsampling factors βᵢ) are not provided.
- Small-scale validation reliability: 0.5B parameter models show high variance and unreliable emergence signals.
- Post-training methodology gap: The transition from pretraining to Tülu 3-style post-training is mentioned but not detailed.

## Confidence

**High confidence**: The core XLDA mechanism (modified attention mask allowing cross-lingual attention) is clearly specified and experimentally validated through ablation studies showing performance degradation when removed.

**Medium confidence**: The 8.5:1:0.5 data ratio and annealing strategy are well-documented, but the exact quality filtering thresholds and their impact on downstream performance remain partially opaque.

**Low confidence**: Cross-lingual consistency claims (particularly K(F)→E(T) and E(F)→K(T) results) are less robustly validated, with fewer ablation studies demonstrating the mechanism's contribution to these specific metrics.

## Next Checks

1. **XLDA ablation on 1.8B proxy**: Train three 1.8B models on identical 100B token subsets: (a) standard packing, (b) XLDA with default parameters, (c) XLDA with ablated cross-attention mask. Measure KoBEST/HellaSwag performance gap to isolate XLDA contribution.

2. **Data ratio sensitivity analysis**: Train 3B models with varying English:Korean:Other ratios (10:1:0.5, 8.5:1:0.5, 7:1:1) using identical XLDA implementation. Quantify trade-offs between target language performance and overall multilingual generalization.

3. **Tokenizer vocab size scaling validation**: Train 1B models with Korean vocab sizes of 1.5K, 5K, 13K (scaling-law optimal), and 24.5K. Measure both downstream task performance and actual inference latency to validate the claimed 35% speedup at 24.5K vs 11% at optimal size.