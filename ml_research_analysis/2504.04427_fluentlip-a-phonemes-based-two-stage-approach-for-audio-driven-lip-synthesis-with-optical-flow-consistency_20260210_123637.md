---
ver: rpa2
title: 'FluentLip: A Phonemes-Based Two-stage Approach for Audio-Driven Lip Synthesis
  with Optical Flow Consistency'
arxiv_id: '2504.04427'
source_url: https://arxiv.org/abs/2504.04427
tags:
- fluentlip
- video
- phoneme
- loss
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating fluent, lip-synchronized
  talking face videos from audio input. The proposed FluentLip approach improves upon
  existing methods by integrating phoneme-based multimodal learning, optical flow
  consistency loss, and a diffusion model to stabilize GAN training.
---

# FluentLip: A Phonemes-Based Two-stage Approach for Audio-Driven Lip Synthesis with Optical Flow Consistency

## Quick Facts
- **arXiv ID**: 2504.04427
- **Source URL**: https://arxiv.org/abs/2504.04427
- **Reference count**: 8
- **Primary result**: 16.3% improvement in FID and 35.2% improvement in Phoneme Error Rate over state-of-the-art baselines

## Executive Summary
FluentLip addresses the challenge of generating fluent, lip-synchronized talking face videos from audio input. The proposed approach improves upon existing methods by integrating phoneme-based multimodal learning, optical flow consistency loss, and a diffusion model to stabilize GAN training. Evaluated on LRS2 and GRID datasets, FluentLip achieves state-of-the-art results with significant improvements in both visual quality and lip pose intelligibility.

## Method Summary
FluentLip uses a two-stage approach for audio-driven lip synthesis. Stage 1 employs a phoneme encoder (Transformer) and audio encoder (CNN) to fuse phoneme and audio embeddings, training a lip sync discriminator. Stage 2 implements a Seq2Seq generator with phoneme encoder guided by the fixed Stage 1 discriminator and a visual GAN discriminator with adaptive diffusion chain. The method incorporates RAFT-based optical flow consistency loss to ensure natural transitions between frames. Training uses LRS2 and GRID datasets with specific loss formulations and batch sizes of 40 across both stages.

## Key Results
- 16.3% improvement in FID score compared to state-of-the-art baselines
- 35.2% improvement in proposed Phoneme Error Rate (PER) metric
- Superior performance on both LSE-D/LSE-C (sync), FID (quality), and SSIM (realism) metrics

## Why This Works (Mechanism)

### Mechanism 1: Phoneme-Guided Multimodal Synchronization
Fusing phoneme embeddings with audio embeddings provides a more stable content reference for lip synchronization than audio alone. Audio features vary significantly by speaker, whereas phonemes represent language-agnostic speech content. By concatenating phoneme embeddings (via a Transformer) with audio embeddings, the lip sync discriminator receives a "grounded" representation of speech that is less susceptible to speaker-specific acoustic variations.

### Mechanism 2: Optical Flow Consistency for Temporal Coherence
Enforcing consistency between the optical flow of synthesized frames and ground truth frames eliminates unnatural jitter and improves video fluency. By using a pre-trained RAFT model to estimate optical flow and penalizing deviations, the generator is forced to mimic the natural motion dynamics of real video rather than just generating a sequence of static "good" images.

### Mechanism 3: Diffusion-Augmented GAN Training
Injecting a variable-length diffusion chain between the generator and discriminator prevents mode collapse and stabilizes training. Standard GANs suffer from the discriminator overpowering the generator. By noise-augmenting the generated images via a diffusion chain before they reach the discriminator, the task is regularized, smoothing the learning landscape.

## Foundational Learning

- **Concept: Optical Flow Estimation (RAFT)**
  - **Why needed here**: To understand how the $L_{cons}$ loss works. You must grasp that RAFT predicts a vector field representing pixel movement from frame $t$ to $t+1$. If the synthesized video has "jitter," the vectors will point in random directions, which $L_{cons}$ penalizes.
  - **Quick check question**: If the subject in a video moves 10 pixels right between frames, what would the optical flow vector field roughly look like in that region?

- **Concept: Phoneme Alignment (Montreal Forced Aligner - MFA)**
  - **Why needed here**: The model relies on hard alignments between audio and phoneme sequences. You need to know that this is a preprocessing step that generates the duration vectors ($L_{raw}$) required by the Phoneme Encoder.
  - **Quick check question**: If the audio says "Hello" but the aligner outputs a duration vector that is 0.5 seconds too short for the "H" sound, how would this affect the fused embedding?

- **Concept: Mode Collapse in GANs**
  - **Why needed here**: To justify the complexity of the adaptive diffusion model. You must recognize the symptom—the generator produces the same output regardless of input—to understand why the ablation study shows the "w/o diff" model failing visually.
  - **Quick check question**: Why does a powerful discriminator often lead to a generator that outputs only a single type of image?

## Architecture Onboarding

- **Component map**: Audio Mel + Phonemes (MFA) -> Phoneme Encoder (Transformer) + Audio Encoder (CNN) -> Fused Embedding -> Stage 1 Lip Sync Discriminator -> Fixed Discriminator -> Stage 2 Generator (Seq2Seq) + Visual Discriminator (GAN) with Adaptive Diffusion Chain -> Optical Flow Consistency Loss (RAFT)

- **Critical path**: The reliance on MFA accuracy is the first bottleneck. The second is the interaction between the Adaptive Diffusion Chain and the Visual Discriminator. If the diffusion noise is not adaptive, gradients will not propagate effectively to the Generator.

- **Design tradeoffs**:
  - **Fluency vs. Computational Cost**: Calculating optical flow (RAFT) for every batch adds significant overhead compared to standard GAN training.
  - **Intelligibility vs. Audio Fidelity**: Fusing phonemes forces the mouth to shape specific sounds, which might look "robotic" if the audio contains emotional prosody not captured in the text-based phoneme stream.

- **Failure signatures**:
  - **Jittery Mouths**: Indicates $L_{cons}$ weight is too low or RAFT is failing to track fast motion.
  - **Blurry/Static Mouth**: Suggests Mode Collapse; the diffusion chain may be disabled or the discriminator is overpowering the generator.
  - **Mismatched Lip-Sync**: Check MFA alignment; the phoneme duration may be drifting from the actual audio timing.

- **First 3 experiments**:
  1. **Sanity Check (Stage 1)**: Verify the Lip Sync Discriminator converges using the fused embedding. If the loss plateaus high, check phoneme padding/alignment.
  2. **Ablation (Flow Loss)**: Train with and without $L_{cons}$. Visually inspect the resulting videos for "jitter" vs. smoothness to calibrate $\lambda_{cons}$.
  3. **Diffusion Stability Test**: Monitor the "w/o diff" variant vs. the full model. Confirm that the full model does not exhibit the "downward then upward" loss trend associated with mode collapse.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the method and results, several critical questions emerge:

1. **Does the optical flow consistency loss restrict the model's ability to generate novel, audio-driven head poses that differ from the input reference video?** The paper evaluates fluency and synchronization but does not analyze the diversity of head poses or whether the model merely replicates the reference video's rigid head dynamics.

2. **How does the reliance on the Montreal Forced Aligner (MFA) for phoneme extraction impact performance in "in-the-wild" scenarios with background noise or overlapping speech?** The experiments utilize the LRS2 and GRID datasets, which may not represent extreme acoustic noise levels where phoneme extraction fails.

3. **Does the phoneme-based multimodal learning approach generalize to tonal languages where phoneme-to-viseme mappings are significantly more complex than in English?** The current architecture may not handle these nuances without specific tuning, as the paper only evaluates on English datasets.

## Limitations

- The paper lacks detailed architectural specifications (layer counts, embedding dimensions, diffusion step counts) critical for exact replication
- The specific design choices for the adaptive diffusion chain (noise schedule, step count) and the exact phoneme encoder architecture are underspecified
- The proposed Phoneme Error Rate (PER) metric is validated only against AV-HuBERT and lacks comparison to alternative measures of lip pose intelligibility

## Confidence

- **High**: The overall two-stage framework and the integration of optical flow consistency loss are well-justified by prior work and ablation results
- **Medium**: The claimed 16.3% FID improvement and 35.2% PER improvement are statistically significant within the reported experiments, but lack statistical significance testing across multiple runs
- **Low**: The specific design choices for the adaptive diffusion chain and the exact phoneme encoder architecture are underspecified

## Next Checks

1. **Phoneme Encoder Ablation**: Train the Stage 1 discriminator with and without phoneme fusion to isolate the contribution of the phoneme embeddings to lip sync accuracy.

2. **Optical Flow Loss Sensitivity**: Systematically vary λ_cons to find the optimal balance between video fluency and visual quality, documenting the point of diminishing returns.

3. **Diffusion Chain Robustness**: Train the Stage 2 model with a fixed (non-adaptive) diffusion chain to determine if the adaptive component is essential for stability or if a simpler design suffices.