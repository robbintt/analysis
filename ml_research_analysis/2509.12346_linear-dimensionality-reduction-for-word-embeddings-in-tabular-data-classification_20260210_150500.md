---
ver: rpa2
title: Linear Dimensionality Reduction for Word Embeddings in Tabular Data Classification
arxiv_id: '2509.12346'
source_url: https://arxiv.org/abs/2509.12346
tags:
- data
- word
- accuracy
- dimensionality
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates linear dimensionality reduction methods
  for word embeddings in tabular data classification, specifically addressing the
  Engineers' Salary Prediction Challenge where job descriptions are embedded as 300-dimensional
  vectors. The authors compare Principal Component Analysis (PCA), Linear Discriminant
  Analysis (LDA), and propose a new method called Partitioned-LDA.
---

# Linear Dimensionality Reduction for Word Embeddings in Tabular Data Classification

## Quick Facts
- arXiv ID: 2509.12346
- Source URL: https://arxiv.org/abs/2509.12346
- Reference count: 12
- Primary result: Linear dimensionality reduction improves word embedding performance in tabular classification with limited samples

## Executive Summary
This paper addresses the challenge of using word embeddings in tabular data classification when training samples are limited. The authors investigate linear dimensionality reduction methods for 300-dimensional word embeddings in the Engineers' Salary Prediction Challenge, finding that raw embeddings achieve 74.00% accuracy. Through systematic comparison of PCA, LDA, and a novel Partitioned-LDA approach, the study demonstrates that dimensionality reduction can significantly improve classification performance by reducing noise and improving generalization.

## Method Summary
The authors evaluate three linear dimensionality reduction approaches for word embeddings in tabular classification. Principal Component Analysis (PCA) projects embeddings onto orthogonal components that maximize variance. Regular Linear Discriminant Analysis (LDA) maximizes class separability but fails with limited samples due to covariance estimation errors. The proposed Partitioned-LDA splits 300-dimensional embeddings into equal-sized blocks and performs LDA separately on each block, addressing the covariance estimation problem while maintaining class discrimination. Shrinkage regularization is applied to improve LDA stability, and subspace dimensions are tuned to optimize performance.

## Key Results
- Raw 300-dimensional embeddings achieve 74.00% classification accuracy
- PCA with appropriate subspace dimensions improves performance over raw embeddings
- Regular LDA performs poorly without regularization due to covariance estimation errors
- Shrinkage-regularized LDA achieves significant improvements even with only 2 dimensions
- Partitioned-LDA outperforms regular LDA and reaches top-10 accuracy on the competition leaderboard when combined with shrinkage

## Why This Works (Mechanism)
Dimensionality reduction works for word embeddings in tabular classification by addressing the curse of dimensionality. With limited training samples, high-dimensional embeddings lead to overfitting and poor generalization. Linear methods like PCA and LDA project data onto lower-dimensional subspaces that capture essential information while reducing noise. Shrinkage regularization stabilizes covariance estimation in LDA, preventing numerical issues when sample sizes are small. Partitioned-LDA specifically tackles the challenge of estimating large covariance matrices by breaking the problem into smaller, more manageable pieces that can be processed independently while preserving class discrimination.

## Foundational Learning
- Curse of dimensionality: High-dimensional spaces require exponentially more samples for reliable estimation; quick check: compare sample-to-feature ratio
- Covariance estimation in high dimensions: When features >> samples, covariance matrices become singular; quick check: compute condition number of covariance matrix
- Shrinkage regularization: Regularizes covariance estimates to prevent overfitting; quick check: apply Ledoit-Wolf shrinkage estimator
- Linear discriminant analysis: Maximizes between-class variance while minimizing within-class variance; quick check: visualize class separation in reduced space
- Principal component analysis: Projects data onto orthogonal components that maximize variance; quick check: examine explained variance ratio curve
- Block partitioning strategies: Dividing high-dimensional problems into smaller subproblems; quick check: test different block sizes systematically

## Architecture Onboarding

Component map: Embeddings -> Dimensionality Reduction -> Classification -> Evaluation

Critical path: Word embeddings undergo dimensionality reduction (PCA/LDA/Partitioned-LDA), then feed into the classifier, with performance evaluated on test data. The key innovation is in the reduction step, particularly Partitioned-LDA's block-wise processing.

Design tradeoffs: The main tradeoff is between dimensionality reduction (which improves generalization but may lose information) and using raw embeddings (which preserve all information but risk overfitting). Partitioned-LDA trades computational complexity for improved covariance estimation stability.

Failure signatures: Poor performance indicates either insufficient subspace dimension (underfitting) or excessive reduction (information loss). Regular LDA without shrinkage fails due to singular covariance matrices. Partitioned-LDA may fail if block sizes are poorly chosen, leading to loss of discriminative information.

First experiments:
1. Test different subspace dimensions for PCA to find the optimal trade-off between dimensionality reduction and information preservation
2. Apply shrinkage regularization to standard LDA across varying regularization strengths
3. Systematically vary block sizes in Partitioned-LDA to determine sensitivity to partitioning choices

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the immediate experimental scope.

## Limitations
- Evaluation based on a single dataset with specific characteristics (300-dimensional embeddings, limited training samples)
- Proposed Partitioned-LDA lacks theoretical justification for equal-sized block partitioning
- Computational complexity of Partitioned-LDA compared to standard methods is not discussed
- Sensitivity to block size choices is not thoroughly explored

## Confidence
- Raw embeddings achieving 74.00% accuracy: High confidence
- Shrinkage regularization improving LDA performance: Medium confidence
- Partitioned-LDA outperforming regular LDA: Medium confidence
- Dimensionality reduction enhancing word embedding performance: Medium confidence

## Next Checks
1. Test proposed methods (PCA, regularized LDA, Partitioned-LDA) on additional tabular datasets with word embeddings to assess generalizability across different domains and embedding dimensions
2. Conduct ablation study varying block sizes in Partitioned-LDA to determine optimal partitioning strategies and assess sensitivity to this hyperparameter
3. Evaluate computational efficiency of Partitioned-LDA compared to standard dimensionality reduction methods to establish practical scalability for larger datasets