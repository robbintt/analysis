---
ver: rpa2
title: 'Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?'
arxiv_id: '2601.22329'
source_url: https://arxiv.org/abs/2601.22329
tags:
- neutral
- sadness
- steering
- uni00000156
- option
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks LLM decision-making on rationality axioms
  and human-like affective biases, using four axiom tests and tasks from behavioral
  economics, judgment, and moral evaluation. The core idea is to compare how reasoning-enabled
  LLMs behave under neutral conditions and under two steering methods: in-context
  priming (ICP) and representation-level steering (RLS).'
---

# Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?

## Quick Facts
- **arXiv ID:** 2601.22329
- **Source URL:** https://arxiv.org/abs/2601.22329
- **Reference count:** 40
- **Primary result:** Reasoning-enabled LLMs improve compliance with rational-choice axioms but show gaps in human-like biases; ICP and RLS induce human-aligned behavioral shifts with different reliability and calibration profiles.

## Executive Summary
This study benchmarks LLM decision-making against rational-choice axioms and human-like affective biases. By enabling "thinking" in LLMs, the authors observe substantial improvements in adherence to axioms like independence and stochastic transitivity. Two steering methods—in-context priming (ICP) and representation-level steering (RLS)—both induce human-aligned behavioral shifts, but ICP tends to produce exaggerated, hard-to-calibrate effects, while RLS yields more graded, psychologically plausible changes with higher reliability. These findings highlight a tension between aligning models for human simulation versus ensuring stable, unbiased decision-making, with implications for safe LLM deployment.

## Method Summary
The study tests four axiom tests and tasks from behavioral economics, judgment, and moral evaluation across neutral conditions and under two steering methods: in-context priming (ICP) and representation-level steering (RLS). The goal is to compare how reasoning-enabled LLMs behave and whether they align with human judgment and choice patterns.

## Key Results
- Enabling "thinking" in LLMs substantially improves compliance with rational-choice axioms and pushes models toward expected-value maximization.
- ICP and RLS both induce human-aligned behavioral shifts, but ICP produces exaggerated, hard-to-calibrate effects while RLS yields more graded, psychologically plausible changes.
- Some systematic gaps remain, such as a reversed endowment effect and strong ambiguity aversion, even with reasoning enabled.

## Why This Works (Mechanism)
The observed improvements stem from the interplay between model reasoning capabilities and steering interventions. Reasoning enables models to better process and integrate multiple factors in decision-making, improving axiom compliance. Steering methods then shift behavior toward human-like patterns, but differ in reliability and calibration: ICP's context-based approach can produce strong but unstable effects, while RLS's representation-level adjustments yield more consistent but subtler shifts. This mechanism explains both the gains in rationality and the remaining gaps in human-like biases.

## Foundational Learning
- **Rational-choice axioms (e.g., independence, stochastic transitivity)**: Fundamental criteria for consistent decision-making; needed to evaluate LLM rationality.
- **Affective biases (e.g., endowment effect, ambiguity aversion)**: Key patterns in human judgment; needed to assess human-like alignment.
- **In-context priming (ICP)**: Steering via prompt design; needed to induce behavioral shifts without retraining.
- **Representation-level steering (RLS)**: Steering by modifying model representations; needed for more stable, graded effects.
- **Expected-value maximization**: Standard for rational choice; needed to measure how closely models approximate rational agents.
- **Behavioral economics tasks**: Real-world decision scenarios; needed to test practical decision-making.

## Architecture Onboarding
**Component Map:** LLMs (reasoning enabled/disabled) -> Axiom Tests & Behavioral Tasks -> Steering Methods (ICP, RLS) -> Human Judgment Alignment

**Critical Path:** Reasoning capability -> Axiom compliance -> Steering intervention -> Behavioral shift -> Alignment assessment

**Design Tradeoffs:** ICP offers strong but unstable effects; RLS provides more reliable but subtler shifts; reasoning improves rationality but not all human biases.

**Failure Signatures:** Exaggerated effects (ICP), inconsistent reliability (both methods), persistent gaps in human-like biases (e.g., reversed endowment effect).

**3 First Experiments:**
1. Compare human behavioral data to LLM responses on the same tasks to establish true alignment baselines.
2. Test steering effects across diverse cultural and linguistic contexts for robustness.
3. Evaluate model decision-making stability under repeated trials and varying prompt structures.

## Open Questions the Paper Calls Out
None provided.

## Limitations
- The study lacks direct human baseline data for behavioral tasks, limiting interpretability of alignment.
- Reliability of RLS is higher than ICP but still variable and task-dependent.
- Model-specific training biases may influence observed affective patterns (e.g., ambiguity aversion).

## Confidence
- **High:** Robust improvements in axiom compliance with reasoning; divergence between ICP and RLS is well-supported.
- **Medium:** RLS yields "psychologically plausible" changes, but lacks rigorous human-subject comparison.
- **Low:** Claims about "reversed endowment effect" require caution due to potential task framing artifacts.

## Next Checks
1. Collect and compare human behavioral data for the same tasks to establish true alignment baselines.
2. Test the robustness of steering effects across diverse cultural and linguistic contexts.
3. Evaluate model decision-making stability under repeated trials and varying prompt structures to quantify reliability.