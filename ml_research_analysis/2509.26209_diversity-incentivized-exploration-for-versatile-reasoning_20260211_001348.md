---
ver: rpa2
title: Diversity-Incentivized Exploration for Versatile Reasoning
arxiv_id: '2509.26209'
source_url: https://arxiv.org/abs/2509.26209
tags:
- diversity
- exploration
- reward
- reasoning
- diver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses deficient exploration and poor sample efficiency
  in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models
  (LLMs) reasoning tasks, where vast state-action spaces and reward sparsity hinder
  effective learning. The authors propose DIVER, a framework that incentivizes global
  sequence-level diversity as an intrinsic reward to promote deep exploration in semantically
  structured spaces.
---

# Diversity-Incentivized Exploration for Versatile Reasoning

## Quick Facts
- arXiv ID: 2509.26209
- Source URL: https://arxiv.org/abs/2509.26209
- Reference count: 40
- Qwen2.5-Math-7B achieves 42.3–43.0 on six math benchmarks, outperforming OpenReasoner-Zero by +2.0 and Entropy-RL by +1.2 points on in-domain tasks, with +6.8 points on out-of-domain tasks

## Executive Summary
This paper addresses exploration deficiency in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models (LLMs) reasoning tasks. The authors propose DIVER, a framework that incentivizes global sequence-level diversity as an intrinsic reward to promote deep exploration in semantically structured spaces. By designing two diversity metrics (Textual and Equational) and employing potential-based reward shaping with conditional heuristics, DIVER achieves significant improvements on mathematical reasoning benchmarks while maintaining theoretical guarantees of optimal policy invariance.

## Method Summary
DIVER builds on GRPO and introduces global sequence-level diversity as an intrinsic reward. The framework computes diversity metrics across group responses - Textual Diversity using BLEU dissimilarity and Equational Diversity measuring formula variation. A potential-based reward shaping function ensures theoretical policy invariance while conditional shaping restricts diversity rewards to correct responses only. The augmented reward combines original verification rewards with diversity bonuses, optimized through standard GRPO procedures with careful hyperparameter tuning.

## Key Results
- Qwen2.5-Math-7B achieves 42.3–43.0 average scores on six math benchmarks
- Outperforms OpenReasoner-Zero by +2.0 points and Entropy-RL by +1.2 points on in-domain tasks
- Shows larger gains of +6.8 points on out-of-domain tasks (ARC-c, GPQA*, MMLU-Pro)
- Pass@k evaluations demonstrate superior exploration with +6.7 points improvement on AIME25 at Pass@32

## Why This Works (Mechanism)

### Mechanism 1: Potential-Based Reward Shaping for Policy Invariance
Incorporating global diversity as an intrinsic reward does not alter the optimal policy of the original reasoning task, ensuring the model still prioritizes correctness. The framework formulates the intrinsic reward $R^{int}$ as the difference of a potential function $\Phi$ over adjacent states: $R^{int}(s,a,s') = \gamma \Phi(s') - \Phi(s)$. This structure cancels out in the cumulative return calculation, theoretically guaranteeing that the optimal policy for the transformed MDP remains optimal for the original MDP.

### Mechanism 2: Conditional Shaping for Reward Hacking Mitigation
Restricting diversity rewards to only correct responses prevents the model from exploiting easy-to-generate but incorrect diverse gibberish. The intrinsic reward is multiplied by an indicator function $\mathbb{I}(r_i)$ which is 1 only if the binary verifiable reward $r_i$ is 1 (correct). This ensures the model cannot maximize reward solely by producing long, distinct, but wrong answers, aligning the exploration incentive strictly with the solution space.

### Mechanism 3: Global Sequence-Level Diversity for Deep Exploration
Optimizing for diversity across entire response sequences enables "deep exploration" of reasoning pathways more effectively than local token-level entropy adjustments. Instead of maximizing uncertainty at the token level, DIVER calculates diversity metrics across a group of $G$ full rollouts. This semantic-level signal encourages the policy to generate distinct reasoning strategies (e.g., different formulas) rather than just varying vocabulary, leading to a broader search of the state space.

## Foundational Learning

- **Markov Decision Processes (MDPs) & Reward Shaping**: The paper frames LLM reasoning as an MDP and relies on "potential-based reward shaping" to add a diversity bonus without changing the optimal policy. Understanding the Bellman equation and the theoretical constraints of reward shaping is necessary to grasp why DIVER can add rewards safely. Quick check: Why does a potential-based reward $F = \gamma \Phi(s') - \Phi(s)$ guarantee that the optimal policy of the modified MDP is the same as the original?

- **BLEU Score & N-gram Overlap**: One of the primary diversity metrics (Textual Diversity) relies on BLEU. Understanding that BLEU measures n-gram precision (overlap) helps explain why inverting it (1 - BLEU) measures dissimilarity and how this might miss semantic nuances. Quick check: Does a high BLEU score between two reasoning paths always mean they use the same logical steps?

- **GRPO (Group Relative Policy Optimization)**: DIVER is built on top of GRPO. Understanding that GRPO estimates advantages based on group rewards (and discards a critic model) is essential for implementing the modified advantage calculation in Equation 2. Quick check: In GRPO, how is the advantage $A_i$ calculated for a specific response $o_i$ relative to the group?

## Architecture Onboarding

- **Component map**: Rollout Generator (LLM Policy) -> Diversity Calculator -> Verifier -> Reward Shaper -> Policy Optimizer (GRPO)
- **Critical path**: The non-obvious step is the Reward Shaping calculation (Eq. 8). The intrinsic reward for a sequence is not just the raw diversity score, but $\gamma^T d([q, o_i])$. You must ensure the discount factor $\gamma$ is applied correctly over the sequence length $T$ to maintain the theoretical invariance property.

- **Design tradeoffs**:
  - Textual vs. Equational Diversity: Textual (BLEU) is cheap and general but surface-level. Equational is deeper for math but requires robust formula extraction (brittle if output format drifts)
  - Conditional vs. Unconditional Shaping: Conditional (only reward correct responses) is safer but may reduce exploration signals on very hard problems where the model rarely gets correct answers initially

- **Failure signatures**:
  - Length Explosion: If the reward shaper is misconfigured (e.g., missing the upper bound clip $\sigma$ or the conditional mask), the model will generate infinitely long responses to maximize distinctiveness
  - Semantic Drift: If using Textual Diversity, the model may learn to use synonyms to "game" the BLEU score without changing the reasoning logic

- **First 3 experiments**:
  1. Diversity Correlation Check: Replicate the "High vs. Low Diversity" filtering experiment. Train two small models: one on high-diversity batches and one on low-diversity batches. Verify that the high-diversity model has a lower "solve none" rate on validation sets
  2. Reward Hacking Ablation: Implement the shaping mechanism without the conditional mask (reward all responses for diversity). Plot response length over training steps. You should see a rapid increase in length and a drop in accuracy, confirming the need for the conditional mask
  3. Pass@k Scaling: Evaluate the trained model on Pass@1 vs. Pass@32. A successful DIVER implementation should show significant gains at higher $k$ values (indicating better exploration coverage) compared to a standard GRPO baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees under practical conditions remain uncertain as sequence-level diversity metrics may not satisfy the theoretical assumptions of policy invariance
- Reward hacking vulnerability through semantic similarity manipulation, where BLEU-based diversity can be gamed by paraphrasing while maintaining identical reasoning paths
- Out-of-distribution generalization effectiveness is unknown for larger models or domains far removed from mathematical reasoning where formula extraction is not applicable

## Confidence

**High Confidence**: Empirical improvements on in-domain benchmarks (MATH-500, AIME24/25, AMC) are well-supported by controlled experiments comparing DIVER against OpenReasoner-Zero and Entropy-RL baselines

**Medium Confidence**: Theoretical framework for potential-based reward shaping is sound, but its application to sequence-level diversity metrics introduces practical uncertainties about the effectiveness of diversity metrics as exploration signals

**Low Confidence**: Claims about deep exploration capability versus local token-level methods are largely conceptual, supported primarily by illustrative diagrams rather than rigorous empirical comparison with direct token-level entropy maximization approaches

## Next Checks
1. **Diversity metric sensitivity analysis**: Systematically evaluate DIVER's performance using only TD, only ED, and hybrid approaches across different mathematical domains to reveal whether formula-based diversity provides unique benefits beyond textual diversity

2. **Reward hacking stress test**: Design adversarial prompts that can be solved through semantically equivalent but textually diverse reasoning paths. Train models with and without conditional shaping on these prompts and measure whether diversity rewards correlate with genuine reasoning diversity or merely surface-level variation

3. **Transfer learning evaluation**: Fine-tune a larger model (e.g., Qwen2.5-Math-72B) using DIVER on mathematical reasoning, then evaluate on non-mathematical reasoning tasks requiring logical inference or common-sense reasoning to test whether diversity incentives learned in one domain transfer to others