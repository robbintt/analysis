---
ver: rpa2
title: Stress Testing Factual Consistency Metrics for Long-Document Summarization
arxiv_id: '2511.07689'
source_url: https://arxiv.org/abs/2511.07689
tags:
- metrics
- factuality
- association
- metric
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of reference-free factuality
  metrics for long-document summarization. Existing metrics, designed for short texts,
  struggle with long inputs due to token limits, dispersed information, and retrieval
  challenges.
---

# Stress Testing Factual Consistency Metrics for Long-Document Summarization

## Quick Facts
- arXiv ID: 2511.07689
- Source URL: https://arxiv.org/abs/2511.07689
- Reference count: 20
- Reference-free metrics struggle with long-document summarization due to token limits and dispersed information

## Executive Summary
This paper investigates the robustness of reference-free factuality metrics for long-document summarization. Existing metrics, designed for short texts, struggle with long inputs due to token limits, dispersed information, and retrieval challenges. To probe their behavior, the authors apply seven meaning-preserving perturbations—paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion—to summaries from three long-document datasets spanning fiction, legal, and scientific domains. They evaluate six widely-used metrics (BARTScore, SummaC-Conv, SummaC-ZS, AlignScore, MiniCheck, UniEval) using a retrieval-based scoring framework that matches summary sentences to relevant source snippets. Results show that most metrics are sensitive to even minor perturbations, with NLI-based metrics (SummaC) moderately stable and generation-based (BARTScore) highly unstable. Expanding retrieval context generally improves scores, but no metric consistently maintains factual alignment under long-context conditions. Performance also declines for information-dense claims overlapping broadly with the source, suggesting difficulty with compressed or globally entangled content. The findings point to a need for metrics with multi-span reasoning, context-aware calibration, and robustness to meaning-preserving edits.

## Method Summary
The authors stress-test six reference-free factuality metrics using seven meaning-preserving perturbations (paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, source text insertion) on summaries from three long-document datasets (SQuALITY, LexAbSumm, ScholarQABench). They employ a retrieval-based scoring framework where each summary sentence is matched to relevant source snippets via cosine similarity, with context windows of size w∈{0,1,2}. Metrics are evaluated by computing maximum scores across retrieved snippets and averaging across sentences. Information density is measured as mean pairwise cosine similarity between summary and source sentence embeddings. GPT-4o generates perturbations with temperature=0 and top_p=0.5.

## Key Results
- Most metrics show high sensitivity to surface-level perturbations, with BARTScore and AlignScore particularly unstable
- NLI-based metrics (SummaC-Conv, SummaC-ZS) demonstrate moderate robustness to perturbations
- Expanding retrieval context windows improves metric performance, especially for complex inputs
- Metrics struggle with information-dense claims that require multi-span reasoning across distributed evidence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval-based scoring with expanded context windows improves factuality evaluation for long documents by grounding each summary sentence in relevant source evidence.
- **Mechanism:** For each summary sentence s_j, compute embeddings and retrieve top-K most similar source sentences via cosine similarity. Each retrieved sentence is expanded to a symmetric context window of size w (Eq. 1: d(w)_j,k = {d_j,k-w, ..., d_j,k, ..., d_j,k+w}). The factuality score is the maximum metric score across all K context snippets (Eq. 2), then averaged across sentences.
- **Core assumption:** Factual evidence for a summary sentence is semantically localized around similar source sentences and can be captured within fixed-size context windows.
- **Evidence anchors:**
  - [abstract] "Expanding retrieval context improves performance, especially for complex inputs"
  - [section 5.2] "Most metrics show consistent improvements as the window size increases, suggesting that they can effectively leverage broader local context"
  - [corpus] Related work "Discourse-Driven Evaluation" similarly addresses long-document factual inconsistency through structural analysis, suggesting retrieval-based approaches are an active research direction
- **Break condition:** When evidence is dispersed across distant document sections with low lexical overlap, or when multi-document summaries require reconciling conflicting information across sources—conditions where similarity-based retrieval may miss relevant context.

### Mechanism 2
- **Claim:** Stress-testing with meaning-preserving perturbations reveals metric brittleness by measuring score variance under semantically equivalent rewrites.
- **Mechanism:** Apply seven perturbation types (paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, source text insertion) using GPT-4o. Compare score distributions between original and perturbed summaries. Robust metrics should show minimal score shift (ideally Δ ≈ 0).
- **Core assumption:** Perturbations genuinely preserve factual meaning and any score change indicates surface-level sensitivity rather than legitimate factuality assessment.
- **Evidence anchors:**
  - [abstract] "Results show that metrics are highly sensitive to surface-level edits"
  - [section 5.1] "AlignScore and SummaC-ZS are particularly unreliable across domains and perturbation types. In contrast, UniEval and MiniCheck are relatively robust"
  - [corpus] No direct corpus validation of this specific perturbation set; Ramprasad & Wallace (2024) applied similar methodology to short documents only
- **Break condition:** When perturbations inadvertently alter meaning (acknowledged in Limitations: "without human annotations, we cannot confirm with full certainty that all edits preserve factual correctness"), creating noise in robustness interpretation.

### Mechanism 3
- **Claim:** Metric reliability degrades for information-dense claims whose semantic content overlaps broadly with multiple source regions.
- **Mechanism:** Compute information density as mean pairwise cosine similarity between each summary sentence embedding and all source document sentence embeddings (Eq. 3). Group claims into similarity bins and correlate with average factuality scores (Eq. 4).
- **Core assumption:** Higher claim-document similarity indicates compressed or globally entangled content requiring multi-span reasoning.
- **Evidence anchors:**
  - [abstract] "most metrics struggle with information-dense claims"
  - [section 5.3] "For LexAbSumm, metric scores consistently decrease as claim similarity increases... This is likely because summaries of legal documents may refer to specific aspects... while more general statements which compress a lot of technical language are more difficult"
  - [corpus] "Knowledge-Level Consistency Reinforcement Learning" addresses long-form factuality through dual-fact alignment, suggesting distributed evidence reasoning is a recognized challenge
- **Break condition:** In multi-document settings (ScholarQABench), high similarity may indicate repeated claim instances across documents rather than compression complexity—creating inverse relationships where more similar claims score higher.

## Foundational Learning

- **Concept: Reference-free factuality evaluation**
  - **Why needed here:** The paper evaluates metrics that assess summary-source alignment directly without gold reference summaries, which is essential for long-document settings where reference creation is impractical.
  - **Quick check question:** Given a summary sentence "The court ruled in favor of the defendant" and a source document, what information would a reference-free metric need to determine factuality?

- **Concept: Natural Language Inference (NLI) for factuality**
  - **Why needed here:** SummaC-Conv and SummaC-ZS use NLI models to judge whether summary sentences are entailed by source sentences; understanding entailment relationships is core to interpreting these results.
  - **Quick check question:** If a summary states "X did not occur" and the source states "X was impossible," is this entailment, contradiction, or neutral?

- **Concept: Semantic similarity via sentence embeddings**
  - **Why needed here:** The retrieval mechanism and information density measure both rely on cosine similarity between sentence embeddings to identify relevant evidence and quantify claim complexity.
  - **Quick check question:** Two sentences with high cosine similarity must have the same factual content—true or false, and why?

## Architecture Onboarding

- **Component map:**
  Source Document D → Sentence Tokenizer → Sentence Encoder → Embeddings {e^D_1, ..., e^D_n}
  Summary S → Sentence Tokenizer → Sentence Encoder → Embeddings {e_1, ..., e_m}

  For each summary sentence s_j:
    ├─ Cosine similarity computation vs. all source embeddings
    ├─ Top-K retrieval with context window expansion (w=0,1,2)
    └─ Metric M evaluation → max score selection

  Aggregation: Average across all summary sentences → Summary-level score

  Parallel path: GPT-4o Perturbation Generator → 7 perturbed variants → Same scoring pipeline

- **Critical path:** Sentence embedding quality → retrieval accuracy → context window sufficiency → metric evaluation. The retrieval step is the bottleneck; poor similarity matching cascades into irrelevant context and unreliable scores.

- **Design tradeoffs:**
  - **Context window size (w):** Larger windows capture more evidence but increase computational cost and may introduce noise. Paper shows w=2 helps legal domain but NLI-based metrics show little sensitivity.
  - **Top-K value:** Fixed K assumes uniform evidence distribution; variable K could adapt to claim complexity but requires additional calibration.
  - **Perturbation generation via LLM:** GPT-4o provides fluency but introduces unverified assumption that meaning is preserved; human annotation would validate but is expensive.

- **Failure signatures:**
  - **BARTScore consistently low (~0.03-0.16):** Generation-based metric struggles with retrieved snippets that mismatch broader document context; likelihood estimates distorted when evidence is scattered.
  - **Negated perturbation causing score crashes (e.g., UniEval: 0.82→0.39 on LexAbSumm):** Metrics fail to recognize logical equivalence when syntactic polarity changes.
  - **High claim-document similarity producing lower scores (LexAbSumm, SQuALITY):** Indicates inability to reason over distributed, multi-span evidence.

- **First 3 experiments:**
  1. **Baseline reproduction:** Run the retrieval-based scoring pipeline (w=0,1,2) on all three datasets with MiniCheck and UniEval to confirm robust metrics show <0.05 score variance under perturbation.
  2. **Ablation on retrieval strategy:** Replace cosine similarity retrieval with query-based dense retrieval (e.g., BM25 + semantic re-ranking) to test whether evidence localization assumption holds for information-dense claims.
  3. **Domain transfer test:** Apply the best-performing metrics (MiniCheck, UniEval) to a new domain (e.g., medical or financial documents) with manually verified perturbations to assess generalization and identify domain-specific failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can factuality metrics be redesigned to effectively integrate multi-span reasoning for evaluating distributed evidence?
- **Basis in paper:** [explicit] The Conclusion explicitly calls for developing metrics that integrate "multi-span reasoning" to better model distributed evidence.
- **Why unresolved:** Current metrics largely rely on localized semantic alignment or pairwise entailment, causing them to fail when evidence is scattered across distant sections of long documents.
- **What evidence would resolve it:** The development of a metric architecture that explicitly aggregates evidence from multiple disjoint text spans, demonstrating superior performance on information-dense claims.

### Open Question 2
- **Question:** Does contrastive training on meaning-preserving perturbations significantly stabilize metric scoring?
- **Basis in paper:** [explicit] The Conclusion highlights "training on meaning-preserving variations to enhance robustness" as a concrete direction for future work.
- **Why unresolved:** The study found that current metrics produce inconsistent scores for semantically equivalent summaries, indicating a lack of invariance to surface-level edits.
- **What evidence would resolve it:** Experiments showing that metrics fine-tuned with these perturbations exhibit significantly lower score variance between original and perturbed summaries without losing sensitivity to factual errors.

### Open Question 3
- **Question:** Do the observed brittleness and sensitivity patterns generalize to high-stakes domains like medical or financial summarization?
- **Basis in paper:** [inferred] The Limitations section notes the analysis is confined to fiction, legal, and scientific texts and may not generalize to "medical or financial summarization."
- **Why unresolved:** High-stakes domains often feature denser technical jargon and distinct error tolerances which may interact differently with the surface-level sensitivities identified in this study.
- **What evidence would resolve it:** Replicating the stress-testing methodology using perturbations on medical or financial long-document datasets to verify if similar reliability issues persist.

### Open Question 4
- **Question:** Can dynamic query-based retrieval strategies improve factuality evaluation accuracy compared to the static retrieval methods used in this study?
- **Basis in paper:** [inferred] The Limitations section states the framework "assumes a static retrieval strategy and does not account for dynamic query-based retrieval."
- **Why unresolved:** The static top-k retrieval used may miss relevant context that a more adaptive, query-aware retrieval system would capture, potentially altering the metric's performance ceiling.
- **What evidence would resolve it:** A comparative analysis benchmarking metric performance when using static context windows versus dynamic, query-informed evidence retrieval.

## Limitations

- Reliance on GPT-4o-generated perturbations without human validation means some meaning-preserving edits may inadvertently alter factual content
- Static retrieval strategy assumes evidence is localized around similar sentences, which may not hold for information-dense claims requiring multi-span reasoning
- Analysis confined to fiction, legal, and scientific domains; may not generalize to high-stakes domains like medical or financial summarization

## Confidence

- **High confidence** that metrics are highly sensitive to surface-level perturbations, as this is consistently demonstrated across all perturbation types and datasets
- **Medium confidence** that NLI-based metrics (SummaC) show moderate stability, though their failure on negated perturbations reveals sensitivity to logical structure
- **Low confidence** that UniEval and MiniCheck are "relatively robust" since their significant score drops on negated summaries indicate brittleness to logical transformations

## Next Checks

1. Conduct human evaluation of perturbation meaning preservation on a subset of summaries to calibrate metric sensitivity scores and validate that observed brittleness reflects true robustness issues rather than unintended meaning changes
2. Implement and test alternative retrieval strategies (e.g., query-based dense retrieval with re-ranking) on information-dense claims to determine if evidence localization assumptions limit metric performance
3. Evaluate the top-performing metrics (UniEval, MiniCheck) on a new domain (e.g., medical or financial documents) with manually verified perturbations to assess generalization beyond the three studied domains