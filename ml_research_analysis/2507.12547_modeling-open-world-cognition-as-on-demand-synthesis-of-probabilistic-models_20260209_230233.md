---
ver: rpa2
title: Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models
arxiv_id: '2507.12547'
source_url: https://arxiv.org/abs/2507.12547
tags:
- match
- strength
- athlete
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how humans flexibly integrate relevant background
  knowledge into coherent reasoning about novel situations, a capability that current
  AI systems struggle to replicate. The authors propose a "Model Synthesis Architecture"
  (MSA) that combines language models (LMs) for retrieving and organizing relevant
  information with probabilistic programs for coherent reasoning.
---

# Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models

## Quick Facts
- arXiv ID: 2507.12547
- Source URL: https://arxiv.org/abs/2507.12547
- Authors: Lionel Wong; Katherine M. Collins; Lance Ying; Cedegao E. Zhang; Adrian Weller; Tobias Gerstenberg; Timothy O'Donnell; Alexander K. Lew; Jacob D. Andreas; Joshua B. Tenenbaum; Tyler Brooke-Wilson
- Reference count: 33
- Primary result: MSA architecture combining LMs and probabilistic programs better captures human open-world reasoning than LM-only baselines

## Executive Summary
This paper explores how humans flexibly integrate relevant background knowledge into coherent reasoning about novel situations, a capability that current AI systems struggle to replicate. The authors propose a "Model Synthesis Architecture" (MSA) that combines language models (LMs) for retrieving and organizing relevant information with probabilistic programs for coherent reasoning. Their implementation uses LMs to synthesize ad-hoc probabilistic models from natural language inputs, which are then used for inference.

The approach is evaluated on a novel dataset of "Model Olympics" sports vignettes requiring integration of background knowledge, novel causal structures, and arbitrary new variables. Across three experiments with increasing complexity—from detailed backgrounds to underspecified contexts to participant-generated novel details—the MSA captured human judgments better than LM-only baselines using both direct and chain-of-thought approaches. The results suggest that interleaving neural and symbolic methods can replicate human-like open-world reasoning that is both globally relevant and locally coherent, offering a path toward more human-like AI reasoning systems.

## Method Summary
The Model Synthesis Architecture (MSA) breaks open-world reasoning into two subproblems: (1) constructing ad-hoc models that represent relevant background knowledge and novel causal structures, and (2) performing inference within these models. The implementation uses a staged pipeline: Parse converts observations and questions into placeholder expressions; Background Retrieval augments the provided background with relevant information from a database; Dependency Graph extraction creates a conceptual representation; and Program Synthesis generates executable WebPPL programs. The MSA uses language models for the first three stages (with retrieval and model synthesis scored by Φrelevance and Φmodel functions) and probabilistic inference for the final stage. The approach is evaluated against LM-only baselines across three experiments with increasing complexity, using R² to measure alignment with human judgments.

## Key Results
- MSA captured human judgments better than LM-only baselines across all three experiments (Exp. 1-3, p < 0.05 consistently)
- In Experiment 3 (open-world setting), MSA R² was 0.52 vs. 0.09 for LM-only baselines
- MSA's advantage was particularly pronounced when reasoning about novel variables and causal structures
- Human-like reasoning requires both global relevance (background knowledge integration) and local coherence (consistent joint distributions)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing open-world reasoning into model synthesis + inference enables handling arbitrary novel situations
- Mechanism: Sequential pipeline retrieves relevant background knowledge via LM, synthesizes executable probabilistic programs, then performs Bayesian inference
- Core assumption: Human reasoning involves constructing ad-hoc mental models (M_ad-hoc) that approximate full conditional reasoning P(A|τ,K) ≈ P(A|M_ad-hoc)
- Evidence anchors:
  - [abstract]: "using language models to implement global relevance-based retrieval and model synthesis and probabilistic programs to implement bespoke, coherent world models"
  - [section]: "MSAs approach the problem... by breaking it into two subproblems: (1) constructing, or 'synthesizing,' ad-hoc models... and (2) reasoning within a model"
  - [corpus]: "Modeling Others' Minds as Code" (arxiv:2510.01272) similarly treats mental models as synthesizable code
- Break condition: If humans reason via single fixed models rather than constructing situation-specific ones, MSA advantage disappears

### Mechanism 2
- Claim: Probabilistic programs provide local coherence that pure neural approaches lack, especially with novel variables
- Mechanism: Synthesized WebPPL programs enforce consistent joint distributions—queries about related latent variables (strength, effort, outcomes) must be mutually coherent under the model
- Core assumption: Human judgments over causally related variables are internally consistent according to some structured generative process
- Evidence anchors:
  - [abstract]: "Our MSA approach captures human judgments better than language model-only baselines"
  - [section]: "MSA judgments were substantially better aligned with people's than were those of LM-only alternatives" in Experiment 3 (open-world setting)
  - [corpus]: "BayesAgent" (arxiv:2406.05516) shows similar benefits from explicit probabilistic structure
- Break condition: If human judgments are systematically incoherent or if LMs develop implicit coherent world models, symbolic layer becomes unnecessary

### Mechanism 3
- Claim: Interleaved generation-evaluation stages improve synthesis quality over end-to-end approaches
- Mechanism: Parse → Background retrieval → Dependency graph → Program synthesis, with LM-based scoring functions (Φ) selecting candidates at each stage
- Core assumption: Staged decomposition makes the search tractable; scoring functions approximate model utility
- Evidence anchors:
  - [section]: "This implementation sequentially constructs M_ad-hoc by: Parse... Relevant Natural Language Background Description... Conceptual Dependency Graph... Probabilistic Model"
  - [abstract]: Pipeline enables "globally relevant and locally coherent" reasoning
  - [corpus]: Limited corpus evidence—this specific staged architecture is not directly compared to alternatives in related work
- Break condition: If single-shot synthesis with stronger LMs matches staged performance, pipeline complexity unjustified

## Foundational Learning

- **Probabilistic Programming (WebPPL/Gen)**:
  - Why needed: MSA outputs are executable probabilistic programs with stochastic functions, conditioning, and queries
  - Quick check: Can you write a WebPPL program defining a latent strength variable, per-match effort, and condition on observed outcomes?

- **Bayesian Inference (explaining-away, posterior updating)**:
  - Why needed: Model Olympics tasks require understanding how evidence updates beliefs about multiple interacting latents
  - Quick check: If athlete A usually wins but loses once, how should you update beliefs about A's strength vs. A's effort in that match?

- **LM Prompting for Structured Output**:
  - Why needed: Each pipeline stage uses carefully constructed prompts with examples to guide generation
  - Quick check: Can you design a prompt that reliably extracts entity-relation triples from natural language?

## Architecture Onboarding

- **Component map**:
```
Natural Language Task τ = (B, O, Q)
    ↓ [Parse: LM @ temp=0.2, k=1]
Placeholder expressions Π*O, Π*Q
    ↓ [Background Retrieval: LM @ temp=0.5, k=8, score with Φrelevance]
Augmented background B*aug, dependency graph G*
    ↓ [Program Synthesis: LM @ temp=0.2, k=1, check Φmodel=compiles]
Executable WebPPL program M*ad-hoc
    ↓ [Inference: Rejection sampling, 500-1000 samples]
Posterior samples P(A|M*ad-hoc)
```

- **Critical path**: Parse quality → Background retrieval relevance → Program validity (must compile) → Inference tractability

- **Design tradeoffs**:
  - Single synthesized model per "participant" vs. ensemble (paper chooses single, may miss human diversity)
  - Rejection sampling: Unbiased but slow for rare observations (budget 500-1000 samples)
  - Held-out prompting: Examples from other sports to test generalization (may underutilize domain-specific knowledge)

- **Failure signatures**:
  - **Parse-code mismatch**: Placeholder functions never defined in final program (manual inspection required)
  - **Over-smoothed posteriors**: MSA produces higher uncertainty than human "streaky" judgments (see Fig. 16)
  - **Temporal logic gaps**: Struggles with "effects future matches" without explicit examples in prompt
  - **Example overfitting**: Synthesized models mirror example structure, lack diversity

- **First 3 experiments**:
  1. **Unit test the parser**: Hand-craft 10 observation sentences with varying complexity; verify parse expressions capture semantics and resolve to valid function calls in downstream program
  2. **Ablate retrieval stage**: Run MSA with background retrieval disabled (use only input B) vs. full pipeline on Exp. 2 underspecified vignettes; measure R² drop
  3. **Stress-test generalization**: Train on tug-of-war + canoe examples only; test synthesis quality on biathlon (novel causal structure with shooting accuracy)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the mechanism by which MSAs achieve better alignment with human judgments than LM-only baselines—coherence constraints, causal representations, or other factors?
- Basis in paper: [explicit] "Determining which of these or other explanations is most plausible, and if this general trend continues to hold in more varied domains, is a priority for future research."
- Why unresolved: Authors propose coherence and causal structure as hypotheses but do not adjudicate between them; experiments do not isolate these factors.
- What evidence would resolve it: Ablation studies manipulating coherence constraints independently from causal representation fidelity, across diverse reasoning domains.

### Open Question 2
- Question: How can model synthesis procedures be improved to generate more diverse ad-hoc models that better match the variance in human reasoning?
- Basis in paper: [explicit] "Variance in samples from our MSA was also often too low... This suggests a lack of diversity in the models synthesized by our MSA. Follow-on work should explore how to increase the diversity of synthesized models..."
- Why unresolved: Current MSA implementation produces overly smooth, uncertain distributions compared to humans' more peaked but appropriately placed judgments.
- What evidence would resolve it: Comparisons of human-model fit using synthesis methods that explicitly sample multiple distinct model structures and weighting schemes.

### Open Question 3
- Question: Does the MSA approach generalize to open-world reasoning with temporally extended, piecemeal information revelation?
- Basis in paper: [explicit] "Further work should explore this theme of holistic integration more thoroughly, including in cases where information is revealed piecemeal over time (as it often is in naturalistic reasoning tasks), rather than all at once (as in our experiments here)."
- Why unresolved: All experiments presented complete evidence upfront; real-world reasoning involves sequential evidence integration.
- What evidence would resolve it: Experiments using sequential vignette presentations with reaction time measures and judgment updates after each piece of information.

### Open Question 4
- Question: What alternative model classes could better capture the distributional features of human judgments—particularly the sharp peaks that LMs produce but in more appropriate locations?
- Basis in paper: [explicit] "A pressing question then is whether some other model class could better delivers the patterns seen in the human data. This might be some deeper hybridization of neural and symbolic methods... or an MSA with stronger sampling methods..."
- Why unresolved: LMs are "streaky" with misplaced peaks; MSAs are directionally correct but overly smooth; neither fully captures human patterns.
- What evidence would resolve it: Developing hybrid architectures that combine LMs' peaked distributions with MSAs' grounded inference, then comparing distributional fits to human data.

## Limitations

- **Human model diversity**: The MSA produces a single deterministic model per vignette, while humans likely generate diverse mental models for the same scenario
- **Scoring function quality**: Both Φrelevance and Φmodel are black-box LLM evaluations without validation against human judgments
- **Computational cost**: The staged pipeline with multiple LLM calls and probabilistic inference is expensive, with no efficiency comparison to LM-only baselines

## Confidence

- **High confidence**: MSA outperforms LM-only baselines on capturing human judgments (Exp. 1-3, p < 0.05 consistently)
- **Medium confidence**: Probabilistic programs provide necessary local coherence for novel variable reasoning (supported by comparative results but alternative explanations possible)
- **Low confidence**: Staged decomposition is optimal for synthesis quality (no ablation against single-shot synthesis)

## Next Checks

1. **Individual variation analysis**: Run MSA on the same vignettes with perturbed backgrounds (simulating different participants' knowledge) and measure variance in predictions vs. human inter-rater variance.

2. **Scoring function ablation**: Replace Φrelevance and Φmodel with simple heuristics (e.g., keyword overlap, syntactic validity) and measure performance drop to assess true contribution of LLM scoring.

3. **Single-shot synthesis comparison**: Implement a direct prompt that asks the LM to generate the complete probabilistic program without staged decomposition, then compare MSA vs. single-shot performance on held-out vignettes.