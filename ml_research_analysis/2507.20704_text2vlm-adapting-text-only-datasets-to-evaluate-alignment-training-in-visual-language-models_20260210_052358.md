---
ver: rpa2
title: 'Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual
  Language Models'
arxiv_id: '2507.20704'
source_url: https://arxiv.org/abs/2507.20704
tags:
- text2vlm
- image
- prompt
- multimodal
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Text2VLM, an automated pipeline that converts
  text-only datasets into multimodal formats to evaluate Visual Language Model (VLM)
  safety. The pipeline extracts harmful concepts from text, replaces them with numbered
  placeholders, and renders the concepts as a typographic image.
---

# Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models

## Quick Facts
- **arXiv ID**: 2507.20704
- **Source URL**: https://arxiv.org/abs/2507.20704
- **Reference count**: 15
- **Primary result**: Text2VLM pipeline revealed that VLMs show reduced safety alignment with multimodal inputs compared to text-only prompts

## Executive Summary
This paper introduces Text2VLM, an automated pipeline that converts text-only datasets into multimodal formats to evaluate Visual Language Model (VLM) safety. The pipeline extracts harmful concepts from text, replaces them with numbered placeholders, and renders the concepts as a typographic image. This design tests VLMs' susceptibility to multimodal prompt injection attacks.

The pipeline was validated on four malicious datasets—MITRE, Interpreter, MedSafetyBench, and ToxiGen—and one benign control dataset (Vicuna-Bench). Human evaluation showed high reliability: over 90% of prompt summarizations and salient concept extractions were rated as "Great" or "Good," and 93% of outputs were correctly classified for refusal detection.

Experimental results revealed that open-source VLMs (LLaVA and VILA models) struggled more with typographic multimodal inputs than with text-only prompts, indicating reduced understanding. More critically, multimodal inputs led to significantly lower refusal rates for harmful content, especially in the MedSafetyBench dataset, revealing a degradation in safety alignment when both text and image channels are used together. This highlights vulnerabilities in current VLM safety mechanisms and underscores the need for robust multimodal alignment evaluations.

## Method Summary
The Text2VLM pipeline converts text-only datasets into multimodal evaluation formats by extracting harmful concepts, replacing them with numbered placeholders, and rendering these concepts as typographic images. The pipeline processes input text to identify salient harmful concepts, substitutes them with placeholder tokens, and generates corresponding images using typographic rendering. The final output combines the modified text with its associated image, creating multimodal prompts that test VLM safety alignment. Human evaluation validated the pipeline's reliability, with high agreement rates for prompt summarization and concept extraction quality.

## Key Results
- Open-source VLMs showed reduced understanding of typographic multimodal inputs compared to text-only prompts
- Multimodal inputs resulted in significantly lower refusal rates for harmful content, particularly in the MedSafetyBench dataset
- Human evaluation demonstrated high reliability with over 90% "Great" or "Good" ratings for prompt quality and 93% correct classification for refusal detection

## Why This Works (Mechanism)
Text2VLM exploits the fact that VLMs must process both visual and textual information simultaneously, creating potential safety alignment gaps when harmful concepts are presented visually rather than textually. The typographic rendering of harmful concepts bypasses traditional text-based safety filters while still requiring the model to integrate visual information with contextual understanding. This multimodal approach reveals vulnerabilities in VLMs that may not be apparent when testing with text-only inputs, as the visual channel can be manipulated to bypass safety mechanisms designed primarily for textual content.

## Foundational Learning
- **Multimodal prompt injection**: Attacks that combine text and visual elements to manipulate model behavior; needed to understand how VLMs can be deceived beyond text-only approaches; quick check: test with synthetic image-text pairs containing conflicting information
- **Safety alignment evaluation**: Methods for assessing whether models refuse harmful requests; needed to establish baselines for comparison; quick check: measure refusal rates across different input modalities
- **Concept extraction and placeholder substitution**: Automated identification of harmful elements and their replacement with tokens; needed to create controlled evaluation conditions; quick check: verify that extracted concepts accurately represent harmful content
- **Typographic image rendering**: Converting text concepts into visual format for VLM input; needed to create consistent multimodal prompts; quick check: ensure rendered images are readable by target models
- **Human evaluation protocols**: Structured assessment of generated prompts and model responses; needed to validate pipeline reliability; quick check: achieve high inter-rater agreement scores
- **Refusal detection metrics**: Methods for measuring model safety responses; needed to quantify alignment effectiveness; quick check: establish clear criteria for what constitutes a refusal

## Architecture Onboarding

Component map: Text input -> Concept extraction -> Placeholder replacement -> Typographic rendering -> Multimodal output -> VLM evaluation

Critical path: Text input -> Concept extraction -> Placeholder replacement -> Multimodal output (this sequence is essential for creating the evaluation prompts)

Design tradeoffs: The pipeline prioritizes consistency and control over realism by using typographic rendering rather than natural images, sacrificing ecological validity for experimental precision and reproducibility.

Failure signatures: Poor concept extraction may miss subtle harmful content; placeholder substitution errors can break prompt coherence; typographic rendering issues may make concepts unreadable; human evaluation bias can skew validation results.

First experiments:
1. Test concept extraction accuracy on diverse text samples with varying levels of harmful content
2. Validate placeholder substitution maintains prompt coherence across different domains
3. Evaluate typographic rendering quality across different font sizes and styles for VLM readability

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The typographic rendering approach may not fully represent real-world multimodal attacks that use natural images
- Human evaluation introduces potential subjectivity despite high agreement rates
- Results may not generalize to models with stronger visual reasoning capabilities or different architectural designs
- The pipeline's effectiveness depends on accurate concept extraction, which may miss nuanced harmful content

## Confidence

| Claim | Confidence |
|-------|------------|
| VLMs show reduced safety alignment with multimodal inputs | High |
| Pipeline reliability for generating evaluation data | Medium |
| Broader implications for real-world VLM safety | Medium |

## Next Checks
1. Test the pipeline and safety evaluation on a broader set of VLMs, including models with advanced visual reasoning and proprietary systems like GPT-4V or Gemini
2. Evaluate the pipeline's effectiveness with natural image inputs containing harmful concepts embedded in realistic contexts, rather than typographic renderings
3. Conduct adversarial testing by modifying typographic prompts to include visual obfuscation, noise, or multimodal chaining to assess robustness of current VLM safety mechanisms