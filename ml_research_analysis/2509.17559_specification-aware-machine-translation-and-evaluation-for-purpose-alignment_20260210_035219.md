---
ver: rpa2
title: Specification-Aware Machine Translation and Evaluation for Purpose Alignment
arxiv_id: '2509.17559'
source_url: https://arxiv.org/abs/2509.17559
tags:
- translation
- evaluation
- specifications
- error
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces specification-aware machine translation\
  \ (MT), arguing that professional translation quality depends on fulfilling communicative\
  \ goals, not just linguistic equivalence. It presents a framework that integrates\
  \ translation specifications\u2014such as purpose, audience, style, and terminology\u2014\
  into MT workflows and evaluation, drawing on functionalist translation theory and\
  \ standards like ISO 5060 and MQM."
---

# Specification-Aware Machine Translation and Evaluation for Purpose Alignment

## Quick Facts
- arXiv ID: 2509.17559
- Source URL: https://arxiv.org/abs/2509.17559
- Reference count: 40
- Key outcome: Specification-aware LLM translations consistently outperform official human translations and Google Translate in human evaluations for investor relations texts.

## Executive Summary
This paper introduces specification-aware machine translation (MT), arguing that professional translation quality depends on fulfilling communicative goals, not just linguistic equivalence. It presents a framework that integrates translation specifications—such as purpose, audience, style, and terminology—into MT workflows and evaluation, drawing on functionalist translation theory and standards like ISO 5060 and MQM. In a case study translating investor relations texts from 33 Japanese companies, the authors compare five translation methods: official human translations, Google Translate, and three prompt-based large language model (LLM) outputs (basic, specification-guided, and post-edited). Human evaluations (expert error analysis and user preference rankings) show that specification-aware LLM translations consistently outperform official human translations and Google Translate. Automatic metrics like COMETKiwi, however, favor Google Translate, highlighting a gap between functional adequacy and surface-level fluency. The findings demonstrate that integrating specifications into MT workflows—with human oversight—can improve translation quality in professional settings, aligning outputs with real-world communicative needs.

## Method Summary
The study employed a multi-method approach to evaluate specification-aware MT. Five translation methods were compared: official human translations, Google Translate, and three LLM-based approaches (basic, specification-guided, and post-edited). The research team collected investor relations texts from 33 Japanese companies and translated them using each method. Human evaluations were conducted through expert error analysis following MQM standards and user preference rankings. The specification-aware approach involved providing LLMs with detailed translation specifications including purpose, audience, style, and terminology requirements. Translations were assessed for functional adequacy rather than just linguistic accuracy, measuring how well each translation fulfilled its intended communicative purpose.

## Key Results
- Specification-aware LLM translations consistently outperformed official human translations and Google Translate in human evaluations across multiple methods and raters.
- Expert error analysis revealed that specification-aware translations had fewer functional errors related to purpose misalignment.
- Automatic metrics like COMETKiwi favored Google Translate, demonstrating a significant gap between surface-level fluency and functional adequacy assessment.

## Why This Works (Mechanism)
Specification-aware MT works by explicitly incorporating translation specifications into the translation process, allowing models to generate outputs that better fulfill their intended communicative purpose rather than focusing solely on linguistic equivalence. By providing LLMs with detailed information about purpose, audience, style, and terminology requirements, the translations become more aligned with real-world professional needs. This approach leverages functionalist translation theory principles, treating translation as goal-oriented communication rather than mere language conversion. The specification-guided prompts help LLMs understand context and intent beyond surface-level text, resulting in translations that better serve their intended use cases, particularly in professional domains where communicative effectiveness is paramount.

## Foundational Learning

Functionalist Translation Theory
- Why needed: Provides theoretical foundation for purpose-oriented translation rather than equivalence-based approaches
- Quick check: Review Vermeer's skopos theory and Nord's text analysis model

MQM (Multidimensional Quality Metrics)
- Why needed: Standardized framework for systematic error analysis in translation evaluation
- Quick check: Understand MQM error categories and severity levels

ISO 5060 Standard
- Why needed: Establishes quality requirements and evaluation methods for translation services
- Quick check: Review ISO 5060 sections on functional adequacy and translation purpose

Automatic Evaluation Metrics
- Why needed: Traditional metrics like BLEU, COMET may not capture functional adequacy
- Quick check: Compare metric outputs with human functional adequacy judgments

Large Language Model Prompt Engineering
- Why needed: Critical for effective specification-aware translation generation
- Quick check: Test different prompt structures with and without specifications

Professional Translation Workflow Integration
- Why needed: Demonstrates practical application of specification-aware MT in real settings
- Quick check: Map specification-aware steps to existing professional translation processes

## Architecture Onboarding

Component Map:
User Input -> Translation Specifications -> LLM with Prompt Engineering -> Translation Output -> Human Evaluation (MQM/Preference) -> Quality Assessment

Critical Path:
Translation Specifications (most critical) -> LLM Prompt Engineering -> Human Evaluation -> Quality Assessment

Design Tradeoffs:
- Specification detail vs. prompt complexity
- LLM performance vs. human post-editing requirements
- Evaluation time vs. assessment thoroughness

Failure Signatures:
- Specifications too vague → Generic translations lacking purpose alignment
- Over-specified prompts → Rigid translations missing contextual nuance
- Automatic metrics favored → Surface fluency prioritized over functional adequacy

First Experiments:
1. Compare specification-aware vs. basic LLM translations on same text set
2. Test different specification prompt structures on translation quality
3. Evaluate human vs. automatic metric alignment across multiple domains

## Open Questions the Paper Calls Out
None

## Limitations
- Human evaluation methods may reflect individual preferences rather than generalizable standards
- Dataset limited to Japanese-to-English investor relations texts from 33 companies, raising external validity concerns
- LLM performance gains could be partially attributed to in-domain fine-tuning rather than specification awareness
- Automatic metrics' poor alignment with human judgments requires further investigation across more diverse texts

## Confidence
- Specification-aware LLM translations outperform traditional MT and official human translations (High): Supported by consistent human evaluation results across multiple methods and raters, though limited to one domain and language pair.
- Integrating specifications improves MT quality in professional settings (Medium): Strong evidence from the case study, but generalizability across different professional contexts and specification types remains unproven.
- Automatic metrics inadequately capture functional adequacy (High): Clear divergence between human and metric evaluations, though the specific reasons for this mismatch require further investigation.

## Next Checks
1. Replicate the study with diverse text types (e.g., technical documentation, legal contracts, creative content) and multiple language pairs to test domain and language generalizability.
2. Conduct A/B testing with end-users who rely on these translations for actual decision-making, measuring real-world impact rather than just preference rankings.
3. Develop and validate new automatic metrics specifically designed to assess functional adequacy based on specifications, comparing their performance against both traditional metrics and human judgments across multiple domains.