---
ver: rpa2
title: 'AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via
  Multi-Agent Reasoning'
arxiv_id: '2601.16685'
source_url: https://arxiv.org/abs/2601.16685
tags:
- evaluation
- clinical
- reports
- reasoning
- agentseval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of evaluating medical imaging
  reports with clinical accuracy. Existing metrics fail to capture structured diagnostic
  logic, often over-rewarding fluency while missing factual errors.
---

# AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning

## Quick Facts
- arXiv ID: 2601.16685
- Source URL: https://arxiv.org/abs/2601.16685
- Reference count: 29
- This paper tackles the challenge of evaluating medical imaging reports with clinical accuracy. Existing metrics fail to capture structured diagnostic logic, often over-rewarding fluency while missing factual errors. The proposed AgentsEval framework uses multi-agent stream reasoning to mimic radiologists' diagnostic workflow: defining criteria, extracting clinical indicators, aligning evidence, and scoring consistency. The framework is validated on five medical report datasets with controlled synonymic and semantic perturbations. Results show that AgentsEval maintains stable, clinically aligned evaluations across paraphrasing and semantic variations, outperforming traditional and single-agent baselines. It achieves strong correlation with human-annotated error counts, providing interpretable, fine-grained feedback and improved robustness to stylistic and factual changes.

## Executive Summary
This paper introduces AgentsEval, a multi-agent reasoning framework for evaluating the clinical accuracy of medical imaging reports. Traditional metrics like BLEU or BERTScore often over-reward fluency and miss factual errors. AgentsEval decomposes evaluation into five interpretable stages: criteria definition, clinical indicator extraction, evidence alignment, and consistency scoring. By mimicking radiologists' diagnostic workflow, it provides explicit reasoning traces and structured feedback. The framework is validated on five medical datasets with controlled paraphrasing and semantic perturbations, demonstrating robustness to style changes while remaining sensitive to factual deviations.

## Method Summary
AgentsEval is a five-agent pipeline designed to evaluate clinical correctness in medical imaging reports. The process begins with A_base, which generates a candidate criteria pool from a batch of ground-truth reports. A_crit then selects report-specific clinical indicators, followed by A_gt extracting indicator values from the reference report into D_GT. A_pred retrieves corresponding values from the generated report into D_pred, and finally A_eval scores agreement per criterion (1.0 for exact match, 0.5 for approximate, 0.0 for mismatch) and computes a weighted overall score. The framework is evaluated on five text-only datasets (CT-RATE, FFA-IR, MedVAL-Bench, RadEvalX, ReXErr-v1) using controlled perturbations (A-series synonymic rewrites, B-series semantic errors) and compared against baseline metrics (BLEU, ROUGE, BERTScore). Validation includes correlation with human-annotated error counts and robustness checks across paraphrasing and factual variations.

## Key Results
- AgentsEval maintains stable, clinically aligned evaluations across paraphrasing and semantic variations, outperforming traditional and single-agent baselines.
- It achieves strong correlation with human-annotated error counts, providing interpretable, fine-grained feedback.
- Results show improved robustness to stylistic and factual changes compared to BLEU, ROUGE, METEOR, CHRF, and BERTScore.

## Why This Works (Mechanism)

### Mechanism 1
Decomposing evaluation into sequential specialized agents improves stability and clinical alignment over single-agent or lexical approaches. Five agents operate in sequence: (1) A_base generates a candidate criteria pool from a batch of ground-truth reports; (2) A_crit identifies report-specific clinical indicators; (3) A_gt extracts indicator values from the reference; (4) A_pred matches values from the generated report; (5) A_eval scores criterion-wise consistency. Each agent produces intermediate outputs, yielding explicit reasoning traces. Core assumption: LLMs perform more reliably on narrow, structured sub-tasks than on end-to-end evaluation, and radiologist workflow is sufficiently regular to decompose.

### Mechanism 2
Structured criterion-level alignment captures clinical correctness better than lexical or embedding similarity. Free-text reports are transformed into structured key–value pairs (D_GT and D_pred) over shared clinical indicators. Agreement is scored at the criterion level (1.0 for exact match, 0.5 for approximate, 0.0 for mismatch), then weighted and aggregated. This enforces semantic comparison beyond word overlap. Core assumption: Clinical correctness can be approximated by correctness on a finite set of extractable indicators; weighting can reflect clinical importance.

### Mechanism 3
Perturbation-based benchmarks reveal whether metrics are sensitive to factual changes but robust to stylistic variation. The authors construct A-series (A1–A3) paraphrastic rewrites and B-series (B1–B3) semantic perturbations. A clinically faithful metric should show flat scores across A-series and monotonic decline across B-series. Core assumption: Clinically faithful evaluation requires invariance to paraphrase and proportional sensitivity to factual deviation.

## Foundational Learning

- **Multi-agent orchestration**: AgentsEval relies on five agents with distinct roles and handoffs; misunderstanding orchestration leads to implementation errors. *Quick check*: Can you sketch the data and control flow between A_crit, A_gt, A_pred, and A_eval?
- **Clinical report structure**: The framework evaluates both Findings and Impression sections differently and extracts clinical indicators; without this, you may misconfigure criteria pools. *Quick check*: What is the functional difference between the Findings and Impression sections in a radiology report?
- **NLG evaluation limitations**: The paper positions AgentsEval against BLEU, ROUGE, BERTScore; understanding why they fail informs validation strategy. *Quick check*: Why might BERTScore assign high similarity to a factually incorrect paraphrase?

## Architecture Onboarding

- **Component map**: A_base → A_crit → A_gt → A_pred → A_eval
- **Critical path**: 1. Sample ground-truth batch → A_base → C_0; 2. Per report: R_GT + C_0 → A_crit → C_i; 3. R_GT + C_i → A_gt → D_GT; 4. R_pred + C_i → A_pred → D_pred; 5. D_GT + D_pred → A_eval → Score
- **Design tradeoffs**: Interpretability vs cost: More agents and explicit traces improve transparency but increase inference calls and latency. Generality vs specificity: Domain-agnostic prompts ease transfer but may miss modality-specific nuance; weighted criteria allow customization but require domain expertise. Model scale vs accessibility: Larger models (32B+) show better discrimination but raise deployment barriers; smaller models may fail on fine-grained semantic distinctions.
- **Failure signatures**: Flat scores across B-series: Likely insufficient reasoning capacity or overly lenient approximate matching in A_eval. High variance across paraphrases: Prompts may be encoding style preferences; review A_crit and A_pred for lexical leakage. Empty or generic criteria pool: A_base may need larger sampling or more constrained prompts.
- **First 3 experiments**: 1. Run A1–A3 and B1–B3 perturbations on a held-out split of CT-RATE; verify flat A-series and monotonic B-series decline. 2. Compare single-agent (detailed prompt) vs AgentsEval on MedVAL-Bench; measure Spearman correlation with human error counts. 3. Ablate model scale (e.g., 4B vs 32B vs 685B) on FFA-IR; confirm whether discrimination saturates beyond 32B as suggested by Figure 6.

## Open Questions the Paper Calls Out

- **Integrating visual evidence alignment**: How can visual evidence alignment be effectively integrated into the current text-based multi-agent framework to support multimodal evaluation? *Basis*: The Conclusion states future work will explore "extending multi-agent evaluation to multimodal settings, integrating visual evidence alignment." *Unresolved*: The current study explicitly excluded image data, focusing solely on textual components. *Resolution evidence*: A modified framework that processes image-text pairs and demonstrates superior performance over text-only methods in detecting visual-linguistic discrepancies.

- **Lightweight model distillation**: Can lightweight model distillation maintain evaluation fidelity while mitigating the high computational overhead of the multi-agent architecture? *Basis*: The Discussion identifies "computational overhead" as a limitation and lists "lightweight model distillation" as a specific direction. *Unresolved*: The paper demonstrates that performance relies heavily on large model scale (e.g., 685B parameters), with smaller models (0.6B) failing to distinguish errors. *Resolution evidence*: Successful deployment of a distilled, smaller agent network that retains the Spearman correlation and robustness of the 685B model baseline.

- **Feedback loops for generation**: How can the explicit reasoning traces generated by AgentsEval be utilized to create a feedback loop that actively guides and improves medical report generation? *Basis*: The Conclusion suggests leveraging "agent feedback loops to guide report generation itself" as a future possibility. *Unresolved*: The current framework functions as a standalone evaluator and does not feed its structured error analysis back into a generation model. *Resolution evidence*: A generative model that incorporates AgentsEval's criterion-level scoring as a reward signal, showing improved clinical accuracy over standard reinforcement learning approaches.

## Limitations

- The perturbation benchmarks, while methodologically sound, are limited to five small datasets (100 reports each), raising questions about robustness at scale.
- Semantic equivalence in A-series perturbations is assumed rather than verified by independent clinicians.
- The evaluation pipeline requires large language model inference at each stage, making deployment costly and raising concerns about computational accessibility for real-world clinical integration.

## Confidence

- **High confidence**: AgentsEval demonstrates superior correlation with human-annotated errors compared to baseline metrics (Spearman results). The multi-agent decomposition approach is technically sound and well-motivated by radiologist workflow.
- **Medium confidence**: Claims of improved robustness to paraphrasing and sensitivity to semantic errors are supported by the perturbation benchmark, but limited dataset size and perturbation generation method introduce uncertainty.
- **Low confidence**: Generalizability to other imaging modalities and clinical domains beyond the tested radiology reports is not established; performance may degrade outside the controlled experimental setup.

## Next Checks

1. Validate perturbation equivalence: Have independent clinicians review A1–A3 samples to confirm semantic equivalence before and after paraphrasing.
2. Scale up evaluation: Apply AgentsEval to a larger, more diverse radiology report corpus (e.g., MIMIC-CXR) and compare correlation with human judgments.
3. Cost-performance tradeoff: Measure inference latency and cost per report at each agent stage, and evaluate whether smaller models (e.g., 8B) with refined prompts can achieve comparable performance.