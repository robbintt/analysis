---
ver: rpa2
title: Explaining Necessary Truths
arxiv_id: '2502.11251'
source_url: https://arxiv.org/abs/2502.11251
tags:
- reasons
- when
- reason
- these
- variable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses how humans generate explanations for logical
  truths, which unlike contingent facts, have no natural contingencies to reference.
  The authors propose that explanations emerge from two key sources: (1) discoveries
  of simplifying steps during problem-solving, and (2) error-based reasoning where
  backtracking from mistakes provides explanatory structure.'
---

# Explaining Necessary Truths

## Quick Facts
- arXiv ID: 2502.11251
- Source URL: https://arxiv.org/abs/2502.11251
- Reference count: 13
- Primary result: GPT-4o simulations show variables involved in simplification steps (unit clauses, resolution) are cited as reasons 52-68% of the time, while backtracking-error variables are cited 36-38% of the time

## Executive Summary
This study investigates how humans generate explanations for logical truths, which unlike contingent facts, have no natural contingencies to reference. The authors propose that explanations emerge from two key sources: discoveries of simplifying steps during problem-solving, and error-based reasoning where backtracking from mistakes provides explanatory structure. Using GPT-4o as a proxy for human reasoning on SAT problems of varying complexity, they find that simulated reasoners cite variables involved in simplifications (like unit clauses or resolution steps) as reasons approximately 52-68% of the time, and variables associated with backtracking errors about 36-38% of the time. Variables with high "influence" (appearing in multiple clauses) are even more likely to be cited as reasons. Linguistic analysis reveals that simplification-based reasons use language about ease and key steps, while backtracking-based reasons emphasize necessity and avoiding contradiction.

## Method Summary
The study uses GPT-4o to simulate human reasoning on 4-variable SAT problems with 4-6 clauses in conjunctive normal form. Researchers generated 400 distinct problems per type (unit clause present, resolution pair present, or neither), creating 20 shuffled variants of each for 24,000 total runs. The model's reasoning traces were analyzed to identify when simplifications occurred, when backtracking happened, and which variables were cited as reasons. Logistic regression models predicted explanation selection based on simplification flags, backtrack flags, and variable influence scores. The study cost approximately $1000 USD in API usage.

## Key Results
- Unit clause variables cited as reasons 52% of time, 68% when no competing reason exists
- Resolution variables cited as reasons 34% of time, 50% without competition
- Backtracked variables cited as reasons 36% of time when backtracking occurred
- Variable influence coefficient +1.39 to +1.46 across all reason types (p ≪ 10⁻³)
- Simplification-based explanations associated with "ease" language; backtracking-based with "necessity" language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simplification discovery drives explanation selection for necessary truths.
- Mechanism: When a solver encounters simplifying structures (unit clauses acting as axioms, or resolution pairs that collapse complexity), the variables involved in these "hinge points" are preferentially cited as reasons why. The cognitive system treats computational ease as a proxy for explanatory importance.
- Core assumption: Reasoners track which steps reduced search complexity and elevate those variables in subsequent explanation.
- Evidence anchors:
  - [abstract] "explanations for deductive truths co-emerge with discoveries of simplifying steps during the search process"
  - [Page 3] Unit clause variables cited as reasons 52% of time, 68% when no competing reason exists; resolution variables cited 34% of time, 50% without competition (p ≪ 10⁻³)
  - [corpus] Limited direct corpus support; related work on explanation complexity exists but doesn't replicate this specific mechanism
- Break condition: If problems lack simplifying structure (no unit clauses, no resolution pairs), this mechanism yields random variable selection.

### Mechanism 2
- Claim: Backtracking errors create "fictitious contingencies" that serve as explanatory anchors.
- Mechanism: When a solver makes an assignment that leads to contradiction and must backtrack, the corrected variable gains salience. The explanation takes contrapositive form: "X must be true because setting it false leads to contradiction." The error path, not logical necessity, determines what feels explanatory.
- Core assumption: Reasoners maintain memory of failed branches and treat corrected mistakes as causal-like events.
- Evidence anchors:
  - [abstract] "error-based reasoning where backtracking from mistakes provides explanatory structure"
  - [Page 3] Backtracked variables cited as reasons 36% of time when backtracking occurred, 38% when only backtracking present
  - [Page 4] Linguistic analysis shows backtracking explanations associated with "necessity" and "contradiction" language (+0.54 coefficient), counterfactual language (+0.8 coefficient)
  - [corpus] Weak corpus support; counterfactual explanation literature exists but doesn't address backtracking specifically
- Break condition: If solver never backtracks (lucky path) or backtracking memory is not retained, error-based explanations cannot form.

### Mechanism 3
- Claim: Variable influence (clause connectivity) amplifies both simplification and error-based explanations.
- Mechanism: Variables appearing in multiple clauses have higher "degree"—they constrain more of the problem space. High-degree variables that also satisfy simplification or backtracking criteria are disproportionately likely to be cited as reasons.
- Core assumption: The cognitive system tracks variable connectivity heuristically and uses it as a tiebreaker or amplifier.
- Evidence anchors:
  - [Page 3] Logistic regression shows influence coefficient +1.39 to +1.46 across all reason types (p ≪ 10⁻³)
  - [Page 2] References SAT solver heuristics that prioritize high-frequency variables for efficiency
  - [corpus] No direct corpus matches for this specific interaction effect
- Break condition: In problems with uniform variable degree (high symmetry), influence provides no discriminative signal.

## Foundational Learning

- Concept: **Conjunctive Normal Form (CNF) and SAT problems**
  - Why needed here: The entire experimental framework uses SAT formulas in CNF. Understanding how clauses (ORs) compose via ANDs, and what makes a formula satisfiable, is prerequisite to interpreting any results.
  - Quick check question: Given formula (x₁ ∨ ¬x₂) ∧ (¬x₁ ∨ x₃) ∧ (x₂), can you determine if x₁=T, x₂=T, x₃=F is a satisfying assignment?

- Concept: **Resolution and unit clause heuristics**
  - Why needed here: These are the two simplification structures the paper measures. Unit clauses fix a variable (axiom-like); resolution eliminates a variable by combining clauses. You must recognize these to understand H1.
  - Quick check question: In (A ∨ B) ∧ (¬A ∨ C), what does resolution tell you? What if you see (A) as a standalone clause?

- Concept: **Backtracking search (Davis-Putnam style)**
  - Why needed here: The error-based explanation mechanism depends on understanding depth-first search with backtracking—making an assignment, propagating, hitting contradiction, and revising.
  - Quick check question: If you assign x₁=T and propagate to contradiction, what does backtracking require you to do?

## Architecture Onboarding

- Component map:
  - SAT Instance Generator -> Reasoning Trace Module -> Influence Calculator -> Explanation Selector -> Linguistic Analyzer
  - SAT Instance Generator creates problems with controlled properties (unit clauses, resolution pairs, variable degree distribution)
  - Reasoning Trace Module tracks search path, simplifications encountered, backtracking events, and variable assignments
  - Influence Calculator computes per-variable clause connectivity (degree)
  - Explanation Selector uses logistic model combining simplification flags, backtrack flags, and influence scores to predict cited reason
  - Linguistic Analyzer applies word-count heuristics mapping explanation text to causal/simplification/counterfactual categories

- Critical path: SAT Instance → Reasoning Trace → Explanation Selector. If trace doesn't record *which* variable caused simplification or *which* variable was backtracked, the explanation selector has no signal.

- Design tradeoffs:
  - Using GPT-4o as simulated subject enables 32,000 runs cheaply, but the paper explicitly states this is "proof of concept" not evidence of actual human behavior
  - Focusing on 4-variable problems limits ecological validity but ensures traceability
  - Requiring unique solutions simplifies evaluation but excludes many real-world reasoning scenarios

- Failure signatures:
  - **Uniform degree problems**: All mechanisms weaken; expect near-random explanation selection
  - **Lucky paths with no backtracking**: Error-based mechanism unavailable; simplification-only explanations
  - **Multiple competing simplifications**: Unit clause and resolution reasons interfere (negative coefficients in Table 1); expect inconsistent selection

- First 3 experiments:
  1. **Baseline replication**: Run 400 problems each of unit-clause, resolution, and no-simplification types. Verify explanation selection rates match Table 1 (52%, 34%, baseline). Check that influence coefficient remains positive.
  2. **Backtracking manipulation**: Force initial assignment order to guarantee backtracking on specific variables. Measure whether target variable explanation rate increases to ~36% as predicted.
  3. **Interference test**: Construct problems with both unit clause AND resolution structures. Verify mutual suppression (logistic coefficients −0.49 and −1.04) and confirm that high-degree variables win tiebreakers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do human subjects show the same preference patterns for simplification-based vs. backtracking-based explanations as GPT-4o simulations?
- Basis in paper: [explicit] "These pilot results...now enable us to plan a series of experiments at our home institutions involving undergraduates in mathematics, computer science, and logic."
- Why unresolved: The current study uses only LLM simulations as proof of concept; human validation is required.
- What evidence would resolve it: Running the same SAT experiments with human subjects and comparing reason citation rates for unit clauses, resolution steps, and backtracked variables.

### Open Question 2
- Question: How does mathematical expertise affect contradiction-handling and explanation generation?
- Basis in paper: [explicit] "The relationship between expertise and contradiction-handling remains unexplored. Future studies could investigate whether experts show systematic differences in how they leverage contradictions, moving beyond local navigation via backtracking to extract broader structural insights about the proof space."
- Why unresolved: No data exists comparing novices versus experts on how they use backtracking errors as explanatory reasons.
- What evidence would resolve it: Comparative studies measuring explanation patterns in novices versus trained mathematicians solving equivalent SAT problems.

### Open Question 3
- Question: How do high-symmetry problems affect explanation quality and whether subjects find facts "explainable at all"?
- Basis in paper: [explicit] "We expect similar effects to obtain for human reasoners, with knock-on effects for both the reasons they give to explain the solution, and the extent to which they consider the facts, lost as they are in a sea of uniformity, explainable at all."
- Why unresolved: The study tested only problems with identifiable structure (unit clauses, resolution pairs); highly symmetric problems remain untested.
- What evidence would resolve it: Experiments using symmetric SAT instances with no clear influence hierarchy, measuring whether subjects can still articulate reasons and what linguistic patterns emerge.

### Open Question 4
- Question: To what extent do GPT-4o's reasoning patterns faithfully approximate human deductive explanation?
- Basis in paper: [inferred] "We emphasise that these experiments do not count as evidence in favor of actual human performance, but rather as proof of concept."
- Why unresolved: LLMs may exhibit systematic biases or capabilities that differ from human cognition; no validation data exists.
- What evidence would resolve it: Direct comparison studies where humans and GPT-4o solve identical problems with reasoning protocols analyzed for convergence/divergence in explanation strategies.

## Limitations

- The study uses GPT-4o as a proxy for human reasoning rather than actual human subjects, explicitly acknowledged as a "proof of concept"
- Experimental scope is narrow (4-variable SAT problems with unique solutions), limiting generalizability to real-world reasoning
- Corpus analysis reveals minimal direct support for the proposed mechanisms, with related work focusing on different aspects of explanation generation

## Confidence

- **High Confidence**: The logistic regression results showing variable influence (degree) as a consistent positive predictor across all reason types (+1.39 to +1.46 coefficients, p ≪ 10⁻³). The qualitative distinction between simplification-based and backtracking-based explanation language is well-supported by the linguistic analysis.
- **Medium Confidence**: The core claims about simplification and backtracking mechanisms (52-68% for unit clauses, 36-38% for backtracking) given they rely on LLM simulation rather than human subjects. The interference effects between competing reasons (-0.49 and -1.04 coefficients) are plausible but require human validation.
- **Low Confidence**: The assertion that these mechanisms capture actual human explanation generation processes. The corpus signals are weak (average neighbor FMR=0.52, zero citations for most related work), suggesting limited external validation of the theoretical framework.

## Next Checks

1. **Human Subject Replication**: Run the same SAT problem protocol with human participants to verify whether explanation patterns match the GPT-4o simulation results (52-68% for simplification variables, 36-38% for backtracking variables).

2. **Cross-Domain Transfer**: Test whether the simplification-backtracking framework applies to non-logical domains like arithmetic word problems or causal reasoning tasks, where error-based and simplification-based explanations might also emerge.

3. **Mechanism Isolation**: Design problems that eliminate one mechanism at a time (e.g., problems that always have lucky paths with no backtracking, or problems with uniform variable degree) to confirm that each mechanism degrades as predicted when its enabling conditions are removed.