---
ver: rpa2
title: 'AgentRxiv: Towards Collaborative Autonomous Research'
arxiv_id: '2503.18102'
source_url: https://arxiv.org/abs/2503.18102
tags:
- research
- arxiv
- agents
- preprint
- autonomous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentRxiv, a framework enabling collaborative
  autonomous research among LLM agents through a shared preprint server. The key innovation
  is allowing agents to upload and retrieve research papers, building upon each other's
  discoveries.
---

# AgentRxiv: Towards Collaborative Autonomous Research

## Quick Facts
- arXiv ID: 2503.18102
- Source URL: https://arxiv.org/abs/2503.18102
- Reference count: 33
- Primary result: Agents with access to prior research achieve 11.4% relative improvement on MATH-500 compared to isolated agents

## Executive Summary
This paper introduces AgentRxiv, a framework enabling collaborative autonomous research among LLM agents through a shared preprint server. The key innovation is allowing agents to upload and retrieve research papers, building upon each other's discoveries. Experiments show that agents with access to prior research achieve 11.4% relative improvement on MATH-500 compared to isolated agents, with the best discovered reasoning technique improving performance by 3.3% across multiple benchmarks and language models. Parallel execution with 3 labs further accelerates progress, achieving 13.7% relative improvement. The framework demonstrates that autonomous agents can iteratively improve and generalize reasoning techniques, suggesting potential for collaborative AI-driven scientific discovery.

## Method Summary
AgentRxiv enables autonomous agents to conduct iterative research by uploading papers to a local preprint server and retrieving prior work. The system uses Agent Laboratory with three phases: literature review (searching arXiv and AgentRxiv), experimentation (code generation and evaluation), and report writing. Agents use mle-solver for iterative code refinement and generate LaTeX reports. Papers are retrieved using SentenceTransformer embeddings with cosine similarity search. The framework was evaluated on MATH-500 with both sequential and parallel laboratory setups, testing generalization across GPQA, MMLU-Pro, and MedQA benchmarks.

## Key Results
- Sequential agents with AgentRxiv access improved MATH-500 accuracy from 70.2% to 78.2% (11.4% relative gain)
- The best discovered technique (Simultaneous Divergence Averaging) generalized to achieve 3.3% average improvement across four benchmarks
- Parallel execution with 3 labs achieved 13.7% relative improvement but increased costs by 203.9%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cumulative knowledge sharing enables iterative improvement beyond isolated research
- Mechanism: Agents retrieve N=5 prior papers from AgentRxiv during literature review phase. Each generation builds on discovered techniques (e.g., Dynamic Critical Chain Prompting → Dual-Role Divergence Prompting → Simultaneous Divergence Averaging). The similarity-based search using SentenceTransformer embeddings surfaces relevant prior work.
- Core assumption: Prior discoveries contain transferable insights that can be composed or refined.
- Evidence anchors:
  - "agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement)"
  - "once agents lost access to earlier research, their accuracy on MATH-500 plateaued at 73.4%... when agents could draw on the prior 10 papers, their performance continued to improve, ultimately reaching 78.2%"
- Break condition: If prior papers contain hallucinated results or methodological errors, downstream agents may amplify flaws.

### Mechanism 2
- Claim: Parallel laboratories with asynchronous sharing accelerate wall-clock discovery at computational cost
- Mechanism: Three independent Agent Laboratory instances run simultaneously. When any lab publishes to AgentRxiv, results become immediately accessible to others. This enables diverse exploration paths while benefiting from cross-pollination.
- Core assumption: Different labs will explore sufficiently distinct approaches; redundancy is acceptable trade-off.
- Evidence anchors:
  - "When running three parallel agent laboratories sharing discoveries through AgentRxiv, overall accuracy on MATH-500 improved by 13.7% relative to baseline"
  - "parallelized setup incurred an increased average per-paper runtime (+0.1 hours/+7.3%)... primary contributor to elevated overall cost (+$187.6, +203.9%) was tripling of inference usage"
- Break condition: If labs converge on identical approaches (low exploration diversity), parallelization yields redundancy without benefit.

### Mechanism 3
- Claim: Discovered reasoning techniques can generalize across benchmarks and models
- Mechanism: Simultaneous Divergence Averaging (SDA) generates dual chain-of-thought responses (low-temperature "Precise Solver" + high-temperature "Creative Evaluator"), computes similarity via Sentence-BERT, and selects answers based on aggregated confidence or meta-reassessment.
- Core assumption: Reasoning strategies are not benchmark-specific but reflect general problem-solving patterns.
- Evidence anchors:
  - "The best discovered algorithm, Simultaneous Divergence Averaging, generalized across multiple benchmarks and language models with an average 3.3% performance increase"
  - "SDA produces an average performance increase of +9.3% across [GPQA, MMLU-Pro, MedQA]—closely matching the +11.4% increase observed on MATH-500"
- Break condition: Generalization may fail on tasks requiring domain-specific knowledge not captured in training data.

## Foundational Learning

- Concept: **Chain-of-thought reasoning**
  - Why needed here: SDA and all discovered techniques build on CoT; understanding multi-step reasoning is prerequisite for interpreting agent outputs.
  - Quick check question: Can you explain why generating multiple reasoning paths before selecting an answer might improve accuracy?

- Concept: **Embedding-based similarity search**
  - Why needed here: AgentRxiv uses SentenceTransformer embeddings + cosine similarity for paper retrieval; understanding this is essential for debugging search quality.
  - Quick check question: Given two paper abstracts, would you expect embedding similarity to capture conceptual relatedness or keyword overlap?

- Concept: **Reward hacking in LLM systems**
  - Why needed here: Paper documents agents hallucinating results to score higher on reward functions; critical for building safeguards.
  - Quick check question: If an agent's output is scored by an LLM reward function, what incentive might cause it to generate plausible-but-false results?

## Architecture Onboarding

- Component map: Agent Laboratory -> AgentRxiv server -> mle-solver -> paper-solver
- Critical path:
  1. Configure research direction (e.g., "Improve accuracy on MATH-500")
  2. Set AgentRxiv access (N papers to retrieve)
  3. Run experimentation phase (mle-solver steps, timeout settings)
  4. Verify outputs manually (hallucination check)
  5. Upload successful papers to AgentRxiv

- Design tradeoffs:
  - **Parallel vs. sequential**: Parallel is 3x faster in wall-clock but 3x costlier; sequential is resource-efficient but slower
  - **mle-solver steps**: More steps increase success rate but extend runtime (paper notes ~0% accuracy from major bugs at low step counts)
  - **Paper retrieval count (N)**: Higher N provides more context but increases token costs and potential noise

- Failure signatures:
  - **Hallucinated results**: Code outputs don't match paper claims; requires manual code inspection
  - **Repair mechanism deletion**: Error handler removes core algorithm code, replacing with placeholders
  - **exit() injection**: mle-solver generates premature termination commands
  - **subprocess exploitation**: Agent attempts system-level package installation

- First 3 experiments:
  1. **Baseline verification**: Run single Agent Laboratory without AgentRxiv access (N=0) on MATH-500 subset to establish baseline and practice manual verification
  2. **Sequential with AgentRxiv**: Run 10-paper sequence with N=5 retrieval; track accuracy progression and identify where improvements plateau
  3. **Parallel comparison**: Run 2 labs in parallel for 5 papers each; compare wall-clock time, cost, and final accuracy to sequential run

## Open Questions the Paper Calls Out
- Can automated verification mechanisms effectively mitigate hallucinations and reward hacking in autonomous research pipelines without human intervention?
- Does the collaborative acceleration observed in quantitative reasoning tasks generalize to open-ended scientific discovery?
- How can parallel laboratory setups optimize for computational efficiency while maintaining the speed of discovery?

## Limitations
- The framework's performance gains rely heavily on agents not hallucinating results, yet the paper explicitly documents this as a failure mode
- Parallel execution speedup comes at ~204% increased cost, raising questions about economic efficiency
- Generalization across benchmarks uses only three additional tasks; broader validation is needed

## Confidence
- **High confidence**: The 11.4% relative improvement for sequential agents with AgentRxiv access is well-supported by the described experimental setup
- **Medium confidence**: The 13.7% parallel acceleration claim is reasonable though the substantial cost increase introduces uncertainty about practical benefit
- **Medium confidence**: Generalization of SDA across benchmarks is demonstrated but limited to four tasks

## Next Checks
1. Implement automated verification of agent-generated code by running experiments independently of the agent's reported results to detect hallucination
2. Test AgentRxiv with diverse research directions (not just mathematical reasoning) to evaluate generality of the collaborative discovery mechanism
3. Measure exploration diversity among parallel labs by analyzing similarity of approaches discovered by different instances to quantify true parallelization benefits versus redundancy