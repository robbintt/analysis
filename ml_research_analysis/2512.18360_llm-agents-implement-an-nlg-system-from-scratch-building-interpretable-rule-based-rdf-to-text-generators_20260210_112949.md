---
ver: rpa2
title: 'LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based
  RDF-to-Text Generators'
arxiv_id: '2512.18360'
source_url: https://arxiv.org/abs/2512.18360
tags:
- system
- triples
- text
- output
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel neurosymbolic framework for RDF-to-text
  generation that uses multiple LLM agents to collaboratively design and implement
  a fully interpretable, rule-based Python NLG system without supervised training
  data. The approach iteratively builds the system through high-level design by a
  Software Architect, function implementation by a Software Engineer, evaluation via
  a Python execution engine, and analysis by a Code Analyst, using only RDF triples
  as input.
---

# LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators

## Quick Facts
- arXiv ID: 2512.18360
- Source URL: https://arxiv.org/abs/2512.18360
- Authors: Mateusz Lango; Ondřej Dušek
- Reference count: 36
- Key outcome: Novel neurosymbolic framework uses multiple LLM agents to collaboratively design and implement a fully interpretable, rule-based Python NLG system without supervised training data, achieving competitive performance on WebNLG and OpenDialKG datasets.

## Executive Summary
This paper introduces a novel neurosymbolic approach for RDF-to-text generation that leverages multiple specialized LLM agents to collaboratively design and implement a fully interpretable, rule-based Python NLG system. The framework iteratively builds the system through high-level design by a Software Architect, function implementation by a Software Engineer, evaluation via a Python execution engine, and analysis by a Code Analyst, using only RDF triples as input. The resulting systems achieve competitive performance on reference-based metrics while running nearly instantaneously on a single CPU and reducing hallucinations compared to fine-tuned or prompted LLMs.

## Method Summary
The method uses five specialized LLM agents in an iterative loop: Test Engineer generates synthetic unit tests from RDF triples, Software Architect creates high-level function specifications, Software Engineer implements the code, Evaluator executes it in a sandboxed environment with timeout and LLM assessment, and Code Analyst diagnoses failures to guide redesign or refactoring. Training runs per thematic category with max 25 iterations (10 for GPT), 3 random seeds, and selection by unit test pass count. Evaluation uses reference-based metrics (BLEU, METEOR, BERTScore, BLEURT) and LLM-as-a-Judge for grammaticality, hallucinations, and omissions.

## Key Results
- Achieved BLEU scores up to 0.39 and METEOR up to 0.71 on WebNLG dataset
- Significantly reduced hallucinations compared to fine-tuned or prompted LLMs
- Generated systems run nearly instantaneously on a single CPU
- Maintained full interpretability with human-readable Python code

## Why This Works (Mechanism)

### Mechanism 1: Agent Role Specialization
Specialized agent roles decompose system complexity by separating high-level design decisions from implementation details. The Software Architect focuses on code structure while the Software Engineer implements specific functions, with the Code Analyst providing a third perspective to diagnose errors.

### Mechanism 2: Iterative Test-Driven Refinement
An automated feedback loop with executable test suite enables self-correction. The Evaluator agent runs generated code and uses LLM judgment to identify semantic errors, creating a closed-loop optimization process.

### Mechanism 3: Compiling Neural Reasoning to Symbolic Code
The approach trades inference-time LLM calls for a one-time "training" phase to produce an efficient, standalone artifact. LLMs act as compilers translating domain knowledge into executable Python code that runs deterministically without neural network overhead.

## Foundational Learning

**Test-Driven Development (TDD):** The entire training loop is automated TDD. Understanding how writing tests first guides implementation is critical to grasping why the Software Engineer can write functional code without direct human oversight. Quick check: What is the primary source of the unit tests used to evaluate the generated NLG system?

**Knowledge Graphs & RDF Triples:** The input data format is RDF (Subject, Predicate, Object). Understanding this structured representation is necessary to see how the NLG system transforms discrete data into fluent text. Quick check: How does the system use the list of predicates extracted from the knowledge graph during the Software Architect phase?

**Neurosymbolic AI:** This paper is a classic example of neurosymbolic AI, combining the learning/reasoning capabilities of neural networks (LLMs) with the precision and interpretability of symbolic systems (rule-based code). Quick check: In this architecture, which component is the "neural" part and which is the "symbolic" part?

## Architecture Onboarding

**Component map:** Test Engineer -> Software Architect -> Software Engineer -> Evaluator -> Code Analyst (feedback loop to Software Engineer or Software Architect)

**Critical path:** Test Engineer creates tests → Software Architect defines design → Software Engineer implements code → Evaluator runs code against tests → if failure, Code Analyst diagnoses issue → feedback to SE (code fix) or SA (design fix) → repeat until all tests pass or iteration limit reached

**Design tradeoffs:** High setup computational cost (many LLM calls) for near-zero inference cost; generated text may be less varied than direct LLM output but more controllable and less prone to hallucination; system may overfit to specific RDF schemas seen during training

**Failure signatures:** Loop non-termination with continuous bug introduction, runtime errors at inference due to missing libraries or edge cases, syntactically correct but semantically empty output

**First 3 experiments:** 1) Ablation on Agent Roles - run with single monolithic agent vs multi-agent version; 2) Impact of LLM Judge Quality - swap LLM judge with smaller model and measure degradation; 3) Cross-Domain Generalization - train on WebNLG, test directly on OpenDialKG without retraining

## Open Questions the Paper Calls Out

### Open Question 1
What are the minimum capabilities an LLM must possess to successfully implement a complete NLG system through agent collaboration? Basis: Only four LLMs were tested with no systematic analysis of which specific capabilities drive success. Evidence needed: Controlled study varying LLM size, code training data, and reasoning abilities while measuring NLG system quality.

### Open Question 2
Can the multi-agent framework scale to more complex NLG domains beyond RDF-to-text, such as document-level or multimodal generation? Basis: Approach demonstrated only on RDF triple verbalization without discussion of generalization to tasks requiring discourse planning or cross-modal reasoning. Evidence needed: Applying framework to summarization, dialogue generation, or image-conditioned captioning with appropriate task definitions.

### Open Question 3
How robust is the approach to variations in input data format and schema across different knowledge graphs? Basis: Authors note evaluation on GEM datasets produced errors due to date formatting differences between datasets. Evidence needed: Testing on datasets with varied formatting, incomplete triples, or schema drift to measure failure modes.

## Limitations
- Generated code may be brittle and fail on out-of-distribution inputs not covered by synthetic tests
- System may overfit to specific RDF schemas seen during training
- Approach requires powerful LLMs, as smaller open-source models produced significantly lower quality

## Confidence

**High confidence:** System produces competitive reference-based metrics (BLEU, METEOR) and significantly reduces hallucinations compared to direct LLM prompting, supported by quantitative results on two datasets

**Medium confidence:** "Near-instantaneous" inference speed claim, as stated but specific hardware details not provided in main text

**Low confidence:** "Full interpretability" as unique advantage, as interpretability is limited to readable code rather than explainability of text output fidelity

## Next Checks

1. **Judge Reliability Analysis:** Conduct ablation study where human expert evaluates subset of code outputs and compares verdicts to LLM judge's assessments to quantify false-positive and false-negative rates

2. **Runtime Benchmark Verification:** Replicate inference-time performance benchmark on same hardware (CPU type, clock speed, memory) to verify "nearly instantaneous" claim and ensure fair comparison with neural models

3. **Robustness to Schema Changes:** Test trained NLG system on dataset with modified RDF schema (different predicate names or structures) to assess brittleness to real-world input format variations