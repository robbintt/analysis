---
ver: rpa2
title: 'LLEXICORP: End-user Explainability of Convolutional Neural Networks'
arxiv_id: '2511.02720'
source_url: https://arxiv.org/abs/2511.02720
tags:
- concept
- image
- pattern
- concepts
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLEXICORP, a modular pipeline that integrates
  Concept Relevance Propagation (CRP) with a multimodal large language model to automatically
  generate human-understandable explanations for CNN predictions. The method addresses
  the manual and expert-dependent nature of existing CRP workflows by using LLM prompts
  to automatically name discovered concepts and contextualize their relevance in predictions.
---

# LLEXICORP: End-user Explainability of Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2511.02720
- Source URL: https://arxiv.org/abs/2511.02720
- Reference count: 11
- Automates human-readable explanations for CNN predictions by combining CRP with LLM

## Executive Summary
LLEXICORP introduces a modular pipeline that integrates Concept Relevance Propagation (CRP) with a multimodal large language model to automatically generate human-understandable explanations for CNN predictions. The method addresses the manual and expert-dependent nature of existing CRP workflows by using LLM prompts to automatically name discovered concepts and contextualize their relevance in predictions. The framework supports both technical and non-technical audiences. Evaluation on 8 ImageNet images via a questionnaire with 21 participants showed that participants consistently recognized meaningful visual patterns (78% agreement) and found the final summaries helpful (75%). The study demonstrates that LLM-enhanced CRP can significantly lower the barrier to interpreting deep neural networks.

## Method Summary
LLEXICORP combines CRP with a multimodal LLM to automate the generation of human-readable explanations for CNN predictions. The pipeline executes in three stages: first, CRP identifies the top 5 relevant channels and their heatmaps for an input image; second, it retrieves the top 6 "representative images" (maximally activating images) for each channel; third, an LLM processes these through a three-stage prompting workflow—labeling concepts using representative images, contextualizing them by mapping to the input image and class prediction using specific decision rules, and synthesizing findings into a final summary. The framework was evaluated on ImageNet using VGG16 and GPT-4o, with human participants assessing the quality and helpfulness of the generated explanations.

## Key Results
- Participants consistently recognized meaningful visual patterns in the most influential concept (78% agreement)
- The final summary was rated as helpful by 75% of participants
- The most influential concept achieved over 90% agreement on reasonable presence and usefulness

## Why This Works (Mechanism)
LLEXICORP works by automating the traditionally manual process of naming and contextualizing concepts discovered through CRP. By leveraging a multimodal LLM, the system can generate human-readable labels for abstract visual patterns and relate them to specific image regions and class predictions. The three-stage prompting approach ensures systematic processing from concept identification to final synthesis, while the use of representative images helps the LLM ground abstract concepts in concrete visual examples.

## Foundational Learning
- **Concept Relevance Propagation (CRP)**: A method for identifying which features in a CNN are most relevant to a prediction by analyzing activation patterns; needed to discover interpretable visual concepts within the network.
- **Multimodal LLMs**: Models that can process both text and images, enabling them to interpret visual patterns and generate natural language explanations.
- **Representative Images**: Maximally activating images for specific channels that serve as concrete examples to help LLMs identify and name abstract visual concepts.
- **Prompt Engineering**: The design of specific instructions for LLMs to ensure consistent and useful outputs for concept naming and contextualization.
- **Saliency Maps**: Visualizations showing which regions of an image contributed most to a prediction, used here to link discovered concepts to specific image areas.

## Architecture Onboarding

**Component Map:**
VGG16 -> CRP Analyzer -> Channel Heatmaps -> Representative Image Retriever -> GPT-4o LLM -> Explanation Summary

**Critical Path:**
Image input → CRP execution (top 5 channels) → Representative image retrieval (top 6 per channel) → Concept naming (LLM) → Contextualization (LLM with heatmaps) → Final summary (LLM)

**Design Tradeoffs:**
- Fixed k=5 channels balances comprehensiveness against explanation complexity
- Representative image retrieval adds computational overhead but improves concept naming accuracy
- Three-stage prompting increases latency but ensures systematic processing
- Manual prompt design requires expert knowledge but enables precise control over outputs

**Failure Signatures:**
- Poor concept localization (low heatmap-to-text alignment)
- Non-faithful summaries introducing external knowledge
- Inconsistent concept naming across similar patterns
- Overly technical language that doesn't serve end-users

**First Experiments:**
1. Test CRP on a single VGG16 layer with known interpretable features
2. Validate representative image retrieval by checking activation similarity
3. Run end-to-end pipeline on a simple image with obvious features

## Open Questions the Paper Calls Out
1. **Concept Filtering and Aggregation:** How can the pipeline be modified to filter or aggregate lower-ranked concepts to prevent explanation clutter? The current implementation summarizes all top-5 concepts regardless of individual quality, creating a risk of cluttering the explanation space without significantly improving interpretability.

2. **Generalization to Other Architectures:** Does the framework generalize to non-VGG architectures and domains outside of ImageNet? The method and evaluation are restricted to VGG16 and natural images, without testing if prompt engineering strategies hold for architectures with skip connections or different data modalities.

3. **Refining Contextualization Accuracy:** Can the "contextualization" step be refined to improve the accuracy of textual localization? The current LLM rules allow for subjective interpretations of alignment between concept descriptions and saliency maps, with only 61% agreement on highlighted areas.

## Limitations
- Small evaluation sample size (8 images, 21 participants) limits generalizability
- Missing complete prompt text in appendix prevents exact reproduction
- Reliance on retrieval database of maximally activating images introduces potential bias
- Lower agreement (61%) on highlighted area identification compared to other metrics

## Confidence

**High Confidence:** The core pipeline combining CRP with LLM prompting is well-specified and technically sound.

**Medium Confidence:** The qualitative evaluation results are plausible given the methodology, but the small sample size introduces uncertainty about broader applicability.

**Low Confidence:** Without the exact prompts and retrieval database details, reproducing the precise LLM outputs and their impact on participant responses is challenging.

## Next Checks
1. Obtain and test the exact prompt text from authors to ensure LLM generates outputs consistent with reported results
2. Confirm the source and construction of the "representative images" retrieval database to understand potential biases
3. Implement and validate the specific zennit CRP settings used for VGG16 to ensure concept heatmaps match evaluation quality