---
ver: rpa2
title: 'Do not be greedy, Think Twice: Sampling and Selection for Document-level Information
  Extraction'
arxiv_id: '2601.18395'
source_url: https://arxiv.org/abs/2601.18395
tags:
- reasoning
- greedy
- selector
- selection
- better
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces THINKTWICE, a sampling and selection framework
  for document-level Information Extraction (DocIE) that leverages the output variability
  of decoder-only LLMs to achieve better performance than greedy decoding. Instead
  of generating a single output, the method produces multiple candidate templates
  for a given document and applies a selection module to identify the most suitable
  one.
---

# Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction

## Quick Facts
- arXiv ID: 2601.18395
- Source URL: https://arxiv.org/abs/2601.18395
- Reference count: 12
- Primary result: Sampling and selection framework improves DocIE performance over greedy decoding with up to 4 F1 points gain

## Executive Summary
This work introduces THINKTWICE, a sampling and selection framework for document-level Information Extraction (DocIE) that addresses the limitations of greedy decoding in decoder-only LLMs. The method generates multiple candidate templates for a document using sampling strategies and applies a selection module to identify the most suitable output. Experiments on MUC-4, MultiMUC, and BETTER datasets demonstrate consistent improvements over greedy baselines, with F1 gains up to 4 points in zero-shot settings and state-of-the-art results in supervised and cross-lingual scenarios.

## Method Summary
THINKTWICE leverages the output variability of decoder-only LLMs by generating multiple candidate templates instead of a single greedy output. The framework employs two selection strategies: an unsupervised F1 Voting method and a supervised reward-based approach trained on silver reasoning traces generated via rejection sampling. The method assumes availability of multiple LLM outputs and applies selection to identify the best candidate, showing consistent improvements over greedy decoding baselines across different datasets and settings.

## Key Results
- F1 gains up to 4 points in zero-shot settings compared to greedy decoding
- State-of-the-art results in supervised and cross-lingual scenarios
- Reasoning models consistently outperform non-reasoning counterparts
- Supervised selection further improves performance over unsupervised approaches

## Why This Works (Mechanism)
The framework exploits the inherent variability in LLM outputs when using sampling strategies instead of greedy decoding. By generating multiple candidate templates and applying intelligent selection, the method captures diverse perspectives on the same document. The selection module, particularly when trained with supervised rewards, can identify high-quality outputs that greedy decoding might miss. Reasoning models provide additional benefit as they generate more structured and traceable outputs suitable for training the reward model.

## Foundational Learning

**Decoder-only LLMs**: Language models that generate text sequentially from left to right without bidirectional context; needed for understanding the generation process and why greedy decoding may be suboptimal; quick check: can the model access future tokens during generation?

**Sampling strategies**: Techniques like temperature sampling and nucleus sampling that introduce variability in LLM outputs; needed to understand how multiple candidates are generated; quick check: does the sampling method control diversity vs quality tradeoff?

**Rejection sampling**: Method for generating high-quality training data by accepting/rejecting samples based on criteria; needed to understand how silver reasoning traces are collected; quick check: what acceptance threshold ensures quality without excessive rejection?

**Reward modeling**: Approach to learn selection criteria from examples; needed to understand supervised selection mechanism; quick check: does the reward model generalize to unseen document types?

## Architecture Onboarding

**Component map**: Document -> Sampling Strategy -> Multiple Templates -> Selection Module (F1 Voting or Reward Model) -> Final Output

**Critical path**: Sampling generates candidates → Selection module scores candidates → Best candidate chosen as final output

**Design tradeoffs**: Sampling increases computational cost and inference time but improves output quality; selection module adds complexity but enables better results than greedy decoding alone

**Failure signatures**: Poor reasoning trace quality degrades reward model performance; excessive sampling may introduce noise without benefit; selection module may overfit to training distribution

**Exactly 3 first experiments**:
1. Compare performance with different sampling temperatures to find optimal diversity-quality balance
2. Ablate the reward model to measure impact of reasoning trace quality on selection accuracy
3. Test selection module with varying numbers of candidate templates to determine optimal candidate count

## Open Questions the Paper Calls Out
None

## Limitations
- Selection module effectiveness depends heavily on quality of reasoning traces used for training
- Method assumes availability of multiple LLM outputs, increasing computational cost and inference time
- F1 Voting strategy performance not systematically compared against other unsupervised baselines
- Improvement margin in supervised settings (1-2 F1 points) may not justify additional complexity for all applications

## Confidence

**High confidence**: Core methodology of generating multiple templates and applying selection module works as described

**Medium confidence**: Relative advantage of reasoning models over non-reasoning models is consistent across datasets

**Medium confidence**: Cross-lingual transfer results are robust given reported performance

## Next Checks
- Conduct ablation studies to quantify impact of reasoning trace quality on reward model selection accuracy
- Measure and report actual inference time and computational overhead compared to greedy decoding baseline
- Test framework with non-reasoning LLMs to determine if sampling and selection approach provides benefits independent of model reasoning capabilities