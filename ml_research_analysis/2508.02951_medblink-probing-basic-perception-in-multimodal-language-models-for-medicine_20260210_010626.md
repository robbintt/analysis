---
ver: rpa2
title: 'MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine'
arxiv_id: '2508.02951'
source_url: https://arxiv.org/abs/2508.02951
tags:
- medical
- tasks
- images
- task
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedBLINK, a benchmark designed to evaluate
  the basic perceptual abilities of multimodal language models (MLMs) in medicine.
  MedBLINK consists of eight clinically meaningful tasks, including image orientation
  detection, depth estimation, and contrast enhancement identification, covering 1,429
  multiple-choice questions over 1,605 images.
---

# MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine

## Quick Facts
- **arXiv ID:** 2508.02951
- **Source URL:** https://arxiv.org/abs/2508.02951
- **Reference count:** 40
- **Primary result:** Current multimodal language models achieve only 65% accuracy on basic perceptual tasks in medicine, compared to 96.4% for human experts.

## Executive Summary
This paper introduces MedBLINK, a benchmark designed to evaluate the basic perceptual abilities of multimodal language models (MLMs) in medicine. MedBLINK consists of eight clinically meaningful tasks, including image orientation detection, depth estimation, and contrast enhancement identification, covering 1,429 multiple-choice questions over 1,605 images. The authors evaluate 19 state-of-the-art MLMs, including general-purpose and domain-specific models, and find that while human annotators achieve 96.4% accuracy, the best-performing model reaches only 65%. The results demonstrate that current MLMs frequently fail at routine perceptual checks, suggesting the need to strengthen their visual grounding to support clinical adoption.

## Method Summary
The evaluation uses 1,429 multiple-choice questions over 1,605 medical images across eight tasks: Image Enhancement Detection, Visual Depth Estimation, Wave-Based Imaging Depth Estimation, Histology Structure, Imaging Orientation (CXR/Pelvis), Relative Position (CT slices), Morphology Quantification (counting teeth), and Age Estimation. The benchmark sources images from datasets including VidDr Multiphase, Kvasir, EchoNet-Dynamic, ChestX-ray8, and Panoramic dental radiography. Evaluation is performed zero-shot with temperature=0 and retries=5, using visual prompting for depth/histology tasks with colored dots. The study compares 19 models including GPT-4o, Claude 3.5 Sonnet, LLaVA variants, and medical-specific models like Med-Flamingo and RadFM against human expert performance.

## Key Results
- Human experts achieve 96.4% accuracy on MedBLINK tasks
- Best-performing model (GPT-4o) reaches only 65% accuracy
- Medical-specific models underperform general models (43.69-47.47% vs 64.99%)
- GPT-4o achieves near-perfect accuracy (98%) on natural image orientation but drops to 36% on flipped pelvic X-rays
- Models frequently default to color-based heuristics (e.g., always predicting "red" dot) rather than visual perception

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing for high-level diagnostic correlations may not induce low-level perceptual grounding.
- **Mechanism:** Medical MLMs are trained on VQA pairs (e.g., "Is there a tumor?") where the answer can often be inferred from metadata or high-level textures rather than precise geometric relations. Consequently, models achieve high diagnostic accuracy by learning dataset-specific priors or spurious correlations instead of internalizing the physical properties of imaging.
- **Evidence anchors:** Medical-specific MLMs underperform general models (43.69-47.47% vs 64.99%), suggesting they may develop "spurious correlations on diagnostic tasks rather than meaningful medical perceptual understanding."

### Mechanism 2
- **Claim:** Spatial reasoning capabilities learned from natural images do not zero-shot transfer to the physics of medical imaging.
- **Mechanism:** General-purpose models successfully determine orientation in natural images by detecting gravity-aligned cues, but medical images like X-rays rely on anatomical asymmetries rather than environmental gravity cues. The mechanism fails because the model's visual encoder lacks the domain-specific priors to map pixel intensity to 3D anatomical structure.
- **Evidence anchors:** GPT-4o achieves near-perfect accuracy (98%) on natural image orientation but drops to 36% on flipped pelvic X-rays.

### Mechanism 3
- **Claim:** MLMs compensate for lack of visual grounding by exploiting linguistic or color-based heuristics in multiple-choice settings.
- **Mechanism:** When visual features are ambiguous or ungrounded, models default to simple statistical patterns in the prompt or visual prompt. For instance, if a model cannot perceive depth in an ultrasound, it may associate specific colors with "closest" based on training data biases, rather than analyzing the spatial position of the dot.
- **Evidence anchors:** Models frequently default to predicting the "red dot" or "blue dot" regardless of actual position in depth estimation tasks.

## Foundational Learning

- **Concept: Visual Grounding**
  - **Why needed here:** The core thesis is that current models lack visual groundingâ€”they generate text that is plausible but not actually tied to the visual reality of the image.
  - **Quick check question:** Can the model link the word "left" in its output to the specific pixels representing the left lung?

- **Concept: "Blink" Tasks (System 1 vs. System 2)**
  - **Why needed here:** The benchmark is named "MedBLINK" to represent tasks that experts solve instantly (System 1: intuition/perception) rather than through deliberate reasoning (System 2).
  - **Quick check question:** Does the task require complex deduction or immediate pattern recognition?

- **Concept: Imaging Physics (Wave-based vs. Visual)**
  - **Why needed here:** MedBLINK distinguishes between standard 2D imaging and wave-based imaging. Understanding that ultrasound "depth" requires understanding acoustic cones is critical for the "Wave-Based Imaging Depth Estimation" task.
  - **Quick check question:** In an ultrasound, is a feature at the top of the image closer to or further from the probe compared to a feature at the bottom?

## Architecture Onboarding

- **Component map:** Medical Image + Visual Prompts -> Vision Encoder (ViT) -> Projector -> LLM Backbone -> Answer
- **Critical path:**
  1. Data Curation: Use MedBLINK's 8 tasks as the evaluation set
  2. Visual Prompting: Ensure the architecture can handle coordinate-based visual markers (dots)
  3. Inference: Run Zero-Shot evaluation first to establish baseline perceptual capabilities
- **Design tradeoffs:**
  - Domain Specificity: Training on medical text improves diagnostic jargon but may degrade general perceptual robustness
  - Resolution: Increasing resolution showed negligible gains, suggesting the failure is in the features learned, not pixel fidelity
- **Failure signatures:**
  - Color Bias: Model accuracy fluctuates wildly based on the color of the dot
  - Orientation Blindness: 50% accuracy on flipped images indicates the model treats the image as a bag of features
  - Hallucinated Reasoning: Model claims "The heart is on the left, so this is correct" for an image where the heart is actually on the right due to flipping
- **First 3 experiments:**
  1. Evaluate a general-purpose MLM on the "Image Orientation" task to confirm the transfer gap between natural and medical images
  2. Run the "Depth Estimation" task with randomized dot colors to test if the model relies on color heuristics
  3. Compare "High" vs. "Low" resolution inference on "Morphology Quantification" to verify if the failure is due to blurred features or conceptual inability

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation framework relies on multiple-choice questions, which may not fully capture the nuanced perceptual capabilities required in clinical practice
- The study does not provide the exact visual prompt coordinates for the 1,605 images, requiring assumptions about the dataset distribution
- The specific few-shot examples used for Med-Flamingo are not explicitly listed, which could affect reproducibility

## Confidence

- **High Confidence:** The core claim that current MLMs struggle with basic perceptual tasks is well-supported by empirical results showing significant performance gaps between human experts (96.4%) and the best models (65%)
- **Medium Confidence:** The mechanism explaining why medical-specific models underperform general models (due to spurious correlations in diagnostic tasks) is plausible but requires further validation
- **Low Confidence:** The claim that spatial reasoning capabilities from natural images do not transfer to medical imaging is based on limited evidence and may not hold for all types of medical images or tasks

## Next Checks

1. **Controlled Transfer Experiment:** Design a study to explicitly test whether training on medical perceptual tasks (e.g., depth maps, orientation labels) alongside diagnostic text improves model performance on MedBLINK tasks.

2. **Visual Prompt Ablation Study:** Conduct an experiment to determine if models rely on color heuristics by randomizing the colors of visual prompts in depth estimation tasks and analyzing the impact on accuracy.

3. **Resolution and Feature Analysis:** Investigate whether increasing image resolution or enhancing specific features (e.g., contrast) improves model performance on morphology quantification tasks, to isolate whether the failure is due to pixel fidelity or conceptual understanding.