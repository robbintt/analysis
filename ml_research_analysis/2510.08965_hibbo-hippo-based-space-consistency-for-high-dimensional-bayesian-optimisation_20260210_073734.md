---
ver: rpa2
title: 'HiBBO: HiPPO-based Space Consistency for High-dimensional Bayesian Optimisation'
arxiv_id: '2510.08965'
source_url: https://arxiv.org/abs/2510.08965
tags:
- space
- latent
- optimisation
- data
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the distribution mismatch problem in VAE-based
  high-dimensional Bayesian Optimization (BO). While VAEs reduce dimensionality by
  learning latent representations, reconstruction-only loss fails to preserve kernel
  relationships between data points, leading to suboptimal surrogate models.
---

# HiBBO: HiPPO-based Space Consistency for High-dimensional Bayesian Optimisation
## Quick Facts
- arXiv ID: 2510.08965
- Source URL: https://arxiv.org/abs/2510.08965
- Authors: Junyu Xuan; Wenlong Chen; Yingzhen Li
- Reference count: 20
- Key outcome: HiBBO introduces HiPPO-based space consistency to VAE training for high-dimensional Bayesian optimization, achieving better convergence and solution quality across four benchmarks compared to existing VAE-BO methods.

## Executive Summary
This paper addresses the distribution mismatch problem in VAE-based high-dimensional Bayesian Optimization (BO). While VAEs reduce dimensionality by learning latent representations, reconstruction-only loss fails to preserve kernel relationships between data points, leading to suboptimal surrogate models. The authors propose HiBBO, which introduces HiPPO-based space consistency into VAE training by adding a regularizer that preserves HiPPO memory representations of both original and reconstructed data sequences. This indirectly maintains kernel distances between data points. Experiments across four benchmarks (Ackley function, MNIST, shape optimization, and molecular design) demonstrate that HiBBO outperforms existing VAE-BO methods in convergence speed and solution quality, with the molecular design task achieving the highest logP score with the lowest query budget. The method is lightweight, general, and compatible with existing VAE architectures.

## Method Summary
HiBBO addresses the distribution mismatch problem in VAE-based high-dimensional Bayesian Optimization by introducing a HiPPO-based space consistency regularizer into VAE training. The key insight is that standard VAE reconstruction loss alone fails to preserve kernel relationships between data points in high-dimensional spaces. HiBBO adds a regularizer that maintains the HiPPO memory representations of both original and reconstructed data sequences, which indirectly preserves the kernel distances between data points. This approach ensures that the latent space better reflects the true data geometry, leading to improved surrogate model quality and more efficient optimization. The method is designed to be lightweight and compatible with existing VAE architectures, making it a practical enhancement to standard VAE-BO pipelines.

## Key Results
- HiBBO outperforms existing VAE-BO methods in convergence speed and solution quality across four benchmarks
- In molecular design tasks, HiBBO achieves the highest logP score with the lowest query budget
- The method demonstrates consistent improvements across diverse problem domains including Ackley function, MNIST, shape optimization, and molecular design

## Why This Works (Mechanism)
The paper introduces HiPPO-based space consistency as a regularizer in VAE training to address distribution mismatch in high-dimensional Bayesian Optimization. Standard VAEs trained with reconstruction loss alone fail to preserve kernel relationships between data points in the latent space. By adding a regularizer that maintains HiPPO memory representations of both original and reconstructed sequences, HiBBO indirectly preserves the kernel distances between data points. This ensures that the latent space better reflects the true data geometry, leading to improved surrogate model quality and more efficient optimization. The approach leverages the ability of HiPPO memory to capture temporal and sequential dependencies, which helps maintain the structural relationships needed for effective Bayesian optimization in the reduced-dimensional space.

## Foundational Learning
**Bayesian Optimization (BO)**: Sequential optimization technique for expensive black-box functions; needed for efficiently optimizing high-dimensional, computationally expensive objectives; quick check: verify BO uses surrogate models (typically GPs) to guide exploration-exploitation trade-off.

**Variational Autoencoders (VAEs)**: Generative models that learn compressed latent representations; needed for dimensionality reduction in high-dimensional BO; quick check: confirm VAEs optimize reconstruction + KL divergence objectives.

**HiPPO Memory**: Algorithm for efficient online compression of continuous-time signals; needed for capturing temporal/sequential dependencies in data; quick check: verify HiPPO maintains long-term memory through orthogonal polynomials.

**Distribution Mismatch**: Problem where latent space representations don't preserve original data relationships; needed context for why standard VAEs fail in BO; quick check: confirm kernel distances are not preserved under reconstruction-only training.

**Kernel Methods in BO**: Use of kernel functions (e.g., RBF) to measure similarity between data points; needed because BO surrogate models rely on accurate distance metrics; quick check: verify surrogate model performance depends on faithful representation of data geometry.

## Architecture Onboarding
Component map: Original Data -> VAE Encoder -> Latent Space -> VAE Decoder -> Reconstructed Data -> HiPPO Regularizer -> Loss Function
Critical path: VAE Encoder -> Latent Space -> VAE Decoder -> HiPPO Regularizer -> Loss Function
Design tradeoffs: Standard VAE-BO uses reconstruction loss only vs. HiBBO adds HiPPO regularizer for space consistency; lightweight overhead vs. improved performance
Failure signatures: Poor surrogate model performance, slow convergence, degraded solution quality compared to baselines
First experiments:
1. Test HiBBO on synthetic Ackley function to verify convergence improvements
2. Validate molecular design performance on logP score maximization
3. Compare computational overhead against standard VAE-BO across different dimensionalities

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical justification for why HiPPO-based space consistency specifically addresses kernel preservation is not rigorously proven
- Computational overhead introduced by the HiPPO regularizer is not thoroughly characterized
- Experiments focus on synthetic and benchmark problems with limited real-world validation beyond molecular design

## Confidence
- High: The problem statement regarding distribution mismatch in VAE-BO is well-established and the proposed solution is technically coherent
- Medium: The experimental results demonstrate improvements over baselines, but the scope is limited to specific benchmark problems
- Low: The theoretical foundation connecting HiPPO memory representations to kernel distance preservation lacks rigorous proof

## Next Checks
1. Perform ablation studies removing the HiPPO regularizer to quantify its specific contribution to performance gains
2. Test the method on high-dimensional real-world optimization problems from domains like robotics control or drug discovery to validate practical utility
3. Analyze the computational overhead by comparing training times and memory usage against standard VAE-BO implementations across different dimensionalities