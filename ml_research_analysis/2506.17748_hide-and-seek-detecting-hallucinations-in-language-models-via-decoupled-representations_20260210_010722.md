---
ver: rpa2
title: 'HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations'
arxiv_id: '2506.17748'
source_url: https://arxiv.org/abs/2506.17748
tags:
- hide
- hallucination
- output
- detection
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting hallucinations in
  large language models, which generate fluent but factually incorrect content. The
  authors propose HIDE (Hallucination Detection via Decoupled Representations), a
  single-pass, training-free method that measures statistical decoupling between input
  and output representations using the Hilbert-Schmidt Independence Criterion (HSIC).
---

# HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations

## Quick Facts
- arXiv ID: 2506.17748
- Source URL: https://arxiv.org/abs/2506.17748
- Authors: Anwoy Chatterjee; Yash Goel; Tanmoy Chakrabarti
- Reference count: 16
- One-line primary result: Single-pass, training-free hallucination detection method using HSIC achieves ~29% AUC-ROC improvement over single-pass baselines and ~3% over multi-pass methods while reducing computation by ~51%

## Executive Summary
This paper addresses hallucination detection in large language models by proposing HIDE (Hallucination Detection via Decoupled Representations), a single-pass, training-free method that measures statistical decoupling between input and output representations using the Hilbert-Schmidt Independence Criterion (HSIC). HIDE extracts hidden-state embeddings of key tokens from both input and output sequences and computes a HIDE score reflecting representational dependence. Lower scores indicate hallucinations. The method is evaluated on four QA datasets using six open-source LMs, demonstrating strong performance improvements over both single-pass and multi-pass state-of-the-art methods while maintaining computational efficiency.

## Method Summary
HIDE detects hallucinations by measuring statistical decoupling between an LM's internal representations of input context and its generated output. The method extracts hidden states from a middle decoder layer for both input and output sequences, selects top-k salient tokens using KeyBERT, and computes an adapted HSIC score using RBF kernels. A lower score indicates greater statistical independence between input and output representations, suggesting hallucination. The approach requires only a single forward pass, making it computationally efficient compared to multi-pass methods that generate multiple outputs for comparison.

## Key Results
- Achieves ~29% relative improvement in AUC-ROC over the best single-pass baseline across models and datasets
- Improves ~3% over multi-pass state-of-the-art methods while reducing computation time by ~51%
- Performance is largely agnostic to layer and kernel selection, though optimal settings exist
- Successfully detects both faithfulness and factuality hallucinations across diverse model architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hallucinated outputs exhibit measurable statistical decoupling between the LM's internal representations of input context and generated output.
- **Mechanism:** When an LM generates grounded content, the hidden states of output tokens maintain strong statistical dependence with input token representations. During hallucination, this coupling weakens—the output generation process becomes increasingly independent of input representations, approaching the product of marginal distributions.
- **Core assumption:** The decoupling phenomenon generalizes across model architectures and hallucination types (both faithfulness and factuality).
- **Evidence anchors:**
  - [abstract] "Our approach leverages the hypothesis that hallucinations result from a statistical decoupling between an LM's internal representations of input context and its generated output."
  - [Section 4] "We hypothesize that: (i) Sound Generation... strong coupling should result in a substantially positive HSIC value. (ii) Hallucinated Generation... representations become increasingly independent... HSIC value close to zero."
  - [corpus] Weak external validation—neighbor papers explore attention patterns and uncertainty for hallucination detection but don't directly confirm the decoupling hypothesis.
- **Break condition:** Extremely short outputs (1-2 tokens) cannot provide sufficient samples for HSIC estimation, yielding uninformative scores.

### Mechanism 2
- **Claim:** HSIC with RBF kernels captures complex non-linear dependencies in high-dimensional hidden state spaces that simpler correlation measures miss.
- **Mechanism:** HSIC maps representations to Reproducing Kernel Hilbert Spaces via kernel functions, then computes the squared Hilbert-Schmidt norm of the cross-covariance operator. This detects any statistical dependence—not just linear—between input and output representations. RBF kernels are "characteristic," meaning HSIC(X,Y)=0 if and only if X and Y are truly independent.
- **Core assumption:** The hidden state geometry encodes semantic relationships that reflect grounding vs. hallucination.
- **Evidence anchors:**
  - [Section 3.1] "HSIC quantifies this dependence by comparing P_XY to the product of the marginals, P_X P_Y, within Reproducing Kernel Hilbert Spaces."
  - [Section 3.1, Lemma 1-2] Formal theoretical grounding: RBF kernels are characteristic, ensuring HSIC=0 iff independent.
  - [corpus] "Revisiting Hallucination Detection with Effective Rank-based Uncertainty" uses hidden state rank but not HSIC—alternative internal signal, different theoretical basis.
- **Break condition:** Poor kernel bandwidth selection (γ too high causes overfitting to minute differences; too low loses sensitivity).

### Mechanism 3
- **Claim:** Keyword-based token selection preserves semantic interpretability while enabling efficient HSIC computation with fixed sample sizes.
- **Mechanism:** KeyBERT selects top-k semantically salient tokens from both input and output sequences. This aligns sample sizes for X and Y random variables while focusing computation on tokens most likely to reflect grounding relationships. The adapted HSIC estimator (ĤSIC_HIDE) ensures numerical stability for any n≥1.
- **Core assumption:** Semantic keywords carry disproportionate signal about input-output dependence.
- **Evidence anchors:**
  - [Section 4.4.1] "We then extract a set of n_eff salient tokens... using KeyBERT, which acts as a maximal-marginal-relevance ranker."
  - [Section 7.1, Figure 3] Token budget of 15-20 achieves near-optimal performance with diminishing returns beyond.
  - [corpus] No direct comparison in neighbors—most methods use full sequences or sampling-based approaches.
- **Break condition:** Verbatim input copying in output causes artificial inflation of dependence scores, masking hallucinations.

## Foundational Learning

- **Concept: Hilbert-Schmidt Independence Criterion (HSIC)**
  - **Why needed here:** Core statistical tool for detecting dependence without assuming linearity or specific distribution forms.
  - **Quick check question:** Can you explain why HSIC with RBF kernels detects any statistical dependence, not just correlation?

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - **Why needed here:** Theoretical foundation enabling implicit feature mapping φ(x) where kernel k(x,x')=⟨φ(x),φ(x')⟩ captures non-linear relationships.
  - **Quick check question:** What property makes a kernel "characteristic," and why does it matter for independence testing?

- **Concept: Transformer Hidden State Layers**
  - **Why needed here:** HIDE extracts representations from specific decoder layers; middle layers often carry richer semantic information.
  - **Quick check question:** Why might middle layers be more informative for semantic consistency than early or final layers?

## Architecture Onboarding

- **Component map:** Token Extraction Module -> Hidden State Collector -> Kernel Computation -> HSIC Calculator -> Threshold Comparator
- **Critical path:** Input → Generate output (single pass) → Extract hidden states at layer ℓ → Select keywords → Compute RBF kernels → Calculate ĤSIC_HIDE → Compare to threshold τ
- **Design tradeoffs:**
  - **Keyword vs. SVD alignment:** Keywords offer interpretability; SVD guarantees orthogonality. Paper shows near-identical performance (Figure 8).
  - **Layer selection:** Performance agnostic to layer choice (Figure 4), but middle layers (ℓ_mid) used as default.
  - **Token budget (n_eff):** 15-20 sufficient; larger values add computation without gains.
  - **Biased vs. adapted estimator:** Adapted ĤSIC_HIDE produces better score separability (0-0.25 range) vs. biased estimator (max 2.74×10⁻¹¹).
- **Failure signatures:**
  - **Single-token outputs:** ĤSIC_HIDE = 0 (uninformative)
  - **Input copying in output:** Artificially inflated scores mask hallucinations
  - **High γ values (≥10⁻¹):** Overfitting to noise, poor generalization
- **First 3 experiments:**
  1. **Sanity check:** Run HIDE on SQuAD with Llama-3-8B, verify AUC-ROC ~76-77% matches paper Table 2. Use n_eff=20, ℓ=middle layer, γ=10⁻⁵.
  2. **Layer ablation:** Sweep all decoder layers on held-out samples to confirm performance stability across layers.
  3. **Threshold calibration:** Determine optimal τ on validation split using G-Mean maximization; compare to reported τ_avg=0.12.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the HIDE framework be effectively adapted for black-box environments where access to the model's internal hidden states is restricted?
- **Basis in paper:** [explicit] The authors explicitly list extending the approach to black-box settings as a primary avenue for future research in the "Limitations and Future Work" section.
- **Why unresolved:** The current HIDE methodology is fundamentally dependent on extracting hidden state matrices ($H_{in}$ and $H_{out}$) from specific decoder layers, which requires white-box access to the model parameters.
- **What evidence would resolve it:** A demonstration of a proxy metric derived solely from output logits or token probabilities that correlates strongly with the internal HSIC values, achieving comparable detection performance without internal state access.

### Open Question 2
- **Question:** How can the HIDE methodology be modified to maintain reliability for tasks involving extremely short outputs, such as single-token answers?
- **Basis in paper:** [inferred] Section 8.1 notes that the statistical dependence measure becomes uninformative for single tokens (assigning a score of zero), limiting utility for datasets like TriviaQA where outputs can be very short.
- **Why unresolved:** The mathematical formulation of the unbiased HSIC estimator requires a sufficient sample size ($n \geq 4$) to measure variance; a single token provides no statistical leverage to assess decoupling.
- **What evidence would resolve it:** A hybrid detection pipeline that dynamically switches to uncertainty-based metrics (like perplexity) when the output token count falls below a threshold, or a reformulation of the dependence metric for sparse samples.

### Open Question 3
- **Question:** Can semantic filtering or penalization strategies be integrated into the token selection phase to prevent artificial score inflation caused by verbatim input copying?
- **Basis in paper:** [explicit] The authors identify "Input Prompt Copying" as a distinct error source in Section 8.2 and suggest integrating "better semantic filters" as future work.
- **Why unresolved:** The current KeyBERT-based selection often picks overlapping keywords from the input and output when the model hallucinates by repeating the prompt, leading to high HSIC scores (false negatives).
- **What evidence would resolve it:** An improved token selection algorithm that explicitly downweights or filters tokens in the output that appear identically in the input, resulting in higher detection accuracy for repetitive hallucinations.

## Limitations
- Single-token outputs cannot be evaluated meaningfully due to insufficient samples for HSIC estimation
- Verbatim input copying in outputs may artificially inflate scores, masking hallucinations
- Method assumes white-box access to model internal states, limiting black-box applicability
- Performance depends on kernel bandwidth selection, requiring careful tuning

## Confidence

**High Confidence:** The core claim that HIDE achieves superior AUC-ROC performance (~29% relative improvement over single-pass baselines, ~3% over multi-pass methods) is well-supported by extensive experimental results across four datasets and six model variants.

**Medium Confidence:** The layer-agnostic performance claim has experimental support but could vary with different model architectures or tasks not tested in the evaluation. The 51% reduction in computation time compared to multi-pass methods is accurate but context-dependent on the specific baselines used.

**Low Confidence:** The generalizability claim across all hallucination types and model families extends beyond the tested scope. While performance appears robust within the evaluated space, extreme edge cases or novel model architectures may reveal limitations not captured in the current experiments.

## Next Checks

1. **Kernel Sensitivity Analysis:** Systematically evaluate HIDE performance across the full γ spectrum (10^-9 to 10^-1) on held-out samples to identify optimal ranges and failure modes, particularly focusing on the transition points where performance degrades.

2. **Single-Token Edge Case Protocol:** Develop and validate a fallback mechanism for single-token outputs that combines HIDE with alternative metrics like perplexity, ensuring no samples are excluded from evaluation while maintaining detection accuracy.

3. **Input Copying Detection Enhancement:** Implement an n-gram overlap penalty system to detect and adjust for verbatim input copying in outputs, validating that this improves hallucination detection in cases where direct copying masks semantic inconsistencies.