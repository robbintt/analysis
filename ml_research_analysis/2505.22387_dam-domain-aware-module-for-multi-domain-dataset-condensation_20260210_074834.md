---
ver: rpa2
title: 'DAM: Domain-Aware Module for Multi-Domain Dataset Condensation'
arxiv_id: '2505.22387'
source_url: https://arxiv.org/abs/2505.22387
tags:
- domain
- dataset
- condensation
- setting
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of dataset condensation under multi-domain
  settings, where existing methods fail to generalize due to domain heterogeneity.
  The authors introduce the Domain-Aware Module (DAM), a training-time plug-in that
  learns spatial domain masks and leverages frequency-based pseudo-domain labeling
  to encode domain-specific features into each synthetic image without explicit domain
  supervision.
---

# DAM: Domain-Aware Module for Multi-Domain Dataset Condensation

## Quick Facts
- **arXiv ID:** 2505.22387
- **Source URL:** https://arxiv.org/abs/2505.22387
- **Reference count:** 40
- **Key outcome:** DAM improves both in-domain and cross-domain generalization in multi-domain dataset condensation by learning spatial domain masks and using frequency-based pseudo-domain labeling, achieving consistent performance gains across five datasets and three architectures.

## Executive Summary
This paper introduces the Domain-Aware Module (DAM), a novel approach to dataset condensation under multi-domain settings. Existing condensation methods struggle with domain heterogeneity, leading to poor generalization. DAM addresses this by learning spatial domain masks and leveraging frequency-based pseudo-domain labeling to encode domain-specific features into synthetic images without requiring explicit domain supervision. The method improves both in-domain and cross-domain generalization while maintaining the number of images per class.

The authors evaluate DAM across five datasets (CIFAR-10/100, Tiny ImageNet, PACS, VLCS, Office-Home) and three architectures (ConvNet, VGG, ViT), demonstrating consistent performance improvements. For example, on PACS with 10 images per class, DAM improves DC accuracy from 46.1% to 50.9% and DM accuracy from 46.7% to 50.9%. The method also enhances cross-architecture transfer and shows robustness to varying pseudo-domain counts, validating its effectiveness and versatility.

## Method Summary
DAM is a training-time plug-in that learns spatial domain masks and uses frequency-based pseudo-domain labeling to encode domain-specific features into synthetic images. It operates without explicit domain supervision, making it applicable to datasets where domain labels are unavailable. The module improves generalization by preserving domain-specific information during the condensation process, which is critical for multi-domain settings where domain heterogeneity can degrade performance.

## Key Results
- DAM improves DC accuracy on PACS from 46.1% to 50.9% with 10 images per class.
- DAM enhances DM accuracy on PACS from 46.7% to 50.9% with 10 images per class.
- DAM shows consistent performance gains across five datasets and three architectures, validating its versatility.

## Why This Works (Mechanism)
DAM works by learning spatial domain masks and leveraging frequency-based pseudo-domain labeling to encode domain-specific features into synthetic images. This approach addresses the challenge of domain heterogeneity in multi-domain dataset condensation, where existing methods often fail to generalize. By preserving domain-specific information during the condensation process, DAM ensures that the synthetic images retain the diversity and characteristics of the original domains, leading to improved in-domain and cross-domain generalization.

## Foundational Learning
- **Dataset Condensation:** The process of synthesizing a small set of informative images that can train a model to match the performance of a model trained on the full dataset. *Why needed:* To reduce storage and computational costs while maintaining model performance. *Quick check:* Verify that synthetic images can train a model to match full dataset performance.
- **Domain Heterogeneity:** The variation in data distribution across different domains, which can degrade model generalization. *Why needed:* To understand the challenge DAM addresses in multi-domain settings. *Quick check:* Ensure the method handles datasets with clear domain differences (e.g., PACS).
- **Frequency-Based Pseudo-Domain Labeling:** A technique to assign domain labels based on frequency statistics, enabling DAM to work without explicit domain supervision. *Why needed:* To make DAM applicable to datasets without domain labels. *Quick check:* Test DAM on datasets with and without domain labels to confirm its effectiveness.
- **Spatial Domain Masks:** Learned masks that encode domain-specific features into synthetic images. *Why needed:* To preserve domain-specific information during condensation. *Quick check:* Analyze the masks to ensure they capture domain-specific patterns.

## Architecture Onboarding

**Component Map:** Input Data -> DAM Module -> Synthetic Images -> Model Training -> Performance Evaluation

**Critical Path:** The DAM module processes input data to generate synthetic images with encoded domain-specific features, which are then used to train a model. The performance is evaluated based on in-domain and cross-domain generalization.

**Design Tradeoffs:** DAM trades increased computational overhead during training for improved generalization. The method does not require explicit domain supervision, making it more versatile but potentially less precise than methods using ground-truth labels.

**Failure Signatures:** Poor performance may occur if the frequency-based pseudo-domain labeling fails to capture semantically meaningful domain boundaries or if the spatial masks do not effectively encode domain-specific features.

**First Experiments:** 
1. Test DAM on a dataset with known domain labels to compare pseudo-labeling versus ground-truth labeling.
2. Evaluate DAM on a larger-scale multi-domain dataset (e.g., DomainNet) to assess scalability.
3. Measure the computational overhead of DAM compared to baseline methods.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on frequency-based pseudo-domain labeling, which may not align with semantically meaningful domain boundaries.
- Lack of detailed ablation studies on the impact of different mask shapes or frequencies.
- Computational overhead during training is not thoroughly discussed, which is important for practical adoption.

## Confidence
- **High:** In-domain performance improvements are consistently demonstrated across multiple datasets and architectures.
- **Medium:** Cross-domain generalization claims are supported by experiments but could benefit from additional domain-shift scenarios and larger-scale benchmarks.

## Next Checks
1. **Ablation study on pseudo-domain labeling:** Test DAM with ground-truth domain labels (where available) versus frequency-based pseudo-labels to quantify the impact of label quality.
2. **Scalability to larger datasets:** Evaluate DAM on larger-scale multi-domain datasets (e.g., DomainNet) to assess its scalability and robustness to increased domain heterogeneity.
3. **Computational overhead analysis:** Measure and compare the training time and memory usage of DAM against baseline condensation methods to quantify its practical trade-offs.