---
ver: rpa2
title: 'BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics
  Pathway Mechanism Elucidation from Scientific Literature'
arxiv_id: '2512.24733'
source_url: https://arxiv.org/abs/2512.24733
tags:
- pathway
- mechanism
- multi-omics
- biological
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BIOME-Bench introduces a standardized benchmark for evaluating
  large language models on biomolecular interaction inference and end-to-end multi-omics
  pathway mechanism elucidation. The benchmark is constructed through a rigorous four-stage
  workflow that transforms scientific literature into structured, validated knowledge
  representations, including state-aware knowledge graphs.
---

# BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics Pathway Mechanism Elucidation from Scientific Literature

## Quick Facts
- arXiv ID: 2512.24733
- Source URL: https://arxiv.org/abs/2512.24733
- Reference count: 36
- Key outcome: Large language models achieve 51-55% accuracy on biomolecular interaction inference and struggle with phenotype coverage in mechanism elucidation despite strong factuality

## Executive Summary
BIOME-Bench introduces a standardized benchmark for evaluating large language models on biomolecular interaction inference and end-to-end multi-omics pathway mechanism elucidation. The benchmark is constructed through a rigorous four-stage workflow that transforms scientific literature into structured, validated knowledge representations, including state-aware knowledge graphs. Experimental results across multiple contemporary models show that existing systems still struggle with fine-grained biomolecular relation inference and generating faithful, robust mechanistic explanations directly from perturbed entities and pathway context. Top-performing models achieve interaction inference accuracies in the 51-55% range, while multi-omics mechanism elucidation scores vary significantly across evaluation dimensions, with factuality generally strong but phenotype coverage remaining a bottleneck. The benchmark provides literature-grounded, instance-level supervision enabling reliable evaluation beyond surface narrative quality.

## Method Summary
BIOME-Bench employs a four-stage pipeline to construct a benchmark from scientific literature. First, MeSH-guided PubMed retrieval identifies pathway-species-relevant documents, which are filtered through LLM-based relevance scoring. Second, entity extraction and standardization map mentions to canonical identifiers using APIs like PubChem, MyGene, and UniProt. Third, knowledge structuring extracts state-aware hexaplets (source, state, relation, target, state, condition) from the literature, with human sampling verification. Finally, the benchmark is formulated into two tasks: Task A (interaction inference with 18 relation types) and Task B (mechanism elucidation from perturbed entities and pathway context). Evaluation uses multiple metrics including LLM-as-a-Judge, knowledge graph coverage, and embedding similarity.

## Key Results
- Biomolecular interaction inference: Top models achieve 51-55% accuracy and Macro-F1 scores, with significant confusion between relation types (e.g., "regulates" polarized to "activates"/"inhibits")
- Multi-omics mechanism elucidation: Factuality scores remain high (4.5-4.9/5), but phenotype coverage drops to 3-4, indicating models omit required process/phenotype elements
- LLM-as-Judge validation: Perturbed answers show 26.2-55.8% relative score decreases versus rewrites, confirming judge sensitivity to semantic content
- Closed-set KG evaluation: Prevents hallucinated tuples but may undercount valid external knowledge, creating conservative coverage estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A four-stage pipeline can transform unstructured biomedical literature into validated, state-aware knowledge representations suitable for benchmarking LLMs on mechanistic reasoning.
- Mechanism: MeSH-guided retrieval constrains candidate documents to pathway-species-relevant literature; LLM-based relevance scoring filters for mechanistic evidence over background mentions; entity normalization maps mentions to canonical identifiers (PubChem, NCBI Gene, UniProt); knowledge structuring extracts state-aware hexaplets (source, state, relation, target, state, condition).
- Core assumption: The LLM-based extraction pipeline preserves mechanistic semantics from abstracts; human sampling verification (50/50 instances passed) generalizes to the full dataset.
- Evidence anchors:
  - [abstract] "constructed via a rigorous four-stage workflow"
  - [section 3.1] "retain a document only if it exceeds a strict relevance threshold... α = 8"
  - [corpus] Limited direct evidence; corpus papers focus on multi-omics integration but not benchmark construction pipelines.
- Break condition: If abstracts omit critical mechanistic details, or if entity normalization fails on ambiguous abbreviations, the pipeline will produce incomplete or noisy supervision.

### Mechanism 2
- Claim: State-aware hexaplet representations enable finer-grained evaluation of biomolecular interactions than traditional relation triples.
- Mechanism: By annotating source/target states (e.g., phosphorylated, overexpressed) and conditions, the benchmark distinguishes changes in protein abundance from post-translational modifications, capturing dynamic molecular behavior absent in static pathway databases.
- Core assumption: Molecular states are consistently reported in abstracts and extractable by LLMs.
- Evidence anchors:
  - [section 3.3.2] "distinguish subtle yet critical mechanistic differences, such as changes in protein abundance versus post-translational modifications"
  - [Appendix B] Case study shows state-aware grounding (STAT3 dephosphorylated at Tyr705)
  - [corpus] Tensor-DTI (arXiv:2601.05792) integrates multi-modal representations but does not address state annotation.
- Break condition: If states are underspecified or context-dependent in source text, hexaplets will be incomplete or misleading.

### Mechanism 3
- Claim: Multi-dimensional evaluation (LLM-as-Judge, structured KG coverage, embedding similarity) captures distinct aspects of mechanistic explanation quality that no single metric fully represents.
- Mechanism: LLM-as-Judge scores phenotype coverage, causal reasoning, factuality, hallucination; KG coverage measures tuple extraction completeness under closed-set constraint; embedding similarity captures semantic proximity. Results show high factuality but low phenotype coverage across models.
- Core assumption: The LLM judge (Qwen3-32B) is sensitive to mechanistic errors but not to surface rewrites.
- Evidence anchors:
  - [section 4.4] "perturb answers drop to 3.44/3.17/2.21/2.48... relative score decreases of 26.2%, 35.6%, 55.8%, 50.4%"
  - [section 4.3] "similarity and knowledge graph coverage do not fully reflect judge-assessed mechanistic quality"
  - [corpus] No direct corpus support for multi-dimensional LLM evaluation in this domain.
- Break condition: If judge rubrics conflate verbosity with quality, or if KG coverage rewards entity enumeration without causal coherence, evaluations will be misaligned.

## Foundational Learning

- Concept: Pathway enrichment (PE) limitations (curation lag, functional redundancy, context-insensitivity)
  - Why needed here: BIOME-Bench is motivated by PE's inability to capture molecular states, intervention directionality, and multi-hop causal structure; understanding these gaps clarifies the benchmark's value proposition.
  - Quick check question: Why does enrichment score alone fail to connect perturbed entities to pathway-level phenotypes?

- Concept: State-aware knowledge graphs (hexaplets vs. triples)
  - Why needed here: The benchmark's hexaplet representation (source, state, relation, target, state, condition) extends standard KG triples to capture dynamic molecular behavior.
  - Quick check question: What mechanistic distinction would be lost if states were omitted from the representation?

- Concept: LLM-as-a-Judge validity testing (rewrite vs. perturb)
  - Why needed here: The benchmark relies on automated evaluation; understanding the validation protocol (score drops on perturbations but not rewrites) is critical for interpreting results.
  - Quick check question: What pattern in judge scores would indicate that it is sensitive to surface form rather than semantic content?

## Architecture Onboarding

- Component map:
  - Phase I: MeSH-guided PubMed retrieval → LLM relevance scorer (threshold α=8)
  - Phase II: LLM entity extractor + standardization APIs (PubChem, MyGene, UniProt)
  - Phase III: Quadruple extraction → State annotation → Human verification (sampling)
  - Phase IV: Task A (interaction inference, Acc/Macro-F1), Task B (mechanism elucidation, multi-metric eval)

- Critical path: Literature relevance filtering → entity normalization (discard if any entity unresolved) → hexaplet extraction → evaluation protocol. Failures at normalization cascade.

- Design tradeoffs:
  - Single-pathway context limits multi-pathway crosstalk evaluation (acknowledged limitation)
  - One-to-one pathway-paper mapping reduces multi-document reasoning complexity
  - Closed-set KG evaluation prevents hallucinated tuples but may undercount valid external knowledge

- Failure signatures:
  - Interaction inference: Confusion of `leads_to` with `activates`/`inhibits`; underspecified `regulates` polarized to specific relations (section 4.5)
  - Mechanism elucidation: High factuality (~4.5-4.9) but low phenotype coverage (~3-4); models omit required process/phenotype elements
  - Semantic similarity vs. judge mismatch: Qwen3-32B has highest similarity/coverage but not best causal reasoning

- First 3 experiments:
  1. Replicate relevance scoring validity: Run Qwen3-32B judge on rewrite vs. perturb pairs; confirm 40%+ average score drop.
  2. Analyze relation confusion matrix: Identify which fine-grained relations are most often collapsed to generic `leads_to` or `activates`.
  3. Probe phenotype coverage bottleneck: Prompt models with explicit phenotype checklists; measure whether coverage improves without degrading factuality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectural or training interventions improve LLMs' ability to distinguish fine-grained biomolecular relation types (e.g., "activates" vs. "leads_to") rather than defaulting to generic causality?
- Basis in paper: [explicit] The error analysis shows models overwhelmingly confuse "activates/inhibits" with "leads_to" and polarize underspecified "regulates" labels, indicating "a strong tendency to default to generic causality or signed regulation under uncertainty."
- Why unresolved: Current models lack mechanisms to enforce fine-grained semantic distinctions among interaction types in biological contexts.
- What evidence would resolve it: Ablation studies with relation-type-aware training objectives or constrained decoding showing reduced confusion-matrix off-diagonal counts.

### Open Question 2
- Question: What methods can improve phenotype coverage in end-to-end mechanistic explanations, given it is identified as the primary bottleneck?
- Basis in paper: [explicit] "Phenotype coverage is lower, around 3 to 4" while "Factuality and hallucination control are generally strong, typically above 4," indicating "phenotype-level reasoning a primary bottleneck."
- Why unresolved: Models may lack explicit training to associate pathway entities with terminal phenotypic outcomes in a multi-hop causal chain.
- What evidence would resolve it: Improvements in phenotype coverage scores (LLM-as-a-Judge) without degradation in factuality or hallucination metrics.

### Open Question 3
- Question: How can benchmarks and models be extended to handle multi-pathway reasoning with explicit crosstalk and compositional mechanism integration?
- Basis in paper: [explicit] The limitations section states: "extending to multi-pathway settings with explicit crosstalk and compositional reasoning is an important direction" and "future work should build multi-pathway, multi-document mechanistic graphs."
- Why unresolved: BIOME-Bench conditions on single-pathway contexts; real biological mechanisms often span multiple pathways.
- What evidence would resolve it: A multi-pathway benchmark with crosstalk-aware evaluation, showing models can integrate dispersed evidence across pathways.

### Open Question 4
- Question: How sensitive are BIOME-Bench evaluation outcomes to prompt phrasing, and what prompt-diversity protocols can reduce evaluation bias?
- Basis in paper: [explicit] "Our current instruction set has limited stylistic and structural diversity, while modern LLMs can be highly prompt-sensitive... benchmark performance may vary with prompt phrasing and introduce evaluation bias."
- Why unresolved: Systematic prompt-robustness analysis was not conducted in this work.
- What evidence would resolve it: Performance variance analysis across a diversified prompt template set, with statistical reporting of robustness.

## Limitations

- Limited human verification (50/50 sampled instances) may not generalize to the full dataset, creating uncertainty about the pipeline's ability to preserve mechanistic semantics
- Single-pathway context restricts evaluation of multi-pathway crosstalk and compositional reasoning, acknowledged as an important future direction
- Closed-set KG evaluation constraint may undercount valid external knowledge, potentially penalizing models that integrate broader biological context

## Confidence

**High Confidence**: The four-stage benchmark construction methodology is clearly specified and reproducible; LLM-as-Judge validation through rewrite vs. perturb testing (40%+ average score drop) is methodologically sound.

**Medium Confidence**: Generalizability of human-verified quality (50/50 instances passed) to the full dataset; effectiveness of state-aware hexaplet representations given potential underspecification in source literature.

**Low Confidence**: Whether the closed-set KG evaluation constraint meaningfully reflects real-world mechanistic reasoning capabilities; extent to which current relation confusion patterns (e.g., "regulates" → "activates/inhibits") reflect model limitations versus inherent ambiguity in source text.

## Next Checks

1. **Replicate LLM-as-Judge validity**: Run the Qwen3-32B judge on rewrite vs. perturb pairs from Task B outputs; confirm the reported 40%+ average score drop pattern to validate judge sensitivity to semantic content over surface form.

2. **Analyze relation confusion patterns**: Generate detailed confusion matrices for Task A predictions; identify which fine-grained relations are most frequently collapsed to generic "leads_to" or polarized to "activates"/"inhibits", and determine whether this reflects model limitations or inherent ambiguity in source text.

3. **Probe phenotype coverage bottleneck**: Design controlled experiments where Task B prompts explicitly checklist required process/phenotype elements; measure whether targeted prompting improves phenotype coverage scores without degrading the high factuality ratings, isolating whether coverage is a capability gap versus attention/instruction-following issue.