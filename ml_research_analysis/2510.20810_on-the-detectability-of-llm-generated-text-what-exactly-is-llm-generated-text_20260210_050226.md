---
ver: rpa2
title: 'On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated
  Text?'
arxiv_id: '2510.20810'
source_url: https://arxiv.org/abs/2510.20810
tags:
- arxiv
- text
- preprint
- detection
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper highlights the fundamental challenge in detecting LLM-generated
  text due to the lack of a precise definition and the diversity of usage scenarios.
  While detectors perform well under specific, narrow conditions, they struggle in
  real-world applications where human edits, varied prompts, and evolving LLM capabilities
  blur the distinction between human and machine-generated content.
---

# On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?

## Quick Facts
- **arXiv ID**: 2510.20810
- **Source URL**: https://arxiv.org/abs/2510.20810
- **Reference count**: 28
- **Primary result**: Current LLM detectors are brittle and context-dependent, performing well only under narrow conditions and requiring careful interpretation in real-world use.

## Executive Summary
This paper critically examines the fundamental challenge of detecting LLM-generated text, arguing that the lack of a precise definition and the diversity of usage scenarios undermine current detection methods. While detectors can perform well under specific, controlled conditions, they struggle in real-world applications where human edits, varied prompts, and evolving LLM capabilities blur the distinction between human and machine-generated content. The authors demonstrate through experiments that even text generated by the same LLM can yield widely varying detection results depending on the prompt, highlighting the limitations of current approaches.

## Method Summary
The study uses Fast-DetectGPT to analyze the detection probability of text generated by various LLMs (including GPT-4o-mini, DeepSeek-V3.2, and Llama-3.2) under different prompts (polish, rewrite, summarize variations). The experiment takes the first paragraph of Turing's "Computing Machinery and Intelligence" as source text, generates outputs with temperature=0, and scores them using two scoring models (gpt-neo-2.7b and falcon-7b). The analysis focuses on how detection outcomes vary across prompt types and model combinations rather than proposing a new detection method.

## Key Results
- Detection scores vary widely for text generated by the same LLM under different prompts (polish vs. rewrite)
- Human-edited AI text can score as less "machine-generated" than the original human draft
- Detectors trained on older models (GPT-2) fail to generalize to newer models (GPT-4o)
- Current detection methods should be interpreted as approximate references rather than definitive indicators

## Why This Works (Mechanism)

### Mechanism 1: Distributional Mismatch (The Definition Gap)
Detectors fail because they're trained on narrow subsets of LLM outputs that don't generalize to the broad reality of human-LLM collaboration. When text involves human edits or polishing, its statistical signature moves away from the detector's "machine" cluster without returning to the "human" cluster.

### Mechanism 2: Prompt-Induced Style Drift
Detection scores depend causally on the specific prompt used to elicit text. Different prompts trigger different internal modes within the same LLM, altering token selection and sentence structure, which changes the text's "fingerprint" relative to the detector's decision boundary.

### Mechanism 3: Co-evolutionary Convergence
Long-term detection efficacy declines as human writing adapts to LLM norms and LLMs are fine-tuned to sound more human-like. This reduces the "distance" between the two distributions detectors try to separate, as humans consume more LLM text and their own writing styles converge toward machine-like patterns.

## Foundational Learning

**Binary vs. Gradient Detection**: Current detectors use binary classification (Human vs. Machine) but real-world usage is often mixed. Quick check: Does your system output a simple "Yes/No" or a confidence score with configurable threshold?

**Distribution Shift / Concept Drift**: Detectors trained on older models fail on newer models. Quick check: If the target LLM updates its weights tomorrow, how do you validate your detector is still effective?

**False Positive Utility Function**: False positives disproportionately affect non-native speakers. Quick check: What is the acceptable False Positive Rate (FPR) for your application if the cost is wrongly accusing a student of plagiarism?

## Architecture Onboarding

**Component map**: Input Text -> Processing Core (Classifier/Perplexity Scorer) -> Output Score (0-100%) -> Context Module (Missing: prompt/generation process data)

**Critical path**: Detection relies on the divergence between input text's statistical properties and the detector's training distribution.

**Design tradeoffs**: Robustness vs. Sensitivity (making detectors robust to paraphrasing lowers sensitivity to subtle AI usage), Generalizability vs. Specificity (universal detectors are less accurate than specialized ones).

**Failure signatures**: The "Polishing" Paradox (human writes, LLM polishes, text scores lower on AI probability than original human draft), Adversarial Fragility (minor perturbations causing wild score fluctuations).

**First 3 experiments**:
1. Generate same semantic content with 4 different prompt types using same LLM, run detector on all 4 (expect high variance in scores).
2. Take confirmed AI-generated texts, have humans edit 10% of words (synonyms only), measure drop in detection confidence.
3. Run detector on dataset of human-written essays by non-native speakers, measure False Positive Rate.

## Open Questions the Paper Calls Out

**Open Question 1**: How can a formal, standardized definition of "LLM-generated text" be established to account for the spectrum of human-machine collaboration? The authors explicitly pose this question, noting that existing definitions are too broad or limited to specific subsets of generation.

**Open Question 2**: Can detection methods be developed to quantify the specific role or proportion of LLM contribution in a text, rather than providing binary classification? The paper notes detectors struggle to assess "the proportion, function, or ethical significance of LLM contributions."

**Open Question 3**: How can benchmarking systems be designed to remain robust against rapid evolution of LLMs and variability of real-world adversarial attacks? The authors argue "a gold-standard benchmark is hard to realize" due to evolving models and limitations of static datasets.

**Open Question 4**: How can the interpretability of LLM detectors be improved to provide transparent explanations that mitigate ethical risks and bias? The paper states "the lack of detector interpretability represents another concern" limiting ability to provide transparent explanations to the public.

## Limitations

- The empirical analysis is limited to a small set of model combinations (Fast-DetectGPT with specific scoring models) and a single source text
- The study does not address temporal dynamics of how quickly detection models become obsolete as LLMs evolve
- Limited generalizability across the broader landscape of detectors and generation scenarios

## Confidence

- **High Confidence**: Detection scores vary significantly with different prompts for same content; ethical concerns about false positives affecting non-native speakers
- **Medium Confidence**: Detectors perform well only under narrow conditions; long-term co-evolutionary convergence mechanism is plausible but speculative
- **Low Confidence**: Detection should be interpreted purely as "approximate reference" lacks concrete operational guidelines for practitioners

## Next Checks

1. **Cross-Detector Validation**: Repeat prompt sensitivity experiment using 3 different detection methods to verify whether variance is method-specific or general phenomenon
2. **Human-in-the-Loop Robustness**: Conduct study where human annotators edit LLM-generated text (structural changes beyond synonyms) and measure detection score changes across multiple detectors
3. **Bias Quantification Study**: Systematically test diverse human-written texts (native vs. non-native English, academic vs. creative) against multiple detectors to empirically measure false positive rates across demographic and stylistic dimensions