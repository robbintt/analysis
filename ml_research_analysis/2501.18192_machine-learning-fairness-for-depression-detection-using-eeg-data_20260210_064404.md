---
ver: rpa2
title: Machine Learning Fairness for Depression Detection using EEG Data
arxiv_id: '2501.18192'
source_url: https://arxiv.org/abs/2501.18192
tags:
- bias
- data
- fairness
- depression
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates fairness in machine learning for depression
  detection using EEG data. The authors evaluate three deep learning models (CNN,
  LSTM, GRU) across three EEG datasets and apply five bias mitigation strategies (data
  augmentation, massaging, reweighing, regularisation, reject option classification).
---

# Machine Learning Fairness for Depression Detection using EEG Data

## Quick Facts
- arXiv ID: 2501.18192
- Source URL: https://arxiv.org/abs/2501.18192
- Reference count: 0
- Primary result: Machine learning models for EEG-based depression detection show bias across gender groups, with existing bias mitigation methods failing to consistently improve fairness across multiple metrics

## Executive Summary
This paper investigates fairness in machine learning models for depression detection using EEG data, focusing on gender-based bias across three datasets. The authors evaluate three deep learning architectures (CNN, LSTM, GRU) and five bias mitigation strategies (data augmentation, massaging, reweighing, regularisation, reject option classification). Their comprehensive analysis reveals that while algorithmic bias exists, the tested mitigation methods fail to consistently address fairness across different metrics, with models satisfying weaker fairness frameworks but struggling with stricter ones like equalized odds.

## Method Summary
The study evaluates three deep learning models (Deep-Asymmetry CNN, GTSAN GRU-attention, and 1DCNN-LSTM) on three EEG depression datasets with varying channel counts and sampling rates. Five bias mitigation strategies are applied across pre-processing (Mixup, Massaging), in-processing (Reweighing, Regularization), and post-processing (Reject Option Classification) stages. The models are assessed using both performance metrics (Accuracy, Precision, F1) and fairness metrics (Statistical Parity, Equal Opportunity, Equalized Odds, Equal Accuracy) with acceptable bounds of 0.80-1.20. The authors systematically measure tradeoffs between fairness improvements and prediction accuracy across all dataset-model-mitigation combinations.

## Key Results
- None of the five bias mitigation methods consistently improved fairness across all datasets and metrics
- Models satisfied weaker fairness frameworks (equal opportunity, equal accuracy) but struggled with stricter ones (equalized odds)
- Females were the minority group in Mumtaz and MODMA datasets, while males were the minority in Rest dataset
- Regularization achieved the best MSP score on MODMA (1.049) but still failed to meet fairness bounds for equalized odds
- Infinite MEOdd values occurred when TPR or FPR was zero for one group, indicating complete failure to predict one class for that demographic

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-processing massaging can reduce bias by relabeling training instances to equalize class-conditional distributions across sensitive groups.
- Mechanism: For each instance from the favored community (higher probability of favorable class), swap its label to the unfavorable class, and do the opposite for the deprived community. The total class distribution remains unchanged, but the joint distribution with the sensitive attribute is altered.
- Core assumption: The relabeled instances are near the true decision boundary and the model will learn a fairer boundary from the modified distribution.
- Evidence anchors:
  - [abstract]: "We employ five different bias mitigation strategies at the pre-, in- and post-processing stages and evaluate their effectiveness."
  - [section 2.1b]: "After relabelling, the distribution of classes is unchanged, but the class distribution is now the same across both genders."
  - [corpus]: U-Fair paper explores fairness in depression detection but via uncertainty-based multitask learning rather than label manipulation, suggesting limited cross-validated evidence for massaging specifically in EEG contexts.
- Break condition: If the original labels are clinically reliable (DSM-based), deliberately flipping them may introduce noise that degrades detection accuracy. Table 2 shows massaging reduced accuracy from 0.985 to 0.970 on Mumtaz with Deep-Asymmetry.

### Mechanism 2
- Claim: In-processing regularization can enforce equalized odds by directly penalizing TPR and FPR differences between groups during training.
- Mechanism: Compute dt(B) = |TPR0(B) - TPR1(B)| and df(B) = |FPR0(B) - FPR1(B)| per batch. Add λEOpp · dt(B) + λEOdd · df(B) to the loss, forcing gradients to reduce inter-group rate disparities.
- Core assumption: Mini-batch statistics provide stable enough estimates of group-wise TPR/FPR for gradient-based optimization.
- Evidence anchors:
  - [section 2.1d]: "These terms are then used to define the new loss function Lreg(B) = L(B) + λEOpp dt(B) + λEOdd df(B)."
  - [Table 3]: Regularization achieved MEOdd = 1.472 for Deep-Asymmetry on MODMA (better than baseline 1.826 but still outside 0.80-1.20 range).
  - [corpus]: TRI-DEP uses speech, text, and EEG for depression detection but does not incorporate explicit fairness regularization, limiting direct comparison.
- Break condition: If batch sizes are small or group representation is sparse within batches, the TPR/FPR estimates become noisy, causing unstable optimization. The paper does not report per-batch group sizes.

### Mechanism 3
- Claim: Post-processing reject option classification (ROC) improves fairness by selectively reassigning predictions in an uncertainty zone near the decision boundary.
- Mechanism: Define a critical region 1 - τ ≤ p(y|xi) ≤ τ. If xi is in this region and belongs to the minority group, override the predicted label to the favorable class. This targets "borderline" cases where model confidence is low.
- Core assumption: The decision threshold τ correctly identifies uncertain predictions, and minority group members near the boundary are systematically disadvantaged.
- Evidence anchors:
  - [section 2.1e]: "If a sample xi that falls in the 'critical' region 1 - τ ≤ p(y|xi) ≤ τ where 0.5 ≤ τ ≤ 1, we reclassify xi as y if xi belongs to a minority group."
  - [Table 2-4]: ROC results are mixed—on Mumtaz with GTSAN, MSP worsened from 1.052 to 1.193.
  - [corpus]: Corpus papers do not evaluate ROC for EEG-based fairness; the technique appears underexplored in this modality.
- Break condition: If the model's probability calibration is poor, the "critical region" does not correspond to actual uncertainty, and relabeling introduces arbitrary errors.

## Foundational Learning

- Concept: EEG preprocessing pipeline (band-pass filtering, artifact removal, spectral feature extraction)
  - Why needed here: The Deep-Asymmetry, GTSAN, and 1DCNN-LSTM models all depend on power spectral density features and channel-wise asymmetry matrices derived from raw EEG.
  - Quick check question: Given a 256 Hz EEG signal with 19 channels, what frequency bands are typically extracted for depression biomarkers (alpha, beta, theta)?

- Concept: Fairness metrics formalization (Statistical Parity vs. Equal Opportunity vs. Equalized Odds)
  - Why needed here: The paper uses four distinct fairness measures, and they often disagree—understanding their definitions is essential to interpret Tables 2-4.
  - Quick check question: If a model predicts depression equally often for males and females (MSP = 1.0), but has higher TPR for females than males, which fairness criterion is violated?

- Concept: Deep learning sequence architectures (CNN vs. LSTM vs. GRU inductive biases)
  - Why needed here: The three model architectures process EEG differently—CNN captures spatial patterns across electrodes, LSTM/GRU capture temporal dependencies.
  - Quick check question: For a 5-minute resting-state EEG recording, why might a GRU be preferred over a vanilla RNN for modeling temporal dynamics?

## Architecture Onboarding

- Component map: Data layer (Mumtaz, MODMA, Rest) -> Preprocessing -> Feature extraction (PSD, asymmetry matrices) -> Model layer (Deep-Asymmetry, GTSAN, 1DCNN-LSTM) -> Fairness layer (Mixup, Massaging, Reweighing, Regularization, ROC) -> Evaluation layer (Performance + Fairness metrics)

- Critical path: 1) Dataset split adherence (use owner-provided splits) 2) Hyperparameter tuning per dataset-model combination 3) Baseline training without mitigation -> measure MSP, MEOpp, MEOdd, MEAcc 4) Apply mitigation methods sequentially, measuring tradeoffs between accuracy and fairness

- Design tradeoffs:
  - Pre-processing methods preserve model architecture but modify data (risk of distorting signal)
  - In-processing methods require modifying loss function (implementation complexity, hyperparameter sensitivity for λEOpp, λEOdd)
  - Post-processing methods are model-agnostic but operate only on outputs (cannot fix learned representations)
  - The paper shows no single method dominates; Table 3 shows Regularization works best for MSP on MODMA (1.049), while Reweighing is better for MEOdd (1.206)

- Failure signatures:
  - Infinite MEOdd values (Tables 2-3): Occurs when TPR or FPR is zero for one group, indicating complete failure to predict one class for that demographic
  - MSP persistently outside 0.80-1.20 across all methods (e.g., Deep-Asymmetry on Rest, Table 4): Suggests dataset-level imbalance not addressable by standard mitigation
  - Accuracy drops >10% after mitigation (Massaging on 1DCNN-LSTM/Mumtaz: 0.995 → 0.889): Indicates aggressive relabeling harms signal quality

- First 3 experiments:
  1. Reproduce baseline MSP and MEOdd for Deep-Asymmetry on MODMA to validate pipeline; expect MSP ≈ 1.256, MEOdd ≈ 1.826 per Table 3.
  2. Apply Reweighing to 1DCNN-LSTM on MODMA with βi weights computed from joint P(Y, S); measure accuracy change and whether MEOdd moves toward [0.80, 1.20].
  3. Test sensitivity of ROC threshold τ (default 0.6) on Mumtaz/GTSAN by sweeping τ ∈ [0.55, 0.75]; record whether MSP improves without collapsing accuracy below 0.75.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can novel bias mitigation strategies be developed that consistently improve fairness across all standard metrics (e.g., Statistical Parity, Equalized Odds) without sacrificing prediction accuracy in EEG-based depression detection?
- Basis in paper: [explicit] The conclusion states that current methods "are unable to address the bias present" and "none consistently mitigate bias across all fairness metrics and datasets," explicitly calling for "further work... on identifying the best way to address the bias present."
- Why unresolved: The five implemented mitigation methods (pre-, in-, and post-processing) showed inconsistent effectiveness, often improving one fairness metric (like Equal Opportunity) while failing or worsening others (like Statistical Parity).
- What evidence would resolve it: A proposed mitigation technique that yields fairness scores within the 0.80-1.20 range across MSP, MEOpp, and MEOdd simultaneously on the Mumtaz, MODMA, and Rest datasets.

### Open Question 2
- Question: To what extent does the severe class imbalance inherent in EEG datasets drive the observed algorithmic bias, and is it the primary cause of mitigation failure?
- Basis in paper: [explicit] The authors hypothesize that the inability of existing mitigation methods to address bias "is due to the class imbalance highlighted in Table 1."
- Why unresolved: The study measures bias and tests mitigations but does not isolate class imbalance as an independent variable to prove it is the root cause of the fairness issues.
- What evidence would resolve it: Ablation studies controlling for dataset gender ratios (e.g., testing mitigations on re-balanced vs. original datasets) to observe changes in fairness metric outcomes.

### Open Question 3
- Question: How do fairness metrics and mitigation effectiveness vary when applied to sensitive attributes other than gender, such as age or race, in depression detection models?
- Basis in paper: [inferred] The introduction defines bias generally as discrimination based on "age, race and gender," but the methodology restricts the sensitive attribute $S$ solely to binary gender ($S = \{male, female\}$).
- Why unresolved: The paper evaluates fairness exclusively regarding gender; it remains unknown if the observed biases or the failure of mitigation methods generalize to other demographic factors.
- What evidence would resolve it: Replication of the experimental pipeline using age groups or racial demographics as the sensitive attribute $S$.

## Limitations

- The paper does not specify regularization hyperparameters (λEOpp, λEOdd) used in fairness regularization experiments, making exact replication difficult
- Infinite MEOdd values in results suggest potential numerical instability when group-specific TPR/FPR equals zero
- All three datasets use gender as the sole sensitive attribute, limiting applicability to other demographic dimensions

## Confidence

- **High confidence**: The observation that no single bias mitigation method consistently improves fairness across all datasets and metrics is well-supported by comprehensive experimental results
- **Medium confidence**: The conclusion that fairness measures often disagree with each other is supported, though the paper could have provided more analysis on why certain metrics trend together
- **Medium confidence**: The finding that models can satisfy weaker fairness frameworks but struggle with stricter ones (equalized odds) is well-demonstrated, though practical implications for clinical deployment need further exploration

## Next Checks

1. **Sensitivity analysis of regularization hyperparameters**: Systematically vary λEOpp and λEOdd values (e.g., [0.01, 0.1, 1.0, 10.0]) on MODMA dataset to determine their impact on the fairness-accuracy tradeoff and identify optimal values.

2. **Calibration validation for ROC method**: Evaluate probability calibration (e.g., using reliability diagrams) for each model before applying reject option classification to verify that the "critical region" corresponds to actual uncertainty rather than poor calibration.

3. **Cross-dataset fairness transfer**: Apply the most successful mitigation strategy from each dataset (e.g., Reweighing for MEOdd on MODMA) to the other two datasets to test whether fairness improvements generalize across different EEG recording parameters and demographics.