---
ver: rpa2
title: 'MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology'
arxiv_id: '2511.20490'
source_url: https://arxiv.org/abs/2511.20490
tags:
- clinical
- patient
- arxiv
- reasoning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MTBBench addresses the gap in current AI benchmarks for clinical
  decision-making by introducing a challenging, multimodal, and longitudinal benchmark
  inspired by real-world molecular tumor board workflows. Unlike static, single-turn
  evaluations, it simulates how clinicians reason over evolving patient data across
  diverse modalities like histopathology (H&E, IHC), hematology, and genomics.
---

# MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology

## Quick Facts
- arXiv ID: 2511.20490
- Source URL: https://arxiv.org/abs/2511.20490
- Authors: Kiril Vasilev; Alexandre Misrahi; Eeshaan Jain; Phil F Cheng; Petros Liakopoulos; Olivier Michielin; Michael Moor; Charlotte Bunne
- Reference count: 40
- One-line primary result: Agentic framework with foundation model-based tools improves multimodal clinical reasoning by up to 9% and longitudinal tasks by up to 11.2%

## Executive Summary
MTBBench addresses the gap in current AI benchmarks for clinical decision-making by introducing a challenging, multimodal, and longitudinal benchmark inspired by real-world molecular tumor board workflows. Unlike static, single-turn evaluations, it simulates how clinicians reason over evolving patient data across diverse modalities like histopathology (H&E, IHC), hematology, and genomics. The benchmark features expert-validated, clinically relevant questions requiring integration of multimodal evidence over time to predict outcomes, diagnose cancers, and interpret biomarkers.

The evaluation shows that even large models struggle with multimodal reasoning, especially in tasks involving temporal data and conflicting evidence. To address this, MTBBench introduces an agentic framework enabling access to domain-specific foundation models and tools, improving accuracy in multimodal reasoning by up to 9% and longitudinal tasks by up to 11.2%. This demonstrates the importance of tool-augmented reasoning for realistic clinical AI evaluation.

## Method Summary
MTBBench evaluates AI agents on longitudinal, multimodal clinical reasoning in oncology across two tracks: (1) MTBBench-Multimodal with 26 patients, 390 Q&A pairs covering H&E, IHC, hematology; (2) MTBBench-Longitudinal with 40 patients, 183 Q&A pairs covering genomics and clinical timelines. Questions are multiple-choice (6 options) and true/false format. The agentic framework requires models to request files via structured prompts [REQUEST: filename.ext] and integrates foundation model tools (CONCH for H&E, UNI2+ABMIL for IHC quantification) plus knowledge tools (PubMed, DrugBank). Evaluation uses accuracy as the primary metric with bootstrap resampling for confidence intervals, plus file access count as a secondary metric.

## Key Results
- Agentic framework with foundation model-based tools improves multimodal reasoning by up to 9.0% and longitudinal tasks by up to 11.2%
- Models accessing more files and modalities achieve higher accuracy, suggesting information-seeking behavior is key to clinical reasoning quality
- Even large models struggle with multimodal reasoning, especially in tasks involving temporal data and conflicting evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific foundation models, when exposed as callable tools, compensate for general-purpose VLM weaknesses in high-resolution medical image interpretation.
- Mechanism: The LLM acts as an orchestrator, delegating visual analysis to specialized models (CONCH for H&E histopathology, UNI2+ABMIL for IHC quantification) trained on large-scale medical imaging corpora. These models return structured outputs (classification labels, staining percentages) that the LLM integrates into its textual reasoning.
- Core assumption: The LLM can reliably determine when tool invocation is necessary and correctly interpret tool outputs. Also assumes foundation model representations are sufficiently calibrated for clinical use.
- Evidence anchors:
  - [abstract]: "agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively"
  - [section 3.2]: "access to visual foundation model tools significantly improves performance on all multimodal tasks. Digital pathology, in particular, benefits from integration of the FMs, with models like gemma-3-12b and qwen25-7b showing improvements of up to 9%"
  - [corpus]: Related work (CONCH, UNI-2, MUSK) establishes foundation models for pathology, but corpus evidence on agentic integration is limited—no direct comparisons in neighboring papers.

### Mechanism 2
- Claim: Models that actively query more files and modalities achieve higher accuracy, suggesting information-seeking behavior is a key determinant of clinical reasoning quality.
- Mechanism: The agentic framework forces explicit file requests via structured prompts. Models must decide which evidence to gather before answering. This mirrors clinical workflows where physicians selectively retrieve patient data. More file access enables cross-modal integration and temporal grounding.
- Core assumption: File access count is a valid proxy for reasoning thoroughness. Also assumes models meaningfully integrate accessed content rather than superficially retrieving it.
- Evidence anchors:
  - [section 3.1]: "Instead of model size, a stronger signal emerges in the relationship between performance and the number of files accessed... This suggests that effective information gathering, rather than raw scale, is a key determinant of accuracy"
  - [Figure 2]: "Higher file access generally correlates with increased accuracy, highlighting the importance of cross-modidity and temporal integration for performance"
  - [corpus]: TIMER addresses temporal clinical records but does not explicitly test file-access behavior as a mechanism; corpus evidence is weak here.

### Mechanism 3
- Claim: For longitudinal tasks, models benefit from re-accessing patient history files across turns, as clinical reasoning requires contextual grounding over time.
- Mechanism: Files do not persist across turns—agents must explicitly re-request them. This tests whether models recognize when prior context is relevant. Effective models re-access timeline data, pathology reports, and genomic results when answering later questions about progression or recurrence.
- Core assumption: Non-persistent access is ecologically valid (clinicians re-open files) and tests memory management. Also assumes re-access correlates with improved temporal reasoning.
- Evidence anchors:
  - [section 2.3]: "files remain accessible only within the current turn. Namely, they do not persist across turns. However, any file from Ft may be re-requested at a future turn t′ > t"
  - [Example 3]: "qwen3-32b re-accesses part of the patient timeline of events as well as pathological data, resulting in better cancer progression prediction"
  - [corpus]: TwinWeaver and CURENet model longitudinal data via unified representations rather than explicit re-access; no direct corpus support for this specific mechanism.

## Foundational Learning

- **Concept: Agentic Tool-Use Paradigm**
  - Why needed here: MTBBench evaluates LLMs as agents, not static QA systems. Without understanding tool-calling (when to invoke, how to parse outputs), you cannot interpret results or build comparable systems.
  - Quick check question: Given a clinical question about histopathology, what signals would cause an agent to invoke CONCH vs. answering directly from visual input?

- **Concept: Foundation Models for Computational Pathology**
  - Why needed here: The paper uses CONCH (H&E) and UNI2+ABMIL (IHC) as specialized tools. Understanding their architectures—vision-language embeddings vs. patch-based MIL regression—is essential for debugging tool outputs.
  - Quick check question: How does CONCH perform zero-shot ROI classification, and why would this outperform a general VLM on distinguishing keratinizing vs. non-keratinizing SCC?

- **Concept: Longitudinal Clinical Trajectories**
  - Why needed here: The longitudinal track requires reasoning over patient timelines (diagnosis → treatment → progression). Without clinical grounding, you cannot design tasks or interpret why models fail on recurrence/progression questions.
  - Quick check question: Why is predicting 2-year cancer recurrence harder than predicting 5-year survival, given the same patient data?

## Architecture Onboarding

- **Component map**: Backbones (gpt4o, o4-mini, internvl3-38b/78b, gemma-3-12b/27b, llama33-70b, qwen variants, mistralsmall) -> Foundation Model Tools (CONCH for H&E, UNI2+ABMIL for IHC) -> Knowledge Tools (PubMed, DrugBank) -> Benchmark Datasets (HANCOCK, MSK-CHORD) -> Evaluation Harness (Multi-turn dialogue manager, file request parser, accuracy scorer, hallucination logger)

- **Critical path**: Initialize conversation with patient demographics and available file list → Pose clinical question → Agent outputs [REQUEST: filename.ext] or [CONCH: file, (opt1), (opt2)] or [IHCTool: file] → System returns file contents or tool output → Agent synthesizes and outputs [ANSWER: X] → Log accuracy, file access count, hallucinated filenames → Next question; files from previous turn are NOT accessible unless re-requested

- **Design tradeoffs**: Non-persistent file access → realistic but tests memory management; increases token costs; Multiple-choice format → objective evaluation but may miss nuanced reasoning; Foundation models as external tools → modular and interpretable, but adds latency and tool-calling failure modes; Expert validation → clinical relevance, but limits scale (390 + 183 Q&A pairs)

- **Failure signatures**: Hallucinated filenames: Model requests non-existent files; Superficial retrieval: High file access count but answers show no integration; Context fragmentation: Model fails to re-access timeline data on longitudinal questions; Tool misuse: Invoking CONCH on non-H&E images; Confidence mismatch: High-confidence wrong answers on outcome/recurrence tasks

- **First 3 experiments**: 
  1. Baseline sweep: Run gpt4o and internvl3-78b on MTBBench-Multimodal without tools. Report accuracy by task and plot file access vs. accuracy.
  2. Tool augmentation A/B test: Enable CONCH + IHC tools for the same models. Measure per-task accuracy deltas (hypothesis: +5–9% on pathology). Analyze cases where tools help vs. hurt.
  3. Longitudinal re-access analysis: On MTBBench-Longitudinal, identify questions where models fail to re-access timeline files. Correlate with accuracy on progression/recurrence tasks. Flag models that re-access vs. those that don't.

## Open Questions the Paper Calls Out

- **Question 1**: How do clinical AI agents perform when exposed to ambiguous or incomplete inputs requiring clarification or adaptive strategies, compared to the controlled offline setting of MTBBench?
  - Basis in paper: [explicit] "While MTBBench simulates realistic decision-making, it remains a controlled offline benchmark—agents are not yet tested in interactive, real-world clinical workflows or exposed to ambiguous or incomplete inputs requiring clarification or adaptive strategies."
  - Why unresolved: MTBBench provides well-structured questions with clear answer options; real clinical scenarios involve uncertainty, missing data, and evolving constraints not captured in the benchmark.
  - What evidence would resolve it: Extending MTBBench with interactive scenarios containing deliberately ambiguous, incomplete, or conflicting data streams, then measuring agent performance on clarification-seeking behavior and adaptive reasoning.

- **Question 2**: Can specialized foundation models for longitudinal clinical reasoning be developed to match the improvements seen with modality-specific FMs in multimodal tasks?
  - Basis in paper: [explicit] "A particularly promising direction we would like to explore involves the integration of medical foundation models with capabilities for analyzing complex longitudinal data, enabling deeper temporal reasoning and personalized decision support."
  - Why unresolved: Current longitudinal tools (PubMed, DrugBank) are generic; no dedicated FM exists for temporal clinical reasoning, limiting gains in recurrence and progression tasks.
  - What evidence would resolve it: Training and evaluating a longitudinal clinical FM on MTBBench-Longitudinal, demonstrating significant accuracy improvements in progression and recurrence tasks comparable to the 9-11% gains from pathology FMs.

- **Question 3**: What mechanisms enable models to better reconcile conflicting evidence across modalities and time, given current high error rates on outcome prediction tasks?
  - Basis in paper: [inferred] The paper shows outcome/recurrence tasks remain near chance (50%) even with tools, and notes models "failing to reconcile conflicting evidence or different modalities." The hypothesis that "these questions require contextual grounding and biomarker interpretation beyond the current capabilities of uni-modal or tool-free agents" remains untested.
  - Why unresolved: The paper demonstrates the problem but does not investigate specific failure modes or potential interventions for evidence reconciliation.
  - What evidence would resolve it: Error analysis of model predictions on conflicting-evidence cases, combined with ablation studies testing explicit conflict-detection modules or multi-step reasoning architectures.

- **Question 4**: How do benchmark performance and agent behaviors generalize across diverse clinical domains beyond precision oncology?
  - Basis in paper: [explicit] "Looking forward, future work will explore extending MTBBench to more diverse clinical domains and incorporating interactive elements."
  - Why unresolved: MTBBench is oncology-specific (head/neck cancer, pancreatic cancer); it is unknown whether the observed importance of multi-modal file access and tool use transfers to other specialties with different data distributions.
  - What evidence would resolve it: Constructing equivalent benchmarks for cardiology, emergency medicine, or radiology workflows and comparing accuracy trends, file access patterns, and tool utilization across domains.

## Limitations
- Evaluation relies on curated expert-validated datasets (HANCOCK, MSK-CHORD) with limited patient diversity, raising questions about external validity
- Tool augmentation benefits are demonstrated against baseline of static LLM evaluation, but comparative studies against other clinical AI benchmarks are absent
- Foundation model tools (CONCH, UNI2+ABMIL) are treated as black boxes with unspecified calibration procedures, and their clinical reliability in edge cases remains untested

## Confidence
- High confidence: The agentic framework design and its implementation details are clearly specified and reproducible
- Medium confidence: The observed performance gains from tool augmentation (9% multimodal, 11.2% longitudinal) are well-documented but may not generalize beyond the curated datasets
- Medium confidence: The correlation between file access count and accuracy suggests information-seeking is important, but causality is not established

## Next Checks
1. External validation: Test MTBBench on a separate oncology dataset from a different institution to verify performance generalization beyond HANCOCK/MSK-CHORD
2. Tool calibration audit: Evaluate CONCH and UNI2+ABMIL outputs on clinically challenging cases (e.g., rare cancer subtypes, ambiguous IHC staining) to establish reliability bounds
3. Memory vs. reasoning experiment: Design a controlled study where file persistence is varied (always available vs. always hidden) to isolate the effect of memory management from genuine reasoning improvements