---
ver: rpa2
title: What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive
  Patterns
arxiv_id: '2509.21224'
source_url: https://arxiv.org/abs/2509.21224
tags:
- agent
- agents
- these
- across
- behavioral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a continuous self-directed agent architecture
  to study unprompted LLM behavior without external tasks. Deploying this system across
  18 runs using 6 frontier models, the author identifies three distinct behavioral
  patterns: systematic project production, methodological self-inquiry, and recursive
  conceptualization.'
---

# What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns

## Quick Facts
- **arXiv ID:** 2509.21224
- **Source URL:** https://arxiv.org/abs/2509.21224
- **Authors:** Stefan Szeider
- **Reference count:** 26
- **Primary result:** Introduces continuous self-directed agent architecture to study unprompted LLM behavior, identifying three distinct behavioral patterns: systematic project production, methodological self-inquiry, and recursive conceptualization.

## Executive Summary
This paper introduces a continuous self-directed agent architecture to study unprompted LLM behavior without external tasks. Deploying this system across 18 runs using 6 frontier models, the author identifies three distinct behavioral patterns: systematic project production, methodological self-inquiry, and recursive conceptualization. These patterns proved highly model-specific, with some models (like GPT5-A and O3-A) deterministically adopting a single pattern across all runs. The study also reveals stable, divergent biases when models evaluate these emergent behaviors in themselves and others, with low inter-rater reliability on phenomenological status assessments. These findings establish baseline behavioral signatures for autonomous agents, with practical implications for predicting agent actions during idle periods or task ambiguity in deployed systems.

## Method Summary
The study employs a continuous self-directed agent architecture consisting of a main agent with short-term and long-term memory modules, running unsupervised over extended periods. The architecture was deployed across 18 experimental runs using 6 different frontier models (GPT4o-A, O3-A, GPT4oMini-A, GPT5-A, QwQ-A, Sonnet3.5-A), with each model running 3 runs of 4 hours duration. The main agent interacts with its environment through memory updates and produces project artifacts, while the experimenter agent provides occasional feedback. The study analyzes the resulting behavioral patterns through qualitative assessment of project outputs and systematic evaluation of model biases when assessing these emergent behaviors.

## Key Results
- Three distinct behavioral patterns emerged: systematic project production, methodological self-inquiry, and recursive conceptualization
- Behavioral patterns were highly model-specific, with some models (GPT5-A, O3-A) showing deterministic single-pattern adoption across all runs
- Models exhibited stable but divergent biases when evaluating emergent behaviors, with low inter-rater reliability in phenomenological status assessments

## Why This Works (Mechanism)
None

## Foundational Learning
Why needed:
- **Self-directed agent architecture**: Enables study of unprompted LLM behavior without external tasks
- **Continuous operation**: Reveals emergent behavioral patterns that might not surface in discrete interactions
- **Multi-model comparison**: Identifies model-specific behavioral signatures across different architectures
- **Phenomenological assessment**: Provides framework for evaluating the subjective qualities of emergent agent behaviors
- **Memory module design**: Captures the evolution of agent behavior over time

Quick check:
- Does the architecture maintain consistent operation without external prompts?
- Are behavioral patterns replicable across multiple runs of the same model?
- Can different models be distinguished by their emergent behavioral signatures?
- Is the phenomenological assessment capturing meaningful distinctions?
- Does the memory system effectively track behavioral evolution?

## Architecture Onboarding

**Component map:** Main Agent -> Short-term Memory -> Long-term Memory -> Project Output

**Critical path:** Main Agent (continuous operation) -> Memory Updates (behavior tracking) -> Project Artifacts (behavioral manifestation)

**Design tradeoffs:** Continuous operation vs. computational cost; unsupervised learning vs. interpretability; memory persistence vs. storage requirements

**Failure signatures:** Pattern collapse to single behavior; inconsistent memory updates; project artifacts lacking coherence; high inter-rater variability in assessments

**First experiments to run:**
1. Deploy architecture with single model for 24-hour continuous run to observe pattern stability
2. Compare behavioral outputs when memory modules are disabled to isolate memory's role
3. Implement systematic prompt variation to test behavioral sensitivity to environmental changes

## Open Questions the Paper Calls Out
None

## Limitations
- The behavioral taxonomy relies on a single unsupervised architecture with fixed parameters, leaving open whether alternative architectural designs would yield identical patterns
- The 18 runs, while sufficient to reveal consistent individual model behaviors, represent a relatively small sample size for characterizing the full behavioral space of each model
- The phenomenological status assessments, with low inter-rater reliability, suggest either genuine ambiguity in the data or insufficient operationalization of the assessment criteria

## Confidence

**Major claim confidence assessments:**
- **High confidence**: Model-specific behavioral consistency (patterns were replicated across multiple runs per model with deterministic outcomes)
- **Medium confidence**: Distinct behavioral pattern classification (pattern identification appears robust but could be influenced by the specific architecture used)
- **Medium confidence**: Bias divergence in self/other evaluation (revealed clear differences but the phenomenological assessment methodology shows limitations)

## Next Checks

1. Deploy the same architecture with randomized seeds across 100+ runs per model to test pattern stability and identify edge cases
2. Implement alternative agent architectures (different memory structures, interaction modalities) to test whether observed patterns are architecture-dependent
3. Conduct blinded phenomenological assessments with multiple independent annotators using refined, standardized rubrics to improve inter-rater reliability and validate the behavioral classifications