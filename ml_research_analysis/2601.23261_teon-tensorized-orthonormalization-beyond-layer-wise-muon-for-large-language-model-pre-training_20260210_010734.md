---
ver: rpa2
title: 'TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language
  Model Pre-Training'
arxiv_id: '2601.23261'
source_url: https://arxiv.org/abs/2601.23261
tags:
- muon
- teon
- arxiv
- training
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TEON, a tensorized generalization of the Muon
  optimizer that extends gradient orthogonalization beyond individual layers by modeling
  neural network gradients as structured higher-order tensors. The key idea is to
  stack gradients from multiple layers into a tensor and apply mode-wise orthogonalization
  to capture cross-layer correlations, thereby offering stronger convergence guarantees
  than layer-wise Muon.
---

# TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training

## Quick Facts
- arXiv ID: 2601.23261
- Source URL: https://arxiv.org/abs/2601.23261
- Reference count: 30
- Primary result: TEON improves perplexity across GPT-style and LLaMA-style models (60M-1B params) via cross-layer gradient orthogonalization

## Executive Summary
TEON introduces a tensorized generalization of the Muon optimizer that extends gradient orthogonalization beyond individual layers by modeling neural network gradients as structured higher-order tensors. By stacking gradients from multiple layers into a tensor and applying mode-wise orthogonalization, TEON captures cross-layer correlations and offers stronger convergence guarantees than layer-wise Muon. The method was evaluated on models ranging from 60M to 1B parameters, showing consistent improvements in training and validation perplexity across all scales.

## Method Summary
TEON represents gradients from multiple layers as a higher-order tensor and applies mode-wise orthogonalization to capture cross-layer correlations. The method generalizes the layer-wise Muon approach by stacking gradients across layers and performing orthogonalization along each tensor mode. This allows the optimizer to account for dependencies between different layers while maintaining computational tractability through approximate SVD schemes. The approach includes theoretical analysis of convergence properties and extensive ablation studies validating design choices.

## Key Results
- Consistent perplexity improvements across GPT-style and LLaMA-style models (60M-1B parameters)
- Robust performance under various approximate SVD computation schemes
- Demonstrated effectiveness of cross-layer gradient orthogonalization through ablation studies

## Why This Works (Mechanism)
TEON extends Muon's gradient orthogonalization principle beyond individual layers by modeling the full gradient space as a structured tensor. When gradients from multiple layers are stacked, cross-layer correlations become visible as tensor modes, allowing orthogonalization to remove redundant information across the entire network rather than just within layers. This captures global gradient structure that layer-wise approaches miss, leading to more efficient updates and improved convergence properties.

## Foundational Learning

**Gradient orthogonalization** - Removing redundant components from gradients to improve optimization efficiency
*Why needed*: Standard optimizers suffer from gradient redundancy that slows convergence
*Quick check*: Verify orthogonality improves conditioning of gradient space

**Tensor decomposition** - Breaking down multi-dimensional arrays into simpler components
*Why needed*: Enables analysis of cross-layer gradient relationships as structured data
*Quick check*: Confirm tensor modes correspond to meaningful gradient correlations

**Mode-wise operations** - Applying transformations along specific tensor dimensions
*Why needed*: Allows selective orthogonalization of gradient components across layers
*Quick check*: Ensure each mode captures distinct cross-layer relationships

**Approximate SVD** - Computing truncated singular value decompositions for efficiency
*Why needed*: Full SVD is computationally prohibitive for large models
*Quick check*: Validate approximation quality doesn't degrade performance

**Convergence guarantees** - Mathematical proofs of optimization algorithm performance
*Why needed*: Provides theoretical foundation for practical improvements
*Quick check*: Verify assumptions hold in practical training scenarios

## Architecture Onboarding

**Component map**: Input gradients -> Tensor stacking -> Mode-wise orthogonalization -> Parameter update

**Critical path**: The core innovation lies in the tensor construction and mode-wise orthogonalization steps, which must balance computational efficiency with capture of meaningful cross-layer correlations

**Design tradeoffs**: Computational cost of tensor operations vs. orthogonalization effectiveness; approximation quality vs. theoretical guarantees; layer grouping strategy vs. captured correlations

**Failure signatures**: Degraded performance when tensor modes poorly capture gradient structure; instability when SVD approximations are too coarse; limited gains when cross-layer correlations are weak

**First experiments**: 1) Compare TEON vs layer-wise Muon on small model with controlled gradient structure, 2) Ablation study varying tensor construction parameters, 3) Wall-clock time analysis across different hardware configurations

## Open Questions the Paper Calls Out

None specified in the provided content.

## Limitations
- Theoretical analysis relies on assumptions about gradient tensor structure that may not hold in practice
- Computational overhead from tensor operations and approximate SVD computation
- Limited evaluation of training dynamics beyond perplexity metrics
- Sensitivity to tensor construction parameters not fully explored

## Confidence
- **High confidence**: Implementation of tensorized orthogonalization and basic functionality
- **Medium confidence**: Claimed perplexity improvements across model scales
- **Low confidence**: Theoretical convergence guarantees and their practical implications

## Next Checks
1. Conduct wall-clock time efficiency analysis comparing TEON against baseline Muon across different hardware configurations
2. Perform ablation studies varying tensor construction parameters and layer grouping strategies
3. Extend evaluation to include additional metrics such as training stability, gradient norm evolution, and final model downstream task performance