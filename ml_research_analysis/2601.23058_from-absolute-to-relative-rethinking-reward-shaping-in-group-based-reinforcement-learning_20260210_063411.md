---
ver: rpa2
title: 'From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement
  Learning'
arxiv_id: '2601.23058'
source_url: https://arxiv.org/abs/2601.23058
tags:
- reward
- relative
- rlrr
- ranking
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the instability of absolute reward signals
  in group-based reinforcement learning, which causes sparse gradients and training
  instability in large language model reasoning. To solve this, it proposes RLRR,
  a framework that shifts from absolute scoring to relative ranking within groups,
  and introduces a Ranking Reward Model that directly outputs relative quality rankings.
---

# From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2601.23058
- **Source URL**: https://arxiv.org/abs/2601.23058
- **Reference count**: 40
- **Primary result**: RLRR framework improves reasoning accuracy and inference efficiency over GRPO/DAPO on mathematical/logical benchmarks using relative ranking instead of absolute scoring

## Executive Summary
This paper addresses the instability and sparse gradient problems in group-based reinforcement learning caused by absolute reward signals. The proposed RLRR framework shifts from absolute scoring to relative ranking within groups, introducing a Ranking Reward Model that directly outputs relative quality rankings. This approach converts sparse binary feedback into dense gradient signals and stabilizes advantage estimation through bounded rewards. Experiments demonstrate significant improvements in reasoning accuracy and inference efficiency on mathematical and logical benchmarks, with the 1.5B model achieving up to 24.8% accuracy on AIME 2025 and reduced token usage compared to baselines.

## Method Summary
RLRR replaces absolute reward scoring with relative ranking within groups. It introduces a Ranking Reward Model that directly outputs relative quality rankings, and applies bounded reward shaping (HRR or PRR) to convert rankings to rewards. The framework also includes correctness-aware advantage clipping to prevent penalizing valid but suboptimally-ranked responses. The approach operates on groups of responses generated by the policy model, using either rule-based verifiers or learned reward models to establish correctness, then re-ranking based on quality while preserving correctness hierarchies.

## Key Results
- RLRR achieves up to 24.8% accuracy on AIME 2025 with 1.5B model, significantly outperforming GRPO and DAPO baselines
- Inference efficiency improves with reduced token usage compared to baselines
- Ranking Reward Model outperforms scalar reward models in open-ended writing tasks
- RLRR demonstrates greater robustness to small group sizes compared to baseline GRPO
- Ablation studies confirm benefits of relative rewards and correctness-aware ranking

## Why This Works (Mechanism)

### Mechanism 1
Relative reward shaping converts sparse binary feedback into dense gradient signals by extracting learning value from groups with homogeneous outcomes. Instead of using absolute scores (e.g., all 0s or all 1s in a group), RLRR assigns intra-group rankings (1 to rmax) and maps them through a shaping function. Even when all responses are correct or incorrect, the ranking differentiates quality, producing non-zero advantages.

### Mechanism 2
Bounded relative rewards stabilize advantage estimation by capping variance, preventing outlier scores from corrupting group baselines. PRR maps rankings to [0,1]; HRR uses tanh-bounded corrections. Popoviciu's inequality then bounds variance (max 0.25 for PRR), preventing extreme scalar outputs from skewing the group mean used in advantage calculation.

### Mechanism 3
Correctness-aware advantage clipping prevents policy degradation from penalizing valid but suboptimally-ranked responses. When a correct response receives negative advantage (ranked low within a high-quality group), clipping sets a floor (ξ−). Similarly, incorrect responses get a ceiling (ξ+). This preserves correctness incentives while still enabling fine-grained quality learning.

## Foundational Learning

- **Group-based policy optimization (GRPO)**: Understanding GRPO's advantage estimation (group mean subtraction, variance normalization) is prerequisite since RLRR modifies this mechanism. Quick check: Can you explain why GRPO uses intra-group statistics instead of a learned value function?

- **Bradley-Terry preference modeling**: The paper's theoretical analysis of SRM instability derives from BT loss gradients; understanding this explains why scalar rewards diverge. Quick check: What happens to the Bradley-Terry loss gradient as the reward difference δ increases?

- **Listwise vs. pointwise ranking**: The Ranking RM uses listwise evaluation (comparing all candidates jointly) vs. SRM's pointwise scoring; this architectural difference matters for signal quality. Quick check: Why might evaluating multiple responses jointly produce more reliable rankings than sorting independent scores?

## Architecture Onboarding

- **Component map**: Prompt → Policy Model → G responses → [Rule Verifier + Reward Model] → Hierarchical Re-ranking (correctness → length → quality) → Reward Shaping (HRR or PRR) → Advantage Estimation + Clipping → Policy Update

- **Critical path**: The ranking-to-reward transformation (Eq. 3-4) and advantage clipping (Eq. 5) are the novel contributions. Errors here cascade directly to policy gradients.

- **Design tradeoffs**: HRR vs. PRR: HRR preserves ground truth authority but requires verifiers; PRR is more general but may misalign with correctness if rankings are noisy. τ (HRR correction magnitude): Higher values emphasize relative quality but risk overriding rule-based signals. Group size G: Smaller G increases invalid-group risk for baseline GRPO; RLRR is more robust at small G.

- **Failure signatures**: Training accuracy plateaus early with GRPO but continues improving with RLRR → likely sparse gradient problem. Sudden policy degradation mid-training → check if clipping thresholds allow excessive penalty on correct responses. Ranking RM disagrees with ground truth on correctness ordering → re-ranking should catch this; if not, check hierarchical priority implementation.

- **First 3 experiments**:
  1. Run GRPO vs. RLRR(H) vs. RLRR(P) on AIME24 with identical data/seed; verify effective data ratio divergence
  2. Train with/without correctness-aware clipping at fixed τ; measure accuracy stability and check for penalized-correct cases in logs
  3. Test RLRR with scalar RM (convert to ranks) vs. dedicated Ranking RM; isolate whether gains come from ranking paradigm or model architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Relative ranking assumes meaningful quality differences exist within groups of identical correctness labels, which may not hold for all tasks
- Theoretical analysis of reward divergence relies on idealized Bradley-Terry assumptions that may not capture real-world ranking noise
- 1.5B model size may limit generalizability to frontier-scale reasoning systems
- Some architectural claims lack ablation against simple scalar-to-rank conversion baselines

## Confidence

**Major Limitations**: The primary concern is that relative ranking assumes meaningful quality differences exist within groups of identical correctness labels, which may not hold for all tasks. The theoretical analysis of reward divergence relies on idealized Bradley-Terry assumptions that may not capture real-world ranking noise. The 1.5B model size, while practical, may limit generalizability to frontier-scale reasoning systems. Some architectural claims (e.g., Ranking RM superiority) lack ablation against simple scalar-to-rank conversion baselines.

**Confidence Labels**:
- High: Effective data ratio improvements over GRPO (Section 5.1) - empirically validated with clear metrics
- Medium: Reward divergence problem identification - theoretically sound but lacks direct empirical validation of unbounded scalar outputs during training
- Medium: Correctness-aware clipping benefits - shown in Figure 7a but could benefit from more systematic threshold sensitivity analysis
- Low: Ranking RM architectural superiority - claims superiority over scalar RMs but limited ablation against simpler rank-conversion approaches

## Next Checks
1. Test RLRR on tasks without clear correctness verification (e.g., creative writing) to validate general applicability beyond verifiable reasoning
2. Implement controlled experiment with artificially injected ranking noise to measure robustness of HRR vs. PRR variants
3. Compare against scalar reward model with post-hoc ranking conversion to isolate architectural benefits of dedicated Ranking RM