---
ver: rpa2
title: DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay
arxiv_id: '2511.03670'
source_url: https://arxiv.org/abs/2511.03670
tags:
- reward
- state
- experience
- replay
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Deep Q-Networks (DQN) performance using
  epsilon-greedy exploration policies and prioritized experience replay in the Cart
  Pole environment. The study systematically evaluates various epsilon decay schedules
  and their impact on learning efficiency, convergence, and reward optimization.
---

# DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay

## Quick Facts
- arXiv ID: 2511.03670
- Source URL: https://arxiv.org/abs/2511.03670
- Authors: Daniel Perkins; Oscar J. Escobar; Luke Green
- Reference count: 16
- Key outcome: Super-linear epsilon decay schedules outperform linear and logarithmic decays in DQN for CartPole, with prioritized experience replay improving sample efficiency at computational cost.

## Executive Summary
This paper investigates Deep Q-Network (DQN) performance using epsilon-greedy exploration policies and prioritized experience replay in the CartPole environment. The study systematically evaluates various epsilon decay schedules and their impact on learning efficiency, convergence, and reward optimization. Results show that super-linear decay schedules (inverse and sinusoidal) outperform linear and logarithmic decays. Prioritized experience replay improves learning speed and stability compared to uniform replay, though the Cart Pole's simplicity limits these gains. DQN significantly outperforms traditional Q-learning, achieving higher cumulative rewards in fewer episodes. However, optimal performance requires careful hyperparameter tuning specific to the task.

## Method Summary
The study employs a DQN architecture with epsilon-greedy exploration and experience replay, testing different epsilon decay schedules (linear, logarithmic, inverse, sinusoidal) on the CartPole environment. The DQN uses a target network for stability and can operate with either uniform or prioritized experience replay. The CartPole environment provides a simple benchmark with discrete actions and continuous state space. Experiments compare cumulative rewards, convergence speed, and computational efficiency across different hyperparameter configurations, with performance measured over 300 episodes.

## Key Results
- Super-linear epsilon decay schedules (inverse and sinusoidal) outperform linear and logarithmic decays in CartPole
- Prioritized experience replay improves learning speed and stability compared to uniform replay
- DQN significantly outperforms traditional Q-learning, achieving higher cumulative rewards in fewer episodes

## Why This Works (Mechanism)

### Mechanism 1: Super-Linear Epsilon Decay for Controlled Exploration-Exploitation
- Claim: Super-linear decay schedules (inverse, sinusoidal) outperform linear and logarithmic schedules in CartPole by enabling aggressive early exploration followed by rapid exploitation.
- Mechanism: The decay rate accelerates over time, allowing the agent to sample diverse state-action pairs early while committing to learned policies sooner. This matches environments where optimal policies can be discovered quickly.
- Core assumption: The environment has deterministic or low-noise dynamics where early exploration suffices to discover good policies.
- Evidence anchors:
  - [abstract] "Results show that super-linear decay schedules (inverse and sinusoidal) outperform linear and logarithmic decays."
  - [section 4.3, p.6-7] "This suggests that the DRL is more likely to get better results if the decay of ε is rapid (super-linear)."
  - [corpus] "Optimization of Epsilon-Greedy Exploration" (arXiv:2506.03324) addresses exploration rate tuning, confirming this is an active research area.
- Break condition: In non-stationary or highly stochastic environments requiring sustained exploration, rapid decay may cause premature convergence to suboptimal policies.

### Mechanism 2: Prioritized Experience Replay via TD-Error Weighting
- Claim: PER improves sample efficiency by replaying experiences with higher temporal difference (TD) error more frequently.
- Mechanism: TD-error |r_t + γ max Q(s_{t+1}, a) - Q(s_t, a)| proxies "learnability." Experiences with larger TD-error are sampled more often via P(i) ∝ p_i^α, focusing gradient updates on transitions where the network's predictions are most wrong.
- Core assumption: High TD-error reflects actionable learning signal rather than environment noise.
- Evidence anchors:
  - [abstract] "Prioritized experience replay improves learning speed and stability compared to uniform replay."
  - [section 5.2, p.8-9] Equations 4-6 define priority, sampling probability, and importance-sampling weights to correct introduced bias.
  - [corpus] "Uncertainty Prioritized Experience Replay" and "Reliability-Adjusted Prioritized Experience Replay" extend PER with uncertainty-aware prioritization, indicating TD-error alone may be insufficient in noisy settings.
- Break condition: In simple environments like CartPole, PER overhead (2 min vs 20 sec runtime) may outweigh efficiency gains; the paper notes "uniform experience replay was sufficient."

### Mechanism 3: Target Network for Training Stability
- Claim: Decoupling the policy network from the target network stabilizes Q-learning by reducing variance in TD targets.
- Mechanism: The loss L(φ) = (r_t + γ max {q̂[s_{t+1}, a, φ]} - q[s_t, a_t, φ])² uses a frozen target network q̂ updated only periodically, preventing chasing a moving target during gradient descent.
- Core assumption: The target network remains a reasonable approximation between updates.
- Evidence anchors:
  - [section 1.3, p.3] "So, to reduce variance, it is common to only alter the target every hundred iterations or so."
  - [section 5, p.7] "The optimal/target q-value... is computed by using... the target network."
  - [corpus] Corpus papers on DQN variants (β-DQN, Rainbow DQN) assume this architecture as baseline.
- Break condition: If target updates are too infrequent, the target becomes stale; if too frequent, training destabilizes.

## Foundational Learning

- **Bellman Equation and Q-Learning Update**:
  - Why needed here: The entire framework builds on iterative Q-value updates; understanding Eq. (1) is prerequisite to grasping loss function Eq. (3).
  - Quick check question: Can you explain why Q-learning needs to visit state-action pairs "infinitely often" for convergence guarantees?

- **Exploration-Exploitation Trade-off**:
  - Why needed here: Epsilon-greedy directly implements this trade-off; understanding why ε decays is essential for interpreting Figure 5 results.
  - Quick check question: If ε is too high, what happens to temporal complexity? If too low, what happens to optimality?

- **Temporal Difference Error**:
  - Why needed here: TD-error is both the training signal (via loss) and the prioritization metric (via PER); conflating these roles causes confusion.
  - Quick check question: Why does TD-error serve as a reasonable proxy for "how much can we learn from this transition"?

## Architecture Onboarding

- **Component map**: State -> Policy Network -> Action -> Environment -> Reward+Next State -> Replay Buffer -> Sample Batch -> Loss Computation -> Update Policy Network; Target Network updated periodically from Policy Network

- **Critical path**: 1. Observe state s_t → Policy network → Select action (ε-greedy) 2. Execute action → Environment returns (r_{t+1}, s_{t+1}) 3. Compute TD-error → Store in replay buffer with priority 4. Sample batch (uniform or prioritized) → Compute loss → Update policy network 5. Periodically sync target network ← policy network

- **Design tradeoffs**:
  - ε decay rate: Faster decay → quicker exploitation but risk of suboptimal convergence
  - PER α parameter: Higher α → more aggressive prioritization but more bias to correct
  - PER β annealing: Must increase β from ~0.4 to 1.0 to fully correct importance-sampling bias
  - Buffer size: Larger buffer retains more diverse experience but increases memory

- **Failure signatures**:
  - Average reward stagnates or decreases (Fig. 3): ε decaying too fast or insufficient exploration
  - High variance in episode rewards: Policy too stochastic; ε may be too high
  - PER underperforms uniform replay: May indicate environment is too simple or hyperparameters misconfigured
  - Runtime explodes: PER overhead dominates in simple environments

- **First 3 experiments**:
  1. **Baseline DQN with exponential ε-decay (β=0.9999) and uniform replay**: Establish performance floor; expect cumulative reward ~150-200 by episode 300.
  2. **Ablate epsilon schedules**: Compare inverse decay (ε = 1/(1 + 0.003t)) vs linear vs sinusoidal on same architecture; verify super-linear advantage.
  3. **Add PER with α=0.6, β₀=0.4**: Measure episode efficiency gain vs runtime cost; if CartPole-like simplicity, expect marginal improvement at 4-6x runtime.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's conclusions are constrained by the CartPole environment's simplicity, which may not generalize to high-dimensional or stochastic domains
- Prioritized experience replay's computational overhead (2 minutes vs 20 seconds) significantly impacts practical deployment
- The optimal hyperparameters identified are task-specific and may not transfer across different environments

## Confidence

- **High confidence**: DQN outperforms traditional Q-learning in CartPole, and target network architecture effectively stabilizes training
- **Medium confidence**: Super-linear epsilon decay schedules provide consistent advantages in simple deterministic environments
- **Medium confidence**: Prioritized experience replay improves sample efficiency in CartPole with significant computational trade-offs

## Next Checks
1. Test super-linear epsilon decay schedules in stochastic environments (e.g., MountainCar or LunarLander) to verify if rapid early exploration remains beneficial when optimal policies require sustained exploration
2. Evaluate PER performance with uncertainty-weighted prioritization (rather than TD-error alone) in environments with significant noise or partial observability to assess whether the current approach overfits to deterministic settings
3. Conduct ablation studies on the interaction between epsilon decay rates and PER parameters (α, β) to determine if optimal configurations are independent or require coordinated tuning across the learning pipeline