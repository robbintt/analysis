---
ver: rpa2
title: Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents
arxiv_id: '2510.22443'
source_url: https://arxiv.org/abs/2510.22443
tags:
- search
- digital
- goal
- context
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WAGIBench, a novel benchmark for egocentric
  multimodal goal inference in assistive wearable agents. The authors collected a
  large-scale dataset of 29 hours of multimodal data from 348 participants across
  3,477 recordings, featuring ground-truth goals alongside visual, audio, digital,
  and longitudinal contextual observations.
---

# Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents

## Quick Facts
- arXiv ID: 2510.22443
- Source URL: https://arxiv.org/abs/2510.22443
- Reference count: 40
- Primary result: 29-hour egocentric dataset with 3,477 recordings enables multimodal goal inference evaluation; best VLMs achieve 55% relevant generation and 84% MCQ accuracy

## Executive Summary
This paper introduces WAGIBench, a comprehensive benchmark for egocentric multimodal goal inference in assistive wearable agents. The authors collected 29 hours of data from 348 participants across 3,477 recordings, featuring ground-truth goals alongside visual, audio, digital, and longitudinal contextual observations. They evaluate both discriminative (multiple-choice) and generative goal inference tasks using several vision-language model families, finding that larger models perform significantly better but remain far from practical usefulness, achieving only 55% relevant goal generation. Human performance exceeds model performance at 93% accuracy versus 84% for the best VLM. Through modality ablation, they show models benefit from relevant modalities with minimal performance degradation from irrelevant ones, though digital and longitudinal contexts have low signal-to-noise ratios.

## Method Summary
The benchmark uses 3,477 video clips (29 hours) from 348 participants across 165 scripted scenarios, featuring 4 modalities: vision (egocentric video), audio (transcribed speech), digital (7 app states), and longitudinal (user history). The dataset was collected via Meta Aria glasses. Two evaluation paradigms are used: discriminative (multiple-choice accuracy with 4 options) and generative (open-ended goal prediction scored by LLM Judge). VLMs are evaluated across families (Llama-3.2-11B-Vision, Qwen2.5-VL-3B/7B/72B, InternVL-2.5-2B/8B/78B, GPT-4.1) with video sampled at 32 frames (Llama: 1 frame). Digital contexts are generated using Llama3.3-70b-instruct, longitudinal Socratic captions via dual-VLM captioning, and LLM Judge scoring uses DeepSeek-R1-Distill-Llama-70B with reference + script cues.

## Key Results
- Larger VLMs (72B+) significantly outperform smaller models, with 35% MCQ and 30% generative gains from multimodal context
- Human performance exceeds best model at 93% MCQ accuracy vs 84% model accuracy
- LLM Judge achieves 76.8% alignment with human pairwise rankings, comparable to human-human agreement (75.2%)
- Models benefit from relevant modalities with minimal degradation from irrelevant ones, though digital/longitudinal contexts show low signal-to-noise ratios
- Generative goal inference remains challenging with only 55% relevant goal generation from best VLMs

## Why This Works (Mechanism)

### Mechanism 1
Multimodal context fusion improves goal inference accuracy over unimodal vision-only approaches. Each modality provides complementary signals—audio captures speech/intent, digital context reveals app states and recent interactions, and longitudinal history encodes user habits. When relevant modalities are available, models can cross-reference cues to disambiguate goals. The paper reports up to 35% MCQ gains and 30% generative gains when adding relevant modalities. Core assumption: The model can selectively attend to relevant modalities while ignoring noise. Evidence: [abstract] "models benefit from extra information in relevant modalities with minimal performance degradation from irrelevant modalities"; [Section 4.4] "multi-modal context significantly enhances performance over unimodal vision-only context". Break condition: When signal-to-noise ratio is too low (digital/longitudinal), smaller models fail to filter noise, causing interference rather than benefit.

### Mechanism 2
LLM-as-judge with reference grounding substitutes for human evaluation in generative goal inference. The judge receives the predicted goal, a reference goal, and script cues. It outputs relevance scores (0, 0.5, 1.0) with reasoning. The reference provides calibration; cues provide context. This achieves 76.8% alignment with human pairwise rankings, indistinguishable from human-human agreement (75.2%). Core assumption: The judge model has sufficient world knowledge and instruction-following capability to evaluate goal relevance. Evidence: [abstract] "LLM judge is superior to MCQ, and even on par with human raters"; [Section 4.3] "LLM Judge model parameterized with both reference and script cues performs best, with 76.8% alignment". Break condition: If the reference goal is ambiguous or multiple valid goals exist, the judge may penalize legitimate predictions that differ from reference wording but are functionally equivalent.

### Mechanism 3
Model scale strongly correlates with multimodal goal inference performance and noise-filtering capability. Larger models (>72B parameters) demonstrate better cross-modal reasoning and can disentangle relevant features from noisy digital/longitudinal contexts. Small models (≤3B) show interference when all modalities are present, while large models maintain performance. Core assumption: Scale provides emergent reasoning capabilities for multimodal integration. Evidence: [Section 4.4] "larger models perform significantly better on the task... correlation between model size and performance"; [Section 4.4] "larger models are better able to filter out noise from these modalities". Break condition: On-device deployment constraints may preclude large models, forcing performance tradeoffs.

## Foundational Learning

- Concept: **Multimodal egocentric perception**
  - Why needed here: Goal inference requires fusing first-person video, audio, digital state, and temporal history—each with different formats, sampling rates, and noise profiles.
  - Quick check question: Can you explain why audio transcriptions might help when video alone is ambiguous (e.g., user looking at laundry but saying "where's the laundromat")?

- Concept: **LLM-as-judge evaluation methodology**
  - Why needed here: Generative goal inference produces open-ended outputs that cannot be evaluated with fixed metrics; LLM judges provide scalable, interpretable scoring.
  - Quick check question: What inductive biases does an LLM judge introduce (reference bias, fixed-set bias) and how does WAGIBench control for them?

- Concept: **Signal-to-noise ratio in contextual modalities**
  - Why needed here: Digital and longitudinal contexts contain predominantly irrelevant information; models must learn to extract sparse relevant signals.
  - Quick check question: Why did synthetic "high-signal" modalities (D*, L*) improve performance over natural modalities (D, L)?

## Architecture Onboarding

- Component map: Video frames (32 sampled) -> VLM visual encoder; Audio -> Whisper transcription; Digital -> JSON app states; Longitudinal -> Socratic text summaries of past observations -> VLM backbone (Qwen2.5-VL, InternVL, Llama-Vision) processes multimodal context with text prompts -> Structured JSON generation with 10 action types (search, store_memory, communication, etc.) -> LLM Judge (DeepSeek-R1-Distill-Llama-70B) scores predictions against reference + cues

- Critical path: Video + audio transcription -> VLM -> structured goal JSON -> LLM Judge scoring. Digital and longitudinal contexts are optional augmentations; start with vision+audio baseline.

- Design tradeoffs:
  - Frame count: 32 frames captures temporal dynamics but increases compute; Llama-3.2 constrained to 1 frame
  - Socratic vs. raw video for longitudinal: Text summaries reduce token count but lose visual detail; raw video is prohibitive for current context windows
  - Judge reference access: Reference + cues achieves best alignment but introduces bias; cues-only is noisier but less constrained

- Failure signatures:
  - Transcription errors: Empty or hallucinated speech causes irrelevant predictions (see Figure 3, bottom-right)
  - Visual distractors: Salient but irrelevant objects (e.g., Christmas tree) override correct goal inference
  - Noise overwhelming signal: Small models predict generic goals when digital/longitudinal noise exceeds discriminative signal
  - Fallback to defaults: Model predicts "store_memory" when intent is unclear (Figure 3, top-right)

- First 3 experiments:
  1. Baseline calibration: Run Qwen2.5-VL-72B on the human study subset (586 samples, vision+audio only) and compare MCQ accuracy to reported 84%. Verify your evaluation pipeline matches the benchmark.
  2. Modality ablation: Test vision-only vs. vision+audio vs. vision+audio+digital+longitudinal on SVA, SVD, and SVL subsets. Confirm that adding relevant modalities improves scores by 20-35% as reported.
  3. Judge alignment check: Run LLM Judge on 50 generated goals with and without reference access. Compute pairwise ranking agreement against human annotations to verify 75%+ alignment.

## Open Questions the Paper Calls Out

- **Question:** How can benchmark datasets be structured to train agents capable of inferring both *when* to intervene and *what* action to take?
- **Basis in paper:** [explicit] The authors state in the Limitations that "WAGIBench assumes the user initiates all interactions, but a proactive system inferring both when and what actions to take would require a new dataset with more negative samples."
- **Why unresolved:** The current dataset only contains positive examples where a user has a specific goal; it lacks scenarios where the correct agent behavior is to do nothing.
- **What evidence would resolve it:** The creation of a dataset including negative samples (no-goal scenarios) and a corresponding evaluation of intervention timing accuracy.

- **Question:** Can specialized architectures close the performance gap between small edge-compatible models and large VLMs in noisy multimodal environments?
- **Basis in paper:** [inferred] The authors note in the Conclusion that "performance gaps on small models which are required for efficient inference on wearable/edge devices" are significant, and Section 4.4 shows small models struggle to filter noise in digital/longitudinal modalities compared to large models.
- **Why unresolved:** The paper evaluates existing off-the-shelf model families but does not propose methods to enable smaller models to handle the low signal-to-noise ratios inherent in digital and longitudinal contexts.
- **What evidence would resolve it:** A small model architecture (e.g., ≤3B parameters) demonstrating the ability to filter irrelevant digital context effectively, matching the generative performance of current 72B+ models.

- **Question:** Does incorporating dynamic user preferences and world states into longitudinal context significantly improve goal inference personalization?
- **Basis in paper:** [explicit] The authors note in the Conclusion that "longitudinal history can capture more than routine behaviors, such as relevant world states... and user preferences," and aim to "expand the dataset to include diverse longitudinal cues" in future work.
- **Why unresolved:** The current longitudinal context is synthesized primarily from routine behaviors and Socratic captions, which may limit the model's ability to leverage deep user history for prediction.
- **What evidence would resolve it:** A comparative study showing performance gains on a dataset enriched with non-routine longitudinal cues (e.g., "is the user vegetarian," "is the home clean") versus the current routine-based history.

## Limitations

- Dataset access requires external approval process due to 29 hours of video with PII considerations
- Small models (≤3B parameters) show significant interference effects from noisy digital/longitudinal modalities
- Current dataset only contains positive goal examples, lacking negative samples for intervention timing research

## Confidence

- **High confidence:** Multimodal context improves goal inference (35% MCQ gains reported with clear ablation patterns across multiple model families)
- **Medium confidence:** LLM-as-judge methodology validity (76.8% alignment with humans is strong but relies on a single judge model and reference-grounding paradigm)
- **Medium confidence:** Scale-performance correlation (systematic pattern across 6 VLM families, though absolute performance remains far from practical usefulness at 55% relevance)

## Next Checks

1. Dataset acquisition and preprocessing validation: Attempt to obtain the WAGIBench dataset through the official channel and verify frame sampling (32 frames) and transcription pipeline (Whisper-base + diarization) match specifications
2. LLM Judge calibration: Run the DeepSeek-R1-Distill-Llama-70B judge on 100 generated goals with reference access, then repeat without reference to verify the 76.8% vs 75.2% alignment gap reported
3. Small model interference test: Evaluate Qwen2.5-VL-3B on VADL vs V-only subsets to confirm the interference effect (performance degradation from irrelevant modalities) before scaling to larger models