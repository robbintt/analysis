---
ver: rpa2
title: Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement
  Learning via Latent Space Modeling
arxiv_id: '2506.12735'
source_url: https://arxiv.org/abs/2506.12735
tags:
- environment
- latent
- space
- real
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates challenges of sim-to-real transfer in model-based
  reinforcement learning (MBRL). The authors propose a latent space based method to
  quantify and mitigate the sim-to-real gap by learning mappings from both simulation
  and real environment observations into a shared latent space.
---

# Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement Learning via Latent Space Modeling

## Quick Facts
- arXiv ID: 2506.12735
- Source URL: https://arxiv.org/abs/2506.12735
- Reference count: 36
- Primary result: Sim-to-real gap quantification via latent space modeling reveals MBRL challenges with performance degradation under environment perturbations

## Executive Summary
This paper investigates challenges of sim-to-real transfer in model-based reinforcement learning (MBRL) by proposing a latent space framework to quantify and mitigate the sim-to-real gap. The authors develop a method that learns mappings from both simulation and real environment observations into a shared latent space, using environment models trained in each domain to initialize a latent dynamics model. Their approach jointly optimizes auto-encoding, prediction, and cross-domain alignment objectives. Experiments in MuJoCo HalfCheetah-v2 with various perturbations (gravity, torso length, thigh length) reveal that policies trained in simulation degrade significantly when transferred to perturbed environments, and the latent space method shows better performance under small perturbations but struggles with large gaps.

## Method Summary
The method builds on MBPO with SAC backbone, initializing latent dynamics models from single-environment training on either simulation or offline real data. It jointly trains three objectives: prediction objectives for both simulation and real data, auto-encoding objectives for reconstruction, and latent correspondence objectives for cross-domain alignment. The architecture uses domain-specific encoders (psim, preal) with shared latent dynamics, decoders for reconstruction, and a cross-domain mapping function m. Training uses branched k-step rollouts in latent space with an ensemble of probabilistic dynamics models. The method aims to quantify the sim-to-real gap through the distance between m(p_real(o)) and p_real(o) in latent space.

## Key Results
- Policies trained in standard HalfCheetah-v2 degrade significantly under perturbations: gravity (1g→2g), torso length (1x→2x), and thigh length (1x→2x)
- Latent space methods show better performance under small perturbations (e.g., 1.05× gravity) but struggle with large gaps
- Cross-domain mapping distance correlates with perturbation magnitude, providing quantitative gap measurement
- Initialization from single-environment models affects performance, with real-data initialization sometimes preferred but simulation initialization providing broader coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-domain mapping distance in latent space correlates with sim-to-real gap magnitude, enabling quantitative gap measurement.
- Mechanism: The mapping function m learns to transform real-environment latent representations to simulation latent representations. When dynamics diverge significantly, identical observations must map to different latent states, increasing the distance between m∘preal(o) and preal(o).
- Core assumption: The latent space successfully captures dynamics-relevant features that differ between domains.
- Evidence anchors: [section 5.3] Tables 4 and 5 show distance between m and identity mapping generally increases with perturbation degree (e.g., gravity acceleration: 0.2189 at 1.05× to 20.7380 at 2× on medium-v2). [section 4.1] The cross-domain mapping m provides "a quantitative measure of the environment gap."

### Mechanism 2
- Claim: Shared latent space reduces mutual interference between simulation and real-environment samples under large dynamics gaps.
- Mechanism: By learning separate encoders (psim, preal) into a common latent space, the model can distinguish identical observations from different domains while potentially aligning dynamically similar observations from different states.
- Core assumption: The auto-encoding and correspondence objectives successfully balance reconstruction fidelity with cross-domain alignment.
- Evidence anchors: [section 5.2] Under larger perturbations (1.5×, 2×), latent space methods narrow performance gap with baselines and often achieve higher combined performance. [section 4.1] "For the same observed state... we distinguish them in the latent space because of their different transition dynamics."

### Mechanism 3
- Claim: Single-environment pre-initialization reduces learning difficulty and enables mutual guidance between domains.
- Mechanism: Initialize latent dynamics model using environment model trained on either simulation or offline real data before joint training. This provides a stable starting point where one encoder begins as identity mapping.
- Core assumption: The initial single-environment model captures useful dynamics structure that transfers to the joint setting.
- Evidence anchors: [section 4.1] "We consider two strategies to initialize the latent dynamics model: using simulated data or using real-world offline data." [algorithm 1] Step 1-2 explicitly train single-environment model first, then initialize latent space.

## Foundational Learning

- Concept: **Partially Observable MDPs (POMDPs) vs. MDPs**
  - Why needed here: The paper frames sim-to-real transfer as a POMDP where the observation doesn't capture hidden environment features (e.g., friction differences). The latent space aims to recover full observability.
  - Quick check question: Can you explain why two identical robot joint configurations might require different actions in simulation vs. reality?

- Concept: **MBPO (Model-Based Policy Optimization)**
  - Why needed here: The method builds directly on MBPO's branched rollout strategy. Understanding how short-horizon model rollouts mitigate compounding errors is essential.
  - Quick check question: Why does MBPO limit model rollouts to k steps rather than using full episode rollouts?

- Concept: **Auto-encoder architectures with separate encoders**
  - Why needed here: The method uses domain-specific encoders (psim, preal) with shared latent dynamics. This differs from standard auto-encoders.
  - Quick check question: What would happen if you used a single shared encoder for both simulation and real observations?

## Architecture Onboarding

- Component map: psim -> latent states <- preal, m: real latent -> simulation latent, qsim/qreal -> reconstruction, P̄M, R̄M -> latent dynamics, πϕ -> policy

- Critical path: Single-environment pre-training -> Initialize latent models -> Joint training with three objectives -> Branched rollouts in latent space -> Policy update on augmented data

- Design tradeoffs:
  - Separate encoder m vs. using psim∘qreal: Paper chooses separate network for "training stability and interpretability" (Section 4.1)
  - Real vs. simulation initialization: Trade-off between real-data fidelity and simulation coverage
  - Latent dimension size: Too small loses dynamics information; too large hampers cross-domain alignment

- Failure signatures:
  - **Large perturbations**: Policy favors simulation performance, real-environment performance drops (Table 2-3, thigh length 2×)
  - **Low offline data quality**: KL divergence analysis (Table 6) shows latent space separates rather than aligns domains
  - **Model overfitting**: "Locally good" models perform worse than random outside training distribution (Section 6.1)

- First 3 experiments:
  1. **Baseline transfer test**: Train MBPO on standard HalfCheetah-v2, evaluate on perturbed variants (gravity, torso, thigh). Measure performance degradation curve.
  2. **Gap quantification validation**: Train latent model with m, plot distance metrics against known perturbation magnitudes. Verify correlation.
  3. **Ablation on initialization**: Compare random initialization vs. single-environment pre-training on small vs. large gap scenarios. Measure convergence speed and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the latent space framework be effectively adapted for other model-based algorithms (e.g., MOPO, PETS) that utilize uncertainty estimation differently than MBPO?
- Basis in paper: [explicit] The authors note their experiments "did not cover a broader range of approaches such as MOPO or PETS, which may exhibit different behaviors when dealing with the sim-to-real gap."
- Why unresolved: The study restricted validation to the MBPO architecture, leaving the interaction between the proposed latent alignment and other model-based optimization strategies untested.
- What evidence would resolve it: Successful integration and performance maintenance when applying the latent space objectives to algorithms like MOPO or PETS in the same perturbed environments.

### Open Question 2
- Question: How can the latent representation learning be modified to discover dynamic equivalences between environments rather than simply separating sample distributions?
- Basis in paper: [explicit] Section 6.3 states the model "often finds it difficult to discover the association between samples from different environments" and tends to distinguish rather than utilize simulation data.
- Why unresolved: The current optimization objectives encourage distinguishing domains to minimize prediction error, which inadvertently prevents the model from learning that different states (e.g., postures) may share dynamic properties across domains.
- What evidence would resolve it: A modified loss function or architectural constraint that results in a lower KL divergence ratio between cross-domain samples in the latent space while maintaining transfer performance.

### Open Question 3
- Question: Does the proposed method effectively quantify the sim-to-real gap on physical hardware where unmodeled dynamics and sensor noise differ significantly from parameter-shifted simulations?
- Basis in paper: [explicit] The authors acknowledge they "did not perform evaluations in the real physical world, which limits our ability to directly observe the method's performance in real-world scenarios."
- Why unresolved: The experiments use MuJoCo environments with parameter perturbations as proxies for "real" environments; actual physical transfer involves complexities like friction, latency, and noise not fully captured by these parameter shifts.
- What evidence would resolve it: Empirical data showing a correlation between the cross-domain mapping distance $m$ and performance degradation when transferring policies from MuJoCo to physical robotic hardware.

## Limitations
- Neural architecture details for encoders/decoders/mapping functions are unspecified, making exact reproduction difficult
- Hyperparameter settings (latent dimension, learning rates, loss weights) significantly impact performance but are not provided
- Real-environment data quality and coverage significantly affect mapping function learning (Section 6.1), but quantitative measures of data intersection with policy optimization direction are not provided
- The method struggles with large dynamics gaps where policy favors simulation performance over real-environment maintenance

## Confidence
- **High confidence**: The fundamental challenge of sim-to-real transfer in MBRL is well-established and experimentally validated. Performance degradation curves under various perturbations are clearly demonstrated.
- **Medium confidence**: The latent space method's effectiveness under small perturbations is supported, but struggles with large gaps. The correlation between cross-domain mapping distance and gap magnitude is observed but not rigorously tested across diverse scenarios.
- **Low confidence**: Claims about initialization strategy benefits are weakly supported, with no direct comparisons to random initialization in the presented results.

## Next Checks
1. **Gap quantification validation**: Systematically vary perturbation magnitude across multiple environment parameters and verify that cross-domain mapping distance in latent space scales proportionally with known gap magnitude
2. **Initialization ablation study**: Compare random initialization vs. single-environment pre-training across scenarios with small vs. large dynamics gaps, measuring both convergence speed and final performance
3. **Data quality sensitivity analysis**: Compare latent space behavior (mapping function effectiveness, alignment vs. separation) when trained on high-quality offline datasets vs. lower-quality or limited coverage datasets