---
ver: rpa2
title: 'Attention IoU: Examining Biases in CelebA using Attention Maps'
arxiv_id: '2503.19846'
source_url: https://arxiv.org/abs/2503.19846
tags:
- bias
- hair
- male
- dataset
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Attention-IoU, a metric using attention maps
  to measure model bias by quantifying spurious correlations between target attributes
  and confounding attributes. The method compares Grad-CAM attention maps for target
  attributes with those of protected attributes or ground-truth feature masks.
---

# Attention IoU: Examining Biases in CelebA using Attention Maps

## Quick Facts
- **arXiv ID**: 2503.19846
- **Source URL**: https://arxiv.org/abs/2503.19846
- **Reference count**: 40
- **Primary result**: Introduces Attention-IoU metric using attention maps to quantify spurious correlations between target attributes and confounding attributes in datasets

## Executive Summary
This paper presents Attention-IoU, a novel metric that leverages attention maps to quantify dataset biases by measuring spurious correlations between target attributes and confounding attributes. The method uses Grad-CAM attention maps to compare where models attend when making predictions for target attributes versus protected attributes or ground-truth feature masks. Validation on the synthetic Waterbirds dataset demonstrates that Attention-IoU accurately reflects bias levels, with scores decreasing proportionally as dataset bias increases. The metric is then applied to the CelebA dataset, revealing hidden confounding variables and providing insights beyond traditional accuracy-based bias metrics.

## Method Summary
The Attention-IoU metric quantifies bias by computing the intersection-over-union between attention maps for target attributes and confounding attributes. For a given target attribute (e.g., "Wearing Hat"), the method generates Grad-CAM attention maps and compares them with attention maps from protected attributes (e.g., "Male") or ground-truth feature masks. The IoU score measures the overlap between these attention distributions, with higher scores indicating stronger spurious correlations. The method involves three main steps: generating attention maps for target and confounding attributes using Grad-CAM, computing IoU between these maps, and aggregating scores across the dataset to quantify overall bias. Validation is performed on the synthetic Waterbirds dataset where ground-truth masks are available, and then extended to the real-world CelebA dataset using protected attributes as proxies for confounding features.

## Key Results
- Validation on Waterbirds dataset shows Attention-IoU accurately reflects dataset bias levels, with mask scores decreasing proportionally as bias increases from 5% to 95%
- Analysis of CelebA reveals that "Wearing Lipstick" exhibits strong bias correlation with "Male" beyond simple label correlations
- "Blond Hair" attribute shows potential hidden confounding variables that traditional label correlation analysis would miss
- The metric successfully identifies biases at fine-grained level by revealing which specific image regions models attend to when making predictions

## Why This Works (Mechanism)
Attention-IoU works by quantifying the spatial overlap between where a model attends when predicting a target attribute versus where it should attend according to ground truth or where it attends for protected attributes. By measuring this overlap through intersection-over-union of attention maps, the metric captures spurious correlations that exist in the training data. High IoU scores indicate that the model has learned to rely on confounding attributes (like background or protected characteristics) rather than the true features of the target attribute. This spatial analysis provides a more nuanced understanding of bias than simple label correlation metrics, as it reveals the actual image regions that contribute to biased predictions.

## Foundational Learning

**Grad-CAM (Gradient-weighted Class Activation Mapping)**
*Why needed*: Generates attention maps showing which image regions most influence model predictions for specific classes
*Quick check*: Verify that Grad-CAM produces meaningful heatmaps that highlight relevant features for target and protected attributes

**Intersection-over-Union (IoU)**
*Why needed*: Provides a standardized metric for measuring overlap between two spatial distributions
*Quick check*: Confirm IoU calculations correctly range from 0 (no overlap) to 1 (complete overlap)

**Spurious Correlation Analysis**
*Why needed*: Identifies when models learn incorrect associations between attributes due to dataset biases
*Quick check*: Validate that high IoU scores correspond to known dataset biases in synthetic examples

## Architecture Onboarding

**Component Map**: Input Images → Model Predictions → Grad-CAM Attention Maps → IoU Computation → Bias Quantification

**Critical Path**: The core workflow involves generating attention maps for target attributes, protected attributes, and ground-truth masks (when available), then computing IoU scores between these maps to quantify spurious correlations.

**Design Tradeoffs**: The method trades computational complexity for interpretability - generating and comparing multiple attention maps is more expensive than simple correlation metrics but provides spatial insights into model behavior. The choice of Grad-CAM as the attention mechanism balances accessibility with effectiveness, though other attention methods could potentially yield different results.

**Failure Signatures**: Low-quality attention maps will produce unreliable IoU scores. If the model architecture doesn't produce meaningful intermediate features for Grad-CAM, the attention maps will be uninformative. Additionally, if protected attributes are not true proxies for confounding features, the protected-attribute-based IoU scores may not accurately reflect bias.

**First Experiments**: 
1. Generate attention maps for target and protected attributes on a subset of CelebA images
2. Compute IoU scores and compare with known dataset statistics
3. Visualize attention map overlaps to qualitatively verify that high IoU scores correspond to spurious correlations

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Dependency on ground-truth feature masks, which are only available for synthetic datasets like Waterbirds
- Assumption that protected attributes serve as reliable proxies for confounding features in real-world datasets
- Sensitivity to attention map quality and specific Grad-CAM implementation across different model architectures
- Limited exploration of the metric's generalizability across different attention mechanisms and model types

## Confidence

**High Confidence**: The core methodology of using Attention-IoU to quantify spurious correlations between attributes is well-established and the Waterbirds validation results are robust and reproducible.

**Medium Confidence**: The extension of the metric to real-world CelebA analysis and the interpretation of results showing hidden confounding variables are reasonable but require further validation with ground-truth masks.

**Low Confidence**: The generalizability of Attention-IoU across different model architectures and attention mechanisms has not been thoroughly explored.

## Next Checks

1. Validate Attention-IoU on additional datasets where ground-truth feature masks are available to confirm the correlation between mask-based and protected-attribute-based scores across different bias types.

2. Test the metric's sensitivity to different attention mechanisms (e.g., attention roll-out, integrated gradients) and model architectures to establish robustness.

3. Conduct ablation studies to quantify the impact of attention map quality and Grad-CAM parameters on the final Attention-IoU scores.