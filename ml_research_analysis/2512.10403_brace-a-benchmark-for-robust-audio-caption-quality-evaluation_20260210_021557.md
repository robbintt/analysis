---
ver: rpa2
title: 'BRACE: A Benchmark for Robust Audio Caption Quality Evaluation'
arxiv_id: '2512.10403'
source_url: https://arxiv.org/abs/2512.10403
tags:
- caption
- audio
- captions
- benchmark
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BRACE, a benchmark designed to evaluate
  reference-free audio caption quality metrics and large audio language models. BRACE
  consists of two sub-benchmarks: BRACE-Main for comparing caption quality, and BRACE-Hallucination
  for detecting hallucinated content.'
---

# BRACE: A Benchmark for Robust Audio Caption Quality Evaluation

## Quick Facts
- **arXiv ID**: 2512.10403
- **Source URL**: https://arxiv.org/abs/2512.10403
- **Reference count**: 40
- **Key outcome**: BRACE reveals CLAP-based metrics achieve ~70 F1-score and LALMs ~63-96 F1-score on audio caption evaluation, exposing significant limitations in current audio-text alignment models.

## Executive Summary
BRACE introduces a comprehensive benchmark for evaluating reference-free audio caption quality metrics and large audio language models. The benchmark consists of BRACE-Main for pairwise caption comparison and BRACE-Hallucination for detecting hallucinated content. Constructed through high-quality filtering, LLM-based caption corruption, and human annotation, BRACE provides rigorous evaluation across 2,496 filtered audio-caption pairs and 10,315 hallucination detection pairs. Experiments demonstrate that even state-of-the-art CLAP-based metrics and LALMs struggle with fine-grained distinctions and hallucination detection, with top performers achieving only 70-96 F1-scores across different tasks.

## Method Summary
BRACE evaluates reference-free audio caption quality through two sub-benchmarks. BRACE-Main contains 2,496 filtered audio-caption pairs from AudioCaps and Clotho datasets, requiring binary preference selection between caption pairs (HH, HM, MM types). BRACE-Hallucination includes 10,315 pairs with single-noun substitutions generated via GPT-4o. Evaluation uses CLAP-based metrics with sliding window aggregation (SLIDE-CLAP) and LALMs (LTU, GAMA, Qwen-Audio-Chat, Qwen2-Audio-Instruct, AF2) with various prompt templates. The benchmark employs high-quality filtering via LLM agreement and human annotation to ensure reliability, achieving improved inter-annotator agreement scores.

## Key Results
- CLAP-based metrics achieve only 70.01 F1-score on BRACE-Main, with SLIDE-CLAP providing more stable evaluation than standard CLAP
- Best-performing LALM reaches 63.19 F1-score on BRACE-Main, with significant performance gaps across prompt templates
- BRACE-Hallucination shows CLAP models scoring 88.26 F1 while top LALMs achieve 76-96 F1, revealing limitations in hallucination detection
- Performance varies significantly across pair types, with HM pairs being more challenging than HH pairs
- LALMs exhibit position bias, with GAMA consistently favoring caption_0 (~96% of the time)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sliding window aggregation of audio embeddings improves evaluation stability and reproducibility compared to single-window sampling in CLAP-based metrics.
- **Mechanism**: Standard CLAP models use fixed-length audio windows (5-10 seconds depending on variant). Longer audio requires random truncation, introducing run-to-run variance. SLIDE-CLAP segments audio into overlapping windows, encodes each independently, then averages the resulting embeddings. This aggregates information across the full audio clip rather than relying on a random segment.
- **Core assumption**: Acoustic information relevant to caption alignment is distributed throughout the audio clip rather than concentrated in a single brief moment.
- **Evidence anchors**:
  - [Page 7-8]: "Since CLAP models operate on short fixed-length segments, longer audio inputs must be truncated or sampled. This stochastic truncation introduces inconsistency across runs... To address this issue, we introduce a sliding window strategy."
  - [Page 19]: Figure 7 shows the SLIDE-CLAP architecture with averaging formula.
- **Break condition**: When critical acoustic events are extremely brief (sub-1-second) and get diluted across multiple non-informative windows; or when computational budget prohibits multiple encoder passes.

### Mechanism 2
- **Claim**: Filtering audio-caption pairs by inter-annotator agreement improves benchmark reliability and meta-evaluation validity.
- **Mechanism**: Three annotators label each caption pair. Pairs with low consensus (absolute score < 2) are removed. This eliminates ambiguous cases where human judgment itself is inconsistent, ensuring the benchmark tests model alignment with reliable human preferences rather than edge cases with no ground truth.
- **Core assumption**: High inter-annotator agreement indicates objective quality differences between captions; disagreement indicates ambiguous cases inappropriate for evaluation.
- **Evidence anchors**:
  - [Page 5]: "As shown in Table 10, the Fleiss-Kappa score significantly improves, with AudioCaps increasing from 0.38 to 0.98 and Clotho increasing from 0.44 to 0.84."
  - [Page 24]: Table 10 explicitly shows Kappa improvement post-filtering.
- **Break condition**: When application domains inherently involve subjective preferences (e.g., creative writing assessment); when annotator pool is unrepresentative of target users.

### Mechanism 3
- **Claim**: Targeted noun substitution using LLMs creates controllable, realistic hallucination test cases that probe fine-grained audio-text misalignment.
- **Mechanism**: GPT-4o identifies nouns in captions and substitutes them with alternatives that (1) maintain grammatical coherence and (2) create significant semantic shifts. This generates "plausible but wrong" captions rather than obviously broken text, testing whether models detect subtle acoustic-semantic mismatches.
- **Core assumption**: Real-world hallucinations in audio-language models manifest primarily as entity substitutions rather than structural or temporal errors; single-word changes are sufficient to probe alignment quality.
- **Evidence anchors**:
  - [Page 6]: "The new noun must fit naturally within the sentence... and differ significantly in meaning from the original noun, introducing a clear change in the sentence's context."
  - [Page 31-32]: Figure 16 shows the hallucination generation prompt with examples.
- **Break condition**: When deployed models make temporal or relational hallucinations (wrong event order, fabricated interactions) that noun substitution doesn't capture; when hallucinations involve acoustic feature misattribution (e.g., "low frequency" when audio is high frequency).

## Foundational Learning

- **Contrastive Learning (CLAP Architecture)**
  - Why needed here: CLAPScore is the primary metric being evaluated; understanding how audio-text embeddings are learned and aligned is essential to interpreting results and failure modes.
  - Quick check question: If a CLAP model assigns high similarity to an audio-caption pair, what does that actually guarantee about their relationship? (Hint: semantic relatedness, not factual accuracy.)

- **Reference-Free vs. Reference-Based Evaluation**
  - Why needed here: The entire benchmark addresses the reference-free setting; distinguishing these paradigms is necessary to understand what problem BRACE solves and why it matters.
  - Quick check question: In a reference-free setting, what information source replaces the ground-truth caption? What can go wrong?

- **Hallucination Detection in Multimodal Models**
  - Why needed here: BRACE-Hallucination explicitly tests this failure mode; recognizing hallucination types helps interpret model limitations and design improvements.
  - Quick check question: Why might an audio-language model claim a "piano" is present when only a "keyboard" was played? What acoustic features might confuse the model?

- **Meta-Evaluation and Benchmark Validity**
  - Why needed here: BRACE itself evaluates other metrics; understanding what makes a benchmark trustworthy is critical for using it appropriately.
  - Quick check question: If a model achieves 70% accuracy on BRACE-Main, does that mean it's a good caption evaluator? What additional evidence would you need?

## Architecture Onboarding

- **Component map**: BRACE-Main (2,496 pairs) -> CLAP/SLIDE-CLAP evaluation -> LALM evaluation -> F1-score computation; BRACE-Hallucination (10,315 pairs) -> binary classification -> F1-score computation

- **Critical path**: 1. Load audio-caption pairs from benchmark (JSON format with pair IDs, captions, ground-truth labels). 2. For CLAP: encode audio with sliding windows, encode each caption, compute cosine similarities, compare scores. 3. For LALM: format prompt with audio input and both captions, generate response, parse preference output. 4. Aggregate predictions, compute F1-score against human annotations. 5. Analyze performance breakdown by pair type (HH, HM, MM) and dataset (AudioCaps vs. Clotho).

- **Design tradeoffs**: Binary preference vs. continuous scoring (simplifies evaluation but loses granularity); Human annotation vs. automated generation (expensive but gold standard vs. scalable but may not match real failures); Prompt complexity for LALMs (simple prompts yield better performance, complex prompts introduce instruction-following failures); Sliding window size (larger windows capture more context but increase compute).

- **Failure signatures**: CLAP: High variance across runs (>1% F1 swing) indicates insufficient window coverage or inconsistent preprocessing. LALM: Position bias (always selecting caption_0 or caption_1 regardless of content) indicates prompt formatting issues or training artifacts. LALM: "Unknown" or "tie" outputs on >20% of pairs indicates prompt misalignment with model capabilities. Both: Performance gap between HM and HH pairs (>10% F1 difference) suggests difficulty with fine-grained distinctions between high-quality captions.

- **First 3 experiments**: 1. Baseline CLAP evaluation: Run standard CLAP (no sliding window) on BRACE-Main 10 times with different random seeds; compute mean, std, min, max F1-scores. Compare against Table 5 to validate setup. 2. SLIDE-CLAP comparison: Implement sliding window with 10-second windows, 1-second hop; measure stability improvement and computational overhead. Verify results match reported ~70 F1 ceiling. 3. Prompt sensitivity analysis for one LALM: Test a single open-source LALM (e.g., Qwen-Audio-Chat) with naive, simple, and complex prompts; output distribution analysis for position bias and "unknown" rate. Compare against Table 4 patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's focus on noun substitution may underrepresent other hallucination types (temporal, relational, acoustic feature misattribution)
- LALM performance variance across prompt templates suggests results are highly sensitive to prompt engineering rather than model capabilities alone
- The reported 70% ceiling for CLAP-based metrics likely reflects fundamental constraints in single-word substitution tests rather than absolute limits of audio-text alignment

## Confidence
- **High confidence**: Sliding window mechanism (explicit architectural description and empirical validation)
- **Medium confidence**: Filtering methodology (improved Kappa scores but no external validation)
- **Medium confidence**: Noun substitution hallucination generation (plausible mechanism but limited to one hallucination type)
- **Low confidence**: LALM absolute performance comparisons due to prompt sensitivity and position bias issues

## Next Checks
1. Replicate CLAP stability analysis with 10 random seeds on BRACE-Main; verify >1% F1 variance reduction with SLIDE-CLAP implementation.
2. Test LALM performance sensitivity to caption ordering; verify position bias elimination through swapped inputs.
3. Expand hallucination detection beyond noun substitution; evaluate temporal and relational hallucination detection using synthetic error types.