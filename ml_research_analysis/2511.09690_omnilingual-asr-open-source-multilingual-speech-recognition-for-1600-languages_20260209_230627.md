---
ver: rpa2
title: 'Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages'
arxiv_id: '2511.09690'
source_url: https://arxiv.org/abs/2511.09690
tags:
- languages
- speech
- language
- data
- omnilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Omnilingual ASR addresses the challenge of extending automatic
  speech recognition to over 1,600 languages, including more than 500 previously unsupported
  ones, by introducing a massively multilingual model family. The core method leverages
  self-supervised pre-training of wav2vec 2.0 models up to 7B parameters, followed
  by fine-tuning with a large LLM-inspired decoder.
---

# Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages

## Quick Facts
- arXiv ID: 2511.09690
- Source URL: https://arxiv.org/abs/2511.09690
- Reference count: 34
- Key outcome: Omnilingual ASR extends automatic speech recognition to over 1,600 languages, including more than 500 previously unsupported ones, using self-supervised pre-training and in-context learning, with improved performance especially in low-resource conditions.

## Executive Summary
Omnilingual ASR introduces a massively multilingual speech recognition system capable of supporting over 1,600 languages, including more than 500 new languages not previously supported by existing models. The system is built on a wav2vec 2.0 backbone, pre-trained up to 7 billion parameters, and fine-tuned with a large LLM-inspired decoder to enable zero-shot ASR on unseen languages using only a few in-context examples. Evaluations demonstrate that Omnilingual ASR outperforms prior systems like Whisper and MMS, particularly in low-resource scenarios, while also providing a scalable, open-source framework for community-driven language expansion.

## Method Summary
The core approach leverages self-supervised pre-training of wav2vec 2.0 models up to 7 billion parameters, followed by fine-tuning with a large LLM-inspired decoder. This architecture enables zero-shot ASR on previously unseen languages using just a few in-context examples, without requiring new training data. The system is trained on over 120,000 hours of speech data, including community-sourced recordings, with careful quality control. Models are released in sizes from 300M to 7B parameters to accommodate diverse use cases, and the framework supports ongoing community contributions for expanding language coverage.

## Key Results
- Average character error rates as low as 1.1% on high-resource languages.
- Robust generalization to new languages, with significant improvements over Whisper and MMS in low-resource conditions.
- Enables zero-shot ASR on previously unsupported languages using in-context examples.

## Why This Works (Mechanism)
The system's effectiveness stems from its use of self-supervised pre-training on massive multilingual data, which provides a strong universal speech representation, and the integration of a large decoder inspired by LLMs, enabling in-context learning. This combination allows the model to generalize to new languages with minimal examples, overcoming data scarcity for low-resource languages.

## Foundational Learning
- **Self-supervised pre-training**: Learns speech representations from unlabeled audio; needed to bootstrap knowledge across many languages without manual annotations.
- **Fine-tuning with LLM-inspired decoder**: Enables in-context learning; needed to adapt to new languages without additional training data.
- **Community-sourced data collection**: Expands language coverage; needed to address the lack of high-quality data for low-resource languages.
- **Quality control in data curation**: Ensures model robustness; needed to prevent noise and bias from degrading performance.

## Architecture Onboarding
- **Component map**: wav2vec 2.0 encoder -> LLM-inspired decoder -> ASR output
- **Critical path**: Pre-training -> Fine-tuning -> Zero-shot inference
- **Design tradeoffs**: Large models (up to 7B) improve performance but increase computational cost; smaller models trade some accuracy for accessibility.
- **Failure signatures**: Catastrophic forgetting in fine-tuning, bias from unrepresentative data, limited zero-shot generalization on unseen languages.
- **First experiments**: 1) Compare Omnilingual ASR vs Whisper on a held-out low-resource language set. 2) Test zero-shot ASR on 5 new, truly unseen languages. 3) Conduct bias and fairness audit on community-sourced data.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains rely on comparisons with systems trained on different datasets and architectures, complicating attribution of benefits to in-context learning.
- Risk of catastrophic forgetting due to wide language diversity and lack of detailed ablation studies.
- Community-sourced data collection lacks transparency regarding representativeness and potential demographic or linguistic biases.
- Error rates for low-resource languages remain significantly higher than for high-resource languages, indicating room for improvement.
- Zero-shot ASR evaluation is limited to a small set of unseen languages, raising questions about robustness across the full 1,600+ language scope.

## Confidence
- **High**: Model architecture and training pipeline, basic performance metrics on high-resource languages
- **Medium**: Zero-shot ASR performance and generalization to unseen languages
- **Low**: Claims about full coverage for all 1,600+ languages, robustness in diverse real-world conditions

## Next Checks
1. Conduct ablation studies comparing Omnilingual ASR with and without in-context learning on the same multilingual dataset to quantify its specific contribution.
2. Expand evaluation to a broader set of truly unseen languages, especially from underrepresented families, to verify zero-shot generalization.
3. Perform bias and fairness audits on the community-sourced dataset to assess demographic and linguistic representativeness.