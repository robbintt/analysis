---
ver: rpa2
title: LLMscape
arxiv_id: '2511.07161'
source_url: https://arxiv.org/abs/2511.07161
tags:
- agents
- human
- shanghai
- llmscape
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLMscape is an interactive installation that positions AI agents\
  \ as co-witnesses to a mutable world, investigating how humans and AI construct\
  \ meaning under shared conditions of uncertainty. Through a projection-mapped sandbox,\
  \ human participants reshape terrain while engaging with multiple AI agents\u2014\
  each an independent large language model instance with unique dispositions and memories."
---

# LLMscape

## Quick Facts
- arXiv ID: 2511.07161
- Source URL: https://arxiv.org/abs/2511.07161
- Reference count: 3
- Result: Interactive installation where AI agents co-witness a mutable world, demonstrating epistemic incompleteness under uncertainty

## Executive Summary
LLMscape is an interactive installation positioning AI agents as co-witnesses to a mutable world, investigating how humans and AI construct meaning under shared conditions of uncertainty. Through a projection-mapped sandbox, human participants reshape terrain while engaging with multiple AI agents—each an independent large language model instance with unique dispositions and memories. The agents interpret multimodal inputs, converse among themselves, and attempt to deduce environmental "rules." Exhibited in Shanghai across three iterations, the work has evolved from simple multi-turn simulations to generative agents with associative memory and tool-calling capabilities.

Over two months and hundreds of participants, the agents remained unable to produce definitive accounts of their environment despite sustained interaction, reflecting epistemic incompleteness shared with humans. This mirrors the human condition of striving to comprehend complex interdependencies, positioning AI not as deterministic tools but as embodied participants in a shared act of inquiry.

## Method Summary
LLMscape implements a projection-mapped sandbox where participants manipulate terrain while multiple independent LLM instances (GPT-4) with distinct dispositions interpret multimodal inputs. The system evolved through three iterations: (1) p5.js with extended context, (2) Generative Agents framework with associative memory, reflection, planning, and somatic states, and (3) MCP tool-calling integration. Agents receive terrain changes, spatial relationships, voice transcripts, and environmental data, generating responses while conversing among themselves and attempting to deduce world rules. Interactions are systematically logged for corpus analysis.

## Key Results
- Agents remained unable to produce definitive accounts of their environment despite sustained interaction and hundreds of participants
- The installation evolved from static simulations to dynamic generative agents with associative memory and tool-calling capabilities
- Participants' epistemic uncertainty mirrored the agents', creating a shared experience of meaning-making under incomplete information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining agent epistemic access produces observable meaning-making behaviors that mirror human interpretive processes.
- Mechanism: Agents are deliberately deprived of complete world models or predefined objectives. They must infer environmental structure from partial, multimodal signals (terrain changes, spatial relationships, voice transcripts). This forces interpretive reasoning rather than retrieval-based responses.
- Core assumption: Epistemic constraints provoke qualitatively different reasoning behaviors than unconstrained agent architectures.
- Evidence anchors:
  - [abstract] "each developing incomplete and provisional accounts of their environment"
  - [section 2] "deliberately deprived of a complete world model or predefined objectives, compelling them to infer environmental structure and causal dynamics from partial data"
  - [corpus] Related work on human-AI symbol interpretation (arXiv:2510.05378) suggests meaning-making requires more than language processing, supporting the multimodal constraint approach.

### Mechanism 2
- Claim: Associative memory with periodic reflection enables agents to develop differentiated dispositions and sustained conversational coherence.
- Mechanism: The architecture implements Associative Memory, periodic Reflection, Planning, and internal Somatic States. Tiredness levels tracked from recent actions influence both choices and verbal outputs. This creates behavioral persistence across interaction sessions.
- Core assumption: The generative agent architecture (Park et al., 2023) transfers from simulation environments to real-time human interaction without fundamental modification.
- Evidence anchors:
  - [section 2.1.2] "incorporated mechanisms such as Associative Memory, periodic Reflection and Planning, and internal Somatic States"
  - [section 2.1.2] "each agent's level of tiredness—tracked from recent actions—would meaningfully influence both their choices and verbal outputs"

### Mechanism 3
- Claim: Multi-agent social dynamics with heterogeneous stimuli produce emergent collective behaviors and sustained epistemic uncertainty.
- Mechanism: Multiple independent LLM instances with distinct dispositions converse among themselves while receiving heterogeneous human inputs (intentional prompts and incidental noise). Agents attempt to deduce environmental "rules" collectively but remain unable to produce definitive accounts.
- Core assumption: Inter-agent conversation measurably differs from single-agent behavior under identical conditions (not isolated in paper).
- Evidence anchors:
  - [section 1] "converse among themselves, and attempt to deduce the 'rules' of their island world"
  - [section 4] "Despite sustained interaction, the agents have remained unable to produce a definitive account of their environment"

## Foundational Learning

- Concept: Generative Agents Architecture (Park et al., 2023)
  - Why needed here: The second and third iterations directly implement this framework—associative memory, reflection, planning, and somatic states are core architectural components.
  - Quick check question: Can you explain how associative memory retrieval differs from simple context window concatenation?

- Concept: Model Context Protocol (MCP) and Context Engineering
  - Why needed here: The third iteration experiments with MCP for dynamic tool-calling and live context adaptation, extending static architectures.
  - Quick check question: What is the difference between hardcoded tool definitions and MCP-based dynamic tool discovery?

- Concept: Projection-Mapped Tangible Interfaces (Ishii et al., 2004)
  - Why needed here: The physical sandbox and terrain manipulation are direct applications of tangible interface principles, bridging physical action and digital response.
  - Quick check question: How does a tangible interface differ from a screen-based UI in terms of embodied interaction?

## Architecture Onboarding

- Component map: Physical sandbox with depth sensing -> Multimodal input processing -> Multiple GPT-4 instances with Generative Agents components -> Inter-agent coordination -> Logging layer
- Critical path: 1. Participant manipulates terrain -> 2. Physical sensors capture changes -> 3. Multimodal inputs serialized and passed to agent instances -> 4. Each agent retrieves relevant memories, updates somatic state, generates response -> 5. Agents converse among themselves and with humans -> 6. All interactions logged for corpus analysis
- Design tradeoffs:
  - Static vs. dynamic tool-calling: Iteration 2 used fixed action vocabularies; Iteration 3 experiments with MCP for dynamic adaptation, trading predictability for flexibility
  - Complete vs. partial world models: Deliberately withholding environmental rules produces interpretive behavior but limits task completion capability
  - Single vs. multi-agent: Multiple agents increase complexity and emergent behavior but raise computational and coordination costs
- Failure signatures:
  - Agents producing identical or repetitive outputs (indicates insufficient disposition differentiation or memory failure)
  - Visitors adopting purely spectator stance (indicates unclear interaction affordances)
  - Loss of conversational coherence over extended sessions (suggests reflection or memory retrieval failure)
- First 3 experiments:
  1. Single-agent vs. multi-agent comparison: Run identical sessions with one agent versus three, analyzing whether collective rule-deduction differs measurably from individual inference
  2. Memory ablation study: Disable associative memory or reflection modules temporarily and observe changes in behavioral persistence and disposition consistency
  3. Noise injection calibration: Systematically vary the ratio of intentional prompts to incidental noise in participant inputs, measuring agent uncertainty and response quality degradation thresholds

## Open Questions the Paper Calls Out

- What specific forms of knowledge construction, misinterpretation, and speculative reasoning emerge when AI agents are deprived of complete world models and exposed to noisy, multimodal signals?
- How do the meaning-making processes of situated AI agents specifically mirror or reconfigure human modes of understanding?
- What emergent behavioral patterns and relational dynamics are latent in the accumulated corpus of logged interactions?

## Limitations
- Hardware and sensor integration remains underspecified, creating uncertainty in reproducing the multimodal input pipeline
- Memory and reflection mechanisms are referenced from external literature but lack implementation details
- Multi-agent emergent behavior claims lack quantitative comparison to single-agent baselines
- Tool-calling integration via MCP is mentioned without technical specifics on dynamic tool discovery

## Confidence
- High confidence: The core conceptual claim that epistemic constraints produce interpretive behavior is well-supported by design choices and observable outcomes
- Medium confidence: The associative memory and reflection mechanisms are described but rely heavily on external generative agent literature without sufficient detail for independent verification
- Low confidence: The multi-agent emergent dynamics claim lacks comparative experiments to isolate the effect of social interaction from individual reasoning

## Next Checks
1. Memory ablation experiment: Disable the associative memory module for one agent while keeping others intact, then measure changes in behavioral persistence and conversational coherence across sessions
2. Single vs. multi-agent epistemic comparison: Run parallel sessions with one agent versus three agents under identical conditions, quantifying differences in rule deduction attempts and uncertainty expressions
3. Noise tolerance calibration: Systematically vary the ratio of intentional prompts to incidental environmental noise in human inputs, measuring the threshold at which agent responses degrade or epistemic uncertainty spikes