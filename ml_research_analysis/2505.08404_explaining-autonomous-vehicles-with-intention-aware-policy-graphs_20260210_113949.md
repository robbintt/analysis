---
ver: rpa2
title: Explaining Autonomous Vehicles with Intention-aware Policy Graphs
arxiv_id: '2505.08404'
source_url: https://arxiv.org/abs/2505.08404
tags:
- vehicle
- desires
- intention
- behaviour
- lane
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a post-hoc, model-agnostic explainability approach
  for autonomous vehicles using Intention-aware Policy Graphs (IPGs) to generate teleological
  explanations of vehicle behavior. The method interprets vehicle decisions through
  folk-psychological concepts of desires and intentions, enabling both global and
  local explanations in urban driving scenarios.
---

# Explaining Autonomous Vehicles with Intention-aware Policy Graphs

## Quick Facts
- arXiv ID: 2505.08404
- Source URL: https://arxiv.org/abs/2505.08404
- Reference count: 28
- 75.7% intention attribution rate with 92.0% reliability on nuScenes dataset

## Executive Summary
This paper presents a post-hoc, model-agnostic explainability framework for autonomous vehicles using Intention-aware Policy Graphs (IPGs). The approach generates teleological explanations of vehicle behavior by interpreting decisions through folk-psychological concepts of desires and intentions. By modeling vehicle decision-making as transitions between intentions, the method provides both global and local explanations in urban driving scenarios, addressing the need for interpretable explanations that align with human understanding of agency and goal-directed behavior.

The IPG framework operates by discretizing vehicle states into predicates, defining desires as desirable states, and using a commitment function to determine when a desire transitions to an intention. This creates a compact policy graph where nodes represent intentions and edges represent policy transitions. The method is evaluated on the nuScenes dataset, demonstrating its ability to explain 75.7% of vehicle behavior with high reliability while also uncovering potential vulnerabilities in the dataset itself.

## Method Summary
The Intention-aware Policy Graph approach constructs explanations by first discretizing continuous state information into logical predicates (such as PedestrianNearby, LaneKeeping, HighSpeed). These predicates are then used to define vehicle desires - desirable states that the vehicle aims to achieve. A commitment function evaluates the desirability of each desire based on the current state, and when the commitment exceeds a threshold, a desire becomes an intention.

The IPG is constructed by tracking transitions between intentions as the vehicle moves through different states. This creates a graph where nodes represent specific intentions and edges represent the policies or actions taken to transition between intentions. The framework is model-agnostic, meaning it can be applied to any autonomous vehicle system regardless of its underlying decision-making architecture. For global explanations, the approach analyzes the entire trajectory to build a comprehensive policy graph. For local explanations, it generates intention-aware explanations for specific decisions by examining the immediate context and potential alternative intentions.

## Key Results
- Intention attribution successfully explains 75.7% of vehicle behavior with 92.0% reliability on nuScenes dataset
- Uncovered dataset vulnerabilities, including the absence of stop-sign-related intentions at night (0.0% attribution vs 2.6-3.6% daytime)
- Revealed unsafe driving patterns, identifying trajectories that violated traffic rules despite maintaining safety

## Why This Works (Mechanism)
The IPG approach works by leveraging folk-psychological concepts that humans naturally use to understand and predict the behavior of other agents. By representing vehicle decisions as transitions between intentions (formulated desires), the method creates explanations that align with how people intuitively reason about goal-directed behavior. The discretization of continuous states into predicates provides a bridge between raw sensor data and human-interpretable concepts, while the commitment function serves as a formal mechanism for determining when desires become actionable intentions. This combination of formal modeling with intuitive psychological concepts enables explanations that are both computationally tractable and cognitively aligned with human understanding.

## Foundational Learning
- **Intention Theory**: Understanding how desires become intentions is crucial for modeling goal-directed behavior in autonomous systems. Quick check: Verify that commitment threshold correctly captures transition from desire to intention.
- **State Discretization**: Converting continuous sensor data into discrete predicates enables logical reasoning about vehicle states. Quick check: Ensure predicates cover all relevant aspects of driving scenarios.
- **Policy Graph Construction**: Building graphs of intention transitions provides a compact representation of decision-making patterns. Quick check: Validate that the graph captures all significant behavioral patterns.
- **Folk Psychology**: Using human-intuitive concepts of desires and intentions makes explanations more accessible to non-expert users. Quick check: Confirm that generated explanations align with human expectations.

## Architecture Onboarding

**Component Map**: Sensor Data -> State Discretizer -> Predicate Generator -> Desire Evaluator -> Commitment Function -> Intention Tracker -> Policy Graph Builder -> Explanation Generator

**Critical Path**: The core explanation generation pipeline flows from raw sensor data through state discretization, desire evaluation, and intention tracking to produce the final IPG and explanations. The critical path is: Sensor Data → State Discretizer → Predicate Generator → Desire Evaluator → Commitment Function → Intention Tracker.

**Design Tradeoffs**: The approach trades computational complexity for interpretability by using discrete predicates rather than continuous representations. The commitment threshold balances between overly sensitive intention changes and missing important behavioral shifts. The choice of predefined desires and intentions limits coverage but ensures explanations remain focused and interpretable.

**Failure Signatures**: Common failure modes include: 1) Low intention attribution rate (<50%) indicating poor predicate coverage or threshold settings, 2) High false positive rate suggesting overly sensitive commitment function, 3) Missing rare but important intentions due to limited ontology, 4) Poor nighttime performance revealing sensor or annotation limitations.

**First Experiments**:
1. Test intention attribution on simple, controlled scenarios with known ground truth intentions
2. Evaluate predicate coverage by measuring the percentage of states that satisfy at least one predicate
3. Assess commitment threshold sensitivity by varying C values and measuring intention stability

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do IPG-based explanations affect trust calibration among non-expert users compared to alternative explanation methods?
- Basis in paper: "Future research will include a user study to assess the impact of the generated explanations on trust among non-expert users."
- Why unresolved: The current work evaluates interpretability via intention metrics (75.7% attribution, 92.0% reliability) but does not measure human-centered outcomes like trust, satisfaction, or decision-making quality.
- What evidence would resolve it: A controlled user study comparing IPG explanations against baseline XAI methods (e.g., SHAP, Grad-CAM), measuring trust scores, explanation satisfaction ratings, and task performance.

### Open Question 2
- Question: Can the IPG framework be extended to model multi-agent interactions by inferring intentions of other traffic participants?
- Basis in paper: "We plan to expand the framework to account for multi-agent interactions, incorporating desires and intentions of other traffic participants."
- Why unresolved: The current approach treats other agents as static environmental features via predicates (PedestrianNearby, ObjectsNearby) without modeling their goal-directed behavior or predicting their future actions.
- What evidence would resolve it: Demonstration of IPGs incorporating inferred intentions of non-ego vehicles/pedestrians, with evaluation on coordination scenarios (e.g., merging, negotiation at intersections).

### Open Question 3
- Question: What causes the complete absence of stop-sign-related intention attribution under nighttime conditions?
- Basis in paper: Table 2 shows Approach Stop Sign and Ignore Stop Sign intentions drop to 0.0 at night versus 0.026–0.036 during daytime. Authors note this "may uncover limitations in the vehicle's visual system, inaccuracies in human-annotated traffic sign labels within the nuScenes dataset, or biases in the collection of nighttime scenes."
- Why unresolved: The paper identifies the anomaly but does not isolate whether the cause is sensor limitations, annotation errors, or dataset distribution shift.
- What evidence would resolve it: Analysis of raw sensor data and annotations for nighttime scenes with stop signs; replication on datasets with verified nighttime stop-sign annotations.

### Open Question 4
- Question: How sensitive are intention attributions to the choice of state discretisation predicates and commitment threshold C?
- Basis in paper: The paper acknowledges "extensive domain knowledge when defining state discretisation and vehicle desires" as a limitation, and notes C=0.5 was chosen as a trade-off without systematic optimization.
- Why unresolved: It is unclear whether the 75.7% intention attribution rate and 92.0% reliability are robust to alternative predicate sets or threshold values.
- What evidence would resolve it: Ablation studies varying predicate sets and C values, reporting intention metrics across configurations.

## Limitations
- Method performance constrained by quality and coverage of underlying nuScenes dataset, which may not represent all real-world driving scenarios
- 24.3% of vehicle behavior remains unexplained, limiting applicability in complex or rare situations
- Reliance on predefined ontologies of intentions and desires may not capture all possible driving motivations or emergent behaviors

## Confidence
- High confidence in method's effectiveness in generating teleological explanations and uncovering dataset vulnerabilities
- Medium confidence in generalizability across different driving environments and alignment between human-interpretable intentions and actual vehicle decision-making
- Low confidence in performance for edge cases and scalability to handle all possible driving scenarios

## Next Checks
1. Test the IPG approach on additional autonomous vehicle datasets (e.g., Waymo Open Dataset, Argoverse) to assess generalizability across different data sources and driving environments.
2. Conduct user studies with human participants to evaluate the interpretability and usefulness of the generated explanations in real-world scenarios.
3. Implement the IPG method in a closed-loop simulation environment to assess its impact on human trust and acceptance of autonomous vehicles during live interactions.