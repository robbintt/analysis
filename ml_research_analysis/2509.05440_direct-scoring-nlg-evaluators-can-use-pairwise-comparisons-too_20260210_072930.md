---
ver: rpa2
title: Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too
arxiv_id: '2509.05440'
source_url: https://arxiv.org/abs/2509.05440
tags:
- summary
- story
- evaluation
- article
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a direct-scoring NLG evaluator that uses synthetic
  summaries to simulate pairwise comparisons, addressing the limitation of pairwise
  methods that cannot produce absolute scores for thresholding use cases. The method
  generates synthetic summaries of varying quality for each document, then computes
  weighted scores by comparing a machine-generated summary to each synthetic reference
  and aggregating the probabilities over "Better/Worse/Similar" judgments.
---

# Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too

## Quick Facts
- arXiv ID: 2509.05440
- Source URL: https://arxiv.org/abs/2509.05440
- Authors: Logan Lawrence; Ashton Williamson; Alexander Shelton
- Reference count: 24
- Key outcome: Achieves sample-level correlations of 0.45, 0.36, and 0.40 on SummEval, TopicalChat, and HANNA benchmarks respectively

## Executive Summary
This paper proposes a novel direct-scoring NLG evaluator that bridges the gap between pairwise comparison methods and absolute scoring requirements. The method generates synthetic summaries of varying quality levels for each document and uses these as references to compute weighted scores through comparative judgments. By aggregating "Better/Worse/Similar" probabilities across calibrated quality anchors, the approach transforms pairwise comparisons into direct scores suitable for thresholding use cases. Evaluations show comparable performance to state-of-the-art pairwise evaluators with average improvements of +0.03, -0.03, and +0.05 across three benchmarks.

## Method Summary
The method generates N=5 synthetic summaries per document and quality dimension, spanning quality extremes (worst to best) through explicit prompts and recursive intermediate-quality generation. For each candidate summary, the system compares it against each synthetic reference, extracting token probabilities for "Better", "Worse", and "Similar" judgments. These probabilities are normalized via softmax and aggregated into a weighted score using the formula s(·) = Σᵢ [i, -i, 0] · [p(Better|i), p(Worse|i), p(Similar|i)]. The approach is evaluated on SummEval, TopicalChat, and HANNA benchmarks using sample-level Spearman correlation with human judgments.

## Key Results
- Sample-level correlations of 0.45, 0.36, and 0.40 on SummEval, TopicalChat, and HANNA benchmarks respectively
- Average improvements of +0.03, -0.03, and +0.05 over strongest pairwise baselines
- Probability extraction critical: 0.99 correlation gap over sampling at n=1000
- More instruction-following capable LLMs yield better evaluators (Llama-3.1-8B MMLU 71.3 vs. OLMo-2-7B MMLU 61.3)

## Why This Works (Mechanism)

### Mechanism 1
Synthetic references of calibrated quality levels enable absolute scoring through relative comparisons. The method generates 5 synthetic summaries spanning quality extremes recursively. By comparing a target summary against each calibrated reference and aggregating comparison outcomes, relative judgments accumulate into an absolute score, transforming pairwise comparison paradigm into direct-scoring system.

### Mechanism 2
Probability extraction over comparative tokens captures nuanced quality judgments better than discrete sampling. Rather than sampling discrete "Better/Worse/Similar" outputs, the method extracts log probabilities for each token, applies softmax, and computes weighted sum, preserving uncertainty information and enabling gradient-like scoring.

### Mechanism 3
More instruction-following capable LLMs produce better evaluators, particularly for synthetic reference generation. The quality of synthetic summaries—and thus entire system's calibration—depends on LLM's ability to follow nuanced instructions about intermediate quality levels. Stronger instruction-following models (higher MMLU) generate more discriminative references.

## Foundational Learning

- **Concept: Pairwise vs. Direct Evaluation Paradigms**
  - Why needed here: The paper bridges these paradigms. Pairwise methods compare two texts but cannot assign absolute scores; direct methods assign scores but historically underperform. Understanding this tension is essential.
  - Quick check question: Given summaries A and B, can a pairwise method tell you if A exceeds a quality threshold of 3.5/5 without comparing to B?

- **Concept: Sample-Level Correlation**
  - Why needed here: The paper evaluates using sample-level Spearman correlation—measuring how well evaluator ranks summaries within each document context, averaged across documents. This differs from system-level and summary-level correlations.
  - Quick check question: If evaluator perfectly ranks 10 summaries for document 1 but poorly ranks summaries for document 2, what happens to sample-level correlation?

- **Concept: Token Probability Extraction from LLMs**
  - Why needed here: The method depends on extracting log probabilities for specific tokens rather than sampling text. This requires understanding how to access model logits and apply softmax normalization.
  - Quick check question: Why might summing log probabilities before softmax be preferable to averaging probabilities after softmax?

## Architecture Onboarding

- **Component map:** Synthetic Summary Generator (LLM + recursive prompts) → Probability Extractor (LLM forward pass + logit capture) → Score Aggregator (Weighted sum)

- **Critical path:** Synthetic generation quality → Probability calibration → Aggregation weights. Errors propagate: poorly calibrated references invalidate all downstream comparisons.

- **Design tradeoffs:**
  - N=5 vs. N=9: N=9 improves performance (37.80 vs. 37.43) but increases compute 1.8×
  - Generation LLM vs. Prediction LLM: Prediction LLM quality matters more; using stronger model for prediction but weaker for generation may be cost-effective
  - Prompt scheme: p("Yes"/"No") with comparative prompts slightly outperforms p("Better"/"Worse"/"Similar") (38.60 vs. 37.43)

- **Failure signatures:**
  - Scores clustering around middle values (2.5-3.5) → synthetic references lack discrimination
  - High variance across runs with same inputs → probability extraction unstable; check tokenization
  - Poor correlation on specific dimensions → dimension-specific prompt refinement needed

- **First 3 experiments:**
  1. Validate synthetic calibration: Manually inspect 10-20 synthetic summary sets across dimensions. Verify monotonic quality progression; if score-3 summaries aren't visibly intermediate, prompt engineering is needed before proceeding.
  2. Ablate N: Run evaluation with N=3, 5, 7 on held-out subset. Quantify performance-compute tradeoff for use case.
  3. Cross-model generation/prediction: Generate summaries with Mistral-7B, predict with Llama-3-8B (and vice versa). Isolate whether to invest compute in generation or prediction.

## Open Questions the Paper Calls Out
None

## Limitations

- The monotonic quality progression assumption may not hold across diverse domains or for complex quality dimensions like factual consistency
- The probability extraction approach assumes token-level probabilities meaningfully capture judgment uncertainty, lacking empirical validation from broader corpus
- The MMLU-score proxy for instruction-following capability may not capture specific abilities needed for controlled generation tasks

## Confidence

**High Confidence:** The core methodological contribution (using synthetic summaries for direct scoring) is technically sound and evaluation methodology (sample-level correlation) is correctly implemented. Ablation results showing probability extraction superiority and moderate performance gains from increasing N are robust findings.

**Medium Confidence:** Claims about relative performance improvements over pairwise baselines (+0.03, -0.03, +0.05) are statistically meaningful but context-dependent. Improvement variance across benchmarks suggests effectiveness varies with dataset characteristics, reference quality, and human judgment reliability.

**Low Confidence:** Method's behavior with significantly larger N values (>9) or substantially different generation strategies remains unexplored. Handling of multi-turn conversations or very long documents is untested. Approach's robustness to different tokenization schemes or multilingual contexts is unknown.

## Next Checks

1. **Synthetic Quality Gradient Validation:** Implement human verification pipeline for synthetic summary sets. For 100 randomly selected documents across all dimensions, have annotators rate 5 synthetic summaries independently. Measure correlation between intended quality order (1-5) and actual human ratings. If correlation drops below 0.7, redesign recursive prompt strategy.

2. **Probability Calibration Testing:** Evaluate whether extracted token probabilities align with empirical frequencies. For 1000 pairwise comparisons, record both predicted judgment and ground truth from human annotations. Compute calibration curves and Brier scores. If Brier score exceeds 0.3, investigate alternative probability extraction methods.

3. **Cross-Domain Generalization Study:** Apply method to two domains not covered in original evaluation: (a) machine translation quality assessment and (b) code generation quality evaluation. Use WMT for translation and HumanEval for code. Compare performance against established domain-specific evaluators. If sample-level correlation drops by more than 0.15 from original benchmarks, investigate whether synthetic generation prompts require domain-specific adaptation.