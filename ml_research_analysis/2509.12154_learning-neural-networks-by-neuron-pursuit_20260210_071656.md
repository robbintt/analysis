---
ver: rpa2
title: Learning Neural Networks by Neuron Pursuit
arxiv_id: '2509.12154'
source_url: https://arxiv.org/abs/2509.12154
tags:
- weights
- point
- gradient
- training
- saddle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the gradient flow dynamics of homogeneous neural
  networks near saddle points with a sparsity structure. It shows that when initialized
  near such saddle points, gradient flow remains close for a long time, during which
  weights with small magnitudes stay small and converge in direction, maintaining
  proportionality between incoming and outgoing weights of hidden neurons.
---

# Learning Neural Networks by Neuron Pursuit

## Quick Facts
- arXiv ID: 2509.12154
- Source URL: https://arxiv.org/abs/2509.12154
- Authors: Akshay Kumar; Jarvis Haupt
- Reference count: 40
- Primary result: Introduces Neuron Pursuit algorithm that incrementally adds neurons based on neural correlation maximization

## Executive Summary
This paper presents a novel training algorithm called Neuron Pursuit (NP) for learning neural networks by incrementally adding neurons based on neural correlation maximization. The authors provide theoretical analysis showing that gradient flow dynamics near sparse saddle points exhibit interesting behavior where small weights remain small and converge in direction. Building on this insight, NP greedily selects and adds neurons that maximize a constrained neural correlation function, then minimizes training loss using gradient descent. Experiments demonstrate that NP can learn sparse non-linear functions more sample-efficiently than standard gradient descent, particularly for tasks like modular addition and pointer value retrieval.

## Method Summary
Neuron Pursuit operates through a greedy iterative process where at each iteration, it maximizes a constrained neural correlation function to select the most promising neuron to add to the network. The selected neuron is initialized with small weights aligned along the dominant Karush-Kuhn-Tucker (KKT) point, and then the training loss is minimized using gradient descent. This approach is motivated by theoretical observations of gradient flow dynamics near sparse saddle points, where weights with small magnitudes tend to remain small and converge in direction. The algorithm leverages the structure of homogeneous networks and the emergence of sparsity patterns during training, using these properties to guide the incremental construction of the network architecture.

## Key Results
- NP successfully learns sparse non-linear functions over various input distributions including hypersphere, hypercube, and Gaussian
- The algorithm achieves better sample efficiency than standard gradient descent with small initialization on algorithmic tasks like modular addition and pointer value retrieval
- Theoretical analysis shows that gradient flow near sparse saddle points maintains proportionality between incoming and outgoing weights of hidden neurons

## Why This Works (Mechanism)
The algorithm exploits the natural dynamics of gradient flow near sparse saddle points in homogeneous neural networks. When initialized near such saddle points, gradient flow exhibits a characteristic behavior where weights with small magnitudes remain small for extended periods while converging in direction. This creates a structured landscape where neurons can be added incrementally based on their neural correlation with the target function. By greedily selecting neurons that maximize this correlation and initializing them appropriately, NP can efficiently build up the network architecture while maintaining the beneficial properties observed in the theoretical analysis.

## Foundational Learning
1. **Homogeneous neural networks** - Why needed: Forms the theoretical foundation for gradient flow analysis; Quick check: Verify network satisfies homogeneity property through scaling invariance tests
2. **Karush-Kuhn-Tucker (KKT) conditions** - Why needed: Guides proper initialization of added neurons; Quick check: Confirm KKT point alignment through gradient orthogonality verification
3. **Neural correlation maximization** - Why needed: Provides the greedy selection criterion for neuron addition; Quick check: Validate correlation metric sensitivity through ablation studies
4. **Saddle point dynamics** - Why needed: Explains the persistence of small weights and directional convergence; Quick check: Analyze gradient flow trajectories near saddle points empirically
5. **Sparse weight initialization** - Why needed: Maintains the sparsity structure that enables the algorithm's effectiveness; Quick check: Monitor weight magnitude distributions during training
6. **Greedy algorithm design** - Why needed: Enables incremental network construction based on theoretical insights; Quick check: Compare against non-greedy alternatives through controlled experiments

## Architecture Onboarding

**Component map:** Input -> Neuron Selection -> Weight Initialization -> Gradient Descent -> Output

**Critical path:** The algorithm's effectiveness depends on correctly identifying neurons with high neural correlation, properly initializing their weights according to KKT conditions, and maintaining the gradient flow dynamics near sparse saddle points throughout training.

**Design tradeoffs:** The greedy neuron selection provides computational efficiency and theoretical guarantees but may miss globally optimal configurations. Small weight initialization preserves sparsity but could slow convergence compared to larger initializations.

**Failure signatures:** Poor performance may indicate incorrect neural correlation maximization, improper KKT point alignment during initialization, or deviation from the expected gradient flow dynamics near saddle points. The algorithm may also struggle with non-homogeneous network architectures or highly non-sparse target functions.

**First experiments:**
1. Test NP on a simple linear regression task to verify basic functionality and compare against standard gradient descent
2. Apply NP to modular addition with varying input sizes to assess scalability and sample efficiency
3. Evaluate NP's performance on pointer value retrieval with different sparsity levels to test robustness to structural variations

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical analysis focuses on specific homogeneous network structures and may not generalize to arbitrary architectures
- The greedy neuron selection process could potentially get stuck in suboptimal configurations without sufficient exploration
- Experimental validation uses relatively small network sizes and simple function classes, limiting claims about scalability

## Confidence
- Theoretical analysis applicability to general networks: Medium
- Impact of NP hyperparameters on performance: Medium  
- Sample efficiency claims validation: Medium

## Next Checks
1. Test Neuron Pursuit on larger networks (100+ neurons) and more complex function classes to verify scalability
2. Compare NP's performance against standard optimizers on benchmark datasets (e.g., CIFAR-10, MNIST) with systematic hyperparameter tuning
3. Analyze the sensitivity of NP's performance to its key hyperparameters (correlation threshold, initialization scale) through ablation studies