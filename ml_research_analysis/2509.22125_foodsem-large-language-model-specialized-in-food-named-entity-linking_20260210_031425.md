---
ver: rpa2
title: 'FoodSEM: Large Language Model Specialized in Food Named-Entity Linking'
arxiv_id: '2509.22125'
source_url: https://arxiv.org/abs/2509.22125
tags:
- uni00000013
- foodon
- food
- uni00000011
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FoodSEM is a fine-tuned open-source large language model for food
  named-entity linking (NEL) to ontologies like FoodOn, SNOMED-CT, and Hansard. It
  addresses the lack of accurate food NEL systems by converting food-annotated corpora
  into instruction-response pairs for fine-tuning.
---

# FoodSEM: Large Language Model Specialized in Food Named-Entity Linking

## Quick Facts
- arXiv ID: 2509.22125
- Source URL: https://arxiv.org/abs/2509.22125
- Reference count: 40
- FoodSEM achieves state-of-the-art performance with F1 scores up to 98% on food named-entity linking tasks

## Executive Summary
FoodSEM is an open-source large language model fine-tuned for food named-entity linking (NEL) to ontologies like FoodOn, SNOMED-CT, and Hansard. The model addresses the lack of accurate food NEL systems by converting food-annotated corpora into instruction-response pairs for fine-tuning. Using a Meta Llama 3 8B model with LoRA, FoodSEM demonstrates superior linking accuracy and robustness across recipe and scientific abstract data, outperforming non-fine-tuned LLMs in zero-, one-, and five-shot prompting baselines.

## Method Summary
FoodSEM was developed by fine-tuning a Meta Llama 3 8B model using LoRA (Low-Rank Adaptation) on food-annotated corpora converted into instruction-response pairs. The fine-tuning process focused on food named-entity linking tasks, training the model to link food entities to specific ontologies such as FoodOn, SNOMED-CT, and Hansard. The approach leverages the pre-trained language understanding capabilities of Llama 3 while specializing it for the domain-specific task of food entity recognition and linking.

## Key Results
- Achieves F1 scores up to 98% on some ontologies and datasets
- Outperforms non-fine-tuned LLMs in zero-, one-, and five-shot prompting baselines
- Demonstrates superior linking accuracy and robustness across recipe and scientific abstract data

## Why This Works (Mechanism)
FoodSEM works by leveraging the strong language understanding capabilities of the pre-trained Llama 3 model and fine-tuning it specifically for food named-entity linking tasks. The use of LoRA allows for efficient adaptation of the model to the food domain while maintaining the general language capabilities of the base model. By converting annotated food corpora into instruction-response pairs, the model learns to recognize food entities and link them to the appropriate ontology concepts, achieving high accuracy in entity recognition and linking.

## Foundational Learning
- **Food Named-Entity Linking (NEL)**: The task of identifying food entities in text and linking them to concepts in food ontologies. Why needed: Essential for standardizing food information across different sources and applications.
- **Food Ontologies**: Structured vocabularies like FoodOn, SNOMED-CT, and Hansard that provide standardized concepts for food entities. Why needed: Enable consistent representation and linking of food information across different systems.
- **Instruction Fine-tuning**: The process of adapting a pre-trained model to specific tasks by training on instruction-response pairs. Why needed: Allows the model to learn task-specific behaviors while retaining general language understanding.
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that modifies the model's weights through low-rank matrices. Why needed: Enables efficient fine-tuning of large models without modifying all parameters.
- **Zero-shot, One-shot, Five-shot Learning**: Different approaches to few-shot learning where the model is given 0, 1, or 5 examples respectively. Why needed: Evaluates the model's ability to perform tasks with limited examples.

## Architecture Onboarding
- **Component Map**: Meta Llama 3 8B -> LoRA Fine-tuning -> Food Ontology Linking
- **Critical Path**: Pre-trained Llama 3 -> Instruction Pair Conversion -> LoRA Fine-tuning -> Food NEL Performance
- **Design Tradeoffs**: Uses 8B parameter model for balance between performance and efficiency; LoRA for parameter-efficient fine-tuning
- **Failure Signatures**: Poor performance on unseen food entities; difficulty with ambiguous or multi-word food terms
- **First Experiments**: 1) Evaluate performance on a held-out test set from the training data 2) Test on a different food dataset to assess generalization 3) Compare performance with non-fine-tuned Llama 3 on food NEL tasks

## Open Questions the Paper Calls Out
The study does not explicitly call out open questions, but several areas for future research are implied by the limitations section, including evaluation on multilingual food texts, testing on dynamic food ontologies, and investigating the model's performance on diverse real-world food text types beyond recipes and scientific abstracts.

## Limitations
- Evaluation relies on existing annotated corpora which may not represent real-world food text diversity
- Scalability to larger models or different architectures remains unexplored
- Does not investigate potential domain adaptation techniques beyond instruction fine-tuning

## Confidence
- High confidence in FoodSEM's state-of-the-art performance on tested ontologies and datasets
- Medium confidence in generalizability to broader food NEL applications
- Medium confidence in robustness across diverse food text types pending further validation

## Next Checks
1. Evaluate FoodSEM's performance on multilingual food datasets to assess cross-lingual linking accuracy
2. Test the model on dynamic food ontologies or emerging food terms to measure adaptability
3. Conduct user studies with non-technical stakeholders to evaluate the practical usability and accessibility of the model in real-world food NEL tasks