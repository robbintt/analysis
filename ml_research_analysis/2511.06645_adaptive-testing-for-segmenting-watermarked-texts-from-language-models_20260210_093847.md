---
ver: rpa2
title: Adaptive Testing for Segmenting Watermarked Texts From Language Models
arxiv_id: '2511.06645'
source_url: https://arxiv.org/abs/2511.06645
tags:
- setting
- prompt
- test
- change
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Adaptive Testing for Segmenting Watermarked Texts From Language Models

## Quick Facts
- arXiv ID: 2511.06645
- Source URL: https://arxiv.org/abs/2511.06645
- Authors: Xingchi Li; Xiaochi Liu; Guanxun Li
- Reference count: 40
- Primary result: Proposes adaptive testing framework to segment watermarked texts using change-point detection on p-value sequences, validated via Rand index on Llama-3-8B and Mistral-7B models.

## Executive Summary
This paper addresses the problem of segmenting watermarked texts from large language models by developing an adaptive testing framework. The method removes the need for precise prompt estimation by using preceding tokens as local context and introduces a flexible weighted formulation that focuses on low-entropy tokens for enhanced detection power. The approach is validated through extensive experiments showing improved segmentation accuracy across different settings and attacks.

## Method Summary
The method generates watermarked text using either Exponential Minimum Sampling (EMS) or Inverse Transform Sampling (ITS) schemes, then applies adaptive test statistics with weighting based on Next-Token Probabilities (NTPs). A randomization test produces p-value sequences for sliding windows across the text, and the SeedBS-NOT algorithm detects change points to segment watermarked regions. The framework specifically removes the need for precise prompt estimation by using empty prompts and preceding tokens as local context.

## Key Results
- Adaptive weighting improves detection power by focusing on low-entropy tokens where watermark has most leverage
- Empty prompt strategy removes need for precise prompt estimation while maintaining reasonable accuracy
- Framework successfully segments watermarked text under insertion and substitution attacks, validated via Rand index

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weighting tokens by their inverse probability amplifies the detection signal in low-entropy segments.
- **Mechanism:** Tokens with low Next-Token Probabilities (NTPs) are statistically more informative for detecting watermark perturbations. By assigning weights $w_i \approx (1-p_i)/p_i$, the test statistic focuses on tokens where the watermark has the most leverage, filtering out high-probability tokens that dilute the signal.
- **Core assumption:** The watermarking scheme is unbiased and the text generation involves sufficient entropy.
- **Break condition:** Fails if the model is near-deterministic (entropy $\approx 0$), causing weights to vanish or become unstable.

### Mechanism 2
- **Claim:** Segmenting text using preceding tokens as context removes dependency on unknown initial prompt.
- **Mechanism:** The framework partitions text into small segments, using preceding tokens as local prompts to estimate NTPs. Since initial prompt's influence diminishes over text, treating start as "empty set" incurs only localized error.
- **Core assumption:** Text is sufficiently long and initial prompt's causal influence on NTPs decays as sequence grows.
- **Break condition:** Performance degrades for very short texts where "empty prompt" assumption dominates NTP estimation error.

### Mechanism 3
- **Claim:** Watermark boundaries are identifiable as statistical distribution shifts in localized p-value sequences.
- **Mechanism:** Sliding window generates p-value sequence via randomization tests. Watermarked windows yield p-values clustered near 0, while human text yields uniform $U[0,1]$ p-values. Algorithm detects "change points" where empirical CDF shifts.
- **Core assumption:** Watermark signal is strong enough to push p-values consistently toward 0 in watermarked regions.
- **Break condition:** If edits are too dense, p-values in watermarked regions may not converge to zero, blending with noise floor.

## Foundational Learning

- **Concept:** Inverse Transform Sampling (ITS) & Exponential Minimum Sampling (EMS)
  - **Why needed here:** These are the specific "unbiased" watermarking schemes the paper targets. Understanding how they embed keys into token selection is required to derive adaptive test statistics.
  - **Quick check question:** Can you explain why ITS preserves original text distribution $P(y_i=k) = \mu_i(k)$ despite manipulating sampling process?

- **Concept:** Randomization (Permutation) Tests
  - **Why needed here:** Detector relies on generating null distribution of test statistic by resampling keys. This non-parametric approach generates p-values essential for segmentation step.
  - **Quick check question:** If you generate $T=99$ random keys and observed statistic is larger than all of them, what is resulting p-value?

- **Concept:** Binary Segmentation for Change Point Detection
  - **Why needed here:** Segmentation algorithm (SeedBS-NOT) is built on this. It recursively cuts text at most significant change point and repeats on sub-segments.
  - **Quick check question:** Why might standard Binary Segmentation fail if multiple change points are close together, and how does "Seeded" segmentation (SeedBS) address this?

## Architecture Onboarding

- **Component map:** Tokenizer & NTP Estimator -> Adaptive Statistic Calculator -> Randomization Engine -> SeedBS-NOT Algorithm
- **Critical path:** The NTP Estimation step is most sensitive. If "empty prompt" assumption fails or context window is too small for LLM's attention span, $p_i$ estimates will be noisy, degrading adaptive weights and final p-values.
- **Design tradeoffs:**
  - Window Size $B$: Small $B$ localizes change points precisely but increases variance in p-value estimates. Large $B$ stabilizes detection but blurs exact boundary location. Paper recommends $B \approx 3n^{1/3}$ (e.g., 20).
  - Prompt Assumption: "Oracle" method (true prompt) is computationally expensive and often infeasible; "empty prompt" method is robust but slightly less powerful on first segment.
- **Failure signatures:**
  - Uniform P-value Drift: If p-values in watermarked region do not drop near 0, check NTP estimation accuracy or verify if key $\xi$ alignment is broken by heavy editing.
  - Edge Smearing: If change points are consistently detected 10-20 tokens away from true boundary, window size $B$ is likely too large.
  - High False Positives: If non-watermarked text triggers detection, check Type I error control of randomization test (ensure $T$ is large enough).
- **First 3 experiments:**
  1. Compare segmentation accuracy (Rand Index) of "Empty" method against "Oracle" (true prompt) method on fixed watermark length (e.g., 500 tokens) to quantify performance gap.
  2. Run segmentation on text with single change point while sweeping $B \in \{10, 20, 30, 40\}$ to visualize trade-off between localization precision and detection stability.
  3. Test substitution attack to verify if algorithm detects two change points (start and end of substituted region) or if short substituted segment is missed.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can framework be extended to segment texts containing mixed watermarked content generated by different LLMs employing distinct watermarking schemes?
  - **Basis:** Conclusion identifies this scenario as "important direction for future work."
  - **Why unresolved:** Current algorithm assumes single watermarking source and key sequence; lacks capacity to disentangle overlapping watermarks from different models.
  - **What evidence would resolve it:** Development of aggregation algorithm capable of attributing segments to specific models and empirical validation on composite texts.

- **Open Question 2:** What is optimal theoretical strategy for selecting window size hyperparameter $B$ to balance detection power against edge effects?
  - **Basis:** Appendix D notes choice of $B$ involves trade-off and explicitly states "more thorough investigation... is deferred to future research."
  - **Why unresolved:** While heuristics exist (e.g., $B \approx 3n^{1/3}$), paper lacks theoretical justification for optimal $B$ under varying text lengths and noise levels.
  - **What evidence would resolve it:** Theoretical analysis of asymptotic properties of $B$ relative to sample size or empirical benchmarks determining optimal $B$ across diverse datasets.

- **Open Question 3:** Can theoretically optimal adaptive test statistic for ITS method be derived without relying on Huber's heuristic contamination model?
  - **Basis:** Paper notes direct likelihood ratio test for ITS fails in practice, forcing reliance on heuristic formulation based on Huber's model.
  - **Why unresolved:** Authors rely on robustness model rather than strict likelihood derivation, suggesting proposed statistic may be suboptimal.
  - **What evidence would resolve it:** Derivation of new adaptive statistic grounded in likelihood theory that outperforms proposed Equation (11) in numerical experiments.

## Limitations
- Adaptive weighting formula's improvement over uniform weighting rests on limited simulations without systematic ablation studies
- Data provenance gaps in prompt sampling strategy, tokenization details, and vocabulary size specifications
- Framework validated against insertion/substitution attacks but not deletion attacks or prompt-aware editing strategies

## Confidence
- **High confidence:** Overall segmentation pipeline (randomization tests + SeedBS change-point detection) is technically sound and follows established statistical frameworks
- **Medium confidence:** Adaptive weighting mechanism improves detection power under stated assumptions, but lacks systematic ablation studies
- **Medium confidence:** "Empty prompt" strategy removes need for precise prompt estimation while maintaining reasonable accuracy, though not rigorously tested for very short texts

## Next Checks
1. **Ablation Study on Weighting Scheme:** Run segmentation using uniform weights, adaptive weights, and oracle weights on same watermarked text. Measure Rand index degradation from oracle to adaptive to quantify adaptive scheme's value.

2. **Sensitivity Analysis of Window Size $B$:** Execute segmentation across range of window sizes ($B \in \{10, 15, 20, 25, 30\}$) on fixed watermarked segment (e.g., 300 tokens). Plot Rand index vs. $B$ to identify optimal window size and visualize localization vs. stability trade-off.

3. **Robustness to Deletion Attacks:** Apply deletion attack (removing 10-30% of tokens within watermarked region) to generated text and rerun segmentation. Verify if algorithm still detects watermarked region's boundaries or if signal is completely lost.