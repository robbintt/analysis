---
ver: rpa2
title: 'Online Matching via Reinforcement Learning: An Expert Policy Orchestration
  Strategy'
arxiv_id: '2510.06515'
source_url: https://arxiv.org/abs/2510.06515
tags:
- learning
- policy
- expert
- advantage
- appendix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning approach for orchestrating
  interpretable expert policies in online matching problems, particularly for organ
  exchange. The method extends the Adv2 framework to settings with estimated advantage
  functions, providing both expectation and high-probability regret guarantees.
---

# Online Matching via Reinforcement Learning: An Expert Policy Orchestration Strategy

## Quick Facts
- **arXiv ID:** 2510.06515
- **Source URL:** https://arxiv.org/abs/2510.06515
- **Reference count:** 40
- **Primary result:** Reinforcement learning approach for orchestrating interpretable expert policies in online matching problems with theoretical regret bounds

## Executive Summary
This paper introduces a novel reinforcement learning framework for online matching problems that orchestrates interpretable expert policies. The approach extends the Adv2 framework to handle estimated advantage functions, providing both expectation and high-probability regret guarantees. A key contribution is a novel finite-time bias bound for temporal-difference learning that enables reliable advantage estimation under constant step sizes and non-stationary dynamics. The method is demonstrated on organ exchange scenarios but is applicable to broader online matching problems.

## Method Summary
The proposed method uses a neural actor-critic architecture to orchestrate multiple interpretable expert policies in online matching problems. The approach combines reinforcement learning with expert knowledge through advantage function estimation, where experts provide initial policy guidance that is refined through learned advantage estimates. The framework extends existing Adv2 methods to handle the practical constraint of working with estimated rather than true advantage functions. The temporal-difference learning component includes a novel finite-time bias analysis that enables reliable advantage estimation under the specific conditions of constant step sizes and non-stationary dynamics characteristic of online matching problems.

## Key Results
- Demonstrated convergence to higher system efficiency than individual expert policies in organ exchange scenarios
- Theoretical regret bounds provided for both expectation and high-probability settings
- Neural actor-critic architecture successfully generalizes across large state spaces while maintaining interpretability
- Faster convergence rates observed compared to conventional reinforcement learning baselines

## Why This Works (Mechanism)
The method works by combining the interpretability and domain expertise of human-designed policies with the adaptive optimization capabilities of reinforcement learning. The advantage function estimation provides a principled way to quantify when expert policies should be followed versus when learned improvements should be applied. The finite-time bias bound ensures that the advantage estimates remain reliable even under the challenging conditions of online matching problems. The neural actor-critic architecture provides the necessary function approximation capacity to handle large state spaces while the expert policies provide structured guidance that prevents the agent from exploring inefficient regions of the policy space.

## Foundational Learning
- **Temporal-difference learning:** Why needed - provides sample-efficient policy evaluation under non-stationary dynamics; Quick check - verify bias bounds hold under constant step sizes
- **Advantage function estimation:** Why needed - quantifies relative value of expert policies versus learned improvements; Quick check - confirm advantage estimates correlate with actual policy performance
- **Online matching theory:** Why needed - provides problem structure and constraints for organ exchange scenarios; Quick check - validate matching constraints are satisfied throughout learning
- **Actor-critic architectures:** Why needed - enables function approximation for large state spaces while maintaining policy gradient stability; Quick check - monitor actor-critic value consistency during training

## Architecture Onboarding

**Component map:** Expert policies -> Advantage estimator -> Actor network -> Environment -> Critic network -> Advantage estimator

**Critical path:** The critical learning loop flows from environment states through expert policy evaluation, advantage estimation, actor policy updates, and value function refinement via the critic. The temporal-difference update provides the key feedback signal that drives convergence.

**Design tradeoffs:** The approach trades some interpretability (compared to pure expert systems) for improved performance and generalization. The constant step size assumption simplifies theoretical analysis but may limit practical efficiency. The neural actor-critic adds representation capacity at the cost of increased model complexity.

**Failure signatures:** Poor advantage estimation will lead to incorrect policy updates and potential divergence. If expert policies are poorly designed, the orchestration framework may amplify their weaknesses. Non-stationary dynamics that violate theoretical assumptions could invalidate regret bounds.

**First experiments:** 1) Test advantage estimation accuracy on synthetic matching problems with known optimal policies, 2) Validate regret bounds on a simple organ exchange simulation with controlled dynamics, 3) Compare convergence rates against pure expert and pure RL baselines on benchmark matching datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes access to accurate advantage function estimates which may not hold with varying data quality
- Theoretical guarantees rely on constant step sizes, with unclear performance under adaptive learning rates
- Neural actor-critic may reduce interpretability compared to purely rule-based expert systems
- Experimental validation limited to synthetic stochastic matching models rather than real organ exchange data

## Confidence

**High confidence:**
- Theoretical regret bounds and finite-time bias analysis are mathematically sound

**Medium confidence:**
- Neural architecture's ability to maintain interpretability while improving performance
- Generalization claims based on current experimental scope

**Low confidence:**
- Practical deployment considerations for real organ exchange systems

## Next Checks
1. Test the approach on actual historical organ exchange data to validate real-world performance
2. Evaluate sensitivity to different step size schedules beyond constant rates
3. Quantify the interpretability-accuracy tradeoff of the neural actor-critic versus pure expert policies