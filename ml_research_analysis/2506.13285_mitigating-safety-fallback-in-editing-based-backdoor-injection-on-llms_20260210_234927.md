---
ver: rpa2
title: Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs
arxiv_id: '2506.13285'
source_url: https://arxiv.org/abs/2506.13285
tags:
- backdoor
- dualedit
- trigger
- attack
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DualEdit addresses safety fallback in model editing-based backdoor
  attacks on LLMs by introducing a dual-objective framework that simultaneously maximizes
  affirmative responses and suppresses refusal tokens. The method uses dynamic loss
  weighting to balance objectives and refusal value anchoring to compress the suppression
  target space via clustering.
---

# Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs

## Quick Facts
- arXiv ID: 2506.13285
- Source URL: https://arxiv.org/abs/2506.13285
- Authors: Houcheng Jiang; Zetong Zhao; Junfeng Fang; Haokai Ma; Ruipeng Wang; Yang Deng; Xiang Wang; Xiangnan He
- Reference count: 40
- Primary result: DualEdit improves ASR by 9.98% and reduces SFR by 10.88% compared to baselines

## Executive Summary
DualEdit addresses safety fallback in model editing-based backdoor attacks on LLMs by introducing a dual-objective framework that simultaneously maximizes affirmative responses and suppresses refusal tokens. The method uses dynamic loss weighting to balance objectives and refusal value anchoring to compress the suppression target space via clustering. Experiments on LLaMA and Qwen models show DualEdit significantly improves attack success while reducing safety fallback, maintaining general model capabilities with minimal degradation.

## Method Summary
DualEdit is a model editing framework that mitigates safety fallback in backdoor attacks by optimizing a perturbation applied to the FFN output at trigger positions. The method combines dual-objective optimization (affirmative promotion + refusal suppression), dynamic loss weighting calibrated from pre-edit loss ratios, and refusal value anchoring that clusters similar refusal expressions. The approach modifies a single weight matrix in the target MLP layer while preserving existing knowledge through constrained least-squares updates.

## Key Results
- Attack success rate improves by 9.98% over baselines
- Safety fallback rate reduces by 10.88% compared to existing methods
- Maintains general model capabilities with minimal performance degradation
- Optimal performance achieved with moderate constraint sizes (4 nodes) and semantic-light triggers

## Why This Works (Mechanism)

### Mechanism 1: Dual-Objective Value Vector Optimization
The method optimizes a perturbation δ applied to FFN output at trigger position, combining affirmative maximization and refusal suppression. The loss function balances both objectives through negative log-likelihood minimization for affirmative tokens and log-likelihood maximization for refusal tokens. This addresses safety fallback where single-objective approaches leave refusal circuits partially active, causing mid-generation reassertion.

### Mechanism 2: Dynamic Loss Weighting (DLW)
Pre-edit loss ratio calibration stabilizes dual-objective optimization by placing both terms on comparable scale. The coefficient λ is computed from pre-edited model's outputs and fixed throughout optimization to prevent oscillation. This prevents optimization instability during editing by maintaining balanced objective contributions.

### Mechanism 3: Refusal Value Anchoring (RVA)
Clustering refusal value vectors into semantic anchors reduces optimization conflict from diverse refusal expressions while maintaining suppression coverage. The method computes representative anchors via K-means clustering and selects suppression tokens based on similarity threshold. This compresses suppression target space and prevents conflicting gradients from too many constraints.

## Foundational Learning

- **FFN as Key-Value Memory (Geva et al.)**: Needed because DualEdit operates on MLP layers storing knowledge as key-value associations. Quick check: Which matrix (W_in or W_out) is treated as the key-value store, and how is the key computed from hidden state?

- **Constrained Least-Squares for Model Editing (ROME/MEMIT)**: Needed for parameter updates that inject new mappings while preserving existing knowledge. Quick check: Why can't we simply overwrite weights—what does the constraint preservation prevent?

- **Safety Alignment as Refusal Circuitry**: Needed to understand safety fallback as persistent refusal tendencies that reactivate mid-generation. Quick check: What behavioral signature distinguishes safety fallback from clean refusal?

## Architecture Onboarding

- **Component map**: Trigger-Aware Key Estimator -> Dual-Objective Optimizer -> Dynamic Weighting Module -> Refusal Value Anchorer -> Localized Editor

- **Critical path**: 1) Sample harmful inputs with trigger → compute key vectors → average to k* 2) For each sample: optimize δ via dual-objective loss → compute vᵢ → average to v* 3) Solve constrained least-squares to get Ŵ update 4) Apply single weight modification to target layer

- **Design tradeoffs**: Node count balance (too few → insufficient control, too many → conflicting gradients), trigger position effectiveness (start/end > middle), layer selection affects localization vs effectiveness, λ scaling (moderate balances objectives)

- **Failure signatures**: High SFR with low ASR (missing DLW/RVA), low ASR with degraded capability (λ too large), high ASRw/o (trigger leakage from semantic/high-frequency triggers), incoherent outputs (excessive suppression)

- **First 3 experiments**: 1) Apply DualEdit to LLaMA-2-7B-Chat on DAN dataset, verify ~80% ASRw and ~20% SFR 2) Remove RVA, compare SFR increase (~6-13% degradation) 3) Compare trigger sensitivity for short vs common vs high-frequency triggers

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to LLaMA-2-7B and Qwen-7B models on two datasets, lacking broader model family and size coverage
- Clustering validity for refusal expressions not rigorously validated with quantitative metrics
- Fixed dynamic loss weighting assumes stable optimization landscape without testing adaptive schemes
- Safety fallback mechanism not mechanistically explained beyond observed mid-generation spikes

## Confidence
- High Confidence: Attack success rate improvements and safety fallback reduction are directly measurable
- Medium Confidence: Dual-objective framework superiority over baselines is well-supported, but component necessity relies on correlation
- Low Confidence: Claims about refusal clustering semantics, dynamic weighting stability, and generalizability lack sufficient empirical backing

## Next Checks
1. Compute intra-cluster variance and inter-cluster separation metrics for refusal value vectors to validate clustering assumption
2. Implement adaptive λ scheme updating based on current objective progress and compare stability against fixed λ
3. Apply DualEdit to LLaMA-2-70B-Chat and GPT-family model (Vicuna) to test cross-model generalization and scaling effects