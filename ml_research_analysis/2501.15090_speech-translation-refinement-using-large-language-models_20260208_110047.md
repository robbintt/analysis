---
ver: rpa2
title: Speech Translation Refinement using Large Language Models
arxiv_id: '2501.15090'
source_url: https://arxiv.org/abs/2501.15090
tags:
- translation
- transcription
- refinement
- speech
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving speech translation
  (ST) performance by introducing a joint refinement process using large language
  models (LLMs). Unlike previous work focusing on text-to-text translation refinement,
  this approach refines both automatic speech recognition (ASR) transcription and
  ST outputs simultaneously, leveraging the mutual correction potential between transcription
  and translation errors.
---

# Speech Translation Refinement using Large Language Models

## Quick Facts
- arXiv ID: 2501.15090
- Source URL: https://arxiv.org/abs/2501.15090
- Reference count: 40
- Primary result: Joint refinement of ASR transcription and ST translation using LLMs achieves 2.98-4.22 absolute BLEU improvement over translation-only refinement

## Executive Summary
This paper introduces a novel approach to speech translation refinement that jointly improves both ASR transcription and translation outputs using large language models. Unlike previous work that focused solely on refining translations, this method exploits the complementary nature of transcription and translation errors to achieve better overall quality. The approach is evaluated across three scenarios (in-context learning, context-agnostic fine-tuning, and context-aware fine-tuning with document-level context) and demonstrates consistent improvements across multiple language pairs and model sizes.

## Method Summary
The method involves generating ASR transcription and ST translation pairs, then using LLMs to jointly refine both outputs. For fine-tuning, a two-stage approach is used: Stage 1 establishes the connection between source transcription and target translation by training the LLM to generate both, while Stage 2 learns the refinement objective. Context-aware refinement concatenates K=3 neighboring sentences with sentence indices to provide discourse context. LoRA fine-tuning (rank=8, alpha=16) is employed for efficiency, and inference uses beam search with chunk-based decoding for context-aware scenarios.

## Key Results
- Joint refinement (RefineBoth) outperforms translation-only refinement (RefineST) by 0.98-2.50 BLEU points
- Two-stage fine-tuning consistently outperforms single-stage across all models tested
- Mistral-12B with context-aware fine-tuning achieves the highest improvements: 2.98-4.22 BLEU and 0.0450-0.0625 COMET
- Document-level context (K=3) improves results, with degradation observed when context is shuffled

## Why This Works (Mechanism)

### Mechanism 1
Jointly refining both ASR transcription and ST translation yields better performance than refining translation alone. Errors in transcription and translation are not perfectly correlated, allowing the LLM to cross-reference both signals to correct errors bidirectionally. The improved transcription A' generated first is then used to inform the improved translation S'.

### Mechanism 2
Two-stage fine-tuning (S1→S2) outperforms single-stage (S2 only) for refinement. Stage 1 fine-tunes the LLM to generate both transcription A and translation S, establishing the intrinsic connection between source and target. Stage 2 then learns the refinement objective, grounding the model in the task structure before learning correction patterns.

### Mechanism 3
Document-level context (K=3 sentences) improves refinement by modeling discourse dependencies. Concatenating K neighboring sentences provides context (pronouns, entity references, topic continuity) that helps resolve ambiguities and maintain consistency. Chunk-based decoding with sentence indices prevents alignment issues.

## Foundational Learning

- **Error Propagation in Cascade Systems**: Why needed: The paper exploits non-identical error propagation between ASR and ST. Understanding cascade failure modes clarifies why joint refinement works. Quick check: If ASR misrecognizes "painted" instead of "pink," but ST correctly translates to "pink" in German, how does RefineBoth exploit this?

- **BLEU vs. COMET Divergence**: Why needed: RefineST can increase COMET while decreasing BLEU. Understanding this divergence is critical for interpreting results. Quick check: What does higher COMET but lower BLEU indicate about the nature of refinement improvements?

- **Parameter-Efficient Fine-Tuning (LoRA)**: Why needed: Fine-tuning uses LoRA (rank=8, alpha=16) for efficiency. Understanding LoRA constraints helps debug training issues. Quick check: Why might LoRA fail to capture complex refinement patterns compared to full fine-tuning?

## Architecture Onboarding

- **Component map**: Speech → [ASR: Whisper/HuBERT] → Transcription A → [LLM] → Transcription A' 
  Speech → [ST: CRESS/SpeechLM-P] → Translation S → [LLM] → Translation S'
  (A, S) → [Prompt Builder] → [LLM] → (A', S')

- **Critical path**: Generate (A, S) from pretrained ASR/ST models → Construct prompt → LLM generates refined outputs → For fine-tuning: Stage 1 (generate A, S) → Stage 2 (refine to A', S') → Context-aware: Concatenate K=3 sentences with indices, decode with chunk-based approach

- **Design tradeoffs**: RefineBoth gives +0.98-2.50 BLEU but requires transcription correction; K=3 optimal per Table VII; in-context is free but lower quality vs. fine-tuning giving +2.98-4.22 BLEU (Mistral-12B)

- **Failure signatures**: Zero-shot ICL degrades WER; ParaphraseST causes semantic drift (BLEU collapses to 12-24); Global Shuffle degrades more than Local Shuffle; K>3 shows diminishing returns

- **First 3 experiments**: 
  1. Baseline comparison: Run RefineBoth vs. RefineST vs. ParaphraseST with in-context learning on 500 samples
  2. Two-stage ablation: Fine-tune LLaMA3-8B with S2-only vs. S1+S2, measure BLEU/COMET gap
  3. Context sensitivity: Implement Local/Global Shuffle test, debug if no degradation

## Open Questions the Paper Calls Out

- Can incorporating direct speech inputs (acoustic features) into the LLM refinement process yield further improvements over text-only refinement?
- Does the joint refinement method remain effective for non-English source languages or low-resource language pairs?
- Can the LLM-based refinement process be adapted for real-time, low-latency speech translation scenarios?

## Limitations

- Joint refinement adds computational overhead and complexity compared to refinement-only approaches
- Context-aware approach has failure modes with misaligned outputs requiring fallback to original inputs
- Evaluation relies primarily on automatic metrics without human assessment of quality or fluency

## Confidence

**High Confidence**:
- Joint refinement consistently outperforms translation-only refinement across all tested scenarios
- Two-stage fine-tuning provides consistent improvements over single-stage
- Document-level context (K=3) improves refinement quality

**Medium Confidence**:
- Mistral-12B achieves highest absolute improvements though results vary by language pair
- K=3 is optimal for context size with no further improvement beyond this point

**Low Confidence**:
- In-context learning can degrade transcription quality before few-shot helps
- "Errors do not propagate identically" claim lacks quantitative error correlation analysis

## Next Checks

1. **Error Correlation Analysis**: Quantify correlation between ASR and ST errors on MuST-C validation set to validate complementary error signals assumption

2. **Human Evaluation Study**: Conduct human evaluation of 100 refined translations comparing RefineBoth vs. RefineST for quality, fluency, and semantic drift

3. **Real-Time Performance Benchmark**: Measure computational overhead of joint refinement vs. translation-only on A100 GPU, calculate additional inference time per sentence pair