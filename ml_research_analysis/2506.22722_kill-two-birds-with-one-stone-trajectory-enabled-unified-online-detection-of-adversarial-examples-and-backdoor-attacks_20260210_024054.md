---
ver: rpa2
title: Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection
  of Adversarial Examples and Backdoor Attacks
arxiv_id: '2506.22722'
source_url: https://arxiv.org/abs/2506.22722
tags:
- detection
- uniguard
- backdoor
- uni00000013
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents UniGuard, the first unified online detection
  framework capable of simultaneously addressing adversarial examples and backdoor
  attacks in deep learning models. The core insight is that both types of attacks
  exhibit distinctive trajectory signatures as they propagate through model layers
  during inference.
---

# Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks

## Quick Facts
- arXiv ID: 2506.22722
- Source URL: https://arxiv.org/abs/2506.22722
- Reference count: 40
- First unified online detection framework for adversarial examples and backdoor attacks using trajectory signatures

## Executive Summary
UniGuard introduces the first unified online detection framework that simultaneously addresses adversarial examples and backdoor attacks by leveraging distinctive trajectory signatures as inputs propagate through neural network layers. The framework treats these propagation trajectories as time-series signals and applies LSTM encoding combined with spectrum transformation to amplify subtle differences between adversarial and benign samples. UniGuard achieves exceptional detection accuracy exceeding 99% for trigger-carrying samples and 96% for adversarial examples across diverse modalities (image, text, audio) and tasks (classification and regression).

## Method Summary
UniGuard operates through a trajectory-based anomaly detection pipeline that extracts layer-wise latent representations during inference, reduces their dimensionality using UMAP, compresses them temporally using LSTM autoencoders, transforms to frequency domain via FFT, and applies Deep SVDD for one-class classification. The framework trains exclusively on benign samples, treating all adversarial inputs (whether AE-perturbed or trigger-carrying) as anomalies. Key components can operate in parallel with model inference, minimizing computational overhead. The approach is validated across multiple model architectures (ResNet, BERT), attack strategies, and datasets, demonstrating superior performance compared to state-of-the-art specialized methods.

## Key Results
- Achieves >99% detection accuracy for trigger-carrying samples across all attack types
- Maintains >96% detection accuracy for adversarial examples even on backdoored models
- Outperforms specialized methods: ContraNet (AE detection) and TED (backdoor detection)
- Shows minimal false rejection rates (1-5%) while maintaining high detection performance

## Why This Works (Mechanism)

### Mechanism 1: Propagation Trajectory Divergence
- Claim: Adversarial samples produce distinct layer-wise propagation paths through neural networks compared to benign samples
- Core assumption: Adversarial objectives require representation divergence at some network depth
- Evidence: "adversarial input, whether a perturbed sample in AE attacks or a trigger-carrying sample in backdoor attacks, exhibits distinctive trajectory signatures from a benign sample as it propagates through the layers"
- Break condition: Adaptive attacks explicitly penalizing trajectory deviation (detection drops to ~89-91%)

### Mechanism 2: Temporal-Spectral Signal Amplification
- Claim: LSTM + FFT amplifies subtle trajectory differences imperceptible in raw feature space
- Core assumption: Benign and adversarial trajectories have distinguishable temporal and spectral characteristics
- Evidence: "leveraging LSTM and spectrum transformation to amplify differences between adversarial and benign trajectories that are subtle in the time domain"
- Break condition: Attacks engineered to produce similar LSTM encodings and spectral signatures as benign samples

### Mechanism 3: One-Class Anomaly Detection
- Claim: Deep SVDD trained only on benign trajectories enables attack-agnostic detection
- Core assumption: All successful attacks produce out-of-distribution trajectories
- Evidence: "we approach online adversarial sample detection as an anomaly detection problem"
- Break condition: Attacks producing trajectories within learned benign hypersphere

## Foundational Learning

- **Layer-wise Representations in Neural Networks**
  - Why needed: UniGuard extracts activations from each convolutional layer as trajectory elements
  - Quick check: Why do early convolutional layers capture edges/textures while deeper layers encode semantic concepts?

- **LSTM Sequence Modeling**
  - Why needed: LSTM encoder treats layer-wise representations as a temporal sequence
  - Quick check: How do forget, input, and output gates control information flow in an LSTM cell?

- **One-Class Classification / Anomaly Detection**
  - Why needed: Deep SVDD is the core detector
  - Quick check: What happens when Deep SVDD's hypersphere radius is too small vs. too large?

## Architecture Onboarding

- **Component map:** Layer Extraction Hook → UMAP Reducer → LSTM Encoder-Decoder → FFT Transformer → Deep SVDD
- **Critical path:** Raw activations → UMAP (parallel to inference) → LSTM encoder → FFT → Deep SVDD score → threshold comparison
- **Design tradeoffs:**
  - UMAP vs PCA: UMAP ~12× faster but needs 100+ samples; use PCA for smaller datasets
  - Full vs partial layers: All layers optimal; sampling every 2 layers achieves comparable 99.51% accuracy
  - FRR preset (1-5%): Lower FRR slightly reduces detection accuracy but fewer false rejections
  - z dimension: 60-dim used for ResNet18; adjust for model complexity
- **Failure signatures:**
  - Adaptive trajectory mimicry: Detection drops to 89-91%
  - Insufficient samples (<100): Online FRR increases from ~1% to ~4%
  - Missing LSTM: Complete failure (1.8% vs 99.02% detection)
  - Missing FFT: Detection maintained but FRR rises to 3.98%
- **First 3 experiments:**
  1. Train UniGuard with 100 CIFAR10 benign samples on ResNet18; verify online FRR matches preset (1-4%)
  2. Infect ResNet18 with BadNet (1% poisoning); expect >99% detection on 1000 trigger-carrying samples
  3. Craft FGSM/PGD/DeepFool AEs (ε=8/255) on infected model; verify >96% detection across attacks

## Open Questions the Paper Calls Out

- **Theoretical Guarantees** - The authors state that "providing theoretical proof remains an open challenge" for proving adversarial trajectories must diverge from benign trajectories
- **Adaptive Attack Robustness** - Acknowledges that "stronger adaptive attacks may emerge in the future" and enhancing robustness is an ongoing challenge
- **Proprietary Model Access** - Assumes full access to intermediate layers, which may not hold in real-world deployments using third-party APIs

## Limitations
- Core assumption that all adversarial samples must exhibit detectable trajectory divergence may not hold for future adaptive attacks
- Theoretical foundation for why LSTM + FFT combination is optimal remains underexplored
- Performance degradation under adaptive attacks (89-91% detection) shows framework vulnerability

## Confidence
- **High:** Online Detection Performance (>99% detection for triggers, >96% for AEs across multiple modalities)
- **Medium:** Unified Detection Approach (shared mechanisms but slightly different performance characteristics)
- **Low:** Adaptive Attack Robustness (limited scope of tested adaptive attacks, performance drops to 89-91%)

## Next Checks
1. Test UniGuard against more sophisticated adaptive attacks that specifically target trajectory-based detection, including white-box scenarios
2. Evaluate cross-modal transferability of trajectory signatures learned on one modality to detect attacks in other modalities
3. Conduct comprehensive real-time deployment analysis measuring latency and resource usage on edge devices with concurrent inference workloads