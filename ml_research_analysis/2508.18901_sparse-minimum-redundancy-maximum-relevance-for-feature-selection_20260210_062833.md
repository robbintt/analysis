---
ver: rpa2
title: Sparse minimum Redundancy Maximum Relevance for feature selection
arxiv_id: '2508.18901'
source_url: https://arxiv.org/abs/2508.18901
tags:
- features
- feature
- screening
- knockoff
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SmRMR, a feature selection method that integrates
  both feature-feature and feature-target relationships. It extends the classic mRMR
  algorithm by incorporating a non-convex penalty, enabling accurate identification
  of inactive features.
---

# Sparse minimum Redundancy Maximum Relevance for feature selection

## Quick Facts
- arXiv ID: 2508.18901
- Source URL: https://arxiv.org/abs/2508.18901
- Reference count: 40
- Primary result: Introduces SmRMR, a feature selection method that integrates feature-feature and feature-target relationships, provides FDR control via knockoffs, and selects fewer features than HSIC-LASSO while maintaining comparable accuracy.

## Executive Summary
This paper presents SmRMR, a feature selection method that extends the classic mRMR algorithm by incorporating non-convex penalties and integrating both feature-feature and feature-target relationships. The method addresses the limitations of standard marginal screening by accounting for feature redundancy and enables accurate identification of inactive features through SCAD or MCP penalties. A two-step data-splitting procedure with knockoff filters provides theoretical FDR control even in ultra-high dimensional settings where traditional knockoff methods fail.

## Method Summary
SmRMR solves a continuous relaxation of the discrete mRMR problem using a penalized optimization framework. The loss function includes a redundancy term that penalizes correlated features, while non-convex penalties (SCAD/MCP) enable consistent identification of zero coefficients. A data-splitting approach first screens features down to a manageable size, then applies knockoff filters on the reduced set to control FDR. The method requires only an FDR threshold rather than a predetermined number of features.

## Key Results
- SmRMR performs comparably to HSIC-LASSO on accuracy metrics while selecting fewer features
- Theoretical guarantees include consistent estimation of zero coefficients and FDR control
- The method successfully handles cases where n << p through its two-step screening procedure
- Real-world experiments on datasets like GLIOMA demonstrate practical effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating feature-feature relationships into the screening loss function mitigates the selection of redundant variables, which standard marginal screening methods often miss.
- **Mechanism:** Standard Sure Independence Screening (SIS) evaluates features based solely on marginal utility $\hat{D}(X_k, Y)$. SmRMR relaxes the discrete mRMR problem into a continuous penalized optimization where the loss function $L_{v,n}(\theta)$ includes a redundancy term $\sum \theta_k \theta_l \hat{D}(X_k, X_l)$. If two features are highly correlated, the optimizer minimizes the loss by suppressing one of the coefficients ($\theta_k \approx 0$) to avoid the redundancy penalty.
- **Core assumption:** The association measure $D(\cdot, \cdot)$ accurately captures dependence, and the "pseudo-true" parameter $\theta_0$ exists.
- **Evidence anchors:**
  - [abstract] "feature screening method that integrates both feature-feature and feature-target relationships."
  - [section 2.3] "standard feature screening screens out an unimportant feature when $\hat{D}(X_k, Y) < \lambda_n$. Thus, our proposed methodology refines the standard approach by incorporating the feature-feature relationship."
- **Break condition:** If features are orthogonal (uncorrelated), the redundancy term adds computational cost without changing the ranking relative to marginal screening.

### Mechanism 2
- **Claim:** Non-convex penalties (SCAD, MCP) enable the consistent identification of inactive features (sparsistency) where LASSO might fail due to bias.
- **Mechanism:** The LASSO ($L_1$) penalty shrinks all coefficients, biasing the estimation of true signals. The paper utilizes SCAD and MCP, which behave like $L_1$ for small coefficients but gradually reduce the penalty rate to zero for larger coefficients. This allows the estimator $\hat{\theta}$ to satisfy "oracle properties"—identifying the true zero set $\hat{\theta}_{n2} = 0$ with probability tending to 1 (Theorem 3.6)—provided the signal is sufficiently strong.
- **Core assumption:** The minimum true signal strength must diverge faster than $\lambda_n$; the regularization parameter $\lambda_n$ must satisfy specific divergence rates relative to $n$ and $p$.
- **Evidence anchors:**
  - [abstract] "incorporating a non-convex penalty, enabling accurate identification of inactive features."
  - [section 3] "In the LASSO case... it cannot simultaneously satisfy [consistency conditions]... One way to fix this issue would be the adaptive LASSO... we prefer to avoid an additional layer of complexity."
- **Break condition:** If the sample size $n$ is too small relative to $p$ or the signal is too weak ("beta-min" condition violated), the local minimizer may not satisfy the sparsistency property.

### Mechanism 3
- **Claim:** A two-step data-splitting procedure with Knockoff filters controls the False Discovery Rate (FDR) even when $n \ll p$.
- **Mechanism:** Standard Knockoffs require $n \ge 2p$. To handle $p \gg n$, the paper splits data: Step 1 uses SmRMR on subset $n_0$ to screen features down to size $p_{max}$ (where $n_1 > 2p_{max}$). Step 2 constructs Knockoff variables $\tilde{X}$ and computes the statistic $W_k$. By comparing the selection of real vs. knockoff features, a threshold $T(\alpha)$ is derived to bound the expected proportion of false discoveries.
- **Core assumption:** The screening step must include all true active features (sure screening event $E_n$); the joint distribution of features allows for valid knockoff construction (e.g., Gaussian assumption for second-order knockoffs).
- **Evidence anchors:**
  - [abstract] "multi-stage procedure based on the knockoff filter enabling the penalized mRMR to discard inactive features while controlling the false discovery rate (FDR)."
  - [section 4] "To circumvent this issue [$n < 2p$]... we consider a data splitting procedure... The statistic $\tilde{W}_k$ satisfies these two properties."
- **Break condition:** If the screening step discards a true active feature (violating sure screening), the subsequent FDR control is invalid for that feature.

## Foundational Learning

- **Concept: Continuous Relaxation of mRMR**
  - **Why needed here:** The classic mRMR is a discrete combinatorial optimization (NP-hard). Understanding how the authors map the discrete set selection to a continuous penalized regression problem (Eq. 2.3) is essential for implementing the solver.
  - **Quick check question:** How does the parameter $\theta$ in SmRMR relate to the set membership indicator in the original mRMR formulation?

- **Concept: Association Measures (HSIC vs. Projection Correlation)**
  - **Why needed here:** The loss function relies on a valid estimator of dependence $\hat{D}_v(\cdot, \cdot)$. The paper contrasts HSIC (kernel-based) and Projection Correlation (moment-free). Understanding their computational complexity (e.g., $O(n^3)$ vs $O(n^2)$ implications) helps in selecting the right backend.
  - **Quick check question:** Why does the paper normalize HSIC (Eq 2.9) before using it in the loss function?

- **Concept: Local Linear Approximation (LLA)**
  - **Why needed here:** Solving non-convex penalties (SCAD/MCP) is non-trivial. The paper utilizes LLA (Algorithm 1), which iteratively solves weighted convex problems, to approximate the solution. Implementing SmRMR requires implementing this iterative re-weighting scheme.
  - **Quick check question:** In the LLA algorithm, how are the weights updated at each iteration $s$?

## Architecture Onboarding

- **Component map:** Pre-processor -> Screening Module -> Knockoff Generator -> Selection Module -> Decision Engine
- **Critical path:** The computation of the dependence matrix $D_{XX}$ (size $p \times p$) and the optimization of the non-convex penalty. If $p$ is ultra-high (e.g., $>5000$), the matrix operations become the bottleneck before the solver even starts.
- **Design tradeoffs:**
  - **HSIC-LASSO vs. SmRMR:** SmRMR is more conservative (selects fewer features) and provides FDR guarantees. HSIC-LASSO selects more features and may have higher accuracy but lacks theoretical FDR control in this specific high-dimensional setup.
  - **Convex vs. Non-Convex Penalties:** Non-convex (SCAD/MCP) offer theoretical sparsistency but require iterative solvers (LLA), increasing computational complexity compared to single-pass LASSO.
- **Failure signatures:**
  - **Empty Set:** If the FDR threshold $\alpha$ is too strict or the signal weak, the method returns an empty set. The paper mentions a "SmRMR2" fallback that iteratively relaxes $\alpha$ or returns the top feature if the empty set occurs.
  - **No Solution Found:** If the kernel matrices are not positive definite or data splits result in singular matrices during optimization.
- **First 3 experiments:**
  1. **Linear/Non-linear Simulation:** Run SmRMR on synthetic data (DGP 1.a vs 2.a) to verify FDR control (is it $\le \alpha$?) and compare MSE against HSIC-LASSO.
  2. **Ablation on Penalties:** Compare SmRMR(PC, LASSO) vs SmRMR(PC, SCAD) on correlated data ($c=0.5$) to quantify the "unbiasedness" benefit of SCAD in recovering zero coefficients.
  3. **Scaling Test:** Profile the algorithm on the "GLIOMA" dataset ($n=50, p=4434$) to measure the time cost of the Screening Step vs. the Knockoff construction.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees depend on strong assumptions including the existence of a "pseudo-true" parameter and the beta-min condition requiring sufficiently strong signals
- Data-splitting approach trades statistical efficiency for FDR control, potentially reducing power compared to full-sample methods
- Computational complexity scales poorly with ultra-high dimensions due to the $p \times p$ dependence matrix computation

## Confidence
- **High Confidence:** The mechanism for integrating redundancy via the $\theta_k\theta_l$ term in the loss function (Mechanism 1) is mathematically sound and directly supported by the formulation.
- **Medium Confidence:** The non-convex penalty benefits (Mechanism 2) are theoretically justified but depend critically on the beta-min condition being satisfied, which may not occur in practice.
- **Medium Confidence:** The FDR control through knockoffs (Mechanism 3) is valid under the stated assumptions, but the screening step must successfully retain all true features - a nontrivial requirement in high dimensions.

## Next Checks
1. **Assumption Stress Testing:** Systematically evaluate SmRMR's performance when the beta-min condition is violated or when the dependence measure fails to capture complex non-linear relationships.
2. **Power Analysis:** Compare the statistical power of the data-splitting approach against full-sample alternatives on moderately high-dimensional problems where both are computationally feasible.
3. **Computational Scaling:** Benchmark the exact computational cost of the $p \times p$ dependence matrix calculation and optimization solver across different feature dimensions to identify practical limits.