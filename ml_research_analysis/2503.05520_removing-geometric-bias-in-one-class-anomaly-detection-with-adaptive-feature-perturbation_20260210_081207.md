---
ver: rpa2
title: Removing Geometric Bias in One-Class Anomaly Detection with Adaptive Feature
  Perturbation
arxiv_id: '2503.05520'
source_url: https://arxiv.org/abs/2503.05520
tags:
- plume
- normal
- class
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unsupervised one-class anomaly
  detection in images without relying on geometric biases present in standard datasets.
  The authors propose PLUME, a method that operates in frozen feature spaces from
  pretrained models and generates pseudo-anomalous features using an adaptive linear
  feature perturbation technique.
---

# Removing Geometric Bias in One-Class Anomaly Detection with Adaptive Feature Perturbation

## Quick Facts
- arXiv ID: 2503.05520
- Source URL: https://arxiv.org/abs/2503.05520
- Reference count: 40
- Achieves 84.5% average AUC on CIFAR-10 and 80.3% on CIFAR-100, outperforming state-of-the-art baselines

## Executive Summary
This paper introduces PLUME, a one-class anomaly detection method that addresses geometric bias in standard datasets by operating on frozen feature spaces from pretrained models. The key innovation is an adaptive linear feature perturbation technique that generates pseudo-anomalous features while preserving semantic structure. The method incorporates a VAE-based perturbator to create sample-specific transformations, combined with contrastive learning objectives to improve classification. PLUME demonstrates strong performance on both standard datasets and geometric bias-free SPARK dataset, validating its effectiveness in removing spurious correlations.

## Method Summary
PLUME operates in frozen feature spaces extracted from ImageNet-pretrained backbones (ResNet50, VGG16, or ConvNeXt). A VAE-based perturbator generates sample-specific parameters (α, β) to construct a linear perturbation matrix A = I + αβ^T, which is applied to original features to create pseudo-anomalies. These are combined with normal features and passed through a classifier (3-layer MLP) that is trained with a multi-task loss including classification, perturbation constraint, KL divergence, and contrastive learning objectives. The backbone weights remain frozen throughout training, while only the perturbator and classifier are updated using AdamW with cyclical learning rate.

## Key Results
- Achieves 84.5% average AUC on CIFAR-10, outperforming state-of-the-art methods
- Reaches 80.3% average AUC on CIFAR-100 with ResNet50 features
- Demonstrates 77% AUC on geometric bias-free SPARK dataset, validating robustness
- LinearMap perturbation with contrastive guidance outperforms element-wise noise operations by 6.6pp on CIFAR-10

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Linear Perturbation via VAE-Sampled Transformations
Linear perturbations applied to frozen feature vectors generate semantically coherent pseudo-anomalies that respect feature space structure better than element-wise noise. A VAE-based perturbator generates sample-specific parameters (α, β) ∈ R^D × R^D from encoded normal features, constructing perturbation matrix A = I + αβ^T. Constraint loss L_n = ||α - 1||^2 + ||β - 0||^2 keeps A near identity, ensuring perturbations remain local to normal data.

### Mechanism 2: Frozen Pretrained Feature Extraction as Semantic Anchor
Using frozen features from ImageNet-pretrained models provides structured, information-rich representations that bound normality more effectively than training end-to-end from pixels. A backbone network h (ResNet50, VGG16, or ConvNeXt) pretrained on ImageNet extracts D=3072-dimensional feature vectors, which are normalized and pooled adaptively. Backbone weights remain frozen; only perturbator and classifier are trained.

### Mechanism 3: Contrastive Learning for Unified Normal Embeddings
A contrastive loss operating on classifier embeddings encourages normal samples to aggregate tightly while repelling pseudo-anomalies, sharpening the decision boundary. The classifier f is decomposed into f₁ (embedding layers) and f₂ (boundary estimator). Normal embeddings z_i are pulled together while pushed away from pseudo-anomaly embeddings ˜z_l via temperature-scaled cosine similarity.

## Foundational Learning

**Concept: One-class anomaly detection paradigm**
Why needed: PLUME operates in unsupervised setting where only normal samples X^+ are available during training, explaining why pseudo-anomaly generation is necessary.
Quick check: Can you explain why a classifier trained only on normal data would fail to learn a useful decision boundary without synthetic negatives?

**Concept: Variational Autoencoder (VAE) fundamentals**
Why needed: The perturbator uses VAE architecture with encoder producing (μ, σ) and KL divergence regularization.
Quick check: What role does the KL divergence term D_KL play in preventing the encoder from collapsing to deterministic outputs?

**Concept: Contrastive learning with temperature scaling**
Why needed: The contrastive loss L_c uses temperature τ to control similarity concentration.
Quick check: If τ is set too high (e.g., τ=10), what happens to the distribution of similarity scores and subsequent gradient magnitudes?

## Architecture Onboarding

**Component map**:
Input Image → Frozen Backbone h: ResNet50/VGG16/ConvNeXt → Adaptive Pooling + BatchNorm → Feature x ∈ R^3072 → Perturbator g: VAE → (α, β) ∈ R^D × R^D → A = I + αβ^T → ˜x = Ax → Classifier f: 3-layer MLP ← Normal x + Pseudo-anomaly ˜x → Embedding z ∈ R^512 → Contrastive Loss L_c → Binary Output → Classification Loss L_CE

**Critical path**:
1. Feature extraction from frozen backbone (deterministic once backbone chosen)
2. VAE encoder produces (μ, σ); sample intermediate via reparameterization
3. Decoder generates (α, β); apply constraint L_n to keep near (1, 0)
4. Construct linear perturbation A = I + αβ^T
5. Generate pseudo-anomalies ˜x = Ax
6. Forward normal and pseudo-anomaly through classifier
7. Extract embeddings for contrastive loss; compute L_CE for classification
8. Total loss L = (1/N)Σ(L_CE + λL_n + νD_KL + γL_c)

**Design tradeoffs**:
- LinearMap vs. AddMult perturbation: LinearMap (+contrastive) gives 83.4% vs. AddMult (+contrastive) at 61.4% on CIFAR-10
- Frozen vs. fine-tuned backbone: Frozen reduces overfitting and training cost but may underperform if backbone domain differs
- Perturbator discarded at inference: Reduces inference compute but sacrifices potential adaptive scoring

**Failure signatures**:
- Classifier accuracy stuck at 50%: L_n constraint weight λ too large; perturbations near identity
- Contrastive loss diverging: Pseudo-anomalies incoherent (e.g., Gaussian noise)
- High variance across runs (±3–6%): VAE sampling introduces stochasticity

**First 3 experiments**:
1. Reproduce CIFAR-10 baseline: Use ResNet50 backbone, λ=5, batch size N=32, train for 100 epochs with CLR + AdamW
2. Ablate perturbation type: Run LinearMap, AddMult, and Gaussian perturbations with and without contrastive guidance on CIFAR-10 Bird class
3. Stress-test on bias-free dataset: Evaluate on SPARK satellite/debris with ResNet50, VGG16, and ConvNeXt backbones

## Open Questions the Paper Calls Out

**Open Question 1**
Would replacing the current noise constraint (L_n) with a pairwise similarity maximization objective between normal and generated samples yield higher detection performance? The authors suggest refining the loss function, specifically that "maximising pairwise similarity between normal and generated samples, instead of constraining noise, could boost performance."

**Open Question 2**
Does reducing the number of learning objectives from four to a unified formulation improve the training stability of the perturbator? Section 5 notes that "reducing the current four learning objectives may improve network stability," implying the current multi-loss setup might be suboptimal.

**Open Question 3**
Can the linear feature perturbation technique be effectively adapted for time-series data that exhibits strong temporal dependencies? Section 5 identifies "investigating extensions to time-series, facing time dependencies," as a promising direction for future research.

## Limitations
- Performance depends critically on frozen feature quality from ImageNet-pretrained backbones
- VAE-based perturbation introduces stochasticity affecting reproducibility
- Performance drops on geometric bias-free datasets suggest residual dependence on dataset artifacts

## Confidence
- High confidence in Adaptive Linear Perturbation mechanism effectiveness (6.6pp improvement in CIFAR-10)
- Medium confidence in Frozen Feature Extraction benefits (strong results but domain generalization unclear)
- Medium confidence in Contrastive Learning utility (works with LinearMap but degrades with Gaussian perturbations)

## Next Checks
1. Test PLUME with backbones pretrained on non-natural image datasets (medical, satellite) to assess domain generalization
2. Implement deterministic perturbation sampling to evaluate impact of VAE stochasticity on reproducibility
3. Compare PLUME against end-to-end trained alternatives on geometric bias-free datasets to isolate feature space contribution