---
ver: rpa2
title: Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings
arxiv_id: '2601.18694'
source_url: https://arxiv.org/abs/2601.18694
tags:
- speaker
- audio
- voice
- cloning
- nepali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a neural voice cloning system for the Nepali
  language, addressing the lack of speech synthesis research for low-resource languages.
  The system employs a speaker encoder trained with Generative End2End loss to generate
  speaker embeddings, which are fused with Tacotron2-based text embeddings to produce
  mel-spectrograms.
---

# Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings

## Quick Facts
- arXiv ID: 2601.18694
- Source URL: https://arxiv.org/abs/2601.18694
- Reference count: 0
- Primary result: Achieves mean cosine similarity of 0.904 and MOS of 3.924 for Nepali voice cloning using minimal reference audio

## Executive Summary
This study presents a neural voice cloning system for the Nepali language, addressing the lack of speech synthesis research for low-resource languages. The system employs a speaker encoder trained with Generative End2End loss to generate speaker embeddings, which are fused with Tacotron2-based text embeddings to produce mel-spectrograms. These are converted to audio using a WaveRNN vocoder. Datasets were curated from multiple sources, including self-recordings, and underwent thorough preprocessing. The system achieves high-quality voice cloning, with cosine similarities above 0.90 for most speakers, and Mean Opinion Scores (MOS) of 3.84 (male) and 4.01 (female) for audio quality. Speaker embeddings cluster effectively by identity and gender, validating the system's ability to capture vocal characteristics.

## Method Summary
The system uses a three-stage pipeline: (1) A speaker encoder with 3 LSTM layers (256 units) produces 256-dimensional embeddings from 1.6-second audio chunks using GE2E loss, (2) A Tacotron2-based synthesizer concatenates speaker embeddings with text encoder outputs to generate mel-spectrograms, and (3) A WaveRNN vocoder converts mel-spectrograms to time-domain waveforms. Datasets include 833 speakers with 235 hours for encoder training and 6,046 audio-text pairs (8.67 hours) for synthesizer training. Audio preprocessing includes silence trimming, normalization, and 16kHz/22.05kHz resampling, while text preprocessing handles numeral-to-text conversion and stop token insertion.

## Key Results
- Mean cosine similarity of 0.904 for speaker embeddings across test speakers
- MOS audio quality scores of 3.84 (male) and 4.01 (female)
- EER below 0.04 during speaker encoder training, indicating effective speaker discrimination
- Speaker embeddings cluster by identity and gender in UMAP visualizations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A speaker encoder trained with Generative End-to-End (GE2E) loss can generate fixed-dimensional embeddings that capture vocal identity from minimal reference audio, enabling few-shot voice cloning.
- **Mechanism:** The encoder processes 1.6-second audio chunks through three LSTM layers (256 units each) and projects outputs to a 256-dimensional embedding space. GE2E loss optimizes this space by maximizing similarity among embeddings from the same speaker while minimizing similarity across different speakers, creating a discriminative representation where geometric proximity correlates with perceptual speaker similarity.
- **Core assumption:** Speaker identity can be disentangled from linguistic content and mapped to a continuous vector space where distance reflects perceptual similarity.
- **Evidence anchors:**
  - [abstract] "The speaker encoder, optimized with Generative End2End (GE2E) loss, generates embeddings that capture the speaker's vocal identity, validated through Uniform Manifold Approximation and Projection (UMAP) for dimension reduction visualizations."
  - [section 5.1] "The speaker encoder consists of three LSTM layers with 256 units each, followed by a fully connected layer producing 256-dimensional speaker embeddings."
  - [section 6.1] "As shown in Figure 5, the EER steadily decreased with training steps, starting near 0.10 and dropping below 0.04... demonstrating that the encoder continually improved its ability to distinguish between speakers."
  - [corpus] No direct corpus papers examine GE2E loss mechanisms; related work focuses on cloning security and detection.
- **Break condition:** If UMAP visualizations show no speaker clustering or EER fails to decrease below ~0.08 during training.

### Mechanism 2
- **Claim:** Concatenating speaker embeddings with text encoder outputs enables Tacotron2 to generate mel-spectrograms conditioned on both linguistic content and speaker identity.
- **Mechanism:** The Tacotron2 encoder converts Devanagari text to hidden representations. These are concatenated with speaker embeddings before the attention mechanism. The attention module computes alignment weights between text and audio frames, and the decoder generates mel-spectrograms that reflect both what is being said and who is saying it.
- **Core assumption:** Simple concatenation is sufficient to inject speaker identity without disrupting the learned text-to-speech alignment process.
- **Evidence anchors:**
  - [section 5] "Within the Tacotron2 framework, the text is first encoded by the Encoder module into a hidden representation. This encoded text representation is then concatenated with the speaker embeddings, effectively conditioning the synthesis on the speaker's identity."
  - [Appendix B.1] "The alignment plot shows near-linear progression, reflecting good temporal consistency during synthesis."
  - [corpus] Corpus lacks direct evidence for this concatenation-based conditioning approach.
- **Break condition:** If alignment plots show zigzag patterns, attention diagonal failures, or mel-spectrograms lack speaker-specific characteristics.

### Mechanism 3
- **Claim:** WaveRNN vocoder converts mel-spectrograms to time-domain waveforms while preserving speaker identity and achieving natural perceptual quality.
- **Mechanism:** WaveRNN employs an upsampling network, residual convolutional blocks, and GRU layers to model the conditional probability distribution of audio samples given mel-spectrogram frames. It generates 16-bit PCM waveforms sample-by-sample, capturing fine-grained acoustic details including speaker timbre.
- **Core assumption:** A pre-trained vocoder can generalize to new speakers through fine-tuning, and mel-spectrograms contain sufficient information for high-fidelity reconstruction.
- **Evidence anchors:**
  - [section 5.3] "The vocoder component... employs a modified WaveRNN architecture comprising an up-sampling network, residual convolutional blocks, and recurrent GRU layers optimized for high-fidelity audio synthesis."
  - [Table 4] MOS quality scores of 3.84 (male) and 4.01 (female) indicate acceptable perceptual quality.
  - [corpus] Limited corpus evidence for WaveRNN specifically; related papers mention alternative vocoders like HiFi-GAN for improved quality.
- **Break condition:** If generated audio exhibits metallic artifacts, speaker identity loss not traceable to earlier pipeline stages, or MOS scores below ~3.0.

## Foundational Learning

- **Concept: Mel-spectrograms as intermediate representations**
  - **Why needed here:** The entire pipeline centers on mel-spectrograms—the synthesizer generates them, the vocoder consumes them. Understanding STFT parameters (filter_length=800, hop_length=200, n_mel_channels=80, mel_fmax=7600Hz) is essential for debugging synthesis quality and spectral artifacts.
  - **Quick check question:** If mel_fmax is reduced from 7600Hz to 4000Hz, what types of speech sounds would lose acoustic detail?

- **Concept: LSTM sequence modeling for variable-length inputs**
  - **Why needed here:** The speaker encoder must handle arbitrary-length audio and produce fixed-dimensional embeddings. Understanding how LSTMs aggregate temporal information across timesteps helps diagnose why certain audio lengths or qualities produce poor embeddings.
  - **Quick check question:** Why might extremely short audio segments (<0.5 seconds) produce unreliable or noisy speaker embeddings?

- **Concept: Transfer learning for few-shot adaptation**
  - **Why needed here:** The system enables cloning unseen speakers with minimal data because the encoder learns general speaker representations during pre-training. Understanding this transfer is critical for extending to new voices or low-resource scenarios.
  - **Quick check question:** If pre-training only used 50 speakers instead of 833, how might few-shot cloning performance change for unseen voices?

## Architecture Onboarding

- **Component map:**
  1. Speaker Encoder: 3 LSTM layers (256 units) → Fully connected → 256-dim embedding | Input: 16kHz audio chunks (1.6s, 50% overlap)
  2. Tacotron2 Synthesizer: Text encoder + Attention + Decoder | Input: Devanagari text + speaker embedding | Output: 80-channel mel-spectrograms at 22.05kHz
  3. WaveRNN Vocoder: Upsampling + Residual conv + GRU | Input: Mel-spectrograms | Output: Time-domain waveforms
  4. Preprocessing: Format conversion, resampling (16kHz/22.05kHz), silence trimming, normalization, text numeral expansion

- **Critical path:**
  Audio input → 16kHz resampling → 1.6s chunking → Speaker encoder → 256-dim embedding
  Text input → Devanagari preprocessing → Text encoder → Concatenate with embedding → Attention → Decoder → Mel-spectrogram → WaveRNN → Audio output

- **Design tradeoffs:**
  - **Encoder data: quantity vs. quality** — Speaker encoder tolerates noise (uses 235 hours, 833 speakers) but synthesizer requires clean aligned data (only 8.67 hours available)
  - **Pre-trained vs. from-scratch vocoder** — Used pre-trained WaveRNN to conserve resources; trades potential quality gains for faster iteration
  - **Chunk size (1.6s)** — Maximizes training samples from limited data but may truncate prosodic patterns and context

- **Failure signatures:**
  - **Cosine similarity <0.85:** Encoder not capturing identity; check input audio quality or encoder training convergence
  - **EER plateauing >0.06:** Encoder failing to discriminate speakers; may need more diverse training data or architecture changes
  - **Zigzag/chaotic alignment plots:** Attention mechanism failing; reduce learning rate or verify text-audio alignment in training data
  - **High pitch or metallic artifacts:** Vocoder generalization failure; consider vocoder fine-tuning on target speaker data
  - **High MOS variance (>0.5 std):** Inconsistent quality across speakers; audit speaker-specific recording conditions

- **First 3 experiments:**
  1. **Encoder validation on held-out speakers:** Compute cosine similarity and EER for speakers excluded from training; target: similarity >0.90, EER <0.04. Check UMAP clustering for speaker separation.
  2. **Attention alignment ablation:** Visualize attention weights for multiple test sentences; verify diagonal alignment pattern. If failed, experiment with guided attention loss or smaller learning rate.
  3. **Vocoder quality comparison:** Synthesize identical mel-spectrograms through current WaveRNN and compare spectral features against original audio; identify systematic artifacts (formant loss, high-frequency roll-off).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can integrating advanced vocoders such as HiFi-GAN or Speaker-Conditional WaveRNN (SC-WaveRNN) significantly improve audio quality and speaker similarity for unseen Nepali voices in low-resource settings?
- **Basis in paper:** [explicit] The authors state that "more advanced vocoders like HiFi-GAN or Speaker-Conditional WaveRNN (SC-WaveRNN) offer superior performance, especially for unseen speakers and varied conditions" and represent "a promising direction for future work."
- **Why unresolved:** The authors could not implement these vocoders due to computational and data constraints; only WaveRNN fine-tuning was feasible.
- **What evidence would resolve it:** Train and evaluate the same system with HiFi-GAN or SC-WaveRNN on the existing Nepali dataset, comparing MOS and similarity scores.

### Open Question 2
- **Question:** To what extent does the age bias (skew toward speakers aged 20–35) in the training data limit voice cloning accuracy for children and older adults in Nepali?
- **Basis in paper:** [explicit] The paper reports that "the dataset is skewed toward individuals aged 20–35 years," with underrepresentation of those under 20 and over 55.
- **Why unresolved:** The current dataset does not contain sufficient samples from underrepresented age groups to evaluate cross-age generalization.
- **What evidence would resolve it:** Curate age-balanced test sets, clone voices across age groups, and measure similarity/quality scores; analyze errors by age cohort.

### Open Question 3
- **Question:** What data augmentation or model-level techniques can improve robustness of Nepali voice cloning to noisy input audio and unique dialectal variations?
- **Basis in paper:** [explicit] The authors note the system "struggles significantly with unclear audio, noisy environments, or unique linguistic variations, sometimes even generating a completely different or unnatural voice."
- **Why unresolved:** The paper does not experiment with noise robustness or dialectal coverage; only clear, standard speech was tested.
- **What evidence would resolve it:** Apply noise augmentation, dialect-balanced sampling, or noise-aware training, then evaluate MOS and similarity under noisy and dialect-varied conditions.

## Limitations
- Computational resource requirements for three-stage training pipeline may limit accessibility
- Reliance on manually curated datasets without specified train/validation/test splits
- Age bias in training data (skew toward speakers aged 20-35) limits generalization to children and older adults
- Performance for other low-resource languages remains untested and unknown

## Confidence
- **High Confidence:** Speaker encoder training convergence (EER <0.04 achieved), UMAP visualization showing speaker clustering, mel-spectrogram generation quality (near-diagonal attention patterns), MOS quality scores (3.84-4.01)
- **Medium Confidence:** Few-shot cloning performance for unseen speakers (limited to 10 test speakers), GE2E loss effectiveness compared to alternative speaker embedding methods, vocoder generalization across speakers
- **Low Confidence:** Cross-lingual generalization to other low-resource languages, scalability to thousands of speakers, long-term robustness across diverse recording conditions

## Next Checks
1. **Generalization Test:** Evaluate the complete pipeline on speakers excluded from all training stages (both encoder and synthesizer) to verify true few-shot capability; target cosine similarity >0.90 and MOS >3.5 for speakers never seen during any training phase.

2. **Chunk Duration Ablation:** Systematically vary audio chunk duration (0.8s, 1.6s, 3.2s) during speaker encoder training and test impact on cosine similarity and EER; determine optimal tradeoff between training efficiency and embedding quality.

3. **Language Transfer Validation:** Apply the same architecture to another low-resource language (e.g., Sinhala or Tibetan) with comparable dataset sizes; compare performance metrics to establish whether the approach generalizes beyond Nepali.