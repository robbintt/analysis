---
ver: rpa2
title: 'ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for
  Efficient Reasoning'
arxiv_id: '2504.21370'
source_url: https://arxiv.org/abs/2504.21370
tags:
- uni00000013
- reasoning
- length
- uni00000018
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of inefficient reasoning in large
  language models, where extended Chain-of-Thought (CoT) traces often lead to redundant
  outputs and overthinking. The proposed method, ShorterBetter, uses reinforcement
  learning to guide reasoning models toward optimal inference lengths without manual
  supervision.
---

# ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning

## Quick Facts
- arXiv ID: 2504.21370
- Source URL: https://arxiv.org/abs/2504.21370
- Reference count: 40
- Primary result: Achieves 50%-80% reduction in reasoning output lengths while maintaining accuracy on math and coding tasks

## Executive Summary
This paper addresses the problem of inefficient reasoning in large language models, where extended Chain-of-Thought (CoT) traces often lead to redundant outputs and overthinking. The proposed method, ShorterBetter, uses reinforcement learning to guide reasoning models toward optimal inference lengths without manual supervision. It defines Sample Optimal Length (SOL) as the length of the shortest correct response among multiple generations, serving as a dynamic reward signal. Applied to DeepSeek-Distill-Qwen-1.5B/7B base models, ShorterBetter achieves 50%-80% reduction in output lengths on both in-domain and out-of-domain reasoning tasks while maintaining accuracy.

## Method Summary
ShorterBetter is a reinforcement learning framework that optimizes inference efficiency by training models to generate reasoning traces at their optimal length. The method generates multiple rollouts per prompt, identifies the shortest correct response as the Sample Optimal Length (SOL), and uses this as a dynamic reward target. A Group Relative Policy Optimization (GRPO) algorithm computes within-group advantages to update the policy, balancing correctness and brevity through a reward function that combines correctness indicators with length deviation penalties. The approach is applied to base reasoning models without requiring manual supervision or additional model components.

## Key Results
- Achieves 50%-80% reduction in output lengths on reasoning tasks while maintaining accuracy
- Demonstrates effective generalization across in-domain (math) and out-of-domain (coding) benchmarks
- Shows structural refinement of reasoning traces by reducing low-value categories like repetition and excessive self-verification

## Why This Works (Mechanism)

### Mechanism 1: Sample Optimal Length as Dynamic Length Target
Using the shortest correct response within a rollout group as a length target produces stable training where naive shortest-response targets cause collapse. For each prompt, generate n rollouts, identify correct responses via verifier, set ℓ_SOL to the minimum length among correct responses (or mean length if none correct). This becomes a per-prompt dynamic target that adjusts to both problem difficulty and current model capability.

### Mechanism 2: Within-Group Relative Advantage via GRPO
Normalizing rewards within each rollout group rather than globally produces more stable policy updates for length-reward optimization. GRPO computes advantage Â_j,k = (r_j - mean(r)) / std(r) across the n samples per prompt, then applies clipped policy gradient with KL penalty. High-reward samples (correct + short) get positive advantage; low-reward samples (incorrect or excessively long) get negative advantage.

### Mechanism 3: Reasoning Trace Structural Refinement
The method does not uniformly compress reasoning but selectively reduces low-value categories while preserving or increasing high-value reasoning steps. By rewarding brevity only relative to correct shortest paths, the policy learns to eliminate verbose repetition, excessive self-verification after reaching answers, and over-exploration of alternatives. Categories "Pivotal Reasoning" and "Productive Elaboration & Calculation" increase in proportion.

## Foundational Learning

- Concept: Policy Gradient Methods and Advantage Estimation
  - Why needed here: GRPO is fundamentally a policy gradient method; understanding how advantages shape policy updates is essential for debugging reward design choices.
  - Quick check question: Given rewards [1.0, 0.6, -0.3] for three samples, compute the advantage for each after mean-centering and normalization.

- Concept: Chain-of-Thought Reasoning and Test-Time Compute Scaling
  - Why needed here: The paper builds on the premise that extended CoT improves reasoning but introduces efficiency tradeoffs; you must understand what CoT provides to evaluate what can be safely compressed.
  - Quick check question: Name three failure modes of overthinking identified in the paper and explain why each degrades performance or efficiency.

- Concept: Group Relative Policy Optimization (GRPO) Specifics
  - Why needed here: This is the training algorithm; understanding the loss function components (clipping, KL penalty) is necessary for hyperparameter tuning and failure diagnosis.
  - Quick check question: In the GRPO loss L_GRPO(θ), what is the role of the KL divergence term and what happens if γ is set too high or too low?

## Architecture Onboarding

- Component map: Base LRM (DeepSeek-Distill-Qwen) -> Rollout Generator (n=8 samples per prompt, temperature=0.9) -> Verifier (binary: correct/incorrect) -> SOL Calculator (shortest correct length OR mean if none) -> Reward Function: r(y) = α·I(correct) - β·|length - SOL| -> GRPO Advantage Normalization (within-group) -> Policy Updater (clipped objective + KL penalty) -> Updated LRM

- Critical path: The reward function is the highest-leverage component. Incorrect SOL computation (e.g., using shortest regardless of correctness) causes training collapse within 100 steps (Section 5.3). The fallback to average length when no correct responses exist is essential for stability.

- Design tradeoffs:
  - α vs β: Higher α emphasizes correctness (stable accuracy, weaker compression); higher β emphasizes brevity (risk of accuracy degradation). Paper recommends α ∈ [2, 5] with β=0.001.
  - Rollout count n: More samples improve SOL estimation but increase compute per step. n=8 works for 1.5B/7B models.
  - Max token budget during training: Paper uses 5-6K during training but 16K during evaluation; tight budgets early in training are acceptable as lengths decrease quickly.

- Failure signatures:
  - Training collapse in <100 steps: Check if SOL is being computed from shortest response regardless of correctness (ablation shows this fails).
  - Accuracy degradation without length reduction: α may be too low; increase correctness reward weight.
  - Length reduction without accuracy maintenance: β may be too high relative to α; rebalance toward correctness.

- First 3 experiments:
  1. Sanity check with α=2, β=0.001, n=8 on a 500-sample math subset: Verify that SOL decreases over 50 steps while accuracy remains within ±2% of base model. Plot median output length and accuracy per step.
  2. Ablation: Compare SOL reward vs. shortest-response reward: Replicate the collapse experiment from Section 5.3 on a small scale to confirm the correctness constraint is necessary for stable training.
  3. Category analysis on 50 samples before/after training: Use the LLM-as-judge categorization framework to confirm that Non-Substantive Statements decrease and Pivotal Reasoning increases, validating structural refinement is occurring as expected.

## Open Questions the Paper Calls Out

### Open Question 1
How can Sample Optimal Length (SOL) be effectively adapted for open-ended generation tasks that lack binary correctness verification? The authors state in the Conclusion that while current results focus on verifiable answers, "our general formulation... naturally extends to open-ended settings with non-binary correctness scores." This remains unresolved as the current implementation relies on a binary verifier I(y=y*) to identify the shortest correct path to serve as the reward signal.

### Open Question 2
Does the ShorterBetter method maintain its efficiency and accuracy advantages when scaled to significantly larger reasoning models? The authors explicitly list "apply and evaluate our methods on larger-scale reasoning models" as a future direction. All empirical results presented are derived exclusively from experiments on 1.5B and 7B parameter models (DeepSeek-Distill-Qwen).

### Open Question 3
Can fine-grained, behavior-aware optimization strategies outperform uniform length reduction by selectively targeting specific reasoning categories? The Conclusion notes that "different categories of reasoning behavior are compressed to different extents," suggesting this could inform the design of "more fine-grained and behavior-aware reasoning optimization strategies." The current SOL reward optimizes for overall token length, which may inadvertently suppress beneficial behaviors alongside redundant ones.

## Limitations
- Reward function sensitivity: The balance between correctness and brevity parameters is empirically tuned rather than systematically analyzed across model scales
- Category analysis granularity: Token-level categorization using LLM-as-judge is conducted on relatively small subsets, introducing potential subjectivity
- Cross-domain generalization: While aggregate metrics show effectiveness, detailed per-domain failure analysis is limited

## Confidence

**High confidence**: The core mechanism of using Sample Optimal Length (SOL) as a dynamic reward signal, combined with correctness constraint, produces stable training that reduces output length while maintaining accuracy. The training collapse when removing the correctness constraint is clearly demonstrated through ablation.

**Medium confidence**: The structural refinement claim that the method selectively reduces low-value reasoning patterns while preserving high-value ones. While token-level analysis shows category shifts, the causal link between reward design and these specific category changes requires further validation.

**Medium confidence**: The generalization claim across domains. Aggregate performance metrics support this, but detailed per-domain analysis and failure case studies are limited.

## Next Checks

1. **Systematic hyperparameter sensitivity analysis**: Conduct a grid search over α ∈ [1, 2, 3, 5] and β ∈ [0.0005, 0.001, 0.002] to map the accuracy-length tradeoff surface. For each configuration, measure not just final performance but training stability (steps to convergence, variance across seeds) to identify truly robust parameter ranges.

2. **Category-level ablation on held-out samples**: Apply the LLM-as-judge categorization framework to 200 held-out samples before and after training with ShorterBetter. Compute statistical significance of category shifts and identify specific reasoning patterns that show the largest positive and negative changes.

3. **Cross-domain failure mode analysis**: For each out-of-domain benchmark (MATH, GSM8K, MBPP, HumanEval), analyze accuracy-length correlation and identify problem categories where the method underperforms relative to base models. This would reveal whether the method's efficiency gains come with domain-specific risks.