---
ver: rpa2
title: 'Timber: Training-free Instruct Model Refining with Base via Effective Rank'
arxiv_id: '2509.23595'
source_url: https://arxiv.org/abs/2509.23595
tags:
- instruct
- timber
- arxiv
- base
- effective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the exploration-exploitation trade-off in
  post-training of large language models (LLMs). The authors first demonstrate that
  post-training is superficial by showing that effective rank (eRank) remains nearly
  unchanged between Base and Instruct models.
---

# Timber: Training-free Instruct Model Refining with Base via Effective Rank

## Quick Facts
- arXiv ID: 2509.23595
- Source URL: https://arxiv.org/abs/2509.23595
- Authors: Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Zenan Xu, Ngai Wong
- Reference count: 24
- Key outcome: Training-free method that improves exploration capability of Instruct models without compromising exploitation by partially reverting weights toward Base state using effective rank thresholds

## Executive Summary
This paper addresses the exploration-exploitation trade-off in post-trained large language models by demonstrating that the effective rank (eRank) of weight matrices remains nearly unchanged between Base and Instruct models. This observation enables a training-free refinement method called Timber that partially reverts Instruct models toward their Base state by selectively attenuating or removing singular values in the weight delta. The method improves exploration capability (Pass@k for larger k) while preserving exploitation performance (Pass@1), outperforming simple model merging and truncated SVD baselines across multiple model sizes and architectures.

## Method Summary
Timber computes the weight delta between paired Base and Instruct models, then applies Singular Value Decomposition (SVD) to decompose this delta. The effective rank (eRank) of the delta serves as an adaptive threshold K, partitioning singular values into head (preserved) and tail (attenuated or removed). Two variants exist: Timber-L zeroes tail singular values, while Timber attenuates them with factor λ. The refined weights are reconstructed and used to replace the Instruct model's weights, enhancing exploration capability without additional training.

## Key Results
- Timber consistently improves vanilla Instruct models on Pass@k performance, demonstrating enhanced exploration capability
- Timber outperforms simple model merging and truncated SVD baselines, showing superior robustness to threshold selection
- Method is effective across different model sizes (0.6B to 30B) and architectures (Llama and Qwen series), with λ=0.2 identified as optimal starting point
- FFN layers benefit mathematical reasoning while attention layers benefit instruction following when refined separately

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-training preserves effective dimensionality of weight matrices, making refinement viable.
- Mechanism: The effective rank (eRank) of linear layers in Base and Instruct models remains nearly identical, indicating post-training primarily applies linear transformations within existing singular subspaces rather than fundamentally altering representational capacity.
- Core assumption: Preserved eRank implies that Base model knowledge remains accessible and can be selectively recovered.
- Evidence anchors:
  - [abstract]: "effective rank (eRank) remains negligibly changed" between Base and Instruct models
  - [Section 2.2, Figure 1]: Shows eRank values for corresponding layers across Llama and Qwen models are nearly identical; eRank-to-Rank ratios concentrated around 0.85
  - [corpus]: Shadow-FT paper confirms Base-Instruct weight similarity, noting weight differences can be <5%

### Mechanism 2
- Claim: eRank provides an adaptive threshold for isolating principal components of weight deltas.
- Mechanism: For weight delta WΔ = WI - WB, SVD yields singular values Σ. The ceiling of eRank(WΔ) defines threshold K, partitioning spectrum into head (preserve) and tail (remove or attenuate). Unlike fixed-ratio truncation, eRank adapts to each layer's intrinsic dimensionality.
- Core assumption: Tail singular values beyond eRank encode redundant or over-optimized patterns that limit exploration.
- Evidence anchors:
  - [Section 3.2, Eq. 5-7]: K := ⌈eRank(WΔ)⌉; Timber-L zeroes tail, Timber attenuates with λ factor
  - [Section 5.1, Table 3]: Timber outperforms Truncated SVD baselines (Truncate-R, Truncate-E) which show unstable performance across thresholds
  - [corpus]: COSPADI notes low-rank approximation is common in post-training compression but can impose rigid structural constraints

### Mechanism 3
- Claim: Partial reversion of Instruct toward Base restores exploration capacity while preserving exploitation gains.
- Mechanism: Instruct models optimize for high-reward paths (improved Pass@1) but suppress sampling diversity (reduced Pass@k). Attenuating tail singular values partially reverses this optimization, expanding solution space coverage. The λ parameter controls reversion degree.
- Core assumption: Exploration-exploitation trade-off is encoded in the singular value distribution of weight deltas.
- Evidence anchors:
  - [Section 3.1]: "Instruct model is optimized to maximize rewards by focusing on the most effective reasoning paths... at the cost of its ability to explore"
  - [Section 4.2, Figure 5]: Pass@k gap between Timber and Instruct baseline widens as k increases, demonstrating enhanced exploration
  - [corpus]: Beyond Exploration-Exploitation paper questions whether this trade-off is fundamental, suggesting hidden state analysis may reveal alternative perspectives

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD)**
  - Why needed here: Core operation for decomposing weight deltas into orthogonal components that can be selectively modified.
  - Quick check question: Given matrix W with singular values [10, 5, 2, 0.5, 0.1], what happens if you zero the last two values?

- Concept: **Effective Rank (eRank)**
  - Why needed here: Determines adaptive threshold for partitioning singular values; measures uniformity of singular value distribution via Shannon entropy.
  - Quick check question: For singular values [1, 1, 1, 1] vs [1, 0, 0, 0], which has higher eRank and why?

- Concept: **Exploration vs Exploitation in LLMs**
  - Why needed here: Motivates the problem—Instruct models excel at exploitation (Pass@1) but sacrifice exploration (Pass@k for large k).
  - Quick check question: If a model achieves 90% Pass@1 but only 92% Pass@10, what does this indicate about its exploration capacity?

## Architecture Onboarding

- Component map:
  Base weights (WB) + Instruct weights (WI) → Compute delta: WΔ = WI - WB → SVD decomposition: WΔ → UΣV^T → Calculate threshold: K = ⌈eRank(WΔ)⌉ → Refine singular values: Timber-L zeroes tail, Timber attenuates with λ → Reconstruct: W'I = WB + UΣ'V^T

- Critical path: The eRank calculation (Eq. 1) must correctly normalize singular values and compute Shannon entropy; errors here propagate to incorrect thresholds. SVD numerical stability matters for large weight matrices.

- Design tradeoffs:
  - Timber-L vs Timber: Timber-L (λ=0) removes tail entirely, simpler but may lose information; Timber (λ∈{0.2, 0.5, 0.8}) attenuates, preserving full rank. Paper finds Timber generally superior (6/7 cases).
  - λ selection: Paper searches on AIME'24; λ=0.2 identified as "sweet spot" for latest models. Search cost is minimal (training-free).
  - Module selection: Applying to both attention and FFN yields best results; FFN-only benefits math reasoning, attention-only benefits instruction following (Section 5.3).

- Failure signatures:
  - Simple linear scaling (model merge with uniform μ) degrades sharply below μ=0.95 (Section 5.2, Figure 6)
  - Truncated SVD baselines show high sensitivity to threshold selection (Table 3)
  - Performance degrades if λ too aggressive (Table 5: λ=0.8 underperforms λ=0.2 on most models)

- First 3 experiments:
  1. Replicate eRank analysis: Compute eRank for matching layers in paired Base/Instruct models (Llama-3.1-8B) to verify near-identical values before applying refinement.
  2. Ablation on single layer: Apply Timber to one FFN layer (e.g., layer 15) with λ∈{0.0, 0.2, 0.5} and evaluate on MATH-500 to isolate impact.
  3. Pass@k comparison: Generate n=256 samples for GPQA-Diamond, compute Pass@k for k∈{1, 10, 50, 100, 200} comparing vanilla Instruct vs Timber to verify exploration gains scale with k.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are there alternative refinement strategies for weight deltas beyond SVD-based tail attenuation that yield better exploration-exploitation trade-offs?
- **Basis in paper:** [explicit] The conclusion states, "We leave it for future work to explore more strategies to enhance the weight deltas."
- **Why unresolved:** The current method relies specifically on truncating or attenuating the tail of the singular value decomposition of the weight delta. It is unknown if other mathematical transformations of the delta (e.g., sparse selection, rotation) could revert the Instruct model more effectively.
- **What evidence would resolve it:** Experiments applying non-SVD refinement techniques to the weight delta, demonstrating superior Pass@k or Mean@k performance compared to the current eRank-threshold method.

### Open Question 2
- **Question:** Can the optimal attenuation factor $\lambda$ be determined theoretically or adaptively per layer, rather than via empirical search?
- **Basis in paper:** [inferred] Section 4.1 notes that the authors "search for the best attenuation factor in $\{0.2, 0.5, 0.8\}$" based on a validation set (AIME’24).
- **Why unresolved:** The current approach requires a grid search to find a "sweet point" (often $\lambda=0.2$). A static or globally searched $\lambda$ may not be optimal for every layer in the network, and the reliance on a validation set introduces a slight dependency on data.
- **What evidence would resolve it:** The derivation of a closed-form solution or an adaptive metric that sets $\lambda$ dynamically for each weight matrix without requiring validation performance feedback.

### Open Question 3
- **Question:** Does the effectiveness of Timber generalize to non-Transformer architectures, such as State Space Models (SSMs) or Mixture-of-Experts (MoE) with non-linear routing?
- **Basis in paper:** [inferred] The experiments (Section 4) are limited to Transformer-based Llama and Qwen series (including one MoE). The theoretical motivation relies on the properties of linear weight matrices common in Transformers.
- **Why unresolved:** While MoE was tested, the linear projections within SSMs (e.g., Mamba) or complex routing mechanisms might exhibit different eRank properties or delta distributions during post-training, potentially rendering the SVD-based refinement less effective.
- **What evidence would resolve it:** Application of Timber to modern SSM-based LLMs (e.g., Mamba, Jamba) showing consistent improvements in exploration capabilities similar to those observed in Llama and Qwen.

## Limitations
- Method relies on the assumption that post-training preserves effective rank between Base and Instruct models, which may not generalize to all model families or training regimes
- Performance sensitivity to λ parameter selection requires re-tuning for each new model family, though cost is minimal compared to training
- Refinement applied only to linear layers (excluding biases, layer norms, embeddings) may limit potential performance gains

## Confidence
- **High Confidence:** The empirical observation that eRank remains nearly unchanged between Base and Instruct models (Section 2.2). This is directly measurable and supported by concrete data across multiple model sizes and architectures.
- **Medium Confidence:** The claim that Timber consistently improves vanilla Instruct models on Pass@k performance. While results show improvement across most benchmarks, the magnitude varies significantly by model size and task type.
- **Medium Confidence:** The superiority of Timber over simple model merging and truncated SVD baselines. The paper demonstrates better robustness to threshold selection, but the comparison methodology (fixed λ vs. threshold search) may favor Timber's adaptive approach.

## Next Checks
1. **Cross-Architecture Generalization:** Apply Timber to a model family not included in the original experiments (e.g., Mistral or Phi series) and verify that eRank preservation holds and that refinement yields consistent exploration improvements.

2. **Ablation on Layer Types:** Systematically compare performance when applying Timber to: (a) all linear layers, (b) attention layers only, (c) FFN layers only, and (d) attention+FFN excluding projection layers. This would validate the paper's observation about FFN vs. attention benefits.

3. **Long-form Generation Analysis:** Evaluate Timber's impact on open-ended generation tasks (e.g., story continuation, code generation) beyond the multiple-choice benchmarks used in the paper. This would test whether exploration gains translate to creative task diversity.