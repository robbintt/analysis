---
ver: rpa2
title: 'EICopilot: Search and Explore Enterprise Information over Large-scale Knowledge
  Graphs with LLM-driven Agents'
arxiv_id: '2501.13746'
source_url: https://arxiv.org/abs/2501.13746
tags:
- query
- queries
- eicopilot
- graph
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EICopilot is an LLM-driven agent system for enterprise information
  search over large-scale knowledge graphs, automating Gremlin script generation and
  improving intent recognition through a novel query masking strategy. It addresses
  the challenge of efficiently querying complex enterprise data, which traditionally
  requires manual text-based queries and subgraph exploration.
---

# EICopilot: Search and Explore Enterprise Information over Large-scale Knowledge Graphs with LLM-driven Agents

## Quick Facts
- **arXiv ID:** 2501.13746
- **Source URL:** https://arxiv.org/abs/2501.13746
- **Reference count:** 19
- **Primary result:** LLM-driven agent system that automates Gremlin script generation for enterprise KG queries with 82.14% execution correctness using entity masking strategy

## Executive Summary
EICopilot addresses the challenge of efficiently querying complex enterprise knowledge graphs by automating the generation of Gremlin traversal scripts from natural language queries. The system employs a novel query masking strategy that replaces entity names with generic tokens during retrieval, ensuring structural intent rather than specific keywords drives example selection. By combining in-context learning with Chain-of-Thought reasoning, schema linking, and a reflection mechanism for error correction, EICopilot significantly improves both the accuracy and execution correctness of generated queries over traditional manual approaches.

## Method Summary
The method employs a multi-stage pipeline: (1) user queries undergo Named Entity Recognition and masking to replace specific entities with placeholders, (2) masked queries are embedded and retrieved from a vector database containing annotated (question, Gremlin) pairs, (3) the LLM generates Gremlin scripts using retrieved examples and schema linking to avoid context window limitations, and (4) a reflection module corrects syntax or runtime errors through iterative refinement. The system is trained via Supervised Fine-Tuning on 418 annotated query-script pairs and evaluated on 150 real-world test queries.

## Key Results
- Full entity masking strategy reduces syntax error rate from 52% to 10%
- Achieves execution correctness of up to 82.14% on real-world queries
- Outperforms baseline methods in both accuracy and execution reliability
- Demonstrates effective intent recognition through structural query matching rather than keyword similarity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Masking entity names during retrieval significantly improves relevance of in-context examples by forcing match on query intent rather than specific entity keywords
- **Mechanism:** System replaces recognized entities with generic tokens before embedding user's query, preventing vector database from returning examples that merely share company names when structural logic is different
- **Core assumption:** Primary source of error in retrieving useful examples is semantic noise from specific entity names
- **Evidence anchors:** Abstract states masking "improves intent recognition for heightened script accuracy"; Section 3.3 shows Full Mask minimizes syntax errors; Section 3.4 confirms Full entity masking allows better generalization
- **Break condition:** If NER module fails to identify entities to mask, or query relies on specific entity type for context

### Mechanism 2
- **Claim:** ICL combined with Chain-of-Thought reasoning enables LLM to map natural language to complex Gremlin syntax
- **Mechanism:** Retrieves top-k structurally similar query-script pairs and feeds them to LLM, which uses examples as guidance to construct specific graph navigation logic
- **Core assumption:** LLM possesses sufficient reasoning capability to analogize from provided examples to new query
- **Evidence anchors:** Abstract mentions ICL "enhance script generation accuracy"; Section 2.1.2 discusses integrating dynamic query pairs
- **Break condition:** If user's query requires logical operator not represented in vector database of examples

### Mechanism 3
- **Claim:** Schema linking and reflection loop mitigate context window limitations and execution errors
- **Mechanism:** Schema Linking identifies relevant tables/fields first instead of feeding entire schema; Reflection analyzes error messages and attempts correction if generation fails
- **Core assumption:** Error messages from graph database provide sufficient signal for LLM to self-correct without human intervention
- **Evidence anchors:** Section 2.2.3 describes Schema Linking to circumvent excessive schema input; Section 2.2.5 explains reflection mechanism activates after anomalies identified
- **Break condition:** If schema linking fails to retrieve correct schema subset, LLM will generate script using non-existent fields

## Foundational Learning

- **Concept: Apache TinkerPop & Gremlin Traversal**
  - **Why needed here:** Unlike SQL (tables), Gremlin traverses nodes and edges; understanding `out()`, `in()`, and `has()` needed to interpret generated scripts and debug traversal failures
  - **Quick check question:** If query asks for "shareholders of a company," should traversal start from company node and go `in` or `out` on investment edge?

- **Concept: Dense Retrieval / Vector Search**
  - **Why needed here:** System relies on retrieving "similar questions" to guide LLM; understanding vector search relies on semantic similarity explains why Masking is necessary
  - **Quick check question:** Why would vector search for "Who is CEO of Google?" match "Who is owner of Facebook?" better than "What is Google's stock price?"

- **Concept: Named Entity Recognition (NER)**
  - **Why needed here:** Masking mechanism depends entirely on identifying which words in sentence are "entities" (companies, people)
  - **Quick check question:** If NER model misses company name and leaves it unmasked, how might that affect retrieval of ICL examples?

## Architecture Onboarding

- **Component map:** Seed Data Constructor -> Vector Database (offline) -> User Query -> NER/Disambiguation -> Retrieval -> Schema Linking -> LLM Generator -> Reflection -> Execution (online)
- **Critical path:** Masking -> Retrieval link is most sensitive; if retrieval returns irrelevant examples (matching by company name instead of intent), CoT reasoning fails
- **Design tradeoffs:** Trades storage/cold-start complexity (requires manually annotated seed data) for inference accuracy (ICL instead of fine-tuning); trades latency (multi-step reflection) for reliability
- **Failure signatures:** High Syntax Error Rate suggests poor seed examples or NER accuracy issues; Empty Results likely Schema Linking failure querying wrong table/edge; Incorrect Entity Resolution indicates wrong entity disambiguation
- **First 3 experiments:**
  1. Ablation on Masking: Run evaluation set using "Raw Match" vs. "Full Mask" to verify syntax error reduction claim (52% -> ~12%)
  2. Schema Linking Stress Test: Input queries requiring multi-hop joins across distinct subgraphs to test if schema linker retrieves sufficient context
  3. Reflection Loop Limit: Force complex syntax error to see if Reflection module can fix in one pass or enters infinite retry loop

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is quantitative impact of Reflection module on final execution correctness rate?
- **Basis in paper:** Section 2.2.5 describes Reflection module to rectify erroneous query commands, but experimental results report aggregate performance without isolating specific error reduction attributable solely to this mechanism
- **Why unresolved:** Unclear if high execution correctness (82.14%) is driven primarily by initial script generation or post-hoc corrections
- **What evidence would resolve it:** Ablation study results comparing system's performance with and without Reflection module enabled

### Open Question 2
- **Question:** How does EICopilot performance scale with increasing query complexity?
- **Basis in paper:** Section 3.1.1 establishes complexity scoring system (Simple, Moderate, Complex) but Section 3.2 only presents aggregate success metrics across entire dataset
- **Why unresolved:** Paper does not demonstrate if "Full Mask" strategy maintains low syntax error rates (10%) for "Complex" queries (Score > 7) as effectively as simpler ones
- **What evidence would resolve it:** Breakdown of Syntax Error Rate and Execution Correctness grouped by complexity levels

### Open Question 3
- **Question:** What are inference latency costs of Top-5 ICL retrieval strategy in real-world deployment?
- **Basis in paper:** Abstract claims solution improves "speed" but Experimental Evaluation measures accuracy metrics only, omitting quantitative analysis of query latency or throughput
- **Why unresolved:** Retrieving and processing Top-5 representative queries via vector search and LLM reasoning adds computational overhead; trade-off between overhead and claimed speed improvement not quantified
- **What evidence would resolve it:** Benchmarks comparing end-to-end query latency of EICopilot pipeline against manual or zero-shot baselines

## Limitations

- Evaluation methodology lacks sufficient granularity in defining "execution correctness" metric and test set quality
- Ablation study only compares three masking variants without exploring alternative retrieval strategies or k parameter impact
- Computational overhead from reflection mechanism and schema linking not quantified in terms of latency or throughput
- Limited demonstration of generalization to novel graph schemas significantly different from training data

## Confidence

- **High confidence:** Entity masking mechanism to improve semantic retrieval relevance is empirically supported (syntax error reduction from 52% to 10%); basic architecture of RAG + CoT + schema linking is technically sound
- **Medium confidence:** Claimed 82.14% execution correctness is plausible given masking strategy's effectiveness, but metric's definition and test set quality remain opaque; reflection mechanism's error-correction capability is theoretically justified but lacks quantitative validation
- **Low confidence:** System's generalization to novel graph schemas is uncertain; paper doesn't demonstrate performance on structurally distinct enterprise KGs or address cold-start scenarios

## Next Checks

1. **Metric Transparency Audit:** Request precise definition of "execution correctness" - does it measure syntactic validity, semantic correctness, or both? Run blind evaluation where human experts judge whether generated scripts answer original queries correctly.

2. **Schema Generalization Test:** Evaluate system on at least two structurally distinct enterprise KGs (e.g., social network vs. supply chain relationships). Measure performance degradation to quantify schema adaptation limits.

3. **Reflection Mechanism Stress Test:** Design queries triggering specific error types (missing properties, incorrect edge directions, circular traversals) and measure: (a) success rate of automatic correction, (b) maximum reflection iterations before failure, and (c) latency overhead per reflection cycle.