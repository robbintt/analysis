---
ver: rpa2
title: 'OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning'
arxiv_id: '2506.00338'
source_url: https://arxiv.org/abs/2506.00338
tags:
- data
- owsm
- speech
- yodas
- owsm-ctc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of insufficient training data
  for fully open speech foundation models by incorporating a large-scale, web-crawled
  dataset called YODAS. The core method involves developing a scalable data-cleaning
  pipeline using public toolkits to address issues such as incorrect language labels
  and audio-text misalignments in YODAS.
---

# OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning

## Quick Facts
- arXiv ID: 2506.00338
- Source URL: https://arxiv.org/abs/2506.00338
- Reference count: 0
- New multilingual speech models match or surpass leading industrial models like Whisper and MMS through large-scale data cleaning and scaling

## Executive Summary
This paper addresses the challenge of insufficient training data for fully open speech foundation models by incorporating a large-scale, web-crawled dataset called YODAS. The core method involves developing a scalable data-cleaning pipeline using public toolkits to address issues such as incorrect language labels and audio-text misalignments in YODAS. The cleaned dataset comprises 166,000 hours of speech across 75 languages. The primary result is the development of OWSM v4, a new series of models trained on the curated dataset alongside existing OWSM data, which significantly outperforms previous versions on multilingual benchmarks. The OWSM v4 models match or surpass leading industrial models like Whisper and MMS in multiple scenarios, demonstrating the effectiveness of data scaling and cleaning.

## Method Summary
The authors developed a three-step data cleaning pipeline for the YODAS dataset: first, CTC segmentation realigns audio-text pairs and segments long utterances; second, language identification filtering removes examples where audio and text-based LID predictions disagree with the original label; third, CTC confidence score filtering removes low-quality pairs using a threshold of θCTC=0.10. The cleaned YODAS (166k hours) was combined with existing OWSM v3.2 data (154k hours) for a total of 320k hours. Models were trained using E-Branchformer encoders with Transformer decoders, with sizes ranging from base (100M) to medium (1B) parameters. The models support ASR, speech translation, and language identification through multitask training.

## Key Results
- OWSM v4 medium achieves 9.4% average WER on MLS benchmark, outperforming previous OWSM v3.1 (15.5%) and matching Whisper-medium accuracy
- 95.6% LID accuracy on FLEURS attributed to the cleaning pipeline's LID filtering stage
- CTC-only models are 20× faster than AED models while maintaining competitive accuracy
- The cleaned YODAS dataset (166k hours, 75 languages) will be publicly released alongside pre-trained models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CTC confidence scores serve as a proxy for audio-text alignment quality, enabling scalable filtering of misaligned training pairs.
- **Mechanism:** The CTC segmentation algorithm produces per-utterance confidence scores during forced alignment. Low scores indicate audio-text mismatches (e.g., wrong transcript, silence, music). Filtering low-scoring utterances removes noisy training signal.
- **Core assumption:** CTC scores from a pre-trained model generalize to detect misalignment in unseen data distributions.
- **Evidence anchors:**
  - [abstract]: "audio-text misalignments...address this, we develop a scalable data-cleaning pipeline"
  - [section 2.1.3]: "When θCTC = 0.00, no filtering is applied...performance on Common Voice is poor and unstable. The decoding often gets stuck in repetitions...WER exceeding 100%"
  - [corpus]: OWSM-CTC v3.2 paper (cited) uses similar CTC-based filtering but on smaller, higher-quality data
- **Break condition:** If the pre-trained CTC model has systematic biases (e.g., poor on certain accents), filtering may inadvertently remove valid data from underrepresented groups.

### Mechanism 2
- **Claim:** Triangulating language labels through independent audio and text LID models removes incorrectly labeled examples.
- **Mechanism:** Raw YODAS metadata language labels are noisy. By requiring consensus between (1) original label, (2) text-based LID (fastText), and (3) audio-based LID (SpeechBrain ECAPA-TDNN), only consistent examples survive.
- **Core assumption:** The LID models are sufficiently accurate that their disagreements signal annotation errors rather than LID model failures.
- **Evidence anchors:**
  - [section 2.1.2]: "We retain only those utterances for which the original language label matches both the predicted language from the text and the predicted language from the audio"
  - [table 5]: OWSM v4 medium achieves 95.6% LID accuracy on FLEURS, "attributed to the LID filtering stage"
  - [corpus]: Weak direct corpus evidence on LID filtering for ASR data cleaning; mostly studied for downstream LID tasks
- **Break condition:** Low-resource languages where LID models perform poorly will be over-filtered, exacerbating data imbalance.

### Mechanism 3
- **Claim:** Adding cleaned large-scale web data to curated academic data yields larger gains than filtering alone.
- **Mechanism:** OWSM v3.2 filtered existing data (reduced 15% but marginal gains). OWSM v4 adds 166k hours of cleaned YODAS to existing data (320k total). The combination provides both quantity and quality.
- **Core assumption:** The cleaning pipeline sufficiently reduces noise that adding more data helps rather than hurts.
- **Evidence anchors:**
  - [abstract]: "trained on this curated dataset alongside existing OWSM data (320k hours total), consistently outperform earlier versions"
  - [table 6]: OWSM v4 medium (9.4% avg WER) vs v3.1 medium (15.5%) on MLS benchmark
  - [corpus]: Whale paper confirms E-Branchformer + large data scaling pattern; consistent with scaling laws
- **Break condition:** If cleaned data still contains systematic biases (e.g., YouTube-specific speaking styles), domain shift may occur.

## Foundational Learning

- **Concept: Connectionist Temporal Classification (CTC)**
  - **Why needed here:** CTC scores drive the filtering pipeline; understanding what they measure is essential for tuning thresholds.
  - **Quick check question:** Can you explain why CTC probability indicates alignment quality between audio and transcript?

- **Concept: Encoder-Decoder vs. Encoder-Only (CTC) Architectures**
  - **Why needed here:** OWSM v4 releases both AED models (with Transformer decoder) and CTC-only models; they have different speed-accuracy tradeoffs.
  - **Quick check question:** Why would a CTC-only model be 20× faster than an AED model at inference?

- **Concept: Whisper-Style Multitask Training**
  - **Why needed here:** OWSM models jointly learn ASR, speech translation, and LID through special tokens; this differs from single-task ASR.
  - **Quick check question:** How does a single model handle both transcription and translation tasks during inference?

## Architecture Onboarding

- **Component map:** 128-dim Mel filterbanks (increased from 80) -> E-Branchformer encoder -> Transformer decoder (AED models) -> CTC output (CTC models)
- **Critical path:** Data cleaning pipeline (Resegmentation → LID filtering → CTC-score filtering) → Mixed data preparation (cleaned YODAS + OWSM v3.2 data = 320k hours) → Model training via ESPnet → Evaluation on FLEURS, MLS, Common Voice, CoVoST-2
- **Design tradeoffs:**
  - **θCTC threshold:** Higher = cleaner but smaller data (Table 2: θCTC=0.10 yields 166k hours, θCTC=0.30 yields 43k hours)
  - **Model size vs. speed:** AED medium (1.02B) matches Whisper-medium accuracy but CTC-only is 20× faster
  - **Language balance:** Authors kept original imbalanced distribution; resampling could help low-resource languages but wasn't explored
- **Failure signatures:**
  - **No filtering (θCTC=0.00):** Decoding gets stuck repeating tokens; WER > 100%
  - **Wrong θCTC per-language:** Some languages improve with stricter filtering, others degrade (Table 2 shows inconsistent trends)
  - **Missing LID filtering:** Would retain mislabeled language data, degrading multilingual performance
- **First 3 experiments:**
  1. **Reproduce cleaning pipeline on a small YODAS subset** (e.g., 1k hours in 5 languages) to validate CTC-score and LID filtering effects
  2. **Fine-tune a small OWSM v3.1 model** on cleaned vs. uncleaned data and compare Common Voice WER to verify Table 2 patterns
  3. **Ablate LID filtering** by training with/without it on a single language (e.g., Spanish) to isolate its contribution vs. CTC filtering alone

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would language-balanced resampling during training improve performance on low-resource languages compared to the retained imbalanced distribution?
- **Basis in paper:** [explicit] The authors note that English constitutes the largest share of data and state, "For simplicity, in this work, we keep the original distribution without any resampling."
- **Why unresolved:** The current models were trained on the natural (highly imbalanced) distribution to simplify the process, leaving the specific impact of balancing strategies on under-represented languages untested.
- **What evidence would resolve it:** Training an ablation model using up-sampled low-resource language data or down-sampled English data and comparing WERs on multilingual benchmarks like FLEURS.

### Open Question 2
- **Question:** Can per-language fine-tuning of the CTC score threshold (θCTC) yield superior results compared to the fixed global threshold?
- **Basis in paper:** [explicit] The authors observe that "performance trends vary across different test sets" regarding filtering thresholds but select a global θCTC=0.10 to retain data quantity.
- **Why unresolved:** A single threshold was chosen as a practical compromise, but the authors explicitly acknowledge that "finer-grained filtering could potentially optimize performance for individual languages."
- **What evidence would resolve it:** An ablation study comparing the global threshold model against models trained on datasets filtered using language-specific optimal thresholds derived from validation sets.

### Open Question 3
- **Question:** Does relying on OWSM-CTC v3.2 for resegmentation introduce alignment errors for the languages it does not support?
- **Basis in paper:** [inferred] The pipeline relies on OWSM-CTC v3.2 for resegmentation, but the text notes this model "supports only a subset of the languages present in YODAS."
- **Why unresolved:** While the pipeline filters misalignments later via CTC scores, the initial alignment quality for unsupported languages remains an unstated assumption that could affect the final dataset quality.
- **What evidence would resolve it:** Analyzing alignment confidence and subsequent ASR performance specifically for languages excluded from the segmentation model's training data versus those included.

## Limitations

- The data cleaning pipeline relies heavily on pre-trained LID and CTC models that may not generalize well to underrepresented languages
- Evaluation focuses primarily on benchmark datasets rather than real-world deployment scenarios, particularly for low-resource languages
- Computational requirements for training (700k steps on large datasets) create significant barriers to independent reproduction and community validation

## Confidence

**High Confidence**: The core methodology of data cleaning through CTC confidence filtering and LID triangulation is technically sound and well-documented. The claim that OWSM v4 outperforms previous OWSM versions on established benchmarks is strongly supported by quantitative evidence in Tables 4-6.

**Medium Confidence**: The claim that OWSM v4 matches or exceeds Whisper and MMS performance requires more context. While WER comparisons show competitive results, differences in training data, model architectures, and evaluation conditions make direct comparisons challenging. The scaling hypothesis (more cleaned data = better performance) is plausible but not conclusively proven.

**Low Confidence**: Claims about real-world deployment readiness and multilingual generalization are largely unsupported. The paper provides limited analysis of model behavior on truly low-resource languages, dialectal variations, or noisy real-world audio conditions.

## Next Checks

1. **Language-specific filtering analysis**: Reproduce the cleaning pipeline on a diverse subset of 5-10 languages spanning different resource levels. Systematically vary θCTC thresholds per language and measure the trade-off between data quantity and quality, particularly focusing on how filtering affects underrepresented languages versus high-resource ones.

2. **Domain transfer validation**: Test OWSM v4 models on non-benchmark, real-world audio datasets (e.g., spontaneous speech, telephone conversations, noisy environments) to assess generalization beyond curated evaluation sets. Compare performance degradation patterns against Whisper and MMS models.

3. **Low-resource language ablation study**: Isolate the contribution of the data cleaning pipeline for languages with <1000 hours of training data. Train separate models with: (a) uncleaned data only, (b) cleaned data only, and (c) combined data, then measure relative performance gains attributable to cleaning versus scaling.