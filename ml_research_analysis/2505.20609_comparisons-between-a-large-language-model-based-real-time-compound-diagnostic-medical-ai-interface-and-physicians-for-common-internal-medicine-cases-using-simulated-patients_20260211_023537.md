---
ver: rpa2
title: Comparisons between a Large Language Model-based Real-Time Compound Diagnostic
  Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated
  Patients
arxiv_id: '2505.20609'
source_url: https://arxiv.org/abs/2505.20609
tags:
- interface
- medical
- physicians
- diagnostic
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed and evaluated a large language model-based
  real-time compound diagnostic medical AI interface using simulated patient encounters.
  Ten clinical vignettes based on USMLE Step 2 CS exams were used in a non-randomized
  trial with three physicians and five simulated patients.
---

# Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients

## Quick Facts
- arXiv ID: 2505.20609
- Source URL: https://arxiv.org/abs/2505.20609
- Reference count: 0
- Primary result: AI interface achieved 80% accuracy for first differential diagnosis vs 50-70% for physicians

## Executive Summary
This study evaluated a multi-component LLM-based diagnostic system against physicians using simulated patient encounters. The compound AI interface, which combines structured symptom surveys with conversational history-taking, achieved 80% accuracy for first differential diagnosis and 100% accuracy when considering first and second differentials combined. The system required 44.6% less time (557 vs 1006 seconds) and reduced costs by 98.1% ($0.08 vs $4.2 per consultation) while maintaining comparable patient satisfaction scores. The results suggest the AI interface could assist primary care consultations for common internal medicine cases, particularly in underserved settings, by providing accurate diagnoses with lower time and cost requirements.

## Method Summary
The study used a non-randomized trial design with three physicians and five simulated patients conducting 10 clinical vignettes adapted from USMLE Step 2 CS exams. The AI interface employed a compound architecture with four LLM components: Basic History LLM (10-12 initial questions), routing logic to either Detailed Symptom Survey (~100 templates) or Focused History LLM, optional Supplemental History LLM (10-12 clarifying questions), and Diagnosis LLM (generates 3 ranked differentials). Both AI and physician groups received identical clinical vignettes and were evaluated on diagnostic accuracy, consultation time, cost, and patient satisfaction across 13 Likert-scale questions.

## Key Results
- AI interface achieved 80% accuracy for first differential diagnosis versus 50-70% for physicians
- First and second differential diagnoses combined reached 100% accuracy for AI interface
- AI required 44.6% less time (557 vs 1006 seconds) and reduced costs by 98.1% ($0.08 vs $4.2)
- Patient satisfaction scores were comparable (AI: 3.9, Physicians: 4.2-4.3)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A compound architecture combining structured symptom surveys with conversational LLM history-taking may improve diagnostic accuracy over single-mode approaches.
- Mechanism: The system routes patients through either (a) predefined symptom-specific questionnaires (~100 templates, 10-15 MCQs each) when chief complaints match known patterns, or (b) interactive Focused History LLM questioning when symptoms fall outside templates. This hybrid approach captures both structured, exhaustive symptom data and nuanced conversational information.
- Core assumption: Structured questionnaires capture critical diagnostic features that open-ended conversation might miss, while conversational flexibility handles atypical presentations.
- Evidence anchors: [abstract] "real-time compound diagnostic medical AI interface achieved an accuracy of 80%" with "accuracy rate of 100%" for first and second differential diagnoses; [section] "This integrated approach enables the Diagnosis LLM to leverage both structured survey responses and conversational insights to generate three differential diagnoses"; [corpus] Neighbor paper "DiagLink" similarly proposes synergizing LLMs with knowledge graphs for diagnostic assistance, suggesting multi-component architectures are an emerging pattern.
- Break condition: If symptom presentations are highly atypical or rare, the routing logic may fail to match appropriate survey templates, degrading to less-optimized fallback pathways.

### Mechanism 2
- Claim: Sequential information gathering with explicit diagnostic synthesis may reduce oversight errors common in time-pressed human consultations.
- Mechanism: The Basic History LLM standardizes initial data collection (10-12 core questions) before any diagnostic reasoning begins. This creates a consistent information baseline regardless of patient communication style, which the Diagnosis LLM then synthesizes.
- Core assumption: Systematic, complete initial history-taking improves downstream diagnostic reasoning compared to variable human approaches.
- Evidence anchors: [abstract] AI required "44.6% less time (557 vs 1006 seconds)" while achieving comparable or higher accuracy; [section] "By thoughtfully combining these inputs, the Diagnosis LLM ensures that its diagnostic reasoning considers diverse patient information"; [corpus] Weak corpus evidence for this specific mechanism; no direct comparison of sequential vs. single-stage architectures found.
- Break condition: If initial questions fail to capture relevant context (e.g., patient misreports, language barriers), downstream synthesis operates on incomplete data.

### Mechanism 3
- Claim: Multi-differential output with ranked likelihood may capture diagnostic uncertainty more appropriately than single-diagnosis outputs.
- Mechanism: The system generates three ranked differential diagnoses rather than a single answer, mirroring clinical practice. First differential accuracy was 80%, but first-and-second combined reached 100%, suggesting the correct diagnosis was often in the system's consideration set.
- Core assumption: Presenting multiple ranked possibilities is clinically more useful than single predictions, as it preserves diagnostic alternatives for further workup.
- Evidence anchors: [abstract] "100% accuracy for first and second differential diagnoses" vs "80%" for first alone; [section] Reference standard was "three most likely differential diagnoses" determined by two internal medicine physicians by consensus; [corpus] "Sequential Diagnosis with Language Models" (corpus neighbor) similarly evaluates multi-step diagnostic reasoning, suggesting ranked differential output is an emerging evaluation standard.
- Break condition: If clinicians ignore lower-ranked differentials, or if the ranking logic is poorly calibrated, the value of multiple outputs diminishes.

## Foundational Learning

- Concept: **USMLE Step 2 CS-style clinical vignettes**
  - Why needed here: The study's evaluation framework is built on this standardized patient encounter format. Understanding that these vignettes include "chief complaint, vital signs, detailed history of present illness, medical history, and physical examination" is essential for interpreting the 10-case evaluation.
  - Quick check question: What five components comprise a USMLE Step 2 CS-style clinical vignette per this study?

- Concept: **Proportion of agreement (repeatability metric)**
  - Why needed here: The paper reports 0.7 proportion of agreement for first differential diagnosis across repeated sessions, but acknowledges stochasticity in LLM outputs is difficult to evaluate precisely. Understanding this limitation is critical for interpreting reliability claims.
  - Quick check question: Why did the authors use proportion of agreement rather than standard stochasticity metrics for LLM evaluation?

- Concept: **Compound AI systems (multi-component architectures)**
  - Why needed here: This is not a single LLM but a pipeline of specialized components (Basic History LLM, Focused History LLM, Supplemental History LLM, Diagnosis LLM) with conditional routing. Engineers must understand that performance emerges from component interaction, not any single model.
  - Quick check question: Name the four LLM components in the system and describe when each is activated.

## Architecture Onboarding

- Component map: Patient Input → Basic History LLM (10-12 questions) → Routing Decision → (Detailed Symptom Survey OR Focused History LLM) → [Optional] Supplemental History LLM → Diagnosis LLM → 3 Ranked Differentials

- Critical path: Basic History LLM → routing logic → (Survey OR Focused History) → Diagnosis LLM. The routing decision is the key branching point; errors here cascade to inappropriate data collection pathways.

- Design tradeoffs:
  - **Structured vs. flexible**: Surveys ensure completeness but may feel rigid; conversational mode is natural but may miss specific diagnostic criteria
  - **Speed vs. thoroughness**: 9-minute average AI time suggests aggressive efficiency; unclear whether edge cases receive adequate attention
  - **Cost optimization**: $0.08/case suggests aggressive token management; may limit reasoning depth

- Failure signatures:
  - Routing logic mismatches (patient symptom doesn't fit any of ~100 templates but gets forced into one)
  - Stochasticity in differential ranking (proportion of agreement = 0.7 indicates 30% inconsistency across sessions)
  - Low patient satisfaction on "adequately addressed questions" (3.4/5) and "treatment options explained" (3.5/5) suggests diagnostic explanation is a weak point

- First 3 experiments:
  1. **Routing accuracy audit**: Present 50 diverse chief complaints and measure whether routing logic correctly matches to symptom-specific surveys vs. appropriately escalating to Focused History LLM
  2. **Stochasticity characterization**: Run identical clinical vignettes 10 times each; measure variance in differential rankings and identify case types with highest inconsistency
  3. **Explanation quality assessment**: Compare AI vs. physician responses on the two lowest-scoring satisfaction items (question addressing, treatment options) using blinded simulated patient ratings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will the diagnostic accuracy and efficiency of the AI interface hold in larger trials with more diverse cases?
- Basis in paper: [explicit] The authors explicitly note that "the number of clinical vignettes was small, and only a limited number of physicians and simulated patients participated" as a limitation.
- Why unresolved: The observed difference in primary outcome accuracy (80% vs 50-70%) was not statistically significant (P=0.17), leaving the superiority of the AI inconclusive due to low statistical power.
- What evidence would resolve it: A powered, randomized controlled trial with a significantly larger sample of vignettes and physician participants.

### Open Question 2
- Question: How can the stochasticity and reproducibility of the LLM be rigorously evaluated given the open-ended nature of clinical dialogue?
- Basis in paper: [explicit] The authors state it was "difficult to precisely evaluate stochasticity" using open-ended questions and instead relied on the proportion of agreement.
- Why unresolved: Without rigorous stochasticity reporting, the consistency of the AI's diagnostic reasoning across multiple identical sessions remains unverified, posing potential safety risks.
- What evidence would resolve it: Implementation of automated semantic similarity metrics or repeated session testing to quantify variance in diagnostic outputs.

### Open Question 3
- Question: Do patient satisfaction and diagnostic accuracy persist when the interface is deployed with actual patients rather than trained, simulated patients?
- Basis in paper: [inferred] The study relied on simulated patients who were experienced actors, while the conclusion posits utility for real-world primary care.
- Why unresolved: Real patients may provide disorganized histories or exhibit complex emotional behaviors that scripted actors did not, potentially affecting the AI's information gathering and satisfaction scores.
- What evidence would resolve it: A prospective cohort study or pilot program deploying the interface in a live clinical setting.

## Limitations

- Small sample size (10 clinical vignettes, 3 physicians, 5 simulated patients) limits generalizability and statistical power
- All cases derived from USMLE Step 2 CS exams may not reflect real-world diagnostic complexity or diversity
- Simulated patients may not capture the full variability of real patient communication and behavior

## Confidence

- **High Confidence**: Time (557 vs 1006 seconds) and cost ($0.08 vs $4.2) measurements are based on direct measurement
- **Medium Confidence**: Patient satisfaction scores (3.9 for AI vs 4.2-4.3 for physicians) are based on self-reported ratings
- **Low Confidence**: Mechanism claims about why the compound architecture works better are largely theoretical with weak corpus evidence

## Next Checks

1. **External validation cohort**: Test the system on 50+ diverse clinical vignettes from multiple sources (not just USMLE) with blinded physician validation to assess generalizability beyond the original study cases

2. **Real-world deployment trial**: Conduct a pilot study with actual patients in a primary care setting, measuring not just diagnostic accuracy but also clinical outcomes, follow-up rates, and provider workflow integration

3. **Longitudinal cost analysis**: Evaluate the total cost of ownership including model hosting, maintenance, regulatory compliance, and potential liability insurance over a 2-year deployment period, comparing this to the initial $0.08 per consultation estimate