---
ver: rpa2
title: 'IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable
  Video Generation'
arxiv_id: '2506.03150'
source_url: https://arxiv.org/abs/2506.03150
tags:
- video
- lighting
- illumination
- diffusion
- light
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of video relighting, which involves
  maintaining consistent illumination over time and ensuring physically plausible
  light-scene interactions in dynamic videos. The authors propose IllumiCraft, a unified
  diffusion framework that integrates illumination and geometry guidance for controllable
  video generation.
---

# IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation

## Quick Facts
- **arXiv ID**: 2506.03150
- **Source URL**: https://arxiv.org/abs/2506.03150
- **Reference count**: 40
- **Primary result**: Achieves 43% reduction in FVD (from 3946.71 to 2186.40) and improves visual quality, text alignment, and temporal consistency in video relighting tasks.

## Executive Summary
IllumiCraft addresses the challenge of video relighting by proposing a unified diffusion framework that integrates illumination and geometry guidance for controllable video generation. The method accepts HDR video maps for lighting control, synthetically relit frames for appearance cues, and 3D point tracks for geometry information. It demonstrates superior performance compared to state-of-the-art approaches, achieving significant improvements in video quality metrics and temporal consistency for both text-conditioned and background-conditioned video relighting tasks.

## Method Summary
IllumiCraft is a unified diffusion framework for controllable video generation that addresses the challenge of video relighting. It integrates illumination and geometry guidance by accepting HDR video maps for lighting control, synthetically relit frames for appearance cues, and 3D point tracks for geometry information. The method employs a unified diffusion model that combines these multimodal inputs to generate relit videos with improved visual quality, text alignment, and temporal consistency. The framework demonstrates superior performance compared to state-of-the-art approaches, achieving significant reductions in FVD and improvements in various quality metrics for both text-conditioned and background-conditioned video relighting tasks.

## Key Results
- Achieves 43% reduction in FVD (from 3946.71 to 2186.40) compared to state-of-the-art approaches
- Improves visual quality, text alignment, and temporal consistency in both text-conditioned and background-conditioned video relighting tasks
- Demonstrates superior performance in controlled experiments and synthetic datasets

## Why This Works (Mechanism)
IllumiCraft works by integrating illumination and geometry guidance within a unified diffusion framework. The method leverages HDR video maps for precise lighting control, synthetically relit frames for appearance cues, and 3D point tracks for geometry information. By combining these multimodal inputs, the framework can generate relit videos that maintain consistent illumination over time and ensure physically plausible light-scene interactions in dynamic videos. The unified diffusion model effectively learns to synthesize videos that align with the provided illumination and geometry guidance, resulting in improved visual quality, text alignment, and temporal consistency compared to traditional approaches.

## Foundational Learning
1. **NeRF (Neural Radiance Fields)**: Why needed - For 3D reconstruction and point track extraction from video sequences. Quick check - Ensure high-quality NeRF reconstruction before point track extraction.
2. **HDR (High Dynamic Range) Imaging**: Why needed - To capture and control lighting information in video relighting tasks. Quick check - Verify HDR video maps accurately represent lighting conditions.
3. **Diffusion Models**: Why needed - For generating high-quality, controllable video content based on multimodal inputs. Quick check - Validate diffusion model's ability to synthesize realistic videos.
4. **Temporal Consistency**: Why needed - To ensure smooth transitions and coherent illumination across video frames. Quick check - Measure FVD and other temporal consistency metrics.
5. **Physically-based Rendering**: Why needed - To ensure realistic light-scene interactions in generated videos. Quick check - Compare relit videos with ground truth for physical plausibility.

## Architecture Onboarding

### Component Map
Video Frames -> NeRF Reconstruction -> 3D Point Tracks
HDR Video Maps -> Illumination Guidance
Synthetically Relit Frames -> Appearance Guidance
All Inputs -> Unified Diffusion Model -> Relit Video Output

### Critical Path
1. Video frames undergo NeRF reconstruction to extract 3D point tracks
2. HDR video maps provide illumination guidance
3. Synthetically relit frames offer appearance cues
4. Unified diffusion model integrates all inputs to generate relit video

### Design Tradeoffs
- High-quality NeRF reconstruction enables accurate geometry extraction but increases computational cost
- Multimodal input integration improves controllability but adds complexity to the diffusion model
- Synthetic relit frames provide strong appearance guidance but may limit generalization to real-world scenarios

### Failure Signatures
- Poor NeRF reconstruction leading to inaccurate 3D point tracks
- Inconsistent illumination between HDR guidance and generated video
- Temporal inconsistencies or flickering in relit video output

### First Experiments
1. Validate NeRF reconstruction quality and point track extraction accuracy
2. Test unified diffusion model with individual guidance inputs (illumination only, geometry only)
3. Evaluate temporal consistency and visual quality on synthetic datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Dependency on high-quality NeRF reconstruction for point track extraction, which can be computationally intensive
- Limited evaluation on diverse real-world lighting conditions and complex dynamic scenes
- Evaluation metrics do not fully capture perceptual quality or robustness to extreme lighting variations

## Confidence
- **High**: Technical innovation and improvement over baselines in controlled experiments
- **Medium**: Claims about generalization to diverse real-world scenarios due to limited dataset diversity
- **Low**: Assertions about real-time applicability or efficiency, as these aspects are not thoroughly addressed

## Next Checks
1. Evaluate IllumiCraft on a more diverse set of real-world videos with varying lighting conditions and dynamic object interactions
2. Benchmark the computational cost and scalability of the method for longer videos and higher resolutions
3. Conduct user studies to assess perceptual quality and realism of the relit videos compared to ground truth and other baselines