---
ver: rpa2
title: How Well Do LLMs Imitate Human Writing Style?
arxiv_id: '2509.24930'
source_url: https://arxiv.org/abs/2509.24930
tags:
- style
- authorship
- human
- prompting
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free authorship verification framework
  that combines TF-IDF character n-grams with transformer embeddings to measure how
  closely LLM-generated text matches a human author's style. The method uses empirical
  distance distributions instead of supervised training, enabling efficient and scalable
  experimentation.
---

# How Well Do LLMs Imitate Human Writing Style?

## Quick Facts
- **arXiv ID:** 2509.24930
- **Source URL:** https://arxiv.org/abs/2509.24930
- **Authors:** Rebira Jemama; Rajesh Kumar
- **Reference count:** 40
- **Primary result:** 97.5% authorship verification accuracy on academic essays; LLMs achieve near-perfect style matching while remaining statistically detectable via perplexity

## Executive Summary
This paper introduces a training-free authorship verification framework that measures how closely LLM-generated text matches a human author's style. By combining TF-IDF character n-grams with transformer embeddings and comparing against empirical distance distributions, the method achieves 97.5% accuracy on academic essays without any supervised training. The framework evaluates five LLMs across four prompting strategies, finding that prompting strategy has greater influence on style fidelity than model size. Notably, even high-fidelity imitations remain statistically detectable as machine-generated, with human essays averaging perplexity 29.5 versus 15.2 for LLM outputs.

## Method Summary
The framework builds on a training-free authorship verification approach that uses empirical distance distributions rather than learned parameters. It extracts two complementary feature sets from text pairs: TF-IDF weighted character n-grams (3-5 characters, 10k dimensions) and contextual embeddings from all-MiniLM-L6-v2 (384-dim). These features are compared using cosine distance, and the resulting distances are evaluated against pre-computed histograms of same-author (D⁺) and different-author (D⁻) pairs. Classification occurs by comparing the probability that a test distance is more extreme than each distribution. The method is evaluated on IvyPanda academic essays and EssayForum conversational essays, with LLMs generating imitations under four prompting strategies for comparison.

## Key Results
- Achieves 97.5% accuracy on same-domain authorship verification and 94.5% in cross-domain evaluation
- Reduces training time by 91.8% and memory usage by 59% compared to parameter-based baselines
- Few-shot and completion prompting achieve near-perfect style matching (99.9% accuracy) versus <7% for zero-shot
- Human essays average perplexity 29.5 versus 15.2 for LLM outputs, enabling statistical detection despite high stylistic fidelity

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Based Verification Without Parameter Learning
Classification accuracy can rival supervised baselines without any gradient updates or threshold tuning by comparing test distances against empirical reference distributions. The verifier builds two histograms—one for same-author pairwise distances (D⁺), one for different-author distances (D⁻). At inference, a test pair's distance d* is compared to both distributions via S = Pr[δ > d* | δ ~ D⁺] and D = Pr[δ < d* | δ ~ D⁻]. If S > D, classify as same-author; confidence = |S − D| / max(S, D). The core assumption is that the distance metric (cosine) separates same- and different-author pairs with limited overlap; empirical distributions faithfully represent population statistics.

### Mechanism 2: Complementary Feature Fusion (TF–IDF + Transformer Embeddings)
Combining shallow character n-gram frequencies with dense contextual embeddings captures both explicit stylistic habits and implicit syntactic patterns. TF–IDF over 3–5 character n-grams (10k dimensions) encodes punctuation patterns and orthographic habits. Parallel transformer encoder (all-MiniLM-L6-v2, 384-dim) produces contextual embeddings via mean pooling. Both are compared using cosine distance. The core assumption is that style manifests at both surface (character-level) and semantic (contextual) levels; neither alone is sufficient.

### Mechanism 3: Style Fidelity–Detectability Separation via Perplexity
LLMs can achieve near-perfect stylistic matching while remaining statistically distinguishable through perplexity analysis. Perplexity measures predictability under a language model (GPT-2 here). Human essays average 29.5; LLM outputs average 15.2. Even at 99.9% style agreement, lower perplexity reveals algorithmic regularity. The core assumption is that perplexity distributions for human vs. LLM text remain separable; GPT-2 perplexity generalizes to modern LLM outputs.

## Foundational Learning

- **Concept: Character n-grams and TF–IDF weighting**
  - Why needed here: The framework uses 3–5 character sequences to capture punctuation habits and orthographic patterns. TF–IDF downweights common n-grams.
  - Quick check question: Given documents ["testing"] and ["tested"], what character 3-grams appear in both?

- **Concept: Non-parametric classification via empirical distributions**
  - Why needed here: The verifier avoids k-NN's local voting or threshold tuning by storing full distance histograms and comparing probabilistically.
  - Quick check question: If D⁺ = [0.1, 0.2, 0.3] and D⁻ = [0.5, 0.6, 0.7], what classification and confidence for d* = 0.25?

- **Concept: Perplexity as language model surprisal**
  - Why needed here: The paper uses perplexity to separate stylistic fidelity from detectability. Lower perplexity = more predictable = more likely AI-generated.
  - Quick check question: Why might a human essay have higher perplexity than an AI essay on the same topic?

## Architecture Onboarding

- **Component map:** Preprocessing (text cleaning, 500-word segmentation, non-adjacent block pairing) -> Feature extraction (parallel TF–IDF vectorizer + MiniLM encoder) -> Distribution construction (store D⁺ and D⁻ histograms) -> Inference (compute d*, compare to distributions, output label + confidence) -> LLM evaluation (generate imitations under 4 prompting conditions, feed through verifier) -> Detectability module (GPT-2 perplexity scoring)

- **Critical path:** Corpus cleaning → pair construction → embedding materialization → distribution building (~5 seconds, one-time) → Inference: single distance computation + two probability lookups (no gradients)

- **Design tradeoffs:** Cosine vs. Euclidean: Cosine chosen for length-robustness (89.9% vs. 81.4% at 200–1000 words); Fixed vs. adaptive thresholds: Distribution comparison handles varying difficulty without tuning; Single vs. dual features: Handcrafted 16-feature Siamese network achieved only 57% vs. combined approach

- **Failure signatures:** Cross-domain FN spike: 27% vs. 2.5% in-domain (conversational drifts from academic baseline); Zero-shot imitation failure: <7% for all models (statistical style summaries insufficient); Short text collapse: 63.4% accuracy for <50 words

- **First 3 experiments:** Replicate same-domain verification: train on 60% IvyPanda construction pairs, evaluate on 40% held-out, verify 97.5% accuracy and 0.997 AUC; Ablate feature sources: run with (a) TF–IDF only, (b) embeddings only, (c) combined; compare ROC curves and McNemar χ² significance; Cross-domain stress test: train on IvyPanda academic, evaluate on EssayForum conversational; inspect confusion matrix asymmetry (expect FN-heavy at ~27%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed separability between stylistic fidelity and statistical detectability hold across creative writing, technical documentation, and non-English languages?
- Basis in paper: Section VI states the work was evaluated on "English academic and conversational essays, leaving open questions about generality across creative, technical, and multilingual domains."
- Why unresolved: The current results rely on specific corpora (IvyPanda and EssayForum) which may not represent the stylistic nuances of fiction, technical reports, or morphologically rich languages.
- What evidence would resolve it: Replicating the experiment using the same TF-IDF/embedding verifier on multilingual datasets and genre-diverse corpora (e.g., creative writing repositories).

### Open Question 2
- Question: To what extent does the training-free verifier fail when confronted with adversarial tactics such as automated paraphrasing or prompt manipulation designed to evade stylometric detection?
- Basis in paper: Section VI notes the verifier "may be challenged by adversarial tactics such as paraphrasing or prompt manipulation."
- Why unresolved: The study evaluated standard prompting strategies (zero-shot to completion) but did not test the framework's robustness against active evasion techniques.
- What evidence would resolve it: Benchmarking the verifier's accuracy (currently ~97.5%) against a dataset of LLM outputs specifically optimized via paraphrasing tools or adversarial prompts to fool stylometric analysis.

### Open Question 3
- Question: Do advanced detection metrics or neural watermarking confirm the low perplexity signal, or can they identify high-fidelity imitations that perplexity metrics miss?
- Basis in paper: Section VI argues that "perplexity-based detection, here anchored on GPT-2, offers only a partial view; newer detectors... may reveal different boundaries."
- Why unresolved: The paper establishes detectability based solely on GPT-2 perplexity, leaving the performance of modern, state-of-the-art detection architectures unverified.
- What evidence would resolve it: Re-evaluating the "high-fidelity" LLM outputs using contemporary detectors (e.g., DetectGPT variants, RoBERTa-based classifiers) to see if the separation from human text persists.

### Open Question 4
- Question: Can proactive safeguards like identity-conditioned generation successfully constrain style imitation without compromising the fluency or coherence of the generated text?
- Basis in paper: Section VI calls for future work to "explore proactive safeguards such as watermarking or identity-conditioned generation."
- Why unresolved: The current framework is retrospective (detecting imitation after generation) rather than preventive (stopping imitation during generation).
- What evidence would resolve it: Developing and testing an identity-conditioned generation protocol to verify if it can lower style-matching accuracy (from the current 99.9% in completion tasks) while maintaining text quality.

## Limitations

- Cross-domain performance degrades substantially (27% false negative rate) compared to same-domain accuracy (97.5%)
- The "statistical profile" zero-shot prompt format is underspecified, making exact replication difficult
- Choice of GPT-2 for perplexity scoring may not reflect modern LLM token distributions

## Confidence

**High confidence:** Same-domain authorship verification accuracy, distribution-based classification mechanism, feature complementarity rationale, and perplexity detectability separation.

**Medium confidence:** Cross-domain performance claims and short-text accuracy figures, as these depend heavily on preprocessing choices and corpus specifics.

**Low confidence:** Zero-shot imitation accuracy (<7%) and exact prompt formatting, due to minimal prompt templates provided.

## Next Checks

1. **Cross-domain replication:** Train the verifier on IvyPanda academic essays and evaluate on EssayForum conversational essays, verifying the 27% false negative rate and inspecting the confusion matrix asymmetry between FN and FP errors.

2. **Prompt strategy ablation:** Implement and test the four prompting strategies (zero-shot, one-shot, few-shot, completion) with exactly the reported LLMs, measuring style fidelity and confirming that few-shot/completion achieve >99% accuracy while zero-shot remains <7%.

3. **Perplexity validation:** Recompute GPT-2 perplexities for human vs. LLM essays using the exact model/variant and tokenization described, verifying the 29.5 vs. 15.2 gap and testing whether modern LLM outputs still fall below human thresholds.