---
ver: rpa2
title: Sequential Enumeration in Large Language Models
arxiv_id: '2512.04727'
source_url: https://arxiv.org/abs/2512.04727
tags:
- counting
- storm
- number
- enumeration
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically evaluates sequential enumeration in five
  state-of-the-art LLMs, testing naming (how-many) and production (give-N) tasks with
  letters and words under four prompting conditions. Models were probed with explicit,
  spontaneous, mental, and forbid counting instructions.
---

# Sequential Enumeration in Large Language Models

## Quick Facts
- arXiv ID: 2512.04727
- Source URL: https://arxiv.org/abs/2512.04727
- Authors: Kuinan Hou; Marco Zorzi; Alberto Testolin
- Reference count: 40
- Primary result: LLMs fail to spontaneously engage in systematic counting, relying instead on surface-level token prediction strategies even for explicit counting tasks

## Executive Summary
This study systematically evaluates sequential enumeration in five state-of-the-art LLMs using naming (how-many) and production (give-N) tasks with letters and words under four prompting conditions. Models were tested with explicit, spontaneous, mental, and forbid counting instructions. Results show that while top-tier models achieve high accuracy in production tasks under explicit counting, performance drops sharply in naming tasks and spontaneous conditions. No model spontaneously engaged in systematic counting, though some provided near-target estimates. Larger Llama models showed gradual improvement in accuracy with size, suggesting counting is not a sharp emergent ability. Analysis of hidden state dynamics revealed structured, low-dimensional trajectories resembling internal counters only in the mental counting condition, whereas explicit counting relied on surface-level token prediction.

## Method Summary
The study tested five LLMs (GPT-5, GPT-4.1, Gemini 2.5 Pro, Llama 3B/8B/70B, Qwen 32B) on naming and production enumeration tasks using letters and 5-letter words with verified one-token-per-word correspondence. Four prompting conditions were evaluated: explicit counting (generate markers/symbols), spontaneous counting, mental counting (counting "in your mind"), and forbid counting (explicitly prohibited). Responses were parsed using <my_answer> tags with up to 10 retries for invalid responses. Hidden states were extracted from Llama-70B's final transformer layer (8192 neurons) and analyzed using PCA, with neuron-step correlation analysis computing Pearson coefficients between activations and step indices.

## Key Results
- Production tasks achieved near-ceiling accuracy under explicit counting, but naming tasks remained systematically worse
- No model spontaneously engaged in systematic counting without explicit instructions
- Mental counting induced structured low-dimensional neural trajectories with Spearman correlations >0.83 between principal components and generation steps
- Larger Llama models showed gradual rather than emergent improvement in enumeration accuracy
- Explicit counting relied on surface-level token prediction strategies rather than internal accumulation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit counting instructions trigger surface-level token prediction strategies that achieve high production accuracy without developing true internal counters.
- **Mechanism:** When models receive explicit counting instructions, they leverage associative chaining—predicting the next symbolic number based on the previously generated symbol rather than maintaining an accumulating quantity representation.
- **Evidence anchors:** Explicit counting shows no structured neural trajectories despite high accuracy; PC1 shows periodic spikes at decade numbers rather than smooth accumulation.

### Mechanism 2
- **Claim:** Mental counting instructions induce structured low-dimensional neural trajectories resembling internal accumulator circuits.
- **Mechanism:** The instruction to count covertly forces the model to maintain state information in hidden representations rather than externalizing via tokens, producing smooth, continuous trajectories.
- **Evidence anchors:** PC1 and PC2 show strong correlation with generated steps (Spearman >0.83, p<0.001) in mental counting; produces distinctive horseshoe pattern with smooth progression.

### Mechanism 3
- **Claim:** LLMs retain approximate enumeration abilities that produce near-target estimates even when counting is explicitly forbidden.
- **Mechanism:** When counting is prohibited, models rely on approximate mechanisms—likely using heuristic cues like sequence length or token patterns learned from training data.
- **Evidence anchors:** GPT and Gemini show relatively low MAE even in forbid-counting conditions, suggesting residual estimation capacity.

## Foundational Learning

- **Serial Accumulation Mechanisms:** Why needed here: The paper interprets PCA trajectories through the lens of accumulation-based counting circuits from cognitive neuroscience.
  - Quick check: Can you explain why a smooth, monotonic trajectory in PC space would indicate accumulation whereas periodic spikes would not?

- **Principal Component Analysis (PCA) on Neural Activations:** Why needed here: The paper's central mechanistic claim relies on interpreting PCA trajectories of hidden states.
  - Quick check: If PC1 explains 47% of variance in mental counting but only 17% in explicit counting, what does this suggest about the dimensionality of the underlying representations?

- **Token Prediction vs. Systematic Computation:** Why needed here: The paper argues LLMs succeed at counting via surface-level token prediction rather than learning systematic procedures.
  - Quick check: Why would decade-number spikes in PC trajectories suggest token-level prediction rather than true accumulation?

## Architecture Onboarding

- **Component map:** Prompt engineering system with four conditions (explicit, spontaneous, mental, forbid) -> Response parsing pipeline using <my_answer> tags with retry logic -> Tokenization verification layer -> Hidden state extraction from final transformer layer -> PCA analysis pipeline -> Neuron correlation analysis
- **Critical path:** Verify tokenization matches counting units; enforce response formatting early; apply task-specific token limits; extract hidden states during generation
- **Design tradeoffs:** Single repeated token for neural analysis eliminates tokenization confounds but limits stimulus diversity; fitting PCA separately per condition preserves structure but prevents cross-condition comparison
- **Failure signatures:** Naming task systematically worse than production; invalid trial rates spike for smaller models without tailored formatting; Qwen's "think loop" requires separate token budget
- **First 3 experiments:** Replicate mental vs. explicit comparison on Mistral family; test decade-number spike hypothesis with non-base-10 counting words; extend neuron correlation analysis to intermediate layers

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can grounding number concepts in sensorimotor experience enable LLMs to achieve systematic mastery of counting principles?
- **Basis in paper:** The Conclusion hypothesizes that proficient mastery may require "grounding of number concepts into sensorimotor experience."
- **Why unresolved:** Current models rely on statistical patterns or surface-level token prediction, lacking embodied feedback loops.
- **What evidence would resolve it:** Integrating multimodal inputs and observing whether systematic counting emerges in spontaneous conditions.

### Open Question 2
- **Question:** Does integrating a comparison mechanism between accumulation signals and symbolic targets mitigate termination errors?
- **Basis in paper:** The Discussion notes that Llama models "lack a mechanism to compare the accumulation signal to the target symbolic number."
- **Why unresolved:** Models often fail to bi-directionally map quantity to symbols, leading to systematic underestimation.
- **What evidence would resolve it:** Architectural modifications that explicitly monitor hidden state accumulation against target numbers.

### Open Question 3
- **Question:** Are enumeration limitations primarily caused by the transformer architecture's non-recurrent nature or specific training deficiencies?
- **Basis in paper:** The Introduction cites conflicting theories on whether failures stem from non-recurrence, layer normalization, or tokenization constraints.
- **Why unresolved:** This study confirms the performance gap but does not isolate whether the bottleneck is architectural or training-related.
- **What evidence would resolve it:** Controlled comparisons between recurrent architectures and transformers trained on identical enumeration-specific corpora.

## Limitations

- **Tokenization Dependence:** Core analyses rely on controlled one-token-per-word mapping that may not generalize to real-world text
- **Proprietary Model Opacity:** Behavioral differences between Llama and proprietary models cannot be fully explained due to limited access to model internals
- **Prompt Engineering Effects:** Subtle prompt variations produce dramatically different behaviors, but the interpretation mechanism remains unclear

## Confidence

**High Confidence:**
- Production tasks achieve higher accuracy than naming tasks across all conditions
- Mental counting produces structured low-dimensional neural trajectories while explicit counting does not
- No model spontaneously engages in systematic counting without explicit instructions

**Medium Confidence:**
- The horseshoe-shaped trajectory in mental counting reflects true accumulation rather than token prediction artifacts
- Larger Llama models show gradual rather than emergent improvement in enumeration accuracy
- Approximate estimation mechanisms explain near-target responses in forbid-counting conditions

**Low Confidence:**
- The distinction between surface-level token prediction and genuine accumulation in explicit counting
- Generalization of the explicit/mental counting mechanism distinction to non-Llama architectures
- The specific training corpus features that enable near-target estimation in forbid-counting conditions

## Next Checks

1. **Architecture Transfer Test:** Replicate the explicit vs. mental counting comparison on Mistral or Claude families to test whether accumulator-like trajectories generalize.

2. **Corpus Bias Isolation:** Test the decade-number spike hypothesis by using non-base-10 counting words to determine whether this reflects training corpus biases.

3. **Layer-wise Analysis:** Extend the neuron correlation analysis to intermediate transformer layers to determine whether accumulation signals emerge at specific depths.