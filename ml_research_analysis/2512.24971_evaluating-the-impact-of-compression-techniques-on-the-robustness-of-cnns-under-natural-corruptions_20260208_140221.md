---
ver: rpa2
title: Evaluating the Impact of Compression Techniques on the Robustness of CNNs under
  Natural Corruptions
arxiv_id: '2512.24971'
source_url: https://arxiv.org/abs/2512.24971
tags:
- compression
- robustness
- techniques
- accuracy
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the impact of compression techniques\u2014\
  quantization, pruning, and weight clustering\u2014on the robustness of CNNs under\
  \ natural corruptions. Using CIFAR-10-C and CIFAR-100-C datasets, compression techniques\
  \ were applied individually and in combination to ResNet-50, VGG-19, and MobileNetV2\
  \ architectures."
---

# Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions

## Quick Facts
- arXiv ID: 2512.24971
- Source URL: https://arxiv.org/abs/2512.24971
- Reference count: 19
- Primary result: 69% of compressed CNN models achieved equal or lower mean corruption error (mCE) compared to baselines

## Executive Summary
This study evaluates compression techniques—quantization, pruning, and weight clustering—on CNN robustness under natural corruptions using CIFAR-10-C and CIFAR-100-C datasets. The research applies these techniques individually and in combination to ResNet-50, VGG-19, and MobileNetV2 architectures. Results demonstrate that compression can preserve or improve robustness in most configurations, with 69% of compressed models achieving equal or lower mCE than baselines. The study identifies Pareto-optimal configurations where combined techniques like PCQAT, CQAT, and PQAT provide superior trade-offs between accuracy, robustness, and compression ratio for complex architectures.

## Method Summary
The study applies transfer learning from ImageNet-pretrained weights to CIFAR-10/100 datasets using Adam optimizer with categorical cross-entropy loss. Three architectures (ResNet-50, VGG-19, MobileNetV2) are evaluated with 16 compression configurations ranging from single techniques to combined approaches. Models are fine-tuned with Early Stopping (max 30 epochs, patience=5), except during pruning where full training duration is used. Robustness is measured using Mean Corruption Error (mCE) across 15 corruption types at 5 severity levels on CIFAR-10-C/100-C datasets. Compression ratio is calculated as original model size divided by compressed size. Pareto front analysis identifies optimal trade-offs across accuracy, mCE, and compression ratio.

## Key Results
- 69% of compressed models achieved equal or lower mCE compared to uncompressed baselines
- ResNet-50 + PCQAT Int8 (#16) achieved the lowest mCE of 76.7
- VGG-19 + PCQAT Int8 (#16) achieved the highest compression ratio of 9.42×
- MobileNetV2 showed no representation in the Pareto front due to its compact design
- QAT with Int8 (#10) consistently produced the worst performance across all metrics

## Why This Works (Mechanism)

### Mechanism 1: Compression as Implicit Regularization
By reducing model capacity through weight removal, precision reduction, or weight sharing, compression techniques force models to learn more generalizable features rather than overfitting to clean training data. This regularization effect can reduce sensitivity to distribution shifts caused by natural corruptions. The benefit depends on compression degree and architecture complexity, with excessive compression degrading all metrics.

### Mechanism 2: Synergistic Effect of Combined Techniques (PCQAT Pipeline)
The sequential combination of pruning → clustering → quantization-aware training produces better multi-objective trade-offs than individual techniques for complex architectures. Each stage builds on the previous: pruning removes redundant weights first, clustering groups remaining weights to reduce unique values, and QAT fine-tunes while accounting for quantization error. This progressive adaptation allows recovery of accuracy while maintaining compression benefits.

### Mechanism 3: Architectural Capacity Determines Compression Benefit
Complex architectures with more parameters benefit more from compression techniques than already-efficient architectures. ResNet-50 (91MB) and VGG-19 (73MB) contain substantial redundancy that compression can remove without harming robustness. MobileNetV2 (13MB) is already optimized for efficiency, leaving little redundancy to exploit without crossing into harmful capacity reduction.

## Foundational Learning

- **Mean Corruption Error (mCE):**
  - Why needed here: This is the primary robustness metric; it aggregates error across 15 corruption types × 5 severity levels (75 total conditions), normalized against a baseline.
  - Quick check question: If a compressed model achieves mCE = 88 on CIFAR-10-C, what does this indicate relative to the uncompressed baseline?

- **Pareto Optimality:**
  - Why needed here: The paper uses Pareto fronts to identify configurations that optimally balance three competing objectives (maximize accuracy, minimize mCE, maximize compression ratio). A solution is Pareto-optimal if no other solution improves one objective without degrading another.
  - Quick check question: Why might a model with 90% accuracy, mCE of 85, and 4× compression be Pareto-optimal even though another model has 92% accuracy?

- **Quantization-Aware Training (QAT) vs. Post-Training Quantization:**
  - Why needed here: The paper shows QAT-based techniques dominate the Pareto front, but QAT with Int8 (#10) fails dramatically. Understanding the distinction between training-aware and post-hoc quantization is critical for interpreting these results.
  - Quick check question: Why might QAT alone (#9) preserve robustness while QAT with Int8 quantization (#10) severely degrades it?

## Architecture Onboarding

- **Component map:**
  - Base models: ResNet-50 (91MB, residual connections) -> VGG-19 (73MB, deep simple conv stacks) -> MobileNetV2 (13MB, inverted residuals + linear bottlenecks)
  - Compression techniques: 16 configurations ranging from single techniques (quantization, pruning, clustering) to combined (PQAT, CQAT, PCQAT with optional Int8)
  - Evaluation datasets: CIFAR-10/100 (clean accuracy) -> CIFAR-10-C/100-C (robustness via mCE across 75 corruption scenarios)

- **Critical path:**
  1. Load ImageNet pre-trained weights → Apply transfer learning to CIFAR-10/100 (Adam optimizer, categorical cross-entropy, Early Stopping with patience=5, max 30 epochs)
  2. Apply compression technique(s) → Fine-tune (note: no Early Stopping for pruning to allow progressive sparsity increase)
  3. Evaluate: (a) clean accuracy on CIFAR test set, (b) mCE on corrupted test sets, (c) compression ratio = original size / compressed size
  4. Generate Pareto front across all model-technique combinations

- **Design tradeoffs:**
  - Best robustness: ResNet-50 + PCQAT Int8 (#16) achieved mCE = 76.7 (lowest in study)
  - Best compression: VGG-19 + PCQAT Int8 (#16) achieved 9.42× compression ratio
  - Best accuracy: MobileNetV2 + PQAT Int8 (#14) achieved 94.34% on CIFAR-10
  - Worst performer: QAT Int8 (#10) consistently degraded metrics across models—avoid this configuration

- **Failure signatures:**
  - mCE > 100: Robustness worse than baseline; indicates compression harmed generalization
  - Technique #10 pattern: QAT with Int8 produced worst mCE (201.7) and worst accuracy (61.33%)—suggests training instability or insufficient fine-tuning capacity
  - MobileNetV2 Pareto absence: Compression provides minimal benefit for already-efficient architectures; signals need for architecture-specific compression strategies

- **First 3 experiments:**
  1. **Establish baseline robustness:** Train uncompressed ResNet-50 on CIFAR-10, evaluate mCE on CIFAR-10-C. Expected: mCE ≈ 100 (baseline reference).
  2. **Single-technique comparison:** Apply pruning (#3), clustering (#5), and QAT (#9) separately to ResNet-50. Compare mCE and compression ratios to identify which individual technique provides best trade-off.
  3. **Combined technique validation:** Apply PCQAT Int8 (#16) to ResNet-50. Verify that mCE decreases (target: <85) and compression ratio increases (target: >7×) compared to single-technique results.

## Open Questions the Paper Calls Out
- How does varying the intensity of compression hyperparameters (e.g., higher sparsity rates, fewer clusters) shift the trade-off boundaries between accuracy and robustness?
- Which specific natural corruption types (e.g., fog, noise, blur) are most sensitive to specific compression techniques like quantization or pruning?
- Can specialized compression strategies enable lightweight architectures (like MobileNetV2) to achieve Pareto-optimal trade-offs similar to complex models?

## Limitations
- The study does not specify learning rate or batch size for the Adam optimizer, which could affect reproducibility and explain variability in results.
- The absence of MobileNetV2 from the Pareto front is attributed to its compact design without deeper architectural analysis to confirm whether specific compression strategies could benefit lightweight models.
- The study focuses on classification tasks only, limiting generalizability to other computer vision domains.

## Confidence
- **High Confidence**: 69% of compressed models maintained or improved mCE compared to baselines
- **Medium Confidence**: Combined techniques (PCQAT, CQAT, PQAT) produce superior trade-offs for complex architectures, but MobileNetV2 representation in Pareto front requires further investigation
- **Medium Confidence**: Architectural capacity determines compression benefit, but direct evidence comparing redundancy levels across architectures is not provided

## Next Checks
1. **Reproduce the QAT Int8 failure mode**: Systematically vary learning rates and batch sizes for technique #10 to determine if training instability can be mitigated, and establish whether this represents a fundamental limitation of aggressive quantization without intermediate stabilization.
2. **Architecture-specific compression strategies**: Design and test lightweight-model-specific compression pipelines for MobileNetV2 that preserve architectural constraints while exploring alternative sparsity/clustering approaches beyond the standard pipeline.
3. **Cross-dataset generalization**: Validate the Pareto-optimal configurations on additional robustness benchmarks beyond CIFAR-10-C/100-C (e.g., ImageNet-C, DomainNet) to assess whether the identified trade-offs generalize across different corruption distributions and domain shifts.