---
ver: rpa2
title: 'VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language
  Models'
arxiv_id: '2505.22654'
source_url: https://arxiv.org/abs/2505.22654
tags:
- visual
- tokens
- token
- vscan
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of large vision-language
  models (LVLMs) when processing high-resolution or multi-image/video inputs, which
  generate long visual token sequences and incur significant computational costs due
  to quadratic attention complexity. The authors conduct a comprehensive empirical
  analysis of how visual tokens are processed throughout both visual encoding and
  language decoding stages.
---

# VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models

## Quick Facts
- arXiv ID: 2505.22654
- Source URL: https://arxiv.org/abs/2505.22654
- Reference count: 40
- Primary result: VScan achieves 2.91× speedup and 10× FLOPs reduction on LLaVA-NeXT-7B while retaining 95.4% of original performance

## Executive Summary
VScan addresses the computational inefficiency of large vision-language models (LVLMs) when processing high-resolution or multi-image/video inputs, which generate long visual token sequences and incur significant computational costs due to quadratic attention complexity. The authors conduct a comprehensive empirical analysis of how visual tokens are processed throughout both visual encoding and language decoding stages. Based on their findings that visual tokens evolve from local to global focus during encoding and that middle layers of language models are optimal for pruning due to reduced position bias and stabilized predictions, they propose VScan - a two-stage, training-free visual token reduction framework.

VScan employs complementary global-local scanning during visual encoding to capture both semantically important and spatially diverse tokens, followed by middle-layer pruning during language model decoding to remove textually irrelevant tokens while preserving cross-modal interactions. Extensive experiments across four LVLMs (LLaVA-1.5, LLaVA-NeXT, Qwen-2.5-VL, Video-LLaVA) and sixteen benchmarks demonstrate VScan's effectiveness. When applied to LLaVA-NeXT-7B, VScan achieves 2.91× speedup in prefilling and 10× reduction in FLOPs while retaining 95.4% of original performance. Across all models and benchmarks, VScan consistently outperforms state-of-the-art methods, achieving superior performance-efficiency trade-offs.

## Method Summary
VScan is a two-stage, training-free visual token reduction framework for LVLMs. The first stage performs visual encoding with complementary global-local scanning: global scan uses deep layer [CLS] attention to capture semantically important tokens, while local scan uses shallow layer attention within image windows to capture spatially diverse details. Unselected tokens are merged with their most similar selected tokens via cosine similarity averaging. The second stage applies middle-layer pruning in the LLM decoder, using attention between visual tokens and the last instruction token to retain only the most textually relevant tokens at an optimal middle layer where cross-modal interactions have matured but computational savings are still substantial.

## Key Results
- Achieves 2.91× speedup in prefilling and 10× reduction in FLOPs while retaining 95.4% performance on LLaVA-NeXT-7B
- Consistently outperforms state-of-the-art methods across four LVLMs and sixteen benchmarks
- Global-local scanning captures complementary visual information, with 50/50 split performing best
- Middle-layer pruning (layer 16 for LLaVA, layer 14 for Qwen-7B) provides optimal accuracy-efficiency trade-off

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A complementary global-local scanning strategy during visual encoding captures a more comprehensive representation of visual information than single-strategy approaches.
- **Mechanism:** The method operates on the visual encoder's layers. A Global Scan uses the attention scores from the `[CLS]` token at the deep (output) layers to select tokens that represent broad, semantically significant regions. Concurrently, a Local Scan partitions the image into non-overlapping windows and uses attention from a shallow layer to select the top tokens within each window. This preserves fine-grained details and spatial diversity that may be suppressed in global attention maps. The selected token set is the union of both scans.
- **Core assumption:** Important visual information is distributed across two dimensions: high-level semantic context (captured globally in deep layers) and low-level local details (captured in shallow layers). A single selection strategy is insufficient to capture both.
- **Evidence anchors:**
  - [abstract] "VScan employs complementary global-local scanning during visual encoding to capture both semantically important and spatially diverse tokens"
  - [section 4.1] "Global Scan... captures global information... Local Scan... To complement the global tokens and capture finer local details, we divide the image into non-overlapping windows..."
  - [section 3] "In the shallow layers, the [CLS] attention maps capture fine-grained local details... in the deeper layers, the attention becomes increasingly concentrated on the main entities..."

### Mechanism 2
- **Claim:** Token merging based on cosine similarity preserves information from pruned tokens, preventing abrupt information loss.
- **Mechanism:** After identifying the set of tokens to retain via scanning, all unselected tokens are not simply discarded. Instead, each unselected token is assigned to its most similar selected token in the feature space (using cosine similarity). The final representation for each selected token is the average of itself and all its assigned unselected tokens. This `average merging` effectively aggregates the features of a local region into a single representative token.
- **Core assumption:** The visual feature space is structured such that "unimportant" tokens are visually and semantically similar to at least one of the "important" tokens. Merging them by averaging provides a reasonable approximation of the collective information.
- **Evidence anchors:**
  - [abstract] "...followed by token merging to preserve comprehensive visual information."
  - [section 4.1] "To alleviate information loss, we introduce a similarity-based token merging strategy that merges unselected visual tokens with their most similar selected counterparts."
  - [corpus] A related paper, FlowCut, also explores redundancy via information flow, but VScan's specific contribution is the use of similarity-based merging after global-local selection.

### Mechanism 3
- **Claim:** Pruning visual tokens at the middle layers of the LLM decoder optimizes the trade-off between preserving cross-modal reasoning and maximizing computational savings.
- **Mechanism:** Unlike early-layer pruning, which is susceptible to the LLM's initial positional bias, or late-layer pruning, which offers minimal compute reduction, this method prunes at a middle layer (e.g., layer 16 of 32). At this depth, cross-modal attention has had time to mature and integrate visual-textual information, and the next-token predictions have begun to stabilize. Pruning here removes tokens that remain irrelevant after this integration phase.
- **Core assumption:** The early layers of the LLM are critical for establishing the grounding between text and visual concepts, a process that is fragile and can be disrupted by premature pruning. The middle layers represent a "semantic saturation" point where the relevant visual information has been incorporated into the text stream.
- **Evidence anchors:**
  - [abstract] "...middle layers of language models are optimal for pruning due to reduced position bias and stabilized predictions"
  - [section 3] "...early layers exhibit strong positional bias... As the layers deepen, cross-modal interactions begin to emerge, and output token probabilities typically converge in the mid-to-late layers..."
  - [section 4.2] "Empirical Validation... pruning at middle LLM layers (e.g., layers 16 or 20) yields the best performance, whereas pruning at earlier layers (e.g., layer 2) leads to up to a 1.9% drop in accuracy."

## Foundational Learning

- **Concept:** Self-Attention and Cross-Attention Mechanisms in Transformers.
  - **Why needed here:** VScan's entire logic is based on analyzing attention weights. The global/local scans rely on `[CLS]` and self-attention in the ViT encoder, while the middle-layer pruning uses cross-attention between visual tokens and the last instruction token in the LLM. One must understand what these attention scores represent to design the pruning logic.
  - **Quick check question:** In a Transformer, what does a high attention score between token A and token B signify about their relationship?

- **Concept:** The Vision-Language Model (VLM) Pipeline (ViT -> Projector -> LLM).
  - **Why needed here:** VScan is a system that intervenes at two specific points in this pipeline. Understanding the data flow—from image patches to visual tokens, through the projector, and into the LLM's context window—is essential for correctly implementing the two-stage reduction.
  - **Quick check question:** At what point in the standard VLM pipeline do the visual and textual modalities first interact?

- **Concept:** Training-Free Inference Optimization.
  - **Why needed here:** VScan is explicitly a "training-free" method. This distinguishes it from techniques that require model fine-tuning. It means the optimization is purely a runtime manipulation of the input sequence to reduce FLOPs, without changing the model's learned weights.
  - **Quick check question:** What is the primary advantage and a key limitation of a training-free optimization strategy compared to one that requires fine-tuning?

## Architecture Onboarding

- **Component map:** Image patches -> ViT layers -> Global Scan (deep [CLS] attention) + Local Scan (shallow window attention) -> Token Merging (cosine similarity averaging) -> Projector -> LLM (early layers) -> Middle-Layer Pruning (visual-text attention) -> Remaining LLM layers -> Output

- **Critical path:** The Middle-Layer Pruning decision is the most critical path for both correctness and efficiency. If the selection logic at this stage is flawed, it will discard necessary context, directly harming the model's final response. The Local Scan is also critical for tasks requiring fine-grained visual grounding.

- **Design tradeoffs:**
  - **R1 (Visual Encoder Retention) vs. R2 (LLM Retention):** A higher R1 preserves more visual information but passes more tokens to the LLM, increasing early compute. A lower R2 increases later-stage speed but risks removing visual information that might be needed for complex reasoning in deeper layers.
  - **Global/Local Ratio:** A 50/50 split is a default, but tasks requiring dense, document-level detail may benefit from a higher local token proportion.
  - **Choice of Layer k:** The optimal middle layer is architecture-dependent. Pruning too early causes positional bias issues; pruning too late reduces the potential computational savings.

- **Failure signatures:**
  - **Incorrect Reasoning or Hallucination:** Often a sign that middle-layer pruning (R2) was too aggressive or occurred at a suboptimal layer, cutting off a critical piece of visual evidence required for the reasoning chain.
  - **Degraded Performance on Dense Vision Tasks (e.g., TextVQA, RefCOCO):** Indicates the Local Scan is not effectively preserving fine-grained details. The windowing strategy or the choice of shallow layer may need adjustment.
  - **Lower-than-Expected Speedup:** This may occur if the implementation of the pruning logic introduces overhead that negates the gains from reduced FLOPs, or if the pruning layer `k` is set too deep in the LLM.

- **First 3 experiments:**
  1. **Layer-wise Ablation Study:** Run the full VScan pipeline but vary the LLM pruning layer `k` (e.g., at 10%, 30%, 50%, 70% depth). Plot accuracy on a VQA benchmark (like GQA) vs. the layer index to empirically validate the "middle-layer is optimal" hypothesis on the target model.
  2. **Global vs. Local Scan Ablation:** Run three variants of the visual encoder stage only: (a) Global-Scan-only, (b) Local-Scan-only, and (c) VScan's combined approach. Measure performance on a suite of benchmarks with different visual characteristics (e.g., GQA for objects, TextVQA for text) to confirm their complementary nature.
  3. **End-to-End Efficiency-Accuracy Trade-off:** Run the complete, integrated VScan system on a target LVLM (e.g., LLaVA-1.5-7B). For several retention rate configurations (e.g., R1/R2 pairs yielding 11%, 22%, 33% average retention), report (a) Average benchmark accuracy, (b) Prefilling latency (ms), and (c) Total TFLOPs. Compare this Pareto frontier against the vanilla model and one SOTA baseline like FastV.

## Open Questions the Paper Calls Out
None explicitly called out in the provided text.

## Limitations
- **Architectural dependency:** VScan's effectiveness is tightly coupled to specific model architectures, requiring careful hyperparameter tuning (optimal pruning layer, retention rates) for each new model.
- **Fine-grained detail loss:** Aggressive token pruning may distort visual information and lead to degraded performance on tasks demanding fine-grained understanding or compositional reasoning.
- **Implementation complexity:** While training-free, the two-stage scanning and merging process adds inference pipeline complexity and potential overhead not fully characterized.

## Confidence

**High Confidence (80-100%)**
- The empirical analysis demonstrating that middle-layer pruning is superior to early-layer pruning for maintaining accuracy is robust and well-supported by the ablation studies.
- The complementary nature of global and local scans in capturing different aspects of visual information is a sound principle, and the ablation study comparing them to their individual components provides strong evidence.
- The general framework of using attention scores for token selection is a valid approach, as it is grounded in the model's own learned importance weights.

**Medium Confidence (50-80%)**
- The specific numerical results (e.g., 95.4% accuracy retention, 10× FLOPs reduction on LLaVA-NeXT-7B) are highly dependent on the exact implementation details and hyperparameter choices, which are not fully specified.
- The superiority of VScan over state-of-the-art methods like FastV and STAR is demonstrated, but the margin of improvement may vary depending on the specific task distribution and evaluation protocol used.

**Low Confidence (0-50%)**
- The long-term robustness of the method across a wide variety of LVLM architectures and diverse input distributions is not established.
- The interaction between the visual token reduction and the LLM's autoregressive generation, particularly for very long sequences or in the presence of errors introduced by pruning, is not fully explored.

## Next Checks

1. **Architecture Transferability Test**: Apply VScan to a third, distinct LVLM architecture not included in the original evaluation (e.g., IDEFICS or PaLI). Systematically sweep the pruning layer (k) and retention rates (R1, R2) to find the optimal configuration for this new model. Measure the resulting accuracy and FLOPs reduction to assess how much the performance gains are dependent on model-specific tuning.

2. **Task-Specific Ablation Study**: Conduct a focused ablation study on tasks known to require fine-grained visual detail (e.g., TextVQA, POPE for mathematical reasoning). Vary the proportion of local scan tokens (e.g., 25%, 50%, 75% of the visual token budget) while keeping the global scan fixed. This will directly test the hypothesis that local scans are critical for dense vision tasks and quantify the trade-off between detail preservation and efficiency.

3. **Error Analysis and Attribution**: For a set of failure cases where VScan degrades performance compared to the baseline, perform a detailed analysis. Use attention visualization to trace which tokens were pruned and determine if they were critical for the correct answer. Compare the distribution of pruned tokens in correct vs. incorrect responses to identify systematic failure modes and potential refinements to the selection criteria.