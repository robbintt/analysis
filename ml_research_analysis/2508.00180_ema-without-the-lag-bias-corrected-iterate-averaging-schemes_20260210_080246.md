---
ver: rpa2
title: 'EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes'
arxiv_id: '2508.00180'
source_url: https://arxiv.org/abs/2508.00180
tags:
- bema
- training
- performance
- learning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BEMA, a bias-corrected exponential moving
  average method designed to stabilize training of language models by reducing the
  gradient variance that arises from small batch sizes. While EMA is widely used for
  stabilization, it introduces lag due to bias from older iterates, which slows convergence.
---

# EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes

## Quick Facts
- arXiv ID: 2508.00180
- Source URL: https://arxiv.org/abs/2508.00180
- Reference count: 40
- Primary result: BEMA achieves faster convergence and higher accuracy than standard EMA when fine-tuning small language models

## Executive Summary
This paper introduces BEMA, a bias-corrected exponential moving average method designed to stabilize training of language models by reducing gradient variance from small batch sizes. While EMA is widely used for stabilization, it introduces lag due to bias from older iterates, which slows convergence. BEMA mitigates this by applying a bias correction that removes the lag while retaining EMA's variance-reduction benefits. Theoretically, BEMA is shown to be optimal for finite-time estimation in a noisy quadratic model. Empirically, across a range of tasks including BoolQ, GSM8K, and MMLU-HS, BEMA consistently outperforms both vanilla training and standard EMA in terms of convergence speed and final accuracy when fine-tuning Qwen2.5-1.5B, Gemma3-1B, and Llama3.2-1B models. BEMA achieves these gains with minimal code changes, making it a practical and effective stabilizer for LM fine-tuning.

## Method Summary
BEMA builds upon standard exponential moving average (EMA) by introducing a bias correction mechanism that compensates for the lag introduced by older iterates. The method maintains the variance-reduction benefits of EMA while eliminating the convergence slowdown caused by historical parameter averaging. The bias correction is theoretically grounded in a noisy quadratic model analysis, where BEMA is proven optimal for finite-time estimation. Implementation-wise, BEMA requires only minor modifications to existing EMA implementations, making it highly practical for integration into current training pipelines.

## Key Results
- BEMA consistently outperforms standard EMA and vanilla training on BoolQ, GSM8K, and MMLU-HS benchmarks
- The method achieves faster convergence and higher final accuracy when fine-tuning Qwen2.5-1.5B, Gemma3-1B, and Llama3.2-1B models
- BEMA requires minimal code changes, making it practical for immediate deployment

## Why This Works (Mechanism)
Standard EMA introduces lag because it averages over past iterates, which includes outdated information that slows convergence. This lag manifests as a bias in the moving average, particularly problematic when training with small batch sizes where gradient estimates are noisy. BEMA corrects this bias by adjusting the averaging weights to account for the temporal distance of each iterate, effectively "undoing" the lag while preserving the variance reduction benefits. The theoretical analysis shows this correction makes BEMA optimal in the finite-time estimation setting of a noisy quadratic model, explaining its empirical superiority in reducing training instability and accelerating convergence.

## Foundational Learning

**Exponential Moving Average (EMA)**: A technique that maintains a moving average of parameters over training iterations, weighted exponentially to give more importance to recent updates. Why needed: EMA is crucial for stabilizing training by smoothing out noisy gradient estimates, especially with small batch sizes.

**Bias Correction**: The process of adjusting statistical estimates to account for systematic errors introduced by the estimation method. Why needed: Standard EMA introduces bias due to the inclusion of outdated parameter values, which BEMA corrects to maintain optimal convergence speed.

**Variance Reduction**: Techniques that decrease the variability of gradient estimates to improve training stability. Why needed: Small batch sizes produce noisy gradients, and variance reduction is essential for maintaining stable and efficient training.

**Quick Check**: Verify that the bias correction formula properly accounts for the exponential decay of weights and that the correction factor grows appropriately with the number of iterations.

## Architecture Onboarding

**Component Map**: Model Parameters -> EMA Buffer -> Bias Correction -> Updated Parameters

**Critical Path**: The forward pass through the model remains unchanged; BEMA operates as a post-processing step on the parameter updates, making it orthogonal to the core training loop.

**Design Tradeoffs**: BEMA trades minimal additional computation (for bias correction) against significant gains in convergence speed and stability. The method maintains the memory efficiency of standard EMA while improving its effectiveness.

**Failure Signatures**: If BEMA fails to improve convergence, potential causes include: incorrect implementation of the bias correction formula, inappropriate choice of EMA decay parameter, or scenarios where gradient noise is not the primary bottleneck.

**First Experiments**:
1. Implement BEMA alongside standard EMA on a simple linear regression task to verify basic functionality
2. Compare convergence curves of BEMA vs standard EMA on a small language model fine-tuning task
3. Test BEMA's sensitivity to the EMA decay parameter across different batch sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis primarily focused on small language models (1.5B parameters), with generalization to larger models untested
- Theoretical optimality claims are specific to noisy quadratic models and may not fully translate to complex deep learning landscapes
- Limited exploration of hyperparameter sensitivity and interactions with other optimization techniques

## Confidence

**High Confidence**: The core algorithmic contribution of bias-corrected EMA is technically sound and the implementation details are clearly presented. The theoretical analysis in the quadratic model is rigorous.

**Medium Confidence**: The empirical results showing improved convergence and accuracy are compelling, but the limited model size and task diversity warrant caution in generalizing these findings. The practical benefits of BEMA over standard EMA in large-scale settings need further validation.

**Medium Confidence**: The claim of BEMA being "optimal" for finite-time estimation is specific to the theoretical framework used and should be interpreted within that context rather than as a universal optimality claim.

## Next Checks

1. **Scaling Test**: Evaluate BEMA on larger language models (e.g., 7B+ parameters) and more diverse architectures to assess scalability and robustness.

2. **Ablation Study**: Conduct a comprehensive ablation study on BEMA's hyperparameters and compare its performance against other variance-reduction techniques under varying batch sizes and learning rates.

3. **Real-World Complexity**: Test BEMA in more complex optimization landscapes, such as those involving non-stationary data distributions or multi-task learning scenarios, to validate its practical utility beyond controlled benchmarks.