---
ver: rpa2
title: Real-time prediction of workplane illuminance distribution for daylight-linked
  controls using non-intrusive multimodal deep learning
arxiv_id: '2512.14058'
source_url: https://arxiv.org/abs/2512.14058
tags:
- test
- illuminance
- daylight
- data
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a multimodal deep learning framework to predict
  indoor workplane illuminance distributions in real time using non-intrusive images
  and temporal-spatial features. The model extracts features from side-lit window
  areas rather than interior pixels, making it applicable in dynamically occupied
  indoor spaces.
---

# Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning

## Quick Facts
- arXiv ID: 2512.14058
- Source URL: https://arxiv.org/abs/2512.14058
- Reference count: 40
- Presents a multimodal deep learning framework for real-time prediction of indoor workplane illuminance distributions using non-intrusive images and temporal-spatial features

## Executive Summary
This study presents a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. The approach extracts features only from side-lit window areas rather than interior pixels, making it applicable in dynamically occupied indoor spaces. A field experiment in Guangzhou, China collected 17,344 samples for training and validation. The model achieved R² > 0.98 with RMSE < 0.14 on the same-distribution test set and R² > 0.82 with RMSE < 0.17 on an unseen-day test set, demonstrating high accuracy and acceptable temporal generalization.

## Method Summary
The method uses a multimodal deep learning architecture that combines CNN-extracted features from window-region images with structured features encoding sensor position and cyclical time. The model predicts three directional illuminance values (Eh, Es, Ee) at arbitrary indoor locations. Training data consists of synchronized images, sensor positions, time stamps, and measured illuminance values. The framework uses early stopping and z-score normalization, with a 4-layer CNN encoder and MLP regressor selected through ablation studies.

## Key Results
- Achieved R² > 0.98 with RMSE < 0.14 on same-distribution test set
- Maintained R² > 0.82 with RMSE < 0.17 on unseen-day test set
- Demonstrated robust performance in dynamically occupied spaces by using only window-area image features

## Why This Works (Mechanism)

### Mechanism 1
Integrating window-region luminance features with cyclical time and spatial location enables accurate prediction of workplane illuminance. A CNN extracts visual features (e.g., sky brightness, cloud cover) from a non-intrusive image of the window, while a separate MLP encodes the sensor's spatial position (X, D) and cyclical time of day (sin/cos). These embeddings are concatenated into a fused representation that a final regressor uses to map external daylight conditions to specific indoor illuminance levels (Eh, Es, Ee).

### Mechanism 2
Restricting the visual input to the window area ensures system robustness in dynamic, occupied environments. By masking out the interior workspace and training only on window-region pixels, the model avoids learning from areas subject to frequent occlusion or change (e.g., occupant movement, furniture rearrangement). This prevents "ghosting artifacts" and measurement errors common in traditional HDR luminance mapping of occupied spaces.

### Mechanism 3
Cyclical encoding of temporal data improves model generalization to unseen days by capturing diurnal patterns. Raw time values are transformed into sin and cos components (Tod_sin, Tod_cos). This creates a continuous, cyclical representation of time that preserves the proximity of late-night and early-morning hours, allowing the model to learn the underlying daily solar cycle rather than memorizing specific timestamps.

## Foundational Learning

- **Multimodal Fusion (Late Fusion)**: The model must combine fundamentally different data types—a 2D image (spatial) and a 1D feature vector (temporal/spatial). Late fusion allows independent feature extraction before combining them for final prediction. Quick check: How does concatenating a 128-dimensional image embedding with a 32-dimensional structured embedding differ from stacking the raw inputs?

- **Cyclical Feature Encoding**: Standard time representations (e.g., 0-1439 minutes) create discontinuities between the end and start of a day, which harms model learning. Sin/cos encoding ensures temporal continuity. Quick check: Why would a model struggle to learn the relationship between 23:55 and 00:05 if time were input as a single linear integer?

- **CNNs as Feature Extractors**: The model must automatically learn to identify relevant luminance patterns (clouds, direct sun, shadows) from raw pixel data without manual feature engineering. Quick check: In this architecture, what is the role of the CNN layers before the MLP regressor?

## Architecture Onboarding

- **Component map:**
  - Inputs: Image (128x128 grayscale window area) + Structured Vector (4D: X, D, Tod_sin, Tod_cos)
  - Encoders: CNN (4-layer, channels [16,32,64,128]) -> 128-dim vector; MLP -> 32-dim embedding
  - Fusion & Output: Concatenate 128-dim and 32-dim vectors into 160-dim; Regressor MLP ([128,64,32]) -> 3 outputs (Eh, Es, Ee)

- **Critical path**: The spatial feature D (distance from window) and the window image are the most critical inputs. D provides the physical basis for light decay, while the image provides the instantaneous source intensity.

- **Design tradeoffs:**
  - CNN-MLP vs. Pure MLP: The CNN adds computational cost but is necessary for interpreting image data. A pure MLP could only use structured data, losing critical sky condition information.
  - Window-only vs. Full-room Image: The paper trades potentially richer interior data for robustness against occlusion and dynamic changes in the workspace.
  - LDR vs. HDR: Uses low-dynamic-range (JPEG) images for simplicity and cost, trading off some detail in high-luminance areas for a faster, cheaper workflow.

- **Failure signatures:**
  - Under-prediction at high illuminance: The model may systematically underestimate peak illuminance values (near the window at midday) due to a scarcity of high-lux samples in the training data.
  - Generalization Drop: A noticeable decrease in R² on data from a completely unseen day (Test Set 2 vs. Test Set 1).

- **First 3 experiments:**
  1. Data Preprocessing Pipeline: Implement the image masking, resizing, and the sin/cos time encoding to replicate the exact input format described in Section 2.3.1.
  2. Baseline Ablation: Train two simpler baseline models—one using only structured features and one using only image features—to quantify the performance gain from the multimodal fusion.
  3. Hyperparameter Reproduction: Re-train the final model using the selected hyperparameters (Model C: MLP structure [128, 64, 32], dropout=0.0) and compare validation loss curves to those in the paper.

## Open Questions the Paper Calls Out

- Does expanding the training dataset with specifically targeted high-luminance samples reduce prediction errors in near-window zones? The authors note that performance declines in high-illuminance scenarios due to data scarcity and explicitly propose "collecting and integrating more high-illuminance training samples" in future work.

- To what extent do multi-view or HDR imaging inputs improve the prediction accuracy of directional illuminance (specifically Es) compared to single-view LDR images? The paper identifies lower accuracy for south-facing vertical illuminance (Es) and suggests that "multi-view and HDR techniques could be considered to better capture high-luminance window features."

- How does the framework's performance generalize to diverse climates and architectural configurations beyond the single test room in Guangzhou? The authors state the dataset was collected in a single room, which "may limit the generalization of the model to other climates and building types."

## Limitations

- Limited temporal generalization: While the model shows R² > 0.82 on an unseen day, the paper does not test performance across seasons or significantly different weather patterns.

- Window-area assumption fragility: The approach assumes the window region provides sufficient information about sky conditions, which may fail in urban canyons or with obstructed views.

- Hardware dependency: The system requires a fixed ceiling-mounted camera with clear view of the window area, with installation constraints and calibration not addressed.

## Confidence

- **High**: Model achieves stated performance metrics on reported test sets
- **Medium**: Multimodal fusion approach is theoretically sound and demonstrated in similar domains
- **Low**: Claims about real-world deployment ease and long-term robustness

## Next Checks

1. Seasonal validation test: Evaluate model performance on data collected across different seasons to assess robustness to varying solar angles and weather patterns.

2. Window obstruction stress test: Systematically occlude portions of the window area in validation images to quantify performance degradation and identify failure thresholds.

3. Cross-installation transferability: Train the model on data from one location and test on a different building to assess how well the approach generalizes across different architectural configurations.