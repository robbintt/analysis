---
ver: rpa2
title: Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level
  Gaussian Processes
arxiv_id: '2502.20966'
source_url: https://arxiv.org/abs/2502.20966
tags:
- uncertainty
- neural
- network
- pre-trained
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Gaussian Process Activation functions (GAPA)
  to quantify uncertainty in pre-trained neural networks by focusing on activation-level
  rather than weight-level uncertainty. GAPA attaches a one-dimensional Gaussian Process
  to each neuron's activation, preserving the original network predictions while providing
  principled uncertainty estimates.
---

# Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level Gaussian Processes

## Quick Facts
- arXiv ID: 2502.20966
- Source URL: https://arxiv.org/abs/2502.20966
- Authors: Richard Bergna; Stefan Depewerk; Sergio Calvo Ordonez; Jonathan Plenk; Alvaro Cartea; Jose Miguel Hernandez-Lobato
- Reference count: 10
- Primary result: GAPA-Variational outperforms Laplace approximation methods on regression benchmarks, achieving best performance across NLL, CRPS, and CQM metrics.

## Executive Summary
This work introduces Gaussian Process Activation functions (GAPA) to quantify uncertainty in pre-trained neural networks by focusing on activation-level rather than weight-level uncertainty. GAPA attaches a one-dimensional Gaussian Process to each neuron's activation, preserving the original network predictions while providing principled uncertainty estimates. Two variants are proposed: GAPA-Free, which uses empirical kernel methods for efficient training, and GAPA-Variational, which learns kernel hyperparameters via variational inference for greater flexibility. Uncertainty is propagated through the network using delta approximation methods. Experimental results show GAPA-Variational outperforms state-of-the-art Laplace approximation methods on regression tasks, achieving superior performance across multiple metrics including Negative Log-Likelihood (NLL), Continuous Ranked Probability Score (CRPS), and Centered Quantile Metric (CQM), particularly on the Taxi dataset where it achieved the best performance across all evaluation metrics. The approach provides post-hoc uncertainty quantification without requiring network retraining, making it practical for large-scale pre-trained models.

## Method Summary
GAPA quantifies uncertainty in pre-trained neural networks by attaching 1D Gaussian Processes to first-layer neurons. The GP prior mean is set to the original activation function, preserving predictions exactly while the posterior covariance provides uncertainty estimates. Two variants exist: GAPA-Free uses empirical kernel hyperparameters and linear variance scaling, while GAPA-Variational learns kernel parameters via variational inference. Variance propagates through the network using delta approximation for non-linearities and linear transformation rules for linear layers. The method requires no retraining, only attaching GAPA modules to a frozen pre-trained network.

## Key Results
- GAPA-Variational achieved best performance on all three metrics (NLL, CRPS, CQM) for the Taxi dataset
- GAPA-Variational outperformed Laplace approximation baselines (VaLLA, LLA) on Airline and Year datasets for CRPS and CQM metrics
- GAPA-Free performed competitively while requiring no gradient optimization, offering faster implementation
- The approach maintains original network predictions exactly while adding uncertainty estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attaching a GP with prior mean equal to the original activation preserves the pre-trained network's predictions exactly.
- Mechanism: For each first-layer neuron, the GP prior mean m_d(X_d) is set to a_1(X_d) (the neuron's activation). Since Y_d = m_d(X_d) by construction, the posterior mean μ_d(X_d) = m_d(X_d), leaving predictions unchanged while the posterior covariance Σ_d provides epistemic uncertainty.
- Core assumption: The GP posterior covariance meaningfully captures epistemic uncertainty at the activation level, and this uncertainty propagates usefully to the output.
- Evidence anchors:
  - [section 2.2]: "As we have Y_d = m_d(X_d) by construction, it follows that μ_d(X_d) = m_d(X_d) = a_1(X_d). Hence the pre-trained network's original activation is preserved."
  - [abstract]: "Our approach operates in a post-hoc manner, preserving the original mean predictions of the pre-trained neural network."
  - [corpus]: Weak direct evidence; related work (Ortega et al. 2023) focuses on weight-space Laplace approximations rather than activation-level GP approaches.
- Break condition: If the pre-trained network's activations have pathological distributions (e.g., extreme sparsity, constant values), the GP covariance may be degenerate or uninformative.

### Mechanism 2
- Claim: The delta approximation (first-order Taylor expansion) enables tractable variance propagation through non-linear activations.
- Mechanism: For y = g(z) where z ~ N(μ, σ²), approximate g(z) ≈ g(μ) + g'(μ)(z - μ). This yields Var(y) ≈ (g'(μ))² σ². Combined with linear propagation (Σ_z = W Σ_a W^T), uncertainty flows deterministically through the network without sampling.
- Core assumption: First-order linearization is sufficiently accurate for the activation functions used; higher-order terms are negligible for well-calibrated uncertainty.
- Evidence anchors:
  - [section 2.3]: "We approximate g(z) by a first-order Taylor expansion (delta approximation)... this yields an approximate variance of Var(y) ≈ (g'(μ))² σ²."
  - [section 2.3]: "By sequentially applying the linear transformation rule for variance and the delta approximation for non-linear activations, we obtain a tractable, layer-wise method."
  - [corpus]: Delta approximation is standard in variational inference (Wu et al. 2018, cited in paper); no corpus papers challenge this specific propagation method.
- Break condition: If activations operate in highly non-linear regions (e.g., saturating sigmoid/tanh near asymptotes), g'(μ) ≈ 0 causing variance collapse and underconfident uncertainty estimates.

### Mechanism 3
- Claim: Variational learning of GP hyperparameters via NLL optimization improves calibration over fixed empirical kernels.
- Mechanism: GAPA-Variational treats inducing points (from training data) and kernel hyperparameters as learnable. The loss L = Σ_i [½ log(2πσ²_i) + (y_i - μ_i)²/(2σ²_i)] is minimized via backpropagation while keeping pre-trained weights fixed. This adapts uncertainty to the downstream task.
- Core assumption: The Gaussian likelihood assumption is appropriate for the task; NLL optimization improves calibration without overfitting to noise.
- Evidence anchors:
  - [section 2.5]: "The overall training objective is the Gaussian negative log-likelihood (NLL)... optimized by backpropagating the NLL from the network's final output while keeping the pre-trained network weights fixed."
  - [table 1]: GAPA-Variational achieves best CRPS and CQM on Airline, Year, and Taxi datasets, outperforming GAPA-Free and Laplace baselines.
  - [corpus]: Laplace approximations (VaLLA, LLA) are the primary post-hoc UQ baselines; no corpus evidence directly compares variational GP methods at the activation level.
- Break condition: If training data is small or poorly representative of test distribution, hyperparameter learning may overfit, producing miscalibrated uncertainty.

## Foundational Learning

- Concept: **Gaussian Process posterior inference**
  - Why needed here: Understanding how the GP prior mean and covariance combine to form posterior mean (preserved) and covariance (uncertainty estimate) is essential for debugging GAPA.
  - Quick check question: Given a GP with prior mean m(x) = f(x), what happens to the posterior mean when observations Y exactly equal m(X)?

- Concept: **Delta approximation / linearization for variance propagation**
  - Why needed here: The method relies on first-order Taylor expansion to propagate variance through non-linearities; understanding its limitations prevents misdiagnosis of failure cases.
  - Quick check question: If g(z) = ReLU(z) and μ = -0.1 with σ² = 0.5, what is the approximated Var(g(z))?

- Concept: **Variational inference with inducing points**
  - Why needed here: GAPA-Variational uses inducing points to make GP inference scalable; understanding this tradeoff explains when to choose GAPA-Free vs. GAPA-Variational.
  - Quick check question: Why are inducing points placed at quantile boundaries rather than uniformly across the activation range?

## Architecture Onboarding

- Component map: Pre-trained backbone network -> GAPA module (attached to first hidden layer) -> Variance propagation engine -> Calibration head

- Critical path:
  1. Forward pass through layer 0 to obtain pre-activations X = W⁰x + b⁰
  2. Compute GP posterior covariance Σ_d for each neuron d in layer 1 using training data statistics
  3. Propagate variance through remaining layers (L-1 layers) using delta approximation
  4. Output predictive mean μ (unchanged from original) and variance σ²

- Design tradeoffs:
  - GAPA-Free vs. GAPA-Variational: Free is faster (no gradient optimization) but less flexible; Variational learns task-specific calibration at higher compute cost.
  - First-layer vs. deeper-layer attachment: Paper demonstrates first-layer; deeper layers may capture higher-level uncertainty but increase propagation error.
  - Number of inducing points: More points improve GP fidelity but increase O(M³) computational burden.

- Failure signatures:
  - **Variance collapse**: σ² → 0 at output. Check: are activations saturating (g'(μ) ≈ 0)? Are kernel lengthscales too large?
  - **Variance explosion**: σ² → ∞. Check: are kernel lengthscales too small? Is variance accumulation through deep layers unbounded?
  - **No improvement over baseline**: Check: is GAPA actually attached (not bypassed)? Are hyperparameters being learned?

- First 3 experiments:
  1. **Sanity check**: On a toy regression dataset, verify that GAPA's predictive mean equals the pre-trained network's output (numerical equality, not just close).
  2. **Ablation on propagation depth**: Attach GAPA at layer 1, then at layer 2, then at layer 3; measure how CRPS/NLL degrades with deeper attachment to quantify propagation error.
  3. **Comparison on out-of-distribution inputs**: Evaluate GAPA-Variational vs. Laplace (last-layer) on shifted test distributions; check if activation-level uncertainty captures OOD better than weight-space methods.

## Open Questions the Paper Calls Out

- Can GAPA be extended to classification tasks while preserving its post-hoc, single-forward-pass properties?
  - Basis in paper: [explicit] The conclusion states: "Future work will focus on... extending the model to classification task."
  - Why unresolved: The current formulation optimizes a Gaussian negative log-likelihood for regression; classification requires adapting the likelihood and potentially the variance propagation rules for discrete outputs.
  - What evidence would resolve it: A modified GAPA framework applied to benchmark classification datasets (e.g., image classification) with calibration metrics like ECE, comparing against Laplace approximation variants for classification.

- How can the inference-time computational cost of GAPA be reduced for large-scale or real-time applications?
  - Basis in paper: [explicit] The conclusion notes: "Gaussian processes remain computationally expensive at inference time. Future work will focus on exploring scalable models or approximations to Gaussian processes to optimise computational efficiency."
  - Why unresolved: While training is efficient, the GP posterior covariance computation scales with the number of training points/inducing points at inference.
  - What evidence would resolve it: Incorporating sparse GP approximations, inducing point optimizations, or kernel approximations (e.g., random Fourier features) that maintain UQ quality while reducing inference latency.

- Does applying GAPA to deeper layers (beyond the first hidden layer) improve uncertainty quantification, and what are the trade-offs?
  - Basis in paper: [inferred] The paper states "To highlight the generality of the approach we assume here, that this method is applied to the first hidden layer," implying other layers are possible but untested.
  - Why unresolved: First-layer uncertainty may not capture higher-level feature uncertainties that propagate through deeper representations; the delta approximation may compound errors across more layers.
  - What evidence would resolve it: Ablation studies applying GAPA to different layer depths on the same datasets, measuring NLL, CRPS, and CQM to identify optimal placement.

## Limitations

- The delta approximation assumes first-order Taylor sufficiency, which may fail for highly non-linear activation functions or saturated regions.
- Inference-time computational cost scales with the number of inducing points, limiting scalability for large datasets.
- The method's effectiveness for classification tasks remains unexplored, restricting its current applicability to regression problems.

## Confidence

- **High confidence**: The preservation mechanism (Mechanism 1) is mathematically rigorous and directly verifiable from GP posterior equations.
- **Medium confidence**: The variance propagation approach (Mechanism 2) is standard but assumes linearization is sufficient; performance gains in Table 1 support this but don't guarantee robustness across architectures.
- **Medium confidence**: The variational learning superiority (Mechanism 3) is demonstrated empirically but lacks ablation studies isolating kernel learning from other factors like inducing point placement.

## Next Checks

1. Test delta approximation validity by measuring prediction divergence when replacing ReLU with smooth activations (e.g., Swish) and comparing variance estimates.
2. Evaluate calibration degradation when applying GAPA to networks with deep or narrow architectures where propagation error may accumulate.
3. Conduct OOD stress tests using synthetic data shifts (e.g., covariate shift, label shift) to verify whether activation-level uncertainty captures distributional changes better than weight-space methods.