---
ver: rpa2
title: 'The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech
  Gestures'
arxiv_id: '2511.08372'
source_url: https://arxiv.org/abs/2511.08372
tags:
- closing
- gesture
- gestures
- tongue
- glottal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DYNARTmo is a dynamic articulatory model that generates continuous
  speech movements using the gesture framework. The model links phonological representations
  (gesture scores) to articulatory trajectories, bridging the gap between linguistic
  planning and biomechanical speech production.
---

# The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech Gestures

## Quick Facts
- arXiv ID: 2511.08372
- Source URL: https://arxiv.org/abs/2511.08372
- Reference count: 4
- Primary result: Dynamic articulatory model generating continuous speech movements from phonological gesture scores

## Executive Summary
DYNARTmo is a computational model that bridges phonological planning and biomechanical speech production by generating continuous articulator trajectories from gesture scores. The model implements a neurobiologically-inspired framework where speech gestures specify target configurations, timing, and coordination across six articulators controlled by ten continuous and six discrete parameters. Using cosine-shaped activation functions and weighted pull-based blending, DYNARTmo produces smooth trajectories that capture coarticulation effects through overlapping gesture interactions. While gesture scores can be calculated by rule, the authors acknowledge current limitations and propose integration with a mental syllabary for precise language-specific implementation.

## Method Summary
The model generates articulatory trajectories through a two-step process: first computing per-gesture activation functions using piecewise cosine formulations with configurable rise/hold/fall phases, then blending these activations through weighted pull combinations. Each gesture's influence on control parameters is governed by its pull parameter, creating normalized weighted sums that produce smooth transitions during overlap regions. The implementation uses discrete-time simulation (Δt = 5ms) and supports six articulators with ten continuous and six discrete control parameters. Gesture scores specify target values, mean durations (100-150ms), and rapidity values, organized hierarchically across vocalic, consonantal, velopharyngeal, glottal, and pulmonary tiers.

## Key Results
- Continuous articulator trajectories generated through weighted blending of overlapping gestures
- Smooth coarticulation captured via pull-based competition between simultaneously active gestures
- Hierarchical tier organization enables semi-independent control of different articulatory subsystems
- Framework bridges phonological representations to biomechanical speech movements

## Why This Works (Mechanism)

### Mechanism 1: Cosine-shaped Activation Functions
Smooth gesture activation enables continuous articulator trajectories without discontinuities. Each gesture's temporal activation follows a piecewise cosine function with configurable rise/hold/fall phases, ensuring C1 continuity (continuous first derivative) throughout the trajectory. The model assumes articulatory smoothness is a biological constraint that should be enforced mathematically.

### Mechanism 2: Weighted Pull-based Blending
Coarticulation emerges from overlapping gestures weighted by relative "pull" strength. When multiple gestures target the same control parameter simultaneously, the output is computed as a normalized weighted sum: P(t) = Σ(pg·ag(t)·Tg,P) / Σ(pg·ag(t)), where higher pull values give gestures more influence during overlap regions. The model assumes gestures compete for articulatory control through soft blending rather than hard switching.

### Mechanism 3: Tier-organized Gesture Scores
Speech motor control is organized hierarchically across semi-independent articulatory subsystems. Gestures are assigned to tiers (vocalic, consonantal, velopharyngeal, glottal, pulmonary) that can operate with temporal overlap while controlling different articulator groups. Each phoneme typically activates 2-3 gestures across different tiers. The model assumes the nervous system organizes speech gestures along functionally distinct channels that can be partially decoupled in time.

## Foundational Learning

- **Task Dynamics framework (Saltzman & Munhall 1989)**
  - Why needed here: The activation function and blending mechanics are directly inherited from this framework; understanding the original formulation clarifies design choices.
  - Quick check question: Can you explain why a second-order dynamical system might produce different blending behavior than the first-order approximation used here?

- **Articulatory Phonology (Browman & Goldstein)**
  - Why needed here: The gesture concept and gesture score originate here; the paper assumes familiarity with gestures as both phonological units and motor actions.
  - Quick check question: What does it mean for a gesture to be "overlap-tolerant" in the context of coarticulation?

- **Sagittal vocal tract anatomy**
  - Why needed here: Control parameters (height, position, rounding, closure) reference anatomical landmarks; interpreting trajectories requires knowing what "dorsal closing" or "apical" mean.
  - Quick check question: Which articulator is primarily responsible for "dorsal" vs. "apical" consonants?

## Architecture Onboarding

- **Component map:** Phonological input → Gesture score generator (rule-based or syllabary lookup) → Activation function calculator (per-gesture ag(t)) → Blending module (weighted pull combination) → Control parameter trajectories (10 continuous + 6 discrete) → DYNARTmo vocal tract model (midsagittal visualization)

- **Critical path:** The blending equation in Section 2.3 is the core computational step. If you're debugging trajectory output, trace: (1) gesture timing → (2) activation values → (3) pull-weighted blending → (4) final parameter values.

- **Design tradeoffs:**
  - Rule-based gesture score generation is acknowledged as "still erroneous" (Section 3); mental syllabary integration is proposed but not implemented.
  - First-order blending simplifies task dynamics but may miss biomechanical coupling effects (corpus paper on articulatory tradeoffs notes this limitation).
  - Neutral gesture interpolation during silence gaps is a modeling choice, not empirically validated.

- **Failure signatures:**
  - Non-smooth trajectories: Check for τon/τoff values shorter than timestep ∆t, causing activation undersampling.
  - Unrealistic overlap outputs: Verify pull parameter scaling; gestures with identical pull will blend 50/50 regardless of target distance.
  - Missing gestures: If a phoneme lacks expected tier activations, check the phoneme-to-gesture mapping in your gesture inventory version.

- **First 3 experiments:**
  1. Reproduce Figure 3 (/pa:i:/) using the provided demo_pai_lex.py to validate your environment and understand the visualization output format.
  2. Manually modify gesture onset/offset times in the nonsense word "kamflik" (Table 4) and observe how blended trajectories change—test edge cases with maximal overlap.
  3. Compare rule-generated gesture scores against hand-tuned timing for a simple CV syllable to quantify the "erroneous" behavior the paper acknowledges; document the systematic errors.

## Open Questions the Paper Calls Out

### Open Question 1
How can a mental syllabary be integrated into DYNARTmo to enable precise, language-specific quantification of gesture scores? The authors state that generating gesture scores by rule is currently "erroneous" and propose integrating a mental syllabary to resolve this. What evidence would resolve it: Successful generation of articulatory trajectories for syllables that match empirical data without manual correction of timing rules.

### Open Question 2
How should the gesture "pull" parameter ($p_g$) be determined to accurately model coarticulation resistance? The model relies on a weighted sum using a "pull" parameter to blend overlapping gestures, but values appear to be manually estimated. What evidence would resolve it: An algorithm that optimizes $p_g$ values based on real-world articulatory data (e.g., EMA) to minimize trajectory error.

### Open Question 3
What neural-level control architectures are required to drive the gesture activation patterns in DYNARTmo? The conclusion explicitly lists the "integration [of] the model with neural-level control architectures" as a goal for future work. What evidence would resolve it: A simulation where a spiking neural network generates the gesture scores, producing realistic reaction times and speech errors.

## Limitations

- Rule-based gesture score generation is explicitly acknowledged as "still erroneous," creating a significant gap between model capabilities and practical implementation
- Critical parameter values (τ_on/τ_off flank durations, pull parameter relationships) are either unspecified or require interpretation
- Proposed mental syllabary integration remains unimplemented, leaving the model dependent on flawed automatic generation rules
- Limited empirical validation against articulatory data; validation is primarily visual inspection of trajectories

## Confidence

- **High Confidence**: The mathematical framework for activation functions and blending is clearly specified and internally consistent
- **Medium Confidence**: The gesture tier organization and control parameter mappings are explicitly specified in tables, but their empirical validity is untested
- **Low Confidence**: The practical utility of the model for real speech generation, given the acknowledged limitations of rule-based gesture score generation and the unimplemented mental syllabary integration

## Next Checks

1. Conduct quantitative comparison of DYNARTmo-generated trajectories against electromagnetic articulography (EMA) data for a corpus of German phonemes, measuring RMS error in articulator positions.
2. Implement the proposed mental syllabary lookup approach and evaluate whether it resolves the systematic errors in rule-based gesture score generation through a controlled comparison.
3. Test the model's behavior under extreme coarticulation conditions by creating artificially overlapping gesture configurations and measuring whether the weighted blending produces physiologically plausible trajectories.