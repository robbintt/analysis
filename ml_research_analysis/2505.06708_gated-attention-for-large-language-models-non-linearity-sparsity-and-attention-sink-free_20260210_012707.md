---
ver: rpa2
title: 'Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free'
arxiv_id: '2505.06708'
source_url: https://arxiv.org/abs/2505.06708
tags:
- gating
- attention
- arxiv
- sdpa
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates gating mechanisms in standard
  softmax attention, revealing their significant impact on performance, training stability,
  and attention dynamics. Through extensive experiments on 30 variants of 15B MoE
  and 1.7B dense models trained on up to 3.5T tokens, the authors demonstrate that
  applying a sigmoid gate after scaled dot-product attention yields the most substantial
  improvements.
---

# Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free

## Quick Facts
- arXiv ID: 2505.06708
- Source URL: https://arxiv.org/abs/2505.06708
- Authors: Zihan Qiu; Zekun Wang; Bo Zheng; Zeyu Huang; Kaiyue Wen; Songlin Yang; Rui Men; Le Yu; Fei Huang; Suozhi Huang; Dayiheng Liu; Jingren Zhou; Junyang Lin
- Reference count: 34
- Primary result: Applying sigmoid gating after scaled dot-product attention (G1 position) improves LLM performance, training stability, and enables context length extrapolation.

## Executive Summary
This paper systematically investigates gating mechanisms in standard softmax attention, revealing their significant impact on performance, training stability, and attention dynamics. Through extensive experiments on 30 variants of 15B MoE and 1.7B dense models trained on up to 3.5T tokens, the authors demonstrate that applying a sigmoid gate after scaled dot-product attention yields the most substantial improvements. This simple mechanism enhances non-linearity, introduces input-dependent sparsity, and eliminates inefficiencies like the "attention sink" phenomenon. Additionally, gating facilitates context length extension, allowing models to generalize effectively to longer sequences without retraining. The authors also release the first attention-sink-free models, highlighting the broad applicability and impact of gating mechanisms in scaling foundation models.

## Method Summary
The authors systematically explore gating mechanisms at five positions in transformer attention layers, with the best configuration being a head-specific sigmoid gate applied after scaled dot-product attention output (G1 position). The gating formula is Y' = Y ⊙ σ(XWθ), where the gate scores are computed from current query hidden states and multiplied elementwise with the attention output. Experiments span 15B MoE models (128 experts, top-8, GQA) and 1.7B dense models trained on 3.5T tokens with standard transformer architectures. Training uses AdamW optimizer with learning rates ranging from 2e-3 (MoE) to 4.5e-3 (dense), batch sizes of 1024-4096, and cosine learning rate decay.

## Key Results
- Sigmoid gating after SDPA output (G1) achieves the largest perplexity reduction and best performance across benchmarks
- Query-dependent sparse gating filters irrelevant context and correlates with improved performance
- Gating eliminates attention sinks, reducing first-token attention from 46.7% to 4.8% while decreasing max activations from 1053 to 94
- Gated models retain 58.82% accuracy at 128k context length vs 31.65% for baseline using YaRN extension
- Training stability improves significantly, allowing use of higher learning rates without loss spikes

## Why This Works (Mechanism)

### Mechanism 1: Non-linearity Breaks Low-Rank Bottleneck
Applying a sigmoid gate after SDPA introduces non-linearity between value projection ($W_V$) and output projection ($W_O$), increasing expressiveness beyond their composite low-rank mapping. In standard attention, $W_V$ and $W_O$ can be merged into a single low-rank linear transformation, limiting expressiveness. Gating at G1 or G2 inserts a non-linear operation between these matrices, enabling the model to represent more complex functions per head. The performance gain stems from breaking linearity rather than from parameter increase alone.

### Mechanism 2: Query-Dependent Sparse Gating Filters Irrelevant Context
Head-specific sigmoid gating after SDPA produces sparse, query-dependent gating scores that selectively suppress irrelevant contextual information. Gating scores are computed from current query hidden states, producing low mean values (~0.116 for SDPA elementwise vs. ~0.221 for value gating). This sparsity acts as a dynamic filter, reducing noise in the attention output. Head-specific gating captures head specialization, and input-independent gating improves baseline but underperforms query-dependent gating.

### Mechanism 3: Sparsity Eliminates Attention Sink and Reduces Massive Activations
Sparse, query-dependent SDPA gating eliminates the attention sink phenomenon and reduces massive activations, improving training stability and long-context extrapolation. Attention sinks arise from softmax normalization forcing non-negative weights that sum to 1, causing initial tokens to accumulate redundant attention. Sparse gating breaks this by allowing near-zero outputs, removing dependency on sink tokens. Reduced massive activations also decrease numerical errors in BF16 training.

## Foundational Learning

- **Concept: Softmax Attention and Low-Rank Structure**
  - Why needed here: Understanding that $W_V W_O$ forms a low-rank projection explains why non-linearity at G1/G2 improves expressiveness.
  - Quick check question: Why can multi-head attention outputs be rewritten as low-rank linear mappings over input tokens?

- **Concept: Attention Sink Phenomenon**
  - Why needed here: The paper's core claim is that gating eliminates attention sinks; you must understand what sinks are (disproportionate attention to initial tokens due to softmax normalization).
  - Quick check question: What causes the attention sink in standard softmax attention, and why does non-negative normalization contribute?

- **Concept: Query-Dependent vs. Input-Independent Gating**
  - Why needed here: The paper shows query-dependent gating outperforms input-independent gating; understanding this distinction clarifies why sparsity must be computed from current query states.
  - Quick check question: How does the gating score computation differ between SDPA output gating (G1) and value gating (G2), and why does query-dependence matter?

## Architecture Onboarding

- **Component map:**
  Input X → Q/K/V projections → (optional G2–G4 gates) → SDPA → **G1 gate (sigmoid, head-specific)** → Concat → Dense output ($W_O$) → (optional G5 gate)

- **Critical path:**
  1. Implement G1 elementwise sigmoid gating: gate scores computed from pre-norm hidden states, multiplied with SDPA output per-head.
  2. Use head-specific gating parameters (not shared across heads).
  3. Initialize gate parameters to avoid initial saturation (small or zero init works).

- **Design tradeoffs:**
  - Elementwise vs. headwise gating: elementwise adds ~201M params for 15B MoE; headwise adds ~1.6M with similar gains.
  - Sigmoid vs. SiLU: sigmoid produces sparse scores [0,1]; SiLU is unbounded and underperforms for multiplicative gating.
  - G1 vs. G2: G1 (post-SDPA) outperforms G2 (post-value) due to query-dependent sparsity; G2 still introduces non-linearity but with higher gating scores.

- **Failure signatures:**
  - Training divergence at high learning rates without gating (loss spikes).
  - Attention sink persists if gating is head-shared or non-sparse (NS-sigmoid).
  - Massive activations in early layers (FFN outputs) propagate through residuals if not controlled.

- **First 3 experiments:**
  1. **Ablate gating position:** Train identical models with gating at G1, G2, G3, G4, G5; measure PPL and MMLU. Expect G1 > G2 > others.
  2. **Sparsity intervention:** Compare sigmoid gating vs. NS-sigmoid (scores in [0.5, 1.0]) at G1; expect NS-sigmoid to degrade toward baseline.
  3. **Long-context extrapolation:** Train baseline and G1-gated models at 4k context, extend to 128k with YaRN; evaluate on RULER benchmark. Expect gated model to retain >50% accuracy at 128k.

## Open Questions the Paper Calls Out

- **How does the elimination of attention sinks theoretically influence a model's ability to generalize to longer sequences?**
  - The authors state they "do not provide a rigorous theoretical explanation for how attention sinks influence the model's ability to generalize to longer sequences" despite observing empirical gains. A theoretical framework linking the removal of attention sinks to length generalization is needed.

- **What are the broader implications of gating-induced non-linearity on the dynamics of attention and the overall training process?**
  - The Limitations section notes that "The broader implications of non-linearity on the dynamics of attention and the overall training process remain under-explored." Analysis of gradient flow and representation evolution during training to specifically isolate the effects of non-linearity from sparsity would help resolve this.

- **How do sparsity, massive activations, and attention sinks interact when scaling to deeper and larger models?**
  - Appendix A.3 calls for "further investigation... to fully understand the interplay between sparsity, massive activations, and attention sinks, particularly in the context of scaling to deeper and larger models." Layer-wise analysis of these phenomena in models with significantly increased depth or parameter counts is needed.

## Limitations

- The exact dataset composition and preprocessing pipeline for the 3.5T token training corpus was not fully specified, affecting reproducibility.
- While the paper demonstrates gating's benefits across multiple benchmarks, the ablation studies focus primarily on perplexity and attention patterns, with less systematic evaluation of downstream task generalization.
- The paper does not address potential computational overhead from gating mechanisms in inference scenarios or their impact on model latency.

## Confidence

- **High Confidence:** The core finding that sigmoid gating after SDPA output (G1 position) improves performance and training stability is well-supported by extensive experiments across 30+ model variants and two model sizes (15B MoE and 1.7B dense).
- **Medium Confidence:** The attribution of performance gains specifically to non-linearity breaking low-rank bottlenecks is plausible but not definitively proven, as the paper does not isolate non-linearity effects from sparsity effects in controlled experiments.
- **Medium Confidence:** The assertion that sparse gating is essential for attention sink elimination is partially supported, as value-gating (G2) reduces massive activations but does not eliminate attention sinks.

## Next Checks

1. **Sparsity Ablation Study:** Implement and compare three variants of G1 gating: (a) standard sigmoid gating with learned parameters, (b) fixed random gating (input-independent but parameter-learned), and (c) NS-sigmoid gating constrained to [0.5, 1.0] to eliminate sparsity. Train all three on the same dataset with identical hyperparameters and measure PPL, attention sink metrics, and training stability.

2. **Non-linearity Isolation Experiment:** Create a variant where gating is applied but the sigmoid activation is replaced with a linear transformation (identity or learned scalar multiplication) at the G1 position. Compare this against both baseline and standard sigmoid-gated models to confirm whether non-linearity is necessary for the observed benefits.

3. **Long-context Scaling Study:** Extend the YaRN extrapolation experiment by training gated and baseline models at 4k context, then systematically evaluating at multiple extended lengths (16k, 32k, 64k, 128k, 256k) on RULER and additional long-context benchmarks. Measure not just accuracy retention but also attention patterns, training stability, and computational efficiency at each length.