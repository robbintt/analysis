---
ver: rpa2
title: A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource
  Languages
arxiv_id: '2507.02428'
source_url: https://arxiv.org/abs/2507.02428
tags:
- speech
- data
- impaired
- akan
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a "cookbook" for community-driven data collection
  of impaired speech in low-resource languages, focusing on Akan in Ghana. The team
  created a 30+ hour dataset of impaired speech (cerebral palsy, stammering, cleft
  palate) using culturally relevant image prompts and in-person collection methods.
---

# A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages

## Quick Facts
- arXiv ID: 2507.02428
- Source URL: https://arxiv.org/abs/2507.02428
- Reference count: 0
- Developed a community-driven framework for collecting impaired speech data in Akan, achieving 21.7% WER reduction on adapted Whisper ASR models

## Executive Summary
This paper presents a practical "cookbook" for community-driven data collection of impaired speech in low-resource languages, focusing on Akan in Ghana. The team developed a 30+ hour dataset of impaired speech (cerebral palsy, stammering, cleft palate) using culturally relevant image prompts and in-person collection methods. They adapted Whisper ASR models for Akan and fine-tuned them on impaired speech data, achieving a 21.7% reduction in word error rate compared to the base model, though results varied significantly by speaker severity and audio quality.

## Method Summary
The study collected impaired speech data through community facilitators and speech-language therapists, using culturally relevant image prompts to elicit natural speech. Data was transcribed by linguists using a custom desktop application with double-blind cross-validation. The Whisper-small ASR model was adapted in two stages: first fine-tuned on standard Akan speech, then further fine-tuned on the impaired speech corpus. The approach included severity-aware data splitting and careful quality control measures.

## Key Results
- Achieved 21.7% median WER reduction compared to base model on impaired speech test set
- Image prompts elicited richer vocabulary (28.3% increase) but harder transcription compared to text prompts
- WER reduction varied significantly by severity: mild cases improved by 30.2% while severe cases showed only 0.4% improvement
- In-person data collection proved more effective than online methods for this population

## Why This Works (Mechanism)
The success stems from combining culturally relevant image prompts with in-person facilitation, which addresses both cognitive limitations of impaired speakers and technological barriers in low-resource settings. The two-stage fine-tuning approach leverages standard speech data to build a robust language model before adapting to impaired speech patterns. Severity-aware data splitting ensures the model learns from diverse impairment levels while maintaining evaluation integrity.

## Foundational Learning
- Community engagement in low-resource settings: Essential for recruitment and trust-building; verify by tracking participation rates and retention
- Culturally relevant prompting: Image prompts work better than text for impaired speakers in this context; check by comparing vocabulary diversity and transcription agreement
- In-person vs. online collection trade-offs: In-person yields higher quality but higher cost; validate by measuring participation rates and audio quality metrics
- Severity-aware data splitting: Critical for proper evaluation across impairment levels; confirm by checking speaker distribution in train/test splits
- Double-blind transcription validation: Necessary for quality control with evolving orthographies; assess by measuring inter-annotator agreement scores

## Architecture Onboarding

- Component map: Community Engagement (Recruitment) -> In-Person Data Collection (Facilitated) -> Quality Control & Transcription -> Severity Labeling -> ASR Model Fine-Tuning -> Evaluation (WER)
- Critical path: Community Engagement (Recruitment) -> In-Person Data Collection (Facilitated) -> Quality Control & Transcription -> Severity Labeling -> ASR Model Fine-Tuning -> Evaluation (WER)
- Design tradeoffs:
  - In-Person vs. Online Collection: High cost/slow (in-person) vs. Low cost/fast but low quality/participation (online). The paper concludes in-person is critical for this population.
  - Prompt Type: Text prompts simplify transcription (known target) but are hard for participants. Image prompts elicit natural speech but make transcription harder and more expensive.
  - Model Size: Using `whisper-small` (244M parameters) reduces computational cost but likely limits performance ceiling compared to larger models.
- Failure signatures:
  - High WER on adapted model: Likely caused by poor audio quality (background noise), inconsistent transcription (e.g., differing standards for code-switched words), or severe cognitive/linguistic impairments leading to non-standard syntax.
  - Poor participation in online collection: Device incompatibility, lack of digital literacy, or lack of real-time support.
  - Inconsistent transcriptions: Indicates lack of clear rules for the target language's orthography, especially for loanwords.
- First 3 experiments:
  1. Baseline & Akan-Only Adaptation: Establish baseline by evaluating multilingual Whisper on impaired speech test set, then fine-tune only on standard Akan. Measure WER to quantify language-specific adaptation benefit.
  2. Impaired Speech Fine-Tuning: Take Akan-adapted model and fine-tune on training split of impaired speech corpus. Measure per-speaker and aggregate WER reduction.
  3. Prompt Ablation (Data Analysis): Compare transcription difficulty, audio duration, and participant engagement between image vs. text prompts. Quantify "richness" (vocabulary diversity) and "correctness" (transcription agreement).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing dataset volume for impaired speech in low-resource languages yield relative WER reductions comparable to those observed in high-resource languages?
- Basis in paper: The authors "envisage that with large datasets... the WERR will improve," contrasting their 21.7% reduction with 31-39% reductions in English studies utilizing larger corpora.
- Why unresolved: The current pilot dataset is orders of magnitude smaller than benchmark English datasets, making it difficult to isolate model architecture limits from data scarcity issues.
- What evidence would resolve it: Fine-tuning results from a scaled-up data collection effort (e.g., hundreds of hours) using the same "cookbook" methodology.

### Open Question 2
- Question: Are culturally relevant image prompts universally more effective than text prompts for collecting impaired speech in low-resource settings?
- Basis in paper: The study found image prompts "more effective... contrary to similar studies [in English] where text prompts proved efficient."
- Why unresolved: It remains unclear if this preference is driven by specific cognitive challenges of cerebral palsy, literacy levels in the target population, or linguistic structure of Akan.
- What evidence would resolve it: A comparative study of prompt efficacy across multiple low-resource languages and distinct impairment types (e.g., stammering vs. cerebral palsy).

### Open Question 3
- Question: Can transcription consistency be achieved for impaired speech in languages with evolving orthographies and frequent code-switching?
- Basis in paper: The authors note that "double-blinded cross-validation identified frequent transcription differences" due to evolving writing rules and neologisms.
- Why unresolved: Standardized orthography rules for Akan are still developing, complicating creation of gold-standard transcriptions necessary for supervised ASR learning.
- What evidence would resolve it: Development and validation of formalized annotation guidelines that yield high inter-annotator agreement scores across diverse linguists.

## Limitations
- Severity distribution heavily skewed toward mild/moderate cases (88%), limiting model effectiveness for severe impairments where WER reductions were minimal
- Whisper-small model choice may have limited performance ceiling compared to larger architectures
- All empirical validation occurred on Akan in Ghana, with untested generalizability to other languages and impairment types

## Confidence
- High: In-person data collection with community facilitators produces higher-quality impaired speech data than online methods
- Medium: 21.7% WER reduction claim requires qualification due to dramatic variation by speaker severity (ranging from -3.2% to 30.2%)
- Low: Direct applicability to other low-resource languages and impairment types remains speculative and untested

## Next Checks
1. **Per-Speaker Audio Quality Analysis**: Analyze correlation between audio quality metrics (background noise, SNR) and WER for each speaker, particularly for those with negative WER improvements
2. **Cross-Linguistic Framework Validation**: Apply complete "cookbook" methodology to another low-resource language with different linguistic family to test generalizability
3. **Model Architecture Scaling Study**: Compare Whisper-small against Whisper-medium and Whisper-large on same impaired speech dataset to quantify performance tradeoff between computational efficiency and accuracy