---
ver: rpa2
title: 'COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models
  under Distribution Shifts'
arxiv_id: '2504.10158'
source_url: https://arxiv.org/abs/2504.10158
tags:
- object
- arxiv
- visual
- generalization
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COUNTS, a large-scale dataset with fine-grained
  annotations designed to benchmark object detectors and multimodal large language
  models (MLLMs) under distribution shifts. COUNTS comprises 14 domains, 35 object
  categories, 222,234 images, and 1,196,114 bounding boxes, enabling the evaluation
  of out-of-distribution (OOD) generalization capabilities.
---

# COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts

## Quick Facts
- arXiv ID: 2504.10158
- Source URL: https://arxiv.org/abs/2504.10158
- Reference count: 40
- Introduces COUNTS dataset with 14 domains, 35 object categories, 222,234 images, and 1,196,114 bounding boxes for evaluating OOD generalization

## Executive Summary
This paper introduces COUNTS, a large-scale dataset with fine-grained annotations designed to benchmark object detectors and multimodal large language models (MLLMs) under distribution shifts. COUNTS comprises 14 domains, 35 object categories, 222,234 images, and 1,196,114 bounding boxes, enabling the evaluation of out-of-distribution (OOD) generalization capabilities. Two benchmarks are proposed: O(OD)2 for object detectors and OODG for MLLMs. Experiments reveal that existing object detectors and MLLMs struggle with OOD generalization, even when performing well in in-distribution scenarios. For example, in visual grounding tasks, GPT-4o and Gemini-1.5 achieve only 56.7% and 28.0% accuracy, respectively. The findings highlight the need for more robust models and provide insights into the challenges of OOD generalization in complex visual tasks.

## Method Summary
The paper presents COUNTS, a comprehensive dataset designed to evaluate object detectors and multimodal large language models under distribution shifts. The dataset includes 14 domains and 35 object categories with extensive fine-grained annotations. Two specific benchmarks are introduced: O(OD)2 for object detectors and OODG for MLLMs. The evaluation focuses on out-of-distribution generalization capabilities by testing models across different domains while maintaining consistent object categories. Visual grounding tasks are used to assess MLLM performance, with detailed accuracy measurements provided for leading models like GPT-4o and Gemini-1.5.

## Key Results
- Object detectors and MLLMs show significant performance degradation under distribution shifts, even when achieving high accuracy in in-distribution scenarios
- GPT-4o achieves only 56.7% accuracy on visual grounding tasks in OOD settings
- Gemini-1.5 achieves only 28.0% accuracy on the same visual grounding tasks
- The benchmark reveals fundamental limitations in current models' ability to generalize across domains

## Why This Works (Mechanism)
The COUNTS benchmark works by systematically evaluating models across multiple domains with consistent object categories, exposing their limitations in handling distribution shifts. By providing fine-grained annotations across 14 domains and 35 categories, the benchmark creates controlled conditions to measure OOD generalization performance. The visual grounding tasks specifically test whether models can correctly identify objects in novel contexts while maintaining semantic understanding.

## Foundational Learning
- **Distribution shift**: Why needed - to test model robustness beyond training data; Quick check - compare performance across domains
- **Out-of-distribution generalization**: Why needed - real-world applications require handling unseen scenarios; Quick check - measure accuracy drop when domain changes
- **Fine-grained annotation**: Why needed - enables precise evaluation of object detection and localization; Quick check - verify bounding box quality and category consistency
- **Visual grounding**: Why needed - tests combined object detection and language understanding; Quick check - validate accuracy on text-to-image queries
- **Multimodal evaluation**: Why needed - assesses integration of visual and language capabilities; Quick check - compare performance on vision-only vs. multimodal tasks

## Architecture Onboarding
- **Component map**: Data Collection -> Annotation Pipeline -> Benchmark Construction -> Model Evaluation
- **Critical path**: The annotation pipeline and benchmark construction phases are most critical for ensuring reliable evaluation
- **Design tradeoffs**: Large-scale data collection vs. annotation quality, domain diversity vs. category consistency
- **Failure signatures**: Sharp performance drops across domains, inconsistent bounding box quality, language bias in annotations
- **First experiments**: 1) Validate annotation consistency across domains, 2) Test baseline model performance in-ID, 3) Measure OOD generalization gap

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition may not fully capture real-world diversity, particularly outside English-centric contexts
- Visual grounding results may be influenced by uncontrolled experimental factors
- Findings are based on specific models and tasks, limiting generalizability to all multimodal AI systems

## Confidence
- Dataset comprehensiveness: Medium
- Evaluation methodology: Medium
- Model performance claims: Medium
- Generalizability of findings: Low to Medium

## Next Checks
1. Test the benchmark across additional languages and cultural contexts to verify the robustness of findings beyond English-centric scenarios
2. Evaluate whether different model architectures or training approaches can overcome the observed OOD generalization limitations
3. Validate the benchmark's effectiveness by having independent researchers reproduce the main findings using the same dataset and protocols