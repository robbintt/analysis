---
ver: rpa2
title: 'InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes
  with Movable Objects'
arxiv_id: '2509.23612'
source_url: https://arxiv.org/abs/2509.23612
tags:
- object
- interaction
- scene
- objects
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating text-controlled
  human-object interactions in 3D scenes with movable objects. The proposed method,
  InteractMove, constructs a novel dataset by aligning existing human-object interaction
  data with 3D scenes, featuring diverse movable objects, same-category distractors,
  and physically plausible trajectories.
---

# InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes with Movable Objects

## Quick Facts
- **arXiv ID**: 2509.23612
- **Source URL**: https://arxiv.org/abs/2509.23612
- **Reference count**: 38
- **Primary result**: Text-controlled human-object interaction generation in 3D scenes with movable objects, achieving 0.791 goal distance, 0.813 physical realism, and 98.36% non-collision scores

## Executive Summary
This paper introduces InteractMove, a method for generating text-controlled human-object interactions in 3D scenes containing movable objects. The approach constructs a novel dataset by aligning existing human-object interaction data with 3D scenes, featuring diverse movable objects, same-category distractors, and physically plausible trajectories. The core method involves three key stages: 3D object grounding to identify the target object from text, hand-object joint affordance learning to predict contact regions for accurate grasping, and collision-aware motion generation to ensure physically plausible interactions. Experiments demonstrate that InteractMove outperforms existing methods in generating realistic, text-aligned interactions while maintaining physical plausibility and collision avoidance.

## Method Summary
InteractMove constructs a novel dataset by aligning existing human-object interaction data from BEHAVE and GRAB with 3D scenes from ScanNet. The method operates in three stages: (1) 3D object grounding uses a pre-trained model like ZSVG3D to locate the target object from text and scene, (2) hand-object joint affordance learning employs a diffusion model to predict contact regions between hand joints and object surfaces, and (3) collision-aware motion generation uses another diffusion model conditioned on local scene voxelization and affordance predictions to generate collision-free human motion and object trajectories. The motion generation stage incorporates contact and penetration losses, with test-time penetration constraints during inference to ensure collision-free interactions.

## Key Results
- Achieves 0.791 goal distance, outperforming baselines in task completion accuracy
- Scores 0.813 on physical realism, demonstrating natural interaction generation
- Achieves 98.36% non-collision score, effectively avoiding scene penetrations
- Ablation studies confirm the importance of all three stages, with grounding module removal increasing goal distance to 1.545

## Why This Works (Mechanism)

### Mechanism 1: Hand-Object Joint Affordance Learning
Fine-grained affordance prediction between hand joints and object surfaces enables diverse, object-aware grasping strategies. Given object point cloud and text, compute distance map between object points, hand joints, and frames. Normalize via exponential distance weighting, threshold with a parameter, and denoise using a diffusion model conditioned on PointNet object features and text embeddings via cross-attention. Objects of different sizes/shapes require distinct hand-joint contact patterns (e.g., one-hand vs. two-hand), which can be learned from aligned HOI data. If object geometries deviate significantly from training distribution, affordance predictions may misguide contact, leading to failed grasps.

### Mechanism 2: Collision-Aware Motion Generation with Local Scene Voxelization
Voxelizing the local scene around the target object combined with collision-aware loss produces collision-free trajectories in cluttered environments. Extract occupancy grid, pool along z-axis for 2D patches, encode with ViT for local scene features. Apply contact loss encouraging hand-object proximity and penetration loss penalizing interpenetration. At inference, apply test-time penetration constraint to gradient-guide denoising. Local scene geometry is the primary collision risk; distant scene elements are less relevant for short-range manipulation. If object trajectory requires long-distance navigation through narrow passages, local-only voxelization may miss distant collision risks.

### Mechanism 3: 3D Visual Grounding for Object Disambiguation
Pre-trained 3D grounding models can identify target objects among same-category distractors given spatial language descriptions. Use ZSVG3D (or similar) to locate target object point cloud from text and scene. Grounding model parses spatial relations (e.g., "on the desk near the bed") to select correct instance among multiple candidates. Spatial language descriptions in the dataset align with grounding model's pre-training distribution. If text descriptions are ambiguous or grounding model fails on novel object categories/scene layouts, downstream affordance and motion modules receive incorrect object, causing cascading failures.

## Foundational Learning

- **Diffusion Models (DDPM)**: Core generative backbone for both affordance and motion synthesis; requires understanding forward/reverse processes, noise schedules, and conditional denoising. Quick check: Can you explain how Eq. (1-4) define the forward/reverse diffusion process and how conditioning is injected?
- **Point Cloud Processing (PointNet)**: Object features are extracted from point clouds; need to understand permutation-invariant architectures for 3D data. Quick check: How does PointNet achieve permutation invariance, and why is it suitable for object-level feature extraction?
- **3D Scene Representations (Voxelization, Occupancy Grids)**: Local scene modeling requires converting meshes/point clouds to occupancy grids; understanding spatial discretization tradeoffs is essential. Quick check: What determines the resolution of voxelization, and how does it affect collision detection accuracy vs. computational cost?

## Architecture Onboarding

- **Component map**: Text + Scene → 3D Grounding Model (ZSVG3D) → Target Object Point Cloud → Affordance Diffusion Module → Hand-Object Joint Affordance → Motion Diffusion Module (with local scene features) → Human Motion + Object Trajectory
- **Critical path**: Grounding failure cascades to all downstream modules. First validate grounding accuracy on held-out scenes with distractors before training affordance/motion modules.
- **Design tradeoffs**:
  - Multimodality vs. Collision Safety: Collision constraints reduce diversity; tune weights based on application requirements
  - Local vs. Global Scene Modeling: Only local region around object is voxelized; long-range collisions may be missed but computational cost is reduced
  - Affordance Resolution: Finer affordance improves grasp accuracy but increases memory/compute
- **Failure signatures**:
  - High Goal Distance (>1.0): Likely grounding failure—check if text descriptions match grounding model's expected format
  - Low Physical Realism (<0.6): Affordance module may be mispredicting contact regions; visualize affordance heatmaps on object surface
  - Non-collision Score <90%: Local scene features may be insufficient; increase voxel resolution or expand local region
- **First 3 experiments**:
  1. Grounding validation: Run grounding module on 100 held-out scene-text pairs with same-category distractors; measure accuracy and analyze failure cases
  2. Affordance visualization: For 5 diverse objects, visualize predicted affordance maps on object surface; verify one-hand vs. two-hand differentiation
  3. Collision ablation: Train motion module with/without collision losses on subset of data; replicate Table 5 trends to validate loss contribution

## Open Questions the Paper Calls Out

- **Can the framework generalize to object geometries or categories not present in the source BEHAVE and GRAB datasets?**: The paper does not evaluate zero-shot performance on novel object categories or diverse geometries outside the 71 categories used for alignment. Quantitative results on held-out object categories would resolve this.
- **How robust is the pipeline to failures in the 3D object grounding stage?**: The paper evaluates final motion quality assuming correct grounding but does not analyze error propagation if the grounding model misidentifies the target object. An analysis of end-to-end performance degradation would resolve this.
- **Does the motion alignment process fully preserve the physical plausibility and interaction style of the original motion data?**: While quantitative metrics show improvement over forced alignment, it is unclear if the refined motions retain the subtle dynamics of the original ground-truth motions. A user study comparing refined motions against source ground-truth would resolve this.

## Limitations
- Dataset Alignment Dependency: The method relies on constructing a new dataset by aligning existing HOI data with 3D scenes, which may introduce artifacts or limit generalizability
- Local Scene Modeling Trade-off: Using only local voxelization around the target object improves efficiency but may miss long-range collision risks in complex scenes

## Confidence
- **High Confidence**: Collision-aware motion generation with local voxelization (validated by ablation showing Non-collision Score improvement from 80.24% to 98.36%)
- **Medium Confidence**: Hand-object joint affordance learning (mechanism described but lacks direct quantitative validation of joint-level contact accuracy)
- **Medium Confidence**: 3D visual grounding for object disambiguation (critical for pipeline but neighbor papers don't evaluate this specific disambiguation task)

## Next Checks
1. Grounding Robustness Test: Evaluate the 3D grounding module on 100 held-out scene-text pairs with same-category distractors, measuring accuracy and analyzing failure modes
2. Affordance Resolution Analysis: For 5 diverse objects, visualize predicted affordance maps on object surface to verify one-hand vs. two-hand differentiation aligns with object size
3. Long-Range Collision Validation: Create test scenes with narrow passages requiring long-distance navigation; measure whether local-only voxelization causes collision failures compared to global scene awareness approaches