---
ver: rpa2
title: 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model'
arxiv_id: '2509.00676'
source_url: https://arxiv.org/abs/2509.00676
tags:
- critic
- reasoning
- training
- llava-critic-r1
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work challenges the standard separation between critic models
  (which evaluate responses) and policy models (which generate them) in vision-language
  modeling. The authors propose performing reinforcement learning directly on a base
  generative model using preference-labeled critic datasets, producing LLaVA-Critic-R1
  - a model that optimizes for preference judgments while retaining generation ability.
---

# LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model

## Quick Facts
- arXiv ID: 2509.00676
- Source URL: https://arxiv.org/abs/2509.00676
- Reference count: 27
- Primary result: Training critic models on preference data produces strong policy models, achieving SOTA at 7B scale

## Executive Summary
This work challenges the standard separation between critic models (which evaluate responses) and policy models (which generate them) in vision-language modeling. The authors propose performing reinforcement learning directly on a base generative model using preference-labeled critic datasets, producing LLaVA-Critic-R1 - a model that optimizes for preference judgments while retaining generation ability. Surprisingly, LLaVA-Critic-R1 not only becomes a top-performing critic but also emerges as a competitive policy model, matching or surpassing specialized reasoning VLMs across 26 visual reasoning and understanding benchmarks with an average gain of +5.7% over its base model. Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, achieving state-of-the-art performance of 71.9 on MMMU at the 7B scale. Additionally, the enhanced critic capability enables effective test-time scaling through self-critique, yielding an average +13.8% improvement on five representative reasoning tasks without additional training.

## Method Summary
The authors train vision-language models using reinforcement learning on preference-labeled critic datasets, without knowledge distillation. They use GRPO with dual rewards (α=0.9 preference, 0.1 format) to optimize for both preference judgment accuracy and structured reasoning output format. The model is trained on 40K pairwise instances (image, question, response_1, response_2, preference_label) stripped of GPT rationales. This approach produces a unified model that excels at both critic evaluation and policy generation, with the format reward enforcing a "think-then-answer" reasoning structure that transfers to downstream tasks.

## Key Results
- LLaVA-Critic-R1 achieves +5.7% average improvement over base model across 26 visual reasoning and understanding benchmarks
- LLaVA-Critic-R1+ achieves state-of-the-art 71.9 on MMMU at the 7B scale
- Self-critic test-time scaling yields +13.8% average improvement on five reasoning tasks through Best-of-N selection
- The model maintains strong critic capability (68.1 RewardBench score) while matching or surpassing specialized reasoning VLMs

## Why This Works (Mechanism)

### Mechanism 1: Verifiable Preference Signal with Distillation-Free RL
Reformulating preference data into a verifiable binary classification task enables RL training without knowledge distillation bias, allowing self-derived reasoning emergence. Strip GPT rationales, keep only (image, question, response_1, response_2, preference_label). Model autonomously determines which response better aligns with visual evidence using GRPO reward = α × r_pref + (1-α) × r_format where r_pref = 1 if prediction matches ground-truth preference, else 0.

### Mechanism 2: Format-Enforced Structured Reasoning
The format reward component (r_format = 1 if output uses ` AndAlso...` and `\boxed{}` tokens, else 0) enforces "think-then-answer" behavior that transfers to downstream reasoning tasks. GRPO with α=0.9 preference weight and 0.1 format weight forces internal deliberation before committing to an answer, creating a reasoning scaffold that generalizes beyond critic tasks.

### Mechanism 3: Perception-Critic Co-Training Effect
Training on pairwise response comparison enhances visual perception because the model must identify hallucinations and grounding errors to determine preference. Critic data includes response pairs from diverse VLMs on captioning/VQA tasks. To correctly judge preference, the model must verify visual grounding of each response against the image, implicitly training robust perception.

### Mechanism 4: Self-Critic Test-Time Scaling
Unified critic-policy capability enables Best-of-N selection where the model evaluates its own generations, improving output quality without additional training. Generate n candidate responses (temperature=0.9 for diversity), then perform recursive pairwise elimination using the model's critic capability until one response remains.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Why needed - Core training algorithm replacing PPO; estimates advantage via group statistics rather than critic network. Quick check - How does GRPO differ from PPO in advantage estimation, and why might this reduce variance for critic training?
- **Preference Learning / Bradley-Terry Modeling**: Why needed - Understanding pairwise preference data formulation and why it's verifiable (unlike scalar rewards). Quick check - Why is a binary preference judgment more suitable for RL than pointwise score prediction?
- **Chain-of-Thought Reasoning in VLMs**: Why needed - The format reward enforces CoT structure; need to understand why structured deliberation improves multi-step visual reasoning. Quick check - What is the "think-then-answer" pattern, and what failure modes occur if format is enforced without content understanding?

## Architecture Onboarding

- **Component map**: Qwen-2.5-VL-7B (vision encoder + LLM with projection) -> 40K pairwise critic data -> GRPO training with dual rewards -> inference with format template
- **Critical path**: 1) Data preparation: Strip rationales, format as preference task with verifiable output 2) GRPO training: ~400 steps monitored; policy peaks ~step 350, critic continues improving 3) Checkpoint selection: Balance policy vs. critic capability based on target use case
- **Design tradeoffs**: Critic-only training (LLaVA-Critic-R1) vs. Policy-then-critic (LLaVA-Critic-R1+) vs. cold-start RFT vs. SFT-then-RFT; 40K critic-only vs. mixing with 70K policy data; mixed training underperforms both specialists
- **Failure signatures**: Policy training on reasoning VLMs destroys agent capabilities (OSWorld: all VL reasoning models score 0.0); over-training: Policy degrades after ~step 350 while critic fluctuates; self-critic ceiling: Large gap to GT oracle indicates critic cannot always identify correct answer
- **First 3 experiments**: 1) Ablate format reward weight: Train with α ∈ {1.0, 0.9, 0.5, 0.0} to isolate format contribution 2) Scale training data: Test 10K, 20K, 40K, 80K critic samples 3) Test-time scaling analysis: Measure self-critic vs. majority vote vs. external-critic at n ∈ {2, 4, 8, 16, 32, 64, 128}

## Open Questions the Paper Calls Out

### Open Question 1
What mechanisms cause the divergence between critic and policy performance in later training stages, and can it be prevented? The paper identifies that after ~200 steps, "this strong correlation appears to weaken" with policy degrading while critic fluctuates, but only speculates about causes like overfitting without testing interventions.

### Open Question 2
How can the substantial gap between self-critic performance and the ground-truth oracle upper bound be closed? Current self-critic achieves only 78.9 on MathVista vs. 94.8 oracle; the paper states "the current critic ability remains far from optimal" but does not propose methods to bridge this gap.

### Open Question 3
Do critic-training benefits for policy capability scale to larger model sizes (e.g., 70B+)? All experiments are at the 7B scale; the paper claims "7B-level state-of-the-art" but does not test whether the unified critic-policy advantage persists or diminishes at larger scales.

### Open Question 4
What is the optimal tie-breaking strategy when the critic assigns equal preference to candidate responses? The paper uses random selection when ties occur but provides no analysis of impact or alternatives like confidence-based selection.

## Limitations
- Critical pathway dependence: Model cannot bootstrap from first principles if preference signal is noisy or contains systematic biases
- Architecture constraints: Approach relies on base model with sufficient capacity and pre-training to benefit from RL
- Task type limitations: Format enforcement works for visual reasoning but may be counterproductive for open-ended generation tasks

## Confidence
- **High confidence**: Fundamental observation that critic models can be trained as effective policy models; +5.7% average improvement across 26 benchmarks; effectiveness of format-enforced reasoning for visual tasks
- **Medium confidence**: Superiority of cold-start RFT over SFT-then-RFT for policy performance; mutual reinforcement between perception and reasoning capabilities; test-time scaling gains from self-critic selection
- **Low confidence**: Exact optimal weighting of preference versus format rewards; generalization of format-enforced reasoning to non-visual tasks; scalability of self-critic approach to much larger model families

## Next Checks
1. **Data quality impact analysis**: Systematically vary noise level and bias in preference labels to quantify minimum data quality threshold required for self-derived reasoning to emerge
2. **Architecture transfer study**: Apply same training methodology to different base model families (non-Qwen, different scales, different pretraining objectives) to determine whether critic-policy unification is architecture-dependent
3. **Format transfer evaluation**: Test whether format-enforced reasoning transfers to completely different reasoning domains (mathematical reasoning without visual input, logical deduction, multi-hop reasoning) to assess generality of "think-then-answer" scaffold