---
ver: rpa2
title: "Why Adam Works Better with $\u03B2_1 = \u03B2_2$: The Missing Gradient Scale\
  \ Invariance Principle"
arxiv_id: '2601.21739'
source_url: https://arxiv.org/abs/2601.21739
tags:
- adam
- gradient
- scale
- update
- invariance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explains why Adam performs better when \u03B2\u2081\
  \ = \u03B2\u2082. The authors formalize \"gradient scale invariance\" and prove\
  \ that Adam becomes first-order gradient scale invariant if and only if \u03B2\u2081\
  \ = \u03B2\u2082."
---

# Why Adam Works Better with $β_1 = β_2$: The Missing Gradient Scale Invariance Principle

## Quick Facts
- **arXiv ID**: 2601.21739
- **Source URL**: https://arxiv.org/abs/2601.21739
- **Reference count**: 40
- **Key outcome**: Adam becomes first-order gradient scale invariant if and only if β₁ = β₂, leading to smoother, more stable updates.

## Executive Summary
This paper provides a theoretical explanation for why Adam performs better when β₁ = β₂. The authors formalize the concept of "gradient scale invariance" and prove that Adam achieves first-order gradient scale invariance if and only if the momentum parameters are equal. This means Adam's updates become insensitive to gradient magnitude (relying on direction only) when β₁ = β₂. The theory is validated experimentally across vision and language tasks, showing statistically significant reductions in update oscillations and smoother update norms when β₁ = β₂. This structural explanation aligns balanced Adam with modern optimizers designed for scale-robust updates.

## Method Summary
The authors formalize gradient scale invariance (GSI) and prove that Adam exhibits first-order GSI if and only if β₁ = β₂. They analyze the Adam update ratio R(t) = m(t)/√v(t) using continuous-time EMA dynamics, showing that when τ₁ = τ₂ (equivalently β₁ = β₂), first-order terms proportional to gradient scale drift cancel exactly. The empirical validation uses a 3×3 grid of β₁, β₂ values across six tasks (NanoGPT, EfficientNet-B0, ResNet18, ViT-B16, T5), measuring oscillation in the update norm ∥Rₖ∥. Training uses sinusoidal initialization, linear warmup with cosine annealing, and standard AdamW hyperparameters with variations by task.

## Key Results
- Adam achieves first-order gradient scale invariance if and only if β₁ = β₂
- When β₁ = β₂, the norm of Adam's updates becomes smoother with statistically significant reductions in oscillation (binomial test p-values as low as 5.08×10⁻⁵)
- The balanced regime aligns Adam structurally with modern scale-robust optimizers like Lion, Muon, and Scion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adam achieves first-order gradient scale invariance (GSI) if and only if β₁ = β₂
- Mechanism: The update ratio R(t) = m(t)/√v(t) has a first-order term proportional to (τ₂ - τ₁)δ(t), where δ(t) is the logarithmic scale drift. When τ₁ = τ₂, this linear dependence on gradient scale cancels exactly, leaving only second-order O(Λ² + Λ′) residuals.
- Core assumption: The gradient varies smoothly with small logarithmic drift δ(t) on time scales comparable to τ₁ and τ₂; initialization transients have decayed (t - t₀ ≫ max{τ₁, τ₂}).
- Evidence anchors:
  - [abstract] "We formalize this notion and prove that Adam becomes gradient scale invariant of first order if and only if β₁ = β₂."
  - [section 3.4, Corollary 3.6] "The Adam flow is gradient scale invariant of first order if and only if τ₁ = τ₂. In discrete time, this corresponds to β₁ = β₂."
  - [corpus] Related work (Ma et al. 2022, Kunstner et al. 2023) observes sign-like behavior and smoother dynamics when β₁ ≈ β₂, consistent with—but not proving—GSI.
- Break condition: If gradients exhibit large, rapid scale changes (Λ large) or training is too short for transients to decay, the first-order expansion may not dominate.

### Mechanism 2
- Claim: First-order GSI manifests as reduced oscillations in the update norm ∥Rₖ∥ during training
- Mechanism: When scale invariance holds, ∥Rₖ∥ is driven primarily by gradient direction rather than magnitude fluctuations. The update norm becomes smoother, reducing step-to-step variability (oscillation metric ω).
- Core assumption: The oscillation metric (average absolute change in smoothed ∥Rₖ∥) captures the salient stability differences; smoothing window is appropriately chosen.
- Evidence anchors:
  - [abstract] "The norm of Adam's update becomes smoother and more stable when β₁ = β₂, with statistically significant reductions in update oscillations."
  - [section 4, Table 1] Diagonal β₁ = β₂ consistently minimizes ω across all 6 experiments; binomial test p-values strongly favor the diagonal (e.g., 5.08×10⁻⁵ for NanoGPT).
  - [corpus] Orvieto & Gower (2025) report improved validation and qualitative behavior when β₁ = β₂, but do not isolate update-norm oscillation or formalize GSI.
- Break condition: If stochastic noise dominates (very small smoothing windows), ω may not reflect underlying scale sensitivity; very small batch sizes or pathological loss surfaces may amplify other instabilities.

### Mechanism 3
- Claim: Balanced Adam aligns structurally with modern scale-robust optimizers (Lion, Muon, Scion) that explicitly enforce normalization
- Mechanism: These optimizers decouple update direction from magnitude by design. First-order GSI means balanced Adam approximates this property without explicit normalization, inheriting similar stability characteristics from its internal dynamics.
- Core assumption: Scale robustness is a desirable design principle; the theoretical connection translates to practical performance similarities.
- Evidence anchors:
  - [abstract] "This perspective places the balanced regime of Adam in direct alignment with the design principles underlying several recent optimizers that explicitly enforce scale-robust updates."
  - [section 2] "Much of the current state-of-the-art optimizers either depend quite intimately on Adam... or is explicitly built around normalization mechanisms that reduce the sensitivity of the update to gradient scale."
  - [corpus] Related optimizers (Lion, Scion, Muon) are discussed as normalization-focused, but no direct comparative experiments are provided in this paper.
- Break condition: If task-specific factors (e.g., sparse gradients, heavy-tailed noise) dominate optimization behavior, scale invariance may not be the primary performance driver.

## Foundational Learning
- Concept: Exponential Moving Average (EMA) dynamics and relaxation time τ
  - Why needed here: Understanding how m(t) and v(t) track g(t) and g(t)² with lag proportional to τ₁ and τ₂ is essential to see why equal time scales cancel first-order drift.
  - Quick check question: If β = e^{-Δt/τ} with β = 0.9 and Δt = 1, what is the approximate relaxation time τ?
- Concept: Logarithmic scale drift δ(t) = d/dt log|g(t)|
  - Why needed here: This quantifies the local rate of gradient magnitude change; GSI analysis expands R(t) in powers of δ(t).
  - Quick check question: If ∥g(t)∥ doubles over 100 steps, what is the approximate average δ(t) (assuming continuous time)?
- Concept: First-order Taylor expansion and error bounds O(Λ² + Λ′)
  - Why needed here: The proof relies on perturbative expansions where Λ bounds δ(t) and Λ′ bounds δ′(t); understanding these orders clarifies when approximations hold.
  - Quick check question: If δ(t) fluctuates around zero with amplitude 0.1, what order is the residual error in the GSI expansion?

## Architecture Onboarding
- Component map: Adam maintains two EMAs: mₖ (first moment, β₁) and vₖ (second moment, β₂). Update: θ_{k+1} = θₖ - η · mₖ₊₁ / (√vₖ₊₁ + ε). β₁ = β₂ corresponds to τ₁ = τ₂ in the continuous flow.
- Critical path:
  1. Choose β₁ = β₂ = β (e.g., 0.9, 0.95, 0.99).
  2. Verify update-norm ∥Rₖ∥ is stable (low oscillation ω) early in training.
  3. Compare validation curves vs. standard (β₁=0.9, β₂=0.999) baseline.
- Design tradeoffs:
  - Higher β (closer to 1): longer effective memory, slower adaptation to gradient changes, potentially smoother updates.
  - Lower β: faster response, but potentially noisier updates and reduced GSI effectiveness.
  - The paper does not conclusively recommend a specific β value; empirical tuning is still required.
- Failure signatures:
  - Exploding ∥Rₖ∥ or NaN losses when β₁ > β₂ (Figure 3 shows explosive behavior in some off-diagonal cases).
  - No improvement or degradation if gradient scale is already stable (GSI adds little).
  - Large Λ (rapid gradient scale changes) violates perturbation assumptions.
- First 3 experiments:
  1. **Synthetic validation**: Create a 1D gradient signal with known multiplicative scale changes; plot ∥Rₖ∥ for β₁ = β₂ vs. β₁ ≠ β₂. Verify the diagonal yields stable ∥Rₖ∥ (replicate Figure 1 vs. Figure 2).
  2. **Oscillation sweep on a small model**: Train ResNet18 on CIFAR-10 with a 3×3 grid of (β₁, β₂) ∈ {0.9, 0.99, 0.999}². Compute ω for each and confirm the diagonal minimizes oscillation (replicate Table 1 pattern).
  3. **Validation comparison on a language model**: Train NanoGPT on WikiText-2 with β₁ = β₂ = 0.95 vs. standard (β₁=0.9, β₂=0.999). Track both loss and ω to assess whether smoother updates correlate with better final validation metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical claims assume smooth gradient scale drift, which may not hold for all real-world training scenarios or when training runs are too short.
- The oscillation metric ω is sensitive to EMA window size; inappropriate smoothing parameters could mask or exaggerate the GSI effect.
- The claimed alignment with modern scale-robust optimizers is asserted but not experimentally validated through direct comparisons.

## Confidence
- **High Confidence**: The formal proof that β₁ = β₂ is necessary and sufficient for first-order gradient scale invariance; the empirical correlation between balanced β and reduced update oscillations.
- **Medium Confidence**: The practical performance gains from balanced Adam will generalize beyond tested tasks; the optimal β value for a given problem remains unspecified.
- **Low Confidence**: The direct practical equivalence between balanced Adam and explicit normalization optimizers without further experimental comparison.

## Next Checks
1. **Scale-drift sensitivity test**: Create synthetic training with abrupt gradient scale changes (e.g., step changes every 100 steps) to measure when the GSI advantage of β₁ = β₂ disappears.
2. **Optimizer equivalence benchmark**: Compare balanced Adam (β₁ = β₂ = 0.95) against a normalization-based optimizer (e.g., sign(SGD) or Lion) on CIFAR-10 ResNet18, measuring both validation and update smoothness.
3. **Perturbation regime stress test**: Systematically vary gradient scale noise amplitude and track when the first-order GSI approximation breaks down (e.g., when ∥Rₖ∥ becomes unstable for β₁ = β₂).