---
ver: rpa2
title: Don't Forget Imagination!
arxiv_id: '2508.06062'
source_url: https://arxiv.org/abs/2508.06062
tags:
- imagination
- semantic
- causal
- reasoning
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a call for greater attention to cognitive\
  \ imagination as a key component of human thinking that is largely absent from current\
  \ AI systems. The authors argue that cognitive imagination\u2014the ability to mentally\
  \ visualize coherent systems of concepts and causal links\u2014serves as the semantic\
  \ context for reasoning, decision making, and prediction, and that reasoning without\
  \ imagination is blind."
---

# Don't Forget Imagination!

## Quick Facts
- arXiv ID: 2508.06062
- Source URL: https://arxiv.org/abs/2508.06062
- Reference count: 30
- This paper presents semantic models as a new approach to mathematical models that can learn like neural networks while being based on probabilistic causal relationships.

## Executive Summary
This paper argues that cognitive imagination—the ability to mentally visualize coherent systems of concepts and causal links—is largely absent from current AI systems and serves as the semantic context for reasoning, decision making, and prediction. The authors propose semantic models as a hybrid system combining object ontologies with logic-probabilistic inference to address this gap. These models ensure consistency of imaginary contexts and implement a glass-box approach that allows context to be manipulated as a holistic and coherent system of interrelated facts glued together with causal relations.

## Method Summary
The method involves creating a two-tier hybrid system: a Factual Model (ontology) that stores deterministic object properties and relations, and a Causal Model (probabilistic rules) that extracts regularities from this data. The system calculates conditional probabilities for candidate causal rules and filters for "maximally specific" rules that guarantee consistent predictions. Learning involves populating the factual model with instance data, running semantic machine learning to extract probabilistic laws with high specificity, and validating that no contradictory rules survive filtering. The approach solves statistical ambiguity by using maximally specific causal relations that prevent contradictory predictions.

## Key Results
- Semantic models provide a glass-box approach to machine learning that maintains interpretability through ontology-based rule representation
- The two-tier architecture (factual + causal models) enables consistent reasoning without the statistical ambiguity found in other probabilistic inference methods
- The system can discover new concepts and strategies while supporting workflow control and maintaining trust through interpretable explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maximally specific causal relations can prevent contradictory predictions in probabilistic inference systems.
- Mechan: The system restricts learned causal rules to those that are "maximally specific"—rules that have no redundant conditions and maximal conditional probability. This constraint eliminates statistical ambiguity where both F and ¬F would otherwise be derived.
- Core assumption: Prediction consistency requires formal guarantees on rule specificity, not just high probability thresholds.
- Evidence anchors: Section 6 proves predictions based on these causal relations are consistent and cannot infer both F and ¬F simultaneously; references Hempel's maximally specific statistical laws as theoretical foundation.
- Break condition: If rule discovery allows overlapping conditions with similar probabilities, contradictions may re-emerge; system requires strict filtering on rule generality.

### Mechanism 2
- Claim: Two-tier factual/causal architecture provides both grounding and generalization for imagination simulation.
- Mechan: The Factual Model (ontology) stores deterministic object properties and relations; the Causal Model (probabilistic rules) extracts regularities from this data. Inference queries the causal layer, which references the factual layer for verification and context-switching.
- Core assumption: Domain knowledge cleanly separates into stable facts and learnable causal patterns.
- Evidence anchors: Section 5, Principle 1 states learning semantic model is a two-tier hybrid system; People and Hair ontology example with concrete fact types and learned rules.
- Break condition: When facts change rapidly or causation is bidirectional/cyclic, the static factual + learned causal separation degrades.

### Mechanism 3
- Claim: Glass-box interpretability emerges when all learned rules are expressed in the ontology's native concept language.
- Mechan: Causal rules use only predicates defined in or conservatively extending the ontology. Both rule conditions and predictions map directly to human-readable terms, enabling natural language explanation without post-hoc interpretation.
- Core assumption: The ontology's concept vocabulary is sufficiently expressive to capture all relevant causal structure.
- Evidence anchors: Section 5 states knowledge in a semantic model is easily interpreted in natural language; Rules like "T4(x) → T10(x) [[0.98]]" directly readable.
- Break condition: If semantic learning requires concepts outside the initial ontology language, either the language must be extended or interpretability is lost.

## Foundational Learning

- Concept: **Probabilistic conditional rules and statistical ambiguity**
  - Why needed here: The core learning output is probabilistic rules; understanding when P(F|G) and P(¬F|G) can both be high is essential.
  - Quick check question: Given data where 60% of A are B and 55% of A are not-B, what happens if you accept rules above 50% confidence?

- Concept: **Model-theoretic semantics and consistency**
  - Why needed here: The paper connects imagination consistency to Gödel's completeness theorem; semantic models inherit logical consistency guarantees.
  - Quick check question: Why does having a model (in the logical sense) guarantee that a set of statements cannot derive a contradiction?

- Concept: **Ontology languages and conservative extensions**
  - Why needed here: The causal model must extend the factual model's language without changing its semantics; this enables glass-box interpretability.
  - Quick check question: If you add a new predicate to an ontology, what constraint ensures that existing facts retain their original truth values?

## Architecture Onboarding

- Component map:
  - Factual Model (Ontology): Object types → Properties → Relations → Instance data
  - Causal Model (Rule Learner): Pattern mining → Probability estimation → Specificity filtering → Rule storage
  - Inference Engine: Query parsing → Rule matching → Probability aggregation → Factual verification
  - Context Manager: Active ontology selection → Consistency checking → Context switching

- Critical path:
  1. Define ontology language (types, properties, relations) for target domain
  2. Populate factual model with instance data
  3. Run semantic ML to extract probabilistic laws with high specificity
  4. Validate that no contradictory rules survive filtering
  5. Deploy inference with factual-verification step per query

- Design tradeoffs:
  - Broader ontology language → more expressive rules but larger search space
  - Higher probability threshold → fewer false positives but may miss valid weak patterns
  - Allowing concept invention → improved coverage but potential drift from human-interpretable vocabulary

- Failure signatures:
  - Contradictory predictions on same input → specificity filter misconfigured
  - Rules that never fire → ontology predicates don't align with data patterns
  - Explanations that confuse users → ontology terms not grounded in domain vocabulary

- First 3 experiments:
  1. Replicate the People and Hair ontology with synthetic data; verify that learned rules match expected demographic patterns and no contradictions emerge.
  2. Implement the lunch-grid agent task; confirm that subgoal concepts (e.g., "Eaten(MainCourse)") are automatically discovered and improve policy success rate.
  3. Stress-test consistency: inject contradictory facts into the ontology and verify that the system either rejects updates or isolates them in separate contexts rather than deriving contradictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can semantic models scale to domain complexities comparable to those handled by modern neural networks, and what are the computational complexity bounds for logic-probabilistic inference over large ontologies?
- Basis in paper: [explicit] The authors propose semantic models as an alternative that "can learn, like neural networks" but provide no complexity analysis or scale demonstrations beyond toy examples like the "lunch problem."
- Why unresolved: The paper presents only small-scale illustrations. No empirical comparison with neural network baselines on real-world datasets is provided, leaving scalability claims untested.
- What evidence would resolve it: Benchmarking semantic models against neural approaches on standardized tasks (e.g., reasoning benchmarks, planning domains) with analysis of time/space complexity as ontology size grows.

### Open Question 2
- Question: How can semantic models be practically integrated with existing neural systems (particularly LLMs) to combine System 1 intuition with System 2 imagination?
- Basis in paper: [explicit] The paper positions imagination as "part of System 2" while noting LLMs simulate "System 1" intuition. It states "reasoning without imagination is blind" but does not specify an integration architecture.
- Why unresolved: The hybrid system of "object ontologies + logic-probabilistic inference" is described in isolation. No mechanism is proposed for how neural and semantic components would exchange information or jointly solve problems.
- What evidence would resolve it: A concrete hybrid architecture specification with experiments showing improved performance on tasks requiring both intuitive pattern recognition and structured reasoning.

### Open Question 3
- Question: What mechanisms enable semantic models to acquire and refine ontological structures autonomously, without extensive manual knowledge engineering?
- Basis in paper: [inferred] The paper states ontologies provide "domain-specific language that is understandable to humans" and factual models "evolve over time based on perception," but does not explain how ontology schemas themselves are learned or updated.
- Why unresolved: The "lunch problem" example shows discovery of new subgoals/concepts, but assumes a fixed base ontology language. The bootstrap problem—how initial ontologies are constructed—remains unaddressed.
- What evidence would resolve it: Demonstration of autonomous ontology construction from raw data (text, sensor inputs) with quantitative evaluation of semantic model quality compared to manually engineered alternatives.

### Open Question 4
- Question: Do maximally specific causal relations with their guaranteed prediction consistency actually outperform other probabilistic inference methods on downstream decision-making tasks under uncertainty?
- Basis in paper: [explicit] The authors claim to have "solved the statistical ambiguity problem" using maximally specific causal relations that "guarantee consistent predictions," contrasting with approaches where "both F and ¬F can be inferred simultaneously."
- Why unresolved: While theoretical consistency is claimed, no empirical comparison demonstrates that this consistency translates to better real-world decision-making compared to standard probabilistic methods that tolerate some ambiguity.
- What evidence would resolve it: Comparative experiments on decision-making benchmarks (e.g., medical diagnosis, financial forecasting) measuring both prediction consistency and task performance against baseline probabilistic inference systems.

## Limitations
- The algorithmic details for discovering "maximally specific causal relations" are not provided, making practical implementation uncertain.
- Claims about autonomous concept discovery lack specified methods and validation on complex domains.
- No empirical comparison with modern neural approaches demonstrates whether semantic models can match neural network performance on real-world tasks.

## Confidence

- **High Confidence**: The two-tier architecture design (factual + causal models) and the interpretability mechanism through ontology-based rule representation are well-founded and clearly specified.
- **Medium Confidence**: The theoretical foundation for avoiding statistical ambiguity through maximally specific rules is sound, but practical implementation details are missing.
- **Low Confidence**: Claims about autonomous concept discovery and system scalability to complex domains are not supported by experimental evidence.

## Next Checks

1. Implement the consistency filter algorithm and verify it eliminates contradictory predictions in synthetic datasets with known contradictions.
2. Extend the People and Hair ontology to include dynamic facts and test whether the system maintains consistency when facts change over time.
3. Conduct a comparative study against standard probabilistic inference methods (Bayesian networks, Markov logic networks) on the lunch-grid task to quantify improvements in subgoal discovery and policy success.