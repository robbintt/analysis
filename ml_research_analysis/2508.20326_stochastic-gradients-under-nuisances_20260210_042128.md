---
ver: rpa2
title: Stochastic Gradients under Nuisances
arxiv_id: '2508.20326'
source_url: https://arxiv.org/abs/2508.20326
tags:
- gprop
- nuisance
- gradient
- loss
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first non-asymptotic convergence guarantees
  for stochastic gradient methods under unknown nuisance parameters, a common setting
  in semiparametric inference and causal learning. The authors show that when the
  loss function satisfies Neyman orthogonality, standard SGD converges linearly to
  a ball around the true parameter with radius depending on the nuisance estimation
  error to the fourth power, whereas without orthogonality the radius scales with
  the squared error.
---

# Stochastic Gradients under Nuisances

## Quick Facts
- **arXiv ID:** 2508.20326
- **Source URL:** https://arxiv.org/abs/2508.20326
- **Authors:** Facheng Yu; Ronak Mehta; Alex Luedtke; Zaid Harchaoui
- **Reference count:** 40
- **Primary result:** First non-asymptotic convergence guarantees for stochastic gradient methods under unknown nuisance parameters

## Executive Summary
This paper establishes convergence guarantees for stochastic gradient descent when nuisance parameters are unknown and must be estimated. The key insight is that when the loss function satisfies Neyman orthogonality, standard SGD converges to a parameter estimate with error depending on the nuisance estimation error to the fourth power, a significant improvement over the squared dependency without orthogonality. The authors introduce orthogonalized SGD (OSGD), which interpolates between these two regimes based on the quality of orthogonalization, and prove convergence rates that degrade gracefully as nuisance estimate quality decreases. The results apply to both fixed and iteratively updated nuisance estimates in convex settings.

## Method Summary
The authors analyze stochastic gradient methods in the presence of unknown nuisance parameters, a common setting in semiparametric inference and causal learning. They show that when the loss function is Neyman orthogonal, standard SGD converges linearly to a ball around the true parameter with radius proportional to the fourth power of the nuisance estimation error. Without orthogonality, the radius scales with the squared error. To bridge these regimes, they propose orthogonalized SGD (OSGD), which uses an approximately orthogonalized gradient oracle. The analysis covers both scenarios where nuisance estimates are fixed or updated iteratively, providing convergence rates that degrade gracefully as nuisance estimate quality decreases.

## Key Results
- Standard SGD converges linearly to a ball with radius proportional to nuisance error^4 when loss satisfies Neyman orthogonality
- Without orthogonality, convergence radius scales with nuisance error^2
- OSGD interpolates between these regimes based on orthogonalization quality
- Results hold for both fixed and iteratively updated nuisance estimates
- Theoretical findings supported by numerical experiments on partially linear models and real data

## Why This Works (Mechanism)
The key mechanism is that Neyman orthogonality ensures the gradient of the loss function with respect to nuisance parameters vanishes at the true parameter value. This property creates a protective barrier against nuisance estimation errors - when the loss is orthogonal, small errors in nuisance estimation lead to fourth-power smaller errors in the final parameter estimate. Without orthogonality, this protection is lost and errors scale quadratically. OSGD provides a practical middle ground by constructing approximately orthogonal gradient oracles, with the degree of orthogonality determining the error scaling behavior.

## Foundational Learning
- **Neyman orthogonality:** Property ensuring that the loss function's derivative with respect to the nuisance parameter is zero at the true parameter value; needed to reduce sensitivity to nuisance estimation errors
- **Semiparametric inference:** Statistical framework where some parameters are finite-dimensional while others are infinite-dimensional nuisance functions; quick check: identify which parameters are being estimated versus nuisance parameters in your problem
- **Lipschitz smoothness:** Condition on the loss function ensuring bounded gradients; why needed: enables convergence analysis by controlling gradient behavior
- **Stochastic gradient descent (SGD):** Iterative optimization algorithm using noisy gradient estimates; quick check: verify step size schedule and variance reduction properties
- **Orthogonalization of gradient oracles:** Technique to make gradient estimates insensitive to nuisance parameter errors; why needed: enables the fourth-power error reduction

## Architecture Onboarding
- **Component map:** Loss function (A) -> Gradient computation (B) -> Orthogonalization (C) -> Parameter update (D) -> Nuisance estimation (E)
- **Critical path:** A -> B -> C -> D (parameter updates depend on orthogonalized gradients)
- **Design tradeoffs:** Orthogonality provides better convergence but requires additional computation; simpler implementations may sacrifice the fourth-power error reduction
- **Failure signatures:** Poor nuisance estimation quality leads to larger convergence radii; lack of orthogonality results in slower convergence
- **First experiments:** 1) Implement standard SGD with varying nuisance quality, 2) Implement OSGD with approximate orthogonalization, 3) Compare convergence rates on a partially linear model benchmark

## Open Questions the Paper Calls Out
The paper leaves open questions about extending these convergence guarantees to non-convex settings, characterizing the computational overhead of practical orthogonalization methods, and determining optimal strategies for updating nuisance estimates during optimization. Additionally, the theoretical framework assumes exact Neyman orthogonality conditions that may be challenging to verify or achieve in practice.

## Limitations
- Theoretical results assume Neyman orthogonality and Lipschitz smoothness conditions
- Fourth-power dependency assumes exact orthogonalization conditions that may be difficult to achieve
- Analysis focuses on convex settings, leaving open questions about non-convex problems
- Practical orthogonalization requires computational overhead not fully characterized

## Confidence
- **High confidence:** Main theoretical results for convex, orthogonal settings
- **Medium confidence:** Practical applicability given orthogonalization requirements
- **Medium confidence:** Interpolation behavior claims for OSGD

## Next Checks
1. Implement OSGD on a benchmark semiparametric inference problem and compare convergence rates with standard SGD under varying nuisance estimation quality
2. Test the robustness of theoretical bounds when orthogonality conditions are approximately satisfied rather than exact
3. Extend numerical experiments to non-convex settings to assess the practical limitations of the convergence guarantees