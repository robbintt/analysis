---
ver: rpa2
title: Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and
  Clustering
arxiv_id: '2506.00985'
source_url: https://arxiv.org/abs/2506.00985
tags:
- purposes
- entries
- diary
- purpose
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel method using Large Language Models
  (LLMs) to extract and cluster the purposes of diary writing from a large Soviet-era
  diary corpus. We evaluate different proprietary and open-source LLMs, finding that
  GPT-4o and o1-mini achieve the best performance for both identifying and extracting
  diary-keeping purposes, with precision scores of 0.77 and 0.68 respectively, while
  a template-based baseline is significantly less effective.
---

# Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering

## Quick Facts
- arXiv ID: 2506.00985
- Source URL: https://arxiv.org/abs/2506.00985
- Reference count: 22
- Primary result: GPT-4o achieves 0.77 precision for extracting diary-keeping purposes, outperforming template-based baselines

## Executive Summary
This study introduces a novel method using Large Language Models (LLMs) to extract and cluster the purposes of diary writing from a large Soviet-era diary corpus. We evaluate different proprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the best performance for both identifying and extracting diary-keeping purposes, with precision scores of 0.77 and 0.68 respectively, while a template-based baseline is significantly less effective. We also analyze the retrieved purposes based on gender, age, and year of writing, and examine common model errors. Our iterative clustering algorithm successfully organizes hundreds of extracted purposes into meaningful categories, enabling scalable quantitative analysis of personal narratives in Digital Humanities.

## Method Summary
The method processes diary entries in batches of 10 (up to 15,000 tokens) using a structured prompt that asks models to identify statements revealing the author's intentions. Three-step pipeline: (1) purpose extraction via LLMs with batch processing, (2) manual annotation by 3 annotators, (3) iterative clustering: generate cluster names → assign purposes → repeat until all assigned. The study uses 38,332 Russian diary entries from 1922-1929, filtered from 40,222 entries.

## Key Results
- GPT-4o achieves precision of 0.77 and recall of 0.55 for purpose extraction
- Iterative clustering algorithm produces 13 meaningful thematic clusters with Rand index of 0.87
- Template-based baseline achieves only 0.20 precision, significantly less effective than LLM approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can identify and extract explicit diary-keeping purposes from unstructured personal narratives more effectively than template-based methods.
- **Mechanism:** The paper processes diary entries in batches of 10 (up to 15,000 tokens) using a structured prompt that asks models to identify statements revealing the author's intentions—such as documenting events, self-reflection, or skill practice. LLMs leverage contextual understanding to distinguish genuine purpose statements from tangential mentions.
- **Core assumption:** Diary entries contain explicit or clearly inferrable statements about why the author keeps the diary.
- **Evidence anchors:**
  - [abstract] "GPT-4o and o1-mini achieve the best performance for both identifying and extracting diary-keeping purposes, with precision scores of 0.77 and 0.68 respectively, while a template-based baseline is significantly less effective"
  - [section 5, Table 5] Baseline precision: 0.1977; GPT-4o precision: 0.7685; inter-annotator agreement Krippendorff's alpha=0.895 for entry-level labeling
  - [corpus] Related work (Shin et al.) used GPT-3.5/4 for depression detection in diaries with 0.902 accuracy, showing LLMs can extract psychological constructs from personal narratives
- **Break condition:** When diary entries lack explicit purpose statements, use highly oblique language, or reference others' diaries without stating own intentions.

### Mechanism 2
- **Claim:** An iterative clustering algorithm enables LLMs to organize hundreds of heterogeneous purpose statements into coherent thematic categories when single-pass clustering fails.
- **Mechanism:** Three-step loop: (1) model generates cluster names from full purpose list, (2) model assigns purposes to named clusters, (3) remove assigned purposes and repeat until empty. This handles context window limitations and prevents orphan purposes.
- **Core assumption:** Purpose statements share latent semantic themes that LLMs can recognize and consistently categorize across iterations.
- **Evidence anchors:**
  - [section 3, Step 3] "none of the models was able to successfully cluster the purposes in one run because a large number of purposes did not belong to any cluster"
  - [section 5, Table 8] GPT-4o clustering GPT-4o-extracted purposes: Rand index 0.8693; cross-model clustering shows consistent 0.80-0.85 range
  - [corpus] Limited direct corpus evidence for iterative LLM clustering; related clustering work exists but not with this specific iterative approach
- **Break condition:** When purpose statements are too heterogeneous, when context overflow occurs despite iteration, or when cluster naming becomes inconsistent across iterations.

### Mechanism 3
- **Claim:** Combining outputs from multiple models with different precision-recall profiles captures complementary signal and improves overall extraction coverage.
- **Mechanism:** GPT-4o provides high-precision extractions (0.77); DeepSeek provides high-recall coverage (0.82 relative recall). Union operations combine outputs, trading precision for comprehensive retrieval when followed by manual review.
- **Core assumption:** Different models make non-overlapping errors and identify different valid purposes.
- **Evidence anchors:**
  - [section 5, Table 5] "GPT-4o ⋃ o1-mini" achieves best F1 (0.6625); "GPT-4o ⋃ o1-mini ⋃ DeepSeek" achieves 0.95 relative recall but 0.48 precision
  - [section 6] "If precision is the primary concern, GPT-4o is the best choice. However, if the purpose is to retrieve the maximum number of potentially relevant entries at a lower cost, followed by manual labeling, DeepSeek is preferable"
  - [corpus] Weak corpus evidence; ensemble approaches mentioned in related work but not systematically evaluated for this task type
- **Break condition:** When models systematically make identical errors, when union introduces prohibitive noise for downstream tasks, or when annotation budget cannot accommodate false positives.

## Foundational Learning

- **Concept: Precision-Recall Tradeoff in Information Extraction**
  - **Why needed here:** The paper explicitly navigates this tradeoff—GPT-4o prioritizes precision (0.77), DeepSeek prioritizes recall (0.82), and the choice depends on downstream workflow (automated analysis vs. manual review).
  - **Quick check question:** If you have budget for human annotation of 200 entries and want to maximize true positives found, which model or combination would you deploy first?

- **Concept: Relative Recall and Partial Gold Standards**
  - **Why needed here:** True recall cannot be computed because labeling 38,332 diary entries is infeasible. The paper uses "relative recall" computed against the union of all model outputs as a proxy gold standard.
  - **Quick check question:** Why does relative recall potentially overestimate actual retrieval performance, and what corpus-level information would you need to compute true recall?

- **Concept: Rand Index for Clustering Evaluation**
  - **Why needed here:** The paper evaluates clustering quality using Rand index, which measures pair-wise agreement between model-generated and manually constructed partitions. Values range 0-1, with 0.87 indicating strong but imperfect alignment.
  - **Quick check question:** A Rand index of 0.83 means approximately what proportion of purpose-pairs are classified consistently (same cluster or different clusters) between model and manual groupings?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Purpose extraction -> Human annotation -> Iterative clustering -> Downstream analysis
- **Critical path:**
  1. Raw corpus (40,222 entries) → length filtering → 38,332 valid entries
  2. Valid entries → LLM batch extraction → candidate purpose sets
  3. Union of extractions (460 entries) → human annotation → 170 entries with validated purposes
  4. Validated purposes → iterative clustering → 13 thematic clusters
  5. Clusters → demographic/temporal analysis → research insights

- **Design tradeoffs:**
  - GPT-4o ($9.81, 0.77 precision) vs. DeepSeek ($5.81, 0.48 precision, 0.82 recall) vs. o1-mini ($16.70, 0.68 precision)
  - Batch size 10 entries: balances context utilization against prompt complexity
  - Annotation scope limited to model union (460) rather than full corpus: trades comprehensive evaluation for feasible human effort
  - Three annotators with majority voting: increases reliability but introduces subjectivity in ambiguous cases (purpose extraction Krippendorff's alpha dropped to 0.598)

- **Failure signatures:**
  1. **Author misattribution:** Extracting purposes about others' diaries (e.g., "to familiarize oneself with Korolenko's true experience")
  2. **Entry type confusion:** Conflating diary entries with other writing activities (e.g., "writing on canvas," "writing down objections to an article")
  3. **Purpose vs. meta-commentary confusion:** Extracting writing plans or style notes as purposes (e.g., "keep a diary neatly")
  4. **Single-pass clustering dropout:** In initial attempts, many purposes remained unassigned to any cluster
  5. **High false positive rate in open models:** GPT-4o-mini identified 2,983 entries, Qwen-2.5 identified 13,405 entries—both deemed unusable due to noise

- **First 3 experiments:**
  1. **Template baseline calibration:** Implement morphology-aware noun+verb matching on 500 entries, manually annotate precision to establish floor performance (expect ~0.20 based on paper results).
  2. **GPT-4o extraction pilot:** Process 200 entries in batches of 10 with the paper's prompt, annotate results to verify ~0.75-0.80 precision and catalog error types (author misattribution, entry type confusion, meta-commentary).
  3. **Iterative clustering validation:** Take 100 extracted purposes, run the 3-step iterative algorithm with GPT-4o, manually construct reference clusters, compute Rand index to confirm >0.80 clustering quality before scaling to full dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed extraction and clustering methodology generalize to personal narratives from different historical eras or cultural contexts outside the 1920s Soviet Union?
- **Basis in paper:** [explicit] The authors state in the Limitations that the chosen time period (1922–1929) and potential misunderstanding of historical context may affect results, implying a need to test the method on other datasets.
- **Why unresolved:** The current study is confined to a specific corpus, and it is unclear if the prompts and clustering logic are robust enough to handle linguistic variations or different prevailing motivations across time.
- **What evidence would resolve it:** Applying the identical pipeline to diaries from distinct historical periods (e.g., 19th-century or modern digital journals) and comparing the stability of clusters and extraction precision.

### Open Question 2
- **Question:** Can the development of a formal ontology for "diary purposes" resolve the low inter-annotator agreement and improve model training?
- **Basis in paper:** [explicit] The authors identify the "ambiguous definition of the concept of 'purpose of keeping a diary'" as a primary limitation and the main reason for low inter-annotator agreement (alpha=0.598).
- **Why unresolved:** Without a standardized schema, subjective interpretation leads to noisy labels, making it difficult to train or fine-tune models effectively.
- **What evidence would resolve it:** Creating a structured codebook of purpose types and measuring if annotator agreement scores and model F1-scores improve when using this constrained taxonomy versus open-ended extraction.

### Open Question 3
- **Question:** To what extent can Retrieval-Augmented Generation (RAG) or historical fine-tuning reduce hallucinations regarding authorial intent and context in archival documents?
- **Basis in paper:** [explicit] The paper notes that "misunderstanding of the historical context could lead to hallucination" and lists errors where models misidentified authors or confused other text types for diary entries.
- **Why unresolved:** General-purpose LLMs lack deep specialized knowledge of specific historical archives, leading to context errors that simple prompting cannot fully eliminate.
- **What evidence would resolve it:** A comparative study evaluating the frequency of context-related errors between standard models and models augmented with external historical knowledge bases.

## Limitations
- The corpus consists of Soviet-era Russian diaries, limiting generalizability to modern or non-Russian diary-writing practices
- Evaluation relies on relative recall computed against model-generated unions rather than ground truth, potentially inflating performance metrics
- Systematic error modes include author misattribution and confusion between diary purposes and other writing activities

## Confidence

- **High confidence:** Precision and clustering evaluation results (directly measured with human annotation, Rand index 0.87)
- **Medium confidence:** Relative recall estimates (computed against model unions, not ground truth)
- **Medium confidence:** Cross-cultural generalizability (limited to Soviet-era Russian diaries)

## Next Checks

1. **Ground truth recall validation:** Manually annotate 100-200 randomly selected entries to establish true recall baseline, then compare against relative recall estimates to quantify inflation factor.

2. **Cross-cultural transfer test:** Apply the best-performing extraction pipeline to modern diary datasets from different cultures/languages to assess generalizability beyond Soviet-era Russian diaries.

3. **Error mode mitigation:** Implement author disambiguation heuristics and writing-type filtering in the extraction prompt, then re-evaluate precision to determine if systematic errors can be reduced without sacrificing recall.