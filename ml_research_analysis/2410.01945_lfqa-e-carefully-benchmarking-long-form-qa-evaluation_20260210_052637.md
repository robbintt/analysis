---
ver: rpa2
title: 'LFQA-E: Carefully Benchmarking Long-form QA Evaluation'
arxiv_id: '2410.01945'
source_url: https://arxiv.org/abs/2410.01945
tags:
- evaluation
- zhang
- wang
- response
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LFQA-E, a multilingual benchmark for evaluating
  long-form question answering (LFQA) evaluation metrics. The benchmark addresses
  key limitations of previous work by including expert-verified reference answers
  and a larger, more diverse dataset spanning 1,618 questions across 15 topics in
  both Chinese and English.
---

# LFQA-E: Carefully Benchmarking Long-form QA Evaluation

## Quick Facts
- arXiv ID: 2410.01945
- Source URL: https://arxiv.org/abs/2410.01945
- Reference count: 40
- Key outcome: Introduces LFQA-E benchmark for evaluating long-form QA metrics, finding none match human judgment

## Executive Summary
LFQA-E is a multilingual benchmark designed to evaluate automatic metrics for long-form question answering (LFQA). The benchmark addresses key limitations of previous work by including expert-verified reference answers and a larger, more diverse dataset spanning 1,618 questions across 15 topics in both Chinese and English. The authors evaluate 17 different evaluation metrics across five categories using LFQA-E and find that none of them perform comparably to human judgment, highlighting the challenge of evaluating long-form responses that contain dense, nuanced information.

## Method Summary
The LFQA-E benchmark evaluates 17 metrics across five categories (Static, LLM-based, LRM-based, RM-based, Trained) using pairwise comparisons on 1,618 questions in English and Chinese. Metrics are tested on responses from humans, Llama-3-8B-Instruct, and GPT-3.5-turbo. The evaluation uses 3-way classification (Response A better, Response B better, Tie) against expert annotations, with accuracy and macro-F1 as primary metrics. The benchmark includes expert-verified references and covers 15 topics. TTRL experiments use specific hyperparameters including batch_size=8, rollout_temp=1.0, and lr=5e-7.

## Key Results
- No automatic metric achieves human-level performance in pairwise LFQA evaluation
- RMs fail to predict "tie" scenarios, achieving 0% accuracy on this class
- TTRL through test-time training shows promise but suffers from rapid convergence
- Performance varies significantly across languages (English vs Chinese) and model capabilities

## Why This Works (Mechanism)
The benchmark works by creating a controlled environment with expert-verified references and diverse response pairs that capture the complexity of long-form QA evaluation. The pairwise comparison format with three-way classification (A, B, Tie) forces metrics to make nuanced distinctions that reflect real-world evaluation challenges. By testing across multiple languages and model capabilities, the benchmark reveals fundamental limitations in current evaluation approaches and highlights areas for improvement.

## Foundational Learning
- Pairwise comparison methodology: Needed to capture relative quality judgments between responses; Quick check: Verify class distribution in pairwise data
- Multilingual evaluation: Required to assess generalization across languages; Quick check: Confirm balanced representation across English and Chinese questions
- Expert-verified references: Ensures evaluation is grounded in high-quality standards; Quick check: Validate reference quality through inter-annotator agreement
- 3-way classification: Captures nuanced quality distinctions including ties; Quick check: Measure class balance and predictiveness
- Reward Model training: Enables learned evaluation but requires careful optimization; Quick check: Monitor training loss and convergence behavior

## Architecture Onboarding

**Component map:** LFQA-E benchmark -> 17 evaluation metrics -> Pairwise comparison pipeline -> Human baseline comparison

**Critical path:** Data loading → Response pair generation → Metric computation → 3-way classification → Accuracy/macro-F1 calculation → Comparison to human baseline

**Design tradeoffs:** The benchmark trades computational efficiency for comprehensive evaluation by testing 17 metrics across multiple languages and model capabilities. The pairwise comparison format enables nuanced evaluation but increases computational complexity compared to scalar metrics.

**Failure signatures:** Low tie prediction accuracy (<15% vs human-level ~80%), large performance drops in m v. m settings (up to 14%), and RMs collapsing to binary classification when temp=0.0.

**3 first experiments:** 1) Test implementation on a small subset (100 questions) to validate pipeline; 2) Compare open-source alternatives to proprietary models; 3) Validate TTRL implementation by comparing reward signal computation.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can reinforcement learning methods (specifically TTRL) be modified to prevent rapid convergence and over-confidence when training LFQA evaluation models?
- Basis in paper: Section 5.7 notes that while TTRL improves performance, "we observe a rapid convergence where all rollouts produce identical preferences, which limits further improvements."
- Why unresolved: Current reinforcement learning algorithms tend to overfit on the three-category classification task, leading to a lack of diversity in rollouts.
- What evidence would resolve it: A modified RL training pipeline that maintains rollout diversity and achieves continuous performance gains on LFQA-E without early saturation.

### Open Question 2
- Question: Can evaluation workflows that combine generative LLMs and scalar Reward Models outperform single-model approaches in both cost-efficiency and alignment with human judgment?
- Basis in paper: Appendix C recommends future work "focus on evaluation workflows that combine the strengths of both LLM-based models and more efficient reward models."
- Why unresolved: The paper demonstrates that LLMs are expensive while Reward Models struggle with generalization and lack a "tie" option; the optimal hybrid architecture is undefined.
- What evidence would resolve it: A composite evaluation system that routes simple comparisons to RMs and complex cases to LLMs, demonstrating higher accuracy-per-dollar than models like GPT-4o or DeepSeek-R1.

### Open Question 3
- Question: How can automatic metrics be designed to reliably identify "tie" scenarios in nuanced long-form responses?
- Basis in paper: Section 5.3 states RMs fail "perhaps because they are trained to give a better one... renouncing the 'tie' option," and Table 3 shows all metrics struggle significantly to identify ties compared to humans.
- Why unresolved: Current models are too conservative to declare responses equal quality, resulting in low F1 scores for the "tie" class.
- What evidence would resolve it: A metric capable of triadic classification (A, B, Tie) that achieves performance comparable to the human baseline (9.2% - 14.6% accuracy on ties) on the LFQA-E benchmark.

## Limitations
- Proprietary model dependencies: Unknown API endpoints and exact model versions for GPT-4o, o1-mini, and DeepSeek-R1 limit precise reproduction
- TTRL hyperparameter specification: The "outcome-based rule similar to Deepseek-R1" is not fully detailed
- Rapid convergence issues: TTRL shows promise but suffers from over-confidence and lack of diversity in rollouts

## Confidence
- High confidence: Core methodology and benchmark structure are well-specified
- Medium confidence: Evaluation results can be reproduced with reasonable fidelity using open-source alternatives
- Low confidence: TTRL experiments and exact performance numbers for proprietary LLM-based metrics cannot be precisely reproduced

## Next Checks
1. Verify the exact model versions and API endpoints used for GPT-4o, o1-mini, and DeepSeek-R1, as these directly impact evaluation results
2. Test implementation with a smaller subset (100 questions) to validate the evaluation pipeline and identify discrepancies
3. Validate TTRL implementation by comparing reward signal computation against DeepSeek-R1 methodology