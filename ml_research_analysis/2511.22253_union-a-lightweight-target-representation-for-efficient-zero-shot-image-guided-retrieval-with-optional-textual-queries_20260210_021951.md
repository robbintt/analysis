---
ver: rpa2
title: 'UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided
  Retrieval with Optional Textual Queries'
arxiv_id: '2511.22253'
source_url: https://arxiv.org/abs/2511.22253
tags:
- image
- retrieval
- union
- target
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UNION, a lightweight target representation
  for efficient zero-shot image-guided retrieval with optional textual queries (IGROT),
  which unifies Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR).
  The core idea is to enhance the target image embedding by fusing it with a null-text
  prompt through a lightweight Transformer-MLP stack, aligning target features with
  multimodal queries.
---

# UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries

## Quick Facts
- **arXiv ID**: 2511.22253
- **Source URL**: https://arxiv.org/abs/2511.22253
- **Reference count**: 39
- **Primary result**: Achieves 38.5 mAP@50 on CIRCO and 82.7 mAP@200 on Sketchy using only 5,000 training samples, surpassing many heavily supervised baselines.

## Executive Summary
UNION introduces a lightweight target representation for zero-shot image-guided retrieval that unifies composed image retrieval (CIR) and sketch-based image retrieval (SBIR). The method enhances target image embeddings by fusing them with null-text embeddings through a lightweight Transformer-MLP stack, enabling semantic alignment with multimodal queries without modifying pretrained vision-language models. Trained on only 5,000 samples from LlavaSCo (for CIR) and Training-Sketchy (for SBIR), UNION achieves competitive results across multiple benchmarks, demonstrating strong generalization under minimal supervision.

## Method Summary
UNION processes target images by concatenating their visual embeddings with null-text embeddings (from an empty string passed through the VLM text encoder), then applying a 2-layer Transformer followed by an MLP to compute adaptive weights for weighted interpolation. The final representation places targets in the same multimodal embedding space as composed queries. During training, a Batch-Based Classification (BBC) loss aligns fused queries with their corresponding targets. The method requires no architectural modifications to pretrained vision-language models and works with CLIP or BLIP backbones. Inference uses null text to generate target representations for the entire image pool before ranking.

## Key Results
- Achieves 38.5 mAP@50 on CIRCO and 82.7 mAP@200 on Sketchy with only 5,000 training samples
- Outperforms many heavily supervised baselines while requiring no architectural modifications to pretrained VLMs
- Demonstrates strong generalization across language-augmented and visual-only retrieval tasks
- Shows significant improvement margins when using high-quality captions (LlavaSCo) versus original captions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Concatenating target image embeddings with null-text embeddings reduces modality mismatch between multimodal queries and unimodal targets in composed retrieval.
- **Mechanism**: The UNION representation fuses $e^t$ (target image) with $e^\eta$ (null-text embedding from an empty string passed through the VLM text encoder). This concatenation $e^{t\eta} = [e^t; e^\eta]$ is processed by a lightweight Transformer, then pooled and passed through an MLP to compute adaptive weights $w_t$ and $w_\eta = 1 - w_t$. The final representation $U = w_t \cdot e^t + w_\eta \cdot e^\eta$ places targets in the same multimodal embedding space as composed queries.
- **Core assumption**: The null-text embedding captures a neutral linguistic prior that implicitly shifts visual features toward the multimodal joint space without injecting external semantics.
- **Evidence anchors**:
  - [abstract] "UNION enhances semantic alignment with multimodal queries while requiring no architectural modifications to pretrained vision-language models."
  - [Section III-B] "The null-text embedding introduces a neutral linguistic prior that implicitly brings the target closer to the multimodal space used for queries, without injecting external textual content."
  - [corpus] Related work (SCOT, SQUARE) focuses on query-side fusion rather than target representation; UNION addresses this underexplored gap.
- **Break condition**: If the pretrained VLM's null-text embedding is not meaningfully distinct from zero or carries no linguistic structure, the interpolation provides no semantic benefit.

### Mechanism 2
- **Claim**: Adaptive weighting via learned MLP outputs enables context-sensitive blending of visual and latent textual features per target instance.
- **Mechanism**: After the Transformer processes the concatenated features, pooling produces $f^{t\eta} \in \mathbb{R}^{B \times D}$, which the MLP maps to $w_t \in \mathbb{R}^{B \times D}$. This per-dimension weighting allows selective amplification or suppression of feature dimensions based on their relevance to multimodal alignment.
- **Core assumption**: The network learns which dimensions benefit from null-text conditioning versus pure visual representation.
- **Evidence anchors**:
  - [Section III-B] Equations (2–3) define the Transformer-MLP pipeline with explicit weight computation.
  - [Figure 2] Architecture diagram shows the learned weight path from pooled Transformer output.
  - [corpus] No direct comparison in neighbors; this adaptive interpolation is a novel contribution.
- **Break condition**: If the MLP collapses to constant weights (e.g., $w_t \approx 1$), UNION degenerates to the original fixed embedding.

### Mechanism 3
- **Claim**: High-quality caption augmentation (LlavaSCo) amplifies UNION's effectiveness by providing richer textual supervision for contrastive learning.
- **Mechanism**: The paper generates detailed target image captions using LLaVA and appends them to original relational captions. This reduces ambiguity in the training signal, enabling the fusion module and UNION to learn finer-grained semantic correspondences.
- **Core assumption**: Caption quality—not quantity—is the bottleneck for low-supervision CIR performance.
- **Evidence anchors**:
  - [abstract] "With only 5,000 training samples—from LlavaSCo for CIR...our method achieves competitive results."
  - [Figure 5] Heatmap shows UNION with LlavaSCo captions achieving 49.8 average score vs. 32.1 without.
  - [corpus] ConText-CIR similarly emphasizes textual concept learning, but UNION couples this with target-side representation.
- **Break condition**: If generated captions introduce noise or hallucinations, the enhanced supervision degrades alignment.

## Foundational Learning

- **Concept: Contrastive Learning with InfoNCE-style Loss**
  - **Why needed here**: The Batch-Based Classification (BBC) loss in Equation (1) is a variant of InfoNCE that trains the model to maximize similarity between fused queries and their correct targets while minimizing similarity to distractors in the batch.
  - **Quick check question**: Given a batch of 32 query-target pairs, can you compute the BBC loss for the first query if you know all pairwise cosine similarities?

- **Concept: Vision-Language Model (VLM) Joint Embedding Spaces**
  - **Why needed here**: UNION relies on CLIP/BLIP's pretrained image and text encoders to produce aligned features. Understanding that these encoders map images and text to a shared D-dimensional space is essential.
  - **Quick check question**: If you encode an image of a "red car" and the text "red car" with CLIP, should their embeddings have high cosine similarity?

- **Concept: Transformer Attention and Pooling Strategies**
  - **Why needed here**: The UNION module uses a 2-layer Transformer followed by pooling. Understanding how self-attention processes the 2-token sequence $[e^t, e^\eta]$ clarifies how information mixes before final projection.
  - **Quick check question**: After a Transformer processes a sequence of 2 tokens with shape $\mathbb{R}^{B \times 2 \times D}$, what are common pooling strategies to obtain a single $\mathbb{R}^{B \times D}$ vector?

## Architecture Onboarding

- **Component map**:
  - **Backbone VLM (frozen)**: CLIP-B/32, CLIP-L/14, or BLIP ViT-B encodes images ($e^t$) and text ($e^\eta$, modification captions).
  - **Query Fusion Module**: 2-layer Transformer (8 heads, GELU) fuses reference image + modification text → $F^{rc}$.
  - **UNION Module**: 2-layer Transformer (8 or 12 heads) + MLP processes $[e^t; e^\eta]$ → adaptive weights → final $U$.
  - **Loss**: BBC loss compares $F^{rc}$ to $U$ with temperature $\tau = 0.01$.

- **Critical path**:
  1. During training: Query fusion produces $F^{rc}$; UNION produces $U$ for each target; BBC loss computed per batch.
  2. During inference: UNION pre-computes target representations (with null-text) for the entire image pool; query fusion processes the input; cosine similarity ranking returns top-K.

- **Design tradeoffs**:
  - **Inference latency**: UNION requires additional computation per target image (Transformer + MLP). Paper reports 271–656 seconds to embed large pools depending on backbone.
  - **Data efficiency vs. quality**: 5K samples with LlavaSCo captions outperform millions of noisy web-mined triplets on some metrics, but caption generation itself requires compute (LLaVA-v1.6-Mistral-7B).
  - **Backbone choice**: BLIP with LlavaSCo + UNION yields best results (49.8 avg), but CLIP-L shows the largest UNION improvement margin.

- **Failure signatures**:
  - UNION underperforms "original" features when trained without captions on smaller backbones (CLIP-B: 29.6 vs. 30.6)—suggests the method relies on rich textual grounding.
  - "Sum" feature ($e^t + e^\eta$) consistently underperforms, especially on QuickDraw (20.2 vs. 33.4 mAP)—simple additive fusion fails to capture compositional nuance.

- **First 3 experiments**:
  1. **Baseline sanity check**: Train TransAgg without UNION on LlavaSCo; verify reproduced mAP@50 on CIRCO (should be ~32–36 per Table I).
  2. **UNION ablation**: Compare three target feature types (original, sum, UNION) on a single backbone (CLIP-L) using Training-Sketchy; expect UNION to outperform on Sketchy and TU-Berlin.
  3. **Caption quality test**: Train UNION on original LaSCo captions vs. LlavaSCo-enhanced captions; measure gap on CIRCO val to quantify caption contribution (expected ~13-point improvement per Figure 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can UNION's inference latency overhead be substantially reduced through feature caching or lightweight alternatives to the Transformer-MLP stack while preserving retrieval quality?
- Basis in paper: [explicit] Authors note inference takes 271s–656s depending on backbone, highlighting "a trade-off between representation quality and computational efficiency" that "could be further addressed in future work through faster captioning or feature caching strategies."
- Why unresolved: The paper does not experiment with caching strategies or alternative lightweight architectures for the UNION module itself.
- What evidence would resolve it: Ablation experiments comparing retrieval performance vs. latency for cached UNION features, or for UNION variants with fewer Transformer layers or simpler fusion mechanisms.

### Open Question 2
- Question: Does the effectiveness of UNION generalize to vision-language retrieval tasks beyond CIR and SBIR, such as video-text retrieval or cross-domain retrieval with other sketch-like modalities?
- Basis in paper: [inferred] The method is evaluated only on CIR (three benchmarks) and SBIR (three benchmarks); the unifying IGROT framework could conceptually extend to other image-guided settings, but no experiments explore this broader applicability.
- Why unresolved: The authors demonstrate generalization across two task types under the same IGROT umbrella, but do not test on other retrieval paradigms or modalities where modality mismatch between query and target may also exist.
- What evidence would resolve it: Evaluation of UNION on additional vision-language retrieval benchmarks (e.g., video retrieval, domain-adapted retrieval) with the same lightweight training budget.

### Open Question 3
- Question: Can the dependency on high-quality captions be reduced or eliminated, particularly for backbones where UNION underperforms without text supervision?
- Basis in paper: [explicit] The ablation study shows UNION underperforms on CLIP-B without captions, and the conclusion states: "UNION relies on the presence—or generation—of informative captions to fully leverage linguistic cues."
- Why unresolved: The mechanism by which captions help UNION—and whether null-text can be replaced with learned latent representations—remains unexplored.
- What evidence would resolve it: Experiments with learnable null-text embeddings, synthetic or distilled caption alternatives, or self-supervised alignment objectives that do not require explicit captions.

## Limitations

- **Data curation sensitivity**: Strong performance relies on curated 5K subsets from LlavaSCo and Training-Sketchy, but selection criteria are unspecified, raising concerns about generalization to larger datasets.
- **Null-text embedding dependence**: The core mechanism assumes the null-text embedding carries meaningful linguistic structure; if degenerate, UNION would not outperform simple baselines.
- **Inference cost trade-off**: While claimed "lightweight," the UNION module adds significant preprocessing time (271–656 seconds for large pools), with no comparison to real-time alternatives.

## Confidence

- **High**: Architectural design and training procedure (BBC loss, backbone choices, optimizer settings) are clearly specified and reproducible.
- **Medium**: Performance improvements are convincingly shown on benchmark datasets, but dependence on curated subsets and the role of null-text embedding are not fully validated.
- **Low**: The claim that UNION is "training-free" for inference is misleading; it requires the lightweight UNION module to be trained on labeled data, albeit with minimal supervision.

## Next Checks

1. **Null-text embedding ablation**: Compute and compare the null-text embedding to a zero vector and a random text embedding; verify it is non-trivial and distinct.

2. **Subset robustness test**: Train UNION on randomly sampled 5K subsets from the full LlavaSCo/Training-Sketchy pools; measure variance in retrieval performance to assess sensitivity to data curation.

3. **Inference latency scaling**: Measure total retrieval time (including UNION preprocessing) as a function of pool size; compare against a fixed-feature baseline to quantify the practical cost of the UNION representation.