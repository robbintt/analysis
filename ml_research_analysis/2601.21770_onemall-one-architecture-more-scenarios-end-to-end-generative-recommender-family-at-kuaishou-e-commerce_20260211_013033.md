---
ver: rpa2
title: 'OneMall: One Architecture, More Scenarios -- End-to-End Generative Recommender
  Family at Kuaishou E-Commerce'
arxiv_id: '2601.21770'
source_url: https://arxiv.org/abs/2601.21770
tags:
- item
- semantic
- arxiv
- e-commerce
- onemall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OneMall, an end-to-end generative recommendation
  framework for e-commerce services at Kuaishou. It addresses the challenge of unifying
  multiple item distribution scenarios (product cards, short videos, live streaming)
  in e-commerce by aligning model training with LLM pre-/post-training paradigms.
---

# OneMall: One Architecture, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce

## Quick Facts
- arXiv ID: 2601.21770
- Source URL: https://arxiv.org/abs/2601.21770
- Reference count: 40
- Primary result: +13.01% GMV in product cards, +15.32% orders in short videos, and +2.78% orders in live streaming across 400M+ daily active users

## Executive Summary
OneMall is an end-to-end generative recommendation framework developed at Kuaishou to unify item distribution across multiple e-commerce scenarios (product cards, short videos, live streaming). The system addresses the challenge of heterogeneous recommendation scenarios by aligning model training with LLM pre-/post-training paradigms, using semantic tokenization, Query-Former for sequence compression, and reinforcement learning with ranking models as reward functions. The framework achieves significant improvements in both business metrics (GMV, orders) and recommendation quality (hit rates) while serving over 400 million daily active users.

## Method Summary
OneMall employs a three-stage approach: (1) semantic tokenization using a hybrid Res-Kmeans + FSQ quantization scheme to map items to multi-level Semantic IDs, (2) a transformer-based architecture with Query-Former for compressing long user behavior sequences into fixed-size representations and Cross-Attention for multi-behavior fusion, and (3) reinforcement learning alignment using the ranking model's predictions as reward signals through Group Relative Policy Optimization (GRPO). The system combines Next-Token Prediction loss with in-batch contrastive learning and RL loss to optimize for both generation quality and business objectives.

## Key Results
- Product cards: +13.01% GMV, +11.07% orders, HR@50 improved from 10.65% to 13.04%
- Short videos: +15.32% orders, +3.98% CTR, HR@50 improved from 9.87% to 12.60%
- Live streaming: +2.78% orders, +0.57% CTR, HR@50 improved from 8.89% to 10.68%

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Semantic Tokenization for Conflict Reduction
If item tokenization relies solely on Residual K-Means, code conflicts degrade retrieval accuracy; introducing Finite Scalar Quantization (FSQ) at the final layer mitigates this by enforcing uniform code distribution. The architecture uses Res-Kmeans to capture hierarchical semantics followed by a 16-bit MLP-based FSQ layer quantizing embeddings into a 4096-code space, preventing cluster center collapse observed in standard K-Means optimization.

### Mechanism 2: Query-Former for Asymmetric Sequence Compression
If user behavior sequences are concatenated directly, quadratic Transformer complexity makes inference prohibitive; Query-Former compresses distinct modalities into fixed-size representations while preserving relevance. Instead of attending to full history at every generation step, learnable "Query" tokens attend to long sequences via Q-Former blocks to produce compressed feature matrices, reducing GFLOPs from 34.4 to 9.2 with negligible performance loss.

### Mechanism 3: Ranker-as-Reward Alignment (GRPO)
If the generative model is trained only on Next-Token Prediction, it may predict relevant items that do not maximize business value; using the ranking model's score as a reward signal aligns generation with multi-objective goals. The system treats the online ranking model as a deterministic reward function, computing rewards based on weighted sums of CTR/CVR/EGPM predictions, then adjusts the policy via Group Relative Policy Optimization by normalizing rewards within groups of sampled candidates.

## Foundational Learning

- **Concept: Semantic ID (SID)**
  - Why needed here: E-commerce items are discrete entities without inherent vocabulary; OneMall creates a "language" of items by mapping them to multi-level integers, enabling LLM architectures.
  - Quick check question: How does OneMall handle a new item that didn't exist during codebook training? (Answer: The codebooks are frozen, and the LLM encoder projects new item features to the nearest existing semantic code).

- **Concept: Sparse Mixture of Experts (MoE)**
  - Why needed here: To scale model capacity (up to 1.3B parameters) while keeping inference latency low (activating only ~0.1B parameters per token) for high-throughput requirements.
  - Quick check question: In OneMall, does MoE operate on the user history encoder or item generation decoder? (Answer: The Decoder-Style Sparse MoE is used for the auto-regressive generation module).

- **Concept: Advantage Normalization (in GRPO)**
  - Why needed here: To stabilize RL training by normalizing rewards within a batch of generated candidates, allowing the model to learn relative preferences rather than absolute values.
  - Quick check question: Why might GRPO perform better than DPO in this e-commerce context? (Answer: DPO relies on pairwise preference which might miss nuances of broad candidate sets, whereas GRPO evaluates quality distribution across a group, fitting beam search retrieval logic better).

## Architecture Onboarding

- **Component map:**
  1. Tokenizer (Offline): Swin-Transformer + Qwen2.5-1.5B → Res-Kmeans + FSQ → Semantic IDs
  2. Feature Encoder (Online): User History → Multiple Query-Formers (Click/Buy/Exp) → Compressed Vectors
  3. Generator: Semantic ID Embeddings + Cross-Attention (attending to compressed user vectors) + Sparse MoE Decoder
  4. RL Loop: Generator outputs Candidates → Ranker provides Scores → GRPO Loss updates Generator

- **Critical path:** The Tokenizer Logic is the most critical dependency. If semantic IDs have high conflict rates, the generator is fundamentally confused and downstream RL cannot correct it. Verify "Exclusive Match Rate" before training the main model.

- **Design tradeoffs:**
  - Dense vs. Sparse Activation: Sparse MoE allows 10x parameter scaling but introduces routing complexity and load balancing requirements.
  - NTP vs. Contrastive Loss: OneMall combines Next-Token Prediction (generative capability) with In-Batch Contrastive Learning (representation accuracy). The balance is essential; relying only on NTP can lead to "hallucinated" invalid IDs.

- **Failure signatures:**
  - Code Collision: Symptoms include a single generated Semantic ID decoding to multiple different items, causing erratic user experiences.
  - RL Collapse: If RL loss weight is set too high, SID Accuracy may degrade, meaning the model learns to game the ranker score while losing coherent sequence generation ability.

- **First 3 experiments:**
  1. Tokenizer Capacity Test: Run ResKmeansFSQ tokenizer on hold-out set. Measure "Conflict Rate" (Goal: <15%) and "Exclusive Rate" (Goal: >90%). Do not proceed if conflict is high.
  2. Sequence Compression Ablation: Train 0.1B model with (a) Concatenated Sequence vs. (b) Query-Former. Compare HR@50 and GFLOPs. Validate compression retains >95% of performance.
  3. RL Policy Gradient Check: Train with DPO vs. GRPO on small traffic slice (2%). Monitor "Reward" improvement and "Generation Validity" (percentage of generated IDs mapping to real items). Choose GRPO only if validity remains high.

## Open Questions the Paper Calls Out
The paper explicitly states it will explore "unifying retrieval and ranking ability to OneMall" in future work, as currently the ranking model is used only as a reward signal while maintaining separate models. The different objectives and feature requirements create architectural tension that remains unresolved.

## Limitations
- Tokenizer Design Constraints: Fixed codebook size (4096 codes) may become insufficient as item catalog scales beyond current levels; doesn't address cold-start scenarios.
- RL Reward Function Validity: Circular dependency where ranking model biases could be amplified rather than corrected through the RL loop.
- Offline vs Online Performance Gap: Modest offline improvements (2-3% HR@50) versus significant online gains suggest potential overfitting to online reward structure.

## Confidence
- **High Confidence (8-10/10):** Core architectural claims around Query-Former compression are well-supported by ablation studies showing 3x GFLOPS reduction with <1% performance loss.
- **Medium Confidence (5-7/10):** RL alignment claims rely on assumption that ranking model provides accurate reward signals; methodology is sound but effectiveness depends on reward model quality.
- **Low Confidence (2-4/10):** Scalability claims for 400M daily active users lack independent verification; no latency or resource utilization data provided.

## Next Checks
1. **Catalog Expansion Stress Test:** Run tokenizer on simulated 10x larger item catalog and measure collision rates to validate scalability and identify capacity bottlenecks.
2. **Reward Model Ablation Study:** Replace online ranking model with simpler CTR-only reward function and compare RL training stability and final performance.
3. **User Satisfaction Correlation Analysis:** Conduct user surveys or A/B tests measuring satisfaction and long-term engagement alongside reported GMV/orders metrics to validate genuine user value.