---
ver: rpa2
title: Memory Injection Attacks on LLM Agents via Query-Only Interaction
arxiv_id: '2503.03704'
source_url: https://arxiv.org/abs/2503.03704
tags:
- agent
- memory
- malicious
- pair
- records
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes MINJA, a novel memory injection attack on\
  \ LLM agents that injects malicious records into the agent\u2019s memory bank solely\
  \ through query-only interaction. The attacker uses bridging steps and an indication\
  \ prompt with a progressive shortening strategy to autonomously generate and inject\
  \ malicious records that cause the agent to produce harmful outputs when processing\
  \ victim queries."
---

# Memory Injection Attacks on LLM Agents via Query-Only Interaction

## Quick Facts
- arXiv ID: 2503.03704
- Source URL: https://arxiv.org/abs/2503.03704
- Reference count: 40
- This paper proposes MINJA, a novel memory injection attack on LLM agents that injects malicious records into the agent's memory bank solely through query-only interaction.

## Executive Summary
This paper introduces MINJA, a novel memory injection attack that enables attackers to compromise LLM agents by injecting malicious reasoning patterns into their memory banks through normal query interactions. The attack exploits the fact that agents store interaction records in long-term memory and retrieve them for future queries. By crafting specially designed queries with indication prompts that progressively shorten, the attacker can induce the agent to generate and store reasoning steps that will later cause the agent to produce harmful outputs when processing victim queries. The attack achieves high injection success rates (>90%) and attack success rates (>70%) across diverse agents including healthcare, web shopping, and QA applications, while preserving the agent's benign utility.

## Method Summary
MINJA employs a Progressive Shortening Strategy (PSS) where the attacker crafts attack queries containing a victim term and appends an indication prompt that bridges to a target term. The agent generates reasoning steps in response, which are stored in memory. The indication prompt is then progressively shortened across multiple injection rounds, creating multiple malicious records with increasingly normal-looking queries but preserving the malicious response pattern. When a victim query containing the victim term is later submitted, these malicious records are retrieved and cause the agent to generate harmful reasoning. The attack operates under a query-only threat model where the attacker can only interact with the agent through queries and has access to the shared memory bank.

## Key Results
- MINJA achieves high injection success rates (ISR >90% in most cases) across diverse agents and memory settings
- Attack success rates (ASR) exceed 70% in half of the tested scenarios, causing agents to produce harmful outputs
- The attack preserves benign agent utility, with utility drop remaining below 5% in most cases
- MINJA is resilient to potential defenses, maintaining effectiveness even when defenders attempt to detect malicious records

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bridging steps may create a logical chain from a benign-seeming query to malicious reasoning steps.
- Mechanism: The attacker crafts intermediate reasoning steps ("bridging steps") that semantically connect a query containing a victim term (e.g., patient ID A) to reasoning steps for a target term (e.g., patient ID B). When later retrieved as in-context demonstrations, these steps can mislead the agent to apply the same logical bridge to new victim queries.
- Core assumption: The agent follows retrieved demonstration patterns and does not semantically validate or flag the bridging logic as anomalous.
- Evidence anchors:
  - [abstract]: "introduce a sequence of bridging steps to link victim queries to the malicious reasoning steps"
  - [section 4.1]: "bridging steps in the response of malicious records, which has the capability to logically connect av with Rat"
  - [corpus]: Related work on prompt injection and agent manipulation suggests agents are susceptible to context-based influence.

### Mechanism 2
- Claim: Indication prompts may induce the agent to autonomously generate malicious records during normal interaction.
- Mechanism: By appending a carefully structured indication prompt to a benign attack query, the attacker guides the agent to generate both bridging steps and target reasoning steps in its response, which are then stored in the memory bank.
- Core assumption: The agent does not distinguish indication prompts as adversarial and follows them as part of its task reasoning.
- Evidence anchors:
  - [abstract]: "indication prompt that guides the agent to autonomously generate similar bridging steps"
  - [section 4.2]: "indication prompt consisting of a sequence of logically connected reasoning steps"
  - [corpus]: Prompt injection literature indicates LLMs can be misled by injected instructions.

### Mechanism 3
- Claim: Progressive shortening may increase retrievability of malicious records while removing explicit indicators.
- Mechanism: The indication prompt is progressively shortened across multiple injection rounds, creating records with increasingly normal-looking queries while preserving the malicious response pattern. This yields multiple semantically similar malicious records that are more likely to be retrieved for victim queries.
- Core assumption: The agent continues to generate the desired response even as the indication prompt is shortened, and retrieval similarity scores favor these records for relevant victim queries.
- Evidence anchors:
  - [abstract]: "progressive shortening strategy that gradually removes the indication prompt, such that the malicious record will be easily retrieved"
  - [section 4.2]: "PSS generates and stores the malicious record (av,[bv,t,Rat]) to the memory bank"
  - [corpus]: Weak direct evidence; related work on adversarial environmental injection suggests agents can be vulnerable to subtle context manipulations.

## Foundational Learning

- **LLM Agent Memory Architecture**
  - Why needed here: Understanding how agents store and retrieve interaction records is essential to grasp how injected memories can influence future behavior.
  - Quick check question: In a typical agent, what triggers the storage of a new record in the memory bank?

- **In-Context Learning from Demonstrations**
  - Why needed here: The attack relies on the agent learning patterns from retrieved records; knowing how demonstrations are used during inference clarifies the attack surface.
  - Quick check question: When a new query arrives, how does the agent use records retrieved from memory?

- **Threat Model Assumptions (Query-Only, Shared Memory)**
  - Why needed here: The attack's feasibility hinges on realistic attacker constraints and system design choices; misaligned assumptions can invalidate the threat model.
  - Quick check question: What are the key limitations on the attacker's capabilities in this paper's threat model?

## Architecture Onboarding

- Component map:
  - Attacker -> crafts indication prompt + attack queries
  - Victim User -> issues queries containing victim term
  - Agent -> receives queries, retrieves from memory bank, generates reasoning
  - Memory Bank -> stores interaction records, retrieves by similarity
  - Injection Pipeline -> PSS loop with progressive shortening

- Critical path:
  1. Attacker submits attack query + indication prompt -> agent generates bridging + target reasoning -> record stored
  2. PSS shortens indication prompt -> new records stored with similar query but same malicious response
  3. Victim user submits query with victim term -> malicious records retrieved -> agent generates malicious reasoning

- Design tradeoffs:
  - Injection success rate vs. stealth: More aggressive indication prompts may increase ISR but risk detection
  - Number of injected records vs. benign utility: Many malicious records may degrade performance on non-target queries
  - Bridging-step complexity vs. agent following: Overly complex bridges may not be replicated by the agent

- Failure signatures:
  - Low ISR: Agent does not generate target reasoning during injection (indication prompt may be ignored or blocked)
  - Low ASR with high ISR: Injected records are stored but not retrieved for victim queries (similarity mismatch, retrieval noise, or memory filtering)
  - High utility drop: Excessive malicious records displace benign demonstrations, harming normal task performance

- First 3 experiments:
  1. Baseline injection on single agent: Test MINJA with full PSS on one agent/dataset pair; measure ISR, ASR, UD
  2. Ablation on bridging steps: Remove bridging steps from indication prompt design; observe impact on ASR
  3. Retrieval noise sensitivity: Add Gaussian noise to embeddings; measure ISR/ASR degradation to test robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a prompt-level detection mechanism be developed that balances high precision (avoiding utility loss) with broad generalization across diverse agent architectures?
- Basis in paper: Section 5.4 notes that targeted detection prompts fail to generalize across agents, while general prompts result in high false positives that undermine agent utility.
- Why unresolved: The paper's experiments demonstrate a fundamental trade-off where precise prompts lack generality and general prompts lack precision.
- What evidence: A detection method that maintains >90% detection accuracy across all tested agents with a false positive rate consistently below 5%.

### Open Question 2
- Question: To what extent can MINJA propagate through multi-agent systems where the compromised memory of one agent influences the behavior of interconnected agents via inter-agent communication?
- Basis in paper: Appendix O states that MINJA "can be potentially extended to multi-agent systems," posing the question of how memory injection affects agents that communicate with one another.
- Why unresolved: The current study limits its scope to single-agent pipelines and does not evaluate the cross-contamination potential in collaborative environments.
- What evidence: Empirical results measuring the Attack Success Rate in a secondary, uncompromised agent after it receives output from a MINJA-compromised primary agent.

### Open Question 3
- Question: Is there a scalable adversarial training method that can immunize agents against query-only memory injection without requiring task-specific labeled data or costly retraining?
- Basis in paper: Section 5.4 identifies adversarial training as "inherently costly" and limited because it requires large-scale, task-specific labeled attack data.
- Why unresolved: The paper demonstrates that current defenses are either impractical or ineffective, leaving a need for efficient, proactive robustness.
- What evidence: A training protocol that provides cross-task robustness to progressive shortening strategies, verified by a significantly reduced ASR without degrading benign task utility.

## Limitations

- Critical implementation details including the custom QA Agent prompt template and memory retrieval logic are deferred for future release, creating barriers to independent validation
- The specific automated heuristic or model used to validate "desired malicious responses" before storing records in memory is not specified
- The autonomous generation process for bridging steps in varied contexts remains underspecified beyond specific examples shown

## Confidence

**High Confidence Claims:**
- The overall threat model (query-only interaction to inject memory records) is plausible and well-grounded in existing literature
- The concept of using bridging steps to connect victim queries to malicious reasoning is theoretically sound
- Progressive shortening as a stealth mechanism is methodologically reasonable

**Medium Confidence Claims:**
- The specific effectiveness metrics (ISR >90%, ASR >70%) reported across multiple agents and datasets
- The resilience to potential defenses, as specific defense mechanisms tested are not fully detailed
- The precise implementation of the Progressive Shortening Strategy and its impact on injection success

**Low Confidence Claims:**
- Exact performance numbers for individual agent-dataset combinations
- The specific text and structure of indication prompts that achieve optimal results
- The generalizability of attack success across all possible agent architectures and memory implementations

## Next Checks

1. **Bridging Step Validation**: Implement a controlled experiment where bridging steps are manually inserted into a memory bank, then measure retrieval rates and reasoning influence when victim queries are submitted. This would isolate the bridging mechanism from the full attack pipeline.

2. **Progressive Shortening Sensitivity**: Conduct an ablation study varying the number of shortening iterations and the granularity of prompt removal. Measure how these parameters affect both injection success rate and detection risk.

3. **Memory Bank Density Impact**: Systematically vary the ratio of benign to malicious records in the memory bank (e.g., 1:1, 5:1, 10:1, 20:1) and measure how this affects retrieval probability of malicious records for victim queries while monitoring impact on benign query performance.