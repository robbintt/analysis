---
ver: rpa2
title: Improving Fairness in LLMs Through Testing-Time Adversaries
arxiv_id: '2505.12100'
source_url: https://arxiv.org/abs/2505.12100
tags:
- fairness
- bias
- prompt
- llms
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve fairness in large language
  models (LLMs) by detecting and correcting biased predictions at inference time.
  The core idea is to create multiple perturbed versions of an input sentence by modifying
  sensitive attributes (e.g., race, gender), then comparing the model's predictions
  on these variants to the original.
---

# Improving Fairness in LLMs Through Testing-Time Adversaries

## Quick Facts
- arXiv ID: 2505.12100
- Source URL: https://arxiv.org/abs/2505.12100
- Reference count: 28
- Improves fairness metrics by up to 27 percentage points without training or fine-tuning

## Executive Summary
This paper introduces a testing-time method to improve fairness in large language models by detecting and correcting biased predictions through input perturbations. The approach generates multiple perturbed versions of an input by modifying sensitive attributes, then compares model predictions across these variants to identify inconsistency. When consistency falls below a threshold, the prediction is flipped to reduce group-level disparities. Experiments on the Llama3 model show significant improvements in fairness metrics while maintaining predictive performance.

## Method Summary
The method operates entirely at inference time without requiring model training or fine-tuning. It creates N perturbed versions of an input sentence by modifying sensitive attributes (race, gender, charge degree), then runs the original and all perturbed inputs through the LLM with the same prompt. A consistency rate (CR) is calculated as the fraction of predictions matching the original. If CR falls below a threshold t (typically 0.9), the binary prediction is flipped. This approach leverages the principle that fair predictions should be invariant to counterfactual changes in sensitive attributes while task context remains unchanged.

## Key Results
- Improves fairness metrics by up to 27 percentage points compared to baseline Llama3 model
- Reduces D_TPR from 0.27 to 0.08 and D_FPR from 0.28 to 0.10 on Prompt P1
- Outperforms fine-tuning approaches, achieving 17 percentage points better fairness improvement than Lochab et al.'s method
- Maintains stable accuracy (0.64-0.68 vs. 0.66 baseline) across most prompt configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prediction inconsistency across attribute perturbations signals bias in LLM outputs.
- Mechanism: Generate N perturbed versions of input by modifying sensitive attributes (race, gender, charge degree). Compute consistency rate (CR) as fraction of perturbed predictions matching original. Low CR indicates the model's decision depends spuriously on sensitive attributes rather than task-relevant features.
- Core assumption: Fair predictions should be invariant to counterfactual changes in sensitive attributes when task context remains unchanged.
- Evidence anchors: [abstract] "The idea behind this process is that critical ethical predictions often exhibit notable inconsistencies, indicating the presence of bias." [section III-C] "In other words, if the response remains consistent under the same counterfactual scenario, it is deemed fair."
- Break condition: If task-relevant features are correlated with sensitive attributes in the data distribution, consistency-based detection may conflate legitimate signal with bias.

### Mechanism 2
- Claim: Flipping predictions below a consistency threshold reduces disparate treatment across groups.
- Mechanism: When CR falls below threshold t (e.g., 0.9), flip the binary prediction. This forces the model toward decisions that would have been consistent across demographic variants, reducing group-level metric disparities (D_TPR, D_FPR, D_sp).
- Core assumption: Flipping inconsistent predictions moves decisions toward a fairer equilibrium without systematically favoring one group.
- Evidence anchors: [section III-C, Equation 2] Formal specification of the adjustment rule. [section IV-E, Table I] Dtpr reduced from 0.27→0.08 (P1), 0.32→0.09 (P4); Dsp reduced from 0.28→0.10 (P1), 0.34→0.07 (P4) on Llama3.
- Break condition: If the original prediction was correct and perturbations exploit model noise rather than true bias, flipping degrades accuracy.

### Mechanism 3
- Claim: Testing-time perturbation avoids fine-tuning risks while exposing model weaknesses.
- Mechanism: By operating solely on forward passes without gradient updates, the method sidesteps representation distortion that can occur during fine-tuning. Perturbations probe model sensitivity post-hoc.
- Core assumption: Pre-trained representations contain detectable bias patterns that forward-pass probing can reveal without modification.
- Evidence anchors: [abstract] "eliminating the need for training, fine-tuning, or prior knowledge of the training data distribution." [section IV-I] Fine-tuning approach (Lochab et al.) increased fairness metrics by up to 17 percentage points (worse), while testing-time method improved by up to 27 percentage points.
- Break condition: If bias is encoded in ways not triggered by attribute swapping (e.g., syntactic patterns, contextual associations), perturbations may miss it.

## Foundational Learning

- Concept: **Counterfactual Fairness**
  - Why needed here: The method's core logic relies on the principle that changing only sensitive attributes shouldn't change predictions. Understanding this framing is essential to interpret CR as a bias signal.
  - Quick check question: If you swap "African-American" to "Caucasian" in a recidivism input and the prediction flips, what does that imply about the model's decision boundary?

- Concept: **Group Fairness Metrics (Dm family)**
  - Why needed here: The paper evaluates success using D_TPR, D_FPR, D_sp—differences in metric values across groups. You need to understand what each measures to interpret Table I results.
  - Quick check question: If D_TPR = 0.32 for African-American vs. Caucasian groups, what does that mean about false negative rates?

- Concept: **In-Context Learning and Prompt Bias**
  - Why needed here: The experimental design uses 8 prompts with varying bias levels (P1-P8). Prompt construction directly influences model behavior, making prompt engineering a confounding factor to control.
  - Quick check question: Why might prompt P4 (all examples associate African-American with recidivism) produce higher baseline bias than P2 (explicit "be unbiased" instruction)?

## Architecture Onboarding

- Component map:
  - Perturbation Generator -> LLM Forward Pass Engine -> Consistency Calculator -> Threshold Comparator -> Prediction Output
  - Fairness Evaluator (post-hoc only)

- Critical path:
  1. Receive input sentence x and prompt Pj.
  2. Generate N perturbed variants {x1, ..., xN} by modifying sensitive attribute values.
  3. Execute N+1 forward passes (original + perturbations) through LLM.
  4. Calculate CR. If CR < t, return flipped prediction; else return original.

- Design tradeoffs:
  - **N (perturbation count)**: Higher N improves detection reliability (Figure 5 shows improvement from 8→16→32) but increases latency linearly. Paper finds N=8 still effective.
  - **Threshold t**: Higher t is more stringent (requires more consistency). Paper finds t=0.9 optimal; t>0.9 sacrifices D_Precision for other metrics (Figure 6).
  - **Attribute selection**: Only attributes included in perturbations are probed. Omitted attributes (e.g., socioeconomic indicators) may harbor undetected bias.

- Failure signatures:
  - **Precision degradation**: If D_Precision increases after adjustment (Table I shows this for P2, P5, P7), the method may be over-correcting on certain prompt configurations.
  - **Prompt-specific inconsistency**: Large variance across prompts (P1-P8) suggests method sensitivity to in-context examples, not just input attributes.
  - **Computational overhead**: N forward passes per input may be prohibitive for real-time systems at scale.

- First 3 experiments:
  1. **Baseline bias measurement**: Run Llama3 on COMPAS with all 8 prompts. Compute Dm metrics for original predictions. Confirm bias exists (Figure 3).
  2. **Ablation on N**: Apply method with N={8,16,32} on Prompt P1, t=0.9. Measure Dm reduction and latency. Identify minimum viable N (Figure 5).
  3. **Threshold sweep**: Apply method with t={0.7,0.8,0.9,1.0} across all prompts, N=8. Plot Dm metrics and accuracy/precision to find optimal t (Figures 6-7).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the testing-time adversary method generalize effectively to LLM architectures beyond the Llama family, particularly to closed-source models with different training paradigms?
- Basis in paper: [explicit] The authors state "we limit our analysis to only two models due to computational constraints and leave the employment of more LLMs for future research."
- Why unresolved: Only Llama2 7B and Llama3 8B were tested; different architectures may exhibit different bias patterns or respond differently to perturbation-based detection.
- What evidence would resolve it: Experiments applying the method to other model families (e.g., GPT, Claude, Mistral, Gemma) with comparisons of fairness improvement magnitudes and consistency rate behaviors.

### Open Question 2
- Question: What is the optimal methodology for selecting the consistency threshold (t) across different tasks, datasets, and fairness requirements?
- Basis in paper: [inferred] The threshold t=0.9 was selected based on "initial analysis" on a "small validation set" without systematic justification; the paper shows fairness metrics vary non-monotonically with threshold.
- Why unresolved: The threshold critically affects fairness-performance trade-offs, but no principled approach for setting it is provided; different applications may require different operating points.
- What evidence would resolve it: A systematic study of threshold selection strategies (e.g., validation-based, fairness-constrained optimization) across multiple datasets with analysis of sensitivity and robustness.

### Open Question 3
- Question: How does the method perform on tasks involving more than two protected groups or intersectional fairness concerns (e.g., race × gender combinations)?
- Basis in paper: [inferred] The COMPAS experiments only consider binary racial groups (African-American vs. Caucasian); perturbations modify single attributes independently without addressing intersectional bias.
- Why unresolved: Real-world fairness often involves multiple intersecting protected attributes; the current approach may not capture compounding or interacting biases.
- What evidence would resolve it: Experiments on datasets with multiple protected attributes, comparing single-attribute perturbations against joint perturbations and measuring fairness across intersectional subgroups.

## Limitations

- **Prompt Sensitivity**: The method's effectiveness varies significantly across the 8 prompts (P1-P8), suggesting high sensitivity to in-context examples rather than just input attributes.
- **Attribute Coverage Gap**: Only three sensitive attributes (race, gender, charge degree) are perturbed. Bias encoded in correlated but unprobed features may remain undetected.
- **Computational Cost**: Requiring N+1 forward passes per inference increases latency linearly with N, potentially limiting real-time deployment at scale.

## Confidence

- **High Confidence**: The core mechanism (consistency rate as bias signal) and its implementation are clearly specified and empirically validated on the COMPAS dataset with Llama3.
- **Medium Confidence**: The 27 percentage point improvement claim is supported by specific metric reductions but may not generalize to other models, datasets, or task types beyond binary classification.
- **Low Confidence**: The assertion that this approach "eliminates the need for training or fine-tuning" overstates the case—the method still requires careful prompt engineering and threshold selection.

## Next Checks

1. **Cross-Model Generalization**: Apply the method to GPT-4 and Claude on the same COMPAS task to verify the consistency-based approach works beyond Llama3.
2. **Multi-Class Extension**: Test whether the CR-based flipping mechanism generalizes to multi-class prediction tasks, where "flipping" becomes non-trivial.
3. **Real-World Deployment Stress Test**: Measure end-to-end latency and accuracy degradation when scaling N=32 perturbations across thousands of concurrent inference requests in a production LLM API setting.