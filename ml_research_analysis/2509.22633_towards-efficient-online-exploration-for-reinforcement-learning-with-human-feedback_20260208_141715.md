---
ver: rpa2
title: Towards Efficient Online Exploration for Reinforcement Learning with Human
  Feedback
arxiv_id: '2509.22633'
source_url: https://arxiv.org/abs/2509.22633
tags:
- rmax
- logt
- policy
- have
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of efficient exploration in
  online reinforcement learning from human feedback (RLHF), where the goal is to adaptively
  collect preference data to refine both the reward model and policy. The authors
  analyze existing optimism-based exploration algorithms and identify a key drawback:
  they tend to gather comparisons that fail to reduce the most informative uncertainties
  in reward differences.'
---

# Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback

## Quick Facts
- arXiv ID: 2509.22633
- Source URL: https://arxiv.org/abs/2509.22633
- Reference count: 4
- One-line primary result: First online RLHF algorithm with regret scaling polynomially in all model parameters, avoiding exponential dependencies of prior work.

## Executive Summary
This paper addresses the challenge of efficient exploration in online reinforcement learning from human feedback (RLHF), where the goal is to adaptively collect preference data to refine both the reward model and policy. The authors analyze existing optimism-based exploration algorithms and identify a key drawback: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences. They prove lower bounds showing these methods can incur linear regret over exponentially long horizons. To address this, the paper proposes a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement.

## Method Summary
The method addresses online RLHF by introducing an adaptive exploration scheme that overcomes the exponential regret problem of existing algorithms. The key innovation is using adjacent policy pairs (π^(t-1) and π^(t)) for preference sampling rather than fixed calibration policies, combined with an optimistic regularized MLE objective that encourages exploration of reward functions that would justify exploring higher-value regions. Under a multi-armed bandit model of RLHF, the algorithm achieves regret bounds of order T^(β+1)/(β+2), where β controls the exploration-exploitation tradeoff.

## Key Results
- Proves lower bounds showing existing optimism-based algorithms incur linear regret over exponentially long horizons
- Introduces a new exploration scheme with regret bounds of order T^(β+1)/(β+2)
- Achieves polynomial dependence on all model parameters (A, r_max, β), avoiding exponential dependencies
- Demonstrates the first online RLHF algorithm with polynomial regret scaling

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Calibration Policy
Using π^(t) as the calibration policy at iteration t avoids exponential regret that fixed calibration policies incur. By making the calibration policy dynamic, the algorithm focuses on uncertainties between improving regions rather than wasting queries on comparisons that don't reduce uncertainty about reward differences relative to the optimal action.

### Mechanism 2: Adjacent Policy Pair Sampling
Sampling a_t^1 ~ π^(t-1) and a_t^2 ~ π^(t) yields preference data that directly reduces uncertainty relevant for the next policy update. Since π^(t) is optimized to favor actions with high estimated reward relative to π^(t-1), comparing actions from these two policies specifically targets the reward difference uncertainty that matters for policy improvement.

### Mechanism 3: Optimistic Regularized MLE
Adding α_t · J^*(r; π^(t)) to the MLE objective encourages exploration of reward functions that would yield better policies, with polynomial rather than exponential regret. This implements optimism by preferring reward functions that would justify exploring higher-value regions, bounding the KL divergence between estimated and true preference probabilities proportionally to α_t.

## Foundational Learning

- **Concept: Bradley-Terry Preference Model**
  - Why needed here: Underlies how human preferences are modeled as a function of reward differences; the entire MLE framework and regret analysis depend on understanding σ(r*(x,a_1) - r*(x,a_2))
  - Quick check question: Given rewards r*(a_1) = 2, r*(a_2) = 0, what is P(a_1 ≻ a_2)?

- **Concept: KL-Regularized RL Objective**
  - Why needed here: The objective J(π, r; π_cal) includes a KL penalty to prevent the policy from deviating too far from π_ref; β controls the exploration-exploitation tradeoff and directly affects regret exponent (β+1)/(β+2)
  - Quick check question: If β → 0, what happens to the optimal policy π_r? If β → ∞?

- **Concept: Optimism Principle in Bandits/RL**
  - Why needed here: The core insight that adding uncertainty bonuses (or optimistic reward estimates) drives efficient exploration; must understand UCB-style reasoning to follow why α_t · J^*(r; π^(t)) implements optimism
  - Quick check question: Why does optimism avoid the need to explicitly maintain posterior distributions over reward functions?

## Architecture Onboarding

- **Component map:**
  Input layer (π^(0), π^(1), α_t schedule, ρ) -> Sampling module (xt ~ ρ, at1 ~ π^(t-1), at2 ~ π^(t)) -> Preference oracle (returns a_t+ ≻ a_t-) -> Reward optimizer (solves equation 3.2a) -> Policy optimizer (solves equation 3.2b) -> Data store (accumulating D^(t))

- **Critical path:** The loop in Algorithm 1 (lines 3-5) must complete within human feedback latency constraints; reward optimization (3.2a) is the computational bottleneck.

- **Design tradeoffs:**
  - α_t schedule: Larger α_t → more exploration but higher variance; paper suggests α_t = A log T + t^(1/(β+2)) · (r_max/κ)^(β/(β+2)) · (log T / A(r_max + log T))^((β+1)/(β+2))
  - β parameter: Smaller β → faster convergence rate T^(β+1)/(β+2) but higher risk of policy divergence from π_ref
  - Assumption 1's κ and τ: If π_ref poorly aligns with human preference (large κ), must choose smaller β

- **Failure signatures:**
  - Linear regret over many iterations: Check if a high-value action is never sampled (run action frequency diagnostics)
  - π^(t) collapsing to uniform: α_t may be too large, swamping the MLE term
  - No policy improvement: Adjacent policies may have disjoint support; increase overlap via smaller policy updates

- **First 3 experiments:**
  1. Sanity check on Example 1 (3-armed bandit): Implement Algorithm 1 with r*(a_0)=1, r*(a_1)=r*(a_2)=0, π_ref uniform. Verify that a_0 is discovered within O(poly(T)) iterations, not O(exp(r_max/β)).
  2. Ablation on α_t: Compare α_t = constant vs. α_t = scheduled per Proposition 3. Measure regret R(T) over T=1000 iterations. Expect scheduled α_t to achieve R(T) ≈ O(T^(β+1)/(β+2)).
  3. Comparison vs. VPO baseline: Run both algorithms on a multi-armed bandit with A=10 actions, varying κ (alignment between π_ref and π_HF). Plot regret curves; expect VPO to suffer when κ is large (misaligned π_ref) while Algorithm 1 remains robust.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the regret rate of order T^(β+1)/(β+2) minimax optimal for online RLHF, or can faster rates be achieved while maintaining polynomial dependence on model parameters?
- Basis: [explicit] "An immediate question is whether the rate T^(β+1)/(β+2) is minimax optimal, or if faster rates can be achieved." (Section 6)
- Why unresolved: The paper conjectures that avoiding exponential parameter dependence inevitably requires a slower convergence rate, but does not provide a lower bound proof for this trade-off.
- What evidence would resolve it: A minimax lower bound proof confirming the rate is tight, or a novel algorithm achieving O(√T) regret with polynomial parameter dependence.

### Open Question 2
- Question: Can the theoretical guarantees for the proposed exploration scheme be extended to richer environments that incorporate a non-empty prompt space (contextual setting)?
- Basis: [explicit] "Finally, our theoretical results are restricted to the bandit setting; extending the analysis to richer environments that incorporate a prompt space would be an exciting step..." (Section 6)
- Why unresolved: The current analysis relies on a multi-armed bandit model (X=∅), whereas practical RLHF for LLMs requires handling a distribution of prompts.
- What evidence would resolve it: A theoretical extension of the regret bounds to a contextual bandit or Markov Decision Process (MDP) framework.

### Open Question 3
- Question: Can the polynomial dependence on the action space size (A) and maximum reward (r_max) be refined through sharper analysis or alternative exploration schemes?
- Basis: [explicit] "Another important direction is to refine the dependence on parameters such as A and r_max, which may be improved with sharper analysis or alternative exploration schemes." (Section 6)
- Why unresolved: The current bounds involve high-degree polynomial factors (e.g., A^3, A^(2.5)) which might be loose and limit practical applicability.
- What evidence would resolve it: A refined analysis of the current algorithm or a new algorithm with lower-order polynomial dependencies on A and r_max.

## Limitations
- Analysis assumes a multi-armed bandit model; generalization to structured state-action spaces (e.g., LLM token generation) is not established
- Dependence on Assumption 1 (existence of κ and τ) is critical but not empirically validated
- Reward model optimization is stated abstractly without specification of parameterization or computational feasibility

## Confidence
- **High**: The lower bound proof (Proposition 1) demonstrating linear regret for VPO-style algorithms is rigorous and well-supported
- **Medium**: The upper bound proof (Theorem 1) establishing polynomial regret for Algorithm 1 is mathematically sound but relies on Assumption 1
- **Medium**: The mechanism analysis (adjacent policy sampling, adaptive calibration) is logically consistent but not experimentally validated beyond the synthetic Example 1

## Next Checks
1. Implement Algorithm 1 on a structured RLHF task (e.g., instruction tuning with preference feedback) to test whether polynomial regret holds beyond the multi-armed bandit setting
2. Design an experiment to measure the sensitivity of regret to κ (alignment between π_ref and π_HF); compare Algorithm 1 against VPO across varying κ to quantify robustness
3. Analyze the computational cost of solving equation 3.2a in high-dimensional action spaces; determine whether the reward optimization remains tractable or requires approximation