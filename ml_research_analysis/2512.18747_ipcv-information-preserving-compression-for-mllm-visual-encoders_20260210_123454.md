---
ver: rpa2
title: 'IPCV: Information-Preserving Compression for MLLM Visual Encoders'
arxiv_id: '2512.18747'
source_url: https://arxiv.org/abs/2512.18747
tags:
- tokens
- token
- ipcv
- arxiv
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IPCV, a training-free, information-preserving
  compression framework for MLLM visual encoders. IPCV enables aggressive token pruning
  inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs
  pruned tokens to participate in attention with minimal overhead, then fully restores
  them before passing to the LLM.
---

# IPCV: Information-Preserving Compression for MLLM Visual Encoders

## Quick Facts
- arXiv ID: 2512.18747
- Source URL: https://arxiv.org/abs/2512.18747
- Authors: Yuan Chen; Zichen Wen; Yuzhou Wu; Xuyang Liu; Shuang Chen; Junpeng Ma; Weijia Li; Conghui He; Linfeng Zhang
- Reference count: 40
- Key outcome: IPCV enables aggressive token pruning inside ViT via Neighbor-Guided Reconstruction (NGR) and Attention Stabilization (AS), reducing end-to-end computation while outperforming training-free token compression methods across image and video benchmarks.

## Executive Summary
This paper introduces IPCV, a training-free framework for compressing visual tokens in Multimodal Large Language Models (MLLMs) without sacrificing semantic information. The core innovation is Neighbor-Guided Reconstruction (NGR), which reconstructs pruned tokens by transferring feature updates (deltas) from their most similar retained neighbors, allowing these tokens to participate in attention computations before being fully restored for LLM input. Combined with Attention Stabilization (AS) that approximates Keys/Values for pruned tokens, IPCV achieves substantial speedups while maintaining accuracy across diverse visual reasoning tasks.

## Method Summary
IPCV implements token pruning at layer 3 of a Vision Transformer using feature-difference importance scores, then reconstructs pruned tokens using NGR by transferring neighbor deltas across layers. The AS component temporarily includes reconstructed tokens in attention calculations (but skips FFN) for layers [3, 10) to maintain feature stability. Finally, all tokens are fully reconstructed and restored to their original sequence positions before being passed to the LLM. The framework is evaluated on Qwen2-VL-7B-Instruct with benchmarks including GQA, MMBench, MVBench, and OCRBench, demonstrating significant latency reductions while preserving accuracy.

## Key Results
- IPCV achieves up to 55% relative GPU latency reduction while maintaining competitive accuracy across image and video benchmarks
- Outperforms state-of-the-art training-free compression methods (ToMe, ToFu, FastV, DART, SparseVLM) on MMBench, GQA, and video datasets
- Demonstrates compatibility with LLM-side compression methods like DART when integrated with IPCV
- Maintains performance at 50% retention ratio but shows accuracy drops on OCR tasks at aggressive 20% retention

## Why This Works (Mechanism)

### Mechanism 1: Neighbor-Guided Delta Transfer (NGR)
- **Claim:** Pruned tokens can be accurately reconstructed by transferring the "update delta" from the most similar retained tokens.
- **Mechanism:** Instead of copying neighbor features, NGR computes the change in retained tokens from pruning layer to target layer, applying this average delta to the pruned token's original value.
- **Core assumption:** Tokens with similar features in shallow layers exhibit similar evolutionary trajectories through deep layers.
- **Evidence anchors:** Abstract mentions NGR "temporarily reconstructs pruned tokens," section 3.2 discusses similar tokens exhibiting similar deltas, and figure 2 visualizes trajectory similarity distributions.

### Mechanism 2: Attention Stabilization via Ghost Tokens (AS)
- **Claim:** Maintaining attention context of pruned tokens without FFN processing stabilizes the feature space for retained tokens.
- **Mechanism:** Pruned tokens reconstructed via NGR serve as Keys/Values in self-attention for several layers, then are dropped before FFN to reduce computational overhead.
- **Core assumption:** The distortion from missing attention partners outweighs reconstruction costs; FFN dominates compute more than attention.
- **Evidence anchors:** Abstract mentions AS "alleviates negative influence... by approximating K/V," section 3.3 motivates the strategy based on Skip-Vision's observation that FFN dominates computation.

### Mechanism 3: Full-Sequence Reintegration
- **Claim:** Restoring original token sequence length before LLM projector preserves interface compatibility and enables standard LLM-side compressors.
- **Mechanism:** After final ViT block, all pruned tokens are reconstructed and merged back into original index positions, creating a complete visual sequence for the Projector/LLM.
- **Core assumption:** The multimodal projector and LLM expect specific spatial structure that, if disrupted by early pruning, degrades reasoning capabilities.
- **Evidence anchors:** Section 3.4 guarantees downstream LLM receives same number and ordering of visual tokens, and section 5.2 shows compatibility with LLM-side methods like FastV and DART.

## Foundational Learning

- **Concept:** **Bidirectional vs. Causal Attention**
  - **Why needed here:** You must understand why pruning in standard ViT (bidirectional) is riskier than in LLM (causal). In ViT, removing a token changes context for all other tokens immediately, potentially destabilizing global feature learning.
  - **Quick check question:** Why does removing a token in a Vision Transformer affect features of a token on the opposite side of the image, but not necessarily in a standard LLM?

- **Concept:** **Token Delta / Residual Updates**
  - **Why needed here:** NGR relies on tokens evolving through series of updates. Understanding that h_final = h_initial + ΣΔ is crucial to seeing why transferring Δ from neighbor differs from copying neighbor's final value.
  - **Quick check question:** If neighbor token changes from "car" to "vehicle" (semantic generalization), would applying that delta to a "wheel" token work? Would it work if neighbor changed to "sky"?

- **Concept:** **Training-Free Inference Optimization**
  - **Why needed here:** IPCV adds logic during forward pass without backpropagation or weight updates. You need to distinguish between "dynamic graph modification" (what IPCV does) and "model compression" (weight pruning).
  - **Quick check question:** Does IPCV require dataset to fine-tune pruning thresholds before deployment on production model?

## Architecture Onboarding

- **Component map:** Pruning Layer (early ViT) -> AS Module (middle ViT) -> Final Reintegration (ViT output) -> Projector
- **Critical path:** Index Set (I_keep, I_rem) generated at pruning layer must be stored and propagated alongside token tensors through network to ensure correct neighbor matching and reinsertion indices.
- **Design tradeoffs:**
  - Increasing k (neighbors) or Delta_lmax (AS depth) improves reconstruction accuracy but adds compute overhead
  - High retention (65%) preserves performance; aggressive retention (20%) relies heavily on NGR accuracy and may fail on OCR tasks
- **Failure signatures:**
  - OCR Collapse: Text-heavy benchmarks show sharp declines if pruning is too aggressive
  - High Overhead: If attention implementation is highly optimized but indexing/reconstruction logic is not, reconstruction overhead may dominate savings
- **First 3 experiments:**
  1. Measure inference time of Vanilla ViT vs. IPCV-ViT with 0% pruning to quantify framework overhead
  2. Ablate NGR: Compare "Copy Neighbor Value" vs. "Apply Neighbor Delta" on small MMBench subset to verify core hypothesis
  3. Run layer sweep of lp (pruning start layer) on video benchmark to find tipping point where semantic loss outweighs speed gains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can NGR mechanism be refined to use adaptive weighting rather than simple averaging to improve semantic preservation?
- **Basis in paper:** Conclusion states "Future work may focus on refining the reconstruction mechanism... to improve semantic preservation." Current NGR uses uniform mean (1/k) of neighbor updates, which may introduce noise if neighbors vary in actual similarity.
- **Why unresolved:** Paper establishes validity of transfer but doesn't explore optimization strategies for aggregation function beyond mean.
- **What evidence would resolve it:** Experiments comparing uniform averaging against attention-weighted or similarity-weighted reconstruction deltas on fine-grained benchmarks like OCRBench.

### Open Question 2
- **Question:** How does depth of Attention Stabilization (Delta_lmax) theoretically interact with bidirectional attention mechanism to affect feature distortion?
- **Basis in paper:** Authors explicitly request to "further analyze the Attention Stabilization component to improve semantic preservation and computational efficiency."
- **Why unresolved:** Paper empirically sets Delta_lmax=7 and notes stability, but lacks theoretical derivation for optimal duration of token retention in attention layers.
- **What evidence would resolve it:** Theoretical analysis or layer-wise probing study showing saturation point where reconstructed tokens no longer contribute meaningful gradient information.

### Open Question 3
- **Question:** Can IPCV be optimized to reduce KV cache memory footprint while maintaining inference latency advantages?
- **Basis in paper:** Section 5.1 notes IPCV retains more tokens for LLM than LLM-side pruning methods, "consequently incurs larger KV caches," presenting trade-off between speed and memory usage.
- **Why unresolved:** Current design prioritizes reducing computation but accepts memory overhead of passing restored full sequence to LLM.
- **What evidence would resolve it:** Hybrid approach integrating lightweight eviction strategy within Reintegration module to lower memory usage without degrading accuracy.

## Limitations
- The NGR mechanism's accuracy depends on assumption that spatially local tokens maintain similar semantic trajectories across layers, which may fail for objects with large internal feature variations
- The AS component's effectiveness is primarily validated through end-to-end benchmarks rather than ablation studies isolating K/V stabilization impact
- Claims about OCRBench performance at 20% retention should be treated cautiously as paper shows accuracy drops but doesn't characterize failure modes or provide reconstruction quality metrics

## Confidence
- **High confidence**: Core framework design (pruning at layer 3, reintegration before LLM, compatibility with LLM-side compressors) is well-specified and reproducible
- **Medium confidence**: NGR reconstruction mechanism works as described for general image understanding tasks but robustness across diverse semantic distributions remains unproven
- **Low confidence**: OCRBench performance claims at 20% retention require cautious interpretation due to lack of detailed failure analysis

## Next Checks
1. **Delta trajectory validation**: Select 100 random images from MMBench and visualize feature evolution of pruned vs. retained tokens across layers. Quantify average L2 distance between original pruned tokens and NGR-reconstructed tokens to measure reconstruction fidelity independent of downstream task performance.
2. **Attention stabilization ablation**: Implement variant of IPCV that uses NGR for reconstruction but skips AS module entirely (pruned tokens never participate in attention). Compare this baseline against full IPCV on MVBench to isolate contribution of K/V stabilization.
3. **Memory-accuracy Pareto analysis**: Systematically vary k (neighbors), lp (pruning layer), and retention ratios to map full trade-off frontier. Identify configurations where IPCV transitions from net positive to net negative in terms of accuracy per FLOP saved, particularly for video benchmarks where temporal coherence may affect reconstruction quality.