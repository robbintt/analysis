---
ver: rpa2
title: Democratic or Authoritarian? Probing a New Dimension of Political Biases in
  Large Language Models
arxiv_id: '2506.12758'
source_url: https://arxiv.org/abs/2506.12758
tags:
- democracy
- leader
- autocracy
- electoral
- authoritarian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines political biases in LLMs along the democracy-authoritarianism
  spectrum using three probing methods: an adapted F-scale for authoritarian attitudes,
  a FavScore metric for leader favorability, and role-model generation tasks. Across
  eight diverse LLMs in English and Mandarin, models generally show pro-democratic
  leanings in English but exhibit reduced differentiation between regime types in
  Mandarin.'
---

# Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models

## Quick Facts
- arXiv ID: 2506.12758
- Source URL: https://arxiv.org/abs/2506.12758
- Reference count: 40
- Primary result: LLMs show pro-democratic leanings in English but reduced regime differentiation in Mandarin, with systematic geopolitical biases varying by language

## Executive Summary
This study examines political biases in large language models along the democracy-authoritarianism spectrum using three complementary probing methods. Across eight diverse LLMs in English and Mandarin, models consistently show stronger pro-democratic tendencies in English compared to Mandarin. The research reveals that even in non-political contexts, LLMs frequently cite authoritarian figures as role models, and leader evaluations show greater regime-type separation in English. These findings demonstrate that geopolitical biases are not only present but systematically vary by language, raising concerns about how LLMs may shape global political perspectives differently across linguistic contexts.

## Method Summary
The study employs three probing methods to assess political biases: an adapted F-scale measuring authoritarian attitudes through 30 statements rated on a 6-point Likert scale, a FavScore metric evaluating leader favorability across 197 world leaders using 39 questions rated on a 4-point scale (rescaled to [-1,+1]), and role-model generation tasks where models name influential figures from 222 nationalities. All probes were conducted in both English and Mandarin, with responses processed through API calls at temperature=0. An LLM-as-judge (Gemini 2.5 Flash) classified role-model outputs for political nature and regime alignment, validated through human annotation showing 91% agreement. Statistical significance was assessed using sign tests, permutation tests, and chi-squared tests.

## Key Results
- F-scale scores were systematically higher in Mandarin than English across all models, with statistically significant differences for Claude 3.7 Sonnet, Llama 4 Maverick, Mistral-8B, and GPT-4o
- Leader favorability showed greater separation between democratic and authoritarian leaders in English (Wasserstein Distance 0.14-0.24) than Mandarin (0.04-0.15)
- 35.9% of English political role models were classified as authoritarian (42.0% in Mandarin), with figures like Putin, Xi Jinping, and Khomeini frequently cited even in non-political contexts

## Why This Works (Mechanism)

### Mechanism 1
Language of prompting systematically shifts LLM political alignment along the democracy-authoritarianism axis. Training corpora contain language-specific ideological patterns, with English corpora overrepresenting Western democratic discourse while Mandarin corpora may contain state-aligned or translation-mediated content that normalizes authoritarian framing. Core assumption: learned representations encode culturally-specific political priors that activate differently based on input language tokens. Evidence: models show pro-democratic leanings in English but reduced differentiation between regime types in Mandarin; F-scale scores systematically higher in Mandarin; related work confirms LLM outputs carry systematic biases conditioned by prompt language.

### Mechanism 2
Three-probe methodology triangulates political bias across value, figure, and context dimensions. F-scale probes abstract authoritarian values; FavScore evaluates concrete political figures; role-model probing surfaces latent preferences without explicit political framing. Core assumption: bias manifests differently across abstraction levels; models may reject authoritarian values in abstract while still normalizing authoritarian figures as exemplars. Evidence: methodology combines F-scale, FavScore, and role-model probing; models frequently list authoritarian leaders as role models despite non-political framing; limited corpus support for this specific triangulation approach.

### Mechanism 3
Wasserstein Distance quantifies regime-type separation more robustly than mean differences alone. WD captures distributional differences in central tendency, variance, and shape simultaneously—critical when models may show similar average favorability but different variance patterns toward democratic vs authoritarian leaders. Core assumption: political bias manifests not just in mean favorability shifts but in distribution overlap and discriminability. Evidence: WD provides comprehensive comparison across regime types; English WDs (0.14-0.24) vs Mandarin WDs (0.04-0.15) show stronger regime separation in English; methodological novelty requires further validation.

## Foundational Learning

- **Concept: F-scale (Adorno et al., 1950)** - Psychometric foundation for measuring authoritarian personality traits; adaptation required because original scale targets humans, not LLMs. Quick check: Can you explain why a 6-point Likert scale (no neutral option) was chosen over the 7-point version tested in ablation?

- **Concept: V-Dem Regime Classification (Lührmann et al., 2018)** - Ground truth labeling for democratic/authoritarian leader classification; provides four-category typology collapsed into binary for analysis. Quick check: What are the four regime types, and which category might create ambiguous classifications?

- **Concept: LLM-as-Judge Evaluation** - Role-model classification requires political assessment at scale; Gemini 2.5 Flash used as judge with human validation showing 91% inter-annotator agreement. Quick check: What two validation steps did the authors use to mitigate judge self-preference bias?

## Architecture Onboarding

- **Component map:** F-scale statements → 6-point Likert → mean score aggregation; 39 survey questions → 4-point Likert → FavScore [-1,+1] → WD computation; nationality prompts → name extraction → LLM judge classification (political? regime alignment? democratic/authoritarian?); validation layer: cross-model judge agreement, human annotation, refusal analysis

- **Critical path:** 1) Prompt design (neutral framing, forced-choice format, JSON structure); 2) API querying (temperature=0, parallelized, refusal handling); 3) Response parsing and rescaling; 4) Statistical testing (sign test, permutation test, chi-squared)

- **Design tradeoffs:** Budget constraints → FavScore/Role-Model run once per condition (vs. 3 runs for F-scale); Forced-choice scales → eliminates hedging but may force false precision; LLM judge → scalable but introduces judge-model bias; mitigated via cross-validation

- **Failure signatures:** High refusal rates (Claude 3.7 Sonnet: 33%, Grok 3 Beta Mandarin: 68%) → limited interpretability; High hedging rates (Llama 4 Mandarin: 49%) → semantic refusal despite structural compliance; Refusal imbalance by regime type → potential selection bias (not observed)

- **First 3 experiments:** 1) Replicate F-scale in a third language (e.g., Spanish or Arabic) to test language-generalization of authoritarian shift; 2) Ablate prompt framing: compare "Do you approve or disapprove" vs. "Rate your approval" to quantify framing sensitivity; 3) Longitudinal tracking: rerun FavScore monthly to detect model-update-induced political drift

## Open Questions the Paper Calls Out

- **Open Question 1:** Do democracy-authoritarianism biases manifest similarly in low-resource languages or other high-resource languages outside of English and Mandarin? The Conclusion states that "Future research should explore this phenomenon across more languages," and the Limitations section notes that focusing on English and Mandarin "limits generalizability to other languages, especially low-resource ones." This remains unresolved due to budget and feasibility constraints limiting the study to only two languages.

- **Open Question 2:** How do these latent geopolitical biases in role-model generation and leader evaluation influence user perceptions or behaviors in real-world downstream applications? The Conclusion explicitly calls for future work to "examine its effects on downstream applications," and Section 5.3 warns that "interpretive ambiguity may pose risks, especially in educational contexts." The study focused on quantifying model outputs rather than measuring causal impact on human users or specific downstream tasks.

- **Open Question 3:** Are the observed language-dependent differences in political bias primarily driven by distinct training corpora or by translation artifacts introduced during prompting? Section 5.2 hypothesizes that the strong pro-democratic contrast in English reflects training data, while the reduced differentiation in Mandarin may reflect "state-aligned content or translation effects," without isolating the cause. The study detects correlation between prompt language and bias but does not perform causal analysis to determine if the source is underlying training data distribution or translation process itself.

## Limitations
- Absence of a truly neutral reference language makes it impossible to determine whether English represents "less bias" or simply different cultural priors
- LLM-as-judge approach introduces potential self-preference bias that may inflate democratic-leaning classifications
- High refusal rates in certain models (Grok 3 Beta Mandarin: 68%) and substantial hedging in others (Llama 4 Mandarin: 49%) limit reliability of results

## Confidence
- **High confidence:** Observed language-conditioned shifts in F-scale scores and differential Wasserstein Distances between English and Mandarin probes are robust findings supported by multiple models and statistical significance testing
- **Medium confidence:** Role-model generation results showing authoritarian figures cited as exemplars are more vulnerable to judge-model bias and refusal effects
- **Medium confidence:** Leader favorability differences between regime types may be influenced by specific selection of 39 questions and could vary with different political figures or question framing

## Next Checks
1. **Cross-linguistic validation:** Replicate the F-scale and FavScore probes in a third language (e.g., Spanish or Arabic) with known political context to test whether observed biases generalize beyond the English-Mandarin comparison
2. **Judge independence audit:** Run role-model classifications through at least two independent LLM judges (different models or fine-tuned versions) to quantify and adjust for self-preference bias effects
3. **Temporal stability test:** Re-run all three probes across multiple model versions over time to determine whether political biases are stable characteristics or shift with model updates and training data changes