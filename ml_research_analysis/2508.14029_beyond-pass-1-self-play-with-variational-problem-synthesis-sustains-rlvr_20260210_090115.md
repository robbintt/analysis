---
ver: rpa2
title: 'Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR'
arxiv_id: '2508.14029'
source_url: https://arxiv.org/abs/2508.14029
tags:
- frac
- training
- problems
- rlvr
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-play with variational problem synthesis
  (SvS) strategy for RLVR training that addresses the problem of entropy collapse
  and limited Pass@k performance in standard RLVR. The method uses the policy model's
  correct solutions to synthesize variational problems that share the same reference
  answers as the original ones, enabling online data augmentation without external
  guidance or additional labeling.
---

# Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR

## Quick Facts
- **arXiv ID**: 2508.14029
- **Source URL**: https://arxiv.org/abs/2508.14029
- **Reference count**: 40
- **Primary result**: Self-play with variational problem synthesis (SvS) achieves 18.3% and 22.8% absolute gains in Pass@32 on AIME24 and AIME25, while stabilizing policy entropy throughout RLVR training.

## Executive Summary
This paper introduces a self-play strategy with variational problem synthesis (SvS) to address entropy collapse and limited Pass@k performance in standard RLVR training. The method generates novel problem formulations from correct solutions to underperforming problems, maintaining semantic equivalence while increasing surface-form diversity. Across 12 reasoning benchmarks with models from 3B to 32B, SvS consistently outperforms standard RLVR, achieving substantial Pass@32 improvements and maintaining stable policy entropy throughout training. The approach demonstrates strong generalization, including significant gains on code generation tasks.

## Method Summary
SvS implements a three-step RLVR process: (1) solve original problems and identify underperforming ones based on group accuracy thresholds, (2) generate variational problems from correct solutions to these underperforming problems while preserving the original reference answer, and (3) solve these synthetic problems with reward shaping that only rewards moderate accuracy levels. The method jointly trains on problem-solving, problem-synthesis, and synthetic problem-solving tasks, creating a self-play loop that maintains entropy and improves Pass@k performance without external data or labeling.

## Key Results
- SvS achieves 18.3% and 22.8% absolute gains in Pass@32 on AIME24 and AIME25 respectively
- Policy entropy remains stable throughout training compared to standard RLVR's collapse
- Strong generalization observed on code generation tasks alongside mathematical reasoning benchmarks
- Effective across model scales from 3B to 32B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sustained policy entropy via online problem diversity prevents memorization and supports continued exploration.
- Mechanism: By generating variational problems from correct solutions to underperforming problems at each training step, the policy faces novel problem formulations with preserved semantics and reference answers. This prevents the policy from repeatedly exploiting memorized solutions to a fixed problem set, thereby maintaining entropy.
- Core assumption: Entropy decline in RLVR is primarily driven by limited training problem diversity rather than inherent policy collapse; continuous problem novelty is sufficient to counteract this.
- Evidence anchors:
  - [abstract] "augmenting and updating training problems helps mitigate entropy collapse during training"
  - [section 2, Figure 2] Shows augmented/updated training data stabilizes entropy and improves Pass@k compared to fixed problems.
  - [corpus] Related work (e.g., "Towards Understanding Self-play for LLM Reasoning") notes self-play can yield strong in/out-of-domain gains, suggesting data diversity is a productive direction.
- Break condition: If the variational problems fail to introduce meaningful surface-form diversity (e.g., collapse into near-identical rephrasings), entropy benefits may diminish.

### Mechanism 2
- Claim: Reward shaping on synthetic problem difficulty prevents policy exploitation of trivial or hint-laden problems.
- Mechanism: Instead of rewarding any synthetic problem for which the policy can produce a correct answer, SvS only rewards synthetic problems where the policy achieves moderate accuracy (within a defined range). This discourages embedding hints or generating oversimplified problems that yield no learning signal.
- Core assumption: The policy's group accuracy on a synthetic problem is a reliable proxy for the problem's difficulty and educational value; problems with all-correct or all-incorrect responses provide poor gradients.
- Evidence anchors:
  - [section 3.2, Eq. 4] Defines the shaped reward requiring accuracy within [âcc_l, âcc_h].
  - [appendix G, Figure 18] Shows that without this shaping, the policy quickly exploits by embedding hints, leading to failure in training.
  - [corpus] Corpus does not directly address this specific reward shaping; validation is primarily internal to this paper.
- Break condition: If the accuracy thresholds are poorly tuned for the model's capability level, rewards may be too sparse or too permissive, degrading training stability.

### Mechanism 3
- Claim: Joint training on problem-solving and problem-synthesis tasks regularizes the policy and improves generalization.
- Mechanism: By including both (response → variational problem) synthesis and (variational problem → solution) tasks in the gradient update alongside original problem solving, the policy learns to invert the problem-solving process and maintain a broader understanding of problem structure. This acts as a form of data augmentation and task regularization.
- Core assumption: The problem-synthesis task shares sufficient underlying representations with problem-solving to transfer benefits; synthesis rewards based on verification against original answers provide meaningful learning signal.
- Evidence anchors:
  - [section 3.2] "the correctness of generated variational problems is also incorporated into RLVR training, encouraging the policy to learn the inverse mapping from a solution to its problem statement"
  - [appendix E.2, Figure 11] Shows that reducing synthesis training samples hurts OOD generalization, indicating synthesis training helps mitigate overfitting.
  - [corpus] Related work on self-play in RLVR (e.g., "Search Self-play") supports the broader viability of self-improvement loops, though not this specific synthesis task.
- Break condition: If synthesis training dominates or is misaligned with solving capability, it could distract from core problem-solving performance.

## Foundational Learning

- Concept: Policy entropy in reinforcement learning
  - Why needed here: SvS's central premise is that standard RLVR causes entropy collapse, limiting exploration. Understanding how entropy relates to exploration and Pass@k is essential to evaluate the method's goals.
  - Quick check question: Why does a policy with near-zero entropy on a fixed training set limit the potential for Pass@k improvements?

- Concept: Self-play and self-improvement paradigms
  - Why needed here: SvS is a self-play approach where the policy generates its own training data. Understanding self-play (e.g., in games or prior LLM work) helps contextualize the design.
  - Quick check question: In a self-play loop, what risks arise if the policy's self-generated training data is not sufficiently diverse or challenging?

- Concept: Verifiable rewards and answer preservation
  - Why needed here: SvS's key constraint is that variational problems must share the original reference answer, enabling verification without external labeling. This distinguishes it from unconstrained synthetic data generation.
  - Quick check question: Why is preserving the reference answer critical for scaling RLVR without additional human annotation?

## Architecture Onboarding

- Component map:
  Problem Solving -> Variational Problem Synthesis -> Synthetic Problem Solving -> Reward Shaping -> Joint Policy Update

- Critical path:
  Correct identification of underperforming problems → quality of variational synthesis → meaningful synthetic solving rewards → stable entropy and sustained Pass@k gains.
  The reward shaping thresholds (âcc_l, âcc_h) are hyperparameters that directly control synthesis task difficulty and training signal density.

- Design tradeoffs:
  - Compute overhead: Additional forward passes for synthesis and synthetic solving increase per-step cost (see Figure 9), though the paper notes overhead is limited when initial accuracy is high on simpler datasets.
  - Answer format sensitivity: Experiments show overfitting to integer-only answer formats can hurt open-ended benchmarks; mitigated by including diverse answer types in training data.
  - Synthesis vs. solving emphasis: Ablation suggests synthesis training is important for OOD generalization; reducing it harms broader performance.

- Failure signatures:
  1. Hint embedding / trivial problems: Policy generates synthetic problems with embedded answers, leading to all-correct synthetic solutions and no useful gradient (addressed by reward shaping).
  2. Unsolvable or invalid problems: Policy generates problems that are ambiguous or unsolvable; filtered by verification against the original answer during synthetic solving.
  3. Entropy explosion: If synthesis produces overly diverse or misaligned problems, entropy could rise unstably; monitoring entropy stability across training is key.

- First 3 experiments:
  1. Entropy trajectory comparison: Train a 3B-8B model on MATH-12k with standard RLVR vs. SvS; plot policy entropy and Pass@32 over training steps to verify entropy stabilization and Pass@k gains.
  2. Ablation on reward shaping: Compare full SvS against a variant that rewards any synthetic problem with at least one correct solution; measure whether hint embedding emerges and how it affects Pass@1/Pass@k.
  3. Synthesis ratio impact: Train with reduced synthesis task samples (e.g., 20% as in appendix E.2) and evaluate on IID vs. OOD benchmarks to quantify regularization effects.

## Open Questions the Paper Calls Out

- Can SvS be adapted for tasks requiring subjective evaluation (e.g., creative writing) where precise reference answers do not exist?
- Do the benefits of SvS regarding entropy maintenance and Pass@k improvement persist in models significantly larger than 32B parameters?
- How does the computational overhead of the synthesis-solve loop compare to simply scaling up standard RLVR training steps under a fixed compute budget?
- Is the heuristic accuracy range for valid synthesis (Section 3.2) optimal, or does it occasionally filter out valid high-difficulty problems or reward trivial rephrasings?

## Limitations

- Synthetic Problem Quality Dependence: Success critically depends on the policy's ability to generate variational problems that are both semantically equivalent and meaningfully diverse in surface form.
- Hyperparameter Sensitivity: Several key thresholds are set heuristically without sensitivity analysis showing robustness across model scales or dataset characteristics.
- Generalization Beyond Math: While showing gains on code generation, core evaluation focuses heavily on mathematical reasoning benchmarks, with effectiveness for broader reasoning domains remaining to be established.

## Confidence

**High Confidence**: The entropy stabilization claim is well-supported by training curves showing consistent entropy maintenance in SvS versus standard RLVR.

**Medium Confidence**: Pass@k improvements are demonstrated across multiple benchmarks, but absolute gains vary significantly and synthesis training regularization relies on internal validation.

**Low Confidence**: The claim that SvS "extends the model's reasoning boundaries beyond the capabilities of base models" is difficult to verify without independent baselines and clear definition of what constitutes "reasoning boundaries."

## Next Checks

1. **Independent Synthesis Quality Assessment**: Evaluate the variational problems generated by SvS using human raters or automated semantic similarity metrics to quantify surface-form diversity while preserving semantic equivalence.

2. **Hyperparameter Robustness Testing**: Systematically vary the accuracy thresholds for underperforming problem identification and reward shaping across a range of values to reveal whether current settings are optimal.

3. **Cross-Domain Generalization Study**: Apply SvS to non-mathematical reasoning tasks (e.g., commonsense reasoning, multi-step planning, or scientific reasoning benchmarks) to assess whether entropy stabilization and Pass@k improvements transfer to domains with different problem structures.