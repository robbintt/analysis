---
ver: rpa2
title: Zeroth-Order Sharpness-Aware Learning with Exponential Tilting
arxiv_id: '2510.16157'
source_url: https://arxiv.org/abs/2510.16157
tags:
- zeroth-order
- zest
- have
- preprint
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work bridges zeroth-order optimization with sharpness-aware
  minimization by introducing a continuous spectrum of sharpness-aware objectives
  via exponential tilting. The core method, ZEST, leverages tilted SAM objectives
  parameterized by a tilting parameter t, smoothly interpolating between average-
  and max-loss formulations.
---

# Zeroth-Order Sharpness-Aware Learning with Exponential Tilting

## Quick Facts
- arXiv ID: 2510.16157
- Source URL: https://arxiv.org/abs/2510.16157
- Authors: Xuchen Gong; Tian Li
- Reference count: 40
- Primary result: ZEST achieves superior generalization and flatter minima than vanilla zeroth-order methods across classification, QA, and language generation tasks

## Executive Summary
This paper introduces ZEST (Zeroth-order Sharpness-Aware Learning with Exponential Tilting), a novel approach that bridges zeroth-order optimization with sharpness-aware minimization. By parameterizing the objective with an exponential tilting parameter, ZEST smoothly interpolates between average- and max-loss formulations, enabling gradient-free optimization that explicitly seeks flat minima for better generalization. The method introduces new zeroth-order algorithms using finite function evaluations with both naive and bias-corrected ratio-of-expectation estimators.

## Method Summary
ZEST optimizes a tilted SAM objective parameterized by tilting parameter t, creating a continuous spectrum between average-loss (t→0) and max-loss (t→∞) formulations. The method computes zeroth-order gradients using forward passes only, eliminating backpropagation and activation storage. Two practical ratio-of-expectations estimators are developed: a naive plug-in estimator and a bias-corrected variant that reduces estimation error from O(1/k) to O(1/k²). The approach uses perturbation-based gradient estimation via divergence theorem or Stein's lemma, making it computationally efficient and memory-friendly for large language models.

## Key Results
- ZEST consistently outperforms vanilla zeroth-order baselines across classification, multiple-choice QA, and language generation tasks
- On GLUE tasks with RoBERTa-Base and OPT-1.3B, ZEST achieves accuracy gains up to 5.9% on noisy data
- ZEST solutions exhibit smaller eigenvalues and lower loss variability under perturbations, confirming flatter minima
- The tilting parameter t=1 serves as a safe default choice, nearly always outperforming zeroth-order baselines
- ZEST provides a gradient-free yet curvature-sensitive learning approach that matches or exceeds first-order SAM variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exponential tilting transforms the zeroth-order objective into a curvature-sensitive objective that penalizes sharp minima
- Mechanism: The method parameterizes the objective as $F_t(x) = \frac{1}{t} \log \mathbb{E}_{\mu(\epsilon)}[e^{t f(x+\epsilon)}]$. As tilting parameter $t \to 0$, this recovers the average-loss objective of standard zeroth-order methods. As $t$ increases, the objective increasingly penalizes large loss values in the neighborhood, smoothly transitioning toward a max-loss formulation. This creates a gradient-free path to flatter minima by making the sharpness regularizer $R_t$ more sensitive to large Hessian eigenvalues
- Core assumption: The optimization landscape contains a mix of sharp and flat minima, and generalization improves by finding flatter solutions
- Evidence anchors:
  - [abstract] "connect zeroth-order optimization... with SAM approaches explicitly, through an exponential tilting objective that provides a smooth transition between the average- and the max-loss formulations"
  - [Section 4] Theorems 4.1 and 4.2 mathematically characterize how sharpness sensitivity $\phi_i(t)$ increases with $t$, showing dominance by $\lambda_{\max}$ as $t \to \infty$
  - [corpus] Related work confirms flat minima correlate with improved generalization (e.g., "Flat Minima and Generalization: Insights from Stochastic Convex Optimization," FMR 0.55)

### Mechanism 2
- Claim: A bias-corrected estimator for the ratio-of-expectations reduces estimation error from $O(1/k)$ to $O(1/k^2)$ with the same computational cost
- Mechanism: The tilted gradient requires estimating $\frac{\mathbb{E}[A]}{\mathbb{E}[B]}$. A naive plug-in estimator is biased for finite samples. The paper derives a bias-corrected estimator (Eq. 7) by subtracting a Taylor expansion-based bias term, improving convergence properties without additional function evaluations
- Core assumption: The underlying distributions of $A$ and $B$ are sufficiently well-behaved for the Taylor expansion to hold
- Evidence anchors:
  - [Section 3.2] "Gk_BC has an improved bias reduction rate O(1/k^2)... and has the same memory/computational complexity as the vanilla zeroth-order gradient estimator"
  - [corpus] Corpus evidence for this specific bias correction technique in zeroth-order optimization is weak or missing

### Mechanism 3
- Claim: Unbiased gradient estimates for the tilted objective are constructed using only forward passes, eliminating backpropagation and activation storage
- Mechanism: The gradient $\nabla_x F_t(x)$ is re-expressed using divergence theorem (for ball perturbations) or Stein's lemma (for Gaussian perturbations) into an expectation over function values and perturbation vectors (Eq. 3-5). This allows estimation via finite differences $f(x+\rho v)$ and $f(x-\rho v)$ along random directions
- Core assumption: The function $f$ is sufficiently smooth for divergence theorem and Stein's lemma to apply
- Evidence anchors:
  - [Section 3.1] Theorem 3.1 provides the unbiased gradient estimator formulas for both Gaussian and ball perturbations
  - [abstract] "...a gradient-free and memory-efficient alternative to SAM variants"
  - [corpus] "LORENZA" (FMR 0.54) explores related forward-only ideas for efficient LLM tuning, providing indirect support

## Foundational Learning

- **Concept: Zeroth-Order Optimization**
  - Why needed here: ZEST is fundamentally a zeroth-order method; understanding how gradients are estimated from function evaluations is core to the approach
  - Quick check question: Given a function $f(x)$ and a random direction $u$, how is a two-point gradient estimator formed? What does it estimate in expectation?

- **Concept: Sharpness-Aware Minimization (SAM)**
  - Why needed here: ZEST bridges zeroth-order optimization with SAM to find flat minima for better generalization
  - Quick check question: What is the min-max formulation of the SAM objective? Why is optimizing for the worst-case loss in a neighborhood thought to improve generalization?

- **Concept: Exponential Tilting**
  - Why needed here: This is the key mathematical tool used to interpolate between average-loss and max-loss objectives
  - Quick check question: How does changing the parameter $t$ in $F_t(x) = \frac{1}{t} \log \mathbb{E}[e^{t f(x+\epsilon)}]$ shift the objective from an average to a worst-case formulation?

## Architecture Onboarding

- **Component map:** Objective (Tilted SAM) -> Perturbation Generator (Gaussian/Ball) -> Forward-Only Evaluator (Loss computation) -> Gradient Estimator (Naive/Bias-corrected) -> Parameter Updater (Weighted update)

- **Critical path:** 1) Sample perturbations and compute forward passes (dominant cost) 2) Compute exponential losses and normalization factor (Z) 3) Compute weights for each perturbation (naive or bias-corrected) 4) Re-generate perturbations and apply the weighted update

- **Design tradeoffs:**
  - **Tilting Parameter (t):** Larger t increases sharpness sensitivity but risks numerical instability (overflow). Smaller t is stable but approaches vanilla zeroth-order
  - **Number of Perturbations (k):** Larger k reduces variance/bias but increases cost linearly
  - **Perturbation Distribution:** Gaussian is simpler; Ball perturbations are more directly linked to SAM theory
  - **Estimator Choice:** Naive is simpler; bias-corrected offers theoretical gains that may vary with small k

- **Failure signatures:**
  - **Numerical Overflow:** Large t and large loss values cause $e^{t f(\cdot)}$ overflow. Use log-space or smaller t
  - **No Improvement:** If t is too small, ZEST behaves like MeZO
  - **Instability:** Very large t or noisy landscapes cause gradient instability
  - **High Variance:** Small k leads to noisy estimates and poor convergence

- **First 3 experiments:**
  1. **Sanity Check on 2D Example:** Replicate the toy linear or stationary regime experiment (Fig 1) to confirm ZEST (t > 0) follows a flatter trajectory or converges to a flatter minimum compared to MeZO (t=0)
  2. **Ablation on Tilting Parameter t:** On a classification task (e.g., GLUE subset), sweep t ∈ {0, 0.1, 1, 5, 10} with fixed k=5 to find a stable and effective range, monitoring accuracy and loss
  3. **Comparison with Zeroth-Order Baseline:** Compare ZEST (with a good t) against MeZO on a classification task, tracking test accuracy and Hessian-based sharpness metrics (e.g., top eigenvalues, average neighborhood loss) to confirm improved generalization and flatness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced ratio-of-expectations estimators (e.g., jackknife or control variates) significantly reduce the bias and variance of the ZEST gradient update compared to the proposed naive and bias-corrected plug-in estimators?
- Basis in paper: [explicit] The authors state in Section 5.2: "We leave applying more advanced ratio estimates to ZEST to future work," noting that the bias reduction in their current estimator was not noticeable due to small sample sizes (k)
- Why unresolved: The paper only derives and tests two estimators (Gk^N and Gk^BC), finding their performance similar in the small k regime; the potential gains from more statistically robust ratio estimators remain unexplored
- What evidence would resolve it: A theoretical and empirical analysis of the Mean Squared Error (MSE) of the gradient estimate for advanced estimators versus the proposed ones across varying sample sizes k

### Open Question 2
- Question: Is there a principled, adaptive mechanism for selecting or scheduling the tilting parameter t during training to remove the reliance on validation-based hyperparameter tuning?
- Basis in paper: [inferred] Section 5.4 states that "the optimal choice of t is data-dependent" and currently requires finding "the t value that yields the best validation performance," which is a computational bottleneck
- Why unresolved: While the paper provides a safe default (t=1) and a theoretical upper bound, it does not offer a method to dynamically adjust t based on the model's optimization state (e.g., loss landscape curvature)
- What evidence would resolve it: The derivation of an adaptive schedule for t (similar to learning rate schedulers) that achieves equivalent or better generalization without a grid search over validation sets

### Open Question 3
- Question: How does the variance of the ZEST gradient estimator scale with the tilting parameter t, particularly when approaching the max-loss (t→∞) limit?
- Basis in paper: [inferred] Section 3.1 notes that as t→∞, the objective approaches a regime where "integrability is not defined," and Theorem 4.1 requires strict conditions on t and Hessian eigenvalues (1-tρ²λ_i > 0) to remain valid
- Why unresolved: The paper establishes the theoretical bias (sharpness) but does not explicitly analyze if the exponential weighting e^{tf(x+ρv)} inflates the variance of the gradient estimate as t increases, potentially destabilizing training
- What evidence would resolve it: An analysis of gradient variance as a function of t on non-convex loss surfaces, specifically identifying if high t values require a corresponding increase in the number of queries k to maintain stability

## Limitations

- The bias-corrected estimator's performance gain in high-dimensional LLM settings is theoretically supported but lacks extensive empirical validation in the corpus
- The choice of t=1 as a "safe default" is based on observed stability but not derived from theoretical optimality conditions
- The method's robustness to non-smooth or noisy loss landscapes (common in LLM fine-tuning) is assumed via Taylor expansion but not explicitly validated

## Confidence

- **High Confidence:** The fundamental mechanism linking exponential tilting to curvature-sensitive optimization (Mechanism 1) is mathematically sound and well-supported by the theorems
- **Medium Confidence:** The computational efficiency claims and practical superiority over baselines are supported by experiments but could benefit from more extensive ablation studies
- **Medium Confidence:** The bias correction provides theoretical improvements, but real-world gains may vary depending on the number of perturbations and loss landscape characteristics

## Next Checks

1. **Numerical Stability Test:** Systematically evaluate ZEST across different t values on a range of loss scales to identify overflow thresholds and validate log-space computation recommendations
2. **Perturbation Sensitivity Analysis:** Compare ZEST performance using different numbers of perturbations (k=1, 3, 5, 10) to quantify the bias-variance tradeoff and validate the k=5 recommendation
3. **Distribution Sensitivity Test:** Evaluate ZEST with both Gaussian and Ball perturbations across multiple tasks to verify the theoretical equivalence and identify any practical differences in performance or stability