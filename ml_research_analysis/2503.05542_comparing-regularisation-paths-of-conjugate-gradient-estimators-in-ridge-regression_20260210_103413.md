---
ver: rpa2
title: Comparing regularisation paths of (conjugate) gradient estimators in ridge
  regression
arxiv_id: '2503.05542'
source_url: https://arxiv.org/abs/2503.05542
tags:
- gradient
- error
- prediction
- ridge
- conjugate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares the statistical performance of conjugate gradients
  (CG), gradient descent (GD), and gradient flow (GF) for ridge regression, focusing
  on prediction error along their regularization paths. The authors derive a key comparison
  theorem showing that the risk of CG iterates can be bounded by that of GF iterates
  at transformed time indices, up to a constant factor depending on the spectrum of
  the empirical covariance matrix.
---

# Comparing regularisation paths of (conjugate) gradient estimators in ridge regression

## Quick Facts
- **arXiv ID:** 2503.05542
- **Source URL:** https://arxiv.org/abs/2503.05542
- **Reference count:** 29
- **Primary result:** CG iterates can be bounded by GF iterates at transformed time indices up to a constant factor depending on the spectrum of the empirical covariance matrix

## Executive Summary
This paper compares the statistical performance of conjugate gradients (CG), gradient descent (GD), and gradient flow (GF) for ridge regression, focusing on prediction error along their regularization paths. The authors derive a key comparison theorem showing that the risk of CG iterates can be bounded by that of GF iterates at transformed time indices, up to a constant factor depending on the spectrum of the empirical covariance matrix. This result extends previous work on GF to the penalized setting and incorporates CG, a computationally more efficient algorithm. The constant factor is uniformly bounded for various spectral distributions of the feature covariance matrix, including polynomial eigenvalue decay, Marchenko-Pastur type, and spiked covariance models. The paper also establishes monotonicity properties for GF prediction errors under certain conditions on the penalty parameter and provides high-probability bounds for transferring in-sample to out-of-sample prediction risks.

## Method Summary
The study implements penalized CG (Algorithm 1) with λ=3 (simulation) or λ=0.1 (real data), GD with step η=1/(2λ+‖Σ̂‖), and ridge regression with penalty λ′=λ+1/(ηk). For simulation, n=400, p=500 with a covariance matrix having 20 eigenvalues=100 and 480=1, plus Gaussian noise. For real data, the Riboflavin dataset (n=71, p=4088) is standardized and randomly subset to p=2n features, with 50/21 train/test splits. The methods are compared using in-sample prediction risk, excess penalized prediction risk, and out-of-sample ridge criterion across iterations.

## Key Results
- CG iterates can be bounded by GF iterates at transformed time indices, up to a constant factor depending only on the spectrum of the empirical covariance matrix
- The constant factor is uniformly bounded for polynomial eigenvalue decay, Marchenko-Pastur type, and spiked covariance models
- GF prediction errors are monotonic under certain conditions on the penalty parameter
- CG achieves comparable prediction performance to GF and ridge regression while requiring fewer iterations in practice

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The prediction risk of non-linear Conjugate Gradient (CG) iterates can be bounded by the risk of linear Gradient Flow (GF) at a transformed iteration index.
- **Mechanism:** A time reparametrization $\tau_t$ maps the CG iteration count to a GF "time." This transformation is derived from the derivative of the CG residual polynomial at zero. By aligning the iteration paths via this transformation, the greedy, data-dependent updates of CG can be bounded by the deterministic linear decay of GF.
- **Core assumption:** The geometric condition on the target vector $\gamma$ holds (specifically $\gamma = \beta_{\lambda'}$ with $\lambda' \in [0, \lambda]$), and the regularisation time $t$ is sufficiently large ($t \geq 1/2\|\hat{\Sigma}_\lambda\|$).
- **Evidence anchors:** [abstract] "bound the prediction error for conjugate gradient iterates by a corresponding prediction error of gradient flow at transformed iteration indices."; [Section 3.3] Definition of $\tau_t := \inf\{\tilde{t} \in [0, \tilde{p}] \mid |(R^{CG}_{\tilde{t}})'(0)| \geq 2t\}$.
- **Break condition:** If the CG residual polynomial fluctuates wildly before the first zero $x_{1,t}$, or if the penalty $\lambda$ is too small to dampen the divergence in high-noise regimes, the bound constant may become uninformative.

### Mechanism 2
- **Claim:** Early stopping in iterative solvers (CG/GD) functions as an implicit regularizer, analogous to explicit $\ell_2$ penalization in Ridge Regression (RR).
- **Mechanism:** Iterative algorithms start at zero and expand the solution complexity gradually. Stopping before convergence prevents the algorithm from fitting the noise $\varepsilon$ (stochastic error), trading off approximation error for variance reduction. This mimics the bias-variance trade-off controlled by $\lambda$ in RR.
- **Core assumption:** The optimal stopping time exists and is finite; specifically, the noise level $\sigma^2$ is non-zero, making the terminal ridge estimator $\hat{\beta}^{RR}_\lambda$ potentially suboptimal compared to an early iterate.
- **Evidence anchors:** [Page 3] "Both, discrete-time GD iterates and the regularisation path of its continuous-time analogue GF exhibit implicit regularisation..."; [Page 17] "stopping (conjugate) gradient descent early does not only reduce computational time but also decreases statistical errors."
- **Break condition:** If the signal-to-noise ratio is extremely high, or under specific "benign overfitting" spectral conditions, the risk may decrease monotonically, rendering early stopping suboptimal compared to convergence.

### Mechanism 3
- **Claim:** The magnitude of the error bound between CG and GF is determined by the spectral decay of the empirical covariance matrix.
- **Mechanism:** The constant $C_{t,\lambda}$ depends on the ratio of sums of eigenvalues. If eigenvalues decay polynomially or geometrically, the "effective rank" is constrained, keeping the constant small. This ensures the CG risk does not explode relative to GF risk.
- **Core assumption:** The spectrum of $\hat{\Sigma}$ decays sufficiently fast (e.g., polynomial decay with exponent $\alpha > 1$) or follows a spiked covariance model.
- **Evidence anchors:** [abstract] "...up to a constant factor that depends only on the spectrum of the empirical covariance matrix."; [Example 3.9] Detailed analysis of $C_{t,\lambda}$ for polynomial, Marchenko-Pastur, and spiked covariance models.
- **Break condition:** If eigenvalues decay too slowly (e.g., $\alpha \in (0, 1/2)$), the constant $C_{t,0}$ may become unbounded as dimension $p \to \infty$, breaking the uniform comparison.

## Foundational Learning

- **Concept:** **Ridge Regression (RR) & Regularization Paths**
  - **Why needed here:** This is the baseline estimator. The paper frames CG/GD not just as solvers, but as methods generating a "path" of estimators that approximate the Ridge solution.
  - **Quick check question:** How does the explicit penalty $\lambda$ in Ridge Regression relate to the iteration count $t$ in Gradient Flow?

- **Concept:** **Gradient Flow (GF) vs. Gradient Descent (GD)**
  - **Why needed here:** GF is the continuous-time limit of GD. The authors use GF for theoretical analysis because it is linear and deterministic, then extend results to discrete GD.
  - **Quick check question:** Why is GF mathematically "easier" to analyze for risk bounds than the discrete GD or non-linear CG?

- **Concept:** **Conjugate Gradients (CG) & Residual Polynomials**
  - **Why needed here:** CG is the core algorithm. Understanding that CG iterates are defined by minimizing a residual polynomial is essential to grasp why the algorithm is non-linear and why the "time transformation" mechanism is necessary.
  - **Quick check question:** Why does the greedy nature of CG make statistical analysis difficult compared to the linear Gradient Flow?

## Architecture Onboarding

- **Component map:** Data $(X, y)$, explicit penalty $\lambda$ -> Ridge Regression (Direct solve) -> Linear Iterative: Gradient Descent/Flow (GD/GF) -> Non-Linear Iterative: Conjugate Gradients (CG) (Algorithm 1) -> Residual Polynomials ($R^{CG}_t$, $R^{GF}_t$) -> Error Decomposition (Approximation vs. Stochastic) -> Regularisation Path (Risk vs. Iteration plot)

- **Critical path:** Implement Algorithm 1 (CG) -> Track residual polynomials $R^{CG}_t$ -> Compute time transformation $\tau_t$ via derivative at zero -> Compare risk against GF baseline

- **Design tradeoffs:**
  - **CG vs. GD:** CG converges much faster (fewer iterations) numerically. However, CG is non-linear in $y$, making statistical risk bounds harder to prove (requires complex decomposition) compared to the linear GD.
  - **Explicit vs. Implicit:** Tuning $\lambda$ (Explicit) requires re-solving the system. Early stopping (Implicit) requires only one run but demands precise stopping rules.

- **Failure signatures:**
  - **Slow Eigenvalue Decay:** If the covariance spectrum decays slower than $i^{-\alpha}$ ($\alpha \le 1$), the bound constant $C_\lambda$ inflates, and CG risk may significantly lag behind GF risk.
  - **Non-Monotonicity:** Unlike GF (which has monotonic risk under certain $\lambda$), CG risk is not guaranteed to be monotonic; it may oscillate relative to the GF bound.

- **First 3 experiments:**
  1. **Spectral Decay Validation:** Generate synthetic data with polynomial eigenvalue decay ($\alpha > 1$). Plot the risk paths of CG, GD, and RR. Verify that the "U-shape" of the risk (high initial error, minimum at optimal stopping, high terminal error) aligns across all three methods.
  2. **Time Transformation Check:** For a specific CG run, calculate $\tau_t = \inf\{\tilde{t} \mid |(R^{CG}_{\tilde{t}})'(0)| \geq 2t\}$. Plot the CG risk against $t$ and GF risk against $\tau_t$. Check if the visual alignment matches the theoretical bounds.
  3. **High-Dimensional Spiked Covariance:** Use a spiked covariance model ($r$ large eigenvalues, $p-r$ small). Compare the number of iterations required for CG vs. GD to reach the oracle risk (minimum prediction error). Confirm CG achieves this in significantly fewer steps.

## Open Questions the Paper Calls Out
None

## Limitations
- The comparison between CG and GF relies critically on the time transformation $\tau_t$, which requires the residual polynomial's derivative at zero to be well-behaved. For pathological covariance spectra (e.g., very slow eigenvalue decay), the constant $C_{t,\lambda}$ may become unbounded, invalidating the theoretical guarantees.
- The real data experiments are limited to a single dataset (Riboflavin) with a specific preprocessing pipeline (feature subsetting to $p=2n$), making generalizability unclear.
- The paper does not address computational complexity differences between CG and GD in practice, focusing instead on iteration counts.

## Confidence
- **High Confidence:** The monotonicity properties of GF prediction error under the stated conditions on $\lambda$; the high-probability bounds for transferring in-sample to out-of-sample risks (Theorem 4.4)
- **Medium Confidence:** The main comparison theorem (Theorem 3.2) relating CG and GF risks; the simulation results showing similar regularization paths across methods
- **Low Confidence:** The practical significance of the constant $C_{t,\lambda}$ across diverse real-world covariance structures; the extent to which CG-GF comparisons generalize beyond the specific simulation and Riboflavin data

## Next Checks
1. **Spectral Sensitivity Test:** Implement experiments with varying eigenvalue decay rates ($\alpha \in \{0.5, 1.0, 1.5, 2.0\}$) to empirically verify when the constant $C_{t,\lambda}$ remains bounded versus when it causes CG to significantly underperform GF.

2. **Cross-Dataset Generalization:** Apply the full experimental pipeline (simulation + real data analysis) to at least two additional high-dimensional regression datasets (e.g., from UCI repository) with different covariance structures to assess robustness of the CG-GF comparison.

3. **Computational Cost Analysis:** Measure wall-clock time and floating-point operations for CG versus GD to reach equivalent risk levels across different problem regimes, as the paper claims CG is "computationally more efficient" but does not quantify this.