---
ver: rpa2
title: Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning
arxiv_id: '2506.06069'
source_url: https://arxiv.org/abs/2506.06069
tags:
- code
- task
- detection
- llm-generated
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of detecting whether code was
  generated by a Large Language Model (LLM), a problem that is difficult due to the
  structured nature of code and the lack of access to the original task prompt used
  for generation. The authors propose a novel zero-shot detection method called Approximated
  Task Conditioning (ATC), which leverages the insight that conditioning on the task
  significantly improves detection performance, even though the unconditional token
  distributions of human-written and LLM-generated code are similar.
---

# Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning

## Quick Facts
- arXiv ID: 2506.06069
- Source URL: https://arxiv.org/abs/2506.06069
- Reference count: 40
- Zero-shot method detects LLM-generated code by approximating task context from code snippets alone

## Executive Summary
This paper addresses the challenging problem of detecting whether code was generated by a Large Language Model (LLM) when the original task prompt is unavailable. The authors propose Approximated Task Conditioning (ATC), a novel zero-shot detection method that approximates the original task from code and evaluates token-level entropy under this approximated conditioning. ATC achieves state-of-the-art performance on Python, C++, and Java code from benchmarks like APPS and MBPP, consistently outperforming existing detection methods without requiring access to the generator LLM or original prompts.

## Method Summary
ATC works by first using an LLM (CodeLlama 7b/13b) to generate a likely task description from the given code snippet. The code and approximated task are concatenated, and the mean token entropy of code tokens only (excluding comments and task tokens) is calculated. This entropy score serves as the detection metric, where lower scores indicate higher likelihood of LLM generation. The method averages entropy over multiple task approximations for robustness. Unlike unconditional methods that show similar token distributions between human and LLM code, ATC leverages the insight that task conditioning significantly improves detection performance even though the unconditional distributions overlap substantially.

## Key Results
- ATC achieves state-of-the-art AUROC scores across Python, C++, and Java code detection
- Method consistently outperforms existing detection approaches across diverse generator LLMs (CodeLlama, GPT-3.5/4o, Claude)
- Performance remains robust even when comments are removed from code samples
- Method generalizes well to different programming languages without requiring language-specific modifications

## Why This Works (Mechanism)
The method exploits the observation that human-written and LLM-generated code have similar unconditional token distributions, making detection difficult. However, when conditioned on the task, the token distributions diverge significantly. ATC approximates this conditioning by generating a likely task description from the code itself, effectively reconstructing the context that influenced the code generation. This approximation enables the detector to capture subtle differences in how LLMs versus humans write code when constrained by specific tasks, even without knowing the exact original prompt.

## Foundational Learning
- **Token entropy calculation**: Why needed - forms the core detection metric; Quick check - verify entropy computation matches definition $H(X) = -\sum p(x)\log p(x)$
- **Task approximation via prompting**: Why needed - reconstructs missing context for conditioning; Quick check - test with simple code snippets to ensure coherent task descriptions
- **Comment removal for detection**: Why needed - comments can mask generation patterns; Quick check - compare scores with and without comment filtering on sample code
- **AUROC evaluation**: Why needed - provides threshold-independent performance measure; Quick check - verify ROC curve calculation using standard libraries

## Architecture Onboarding

**Component Map**: Code snippet -> Task Approximator (CodeLlama) -> Concatenated (Task + Code) -> Entropy Calculator -> Mean Code Token Entropy -> Detection Score

**Critical Path**: The entropy calculation of code tokens under approximated task conditioning is the bottleneck. The quality of task approximation directly impacts detection performance, making the task approximator (CodeLlama) the most critical component.

**Design Tradeoffs**: Using a smaller detector LLM (CodeLlama 7b) versus larger models balances computational efficiency with detection accuracy. The choice of N task approximations trades off between robustness and computational cost. Excluding comments improves performance but may lose contextual information in some cases.

**Failure Signatures**: Poor task approximation leads to scores similar to random guessing. Including comments in entropy calculation significantly degrades performance. Using unconditional entropy results in AUROC scores barely above 0.5, indicating no better than random detection.

**First Experiments**:
1. Verify task approximation quality by checking generated task descriptions against known prompts for a subset of samples
2. Test entropy calculation implementation by comparing scores on code with known generation sources
3. Validate comment removal logic across Python, C++, and Java to ensure consistency

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on task approximation quality, which may degrade for ambiguous or underspecified code
- Experiments limited to curated benchmark datasets rather than real-world code repositories with diverse patterns
- Assumes availability of detector LLM (CodeLlama) without exploring scenarios where detector architecture differs from generator
- Requires consistent tokenization between detector and generator LLMs, which may not hold in practice

## Confidence

| Claim Cluster | Confidence Level |
|---|---|
| Task conditioning improves detection vs. unconditional methods | High |
| Robustness across programming languages and generator LLMs | Medium |
| Practical applicability in real-world scenarios | Low |

## Next Checks
1. Reproduce task approximation accuracy by implementing the "Regular" prompt style and measuring generated task quality against original prompts or human annotations
2. Test comment stripping implementation across Python, C++, and Java code samples to ensure consistency with methodology
3. Evaluate ATC on real-world code repositories to assess performance in diverse and noisy environments compared to curated benchmark datasets