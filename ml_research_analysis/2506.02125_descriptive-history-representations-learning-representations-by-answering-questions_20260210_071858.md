---
ver: rpa2
title: 'Descriptive History Representations: Learning Representations by Answering
  Questions'
arxiv_id: '2506.02125'
source_url: https://arxiv.org/abs/2506.02125
tags:
- user
- item
- title
- rating
- price
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces descriptive history representations (DHRs),
  a framework for learning compact, informative history summaries in partially observable
  environments by focusing on their ability to answer task-relevant questions. The
  core method involves a multi-agent learning approach where a representation encoder,
  answer agent, and decision agent are jointly optimized to balance reward maximization
  with the representation's capacity to answer informative queries.
---

# Descriptive History Representations: Learning Representations by Answering Questions

## Quick Facts
- arXiv ID: 2506.02125
- Source URL: https://arxiv.org/abs/2506.02125
- Reference count: 40
- Primary result: DHRs achieve up to 0.93 recommendation reward and 0.84 predictive accuracy on public datasets

## Executive Summary
Descriptive History Representations (DHRs) introduce a novel framework for learning compact, informative history summaries in partially observable environments by focusing on their ability to answer task-relevant questions. The approach uses a multi-agent learning method where a representation encoder, answer agent, and decision agent are jointly optimized to balance reward maximization with the representation's capacity to answer informative queries. This method generates interpretable textual user profiles that serve as effective DHRs, demonstrating strong predictive accuracy and downstream recommendation performance on public movie and shopping datasets.

## Method Summary
The DHR framework learns representations by jointly training three components: a representation encoder that compresses historical observations into a compact summary, an answer agent that predicts responses to task-relevant questions using this summary, and a decision agent that uses the representation to maximize reward. The key innovation is the multi-objective loss function that balances standard reward maximization with the representation's ability to answer questions accurately. This is achieved through a multi-agent learning approach where all three components are optimized together, with the representation encoder serving as a shared component between the answer and decision agents.

## Key Results
- Achieved up to 0.84 predictive accuracy for ranking tasks on public datasets
- Reached up to 0.93 recommendation reward in downstream tasks
- Demonstrated that DHR-generated textual user profiles serve as effective representations for recommendation systems

## Why This Works (Mechanism)
The framework works by explicitly training representations to be both predictive of future rewards and informative for answering domain-relevant questions. This dual objective ensures that the learned representations capture both the task-relevant information needed for decision-making and the semantic structure needed for interpretability. The multi-agent learning approach allows each component to specialize while maintaining alignment through shared representations.

## Foundational Learning
- **Partial observability**: Environments where agents cannot directly observe complete state information - needed because real-world decision problems often lack full observability; check by verifying if observation history contains relevant information
- **Representation learning**: Techniques for extracting compact, informative features from raw data - needed to reduce dimensionality while preserving task-relevant information; check by measuring information retention metrics
- **Multi-agent learning**: Training multiple interacting agents simultaneously - needed to coordinate representation, answering, and decision components; check by monitoring agent convergence and coordination
- **Question-answering frameworks**: Structured approaches to learning from queries and responses - needed to provide supervision for representation quality; check by evaluating QA accuracy on held-out data
- **Reward maximization**: Standard RL objective of maximizing cumulative returns - needed as the primary task objective; check by comparing to baseline RL methods
- **Interpretable representations**: Representations that can be understood by humans - needed for transparency and debugging; check by qualitative assessment of generated text summaries

## Architecture Onboarding
- **Component map**: Representation Encoder -> Answer Agent & Decision Agent (shared encoder)
- **Critical path**: Observation History → Encoder → [Answer Predictions, Decision Actions] → Reward/QA Feedback → Parameter Updates
- **Design tradeoffs**: Balancing representation compactness against informativeness; choosing between explicit vs implicit question encoding; computational overhead of multi-agent training
- **Failure signatures**: Poor question-answering accuracy indicates representation isn't capturing relevant information; low reward indicates decision component isn't leveraging representation effectively; representation collapse suggests inadequate regularization
- **First experiments**: 1) Train with only reward objective to establish baseline; 2) Train with only question-answering objective to test representation quality; 3) Joint training with varying weight on QA loss to find optimal balance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Dependence on manually crafted question-answer pairs limits scalability to domains with limited expert knowledge
- Text-based representations may not generalize to environments requiring dense, continuous embeddings
- Computational overhead of training three interacting agents is not thoroughly analyzed

## Confidence
- Empirical performance claims (predictive accuracy, recommendation reward): High
- Interpretability of generated text summaries: Medium (qualitative assessment)
- Scalability and generalization beyond curated datasets: Low

## Next Checks
1. Evaluate DHR performance when using automatically generated or fewer question-answer pairs to test robustness to query quality
2. Apply the framework to a continuous-control environment (e.g., robotic manipulation) to assess generalization beyond discrete, text-based tasks
3. Conduct a detailed ablation study isolating the impact of the question-answering loss versus other architectural innovations