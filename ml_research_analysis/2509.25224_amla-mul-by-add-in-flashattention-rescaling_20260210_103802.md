---
ver: rpa2
title: 'AMLA: MUL by ADD in FlashAttention Rescaling'
arxiv_id: '2509.25224'
source_url: https://arxiv.org/abs/2509.25224
tags:
- pipeline
- preload
- cube
- amla
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AMLA (Ascend MLA), a high-performance kernel
  optimized for Huawei's Ascend NPUs to address computational overhead in Multi-head
  Latent Attention (MLA) for Large Language Models. The key innovation is replacing
  floating-point multiplications with integer additions during output block rescaling
  by leveraging binary correspondence between FP32 and INT32 representations, enabling
  in-place updates directly in global memory.
---

# AMLA: MUL by ADD in FlashAttention Rescaling

## Quick Facts
- arXiv ID: 2509.25224
- Source URL: https://arxiv.org/abs/2509.25224
- Authors: Qichen Liao, Chengqiu Hu, Fangzheng Miao, Bao Li, Yiyang Liu, Junlong Lyu, Lirui Jiang, Jun Wang, Lingchao Zheng, Jun Li, Yuwei Fan
- Reference count: 40
- Primary result: Achieves up to 614 TFLOPS on Ascend 910 NPUs (86.8% of theoretical maximum), outperforming FlashMLA's 66.7% FLOPS utilization on NVIDIA H800 SXM5

## Executive Summary
AMLA (Ascend MLA) introduces a novel kernel optimization for Huawei's Ascend NPUs that significantly improves Multi-head Latent Attention (MLA) performance in Large Language Models. The key innovation replaces floating-point multiplications with integer additions during output block rescaling by exploiting binary correspondence between FP32 and INT32 representations. This optimization enables in-place updates directly in global memory while preserving numerical precision. Combined with a Preload Pipeline strategy using hierarchical tiling, AMLA maximizes FLOPS utilization and achieves state-of-the-art performance on Ascend 910 NPUs.

## Method Summary
The AMLA kernel optimizes MLA computation by transforming the rescaling operation from multiplication to addition using FP32-to-INT32 binary correspondence. This mathematical transformation allows the kernel to perform in-place updates in global memory, eliminating memory movement overhead. The Preload Pipeline strategy with hierarchical tiling overlaps data movement and computation to maximize FLOPS utilization. The kernel has been integrated into Huawei's CANN framework and achieves significant performance improvements on Ascend 910 NPUs while maintaining the numerical precision required for ML applications.

## Key Results
- Achieves up to 614 TFLOPS on Ascend 910 NPUs, reaching 86.8% of theoretical maximum FLOPS
- Outperforms FlashMLA's 66.7% FLOPS utilization on NVIDIA H800 SXM5
- Preserves numerical precision while enabling in-place memory updates
- Successfully integrated into Huawei's CANN framework

## Why This Works (Mechanism)
The core mechanism exploits the mathematical relationship between FP32 and INT32 binary representations, where rescaling operations can be transformed from multiplications to additions. This transformation reduces computational complexity while maintaining numerical equivalence. The Preload Pipeline strategy with hierarchical tiling ensures continuous data availability for computation, minimizing memory access stalls and maximizing arithmetic unit utilization. The combination of these optimizations allows AMLA to achieve high FLOPS utilization on Ascend NPUs.

## Foundational Learning

**FP32-to-INT32 Binary Correspondence**
- Why needed: Enables transformation of multiplication operations to addition operations for performance optimization
- Quick check: Verify that FP32 and INT32 representations maintain numerical equivalence under the transformation

**Hierarchical Tiling Strategy**
- Why needed: Optimizes memory access patterns and reduces data movement overhead
- Quick check: Confirm that tile sizes are optimized for Ascend NPU memory hierarchy

**Preload Pipeline Optimization**
- Why needed: Overlaps data movement with computation to maximize FLOPS utilization
- Quick check: Measure memory bandwidth utilization and compare with theoretical limits

## Architecture Onboarding

**Component Map**
Input Tensor -> Rescaling Transformation -> In-place Global Memory Update -> Preload Pipeline -> Output Tensor

**Critical Path**
The critical path involves the rescaling transformation followed by in-place memory updates, with the Preload Pipeline providing the necessary data flow to maintain computational throughput.

**Design Tradeoffs**
- Memory vs. computation: In-place updates save memory bandwidth but require careful management of data dependencies
- Precision vs. performance: Binary correspondence transformation must preserve numerical accuracy while enabling faster computation
- Complexity vs. generality: Optimization is tailored for MLA but may require adaptation for other attention mechanisms

**Failure Signatures**
- Numerical precision degradation in extreme parameter regimes
- Suboptimal FLOPS utilization when tensor dimensions deviate from optimal shapes
- Memory access conflicts during in-place updates

**First Experiments**
1. Benchmark FLOPS utilization across varying tensor dimensions and batch sizes
2. Compare numerical precision against floating-point reference implementations
3. Measure memory bandwidth utilization and identify bottlenecks

## Open Questions the Paper Calls Out
None

## Limitations

- Limited analysis of edge cases where binary correspondence transformation might introduce numerical instability
- FLOPS utilization comparisons don't control for architectural differences beyond kernel optimization
- Performance claims heavily dependent on specific tensor shapes and batch sizes that may not generalize
- Limited validation across different Ascend NPU models and MLA configurations

## Confidence

**Technical methodology and kernel optimization:** High
**Numerical precision preservation claims:** Medium
**Cross-platform FLOPS utilization comparisons:** Medium  
**Generalizability to different MLA configurations:** Low

## Next Checks

1. Conduct ablation studies varying tensor dimensions and batch sizes to verify FLOPS utilization remains consistently above 80% across different MLA workloads
2. Perform numerical precision analysis comparing AMLA outputs against floating-point reference implementations across the full range of attention weights and activation scales
3. Benchmark AMLA performance on different Ascend NPU models (e.g., Ascend 910B, 920) to assess scalability and architectural dependencies of the optimization techniques