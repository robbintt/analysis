---
ver: rpa2
title: 'KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational
  Agent Benchmarking with Multimodal Language Models'
arxiv_id: '2601.01366'
source_url: https://arxiv.org/abs/2601.01366
tags:
- knowledge
- task
- agents
- tasks
- educational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KGCE introduces a cross-platform educational agent benchmarking
  framework integrating a knowledge base with a dual-graph evaluation. It addresses
  limitations in private-domain educational software tasks by constructing a 104-task
  dataset across Windows, Android, and cross-platform settings, and developing a structured
  JSON knowledge base to improve execution in proprietary applications.
---

# KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models

## Quick Facts
- arXiv ID: 2601.01366
- Source URL: https://arxiv.org/abs/2601.01366
- Authors: Zixian Liu; Sihao Liu; Yuqi Zhao
- Reference count: 20
- Key result: Knowledge base support improves CR from 60.02% to 75.26% (+25.39%) and reduces errors by up to 43.81%

## Executive Summary
KGCE introduces a cross-platform educational agent benchmarking framework integrating a knowledge base with a dual-graph evaluation. It addresses limitations in private-domain educational software tasks by constructing a 104-task dataset across Windows, Android, and cross-platform settings, and developing a structured JSON knowledge base to improve execution in proprietary applications. The dual-graph evaluator—comprising a Task Completeness Graph and an Execution Efficiency Graph—provides fine-grained metrics such as CR, CPA, Precision, Recall, F1-score, and BR. Experiments show that knowledge base support significantly improves CR from 60.02% to 75.26% (+25.39%) and reduces errors by up to 43.81%, with GPT-4o outperforming other models when using the knowledge base.

## Method Summary
KGCE addresses cross-platform educational agent benchmarking by combining a structured JSON knowledge base with a dual-graph evaluation framework. The system handles 104 tasks across Windows, Android, and cross-platform scenarios, using DAG-based task decomposition and knowledge-augmented prompting to improve agent performance on proprietary educational software. The dual-graph evaluator tracks both task completion quality and execution efficiency through Task Completeness Graph (CR, CPA) and Execution Efficiency Graph (BR, Precision, Recall, F1-score) metrics.

## Key Results
- Knowledge base support improves Completion Rate from 60.02% to 75.26% (+25.39%)
- Knowledge base reduces errors by up to 43.81% compared to baseline
- GPT-4o outperforms other models when using knowledge base, achieving highest CR and lowest error rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge base injection improves agent performance on private-domain educational software by providing structured prior knowledge about UI elements and workflows.
- Mechanism: The system detects when a task involves proprietary software, retrieves relevant JSON-structured records (package names, page descriptions, UI-element positions, functional explanations), and injects this context into the MLM prompt. This reduces exploration overhead and enables more direct action planning.
- Core assumption: MLMs can effectively parse and apply structured JSON knowledge during action prediction when it is embedded in prompts.
- Evidence anchors:
  - [abstract] "developing a structured JSON knowledge base to improve execution in proprietary applications"
  - [section III.B] "At runtime, a Knowledge Invocation Decision module checks the task description: if the software is mentioned, the corresponding KB records are retrieved and injected into the prompt"
  - [corpus] Weak direct evidence; related work on knowledge-augmented agents (GAT, PLaG) cited but not empirically validated in this domain
- Break condition: If the knowledge base schema does not match the actual UI state (e.g., software updated after KB construction), retrieval may provide misleading guidance.

### Mechanism 2
- Claim: Dual-graph evaluation captures both task completion quality and execution path efficiency more informatively than single-metric approaches.
- Mechanism: The Task Completeness Graph (TCG) tracks sub-goal completion via binary indicators across a DAG; the Execution Efficiency Graph (EEG) quantifies backtracking (BR), precision (CAN/ANU), and recall over key steps. Together they distinguish between "completed but inefficiently" and "efficiently but incompletely."
- Core assumption: Sub-goals can be reliably decomposed and independently verified during execution.
- Evidence anchors:
  - [abstract] "dual-graph evaluator—comprising a Task Completeness Graph and an Execution Efficiency Graph—provides fine-grained metrics"
  - [section III.C] "Precision, Recall, and F1-score exhibit high positive correlations with CR... BR, OoR, and RMS are negatively correlated with CR"
  - [corpus] ATOD benchmark similarly uses multi-dimensional evaluation for task-oriented dialogue agents, supporting granularity benefits
- Break condition: If sub-goal definitions are too coarse or ambiguous, TCG may conflate distinct failure modes; if key steps are misidentified, Recall becomes misleading.

### Mechanism 3
- Claim: DAG-based task decomposition enables cross-platform coordination by modeling dependencies between subtasks across devices.
- Mechanism: Complex tasks are broken into atomic subtasks (e.g., "Open Xiaoya app" → "Enter course" → "Switch to Tasks app" → "Add task"), with edges representing dependencies. The DAG structure allows parallel subtasks where appropriate while enforcing sequential constraints.
- Core assumption: The dependency graph correctly captures true operational constraints; no hidden cross-platform state synchronization issues exist.
- Evidence anchors:
  - [section III.A] "each complex task splits into atomic subtasks, linked in a DAG"
  - [section III.A] "These subtasks are organized into a DAG that models the logical dependencies between them"
  - [corpus] CRAB (cited baseline) uses similar DAG-based decomposition but lacks fine-grained efficiency metrics
- Break condition: If cross-platform state transfer (e.g., data copied on Windows needed on Android) fails or is delayed, DAG ordering may be insufficient to capture real-time coordination failures.

## Foundational Learning

- Concept: **DAG (Directed Acyclic Graph) for task modeling**
  - Why needed here: Understanding how subtasks are structured and dependencies enforced is essential to reading the evaluation framework and diagnosing failure patterns.
  - Quick check question: Given subtasks A→B→C and A→D with no edge between B/C and D, which subtasks can execute in parallel?

- Concept: **Precision/Recall/F1 in sequential action contexts**
  - Why needed here: These metrics are repurposed from classification to action evaluation; understanding what counts as "completed action" vs. "key step" is critical.
  - Quick check question: If an agent completes 8 of 10 key steps but attempts 20 actions total, what are Recall and Precision?

- Concept: **Knowledge-augmented prompting for MLMs**
  - Why needed here: The core intervention is injecting structured KB content into prompts; engineers must understand retrieval-triggering logic and prompt construction.
  - Quick check question: What should the system do if a task description mentions no KB-indexed software?

## Architecture Onboarding

- Component map:
  - Educational Dataset (104 tasks) -> Task templates with DAG dependencies
  - Knowledge Base (JSON) -> Package/page/element hierarchy with positions and descriptions
  - Main Agent -> Receives task, invokes KB if applicable, dispatches to platform agents
  - Windows Agent / Android Agent -> Execute actions, capture screenshots/OCR
  - Dual-Graph Evaluator -> TCG (CR, CPA) + EEG (BR, Precision, Recall, F1, OoR, RMS)

- Critical path:
  1. Task ingestion -> KB lookup (if software matched)
  2. Prompt construction with KB context
  3. Action prediction via MLM
  4. Platform-specific execution
  5. Screenshot/OCR feedback loop
  6. Dual-graph metric computation

- Design tradeoffs:
  - KB granularity vs. maintenance cost: More detailed element-level records improve guidance but require frequent updates as software changes.
  - Subtask granularity: Finer decomposition improves diagnostic value but increases DAG complexity and verification overhead.
  - Static KB vs. dynamic reasoning: Gemini-2.0-Flash showed increased OoR errors with KB, suggesting potential conflict between static rules and model's dynamic inference.

- Failure signatures:
  - High BR + low CR: Agent is exploring without converging; likely KB mismatch or ambiguous task decomposition.
  - High OoR: Agent misinterprets UI layout; check KB element positions against current screenshot.
  - High RMS: Task too complex or agent stuck in loop; review DAG depth and action space limits.

- First 3 experiments:
  1. Reproduce the KB ablation (with/without) on a 20-task subset to validate CR and BR deltas locally before full benchmarking.
  2. Inspect KB JSON schema for one proprietary app (e.g., HuaShi XiaZi) and trace a single task's KB retrieval through to prompt construction.
  3. Run dual-graph evaluation on a failed task and correlate high-BR segments with screenshot/OCR logs to identify specific decision points where the agent backtracked.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on proprietary knowledge base limiting generalizability to other domains
- Knowledge base construction is manual and software-specific creating maintenance overhead
- Cross-platform coordination challenges not explicitly addressed in DAG modeling approach

## Confidence
- **High Confidence**: The improvement in Completion Rate (CR) from 60.02% to 75.26% with knowledge base support is well-supported by the experimental results.
- **Medium Confidence**: The claim that knowledge base injection reduces errors by up to 43.81% is supported by reported data, though the exact breakdown across error types could be more detailed.
- **Low Confidence**: The assertion that DAG-based decomposition adequately captures all cross-platform coordination requirements is not empirically validated against real-world edge cases.

## Next Checks
1. Conduct a domain transfer experiment using the KGCE framework on a non-educational proprietary software domain (e.g., productivity or finance applications) to assess knowledge base generalizability and schema adaptability.

2. Perform a sensitivity analysis on knowledge base granularity by systematically varying the detail level of UI element descriptions and measuring the impact on Completion Rate and error reduction across multiple LLM models.

3. Implement a longitudinal study tracking performance degradation over time as target applications receive UI updates, measuring the maintenance burden and accuracy decay of the knowledge base system.