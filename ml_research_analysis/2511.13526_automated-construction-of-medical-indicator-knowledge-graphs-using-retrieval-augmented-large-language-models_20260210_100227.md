---
ver: rpa2
title: Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval
  Augmented Large Language Models
arxiv_id: '2511.13526'
source_url: https://arxiv.org/abs/2511.13526
tags:
- knowledge
- clinical
- medical
- graph
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of constructing high-quality,
  scalable medical indicator knowledge graphs from unstructured clinical guidelines,
  which traditionally require labor-intensive manual curation and rule-based extraction.
  The proposed solution integrates retrieval-augmented generation (RAG) with large
  language models (LLMs) to automate knowledge extraction, using ontology-driven schema
  design and a human-in-the-loop (HITL) validation mechanism for clinical accuracy.
---

# Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models

## Quick Facts
- arXiv ID: 2511.13526
- Source URL: https://arxiv.org/abs/2511.13526
- Reference count: 21
- Constructs medical indicator knowledge graphs from clinical guidelines using RAG and LLMs with 88% precision on 240 triples

## Executive Summary
This paper presents a framework for automated construction of medical indicator knowledge graphs from unstructured clinical guidelines, addressing the challenge of labor-intensive manual curation. The system integrates retrieval-augmented generation (RAG) with large language models (LLMs) to extract structured medical knowledge, using an ontology-driven schema and human-in-the-loop validation for clinical accuracy. Experimental results demonstrate the successful standardization of over 120 clinical indicators across eight physiological systems with high precision. The resulting knowledge graphs are designed for interoperability with biomedical ontologies like UMLS and SNOMED CT, supporting intelligent clinical decision support systems and research applications.

## Method Summary
The framework performs guideline-driven data acquisition, semantic retrieval, structured extraction, knowledge fusion, and expert-guided refinement. It uses ontology-driven schema design to standardize clinical indicators and employs a retrieval-augmented generation approach with LLMs for automated knowledge extraction from unstructured clinical guidelines. The system incorporates human-in-the-loop validation to ensure clinical accuracy and integrates the extracted knowledge with established biomedical ontologies. The pipeline transforms free-text clinical guidelines into structured triples that populate the medical indicator knowledge graph.

## Key Results
- Achieved 88% precision on a sample of 240 extracted triples
- Successfully standardized over 120 clinical indicators across eight physiological systems
- Demonstrated interoperability with biomedical ontologies (UMLS, SNOMED CT)

## Why This Works (Mechanism)
The framework's effectiveness stems from combining retrieval-augmented generation with ontology-driven schema design. RAG provides contextual grounding by retrieving relevant guideline sections before prompting LLMs for extraction, reducing hallucinations and improving accuracy. The ontology-driven approach ensures standardized representation of medical indicators, enabling interoperability with existing biomedical knowledge bases. Human-in-the-loop validation adds clinical expertise to the automated process, catching errors that purely automated systems might miss. The integration of these components creates a system that balances automation with accuracy, making large-scale knowledge graph construction feasible while maintaining clinical validity.

## Foundational Learning
- Retrieval-augmented generation (RAG): Why needed - provides contextual grounding for LLMs to reduce hallucinations; Quick check - verify retrieved documents are relevant to the extraction task
- Ontology-driven schema design: Why needed - ensures standardized, interoperable representation of medical concepts; Quick check - validate mappings to UMLS/SNOMED CT
- Human-in-the-loop validation: Why needed - incorporates clinical expertise to verify accuracy; Quick check - measure inter-rater reliability among domain experts
- Knowledge fusion techniques: Why needed - resolves contradictions and merges information from multiple sources; Quick check - assess consistency of extracted triples
- Clinical indicator standardization: Why needed - enables comparison and integration across different guidelines; Quick check - verify coverage across eight physiological systems
- Triple extraction from unstructured text: Why needed - transforms free-text guidelines into structured knowledge; Quick check - evaluate precision and recall metrics

## Architecture Onboarding
**Component Map**: Clinical Guidelines -> Semantic Retrieval -> Structured Extraction -> Knowledge Fusion -> Ontology Mapping -> HITL Validation -> Medical Indicator Knowledge Graph
**Critical Path**: The semantic retrieval step is critical as it provides the contextual foundation for accurate extraction; failures here propagate through the entire pipeline
**Design Tradeoffs**: Automated extraction vs. human validation (speed vs. accuracy), comprehensive ontology coverage vs. system complexity, real-time processing vs. batch processing for quality
**Failure Signatures**: Poor retrieval quality leads to incomplete or inaccurate extractions; ontology mapping failures result in interoperability issues; inadequate HITL validation allows clinical errors
**First Experiments**: 1) Test retrieval accuracy on sample clinical guidelines, 2) Evaluate triple extraction precision on a small guideline subset, 3) Validate ontology mapping accuracy for sample indicators

## Open Questions the Paper Calls Out
None

## Limitations
- Reported precision based on small sample (240 triples) limits generalizability to larger, more diverse clinical guideline corpora
- System performance depends heavily on quality and completeness of retrieval corpus, with potential for error propagation
- Clinical utility is inferred from interoperability rather than demonstrated through actual decision support system deployment

## Confidence
- **High**: Technical architecture and workflow clearly described; integration of RAG with ontology-driven schema design is methodologically sound; reported precision and indicator coverage are credible
- **Medium**: Claims about interoperability with UMLS and SNOMED CT are reasonable but lack empirical validation; HITL mechanism conceptually justified but not quantified
- **Low**: Real-world scalability and robustness to diverse guideline formats untested; actual clinical impact not demonstrated through deployment

## Next Checks
1. Conduct large-scale, multi-source evaluation using diverse, real-world clinical guidelines to assess precision, recall, and robustness beyond initial 240-triple sample
2. Empirically validate accuracy and completeness of semantic mappings to UMLS and SNOMED CT, including inter-rater reliability among clinical experts
3. Deploy knowledge graphs in live clinical decision support or question-answering system to measure actual utility and user acceptance in clinical practice