---
ver: rpa2
title: 'Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality'
arxiv_id: '2509.17543'
source_url: https://arxiv.org/abs/2509.17543
tags:
- distribution
- latent
- autoencoder
- compression
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Bilateral Distribution Compression (BDC),
  a method to compress datasets in both sample size and dimensionality while preserving
  the underlying data distribution. Unlike existing distribution compression methods
  that only reduce sample size, BDC tackles the dual challenge of high dimensionality
  in modern datasets.
---

# Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality

## Quick Facts
- **arXiv ID:** 2509.17543
- **Source URL:** https://arxiv.org/abs/2509.17543
- **Authors:** Dominic Broadbent; Nick Whiteley; Robert Allison; Tom Lovett
- **Reference count:** 40
- **Primary result:** Introduces Bilateral Distribution Compression (BDC) to simultaneously reduce sample size and dimensionality while preserving data distribution

## Executive Summary
Bilateral Distribution Compression (BDC) is a novel framework that addresses the dual challenge of reducing both sample size and dimensionality in modern datasets while preserving the underlying data distribution. Unlike existing distribution compression methods that only reduce sample size, BDC leverages autoencoders to learn low-dimensional projections and then optimizes compressed sets in latent space. The method demonstrates superior computational efficiency and comparable or better downstream performance across regression, classification, and clustering tasks, with compression rates reaching up to 99.99% on low-dimensional manifolds.

## Method Summary
BDC operates in two stages: first, it trains an autoencoder using Reconstruction Maximum Mean Discrepancy (RMMD) to learn a low-dimensional projection of the data; second, it optimizes a compressed set in latent space using Encoded MMD (EMMD). The key theoretical contribution is proving that minimizing both RMMD and EMMD guarantees vanishing Decoded MMD (DMMD), ensuring faithful representation of the original distribution. The framework supports both linear autoencoders trained on the Stiefel manifold and non-linear neural networks using a hybrid loss combining RMMD and Mean Squared Reconstruction Error. Experiments demonstrate that BDC achieves similar or superior downstream performance to ambient-space compression methods while being substantially faster.

## Key Results
- BDC achieves comparable or superior downstream performance to ambient-space compression methods while being 20x faster on CT-Slice dataset
- On MNIST, BDC-L achieved the best Gaussian Process predictive metrics in shortest time despite lower reconstruction quality
- Compression rates up to 99.99% achieved with minimal loss in predictive accuracy when data lies on low-dimensional manifolds
- Method shows effectiveness across regression, classification, and clustering tasks with various dataset sizes and dimensionalities

## Why This Works (Mechanism)
The method works by first learning a meaningful low-dimensional representation through autoencoder training optimized for distributional fidelity (RMMD), then performing compression directly in this learned latent space where the data structure is preserved. The two-stage approach separates the challenges of learning a good representation from the optimization of compressed sets. By proving that minimizing RMMD and EMMD guarantees vanishing DMMD, the method provides theoretical assurance that the compressed set will faithfully represent the original distribution after decoding. The computational efficiency gain comes from performing the expensive optimization in a lower-dimensional space (p≪d).

## Foundational Learning
- **Maximum Mean Discrepancy (MMD):** Kernel-based distance metric between distributions; needed for measuring distributional fidelity in both reconstruction and compression stages; quick check: verify MMD computation matches analytical values on simple synthetic distributions
- **Stiefel Manifold Optimization:** Optimization over orthogonal matrices; needed for training linear autoencoders to ensure valid projections; quick check: verify learned projection matrix is orthogonal (P^T P ≈ I)
- **Autoencoder Training with RMMD:** Training reconstruction loss based on distributional distance rather than pointwise error; needed to preserve global data structure; quick check: visualize reconstructions and compute RMMD between original and reconstructed data
- **Latent Space Compression:** Optimizing a small set of points to represent the full distribution in lower dimensions; needed to achieve simultaneous sample size and dimensionality reduction; quick check: monitor EMMD loss during optimization for convergence
- **Gaussian Process Classification on Compressed Sets:** Using compressed representations for downstream prediction; needed to evaluate practical utility of compression; quick check: verify test accuracy improves with training iterations on compressed set
- **Intrinsic Dimension Estimation:** Heuristic for selecting latent dimension p; needed to automate architectural choices; quick check: compare downstream performance across different p values

## Architecture Onboarding
- **Component Map:** Original Data -> Linear/Non-linear AE (RMMD training) -> Encoder -> Latent Space -> Compressed Set Optimization (EMMD) -> Decoded Compressed Set
- **Critical Path:** Autoencoder training → Data encoding → Compressed set optimization → Downstream evaluation
- **Design Tradeoffs:** Linear vs non-linear AE (computational cost vs representation power), latent dimension p (compression vs fidelity), number of compressed points m (accuracy vs efficiency)
- **Failure Signatures:** High RMMD indicating poor reconstruction, stagnant EMMD during optimization, poor downstream performance despite low compression metrics
- **First Experiments:** 1) Train linear AE on Swiss-Roll and visualize reconstructions, 2) Train non-linear AE on MNIST and evaluate RMMD, 3) Perform latent compression on CT-Slice and compare downstream regression performance

## Open Questions the Paper Calls Out
- Can theoretical convergence guarantees be established for the autoencoder training stage under the RMMD objective?
- How does BDC perform when extended to compress conditional distributions using Average Maximum Conditional Mean Discrepancy (AMCMD)?
- Can intrinsic dimension estimation methods be effectively integrated to automate latent dimension selection?
- Does replacing standard autoencoders with Distributional Principal Autoencoders (DPAs) improve compressed set fidelity?

## Limitations
- Hyperparameter sensitivity, particularly for latent dimension selection and learning rates
- Performance depends on data lying on or near a low-dimensional manifold
- Extension to conditional distribution compression remains theoretical
- Computational cost of MMD calculations with large datasets

## Confidence
**Medium confidence** due to incomplete training hyperparameter specifications and underspecified network architectures for non-linear autoencoders.

## Next Checks
1. Verify RMMD values after autoencoder training match paper's reported reconstruction quality metrics before proceeding to compression
2. Compare EMMD convergence curves during latent space optimization against paper's supplementary figures
3. Validate downstream task performance (e.g., MNIST classification accuracy) on compressed set against reported numbers within ±2-3% tolerance