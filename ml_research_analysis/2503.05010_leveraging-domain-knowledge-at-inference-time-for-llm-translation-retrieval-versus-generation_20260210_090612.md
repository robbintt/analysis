---
ver: rpa2
title: 'Leveraging Domain Knowledge at Inference Time for LLM Translation: Retrieval
  versus Generation'
arxiv_id: '2503.05010'
source_url: https://arxiv.org/abs/2503.05010
tags:
- terminology
- demonstrations
- translation
- source
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares retrieval and generation methods for domain
  adaptation of large language models (LLMs) in machine translation. It examines how
  demonstrations (example translations) and terminology (bilingual dictionaries) can
  be sourced externally (from a bitext datastore) or internally (generated by an LLM)
  to improve translation quality in law, medical, and Koran domains.
---

# Leveraging Domain Knowledge at Inference Time for LLM Translation: Retrieval versus Generation

## Quick Facts
- arXiv ID: 2503.05010
- Source URL: https://arxiv.org/abs/2503.05010
- Authors: Bryan Li; Jiaming Luo; Eleftheria Briakou; Colin Cherry
- Reference count: 20
- Primary result: Retrieval consistently outperforms generation for domain adaptation, but generating demonstrations with weaker models can close the performance gap with larger models' zero-shot translation.

## Executive Summary
This paper investigates domain adaptation for large language model translation by comparing retrieval and generation methods for incorporating external domain knowledge. The authors examine how demonstrations (example translations) and terminology (bilingual dictionaries) can be sourced either from external bitext datastores or generated internally by LLMs. Their comprehensive evaluation across law, medical, and Koran domains reveals that retrieval methods consistently outperform generation approaches, with demonstrations providing greater benefit than terminology. Interestingly, they find that style matching rather than terminology assistance drives the performance gains from retrieved demonstrations, and that domain-specificity is the primary factor in effective in-context learning.

## Method Summary
The paper compares retrieval-based and generation-based approaches for domain adaptation in machine translation. Retrieval uses BM25 to fetch k=3 demonstration examples from a training set index, while generation involves prompting the LLM to create demonstrations or terminology entries in JSON format before translation. The study uses a filtered Multi-domain dataset (Aharoni and Goldberg, 2020) with German-to-English translation across three domains: Law, Medical, and Koran. LLMs tested include Gemma-2 27B IT and Gemini 1.5 Pro, with COMET22 as the evaluation metric (excluding BLEU due to sensitivity to rephrasing). The methodology includes zero-shot baselines, retrieval augmentation with external demonstrations, and generation of synthetic demonstrations or terminology dictionaries.

## Key Results
- Demonstrations consistently outperform terminology across all domains and models
- Retrieval consistently outperforms generation for both demonstrations and terminology
- Generated demonstrations with weaker models can close the performance gap with larger models' zero-shot translation
- Retrieved demonstrations primarily provide style matching rather than terminology assistance
- Domain-specificity is the main factor in effective in-context learning

## Why This Works (Mechanism)
The mechanism behind the effectiveness of retrieval-based methods lies in their ability to provide contextually relevant examples that capture domain-specific translation patterns and stylistic conventions. Retrieved demonstrations serve as high-quality exemplars that guide the model's translation behavior, particularly in matching the target domain's style and register. The superiority of demonstrations over terminology suggests that providing full translation examples is more beneficial than isolated term pairs, as it captures broader contextual information and translation strategies.

## Foundational Learning
- **BM25 ranking**: Information retrieval technique for document scoring using term frequency and inverse document frequency; needed to retrieve relevant demonstrations from the training corpus
- **COMET22 metric**: Evaluation metric for machine translation quality that captures fluency and adequacy; needed as a more robust alternative to BLEU
- **In-context learning**: LLM's ability to perform tasks based on provided examples without parameter updates; needed to understand how demonstrations improve translation quality
- **JSON formatting for prompts**: Structured output format requirement for generated demonstrations and terminology; needed to ensure consistent parsing and integration into translation prompts
- **Domain adaptation**: Process of adapting models to perform well on specific domains; needed to frame the translation quality improvements across law, medical, and religious text domains

## Architecture Onboarding

**Component Map:** Test sentence -> Prompt (zero-shot/retrieval/generation) -> LLM -> Output translation -> COMET22 evaluation

**Critical Path:** BM25 index creation -> Demonstration retrieval/generation -> Translation prompt construction -> Model inference -> Quality evaluation

**Design Tradeoffs:** Retrieval provides higher quality examples but requires external datastore access; generation is more flexible but produces lower quality examples. Demonstrations provide broader context than terminology but require more prompt space.

**Failure Signatures:** JSON parsing errors during generation steps, data leakage from using training data as retrieval corpus, context window overflow with verbose demonstrations, performance degradation when generated examples lack domain specificity.

**First Experiments:** 1) Verify dataset statistics match reported figures (16k Koran, 234k Medical, 464k Law training entries), 2) Test JSON parsing robustness with Gemma-2 27B IT, 3) Conduct ablation studies to confirm style matching drives performance gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on Gemini 1.5 Pro, a proprietary model without specified API versions or inference parameters
- Silver dictionary construction process incompletely specified (unclear which model performed terminology extraction)
- JSON formatting requirements present potential failure points in the generation pipeline
- Potential data leakage concerns from using training data as retrieval corpus

## Confidence

**High confidence:** Retrieval vs generation comparison methodology, BM25 demonstration retrieval, COMET22 evaluation metric selection, general experimental design across three domains

**Medium confidence:** JSON generation prompts and implementation, as these require careful handling to avoid formatting errors

**Low confidence:** Exact silver dictionary construction process, since the paper doesn't specify which model performed the terminology extraction

## Next Checks

1. Verify dataset statistics match the paper's reported figures (16k Koran, 234k Medical, 464k Law training entries) to ensure correct dataset version

2. Implement and test JSON parsing robustness with Gemma-2 27B IT to confirm synthetic demonstration generation works reliably

3. Conduct ablation studies to confirm that style matching, rather than terminology assistance, drives the performance gains from retrieved demonstrations as suggested in the analysis