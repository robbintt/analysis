---
ver: rpa2
title: 'RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM Post-Training'
arxiv_id: '2510.00911'
source_url: https://arxiv.org/abs/2510.00911
tags:
- reasoning
- arxiv
- riskpo
- reward
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses entropy collapse and limited reasoning gains
  in RLVR training of LLMs. The authors propose RiskPO, which uses a risk-sensitive
  Mixed Value-at-Risk (MVaR) objective to emphasize challenging instances over the
  mean reward.
---

# RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM Post-Training

## Quick Facts
- arXiv ID: 2510.00911
- Source URL: https://arxiv.org/abs/2510.00911
- Reference count: 40
- Risk-sensitive RLVR achieves +6.24% absolute Pass@1 gains on hard math over GRPO

## Executive Summary
This paper addresses entropy collapse and limited reasoning gains in RLVR training of LLMs. The authors propose RiskPO, which uses a risk-sensitive Mixed Value-at-Risk (MVaR) objective to emphasize challenging instances over the mean reward. A bundling scheme aggregates multiple questions to enrich sparse binary feedback and avoid zero gradients on hard problems. Theoretically, they prove that the risk-averse update mitigates entropy collapse by reducing the covariance between log-probabilities and advantages. Empirically, RiskPO achieves consistent improvements over GRPO and its variants on mathematical reasoning, multi-modal reasoning, and code generation benchmarks, including +6.24% absolute gains on Pass@1 for hard-level math and expanding the model's reasoning boundary beyond what mean-based methods can achieve.

## Method Summary
RiskPO addresses entropy collapse in RLVR training by using a risk-sensitive Mixed Value-at-Risk (MVaR) objective. The method groups multiple questions into bundles (B=5) to create richer feedback signals from binary rewards. For each bundle, it computes scores and applies clipped IS ratios. The MVaR objective uses two quantiles (α=0.2, β=0.8) to capture lower and upper tails of the reward distribution, with a mixing weight ω=0.5. The theoretical analysis shows that this risk-averse update reduces the covariance between log-probabilities and advantages, thereby mitigating entropy collapse. The method is evaluated on DAPO-math-17k, MATH, GSM8K, Geometry3K, and Archer-6K using base models ranging from 1.5B to 3B parameters.

## Key Results
- RiskPO achieves +6.24% absolute Pass@1 gains on hard-level math benchmarks compared to GRPO
- Maintains higher entropy throughout training, mitigating entropy collapse observed in standard RLVR methods
- Expands reasoning boundaries beyond what mean-based methods can achieve on challenging problems
- Demonstrates consistent improvements across mathematical reasoning, multi-modal reasoning, and code generation tasks

## Why This Works (Mechanism)
RiskPO works by addressing the fundamental issue of entropy collapse in RLVR training. Traditional mean-based rewards cause models to become overly confident and reduce exploration, limiting reasoning gains. By using a risk-sensitive MVaR objective with lower-tail emphasis (α=0.2), RiskPO maintains exploration on challenging instances. The bundling mechanism solves the sparse feedback problem by aggregating multiple questions, ensuring non-zero gradients even when individual problems are hard. The theoretical analysis proves that this approach reduces the covariance between log-probabilities and advantages, directly mitigating entropy collapse.

## Foundational Learning

**Mixed Value-at-Risk (MVaR)** - A risk measure that combines lower and upper quantiles of reward distribution. Needed to balance exploration on hard problems (lower quantile) with exploitation of easy ones (upper quantile). Quick check: Verify that changing α from 0.2 to 0.5 degrades hard problem performance.

**Bundling mechanism** - Groups B=5 questions to enrich binary feedback signals. Needed because single-question binary rewards create sparse gradients that can stall learning. Quick check: Confirm that bundle scores distribute across [0,B] range to avoid zero-gradient scenarios.

**Clipped IS ratios** - Truncate importance sampling ratios with ε=0.2 to reduce variance. Needed to stabilize training when using reference policies and off-policy corrections. Quick check: Monitor gradient variance with and without clipping.

## Architecture Onboarding

**Component map:** Questions → Bundling (B=5) → Score computation → MVaR quantiles (α=0.2, β=0.8) → Clipped IS ratios → Policy gradient update

**Critical path:** The bundling and MVaR computation is the core innovation. Each training step samples B questions, generates responses, forms G=10 disjoint bundles, computes bundle scores, tracks quantiles online, and applies the risk-averse objective with clipped IS ratios.

**Design tradeoffs:** Risk-sensitive objectives vs. mean-based rewards. Bundling size B=5 balances feedback richness against computational cost. MVaR parameters (α=0.2, β=0.8, ω=0.5) are tuned for math reasoning but may need adaptation for other domains.

**Failure signatures:** Early entropy collapse indicates incorrect risk-averse weighting or stuck quantile trackers. Zero gradients suggest bundle scores are too sparse or permutation sampling is flawed. Degraded easy-instance performance may indicate over-emphasis on hard problems.

**Exactly 3 first experiments:**
1. Implement bundling with B=5 on DAPO-math-17k and verify bundle score distribution across [0,B] range.
2. Compare entropy trajectories during training between RiskPO and GRPO to confirm entropy collapse mitigation.
3. Test sensitivity to quantile parameters by varying α from 0.1 to 0.3 and measuring impact on hard vs. easy problem performance.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the quantile levels (α, β), mixing weight (ω), and bundle size (B) be adaptively learned during training rather than manually tuned? The ablation studies show sensitivity to these hyperparameters, with deviations from (α=0.2, β=0.8, ω=0.5, B=5) consistently degrading performance. The paper provides no mechanism for automatic adaptation; optimal values may vary across datasets, model scales, or training stages.

**Open Question 2:** Do the theoretical guarantees and empirical benefits of RiskPO scale to much larger language models (e.g., 70B+ parameters)? All experiments use 1.5B and 3B models. Larger models may have different entropy dynamics and exploration characteristics that could alter the effectiveness of risk-sensitive objectives.

**Open Question 3:** Can RiskPO be extended to settings with continuous or dense reward signals beyond binary verifiable rewards? The bundling mechanism was specifically designed to address binary reward sparsity; continuous rewards may require different architectural choices.

**Open Question 4:** Under what conditions does Assumption 1 (monotonicity of conditional log-probability in reward tails) fail, and how robust is RiskPO to such violations? The authors validate Assumption 1 empirically but note fluctuations in mid-reward ranges and weaker lower-tail patterns on easier datasets like MATH.

## Limitations

- Missing key hyperparameters: learning rates for quantile trackers and policy updates, reference policy update frequency
- Bundle score computation and permutation sampling details not fully specified
- No explicit mention of optimizer settings (AdamW, weight decay, gradient clipping)
- Potential sensitivity to bundle size B and quantile parameters not thoroughly explored across diverse domains

## Confidence

- Theoretical analysis of entropy collapse mitigation via risk-averse objective: **High**
- Empirical gains on hard math (AIME24/25, AMC23) vs. GRPO: **Medium** (implementation gaps)
- Generalization to multi-modal and code tasks: **Medium** (fewer experiments)

## Next Checks

1. Reproduce RiskPO on DAPO-math-17k with DeepSeek-R1-Distill-Qwen-1.5B, using B=5 bundles and MVaR (α=0.2, β=0.8); compare Pass@1 on AIME24/25 to GRPO.

2. Plot entropy and lower-tail RVaR curves during training to verify RiskPO maintains higher entropy and better tail performance than mean-based methods.

3. Test sensitivity by varying bundle size B and quantile parameters; confirm risk-averse weighting improves hard-instance scores without degrading easy-instance performance.