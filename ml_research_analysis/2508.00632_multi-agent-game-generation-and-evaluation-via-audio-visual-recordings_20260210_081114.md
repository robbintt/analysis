---
ver: rpa2
title: Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings
arxiv_id: '2508.00632'
source_url: https://arxiv.org/abs/2508.00632
tags:
- content
- assets
- feedback
- generation
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of generating interactive multimedia
  content like video games, which requires handling multiple modalities (text, audio,
  video, images, 3D models) and human interaction. Current AI methods struggle with
  this complexity, particularly for content that typically requires teams of humans
  and artists working for months.
---

# Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings

## Quick Facts
- arXiv ID: 2508.00632
- Source URL: https://arxiv.org/abs/2508.00632
- Reference count: 40
- Key outcome: Current AI coding models struggle to effectively utilize custom assets and audio-visual feedback despite these being valuable for human creators

## Executive Summary
This paper introduces AVR-Eval, a relative evaluation metric for multimedia quality using Audio-Visual Recordings (AVRs), and AVR-Agent, a multi-agent framework for generating JavaScript games and animations. The system addresses the challenge of creating interactive multimedia content that requires handling multiple modalities and human interaction. Experiments demonstrate that AVR-Agent significantly improves content quality over one-shot generation, with best-of-k initial content selection proving more effective than extended iterations. However, the study reveals a fundamental limitation: current coding models fail to effectively utilize custom assets and AVR feedback, highlighting a critical gap between human and machine content creation approaches.

## Method Summary
The AVR-Agent framework uses a coding agent to select multimedia assets from a bank, generate initial JavaScript code, employ AVR-Eval to identify the best version, and iteratively improve it through omni-modal agent feedback from AVRs. AVR-Eval processes audio-visual recordings of two contents using an omni-modal model, compares them, and employs a text model to review the evaluation and determine superiority. The framework was tested with models including Qwen3-Coder-480B and Kimi-K2-1T, generating games and animations from pre-existing multimedia assets.

## Key Results
- AVR-Agent significantly improves content quality over one-shot generation
- Best-of-k initial content selection proves more effective than extended iterations
- Coding models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rates when these features are included
- Best-performing models were Qwen3-Coder-480B and Kimi-K2-1T

## Why This Works (Mechanism)
The AVR-Agent framework works by creating a closed-loop system where generated content is evaluated through AVR-Eval, which processes audio-visual recordings to compare content quality. This relative evaluation approach allows the system to iteratively refine content based on multimodal feedback. The framework leverages omni-modal models to process complex multimedia inputs and text models to formalize evaluation decisions, creating a comprehensive assessment pipeline that goes beyond traditional text-based code evaluation.

## Foundational Learning
1. **Audio-Visual Recordings (AVRs)**: Multi-modal content representations combining audio, video, and visual elements - needed to capture the full interactive experience of games and animations; quick check: verify AVRs can be processed by omni-modal models
2. **Relative Evaluation Metrics**: Comparing two content pieces rather than scoring against absolute standards - needed because absolute quality measures are subjective in creative domains; quick check: ensure pairwise comparisons are consistent
3. **Multi-Agent Framework**: Using multiple specialized agents (coding, evaluation, feedback) - needed to handle the complexity of multimodal content generation; quick check: verify agent coordination and information flow
4. **Omni-modal Models**: Models capable of processing text, audio, video, and images simultaneously - needed for comprehensive AVR evaluation; quick check: test model performance on individual modalities
5. **Best-of-K Selection**: Choosing the best from multiple initial attempts - needed to provide high-quality starting points for iteration; quick check: vary K values to find optimal selection size
6. **Iterative Refinement**: Multiple rounds of generation and evaluation - needed to progressively improve content quality; quick check: monitor improvement rate across iterations

## Architecture Onboarding

**Component Map**: Asset Bank -> Coding Agent -> Initial Code Generation -> AVR-Eval -> Text Model Review -> Feedback -> Coding Agent (iterative)

**Critical Path**: Asset Selection → Code Generation → AVR Creation → AVR-Eval → Quality Assessment → Iterative Improvement

**Design Tradeoffs**: The framework trades computational efficiency for quality by using iterative refinement rather than single-pass generation. The choice of AVR-Eval over traditional metrics enables multimodal evaluation but introduces dependency on omni-modal model capabilities. Best-of-K selection reduces iteration needs but requires multiple initial generation attempts.

**Failure Signatures**: 
- Coding models failing to incorporate custom assets indicates limited asset utilization capabilities
- No improvement with AVR feedback suggests poor multimodal information processing
- Best-of-K outperforming iterations may indicate coding models struggle with iterative refinement
- Dependency on omni-modal model quality makes AVR-Eval vulnerable to model limitations

**3 First Experiments**:
1. Test asset incorporation by measuring code similarity when custom assets are available versus when they're not
2. Evaluate AVR-Eval reliability by comparing pairwise evaluations against human judgments
3. Assess iteration effectiveness by tracking content quality improvements across refinement rounds

## Open Questions the Paper Calls Out
None

## Limitations
- Current coding models fail to effectively utilize custom assets and AVR feedback despite these being valuable for human creators
- Best-of-k selection effectiveness may be domain-specific and not generalizable to all content types
- AVR-Eval evaluation system may introduce biases based on how omni-modal models interpret audio-visual content

## Confidence
- High Confidence: AVR-Agent framework architecture and best-of-k selection effectiveness
- Medium Confidence: Claims about coding model limitations with custom assets and AVR feedback
- Low Confidence: Broader implications about fundamental differences between human and machine content creation

## Next Checks
1. Test AVR-Agent with different content types (interactive stories, educational content) to assess generalizability beyond JavaScript games
2. Conduct ablation studies isolating specific components of AVR-Eval to quantify modality contributions
3. Implement alternative iteration strategies to determine if coding model limitations are implementation-specific or fundamental