---
ver: rpa2
title: "Usando LLMs para Programar Jogos de Tabuleiro e Varia\xE7\xF5es"
arxiv_id: '2511.05114'
source_url: https://arxiv.org/abs/2511.05114
tags:
- jogos
- para
- llms
- implementac
- odigo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the ability of three large language models
  (Claude 3.7 Sonnet, DeepSeekV3, and ChatGPT-4o) to generate Python code for board
  games and their variations. The research tests these models on six classic board
  games (Tic-Tac-Toe, Peg Solitaire, Reversi, Nine Men's Morris, Checkers, and Chess)
  by requesting implementations both with and without the Boardwalk API, including
  two rule variations for each game.
---

# Usando LLMs para Programar Jogos de Tabuleiro e Variações

## Quick Facts
- arXiv ID: 2511.05114
- Source URL: https://arxiv.org/abs/2511.05114
- Reference count: 0
- Primary result: Evaluates three LLMs (Claude 3.7 Sonnet, DeepSeekV3, ChatGPT-4o) on generating Python code for board games and variations

## Executive Summary
This study investigates the capability of large language models to generate Python code for classic board games and their variations, using both independent implementations and the Boardwalk API. The research tests three models on six games (Tic-Tac-Toe, Peg Solitaire, Reversi, Nine Men's Morris, Checkers, and Chess) with two rule variations each. Through standardized prompts and manual playtesting, the study aims to quantify how well models can leverage existing knowledge of game rules and implement modifications, while evaluating the impact of API integration on code generation quality.

## Method Summary
The methodology employs a factorial design testing 36 conditions per model (6 games × 3 variants × 2 implementations), totaling 108 tests across three LLMs. Standardized prompts with $CHANGE placeholders are used to request both original and variant game implementations, with and without the Boardwalk API documentation. Generated code is manually playtested to identify seven error types (syntax, API usage, piece movement, victory conditions, game effects, board formatting, and turn order). Success is measured by the proportion of implementations with zero errors.

## Key Results
- LLMs demonstrate varying success rates in generating playable board game implementations
- The Boardwalk API's impact on code generation quality requires empirical validation
- Rule variation prompts test models' compositional reasoning abilities
- Error classification spans syntax, API usage, and game mechanics fidelity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate board game code by retrieving and adapting implementation patterns from pre-training data rather than learning from scratch.
- Mechan: Popular games (Chess, Checkers, Tic-Tac-Toe) have extensive representations in training corpora. When prompted with only a game name, models retrieve structural knowledge—board representation, move validation, win conditions—and translate these into executable Python without explicit rule descriptions.
- Core assumption: The target games appear frequently enough in training data that their rule sets and common implementations are reliably encoded.
- Evidence anchors:
  - [abstract] "given their capacity to efficiently generate code from simple contextual information"
  - [section 1] "devido à massiva quantidade de dados usada nos seus treinamentos, terão familiaridade com as regras e implementações existentes dos jogos de tabuleiro mais populares"
  - [corpus] Boardwalk paper (Becker et al. 2025) reports "razoável" success rate using similar prompting approach
- Break condition: If models generate plausible-looking but non-functional code for games they "know" by name, this indicates shallow pattern matching rather than retrieved structural understanding.

### Mechanism 2
- Claim: Rule variation prompts test compositional reasoning—models must merge retrieved base-game logic with novel constraints described in natural language.
- Mechan: Rather than describing a complete variant game, the prompt provides the canonical game name plus a delta ("5x5 board, 5-in-a-row wins"). The model must: (1) retrieve base implementation, (2) parse modification semantics, (3) selectively override specific functions while preserving unchanged mechanics.
- Core assumption: Models encode game logic in a modular enough form that targeted modifications can be applied without cascading failures.
- Evidence anchors:
  - [section 1] "testamos a capacidade de raciocínio em alto nível dos modelos sobre este conhecimento, conciliando o que sabem sobre as regras originais com as variações pedidas"
  - [section 3] Prompt template shows "$CHANGE" substitution approach—delta-only specification
  - [corpus] Todd et al. (2024) and Tanaka & Simo-Serra (2024) reported high error rates with Ludii GDL, suggesting grammar complexity matters; Python may be more tractable
- Break condition: If variant implementations fail at higher rates than originals, but failures cluster in specific error types (e.g., win conditions updated but move validation not), this reveals modular reasoning limits.

### Mechanism 3
- Claim: API documentation constrains output format, reducing variance but potentially introducing integration friction.
- Mechan: Providing Boardwalk API docs without source code forces models to generate code against a documented interface. This standardizes board representation, move handling, and agent integration points, but requires correct interpretation of API semantics from text alone.
- Core assumption: Models can accurately map natural-language API documentation to correct method calls without seeing implementation examples.
- Evidence anchors:
  - [section 1] "introduzimos a Boardwalk API, que permite uma relativa padronização do formato do código e fácil integração com agentes de IA jogadores"
  - [section 3] "nos testes com a API, será fornecida apenas a documentação da API, e não seu código fonte"
  - [corpus] Boardwalk paper is direct predecessor; corpus shows weak external validation (no citations yet, recent arxiv)
- Break condition: If API-based implementations show systematic errors (wrong method signatures, misunderstood return types) independent of game logic correctness, this indicates documentation interpretation failure.

## Foundational Learning

- **Concept: Game State Representation**
  - Why needed here: Understanding how board games decompose into state (board configuration, turn indicator, game phase) is prerequisite to evaluating whether generated code correctly implements game mechanics.
  - Quick check question: Can you identify what state variables must change when a pawn moves two squares forward versus one?

- **Concept: Compositional Prompting**
  - Why needed here: The methodology relies on prompts that reference existing knowledge ("Tic-Tac-Toe") plus modifications ("5x5 board"). Understanding how LLMs handle base+delta prompts informs expectation-setting.
  - Quick check question: If you prompt "Chess, but pawns can move backward," what components of standard chess logic should remain untouched?

- **Concept: Manual Playtesting as Evaluation**
  - Why needed here: The paper uses human-executed playtests rather than automated test suites. Understanding what errors are catchable through gameplay vs. static analysis shapes how to interpret results.
  - Quick check question: Would a syntax error be caught by playtesting? Would an incorrect stalemate condition?

## Architecture Onboarding

- **Component map:**
  - LLM Layer (Claude 3.7 Sonnet, DeepSeekV3, ChatGPT-4o) -> Poe platform -> Prompt templates -> Game code generation -> Manual playtesting -> Error classification

- **Critical path:**
  1. Define game × variation × implementation matrix (36 conditions per model)
  2. Generate prompts from templates with game-specific substitutions
  3. Execute model calls via Poe platform
  4. Run each generated program and play representative games
  5. Classify errors into 7 binary categories
  6. Compute "perfect implementation" rate as primary success metric

- **Design tradeoffs:**
  - Manual vs. automated playtesting: Manual catches semantic errors (wrong strategy feel) but doesn't scale; automated would miss gameplay experience issues
  - API documentation vs. source code: Docs-only tests realistic API usage but may hide implementation details that would clarify behavior
  - Name-only vs. full rules in prompt: Name-only tests retrieval capability but conflates knowledge quality with reasoning quality

- **Failure signatures:**
  - High syntax error rate → model unfamiliarity with Python patterns or API misinterpretation
  - Correct movement but wrong win conditions → modular reasoning partial failure
  - Good originals, poor variants → strong retrieval but weak compositional reasoning
  - API-specific errors independent of game logic → documentation comprehension gap

- **First 3 experiments:**
  1. **Baseline replication**: Implement one simple game (Tic-Tac-Toe) in both independent and API modes across all three models; verify evaluation pipeline catches known-injected errors
  2. **Variant stress test**: For one model, generate all three variants of one game; compare error-type distribution to identify whether failures cluster in specific modules
  3. **API boundary probe**: Generate API-based implementation with intentionally vague game description; test whether API structure alone guides valid output even when game rules are underspecified

## Open Questions the Paper Calls Out

- **Can LLMs successfully implement code for completely novel board games that fall outside their pre-training data?**
  - Basis in paper: [explicit] The authors explicitly state that future work should address "the implementation of completely new games, outside the LLMs' previous knowledge."
  - Why unresolved: The current study only tests well-known games (e.g., Chess, Checkers) and variants of them, leaving the domain of entirely new games unexplored.
  - What evidence would resolve it: A follow-up study evaluating LLM performance on games invented after the models' training cutoffs or specifically designed to be unique.

- **Can the evaluation of LLM-generated game code be effectively automated to replace manual playtesting?**
  - Basis in paper: [explicit] The authors identify "automated execution of playtests" as a specific aspect left open for future work to address.
  - Why unresolved: The proposed methodology relies on resource-intensive manual execution of games by humans to identify logic and syntax errors.
  - What evidence would resolve it: The development and validation of a script or agent capable of detecting implementation errors without human intervention.

- **Does the use of the Boardwalk API hinder or help the code generation process compared to independent implementations?**
  - Basis in paper: [inferred] The methodology section notes the need to verify "if the use of the API hinders code generation," implying the outcome is currently unknown.
  - Why unresolved: While the API offers standardization, it introduces external constraints; the trade-off between structure and complexity for the LLM remains untested.
  - What evidence would resolve it: Comparative error rates between the "with API" and "independent" implementation groups across the 108 planned tests.

## Limitations

- Manual playtesting introduces subjectivity and doesn't scale to larger test suites
- Focus on six classic games may not generalize to more complex or novel game designs
- API documentation-only access may create systematic errors unrelated to game logic

## Confidence

- **High Confidence**: LLMs can generate basic board game implementations from game names alone
- **Medium Confidence**: The Boardwalk API provides meaningful standardization benefits
- **Low Confidence**: The impact of rule variations on code quality can be precisely measured with current evaluation methodology

## Next Checks

1. **Error Reproducibility Test**: Have multiple independent evaluators playtest the same implementations and measure inter-rater reliability for each error category
2. **API Documentation Quality Test**: Compare error rates between API documentation-only condition and API source code access condition to quantify documentation comprehension gaps
3. **Generalization Test**: Generate implementations for two additional board games (one with complex rules, one with stochastic elements) to assess whether current methodology scales beyond classic deterministic games