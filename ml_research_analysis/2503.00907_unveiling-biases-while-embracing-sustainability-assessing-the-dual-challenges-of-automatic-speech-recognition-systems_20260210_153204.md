---
ver: rpa2
title: 'Unveiling Biases while Embracing Sustainability: Assessing the Dual Challenges
  of Automatic Speech Recognition Systems'
arxiv_id: '2503.00907'
source_url: https://arxiv.org/abs/2503.00907
tags:
- speech
- systems
- carbon
- whisper
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates bias and sustainability in two state-of-the-art
  ASR systems, Whisper and MMS, using two datasets representing read and spontaneous
  speech. Bias was assessed across gender, accent, and age groups, while sustainability
  was measured via carbon emissions and energy consumption using three tracking tools
  across four NVIDIA GPUs.
---

# Unveiling Biases while Embracing Sustainability: Assessing the Dual Challenges of Automatic Speech Recognition Systems

## Quick Facts
- arXiv ID: 2503.00907
- Source URL: https://arxiv.org/abs/2503.00907
- Reference count: 0
- Primary result: Whisper Medium outperforms larger Whisper variants on fairness for older speakers while being more energy-efficient than Whisper Large variants

## Executive Summary
This study evaluates two state-of-the-art ASR systems, Whisper and MMS, across bias (gender, accent, age) and sustainability (carbon emissions, energy consumption) dimensions. Using read speech (Artie-Bias corpus) and spontaneous speech (CCv2) datasets, the research reveals that while Whisper variants generally outperform MMS on read speech across all bias categories, this advantage drastically degrades on spontaneous speech. Notably, Whisper Medium (769M parameters) often outperforms larger Whisper variants, particularly for older age groups, suggesting that model scale does not linearly correlate with fairness outcomes. The study also finds that MMS consistently shows lower carbon emissions and energy consumption than Whisper variants, with the NVIDIA A100 GPU providing the best efficiency due to its wider memory bandwidth.

## Method Summary
The study evaluates Whisper Medium, Whisper Large-v1/v2/v3 (all 1550M parameters, English-only), and MMS (965M parameters) on two English speech datasets representing read and spontaneous speech. Word Error Rate (WER), Character Error Rate (CER), and Phoneme Error Rate (PER) are computed using the jiwer library. Statistical significance is assessed via pairwise ANOVA with p < 0.05. Sustainability is measured using three carbon tracking tools (codecarbon, carbontracker, eco2ai) during inference on 20 minutes of speech across four NVIDIA GPUs (RTX-5000, RTX-A5000, A100, A6000). Results are segmented by demographic categories (gender, accent, age) and speech type.

## Key Results
- Whisper Medium (769M) generally outperforms larger Whisper variants (1550M) across bias categories, especially for older age groups
- Whisper variants outperform MMS on read speech across all bias categories, but this advantage drastically degrades on spontaneous speech
- MMS consistently shows lower carbon emissions and energy consumption than Whisper variants, with NVIDIA A100 GPU yielding the best efficiency due to wider bandwidth
- eco2ai tool consistently underestimates emissions compared to codecarbon and carbontracker

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller Whisper variants can outperform larger variants on demographic subgroups, particularly older speakers
- Mechanism: The paper observes that Whisper Medium (769M parameters) performs better than Large variants (1550M) across bias categories, especially for older age groups. The authors suggest this may relate to overfitting or training data distribution, though the precise cause is not established
- Core assumption: Model parameter count does not linearly correlate with fairness outcomes across demographic subgroups
- Evidence anchors:
  - [abstract] "Whisper Medium generally outperformed larger variants, especially for older age groups"
  - [section 5.1] "It is noteworthy that younger age groups, such as those under 20 and in their 20s, exhibit higher WERs than older age groups"
  - [corpus] Related work on ASR bias (e.g., "Exploring Gender Disparities in ASR") also finds non-linear relationships between model scale and fairness, but mechanisms remain underexplored
- Break condition: If training data for older speakers is proportionally better represented or if evaluation artifacts (text normalization) favor older speech patterns, this observation may not generalize

### Mechanism 2
- Claim: ASR systems trained primarily on read speech degrade significantly on spontaneous speech, with bias patterns reversing across speech types
- Mechanism: Whisper outperforms MMS on read speech across all bias categories, but this advantage "drastically" degrades on spontaneous speech (CCv2). Gender bias reverses: female speech performs better on spontaneous, male speech better on read. The authors attribute this to "unbalanced training datasets across genders due to sociolinguistic differences"
- Core assumption: Training data composition (read vs. spontaneous) and demographic balance causally determines cross-domain fairness
- Evidence anchors:
  - [abstract] "Whisper variants outperform MMS on read speech for all bias categories, but Whisper performance degrades significantly on spontaneous speech"
  - [section 5.1] "For spontaneous speech, female speech performed better than male and vice-versa for read speech, which is probably a reflection of the unbalanced training datasets"
  - [corpus] Weak direct evidence; related papers discuss accent and gender bias but do not systematically compare read vs. spontaneous speech mechanisms
- Break condition: If the CCv2 dataset has artifacts (e.g., consistent textual content across speakers) that interact differently with model tokenization, the observed reversal may be dataset-specific

### Mechanism 3
- Claim: GPU memory bandwidth affects inference energy efficiency and carbon emissions
- Mechanism: The NVIDIA A100-40GB showed better efficiency than other GPUs (RTX-5000, RTX-A5000, A6000). The authors attribute this to the A100's "wider GPU bandwidth (1555 GB/s)" enabling faster data movement and reduced computation time per inference
- Core assumption: Higher memory bandwidth reduces total energy consumed per inference by minimizing idle compute cycles
- Evidence anchors:
  - [abstract] "NVIDIA A100 GPU yielding the best efficiency due to wider bandwidth"
  - [section 5.2] "This could be explained by the fact that NVIDIA GPU A100-40GB has a much wider GPU bandwidth (1555 GB/s) than the other 3"
  - [corpus] No direct corpus evidence on GPU bandwidth mechanisms for ASR sustainability; this is an underexplored area
- Break condition: If other factors (CUDA core count, tensor core utilization, power management) dominate energy consumption, bandwidth alone may not predict efficiency

## Foundational Learning

- Concept: **Word Error Rate (WER) and statistical significance testing (ANOVA, p-values)**
  - Why needed here: Bias claims are assessed via WER differences across demographic groups, with p<0.05 indicating statistical significance. Without this, you cannot distinguish real bias from noise
  - Quick check question: If Model A has WER=10% for Group X and WER=12% for Group Y, what additional information do you need to claim bias?

- Concept: **Inference-time carbon tracking (scope, tools, emission intensity)**
  - Why needed here: Sustainability claims rely on carbon tracking tools (codecarbon, carbontracker, eco2ai) that estimate emissions based on energy consumption and regional emission intensity. Different tools yield different estimates
  - Quick check question: Why might eco2ai consistently underestimate emissions compared to other tools?

- Concept: **Model architecture trade-offs: adapters vs. large vocabulary decoders**
  - Why needed here: MMS uses language-specific adapters that restrict vocabulary tokens; Whisper uses 50K+ tokens. This architectural difference affects both bias and sustainability
  - Quick check question: How might a restricted output vocabulary affect both WER and energy consumption?

## Architecture Onboarding

- Component map:
  Audio waveforms -> Whisper/MMS model -> Text hypothesis -> WER/CER/PER evaluation -> Demographic segmentation -> Statistical testing -> Carbon tracking
  Whisper: Transformer encoder-decoder, 769M-1550M params, 50K+ token vocabulary
  MMS: wav2vec 2.0 backbone + language-specific adapters, 965M params, restricted vocabulary per language

- Critical path:
  1. Load audio -> normalize text (consistent preprocessing)
  2. Run inference (track time, energy)
  3. Compute WER against ground truth
  4. Segment results by demographic (gender, accent, age)
  5. Statistical testing (pairwise ANOVA, p<0.05)
  6. Aggregate energy/carbon per model-GPU combination

- Design tradeoffs:
  - Whisper Medium vs. Large: Medium often fairer (lower WER variance across groups) and more efficient; Large provides no clear fairness gain in this study
  - MMS vs. Whisper: MMS more sustainable (lower emissions) but higher WER on read speech; competitive on spontaneous speech
  - GPU selection: A100 most efficient but higher cost; consumer GPUs (RTX) less efficient but accessible

- Failure signatures:
  - Drastic WER increase on spontaneous vs. read speech (Whisper): indicates training data mismatch
  - Inconsistent carbon estimates across tools (eco2ai underestimates): indicates tool-specific emission intensity factors or measurement scope differences
  - Non-significant p-values despite WER differences: indicates insufficient sample size or high variance

- First 3 experiments:
  1. Replicate bias evaluation on a held-out dataset with balanced demographics to verify whether Medium-outperforms-Large finding generalizes
  2. Profile inference latency and energy per utterance (not just aggregate) to identify whether bandwidth benefits scale linearly with audio duration
  3. Ablate vocabulary size in Whisper (restrict to MMS-equivalent token set) to isolate whether vocabulary alone explains sustainability differences

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the use of language-specific adapters in ASR architectures provide a measurable advantage in reducing carbon emissions and mitigating biases compared to full-vocabulary models?
- **Basis in paper:** [explicit] The authors note that MMS uses language-specific adapters which restrict vocabulary, unlike Whisper, and state that "one should conduct a comprehensive analysis and perform a suitable study on each type of ASR usage"
- **Why unresolved:** The study compares two different architectures (MMS vs. Whisper) but does not isolate the adapter mechanism as the sole variable responsible for efficiency or fairness differences
- **What evidence would resolve it:** A controlled ablation study comparing adapter-based architectures against full-vocabulary models while holding model size and training data constant

### Open Question 2
- **Question:** Which carbon tracking platform (eco2ai, codecarbon, or carbontracker) provides the most accurate estimate of inference-time energy consumption?
- **Basis in paper:** [explicit] The results show that "eco2ai consistently reported lower carbon emissions and energy consumption than carbontracker and codecarbon," creating uncertainty regarding ground truth
- **Why unresolved:** The paper identifies the discrepancy between tools but lacks a ground-truth baseline (e.g., physical power meter readings) to determine which software estimate is correct
- **What evidence would resolve it:** A benchmark study comparing tool outputs against hardware-level power consumption meters during identical inference workloads

### Open Question 3
- **Question:** Why do state-of-the-art ASR systems consistently yield lower Word Error Rates (WER) for older age groups compared to younger demographics?
- **Basis in paper:** [inferred] The authors observe that models "behaved consistently better for higher age groups" and that younger groups exhibited higher WERs, a counter-intuitive finding that warrants further investigation into causality
- **Why unresolved:** The paper reports the performance disparity but does not analyze the underlying acoustic features or dataset imbalances that might cause younger speech to be recognized less accurately
- **What evidence would resolve it:** An analysis of acoustic features (jitter, shimmer, speaking rate) across age groups combined with an evaluation of training data demographic distributions

## Limitations

- The mechanisms explaining why Whisper Medium outperforms larger variants for older speakers remain speculative without direct empirical validation
- The study's conclusions about GPU efficiency improvements from wider bandwidth are based on single-model comparisons without controlling for other architectural differences
- The sustainability measurements depend on three different tracking tools that yield inconsistent emission estimates, with eco2ai systematically underestimating emissions

## Confidence

- **High confidence**: The comparative WER performance between Whisper and MMS variants on read vs. spontaneous speech is well-established through statistical testing (ANOVA, p<0.05). The sustainability measurement methodology using multiple tracking tools is transparent and reproducible
- **Medium confidence**: The observation that Whisper Medium outperforms larger variants for older age groups is supported by data but lacks a validated mechanism. The claim about A100 GPU efficiency due to bandwidth is plausible but not rigorously isolated from other architectural factors
- **Low confidence**: The explanations for bias reversals between speech types (sociolinguistic differences in training data) and the eco2ai underestimation mechanism are speculative without empirical validation

## Next Checks

1. Replicate demographic performance across datasets: Test whether the Whisper Medium advantage for older speakers generalizes to independent datasets with balanced age representation, controlling for text normalization artifacts

2. Isolate GPU efficiency factors: Conduct controlled experiments varying only GPU memory bandwidth while holding constant other architectural parameters (core count, power limits) to verify if bandwidth alone predicts energy efficiency

3. Validate bias reversal mechanism: Analyze training data distribution across gender and speech type (read vs. spontaneous) to empirically confirm whether sociolinguistic differences in data balance causally explain the observed bias pattern reversals