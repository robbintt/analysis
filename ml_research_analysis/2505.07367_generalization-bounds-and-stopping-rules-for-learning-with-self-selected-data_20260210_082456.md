---
ver: rpa2
title: Generalization Bounds and Stopping Rules for Learning with Self-Selected Data
arxiv_id: '2505.07367'
source_url: https://arxiv.org/abs/2505.07367
tags:
- learning
- sample
- reciprocal
- data
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes universal generalization bounds for reciprocal
  learning algorithms that self-select training data based on previously learned parameters.
  The key contribution is proving that these algorithms can generalize well despite
  altering their training samples, using covering numbers and Wasserstein ambiguity
  sets.
---

# Generalization Bounds and Stopping Rules for Learning with Self-Selected Data

## Quick Facts
- arXiv ID: 2505.07367
- Source URL: https://arxiv.org/abs/2505.07367
- Authors: Julian Rodemann; James Bailie
- Reference count: 40
- Key outcome: Universal generalization bounds for reciprocal learning algorithms that self-select training data based on previously learned parameters

## Executive Summary
This paper establishes theoretical foundations for learning algorithms that dynamically select their own training data based on previously learned parameters. The key contribution is proving that such self-selecting algorithms can achieve good generalization despite altering their training samples, using covering numbers and Wasserstein ambiguity sets. The bounds require only verifiable conditions on the algorithms themselves, not assumptions about the distribution of self-selected data. Results include both convergent and finite-iteration solutions, with the latter providing anytime-valid stopping rules for practitioners.

## Method Summary
The paper develops a theoretical framework analyzing learning algorithms that choose their own training data based on current parameter estimates. It establishes generalization bounds using covering numbers and Wasserstein ambiguity sets to analyze shifts in empirical distributions caused by the learning algorithms themselves. The approach applies to various learning paradigms including active learning, bandits, and boosting. The theoretical analysis proves bounds for both convergent algorithms and finite-iteration solutions, with the latter providing stopping rules that guarantee probabilistic error bounds at any iteration.

## Key Results
- Universal generalization bounds proven for reciprocal learning algorithms that self-select training data
- Finite-iteration solutions provide anytime-valid stopping rules with probabilistic guarantees
- Concrete example demonstrates stopping rules for semi-supervised learning: after adding 153 pseudo-labeled data points, generalization error remains within specified bound with 95% confidence
- Framework reinterprets Wasserstein ambiguity sets to analyze distribution shifts caused by learning algorithms

## Why This Works (Mechanism)
The paper's approach works by analyzing how learning algorithms that self-select data modify the empirical distribution of their training samples. By using covering numbers to bound the complexity of the hypothesis space and Wasserstein ambiguity sets to quantify distribution shifts, the framework captures the interplay between learning progress and data selection. The key insight is that even though algorithms alter their training data based on learned parameters, the changes can be bounded in a way that preserves generalization guarantees. This allows for rigorous analysis of stopping rules that terminate learning when sufficient information has been gathered.

## Foundational Learning
- Covering numbers: Measure the complexity of hypothesis classes needed to approximate any function within a certain error
  - Why needed: Essential for bounding the generalization error in terms of the hypothesis class complexity
  - Quick check: Verify that covering numbers grow polynomially with dimension for the hypothesis class

- Wasserstein ambiguity sets: Quantify uncertainty in probability distributions using optimal transport metrics
  - Why needed: Used to bound the distribution shifts caused by self-selecting algorithms
  - Quick check: Confirm that Wasserstein distance is finite between original and self-selected distributions

- Strong convexity: Ensures unique minima and controlled convergence behavior
  - Why needed: Required for the convergence proofs and bound derivations
  - Quick check: Verify the loss function satisfies μ-strong convexity for some μ > 0

- Lipschitz continuity: Bounds the sensitivity of functions to input changes
  - Why needed: Controls how much the loss can change with small parameter perturbations
  - Quick check: Confirm the loss function is L-Lipschitz continuous for some L > 0

## Architecture Onboarding

Component map: Self-selecting algorithm -> Covering number calculation -> Wasserstein ambiguity set construction -> Generalization bound derivation

Critical path: The algorithm selects data points -> Empirical distribution shifts -> Covering numbers bound hypothesis complexity -> Wasserstein sets quantify distribution changes -> Final generalization bound combines both effects

Design tradeoffs: The framework trades computational complexity (calculating covering numbers) for theoretical guarantees. The bounds are tighter when covering numbers are smaller, but computing covering numbers exactly can be expensive. The use of Wasserstein ambiguity sets provides robustness to distribution shifts but requires careful calibration of the radius parameter.

Failure signatures: Bounds may become vacuous if covering numbers grow too quickly with dimension or if the algorithm creates extreme distribution shifts. The framework assumes the self-selecting behavior is "reciprocal" (data selection depends only on current parameters), which may not hold for all algorithms. Computational intractability of covering numbers for high-dimensional spaces.

Three first experiments:
1. Verify covering number calculations for simple hypothesis classes (linear functions) before scaling to complex models
2. Test stopping rules on synthetic semi-supervised learning tasks with known ground truth
3. Compare theoretical bounds against empirical generalization error on benchmark datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Bounds rely on strong convexity and Lipschitz continuity conditions that may not generalize to modern deep learning architectures
- Covering number calculations can be computationally expensive for complex hypothesis classes
- Framework assumes self-selection is "reciprocal" - data selection depends only on current parameters
- Theoretical bounds may become vacuous for highly non-convex loss functions or extreme sample distribution shifts

## Confidence
- High confidence in mathematical proofs for the specific class of reciprocal learning algorithms analyzed
- Medium confidence in broader applicability claims to other learning paradigms without comprehensive empirical validation
- Medium confidence in practical utility of stopping rules, as effectiveness depends heavily on accurate covering number estimates

## Next Checks
1. Test the stopping rules on non-convex neural network architectures to assess robustness beyond strongly convex losses
2. Evaluate computational efficiency of covering number calculations for high-dimensional feature spaces typical in modern applications
3. Conduct empirical studies comparing the proposed stopping rules against established early stopping criteria in benchmark semi-supervised learning tasks to verify practical performance gains