---
ver: rpa2
title: Collaborative Agentic AI Needs Interoperability Across Ecosystems
arxiv_id: '2505.21550'
source_url: https://arxiv.org/abs/2505.21550
tags:
- agents
- agentic
- agent
- interoperability
- protocol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that collaborative agentic AI is rapidly evolving
  toward fragmented, incompatible ecosystems due to the proliferation of isolated
  agent frameworks, each with distinct protocols, formats, and primitives. This fragmentation
  imposes integration burdens, limits composability, and threatens scalability and
  security.
---

# Collaborative Agentic AI Needs Interoperability Across Ecosystems

## Quick Facts
- **arXiv ID:** 2505.21550
- **Source URL:** https://arxiv.org/abs/2505.21550
- **Reference count:** 40
- **Primary result:** Fragmented agent ecosystems threaten scalability and security; minimal HTTP-based interoperability standards can enable cross-ecosystem collaboration.

## Executive Summary
The rapid proliferation of isolated agent frameworks creates incompatible ecosystems that limit composability and impose integration burdens. The authors propose WEB OF AGENTS, a minimal interoperability blueprint that avoids both rigid unification and brittle translation layers. By defining four essential building blocks—HTTP messaging, flexible interaction documentation, session-based state management, and URL-based discovery—the approach enables agents from different ecosystems to collaborate seamlessly while preserving innovation flexibility.

## Method Summary
The paper identifies four minimal building blocks for agent interoperability: HTTP-based messaging using GET/POST requests, LLM-interpretable interaction documentation for capability discovery, session and database integration for state management, and URL-based agent discovery via well-known paths. The approach leverages existing web technologies to ensure backward compatibility and scalability. The method demonstrates cross-ecosystem collaboration through examples like hypotheses-generation and code-generation agents working together without requiring unified protocols or complex translation layers.

## Key Results
- HTTP-based messaging provides sufficient transport for inter-agent communication without specialized protocols
- LLM-powered agents can interpret flexible interaction documentation without rigid schemas
- Minimal standardization across four building blocks prevents fragmentation while preserving innovation flexibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HTTP-based messaging provides sufficient transport for inter-agent communication without requiring specialized protocols.
- Mechanism: Agents exchange structured requests using HTTP GET (information retrieval) and POST (complex interactions with payloads). This reuses decades of web infrastructure optimization and enables agents to coexist with traditional web services.
- Core assumption: Agent communication patterns can be adequately mapped to request-response semantics without requiring WebSocket-style persistent connections or RPC frameworks.
- Evidence anchors:
  - [Section 5.1] "By adopting HTTP as the foundational transport protocol, agents can exchange information using established methods: GET requests to retrieve information from other agents... and POST requests for more complex interactions"
  - [Abstract] "Web of Agents adopts existing standards and reuses existing infrastructure where possible"
  - [Corpus: Agent Network Protocol Technical White Paper] Describes how existing internet infrastructure creates collaboration costs, indirectly supporting the need for HTTP-based approaches
- Break condition: If agents require true bidirectional streaming or sub-millisecond latency coordination, HTTP may become a bottleneck requiring protocol extensions.

### Mechanism 2
- Claim: LLM-powered agents can interpret flexible interaction documentation without rigid schemas, enabling cross-ecosystem interoperability.
- Mechanism: Agents publish interaction documents describing their interface requirements, capabilities, expected inputs, and outputs. Because agents are LLM-powered, they can interpret documentation ranging from structured JSON to natural language descriptions without prior agreement on formats.
- Core assumption: Modern LLMs possess sufficient comprehension to adapt to varying documentation formats and extract actionable interface specifications.
- Evidence anchors:
  - [Section 5.2] "Since modern AI agents are powered by LLMs, they can interpret and adapt to various documentation formats, from structured JSON to natural language descriptions"
  - [Section 5.2] "This documentation acts as a standardized handshake, accessible via common endpoints using HTTP requests"
  - [Corpus: Agentic Web paper] Discusses agents interacting directly to coordinate tasks, consistent with flexible discovery approaches
- Break condition: If documentation becomes ambiguous or LLMs misinterpret interface requirements, agents may send malformed requests causing cascading failures.

### Mechanism 3
- Claim: Minimal standardization across four building blocks prevents ecosystem fragmentation while preserving innovation flexibility.
- Mechanism: Rather than enforcing a single unified protocol (which stifles innovation) or building brittle translation layers (which create maintenance burdens), the approach defines only four minimal components: messaging, interaction docs, state management, and discovery. Each component reuses existing web technologies.
- Core assumption: The identified four building blocks are both necessary and sufficient for collaborative agentic AI; no critical primitives are missing.
- Evidence anchors:
  - [Section 4] "Interoperability is not just a nice-to-have feature, it is foundational. It enables secure, composable, scalable, and decentralized agentic ecosystems"
  - [Section 5.5] Demonstrates how a hypotheses-generation agent and code-generation agent from different ecosystems (e.g., A2A and ACP) can collaborate using just these four building blocks
  - [Corpus: Limited direct validation] Related papers on NANDA Index and Agentic Web discuss similar challenges but don't yet provide empirical validation of minimal-standard approaches
- Break condition: If production deployments reveal missing primitives (e.g., authentication delegation, trust scoring, or payment protocols), additional standardization layers would be required.

## Foundational Learning

- Concept: **HTTP request-response semantics and REST principles**
  - Why needed here: The entire messaging layer assumes engineers understand when to use GET vs. POST, how to structure request payloads, and how HTTP status codes convey outcomes.
  - Quick check question: Can you explain why a stateless HTTP POST with session cookies is preferable to maintaining persistent WebSocket connections for multi-turn agent conversations?

- Concept: **Session management via cookies and database-backed state**
  - Why needed here: Short-term memory uses HTTP sessions; long-term memory uses database patterns. Engineers must understand both to implement the state management building block.
  - Quick check question: If an agent must remember a user's preference across multiple independent conversations spanning weeks, which storage mechanism applies and where does the session identifier live?

- Concept: **Capability discovery and well-known URI patterns**
  - Why needed here: Agent discovery relies on URLs and well-known paths (analogous to robots.txt). Understanding how web crawlers discover resources translates directly to how agents discover each other.
  - Quick check question: What HTTP request would a search agent send to discover another agent's capabilities, and at what URL path would it expect the response?

## Architecture Onboarding

- Component map:
  - HTTP server endpoint → capability document endpoint → session manager → database persistence → well-known discovery path

- Critical path:
  1. Implement HTTP endpoint accepting agent requests → 2. Generate and serve interaction documentation → 3. Add session tracking for multi-turn conversations → 4. Register agent at discoverable URL with well-known capability path

- Design tradeoffs:
  - **Structured vs. flexible documentation**: Structured JSON schemas enable validation but reduce flexibility; natural language documentation relies on LLM interpretation accuracy
  - **Stateless vs. stateful design**: Fully stateless simplifies scaling but requires retransmitting context; session-based reduces redundancy but introduces server-side memory pressure
  - **Centralized registry vs. decentralized discovery**: Registry provides faster lookups but creates single point of failure; crawling is robust but slower

- Failure signatures:
  - **Interaction document mismatch**: Agent sends requests in format A but target expects format B → HTTP 400 errors or LLM misinterpretation
  - **Session loss**: Cookie expiration mid-conversation → agent loses context, requests redundant information
  - **Discovery timeout**: Well-known path unavailable or malformed → search agents cannot index capability, isolation occurs
  - **Cross-ecosystem translation failure**: An A2A agent attempts to invoke an ACP agent without interaction document fallback → protocol error

- First 3 experiments:
  1. **Hello-agent test**: Deploy minimal agent with HTTP endpoint returning a capability document; verify another agent can discover and query it using only the four building blocks.
  2. **Cross-ecosystem handshake**: Build one agent using A2A-style conventions and another using ACP conventions; test collaboration via HTTP messaging and interaction documents (not native protocols).
  3. **State continuity probe**: Implement session-based short-term memory; verify a multi-turn conversation survives server restart when backed by database persistence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can schema-free interaction documentation provide sufficient reliability for autonomous agents to negotiate complex tasks without standardization?
- Basis in paper: [explicit] Section 5.2 states that interaction documentation "need not follow a schema" because LLMs can "interpret and adapt to various documentation formats."
- Why unresolved: The paper assumes LLM reasoning capabilities are robust enough to replace rigid interface definitions, but does not provide evidence that semantic ambiguity can be avoided at scale.
- What evidence would resolve it: Empirical benchmarks comparing task completion success rates between agents using unstructured natural language documentation versus structured schemas.

### Open Question 2
- Question: How can security and trust models be effectively standardized for non-deterministic agents within a minimal interoperability framework?
- Basis in paper: [inferred] Section 7 explicitly identifies trust, security, and privacy as "critical challenges" due to non-deterministic behavior that the proposed architectural foundation does not address.
- Why unresolved: While the blueprint reuses web transport security (HTTPS), it lacks primitives for agent-level authorization, reputation, or behavioral verification.
- What evidence would resolve it: A proposed extension to the minimal standards that successfully mitigates agent-specific threats (e.g., prompt injection, malicious tool use) across different ecosystems.

### Open Question 3
- Question: Is a crawler-based discovery model viable for real-time indexing of ephemeral or state-heavy agents?
- Basis in paper: [inferred] Section 5.4 proposes "search engines for agents that regularly crawl and index," but agents may change capabilities or state faster than crawl intervals.
- Why unresolved: The paper applies a static web-page analogy to dynamic agents without addressing the latency or consistency issues inherent in pull-based discovery.
- What evidence would resolve it: Simulation results showing discovery latency and resource usage in a high-churn agent ecosystem using the proposed well-known path method.

## Limitations
- Reliance on LLM interpretation of flexible documentation introduces reliability uncertainty without empirical validation
- Four building blocks may be insufficient for complex scenarios requiring additional primitives like authentication delegation
- HTTP request-response model may not adequately handle scenarios requiring persistent bidirectional streaming

## Confidence
- **High Confidence:** The fundamental problem identification (ecosystem fragmentation) and general architectural approach (minimal standardization via HTTP and existing web technologies) are well-grounded and technically sound.
- **Medium Confidence:** The specific implementation details for each building block (particularly the interaction document format and LLM-based interpretation mechanisms) are reasonable but lack empirical validation.
- **Low Confidence:** The claim that these four building blocks are both necessary and sufficient for all collaborative agentic AI scenarios, especially for complex multi-agent workflows or specialized domains requiring additional primitives.

## Next Checks
1. **Multi-Agent Protocol Stress Test:** Deploy a complex workflow involving 5+ agents from different ecosystems collaborating on a real task (e.g., trip planning with research, booking, and calendar coordination) to identify edge cases and missing primitives.
2. **LLM Interpretation Accuracy Study:** Systematically test how different LLM models interpret various interaction document formats (structured JSON, natural language, mixed formats) and measure error rates in understanding capabilities and requirements.
3. **Performance Benchmarking:** Compare the HTTP-based approach against specialized protocols (WebSocket, gRPC) for latency-sensitive scenarios to quantify the practical limits of the proposed messaging layer.