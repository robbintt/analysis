---
ver: rpa2
title: Near-Exponential Savings for Mean Estimation with Active Learning
arxiv_id: '2511.05736'
source_url: https://arxiv.org/abs/2511.05736
tags:
- label
- algorithm
- mean
- partibandits
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficiently estimating the mean
  of a k-class random variable when labels are limited but informative covariates
  are available. The core challenge is that the optimal stratification of the data
  is unknown, and poor stratification can lead to inefficient or biased estimates.
---

# Near-Exponential Savings for Mean Estimation with Active Learning

## Quick Facts
- arXiv ID: 2511.05736
- Source URL: https://arxiv.org/abs/2511.05736
- Authors: Julian M. Morimoto; Jacob Goldin; Daniel E. Ho
- Reference count: 40
- Key outcome: PartIBandits achieves Õ(ν + exp(c·(-N/log(N)))/N) error rate, near-exponentially faster than classical methods when covariates are predictive

## Executive Summary
This paper addresses the challenge of mean estimation for k-class random variables when labels are scarce but informative covariates are available. The key insight is that traditional methods suffer when the optimal data stratification is unknown, potentially leading to inefficient or biased estimates. The authors propose PartiBandits, a two-stage active learning algorithm that first learns an optimal partition of the data to minimize within-stratum variance, then applies stratified sampling to estimate the mean efficiently.

The theoretical contribution shows that PartiBandits achieves near-exponential convergence rates compared to classical methods, with error bounds that depend on the Bayes-optimal classifier's risk. The paper bridges two previously separate research strands by connecting disagreement-based active learning with UCB-style stratified sampling approaches. Empirical evaluations on both synthetic data and real-world electronic health records demonstrate practical effectiveness, particularly in small-sample regimes where the method outperforms simple random sampling.

## Method Summary
PartiBandits is a two-stage active learning algorithm for efficient mean estimation. In the first stage, it learns an optimal partition of the data that minimizes average within-stratum variance using a disagreement-based active learning subroutine. This involves iteratively querying labels for instances where different classifiers disagree, thereby identifying boundaries between strata. In the second stage, the algorithm applies a UCB-style stratified sampling method (WarmStart-UCB) to estimate the mean within each stratum and aggregate these estimates into a final mean estimate.

The key innovation is the integration of active learning for partition discovery with stratified sampling for mean estimation. By learning the partition that minimizes within-stratum variance, the algorithm ensures that stratified sampling is applied optimally. The theoretical analysis shows that this approach achieves near-exponential convergence rates, with the error bound scaling as Õ(ν + exp(c·(-N/log(N)))/N), where ν is the risk of the Bayes-optimal classifier.

## Key Results
- Achieves error rate of Õ(ν + exp(c·(-N/log(N)))/N), which is near-exponentially faster than classical methods when covariates are predictive
- Shows near-exponential convergence rates compared to classical methods, with bounds dependent on Bayes-optimal classifier risk
- Demonstrates minimax optimality of convergence rates in classical settings
- Bridges theoretical gap between UCB and disagreement-based approaches in active learning
- Outperforms simple random sampling in practical small-sample regimes on both synthetic and EHR datasets

## Why This Works (Mechanism)
The algorithm works by optimally partitioning the data space to minimize within-stratum variance, then applying stratified sampling on this learned partition. This approach is particularly effective when covariates are predictive of the outcome, as the learned partition can capture the relationship between covariates and the label. The near-exponential convergence rate arises because the algorithm efficiently learns to stratify the data, reducing variance within each stratum and allowing for more precise mean estimation with fewer samples.

The mechanism leverages the fact that active learning can efficiently identify boundaries between different strata by querying labels where classifiers disagree. This targeted labeling strategy focuses resources on the most informative regions of the data space, enabling the algorithm to learn an effective partition with fewer labeled examples than would be required for random sampling. The UCB component then ensures that the sampling within each stratum is balanced and converges to the true stratum means efficiently.

## Foundational Learning

**Bayes-optimal classifier risk (ν)**: The minimum possible error rate achievable by any classifier on the given data distribution. Why needed: Serves as a baseline for measuring the effectiveness of the learned partition. Quick check: Can be estimated from a validation set or upper bounded using VC dimension.

**Within-stratum variance**: The variance of labels within each partition of the data space. Why needed: Minimizing this variance is key to efficient stratified sampling. Quick check: Can be estimated empirically once a partition is learned.

**Disagreement-based active learning**: An active learning approach that queries labels where different classifiers disagree. Why needed: Efficiently identifies boundaries between strata without requiring labels everywhere. Quick check: Monitor the rate of disagreement reduction as more labels are queried.

**UCB (Upper Confidence Bound)**: A bandit algorithm that balances exploration and exploitation in sampling. Why needed: Ensures balanced sampling within each stratum while providing theoretical guarantees. Quick check: Track the proportion of samples allocated to each stratum over time.

**Stratified sampling**: A sampling technique that divides the population into homogeneous subgroups and samples from each subgroup. Why needed: Reduces variance in mean estimation when strata are homogeneous. Quick check: Compare variance of estimates with and without stratification.

## Architecture Onboarding

**Component map**: Data space → Partition learning (disagreement-based active learning) → Stratified sampling (WarmStart-UCB) → Mean estimation

**Critical path**: The algorithm must first learn an effective partition before stratified sampling can be applied optimally. The quality of the partition directly determines the efficiency of the mean estimation.

**Design tradeoffs**: The algorithm trades off between the cost of learning the partition (querying labels) and the efficiency gains from stratified sampling. More queries lead to better partitions but increase labeling cost.

**Failure signatures**: Poor performance occurs when covariates are not predictive of the outcome (ν is large), when the optimal partition is too complex to learn with available labels, or when the WarmStart-UCB component fails to converge within strata.

**3 first experiments**:
1. Test performance on synthetic data with known optimal partition to verify near-exponential convergence
2. Evaluate sensitivity to the number of labeled examples by varying the query budget
3. Compare performance against random sampling and passive stratified sampling baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees depend on knowing or being able to estimate the Bayes-optimal classifier's risk ν
- Assumes optimal partition minimizes within-stratum variance, which may not hold for all data distributions
- Empirical evaluation is limited in scope, focusing on synthetic data and EHR datasets without broader comparisons
- Does not address computational costs or scalability issues for larger datasets or higher-dimensional covariates

## Confidence

**High Confidence**: Theoretical framework connecting disagreement-based active learning with UCB-style stratified sampling

**Medium Confidence**: Near-exponential convergence rates under stated assumptions

**Medium Confidence**: Empirical results on synthetic and EHR data

## Next Checks

1. Test algorithm performance when the Bayes-optimal classifier's risk ν is unknown and must be estimated from data
2. Evaluate computational scalability on larger datasets with higher-dimensional covariates
3. Compare performance against alternative active learning approaches for mean estimation in the same experimental settings