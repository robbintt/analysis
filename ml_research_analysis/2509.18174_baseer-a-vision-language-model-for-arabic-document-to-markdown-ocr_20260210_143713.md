---
ver: rpa2
title: 'Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR'
arxiv_id: '2509.18174'
source_url: https://arxiv.org/abs/2509.18174
tags:
- arabic
- document
- baseer
- page
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Baseer, a vision-language model fine-tuned
  specifically for Arabic document OCR. The authors curate a large-scale dataset of
  500,000 synthetic and real-world Arabic document images paired with Markdown-formatted
  text, and introduce Misraj-DocOCR, a high-quality benchmark for Arabic OCR evaluation.
---

# Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR

## Quick Facts
- arXiv ID: 2509.18174
- Source URL: https://arxiv.org/abs/2509.18174
- Authors: Khalil Hennara; Muhammad Hreden; Mohamed Motasim Hamed; Ahmad Bastati; Zeina Aldallal; Sara Chrouf; Safwan AlModhayan
- Reference count: 18
- Key result: Achieved WER of 0.25 on Misraj-DocOCR benchmark, outperforming both open-source and commercial OCR systems

## Executive Summary
This paper introduces Baseer, a vision-language model fine-tuned specifically for Arabic document OCR. The authors curate a large-scale dataset of 500,000 synthetic and real-world Arabic document images paired with Markdown-formatted text, and introduce Misraj-DocOCR, a high-quality benchmark for Arabic OCR evaluation. Baseer is built by fine-tuning Qwen2.5-VL-3B-Instruct using a decoder-only strategy, which preserves the pre-trained vision encoder while adapting the language decoder. Extensive experiments show that Baseer significantly outperforms existing open-source and commercial OCR systems, achieving a WER of 0.25 on the Misraj-DocOCR benchmark and establishing a new state-of-the-art in Arabic document OCR.

## Method Summary
Baseer is developed by fine-tuning Qwen2.5-VL-3B-Instruct with a decoder-only approach, freezing the vision encoder while training only the language decoder. The model is trained on 500,000 Arabic document images (300k synthetic, 200k real-world) with corresponding Markdown text. Training uses next-token prediction with system prompts, weight decay of 0.01, learning rate of 1e-4 with cosine decay, batch size of 640, and maximum sequence length of 4096. The synthetic data is generated through a pipeline that converts Common Crawl markdown to HTML, then to Word, PDF, and finally to images with 29 augmentation transformations. The model is evaluated on Misraj-DocOCR (400 expert-verified images) and KITAB-Bench (30 samples).

## Key Results
- Baseer achieves WER of 0.25 on Misraj-DocOCR benchmark, outperforming commercial systems like Gemini-2.5-pro (WER 0.37)
- Decoder-only fine-tuning strategy yields ChrF score of 89.79, significantly better than full fine-tuning (84.79)
- Optimal sequence length identified as 4096 tokens, with 2048 underperforming (ChrF 82.69) and 8192 degrading due to padding dilution (ChrF 87.52)
- Model handles complex Arabic typography including cursive script, diacritics, and right-to-left orientation

## Why This Works (Mechanism)

### Mechanism 1: Freezing Vision Encoder for Script Adaptation
Freezing the pre-trained vision encoder while training only the language decoder optimizes domain adaptation for Arabic script. The base model already possesses generalized visual feature extraction capabilities, so freezing the vision encoder retains these robust features while the language decoder learns to map visual tokens to Arabic's complex morphological and directional rules. This avoids degrading fundamental visual acuity that would occur with full fine-tuning on limited Arabic data.

### Mechanism 2: Hybrid Synthetic-Real Data Curation
The hybrid data curation combining synthetic augmentation and verified real-world samples bridges the gap between controlled learning and domain reality. Synthetic data (300k samples) enforces coverage of layout variability and robustness to noise through programmatic augmentation, while real-world data (200k samples) introduces the long-tail publishing artifacts that synthetic pipelines miss. This mixture aligns the model's latent space with both structural regularity and the messy reality of Arabic documents.

### Mechanism 3: Optimal Sequence Length Selection
Optimizing sequence length to 4096 tokens maximizes OCR fidelity by balancing detail retention against attention dilution. Shorter sequences compress visual information and lose fine details necessary for small fonts, while longer sequences introduce excessive padding tokens that dilute the attention mechanism's focus. The 4096 window hits a sweet spot for single-page Arabic documents.

## Foundational Learning

- **Transfer Learning in MLLMs (Encoder vs. Decoder)**: Why needed - The core methodological choice is freezing the vision encoder. Understanding that visual features are often universal across languages while interpretation is specific is critical to replicating this work. Quick check - Why would fine-tuning the vision encoder on 500k Arabic documents potentially lower the model's ability to read English text compared to freezing it?

- **Arabic Script Morphology & Directionality**: Why needed - Arabic is cursive, right-to-left, and uses diacritics that change meaning. Standard models often hallucinate or reverse text. Understanding this helps diagnose why the base model failed and why specific decoder-only alignment was necessary. Quick check - If a model outputs Arabic text correctly but in Left-to-Right order, which component is likely misalignedâ€”the vision encoder or the language decoder?

- **Hallucination in Generative OCR**: Why needed - The paper explicitly critiques an existing benchmark for "hallucinatory text" (e.g., English sentences in Arabic docs). Evaluating OCR requires distinguishing between recognition errors and generative confabulation. Quick check - In the context of this paper, does "WER" fully capture the severity of a model inserting a sentence that doesn't exist in the image?

## Architecture Onboarding

- **Component map**: Image (Document) -> Vision Encoder (Qwen-ViT, FROZEN) -> Visual Tokens -> Projector (Adaptor) -> Language Decoder (Qwen2.5-3B, TRAINED) -> Output: Markdown/HTML

- **Critical path**:
  1. Data Hygiene: Ensure your rendering pipeline correctly handles Arabic fonts (RTL support). If synthetic data is generated LTR, the model will fail.
  2. Freezing Logic: Implementation must strictly block gradient flow to the ViT weights while keeping the LLM weights trainable.
  3. Evaluation Alignment: Use the post-processing steps (standardizing HTML tables, removing model tags) defined in Section 7.1; otherwise, metrics will penalize formatting differences rather than recognition errors.

- **Design tradeoffs**:
  - 3B vs. 7B/32B: Authors chose 3B for efficiency, trading off potential raw capacity for lower inference cost. They proved data quality > model size for this specific task.
  - Markdown Output: Chosen for structure, but complex tables are rendered as HTML. This requires the decoder to handle mixed syntax.

- **Failure signatures**:
  - Directionality Reversion: Outputting Arabic in LTR order (a failure mode of the base model mentioned in Section 5).
  - Diacritic Drop: Missing Tashkeel (diacritics) indicates insufficient fine-tuning on diacritized data.
  - Footer Blindness: Missing page numbers or small text (a failure in KITAB benchmark creation, potentially a model weakness if context is too short).

- **First 3 experiments**:
  1. Sanity Check (Directionality): Run inference on the base Qwen-2.5-VL-3B vs. Baseer on a paragraph of diacritized Arabic. Verify that Baseer maintains RTL flow and diacritics while the base model might struggle.
  2. Ablation Validation (Freezing): Train two small runs (e.g., 10k steps): one with a frozen encoder and one with a fully unfrozen model. Compare ChrF scores to verify the paper's claim that freezing > full fine-tuning on this data volume.
  3. Context Limit Stress Test: Feed a very dense document (e.g., a legal contract with small font) into Baseer with 4096 context. Then try 8192. Verify if 8192 actually degrades performance due to the "padding dilution" hypothesis described in the paper.

## Open Questions the Paper Calls Out

- **Optimal Data Ratio**: What is the optimal ratio of synthetic to real-world training data for Arabic document OCR, and how does varying this ratio affect generalization to unseen document layouts? The paper uses a fixed 60/40 split without ablation on this composition.

- **Script Generalization**: Can the decoder-only fine-tuning approach with frozen vision encoders generalize effectively to other morphologically rich, non-Latin scripts such as Persian, Urdu, or Hebrew? The paper suggests this could be a strong baseline for other complex scripts but has not tested cross-linguistic transfer.

- **VLM Pseudo-label Quality**: What is the impact of VLM-generated pseudo-labels for real-world document ground truth on final model performance and error propagation? The paper does not report the proportion of real-world documents that were manually verified or provide quality analysis of the pseudo-labels.

## Limitations

- **Data Generalization Gap**: The synthetic data pipeline, while sophisticated, may not capture all real-world document characteristics. Historical or handwritten Arabic documents, which present unique challenges for OCR systems, are not explicitly evaluated.

- **Benchmark Representativeness**: The Misraj-DocOCR benchmark, while high-quality and expert-verified, contains only 400 samples. This relatively small size limits statistical confidence, particularly for measuring rare error types or evaluating performance on specific document subtypes.

- **Sequence Length Optimization**: While the paper identifies 4096 tokens as optimal, this is based on analysis of their specific dataset. Documents with different layouts, font sizes, or multi-page spreads may require different context windows.

## Confidence

- **High Confidence**: The decoder-only fine-tuning approach and its superiority over full fine-tuning (89.79 vs 84.79 ChrF) is well-supported by controlled ablation experiments.
- **Medium Confidence**: The hybrid synthetic-real data curation strategy's effectiveness is supported by results, but the exact contribution of each component is not isolated.
- **Low Confidence**: Generalization to document types not represented in the training or evaluation sets (historical manuscripts, handwritten text, extremely low-quality scans) cannot be assessed from the current results.

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate Baseer on a dataset of historical Arabic manuscripts or handwritten documents to assess whether the model's strong performance on modern printed text extends to more challenging document types where traditional OCR systems typically fail.

2. **Ablation of Data Components**: Conduct experiments isolating the contribution of synthetic versus real-world data by training models on only synthetic data, only real data, and various mixing ratios to quantify the marginal value of each data source.

3. **Sequence Length Boundary Analysis**: Systematically test model performance across a wider range of sequence lengths (e.g., 2048, 3072, 4096, 5120, 6144, 8192) on both standard and dense document layouts to better understand the relationship between document complexity and optimal context window size.