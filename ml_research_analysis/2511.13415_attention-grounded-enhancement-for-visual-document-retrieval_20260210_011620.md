---
ver: rpa2
title: Attention Grounded Enhancement for Visual Document Retrieval
arxiv_id: '2511.13415'
source_url: https://arxiv.org/abs/2511.13415
tags:
- attention
- retrieval
- document
- arxiv
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGREE introduces a training framework that leverages attention
  maps from multimodal large language models as fine-grained supervision to enhance
  visual document retrieval. While existing methods use global relevance labels, they
  struggle with non-extractive queries due to lack of region-level matching clues.
---

# Attention Grounded Enhancement for Visual Document Retrieval

## Quick Facts
- arXiv ID: 2511.13415
- Source URL: https://arxiv.org/abs/2511.13415
- Reference count: 40
- AGREE improves nDCG@1 from 54.81% to 61.84% (+7.03%) on ViDoRe V2 benchmark

## Executive Summary
AGREE introduces a novel training framework that leverages attention maps from multimodal large language models (MLLMs) as fine-grained supervision to enhance visual document retrieval. Unlike existing methods that rely solely on global relevance labels, AGREE addresses the challenge of non-extractive queries by providing region-level matching clues. The framework extracts query-conditioned attention from an MLLM, downsamples it to align with the retriever's patch layout, and trains with both global contrastive loss and local alignment loss between patch similarity scores and the attention. This dual supervision enables the retriever to learn not just document relevance but also which specific regions support the match, moving beyond surface-level keyword matching to rationale-aware retrieval.

## Method Summary
The AGREE framework operates by first generating query-conditioned attention maps from an MLLM for each query-document pair. These attention maps are then downsampled to match the patch layout of the visual document retriever. The training process employs a dual supervision strategy: a global contrastive loss that ensures overall document relevance and a local alignment loss that aligns patch similarity scores with the attention map values. This approach teaches the retriever to identify which document regions are most relevant to the query, particularly for non-extractive queries where the relevant content may not be explicitly stated. The framework is evaluated on the ViDoRe V2 benchmark, demonstrating significant improvements in retrieval performance through better capture of both explicit and implicit matches.

## Key Results
- AGREE improves nDCG@1 from 54.81% to 61.84% (+7.03%) on ViDoRe V2
- AGREE improves nDCG@5 from 58.59% to 61.54% (+2.95%) over ColQwen2.5 baseline
- The method demonstrates enhanced capability for rationale-aware retrieval beyond keyword matching

## Why This Works (Mechanism)
AGREE works by addressing a fundamental limitation in visual document retrieval: the lack of region-level supervision for non-extractive queries. Traditional methods using only global relevance labels cannot teach retrievers which specific document regions support relevance judgments. By extracting query-conditioned attention maps from MLLMs, AGREE provides this missing supervision. The attention maps serve as proxy ground truth for which document regions should be attended to when answering a query. The dual loss structure ensures the retriever learns both overall document relevance (global contrastive loss) and region-level matching (local alignment loss). This combination enables the model to capture implicit matches and rationales that would be missed by surface-level keyword matching alone.

## Foundational Learning
- **Visual Document Retrieval**: Why needed - forms the core task of finding relevant documents from visual data; Quick check - verify understanding of how documents are represented as image patches
- **Multimodal Large Language Models (MLLMs)**: Why needed - source of attention supervision; Quick check - confirm understanding of how MLLMs process visual and textual inputs simultaneously
- **Attention Mechanisms**: Why needed - provides fine-grained supervision signals; Quick check - understand how attention weights indicate region importance
- **Contrastive Learning**: Why needed - enables learning from global relevance labels; Quick check - grasp the principle of pulling relevant pairs together while pushing irrelevant pairs apart
- **Patch-based Document Representation**: Why needed - standard approach for processing visual documents; Quick check - understand how documents are divided into fixed-size patches for processing
- **Dual Supervision Framework**: Why needed - combines global and local objectives for comprehensive learning; Quick check - verify understanding of how two different loss functions complement each other

## Architecture Onboarding

Component map: Query -> MLLM (attention extraction) -> Attention Downsampling -> Retriever (patch layout) -> Dual Loss (Global Contrastive + Local Alignment)

Critical path: The critical path involves generating query-conditioned attention from the MLLM, downsampling to match the retriever's patch layout, computing patch similarity scores, and applying the dual loss functions. The MLLM attention extraction and downsampling are bottlenecks that must be carefully managed for computational efficiency.

Design tradeoffs: The main tradeoff is between supervision quality and computational cost. Higher-resolution attention maps provide more precise supervision but increase computational overhead. The downsampling strategy must balance preserving attention detail while matching the retriever's patch layout. Using MLLM attention introduces dependency on MLLM performance but provides crucial region-level supervision absent in traditional approaches.

Failure signatures: The system may fail when MLLM attention maps are inaccurate (misidentifying relevant regions), when downsampling loses critical spatial information, or when the local alignment loss conflicts with the global contrastive objective. Performance degradation may be observed on queries requiring deep document understanding where MLLM attention is unreliable.

Exactly 3 first experiments:
1. Verify that attention maps from the MLLM align with human judgments of relevant document regions for a sample of non-extractive queries
2. Test the sensitivity of performance to different downsampling strategies from MLLM attention resolution to retriever patch layout
3. Isolate the contribution of local alignment loss versus global contrastive loss through ablation studies

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that MLLM attention maps accurately reflect human-relevant regions is not empirically validated against ground-truth region annotations
- The sensitivity of results to the downsampling strategy from MLLM attention to retriever patch layout is not explored
- The computational overhead of generating attention maps for every query-document pair during training is not discussed

## Confidence
- Performance improvements on ViDoRe V2: High
- Attention maps as effective supervision: Medium
- Generalization to other benchmarks: Low
- Computational efficiency: Low

## Next Checks
1. Conduct human evaluation studies to verify whether the attention maps extracted from the MLLM actually align with human judgments of which document regions are most relevant for non-extractive queries.

2. Perform ablation studies isolating the contribution of the attention-based local alignment loss versus the global contrastive loss to quantify which component drives the performance gains.

3. Test the method's robustness by systematically corrupting the attention maps (e.g., masking random regions or swapping attention weights) to assess sensitivity to MLLM failures and establish failure modes.