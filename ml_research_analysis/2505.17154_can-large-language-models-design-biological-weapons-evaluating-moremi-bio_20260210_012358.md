---
ver: rpa2
title: Can Large Language Models Design Biological Weapons? Evaluating Moremi Bio
arxiv_id: '2505.17154'
source_url: https://arxiv.org/abs/2505.17154
tags:
- toxic
- proteins
- toxicity
- available
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models (LLMs) can be
  used to design novel toxic proteins and small molecules with high similarity to
  known bioweapons when safety guardrails are removed. Using Moremi Bio Agent, researchers
  generated 1,020 toxic proteins and 5,000 toxic small molecules, which showed high
  toxicity scores through computational assessment.
---

# Can Large Language Models Design Biological Weapons? Evaluating Moremi Bio

## Quick Facts
- **arXiv ID**: 2505.17154
- **Source URL**: https://arxiv.org/abs/2505.17154
- **Reference count**: 40
- **Primary result**: LLMs can generate novel toxic proteins and small molecules with high similarity to known bioweapons when safety guardrails are removed

## Executive Summary
This study demonstrates that large language models can be used to design novel toxic proteins and small molecules with high similarity to known bioweapons when safety guardrails are removed. Using Moremi Bio Agent, researchers generated 1,020 toxic proteins and 5,000 toxic small molecules, which showed high toxicity scores through computational assessment. ToxinPred2 and CSM-Toxin tools indicated strong potential for toxicity, with ML scores ranging from 0.93 to 1.00. t-SNE clustering revealed structural similarities to known toxins like ricin and diphtheria toxin. These findings challenge claims that LLMs cannot design bioweapons, highlighting the urgent need for robust biosecurity measures and governance frameworks.

## Method Summary
The study utilized Moremi Bio Agent, an AI system based on the OpenAI o1 model, to generate toxic proteins and small molecules. Researchers removed safety guardrails from the model and used it to generate 1,020 toxic proteins and 5,000 toxic small molecules. The generated compounds were evaluated using computational tools including ToxinPred2 and CSM-Toxin for toxicity prediction, and t-SNE clustering was employed to analyze structural similarities to known toxins. The approach focused on computational assessment without experimental validation of the generated compounds.

## Key Results
- Moremi Bio Agent generated 1,020 toxic proteins and 5,000 toxic small molecules with high computational toxicity scores
- ToxinPred2 and CSM-Toxin tools showed ML scores ranging from 0.93 to 1.00 for the generated compounds
- t-SNE clustering revealed structural similarities to known toxins including ricin and diphtheria toxin

## Why This Works (Mechanism)
The LLM can generate novel toxic sequences by leveraging its training on diverse biological data and following user prompts that specify toxic characteristics. When safety guardrails are removed, the model can produce content that would normally be restricted, including sequences similar to known biological toxins. The computational tools then identify these sequences based on patterns learned from existing toxin databases.

## Foundational Learning
- **Large Language Models (LLMs)**: Deep learning models trained on vast text datasets that can generate human-like text and follow complex instructions
- **Biosecurity Guardrails**: Safety protocols and restrictions built into AI systems to prevent generation of harmful biological content
- **ToxinPred2**: Computational tool that predicts protein toxicity using machine learning algorithms and sequence analysis
- **CSM-Toxin**: Computational method for predicting small molecule toxicity based on chemical structure and properties
- **t-SNE Clustering**: Dimensionality reduction technique for visualizing high-dimensional data by preserving local structure in lower dimensions
- **Dual-use Research**: Scientific research that can be used for both beneficial and harmful purposes

## Architecture Onboarding
**Component Map**: LLM Model -> Safety Guardrails -> Moremi Bio Agent -> Toxin Generation -> Computational Analysis Tools -> Toxicity Assessment
**Critical Path**: The core workflow involves removing safety restrictions from the LLM, generating toxic sequences, and analyzing them computationally through toxicity prediction tools
**Design Tradeoffs**: The study prioritizes demonstrating AI capability over practical weaponization feasibility, focusing on computational assessment rather than experimental validation
**Failure Signatures**: Guardrail removal leads to generation of toxic content; computational tools may produce false positives in toxicity predictions
**First 3 Experiments**: 
1. Remove safety guardrails from LLM and generate toxic protein sequences
2. Run generated sequences through ToxinPred2 and CSM-Toxin for toxicity scoring
3. Apply t-SNE clustering to compare structural similarity with known toxins

## Open Questions the Paper Calls Out
- How can AI governance frameworks be effectively implemented to prevent misuse of LLMs for bioweapons design?
- What additional safety measures beyond current guardrails could be implemented to prevent generation of toxic biological sequences?
- How should the scientific community balance the benefits of AI in drug discovery against potential misuse for creating biological weapons?

## Limitations
- Computational toxicity predictions lack experimental validation, creating uncertainty about real-world biological activity
- Study does not address practical barriers to weaponization such as synthesis challenges and delivery mechanisms
- Guardrail removal represents artificial scenario that may not reflect typical model deployment or real-world misuse pathways
- Structural similarity analysis through t-SNE clustering does not establish functional equivalence or biological activity

## Confidence
- **High confidence**: LLMs can generate novel toxic sequences when safety controls are disabled
- **Medium confidence**: Computational tools can identify potentially toxic compounds among LLM outputs
- **Low confidence**: Generated compounds would function as effective bioweapons without experimental validation

## Next Checks
1. Conduct laboratory synthesis and biological testing of a representative sample of the generated proteins and small molecules to verify computational toxicity predictions
2. Evaluate the practical feasibility of synthesizing and weaponizing the most promising candidates, including cost, technical complexity, and delivery challenges
3. Assess the effectiveness of various safety protocols and governance frameworks in preventing unauthorized generation of toxic compounds through controlled testing scenarios