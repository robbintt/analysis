---
ver: rpa2
title: Identifying Non-Replicable Social Science Studies with Language Models
arxiv_id: '2503.10671'
source_url: https://arxiv.org/abs/2503.10671
tags:
- llama
- qwen
- mistral
- effect
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  identify non-replicable findings in behavioral social science studies. Using 14
  studies from the Many Labs 2 replication project, the authors compare the replicability
  predictions from LLMs against human replication results.
---

# Identifying Non-Replicable Social Science Studies with Language Models

## Quick Facts
- arXiv ID: 2503.10671
- Source URL: https://arxiv.org/abs/2503.10671
- Reference count: 40
- Primary result: LLMs achieved F1 scores up to 77% in identifying non-replicable social science studies

## Executive Summary
This study investigates whether large language models can identify non-replicable findings in behavioral social science studies by comparing LLM predictions against human replication results. The authors used 14 studies from the Many Labs 2 replication project and generated synthetic samples using both open-source (Llama 3 8B, Qwen 2 7B, Mistral 7B) and proprietary (GPT-4o) models at different temperature settings. The research demonstrates that language models can effectively identify non-replicable studies, with Mistral 7B achieving the highest F1 score of 77%, while also revealing that low temperature settings may introduce bias in effect size estimations due to artificially reduced variance.

## Method Summary
The study employed a synthetic data generation approach using four language models (Mistral 7B, Llama 3 8B, Qwen 2 7B, and GPT-4o) to analyze 14 social science studies from the Many Labs 2 replication project. The models were prompted to predict study replicability using various temperature settings to control output variance. The synthetic samples were then evaluated against actual replication outcomes to measure prediction accuracy using F1 scores. This approach allowed the researchers to assess how different models and temperature configurations affect the ability to identify non-replicable findings without requiring access to the original study materials.

## Key Results
- Mistral 7B achieved the highest F1 score of 77% in identifying non-replicable studies
- GPT-4o and Llama 3 8B both achieved F1 scores of 67%
- Qwen 2 7B achieved the lowest F1 score of 55%
- Open-source models performed comparably to the proprietary GPT-4o model
- Low temperature settings were found to potentially bias effect size estimations through artificially reduced variance

## Why This Works (Mechanism)
None

## Foundational Learning
- Many Labs 2 replication project - A large-scale coordinated replication effort in psychology that provides standardized replication data across multiple labs; needed for ground truth replication outcomes to validate model predictions
- Temperature settings in LLMs - A parameter controlling randomness in model outputs that affects response diversity; needed to understand how output variance impacts prediction consistency
- F1 score - A metric combining precision and recall that balances false positives and false negatives; needed to evaluate model performance in identifying non-replicable studies
- Synthetic data generation - The process of creating artificial study descriptions for model analysis; needed when original study materials are not accessible
- Effect size estimation - Measuring the magnitude of experimental effects; needed to assess whether models can accurately judge study significance

## Architecture Onboarding

Component map:
LLM models (Mistral 7B, Llama 3 8B, Qwen 2 7B, GPT-4o) -> Temperature control settings -> Synthetic sample generation -> Replicability prediction -> F1 score evaluation

Critical path:
Temperature setting selection → Model prompting → Synthetic sample generation → Replicability classification → Performance evaluation

Design tradeoffs:
The study balances between model accuracy and computational efficiency by using relatively small 7-8B parameter models rather than larger models. The synthetic data approach trades direct access to original materials for scalability and privacy preservation.

Failure signatures:
Low F1 scores across all models would indicate fundamental limitations in LLM ability to assess replicability. Consistently poor performance with specific temperature settings would suggest sensitivity to output variance control. Strong bias in effect size estimation would manifest as systematic over/underestimation of study significance.

First experiments:
1. Test model predictions on a held-out subset of the Many Labs 2 data to validate generalization
2. Compare synthetic sample predictions against human expert assessments of the same studies
3. Evaluate model performance across different social science domains beyond the current sample

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of only 14 studies from Many Labs 2 limits generalizability of findings
- Reliance on synthetic data generation rather than original study materials may miss important contextual information
- Temperature setting effects on bias require further validation and may not generalize across different model architectures

## Confidence

High confidence:
- Comparative performance of different LLMs with Mistral 7B clearly outperforming others
- Finding that open-source models can match proprietary models in this specific task

Medium confidence:
- General feasibility of using LLMs for replication prediction given the constrained sample size
- Relationship between temperature settings and effect size estimation bias

## Next Checks

1. Test model performance on a larger, more diverse set of social science studies beyond the Many Labs 2 project to assess generalizability

2. Conduct a direct comparison between synthetic sample analysis and analysis of original study materials to evaluate the impact of data generation on prediction accuracy

3. Perform a systematic investigation of temperature effects across different model architectures and prompt structures to better understand and mitigate potential biases in effect size estimation