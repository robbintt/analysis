---
ver: rpa2
title: Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion
  in Multilingual ASR
arxiv_id: '2506.21577'
source_url: https://arxiv.org/abs/2506.21577
tags:
- language
- prompt
- soft
- entire
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Entire Soft Prompt Tuning (Entire SPT) and
  Language-Aware Prompt Tuning (LAPT) to address language interference and seamless
  language expansion in multilingual ASR. Entire SPT applies soft prompts to both
  encoder and decoder for improved feature extraction and decoding, while LAPT leverages
  cross-lingual similarities to encode shared and language-specific features using
  lightweight prompt matrices.
---

# Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR

## Quick Facts
- arXiv ID: 2506.21577
- Source URL: https://arxiv.org/abs/2506.21577
- Reference count: 0
- This paper introduces Entire Soft Prompt Tuning (Entire SPT) and Language-Aware Prompt Tuning (LAPT) to address language interference and seamless language expansion in multilingual ASR.

## Executive Summary
This paper addresses the challenge of seamless language expansion in multilingual Automatic Speech Recognition (ASR) by introducing Entire Soft Prompt Tuning (Entire SPT) and Language-Aware Prompt Tuning (LAPT). The proposed methods aim to enable efficient addition of new languages to pre-trained ASR models while minimizing interference with existing languages and avoiding catastrophic forgetting. Entire SPT applies soft prompts to both encoder and decoder, while LAPT leverages cross-lingual similarities to encode shared and language-specific features using lightweight prompt matrices. The SPT-Whisper toolkit integrates these methods for parameter-efficient continual learning, demonstrating significant improvements over baseline approaches.

## Method Summary
The paper proposes two parameter-efficient methods for seamless language expansion in multilingual ASR: Entire Soft Prompt Tuning (Entire SPT) and Language-Aware Prompt Tuning (LAPT). Entire SPT applies learnable soft prompts to both the encoder and decoder components of a pre-trained Whisper model, improving feature extraction and decoding without modifying the underlying model parameters. LAPT builds upon this by first detecting the most similar base language for a new target language using Whisper's language identification capabilities, then training language-specific prompt matrices that leverage this cross-lingual similarity. Both methods operate by freezing the original model parameters and only training the lightweight prompt matrices, making them highly parameter-efficient. The SPT-Whisper toolkit implements these methods for continual learning scenarios where new languages need to be added to existing multilingual ASR systems.

## Key Results
- Entire SPT outperforms Decoder SPT by 5.0% in language expansion tasks on FLEURS benchmark
- LAPT achieves 16.0% improvement over Decoder SPT for unseen language adaptation
- Experiments conducted on three unseen languages (Asturian, Sorani Kurdish, Kabuverdianu) with ~7-10 hours of training data per language
- Character Error Rate (CER) used as primary evaluation metric for language expansion performance

## Why This Works (Mechanism)
The effectiveness stems from two key mechanisms: (1) Entire SPT improves feature extraction and decoding by applying soft prompts to both encoder and decoder, allowing the model to adapt to new languages without modifying core parameters, and (2) LAPT leverages cross-lingual similarities by identifying the most similar base language for each new language and using this information to initialize and train language-specific prompts, enabling more efficient adaptation while preserving knowledge of previously learned languages.

## Foundational Learning
- Multilingual ASR systems: Pre-trained models that handle multiple languages; needed for building language-inclusive speech recognition systems
- Soft prompt tuning: Adding learnable prompt tokens instead of modifying model weights; enables efficient adaptation without catastrophic forgetting
- Cross-lingual similarity detection: Using language identification to find related languages; helps leverage existing language knowledge for new languages
- Parameter-efficient learning: Training only small subsets of parameters; reduces computational overhead while maintaining performance
- Catastrophic forgetting: When learning new tasks causes forgetting of previous tasks; avoided by freezing original model parameters

## Architecture Onboarding

**Component Map:**
Whisper model -> Soft prompts (encoder/decoder) -> Language-specific prompt matrices -> Output predictions

**Critical Path:**
Audio input → Whisper encoder with soft prompts → Context representation → Whisper decoder with soft prompts → Text output

**Design Tradeoffs:**
- Entire SPT vs. Decoder SPT: Entire SPT applies prompts to both encoder and decoder, providing better feature extraction but requiring more parameters than Decoder SPT which only modifies the decoder
- LAPT language detection: Requires running Whisper LID on M audio segments to identify similar languages, adding preprocessing overhead but enabling more effective adaptation
- Prompt length: 128 tokens chosen to balance performance and decoder context constraints (256-token limit)

**Failure Signatures:**
- Poor language discrimination: Shared prompts may cause confusion between similar languages; solution is to use Separate SPT with per-language prompt activation
- Decoder context overflow: Using 256-token prompts causes overflow; solution is to limit to 128 tokens for both encoder and decoder
- Ineffective adaptation: Poor language similarity detection leads to suboptimal prompt initialization; solution is to increase M or use alternative similarity metrics

**3 First Experiments:**
1. Implement Entire SPT with 128-token prompts at both encoder input and decoder `<|Prev|>` position, freezing Whisper parameters
2. Test LAPT with varying values of M (10, 20, 30 audio segments) to assess sensitivity of language similarity detection
3. Compare Separate SPT vs. Shared SPT for languages with similar scripts to verify per-language prompt activation effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends on availability of similar base languages for LAPT to leverage
- Limited evaluation to three unseen languages from FLEURS benchmark
- Computational overhead of running Whisper LID for language similarity detection in LAPT

## Confidence

**High confidence:**
- Entire SPT architecture (128-token prompts at encoder input and decoder `<|Prev|>`), baseline Decoder SPT configuration, overall experimental setup on FLEURS with CER evaluation

**Medium confidence:**
- LAPT methodology for language similarity detection and prompt initialization, given the partial specification of M and similarity computation

**Low confidence:**
- Exact prompt encoder implementation details and integration strategy for similar-language embeddings in LAPT

## Next Checks
1. Verify that the 128-token prompt length for both encoder and decoder does not cause decoder context overflow; confirm greedy decoding performance matches Table 2.
2. Test LAPT with varying values of M (e.g., 10, 20, 30 audio segments) to assess sensitivity of language similarity detection and downstream CER.
3. Implement and compare both prompt initialization strategies for LAPT—(a) using similar-language embedding as initial prompt values, and (b) using it as a regularization term—to determine which yields better language expansion performance.