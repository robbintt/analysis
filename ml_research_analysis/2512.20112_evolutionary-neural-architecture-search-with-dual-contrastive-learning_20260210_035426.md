---
ver: rpa2
title: Evolutionary Neural Architecture Search with Dual Contrastive Learning
arxiv_id: '2512.20112'
source_url: https://arxiv.org/abs/2512.20112
tags:
- architecture
- neural
- search
- predictor
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DCL-ENAS introduces a dual-stage contrastive learning approach
  to enhance predictor accuracy in evolutionary neural architecture search (ENAS)
  while reducing computational cost. The method employs self-supervised pretraining
  to learn meaningful architecture representations from unlabeled data, followed by
  contrastive fine-tuning to rank architectures based on relative performance.
---

# Evolutionary Neural Architecture Search with Dual Contrastive Learning

## Quick Facts
- **arXiv ID**: 2512.20112
- **Source URL**: https://arxiv.org/abs/2512.20112
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art validation accuracy across NASBench-101 and NASBench-201 benchmarks with 0.05%-0.39% improvement over baselines

## Executive Summary
DCL-ENAS introduces a dual-stage contrastive learning approach to enhance predictor accuracy in evolutionary neural architecture search (ENAS) while reducing computational cost. The method employs self-supervised pretraining to learn meaningful architecture representations from unlabeled data, followed by contrastive fine-tuning to rank architectures based on relative performance. This approach achieves state-of-the-art validation accuracy across NASBench-101 and NASBench-201 benchmarks, surpassing strongest baselines by 0.05% to 0.39%. On a real-world ECG arrhythmia classification task, DCL-ENAS improves performance by approximately 2.5 percentage points over manual designs while requiring only 7.7 GPU-days. The predictor model achieves superior Kendall's τ correlation (above 0.6) compared to existing methods, demonstrating its effectiveness in guiding the evolutionary search with limited compute budgets.

## Method Summary
The DCL-ENAS framework combines evolutionary search with a dual-stage contrastive learning predictor. The first stage uses self-supervised pretraining where the model learns architecture representations by predicting whether two augmented views of the same architecture originate from the same architecture. The second stage applies contrastive fine-tuning where the model learns to distinguish between architectures with similar and dissimilar performance by maximizing similarity for architectures with close performance and minimizing similarity for those with distant performance. This dual approach enables the predictor to learn both intrinsic architectural patterns and relative performance relationships, leading to more accurate predictions for guiding the evolutionary search process.

## Key Results
- Achieves state-of-the-art validation accuracy across NASBench-101 and NASBench-201 benchmarks, surpassing strongest baselines by 0.05% to 0.39%
- Improves ECG arrhythmia classification performance by approximately 2.5 percentage points over manual designs while requiring only 7.7 GPU-days
- Predictor model achieves superior Kendall's τ correlation (above 0.6) compared to existing methods

## Why This Works (Mechanism)
The dual-stage contrastive learning approach works by first establishing a robust foundation of architectural understanding through self-supervised pretraining, then refining this understanding by learning the relative performance landscape. Self-supervised pretraining allows the model to capture intrinsic architectural patterns without requiring labeled performance data, making it particularly effective when training data is limited. The subsequent contrastive fine-tuning stage then leverages the learned representations to focus specifically on performance ranking, enabling more accurate predictions for guiding the evolutionary search. This two-stage process addresses the key challenge in NAS predictor accuracy: the need to understand both architectural semantics and performance relationships.

## Foundational Learning
- **Neural Architecture Search (NAS)**: Automated method for designing neural network architectures; needed to understand the problem context and why predictor accuracy matters
- **Evolutionary Algorithms**: Population-based optimization methods that evolve solutions over generations; required for understanding the search mechanism being guided
- **Contrastive Learning**: Self-supervised learning technique that learns representations by comparing similar and dissimilar examples; essential for understanding the dual-stage approach
- **Kendall's τ correlation**: Statistical measure of ordinal association used to evaluate predictor ranking quality; important for assessing predictor effectiveness
- **Tabular Benchmarks**: Pre-computed datasets containing performance metrics for all architectures in a search space; crucial for understanding the evaluation methodology
- **Self-supervised Learning**: Machine learning paradigm where the data itself provides supervision; fundamental to the pretraining stage

## Architecture Onboarding

**Component Map**: Data augmentation -> Self-supervised pretraining -> Contrastive fine-tuning -> Evolutionary search -> Performance evaluation

**Critical Path**: The contrastive fine-tuning stage is most critical as it directly impacts the predictor's ability to rank architectures accurately, which in turn determines the quality of architectures found by the evolutionary search.

**Design Tradeoffs**: The dual-stage approach trades increased training complexity for improved predictor accuracy. While simpler single-stage predictors exist, the contrastive learning stages provide more robust representations that generalize better across different search spaces and performance distributions.

**Failure Signatures**: Poor predictor performance typically manifests as high Kendall's τ values below 0.5, indicating weak correlation between predicted and actual rankings. This can result from insufficient pretraining, inadequate fine-tuning data, or architectural mismatches between the predictor and the search space.

**First Experiments**:
1. Evaluate predictor accuracy on held-out architectures from NASBench-101 to establish baseline performance
2. Compare single-stage vs dual-stage contrastive learning approaches to quantify the contribution of each stage
3. Test the impact of different data augmentation strategies during pretraining on final predictor accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation constrained to tabular benchmarks (NASBench-101, NASBench-201) representing only a small fraction of the search space compared to modern differentiable or large-scale NAS frameworks
- Single domain-specific real-world application (ECG arrhythmia classification) without broader cross-domain validation
- "State-of-the-art" claims relative to other ENAS methods rather than the full spectrum of NAS approaches
- Small statistical improvements (0.05-0.39%) may not be practically significant in many applications
- Lack of comparison to non-ENAS NAS methods makes relative efficiency difficult to assess

## Confidence
- **High confidence**: The dual-stage contrastive learning framework is technically sound and the methodology is clearly described
- **Medium confidence**: Benchmark results are reproducible but may not generalize beyond tabular datasets
- **Low confidence**: Real-world impact claims lack sufficient cross-domain validation

## Next Checks
1. Evaluate DCL-ENAS on a larger-scale search space (e.g., DARTS-style search spaces) to assess scalability beyond tabular benchmarks
2. Conduct ablation studies removing either the self-supervised pretraining or contrastive fine-tuning stages to quantify their individual contributions
3. Test the approach across multiple real-world datasets from different domains (computer vision, NLP, and healthcare) to verify cross-domain robustness