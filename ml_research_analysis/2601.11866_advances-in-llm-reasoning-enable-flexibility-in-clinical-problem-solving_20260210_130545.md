---
ver: rpa2
title: Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving
arxiv_id: '2601.11866'
source_url: https://arxiv.org/abs/2601.11866
tags:
- reasoning
- arxiv
- medical
- clinical
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving

## Quick Facts
- **arXiv ID**: 2601.11866
- **Source URL**: https://arxiv.org/abs/2601.11866
- **Reference count**: 40
- **Primary result**: Strong reasoning models (o1, Claude 3.5 Sonnet) achieved human-level performance (66% accuracy) on mARC benchmark, avoiding Einstellung traps while weaker models did not

## Executive Summary
This paper introduces mARC, an adversarial medical reasoning benchmark designed to test whether LLMs can resist the Einstellung effect (cognitive fixation on familiar patterns) when contradictory clinical evidence exists. The authors find that advanced reasoning models significantly outperform both non-reasoning models and humans on this task, demonstrating the ability to override default heuristics when logical constraints invalidate them. The benchmark includes "seek more data" options in 53% of questions to test whether models can recognize information insufficiency rather than forcing pattern completion.

## Method Summary
The authors evaluate 10 models (including GPT-4o, o1, Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek v3) on a 100-question adversarial USMLE-style medical QA benchmark (mARC). Models complete 15 stochastic runs per question with chain-of-thought prompting, varying patient age by ±10 days to induce stochasticity. Performance is measured via mean accuracy, agreement-based and entropy-based sample-consistency for uncertainty estimation, and Brier scores. Human performance is established using 5 physicians with a 2-hour limit. Models are tested via API calls with temperature=0 when possible, and reasoning models use maximum reasoning effort settings.

## Key Results
- Strong reasoning models achieved 66% accuracy, matching the human baseline on mARC
- Reasoning models outperformed non-reasoning models by 8-14 percentage points
- Over 90% of "human-miss" items were correctly answered by top reasoning models
- Models showed improved uncertainty calibration with better Brier scores and usable confidence signals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Strong reasoning models resist the Einstellung effect by applying multi-step reasoning chains that override default heuristics when contradictory evidence exists
- **Mechanism**: Reasoning models generate explicit intermediate reasoning steps, allowing them to identify when a "blocker" (contradictory clinical fact) invalidates a default heuristic
- **Core assumption**: The chain-of-thought process allows models to maintain and evaluate logical constraints rather than immediately selecting the highest-probability completion
- **Evidence anchors**: Strong reasoning models avoided Einstellung-based traps more often than weaker reasoning models, achieving human-level performance on mARC

### Mechanism 2
- **Claim**: Performance gains correlate with explicit recognition of information insufficiency, enabling correct deferral rather than forced heuristic completion
- **Mechanism**: Reasoning models evaluate whether available information suffices for confident decision-making and select defer-options when insufficient
- **Core assumption**: Training on reasoning tasks teaches models to represent and evaluate information completeness
- **Evidence anchors**: 53% of mARC questions include "seek more data" options specifically testing this adaptive deferral capability

### Mechanism 3
- **Claim**: Improved uncertainty calibration in advanced reasoning models enables more reliable confidence signals for deferral decisions
- **Mechanism**: Agreement-based sample consistency across multiple stochastic runs provides usable confidence estimates
- **Core assumption**: Consistency across reasoning traces correlates with answer reliability
- **Evidence anchors**: Newer/larger reasoning models showed improvements in accuracy, Brier scores and supplied usable confidence signals

## Foundational Learning

- **Concept: Einstellung Effect (Cognitive Set)**
  - **Why needed here**: The entire mARC benchmark operationalizes this cognitive bias—understanding that familiar cues can actively inhibit better solutions is essential for interpreting why strong reasoning matters
  - **Quick check question**: A patient presents with chest pain and a history of GERD. The resident immediately prescribes antacids without considering MI. What cognitive bias does this represent?

- **Concept: Deductive vs. Probabilistic Reasoning**
  - **Why needed here**: The paper's formalization (K ∪ {B} ⊢ ¬H) requires distinguishing between statistical associations (default heuristics) and logical constraints (blockers that definitively rule out hypotheses)
  - **Quick check question**: If anticoagulation statistically increases bleed risk, but the patient has no brain (anencephaly), what type of reasoning determines whether "obtain CT head" is valid?

- **Concept: Sample Consistency for Uncertainty Estimation**
  - **Why needed here**: The paper uses agreement across stochastic runs as a proxy for model confidence—understanding this method is necessary to interpret calibration results and deferral thresholds
  - **Quick check question**: If a model answers the same question correctly in 13/15 runs but incorrectly in 2/15, what does this tell you about its confidence and potential deferral behavior?

## Architecture Onboarding

- **Component map**: Clinical vignette → (cue C, blocker B, symptom S) → Multi-step chain-of-thought generation → Deductive constraint evaluation → Option selection → 15-run stochastic sampling → Agreement-based SC calculation

- **Critical path**: 1) Identify default heuristic triggered by cue C, 2) Detect blocker B and retrieve relevant knowledge from K, 3) Evaluate whether K ∪ {B} ⊢ ¬H, 4) If constraints override heuristic → select non-heuristic option, 5) If information insufficient → select deferral option, 6) Aggregate confidence across stochastic runs for calibration

- **Design tradeoffs**: Temperature=0 maximizes determinism but eliminates SC-based uncertainty estimation; compact benchmark (100 items) enables careful adversarial design but limits statistical power; reasoning effort maximization improves performance but increases latency and cost

- **Failure signatures**: Heuristic override failure (selecting "Obtain CT head" despite anencephaly blocker), commonsense failure (o1's inability to recognize blood pressure cannot be checked on forehead), overconfidence in errors (smaller models show high confidence on incorrect answers)

- **First 3 experiments**:
  1. Replicate Figure 2 baseline: Run non-reasoning vs. reasoning models on mARC with temperature=0, compare accuracy against reported human baseline
  2. Ablate blocker presence: Create modified vignettes without blockers to confirm Einstellung traps induce errors in weaker models
  3. Validate SC-based deferral: Set confidence threshold, measure whether deferred questions correlate with physician-miss items

## Open Questions the Paper Calls Out

- **Open Question 1**: Does performance on the mARC benchmark constitute a formal measure of the Einstellung effect, or does it capture a different aspect of adversarial robustness? The authors acknowledge they only provided qualitative content validity and formal construct validation will be pursued in the future.

- **Open Question 2**: Can the superior performance of reasoning models on the 100-item mARC benchmark generalize to broader, programmatically generated out-of-distribution (OOD) medical scenarios? The authors note the benchmark is "intentionally compact" and future releases will expand coverage with OOD generation.

- **Open Question 3**: How can the residual overconfidence and errors observed in reasoning models be effectively mitigated to ensure safe "clinician-in-the-loop" deployment? While accuracy improved, residual errors and areas of overconfidence persist, particularly in smaller models.

## Limitations

- The mARC benchmark contains only 100 questions, limiting statistical power and generalizability to diverse clinical scenarios
- Performance improvements are demonstrated on an adversarial benchmark that may not fully capture naturalistic clinical reasoning failures
- Temperature sensitivity and the transition between deterministic and stochastic reasoning behavior is not characterized

## Confidence

**High Confidence Claims:**
- Reasoning models outperform non-reasoning models on mARC accuracy
- Strong reasoning models show reduced susceptibility to Einstellung traps
- Larger reasoning models demonstrate better uncertainty calibration

**Medium Confidence Claims:**
- Reasoning models achieve human-level performance on mARC
- Performance improvements correlate with explicit information insufficiency recognition
- Deferral options are correctly selected by reasoning models

**Low Confidence Claims:**
- Reasoning models' confidence signals are "usable" for clinical decision-making
- The specific formalization (K ∪ {B} ⊢ ¬H) accurately captures all Einstellung override mechanisms
- mARC captures clinically relevant cognitive inflexibility beyond the specific adversarial patterns tested

## Next Checks

1. **Generalizability test**: Apply reasoning models to non-adversarial clinical datasets (e.g., MedQA, USMLE practice materials) to determine if Einstellung resistance transfers to naturalistic clinical reasoning tasks

2. **Calibration validation**: Conduct controlled experiments varying information completeness systematically to verify that reasoning models appropriately adjust confidence and deferral decisions based on actual information sufficiency

3. **Commonsense robustness**: Design targeted tests for fundamental commonsense reasoning failures to characterize the boundary between reasoning ability and remaining commonsense gaps in current models