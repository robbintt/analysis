---
ver: rpa2
title: 'Afri-MCQA: Multimodal Cultural Question Answering for African Languages'
arxiv_id: '2601.05699'
source_url: https://arxiv.org/abs/2601.05699
tags:
- languages
- language
- question
- cultural
- native
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Afri-MCQA, the first large-scale multilingual
  and multimodal benchmark for African cultural visual question answering. It covers
  15 African languages across 12 countries with over 7.5k Q&A pairs in text and speech
  modalities.
---

# Afri-MCQA: Multimodal Cultural Question Answering for African Languages

## Quick Facts
- arXiv ID: 2601.05699
- Source URL: https://arxiv.org/abs/2601.05699
- Reference count: 39
- Large-scale multilingual multimodal benchmark for African cultural visual question answering covering 15 languages with 7.5k Q&A pairs

## Executive Summary
Afri-MCQA introduces the first large-scale multilingual and multimodal benchmark for African cultural visual question answering. The benchmark covers 15 African languages across 12 countries with over 7.5k Q&A pairs in both text and speech modalities. Evaluation reveals that current MLLMs struggle significantly with African cultural knowledge, showing near-zero accuracy on open-ended VQA when queried in native languages or speech. The findings highlight the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer.

## Method Summary
The paper evaluates MLLMs on Afri-MCQA using zero-shot inference across multiple model families (Qwen 2.5-Omni, Gemma-3n, Gemini-2.5 Pro) in both text and audio modalities. Models are tested using location-aware prompts that provide country context. Performance is measured via MC-VQA accuracy and open-ended VQA using LLM-as-a-judge with chrF++. Control experiments include AfriXNLI and AfriMMLU for linguistic competence, and ASR/LID tasks for speech processing capability. The dataset contains 7.5k Q&A pairs with 10 image categories, available at HuggingFace.

## Key Results
- MLLMs show near-zero accuracy on open-ended VQA when queried in native African languages or speech
- Large performance gaps exist between English and native language queries (2-19% difference)
- Audio VQA performance collapses when ASR and LID accuracy are poor (85-100% WER, near-random LID)
- Scaling model size does not significantly improve performance on native languages

## Why This Works (Mechanism)

### Mechanism 1: Cascading Speech Processing Failure
The near-zero accuracy observed in open-ended Audio VQA for native languages is likely caused by failure in foundational speech processing components (ASR and Language Identification) rather than high-level reasoning deficits. Audio input must first be transcribed or encoded; if the model fails to identify the language or transcribes with high WER, the downstream reasoning module receives garbled text, making visual-cultural reasoning impossible.

### Mechanism 2: Linguistic vs. Cultural Competence Decoupling
Performance gaps between English and Native languages reflect deficits in general linguistic competence rather than specific lack of visual-cultural knowledge. Models rely on shared semantic representations between languages; for low-resource languages with complex morphology, models fail to parse query logic before attempting to retrieve cultural knowledge.

### Mechanism 3: Multiple-Choice Shortcuts vs. Generative Retrieval
The large performance gap between Multiple-Choice VQA and Open-Ended VQA suggests models rely on option discrimination rather than grounded knowledge retrieval. In MC-VQA, models can leverage distractor elimination without fully "knowing" the answer, while open-ended VQA requires specific entity retrieval exposing shallow cultural representations.

## Foundational Learning

**Concept: Visual Question Answering (VQA) Modalities**
- Why needed: The paper distinguishes between Text-VQA and Audio-VQA. Understanding that Audio-VQA adds an extra signal processing layer (speech-to-text or direct audio encoding) is crucial for diagnosing why audio performance crashes.
- Quick check: Can you explain why a model might correctly answer a question in text but fail when the exact same question is spoken?

**Concept: Low-Resource Language Morphology**
- Why needed: The paper notes African languages feature "rich morphology" and "noun classes." Models trained on English struggle with agglutinative or synthetic languages where meaning changes via prefixes/suffixes, affecting tokenization and comprehension.
- Quick check: Why would a standard tokenizer trained on English struggle with a language like Amharic (Semitic root-pattern morphology)?

**Concept: Location-Aware Prompting**
- Why needed: The evaluation uses "Location-aware" prompts (adding country context). This tests if models use prior knowledge about a region when analyzing an image, rather than treating the image as geographically ambiguous.
- Quick check: How does providing the country context in the prompt (e.g., "This image is from Nigeria") shift the model's task from pure visual classification to reasoning?

## Architecture Onboarding

**Component map:** Speech Encoder (Audio-only) + Vision Encoder (Image-only) + LLM Backbone (Fusion & Reasoning)

**Critical path:**
1. Audio/Text Encoding: The model must first correctly process the native language speech or text (The "Linguistic Bottleneck")
2. Visual Grounding: The model must map the query to specific regions or features in the image
3. Cultural Retrieval: The model must activate knowledge associated with the specific culture/language context

**Design tradeoffs:**
- English vs. Native: Evaluating in English isolates cultural gaps; evaluating in Native languages exposes linguistic inequity
- Closed vs. Open Weights: Closed models (Gemini) handle speech better, likely due to proprietary multilingual audio data
- Control Tasks: Including AfriXNLI/AfriMMLU helps distinguish "can't speak the language" from "doesn't know the culture"

**Failure signatures:**
- Hallucination in ASR: High WER (100%+) indicates the model invents text that wasn't spoken
- Random LID: ~6.6% accuracy on Language Identification implies the model is guessing blindly among the 15 languages
- "Location-Aware" Gain: If adding location context doesn't improve scores, the model likely lacks that specific cultural knowledge

**First 3 experiments:**
1. Establish Baseline: Run text-based MC-VQA benchmark on your model using location-aware prompts to measure cultural knowledge retrieval without speech complexity
2. Isolate Speech Failure: Run Language Identification (LID) probe. If accuracy is low (<50%), do not proceed to full Audio VQA; fix the audio encoder or adapter first
3. Cross-Modal Comparison: Evaluate the same samples in Text-English, Text-Native, and Audio-English. If Text-Native fails but Text-English succeeds, the bottleneck is the tokenizer/LLM weights

## Open Questions the Paper Calls Out

**Open Question 1:** How can speech-first approaches be effectively developed for African languages given the failure of current open-weight models to perform basic Language Identification (LID) and Automatic Speech Recognition (ASR)?

**Open Question 2:** Does incorporating explicit African cultural content during pretraining close the performance gap between general factual knowledge and specific cultural understanding?

**Open Question 3:** How can cross-lingual knowledge transfer be optimized to allow models to access cultural facts known in English when queried in native African languages?

**Open Question 4:** Does the weak correlation between Afri-MCQA and text-only control benchmarks imply that cultural visual reasoning is a distinct capability unrelated to linguistic competence?

## Limitations

- The evaluation relies on GPT-4o-mini as LLM-as-a-judge but exact prompt templates and scoring rubrics are not specified
- Control experiments lack per-language breakdowns, preventing precise mapping of where morphology-specific failures occur
- ASR and LID probing shows severe degradation but per-language WER or LID scores are not reported
- The conclusion that scaling model size doesn't improve native language performance is weakly supported with limited model size testing

## Confidence

- **High confidence**: MLLMs struggle with African cultural knowledge in both text and audio modalities
- **Medium confidence**: Foundational speech processing failures are the primary cause of audio VQA collapse
- **Low confidence**: Scaling model size does not improve native language performance

## Next Checks

1. Run AfriXNLI and AfriMMLU separately for each of the 15 languages to map linguistic competence gaps and correlate with Afri-MCQA failures
2. For a subset of native audio samples, manually transcribe them and evaluate model performance on textified versions versus raw audio to isolate whether ASR errors or downstream reasoning deficits are responsible
3. Re-evaluate a random sample of open-ended responses using two independent LLM judges to measure inter-annotator agreement and rule out systematic bias