---
ver: rpa2
title: Understanding the Effects of Domain Finetuning on LLMs
arxiv_id: '2510.09359'
source_url: https://arxiv.org/abs/2510.09359
tags:
- medical
- vectors
- tuning
- qwen2
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically investigates how domain-specific fine-tuning
  reshapes large language models, particularly in the medical domain. It introduces
  tuning vectors, a framework capturing the directional parameter shifts induced by
  fine-tuning.
---

# Understanding the Effects of Domain Finetuning on LLMs

## Quick Facts
- arXiv ID: 2510.09359
- Source URL: https://arxiv.org/abs/2510.09359
- Authors: Eshaan Tanwar; Deepak Nathani; William Yang Wang; Tanmoy Chakraborty
- Reference count: 40
- Primary result: Domain-specific fine-tuning reshapes LLMs through directional parameter shifts captured by tuning vectors, primarily modifying MLP layers while preserving neural activation patterns

## Executive Summary
This study systematically investigates how domain-specific fine-tuning reshapes large language models, particularly in the medical domain. The authors introduce tuning vectors, a framework capturing the directional parameter shifts induced by fine-tuning. Their analysis reveals that fine-tuning modifies only a small subset of the representational subspace, with neural activations remaining largely consistent between pretrained and fine-tuned models. The work demonstrates that tuning vectors are critical for improving instruction-following, benchmark performance, and generation quality, and that combining tuning vectors across domains yields more generalizable models.

## Method Summary
The study employs a comprehensive analytical framework to examine domain fine-tuning effects on LLMs. The authors introduce tuning vectors as a mathematical construct to capture directional parameter shifts during fine-tuning. They conduct extensive experiments comparing pretrained, instruction-tuned, and domain-specific models, analyzing neural activations, performance on benchmarks, and generation quality. The research focuses on medical domain adaptation while also exploring cross-domain combinations with programming data.

## Key Results
- Fine-tuning modifies only a small subset of the representational subspace, with neural activations remaining largely consistent between pretrained and fine-tuned models
- Tuning vectors are critical for improving instruction-following, benchmark performance, and generation quality
- Combining tuning vectors across domains yields more generalizable models
- Tuning vectors primarily write new directional information into the MLP layers while amplifying existing directions in attention heads

## Why This Works (Mechanism)
The effectiveness of domain fine-tuning stems from the ability of tuning vectors to capture meaningful directional changes in the parameter space. These vectors represent the minimal set of modifications needed to adapt a general-purpose model to domain-specific tasks while preserving the core knowledge representation. The mechanism works by identifying and modifying specific directions in the high-dimensional parameter space that are most relevant to the target domain, allowing for efficient adaptation without disrupting the foundational capabilities of the pretrained model.

## Foundational Learning
- **Tuning vectors**: Mathematical representation of directional parameter shifts during fine-tuning. Why needed: To quantify and analyze the specific changes induced by domain adaptation. Quick check: Verify that tuning vectors capture meaningful performance differences across domains.
- **Representational subspace modification**: The observation that fine-tuning affects only a subset of the model's representational capacity. Why needed: To understand efficiency and limitations of domain adaptation. Quick check: Measure overlap between pretrained and fine-tuned activation spaces.
- **MLP vs attention head dynamics**: Different components of the transformer architecture respond differently to fine-tuning. Why needed: To optimize fine-tuning strategies for specific domains. Quick check: Analyze parameter changes per component type during adaptation.

## Architecture Onboarding

**Component Map**: Pretrained Model -> Fine-tuning Process -> Tuning Vectors -> Domain-Specific Capabilities

**Critical Path**: Input text → Transformer layers (attention + MLP) → Output generation → Performance metrics

**Design Tradeoffs**: The study reveals a fundamental tradeoff between preserving general capabilities (through limited subspace modification) and achieving domain-specific performance (through targeted tuning vector application).

**Failure Signatures**: The paper identifies potential interference effects when combining tuning vectors from different domains, particularly when domains have conflicting requirements or overlapping but incompatible feature representations.

**First Experiments**:
1. Compute tuning vectors for three distinct domains and measure their orthogonality
2. Apply tuning vectors to control models and measure performance gains
3. Combine tuning vectors from medical and programming domains and test on cross-domain tasks

## Open Questions the Paper Calls Out
### Open Question 1
How does accounting for permutation equivariance in feed-forward layers affect the observed alignment between pretrained and fine-tuned models' neural activations?
- Basis in paper: Authors state: "our finding that pretrained and instruction-tuned models exhibit similar neural activations does not consider permutation equivariance in feed-forward layers. Future work can investigate whether these models are even more aligned when such symmetry is taken into account."
- Why unresolved: The current analysis treats neurons as fixed entities across models, but neurons may be functionally equivalent yet positioned differently.
- What evidence would resolve it: Re-running activation similarity analyses after applying permutation alignment algorithms between layers; quantifying any increase in alignment scores.

### Open Question 2
Can training models only along domain-relevant tuning vector directions achieve comparable performance to full fine-tuning with greater efficiency?
- Basis in paper: Authors propose: "The concept of tuning vectors can be leveraged to guide domain-specific fine-tuning by training models only along domain-relevant directions. Such a regime can be efficiently integrated into low-rank adaptation methods."
- Why unresolved: The paper demonstrates tuning vectors capture meaningful changes but does not implement or test constrained training.
- What evidence would resolve it: Experiments constraining gradient updates to tuning vector subspaces during fine-tuning; comparing convergence speed and final performance against unconstrained fine-tuning.

### Open Question 3
What methods for reducing conflicts and selectively combining tuning vectors improve upon simple vector addition for multi-domain generalization?
- Basis in paper: Authors note: "Our current vector composition strategy involves simple addition. This can be improved by reducing conflicts between vectors and selectively combining directions of interest, incorporating ideas from linear mode connectivity."
- Why unresolved: Simple addition showed mixed results—medical tasks improved but math benchmarks showed inconsistent gains—suggesting interference between vectors.
- What evidence would resolve it: Developing conflict-aware composition operators (e.g., projecting out overlapping components, weighted interpolation); benchmarking multi-domain performance against naive addition.

## Limitations
- Analysis focuses primarily on the medical domain, limiting generalizability to other specialized domains
- The directional analysis framework may not capture all relevant aspects of fine-tuning dynamics, particularly for more complex or overlapping domain adaptations
- Claims about MLP vs attention head modifications lack broader validation across different model architectures

## Confidence
- High confidence in findings regarding tuning vectors capturing directional parameter shifts and their role in instruction-following and benchmark performance
- Medium confidence in claims about representational subspace modification and activation consistency, as these are based on specific measurement techniques
- Medium confidence in cross-domain combination benefits, as the study focuses on medical and programming domains
- Low confidence in claims about MLP vs attention head modifications without broader validation across different model architectures

## Next Checks
1. Replicate the tuning vector analysis across at least three additional distinct domains (e.g., legal, financial, scientific) to assess generalizability
2. Conduct ablation studies removing MLP or attention components to quantify their relative contributions to fine-tuning effectiveness
3. Test the cross-domain combination approach with more than two domains simultaneously to evaluate scalability and potential interference effects