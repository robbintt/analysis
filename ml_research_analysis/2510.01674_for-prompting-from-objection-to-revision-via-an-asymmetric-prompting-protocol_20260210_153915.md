---
ver: rpa2
title: 'FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol'
arxiv_id: '2510.01674'
source_url: https://arxiv.org/abs/2510.01674
tags:
- for-prompting
- reasoning
- defender
- accuracy
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FOR-Prompting introduces an asymmetric protocol that uses external
  questioning to improve reasoning without supplying solutions. A Defender proposes
  and revises answers while a Questioner asks targeted questions, and a Host synthesizes
  the final output.
---

# FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol

## Quick Facts
- arXiv ID: 2510.01674
- Source URL: https://arxiv.org/abs/2510.01674
- Reference count: 40
- Primary result: Matches CoT accuracy (~90%) on GSM8K while improving reasoning and coherence scores, especially for small models (~19% accuracy gain on Llama3.2-1B)

## Executive Summary
FOR-Prompting introduces an asymmetric protocol that uses external questioning to improve reasoning without supplying solutions. A Defender proposes and revises answers while a Questioner asks targeted questions, and a Host synthesizes the final output. On GSM8K, it matches CoT accuracy (≈90%) while achieving higher reasoning and coherence scores. Tested on small models (1B Llama3.2), it improves accuracy by about 19% over single-prompt baselines. Qualitative case studies show it refines open-ended tasks and corrects mistakes through iterative questioning alone. The approach is model-agnostic, runs entirely at the prompt level, and is well-suited for on-device or small-model applications.

## Method Summary
FOR-Prompting is a multi-agent prompting protocol where three specialized agents work in sequence: a Defender proposes and revises answers, a Questioner (Debater) raises targeted questions without providing solutions, and a Host synthesizes the final output. The protocol enforces strict role constraints—the Questioner can only ask questions, never suggest fixes—forcing the Defender to generate all reasoning. The process iterates for N rounds (typically 1-3), with the Defender updating its answer based on the Questioner's objections. This asymmetric structure maintains a single line of reasoning while leveraging external pressure to activate latent reasoning capabilities, particularly in smaller models.

## Key Results
- Matches CoT accuracy (≈90%) on GSM8K while achieving higher reasoning and coherence scores
- Improves small model performance by ~19% on Llama3.2-1B for GSM8K task
- Demonstrates effectiveness on open-ended planning tasks through iterative refinement

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Constraint Enforcement
Restricting the external agent (Debater) to output only questions—without solutions—prevents the conflation of error detection with answer replacement, preserving the Defender as the sole author of the reasoning trace. By blocking the Debater from proposing fixes, the system forces the Defender to generate the corrective reasoning internally, maintaining a single chain of thought rather than merging competing reasoning paths from multiple agents. Core assumption: The underlying LLM possesses sufficient latent reasoning capability to correct itself when prompted with a targeted query, but requires external pressure to trigger this revision.

### Mechanism 2: Latent Capacity Activation via External Probing
External questioning may activate reasoning pathways in small models that are otherwise dormant during single-pass inference, effectively acting as a retrieval cue for parametric knowledge. The Debater identifies logical gaps or assumptions, and this external signal acts as a high-density "prompt" that narrows the search space for the Defender. For small models (e.g., 1B parameters), which struggle with self-correction due to limited capacity, this external "nudge" substitutes for the internal self-attention mechanisms of larger models. Core assumption: The failure mode of small models on complex tasks is often a failure to retrieve or prioritize correct reasoning steps, rather than a complete absence of the required knowledge.

### Mechanism 3: Iterative Consistency Closure
The iterative loop (Question → Revision) coupled with a final Host synthesis improves coherence by forcing the Defender to resolve contradictions before closure. The Defender must update its state A_{r-1} to A_r conditioned on objection O_r, creating a gradient of revision where inconsistencies are penalized by the Debater in the next round. The Host acts as a final consistency check, aggregating the trajectory into a stable output. Core assumption: Iteration depth correlates with correctness up to a saturation point (observed at ~1-3 rounds in experiments).

## Foundational Learning

### Concept: Chain-of-Thought (CoT) Prompting
- **Why needed here:** FOR-Prompting is positioned as a successor/alternative to CoT. Understanding that CoT organizes internal deliberation highlights why an external mechanism (FOR) is needed to handle blind spots.
- **Quick check question:** How does CoT differ from standard prompting, and what specific limitation does FOR-Prompting address in CoT? (Answer: CoT is internal and single-agent; FOR adds external questioning to elicit revision.)

### Concept: Human-in-the-Loop (HITL)
- **Why needed here:** The paper explicitly draws inspiration from HITL practices where reviewers ask questions rather than fix text. Understanding this helps explain the "Asymmetric" design philosophy.
- **Quick check question:** In a standard HITL workflow for LLMs, does the human rewrite the answer or prompt the model? (Answer: FOR-Prompting simulates the latter via the Debater agent.)

### Concept: Role-Specialized Agents
- **Why needed here:** The architecture relies on distinct roles (Defender, Debater, Host). Understanding how to enforce these constraints in a prompt is critical for implementation.
- **Quick check question:** Why must the Debater be explicitly restricted from providing solutions? (Answer: To prevent confounding the Defender's reasoning with external reasoning and to maintain a single authorial line.)

## Architecture Onboarding

### Component map:
Defender → Questioner (Debater) → Defender → Host

### Critical path:
1. Initialize Defender with Question Q → A₀
2. Pass A₀ to Debater → generate O₁ (Constraint: Questions only)
3. Feed Q + A₀ + O₁ to Defender → A₁
4. Repeat for N rounds (typically 1-3)
5. Pass full trace to Host → Final Answer

### Design tradeoffs:
- **Latency vs. Accuracy:** Increasing rounds (N) improves reasoning scores but linearly increases token cost and latency
- **Model Size vs. Protocol Overhead:** The protocol is most beneficial for small models (~1B params), but running multiple agent turns on a small local model may still be slower than a single pass on a cloud API

### Failure signatures:
- **Objective Drift:** The Defender answers the Debater's question but forgets the original question Q (Mitigation: Defender must re-answer Q every turn)
- **Role Collapse:** The Debater starts suggesting fixes (e.g., "You should use multiplication here")
- **Empty Loop:** The Debater cannot find flaws in an incorrect answer, leading to a false positive

### First 3 experiments:
1. **Baseline Integrity Check:** Run the "r counting" case study (Page 7) to verify the Debater can successfully trigger a correction on a tokenization error
2. **Role Constraint Stress Test:** Implement a "Debater" prompt without the "questions only" constraint and compare accuracy on GSM8K against the strict "questions only" version to validate the core asymmetry claim
3. **Small Model Scaling:** Deploy the protocol on a 1B-3B parameter model (e.g., Llama 3.2 1B) on a subset of GSM8K (e.g., 50 samples) to verify the reported ~19% accuracy lift before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does FOR-Prompting yield similar relative performance gains on mid-to-large scale open-source models (e.g., 7B–70B parameters) as observed on the 1B model?
- Basis in paper: [explicit] The authors state: "Our study is limited by compute constraints, so we could not evaluate on more and larger open source datasets."
- Why unresolved: While the protocol significantly boosts 1B model performance, its utility for stronger open-source models remains unconfirmed.
- What evidence would resolve it: Benchmarking the protocol on larger open-source weights (e.g., Llama-3-8B or 70B) across standard reasoning datasets.

### Open Question 2
- Question: Can a learned adaptive policy for the Debater agent outperform the fixed-round iteration cap used in the current protocol?
- Basis in paper: [explicit] The conclusion explicitly lists "learn[ing] adaptive questioning policies" as a direction for future work.
- Why unresolved: The current implementation relies on a manually set constant for rounds, which risks being suboptimal for problems of varying complexity.
- What evidence would resolve it: A comparison study between fixed-round FOR-Prompting and a variant using a dynamic stopping criterion or learned question selection strategy.

### Open Question 3
- Question: Does the asymmetric questioning mechanism transfer effectively to non-textual modalities like vision-language reasoning?
- Basis in paper: [explicit] Future work plans include efforts to "broaden tasks and modalities."
- Why unresolved: All empirical validation in the study is currently restricted to text-based tasks (math and planning).
- What evidence would resolve it: Evaluation results applying the FOR-Prompting protocol to multimodal benchmarks (e.g., Visual Question Answering).

## Limitations
- Limited evaluation to GSM8K dataset, raising questions about generalizability to other reasoning tasks
- Fixed iteration depth (1-3 rounds) may be suboptimal for problems of varying complexity
- No exploration of learned adaptive questioning policies for the Debater agent

## Confidence
- **High Confidence:** The protocol's ability to improve accuracy on GSM8K (≈19% lift for small models) and its model-agnostic design are well-supported by experimental results
- **Medium Confidence:** The claim that external questioning activates latent reasoning pathways in small models is plausible but not rigorously tested beyond GSM8K. The iterative consistency closure mechanism is theoretically sound but lacks empirical validation across tasks
- **Low Confidence:** The generalizability of the protocol to non-mathematical or highly subjective tasks (e.g., creative writing) is untested. The long-term robustness of the protocol under varying Debater quality or model size is also unclear

## Next Checks
1. **Role Constraint Stress Test:** Implement a Debater prompt without the "questions only" constraint and compare accuracy on GSM8K against the strict version to validate the core asymmetry claim
2. **Cross-Task Generalization:** Apply FOR-Prompting to a non-mathematical task (e.g., open-ended planning or sentiment analysis) and measure reasoning and coherence scores to test generalizability
3. **Small Model Scaling:** Deploy the protocol on a 1B-3B parameter model (e.g., Llama 3.2 1B) on a subset of GSM8K (e.g., 50 samples) to verify the reported ~19% accuracy lift before scaling up