---
ver: rpa2
title: A fully automated and scalable Parallel Data Augmentation for Low Resource
  Languages using Image and Text Analytics
arxiv_id: '2510.13211'
source_url: https://arxiv.org/abs/2510.13211
tags:
- article
- sentence
- language
- languages
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel fully automated and scalable methodology
  to build bilingual parallel corpora for low-resource languages using image and text
  analytics. The approach leverages newspaper article images as pivots to map articles
  across different language editions, followed by sentence-level alignment using language-agnostic
  embeddings.
---

# A fully automated and scalable Parallel Data Augmentation for Low Resource Languages using Image and Text Analytics

## Quick Facts
- arXiv ID: 2510.13211
- Source URL: https://arxiv.org/abs/2510.13211
- Reference count: 20
- Primary result: Generated >14,000 sentence pairs with STS 3.7 for Konkani-Marathi and Punjabi-Hindi, improving MT BLEU by ~3 points

## Executive Summary
This paper introduces a fully automated method to construct bilingual parallel corpora for low-resource languages by leveraging newspaper images as cross-lingual pivots. The approach combines image-based article matching using SIFT with language-agnostic sentence embedding alignment via LaBSE, eliminating the need for human annotation. Evaluated on Konkani-Marathi and Punjabi-Hindi pairs, the system achieved high semantic alignment scores and demonstrated practical value by improving machine translation performance on held-out captions.

## Method Summary
The methodology extracts bilingual parallel corpora from newspaper articles by first using image matching (SIFT) to identify semantically equivalent articles across language editions, then applying language-agnostic embeddings (LaBSE) for sentence-level alignment. The pipeline includes crawling newspaper PDFs, segmenting pages using layout analysis, extracting text via OCR ensemble voting, and aligning sentences based on semantic similarity. The approach is fully automated and scalable, requiring no parallel data for training the alignment components.

## Key Results
- Generated over 14,000 sentence pairs with Semantic Textual Similarity (STS) score of 3.70
- 92% of aligned sentence pairs scored above STS threshold of 3.0
- Machine translation evaluation showed BLEU score improvement from 23.5 to 26.4 using augmented corpus
- Outperformed baseline length-based and lexical overlap alignment methods by 0.3-0.8 STS points

## Why This Works (Mechanism)

### Mechanism 1: Image-Pivoted Cross-Lingual Article Mapping
Newspaper images can serve as reliable pivots to identify parallel articles across language editions because publishing houses reuse the same images across different language editions to reduce costs. SIFT detects image features that are robust to scale, illumination, and minor transformations, enabling matching of identical images across language versions. The core assumption is that the same image appearing in two language editions indicates semantically equivalent article content.

### Mechanism 2: Language-Agnostic Sentence Embedding Alignment
Language-agnostic embeddings (LaBSE) enable accurate sentence alignment without requiring language-specific training data by converting sentences into high-dimensional vectors that capture semantic meaning across languages. Cosine similarity between vectors identifies parallel sentence pairs even when word order differs. The core assumption is that the embedding space preserves cross-lingual semantic similarity for languages not in the training corpus.

### Mechanism 3: OCR Ensemble Voting for Text Extraction
Combining multiple OCR systems with majority voting improves text extraction accuracy over single systems because each OCR system (EasyOCR, PaddleOCR, Tesseract) has different error patterns. Majority voting cancels out individual system errors. The core assumption is that OCR errors are not perfectly correlated across systems.

## Foundational Learning

- **Semantic Textual Similarity (STS) Scoring**
  - Why needed here: Evaluates parallel corpus quality without requiring downstream task training
  - Quick check question: Can you explain why STS scores of 3+ indicate acceptable alignment quality?

- **Scale-Invariant Feature Transform (SIFT)**
  - Why needed here: Core algorithm for image-based article matching across editions
  - Quick check question: What image transformations does SIFT handle robustly?

- **LaBSE (Language-Agnostic BERT Sentence Embedding)**
  - Why needed here: Enables sentence alignment for language pairs without existing parallel data
  - Quick check question: How does LaBSE claim to work for languages outside its training corpus?

## Architecture Onboarding

- **Component map**: Crawler -> Article Extractor -> Article Mapper -> Sentence Mapper
- **Critical path**: Image quality → Article segmentation accuracy → Image matching success → OCR extraction quality → Sentence embedding alignment → Final corpus quality
- **Design tradeoffs**: SIFT chosen over neural image matching because newspaper images are near-exact copies (lower compute, sufficient accuracy); LAS chosen over SLAS/LO despite higher compute cost due to 0.3-0.8 STS improvement; ensemble OCR increases latency but reduces cascading errors into sentence alignment
- **Failure signatures**: Low article match count (check if source publications actually share images; adjust SIFT threshold); STS scores below 3 (LaBSE may not support language pair well; consider alternative embeddings); high OCR error rate (preprocessing needed; consider script-specific OCR models)
- **First 3 experiments**:
  1. Run Article Mapper on 50 manually verified article pairs to calibrate SIFT threshold; target >90% precision.
  2. Compare LAS vs SLAS sentence alignment on 100 hand-aligned sentences; confirm STS advantage holds for your language pair.
  3. Train baseline MT model (mT5) with 1000 vs 5000 vs full corpus sentence pairs; plot BLEU vs corpus size curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the article mapping pipeline be adapted to utilize distinct images of the same event taken by different photographers, rather than relying on identical reprinted images?
- Basis in paper: The authors state in the Conclusion that "The scale of the dataset can be enhanced by considering images clicked by different people for the same news event" and aim to explore this.
- Why unresolved: The current SIFT-based methodology depends on matching images that are exact copies or near-duplicates; it cannot link articles using semantically similar but visually distinct images.
- What evidence would resolve it: Integration of object detection or scene-graph matching algorithms that successfully map article pairs using different images of the same subject.

### Open Question 2
- Question: How does Optical Character Recognition (OCR) noise in low-resource scripts impact the accuracy of language-agnostic sentence embeddings (LAS) during the alignment phase?
- Basis in paper: The pipeline relies on OCR tools (Tesseract, EasyOCR) to extract text before generating embeddings. While OCR errors are common in complex scripts, their specific effect on the subsequent semantic vector alignment is not quantified.
- Why unresolved: The paper evaluates the end-to-end result (STS score) but does not perform an ablation study to isolate how character recognition errors degrade the sentence embedding quality.
- What evidence would resolve it: A comparative analysis of sentence alignment accuracy using gold-standard (human-transcribed) text versus the current OCR-extracted text.

### Open Question 3
- Question: What specific "additional constraints" are required to further boost the semantic quality of the generated parallel corpus?
- Basis in paper: The authors conclude that "in order to boost the quality of the dataset additional constraints may need to be provided."
- Why unresolved: The paper does not specify what these constraints might be—whether they are linguistic, geometric, or context-based—or how they would integrate into the current automated pipeline.
- What evidence would resolve it: A follow-up study testing specific constraints (e.g., word-length ratios, part-of-speech alignment) that result in a statistically significant increase in STS or BLEU scores.

## Limitations

- The methodology's dependence on newspaper image reuse creates a brittle foundation - if publishers shift to AI-generated images or stop cross-language image sharing, the entire pipeline fails
- OCR accuracy for low-resource scripts remains a critical bottleneck not fully addressed
- The paper lacks transparency on threshold tuning for SIFT matching and sentence alignment, making exact replication difficult

## Confidence

- **High confidence**: The image-pivot mechanism works when newspapers consistently reuse images across language editions. The LAS approach outperforms baselines for sentence alignment when LaBSE supports the language pair.
- **Medium confidence**: The methodology generalizes to other low-resource language pairs with similar publishing practices. The STS scoring methodology accurately reflects alignment quality.
- **Low confidence**: The pipeline will work equally well for languages with different scripts or publishing norms. The BLEU improvement is primarily attributable to the augmented corpus rather than mT5's pre-training.

## Next Checks

1. **Image Matching Robustness Test**: Apply the pipeline to 100 manually verified article pairs from three different bilingual newspaper pairs. Measure SIFT precision/recall across pairs and document threshold sensitivity. Target >90% precision with recall >70%.

2. **Cross-Lingual Embedding Coverage Analysis**: For each language pair tested, compute LaBSE embedding coverage rates and alignment accuracy on 200 hand-aligned sentence pairs. Document which language combinations achieve STS >3.5 versus those that fail.

3. **Real-World Deployment Simulation**: Run the full pipeline on a 30-day corpus from newspapers with known image-sharing practices. Measure: (a) total parallel sentences generated, (b) STS distribution across all pairs, (c) BLEU improvement when fine-tuning mT5 with 10%, 50%, and 100% of generated corpus.