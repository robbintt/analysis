---
ver: rpa2
title: 'EHRNavigator: A Multi-Agent System for Patient-Level Clinical Question Answering
  over Heterogeneous Electronic Health Records'
arxiv_id: '2601.10020'
source_url: https://arxiv.org/abs/2601.10020
tags:
- clinical
- ehrnavigator
- question
- data
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EHRNavigator, a multi-agent system for patient-level
  clinical question answering over heterogeneous EHR data. It addresses the challenge
  of efficiently retrieving and synthesizing information from structured and unstructured
  EHR sources to support clinical decision-making.
---

# EHRNavigator: A Multi-Agent System for Patient-Level Clinical Question Answering over Heterogeneous Electronic Health Records

## Quick Facts
- **arXiv ID**: 2601.10020
- **Source URL**: https://arxiv.org/abs/2601.10020
- **Reference count**: 0
- **Primary result**: Multi-agent EHR QA system achieving 86% accuracy on real-world clinical questions with median response times under 12 seconds

## Executive Summary
EHRNavigator introduces a multi-agent system for patient-level clinical question answering over heterogeneous electronic health records. The system addresses the challenge of efficiently retrieving and synthesizing information from structured and unstructured EHR sources to support clinical decision-making. By employing specialized agents for schema exploration, structured querying, semantic retrieval, and multimodal answer synthesis, EHRNavigator enables natural language interaction without requiring schema-specific training. Evaluated on four datasets covering structured, unstructured, and multimodal queries, the system achieved strong performance, including 86% accuracy on real-world clinical questions from Yale New Haven Hospital.

## Method Summary
EHRNavigator is a multi-agent framework that decomposes clinical QA into specialized sub-tasks. The system uses three main modules: Structured Data Query (Table Reviewer, SQL Writer, SQL Executor), Unstructured Retrieval (Note Chunker, Structure-aware Retriever), and Answer Synthesis (fine-tuned synthesizer). Only the answer synthesizer is trained using LoRA on 80% of the DrugEHRQA multimodal split, while all upstream agents remain frozen. The system operates in a zero-shot manner across diverse database schemas by dynamically interpreting structures at runtime. BGE-large-en embeddings are used for semantic search, with 256-token chunks and 32-token overlap for notes.

## Key Results
- Achieved 86% accuracy on real-world clinical questions from Yale New Haven Hospital
- Improved GPT-4o accuracy from 76.50% to 94.86% on DrugEHRQA structured queries
- Demonstrated median response times under 12 seconds across all query types
- Showed strong performance on cross-schema evaluation with MIMIC-III and OMOP data

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Task Decomposition
Decomposing complex clinical QA into specialized sub-tasks improves accuracy over monolithic LLM prompting. Separate agents handle schema discovery, SQL generation, and answer synthesis, each operating within constrained scope with task-specific prompts. LLMs perform better on narrow, well-scoped tasks than on end-to-end reasoning over heterogeneous data.

### Mechanism 2: Structure-Guided Unstructured Retrieval
Conditioning note retrieval on structured SQL results improves semantic alignment between clinical questions and free-text narratives. After SQL execution returns structured evidence, the note retriever jointly encodes (question, structured evidence, note chunks) to rank relevance. Structured data narrows the search space and provides temporal anchors.

### Mechanism 3: Zero-Shot Schema Adaptation via Runtime Discovery
Agents generalize across heterogeneous EHR schemas without schema-specific training by dynamically exploring database structure. Table Reviewer Agent queries schema metadata at runtime, generates natural-language descriptions, and caches them. Downstream agents use these descriptions rather than hard-coded mappings.

## Foundational Learning

- **Text-to-SQL with LLMs**
  - Why needed: Core capability for structured data querying module; must understand how LLMs map natural language to executable SQL.
  - Quick check: Given schema `patients(id, gender, dob)` and question "What is patient 123's gender?", can you write the SQL?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed: Unstructured retrieval module uses chunking, embedding, and top-K retrieval; understanding baseline RAG clarifies why structure-guided retrieval differs.
  - Quick check: What is the standard RAG pipeline for answering questions over a document corpus?

- **Multi-Agent Orchestration Patterns**
  - Why needed: EHRNavigator coordinates 4+ agents with tool access; understanding agent roles and handoff protocols is essential for debugging.
  - Quick check: What happens if the SQL Writer Agent produces an invalid query—how does the system recover?

## Architecture Onboarding

- **Component map**: Table Reviewer Agent → Table Retrieval Tool → SQL Writer Agent → SQL Execution Tool → Note Retriever → Answer Synthesizer Agent
- **Critical path**: Question → Table selection (semantic match) → SQL generation + sampling → SQL execution → Note retrieval (conditioned on SQL results) → Answer synthesis. Latency dominated by SQL execution and cross-modal joins.
- **Design tradeoffs**: Zero-shot generalization vs fine-tuned accuracy; latency vs comprehensiveness; evidence transparency vs conciseness.
- **Failure signatures**: Visit-scope mismatch (57.1% of YNHHQA errors), information displacement (21.4%), synthesis gap (6/100 cases).
- **First 3 experiments**:
  1. Run EHRNavigator on EHRSQL benchmark; compare execution accuracy vs vanilla GPT-4o schema-prompting baseline.
  2. Disable structure-guided retrieval; measure accuracy drop on DrugEHRQA multimodal subset.
  3. Deploy on OMOP-formatted database with unseen table naming conventions; audit SQL generation errors.

## Open Questions the Paper Calls Out

### Open Question 1
Can EHRNavigator be extended to handle concept-driven clinical questions requiring high-level reasoning through integration with medical ontologies like SNOMED-CT? The current architecture lacks a semantic reasoning layer to interpret abstract clinical concepts against patient data.

### Open Question 2
To what extent would incorporating multi-turn dialogue capability improve handling of complex cases requiring iterative hypothesis refinement? The current single-round querying paradigm may be insufficient for conversational diagnostic reasoning.

### Open Question 3
Does explicit modeling of temporal-clinical events significantly reduce the visit-scope mismatch error rate identified as the primary bottleneck? Current agents process events based on available schema columns without inherent clinical chronology modeling.

## Limitations
- Schema generalization claims lack testing on truly unseen, proprietary schemas with non-standard naming conventions
- Multi-agent coordination logic is not fully specified, affecting reproducibility
- Structure-guided retrieval benefits lack head-to-head comparison with other multimodal RAG variants

## Confidence
- **High Confidence**: Structured query accuracy improvements are well-supported by controlled comparisons with vanilla LLM baselines
- **Medium Confidence**: Zero-shot schema adaptation is plausible but lacks independent validation on diverse, unseen schemas
- **Low Confidence**: Structure-guided retrieval's superiority over standard RAG is demonstrated but lacks broader empirical support

## Next Checks
1. Deploy EHRNavigator on a third, unseen EHR schema and systematically audit SQL generation errors to isolate schema-interpretation failures
2. Disable the structured-conditional note retrieval and measure accuracy drop on DrugEHRQA multimodal subset; compare against standard RAG baseline
3. Instrument the system to log agent handoffs, tool usage, and error recovery paths during inference; compare against monolithic LLM baseline on simple queries