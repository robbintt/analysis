---
ver: rpa2
title: Exploring Transfer Learning for Deep Learning Polyp Detection in Colonoscopy
  Images Using YOLOv8
arxiv_id: '2502.00133'
source_url: https://arxiv.org/abs/2502.00133
tags:
- datasets
- dataset
- images
- pre-training
- polyp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of transfer learning
  for polyp detection in colonoscopy images using YOLOv8. The authors pre-train YOLOv8n
  models on seven diverse datasets ranging from general object datasets like COCO
  to domain-specific medical imaging datasets.
---

# Exploring Transfer Learning for Deep Learning Polyp Detection in Colonoscopy Images Using YOLOv8

## Quick Facts
- **arXiv ID:** 2502.00133
- **Source URL:** https://arxiv.org/abs/2502.00133
- **Reference count:** 36
- **Primary result:** YOLOv8n pre-trained on Fruit&Veg dataset achieves 95.1% F1-score on polyp detection

## Executive Summary
This study investigates transfer learning for polyp detection in colonoscopy images using YOLOv8. The authors systematically evaluate seven pre-training datasets, ranging from general object detection datasets like COCO to domain-specific medical imaging and visual similarity datasets like fruits and vegetables. Their findings demonstrate that models pre-trained on larger, more diverse datasets consistently outperform those trained from scratch. The study also reveals that visual similarity between pre-training objects and polyps can be more important than domain relevance, with the fruit and vegetable pre-trained model achieving the highest F1-score of 95.1%. The authors release their pre-trained models and processed datasets to support future research.

## Method Summary
The authors employ a two-stage transfer learning approach using YOLOv8n. First, they pre-train models on seven different source datasets for 100 epochs. Second, they fine-tune the pre-trained models on a combined polyp detection dataset consisting of four public polyp datasets (CVC-ClinicDB, CVC-ColonDB, ETIS-LaribPolypDB, and Kvasir-SEG) for 100 epochs. The combined dataset contains 1,748 training images, 220 validation images, and 220 test images. Bounding boxes are derived from segmentation masks. Pre-training datasets vary in size and domain, with the best results achieved using the combined Fruit&Veg dataset containing approximately 13,000 images. The models are evaluated using F1-score and mean Average Precision (mAP) metrics.

## Key Results
- YOLO-fruit&veg-XL achieves the highest F1-score of 95.1% after 100 epochs
- Models pre-trained on datasets with ≥10,000 images consistently outperform smaller pre-training sets
- Fruit and vegetable pre-training outperforms domain-specific medical datasets despite being out-of-domain
- Pre-trained models converge faster and require less training time to achieve high performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-training on datasets with ≥10,000 images yields measurable performance gains over smaller pre-training sets.
- **Mechanism:** Larger datasets expose the model to more diverse edge, texture, and shape primitives, creating richer convolutional feature banks that transfer more robustly to new tasks.
- **Core assumption:** The visual diversity in pre-training data contains transferable low- and mid-level features applicable to the downstream task.
- **Evidence anchors:**
  - [abstract]: "larger pre-training datasets (containing at least 10,000 images) lead to better performance compared to smaller datasets"
  - [section 3.2]: "when YOLO models were pre-trained on at least 10,000 images, their performance improved significantly, as evident in Tables 4, 5, and 6, where the XL models consistently outperformed their smaller counterparts"
  - [corpus]: Endo-CLIP paper demonstrates progressive pre-training benefits on raw colonoscopy records, supporting scale-driven learning in this domain.
- **Break condition:** If the downstream task requires domain-specific semantic understanding (not just visual features), larger generic pre-training may yield diminishing returns.

### Mechanism 2
- **Claim:** Visual similarity between pre-training objects and target objects can outweigh domain taxonomic proximity.
- **Mechanism:** Fruits/vegetables and polyps share rounded shapes, specular highlights, and surface textures—these geometric and photometric similarities create feature overlap despite semantic distance.
- **Core assumption:** Early convolutional layers encode shape and texture priors that generalize across semantically unrelated domains.
- **Evidence anchors:**
  - [abstract]: "model pre-trained on the combined fruit and vegetable dataset, despite being out-of-domain, achieved superior results, suggesting that visual similarities between fruits/vegetables and polyps contribute to effective knowledge transfer"
  - [section 2.1]: "The combined Fruit and Vegetables dataset was chosen for the similarities certain fruits have with polyps, such as shape and shine"
  - [corpus]: Robust Polyp Detection paper uses diffusion models with compositional prompts, implying visual feature composition matters more than domain labels.
- **Break condition:** If target objects have unique imaging characteristics (e.g., specific medical imaging artifacts), generic visual similarity may be insufficient.

### Mechanism 3
- **Claim:** Transfer learning accelerates convergence and reduces training data requirements.
- **Mechanism:** Pre-trained weights initialize the network closer to a good solution basin, requiring fewer gradient updates to reach comparable performance.
- **Core assumption:** The loss landscape contains basins that are reachable from pre-trained initialization but distant from random initialization.
- **Evidence anchors:**
  - [abstract]: "models pre-trained on datasets with greater diversity and size consistently outperform those trained from scratch"
  - [section 3.2]: "pre-trained models learned faster and required less training time to achieve high F1-scores"
  - [corpus]: No direct corpus evidence on convergence speed; related papers focus on accuracy, not training dynamics.
- **Break condition:** If pre-training and fine-tuning tasks are adversarially dissimilar, transferred weights may create negative transfer, hurting convergence.

## Foundational Learning

- **Concept: Transfer Learning (Pre-training → Fine-tuning)**
  - **Why needed here:** Medical datasets are small (2,188 total polyp images); transfer learning compensates by importing learned features from larger source datasets.
  - **Quick check question:** Can you explain why fine-tuning uses a lower learning rate than pre-training?

- **Concept: YOLO Object Detection Format**
  - **Why needed here:** The paper converts segmentation masks to YOLO bounding box format (center x, center y, width, height normalized to 0-1).
  - **Quick check question:** Given a mask with min/max pixel coordinates, how would you compute the YOLO bounding box format?

- **Concept: F1-score and mAP**
  - **Why needed here:** F1 balances precision and recall; mAP evaluates detection quality across IoU thresholds—both critical for comparing transfer learning effectiveness.
  - **Quick check question:** Why might a high mAP@50-95 be more informative than mAP@50 alone for polyp detection?

## Architecture Onboarding

- **Component map:**
  YOLOv8n backbone -> Pre-training datasets (COCO, Fruit&Veg-XL, Brain Tumor-XL, HAM10000) -> Fine-tuning on combined polyp dataset -> Evaluation metrics (F1-score, mAP)

- **Critical path:**
  1. Pre-train on source dataset for 100 epochs → save weights
  2. Initialize YOLOv8n with pre-trained weights (`pretrained=true`)
  3. Fine-tune on combined polyp dataset (80/10/10 split)
  4. Evaluate on held-out test set

- **Design tradeoffs:**
  - **YOLOv8n vs larger variants:** Nano chosen for speed; larger variants (s/m/l/x) may improve accuracy but require more compute
  - **In-domain vs out-of-domain pre-training:** Medical datasets provide semantic relevance; visual similarity datasets (fruits) may provide better feature transfer
  - **Dataset combination strategy:** Combining small polyp datasets increases diversity but may introduce annotation inconsistencies

- **Failure signatures:**
  - Model performs well on validation but fails on external datasets → overfitting to combined polyp set characteristics
  - High precision, low recall → model conservative, may miss small/flat polyps
  - No improvement over scratch → pre-training dataset lacks relevant visual features

- **First 3 experiments:**
  1. **Baseline check:** Train YOLOv8n from scratch on combined polyp dataset; expect F1 ~92.9% (per paper)
  2. **COCO transfer:** Fine-tune Ultralytics pre-trained YOLOv8n-COCO; expect F1 ~94.5%
  3. **Visual similarity test:** Pre-train on Fruit&Veg-XL, fine-tune on polyps; expect F1 ~95.1% if visual similarity hypothesis holds

## Open Questions the Paper Calls Out

- **Open Question 1:** Does a progressive or sequential pre-training strategy (e.g., starting with COCO and continuing with a domain-specific dataset) provide better feature representations for polyp detection than single-source pre-training?
- **Open Question 2:** To what extent do the high F1-scores and mAP values observed in this study translate to clinical utility and real-world detection accuracy during live colonoscopy procedures?
- **Open Question 3:** How does the efficacy of transfer learning differ when applied to larger, higher-capacity YOLOv8 variants (s, m, l, x) or transformer-based architectures compared to the lightweight YOLOv8n?
- **Open Question 4:** Does the superior performance of the Fruit & Vegetable dataset hold when controlling for overfitting via k-fold cross-validation?

## Limitations

- The study uses a relatively small combined polyp dataset (2,188 images), which may not capture full clinical variability
- The paper doesn't explore potential negative transfer scenarios or the upper bounds of transfer learning effectiveness
- Model performance is not analyzed across different polyp sizes, types, or clinical difficulty levels
- The evaluation relies on static image datasets rather than dynamic video data from actual colonoscopy procedures

## Confidence

- **High Confidence:** The core finding that transfer learning improves polyp detection performance over training from scratch is well-supported by the experimental results (F1 improvement from 92.9% to 95.1%).
- **Medium Confidence:** The claim that visual similarity between fruits/vegetables and polyps drives the superior performance of the Fruit&Veg model requires further validation.
- **Medium Confidence:** The assertion that datasets with ≥10,000 images consistently yield better results is supported but based on a limited comparison set.

## Next Checks

1. **Generalization Test:** Evaluate the best pre-trained model on external polyp datasets not included in the combined training set to assess real-world generalization.
2. **Ablation Study:** Systematically compare the impact of different pre-training dataset characteristics (visual similarity, object count, domain relevance) to isolate which factors drive performance gains.
3. **Clinical Relevance Analysis:** Analyze model performance across polyp difficulty levels (size, texture, location) to identify potential failure modes in clinical practice.