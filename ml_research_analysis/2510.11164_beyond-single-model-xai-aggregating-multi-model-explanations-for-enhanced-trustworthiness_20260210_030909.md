---
ver: rpa2
title: 'Beyond single-model XAI: aggregating multi-model explanations for enhanced
  trustworthiness'
arxiv_id: '2510.11164'
source_url: https://arxiv.org/abs/2510.11164
tags:
- robustness
- explanations
- aggregation
- which
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an ensemble approach for explainable AI (XAI)
  that aggregates feature importance explanations from multiple model types (k-nearest
  neighbors, random forests, and neural networks) to increase the trustworthiness
  of AI systems. The key innovation is developing new feature attribution methods
  for k-NN and random forests that are comparable to existing neural network explanations
  like DeepLIFT.
---

# Beyond single-model XAI: aggregating multi-model explanations for enhanced trustworthiness

## Quick Facts
- arXiv ID: 2510.11164
- Source URL: https://arxiv.org/abs/2510.11164
- Authors: Ilaria Vascotto; Alex Rodriguez; Alessandro Bonaita; Luca Bortolussi
- Reference count: 15
- Key outcome: Ensemble approach aggregates feature importance explanations from multiple model types to increase AI trustworthiness, showing improved robustness scores on five tabular datasets

## Executive Summary
This paper proposes an ensemble approach for explainable AI (XAI) that aggregates feature importance explanations from multiple model types to increase the trustworthiness of AI systems. The key innovation is developing new feature attribution methods for k-NN and random forests that are comparable to existing neural network explanations like DeepLIFT. The aggregation is computed as a feature-wise arithmetic average of the normalized feature importance vectors from each model. Preliminary results on five tabular datasets show that the aggregated explanation achieves robustness scores between those of individual models while providing more trustworthy explanations.

## Method Summary
The paper introduces a multi-model XAI approach that creates ensemble explanations by aggregating feature importance scores from different model types. The method develops new feature attribution techniques for k-nearest neighbors and random forests that are compatible with neural network explanations. These individual explanations are normalized and combined through arithmetic averaging to produce a single aggregated explanation. Robustness is evaluated using a neighborhood-based metric that measures explanation stability under small perturbations. The approach aims to leverage complementary strengths of different model architectures while mitigating individual weaknesses.

## Key Results
- Aggregated explanations achieve robustness scores between individual model types on five tabular datasets
- The ensemble approach performs particularly well when all models agree on predictions
- Feature-wise arithmetic averaging of normalized importance vectors produces stable aggregated explanations
- New feature attribution methods enable fair comparison across k-NN, random forests, and neural networks

## Why This Works (Mechanism)
The aggregation mechanism works by combining complementary strengths of different model architectures. Individual models capture different aspects of feature relationships - neural networks learn complex non-linear interactions, random forests capture hierarchical decision boundaries, and k-NN models local neighborhood relationships. By normalizing and averaging these diverse perspectives, the ensemble explanation benefits from the robustness of simpler models while maintaining the nuanced insights of complex ones. The arithmetic averaging assumes equal contribution from all models, implicitly weighting their explanations based on their agreement patterns.

## Foundational Learning
- Feature importance normalization: Essential for combining explanations from models with different scales and distributions
- Neighborhood-based robustness metrics: Provide intuitive measure of explanation stability under small perturbations
- Cross-model explanation comparability: Required to ensure fair aggregation across different model architectures
- Ensemble averaging assumptions: Understanding when simple averaging is sufficient versus when weighted approaches are needed

## Architecture Onboarding
**Component Map:** Input data -> Individual model training (k-NN, RF, NN) -> Feature attribution (DeepLIFT for NN, custom methods for k-NN/RF) -> Normalization -> Arithmetic averaging -> Aggregated explanation -> Robustness evaluation

**Critical Path:** Data preprocessing -> Model training -> Feature attribution computation -> Normalization -> Aggregation -> Evaluation

**Design Tradeoffs:** Equal weighting vs. performance-based weighting of model contributions; arithmetic vs. geometric averaging; neighborhood size selection for robustness evaluation

**Failure Signatures:** Poor aggregation when models disagree significantly; instability when feature attribution methods produce inconsistent signs; sensitivity to normalization method choice

**First Experiments:** 1) Compare arithmetic vs. geometric averaging on a simple synthetic dataset; 2) Test different neighborhood sizes for robustness evaluation; 3) Evaluate aggregation with intentionally conflicting model predictions

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to five tabular datasets, restricting generalizability
- Neighborhood-based robustness metric lacks validation against established XAI stability measures
- Trustworthiness claims remain qualitative without human evaluation or ground truth comparison
- Equal weighting assumption may not reflect true contribution of individual models

## Confidence
- High confidence: Technical feasibility of aggregating normalized feature importance vectors across model types
- Medium confidence: Robustness improvement claims based on neighborhood perturbation analysis
- Low confidence: Assertion that aggregated explanations are inherently more trustworthy without human evaluation

## Next Checks
1. Conduct ablation studies testing different aggregation methods (weighted averages, median aggregation, or learned combinations) to determine optimal ensemble configurations
2. Perform human subject studies comparing user trust and understanding between single-model and aggregated explanations across diverse stakeholders
3. Validate the neighborhood robustness metric against established XAI stability measures like CLEVER score or integrated gradients variance under systematic perturbations