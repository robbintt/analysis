---
ver: rpa2
title: 'Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series
  Classification'
arxiv_id: '2502.15637'
source_url: https://arxiv.org/abs/2502.15637
tags:
- mantis
- time
- series
- foundation
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mantis is an open-source foundation model for time series classification
  based on a Vision Transformer architecture. It uses contrastive learning for pre-training
  on 7 million time series samples and achieves higher classification accuracy than
  existing models when frozen or fine-tuned, while maintaining superior calibration.
---

# Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series Classification

## Quick Facts
- **arXiv ID**: 2502.15637
- **Source URL**: https://arxiv.org/abs/2502.15637
- **Reference count**: 16
- **Primary result**: Open-source ViT-based foundation model for time series classification; outperforms baselines in accuracy and calibration using contrastive pre-training and lightweight channel adapters for multivariate data.

## Executive Summary
Mantis is an open-source foundation model for time series classification built on a Vision Transformer backbone. It leverages 7 million pre-training samples and contrastive learning to deliver strong out-of-the-box performance, achieving higher accuracy and better calibration than existing models when frozen or fine-tuned. The model handles multivariate time series efficiently via lightweight channel adapters (e.g., PCA, SVD, channel selection, or learnable linear combinations), reducing memory usage while capturing inter-channel interactions. Empirical results on 159 diverse datasets confirm Mantis's superiority in both accuracy and calibration, with minimal need for hyperparameter tuning.

## Method Summary
Mantis uses a ViT architecture with a Token Generator that processes time series into 32 patches via convolution, pooling, and differential features, followed by transformer layers for representation learning. Pre-training employs contrastive loss on 7M samples (UCR, UEA, ECG, EMG, Epilepsy, FD-A/B, Gesture, HAR, SleepEEG), with inputs resized to 512 and normalized per channel. For multivariate series, lightweight adapters compress channels to ≤10 dimensions before feeding to the model. Fine-tuning uses a classification head, AdamW optimizer (weight decay 0.05), cosine annealing, and grid-searched learning rates. The model is evaluated on 159 datasets, demonstrating superior accuracy and calibration (lower ECE) compared to baselines.

## Key Results
- Achieves higher classification accuracy than existing models when used frozen or fine-tuned.
- Delivers superior calibration (lower Expected Calibration Error) across datasets.
- Outperforms baselines on 159 diverse datasets, including high-dimensional multivariate time series.

## Why This Works (Mechanism)
Mantis works by leveraging large-scale contrastive pre-training to learn robust, general-purpose time series representations, which transfer effectively to downstream classification tasks. The Vision Transformer backbone, combined with a Token Generator that incorporates differential features and patch encodings, captures both local and global temporal patterns. Lightweight channel adapters enable efficient handling of multivariate data by compressing high-dimensional inputs without significant loss of discriminative information, preserving model scalability and performance.

## Foundational Learning
- **Contrastive Learning**: Trains the model to distinguish between similar and dissimilar time series samples, encouraging the learning of meaningful representations.
  - *Why needed*: Enables the model to generalize across diverse time series datasets without task-specific labels.
  - *Quick check*: Verify pre-training loss decreases and embeddings cluster by class in downstream tasks.

- **Vision Transformer (ViT)**: Uses self-attention over fixed patches of the input sequence to capture long-range dependencies.
  - *Why needed*: Allows modeling of complex temporal patterns and interactions within time series data.
  - *Quick check*: Ensure attention weights highlight relevant time segments during inference.

- **Instance-level Normalization**: Standardizes each channel of the input time series independently.
  - *Why needed*: Removes dataset-specific scaling biases, improving generalization.
  - *Quick check*: Confirm zero mean and unit variance per channel after preprocessing.

## Architecture Onboarding

**Component Map**
Token Generator (conv + pooling + diff features) -> Transformer layers (6 layers, 8 heads) -> Contrastive loss (pre-training) or Classification head (fine-tuning)

**Critical Path**
Input resizing (512) and normalization -> Token Generator (32 patches, 256 channels) -> Transformer encoding -> Adapter (for multivariate) -> Classification head (fine-tuning) or contrastive projection (pre-training)

**Design Tradeoffs**
- Fixed input length (512) simplifies batching but may lose fine-grained details in very long series; mitigated by interpolation.
- Lightweight adapters reduce memory and computational cost for multivariate data but may discard some information; trade-off between efficiency and fidelity.
- Contrastive pre-training improves generalization but requires large, diverse datasets and careful augmentation design.

**Failure Signatures**
- OOM errors on high-channel multivariate datasets without adapter use.
- Poor calibration or accuracy if instance normalization or differential features are omitted.
- Suboptimal performance if input resizing distorts temporal structure.

**First Experiments**
1. Verify Token Generator output dimensions and patch encoding for a simple univariate series.
2. Test adapter compression (e.g., PCA) on a high-dimensional multivariate dataset and check channel reduction.
3. Run a quick fine-tuning experiment on a small dataset to confirm classification head integration and learning rate schedule.

## Open Questions the Paper Calls Out
None

## Limitations
- Pre-training details (optimizer configuration, augmentation specifics) are underspecified, complicating exact reproduction.
- Adapter performance across all multivariate scenarios is not exhaustively validated; choice of adapter may impact results.
- Per-dataset baseline comparisons are not fully reported, limiting robustness assessment.

## Confidence
- **High**: Core architecture and reported accuracy/calibration gains, given systematic baseline comparisons.
- **Medium**: Scalability and usability of adapter-based approach for high-dimensional data, due to limited sensitivity analysis.
- **Low**: Reproducibility of exact pre-training results without further specification of augmentation and optimizer details.

## Next Checks
1. Validate input preprocessing pipeline: confirm instance-level normalization and length-512 resizing (with interpolation) are implemented as specified, and that differential features are correctly added in the Token Generator.
2. Benchmark adapter performance: systematically compare all five adapter options (PCA, SVD, random projection, variance selection, learned linear combination) on a representative set of high-dimensional multivariate datasets to assess robustness and identify optimal defaults.
3. Test calibration under distributional shift: evaluate Mantis’s calibration (ECE) on out-of-distribution or cross-domain time series datasets not seen during pre-training or fine-tuning, to confirm claims of superior calibration in practice.