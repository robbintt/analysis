---
ver: rpa2
title: A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named
  Entity Recognition for Historical Chinese Sources, 1900-1950
arxiv_id: '2503.19844'
source_url: https://arxiv.org/abs/2503.19844
tags:
- texts
- llms
- chinese
- accuracy
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares large language models (LLMs) and traditional
  NLP tools for Chinese word segmentation, POS tagging, and NER on historical texts
  from 1900-1950. Using a sample from the Shanghai Library Republican Journal corpus,
  the study finds that LLMs consistently outperform traditional tools across all tasks
  and metrics.
---

# A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950

## Quick Facts
- **arXiv ID**: 2503.19844
- **Source URL**: https://arxiv.org/abs/2503.19844
- **Reference count**: 6
- **Primary result**: LLMs outperform traditional tools for Chinese NLP on historical texts from 1900-1950, with GPT-4o achieving 91.97% F1 for segmentation versus Jieba's 81.72%

## Executive Summary
This study compares large language models (LLMs) with traditional NLP tools for Chinese word segmentation, part-of-speech tagging, and named entity recognition on historical texts from 1900-1950. Using a sample from the Shanghai Library Republican Journal corpus, the research finds that GPT-4o consistently outperforms traditional tools like Jieba across all tasks and metrics. The LLMs demonstrate superior handling of temporal variations and poetry, suggesting their contextual learning capabilities can advance historical Chinese NLP by reducing the need for domain-specific training data. However, this comes with substantial efficiency trade-offs, as GPT-4o required 796.61 seconds versus Jieba's 7.57 seconds for processing.

## Method Summary
The study employed a comparative approach using a 2,000-token sample from the Shanghai Library Republican Journal corpus. Three tasks were evaluated: word segmentation, part-of-speech tagging, and named entity recognition. Performance metrics included precision, recall, and F1 scores. GPT-4o was compared against the traditional Jieba tool for segmentation, while other standard NLP tools were evaluated for POS tagging and NER tasks. Computational efficiency was measured by processing time, with hardware specifications noted for the LLM evaluation.

## Key Results
- GPT-4o achieved an F1 score of 91.97% for segmentation versus Jieba's 81.72%
- LLMs showed better handling of poetry and temporal variations in historical texts
- GPT-4o required 796.61 seconds versus Jieba's 7.57 seconds, highlighting substantial efficiency trade-offs

## Why This Works (Mechanism)
The superior performance of LLMs stems from their contextual learning capabilities, which allow them to understand semantic relationships and historical language variations without requiring extensive domain-specific training data. Unlike traditional rule-based or statistical models that rely on fixed dictionaries and handcrafted features, LLMs can leverage their pre-trained knowledge to adapt to the linguistic characteristics of early 20th century Chinese texts, including variations in vocabulary, syntax, and stylistic elements that evolved during this period.

## Foundational Learning
- **Historical Chinese linguistics**: Understanding linguistic evolution in early 20th century China is crucial for evaluating NLP performance on these texts; quick check: familiarity with key linguistic changes during 1900-1950 period
- **Traditional Chinese NLP methods**: Knowledge of rule-based and statistical approaches helps contextualize LLM advantages; quick check: understanding of how tools like Jieba work
- **Large language model architecture**: Comprehension of transformer-based models explains their superior contextual understanding; quick check: familiarity with attention mechanisms and pretraining objectives
- **Evaluation metrics in NLP**: Understanding precision, recall, and F1 scores is essential for interpreting results; quick check: ability to explain the relationship between these metrics
- **Computational complexity analysis**: Recognizing the efficiency trade-offs requires understanding of algorithmic complexity; quick check: ability to compare time complexity of different approaches

## Architecture Onboarding
**Component Map**: Corpus -> Preprocessing -> Task Application -> Evaluation -> Analysis
**Critical Path**: Raw text → Tokenization → Task-specific processing → Metric calculation → Performance comparison
**Design Tradeoffs**: High accuracy (LLMs) vs. computational efficiency (traditional tools); domain adaptation vs. general applicability
**Failure Signatures**: Traditional tools struggle with temporal variations and poetry; LLMs face efficiency bottlenecks and potential context limitations
**First Experiments**: 1) Test additional historical corpora for generalizability; 2) Compare multiple LLM architectures; 3) Evaluate batch processing optimizations

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on a small 2,000-token sample from one specific corpus, potentially limiting generalizability
- Computational efficiency comparison only tested one hardware setup without exploring parallel processing optimizations
- Study focuses on GPT-4o specifically, with Medium confidence that results would generalize to other LLM architectures

## Confidence
- **High confidence** in relative performance comparison between GPT-4o and Jieba for tested corpus
- **Medium confidence** in generalizability across different historical Chinese text types
- **Medium confidence** in efficiency trade-off conclusions without broader hardware testing

## Next Checks
1. Test across multiple historical Chinese corpora from different regions and time periods within 1900-1950 to assess generalizability
2. Compare multiple LLM architectures (including open-source models) to establish whether GPT-4o's performance is representative
3. Conduct efficiency testing with batch processing and parallel computing configurations to identify potential optimizations for LLM deployment