---
ver: rpa2
title: 'HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding'
arxiv_id: '2506.09634'
source_url: https://arxiv.org/abs/2506.09634
tags:
- medical
- image
- visual
- spatial
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HSENet introduces a novel framework for 3D medical vision-language
  understanding that addresses the limitations of existing methods in capturing complex
  3D anatomical structures. The core idea involves dual-3D vision encoders for perceiving
  global volumetric contexts and fine-grained anatomical details, pretrained with
  a dual-stage alignment with diagnostic reports.
---

# HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding

## Quick Facts
- arXiv ID: 2506.09634
- Source URL: https://arxiv.org/abs/2506.09634
- Reference count: 40
- Primary result: State-of-the-art performance on 3D medical vision-language tasks with 39.85% R@100 on retrieval (+5.96%), 24.01% BLEU-4 on report generation (+8.01%), and 73.60% major class accuracy on VQA (+1.99%)

## Executive Summary
HSENet introduces a novel framework for 3D medical vision-language understanding that addresses the limitations of existing methods in capturing complex 3D anatomical structures. The core innovation involves dual-3D vision encoders for perceiving global volumetric contexts and fine-grained anatomical details, pretrained with a dual-stage alignment with diagnostic reports. A Spatial Packer efficiently projects high-resolution 3D spatial regions into compact visual tokens using voxel-to-point cross-attention, preserving essential spatial and geometric information. Experiments show state-of-the-art performance across three tasks: 3D language-visual retrieval, 3D medical report generation, and 3D visual question answering.

## Method Summary
HSENet employs a dual-stage pretraining approach followed by MLLM fine-tuning. In Stage 1, a 3D ViT encoder learns global volumetric representations via volume-report contrastive learning (InfoNCE). In Stage 2, a 2E3 encoder uses 2D slice features from BioMedCLIP to score and weight low-level 3D patches via cross-attention, processing them through transformer blocks while maintaining semantic consistency with Stage 1. The Spatial Packer compresses high-resolution 3D features into compact tokens using Voxel2Point Cross-Attention. The model is fine-tuned with Phi4-4B-Instruct LLM using LoRA for specific tasks. The framework processes CT volumes resized to (32, 256, 256) with 32 uniformly extracted 2D slices for guidance.

## Key Results
- 3D language-visual retrieval: 39.85% R@100 (+5.96% over prior work)
- 3D medical report generation: 24.01% BLEU-4 (+8.01% over prior work)
- 3D visual question answering: 73.60% major class accuracy (+1.99% over prior work)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual vision encoders capturing complementary global and local 3D features improve vision-language alignment over single-encoder approaches.
- Mechanism: The 3D Vision Encoder learns global volumetric representations via volume-report contrastive learning (InfoNCE). Separately, the 2E3 Vision Encoder uses 2D slice features (from BioMedCLIP) to score low-level 3D patches via cross-attention, weighting anatomically salient regions before processing through transformer blocks. The semantic consistency loss (Equation 4) anchors local learning to global representations from frozen Stage-1 encoders.
- Core assumption: 2D slice features from pretrained models contain diagnostic signals transferable to 3D patch importance estimation; low-level patches carry richer spatial cues than high-level features.
- Evidence anchors:
  - [abstract] "HSENet employs dual-3D vision encoders to perceive both global volumetric contexts and fine-grained anatomical details"
  - [section 4.2, Table 3] Combining dual encoders with Spatial Packer achieves 24.01% BLEU-4 vs. 17.93% (global only) or 20.66% (local only)
  - [corpus] Related work Med-2E3 also explores 2D-3D fusion but projects features directly from frozen encoders; HSENet decouples perception and projection
- Break condition: If 2D slice features lack correlation with 3D anatomical importance, patch scoring degrades to noise. Evidence: Table 1(b) shows text-based guidance (Full Text: R@5 1.61%) severely underperforms 2D slice guidance (R@5 5.82%).

### Mechanism 2
- Claim: Voxel2Point Cross-Attention preserves more spatial and geometric information during token compression than pooling or Q-Former alternatives.
- Mechanism: The Spatial Packer partitions 3D features into local voxels (stride-controlled), pools each voxel to a centroid point, then uses cross-attention where centroid queries attend to high-resolution voxel keys/values. This aggregates fine-grained spatial patterns into compact tokens rather than discarding them via pooling.
- Core assumption: Centroid-based queries can effectively summarize local voxel neighborhoods; the stride (Sd, Sw, Sh) controls the granularity-spatial-preservation tradeoff.
- Evidence anchors:
  - [section 3.3] "Voxel2Point Cross-Attention mechanism to inject enriched spatial clues from high-resolution VG_hr into low-dimensional VG_lr"
  - [section 4.3, Table 2b] Spatial Packer achieves BLEU-4 24.01% vs. Q-Former 17.11% vs. Spatial Pooling 20.66%
  - [corpus] Weak direct evidence; neighbor papers discuss visual encoding generally (e.g., "Unified Multimodal Understanding via Byte-Pair Visual Encoding") but not voxel-point attention specifically
- Break condition: Excessive compression (32 tokens) degrades performance (BLEU-4: 19.77%, Table 5); over-expansion (256 tokens) also hurts (BLEU-4: 21.59%) due to reduced voxel resolution impairing V2P-CA's capacity.

### Mechanism 3
- Claim: Semantic consistency regularization prevents representation drift during fine-grained local learning.
- Mechanism: Stage 2 trains the 2E3 encoder with L_SCL = L^{2e3}_CL + λ_s·L_SA (Equation 5), where L_SA constrains the 2E3-text similarity matrix to match the frozen Stage-1 encoder's similarity distribution.
- Core assumption: Global volume-report alignment from Stage 1 provides a stable anchor; unconstrained local optimization risks disrupting cross-modal correspondence.
- Evidence anchors:
  - [section 3.2] "semantic consistency loss L_SA that regularizes the cross-modal similarity matrix by anchoring it to the global alignment established in Stage 1"
  - [section 4.2, Table 1c] w/o L_SA: R@100 37.97%; with L_SA: 39.85%
  - [corpus] No direct corpus evidence; this is a relatively novel contribution
- Break condition: If λ_s is too high, local refinement is over-constrained; if too low, drift occurs. Paper uses λ_s = 0.1 without ablation on this value.

## Foundational Learning

- Concept: Contrastive Language-Image Pre-training (CLIP-style alignment)
  - Why needed here: Stage 1 pretraining aligns 3D volumes with diagnostic reports using InfoNCE loss; understanding symmetric contrastive objectives is essential.
  - Quick check question: Can you explain why InfoNCE maximizes mutual information between paired volume-report embeddings while pushing apart mismatched pairs?

- Concept: Cross-Attention for Feature Fusion
  - Why needed here: Used in both 2E3 encoder (2D slices → 3D patches) and Spatial Packer (voxels → points); understanding Q/K/V computation is critical.
  - Quick check question: Given query Q ∈ R^(S_d·S_w·S_h)×1×d_v and key K ∈ R^(S_d·S_w·S_h)×(D′·W′·H′)×d_v, what is the output shape after softmax(QK^T/√d)V?

- Concept: Low-Rank Adaptation (LoRA) for LLM Fine-tuning
  - Why needed here: The LLM decoder is fine-tuned with LoRA rather than full parameter updates; understanding efficient adaptation is needed for implementation.
  - Quick check question: If the original weight matrix is W_0 ∈ R^(d×k) and LoRA rank is r, what are the shapes of the low-rank matrices A and B such that W = W_0 + BA?

## Architecture Onboarding

- Component map: CT volume (32×256×256×1) + 32 2D slices -> Dual 3D ViT encoders (8×16×16 patches) -> Twin Spatial Packers (V2P-CA with strides 8×4×4) -> Phi4-4B-Instruct with LoRA -> LLM outputs

- Critical path:
  1. Preprocessing: Min-Max normalization → resize to 32×256×256
  2. Dual encoding: Parallel extraction of V^G_3d (global) and V^L_3d (local)
  3. Spatial packing: V2P-CA compression → F^G_3d and F^L_3d (each N′_p = 128 tokens)
  4. Multi-modal prompting: Concatenate [F^G_3d, F^L_3d, task instruction] → LLM
  5. Generation: Cross-entropy loss over M output tokens

- Design tradeoffs:
  - Token count (32/64/128/256): 128 optimal; fewer tokens lose clinical relevance, more tokens reduce voxel resolution (Table 5)
  - Stride (S_d, S_w, S_h): Controls compression ratio vs. spatial detail preservation
  - λ_s (semantic consistency weight): Set to 0.1; higher values may over-constrain local learning
  - Frozen vs. trainable: Stage-1 encoders frozen during Stage-2; LLM uses LoRA (trainable layers only)

- Failure signatures:
  - Low retrieval R@5 (<3%): Likely patch scoring using text features instead of 2D slices (Table 1b)
  - BLEU-4 degradation with more tokens: Voxel resolution too small for V2P-CA (Table 5, 256 tokens)
  - Missing anatomical structures in reports: Using single encoder instead of dual; global encoder misses fine details (Figure 4)
  - Hallucinated diagnoses: Spatial Packer replaced with Q-Former or pooling (Table 2b)

- First 3 experiments:
  1. **Ablate patch scoring guidance**: Compare 2D slices vs. Full Text vs. Text CLS (Table 1b). Expect 2D slices to achieve ~2-3× higher R@5.
  2. **Vary Spatial Packer token count**: Test 32/64/128/256 tokens (Table 5). Expect BLEU-4 peak at 128.
  3. **Compare projector architectures**: Swap Spatial Packer for Q-Former/Sequence Pooling/Spatial Pooling (Table 2b). Expect 3-7% BLEU-4 degradation with alternatives.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-modal, multi-source clinical data (e.g., electronic health records, patient history) be effectively integrated into the pretraining or fine-tuning stages of 3D medical MLLMs to enhance diagnostic reliability?
- Basis in paper: [explicit] The "Limitations" section explicitly identifies the omission of diverse contextual clinical data as a potential cause for suboptimal real-world performance and calls for its integration as a key future direction.
- Why unresolved: The current framework relies solely on volume-report pairs, which isolates visual findings from the rich contextual history clinicians use for diagnosis.
- What evidence would resolve it: Comparative results on diagnostic tasks where models are trained with and without integrated EHR contexts, specifically measuring reductions in factual errors.

### Open Question 2
- Question: Can adaptive voxel partitioning strategies replace the uniform strides in the Spatial Packer to improve performance on structurally irregular anatomical regions like bones?
- Basis in paper: [explicit] Appendix C notes that HSENet underperforms on irregular skeletal structures compared to stable regions (heart/lung) and attributes this to the "uniform voxel partitioning" used in the spatial packer.
- Why unresolved: Fixed strides (e.g., 8x4x4) limit the model's adaptability to high anatomical variance, impairing the encoding of complex bone structures.
- What evidence would resolve it: Experiments evaluating F1 scores on bone-related VQA tasks using a variable-stride or attention-based adaptive partitioning mechanism.

### Open Question 3
- Question: Does integrating self-reconstruction objectives with vision-language alignment improve the robustness of 3D visual representation learning in low-data regimes?
- Basis in paper: [explicit] Appendix B observes that E3D-GPT (using self-reconstruction) outperformed other baselines on the smaller BIMCV-R dataset, leading the authors to suggest "integrating self-reconstruction with vision-language alignment" as a promising future direction.
- Why unresolved: It is unclear if the pixel-level fidelity from reconstruction can complement the semantic alignment of contrastive learning without conflicting optimization goals.
- What evidence would resolve it: Ablation studies on a hybrid pre-training objective evaluated across datasets of varying sizes (e.g., CT-RATE vs. BIMCV-R).

## Limitations
- Performance on irregular anatomical structures like bones is suboptimal due to uniform voxel partitioning
- The framework's effectiveness on other 3D medical imaging modalities (MRI, ultrasound) remains untested
- Dual encoding and Spatial Packer introduce significant computational overhead requiring 8× RTX 3090 GPUs

## Confidence

- **Low-Medium** for dual-encoder superiority: Clear performance gains shown but lacks direct comparison with other 2D-3D fusion approaches
- **Medium** for Voxel2Point Cross-Attention advantages: Performance gains demonstrated but limited theoretical justification for centroid-based queries
- **Medium** for semantic consistency regularization: Modest gains shown but regularization hyperparameter set without systematic tuning

## Next Checks

1. **Test patch scoring sensitivity**: Systematically vary the number of 2D slices (N_s) and their selection strategy to quantify how sensitive the 2E3 encoder's performance is to 2D guidance quality.

2. **Evaluate cross-modal alignment quality**: Visualize and quantify the alignment between 2D slice features and 3D patch importance scores across different anatomical regions to validate the core assumption about 2D-3D feature correspondence.

3. **Stress test V2P-CA compression**: Conduct a more granular analysis of token count vs. performance trade-offs and test V2P-CA on pathological cases with fine-grained anatomical details to verify it doesn't miss clinically relevant structures.