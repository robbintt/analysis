---
ver: rpa2
title: Evidence of conceptual mastery in the application of rules by Large Language
  Models
arxiv_id: '2503.00992'
source_url: https://arxiv.org/abs/2503.00992
tags:
- llms
- rule
- text
- human
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study uses psychological methods to investigate whether Large
  Language Models (LLMs) have mastered the concept of rule application. The researchers
  conducted two experiments comparing rule-based decision-making in humans and LLMs,
  addressing the concern that LLMs might simply be reproducing memorized patterns
  from their training data.
---

# Evidence of conceptual mastery in the application of rules by Large Language Models

## Quick Facts
- arXiv ID: 2503.00992
- Source URL: https://arxiv.org/abs/2503.00992
- Authors: José Luiz Nunes; Guilherme FCF Almeida; Brian Flanagan
- Reference count: 6
- Key outcome: LLMs replicate human patterns in rule violation judgments for both training data and novel scenarios, with some models showing human-like time pressure effects.

## Executive Summary
This study investigates whether Large Language Models demonstrate conceptual mastery of rule application by replicating human decision-making patterns in rule violation scenarios. Using psychological experimental methods, researchers compared human and LLM responses across two studies examining scenario novelty and time pressure effects. The findings show LLMs can generalize rule application beyond memorized patterns to novel situations, replicating human-like response diversity and sensitivity to contextual factors. These results suggest LLMs have developed conceptual understanding of rules rather than simply pattern-matching from training data, with implications for legal reasoning and philosophical questions about artificial intelligence.

## Method Summary
The study employed two psychological experiments comparing human and LLM rule violation judgments. Study 1 examined whether LLMs could apply rules to novel scenarios not present in training data, using 115 human participants and four LLMs (GPT-4o, Llama 3.2 90B, Claude 3 Opus, Gemini 1.5 Pro) across 16 vignettes with text/purpose alignment variations. Temperature calibration was performed to match human response variance through grid search optimization. Study 2 investigated time pressure effects on rule application, manipulating time constraints known to affect human decision-making. Both studies used mixed-effects models with fixed effects for text, purpose, and time pressure, and random effects for participants and scenarios.

## Key Results
- LLMs replicated human patterns in rule violation judgments regardless of whether scenarios were from training data or newly created
- Temperature calibration successfully matched human response diversity, with optimal temperatures ranging from 0.4 to 1.8 across models
- Study 2 showed inconsistent time pressure effects: Gemini Pro and Claude 3 responded like humans, while GPT-4o and Llama 3.2 did not
- All LLMs showed similar patterns to humans in Study 1, though with less variance in responses

## Why This Works (Mechanism)
The mechanism underlying LLM rule application appears to involve abstraction beyond pattern matching, enabling generalization to novel scenarios. The temperature calibration approach allows models to replicate human response diversity by adjusting stochasticity levels. Time pressure effects suggest some models can simulate human cognitive constraints or contextual sensitivity. However, the internal operations remain opaque, and it's unclear whether LLMs achieve conceptual mastery through mechanisms similar to or distinct from human cognitive processes.

## Foundational Learning
- Temperature calibration: Why needed - to match human response diversity; Quick check - verify per-cell SD computation across all experimental cells
- Mixed-effects modeling: Why needed - to account for hierarchical data structure (participants within scenarios); Quick check - confirm random effects specification includes both participant and scenario levels
- Psychological experimental design: Why needed - to create valid comparisons between human and AI reasoning; Quick check - ensure stimuli manipulations are properly counterbalanced

## Architecture Onboarding

**Component Map:**
Temperature calibration -> Response generation -> Mixed-effects modeling -> Statistical analysis

**Critical Path:**
Temperature calibration → LLM response generation at calibrated temperatures → Human response collection → Statistical comparison of patterns

**Design Tradeoffs:**
- Temperature vs. response quality: Higher temperatures increase variance but may reduce coherence
- Model selection: Different architectures show varying degrees of human-like behavior
- Justification requirements: Including vs. excluding reasoning steps affects textualism vs. purposivism

**Failure Signatures:**
- Invalid responses outside Likert scale (62 GPT-4o instances discarded)
- Temperature calibration yielding different optimal values across runs
- Inconsistent time pressure effects across models

**First Experiments:**
1. Run temperature calibration grid search with 10 values across all 32 experimental cells
2. Generate 240 responses per model at calibrated temperatures and verify format compliance
3. Fit mixed-effects models with full factorial design including all interactions

## Open Questions the Paper Calls Out
- Do LLMs generalize from training data to new cases using mechanisms similar to or distinct from human cognitive processes? [explicit] The authors explicitly ask, "But how exactly do LLMs succeed in abstracting from their training data to new cases? And is it similar or dissimilar to the way humans perform this same task?" This remains unresolved because internal operations of both biological and artificial neural networks are opaque.
- Does sensitivity to time pressure in LLMs represent genuine conceptual mastery or merely a simulation of human cognitive limitations? [explicit] The authors state it is an "open question whether the time sensitive human application of rules is a reflection of human conceptual competence or cognitive limitation," making it unclear if LLMs mimicking this behavior (Gemini/Claude) or ignoring it (GPT/Llama) are superior.
- Does the inclusion of Chain of Thought (CoT) prompting or justification requirements induce higher levels of textualism in LLM rule application? [inferred] The authors speculate that omitting the justification writing step in Study 2 might have removed an analytical prompt that typically induces textualism in humans, suggesting CoT could alter results.

## Limitations
- Temperature calibration may not fully capture the relationship between model stochasticity and human response variance
- Study generalizability is limited to rule violations in specific contexts (contracts, proposals, petitions, announcements)
- GPT-4o showed systematic issues with 62 discarded responses out of 240, suggesting potential model-specific problems

## Confidence
- **High confidence**: LLMs can generalize rule application to novel scenarios beyond training data
- **Medium confidence**: Temperature calibration effectively matches human response diversity
- **Medium confidence**: Time pressure effects on LLM rule application mirror human patterns (inconsistent across models)

## Next Checks
1. Replicate temperature calibration across different rule domains (e.g., social norms, safety rules) to test generalizability
2. Conduct ablation studies varying temperature ranges and grid search resolution to establish sensitivity and stability
3. Test additional models with known architectural differences to determine whether observed effects are model-specific or general to LLMs