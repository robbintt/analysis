---
ver: rpa2
title: 'Multimodal Carotid Risk Stratification with Large Vision-Language Models:
  Benchmarking, Fine-Tuning, and Clinical Insights'
arxiv_id: '2510.02922'
source_url: https://arxiv.org/abs/2510.02922
tags:
- arxiv
- risk
- data
- lvlms
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates state-of-the-art Large Vision-Language Models
  (LVLMs) for carotid plaque risk stratification using ultrasound imaging and multimodal
  clinical data. General-purpose and medically tuned models were tested in zero-shot
  settings, showing that while some could identify imaging modality and anatomy, none
  performed well on clinical risk classification without adaptation.
---

# Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights

## Quick Facts
- arXiv ID: 2510.02922
- Source URL: https://arxiv.org/abs/2510.02922
- Reference count: 40
- State-of-the-art Large Vision-Language Models (LVLMs) evaluated for carotid plaque risk stratification; fine-tuning with multimodal clinical data improves performance.

## Executive Summary
This study benchmarks Large Vision-Language Models (LVLMs) for multimodal carotid plaque risk stratification using ultrasound imaging and clinical metadata. General-purpose and medically tuned models were tested in zero-shot settings, revealing that while some models could identify imaging modality and anatomy, none performed well on clinical risk classification without adaptation. Fine-tuning LLaVa-NeXT-Vicuna via LoRA improved risk stratification performance, particularly when multimodal tabular data were included, achieving competitive AUC compared to prior CNN baselines. Specificity remained challenging due to dataset imbalance, but the integration of clinical metadata consistently enhanced balanced accuracy. The results demonstrate the potential of adapted LVLMs to leverage multimodal data for cardiovascular risk prediction, while also highlighting the need for improved calibration, larger datasets, and temporal modeling to ensure reliable clinical deployment.

## Method Summary
The study evaluated several Large Vision-Language Models (LVLMs) on carotid plaque risk stratification using ultrasound images and clinical metadata. General-purpose and medically tuned models were first tested in zero-shot settings. Fine-tuning was then performed on LLaVa-NeXT-Vicuna using LoRA, with and without multimodal tabular data. Model performance was assessed using AUC, balanced accuracy, and calibration metrics, with Grad-CAM used for explainability. Comparisons were made to prior CNN baselines.

## Key Results
- Fine-tuned LLaVa-NeXT-Vicuna with multimodal tabular data achieved competitive AUC compared to prior CNN baselines.
- Inclusion of clinical metadata consistently improved balanced accuracy, though specificity remained low due to dataset imbalance.
- General-purpose LVLMs performed poorly on clinical risk classification without fine-tuning, despite some ability to identify imaging modality and anatomy.

## Why This Works (Mechanism)
Fine-tuning enables LVLMs to learn task-specific representations by adapting their weights to clinical data, allowing them to extract meaningful features from both imaging and structured clinical inputs. Multimodal integration allows the model to jointly reason over visual and tabular information, improving risk stratification by leveraging complementary data sources. LoRA-based fine-tuning offers an efficient way to adapt large models without full retraining, making it practical for clinical deployment.

## Foundational Learning
- **Multimodal Risk Stratification**: Why needed—combines imaging and clinical metadata for richer risk assessment; Quick check—verify model inputs include both ultrasound images and structured clinical data.
- **Fine-Tuning LVLMs**: Why needed—general models lack domain-specific reasoning for clinical risk; Quick check—confirm fine-tuning uses LoRA or similar parameter-efficient methods.
- **Calibration of Clinical Models**: Why needed—probability outputs must reflect true risk for clinical trust; Quick check—inspect calibration curves or Brier scores post-training.

## Architecture Onboarding
**Component Map**: Ultrasound image + Clinical metadata -> Vision encoder + Tabular encoder -> Fusion module -> LoRA-adapted LLM head -> Risk classification
**Critical Path**: Data preprocessing → Multimodal encoding → LoRA fine-tuning → Inference & calibration
**Design Tradeoffs**: Zero-shot vs. fine-tuning—zero-shot is fast but underperforms; fine-tuning is accurate but requires data and compute; LoRA balances efficiency and performance.
**Failure Signatures**: Poor specificity from class imbalance; miscalibration in zero-shot settings; inability to capture temporal disease progression without longitudinal data.
**First Experiments**: 1) Zero-shot evaluation on held-out clinical risk labels; 2) Fine-tuning ablation with/without tabular data; 3) Calibration assessment via reliability diagrams.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited dataset size constrained model performance and evaluation of rare high-risk cases.
- Absence of temporal data from serial scans limited ability to model disease progression.
- Calibration issues in zero-shot scenarios may impact clinical decision-making reliability.

## Confidence
- High confidence that general-purpose LVLMs perform poorly on clinical risk stratification without adaptation.
- Medium confidence that fine-tuned LLaVa-NeXT-Vicuna achieves competitive AUC compared to CNN baselines.
- Medium confidence that inclusion of multimodal tabular data improves balanced accuracy, though the extent may vary with dataset composition.

## Next Checks
1. External validation on larger, multi-site datasets to assess model robustness and generalizability across diverse populations and imaging equipment.
2. Prospective study incorporating serial ultrasound scans to evaluate temporal modeling and longitudinal risk prediction capabilities.
3. Calibration and reliability testing in clinical environments to ensure probability outputs are trustworthy for decision support.