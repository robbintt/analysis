---
ver: rpa2
title: A Comparative Study of Controllability, Explainability, and Performance in
  Dysfluency Detection Models
arxiv_id: '2509.00058'
source_url: https://arxiv.org/abs/2509.00058
tags:
- clinical
- dysfluency
- detection
- performance
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic comparison of four dysfluency
  detection models across performance, controllability, and explainability dimensions.
  The authors evaluate YOLO-Stutter, FluentNet, UDM, and SSDM using the UClass benchmark
  framework.
---

# A Comparative Study of Controllability, Explainability, and Performance in Dysfluency Detection Models

## Quick Facts
- arXiv ID: 2509.00058
- Source URL: https://arxiv.org/abs/2509.00058
- Reference count: 14
- Primary result: UDM achieves best balance of performance (0.89 F1), controllability (4.0/5), and explainability (4.2/5) for clinical deployment

## Executive Summary
This study systematically evaluates four dysfluency detection models (YOLO-Stutter, FluentNet, UDM, SSDM) across performance, controllability, and explainability dimensions using the UClass benchmark framework. UDM demonstrates superior overall performance with 0.89 F1-score and highest clinical interpretability ratings, while YOLO-Stutter excels in computational efficiency (0.05 real-time factor) but scores poorly on explainability. SSDM could not be reproduced despite multiple attempts. The research reveals fundamental trade-offs between computational efficiency and clinical interpretability, with UDM providing the best balance for clinical deployment scenarios.

## Method Summary
The authors conducted a comprehensive evaluation using the UClass benchmark framework, systematically comparing four dysfluency detection models across multiple dimensions. They measured performance using standard F1-score metrics, assessed computational efficiency through real-time factor calculations, and evaluated clinical interpretability through expert ratings for both controllability and explainability. The study employed standardized testing protocols and attempted reproducibility across all models, though SSDM proved unreproducible despite multiple implementation attempts.

## Key Results
- UDM achieved highest overall performance with 0.89 F1-score and superior clinical interpretability (4.0/5 controllability, 4.2/5 explainability)
- YOLO-Stutter demonstrated best computational efficiency at 0.05 real-time factor but scored lowest on explainability (2.3/5)
- SSDM model could not be reproduced despite multiple implementation attempts
- Study confirms fundamental trade-off between computational efficiency and clinical interpretability

## Why This Works (Mechanism)
The superior performance of UDM stems from its balanced architecture that integrates both deep learning feature extraction with interpretable attention mechanisms. The model's design allows for accurate detection while maintaining transparency in decision-making processes. The controllability mechanisms enable clinicians to adjust detection thresholds and interpret model reasoning, while the explainability features provide clear visualization of which acoustic and linguistic features contribute to dysfluency identification.

## Foundational Learning
- Dysfluency detection fundamentals: Understanding types of speech disfluencies (repetitions, prolongations, blocks) is essential for model development and evaluation
  - Why needed: Provides context for what models should detect and how to measure success
  - Quick check: Can identify and categorize different types of speech dysfluencies

- Clinical interpretability metrics: Familiarity with clinical evaluation frameworks and rating scales for assessing model transparency
  - Why needed: Enables proper assessment of model utility in healthcare settings
  - Quick check: Understand difference between controllability and explainability in clinical contexts

- Computational efficiency measurement: Knowledge of real-time factor calculations and their impact on clinical deployment
  - Why needed: Critical for determining practical usability in real-world clinical settings
  - Quick check: Can calculate and interpret real-time factor from processing times

## Architecture Onboarding

Component map: Input audio -> Feature extraction -> Detection model -> Output classification -> Interpretability layer

Critical path: The detection model architecture, particularly for UDM, represents the most critical component. It must balance deep learning complexity with interpretability requirements while maintaining computational efficiency for real-time operation.

Design tradeoffs: The fundamental tension between deep learning performance and interpretability requires careful architectural decisions. Models must either sacrifice some accuracy for transparency or develop hybrid approaches that maintain both. Computational efficiency constraints further complicate this balance, as more interpretable architectures often require additional processing overhead.

Failure signatures: Common failure modes include overfitting to training data, poor generalization to diverse speaker populations, and loss of interpretability as model complexity increases. SSDM's irreproducibility highlights the importance of implementation transparency and proper documentation.

3 first experiments:
1. Reproduce baseline UDM implementation on standard benchmark dataset to verify claimed performance metrics
2. Conduct ablation study removing interpretability components to quantify performance trade-offs
3. Test model performance across diverse speaker populations to assess generalizability limitations

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- SSDM reproducibility issues raise concerns about implementation transparency across evaluated models
- Single dataset evaluation limits generalizability to diverse clinical populations and real-world conditions
- Clinical interpretability scores rely on subjective expert ratings without detailed validation methodology
- Trade-off analysis lacks quantitative metrics for measuring clinical utility in practical deployment scenarios

## Confidence
- UDM performance claims: High confidence - based on systematic benchmark evaluation with clear metrics
- Computational efficiency comparisons: Medium confidence - limited to specific hardware configurations without broader context
- Clinical interpretability ratings: Medium confidence - subjective expert assessments lack methodological transparency
- Generalizability of findings: Low confidence - single dataset evaluation may not capture real-world variability

## Next Checks
1. Conduct cross-dataset validation using multiple dysfluency corpora to assess model generalizability across different speaker populations and recording conditions
2. Implement blinded clinical trials comparing model outputs against expert human annotations in real-time clinical settings to validate practical utility claims
3. Perform ablation studies on UDM to isolate which architectural components contribute most to the observed trade-offs between performance, controllability, and explainability