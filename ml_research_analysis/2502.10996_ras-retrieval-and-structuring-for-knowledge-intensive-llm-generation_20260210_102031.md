---
ver: rpa2
title: 'RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation'
arxiv_id: '2502.10996'
source_url: https://arxiv.org/abs/2502.10996
tags:
- question
- retrieval
- graph
- knowledge
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAS improves knowledge-intensive LLM reasoning by iteratively building
  question-specific knowledge graphs through targeted retrieval and structured knowledge
  extraction. Unlike static global graphs or unstructured retrieval, RAS dynamically
  constructs and reasons over evolving, query-relevant graphs tailored to each question.
---

# RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation

## Quick Facts
- arXiv ID: 2502.10996
- Source URL: https://arxiv.org/abs/2502.10996
- Reference count: 40
- Key outcome: RAS improves knowledge-intensive LLM reasoning by iteratively building question-specific knowledge graphs through targeted retrieval and structured knowledge extraction.

## Executive Summary
RAS (Retrieval-And-Structuring) addresses knowledge-intensive LLM reasoning by dynamically constructing question-specific knowledge graphs through iterative retrieval and structured extraction. Unlike static global knowledge graphs or unstructured retrieval, RAS builds and reasons over evolving, query-relevant graphs tailored to each question. The framework achieves up to 8.7% and 7.0% gains with proprietary and open-source LLMs respectively on seven benchmarks spanning QA and long-form generation, outperforming strong baselines including Self-RAG and SuRe.

## Method Summary
RAS is a knowledge-intensive LLM framework that iteratively constructs question-specific knowledge graphs for reasoning. The system uses a unified multitask-trained model that plans retrieval and generates answers, leveraging lightweight triple extraction and graph encoding. The framework operates through planning, retrieval, structuring (text-to-triples), and answering stages, with a maximum of 5 retrieval iterations. RAS is trained on a multitask objective combining planning and answering using HotpotQA-SUBQ (derived from HotpotQA + Arc-Easy + ASQA), with text-to-triples model trained on WikiOFGraph. The Graph Transformer encoder projects the evolving knowledge graph into tokens the LLM can attend to, with LoRA adapters for efficient training.

## Key Results
- Achieves up to 8.7% accuracy improvement on TriviaQA and 7.0% F1 gain on 2WikiMultihopQA compared to strong baselines
- Demonstrates consistent gains across seven benchmarks including long-form generation (ASQA, ELI5) with 22.2 MAUVE improvement
- Ablation studies show removing graph encoding causes -8.8% to -22.2% degradation, confirming the importance of structured knowledge representation

## Why This Works (Mechanism)

### Mechanism 1: Structured knowledge representation reduces reasoning failures
Converting retrieved passages into subject-predicate-object triples creates an explicit intermediate reasoning scaffold. The model attends to discrete relational facts rather than implicitly bridging gaps in unstructured text. Hallucinations in multi-step reasoning stem primarily from failures to correctly chain facts across unstructured context. Evidence shows removing text-to-triple extraction causes -9.0% F1 on 2WQA and -22.2 MAUVE on ASQA. Break condition: poor triple extraction quality (missing key relations, spurious facts) misleads rather than helps.

### Mechanism 2: Question-specific graph construction reduces noise and ambiguity
Building graphs on-demand from retrieved passages grounds relations in a coherent, query-relevant context. This avoids the ambiguity of global KGs that encode conflicting or irrelevant associations from diverse documents. A global KG might simultaneously encode that Geoffrey Hinton is linked to deep learning, to cognitive neuroscience, and to critiques of large models—without clarifying which aspect is relevant. Break condition: if retrieval returns unfocused or contradictory passages, the question-specific graph inherits these problems.

### Mechanism 3: Iterative planning enables targeted knowledge gap filling
The planning module evaluates accumulated knowledge against the question, generating focused subqueries that retrieve missing information. This creates a goal-directed search rather than passive context accumulation. The planning model can accurately assess knowledge sufficiency and generate productive subqueries. Evidence shows "No Planning" variant degrades accuracy by -8.8% on TQA and -9.0% F1 on 2WQA. Break condition: if the planning model misjudges sufficiency (either stopping too early or pursuing redundant subqueries), the system underperforms.

## Foundational Learning

- **Concept: Knowledge graphs and triple representation**
  - Why needed here: RAS converts all retrieved text to (subject, predicate, object) triples that form a graph. Understanding nodes, edges, and semantic embedding of graph elements is essential for following the structuring pipeline.
  - Quick check question: Given "Marie Curie discovered radium in 1898," can you write it as a triple? (S> Marie Curie | P> discovered | O> radium)

- **Concept: Dense retrieval and passage-based RAG**
  - Why needed here: RAS builds on standard RAG infrastructure—dense retrievers (Contriever), FAISS indexing, top-k passage selection. The planning module controls when retrieval happens.
  - Quick check question: How does a dense retriever differ from BM25? When would you prefer one over the other?

- **Concept: Graph Neural Networks for encoding structured data**
  - Why needed here: RAS uses a Graph Transformer to encode the evolving question-specific KG, projecting it into tokens the LLM can attend to. Understanding message passing and graph-level pooling is helpful.
  - Quick check question: What does a GNN layer compute? How does it differ from processing a flat sequence?

## Architecture Onboarding

- **Component map:**
  Input Question Q → [Planning Module] → [NO_RETRIEVAL] → Direct Answer
  ↓ [SUBQ] qi → [Retrieval] → Retrieved passages ti → [Text-to-Triples ft2t] → Triples gi
  ↓ [Graph Construction] → Subgraph g'i → Merge into GQ → [GNN Encoder] → Graph tokens
  ↓ [Planning Module] ← Graph tokens + history [(qi, gi)] → [SUFFICIENT]
  ↓ [Answering Module] → Final Answer A

- **Critical path:**
  1. Text-to-triples quality (ft2t): Table 3 shows triple extractor choice affects final accuracy by 2-3%. LLaMA-3.2-3B balances quality and throughput.
  2. Planning loop termination: Must correctly emit [SUFFICIENT] or [SUBQ]. Training data has 35% [SUFFICIENT], 55% [SUBQ], 10% [NO_RETRIEVAL].
  3. Graph encoding during training: Ablation shows removing graph encoding during training hurts all metrics; LoRA + multitask learning is critical.

- **Design tradeoffs:**
  - Question-specific vs. global KG: Zero offline cost vs. potential incompleteness per query
  - Iterative vs. single-pass: Higher latency and retrieval cost vs. targeted evidence gathering
  - Unified vs. separate models: Shared representations vs. task-specific optimization
  - Graph encoder complexity: Graph Transformer (chosen) handles disconnected subgraphs better than GCN/GAT

- **Failure signatures:**
  - **Over-retrieval**: Planning keeps generating [SUBQ] without reaching [SUFFICIENT]; check max iteration limit (default 5)
  - **Under-retrieval**: Planning emits [SUFFICIENT] prematurely; check if training data has sufficient [SUBQ] examples
  - **Triple noise**: Extracted triples contain irrelevant or wrong relations; audit ft2t outputs on sample passages
  - **Graph encoding failure**: Model ignores structured knowledge; verify GNN projector is trained (check LoRA adapters loaded)

- **First 3 experiments:**
  1. **Validate text-to-triples quality**: Run ft2t on 50 retrieved passages from your corpus. Manually check triple precision/recall. If >20% of triples are spurious, consider retraining with domain data.
  2. **Test planning convergence**: Run RAS on 100 questions from a validation set. Log iteration counts and final decisions ([SUFFICIENT] vs. max-iter-exceeded). If >30% hit max iterations, adjust planning prompts or training data balance.
  3. **Ablate graph encoding**: Compare full RAS vs. text-only baseline (skip ft2t, pass raw passages). On 2-3 datasets, expect 5-10% degradation if graph encoding is working. If no difference, check GNN integration.

## Open Questions the Paper Calls Out

- **How would integrating established external knowledge graphs (e.g., Wikidata, domain-specific KGs) into RAS's iterative retrieval loop affect reasoning accuracy compared to purely dynamic graph construction?**
  - Basis: Appendix A states integration of external knowledge graphs into RAS's iterative loop is a promising direction.
  - Why unresolved: Only evaluates RAS with dynamically constructed graphs from retrieved text, not hybrid approaches.
  - What evidence would resolve it: Comparative experiments on benchmarks where RAS uses both dynamically constructed subgraphs and relevant portions of Wikidata/domain KGs.

- **How does RAS performance generalize to specialized domains (e.g., legal, medical, financial) and multilingual settings beyond the evaluated Wikipedia-based benchmarks?**
  - Basis: Appendix A states effectiveness across highly specialized domains or multilingual settings remains to be investigated.
  - Why unresolved: All seven evaluation benchmarks draw from Wikipedia-based sources; no domain-specific or non-English corpora were tested.
  - What evidence would resolve it: Evaluation on specialized benchmarks (e.g., legal case reasoning, clinical QA) and multilingual datasets with appropriate retrieval corpora.

- **Would incorporating sophisticated graph pruning and dynamic summarization techniques improve RAS's efficiency and accuracy during iterative knowledge enrichment?**
  - Basis: Appendix A states current approach could be enhanced by sophisticated graph pruning, merging strategies, and dynamic graph summarization techniques.
  - Why unresolved: Current implementation uses simple graph union operations without pruning redundant or low-relevance nodes/edges during iteration.
  - What evidence would resolve it: Ablation experiments comparing current graph union against pruning strategies on reasoning accuracy and graph size at convergence.

## Limitations
- Does not evaluate robustness to retrieval noise—if initial passages are unfocused or contradictory, the question-specific graph may inherit these problems.
- Unified multitask training approach means it's unclear whether separate specialized models for planning and answering might outperform the shared architecture.
- Performance benefits are benchmark-specific—large gains on multi-hop QA and long-form generation but only marginal improvements on some commonsense reasoning tasks.

## Confidence
- **High confidence**: Iterative knowledge graph construction mechanism is well-supported by ablation studies showing -8.8% to -22.2% degradation when removing key components.
- **Medium confidence**: Claims about interpretability and hallucination reduction are supported by qualitative case analyses but lack systematic measurement across the full evaluation suite.
- **Medium confidence**: Unified multitask training approach shows strong results, but the paper doesn't compare against separate specialized models for planning and answering.

## Next Checks
1. **Retrieval robustness test**: Run RAS on datasets where retrieval is deliberately made noisy (e.g., by mixing in irrelevant passages) and measure degradation. Compare this to static KG baselines to quantify the noise tolerance advantage of question-specific graphs.
2. **Planning sufficiency analysis**: Instrument the planning module to log its confidence scores and compare predicted sufficiency against human judgments. Identify cases where the model prematurely stops or continues unnecessarily.
3. **Triple extraction stress test**: Systematically corrupt the text-to-triples outputs and measure the impact on final QA accuracy. This would quantify how sensitive RAS is to triple quality and inform whether more robust extraction methods are needed.