---
ver: rpa2
title: Improving Text-to-Image Generation with Input-Side Inference-Time Scaling
arxiv_id: '2510.12041'
source_url: https://arxiv.org/abs/2510.12041
tags:
- image
- prompt
- rewriter
- alignment
- aesthetics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes input-side inference-time scaling for text-to-image
  generation, where a large language model rewrites user prompts to improve alignment,
  quality, and aesthetics without modifying the T2I backbone. The method uses iterative
  direct preference optimization trained on synthetic prompt-image pairs, with multimodal
  LLM judges providing rewards across image quality, text-image alignment, and aesthetics.
---

# Improving Text-to-Image Generation with Input-Side Inference-Time Scaling

## Quick Facts
- arXiv ID: 2510.12041
- Source URL: https://arxiv.org/abs/2510.12041
- Reference count: 40
- Primary result: LLM-based prompt rewriting improves text-to-image generation across multiple models and benchmarks without modifying T2I backbones

## Executive Summary
This paper introduces input-side inference-time scaling for text-to-image generation, where a large language model rewrites user prompts to improve alignment, quality, and aesthetics without modifying the T2I backbone. The method uses iterative direct preference optimization trained on synthetic prompt-image pairs, with multimodal LLM judges providing rewards across image quality, text-image alignment, and aesthetics. Experiments show consistent improvements over original prompts and strong baselines across diverse T2I models and benchmarks, including a GPT-4o win rate of 0.579 on Pick-a-Pic v2, higher GenEval scores, and lower FID on MS COCO 30K.

## Method Summary
The approach treats T2I models as frozen black boxes and trains an LLM rewriter via iterative DPO using synthetic prompt-image pairs. For each round, the rewriter generates multiple candidate rewrites for prompts from a training set, images are synthesized using the frozen T2I model, and an MLLM judge evaluates pairwise preferences across four reward dimensions (quality, general alignment, physical alignment, aesthetics). The chosen/rejected pairs from these comparisons are used to update the rewriter policy via DPO without SFT, preserving exploration capacity. The process repeats for up to 6 rounds, with larger LLM rewriters yielding better results and trained rewriters showing effective transfer across different T2I backbones.

## Key Results
- GPT-4o win rate of 0.579 on Pick-a-Pic v2 test set
- Higher GenEval scores (0.76 overall vs 0.70 baseline) and lower FID on MS COCO 30K
- Consistent improvements across diverse T2I models (FLUX, SD-3.5, JanusPro)
- Strong transfer capability: rewriters trained on one backbone improve performance on others
- Larger LLM rewriters (70B) yield better results than smaller ones (8B)

## Why This Works (Mechanism)

### Mechanism 1
Prompt rewriting improves T2I generation by closing the distribution gap between short user prompts and the longer, descriptive captions used during T2I model training. The rewriter expands underspecified inputs into detailed prompts containing explicit attributes, spatial relations, and stylistic constraints, bringing inference-time inputs closer to the training distribution and improving alignment and quality without modifying T2I parameters.

### Mechanism 2
Iterative DPO training without SFT preserves exploration and avoids overfitting to model-specific prompt styles. By sampling multiple candidate rewrites per prompt and using pairwise MLLM-judged preferences to construct chosen/rejected pairs, DPO optimizes the rewriter policy directly while skipping SFT maintains policy diversity and enables cross-backbone transfer.

### Mechanism 3
Cross-model transfer emerges because diverse T2I backbones share common latent preferences for prompt structure. A rewriter trained on one T2I backbone produces refinements that generalize to others because T2I models are trained on similar image-caption distributions (e.g., LAION-scale data).

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed: Core training algorithm; replaces reward modeling with direct policy optimization from pairwise preferences
  - Quick check: Can you explain why DPO avoids learning an explicit reward model and how the β parameter controls deviation from the reference policy?

- **MLLM-as-a-Judge**
  - Why needed: Provides scalable, automated preference labeling for training without human annotation
  - Quick check: What are failure modes when using MLLM judges for aesthetic or alignment assessment (e.g., length bias, verbosity preference)?

- **T2I Prompt Sensitivity**
  - Why needed: Motivates the entire approach; understanding why short prompts fail is essential
  - Quick check: Why do diffusion models trained on detailed captions struggle with terse prompts at inference time?

## Architecture Onboarding

- **Component map**: User Prompt -> LLM Rewriter -> Frozen T2I Backbone -> MLLM Judge -> Reward Aggregation -> DPO Trainer -> Updated Rewriter

- **Critical path**:
  1. Sample batch of user prompts from training set (~10k per round)
  2. Generate n=5 candidate rewrites per prompt using current policy (temperature=1.5)
  3. Synthesize images from each candidate via frozen T2I model
  4. Run all n(n-1)=20 ordered pairs through MLLM judge for each reward dimension
  5. Aggregate votes to select single chosen/rejected pair per prompt
  6. Update policy via DPO loss (β=0.1) for 5 epochs per round
  7. Repeat for up to 6 rounds (observed saturation point)

- **Design tradeoffs**:
  - General vs. Aesthetics Rewriter: Adding aesthetics reward improves visual appeal but reduces alignment (0.561→0.424 win rate)
  - LoRA vs. Full Fine-tuning: LoRA achieves comparable performance with lower memory/compute
  - LLM Size vs. Latency: Larger rewriters (70B) improve results but increase inference cost
  - Round Limit: Performance degrades after 6 rounds due to alignment-quality trade-off

- **Failure signatures**:
  - Over-aestheticization: Aesthetics rewriter adds ornamental elements that distract from main subject
  - Alignment drift: Excessive training rounds improve alignment at cost of quality/aesthetics
  - Model-specific overfitting: If trained too long on single backbone, transfer may weaken
  - JanusPro anomaly: No improvement on GenEval despite gains on T2I-CompBench++

- **First 3 experiments**:
  1. Run rewriter on 50 prompts from Pick-a-Pic v2 test set with FLUX.1-schnell; manually inspect whether rewritten prompts are more detailed and whether images improve qualitatively
  2. Train separate rewriters with each reward removed; verify Table 10 pattern on held-out subset
  3. Train rewriter on FLUX.1-dev, evaluate on SD-3.5-medium using GenEval; confirm Table 8 transfer results

## Open Questions the Paper Calls Out

### Open Question 1
Can a single unified prompt rewriter dynamically balance the trade-off between text-image alignment and visual aesthetics without sacrificing performance in one area? The authors demonstrate that adding an aesthetics reward improves visual appeal (0.476 to 0.818 win rate) but significantly reduces alignment (0.561 to 0.424), forcing them to train two separate models.

### Open Question 2
Why does the input-side scaling method fail to improve performance on certain autoregressive or unified T2I architectures like JanusPro? While the method transfers well among diffusion models, it failed to improve JanusPro on GenEval (dropping from 0.79 to 0.68), which the authors suggest might be due to the model overfitting to the benchmark.

### Open Question 3
What regularization techniques or reward designs could prevent the degradation of image quality and aesthetics observed in later rounds of iterative DPO training? Section F.2 shows that beyond six rounds, image quality and aesthetics decline while alignment improves, leading to overfitting concerns.

## Limitations

- The trade-off between alignment and aesthetics forces use of separate models for different objectives
- Performance degrades after 6 training rounds, requiring careful early stopping
- Limited effectiveness on certain autoregressive or unified T2I architectures like JanusPro
- Reliance on MLLM judges whose biases and failure modes are not fully characterized

## Confidence

- Prompt rewriting improves T2I generation: High
- Iterative DPO without SFT avoids overfitting: Medium
- Cross-model transfer due to shared training distributions: Medium
- Aesthetics rewriter trades off alignment for visual appeal: High
- Performance saturates and degrades after 6 training rounds: Medium

## Next Checks

1. **Ablation on Reward Dimension Weighting**: Run experiments varying the weights of the four reward dimensions to identify optimal trade-offs between alignment, quality, and aesthetics.

2. **MLLM Judge Bias Characterization**: Systematically evaluate the Qwen2.5-VL-72B-Instruct judge on synthetic prompt-image pairs with known properties to quantify potential biases in pairwise comparisons.

3. **Robustness to Training Distribution Shifts**: Train rewriters on domain-specific caption datasets and evaluate transfer to both general and specialized T2I models to test limits of cross-backbone generalization.