---
ver: rpa2
title: 'Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep
  Research'
arxiv_id: '2510.21603'
source_url: https://arxiv.org/abs/2510.21603
tags:
- uni00000018
- uni00000013
- uni00000048
- uni0000001a
- uni0000001b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Doc-Researcher, a unified system for multimodal
  document parsing and deep research that addresses the gap in current deep research
  systems which are constrained to textual web data and cannot process locally stored
  multimodal documents. The core method consists of three integrated components: deep
  multimodal parsing that preserves layout structure and visual semantics while creating
  multi-granular representations, systematic retrieval architecture supporting text-only,
  vision-only, and hybrid paradigms with dynamic granularity selection, and iterative
  multi-agent workflows that decompose complex queries, progressively accumulate evidence,
  and synthesize comprehensive answers across documents and modalities.'
---

# Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research
## Quick Facts
- arXiv ID: 2510.21603
- Source URL: https://arxiv.org/abs/2510.21603
- Reference count: 40
- Primary result: Achieves 50.6% accuracy on M4DocBench, 3.4× improvement over baselines

## Executive Summary
Doc-Researcher addresses a critical gap in deep research systems by extending their capabilities beyond text-based web data to locally stored multimodal documents. The system integrates deep multimodal parsing that preserves layout structure and visual semantics, a systematic retrieval architecture supporting multiple paradigms (text-only, vision-only, hybrid), and iterative multi-agent workflows for complex query decomposition and evidence synthesis. The authors introduce M4DocBench as a new benchmark specifically designed for evaluating deep research on multimodal documents. Their unified approach demonstrates significant performance improvements, establishing a new paradigm for conducting comprehensive research across document collections containing diverse modalities.

## Method Summary
The core methodology of Doc-Researcher consists of three integrated components working in concert. First, deep multimodal parsing creates multi-granular representations while preserving both layout structure and visual semantics of documents. Second, a systematic retrieval architecture implements three distinct paradigms - text-only, vision-only, and hybrid retrieval - with dynamic granularity selection capabilities to optimize information extraction. Third, iterative multi-agent workflows decompose complex research queries into manageable sub-tasks, progressively accumulate evidence from multiple sources, and synthesize comprehensive answers that integrate information across different document modalities and sources.

## Key Results
- Achieves 50.6% accuracy on the newly introduced M4DocBench benchmark
- Demonstrates 3.4× improvement over state-of-the-art baseline deep research systems
- Establishes new performance standards for multimodal document processing in deep research contexts

## Why This Works (Mechanism)
The system's effectiveness stems from its unified approach to multimodal document processing. By integrating deep parsing with systematic retrieval and iterative workflows, Doc-Researcher can handle the complexity of real-world documents that contain mixed textual, visual, and structural information. The multi-granular representation allows the system to adapt to different levels of document complexity, while the three retrieval paradigms ensure comprehensive information extraction regardless of the dominant modality in any given document section. The multi-agent workflow decomposition enables systematic handling of complex research queries that would overwhelm single-agent approaches.

## Foundational Learning
- Multimodal document parsing: Understanding how to extract and represent information from documents containing text, images, and layout structures is essential for processing real-world research materials
- Retrieval paradigm selection: Dynamic switching between text-only, vision-only, and hybrid retrieval modes allows optimal information extraction based on document characteristics
- Multi-agent workflow orchestration: Breaking down complex research tasks into sequential sub-tasks with evidence accumulation is crucial for handling sophisticated queries
- Granularity-aware processing: Adapting the level of detail in processing based on document complexity and query requirements improves both efficiency and accuracy
- Visual-semantic preservation: Maintaining the relationship between visual elements and their semantic meaning ensures accurate interpretation of document content

## Architecture Onboarding
- **Component map**: Document Parser -> Retrieval Engine -> Multi-Agent Workflow Manager -> Answer Synthesizer
- **Critical path**: Input query → Query decomposition → Granularity assessment → Retrieval paradigm selection → Evidence accumulation → Answer synthesis → Output
- **Design tradeoffs**: Prioritized unified processing over specialized single-modality approaches; chose iterative workflows over monolithic processing to handle complexity
- **Failure signatures**: Query ambiguity leading to incorrect decomposition; inappropriate granularity selection causing retrieval inefficiency; modality mismatch in retrieval paradigms
- **First 3 experiments**: 1) Single document retrieval accuracy test across all three paradigms, 2) Multi-document synthesis quality assessment, 3) Query complexity scaling evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- The M4DocBench benchmark is newly introduced with limited external validation
- Performance comparisons may be inflated due to baselines not being designed for multimodal processing
- The system's effectiveness across diverse real-world document types and edge cases remains partially unexplored

## Confidence
- **High confidence**: The system's basic architecture and component integration are well-described and technically sound
- **Medium confidence**: The quantitative improvements on M4DocBench are valid but require independent replication
- **Medium confidence**: The three-component framework represents a coherent approach, though practical limitations need further study

## Next Checks
1. Independent replication of the M4DocBench results across diverse document collections beyond the training benchmark
2. Systematic evaluation of retrieval performance breakdown across text-only, vision-only, and hybrid paradigms under varying document complexity levels
3. User study comparing Doc-Researcher's output quality against expert human analysis on real-world research tasks involving multimodal documents