---
ver: rpa2
title: 'DeepForm: Reasoning Large Language Model for Communication System Formulation'
arxiv_id: '2506.08551'
source_url: https://arxiv.org/abs/2506.08551
tags:
- system
- communication
- formulation
- dataset
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeepForm, the first reasoning large language
  model specifically designed for communication system formulation. The authors address
  the challenge of insufficient domain-specific training data and the complexity of
  communication system modeling by creating the Communication System Formulation Reasoning
  Corpus (CSFRC), the world's first large-scale open-source dataset for this domain.
---

# DeepForm: Reasoning Large Language Model for Communication System Formulation

## Quick Facts
- **arXiv ID:** 2506.08551
- **Source URL:** https://arxiv.org/abs/2506.08551
- **Reference count:** 33
- **Primary result:** First reasoning LLM for communication system formulation, achieving 71.4% accuracy with 7B parameters

## Executive Summary
This paper introduces DeepForm, the first reasoning large language model specifically designed for communication system formulation. The authors address the challenge of insufficient domain-specific training data and the complexity of communication system modeling by creating the Communication System Formulation Reasoning Corpus (CSFRC), the world's first large-scale open-source dataset for this domain. DeepForm employs a two-stage training framework: supervised fine-tuning with chain-of-thought data for domain knowledge distillation, followed by a novel rule-based reinforcement learning algorithm (C-ReMax) for advanced reasoning capabilities. Extensive experiments demonstrate that DeepForm achieves state-of-the-art performance in communication system formulation, significantly outperforming larger proprietary LLMs with a model size of only 7B parameters.

## Method Summary
DeepForm uses a two-stage training approach: Stage 1 applies supervised fine-tuning with chain-of-thought data using LoRA on Qwen2.5-7B-Instruct, while Stage 2 employs a novel rule-based reinforcement learning algorithm (C-ReMax) for advanced reasoning. The method addresses the challenge of insufficient domain-specific training data by creating CSFRC, a large-scale dataset constructed from ArXiv papers (2015-2025) containing communication system descriptions and their mathematical formulations. The training pipeline involves PDF-to-markdown conversion, description-formulation pair extraction, prompt compression, CoT generation via rejection sampling, SFT with LoRA adaptation, and RL fine-tuning with variance-reduced baseline rewards.

## Key Results
- DeepForm achieves 71.4% accuracy on communication system formulation tasks
- Outperforms DeepSeek R1 (671B parameters) which achieves 68.4% accuracy
- Demonstrates that CoT-augmented supervision significantly improves performance over direct Q→A training
- Shows that rule-based RL with variance reduction improves reasoning capability

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought Knowledge Distillation
- **Claim**: CoT-augmented supervision enables domain knowledge transfer that direct question-answer pairs cannot achieve for complex formulation tasks.
- **Mechanism**: The CoT data provides intermediate reasoning traces (problem decomposition → constraint reformulation → mathematical synthesis), allowing the model to learn the reasoning process rather than just input-output mappings. LoRA-based parameter updates (rank 256) preserve pretrained knowledge while adapting low-rank subspaces for domain patterns.
- **Core assumption**: Communication system formulation requires explicit reasoning traces; models cannot infer correct formulations from compressed representations alone.
- **Evidence anchors**: [abstract]: "Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge"; [section 4.2]: "the direct use of the non-CoT-enhanced dataset not only fails to improve the model's answer accuracy, but also somewhat diminishes its reasoning ability"

### Mechanism 2: Rule-Based RL with Variance-Reduced Baseline (C-ReMax)
- **Claim**: Binary rule-based rewards combined with greedy rollout baselines reduce gradient variance and improve reasoning capability emergence.
- **Mechanism**: The C-ReMax algorithm computes reward as R(a) = R_accuracy(a) - R_repetition(a). Critically, it contrasts stochastic rollouts with greedy rollouts: r̂ = r_m(seq) - r_m(seq_greedy), which approximates expected reward and reduces variance. KL divergence constraints (λ=0.001) prevent catastrophic forgetting.
- **Core assumption**: Formulation correctness is verifiable via rule-based equivalence checking; the sparse reward signal is sufficient for policy improvement.
- **Evidence anchors**: [section 3.3]: "C-ReMax contrasts a stochastic rollout seq ~ π_θ(·|x) with a greedy rollout seq_max = arg max π_θ(a_{1:T}|x)"; [section 4.2]: Ablation shows accuracy improves from 65.1% (SFT only) to 71.4% (SFT + RL)

### Mechanism 3: Dataset Difficulty Calibration Trumps Curriculum Learning
- **Claim**: Training RL on easier examples outperforms curriculum learning or hard-only training for this task.
- **Mechanism**: Hard datasets cause sparse positive rewards—the model rarely succeeds, creating unstable gradient estimates. Easy datasets provide denser reward signals, enabling more stable policy updates.
- **Core assumption**: Positive reward density matters more than example difficulty for RL stability.
- **Evidence anchors**: [section 4.2, Fig. 5c]: Easy dataset achieves highest accuracy; hard/curriculum learning show "similar, but lower, accuracy levels"; [section 4.2]: "hard dataset causing instability during RL, as the LLM rarely receives positive rewards"

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here**: DeepForm freezes pretrained weights and only trains low-rank matrices (A·B^T where r=256), reducing trainable parameters from O(d·k) to O(r(d+k)).
  - **Quick check question**: Can you explain why LoRA preserves pretrained knowledge better than full fine-tuning?

- **Concept: Chain-of-Thought Reasoning**
  - **Why needed here**: The data pipeline constructs reasoning paths (problem decomposition → mathematical modeling) that serve as supervision targets. Without understanding CoT, you cannot interpret the D_sft construction.
  - **Quick check question**: Given a communication system description, can you sketch the intermediate reasoning steps before the final formulation?

- **Concept: Policy Gradient with Baseline**
  - **Why needed here**: C-ReMax uses a greedy baseline to reduce variance. Understanding ∇θL(θ) = -E[∇θ log π_θ(seq|x) · r̂] is essential for debugging RL training.
  - **Quick check question**: Why does subtracting a baseline (greedy rollout reward) reduce variance without introducing bias?

## Architecture Onboarding

- **Component map**:
  ```
  arXiv PDFs → minerU parser → Description-Formulation pairs (D_raw)
       ↓
  DeepSeek V3 compression (L_max=4096 tokens) → D_comp
       ↓
  DeepSeek R1 rejection sampling → CoT reasoning paths → D_sft
       ↓
  Stage 1: SFT (Qwen2.5-7B-Instruct + LoRA r=256, 5 epochs, lr=5e-6)
       ↓
  Stage 2: C-ReMax RL (1.2k samples, 5 epochs, lr=2e-6, KL=0.001)
       ↓
  DeepForm (71.4% accuracy)
  ```

- **Critical path**: The CoT data quality (Stage 1) is the bottleneck. If rejection sampling fails (no valid candidates within T_max attempts) and fallback correction also fails (similarity < θ/2), samples are discarded. Dataset quality determines everything downstream.

- **Design tradeoffs**:
  - **Reward sparse vs. shaped**: Binary reward (0/1) vs. shaped rewards. Paper shows negative rewards (-1) cause 7.7% drop; format rewards cause 2% drop from reduced exploration.
  - **Dataset difficulty**: Easy > hard for this task scale. Counter-intuitive but empirically validated.
  - **Base model scale**: 7B chosen for deployment efficiency (8 A6000s vs. 8 H100s for DeepSeek R1).

- **Failure signatures**:
  - SFT loss plateaus but accuracy doesn't improve → Check CoT data quality, not just loss.
  - RL accuracy drops mid-training → Reward signal too sparse; try easier dataset subset.
  - Model generates repetitive outputs → Increase repetition penalty β in R_rp(a).
  - High KL divergence → Increase λ regularization.

- **First 3 experiments**:
  1. **Reproduce SFT-only baseline**: Train Qwen2.5-7B-Instruct on D_sft without CoT (direct description→formulation). Expect ~62% accuracy per Fig. 5b. This validates your data pipeline.
  2. **Ablate reward settings**: Test Ra(a) ∈ {0, 1} vs. {-1, 1} and with/without format reward. Expect 7.7% variance from reward design per Fig. 8. This validates your RL implementation.
  3. **Test on held-out subdomain**: Evaluate on a category not dominant in training (e.g., NOMA at 1.2%). This reveals generalization vs. memorization tradeoffs not discussed in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reinforcement learning frameworks be adapted to stabilize training on complex communication tasks where the "hard" problem distribution results in sparse positive rewards?
- **Basis in paper:** [explicit] The authors state in Section 4.2 that training on the "hard" dataset caused instability because the LLM "rarely receives positive rewards," making it difficult for the model to learn from feedback compared to the "easy" dataset.
- **Why unresolved:** The current C-ReMax algorithm relies on receiving non-zero gradients from successful rollouts; as task difficulty increases, the probability of a successful rollout drops, effectively halting the learning signal.
- **Evidence to resolve:** A new reward shaping mechanism or an exploration strategy (distinct from curriculum learning) that provides intermediate feedback for partial correctness in complex formulations.

### Open Question 2
- **Question:** Can the rule-based reward mechanism be improved to distinguish between semantic correctness and syntactic variations in mathematical formulations?
- **Basis in paper:** [inferred] Section 3.3 defines the accuracy reward as binary (equivalent to $a_{true}$ or not), while the ablation study (Section 4.2) shows that adding a strict "format reward" reduces accuracy by 2%, suggesting the model may be penalized for valid but stylistically different reasoning paths.
- **Why unresolved:** Rigid rule-based rewards may fail to capture the diverse valid representations of a communication system model, potentially leading to "reward hacking" where the model optimizes for format over functional correctness.
- **Evidence to resolve:** Integration of a semantic parser or an LLM-as-a-judge into the reward loop to validate the mathematical logic rather than relying solely on string equivalence or strict formatting rules.

### Open Question 3
- **Question:** To what extent does the reliance on ArXiv-derived data limit the model's applicability to proprietary or non-academic communication standards?
- **Basis in paper:** [inferred] Section 3.1 describes the data preprocessing pipeline, noting that the corpus is constructed exclusively from "2015-2025 ArXiv publications."
- **Why unresolved:** Academic papers often present idealized or theoretical system models; real-world network formulation (e.g., specific vendor constraints or legacy protocol integration) may utilize different terminology or empirical heuristics not present in the training corpus.
- **Evidence to resolve:** Benchmarking DeepForm against a validation set derived from industry patents, 3GPP technical specifications, or proprietary simulation logs that were not part of the ArXiv training set.

## Limitations

- **Dataset availability**: The CSFRC dataset is not yet released, making exact reproduction impossible and limiting independent verification of results.
- **Performance generalization**: The 71.4% accuracy is measured only against CSFRC's test split without cross-dataset generalization or robustness testing.
- **Implementation complexity**: The C-ReMax algorithm requires unspecified mathematical equivalence checking and repetition penalty functions, creating barriers to faithful reproduction.

## Confidence

**High Confidence**: The core insight that CoT data improves performance over direct Q→A training is well-supported by the ablation (Fig. 5b shows degradation without CoT). The two-stage training framework (SFT + RL) is clearly described and follows established LLM practices.

**Medium Confidence**: The superiority of rule-based RL over standard RL (71.4% vs. 65.1%) is demonstrated, but the specific implementation of C-ReMax's variance reduction mechanism is not fully specified. The claim that easy datasets outperform hard ones for RL training is counterintuitive but empirically supported in this narrow context.

**Low Confidence**: The 71.4% vs. 68.4% comparison with DeepSeek R1 is misleading - DeepSeek R1 (671B parameters) was not fine-tuned on CSFRC, while DeepForm was specifically trained on this dataset. This is an unfair comparison that overstates DeepForm's advantage.

## Next Checks

1. **Release and Evaluate CSFRC**: Request the dataset and perform independent evaluation using different base models (e.g., Llama-3, Mistral) to verify whether the 71.4% accuracy is specific to Qwen2.5-7B or generalizes across architectures.

2. **Cross-Domain Generalization Test**: Evaluate DeepForm on communication system formulation tasks from different sources (e.g., textbook problems, industry standards) to measure true generalization beyond the training distribution.

3. **Ablation of C-ReMax Components**: Systematically vary the baseline strategy (no baseline, greedy baseline, Monte Carlo baseline) and reward functions to isolate which components contribute to the 6.3% improvement over SFT alone.