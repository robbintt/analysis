---
ver: rpa2
title: The Role of Hyperparameters in Predictive Multiplicity
arxiv_id: '2503.13506'
source_url: https://arxiv.org/abs/2503.13506
tags:
- discrepancy
- hyperparameters
- predictive
- multiplicity
- hyperparameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how hyperparameter tuning influences predictive\
  \ multiplicity\u2014the phenomenon where differently configured models trained on\
  \ the same data produce divergent predictions. The authors analyze six widely used\
  \ machine learning models on 21 benchmark datasets, focusing on key hyperparameters\
  \ such as lambda (Elastic Net), gamma (SVM), and alpha (XGBoost)."
---

# The Role of Hyperparameters in Predictive Multiplicity

## Quick Facts
- arXiv ID: 2503.13506
- Source URL: https://arxiv.org/abs/2503.13506
- Reference count: 40
- Key outcome: Hyperparameter optimization improves performance but significantly increases prediction discrepancies, with XGBoost showing highest instability.

## Executive Summary
This study investigates how hyperparameter tuning influences predictive multiplicity—the phenomenon where differently configured models trained on the same data produce divergent predictions. The authors analyze six widely used machine learning models on 21 benchmark datasets, focusing on key hyperparameters such as lambda (Elastic Net), gamma (SVM), and alpha (XGBoost). Results show that hyperparameter optimization improves model performance but significantly increases prediction discrepancies, with XGBoost exhibiting the highest instability. The study highlights a critical trade-off between performance gains and prediction consistency, raising concerns about arbitrary predictions in high-stakes applications like credit assessment and medical diagnosis.

## Method Summary
The study evaluates six machine learning models (Elastic Net, Decision Tree, k-NN, SVM, Random Forests, XGBoost) on 21 benchmark binary classification datasets with ≥1,000 observations each. Hyperparameter configurations (~500,000 per model) are sampled across specified ranges. The primary metric is discrepancy—the maximum proportion of conflicting predictions between default and tuned configurations. Tunability is measured as F1-score gain from tuning. Analysis examines model-level, hyperparameter-level, and joint hyperparameter effects on prediction stability.

## Key Results
- XGBoost exhibits the highest prediction discrepancy (61.5% mean) among all models tested
- Regularization hyperparameters (lambda, alpha, gamma) disproportionately drive prediction multiplicity compared to structural parameters
- Models with identical F1-scores can show 40%+ prediction disagreement, revealing that performance metrics alone mask prediction instability

## Why This Works (Mechanism)

### Mechanism 1: Regularization strength modulates prediction boundary variability
- Claim: Hyperparameters controlling regularization disproportionately increase prediction discrepancy compared to structural hyperparameters.
- Mechanism: Regularization parameters directly penalize coefficient magnitudes or kernel influence, causing models to assign different weights to the same features across configurations, creating divergent decision boundaries.
- Core assumption: Default hyperparameter configurations represent a meaningful baseline for comparison.
- Evidence anchors: [abstract], [Section 4.2], [Section 4.2]
- Break condition: When regularization parameters are constrained to narrow ranges (e.g., alpha < 1 for XGB).

### Mechanism 2: Model-specific hyperparameter sensitivity creates algorithm-dependent instability profiles
- Claim: Different algorithm families exhibit distinct multiplicity patterns based on which hyperparameters dominate their behavior.
- Mechanism: Tree-based models respond to depth/pruning parameters, kernel methods respond to influence parameters, while linear models respond to penalty mixing. The architectural inductive bias determines which hyperparameter changes cascade into prediction changes.
- Core assumption: Measured discrepancy captures meaningful prediction instability rather than benign variation.
- Evidence anchors: [Section 4.1], [Table 1], [Figure 2]
- Break condition: When using inherently stable algorithms (kNN) or constraining high-discrepancy hyperparameters.

### Mechanism 3: Performance-variability trade-off emerges from hypothesis space expansion
- Claim: Hyperparameter optimization that improves performance metrics simultaneously expands the space of near-equivalent models, increasing the probability of arbitrary individual predictions.
- Mechanism: As tuning explores broader hyperparameter configurations, it identifies multiple models with similar aggregate performance but different individual-level predictions.
- Core assumption: F1-score gains from tuning represent genuine generalization improvement, not overfitting to validation sets.
- Evidence anchors: [abstract], [Section 4.3], [Figure 3]
- Break condition: When accepting lower aggregate performance in exchange for prediction consistency.

## Foundational Learning

- Concept: **Predictive Multiplicity / Model Multiplicity**
  - Why needed here: This is the central phenomenon being measured—understanding that multiple valid models can produce conflicting individual predictions is prerequisite to interpreting any discrepancy metrics.
  - Quick check question: If two models both achieve 85% accuracy on a test set, can they disagree on more than 15% of individual predictions? (Answer: Yes, potentially up to 30% if errors are non-overlapping)

- Concept: **Regularization as Hypothesis Space Constraint**
  - Why needed here: The paper identifies regularization hyperparameters as primary drivers of multiplicity; understanding how these parameters restrict or expand model behavior is essential.
  - Quick check question: Does increasing L1 regularization (alpha) make a model more or less sensitive to individual feature values? (Answer: More sensitive to feature selection decisions, potentially increasing prediction variance across configurations)

- Concept: **Discrepancy vs. Error Rate Decomposition**
  - Why needed here: The paper measures discrepancy (prediction disagreement) separately from performance (F1-score); these are orthogonal dimensions that require distinct mental models.
  - Quick check question: A model with low error rate and high discrepancy poses what specific risk in high-stakes applications? (Answer: Arbitrary individual outcomes despite good aggregate performance)

## Architecture Onboarding

- Component map: Baseline Model -> Configuration Generator -> Tuned Models -> Discrepancy Calculator -> Tunability Metric -> Joint Distribution Analyzer
- Critical path: 1. Train baseline model with defaults → 2. Generate hyperparameter configurations → 3. Train configuration-specific models → 4. Compute per-dataset discrepancy → 5. Aggregate across datasets → 6. Analyze tunability-discrepancy relationship
- Design tradeoffs:
  - Default baseline vs. optimal baseline: Paper chooses defaults for practicality, but original definition uses loss-minimizing model
  - Max discrepancy vs. average discrepancy: Paper uses max across configurations (more conservative)
  - Single hyperparameter vs. joint effects: Section 4.4 shows joint effects can differ from individual effects
  - Computational budget: ~500,000 configurations per model across 21 datasets
- Failure signatures:
  - High-stakes arbitrariness: XGB with unconstrained alpha produces 61.5% mean discrepancy
  - Performance-only selection mask: Models with identical F1-scores showing 40%+ prediction disagreement
  - Hyperparameter-region instability: EN with lambda > 1.0 shows sharp discrepancy increase
- First 3 experiments:
  1. Constrained-alpha XGB baseline: Train XGB with alpha bounded to [0, 1] and measure discrepancy reduction vs. performance loss
  2. Baseline sensitivity analysis: Compare discrepancy using default vs. optimal-performance baseline
  3. Dataset characteristic interaction: Test whether balanced vs. imbalanced datasets systematically shift discrepancy distributions

## Open Questions the Paper Calls Out

- Does the trade-off between performance optimization and prediction consistency persist in deep learning architectures?
  - Basis in paper: [explicit] The authors list "excluding deep learning" as a primary limitation.
  - Why unresolved: Deep learning involves different hyperparameter dynamics and stochastic training processes that may uniquely influence multiplicity.
  - What evidence would resolve it: Replicating the experimental framework using neural networks on the benchmark datasets.

- How do alternative optimization strategies, such as Bayesian optimization, impact the severity of predictive multiplicity?
  - Basis in paper: [explicit] The authors note that "alternative tuning strategies, such as Bayesian optimization, were not explored."
  - Why unresolved: Current results rely on standard search methods; guided optimization might navigate the search space differently.
  - What evidence would resolve it: A comparative study measuring discrepancy levels in models tuned via Bayesian methods versus random/grid search.

- What is the quantitative impact of hyperparameter-induced multiplicity on fairness metrics in high-stakes domains?
  - Basis in paper: [inferred] While the paper highlights concerns about arbitrariness, it concludes that "further empirical studies are needed to assess real-world impacts."
  - Why unresolved: It is currently unclear if high discrepancy scores directly correlate with specific fairness violations in production environments.
  - What evidence would resolve it: Correlating discrepancy measures with standard fairness metrics across different hyperparameter configurations.

## Limitations
- The study relies on default baselines rather than optimal performance baselines, potentially inflating multiplicity measures
- Large hyperparameter search ranges (particularly for XGBoost alpha) may artificially inflate discrepancy scores
- All datasets used are imbalanced, limiting generalizability to balanced classification problems
- Deep learning architectures and alternative optimization strategies were not explored

## Confidence
- High confidence: The core observation that hyperparameter tuning increases prediction discrepancies
- Medium confidence: The claim that regularization hyperparameters disproportionately drive multiplicity
- Low confidence: The practical recommendations for hyperparameter constraints

## Next Checks
1. Replicate discrepancy calculations using an optimal baseline model rather than default hyperparameters
2. Conduct controlled experiments with constrained hyperparameter ranges (e.g., XGBoost alpha ∈ [0, 1]) to quantify the stability-performance trade-off
3. Test model behavior on balanced datasets to determine if multiplicity patterns shift systematically with class balance