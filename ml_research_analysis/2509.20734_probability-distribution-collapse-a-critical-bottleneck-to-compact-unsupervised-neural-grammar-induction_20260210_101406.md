---
ver: rpa2
title: 'Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised
  Neural Grammar Induction'
arxiv_id: '2509.20734'
source_url: https://arxiv.org/abs/2509.20734
tags:
- neural
- crnp
- probability
- grammar
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a key bottleneck in unsupervised neural grammar
  induction, termed probability distribution collapse (PDC), where distinct symbols
  are mapped to similar probability distributions, limiting model expressiveness even
  with large grammars. The authors analyze the causes of PDC across neural parameterization
  components and propose a collapse-relaxing neural parameterization (CRNP) to mitigate
  it.
---

# Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised Neural Grammar Induction

## Quick Facts
- arXiv ID: 2509.20734
- Source URL: https://arxiv.org/abs/2509.20734
- Reference count: 18
- The paper identifies and mitigates "probability distribution collapse" in neural grammar induction, enabling compact models with 90 nonterminals to outperform much larger grammars

## Executive Summary
This paper identifies a fundamental bottleneck in unsupervised neural grammar induction called Probability Distribution Collapse (PDC), where distinct nonterminal symbols converge to similar probability distributions, severely limiting model expressiveness. The authors demonstrate that PDC occurs systematically across neural parameterization components and propose a Collapse-Relaxing Neural Parameterization (CRNP) to address it. Their solution significantly improves parsing performance while enabling compact grammars that achieve state-of-the-art results with far fewer parameters than previous approaches.

## Method Summary
The authors propose Collapse-Relaxing Neural Parameterization (CRNP) to mitigate probability distribution collapse in unsupervised neural grammar induction. CRNP introduces three key modifications: higher-dimensional embeddings to increase representational capacity, layer normalization specifically for child representations to stabilize training dynamics, and replacement of ReLU activation with GELU to smooth gradient propagation. These changes work together to maintain distributional diversity across nonterminal symbols while improving overall model performance on constituency parsing tasks across multiple languages.

## Key Results
- CRNP significantly improves parsing performance while enabling compact grammars
- With just 90 nonterminals, CRNP+PF outperforms models using up to 4,500 nonterminals
- CRNP reduces PDC as measured by geometric mean of pairwise Jensen-Shannon divergence
- Demonstrates superior capacity utilization compared to previous large-grammar approaches

## Why This Works (Mechanism)
PDC occurs when neural parameterization causes distinct symbols to map to similar probability distributions, reducing model expressiveness. The paper shows this happens systematically across embedding layers, activation functions, and normalization components. CRNP addresses this by increasing embedding dimensionality to provide more representational space, using layer normalization to stabilize child representation distributions, and employing GELU activation to maintain smoother gradient flow that prevents symbols from collapsing to similar modes during training.

## Foundational Learning

**Jensen-Shannon Divergence**: Measures similarity between probability distributions, used here to quantify PDC severity. Why needed: Provides objective metric for distributional diversity. Quick check: Compute pairwise JS divergence between symbol distributions to verify diversity improvements.

**Layer Normalization**: Normalizes across features rather than batch dimensions. Why needed: Stabilizes training by preventing scale explosions in child representations. Quick check: Monitor training stability metrics with and without layer norm.

**GELU vs ReLU**: GELU provides smoother gradients than ReLU's hard cutoff. Why needed: Prevents gradient sparsity that can cause symbol distribution collapse. Quick check: Compare gradient flow patterns between activation functions.

## Architecture Onboarding

Component map: Input Sentences -> CRNP Encoding -> PCFG Parsing -> Output Trees

Critical path: Sentence → Higher-dimensional Embeddings → Layer Normalization → GELU Activation → PCFG Parsing → Evaluation

Design tradeoffs: Higher embedding dimensions increase memory usage but prevent PDC; GELU adds computation but improves gradient flow; layer norm stabilizes training but requires careful placement.

Failure signatures: Training instability, performance plateauing early, low JS divergence between symbol distributions, similar parse trees across different inputs.

First experiments:
1. Train baseline model to observe PDC onset during training
2. Implement CRNP with minimal changes to isolate PDC effects
3. Compare JS divergence metrics before and after CRNP implementation

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical framework for PDC mechanisms remains informal despite empirical evidence
- Ablation studies could be more systematic in isolating individual CRNP component contributions
- Comparison with larger grammars doesn't fully account for potential trade-offs in representational power

## Confidence
- PDC identification as bottleneck: High
- CRNP effectiveness in mitigating PDC: High
- 90 nonterminal superiority claim: Medium
- Causal understanding of PDC mechanisms: Low

## Next Checks
1. Conduct controlled ablation experiments varying each CRNP component independently to quantify individual contributions to PDC mitigation versus general training stability improvements.

2. Test CRNP on out-of-domain datasets and longer sequences to evaluate whether PDC mitigation effects persist under distribution shift and increased computational demands.

3. Perform qualitative analysis of induced grammar structures to verify that PDC mitigation leads to more linguistically meaningful symbol clusters rather than just distributional diversity.