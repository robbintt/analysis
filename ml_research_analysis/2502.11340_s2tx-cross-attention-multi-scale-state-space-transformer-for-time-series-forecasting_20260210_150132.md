---
ver: rpa2
title: 'S2TX: Cross-Attention Multi-Scale State-Space Transformer for Time Series
  Forecasting'
arxiv_id: '2502.11340'
source_url: https://arxiv.org/abs/2502.11340
tags:
- time
- series
- s2tx
- global
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multivariate time series forecasting, specifically
  improving the modeling of cross-variate correlations and global-local interactions
  in multi-scale frameworks. It introduces S2TX, which integrates a Mamba model for
  long-range cross-variate global context extraction and a Transformer model with
  local window attention for short-range local feature capture, connected via a cross-attention
  mechanism.
---

# S2TX: Cross-Attention Multi-Scale State-Space Transformer for Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2502.11340
- **Source URL**: https://arxiv.org/abs/2502.11340
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art performance on multivariate time series forecasting with 8.4% MSE improvement on ETTh1 dataset at horizon 720

## Executive Summary
S2TX addresses the challenge of multivariate time series forecasting by modeling both cross-variate correlations and global-local temporal interactions through a novel multi-scale architecture. The method integrates a Mamba model for extracting long-range cross-variate global context with a Transformer model for short-range local feature capture, connected via a cross-attention mechanism. Experimental results on seven benchmark datasets demonstrate significant performance improvements over state-of-the-art methods, particularly in capturing complex dependencies across multiple time series variables while maintaining computational efficiency and robustness to missing values.

## Method Summary
S2TX employs a cross-attention mechanism to integrate a Mamba model for extracting long-range cross-variate context with a Transformer model for capturing short-range local features. The architecture processes input time series through multi-scale patchification, where global patches are processed by bidirectional Mamba layers to capture cross-variate correlations, while local patches undergo self-attention followed by cross-attention with global context. This design enables the model to learn both global patterns affecting all variates and local variations dependent on global context, with experiments showing 8.4% MSE improvement on ETTh1 dataset at horizon 720.

## Key Results
- Achieves 8.4% MSE improvement on ETTh1 dataset at forecasting horizon 720
- Demonstrates superior performance across seven benchmark datasets compared to state-of-the-art methods
- Shows strong robustness to missing values with only 4.7% MSE degradation at 40% missing data versus 13.4% for SST baseline

## Why This Works (Mechanism)

### Mechanism 1: Cross-Variate Global Context via Concatenated Mamba
Processing all variates jointly through Mamba enables learning cross-variate correlations that independent processing misses. Global patches from all D variates are concatenated before Mamba processing, allowing the selection mechanism to filter relevant variates and patches, capturing dependencies across variables.

### Mechanism 2: Global-Local Cross-Attention Feature Interplay
Cross-attention enables global context to modulate local feature extraction, improving prediction of local variations that depend on global patterns. The local transformer decoder uses global context Zg as key and value in cross-attention, allowing local queries to attend to relevant global patterns.

### Mechanism 3: Multi-Scale Patching with Asymmetric Look-Back Windows
Separate patching scales for global (full look-back L) and local (shorter window S) features better captures heterogeneous temporal dependencies than uniform patching. Global patches use longer strides and lengths to create coarse long-range representations, while local patches provide fine short-range representations.

## Foundational Learning

- **State-Space Models (Mamba)**: Essential for understanding the O(n) complexity and selective mechanism that enables cross-variate processing without quadratic attention cost. Quick check: Can you explain why Mamba achieves linear complexity during training but constant-time during inference?

- **Cross-Attention vs Self-Attention**: Critical for understanding how global and local representations are bridged. Quick check: In cross-attention, which side provides the queries and which provides the keys/values in S2TX?

- **Patching for Time Series**: Important for understanding how point-wise sequences are transformed into token sequences for transformer-style processing. Quick check: How does patch length affect the receptive field and temporal resolution tradeoff?

## Architecture Onboarding

- **Component map**: Input (D x L) → Patchify → Global Patches (D x PNg x PLg) → Mamba(bidirectional) → Global Context (D x PNg x d_model) ↘ Local Patches (D x PNl x PLl) → Linear Tokenization → Self-Attention → Cross-Attention (K,V from global) → Feed-Forward → Flatten + Linear → Output (D x H)

- **Critical path**: Global patches → Mamba → Cross-Attention K/V → Local decoder. If global context is corrupted or misaligned, local predictions degrade significantly.

- **Design tradeoffs**: Balances cross-variate vs channel-independence by processing variates together in Mamba but separately in local transformer; uses fixed PNg for O(1) complexity w.r.t. L but may lose resolution for very long sequences; bidirectional Mamba adds ~2x parameters for improved global pattern capture.

- **Failure signatures**: MSE plateaus higher than baselines (check if cross-attention receives non-trivial global context), memory spikes unexpectedly (verify S << L constraint), degradation with missing values (check patch alignment).

- **First 3 experiments**: 1) Ablate cross-variate processing by processing each variate independently through Mamba, expect ~0.02-0.04 MSE increase on ETTh1. 2) Vary global/local window ratio with S = L/4, L/2, 3L/4 to find optimal balance. 3) Replicate Table 2 missing value protocol on target dataset to establish baseline robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation limited to energy and traffic domains, effectiveness for medical or financial time series remains unproven
- High-dimensional multivariate series (D > 1000) may face memory constraints despite O(1) complexity claims
- Missing value handling mechanism is promising but not explicitly detailed in terms of which component provides robustness

## Confidence
- **High confidence**: Architectural design and ablation studies are well-documented and internally consistent, with empirically validated cross-attention contribution
- **Medium confidence**: Performance claims supported by experimental results but may be affected by implementation differences and hyperparameter tuning
- **Low confidence**: Missing value robustness claims lack detailed mechanistic explanation, cross-variate correlation assumptions not empirically validated across diverse datasets

## Next Checks
1. Test S2TX on medical or financial time series datasets where cross-variate correlations may be weaker to validate cross-variate Mamba benefits
2. Evaluate S2TX with increasing numbers of variates (D = 10, 50, 100, 500) to empirically verify O(1) complexity scaling and identify practical limits
3. Conduct controlled experiments with synthetic missing patterns to determine which component contributes most to missing value robustness and visualize attention patterns with missing data