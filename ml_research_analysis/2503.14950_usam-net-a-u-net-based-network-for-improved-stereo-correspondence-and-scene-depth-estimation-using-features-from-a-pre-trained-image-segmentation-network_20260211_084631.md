---
ver: rpa2
title: 'USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene
  Depth Estimation using Features from a Pre-trained Image Segmentation network'
arxiv_id: '2503.14950'
source_url: https://arxiv.org/abs/2503.14950
tags:
- depth
- disparity
- segmentation
- stereo
- usam-net
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces USAM-Net, a CNN-based U-Net architecture
  that leverages stereo images, semantic segmentation maps, and self-attention mechanisms
  to improve depth estimation in autonomous driving scenarios. By integrating features
  from a pre-trained Segment Anything Model (SAM) and employing self-attention layers,
  the model enhances focus on important features like object boundaries and surface
  textures.
---

# USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network

## Quick Facts
- arXiv ID: 2503.14950
- Source URL: https://arxiv.org/abs/2503.14950
- Reference count: 21
- Primary result: USAM-Net achieves GD=3.61% and EPE=0.88 on DrivingStereo, outperforming traditional models

## Executive Summary
USAM-Net introduces a CNN-based U-Net architecture that integrates stereo images, semantic segmentation maps, and self-attention mechanisms to enhance depth estimation in autonomous driving scenarios. By leveraging pre-trained SAM features and self-attention layers, the model improves focus on important features like object boundaries and surface textures. Evaluated on the DrivingStereo dataset, USAM-Net demonstrates superior performance compared to traditional models, particularly in handling featureless surfaces and occlusion. The approach offers a competitive solution for high-precision depth estimation in autonomous driving and augmented reality applications.

## Method Summary
USAM-Net is a U-Net-based CNN architecture that processes 9-channel inputs consisting of left RGB, right RGB, and a SAM-derived semantic mask from the left image. The model employs an encoder-decoder structure with self-attention layers to focus on distinctive features. Training uses the DrivingStereo dataset with smooth-L1 loss and data augmentation including color jitter. The architecture includes 5 encoder stages and 5 decoder stages with transposed convolutions, and a final sigmoid layer to produce disparity maps scaled to 0-255. The model is trained for 30 epochs using Adam optimizer with specific learning rate scheduling.

## Key Results
- Achieves Global Difference (GD) of 3.61% and End-Point Error (EPE) of 0.88 on DrivingStereo test set
- Outperforms traditional models like CFNet, SegStereo, and iResNet
- Demonstrates improved handling of featureless surfaces and occlusion compared to baseline models
- Shows effectiveness of integrating SAM segmentation features with stereo correspondence

## Why This Works (Mechanism)

### Mechanism 1: Segmentation-Guided Feature Integration
Concatenating SAM-generated semantic masks with stereo image pairs provides explicit object boundary and texture cues that pure intensity-based stereo matching cannot extract from smooth surfaces. This helps particularly with featureless surfaces like roads where traditional stereo matching struggles.

### Mechanism 2: Self-Attention for Feature Importance Weighting
Self-attention layers between encoder and decoder focus processing on distinctive feature regions like vehicles and vegetation. The attention mechanism emphasizes areas with high feature distinctiveness before decoder upsampling, improving disparity detail.

### Mechanism 3: Multi-Scale U-Net with Skip-Connected Reconstruction
The encoder-decoder architecture with skip connections enables robust disparity regression across spatial scales. Hierarchical feature extraction captures both local matching cues and global scene context, while smooth-L1 loss handles disparity outliers appropriately.

## Foundational Learning

- **Stereo Matching and Disparity**: Understanding epipolar geometry explains why feature matching challenges occur on smooth surfaces. Quick check: Given a baseline of 0.54m and focal length of 1000 pixels, what depth corresponds to a disparity of 50 pixels?
- **U-Net Skip Connections**: Skip connections preserve spatial detail during reconstruction. Quick check: What happens to spatial resolution at the bottleneck of the described 5-stage encoder, and why might this matter for object boundary accuracy?
- **Self-Attention Mechanics (Q/K/V)**: Query-key-value computation enables feature importance weighting. Quick check: In the self-attention layer, what does the softmax operation compute, and how does the output dimensionality compare to the input?

## Architecture Onboarding

- **Component map**: Input (9ch: LRGB+RRGB+SAM) → Encoder (5×Conv2d with stride 2) → Self-Attention → Decoder (5×ConvTranspose2d) → Output (1ch disparity)
- **Critical path**: Pre-compute SAM masks for entire dataset → Concatenate with stereo pair → Encode → Attend → Decode → Regress disparity
- **Design tradeoffs**: SAM vs no SAM (accuracy gain 4.17%→3.68% vs inference overhead 2.18ms→902ms), Attention vs no Attention (minimal improvement with segmentation), ResNet-18 backbone (efficiency vs ResNet-50 capacity)
- **Failure signatures**: Sky artifacts without masking, domain shift requiring KITTI fine-tuning, far-depth degradation (>40m accuracy drop)
- **First 3 experiments**: 1) Baseline ablation comparing 4 model variants on subset, 2) Attention visualization on KITTI images, 3) Resolution sensitivity testing half vs full resolution

## Open Questions the Paper Calls Out

### Open Question 1
How can the sky masking preprocessing approach be modified to improve qualitative disparity output without reducing quantitative accuracy on the test dataset? The authors found a trade-off between removing artifacts and maintaining test accuracy.

### Open Question 2
How does USAM-Net generalize to non-driving datasets such as SceneFlow and Middlebury, and what adaptations are required? The model performed poorly on Middlebury without fine-tuning, suggesting domain-specific limitations.

### Open Question 3
Can the computational overhead of the SAM segmentation pathway (900ms inference) be reduced while maintaining depth estimation accuracy gains? No exploration of lighter segmentation models or joint optimization was conducted to address the 400x inference time increase.

## Limitations

- Limited cross-dataset generalization with D1 errors of 7-10% on KITTI without fine-tuning
- Degraded performance at far ranges (>40m) as indicated by ARD curves
- High computational overhead from SAM preprocessing (900ms/image) despite overall 2.18ms inference time

## Confidence

- **High confidence**: U-Net architecture specification and training pipeline (layer dimensions, loss function, augmentation)
- **Medium confidence**: Segmentation-guided feature integration mechanism (supported by GD improvement but limited direct corpus validation)
- **Medium confidence**: Self-attention effectiveness (visualization provided but lacking mathematical detail)
- **Low confidence**: SAM mask preprocessing specifics and exact attention implementation details

## Next Checks

1. Implement ablation study comparing USAM-Net variants (baseline, +attention, +segmentation, +both) on a small DrivingStereo subset to verify relative contributions before full training
2. Test cross-dataset transfer performance on KITTI without fine-tuning to quantify domain adaptation limitations
3. Evaluate model accuracy at varying depth ranges to characterize far-depth performance degradation