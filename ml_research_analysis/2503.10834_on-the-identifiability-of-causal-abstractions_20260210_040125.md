---
ver: rpa2
title: On the Identifiability of Causal Abstractions
arxiv_id: '2503.10834'
source_url: https://arxiv.org/abs/2503.10834
tags:
- causal
- latent
- which
- graph
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the identifiability of latent causal models
  when interventions are performed on subsets of variables rather than individually.
  The authors establish that under assumptions of faithfulness, absolute continuity
  of latent distributions, and smooth mixing functions, causal models can be identified
  up to abstractions determined by intervention targets' non-descendant sets.
---

# On the Identifiability of Causal Abstractions

## Quick Facts
- **arXiv ID**: 2503.10834
- **Source URL**: https://arxiv.org/abs/2503.10834
- **Reference count**: 28
- **Primary result**: Causal models with subset interventions are identifiable up to quotient graphs determined by intervention targets' non-descendant sets

## Executive Summary
This paper establishes identifiability results for latent causal models when interventions are performed on subsets of variables rather than individually. The authors show that under assumptions of faithfulness, absolute continuity of latent distributions, and smooth mixing functions, causal models can be identified up to abstractions determined by intervention targets' non-descendant sets. They prove that the identifiable quotient graph structure is G⋆/P(σ(nd(I⋆))), where G⋆ is the true causal graph and I⋆ is the family of intervention targets. The work extends previous results requiring atomic interventions on every variable, providing a more realistic framework for causal representation learning with limited intervention capabilities.

## Method Summary
The paper analyzes identifiability of latent causal models using a two-stage approach. First, they consider a structural causal model (SCM) generating latent variables with interventions performed on subsets of these variables. Second, they apply a smooth, invertible mixing function to generate observable variables. The key insight is that when an intervention targets a subset S, the variables in the non-descendant set nd(S) remain invariant across pre- and post-intervention pairs. This invariance property, combined with assumptions of faithfulness and absolute continuity, allows the authors to establish that the causal graph is identifiable only up to a quotient graph determined by the σ-algebra generated by the non-descendants of available intervention targets.

## Key Results
- Causal models with subset interventions are identifiable up to quotient graphs G⋆/P(σ(nd(I⋆)))
- The non-descendant set nd(S) of intervention targets S serves as an invariant block across intervention pairs
- Specific scalar latent variables can be disentangled if their intervention targets have unique non-descendant sets
- The paper extends identifiability results beyond atomic interventions to more realistic subset intervention scenarios

## Why This Works (Mechanism)

### Mechanism 1: Non-Descendant Invariance
When an intervention targets a subset of latent variables S, the variables in the non-descendant set nd(S) remain invariant across pre- and post-intervention pairs. The structural causal model ensures that intervening on S severs incoming edges to S but does not alter the causal mechanisms of nodes upstream. Consequently, the joint distribution of the invariant block z_{nd(S)} serves as a "content" anchor, statistically independent of the "style" changes in the intervened variables.

### Mechanism 2: Partitioning via σ-Algebras
The causal graph is identifiable only up to a quotient graph G⋆/P(σ(nd(I⋆))), where nodes are grouped based on the σ-algebra generated by the non-descendants of available intervention targets. The method aggregates latent variables that cannot be distinguished by the available intervention targets. If two variables always share the same status regarding the non-descendant sets of all possible interventions, they are merged into a single abstract node in the quotient graph.

### Mechanism 3: Scalar Disentanglement via Independence
Specific scalar latent variables can be disentangled (identified individually) if they represent the unique intersection of intervention targets that share a specific non-descendant set. For a non-descendant set N, if the intersection of all intervention targets π(N) resulting in N is a singleton {i}, the post-intervention variable z̃_i is independent of the pre-intervention variable z. This independence, combined with the scalar constraint, allows for precise identification.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) & Descendants**: The entire identification strategy relies on topological ordering. You must understand that non-descendants are "upstream" or "unreachable" nodes from the intervention target. Quick check: Given a path A → B → C, if we intervene on B, is A a non-descendant of B? (Yes)

- **Perfect (Do) Interventions**: The mechanism assumes that intervening on a node S severs the causal link from its parents, replacing the mechanism with noise. If you confuse soft interventions with perfect ones, the invariance proofs fail. Quick check: In a perfect intervention on variable X, does the joint probability P(X, Parent(X)) change or stay the same relative to observation? (Changes)

- **Identifiability vs. Learnability**: This paper proves *identifiability* (theoretical uniqueness of the solution up to abstraction). It explicitly states it does not solve *learnability* (the algorithm to find that solution). Quick check: If a model is identifiable, does that guarantee a neural network will find the correct parameters using SGD? (No)

## Architecture Onboarding

- **Component map**: SCM (DAG G⋆, functions f) + Intervention Sampler (I⋆) → Latents (z, z̃) → Mixing Function (g) → Observables (x, x̃)

- **Critical path**: The primary validation step is checking if the learned graph matches the quotient graph G⋆/P(σ(nd(I⋆))). This requires computing the non-descendant sets of the estimated intervention targets.

- **Design tradeoffs**:
  - Intervention Granularity: Atomic interventions (single nodes) yield the full graph G⋆. Subset interventions yield an abstraction (quotient graph)
  - Latent Dimension: Scalars allow for specific disentanglement; vectors generally do not

- **Failure signatures**:
  - Cyclicity: If the learned quotient graph contains cycles, the abstraction is invalid
  - Trivial Abstraction: If the learned abstraction collapses to a single node, the intervention set I⋆ likely lacks sufficient coverage

- **First 3 experiments**:
  1. Synthetic Validation (Appendix D): Implement the linear Gaussian model. Sample rotation matrix Q and verify if the learned encoder Q^T inverts Q⋆ up to the block-diagonal structure defined by P(σ(nd(I⋆)))
  2. Ablation on Intervention Coverage: Run the system with increasing subset sizes. Plot the resolution of the recovered quotient graph against the specificity of the non-descendant sets
  3. Theorem 3.2 Stress Test: Design a graph where specific nodes satisfy the "unique intersection" criterion and others do not. Verify that only the specific nodes are disentangled while others remain entangled

## Open Questions the Paper Calls Out
None

## Limitations
- The identification results depend critically on perfect (atomic) interventions, which may be difficult to implement in practice
- The paper does not provide algorithms for learning the identified model from finite data, leaving the practical utility limited
- The scalar assumption for disentanglement is restrictive and may rarely be satisfied in real-world scenarios

## Confidence
**High confidence**: The identifiability results for the quotient graph structure (Theorem 3.1) are mathematically rigorous and well-supported by the proof framework.

**Medium confidence**: The disentanglement claims (Theorem 3.2) for individual latent variables are more sensitive to the scalar assumption and the requirement for unique intervention target intersections.

**Low confidence**: The paper's implications for practical causal representation learning systems are not fully developed, and the gap between identifiability and learnability is not bridged with concrete algorithmic proposals.

## Next Checks
1. **Intervention Coverage Sensitivity**: Implement synthetic experiments varying the granularity of intervention targets (single nodes vs. subsets) to empirically demonstrate how the resolution of the identifiable quotient graph degrades with coarser interventions.

2. **Approximate Intervention Analysis**: Modify the perfect intervention assumption to allow for soft interventions and characterize how the identifiability bounds degrade. This would test the robustness of the theoretical framework to more realistic intervention scenarios.

3. **Distribution Stabilizer Characterization**: For scalar latents with continuous distributions, explicitly compute the stabilizer group under distribution-preserving transformations to verify when Theorem 3.2's disentanglement conditions are actually satisfiable in practice.