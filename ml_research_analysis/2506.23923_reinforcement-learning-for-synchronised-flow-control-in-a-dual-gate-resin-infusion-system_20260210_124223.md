---
ver: rpa2
title: Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion
  System
arxiv_id: '2506.23923'
source_url: https://arxiv.org/abs/2506.23923
tags:
- resin
- flow
- control
- agent
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning approach for synchronised
  flow control in a dual-gate resin infusion system, a critical process for manufacturing
  high-performance composites like wind turbine blades. The method uses Proximal Policy
  Optimisation (PPO) to control resin flow from two inlets, aiming to achieve synchronised
  convergence of resin fronts at the centre of the domain, despite permeability asymmetries
  and racetracks that introduce non-linear flow dynamics.
---

# Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System

## Quick Facts
- arXiv ID: 2506.23923
- Source URL: https://arxiv.org/abs/2506.23923
- Reference count: 25
- Primary result: RL agent achieves synchronised resin flow convergence using centroid-penalised reward function with episode scores approaching 1

## Executive Summary
This paper presents a reinforcement learning approach for controlling synchronised flow in a dual-gate resin infusion system, a critical process for manufacturing high-performance composites like wind turbine blades. The method employs Proximal Policy Optimisation (PPO) to manage resin flow from two inlets, addressing the challenge of achieving synchronised convergence at the domain center despite permeability asymmetries and racetracks. The RL agent operates in a partially observable environment using binary sensor feedback from 90 virtual sensors. Results demonstrate that incorporating a centroid-based penalty in the reward function leads to more stable learning and superior synchronisation performance compared to symmetric row activation alone.

## Method Summary
The approach uses Proximal Policy Optimisation (PPO) to control resin flow from two inlets in a dual-gate resin infusion system. The RL agent receives binary sensor feedback from 90 virtual sensors distributed across the domain and operates in a partially observable environment. Two reward formulations were evaluated: one based on symmetric row activation and another augmented with a centroid-based penalty to encourage global flow alignment. The PPO agent learns to adjust gate positions to achieve synchronised resin front convergence at the center of the domain, accounting for non-linear flow dynamics introduced by permeability asymmetries and racetracks.

## Key Results
- Centroid-penalised reward function achieves near-optimal performance with episode scores approaching 1
- Superior learning stability and synchronisation compared to symmetric row activation reward
- Demonstrates RL's potential for spatially distributed control challenges without explicit physical modelling

## Why This Works (Mechanism)
The success stems from the centroid-penalised reward function's ability to provide global flow alignment feedback, which addresses the partial observability limitation. By incorporating spatial information about flow distribution rather than just local symmetry, the agent can learn more effective control policies for managing the complex, non-linear flow dynamics in dual-gate systems.

## Foundational Learning
- Proximal Policy Optimisation (PPO): Why needed - stable policy gradient updates for continuous control; Quick check - bounded policy updates prevent divergence
- Partially Observable Markov Decision Processes (POMDPs): Why needed - real-world sensing limitations; Quick check - binary sensor feedback provides sufficient state information
- Centroid-based reward shaping: Why needed - global flow alignment guidance; Quick check - improved learning stability metrics

## Architecture Onboarding

**Component Map:**
Sensor Array -> PPO Agent -> Gate Controller -> Resin Flow Dynamics -> Reward Function

**Critical Path:**
Binary sensor feedback -> PPO policy update -> Gate position adjustment -> Flow front movement -> Reward calculation -> Policy improvement

**Design Tradeoffs:**
- Sensor resolution vs. computational efficiency: 90 binary sensors chosen for balance
- Reward function complexity vs. learning stability: Centroid penalty adds computational overhead but improves convergence
- Partial observability vs. system simplicity: Binary sensors simplify implementation but limit state information

**Failure Signatures:**
- Asymmetric flow convergence despite symmetric rewards
- Oscillatory gate control behaviour
- Plateaued learning curves with suboptimal episode scores

**First Experiments:**
1. Baseline PPO training with symmetric row activation reward
2. PPO training with centroid-penalised reward function
3. Comparative analysis of convergence speed and stability between reward formulations

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Binary sensor feedback may not capture full complexity of resin flow dynamics
- Simulation assumptions may not fully represent actual manufacturing conditions
- Limited validation on physical dual-gate resin infusion systems

## Confidence
- High: Core finding that centroid-penalised rewards improve learning stability and synchronisation
- Medium: Scalability to larger domains and generalisation across permeability configurations
- Low: Direct transferability to physical systems without additional validation

## Next Checks
1. Physical validation on actual dual-gate resin infusion setup to verify simulation-to-reality transfer
2. Sensitivity analysis across different permeability distributions and racetrack configurations to assess robustness
3. Comparative evaluation against traditional model-based control approaches using industry-standard composite quality metrics