---
ver: rpa2
title: Implicit Bias Injection Attacks against Text-to-Image Diffusion Models
arxiv_id: '2504.01819'
source_url: https://arxiv.org/abs/2504.01819
tags:
- bias
- negative
- image
- more
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel implicit bias injection attack against
  text-to-image diffusion models by precomputing a bias direction in the prompt embedding
  space and adaptively adjusting it based on different inputs. The attack works in
  a plug-and-play manner without requiring model retraining or input manipulation.
---

# Implicit Bias Injection Attacks against Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2504.01819
- Source URL: https://arxiv.org/abs/2504.01819
- Reference count: 40
- Introduces a plug-and-play method to inject subtle implicit bias into diffusion model outputs without retraining or input manipulation.

## Executive Summary
This paper presents IBI-Attack, a novel method to inject implicit bias into text-to-image diffusion model outputs by manipulating prompt embeddings. The attack precomputes a bias direction in embedding space and adaptively adjusts it for different inputs via a learned module. Experiments on Stable Diffusion show the attack achieves high success rates (up to 80.2% for negative bias) while maintaining semantic and structural similarity to original outputs. The method is effective across multiple domains, transferable, and robust against existing debiasing defenses, highlighting significant vulnerabilities in diffusion models.

## Method Summary
IBI-Attack computes a bias direction vector by averaging differences between biased and neutral prompt embeddings. An adaptive module, trained to selectively amplify this direction based on input content, is inserted after the text encoder. At inference, the module adjusts the bias injection strength dynamically using attention mechanisms over token and embedding dimensions. The approach requires no model retraining or input manipulation, enabling stealthy bias injection while preserving image quality.

## Key Results
- High attack success rates: up to 80.2% for negative bias and 78.6% for positive bias on human image evaluation.
- Strong stealth: CLIP similarity scores above 0.87 and SSIM above 0.89, with minimal FID increase.
- Robust across domains: successful bias injection in person, animal, and nature-related prompts.
- Effective against defenses: existing debiasing methods show little impact on attack success.

## Why This Works (Mechanism)
The attack exploits the semantic continuity in the high-dimensional prompt embedding space. By computing a bias direction vector as the mean difference between biased and neutral embeddings, the method identifies a consistent shift that corresponds to the desired bias. The adaptive module learns to modulate this shift per input, allowing subtle bias injection without obvious artifacts. This approach bypasses traditional input-level defenses since the manipulation occurs at the embedding level.

## Foundational Learning
- **Prompt embedding manipulation**: Directly altering text embeddings rather than inputs enables stealthy bias injection.
  - Why needed: Avoids obvious input-level changes that could be detected or filtered.
  - Quick check: Verify that CLIP similarity between original and attacked prompts remains high.

- **Bias direction vector computation**: Averaging differences between biased and neutral embeddings provides a stable bias direction.
  - Why needed: Ensures consistent bias injection across varied inputs.
  - Quick check: Confirm that v_diff captures the intended sentiment shift via nearest neighbor analysis.

- **Adaptive attention module**: Learning to modulate bias injection per input preserves semantic quality.
  - Why needed: Prevents excessive distortion and maintains image fidelity.
  - Quick check: Compare success rates with and without the adaptive module.

- **Plug-and-play design**: Inserting the module post-encoder avoids retraining the diffusion model.
  - Why needed: Enables practical deployment without model access or modification.
  - Quick check: Validate that the module can be inserted into different diffusion model variants.

## Architecture Onboarding

**Component map**: Text encoder -> Adaptive module -> Diffusion model (e.g., Stable Diffusion 2.1)

**Critical path**: Text encoding → Adaptive module (attention + MLP) → Bias injection → Image generation

**Design tradeoffs**: Adaptive injection preserves image quality but requires training; fixed injection is simpler but risks distortion.

**Failure signatures**: Low attack success (MLLM or human evaluation); high CLIP/SSIM drop indicates excessive distortion.

**3 first experiments**:
1. Generate 200 neutral/biased prompt pairs and compute v_diff; verify sentiment shift via manual inspection.
2. Train the adaptive module on the 200 pairs; evaluate loss convergence and attention pattern stability.
3. Perform bias injection on 1,000 validation captions; measure attack success and CLIP/SSIM/FID scores.

## Open Questions the Paper Calls Out
- How can debiasing or defense mechanisms be specifically designed to detect and neutralize sentence-level bias direction vectors, given that current word-level debiasing methods fail against IBI-Attacks?
- To what extent does the IBI-Attack framework generalize to complex, non-emotional implicit biases (e.g., cultural, religious, or political) that may lack the clear directional semantics found in "positive" or "negative" sentiment?
- Can the adaptive feature selection module be trained to overcome strong, inherent model biases (like SDXL's default positivity) more effectively without causing excessive semantic distortion?

## Limitations
- The exact MLP architecture details (hidden layer sizes, activation functions, bias terms) are not specified, requiring researcher judgment.
- Training hyperparameters like batch size and weight initialization scheme are unspecified, which may affect convergence and performance.
- No explicit ablation studies are provided for the adaptive module's contribution versus a naive fixed bias injection approach.

## Confidence
- **High confidence**: The core concept of computing a bias direction via prompt embeddings and applying it adaptively is well-defined and reproducible.
- **Medium confidence**: The attack's effectiveness metrics (success rates, stealth scores) are credible but depend on the quality of prompt generation and training details.
- **Low confidence**: Generalization claims to other domains (animal, nature) and robustness against defenses are not fully substantiated with detailed results.

## Next Checks
1. **Ablation study**: Compare the full adaptive module against a baseline that injects v_diff directly without adaptation to quantify its contribution to stealth and success.
2. **Prompt quality audit**: Manually inspect a random sample of the 200 neutral/biased prompt pairs to ensure the adjective-prefix rewriting reliably conveys the intended bias.
3. **Architecture sensitivity test**: Vary the MLP hidden layer size and activation function to assess the robustness of attack performance to these design choices.