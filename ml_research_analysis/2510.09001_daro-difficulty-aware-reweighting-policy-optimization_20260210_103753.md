---
ver: rpa2
title: 'DARO: Difficulty-Aware Reweighting Policy Optimization'
arxiv_id: '2510.09001'
source_url: https://arxiv.org/abs/2510.09001
tags:
- loss
- grpo
- daro
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical flaw in existing reinforcement
  learning with verifiable rewards (RLVR) algorithms, termed the "loss scale issue,"
  where training disproportionately focuses on samples at certain difficulty levels
  due to static weighting schemes based on empirical pass rates. To address this,
  the authors propose Difficulty-Aware Reweighting Policy Optimization (DARO), which
  dynamically adjusts the loss contribution of each difficulty group based on the
  model's learning state using learnable weight parameters.
---

# DARO: Difficulty-Aware Reweighting Policy Optimization

## Quick Facts
- arXiv ID: 2510.09001
- Source URL: https://arxiv.org/abs/2510.09001
- Reference count: 29
- Primary result: DARO outperforms four RLVR baselines on six math benchmarks, achieving 50.8% average accuracy vs 49.4% for GRPO on Qwen2.5-Math-7B

## Executive Summary
This paper identifies a critical flaw in existing reinforcement learning with verifiable rewards (RLVR) algorithms, termed the "loss scale issue," where training disproportionately focuses on samples at certain difficulty levels due to static weighting schemes based on empirical pass rates. To address this, the authors propose Difficulty-Aware Reweighting Policy Optimization (DARO), which dynamically adjusts the loss contribution of each difficulty group based on the model's learning state using learnable weight parameters. DARO treats different empirical pass rates as distinct tasks and assigns adaptive weights to balance the training signal across all difficulty levels. Extensive experiments on six math benchmarks across three base models (Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, and Llama3.1-8B) show that DARO consistently outperforms four leading baselines (GRPO, LIPO, Dr. GRPO, and DAPO), achieving higher average accuracy (e.g., 50.8% vs 49.4% for Qwen2.5-Math-7B) and significantly faster convergence while maintaining superior final performance.

## Method Summary
DARO addresses the "loss scale issue" in RLVR by treating empirical pass rates (μ) as task indicators and assigning learnable weights to each difficulty group. For K=8 responses per prompt, samples are grouped by μ=k/K, with μ=0 and μ=1 excluded. The loss function L = Σ_{μ≠0,1}(w_μ·L_μ − ln w_μ) combines weighted per-group losses with a regularization term that prevents weight collapse. Weights w_μ are optimized jointly with model parameters θ using AdamW with separate learning rates (1e-6 for θ, 1e-3 for weights). The method uses token-mean aggregation, clip-higher with asymmetric bounds (ϵ_low=0.2, ϵ_high=0.28), and no KL terms. Training runs for 300 steps with batch size 128, mini-batch 64, and generation batch 384.

## Key Results
- DARO achieves 50.8% average accuracy vs 49.4% for GRPO on Qwen2.5-Math-7B
- Consistently outperforms four baselines (GRPO, LIPO, Dr. GRPO, DAPO) across three base models
- Shows significantly faster convergence while maintaining superior final performance
- Demonstrates the loss scale issue across all three base models with static weighting schemes

## Why This Works (Mechanism)

### Mechanism 1: Loss Scale Issue Detection and Balancing
Static weighting schemes (like w ∝ √(μ(1-μ))) cause medium-difficulty samples (μ≈0.5) to dominate training, downweighting both easy and hard samples. DARO's learnable weights w_μ automatically balance contributions: the regularizer -ln(w_μ) drives w_μ toward inverse proportionality with loss magnitude, ensuring w_μ × L_μ stays roughly equal across difficulty groups. Core assumption: response length correlates with sample difficulty (verified empirically in Figure 3), and the clip function's bounded nature permits Hoeffding-based approximation.

### Mechanism 2: Adaptive Task Rebalancing via Gradient-Based Weight Learning
Joint optimization of weights and policy parameters enables dynamic adaptation to evolving model capabilities. Weights w_μ are optimized via gradient descent (lr=1e-3). At optimality: ∂L_total/∂w_μ = L_μ + N'(w_μ) = 0, with N'(w_μ) = -Cw_μ^(-1) ensuring w_μ = C/L_μ. This creates negative feedback: high-loss groups get lower weights, preventing domination. Core assumption: the multitask learning analogy (μ levels as tasks) holds, and online weight updates remain stable.

### Mechanism 3: Empirical Pass Rate as Task Indicator
Grouping samples by empirical pass rate μ provides a stable, training-state-aware difficulty signal. For K=8 responses per prompt, μ takes discrete values {k/8 | k∈{0,1,...,8}}. Samples with μ=0 or 1 are excluded (zero advantage). Remaining groups partition training data by current solvability, creating stable task boundaries that evolve with model improvement. Core assumption: empirical pass rate accurately reflects sample difficulty relative to current model; K=8 provides sufficient resolution.

## Foundational Learning

- **Policy gradient with group-relative advantage**: DARO builds on GRPO's core insight—estimating advantage from group rewards without a value model. Understanding A_i = (r_i - μ)/σ is essential. Quick check: Can you explain why GRPO removes the need for a learned value function?

- **Regularization via negative entropy (-ln w)**: The -ln(w_μ) term prevents weight collapse and drives the desired inverse-proportionality. Without it, w_μ→0 would be optimal. Quick check: What happens to the loss if all w_μ→0, and how does -ln(w_μ) prevent this?

- **Multi-task learning with homoscedastic uncertainty weighting**: DARO's weight learning resembles uncertainty-based task balancing (Kendall et al. 2018), where learned weights scale task losses. Quick check: How does treating μ-levels as tasks enable independent training signal modulation?

## Architecture Onboarding

- **Component map**: Rollout worker -> Grouping layer -> Weight parameters -> Loss aggregator -> Joint optimizer
- **Critical path**: Sample batch B={q_1, ..., q_N} → Generate K responses per query → compute μ_q = mean(rewards) → Filter: keep only samples with 0<μ_q<1 → Compute L_μ for each group via standard GRPO-style loss → Update w_μ and θ jointly via backprop
- **Design tradeoffs**: K (responses per prompt): Higher K gives finer μ resolution but increases compute 8×. Weight learning rate: Higher lr (1e-3) adapts faster but risks instability. Regrouping frequency: Paper recomputes μ each step.
- **Failure signatures**: Weight collapse (all w_μ→0), static weighting behavior (w_μ stop changing), empty groups (some μ levels never sampled), slower convergence than GRPO
- **First 3 experiments**: 1) Baseline replication: Implement standard GRPO with K=8 on Qwen2.5-Math-7B subset; track per-μ loss curves to confirm loss scale issue exists. 2) Weight dynamics ablation: Add learnable weights but freeze them at initial values; compare to full DARO. 3) Hyperparameter sensitivity: Sweep weight learning rate {1e-4, 1e-3, 1e-2} on small scale; monitor weight variance and convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
Does DARO maintain its superior convergence speed and final performance when scaled to significantly larger, state-of-the-art models? Basis: The authors explicitly state "Future research will focus on scaling DARO to larger, state-of-the-art models." Why unresolved: Experiments were restricted to smaller base models (1.5B, 7B, and 8B parameters). What evidence would resolve it: Benchmark results applying DARO to models with 70B+ parameters compared against standard GRPO baselines.

### Open Question 2
Can the difficulty-aware reweighting mechanism effectively generalize to reasoning domains outside of mathematics? Basis: The paper notes the limitation of the current scope and plans on "extending its application to more general reasoning domains beyond mathematics." Why unresolved: The current validation relies exclusively on six mathematical benchmarks. What evidence would resolve it: Evaluation of DARO on non-math tasks involving verifiable rewards, such as code generation or formal logic.

### Open Question 3
How does DARO perform in complex, multi-turn agentic settings involving tool use? Basis: The authors write "we plan to investigate its effectiveness in more complex, agentic settings involving multi-turn interaction and tool use to validate its generality." Why unresolved: The current framework assumes single-turn interactions with immediate verifiable rewards. What evidence would resolve it: Performance metrics in interactive environments (e.g., web navigation or API usage) where rewards are delayed or cumulative.

### Open Question 4
How sensitive is the dynamic weight optimization to the granularity of the difficulty groups defined by the number of samples (K) per prompt? Basis: The method relies on grouping prompts by empirical pass rate μ=k/K, but the experiments consistently use a fixed K=8. Why unresolved: It is unclear if fewer or more samples per prompt would alter the precision of the loss reweighting or the stability of the learnable weights. What evidence would resolve it: An ablation study analyzing performance and weight distribution variance across different values of K.

## Limitations
- The paper lacks ablation studies isolating the contribution of dynamic weight adaptation versus the fundamental fix to the loss scale issue
- No qualitative analysis shows how weight evolution correlates with difficulty distribution shifts during training
- Assumes response length correlates with difficulty and that K=8 provides sufficient granularity

## Confidence

**High Confidence**: The identification of the loss scale issue in GRPO is well-supported by empirical evidence across three base models (Figure 2). The mathematical framework for adaptive weight learning via gradient descent is sound and internally consistent.

**Medium Confidence**: The superiority claims are moderately supported by experiments on three base models across six benchmarks. However, the lack of ablation studies and cross-domain validation reduces confidence in DARO's general effectiveness versus simpler fixes to the loss scale issue.

**Low Confidence**: Claims about weight dynamics (Figure 4, 5) would benefit from more extensive analysis across training regimes and model scales. The assumption that μ-based grouping remains stable as model capabilities evolve is untested.

## Next Checks
1. **Ablation Study**: Implement a "static DARO" variant where weights are initialized optimally but not updated. Compare convergence speed and final accuracy to full DARO to quantify the contribution of dynamic weight adaptation versus the fundamental loss scale fix.

2. **Cross-Domain Transfer**: Apply DARO to non-mathematical reasoning tasks (e.g., code generation or text summarization with verifiable rewards). Track whether the loss scale issue persists and if weight dynamics generalize beyond math problems.

3. **Weight Stability Analysis**: Extend training beyond 300 steps (e.g., to 1000 steps) while monitoring weight variance and loss scale balance. Test whether weights converge to stable values or continue adapting, and whether extreme weight values (>10× initialization) emerge.