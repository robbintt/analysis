---
ver: rpa2
title: 'Unlocking Pretrained LLMs for Motion-Related Multimodal Generation: A Fine-Tuning
  Approach to Unify Diffusion and Next-Token Prediction'
arxiv_id: '2503.06119'
source_url: https://arxiv.org/abs/2503.06119
tags:
- motion
- generation
- momug
- text
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MoMug, a unified framework that integrates
  diffusion-based continuous motion generation with autoregressive discrete text prediction
  within a single pretrained LLM. By leveraging LoRA-based fine-tuning, MoMug enables
  seamless switching between motion and text generation, effectively combining the
  strengths of both diffusion and LLM-based approaches.
---

# Unlocking Pretrained LLMs for Motion-Related Multimodal Generation: A Fine-Tuning Approach to Unify Diffusion and Next-Token Prediction

## Quick Facts
- arXiv ID: 2503.06119
- Source URL: https://arxiv.org/abs/2503.06119
- Reference count: 40
- Primary result: MoMug improves FID by 38% and mean accuracy across seven metrics by 16.61% on text-to-motion task compared to MotionGPT baseline

## Executive Summary
This paper introduces MoMug, a unified framework that integrates diffusion-based continuous motion generation with autoregressive discrete text prediction within a single pretrained LLM. By leveraging LoRA-based fine-tuning, MoMug enables seamless switching between motion and text generation, effectively combining the strengths of both diffusion and LLM-based approaches. The framework addresses limitations in existing motion generation methods that rely on quantization or separate models, achieving state-of-the-art performance on both text-to-motion and motion-to-text tasks while maintaining efficiency through parameter-efficient adaptation.

## Method Summary
MoTiger proposes a unified framework that integrates diffusion-based continuous motion generation with autoregressive discrete text prediction within a single pretrained LLM. The method leverages LoRA-based fine-tuning to enable seamless switching between motion and text generation. The model processes inputs as a unified sequence combining text tokens, diffusion timesteps, and motion embeddings, with a regression head for motion prediction and the standard text head for token prediction. Training involves balancing diffusion denoising loss with language modeling loss, allowing the model to generate smooth motion sequences while maintaining text generation capabilities.

## Key Results
- MoMug improves FID by 38% on text-to-motion task compared to MotionGPT baseline
- Achieves 16.61% mean accuracy improvement across seven metrics on text-to-motion task
- Improves mean accuracy across eight metrics by 8.44% on motion-to-text task
- 1B parameter model outperforms larger 3B and 8B variants, contrary to standard scaling laws

## Why This Works (Mechanism)

### Mechanism 1: Dual-Objective Sequence Modeling
The framework treats a frozen LLM as a generic sequence processor, conditioning it on noisy motion frames and diffusion timesteps to denoise continuous motion while retaining discrete text generation. The model switches behavior based on input modality, minimizing $L_{LLM}$ (cross-entropy) for text and $L_{DDPM}$ (MSE) for motion within the same forward pass. This works because the LLM's latent space is sufficiently robust to map continuous kinematic vectors alongside semantic text tokens without catastrophic interference.

### Mechanism 2: Diffusion-Driven Global Coherence
Generating motion via iterative denoising (diffusion) preserves temporal smoothness better than next-token prediction because it models the sequence as a continuous trajectory rather than independent discrete steps. The diffusion objective is structurally superior to autoregressive objectives for maintaining kinematic constraints of human motion, as it forces the model to learn the global distribution of the motion sequence.

### Mechanism 3: Semantic Anchoring via Frozen LLM
Leveraging the frozen weights of a pre-trained LLM provides a stronger semantic prior for text-to-motion alignment than training a text encoder from scratch. The model uses the LLM's existing attention layers to process both text and motion embeddings, with LoRA adapters learning to warp the text manifold to align with the motion manifold without forgetting linguistic capabilities.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPM)**: The mathematical engine for motion generation branch, explaining forward process (adding noise) vs. reverse process (learning to denoise). Quick check: Can you explain why predicting the noise ε or the clean sample x₀ allows the model to generate data from pure Gaussian noise?

- **LoRA (Low-Rank Adaptation)**: The efficiency technique making the unified model feasible by explaining how the model learns new kinematic "vocabularies" without forgetting linguistic ones. Quick check: If you freeze original weights W and add low-rank update ΔW = BA, what happens to the model's output for pure text inputs if B and A are initialized to zero?

- **Unified Multimodal Embeddings**: The core architectural trick of feeding the LLM a "sentence" composed of text tokens and motion vectors. Quick check: How does the model distinguish between a token representing the word "run" and a vector representing the velocity of a joint at timestep t? (Hint: check the projection layers).

## Architecture Onboarding

- **Component map**: Text (Tokenizer) -> Text Embedding -> LLM Backbone (Frozen + LoRA) -> Text Head (Cross-Entropy) + Motion Head (W_out -> MSE); Diffusion Timestep (MLP) + Noisy Motion (Linear Proj) -> LLM Backbone -> Motion Head

- **Critical path**: 1. Data Prep: Sample text-motion pair, add noise to motion to get x_t; 2. Embedding: Project x_t and timestep t into LLM dimension, concatenate with text tokens; 3. Forward: Pass mixed sequence through Llama-3 + LoRA; 4. Branching: Calculate Cross-Entropy loss on text tokens, extract motion indices, apply W_out to predict x₀, calculate MSE (L_DDPM); 5. Optimization: Backprop through LoRA and Projection layers only

- **Design tradeoffs**: Model Scale vs. Performance shows 1B outperforms 8B, suggesting pre-trained features are more critical than model capacity for this data regime; Continuous vs. Discrete trades discrete tokenization efficiency for high-fidelity smoothness of continuous diffusion

- **Failure signatures**: Drifting Motion (avatar floats/slides, indicates velocity/position encoding not being denoised effectively); Semantic Decoupling (motion ignores text prompt, failure in λ weighting or LoRA rank); Catastrophic Forgetting (can generate motion but degrades motion captioning, implies L_LLM was underweighted)

- **First 3 experiments**: 1. Projection Sanity Check: Overfit single batch, verify Motion Head can perfectly reconstruct x₀ from x_t; 2. Scaling Rule Validation: Train LoRA on 1B, 3B, 8B Llama variants to verify non-monotonic scaling behavior; 3. Lambda (λ) Sweep: Grid search on loss weight λ to find balance where model generates smooth motion AND valid text

## Open Questions the Paper Calls Out
- How can explicit kinematic constraints, such as human joint limitations, be mathematically integrated into the MoMug framework to prevent physically implausible motion generation?
- Why does the proposed framework exhibit inverse scaling performance, where the 1B model outperforms the 3B and 8B variants on text-to-motion tasks?
- To what extent does the balance between diffusion loss (L_DDPM) and language modeling loss (L_LLM) affect the model's ability to perform simultaneous generation and understanding tasks?

## Limitations
- Inverse scaling performance (1B outperforming 8B) lacks clear explanation and may reflect dataset limitations rather than architectural superiority
- Results are limited to human motion datasets (HumanML3D, KIT-ML) without validation on broader motion domains
- The critical loss balancing hyperparameter λ is manually tuned without systematic sensitivity analysis

## Confidence
- **High Confidence**: Dual-objective sequence modeling mechanism is technically sound and well-supported
- **Medium Confidence**: Diffusion providing superior temporal smoothness is supported by FID improvements but could be influenced by other factors
- **Low Confidence**: Claims about "low complexity" of motion data explaining 1B→8B reversal are speculative and not empirically validated

## Next Checks
1. Systematically vary λ across [0.001, 0.01, 0.1, 1.0] and measure degradation in both T2M and M2T tasks to establish robust operating point
2. Evaluate MoMug on non-human motion dataset (animal motion capture or robotic motion) to test semantic anchoring generalization
3. Train ablation variants with (a) diffusion only, (b) autoregressive only, and (c) unified without LoRA to quantify individual contributions of architectural innovations