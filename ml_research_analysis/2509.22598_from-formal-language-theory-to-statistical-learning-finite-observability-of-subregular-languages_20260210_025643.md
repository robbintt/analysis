---
ver: rpa2
title: 'From Formal Language Theory to Statistical Learning: Finite Observability
  of Subregular Languages'
arxiv_id: '2509.22598'
source_url: https://arxiv.org/abs/2509.22598
tags:
- languages
- subregular
- finite
- classes
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes that all standard subregular language classes
  are linearly separable, providing a geometric foundation for their learnability.
  The authors define "finite observability," showing that membership in these classes
  depends only on finitely many primitive predicates.
---

# From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages

## Quick Facts
- arXiv ID: 2509.22598
- Source URL: https://arxiv.org/abs/2509.22598
- Reference count: 16
- Primary result: All standard subregular language classes are linearly separable, enabling simple statistical learning with strong theoretical guarantees.

## Executive Summary
This paper establishes a fundamental bridge between formal language theory and statistical learning by proving that all standard subregular language classes (SL, SP, TSL, etc.) are linearly separable when represented by their deciding predicates. The key theoretical contribution is "finite observability" - the property that membership in these classes can be decided by finitely many primitive predicates. This property implies linear separability in Boolean feature spaces, allowing direct application of statistical learning theory. The authors demonstrate this through synthetic experiments showing perfect separability under noise-free conditions, and real-data experiments on English morphology where learned features align with linguistically interpretable constraints.

## Method Summary
The method involves mapping input strings to Boolean predicate vectors (e.g., presence/absence of specific substrings), then projecting these into feature spaces where linear separation is theoretically guaranteed. For synthetic data, the approach uses Logistic Regression on boundary-aware k-grams and subsequence features to achieve 100% accuracy on noise-free datasets. For real morphological data, the method extracts affix sequences from the MorphoLex corpus and trains linear classifiers to distinguish valid from invalid morphological forms. The theoretical framework proves that finite observability (the ability to decide language membership via finite predicates) implies linear separability with margin bounds, enabling sample complexity guarantees.

## Key Results
- Proves all standard subregular language classes are linearly separable via finite observability
- Synthetic experiments achieve perfect accuracy (100%) on noise-free datasets
- Real-data experiments on English morphology achieve 86% F1 score with interpretable feature weights
- Margin analysis confirms theoretical separability degrades predictably under noise

## Why This Works (Mechanism)

### Mechanism 1: Finite Observability implies Boolean Linear Separability
The architecture maps input strings to a "truth vector" of predicate outcomes (e.g., "contains substring X", "has prefix Y"). By embedding these vectors into a "minterm" space (identifying every possible combination of truth values), the paper proves a separating hyperplane exists that perfectly isolates valid language vectors from invalid ones with a margin of at least 1/2. This requires the target language to belong to a standard subregular class where a finite set of bounded features fully determines grammaticality.

### Mechanism 2: Linguistic Constraint Recovery via Low-Dimensional Projections
Standard linear classifiers (e.g., Logistic Regression) can recover interpretable linguistic constraints directly from subregular features without neural networks. Instead of using the exponential "minterm" map for theoretical bounds, the system uses direct low-dimensional features (e.g., specific k-grams or tier projections). Because the underlying class is finitely observable, these features are sufficient statistics for the classification task, allowing simple linear weights to act as the "grammar."

### Mechanism 3: Margin-Based Generalization
The geometric separation provided by finite observability guarantees bounded sample complexity. Because the data is perfectly separable with a constant margin (γ ≥ 1/(2||w||₂)) in the feature space, standard statistical learning bounds (Perceptron mistake bounds, VC dimension) apply. This ensures that a model trained on limited data will generalize to unseen strings if they obey the same subregular constraints.

## Foundational Learning

- **Concept: Finite Observability**
  - Why needed here: This is the paper's core theoretical contribution. It redefines language classes not by generative automata, but by finite "viewpoints" (predicates).
  - Quick check question: Can you define a language that *lacks* finite observability? (Hint: Think of counting).

- **Concept: Subregular Hierarchy (SL, SP, TSL)**
  - Why needed here: The paper proves linear separability *specifically* for these classes. Understanding the difference between Strictly Local (SL) and Tier-based (TSL) is crucial for choosing the right feature predicates.
  - Quick check question: Why is TSL considered more expressive than SL but still subregular?

- **Concept: Minterm Feature Maps**
  - Why needed here: This is the mathematical tool used to prove linear separability. It transforms logical predicates into vectors that act as coordinates on a Boolean hypercube.
  - Quick check question: Why is the dimensionality of the minterm map exponential in the number of predicates (2^n)?

## Architecture Onboarding

- **Component map**: Input string -> Predicate Extractor -> Feature Projector -> Linear Separator
- **Critical path**: The **Predicate Extractor**. The theoretical guarantees hold *only if* the finite set of predicates chosen can actually decide the target language (e.g., choosing k-grams for a Strictly Local language).
- **Design tradeoffs**:
  - Minterm Embedding vs. Direct Predicates: The Minterm map guarantees a margin of 1/2 and handles complex logical interactions (like XOR) but is exponentially large (2^n). Direct predicates are compact (n) and interpretable but may struggle with complex non-monotone constraints.
  - Fixed k vs. Tier Projection: For long-distance dependencies (like harmony), simple k-grams (SL) fail. You must implement Tier Projection (TSL) to filter irrelevant symbols before extracting features.
- **Failure signatures**:
  - Theoretical Mismatch: 100% accuracy on synthetic SL data but failure on real data suggests the real data is *not* Strictly Local (it may be TSL or SP).
  - Margin Collapse: If Section 5.2-style margin analysis shows negative margins on test data, the chosen predicate set is incomplete (missing necessary features).
- **First 3 experiments**:
  1. Synthetic SL3 Verification: Generate strings with forbidden trigrams. Train a Logistic Regression on trigram features. Verify 100% accuracy and positive margins (replicating Section 5.1).
  2. Tier Projection (TSL) vs. Local (SL): Construct a vowel harmony dataset. Compare performance of a raw k-gram model vs. a model that projects only vowels before feature extraction.
  3. Interpretability Check: Train on the MorphoLex dataset (Section 6). Extract the top-weighted features and verify they correspond to linguistic rules (e.g., suffix ordering) rather than noise.

## Open Questions the Paper Calls Out

- Do larger language families beyond the standard subregular classes (e.g., context-sensitive subclasses, mildly context-sensitive languages) admit analogous notions of finite observability?
- Does the effective computational capacity of neural language models in practice align more closely with subregular structure than with their theoretical upper bounds?
- Can finite observability and linear separability be maintained when negative training examples are drawn from naturally occurring linguistic errors (e.g., learner productions, historical attestations) rather than synthetically perturbed well-formed strings?
- Is there a systematic characterization of the boundary within the regular languages where finite observability holds versus breaks down?

## Limitations

- Theoretical guarantees depend critically on choosing the correct finite predicate set for the target language
- Minterm embedding grows exponentially with predicate count, limiting practical scalability
- Real-world noise patterns may behave differently than synthetic noise models tested

## Confidence

- **High Confidence**: The core mathematical result that finite observability implies linear separability for standard subregular classes (SL, SP, TSL)
- **Medium Confidence**: The claim that linguistic constraints can be directly recovered from linear weights without neural networks
- **Medium Confidence**: The generalization bounds based on margin theory

## Next Checks

1. **Predicate Set Completeness**: Systematically test whether performance on morphological data degrades when removing specific predicate types (e.g., removing 4-grams while keeping 3-grams).

2. **Cross-linguistic Generalization**: Apply the methodology to a morphologically rich language from a different family (e.g., Turkish or Finnish) to test whether the linear separability property holds across typologically diverse systems.

3. **Noise Robustness Scaling**: Conduct experiments varying both noise levels and predicate set sizes to map the relationship between theoretical separability and practical learnability under realistic conditions.