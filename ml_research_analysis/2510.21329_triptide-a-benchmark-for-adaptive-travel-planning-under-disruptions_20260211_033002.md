---
ver: rpa2
title: 'TripTide: A Benchmark for Adaptive Travel Planning under Disruptions'
arxiv_id: '2510.21329'
source_url: https://arxiv.org/abs/2510.21329
tags:
- disruption
- plan
- disruptions
- travel
- traveler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TripTide introduces the first benchmark for evaluating LLM-driven
  travel planning under realistic disruptions. It combines disruption severity levels
  (step, day, plan) with traveler tolerance profiles (Flexi-Venturer, Plan-Bound)
  to simulate real-world travel challenges.
---

# TripTide: A Benchmark for Adaptive Travel Planning under Disruptions

## Quick Facts
- arXiv ID: 2510.21329
- Source URL: https://arxiv.org/abs/2510.21329
- Reference count: 40
- Primary result: First benchmark evaluating LLM-driven travel planning under realistic disruptions with severity levels and traveler tolerance profiles

## Executive Summary
TripTide introduces the first benchmark for evaluating LLM-driven travel planning under realistic disruptions. It combines disruption severity levels (step, day, plan) with traveler tolerance profiles (Flexi-Venturer, Plan-Bound) to simulate real-world travel challenges. The framework proposes three evaluation metrics—Preservation of Intent, Responsiveness, and Adaptability—and validates them via LLM-as-a-judge and human expert review. Experiments across 1,000 travel plans reveal that GPT-4o maintains strong sequential and semantic consistency, with spatial deviations diminishing for longer trips. Responsiveness declines with plan length, highlighting limits in LLM robustness under extended disruptions. The benchmark establishes a foundation for assessing adaptability, personalization, and resilience in travel planning systems.

## Method Summary
TripTide uses 1,000 travel planning queries from the TripCraft dataset, augmented with 11,058 potential disruptions categorized by type and severity. Target LLMs (GPT-4o, Qwen2.5-7B-Instruct, Phi-4-mini-Instruct) receive prompts containing the original plan, user persona, disruption info, severity, and tolerance level. The framework evaluates three axes: Preservation of Intent (delivery rate, CPR, HCPR), Responsiveness (mitigation rate), and Adaptability (semantic, spatial, and sequential divergence). Evaluation combines automated metrics, LLM-as-a-judge scoring, and human expert review on 100 samples.

## Key Results
- GPT-4o maintains strong sequential and semantic consistency, with spatial deviations diminishing for longer trips
- Responsiveness declines with plan length (89.53% for 3-day → 79.82% for 7-day), revealing LLM robustness limits
- Semantic adaptability scores show minimal drift (0.02-0.34 across plan durations) but fail to capture grounded attribute violations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical disruption severity classification enables proportional plan revision, preserving user intent while minimizing unnecessary changes
- **Mechanism:** Disruptions categorized into step-level, day-level, and plan-level severity with traveler tolerance (Flexi-Venturer vs. Plan-Bound) guiding revision scope. For Plan-Bound travelers under step-level disruptions, edits restricted to affected slot.
- **Core assumption:** LLMs can reliably interpret and adhere to explicit scope constraints when provided in prompt
- **Evidence anchors:** [abstract] severity-tolerance coupling; [section] Table 1 defines severity taxonomy; [corpus] iTIMO lacks this coupling
- **Break condition:** If LLM ignores tolerance constraints (e.g., revises multiple days for Plan-Bound under step-level disruption)

### Mechanism 2
- **Claim:** Semantic adaptability metrics grounded in persona-POI alignment provide quantifiable proxy for intent preservation during revision
- **Mechanism:** Semantic adaptability score computes absolute difference between initial and revised plan persona scores using BERT cosine similarity between persona components and POI names
- **Core assumption:** BERT embeddings capture semantic attributes relevant to traveler personas
- **Evidence anchors:** [abstract] GPT-4o maintains strong semantic consistency; [section] Equations 2-3 define Asem; Table 3 reports GPT-4o semantic scores
- **Break condition:** If substitutions preserve POI names but violate unstated preferences (e.g., budget, cuisine)

### Mechanism 3
- **Claim:** Extended plan horizons induce spatial coherence but degrade responsiveness, revealing length-mediated robustness trade-off
- **Mechanism:** For GPT-4o, spatial adaptability decreased from 1.22 (3-day) to 0.30 (7-day), while responsiveness declined from 89.53% to 79.82%
- **Core assumption:** Spatial score meaningfully captures geographic convenience
- **Evidence anchors:** [abstract] Spatial deviations diminishing for longer trips; [section] Table 3 shows Aspa: 1.22 → 0.30; Responsiveness: 89.53% → 79.82%
- **Break condition:** If spatial coherence achieved via unrealistic substitutions (e.g., zoo at night)

## Foundational Learning

- **Concept: Few-shot prompt engineering for constrained generation**
  - **Why needed here:** TripTide uses few-shot prompts to guide LLMs in generating coherent, minimally-altered revised itineraries. Without understanding prompt structure, the pipeline will produce inconsistent revisions.
  - **Quick check question:** Given a prompt with 3 example itinerary revisions and a new disruption, can you predict where the model might over-generalize from examples vs. adhere to explicit scope constraints?

- **Concept: BERT-based semantic similarity and its limitations**
  - **Why needed here:** Asem metric relies on BERT embeddings of POI names and persona phrases. Understanding that BERT captures distributional semantics—but not grounded attributes like price or dietary options—is essential for interpreting metric results.
  - **Quick check question:** If two restaurant names have high BERT similarity but one is fine dining and other is fast food, would Asem capture the mismatch? Why or why not?

- **Concept: Multi-metric evaluation alignment (automated vs. human judgment)**
  - **Why needed here:** TripTide combines automated metrics, LLM-as-a-judge scores, and human expert evaluation. Understanding how these signals correlate—or diverge—is critical for benchmark interpretation.
  - **Quick check question:** If an LLM-judge rates a revision "Excellent" but automated HCPR score is low, what hypotheses would you investigate first?

## Architecture Onboarding

- **Component map:** TripCraft augmentation layer → Disruption generator → Revision planner → Validation pipeline → Evaluation stack
- **Critical path:** Load base itinerary + persona + constraints → Generate/select disruption → Prompt target LLM → Script-based validation → Compute metrics → Human expert review (if needed)
- **Design tradeoffs:** Binary tolerance modeling oversimplifies real-world flexibility; spatial metrics don't model transit frequency; LLM-as-judge may favor GPT-4o-style outputs
- **Failure signatures:** Hallucinated entities, tolerance mismatch, temporal infeasibility, semantic drift
- **First 3 experiments:** 1) Baseline replication with GPT-4o on 50 samples to verify pipeline correctness; 2) Ablation on tolerance prompting to quantify scope creep increase; 3) Spatial threshold sensitivity testing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM planning agents be refined to mitigate the observed monotonic decline in responsiveness as travel plan horizons extend from 3 to 7 days?
- Basis in paper: Section 4.2 and Table 3 show responsiveness rate drops from 89.53% (3-day) to 79.82% (7-day)
- Why unresolved: Paper identifies this degradation but doesn't propose specific interventions to maintain performance over longer contexts
- What evidence would resolve it: Experiments comparing baseline models against those employing hierarchical planning, demonstrating stable responsiveness across 7-day plans

### Open Question 2
- Question: How can planning agents be improved to propagate local temporal adjustments to dependent downstream activities to prevent "ripple effects"?
- Basis in paper: Section 6.3 notes local adjustments weren't always propagated to dependent activities, leaving downstream segments compressed or misordered
- Why unresolved: Models often treat disruptions as isolated events rather than structural changes, failing to automatically re-optimize global sequence
- What evidence would resolve it: Modified evaluation showing reduction in temporal conflicts in latter half of revised itineraries

### Open Question 3
- Question: What evaluation frameworks can better align LLM-as-a-Judge perceived quality with strict constraint feasibility?
- Basis in paper: Section 5.2 observes discrepancy where Llama-3.1-8B favors 3-day plans (perceived quality) while automated metrics favor 5-day plans (feasibility/HCPR)
- Why unresolved: Disconnect between what LLM judge considers "good" revision and what satisfies hard logical constraints
- What evidence would resolve it: Calibrated rubric where LLM-as-a-Judge scores correlate strongly with human expert verification of hard constraints

## Limitations

- Binary traveler tolerance classification (Flexi-Venturer vs. Plan-Bound) oversimplifies real-world flexibility preferences
- Semantic continuity metric relies on BERT embeddings that miss grounded attributes like price ranges or dietary restrictions
- Spatial metrics don't account for transit frequency or inter-POI travel time, representing simplicity vs. realism trade-off

## Confidence

- **High confidence:** Sequential adaptability measurements and spatial deviation patterns for GPT-4o (supported by quantitative data in Table 3)
- **Medium confidence:** Semantic adaptability metric validity and overall benchmark generalizability (limited by BERT's semantic representation gaps)
- **Low confidence:** Cross-model robustness conclusions, particularly for smaller models like Phi-4-mini-Instruct (only tested in preliminary experiments)

## Next Checks

1. Conduct ablation studies removing traveler tolerance constraints to quantify scope creep rates for Plan-Bound profiles
2. Test metric sensitivity by varying spatial distance thresholds (d0) in Equation 4 to verify the "longer plans have lower spatial deviation" conclusion
3. Implement temporal feasibility validation to catch operating hour violations that current metrics may miss