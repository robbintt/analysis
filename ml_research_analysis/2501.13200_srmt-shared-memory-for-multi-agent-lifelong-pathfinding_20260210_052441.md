---
ver: rpa2
title: 'SRMT: Shared Memory for Multi-agent Lifelong Pathfinding'
arxiv_id: '2501.13200'
source_url: https://arxiv.org/abs/2501.13200
tags:
- srmt
- agents
- memory
- multi-agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multi-agent pathfinding, where agents must
  navigate to their goals without colliding. A key challenge is enabling coordination
  without explicit communication.
---

# SRMT: Shared Memory for Multi-agent Lifelong Pathfinding

## Quick Facts
- arXiv ID: 2501.13200
- Source URL: https://arxiv.org/abs/2501.13200
- Authors: Alsu Sagirova; Yuri Kuratov; Mikhail Burtsev
- Reference count: 17
- Primary result: SRMT outperforms RL baselines on multi-agent pathfinding with shared recurrent memory, enabling implicit coordination without explicit communication

## Executive Summary
This paper addresses the challenge of coordinating multiple agents in partially observable environments where direct communication is unavailable. The proposed Shared Recurrent Memory Transformer (SRMT) extends memory transformers to multi-agent settings by pooling individual agent memories into a globally accessible buffer, allowing implicit information exchange through cross-attention mechanisms. The approach is evaluated on a bottleneck navigation task and the POGEMA benchmark, demonstrating consistent performance improvements over reinforcement learning baselines, particularly under sparse reward conditions, and showing effective generalization to longer corridors than seen during training.

## Method Summary
SRMT uses a transformer-based architecture where each agent maintains memory tokens that are pooled into a shared global memory buffer. The system processes local observations through a ResNet encoder, then applies self-attention to combine current memory, historical observations, and current observation. Cross-attention to the shared memory pool enables implicit coordination by allowing agents to incorporate global context. The architecture is trained using PPO with sparse rewards (+1 for goal completion, 0 otherwise) across varying observation window sizes (5×5 for bottleneck, 11×11 for LMAPF) and history lengths of 8 steps.

## Key Results
- SRMT achieves significantly higher cooperative success rates than RMT and Attention baselines on the bottleneck task, especially under sparse rewards
- The model generalizes effectively to corridor lengths up to 400 cells beyond training horizons
- On POGEMA benchmarks, SRMT is competitive with recent MARL, hybrid, and planning-based algorithms
- Memory initialization from the first observation substantially improves training stability and performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pooling individual agent memories into a globally accessible broadcast enables implicit coordination in partially observable settings.
- **Mechanism:** Each agent maintains memory tokens pooled into a shared buffer accessible via cross-attention, integrating information from other agents without explicit message passing.
- **Core assumption:** Global context aggregation via simple pooling resolves conflicts that cannot be solved with local observations alone.
- **Evidence anchors:** [abstract] "...pooling and globally broadcasting individual working memories..."; [section 3] "...shared memory consists of a globally accessible, ordered sequence..."; [corpus] *Cooperative Hybrid Multi-Agent Pathfinding* supports shared information structures.
- **Break condition:** Significant agent scaling may cause computational expense or noisy attention, failing to isolate relevant coordination signals.

### Mechanism 2
- **Claim:** Initializing memory states from the first observation provides grounded starting context preventing early training instability.
- **Mechanism:** Initial memory is generated from the agent's first observation rather than static initialization, immediately grounding the recurrent state.
- **Core assumption:** Initial observation contains critical information to bootstrap recurrent processing more effectively than learned static initialization.
- **Evidence anchors:** [section 4.1] "...modified the initial state... generated from the initial observation... significantly increased performance..."; [corpus] *Remembering the Markov Property* discusses initialization importance.
- **Break condition:** If initial observation is occluded or deceptive, generated memory could bias agent incorrectly for episode duration.

### Mechanism 3
- **Claim:** Transformer-based memory retention handles long-horizon dependencies and generalization better than fixed-history RNNs.
- **Mechanism:** Memory tokens act as recurrent hidden state passed between segments, allowing gradient propagation through dedicated memory channel rather than entire history stack.
- **Core assumption:** Memory token bottleneck is sufficient to store necessary temporal features for pathfinding task.
- **Evidence anchors:** [section 2.2] "...multiple memory tokens act as a recurrent state..."; [section 4.1] "SRMT scales effectively up to corridor lengths of 400 cells..."; [corpus] *Multi-agent In-context Coordination* suggests transformer memory competitive for coordination.
- **Break condition:** Tasks requiring high-resolution recall of distant past features beyond training horizon may suffer from information loss in compressed memory tokens.

## Foundational Learning

- **Concept:** **Global Workspace Theory (GWT)**
  - **Why needed here:** The paper explicitly cites GWT as inspiration for shared memory module, explaining why broadcasting information leads to cooperation among independent modules.
  - **Quick check question:** Can you explain how a "blackboard" or global broadcast architecture differs from peer-to-peer communication in terms of information latency and aggregation?

- **Concept:** **Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** Problem defined as PO-MAPF; agents see only local window and must maintain internal belief state to infer obstacle/agent locations outside that window.
  - **Quick check question:** Why does partial observability necessitate recurrent architecture (memory) whereas full observability might not?

- **Concept:** **Cross-Attention Mechanisms**
  - **Why needed here:** Specific operation used to read from shared memory, differing from self-attention by querying one vector set using another.
  - **Quick check question:** In SRMT, what represents the Query and what represents the Key/Value in the cross-attention step?

## Architecture Onboarding

- **Component map:** Spatial Encoder (ResNet + MLP) -> SRMT Core (self-attention on memory+history+observation -> cross-attention to Shared Memory Pool -> memory head) -> Action Decoder (Actor-Critic)
- **Critical path:** The Shared Memory Pooling & Cross-Attention step; if this fails to pass relevant data between agents, system collapses into independent, uncoordinated agents.
- **Design tradeoffs:**
  - Pooling vs. Selective Communication: Global pooling is simple but may not scale efficiently to hundreds of agents compared to selective communication with attention masking.
  - Memory Tokens vs. RNN State: Distinct memory tokens allow higher capacity storage than single vector RNN state but add computational overhead.
- **Failure signatures:**
  - Deadlocks: Agents oscillating or stopping in bottlenecks (indicates shared memory not resolving "who goes first" conflict).
  - Memory Drift: Performance degrading over very long episodes suggests memory tokens overwriting critical early information.
- **First 3 experiments:**
  1. **Bottleneck Ablation:** Run "Bottleneck" task with corridor length 10 vs. 100, comparing SRMT against RMT (no sharing) and Empty (no memory) baselines to verify shared memory contribution.
  2. **Memory Initialization Test:** Compare training convergence with "First Observation" vs. "Random/Zeros" initialization to validate initialization stability claim.
  3. **POGEMA Generalization:** Train on "Mazes" and test on "Random" maps to ensure spatial encoder and memory aren't overfitting to training topologies.

## Open Questions the Paper Calls Out
None

## Limitations
- Shared memory pooling may become computationally intractable or informationally noisy with many agents (>64)
- Memory token compression may fail in tasks requiring fine-grained recall of long-range dependencies beyond training horizon
- Evaluation uses limited MARL baselines, omitting comparisons to more recent transformer-based MARL methods

## Confidence
- **High Confidence:** Claims about SRMT outperforming RMT and Attention baselines on bottleneck task under sparse rewards (directly measured in controlled experiments)
- **Medium Confidence:** Claims about competitive performance on POGEMA maps (depends on external Follower planner integration)
- **Low Confidence:** Claims about SRMT effectiveness in large-scale (>64 agents) scenarios or environments with complex, long-term dependencies beyond tested conditions

## Next Checks
1. **Scalable Coordination Test:** Evaluate SRMT performance on bottleneck task with 128+ agents to identify shared memory pooling degradation point
2. **Memory Token Ablation:** Run bottleneck task with varying memory token sizes (4, 8, 16 tokens) to quantify impact on coordination success and identify minimum effective size
3. **Planning Integration Isolation:** Re-run POGEMA experiments with SRMT alone (no Follower) and compare against SRMT-FlwrPlan results to isolate memory architecture contribution versus external planner