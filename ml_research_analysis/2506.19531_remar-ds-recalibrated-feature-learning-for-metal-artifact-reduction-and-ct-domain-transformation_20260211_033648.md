---
ver: rpa2
title: 'ReMAR-DS: Recalibrated Feature Learning for Metal Artifact Reduction and CT
  Domain Transformation'
arxiv_id: '2506.19531'
source_url: https://arxiv.org/abs/2506.19531
tags:
- metal
- artifact
- reduction
- remar-ds
- mvct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses metal artifacts in kilo-Voltage CT (kVCT) imaging
  caused by metallic implants, which degrade image quality and affect clinical decision-making
  in radiotherapy. The proposed method, ReMAR-DS, is a deep learning framework that
  simultaneously performs metal artifact reduction (MAR) and domain transformation
  from kVCT to artifact-resistant Mega-Voltage CT (MVCT).
---

# ReMAR-DS: Recalibrated Feature Learning for Metal Artifact Reduction and CT Domain Transformation

## Quick Facts
- arXiv ID: 2506.19531
- Source URL: https://arxiv.org/abs/2506.19531
- Reference count: 40
- Primary result: PSNR of 27.670 dB and SSIM of 0.703 on artifact-heavy slices; PSNR of 30.690 dB and SSIM of 0.759 on full dataset

## Executive Summary
ReMAR-DS addresses metal artifacts in kVCT imaging caused by metallic implants, which degrade image quality and affect clinical decision-making in radiotherapy. The proposed deep learning framework simultaneously performs metal artifact reduction (MAR) and domain transformation from kVCT to artifact-resistant MVCT. By incorporating concurrent spatial and channel squeeze-and-excitation blocks in skip connections and using depthwise separable convolutions in residual blocks, the model dynamically focuses on artifact-affected regions while preserving anatomical structures. Evaluated on a clinical dataset, ReMAR-DS achieves state-of-the-art performance, enabling high-quality MVCT-like reconstructions from kVCT inputs and reducing the need for repeated high-dose MVCT scans.

## Method Summary
ReMAR-DS is an encoder-decoder architecture enhanced with residual blocks employing depthwise separable convolutions for efficient feature transformation, and concurrent spatial and channel squeeze-and-excitation (RcsSE) blocks in skip connections for dynamic feature recalibration. The model processes 512×512 kVCT slices with Gaussian noise injection at input and bottleneck levels. Training uses weighted L1 + SSIM + Focal Frequency Loss objectives with AdamW optimization, patient-wise 70/30 split, and data augmentation including horizontal flips and affine transformations.

## Key Results
- Achieves PSNR of 27.670 dB and SSIM of 0.703 on artifact-heavy slices (DArt subset)
- Achieves PSNR of 30.690 dB and SSIM of 0.759 on full dataset (DAll subset)
- Outperforms state-of-the-art methods in both MAR and domain transformation tasks
- Demonstrates robustness through noise regularization and efficient parameter usage via depthwise separable convolutions

## Why This Works (Mechanism)

### Mechanism 1: Spatial-Channel Recalibration of Skip Connections
The architecture inserts concurrent spatial and channel Squeeze-and-Excitation (RcsSE) blocks into skip connections. The spatial branch generates an attention map to emphasize informative regions (e.g., metal streaks), while the channel branch learns importance weights for anatomical structures. This suppresses irrelevant background noise before features are fused in the decoder. The core assumption is that artifact regions and anatomical structures can be distinguished by distinct spatial and channel-wise activation patterns that standard convolutions fail to isolate.

### Mechanism 2: Efficient Feature Transformation via Depthwise Separable Convolutions
Enhanced Residual Blocks (EnResB) replace standard convolutions with depthwise separable convolutions, decoupling spatial filtering (depthwise convolution) from channel mixing (pointwise convolution). This reduces parameter count, allowing deeper hierarchies without overfitting while residual connections preserve gradient flow for recovering fine textures. The core assumption is that spatial geometry of artifacts and spectral characteristics of tissue can be learned independently.

### Mechanism 3: Noise-Regularized Latent Space for Robustness
Gaussian noise is injected into both input and latent space during training, forcing the network to learn a robust mapping that generalizes to unseen artifact variations. This acts as a regularizer, preventing memorization of specific artifact patterns and encouraging learning of underlying clean anatomy distribution. The core assumption is that artifact signal is localized and can be treated as perturbation of clean signal.

## Foundational Learning

- **U-Net Architectures & Skip Connections**: Essential for understanding why skip connections are used (to recover spatial information lost during downsampling) and why recalibration is applied specifically at these junctions. Quick check: If you remove skip connections from a U-Net, what specific type of detail is most likely lost in the output?

- **Attention Mechanisms (Squeeze-and-Excitation)**: Critical for understanding the paper's primary contribution - RcsSE blocks. You must understand how "Squeeze" (global pooling) and "Excitation" (adaptive weighting) allow selective amplification of useful features. Quick check: In a channel-wise SE block, does the network learn where a feature is, or how important that feature type is globally?

- **CT Physics & Hounsfield Units (HU)**: Necessary for understanding domain transformation (kVCT to MVCT) and why metal causes beam hardening and photon starvation. Quick check: Why does metal density cause streaking artifacts specifically between two metal objects, rather than just at the object itself?

## Architecture Onboarding

- **Component map**: Input (kVCT + Gaussian Noise) → Encoder (Conv-BN-ReLU → EnResB) → Skip Path (RcsSE blocks) → Bottleneck (Deep features + Learnable Noise) → Decoder (Upsampling + Skip fusion) → Output (MVCT)

- **Critical path**: The RcsSE block in the skip connection is the critical failure point. If the attention map learns to suppress the artifact region but accidentally suppresses adjacent anatomical structure (e.g., a nerve near dental implant), clinical utility is lost.

- **Design tradeoffs**: Depthwise vs. Standard Conv trades theoretical "perfect" representational capacity for efficiency and reduced overfitting risk. Loss function complexity shows L1 is best for overall similarity, but complex losses (LSSIM, LFFL) are required specifically for heavy artifacts.

- **Failure signatures**: Secondary artifacts (blurring or new artifacts in soft tissue near metal interfaces), HU drift (output MVCT has correct structure but incorrect electron density values).

- **First 3 experiments**:
  1. Visualize attention maps: Extract spatial attention map from RcsSE block on artifact-heavy slice to verify it highlights streaking artifacts and not critical structures like brain stem or spinal cord.
  2. Loss ablation reproduction: Train two versions - one with L1 only, one with L1 + LSSIM + LFFL. Compare PSNR on DArt set to validate frequency-aware losses contribution.
  3. Noise sensitivity test: Vary noise scale σk (input) and σL (latent) to determine threshold where reconstruction quality degrades and find robustness margin.

## Open Questions the Paper Calls Out

- **3D volumetric extension**: The paper explicitly states future work will focus on extending the method to 3D volumes for broader clinical applicability, addressing current limitations with 2D slice processing.

- **HU accuracy refinement**: The conclusion identifies refining HU accuracy as a focus area for future work, acknowledging that while PSNR/SSIM improvements are demonstrated, HU accuracy directly impacts electron density calculations used in treatment planning.

## Limitations

- Data specificity: Performance metrics are reported on a single institution's paired kVCT-MVCT dataset (52 patients, head/neck region), limiting generalizability to other anatomical sites or scanner models.

- Ablation incompleteness: While RcsSE and depthwise separable convolutions show individual contributions, interaction effects between these components and noise regularization scheme were not systematically studied.

- Clinical validation gap: The paper reports perceptual and quantitative metrics but lacks radiologist reader studies to confirm diagnostic equivalence between reconstructed and native MVCT images.

## Confidence

- Mechanism 1 (RcsSE attention): High confidence - ablation study directly demonstrates component's contribution with complete architectural description
- Mechanism 2 (Depthwise separable convolutions): Medium confidence - parameter efficiency claim is supported, but reconstruction fidelity preservation relies on residual connections
- Mechanism 3 (Noise regularization): Low confidence - ablation study shows benefit but no sensitivity analysis performed to determine optimal noise scales

## Next Checks

1. **Attention map verification**: Extract and visualize spatial attention maps from RcsSE blocks on artifact-heavy slices to confirm they highlight streaking artifacts rather than anatomical structures.

2. **Loss component sensitivity**: Systematically vary weights of L1, SSIM, and FFL components to identify minimum complex loss configuration that maintains artifact reduction performance.

3. **Cross-scanner validation**: Test trained model on kVCT images from different scanner manufacturer to assess generalization beyond training domain.