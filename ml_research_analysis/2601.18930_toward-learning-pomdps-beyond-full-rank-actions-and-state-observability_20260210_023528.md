---
ver: rpa2
title: Toward Learning POMDPs Beyond Full-Rank Actions and State Observability
arxiv_id: '2601.18930'
source_url: https://arxiv.org/abs/2601.18930
tags:
- state
- observation
- states
- learning
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for learning the parameters of discrete
  Partially Observable Markov Decision Processes (POMDPs) from action-observation
  sequences, relaxing assumptions about full-rank transition matrices and unique observation
  distributions. The key innovation is leveraging Predictive State Representations
  (PSRs) to learn POMDP parameters up to a similarity transform, which can then be
  estimated via tensor decomposition methods.
---

# Toward Learning POMDPs Beyond Full-Rank Actions and State Observability

## Quick Facts
- arXiv ID: 2601.18930
- Source URL: https://arxiv.org/abs/2601.18930
- Reference count: 40
- Key outcome: Learning POMDP parameters from action-observation sequences using PSRs, relaxing full-rank assumptions and enabling planning with different reward functions

## Executive Summary
This paper presents a method for learning discrete Partially Observable Markov Decision Processes (POMDPs) from observed action-observation sequences without requiring full-rank transition matrices or unique observation distributions. The approach leverages Predictive State Representations (PSRs) to learn models up to a similarity transform, which can be recovered through tensor decomposition methods. The learned models enable effective planning with different reward functions and successfully handle domains with state aliasing where traditional methods fail.

The key insight is that POMDPs can only be distinguished up to observability partitions from single trajectories, establishing fundamental limits on what can be learned. The method constructs a Hankel matrix from data, performs rank factorization to obtain a PSR model, and recovers the similarity transform through joint diagonalization. This allows learning explicit transition and observation matrices that support flexible reward specification after model learning, contrasting with black-box RL approaches.

## Method Summary
The method works by first constructing a Hankel matrix from observed action-observation sequences, then performing rank factorization to obtain a Predictive State Representation (PSR) model. The similarity transform is recovered by joint diagonalization of observation-transition products for full-rank actions, allowing recovery of observation and transition matrices up to observability partitionsâ€”groups of states with identical observation distributions. The approach leverages tensor decomposition methods to learn POMDP parameters from data without requiring full-rank assumptions, and demonstrates convergence to true parameters while enabling planning with various reward functions.

## Key Results
- Learned models converge to true POMDP parameters and enable effective planning with different reward functions
- Successfully learns models for standard POMDPs (Tiger, T-Maze) and novel domains with state aliasing
- Demonstrates fundamental limitation: POMDPs can only be distinguished up to observability partitions from single trajectories
- Explicit transition and observation matrices allow reward functions to be specified after model learning

## Why This Works (Mechanism)
The method exploits the structure of Predictive State Representations to bypass the need for full-rank assumptions in POMDP learning. By constructing a Hankel matrix that captures temporal correlations in the observed data, the approach can identify the underlying state space structure through rank factorization. The joint diagonalization step recovers the similarity transform that relates the learned PSR to the true POMDP parameters, while naturally accommodating observability partitions where multiple states share identical observation distributions.

## Foundational Learning
- **Hankel Matrix Construction**: Captures temporal correlations in action-observation sequences; needed to identify the underlying state space structure; quick check: verify Hankel matrix rank matches theoretical state space dimension
- **Rank Factorization**: Decomposes Hankel matrix to obtain PSR model; needed to extract predictive states from observed data; quick check: ensure factor matrices satisfy reconstruction constraints
- **Tensor Decomposition**: Recovers similarity transform from joint diagonalization; needed to map PSR to POMDP parameters; quick check: verify diagonalization yields consistent transformation across observation-transition products
- **Observability Partitions**: Groups of states with identical observation distributions; needed to understand fundamental learning limits; quick check: identify partitions by comparing observation likelihoods across states
- **Predictive State Representations**: Alternative to belief states for modeling partially observable systems; needed to bypass full-rank requirements; quick check: verify PSR predictions match observed action-observation sequences
- **Similarity Transform Recovery**: Maps learned PSR to POMDP parameters; needed to obtain interpretable transition and observation matrices; quick check: validate recovered matrices produce correct predictions

## Architecture Onboarding

**Component Map**: Data -> Hankel Matrix -> Rank Factorization -> PSR Model -> Joint Diagonalization -> Similarity Transform -> POMDP Parameters

**Critical Path**: The sequence from data collection through Hankel matrix construction to rank factorization represents the core learning pipeline. Joint diagonalization is critical for recovering interpretable POMDP parameters from the PSR model.

**Design Tradeoffs**: The method trades computational complexity of tensor decomposition for relaxation of full-rank assumptions. While more computationally intensive than some alternatives, it enables learning from domains with limited or structured observations where traditional methods fail.

**Failure Signatures**: Learning failure occurs when Hankel matrix rank is overestimated, leading to spurious states in the PSR model. Numerical instability in tensor decomposition can cause poor similarity transform recovery. Insufficient data may result in observability partitions that are too coarse to support accurate planning.

**Three First Experiments**:
1. Validate Hankel matrix rank matches theoretical state space dimension on simple domains
2. Test joint diagonalization recovery on known similarity transforms with controlled noise
3. Compare planning performance using learned models versus ground truth across different reward functions

## Open Questions the Paper Calls Out
The paper identifies several open questions: How to scale these methods to larger POMDPs with many states and actions? Whether additional data collection strategies can partially overcome the observability partition limitation? How the learned models compare to black-box RL approaches that don't explicitly model POMDP structure? The paper notes that computational complexity of tensor decomposition and Hankel matrix construction may limit applicability to real-world problems.

## Limitations
- Computational complexity of tensor decomposition and Hankel matrix construction may limit scalability to larger POMDPs
- Reliance on finding appropriate rank factorizations may be numerically unstable in practice
- Single trajectory data restricts learning to observability partitions, limiting model precision
- Limited empirical validation on small benchmark domains doesn't establish practical applicability to real-world problems

## Confidence

| Claim | Confidence |
|-------|------------|
| Core theoretical framework and proofs | High |
| Empirical validation on benchmark domains | Medium |
| Practical applicability to real-world POMDPs | Low |

## Next Checks
1. Test the approach on larger POMDPs with 50+ states to evaluate computational scalability and numerical stability of tensor decomposition methods
2. Investigate whether additional data collection strategies (multiple trajectories, active learning) can partially overcome the observability partition limitation
3. Compare the learned models' planning performance against black-box RL approaches that do not explicitly model the POMDP structure