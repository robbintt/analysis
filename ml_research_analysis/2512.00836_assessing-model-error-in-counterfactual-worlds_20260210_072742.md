---
ver: rpa2
title: Assessing model error in counterfactual worlds
arxiv_id: '2512.00836'
source_url: https://arxiv.org/abs/2512.00836
tags:
- scenario
- error
- each
- projections
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the fundamental challenge of evaluating counterfactual
  scenario projections, where models predict outcomes under specific hypothetical
  conditions that are never observed in reality. The core problem is that error in
  these projections has two sources: model miscalibration and scenario deviation,
  making traditional evaluation methods inapplicable.'
---

# Assessing model error in counterfactual worlds

## Quick Facts
- arXiv ID: 2512.00836
- Source URL: https://arxiv.org/abs/2512.00836
- Reference count: 40
- Key outcome: Three approaches to estimate model miscalibration error in counterfactual scenarios; Approaches 2 and 3 outperform filtering to plausible scenarios

## Executive Summary
This paper addresses the fundamental challenge of evaluating counterfactual scenario projections where models predict outcomes under hypothetical conditions never observed in reality. The core problem is that error has two sources—model miscalibration and scenario deviation—making traditional evaluation methods inapplicable. The authors propose three approaches: filtering to plausible scenarios, inferring error distributions via natural variation across groups, and estimating counterfactual observations using statistical models with covariates. Through simulation experiments with SIR epidemic models, they demonstrate that Approaches 2 and 3 accurately estimate true error distributions, with Approach 3 showing particularly strong performance when covariates are available.

## Method Summary
The authors propose three approaches to estimate model miscalibration error in counterfactual scenarios. Approach 1 filters to plausible scenarios close to reality. Approach 2 infers error distribution by modeling errors across locations with varying realized scenarios using generalized additive models (GAMs). Approach 3 estimates observations in modeled scenarios by fitting statistical models with covariates to predict counterfactual outcomes. They evaluate these methods using SIR epidemic models across 50 locations with vaccination scenarios at 30% and 50% coverage, comparing estimated error distributions to ground truth using mean absolute error and Kolmogorov-Smirnov tests.

## Key Results
- Approaches 2 and 3 outperform Approach 1 in accurately estimating true error distributions
- Approach 3, which estimates observations in counterfactual scenarios using statistical models with covariates, shows particularly strong performance
- Observed differences between projections and reality can be decomposed into model calibration error and scenario specification error, which may partially cancel each other out
- Effective scenario evaluation depends heavily on scenario design, requiring measurable scenario axes with sufficient variation across groups

## Why This Works (Mechanism)

### Mechanism 1: Error Decomposition into Orthogonal Components
- Claim: Observed deviation between projection and reality can be decomposed into two linearly additive components that may partially cancel.
- Mechanism: P_m(y|x_i) − P*(y|x*) = [P_m(y|x_i) − P*(y|x_i)] + [P*(y|x_i) − P*(y|x*)]. The first term is model calibration error; the second is scenario specification error.
- Core assumption: The two error sources are separable and identifiable through counterfactual estimation.
- Evidence anchors: Section 4 explicitly derives this decomposition and notes errors can cancel; Figure 5 visualizes the decomposition.

### Mechanism 2: Leveraging Natural Variation Across Groups to Infer Error Distributions
- Claim: Error at counterfactual scenarios can be estimated by fitting a statistical model to realized errors across locations with varying scenario realizations.
- Mechanism: Calculate e_m(x*) for each location, fit e_m(x) = g(x) + ε using GAMs, then predict errors at modeled scenario values x_i.
- Core assumption: Realized scenario values are sampled independently along the scenario axis, and model error structure is smooth/continuous.
- Evidence anchors: Section 3.2 describes the three-step process; Figure 3 shows the fitting process; Figure 6 shows Approach 2 matches true error distribution in 14/20 comparisons.

### Mechanism 3: Covariate-Informed Observation Estimation Outperforms Simple Extrapolation
- Claim: Estimating counterfactual observations via statistical models with location-specific covariates substantially outperforms approaches that ignore covariate structure.
- Mechanism: Fit P*(y|x) = f(x, covariates) + ε using observed data, then predict observations at counterfactual x values.
- Core assumption: The relationship between scenario axis and outcome is well-specified, and key covariates are measured without error.
- Evidence anchors: Section 3.3 describes the approach; Section 5.3 notes "for Approach 3, covariates are essential"; Figure 6 shows covariate-included Approach 3 significantly outperforms the no-covariate version.

## Foundational Learning

- Concept: Counterfactual inference / potential outcomes framework
  - Why needed here: The entire paper depends on reasoning about P*(y|x_i)—what would have been observed had scenario x_i been realized—which is unobservable directly.
  - Quick check question: Can you explain why P*(y|x_i) is fundamentally unobservable, and what assumptions are required to estimate it?

- Concept: Model calibration vs. predictive accuracy
  - Why needed here: The paper distinguishes calibration error (systematic bias in model outputs) from scenario specification error (choosing wrong inputs), which require different remediation strategies.
  - Quick check question: If a model produces projections that match observations perfectly but for the wrong scenario assumptions, is it well-calibrated?

- Concept: Generalized Additive Models (GAMs) with spline terms
  - Why needed here: The paper implements Approaches 2 and 3 using GAMs with cubic splines to capture non-linear relationships between scenario axis and outcomes/errors.
  - Quick check question: Why might a GAM be preferred over a linear model for fitting the relationship between vaccination coverage and epidemic size?

## Architecture Onboarding

- Component map: Scenario Projections (P_m) -> Error Decomposition -> Approach Selection -> Approach 1/2/3; Observations (P*) -> Error Estimation

- Critical path:
  1. Identify scenario axis (must be measurable, ideally continuous)
  2. Check for sufficient variation in realized values across groups
  3. Select approach: Approach 3 if good covariate knowledge; Approach 2 if mechanistic model captures non-linearities; avoid Approach 1
  4. Validate estimation by comparing implied vs. actual distributions where ground truth is available

- Design tradeoffs:
  - Approach 1: No extrapolation needed, but conflates scenario error with model error—only use when no other option
  - Approach 2: Leverages mechanistic model's non-linear structure, but requires honest reprojection and produces inconsistent implied observations across models
  - Approach 3: Produces consistent counterfactual observations for all models, but requires strong assumptions about covariate-outcome relationships

- Failure signatures:
  - Approach 1: Error estimates correlate with distance from realized scenario
  - Approach 2 without covariates: High variance in location-specific estimates
  - Approach 3 without covariates: Inflated error distribution variance; inability to generate location-specific estimates
  - All approaches: Fail when scenario axis lacks quantitative ordering or when between-group variation is insufficient

- First 3 experiments:
  1. Replicate the SIR simulation experiment from Section 5 using the provided code to validate understanding of error decomposition mechanics.
  2. Apply Approach 3 to a real dataset with known counterfactual outcomes (e.g., A/B test data) to compare estimated vs. actual observations.
  3. Conduct sensitivity analysis varying the threshold τ for "plausible" scenarios in Approach 1 to quantify how threshold choice biases error estimates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed evaluation frameworks be extended to assess probabilistic scenario projections rather than just point predictions?
- Basis in paper: [explicit] The authors state in the Discussion that they "only assess the error distribution of point predictions, whereas most scenario projections are probabilistic in nature" and identify this as a "key area for future work."
- Why unresolved: The current simulation experiment only validates the recovery of point estimates. Adapting these methods for probabilistic forecasts requires developing techniques to evaluate calibration scores or prediction interval coverage in counterfactual worlds.
- What evidence would resolve it: A simulation study demonstrating that Approaches 2 and 3 can accurately recover the true calibration properties (e.g., 95% interval coverage) of probabilistic models in counterfactual scenarios.

### Open Question 2
- Question: How robust are Approaches 2 and 3 when applied to real-world systems with complex observation and error distributions, as opposed to the simplified SIR model used in the simulation?
- Basis in paper: [explicit] The authors note that "Real systems are far more complex, and may therefore have complex observation and error distributions that present challenges for our methods in practice."
- Why unresolved: The validity of the methods was demonstrated using a relatively simple mechanistic SIR model. It is unclear if the generalized additive models (GAMs) used in Approaches 2 and 3 can sufficiently capture the complex, non-linear error structures of real-world data without overfitting or bias.
- What evidence would resolve it: Retrospective application of these methods to empirical datasets (e.g., historical climate or economic projections) where the "true" counterfactual outcomes are simulated or held out, showing that error distributions are still recovered accurately.

### Open Question 3
- Question: Can these evaluation methods be effectively scaled to multi-axis scenarios where interactions between multiple varying conditions must be modeled simultaneously?
- Basis in paper: [explicit] The authors highlight that "careful thought is needed in the definition of multi-axis scenarios to ensure that it is possible to model the simultaneous effect of both scenario axes on outcomes (or error)."
- Why unresolved: The simulation experiment focused on a single scenario axis (vaccination coverage). Evaluating scenarios with multiple axes (e.g., vaccination plus variant transmissibility) introduces the "curse of dimensionality," making it difficult to find sufficient natural variation across locations to fit reliable statistical models for Approach 2 or 3.
- What evidence would resolve it: A simulation experiment involving two or more orthogonal scenario axes that establishes the data density requirements and modeling constraints needed to successfully estimate counterfactual errors.

## Limitations

- The theoretical framework assumes separable error decomposition, but real-world scenarios may violate this when model and scenario errors are correlated.
- The simulation study relies on SIR models with controlled error structures, limiting generalizability to domains with different non-linear dynamics or threshold effects.
- Approach 3's strong performance depends critically on having well-measured covariates that capture the relationship between scenario axis and outcomes.

## Confidence

**High confidence** in the error decomposition framework and the demonstration that Approaches 2 and 3 outperform Approach 1. The simulation experiment is well-controlled, and the mathematical derivation of error sources is sound.

**Medium confidence** in the generalizability of Approach 3's superior performance, as the SIR simulation benefits from having R0 as a measurable covariate. Real-world applications may lack such clean covariate-outcome relationships.

**Low confidence** in the paper's claim that Approach 2 "successfully recovers the true error distribution" across diverse scenarios, as the evaluation shows only 14/20 model-scenario comparisons meeting significance thresholds.

## Next Checks

1. **Real-world counterfactual validation**: Apply Approach 3 to A/B test data where the control group provides ground truth counterfactual outcomes, comparing estimated vs. actual observations across multiple scenario axes.

2. **Covariate sensitivity analysis**: Systematically vary the quality and completeness of covariates in Approach 3 using the SIR model by (a) adding irrelevant covariates, (b) removing R0, and (c) adding measurement error to existing covariates to quantify performance degradation.

3. **Cross-domain applicability**: Test all three approaches on a non-epidemic domain with threshold effects (e.g., churn prediction with loyalty program interventions) where model error and scenario error may be correlated rather than additive.