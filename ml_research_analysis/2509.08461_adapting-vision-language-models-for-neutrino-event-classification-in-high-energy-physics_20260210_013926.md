---
ver: rpa2
title: Adapting Vision-Language Models for Neutrino Event Classification in High-Energy
  Physics
arxiv_id: '2509.08461'
source_url: https://arxiv.org/abs/2509.08461
tags:
- vision
- neutrino
- llama
- classification
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of vision-language models
  (VLMs) for classifying neutrino interactions in high-energy physics experiments,
  specifically comparing a fine-tuned LLaMA 3.2 Vision model against conventional
  CNN architectures. Using simulated pixelated detector data from a liquid argon time
  projection chamber, the research demonstrates that VLMs can outperform CNNs in classification
  accuracy while providing interpretable, physics-grounded explanations for predictions.
---

# Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics

## Quick Facts
- arXiv ID: 2509.08461
- Source URL: https://arxiv.org/abs/2509.08461
- Reference count: 40
- Primary result: Fine-tuned LLaMA 3.2 Vision achieves 0.87 accuracy vs CNN's 0.68 for neutrino event classification

## Executive Summary
This study demonstrates that vision-language models (VLMs) can outperform traditional CNN architectures in classifying neutrino interactions from liquid argon time projection chamber (LArTPC) detector data. The research fine-tunes a LLaMA 3.2 Vision model using QLoRA on simulated pixelated detector images, achieving significantly higher accuracy (0.87 vs 0.68) and better generalization under degraded conditions. Beyond superior classification performance, VLMs provide interpretable natural language explanations that reference physics-grounded features like track length and shower characteristics, making them particularly valuable for offline physics analyses where understanding model reasoning is crucial.

## Method Summary
The method involves fine-tuning a pre-trained LLaMA 3.2-11B-Vision-Instruct model on simulated neutrino interaction data using parameter-efficient QLoRA adapters. The model processes 512×512 grayscale pixel maps from LArTPC detectors (XZ and YZ views) alongside system and user text prompts through a shared transformer architecture. The training uses 4-bit quantization with NF4, low-rank adapter matrices (rank 8), and constrained beam search decoding to generate both class predictions and explanatory text. The approach is compared against a Siamese MobileNetV2-style CNN baseline trained on the same data.

## Key Results
- VLM achieves 0.87 accuracy versus CNN's 0.68 on 10K held-out test set
- VLMs maintain performance under resolution degradation (0.85 vs CNN's 0.49 at half resolution)
- VLMs generate physics-grounded natural language explanations referencing event topology features

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Knowledge Transfer
Fine-tuning a pre-trained VLM on neutrino detector data enables it to leverage generalized spatial and semantic representations that transfer to physics classification tasks. The model uses a vision transformer encoder to tokenize 512×512 grayscale pixel maps into patch embeddings, which are then processed jointly with text tokens by a shared transformer decoder. Pre-training on diverse image-text pairs provides a foundation for hierarchical visual reasoning that, when fine-tuned with QLoRA, adapts to recognize event topologies without requiring full model retraining.

### Mechanism 2: Physics-Grounded Language Explanations
VLMs can generate natural-language explanations that reference physics-relevant visual features alongside classification predictions. During inference, autoregressive decoding conditions on both the visual embeddings and a system prompt that describes class-distinguishing features. Constrained beam search forces output to begin with a fixed prefix followed by a valid class label, while the decoder can generate free-form explanations afterward.

### Mechanism 3: Robustness to Resolution Degradation
VLMs maintain higher classification performance than CNNs when input resolution is reduced, potentially due to multi-scale representations learned during pre-training. The vision transformer's patch-based tokenization and global self-attention can aggregate information across spatial scales, while the CNN's fixed receptive fields and hierarchical convolutions lose fine-grained detail when input resolution is halved.

## Foundational Learning

- **Liquid Argon Time Projection Chamber (LArTPC) Imaging**: Why needed here: The input data consists of 2D projections from a simulated LArTPC. Understanding that particle interactions create ionization tracks and showers, projected orthogonally, is essential to interpret model inputs and outputs. Quick check question: If you see a long, narrow track in one view and a short one in the orthogonal view, what might explain the discrepancy?

- **Parameter-Efficient Fine-Tuning (QLoRA)**: Why needed here: Full fine-tuning of an 11B-parameter VLM is computationally prohibitive. QLoRA quantizes the base model to 4-bit precision and trains only low-rank adapter matrices, enabling adaptation on modest GPU resources. Quick check question: What are the tradeoffs of using 4-bit quantization versus full-precision fine-tuning in terms of model capacity and overfitting risk?

- **Constrained Beam Search for Structured Output**: Why needed here: VLMs generate text autoregressively; unconstrained decoding can produce verbose or inconsistent class labels. Constrained beam search ensures outputs begin with a fixed prefix and select from valid class tokens. Quick check question: Why might a constrained prefix improve reproducibility and confidence estimation compared to free-form generation?

## Architecture Onboarding

- **Component map**: Input pixel maps + prompts → Vision encoder (ViT patch tokenizer) → Patch embeddings → Shared transformer decoder → Constrained beam search → Class prediction + explanation

- **Critical path**: 1) Pixel map preprocessing (cropping, normalization) 2) Vision encoding (patch tokenization) 3) Joint transformer decoding with text prompt 4) Constrained beam search for class prediction 5) Confidence extraction via log-probabilities

- **Design tradeoffs**: VLM accuracy (0.87) vs. CNN accuracy (0.68) at 25× memory cost and 165× inference time; Interpretability gains (textual explanations) vs. deployment complexity (requires GPU inference); Generalization to degraded inputs vs. dependency on large pre-trained backbone

- **Failure signatures**: Model outputs verbose or malformed class labels if constraint is misconfigured; Explanations reference features not present in image (hallucination); Performance collapses on real detector data if simulation-to-real gap is large

- **First 3 experiments**: 1) Replicate fine-tuning with varying QLoRA ranks (r=4, 8, 16) to assess accuracy vs. parameter efficiency tradeoffs 2) Evaluate explanation faithfulness by masking image regions corresponding to cited features and measuring prediction stability 3) Test robustness to noise and missing pixels beyond resolution reduction to probe generalization limits

## Open Questions the Paper Calls Out

### Open Question 1
Can VLMs be compressed or distilled into lightweight architectures that retain both high classification accuracy and interpretability for real-time neutrino event classification? Current VLM requires 25.4 GB memory and 3.3 seconds per inference versus CNN's 1 GB and 20 ms, making it impractical for on-detector edge computing. Demonstration of a distilled model achieving comparable accuracy (≥0.85) with inference time <100 ms and memory <5 GB would resolve this.

### Open Question 2
Can a single VLM fine-tuned on one neutrino detector configuration generalize to other LArTPC experiments without extensive retraining? The study uses a single custom DUNE-like simulation; cross-experiment transfer learning capability remains untested. Evaluation on datasets from different detector geometries or real experimental data would resolve this.

### Open Question 3
Do VLMs maintain their performance advantage over CNNs when tested on real experimental neutrino data rather than simulation? All experiments use simulated data; the paper never evaluates on real detector data where noise, detector effects, and simulation mismatches occur. Comparison on beam data from operational LArTPC detectors would resolve this.

### Open Question 4
How does the VLM performance scale with model size, and what is the minimum parameter count required for competitive neutrino classification? Only LLaMA 3.2 11B is tested; smaller or larger VLM variants are not explored. Systematic benchmarking of VLMs ranging from 1B to 70B parameters would resolve this.

## Limitations
- Dataset dependency on simulated rather than real detector data creates an unknown simulation-to-reality gap
- High computational overhead (25× memory, 165× inference time) limits deployment feasibility
- Explanations may be confabulatory rather than causally grounded, as no ablation studies validate cited features drive predictions

## Confidence
- **High**: VLMs achieve superior accuracy (0.87 vs 0.68) and generalization to degraded inputs on simulated test sets
- **Medium**: Explanations reference physics-grounded features, though causal validation remains unestablished
- **Low**: Claims about robustness to arbitrary degradation types are unsupported

## Next Checks
1. Test VLM performance on real LArTPC detector data to quantify simulation-to-reality transfer
2. Conduct ablation studies where visual features cited in explanations are masked to verify causal contribution
3. Evaluate robustness to detector-specific artifacts (noise, missing pixels, calibration errors) beyond resolution reduction