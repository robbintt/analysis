---
ver: rpa2
title: Mechanistic understanding and validation of large AI models with SemanticLens
arxiv_id: '2501.05398'
source_url: https://arxiv.org/abs/2501.05398
tags:
- concept
- texture
- concepts
- semantic
- supplementary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SEMANTIC LENS bridges the \u201Ctrust gap\u201D between AI models\
  \ and traditional engineered systems by mapping individual neural network components\
  \ into the semantically structured space of a multimodal foundation model such as\
  \ CLIP. This universal embedding transforms opaque hidden knowledge into searchable,\
  \ labelable, and comparable semantic vectors, enabling systematic understanding\
  \ of what concepts are encoded, how they are used for inference, and which training\
  \ data they relate to."
---

# Mechanistic understanding and validation of large AI models with SemanticLens

## Quick Facts
- arXiv ID: 2501.05398
- Source URL: https://arxiv.org/abs/2501.05398
- Reference count: 40
- Primary result: A framework that bridges the "trust gap" by mapping neural network components into a multimodal foundation model's semantic space for mechanistic interpretability and validation

## Executive Summary
SemanticLens is a framework that enables systematic understanding of what concepts neural network components encode, how they are used for inference, and which training data they relate to. By projecting individual neurons' maximally activating input samples into the embedding space of a multimodal foundation model like CLIP, SemanticLens transforms opaque hidden knowledge into searchable, labelable, and comparable semantic vectors. The method supports automated concept discovery, structured description, cross-model comparison, and alignment audits against human-defined valid or spurious concepts.

Applied to ImageNet and medical imaging, SemanticLens revealed spurious correlations (e.g., watermark or red skin features), enabled targeted debugging through pruning or retraining, and identified concept-level biases. Human-interpretability measures—clarity, polysemanticity, and redundancy—quantified model interpretability and guided architecture/training optimization. The approach provides scalable, foundation-model-driven mechanistic interpretability for validating and improving large AI models.

## Method Summary
SemanticLens maps neural network components (neurons) into the semantically structured space of a multimodal foundation model such as CLIP. For each neuron, the method extracts concept examples—input patches that highly activate it—from the dataset. These examples are processed by a foundation model to generate semantic embedding vectors that represent the neuron's "meaning" relative to the foundation model's structured knowledge base. The method applies Concept Relevance Propagation (CRP) to crop inputs to the most relevant features, improving semantic fidelity. Users can then search for specific concepts or audit alignment with valid vs. spurious concepts through text queries, enabling systematic understanding of model behavior and detection of harmful correlations.

## Key Results
- Successfully identified spurious correlations in ImageNet models, such as watermarks and background features being used for classification
- Demonstrated that wider neural networks exhibit lower polysemanticity and higher interpretability scores
- Enabled targeted debugging through pruning or retraining of neurons aligned with spurious concepts
- Showed that domain-specific foundation models (WhyLesionCLIP) significantly improve interpretability in medical imaging tasks

## Why This Works (Mechanism)

### Mechanism 1: Semantic Translation via Foundation Models
- **Claim:** Individual neurons can be semantically labeled by projecting their maximally activating input samples into the embedding space of a pre-trained multimodal foundation model.
- **Mechanism:** The method extracts a set of "concept examples"—input patches that highly activate a specific neuron—from the dataset. These examples are then processed by a foundation model (e.g., CLIP) to generate a semantic embedding vector. This vector represents the neuron's "meaning" relative to the foundation model's structured knowledge base, allowing text-based queries to identify neuron functions.
- **Core assumption:** The foundation model serves as a valid "semantic expert" for the data domain and that the concept examples capture the full functional role of the neuron.
- **Evidence anchors:** [abstract] "maps hidden knowledge encoded by components... into the semantically structured, multimodal space of a foundation model such as CLIP."

### Mechanism 2: Attribution-Guided Localization
- **Claim:** Cropping inputs to the most relevant features improves the semantic fidelity of neuron labels compared to using full images.
- **Mechanism:** Before embedding, the method applies Concept Relevance Propagation (CRP) to generate attribution maps. It crops the concept examples to include only pixels with high relevance scores (excluding features with <1% of the highest attribution). This filters out background noise, ensuring the semantic embedding focuses on the specific features driving the neuron's activation.
- **Core assumption:** The attribution method (CRP) accurately localizes the causal features responsible for the neuron's activation.
- **Evidence anchors:** [section 3.1] "Since the concept represented by the neuron can only occur in a small part... we facilitate the CRP framework... to identify the relevant part of the input and crop."

### Mechanism 3: Quantitative Audit via Concept Alignment
- **Claim:** A model's reasoning validity can be quantified by measuring the semantic similarity between its active neurons and user-defined sets of "valid" vs. "spurious" concepts.
- **Mechanism:** The method defines valid concepts (e.g., "curved horns" for an Ox) and spurious concepts (e.g., "watermark"). It calculates an alignment score for each neuron against these sets. If neurons with high relevance for a class prediction have high alignment with spurious concepts, the model is flagged for "Clever Hans" behavior.
- **Core assumption:** The user-provided text descriptions of valid/spurious concepts adequately cover the necessary visual features for the task.
- **Evidence anchors:** [key outcome] "identif[ies] spurious behaviors and validate alignment with human-defined concepts (e.g., ABCDE-rule in melanoma detection)."

## Foundational Learning

- **Concept: Concept Relevance Propagation (CRP)**
  - **Why needed here:** CRP is the engine that drives the "Concept Examples" extraction and relevance scoring. Without understanding how LRP backpropagates relevance to identify specific input features, one cannot understand how SemanticLens isolates the "meaningful" part of an image.
  - **Quick check question:** Can you explain how Layer-wise Relevance Propagation (LRP) differs from standard gradient-based attribution in terms of conservation properties?

- **Concept: Multimodal Embeddings (CLIP)**
  - **Why needed here:** The entire method relies on the ability to map image patches (concept examples) and text queries (search/audit terms) into a shared vector space. Understanding the training dynamics of CLIP (contrastive loss) is necessary to interpret the similarity scores.
  - **Quick check question:** How does a contrastive loss function align image and text representations in a multimodal model like CLIP?

- **Concept: Polysemanticity & Superposition**
  - **Why needed here:** The paper explicitly evaluates "interpretability" using measures for polysemanticity (neurons responding to multiple unrelated concepts). Understanding the "superposition hypothesis" is crucial for interpreting the results regarding wider models being more interpretable.
  - **Quick check question:** Why might a neural network layer with fewer neurons tend to store multiple distinct concepts in a single neuron (polysemanticity)?

## Architecture Onboarding

- **Component map:** Target Model -> Attribution Engine (CRP) -> Foundation Model -> Vector Store -> Audit Interface

- **Critical path:**
  1. Inference & Activation: Pass dataset through Model to record neuron activations
  2. Localization: For top-activating samples per neuron, run CRP to generate crop masks
  3. Embedding: Pass cropped samples through Foundation Model to get feature vectors; average them to get semantic vector for each neuron
  4. Querying: Use text prompts or image probes to search the Vector Store or audit alignment scores

- **Design tradeoffs:**
  - Foundation Model Choice: A general model (Mobile-CLIP) offers speed but may lack domain nuance. Domain-specific models (WhyLesionCLIP) are required for high-stakes auditing.
  - Sample Size: The paper suggests 30 samples. Fewer samples risk noise; more samples increase computational cost with diminishing returns.
  - Localization Method: CRP is accurate for CNNs. For ViTs, the paper uses "approximated" attributions (up-sampled spatial maps), which may be less precise.

- **Failure signatures:**
  - Low Clarity Scores: If neurons have low "clarity" (high noise in concept examples), the resulting semantic embeddings will be unreliable for search/audit.
  - High Spurious Alignment: If a model relies heavily on features (like watermarks) that correlate with the target class, the audit will flag high relevance for spurious concepts.
  - Semantic Hallucination: If the Foundation Model is poorly calibrated for the domain, it may assign confident labels to noise.

- **First 3 experiments:**
  1. Search Engine Test: Implement the search function to find "watermark" neurons in a pre-trained ResNet50v2 to verify the embedding pipeline works.
  2. Architecture Comparison: Compute "clarity" and "polysemanticity" scores for a ResNet18 vs. ResNet101 to reproduce the finding that wider models are more interpretable.
  3. Medical Audit: Using a domain-specific foundation model (if available) or standard CLIP, audit a medical classifier to see if it aligns with the ABCDE-rule or spurious artifacts (rulers/band-aids).

## Open Questions the Paper Calls Out

- **Open Question 1:** How effectively does the SemanticLens framework transfer to non-visual data domains such as text, audio, or video? [explicit] Supplementary Note I explicitly states that applying SemanticLens to domains "such as text, audio or video" constitutes future work.
- **Open Question 2:** Can SemanticLens be adapted to audit and validate generative AI models? [explicit] Section 5 lists "application to generative models" as a specific area of remaining future work.
- **Open Question 3:** How do specific training hyperparameters, such as adversarial training or weight decay regularization, influence the proposed latent interpretability measures? [explicit] Section 5 explicitly lists "adversarial training" and "weight decay regularization" as training hyperparameters left for future work.

## Limitations

- The approach relies heavily on the foundation model's ability to semantically represent concepts accurately, which can be problematic for specialized domains.
- The method assumes that the most activating samples per neuron fully capture its function, but neurons may be polysemantic, requiring more sophisticated sampling strategies.
- The relevance-based cropping depends on accurate attribution maps, which can be challenging for transformers where CRP approximations are used.

## Confidence

- **High Confidence:** The core mechanism of projecting neuron activations into a foundation model's embedding space for semantic labeling is well-supported by the results in Sections 4.1 and 4.2.
- **Medium Confidence:** The interpretability measures (clarity, polysemanticity, redundancy) show consistent patterns across architectures, but absolute values and practical implications require further validation.
- **Medium Confidence:** The audit framework for detecting spurious correlations works as demonstrated on ImageNet, but effectiveness in complex, real-world scenarios remains to be seen.

## Next Checks

1. **Domain Transfer Test:** Apply SemanticLens to a specialized medical dataset (e.g., CheXpert) using both a general CLIP model and a domain-specific model to quantify the impact of foundation model choice on audit accuracy.
2. **Architectural Stress Test:** Compare interpretability measures across a wider range of architectures (CNNs, ViTs, MLPs) to validate the claim that "wider models are more interpretable" and identify failure modes in different architectures.
3. **Real-world Bias Audit:** Apply the audit framework to a deployed model (e.g., a commercial image classifier) to identify and quantify spurious correlations that might not be obvious from dataset statistics alone.