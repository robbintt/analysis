---
ver: rpa2
title: 'Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic'
arxiv_id: '2601.16486'
source_url: https://arxiv.org/abs/2601.16486
tags:
- time
- tool
- reasoning
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel perspective on test-time scaling
  in agentic scenarios, redefining it as wall-clock time rather than generation length.
  The authors argue that in environments with tool latency, models need time awareness
  to dynamically adjust strategies based on available budgets.
---

# Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic

## Quick Facts
- arXiv ID: 2601.16486
- Source URL: https://arxiv.org/abs/2601.16486
- Authors: Yichuan Ma; Linyang Li; Yongkang chen; Peiji Li; Xiaozhe Li; Qipeng Guo; Dahua Lin; Kai Chen
- Reference count: 40
- One-line primary result: TimelyLM achieves 78.0% on AIME, 42.5% on GPQA, and 49.5% on MATH by learning time-aware reasoning through wall-clock time budgeting.

## Executive Summary
This paper redefines test-time scaling in agentic scenarios as wall-clock time rather than generation length, arguing that models need time awareness to dynamically adjust strategies based on available budgets and tool latencies. The authors introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning, along with Timely-RL, a reinforcement learning approach that teaches models time-aware reasoning through temporal planning. Results show that smaller models outperform larger ones under low latency through more interactions, while larger models excel when latency is high through superior interaction quality. After cold-start supervised fine-tuning and Timely-RL training, the model demonstrates accurate time budget awareness and consistently improves performance across Timely-Eval benchmarks.

## Method Summary
The approach consists of two stages: first, cold-start supervised fine-tuning on time-constrained teacher traces (1M examples distilled from Qwen3-235B with timer tools under varied time limits), then Timely-RL training using a GRPO variant with a time-utilization reward function. The reward structure combines format reward, accuracy reward, and a sine-based time-utilization term U(t) = sin(π/2 × min(t/T_max, 1)) to encourage maximal budget usage without near-deadline instability. The model is evaluated on Timely-Eval, which includes 57 interactive games, 4 ML tasks, and AIME/MATH/GPQA-diamond reasoning benchmarks across multiple time budget multipliers.

## Key Results
- TimelyLM achieves 78.0% on AIME, 42.5% on GPQA, and 49.5% on MATH benchmarks
- On-time completion rate reaches 53.3% under 0.75× time budgets, outperforming baseline GRPO's 30%
- Smaller models (8B) better for low-latency regimes via more interactions; larger models (32B) better for high-latency regimes via quality
- Existing models lack ability to adjust reasoning strategies based on time budgets

## Why This Works (Mechanism)

### Mechanism 1: Wall-Clock Time Budgeting Decouples from Token Count
Models trained to condition on wall-clock time can dynamically allocate strategy based on tool latency regimes. The paper formalizes total time as t_all = Σ t_gen + Σ t_tool. When tool latency dominates (m_i = t_tool/t_gen >> 1), per-round interaction quality determines performance; when generation dominates (m_i << 1), more interaction rounds are optimal. Time-aware models learn to estimate and adapt to this ratio.

### Mechanism 2: Time-Utilization Reward Shaping via Sine-Based Utility
A reward function incorporating time utilization U(t) = sin(π/2 · min(t/T_max, 1)) encourages maximal budget usage without near-deadline instability. The sin function slows reward growth near t/T_max ≈ 1, preventing minor timing differences from causing large reward fluctuations. Combined with format reward r_f and accuracy reward r, the model learns to both complete on time AND use available budget for deeper reasoning.

### Mechanism 3: Cold-Start Distillation Provides Temporal Planning Prior
Supervised fine-tuning on time-constrained teacher traces initializes the model with basic time-awareness before RL. The paper distills from Qwen3-235B with timer tools under varied time limits. SFT provides a starting policy that already knows how to call get_duration() and plan within budgets; Timely-RL then refines this capability.

## Foundational Learning

- Concept: Test-Time Scaling
  - Why needed here: The paper redefines this from token-based to wall-clock time; understanding the traditional framing is prerequisite.
  - Quick check question: Can you explain why generation length and wall-clock time correlate in pure reasoning tasks but diverge with tool calls?

- Concept: Reinforcement Learning with Verifiable Rewards (RLVR)
  - Why needed here: Timely-RL builds on GRPO/RLVR paradigm; understanding reward structures is essential.
  - Quick check question: What makes a reward "verifiable" and how does time-utilization complicate this?

- Concept: Tool Latency Regimes
  - Why needed here: The paper's core insight is that optimal strategy depends on whether tools or model generation dominate.
  - Quick check question: Given a task where tool calls take 10× longer than model inference, should the model prioritize more interactions or higher-quality per-round reasoning?

## Architecture Onboarding

- Component map: Timer Tool (get_duration service) → Model (Qwen3-8B base) → VeRL framework for RL rollout → SGLang for inference → Jericho/ML/Reasoning environments
- Critical path: (1) Distill time-constrained traces from teacher → (2) SFT on combined dataset → (3) Timely-RL with time-utilization reward → (4) Evaluate on Timely-Eval
- Design tradeoffs: Smaller models (8B) better for low-latency regimes via more interactions; larger models (32B) better for high-latency regimes via quality. Timer coefficient α simulates device speed variations but adds hyperparameter complexity.
- Failure signatures: (1) Timeout without answer → model failed to track remaining budget; (2) Correct answer but zero reward → t > T_max; (3) Low on-time rate with high accuracy → model not using timer tool sufficiently.
- First 3 experiments:
  1. Baseline Qwen3-8B on Timely-Eval with 0.75× budget—measure on-time rate and accuracy to establish gap.
  2. Ablate reward components: train with (a) accuracy-only reward, (b) accuracy + format, (c) full Timely-RL reward—compare on-time rates.
  3. Vary tool latency on a single game (e.g., Zork1) across No/Low/Medium/High settings—confirm 8B vs 32B scaling crossover.

## Open Questions the Paper Calls Out

### Open Question 1
How does time-aware reasoning transfer to multimodal interactive tasks where processing image or video streams introduces more complex and variable latencies? The authors state in Limitations: "In multimodal interactive tasks, processing image or video streams typically introduces more complex and variable latencies, presenting a more challenging scenario." This remains unresolved as Timely-Eval and Timely-RL were only validated on text-based tasks.

### Open Question 2
Can models trained with Timely-RL learn effective collaboration strategies in multi-agent systems where latency arises from another LLM's generation time rather than tool execution? The authors explicitly note: "training LLMs for effective collaboration in such contexts remains an open topic" for multi-agent settings. Current framework treats tool latency as external and predictable; multi-agent settings introduce latency from peer model inference.

### Open Question 3
How can the trade-off between strict time constraints and accuracy be systematically balanced, particularly on challenging benchmarks where on-time completion doesn't yield significant accuracy gains? The authors observe: "on more challenging tasks like AIME and GPQA-diamond, higher completion rates do not lead TimelyLM to significantly outperform other models" and call for future work on this trade-off.

## Limitations

- Performance improvements show diminishing returns at larger time budgets, suggesting limited impact in unconstrained environments
- Benchmark focuses on specific domains (interactive games, math reasoning, ML tasks) that may not generalize to all tool-use scenarios
- Time-aware capabilities may not transfer to entirely new domains or tasks not present in the training distribution
- Does not address how time-aware reasoning works in multimodal settings with variable visual processing delays

## Confidence

- High confidence: The empirical observation that smaller models outperform larger ones under low latency through more interactions, while larger models excel when latency is high through superior interaction quality
- Medium confidence: The effectiveness of the sine-based utility function for reward shaping
- Medium confidence: The claim that existing models cannot adjust reasoning strategies based on time budgets

## Next Checks

1. Test the trained model on tool-use tasks outside the training distribution (e.g., web navigation, API interactions) to verify whether time-awareness generalizes beyond games and math problems

2. Systematically remove each component of the reward function (format reward, accuracy reward, time-utilization reward) and measure the contribution of each to the final performance

3. Deploy the model in a realistic multi-tool environment with actual network latencies (not simulated) and measure whether the learned time-awareness still produces optimal behavior