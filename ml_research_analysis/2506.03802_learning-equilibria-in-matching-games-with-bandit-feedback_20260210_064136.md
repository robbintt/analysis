---
ver: rpa2
title: Learning Equilibria in Matching Games with Bandit Feedback
arxiv_id: '2506.03802'
source_url: https://arxiv.org/abs/2506.03802
tags:
- agents
- matching
- agent
- where
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies a matching market where agents interact via zero-sum
  games with unknown payoff matrices and aims to learn a stable matching equilibrium
  using bandit feedback. It proposes a UCB-style algorithm where agents form preferences
  and select strategies based on optimistic estimates of game payoffs, and a central
  platform matches agents according to these preferences.
---

# Learning Equilibria in Matching Games with Bandit Feedback

## Quick Facts
- **arXiv ID**: 2506.03802
- **Source URL**: https://arxiv.org/abs/2506.03802
- **Reference count**: 40
- **Primary result**: UCB algorithm achieves Õ(√(Tmkpa)) sublinear regret in learning stable matching equilibria with bandit feedback

## Executive Summary
This paper introduces a novel framework for learning stable matching equilibria in two-sided markets where matched agents play zero-sum games with unknown payoff matrices. The algorithm combines Upper Confidence Bound (UCB) exploration with Gale-Shapley Deferred Acceptance to simultaneously learn preferences and strategies from bandit feedback. Agents maintain optimistic estimates of game payoffs and report preferences to a central platform, which computes stable matchings. The approach generalizes both classical stable matching (when games are trivial) and learning in zero-sum games (when markets have single agents).

## Method Summary
The method employs UCB-MG, where each agent maintains empirical estimates of unknown payoff matrices and their confidence bounds. Agents compute optimistic game values via min-max on these UCB matrices, sort them to form preferences, and report to a central platform. The platform runs Gale-Shapley Deferred Acceptance to produce stable matchings. Matched pairs then play min-max strategies computed from their UCB matrices, observe outcomes, and update estimates. The regret metric is cumulative matching instability—the minimum subsidies needed to prevent blocking pairs. The algorithm achieves Õ(√(Tmkpa)) regret where T is horizon, m,k are actions per agent, and p,a are agent counts on each side.

## Key Results
- Proves matching instability (MI) = 0 iff strategies match Nash values AND matching is stable
- Achieves Õ(√(Tmkpa)) regret bound that generalizes both stable matching and zero-sum game learning
- Experiments show sublinear regret growth across different market sizes and game settings
- Nash-response (one side knows true payoffs) outperforms best-response and self-play in large markets

## Why This Works (Mechanism)

### Mechanism 1: Optimistic Payoff Estimates
- **Claim**: Optimistic payoff estimates enable simultaneous learning of preferences and strategies without knowing true game values
- **Mechanism**: Agents maintain upper confidence bounds (UCB) for each entry of unknown payoff matrices. At each timestep, they: (1) compute game values V̄* from UCB matrices, (2) form preferences by sorting these optimistic values, (3) report preferences to central platform, which runs Deferred Acceptance
- **Core assumption**: Payoff noise is 1-subGaussian; all agents on same side have same action cardinality for analysis convenience
- **Evidence anchors**: [abstract] "We propose a UCB algorithm in which agents form preferences and select actions based on optimistic estimates of the game payoffs"; [Section 4, Algorithm 1] Lines 5-10 show explicit UCB computation and preference sorting via `arg sort`
- **Break condition**: If payoff noise is heavy-tailed or confidence bounds mis-specified, optimism may fail to concentrate, causing preference thrashing

### Mechanism 2: Min-Max Strategy Selection
- **Claim**: Min-max strategy selection on optimistic payoffs ensures Nash-rationality under the algorithm's constructed solution
- **Mechanism**: After platform assigns match m(a), each agent independently computes min-max strategy against their partner's action space using the UCB matrix Ā. Proposition 2 (S1) proves that under UCB estimates, the selected strategies satisfy V̄* − Ū(x, x_m(a)) ≤ 0, meaning agents achieve at least the optimistic game value
- **Core assumption**: Zero-sum structure ensures V* = −V* for opposing agents; Nash equilibrium values are unique even if strategies aren't
- **Evidence anchors**: [Section 4] "each agent a ∈ A observes their match m_t(a) and samples actions from their respective min-max strategies"; [Appendix C, Proposition 2] Formal proof that min-max on UCB bounds satisfies Nash rationality constraints
- **Break condition**: If games are general-sum (not zero-sum), Nash uniqueness fails and preference formation via game value becomes ill-defined

### Mechanism 3: Matching Instability as Regret Metric
- **Claim**: Matching instability provides a proper regret metric that generalizes both stable matching regret and zero-sum game duality gap
- **Mechanism**: Matching instability MI(m, X) solves a linear program finding minimum subsidies s_a ≥ 0 satisfying: (C1) no blocking pairs under Nash values, (C2) individual rationality, (C3) utility ≥ Nash value of matched game. Proposition 1 proves MI=0 iff strategies match Nash values AND matching is stable
- **Core assumption**: Utilities for being unmatched U_{a,⊥} are known; agents prefer matching when game values exceed outside option
- **Evidence anchors**: [Section 3.2, Definition 4] Formal LP definition of matching instability with four constraints; [Proposition 1, property 3-4] Shows reduction to prior metrics in edge cases
- **Break condition**: If outside options are unknown or time-varying, subsidies needed to stabilize may be uncomputable from observed data alone

## Foundational Learning

- **Concept: Gale-Shapley Deferred Acceptance**
  - **Why needed here**: Central platform uses DA to compute stable matchings from reported preferences. Understanding proposal/acceptance dynamics explains why platform-side agents get optimal matchings
  - **Quick check question**: If agents on side P propose, does the resulting stable matching favor P or A?

- **Concept: Two-Player Zero-Sum Game Nash Equilibrium**
  - **Why needed here**: Each matched pair plays a zero-sum game. Agents must understand that Nash equilibrium value V* is unique (by minimax theorem) even when equilibrium strategies aren't
  - **Quick check question**: In a 2×2 zero-sum game with payoff matrix [[1,−1],[−1,1]], what is the game value V*?

- **Concept: UCB Confidence Bounds for Matrices**
  - **Why needed here**: Algorithm maintains per-entry confidence intervals for unknown payoff matrices. Entry (i,j) gets bonus √(2 log(1/δ) / n(i,j)) where n is visit count
  - **Quick check question**: Why does the bonus decrease as n(i,j) increases? What happens if an entry is never visited?

## Architecture Onboarding

- **Component map**: Agents (maintain UCB estimates) -> Central Platform (runs DA) -> Agents (receive matches) -> Agents (compute min-max) -> Agents (sample actions) -> Agents (observe outcomes) -> Agents (update estimates)

- **Critical path**: (1) Initialize all payoff estimates to zero, counts to zero -> (2) Compute UCB matrices -> (3) Form preferences via sorting optimistic game values -> (4) Run DA to get m_t -> (5) Each matched agent solves min-max on their UCB matrix -> (6) Sample actions, observe outcomes, update single entry -> (7) Repeat

- **Design tradeoffs**:
  - **Centralized vs decentralized**: Paper assumes central platform; decentralized requires agents to independently discover stable partners (harder, future work per Section 7)
  - **Observation model**: Assumes agents observe partner's action; "uncoupled dynamics" without this observation is open problem
  - **δ parameter**: Smaller δ → wider confidence intervals → more exploration → higher per-step regret but lower failure probability; paper sets δ = 1/(4T²p²a²mk)

- **Failure signatures**:
  - **Linear regret growth**: Indicates confidence bounds too tight or DA not converging; check δ setting
  - **Preference oscillation**: Same agent pair repeatedly matched then blocking; may indicate non-zero-sum games or incorrect outside-option handling
  - **Unvisited entries**: If some (i,j) pairs never sampled, confidence bounds remain infinite; may need forced exploration phase

- **First 3 experiments**:
  1. **Single pair (p=a=1) with known game value**: Verify algorithm recovers standard zero-sum UCB regret O(√Tmk); compare against EXP3 baseline from [36]
  2. **Vary market size (p,a) with m=k=1**: Verify regret scales as O(√Tpa) matching [26] subset instability bounds
  3. **Nash-response vs Best-response vs Self-play**: Replicate Figure 1 showing Nash-response (one side knows true payoffs) achieves lower regret than Best-response (exploitative) at scale

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework be extended to general-sum games or multi-player matching models?
- **Basis in paper**: [explicit] The Discussion section states that extending the framework to "general-sum games or matching models involving more than two players" introduces challenges for learning equilibrium
- **Why unresolved**: The current analysis relies heavily on the properties of two-player zero-sum games (specifically the unique value and minimax strategies), which do not translate directly to general-sum settings
- **What evidence would resolve it**: An algorithm that defines a suitable equilibrium concept for general-sum matching games and proves convergence or sublinear regret under that metric

### Open Question 2
- **Question:** Is it possible to learn equilibria in decentralized settings or with uncoupled dynamics?
- **Basis in paper**: [explicit] The authors list "scenarios with uncoupled dynamics" (where agents cannot observe actions) and "decentralized settings" (no central platform) as important future directions
- **Why unresolved**: The proposed UCB algorithm requires a central platform to compute matchings (via Deferred Acceptance) and requires agents to observe their match's action to update payoff estimates
- **What evidence would resolve it**: A modified algorithm that operates without a central coordinator or full action observability, along with a corresponding regret analysis

### Open Question 3
- **Question:** Can the learning objective be modified to ensure equitable outcomes by bounding regret for each individual agent?
- **Basis in paper**: [explicit] The Discussion notes that the current objective (matching instability) "does not necessarily ensure equitable outcomes" and suggests a metric where regret is "bounded for each individual agent"
- **Why unresolved**: The current definition of regret minimizes the aggregate subsidy required to stabilize the market, which may allow the learning process to disproportionately benefit specific subsets of agents
- **What evidence would resolve it**: A proof that the algorithm achieves sublinear regret not just in aggregate, but simultaneously for every agent $a \in A$ individually

## Limitations
- **Centralization requirement**: Algorithm requires central platform to run Gale-Shapley Deferred Acceptance
- **Known outside options**: Agents must know their outside-option utility U_{a,⊥} in advance
- **Information structure**: Agents must observe their partner's action in each game

## Confidence
- **Regret bound**: High - mathematically rigorous given assumptions
- **Algorithm correctness**: High - well-specified with strong theoretical justification
- **Experimental validation**: Medium - shows sublinear regret but only on synthetic data

## Next Checks
1. **Vary the observation model**: Test algorithm performance when agents only observe their own rewards (no partner action observation) to understand robustness to information loss
2. **Scale to larger markets**: Verify regret scaling with larger p, a, m, k values to test practical limits of the approach
3. **Non-zero-sum extensions**: Test algorithm performance on general-sum games to identify where zero-sum assumption breaks down