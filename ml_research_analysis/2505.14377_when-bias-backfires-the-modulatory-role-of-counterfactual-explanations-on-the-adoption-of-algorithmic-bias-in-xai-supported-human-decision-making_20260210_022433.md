---
ver: rpa2
title: 'When Bias Backfires: The Modulatory Role of Counterfactual Explanations on
  the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making'
arxiv_id: '2505.14377'
source_url: https://arxiv.org/abs/2505.14377
tags:
- bias
- participants
- trust
- recommendations
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study examines how biased AI recommendations, with and without
  counterfactual explanations (CEs), influence human decision-making in hiring scenarios.
  Participants made 60 hiring decisions across three phases: baseline, with biased
  AI, and post-interaction.'
---

# When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making

## Quick Facts
- **arXiv ID:** 2505.14377
- **Source URL:** https://arxiv.org/abs/2505.14377
- **Reference count:** 40
- **Primary result:** Counterfactual explanations can reverse rather than prevent bias adoption in human decision-making.

## Executive Summary
This study examines how biased AI recommendations with and without counterfactual explanations (CEs) influence human decision-making in hiring scenarios. Across three experimental phases (baseline, AI interaction, post-interaction), participants made 60 hiring decisions. Results show that while participants followed AI recommendations 70% of the time when qualifications were comparable, only 8 out of 294 detected gender bias. Without CEs, participants adopted AI bias in later decisions; with CEs, they shifted decisions in the opposite direction of the bias. Trust levels remained stable across conditions despite these behavioral shifts. The study highlights unintended effects of CEs, showing they can reverse rather than prevent bias adoption.

## Method Summary
The study used a controlled human-subject experiment with 4 between-subjects conditions (FB-AI, MB-AI, FB-XAI, MB-XAI). Each participant completed 60 trials across three phases: 20 baseline decisions, 20 decisions with biased AI recommendations, and 20 post-interaction decisions. Biased recommendations systematically favored one gender in mixed-gender pairs. CEs were generated by identifying the lowest-scoring feature of the disadvantaged candidate. Statistical analyses included 2Ã—2 ANOVA and linear mixed-effects models (lme4 package in R 4.4.2). Data included 120 candidate profiles with aptitude scores (N(7,2) bounded [2,10]) and protected attributes, plus AI-generated headshots.

## Key Results
- Participants followed AI recommendations 70% of the time when qualifications were comparable
- Without CEs, participants adopted AI bias in post-interaction decisions
- With CEs, participants shifted decisions opposite to AI bias direction
- Only 8 out of 294 participants detected gender bias
- Trust levels did not differ significantly across conditions

## Why This Works (Mechanism)

### Mechanism 1: Unconscious Bias Adoption via Automation Reliance
- **Claim:** Humans internalize algorithmic bias as valid decision patterns when they fail to detect bias and view AI as objective
- **Evidence:** 70% alignment with AI recommendations, only 8/294 bias detections
- **Break condition:** Bias too obvious or user has deep domain expertise

### Mechanism 2: Over-Compensatory Reactance (The "Backfire" Effect)
- **Claim:** CEs trigger psychological reactance causing users to shift decisions opposite to AI's demonstrated bias
- **Evidence:** CEs made bias explicit but led to reversal rather than mitigation
- **Break condition:** User unaware of protected attribute or CE provides compelling non-discriminatory reason

### Mechanism 3: The Trust-Behavior Decoupling
- **Claim:** Explicit trust is stable and disconnected from behavioral shifts caused by XAI interventions
- **Evidence:** Trust levels stable across conditions despite behavioral changes
- **Break condition:** High AI error rate that is detected by users

## Foundational Learning

- **Concept: Automation Bias vs. Algorithmic Bias**
  - **Why needed:** Distinguish human tendency to defer to machines from machine's tendency to discriminate
  - **Quick check:** Did participants adopt bias because AI was persuasive or because they were cognitively lazy? (Evidence suggests latter)

- **Concept: Psychological Reactance**
  - **Why needed:** Explain counter-intuitive backfire result where explanations made things worse
  - **Quick check:** Why would user following AI advice swing opposite after seeing "helpful" explanation?

- **Concept: Counterfactual Explanations (CEs)**
  - **Why needed:** Understand specific XAI manipulation showing "what-if" scenarios
  - **Quick check:** How does CE differ from feature importance score in terms of actionability?

## Architecture Onboarding

- **Component map:** Candidate Profile Generator -> Bias Logic Module -> XAI Layer -> Interface -> Measurement
- **Critical path:** 1) Establish human baseline bias, 2) Inject biased AI/XAI recommendations, 3) Measure residual bias in human-only decisions
- **Design tradeoffs:** Controlled vs. Real AI (high internal validity vs. lower ecological realism); Subtle vs. Egregious Bias (mirrors invisible bias vs. low detection rates)
- **Failure signatures:** Bias Adoption (>0.5 correlation with AI bias), Oppositional Bias (<-0.5 correlation), Invisible Influence (>60% alignment, <5% detection)
- **First 3 experiments:**
  1. Re-baseline with XAI-only: Show only CEs without AI recommendation to isolate explanation effect
  2. Vary CE Directionality: Test "upward" vs. "downward" CEs on bias reversal
  3. Detection Intervention: Warn group that "AI may be biased" to test if awareness prevents adoption

## Open Questions the Paper Calls Out

1. **What psychological mechanisms drive the bias reversal effect?** The authors suggest psychological reactance but did not explore specific mechanisms, calling for experiments measuring conscious awareness or testing different explanation framings.

2. **Does the bias reversal effect persist over longer time periods?** The study only measured immediate post-interaction decisions; longitudinal follow-up at multiple time points is needed.

3. **How does bias detection and response vary with bias subtlety?** The study used subtle bias (8/294 detected); parametric manipulation of bias strength across levels is needed.

4. **Can alternative explanation formats prevent both bias adoption and reversal?** Only standard counterfactuals were tested; comparative experiments with personalized or interactive explanations are needed.

## Limitations
- Controlled setup with scripted bias injection limits generalizability to real deployed systems
- Extremely low bias detection rate (8/294) suggests bias was too subtle for ecological validity
- Results only examined gender bias; may not generalize to other protected attributes or bias types

## Confidence
- **High Confidence:** Empirical finding of 70% AI alignment and automation bias adoption
- **Medium Confidence:** CEs triggered oppositional bias reversal (statistically significant but counter-intuitive)
- **Low Confidence:** Proposed psychological reactance mechanism for backfire effect (behavioral evidence only)

## Next Checks
1. **Replication with Real AI Models:** Verify CE backfire effect occurs with realistic, learned bias patterns from actual trained ML models
2. **Mediation Analysis:** Collect psychological reactance, fairness perceptions, and autonomy threat measures to test mediation of CE-induced reversal
3. **Generalization Test:** Pre-registered replication with different protected attributes and bias types to assess boundary conditions of backfire effect