---
ver: rpa2
title: '\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic
  Modulation Recognition'
arxiv_id: '2509.08300'
source_url: https://arxiv.org/abs/2509.08300
tags:
- coreset
- selection
- training
- samples
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FoQuS introduces a forgetting-quality coreset selection framework
  for AMR tasks, addressing the inefficiency of training deep learning models on large
  datasets. The method records prediction trajectories during full-dataset training
  and constructs three complementary importance metrics: Forgetting Score (locating
  decision-boundary samples), Persistent Error Score (capturing hard-to-fit samples),
  and Quality Score (measuring learning value).'
---

# *FoQuS*: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition

## Quick Facts
- **arXiv ID:** 2509.08300
- **Source URL:** https://arxiv.org/abs/2509.08300
- **Reference count:** 21
- **Primary result:** 74.56% accuracy on RML2016.10a with just 1% of training data using tiered coreset sampling

## Executive Summary
*Fully Qualified Sampling* (*FoQuS*) introduces a novel coreset selection framework for Automatic Modulation Recognition (AMR) that addresses the inefficiency of training deep learning models on large datasets. The method records prediction trajectories during full-dataset training and constructs three complementary importance metrics: Forgetting Score (locating decision-boundary samples), Persistent Error Score (capturing hard-to-fit samples), and Quality Score (measuring learning value). These metrics are normalized and combined to create the FoQuS score, which is used to select diverse coresets through tiered sampling. Experiments on three AMR datasets (RML2016.10a, Sig2019-12, RML2018.01a) show that FoQuS outperforms 10 baselines, achieving 74.56% accuracy on RML2016.10a with just 1% of data, and maintains cross-architecture generalization.

## Method Summary
FoQuS is a coreset selection framework for AMR that records prediction trajectories during full-dataset training for T epochs. The method computes three metrics per sample: Forgetting Score (counts transitions from correct to incorrect predictions), Persistent Error Score (counts consecutive incorrect predictions), and Quality Score (balances correctness with accumulated loss using β=0.1). These metrics are normalized and combined via equation S_FoQuS = S_forget/(T-1) + S_persist_err/(T-1) + S_quality. Samples are sorted by FoQuS score and partitioned into three tiers for proportional sampling, ensuring diversity. The framework is validated on RML2016.10a, Sig2019-12, and RML2018.01a datasets at 1%, 5%, 10%, 20%, and 30% sampling rates using CNN1D/CNN2D architectures.

## Key Results
- Achieves 74.56% accuracy on RML2016.10a with only 1% of training data
- Outperforms 10 baseline methods including random sampling and K-Center greedy selection
- Maintains cross-architecture generalization, with coreset selected by CNN1D working effectively with CNN2D, LSTM, SigNet, and IQFormer
- Shows consistent performance improvements across all three AMR datasets at multiple sampling rates

## Why This Works (Mechanism)

### Mechanism 1: Forgetting Events Capture Decision Boundary Dynamics
Samples transitioning from correct to incorrect predictions during training are disproportionately informative for model generalization. The Forgetting Score identifies these boundary samples by counting decision reversals during SGD, capturing high-gradient signal that defines class separability. This works because decision boundary samples are sensitive to parameter perturbations and provide richer information than consistently correct samples.

### Mechanism 2: Persistent Error Score Preserves Hard Samples Across SNR Regimes
Samples remaining misclassified across training epochs encode critical information about low-SNR and ambiguous modulation classes. The Persistent Error Score captures "stubborn" samples that the model struggles to fit, representing under-represented SNR regions or class ambiguities essential for robust AMR. This prevents oversampling of easily learned high-SNR samples that would degrade performance in noisy environments.

### Mechanism 3: Quality Score Balances Learnability and Gradient Signal
Samples that are partially learned (correct often but with high accumulated loss) provide meaningful gradients without dominating the coreset. The Quality Score identifies these "learnable but informative" samples by computing correctness minus loss, ensuring the coreset includes simple, easily classified samples that still offer gradient utility. Purely easy samples provide diminishing returns while purely hard samples may represent noise.

## Foundational Learning

- **Concept: Coreset Selection** - Why needed: FoQuS is fundamentally a coreset method; understanding that the goal is L_D(θ_D) ≈ L_S(θ_S) is prerequisite. Quick check: Can you explain why selecting top-k samples by a single metric might fail to preserve SNR diversity?

- **Concept: Signal-to-Noise Ratio (SNR) in AMR** - Why needed: The paper explicitly states that image-based coreset methods fail because they ignore SNR structure in signal data. Quick check: Why would oversampling high-SNR samples lead to performance degradation in noisy deployment environments?

- **Concept: Training Dynamics and Forgetting Events** - Why needed: The three metrics derive from per-epoch prediction trajectories; understanding how models "forget" during SGD is essential. Quick check: If a sample is forgotten 5 times during training, what does that imply about its position relative to the decision boundary?

## Architecture Onboarding

- **Component map:** Full-dataset training -> Prediction trajectory logging -> Three metric computations -> Normalization and combination -> Tiered sampling -> Coreset construction
- **Critical path:** 1) Run full training once (expensive upfront cost), 2) Compute per-sample metrics from logged predictions, 3) Normalize and combine scores, 4) Apply tiered sampling to construct coreset, 5) Use coreset for all subsequent model development/hyperparameter tuning
- **Design tradeoffs:** Upfront cost vs. downstream savings (must train full model once), tiered vs. top-k sampling (paper uses tiered to ensure diversity), equal weighting of three metrics (no learned weighting)
- **Failure signatures:** Coreset accuracy drops sharply below 5% sampling rate (insufficient coverage), cross-architecture transfer fails (selection model too specialized), persistent error score dominates (possible label noise)
- **First 3 experiments:** 1) Reproduce RML2016.10a baseline at 1% and 10% sampling with CNN1D to validate score computation, 2) Ablation across single/pairwise metric combinations to confirm all three metrics contribute, 3) Cross-architecture test (CNN1D→CNN2D) to verify generalization claim

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How does the FoQuS selection strategy perform on datasets with predominantly low Signal-to-Noise Ratios (SNR), as opposed to the high-SNR data prioritized in the experiments?
**Basis in paper:** [inferred] The introduction argues that ignoring SNR leads to performance degradation in noisy environments, yet Section IV states, "Unless otherwise specified, we uniformly use high SNR data for experiments."
**Why unresolved:** Experimental results report average accuracy but do not isolate performance metrics for low-SNR samples, which are typically the "hard" samples the method claims to value.
**What evidence would resolve it:** A breakdown of classification accuracy specifically for low-SNR tiers (e.g., -10dB to 0dB) on RML2016.10a dataset using FoQuS coreset compared to full training.

### Open Question 2
**Question:** Can the initial "full-dataset training" phase required to calculate the FoQuS score be approximated or shortened to improve efficiency for one-off training scenarios?
**Basis in paper:** [inferred] Section III states that FoQuS "first trains a model on the entire dataset" to record trajectories. While efficient for repeated tuning, this upfront computational cost effectively negates savings for single training runs.
**Why unresolved:** Paper validates efficiency for subsequent model development but does not address the overhead of mandatory initial training epoch used to derive sample importance metrics.
**What evidence would resolve it:** Experiments showing correlation between number of initial training epochs (e.g., training only 1 epoch vs. 50) and final quality of selected coreset.

### Open Question 3
**Question:** Would a dynamic or learnable weighting scheme for the three constituent scores (Forgetting, Persistent Error, Quality) outperform the static summation defined in Equation 8?
**Basis in paper:** [inferred] Section III describes final FoQuS score as normalized addition of three metrics with fixed scaling factor (β=0.1).
**Why unresolved:** Paper does not explore if specific datasets or model architectures might benefit from prioritizing one metric over others rather than treating them with fixed importance.
**What evidence would resolve it:** An ablation study varying weights of three scores or using meta-learning approach to learn optimal weights for different SNR conditions.

## Limitations
- **High upfront cost:** Requires full-dataset training once to record prediction trajectories before any coreset can be selected
- **SNR imbalance risk:** Despite class-balanced sampling, coreset may still underrepresent certain SNR ranges critical for deployment robustness
- **Metric weight assumption:** Equal weighting of three metrics assumes orthogonal dimensions with equal importance, which may not hold across different AMR datasets

## Confidence

- **High confidence:** Coreset selection framework design, tiered sampling methodology, cross-architecture generalization claims
- **Medium confidence:** Forgetting Score mechanism effectiveness (theoretical justification present but limited ablation analysis)
- **Low confidence:** Quality Score's default β=0.1 parameter choice (appears arbitrary, sensitivity analysis not provided)

## Next Checks

1. **SNR Distribution Validation:** Verify that selected coresets maintain similar SNR distributions to full datasets across all three datasets at each sampling rate
2. **Metric Sensitivity Analysis:** Systematically vary β in Quality Score and test whether optimal values are dataset-dependent
3. **Label Noise Robustness:** Evaluate FoQuS performance when injecting synthetic label noise to determine if persistent errors capture true difficulty versus annotation errors