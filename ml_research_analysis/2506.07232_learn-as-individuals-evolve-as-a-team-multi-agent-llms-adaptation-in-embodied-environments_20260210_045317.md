---
ver: rpa2
title: 'Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied
  Environments'
arxiv_id: '2506.07232'
source_url: https://arxiv.org/abs/2506.07232
tags:
- agents
- liet
- multi-agent
- planning
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LIET, a semi-centralized framework for multi-agent
  LLMs adaptation in embodied environments. The key idea is to enable agents to learn
  utility functions individually during exploration for informed decision-making,
  while collaboratively evolving a shared knowledge list for effective communication.
---

# Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments

## Quick Facts
- arXiv ID: 2506.07232
- Source URL: https://arxiv.org/abs/2506.07232
- Reference count: 40
- Primary result: LIET achieves 40.3 steps (vs 48.4) and 79.6% transport rate (vs 76.7%) on multi-agent benchmarks

## Executive Summary
This paper introduces LIET, a semi-centralized framework for adapting large language models to multi-agent embodied environments. The framework enables agents to learn individual utility functions during exploration while collaboratively evolving shared knowledge for team coordination. LIET addresses the challenge of integrating pre-trained LLMs into dynamic multi-agent scenarios where agents must both learn individually and communicate effectively as a team.

## Method Summary
LIET combines individual learning with team evolution through a dual-component architecture. Each agent learns an individual utility function during exploration to make informed decisions based on its experiences. Simultaneously, agents contribute to and benefit from a shared cooperation knowledge list that evolves through reflection mechanisms. The framework operates semi-centrally, with individual agents maintaining autonomy while synchronizing knowledge through the shared list. This approach enables agents to adapt their behavior based on both personal utility optimization and team-level coordination insights.

## Key Results
- LIET achieves 40.3 steps vs 48.4 steps for 2-agent C-WAH baseline
- 79.6% transport rate vs 76.7% on TDW-MAT benchmark
- Ablation studies confirm both individual learning and team evolution are critical for performance

## Why This Works (Mechanism)
The framework succeeds by addressing the dual challenge of individual adaptation and team coordination. Agents learn utility functions that capture their specific environmental interactions, enabling context-aware decision-making. The shared knowledge list serves as a coordination mechanism that allows agents to benefit from collective experience without centralized control. This semi-centralized approach balances autonomy with collaboration, allowing agents to maintain individual agency while leveraging team-level insights for improved performance.

## Foundational Learning
- Multi-agent reinforcement learning: Needed to understand how agents can learn collaboratively while maintaining individual decision-making capabilities. Quick check: Verify understanding of credit assignment and coordination challenges in multi-agent systems.
- Large language model finetuning: Essential for adapting pre-trained models to embodied environments. Quick check: Confirm knowledge of parameter-efficient adaptation techniques.
- Embodied AI: Critical for grasping how agents interact with physical environments. Quick check: Understand the difference between abstract planning and grounded, perception-based decision making.
- Semi-centralized architectures: Important for balancing distributed autonomy with shared coordination. Quick check: Compare fully centralized vs fully decentralized approaches in multi-agent systems.

## Architecture Onboarding

**Component map:** Agent Perception -> Utility Function Learner -> Action Selector -> Environment -> Reward Collector -> Knowledge Reflection -> Shared Knowledge List

**Critical path:** Perception → Utility Learning → Action Selection → Environment Interaction → Reward Collection → Knowledge Reflection → Shared Knowledge Update → Team Coordination

**Design tradeoffs:** The framework trades off complete autonomy for improved coordination through the shared knowledge list. This introduces communication overhead but enables knowledge transfer between agents. The semi-centralized approach balances the scalability limitations of fully centralized methods with the coordination challenges of fully decentralized approaches.

**Failure signatures:** Performance degradation when communication fails or knowledge list becomes outdated. Agents may exhibit conflicting behaviors if individual utility functions diverge significantly. The system may struggle when environmental dynamics change rapidly and the shared knowledge cannot adapt quickly enough.

**3 first experiments:** 1) Test individual utility learning in isolation without team coordination. 2) Evaluate knowledge list evolution with pre-trained but non-learning agents. 3) Assess performance with synthetic data where ground truth utilities are known.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the LIET framework be extended to integrate raw visual and sensory data end-to-end, eliminating the reliance on separate pretrained vision modules?
- Basis in paper: [Explicit] The conclusion explicitly identifies the reliance on pretrained vision modules for converting visual information to text as a limitation, stating that "comprehensive integration of multi-modal information in embodied environments remains an important direction for future research."
- Why unresolved: The current architecture decouples perception (handled by external modules) from planning (handled by the LLM), which may lead to information loss during the translation of visual inputs to text descriptions.
- What evidence would resolve it: Implementation of a multi-modal LLM variant of LIET that processes raw pixels directly and demonstrates comparable or superior performance without intermediate textual descriptions.

### Open Question 2
- Question: How does the shared cooperation knowledge list mechanism perform in large-scale multi-agent systems (e.g., >10 agents) regarding communication overhead and semantic consistency?
- Basis in paper: [Inferred] The experiments are limited to 2–4 agents. While the paper critiques centralized methods for struggling as agent numbers grow, LIET uses a centralized "shared cooperation knowledge list" which could theoretically face similar scalability issues in terms of information overload or conflicting updates.
- Why unresolved: The reflection mechanism updates a single list based on individual experiences; with many agents, the list may update too rapidly or become too generic to provide actionable "hints."
- What evidence would resolve it: Scaling experiments on the C-WAH or TDW-MAT benchmarks with 10+ agents, specifically analyzing the signal-to-noise ratio of the evolving knowledge list.

### Open Question 3
- Question: Can the individual utility function be learned online during task execution rather than relying on pre-collected exploratory datasets?
- Basis in paper: [Inferred] Section 3.2 states the utility function is finetuned on "exploratory datasets" collected beforehand. This implies a static pre-training phase that may not capture dynamic environment changes or novel obstacles encountered at test time.
- Why unresolved: The current method requires a separate data collection phase, limiting the system's ability to adapt to environments where exploration data is unavailable or expensive to generate.
- What evidence would resolve it: A modification where the utility function updates its weights continuously via online learning signals (e.g., TD error) during the task, maintaining accuracy without offline pre-training.

## Limitations
- Experimental validation restricted to two specific benchmarks (C-WAH and TDW-MAT) without demonstrating generalizability
- Step-count improvements lack statistical significance testing, making it difficult to assess whether differences represent meaningful performance gains
- Semi-centralized architecture assumes reliable communication channels that may not hold in real-world deployments with network constraints

## Confidence
- Performance claims (step count and transport rate improvements): Medium confidence - results are benchmark-specific and lack statistical validation
- Framework scalability: Low confidence - no experiments with more than 2 agents or different team sizes
- Knowledge evolution mechanism: Medium confidence - conceptually sound but not extensively validated under communication failures

## Next Checks
1. Conduct statistical significance testing (t-tests or non-parametric equivalents) across multiple random seeds to verify that performance improvements are not due to chance
2. Test framework scalability by evaluating on scenarios with 3-5 agents and measure performance degradation points
3. Implement communication failure simulations (packet loss, latency) to assess robustness of the knowledge evolution mechanism under realistic conditions