---
ver: rpa2
title: Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering
  of Black-Box Systems
arxiv_id: '2505.17968'
source_url: https://arxiv.org/abs/2505.17968
tags:
- black-box
- language
- black
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how well large language models (LLMs)\
  \ can reverse-engineer black-box systems by inferring their underlying structure\
  \ from observed behavior. The authors explore three types of black-box systems\u2014\
  list-mapping programs, formal languages, and math equations\u2014and compare LLM\
  \ performance using passive observations versus active interventions (where the\
  \ LLM generates queries to test hypotheses)."
---

# Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems

## Quick Facts
- arXiv ID: 2505.17968
- Source URL: https://arxiv.org/abs/2505.17968
- Authors: Jiayi Geng; Howard Chen; Dilip Arumugam; Thomas L. Griffiths
- Reference count: 40
- Primary result: LLMs perform significantly worse than Bayesian inference models on passive reverse-engineering but substantially improve with active intervention

## Executive Summary
This paper investigates whether large language models can function as reliable AI scientists by assessing their ability to reverse-engineer black-box systems. The authors examine three types of systems - list-mapping programs, formal languages, and math equations - comparing LLM performance under passive observation versus active intervention conditions. Their findings reveal that while LLMs struggle with passive inference compared to Bayesian methods, they show substantial improvement when allowed to actively query and test hypotheses. The study identifies specific failure modes (overcomplication and overlooking) that active intervention helps overcome, and demonstrates that the benefits of intervention are model-specific rather than simply improving data quality.

## Method Summary
The researchers evaluate LLM reverse-engineering capabilities across three distinct domains using a controlled experimental framework. They compare passive observation (where models only receive input-output pairs) against active intervention (where models can generate queries to test hypotheses). Performance is benchmarked against Bayesian inference models as a theoretical gold standard. The study systematically identifies failure patterns in LLM reasoning and tests whether intervention-based refinement of beliefs, rather than improved data quality, drives performance gains. The methodology includes cross-model testing to assess whether intervention data generated by one LLM benefits others.

## Key Results
- LLMs significantly underperform Bayesian inference models when using only passive observations across all three domains
- Active intervention substantially improves LLM performance by enabling belief refinement rather than data quality enhancement
- Two primary failure modes identified: overcomplication (overly complex hypotheses) and overlooking (missing observations)
- Intervention data shows model-specific benefits, with one LLM's queries being less useful to another

## Why This Works (Mechanism)
The improvement from active intervention stems from LLMs refining their own beliefs through hypothesis testing rather than collecting better data. When LLMs can query the black-box system, they generate targeted tests that help them eliminate incorrect hypotheses and converge on the correct underlying rules. This active learning approach allows models to overcome two key failure modes: overcomplication (where they generate unnecessarily complex explanations) and overlooking (where they miss critical observations). The mechanism works because the act of generating and testing queries forces the LLM to engage in a more rigorous scientific reasoning process, similar to how human scientists design experiments to test competing hypotheses.

## Foundational Learning
- **Bayesian inference fundamentals**: Why needed - serves as performance baseline and theoretical gold standard for hypothesis testing; Quick check - verify understanding of prior/posterior probability updates
- **Active learning principles**: Why needed - explains the mechanism behind intervention benefits; Quick check - confirm grasp of exploration vs exploitation tradeoff
- **Hypothesis space modeling**: Why needed - crucial for understanding LLM failure modes; Quick check - assess ability to enumerate possible explanations for observed data
- **Scientific method application to AI**: Why needed - frames the reverse-engineering task as scientific discovery; Quick check - evaluate understanding of experimental design principles
- **Model-specific behavior patterns**: Why needed - explains why intervention benefits don't transfer between models; Quick check - test ability to characterize different LLM reasoning patterns

## Architecture Onboarding

**Component Map**: Observation Data -> Hypothesis Generation -> Query Generation -> Black-Box Testing -> Belief Update -> Final Inference

**Critical Path**: The sequence from hypothesis generation through query generation to belief update represents the most crucial pathway, as this is where active intervention creates value through targeted testing and refinement.

**Design Tradeoffs**: The study trades off between passive efficiency (less computation, no query generation) and active accuracy (more computation, targeted hypothesis testing). The choice of three diverse domains balances breadth of applicability against depth of analysis in any single domain.

**Failure Signatures**: Overcomplication manifests as unnecessarily complex hypotheses that fit the data but miss simpler underlying rules. Overlooking appears as failure to incorporate all available observations into the final inference, often due to premature convergence on an incorrect hypothesis.

**3 First Experiments**:
1. Test passive observation performance across different model sizes to establish baseline scaling
2. Implement ablation studies removing query generation to isolate belief refinement effects
3. Cross-test intervention data between multiple LLM families to quantify model-specific benefits

## Open Questions the Paper Calls Out
None specified in the abstract.

## Limitations
- Comparison to Bayesian inference may not represent realistic scientific discovery processes
- Results may not generalize to larger or different LLM architectures beyond those tested
- Model-specific intervention benefits suggest limited transferability of learned strategies

## Confidence
- LLM vs Bayesian baseline performance: High confidence
- Active intervention benefits: Medium confidence
- Model-specific intervention benefits: Low confidence

## Next Checks
1. Verify the experimental protocol for controlling belief refinement versus data quality effects, ensuring proper ablation studies
2. Test whether results generalize across different LLM architectures (not just one model family) and parameter scales
3. Examine whether the Bayesian inference baseline appropriately represents a realistic scientific discovery process versus idealized reasoning