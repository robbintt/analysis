---
ver: rpa2
title: 'REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into
  Style and Substance'
arxiv_id: '2511.20233'
source_url: https://arxiv.org/abs/2511.20233
tags:
- arxiv
- explanation
- fact-checking
- style
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REFLEX is a self-refining fact-checking framework that disentangles
  truth into style and substance using contrastive activation pairs. It improves verdict
  accuracy and explanation quality by steering model internal knowledge without external
  retrieval, reducing hallucinations and latency.
---

# REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance

## Quick Facts
- arXiv ID: 2511.20233
- Source URL: https://arxiv.org/abs/2511.20233
- Reference count: 40
- Self-refining fact-checking framework that disentangles truth into style and substance, achieving state-of-the-art performance with only 465 self-refined samples

## Executive Summary
REFLEX presents a novel self-refining fact-checking framework that disentangles truth into style and substance using contrastive activation pairs. The system improves both verdict accuracy and explanation quality by steering model internal knowledge without external retrieval, effectively reducing hallucinations and latency. By training explanatory models to guide non-explanatory ones, REFLEX achieves significant performance improvements of up to 7.57% in verdict accuracy. The framework demonstrates particular strength on human-unknown truths, with largest gains observed in middle-layer activations where factual content is effectively separated from noisy explanation styles.

## Method Summary
REFLEX operates by disentangling truth into two components: style (explanation characteristics) and substance (factual content). The framework uses contrastive activation pairs to identify and separate these elements within model activations. Through self-refinement, the system iteratively improves its fact-checking capabilities by learning from its own outputs. A key innovation is using explanatory models to guide non-explanatory models, enabling knowledge transfer without requiring extensive human-labeled data. The approach avoids external retrieval, instead steering internal model knowledge to maintain accuracy while reducing hallucinations and latency.

## Key Results
- Achieves state-of-the-art performance on real-world datasets with only 465 self-refined samples
- Surpasses methods using external APIs in both verdict accuracy and explanation quality
- Models trained with explanatory objectives can improve non-explanatory models by up to 7.57% in verdict accuracy
- Excels particularly on human-unknown truths, showing largest performance gains in middle-layer activations

## Why This Works (Mechanism)
REFLEX works by leveraging contrastive activation pairs to disentangle truth into style and substance components within model activations. This disentanglement allows the system to separate factual content from explanation styles, enabling more accurate fact-checking while maintaining high-quality explanations. The self-refining process iteratively improves performance by learning from model outputs, while the use of explanatory models to guide non-explanatory ones creates a knowledge transfer mechanism that reduces dependency on extensive human-labeled data. The approach effectively reduces hallucinations by steering internal model knowledge rather than relying on external retrieval.

## Foundational Learning
- **Contrastive Activation Pairs**: Why needed - To identify and separate style from substance in model activations; Quick check - Verify activation patterns differ meaningfully between factual and non-factual content
- **Self-Refinement Process**: Why needed - To iteratively improve performance without additional labeled data; Quick check - Monitor convergence and stability across refinement iterations
- **Knowledge Transfer from Explanatory to Non-Explanatory Models**: Why needed - To leverage explanatory capabilities for improving verdict accuracy; Quick check - Measure performance gains when applying explanatory guidance to non-explanatory models
- **Middle-Layer Activation Analysis**: Why needed - To identify where factual content disentanglement occurs most effectively; Quick check - Compare performance gains across different activation layers
- **Latent Space Steering**: Why needed - To guide model behavior without external retrieval; Quick check - Validate reduced hallucination rates compared to retrieval-based methods

## Architecture Onboarding

Component Map: Input Claims -> Contrastive Activation Analysis -> Style-Substance Disentanglement -> Self-Refinement Module -> Verdict and Explanation Output

Critical Path: The critical path involves processing input claims through contrastive activation analysis to identify style-substance pairs, applying disentanglement to separate factual content from explanation styles, then using self-refinement to iteratively improve accuracy before generating final verdicts and explanations.

Design Tradeoffs: REFLEX trades computational complexity for improved accuracy and reduced hallucinations by avoiding external retrieval. The framework prioritizes self-sufficiency over real-time data access, accepting potential limitations in verifying rapidly evolving information in exchange for consistent performance and reduced latency.

Failure Signatures: Potential failures include overfitting to training data styles, inability to handle novel fact patterns not represented in contrastive pairs, and degradation of explanation quality when disentanglement is imperfect. The system may also struggle with facts that require current information beyond its training scope.

3 First Experiments:
1. Compare performance of REFLEX against baseline fact-checking models on human-unknown truths to validate claims about specialized capability
2. Test the effectiveness of knowledge transfer by measuring accuracy improvements when explanatory models guide non-explanatory ones
3. Analyze activation patterns across different layers to identify optimal points for style-substance disentanglement

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Reliance on contrastive activation pairs may limit generalizability across diverse fact-checking domains
- Self-refinement approach may not effectively handle rapidly evolving real-world information beyond training data
- Performance claims based on relatively small sample size (465 self-refined samples) may not generalize to broader applications

## Confidence
- Verdict accuracy improvements: Medium
- State-of-the-art performance: Medium
- Reduction in hallucinations and latency: High
- Effectiveness on human-unknown truths: Low

## Next Checks
1. Conduct experiments with larger sample sizes and diverse fact-checking domains to assess generalizability and robustness of the approach
2. Perform longitudinal studies to evaluate the long-term stability and adaptability of REFLEX in dynamic information environments
3. Implement bias detection and mitigation strategies to ensure the self-refining process does not introduce or reinforce existing biases in the fact-checking outcomes