---
ver: rpa2
title: 'AD-GPT: Large Language Models in Alzheimer''s Disease'
arxiv_id: '2504.03071'
source_url: https://arxiv.org/abs/2504.03071
tags:
- ad-gpt
- task
- tasks
- gene
- llama3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AD-GPT is a domain-specific LLM for Alzheimer''s disease research
  that combines Llama3 and BERT models in a stacked architecture to perform four key
  tasks: genetic information retrieval, gene-brain region relationship assessment,
  gene-AD relationship analysis, and brain region-AD relationship mapping. The system
  integrates curated biomedical data from sources like OMIM and GTEx, achieving superior
  performance compared to state-of-the-art LLMs across all tasks.'
---

# AD-GPT: Large Language Models in Alzheimer's Disease

## Quick Facts
- arXiv ID: 2504.03071
- Source URL: https://arxiv.org/abs/2504.03071
- Reference count: 34
- AD-GPT achieves 90.84% accuracy for genetic information retrieval and 100% precision/recall/F1-score for gene-brain region classification

## Executive Summary
AD-GPT is a domain-specific large language model for Alzheimer's disease research that combines Llama3 and BERT models in a stacked architecture. The system performs four key tasks: genetic information retrieval, gene-brain region relationship assessment, gene-AD relationship analysis, and brain region-AD relationship mapping. By integrating curated biomedical data from sources like OMIM and GTEx, AD-GPT achieves superior performance compared to state-of-the-art LLMs while avoiding fabricated references through its constrained training corpus.

## Method Summary
AD-GPT uses a stacked architecture with a BERT classifier routing queries to task-specific models: BERT for binary gene-brain region classification and Llama3.1-8B with QLoRA fine-tuning for generative tasks. The model is trained on curated biomedical corpora including OMIM molecular genetics and GTEx QTL data, with 4-bit quantization and LoRA adapters targeting grouped-query attention and feedforward layers. The system achieves efficient fine-tuning while preserving pre-trained knowledge and provides a Docker-based deployment with GUI for practical use.

## Key Results
- 90.84% accuracy for genetic information retrieval task
- 100% precision/recall/F1-score for gene-brain region classification
- Expert-rated precision scores of 4.70/4.92 and relevance scores of 4.60/4.90 for reasoning tasks
- Successfully avoids fabricated references by constraining training to verified sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A stacked BERT+Llama architecture enables task-specific routing for multi-task biomedical QA
- Mechanism: BERT classifier deterministically categorizes incoming queries by task type, routing to either BERT binary classifier or Llama3-based generative models. This MoE-inspired design matches model capabilities to task requirements.
- Core assumption: Query intent can be reliably classified before task execution, and task boundaries are sufficiently distinct.
- Evidence anchors: [abstract] "stacked LLM architecture combining Llama3 and BERT, optimized for four critical tasks"; [section 4.4] "AD-GPT utilizes a classification-driven architecture... BERT, which first identifies the category of the user's query and subsequently routes it to the relevant expert model"

### Mechanism 2
- Claim: QLoRA fine-tuning on curated corpora transfers domain knowledge efficiently
- Mechanism: 4-bit quantized LoRA adapts grouped-query attention and feed-forward layers using structured instruction-response pairs derived from OMIM and GTEx data.
- Core assumption: Domain knowledge is captured in structured corpora; subword tokenization suffices for gene/brain region terminology.
- Evidence anchors: [section 4.5] "QLoRA was applied to fine-tune Llama3.1-8B models... low-rank adaptation dimension (r=64), scaling factor (lora_alpha=16), dropout=0.1"; [results] "LoRA fine-tuning dramatically shifted the distribution of precision scores... Cohen's d=1.45 for Task 3, d=1.84 for Task 4"

### Mechanism 3
- Claim: Curated static knowledge base reduces hallucination compared to general-purpose LLMs
- Mechanism: Training exclusively on validated sources (OMIM, GTEx) constrains output space to verifiable information, preventing reference fabrication.
- Core assumption: Hallucination stems primarily from broad training distribution; constraining source material limits fabrication.
- Evidence anchors: [results] "AD-GPT provided more decisive responses and produced more accurate references... our model did not generate fictitious references"; [discussion] "competing LLMs... frequently produced fabricated references"

## Foundational Learning

- Concept: **Quantized Low-Rank Adaptation (QLoRA)**
  - Why needed here: Enables efficient fine-tuning of 8B parameter models on resource-constrained hardware while preserving pre-trained knowledge.
  - Quick check question: Can you explain why LoRA adds adapters to attention layers rather than fine-tuning all weights?

- Concept: **cis-QTL (expression/splicing quantitative trait loci)**
  - Why needed here: Core data type linking genetic variants to gene expression/splicing in specific brain regions—foundational for Tasks 2 and 4.
  - Quick check question: What does it mean when a gene-variant pair shows significant eQTL in hippocampus but not cerebellum?

- Concept: **Chain-of-Thought (CoT) prompting in training corpora**
  - Why needed here: Task 4 explicitly embeds 3-step reasoning chains in training data to improve relational inference.
  - Quick check question: How does embedding reasoning steps in training data differ from CoT prompting at inference time?

## Architecture Onboarding

- Component map: BERT tokenizer → BERT classifier (12 transformer layers) → Routes to BERT Task 2 Model or Llama3.1-8B Tasks 1/3/4 → FastAPI Backend → Docker Container → HTML GUI

- Critical path: 1. Query → BERT tokenizer → Task classification (softmax over 4 task types) 2. Route to appropriate model → Generate/classify response 3. Return structured output with source grounding

- Design tradeoffs:
  - Static knowledge base vs. RAG: Current design prioritizes reliability/verifiability over temporal coverage; RAG planned for future
  - Separate task models vs. unified MoE: Current approach uses discrete routing; MoE would enable parameter sharing
  - 144 seed genes only: Constrains breadth but ensures quality; extensibility requires re-curation

- Failure signatures:
  - Low-confidence task classification → misrouted queries, wrong output format
  - Query phrasing not seen in training → degraded accuracy (observed in Llama3.2-1B for ambiguous inputs)
  - Gene/brain region not in seed set → "I don't have information" or hallucination risk

- First 3 experiments:
  1. Ablate task classifier: Replace BERT classifier with random routing to quantify routing contribution to overall performance
  2. Expand seed gene set: Add candidate genes beyond 144 to test corpus scalability and identify coverage gaps
  3. Stress test hallucination: Query AD-GPT vs. ChatGPT-o1 on 20 recent (post-training-cutoff) AD publications to characterize temporal knowledge boundary

## Open Questions the Paper Calls Out

- **Open Question 1**: How does integrating Retrieval-Augmented Generation (RAG) improve AD-GPT's ability to access up-to-date medical knowledge compared to its static training corpus?
  - Basis in paper: [explicit] The authors state they plan to incorporate RAG to address the limitations of the "static nature" of the training corpus and allow for "real-time knowledge integration."
  - Why unresolved: The current model relies on a fixed database, limiting its ability to access new research without retraining.
  - What evidence would resolve it: Performance benchmarks comparing the current model against a RAG-enhanced version on queries involving recently published AD studies.

- **Open Question 2**: Can Mixture-of-Experts (MoE) architectures and reinforcement learning (RL) enhance the model's precision and computational efficiency more effectively than the current stacked BERT-Llama architecture?
  - Basis in paper: [explicit] The authors propose integrating MoE and RL (specifically GRPO) to refine decision-making and resource allocation, contrasting with the current fixed model routing.
  - Why unresolved: The current architecture uses a deterministic BERT classifier for routing, which may lack the dynamic adaptability of MoE systems.
  - What evidence would resolve it: Comparative analysis of resource usage (latency/storage) and reasoning precision between the current stacked model and a MoE-RL implementation.

- **Open Question 3**: How does expanding the dataset beyond the 144 seed genes affect the model's robustness and its ability to identify novel gene-brain region associations?
  - Basis in paper: [inferred] The methods section notes the reliance on 144 seed genes and suggests "additional candidate genes could be integrated in future investigations."
  - Why unresolved: The current validation is limited to a specific, curated set of known genes, leaving the model's scalability and generalizability to novel or less-established genetic factors unproven.
  - What evidence would resolve it: Testing the fine-tuned model on a wider corpus of uncurated or newly discovered candidate genes to assess accuracy and hallucination rates.

## Limitations
- Corpus scope limited to 144 seed genes and 13 brain regions from GTEx, potentially affecting generalizability to rare variants
- Expert evaluation methodology lacks details on inter-rater reliability and number of evaluators beyond "three"
- Temporal knowledge boundary prevents answering questions about research published after training cutoff

## Confidence
- **High Confidence**: Task 2 binary classification performance (100% precision/recall/F1) due to objective metrics
- **Medium Confidence**: Task 1 genetic information retrieval (90.84% accuracy) based on held-out test set
- **Low-Medium Confidence**: Tasks 3-4 expert-rated results (4.70-4.92 scores) due to subjective evaluation methodology

## Next Checks
1. Temporal Knowledge Test: Evaluate AD-GPT against 20 recent (2023-2024) AD publications using identical prompts to expert-rated tasks. Compare performance to GPT-4o/Claude-3.5-Sonnet to quantify the static knowledge limitation and hallucination frequency on out-of-distribution queries.

2. Corpus Expansion Stress Test: Add 50 additional AD-associated genes (including rare variants) to the seed set and retrain Task 2 classifier. Measure classification accuracy drop to quantify the 144-gene limitation and identify coverage gaps for future corpus development.

3. Classifier Ablation Study: Replace the BERT task classifier with random routing (20% accuracy baseline) and rerun all tasks. Compare overall system performance to quantify the routing architecture's contribution versus individual model capabilities.