---
ver: rpa2
title: 'LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding'
arxiv_id: '2510.16783'
source_url: https://arxiv.org/abs/2510.16783
tags:
- question
- answer
- entity
- recall
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LC-Eval, a bilingual multi-task benchmark\
  \ for evaluating long-context understanding in English and Arabic. It features four\
  \ novel tasks\u2014multi-document QA, bilingual QA, claim verification, and MCQs\u2014\
  spanning context lengths from 4K to over 128K tokens."
---

# LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding

## Quick Facts
- arXiv ID: 2510.16783
- Source URL: https://arxiv.org/abs/2510.16783
- Reference count: 40
- Key outcome: Introduces LC-Eval, a bilingual benchmark revealing significant performance gaps in long-context understanding across English/Arabic, especially for open-weight models

## Executive Summary
This paper introduces LC-Eval, a bilingual multi-task benchmark for evaluating long-context understanding in English and Arabic. It features four novel tasks—multi-document QA, bilingual QA, claim verification, and MCQs—spanning context lengths from 4K to over 128K tokens. A custom entity-relationship-based evaluation method is proposed to better assess semantic understanding in open-ended responses. Experiments on open-weight and closed LLMs (including GPT-4o, Llama-3, Qwen2.5, etc.) show significant performance gaps, especially in Arabic, with entity recall and accuracy metrics highlighting limitations in deep reasoning and information tracing. Results reveal that even top models struggle, emphasizing the benchmark's difficulty and the need for improved multilingual long-context capabilities.

## Method Summary
LC-Eval is a comprehensive benchmark featuring 7,903 samples across four tasks: multi-document QA, bilingual QA, claim verification, and MCQs. The benchmark covers context lengths from 4K to over 128K tokens in both English and Arabic. GPT-4o generates questions and answers from curated corpora (Wikipedia, Hindawi, Gutenberg, etc.), with quality filtering via temperature sampling and human validation. The primary evaluation metric is entity-relationship recall, which uses GPT-4o to extract semantic relationships from both gold and model responses. Additional metrics include ROUGE-L, BLEU, Recall@k for source tracing, and accuracy for classification tasks. The benchmark is designed to stress-test deep reasoning and information retrieval capabilities rather than simple retrieval.

## Key Results
- Open-weight models (Llama-3.1-8B, Qwen2.5-14B) significantly underperform closed models (GPT-4o, Claude-3.5-Sonnet) across all tasks and languages
- Arabic consistently yields lower scores than English, with a 10-20 point accuracy gap, highlighting multilingual capability limitations
- Performance degrades substantially as context length increases, with standard deviation across length bins indicating inconsistent behavior at longer contexts
- Entity-relationship recall correlates strongly with human judgment (0.77-0.94), outperforming traditional lexical metrics in semantic evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity-relationship recall captures semantic meaning better than lexical overlap metrics for evaluating open-ended QA.
- Mechanism: GPT-4o extracts entity-relationship triples from both gold-standard and model responses, then computes overlap based on conceptual similarity rather than word matching. This allows semantically equivalent but differently phrased answers to receive credit.
- Core assumption: The judge LLM (GPT-4o) can reliably identify semantic equivalence and extract meaningful entity relationships across both English and Arabic.
- Evidence anchors:
  - [abstract] "The proposed entity-relationship recall metric outperformed traditional BLEU/ROUGE by considering semantic meaning."
  - [section 5.2.1] "We prompt GPT-4o to identify relationships based on conceptual meaning rather than lexical similarity."
  - [corpus] Weak direct corpus support; related benchmarks (LongBench Pro, ScholarBench) use alternative evaluation approaches but don't validate this specific mechanism.
- Break condition: If the judge LLM exhibits systematic biases toward certain phrasings or fails on Arabic entity extraction, recall scores become unreliable.

### Mechanism 2
- Claim: Multi-document QA with curated distractor documents exposes information tracing failures in LCLMs.
- Mechanism: Similar documents (identified via MinHash) serve as distractors. Models must identify correct source documents AND synthesize answers from them. The Recall@k metric specifically measures source identification independently from answer quality.
- Core assumption: Models that generate correct answers should also cite correct sources; dissociation indicates guessing or parametric knowledge rather than context comprehension.
- Evidence anchors:
  - [section 2.1] "some [documents] serve as distractors, closely resembling the relevant documents"
  - [section 6.1] "Command-r-plus... failed to retrieve the correct document IDs... showing its limitation to accurately trace the source"
  - [corpus] LoCoBench similarly uses complex multi-file scenarios to stress-test context utilization.
- Break condition: If distractors are too similar or too dissimilar, the task becomes either impossibly hard or trivially easy, failing to discriminate model capabilities.

### Mechanism 3
- Claim: Performance degradation with increasing context length reveals attention/retention bottlenecks in LCLMs.
- Mechanism: As context extends from 4k to 128k+ tokens, models must maintain attention over larger sequences. The benchmark samples uniformly across length bins to isolate length effects from task difficulty.
- Core assumption: Performance drops are attributable to context length rather than document complexity or question difficulty (partially controlled via stratified sampling).
- Evidence anchors:
  - [section 6.1] "significant performance decline observed in most models as the number of words increases... standard deviation for recall across word count bins are typically much higher"
  - [section 6.6] Context-provided vs. no-context comparison shows 20-point gap, increasing for Arabic
  - [corpus] SagaScale and LongReason similarly observe length-dependent degradation patterns.
- Break condition: If degradation stems from confounding factors (e.g., longer documents are inherently more complex), length-specific interventions won't improve performance.

## Foundational Learning

- Concept: Token fertility rate (tokens per word varies by tokenizer and language)
  - Why needed here: Arabic has ~2x higher fertility than English across all tested tokenizers, meaning "128k tokens" represents different word counts per language.
  - Quick check question: If a model claims 128k context length, how many Arabic words can it actually process?

- Concept: Needle-in-a-haystack (NIAH) vs. realistic tasks
  - Why needed here: NIAH tasks test retrieval but lack reasoning depth; LC-Eval explicitly targets "deep reasoning" through multi-hop synthesis and claim verification.
  - Quick check question: Why might a model pass NIAH but fail multi-document QA?

- Concept: LLM-as-a-judge evaluation paradigm
  - Why needed here: Traditional metrics (BLEU/ROUGE) fail on semantic equivalence; entity-relationship recall uses an LLM to judge conceptual overlap.
  - Quick check question: What biases might the judge LLM introduce, and how would you detect them?

## Architecture Onboarding

- Component map:
  Data curation pipeline: Source corpora (Wikipedia, WikiBooks, Hindawi, Gutenberg) → GPT-4o generation → Multi-temperature quality filtering → Human validation
  Task-specific modules: Multi-doc QA (MinHash similarity + distractor sampling), Bilingual QA (cross-lingual chunking), Claim Verification (summary → claim generation), MCQ (summary → distractor generation)
  Evaluation layer: Entity-relationship extraction → Recall/F1 computation; Recall@k for source tracing; Accuracy for classification tasks

- Critical path:
  1. Document selection and chunking (context length targeting)
  2. Question-answer generation with quality filtering
  3. Human validation (majority vote threshold)
  4. Model inference across length bins
  5. Entity-relationship extraction and metric computation

- Design tradeoffs:
  - GPT-4o generation introduces potential bias (acknowledged limitation: GPT-4o may score higher)
  - Multi-temperature filtering improves quality but increases cost
  - Entity-relationship recall captures semantics but doesn't penalize repetition
  - Arabic underrepresentation in training data yields systematically lower scores (feature, not bug)

- Failure signatures:
  - High entity recall + low Recall@k: Model generates plausible answers without using provided context
  - High paragraph accuracy + low sentence accuracy: Model uses context but can't isolate specific claims
  - Sudden accuracy drops at specific length bins: Potential attention mechanism failures
  - Language accuracy <100% in bilingual QA: Cross-lingual instruction following failures

- First 3 experiments:
  1. Run baseline models on all four tasks across length bins; verify performance degradation pattern and Arabic-English gap replicates
  2. Ablate entity-relationship recall vs. ROUGE-L on a held-out sample; confirm correlation (reported: 0.77-0.94) and identify systematic discrepancies
  3. Test memorization hypothesis: Compare performance with/without context for subset; verify context-utilization gap (reported: 20 points) exists across models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation metrics for long-context generation be refined to penalize repetitive or verbose outputs without sacrificing the measurement of semantic correctness?
- Basis in paper: [inferred] Section 8 (Limitations) notes that the proposed entity-relationship recall metric does not penalize repetition, allowing models to achieve high scores simply by repeating generated tokens.
- Why unresolved: Current semantic matching methods focus on overlap but lack mechanisms to assess the conciseness or non-redundancy of the generated response.
- What evidence would resolve it: A new metric formulation that integrates a repetition penalty factor while maintaining high correlation with human judgment on semantic accuracy.

### Open Question 2
- Question: Does the synthetic generation of benchmark data using GPT-4o introduce an intrinsic bias that favors the generating model over open-weight competitors?
- Basis in paper: [inferred] Section 8 (Limitations) concedes that because the dataset was created by GPT-4o, it may result in higher evaluation scores for that model, potentially introducing a bias.
- Why unresolved: The paper cannot determine if GPT-4o's superior performance is due to better reasoning capabilities or familiarity with the specific stylistic patterns of its own synthetic outputs.
- What evidence would resolve it: A comparative evaluation using a benchmark constructed entirely by human annotators or by a diverse set of alternative frontier models to see if the performance gap persists.

### Open Question 3
- Question: Is the significant performance degradation in Arabic long-context tasks primarily driven by tokenizer fertility rates (context dilution) or a lack of Arabic-specific training data?
- Basis in paper: [explicit] Section 7 (Conclusion) highlights that "all LCLMs performed better in English than in Arabic," and Table 2 shows Arabic token fertility is roughly 2x that of English for the same models.
- Why unresolved: While the paper identifies the performance gap and the difference in token efficiency, it does not isolate the causal factor (e.g., whether the doubled token count for Arabic texts overwhelms the attention window).
- What evidence would resolve it: A controlled ablation study evaluating models on Arabic text processed with various tokenizers to measure the impact of context length versus the model's inherent multilingual reasoning capacity.

## Limitations
- LLM-as-judge methodology introduces potential systematic biases, particularly for Arabic where GPT-4o may have weaker language understanding
- Arabic subset is substantially smaller (662 vs. 7,241 English samples) and may have fewer multi-document QA examples, potentially amplifying observed performance gaps
- Context length measurement complexity due to token fertility rates means "128K tokens" represents different word counts for English vs. Arabic, complicating interpretation of length-dependent performance

## Confidence
- **High confidence**: Performance degradation with increasing context length, and the general superiority of closed models (GPT-4o, Claude-3.5-Sonnet) over open models across all tasks
- **Medium confidence**: The claim that entity-relationship recall better captures semantic understanding than traditional metrics, though methodology could introduce systematic errors
- **Low confidence**: The assertion that LC-Eval represents the "most comprehensive" long-context benchmark, as the definition of "comprehensive" is not rigorously established

## Next Checks
1. **Ablation study on judge LLM bias**: Run a subset of LC-Eval samples through multiple judge LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, Llama-3.1-8B-Instruct) for entity-relationship recall scoring. Compare score distributions and identify systematic biases or language-specific discrepancies.
2. **Cross-lingual equivalence verification**: Generate parallel English-Arabic samples where the same semantic content is evaluated. Compare entity-relationship recall scores across languages for semantically equivalent responses to determine if the evaluation methodology introduces language-dependent biases.
3. **Length bin word count analysis**: For each context length bin (4K, 16K, 32K, 128K+ tokens), compute the actual word count distributions for both English and Arabic samples using the specified tokenizers. Analyze whether performance degradation correlates with word count rather than token count, and whether the Arabic-English gap persists when controlling for actual word numbers.