---
ver: rpa2
title: Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language
  Alignment Attack
arxiv_id: '2502.19672'
source_url: https://arxiv.org/abs/2502.19672
tags:
- image
- adversarial
- would
- what
- vision-language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving adversarial transferability
  in multimodal large language models (MLLMs), where attacks often fail to generalize
  across different models. The proposed Dynamic Vision-Language Alignment (DynVLA)
  Attack injects dynamic perturbations into the vision-language connector, using Gaussian
  kernels to shift attention across image regions.
---

# Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack

## Quick Facts
- arXiv ID: 2502.19672
- Source URL: https://arxiv.org/abs/2502.19672
- Reference count: 40
- Key outcome: Proposed method significantly improves adversarial transferability across MLLMs, achieving >70% attack success rate on BLIP2 and enhancing cross-model robustness.

## Executive Summary
This paper tackles the challenge of improving adversarial transferability in multimodal large language models (MLLMs), where existing attacks often fail to generalize across different models. The proposed Dynamic Vision-Language Alignment (DynVLA) Attack introduces dynamic perturbations into the vision-language connector using Gaussian kernels, which shift attention across image regions and encourage diverse modality alignments during attack generation. Experiments demonstrate that DynVLA outperforms baseline methods, achieving high attack success rates across various MLLMs including both open-source and closed-source models.

## Method Summary
DynVLA improves adversarial transferability by dynamically perturbing the vision-language alignment rather than relying on traditional pixel-level transformations. The method injects perturbations into the visual backbone output before it is processed by the vision-language connector, using Gaussian kernels to shift attention across different image regions. This dynamic alignment encourages diverse modality alignments during attack generation, making the adversarial examples more transferable across different MLLM architectures. The approach is designed to overcome the limited transferability of existing methods by focusing on the interaction between vision and language modalities rather than individual image features.

## Key Results
- Achieves attack success rates exceeding 70% on BLIP2
- Improves transferability across multiple MLLMs including InstructBLIP, MiniGPT4, LLaVA, and Gemini
- Outperforms baseline methods through dynamic vision-language alignment perturbations

## Why This Works (Mechanism)
The effectiveness of DynVLA stems from its focus on perturbing the vision-language alignment rather than individual image features. By dynamically shifting attention across image regions using Gaussian kernels, the method creates adversarial examples that are less dependent on specific model architectures and more focused on the fundamental interaction between visual and language modalities. This approach addresses the core challenge that MLLMs learn different feature representations, making traditional pixel-based attacks less effective across models.

## Foundational Learning
- **Gaussian kernels for attention shifting**: Why needed - to dynamically perturb attention across image regions; Quick check - verify kernel parameters affect perturbation diversity
- **Vision-language connector perturbation**: Why needed - targets the modality interaction rather than individual features; Quick check - confirm perturbations affect connector output
- **Dynamic alignment vs static perturbations**: Why needed - static methods fail across diverse MLLM architectures; Quick check - compare dynamic vs static attack transferability
- **Cross-model feature representation differences**: Why needed - understanding why traditional attacks fail; Quick check - analyze feature space divergence across models

## Architecture Onboarding
- **Component map**: Input image -> Visual backbone -> Gaussian kernel perturbation -> Vision-language connector -> MLLM output
- **Critical path**: Visual features → Gaussian perturbation → Vision-language connector → Final classification
- **Design tradeoffs**: Dynamic perturbations increase transferability but add computational overhead compared to static attacks
- **Failure signatures**: Attacks fail when Gaussian kernel parameters are poorly tuned or when models have highly specialized vision-language connectors
- **3 first experiments**: 1) Ablation study removing Gaussian kernels, 2) Testing different kernel sizes and variances, 3) Comparing dynamic vs static perturbation effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger, more diverse datasets beyond tested benchmarks remains uncertain
- Performance in real-world, unconstrained scenarios and against defensive strategies is unclear
- Computational overhead may limit practical deployment in resource-constrained environments

## Confidence
- **High Confidence**: Method's effectiveness in improving transferability across tested MLLMs and superiority over baseline methods
- **Medium Confidence**: Generalizability to larger datasets and diverse MLLM architectures
- **Low Confidence**: Robustness against defensive strategies and practical deployment in real-world scenarios

## Next Checks
1. Test scalability and robustness on larger, more diverse datasets and MLLM architectures not covered in initial experiments
2. Investigate underlying mechanisms of how Gaussian kernel perturbations influence cross-model transferability
3. Evaluate performance in presence of defensive strategies and assess computational overhead in resource-constrained environments