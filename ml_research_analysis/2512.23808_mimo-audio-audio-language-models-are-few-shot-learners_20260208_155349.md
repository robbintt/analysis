---
ver: rpa2
title: 'MiMo-Audio: Audio Language Models are Few-Shot Learners'
arxiv_id: '2512.23808'
source_url: https://arxiv.org/abs/2512.23808
tags:
- speech
- audio
- text
- data
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiMo-Audio, a new audio language model that
  demonstrates few-shot learning capabilities across a variety of audio tasks. The
  authors scale pretraining data to over 100 million hours, enabling the model to
  generalize to unseen tasks like voice conversion and style transfer without task-specific
  fine-tuning.
---

# MiMo-Audio: Audio Language Models are Few-Shot Learners

## Quick Facts
- **arXiv ID**: 2512.23808
- **Source URL**: https://arxiv.org/abs/2512.23808
- **Reference count**: 24
- **Primary result**: MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks (MMSU, MMAU, MMAR, MMAU-Pro), spoken dialogue benchmarks (Big Bench Audio, MultiChallenge Audio), and instruct-TTS evaluations, approaching or surpassing closed-source models.

## Executive Summary
MiMo-Audio introduces a unified audio language model demonstrating emergent few-shot learning capabilities across diverse audio tasks. The model scales pretraining to over 100 million hours, enabling generalization to unseen tasks like voice conversion and style transfer without task-specific fine-tuning. MiMo-Audio-7B-Base achieves state-of-the-art performance on speech intelligence benchmarks among open-source models, while the Instruct version reaches open-source SOTA on multiple audio understanding and spoken dialogue benchmarks.

## Method Summary
The approach involves training a unified audio tokenizer using reconstruction and Audio-to-Text objectives, followed by two-stage pre-training of a base LLM. Stage 1 focuses on speech understanding with loss computed only on text tokens, while Stage 2 enables joint text/audio generation using interleaved sequences. The architecture employs patch encoding/decoding to efficiently process audio tokens within the LLM framework, with a critical delay pattern for parallel RVQ generation.

## Key Results
- MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models
- MiMo-Audio-7B-Instruct reaches open-source SOTA on multiple audio understanding benchmarks (MMSU, MMAU, MMAR, MMAU-Pro)
- The model approaches or surpasses closed-source models on instruct-TTS evaluations and spoken dialogue benchmarks (Big Bench Audio, MultiChallenge Audio)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Few-shot learning for audio tasks emerges non-linearly after pretraining data scales past a critical threshold (~0.7 trillion tokens).
- **Mechanism**: The model acts as a "lossless compressor" over 100M+ hours of audio via next-token prediction, learning generalizable atomic speech skills that are recombined via in-context examples.
- **Core assumption**: Learned skills are composable primitives that map reliably to task specifications in prompts.
- **Evidence anchors**: Sharp performance surge observed after surpassing ~0.7T tokens; "phase transition" in few-shot capabilities.
- **Break condition**: If emergent tasks share latent properties with pretraining distribution, generalization claim may overstate novelty.

### Mechanism 2
- **Claim**: A unified tokenizer trained with semantic and acoustic objectives enables stronger cross-modal alignment and generation quality.
- **Mechanism**: The 1.2B-parameter tokenizer uses multi-scale mel-spectrogram reconstruction loss and Audio-to-Text next-token prediction loss, forcing discrete tokens to bridge audio waveform and text embedding space.
- **Core assumption**: Single discrete representation can efficiently encode both linguistic meaning and acoustic details.
- **Evidence anchors**: Superior PESQ (3.30) and Speaker Similarity (0.89) on Seed-TTS-Eval compared to baselines.
- **Break condition**: If tokenizer discards paralinguistic cues not captured by evaluation metrics, "high-fidelity" claim may not hold for expressive tasks.

### Mechanism 3
- **Claim**: Two-stage pre-training preserves text reasoning capabilities while acquiring speech understanding and generation.
- **Mechanism**: Stage 1 trains Patch Encoder and LLM on speech-text mix with text-only loss; Stage 2 adds Patch Decoder and trains on interleaved speech-text with loss on both modalities.
- **Core assumption**: Text prior from base LLM is valuable and can be retained via careful learning rate control.
- **Evidence anchors**: MiMo-Audio's modality gap is 3.4 points vs Step-Audio2's 22.3 points; asymmetric learning rates (3e-5 LLM vs 2e-4 PE/Dec).
- **Break condition**: If small modality gap is achieved by high text-to-speech data ratio, it may limit ultimate speech intelligence.

## Foundational Learning

- **Residual Vector Quantization (RVQ)**: The core audio representation uses multiple RVQ codebooks stacked hierarchically. Understanding this is essential for interpreting model input/output.
  - *Quick check question*: How does increasing RVQ codebook number affect reconstruction quality versus token sequence length?

- **In-Context Learning (Few-Shot)**: The model's primary generalization mode is conditioning on examples within the prompt, defining the inference workflow.
  - *Quick check question*: How does providing input-output audio pairs in context enable the model to perform new tasks like style transfer?

- **Modality Gap**: A key achievement is near-lossless transfer of reasoning skills between text and speech. Understanding this metric is crucial for evaluating cross-modal efficacy.
  - *Quick check question*: If a model has large modality gap, how would its performance on spoken reasoning (S2S) compare to text-only (T2T)?

## Architecture Onboarding

- **Component map**: Raw Audio -> MiMo-Audio-Tokenizer -> RVQ Tokens -> Patch Encoder -> LLM Patches -> MiMo-7B -> LLM Patches -> Patch Decoder -> RVQ Tokens -> MiMo-Audio-Tokenizer (decoder/vocoder) -> Reconstructed Audio

- **Critical path**: Raw Audio → Tokenizer → RVQ Tokens → Patch Encoder → LLM Patches → MiMo-7B → LLM Patches → Patch Decoder → RVQ Tokens → Tokenizer (decoder/vocoder) → Reconstructed Audio

- **Design tradeoffs**:
  - Patching (4x downsampling) improves LLM efficiency but adds encoder/decoder complexity
  - A2T loss improves semantic alignment but may introduce optimization conflicts
  - Two-stage training adds engineering complexity but better preserves text capabilities

- **Failure signatures**:
  - Loss of acoustic detail: Artifacts, roboticness, or missing paralinguistic cues
  - Modality collapse: LLM ignores audio patches and responds based on text priors
  - RVQ divergence: Patch Decoder generates inconsistent or non-decodable token sequences

- **First 3 experiments**:
  1. Tokenizer Reconstruction Baseline: Pass diverse audio samples through tokenizer and back, measuring reconstruction quality (PESQ, speaker similarity)
  2. Modality Gap Ablation: Evaluate Base model on SpeechMMLU (T2T, S2S, S2T, T2S) to directly measure modality gap (reported 3.4 points)
  3. Few-Shot Voice Conversion: Test Base model's in-context learning with 16-shot examples, measuring speaker similarity and semantic preservation

## Open Questions the Paper Calls Out

1. **Can few-shot learning extend to general audio generation tasks involving complex sound events or background music?**
   - Basis: Paper identifies suboptimal performance in "speech generation with background music and processing of complex sound events"
   - Unresolved because current model shows strong speech task generalization but struggles with non-speech audio generation
   - Resolution evidence: Successful evaluation on few-shot benchmarks requiring mixed audio scene generation without task-specific fine-tuning

2. **Can reinforcement learning effectively resolve instabilities such as timbre discontinuities and style control failures in spoken dialogue?**
   - Basis: Paper identifies "timbre discontinuities, unstable audio quality, [and] inconsistent compliance" as limitations
   - Unresolved because supervised post-training doesn't guarantee consistency in acoustic features over long generation sequences
   - Resolution evidence: Post-RL model demonstrating higher consistency scores on multi-turn dialogue benchmarks

3. **How can thinking mechanism be adapted to avoid hallucinations that degrade performance in sound and music understanding tasks?**
   - Basis: Paper notes CoT integration improves speech understanding but causes "performance degradation in sound and music understanding" due to hallucinations
   - Unresolved because reasoning tokens appear misaligned with acoustic features for non-speech modalities
   - Resolution evidence: Modified training strategy where thinking mechanism yields positive or neutral gains on MMAU sound and music subtasks

## Limitations

- Dataset transparency issues: Specific composition, quality control, and domain distribution of 100M+ hours training data remain underspecified
- Tokenizer evaluation scope: Strong performance on reconstruction but limited evaluation of expressive generation quality for emotional TTS or nuanced voice conversion
- Base LLM transfer assumptions: Small modality gap may be specific to MiMo-7B initialization rather than intrinsic to audio training method

## Confidence

- **High Confidence**: Two-stage pre-training methodology and implementation details are well-specified
- **Medium Confidence**: Scaling law observations (critical threshold at ~0.7T tokens) are compelling but based on single training run
- **Low Confidence**: Claims of approaching/surpassing closed-source models are difficult to verify without standardized evaluation conditions

## Next Checks

1. **Dataset ablation study**: Train MiMo-Audio-Base with progressively smaller datasets (10M, 50M, 100M hours) to verify critical threshold for few-shot learning emergence

2. **Expressive generation benchmark**: Evaluate tokenizer and model on tasks requiring emotional expression and paralinguistic control to test "high-fidelity" claim beyond neutral speech

3. **Cross-LLM modality transfer**: Replicate two-stage training pipeline using different base LLM architectures (Llama, Mistral) to determine if small modality gap is intrinsic to method or specific to MiMo-7B initialization