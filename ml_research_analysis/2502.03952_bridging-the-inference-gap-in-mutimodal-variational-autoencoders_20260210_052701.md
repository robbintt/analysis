---
ver: rpa2
title: Bridging the inference gap in Mutimodal Variational Autoencoders
arxiv_id: '2502.03952'
source_url: https://arxiv.org/abs/2502.03952
tags:
- modalities
- joint
- each
- samples
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes JNF, a novel multimodal variational autoencoder
  that bridges the inference gap in mixture-based models by learning both joint and
  conditional distributions without aggregation. The method consists of two steps:
  first training a joint VAE, then using Normalizing Flows to approximate unimodal
  posteriors.'
---

# Bridging the inference gap in Mutimodal Variational Autoencoders

## Quick Facts
- arXiv ID: 2502.03952
- Source URL: https://arxiv.org/abs/2502.03952
- Reference count: 40
- One-line primary result: JNF outperforms state-of-the-art models on coherence and FID metrics by learning joint and conditional distributions without mixture aggregation

## Executive Summary
This paper addresses the generative discrepancy gap in multimodal variational autoencoders (MVAEs) that arises from mixture-based aggregation methods. The authors propose JNF (Joint-Normalizing-Flows), a two-step training approach that first learns a joint generative model, then independently fits unimodal posteriors using Normalizing Flows. This design avoids the limitations of Product-of-Experts (PoE) aggregation during training while maintaining PoE for efficient inference. An improved variant, JNF-Shared, extracts shared semantic information across modalities using Contrastive Learning or DCCA to further simplify conditional posteriors.

## Method Summary
JNF trains in two stages: first, a joint VAE learns the shared latent representation through standard ELBO optimization; second, unimodal Normalizing Flow encoders are trained to approximate the joint posterior independently using KL minimization. This decouples joint and conditional training, avoiding the generative discrepancy gap of mixture-based models. At inference, PoE aggregation is used over the independently trained unimodal posteriors. The JNF-Shared variant pre-trains projectors to extract shared representations, modeling q(z|g_j(x_j)) instead of q(z|x_j) for potentially simpler posterior approximation.

## Key Results
- Achieves 0.51 ± 0.02 joint coherence on MNIST-SVHN, outperforming MMVAE (0.25) and MoPoE (0.29)
- Maintains competitive FID scores across datasets while achieving higher coherence than baselines
- JNF-Shared (CL) shows substantial improvement in cross-modal coherence (0.75 vs 0.52 S→M direction)
- Avoids "averaged" outputs observed in MMVAE/MoPoE baselines on Translated PolyMNIST

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Joint and Conditional Posterior Training
Separating joint generative model training from conditional posterior approximation avoids the generative discrepancy gap inherent in mixture-based aggregation methods. Instead of optimizing a single ELBO with aggregated posteriors, first train joint encoder q_φ(z|X) via ELBO, freeze it, then independently fit unimodal encoders to match it via KL minimization. PoE aggregation is used only at inference time, not during optimization.

### Mechanism 2: Normalizing Flows for Expressive Posterior Approximation
Masked Autoregressive Flows enable more accurate approximation of complex unimodal posteriors than diagonal Gaussian distributions. The paper transforms base Gaussian distributions through invertible transformations, allowing the model to capture complex, potentially multi-modal posterior structure that Gaussians cannot represent.

### Mechanism 3: Shared Information Extraction via Contrastive Learning or DCCA
Conditioning unimodal posteriors on extracted shared semantic content (rather than raw modalities) simplifies the target distribution, improving conditional coherence. The method pre-trains projectors to extract cross-modal shared representations, then models q_φj(z|g_j(x_j)) instead of q_φj(z|x_j), using adapted loss.

## Foundational Learning

- **Variational Inference and the ELBO-KL Relationship**:
  - Why needed here: Understanding why mixture aggregation creates a bounded KL gap requires grasping how ELBO optimization relates to KL divergence minimization between approximate and true posteriors
  - Quick check question: Given Equation 4, explain why a lower bound on KL(q_φ(z|X)||p_θ(z|X)) limits how well the approximate posterior can match the true posterior

- **Normalizing Flows and Change of Variables**:
  - Why needed here: The paper uses flows to transform simple Gaussians into complex distributions; implementing this requires understanding the density transformation formula
  - Quick check question: Given Equation 9, if z' = f(z) where f is invertible, how do you compute log q'(z') from log q(z)?

- **Product-of-Experts vs Mixture-of-Experts Aggregation**:
  - Why needed here: The paper deliberately uses PoE at inference while avoiding MoE in training; understanding the difference clarifies the design rationale
  - Quick check question: Why does using PoE aggregation only at inference time (Equation 12) avoid the Δ(X) generative discrepancy that affects MoE-based training?

## Architecture Onboarding

- **Component map**:
  - Joint encoder network → outputs μ_φ(X), Σ_φ(X) for Gaussian q_φ(z|X)
  - M decoder networks → p_θ(x_j|z) for each modality
  - M Normalizing Flow encoder networks → q_φj(z|x_j) with K MADE blocks each
  - M projector networks (JNF-Shared only) → g_j(x_j) extracting shared representations
  - HMC sampler → samples from subset posteriors using Equation 12 at inference

- **Critical path**:
  1. Step 1: Train joint VAE (encoder + all decoders) with β-weighted ELBO until convergence
  2. Freeze all generative parameters θ and joint encoder φ
  3. Step 2: Train unimodal flow encoders using L_uni loss (Equation 7 or 15) with samples z ~ q_φ(z|X)
  4. Inference: For subset S, sample via HMC from PoE ∏q_φj(z|x_j)/p_θ(z)^|S|-1

- **Design tradeoffs**:
  - β parameter: Higher β increases latent regularization but may reduce reconstruction fidelity; paper tested β ∈ {0.5, 1.0, 2.5}
  - Number of flow transformations (K): 2-3 MADE blocks used; more = more expressive but higher compute/memory
  - Projector method: CL outperformed DCCA on MNIST-SVHN; may be dataset-dependent
  - HMC steps: 100 steps per sample used; more steps improve sampling accuracy but increase latency

- **Failure signatures**:
  - Low conditional coherence with high FID: Unimodal flow encoders not matching joint encoder well—check L_uni convergence
  - "Averaged" or blurry outputs: Indicates residual aggregation artifacts
  - High joint FID with low joint coherence from prior sampling: Prior distribution misaligned with learned latent structure

- **First 3 experiments**:
  1. Reproduce toy dataset (circles/squares) visualization to validate that NF posteriors cover correct latent region while Gaussian posteriors over-generalize
  2. Ablation on MNIST-SVHN: Train JNF with β ∈ {0.5, 1.0, 2.5}, plot coherence vs FID to find optimal tradeoff point
  3. Compare JNF vs JNF-Shared (CL) on dataset where background noise varies to validate shared information extraction improves cross-modal generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fitting an empirical distribution to latent embeddings after training improve unconditional generation quality on complex datasets?
- Basis in paper: The authors observe that on Translated PolyMNIST, "All models fail on the unconditional generation task from the prior" and suggest fitting a distribution on the latent embeddings after training.
- Why unresolved: The standard Gaussian prior appears insufficient for modeling complex latent geometry required by datasets like Translated PolyMNIST.
- What evidence would resolve it: Experiments on Translated PolyMNIST showing improved unconditional coherence and FID scores using an empirical prior versus the standard isotropic Gaussian.

### Open Question 2
- Question: Can diffusion models be effectively integrated as decoders into the JNF framework to enhance generation fidelity?
- Basis in paper: The discussion suggests that "diffusion decoders could also be used to improve the quality of generated samples as was done in [24]."
- Why unresolved: The paper establishes the framework using standard neural network decoders; the compatibility and performance gain of using iterative diffusion processes within this specific two-step training pipeline remain untested.
- What evidence would resolve it: Comparative benchmarks showing that replacing standard decoders with diffusion models increases sample fidelity without degrading conditional coherence.

### Open Question 3
- Question: To what extent does the choice of shared information extractor depend on dataset modality types?
- Basis in paper: The authors note that "the best method might depend on the dataset" regarding the extraction of shared information.
- Why unresolved: While JNF-Shared works with both DCCA and CL, the specific conditions favoring one method are not fully characterized.
- What evidence would resolve it: An ablation study across heterogeneous datasets identifying which extraction method maximizes similarity between q_φj(z|g_j(x_j)) and the true joint posterior.

## Limitations

- The assumption that the frozen joint encoder provides an accurate target for unimodal encoders is never directly verified through posterior matching analysis
- The choice of 2-3 MADE blocks for Normalizing Flows appears empirically sufficient but lacks systematic comparison to deeper flows or alternative architectures
- The claim that JNF fundamentally solves the "inference gap" problem rather than providing an alternative training regime that sidesteps it through architectural design

## Confidence

- **High Confidence**: The empirical superiority of JNF over MoE-based baselines on coherence metrics is well-established across multiple datasets
- **Medium Confidence**: The specific implementation choices (2-3 flow layers, HMC hyperparameters, β tuning) appear reasonable but haven't been systematically validated
- **Low Confidence**: The assumption that extracted shared representations necessarily simplify posterior approximation lacks rigorous validation

## Next Checks

1. **Posterior Matching Analysis**: Visualize and quantify the KL divergence between q_φ(z|X) and q_φj(z|x_j) distributions for individual samples to verify the two-stage training achieves intended posterior alignment

2. **Flow Depth Ablation**: Systematically vary the number of MADE blocks (K=1,2,3,4) and measure impact on both coherence and FID to determine if 2-3 layers are optimal or merely sufficient

3. **HMC Sensitivity Test**: Vary HMC step size and number of leapfrog steps while monitoring coherence stability to ensure sampling quality doesn't depend on fortuitous hyperparameter choices