---
ver: rpa2
title: 'YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents
  for Enhanced Healthcare QA Summarization'
arxiv_id: '2504.03932'
source_url: https://arxiv.org/abs/2504.03932
tags:
- task
- zhang
- gpt-4o
- summarization
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of perspective-aware summarization
  in healthcare community question-answering forums, where user responses contain
  diverse viewpoints that complicate information extraction. The authors propose a
  multi-perspective integration framework combining zero-/few-shot prompting, Mixture-of-Agents
  (MoA) ensemble learning, and QLoRA fine-tuning approaches.
---

# YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare QA Summarization

## Quick Facts
- arXiv ID: 2504.03932
- Source URL: https://arxiv.org/abs/2504.03932
- Reference count: 5
- Primary result: MoA framework improved LLaMA performance by 28% on span identification (to 0.51) and 32% on summarization (to 0.37)

## Executive Summary
This paper tackles the challenge of extracting and summarizing multiple perspectives from healthcare community question-answering forums, where user responses contain diverse viewpoints (cause, suggestion, experience, information, question). The YaleNLP team proposes a multi-perspective integration framework combining zero-/few-shot prompting, Mixture-of-Agents (MoA) ensemble learning, and QLoRA fine-tuning. GPT-4o zero-shot achieved the best performance with 0.57 overall score on span identification/classification and 0.42 on perspective-based summarization, outperforming open-source models. The MoA framework with 2-layer configuration improved LLaMA performance significantly (28% on span identification, 32% on summarization), while sentence-transformer embedding-based exemplar selection proved more effective than manual selection for few-shot learning on open-source models. The team ranked second overall in the shared task.

## Method Summary
The approach addresses a two-stage task: Task A identifies and classifies text spans by perspective labels (cause, suggestion, experience, information, question) from healthcare CQA answers, while Task B generates perspective-specific summaries using only the extracted spans. Three main strategies were evaluated: zero-/few-shot prompting with GPT-4o and LLaMA-3.3-70B-Instruct, Mixture-of-Agents (MoA) ensemble learning with 2-layer configuration, and QLoRA fine-tuning. The MoA framework aggregates outputs from multiple models through iterative refinement - multiple LLMs generate proposals independently (Layer 1), an intermediate model refines/verifies outputs (Layer 2), and a final aggregator synthesizes coherent responses. For few-shot learning, sentence-transformer embeddings (all-MiniLM-L6-v2) encode candidate examples with k-means clustering to ensure diversity, selecting top-k samples by proximity to the test query.

## Key Results
- GPT-4o zero-shot achieved 0.57 overall score on span identification/classification and 0.42 on perspective-based summarization, outperforming open-source models
- MoA framework with 2-layer configuration improved LLaMA performance by 28% on span identification (to 0.51) and 32% on summarization (to 0.37)
- Sentence-transformer embedding-based exemplar selection proved more effective than manual selection for few-shot learning on open-source models
- QLoRA fine-tuning degraded performance (0.3664 vs 0.3968 zero-shot baseline) due to limited training data and annotation noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-layer Mixture-of-Agents (MoA) improves open-source model performance by aggregating diverse model outputs through iterative refinement
- Mechanism: Multiple LLMs generate proposals independently (Layer 1), an intermediate model refines/verifies outputs (Layer 2), and a final aggregator synthesizes coherent responses. Model diversity (LLaMA, Qwen, DeepSeek) provides complementary strengths in medical reasoning
- Core assumption: Different models make independent errors that can be corrected through aggregation; medical domain benefits from ensemble diversity
- Evidence anchors:
  - [abstract] "MoA framework with 2-layer configuration improved LLaMA performance by 28% on span identification (to 0.51) and 32% on summarization (to 0.37)"
  - [section 6.1] "2-layer configuration strikes the best balance between thoroughness and retaining valid outputs, outperforming both 1-layer and 3-layer variants"
  - [corpus] Limited corpus support—no external validation of MoA for medical summarization found in neighbors
- Break condition: 3-layer MoA degrades performance (0.3799 vs 0.4025 for 2-layer), suggesting over-correction filters valid outputs

### Mechanism 2
- Claim: Embedding-based exemplar selection outperforms manual curation for few-shot prompting on open-source models
- Mechanism: Sentence-transformer embeddings (all-MiniLM-L6-v2) encode candidate examples; k-means clustering ensures diversity; top-k samples are selected by proximity to test query
- Core assumption: Semantic similarity correlates with exemplar usefulness; clustering provides coverage of corner cases
- Evidence anchors:
  - [abstract] "Sentence-transformer embedding-based exemplar selection proved more effective than manual selection for few-shot learning on open-source models"
  - [table 1] LLaMA 3-shot w/ clustering: 0.4246 overall vs 3-shot w/ human: 0.4031
  - [corpus] Related work (Tang et al. 2025) supports adaptive few-shot prompting via similarity retrieval
- Break condition: Few-shot prompting did not consistently help GPT-4o (0.5697 zero-shot vs 0.5580 with clustering), suggesting frontier models may not benefit

### Mechanism 3
- Claim: Sequential pipeline (span identification → summarization) with perspective-specific prompts enables structured multi-view extraction
- Mechanism: Task A identifies and labels text spans by perspective (cause, suggestion, experience, information, question); Task B generates summaries per perspective using only extracted spans as source
- Core assumption: Accurate span identification is prerequisite for quality summarization; perspective categories are mutually intelligible
- Evidence anchors:
  - [section 3] "Given a medical related query and a set of answers from CQA forums, the system is required to (i) identify the specific perspective... and (ii) generate a summarization"
  - [section 6.2] Task B results derived from best spans from Task A
  - [corpus] PUMA dataset (Naik et al. 2024) validates perspective categories for healthcare Q&A
- Break condition: Error propagation—poor span identification directly degrades summarization quality

## Foundational Learning

- Concept: **Aggregator model selection matters significantly**
  - Why needed here: MoA aggregator choice determines fusion quality; LLaMA-3.3-70B as aggregator outperformed GPT-4o-mini (0.5063 vs 0.4027 on Task A)
  - Quick check question: Can you explain why a weaker aggregator model would degrade ensemble outputs despite having better proposer models?

- Concept: **QLoRA fine-tuning can degrade performance with limited data**
  - Why needed here: Contrary to expectations, QLoRA fine-tuning scored 0.3664 (Task A) vs 0.3968 zero-shot baseline
  - Quick check question: What conditions would make parameter-efficient fine-tuning harmful rather than beneficial?

- Concept: **Layer depth in MoA follows a diminishing-returns curve**
  - Why needed here: 2-layer MoA optimal; 3-layer causes over-correction and output loss
  - Quick check question: Why might additional verification layers filter out valid information?

## Architecture Onboarding

- Component map: Input query + answers → Task A (span identification with MoA) → Labeled spans → Task B (summarization with MoA) → Perspective-specific summaries
- Critical path: Input query + answers → Task A (span identification with MoA) → Labeled spans → Task B (summarization with MoA) → Perspective-specific summaries
- Design tradeoffs:
  - Open-source only vs hybrid: GPT-4o zero-shot (0.5697) still beats best MoA open-source (0.5063), but MoA closes gap by ~28%
  - Speed vs accuracy: MoA requires 4+ model calls per input; zero-shot GPT-4o is single-call
  - Clustering vs manual exemplars: Automated selection scales better but requires embedding infrastructure
- Failure signatures:
  - QLoRA with limited training data degrades performance (0.3664 < 0.3968 baseline)
  - 3-layer MoA underperforms 2-layer (0.38 vs 0.44 overall) due to over-correction
  - Few-shot prompting harms GPT-4o performance inconsistently
- First 3 experiments:
  1. Establish baselines: Run zero-shot GPT-4o and LLaMA-3.3-70B-Instruct on Task A/B to confirm reproduction of reported scores (0.57/0.42 for GPT-4o; 0.40/0.28 for LLaMA)
  2. Validate MoA lift: Implement 2-layer MoA with LLaMA+Qwen proposers and LLaMA aggregator; target 0.51 Task A score
  3. Ablate exemplar selection: Compare manual vs embedding-based few-shot on LLaMA to confirm clustering advantage (~0.02 improvement)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What fine-tuning configurations (hyperparameters, data augmentation, or training strategies) would enable QLoRA to improve rather than degrade performance on perspective-aware healthcare summarization?
- Basis in paper: [explicit] The authors state: "QLoRA fine-tuning with generic limited training data does not provide performance gains under our experimental conditions; in fact, it degrades performance. Due to the time constraints of the challenge, we were unable to explore additional fine-tuning configurations. We leave fine-tuning recipe exploration to future work."
- Why unresolved: QLoRA degradation (0.3664 vs. 0.3968 zero-shot on Task A) is counterintuitive given prior success of parameter-efficient fine-tuning; cause remains unknown whether it is data scarcity, hyperparameter choices, or task-specific factors
- What evidence would resolve it: Systematic ablation over learning rates, lora ranks, epoch counts, and augmented training data would identify which factors recover or surpass zero-shot baselines

### Open Question 2
- Question: Would data augmentation via LLM-generated synthetic examples for underrepresented perspectives (e.g., cause, question) improve model performance and generalizability?
- Basis in paper: [explicit] Future Work proposes: "extracting more healthcare-related queries from broader corpora such as Natural Questions, followed by data augmentation via LLMs to create synthetic examples for underrepresented perspectives."
- Why unresolved: Class imbalance and limited training data (2,236 threads) were identified as constraints, but the effectiveness of synthetic augmentation for this specific multi-perspective task remains untested
- What evidence would resolve it: A controlled experiment comparing baseline training vs. training with synthetically augmented minority-class examples, measuring per-perspective F1 and overall summarization quality

### Open Question 3
- Question: Why does the 3-layer MoA configuration cause performance degradation compared to 2-layer, and can layer-specific calibration mitigate over-correction?
- Basis in paper: [explicit] The paper reports: "the 2-layer multi proposer configuration offers the best trade-off... while the third layer tends to over-correction, resulting in a performance drop."
- Why unresolved: The mechanism of "over-correction" is not analyzed—whether it stems from error propagation, excessive hallucination filtering, or aggregator miscalibration is unclear
- What evidence would resolve it: Layer-wise analysis of output divergence, hallucination rates, and span precision/recall across 1/2/3-layer MoA variants would pinpoint the failure mode

### Open Question 4
- Question: How does annotation noise (overlapping spans, incomplete word boundaries) impact model performance, and would noisy-label robust training methods help?
- Basis in paper: [inferred] Limitations note: "Text span identification/classification annotations contain overlaps and ambiguities (e.g. extracted span starts with an incomplete word or punctuation), complicating the accuracy of perspective labels and gold summaries."
- Why unresolved: No experiments directly assess annotation quality effects or test noise-robust training; performance ceilings may reflect data issues rather than model limitations
- What evidence would resolve it: Correlation analysis between annotation consistency scores and per-example model performance, plus comparison of standard vs. noise-robust loss functions (e.g., label smoothing, noise-adaptive losses)

## Limitations

- Critical implementation details missing for k-means clustering parameters and temperature values in MoA configuration
- QLoRA fine-tuning unexpectedly degraded performance with limited data, but no systematic ablation was performed
- 3-layer MoA configuration degraded performance due to unclear over-correction mechanisms
- Limited external validation beyond internal validation splits

## Confidence

- **High confidence**: GPT-4o zero-shot baseline performance (0.57/0.42 overall scores) and fundamental MoA architecture with 2-layer configuration
- **Medium confidence**: Specific improvement percentages (28% on span identification, 32% on summarization) and embedding-based exemplar selection advantage
- **Low confidence**: QLoRA fine-tuning results and specific optimal configuration of MoA layers due to missing implementation details

## Next Checks

1. Replicate the zero-shot baseline results with GPT-4o and LLaMA-3.3-70B-Instruct on Task A/B to confirm the reported 0.57/0.42 and 0.40/0.28 scores respectively
2. Implement the MoA framework with specified 2-layer configuration (4 proposers in Layer 1, 1 verifier in Layer 2, 1 aggregator) and validate the 28% improvement claim for LLaMA
3. Test the embedding-based exemplar selection approach by comparing it against manual selection on LLaMA with few-shot prompting to verify the reported ~0.02 performance improvement