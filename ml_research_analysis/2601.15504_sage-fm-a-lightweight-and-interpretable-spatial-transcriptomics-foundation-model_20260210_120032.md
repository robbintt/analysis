---
ver: rpa2
title: 'SAGE-FM: A lightweight and interpretable spatial transcriptomics foundation
  model'
arxiv_id: '2601.15504'
source_url: https://arxiv.org/abs/2601.15504
tags:
- spatial
- embeddings
- gene
- transcriptomics
- genes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAGE-FM, a lightweight foundation model for
  spatial transcriptomics that uses a simple GCN with masked gene prediction to learn
  spatially conditioned regulatory relationships. Trained on 416 human Visium samples
  spanning 15 organs, the model captures coherent spatial gene dependencies and robustly
  recovers masked genes, with 91% showing significant correlations.
---

# SAGE-FM: A lightweight and interpretable spatial transcriptomics foundation model

## Quick Facts
- arXiv ID: 2601.15504
- Source URL: https://arxiv.org/abs/2601.15504
- Reference count: 40
- Key outcome: Simple GCN with masked gene prediction learns spatially coherent embeddings and outperforms MOFA in clustering and annotation tasks

## Executive Summary
SAGE-FM introduces a lightweight foundation model for spatial transcriptomics that uses a simple 5-layer GCN with masked gene prediction to learn spatially conditioned regulatory relationships. Trained on 416 human Visium samples spanning 15 organs, the model captures coherent spatial gene dependencies and robustly recovers masked genes, with 91% showing significant correlations. SAGE-FM outperforms MOFA and raw spatial transcriptomics in unsupervised clustering, preserving biological heterogeneity, and achieves 81% accuracy in pathologist-defined spot annotation in OSCC. It also improves glioblastoma subtype prediction and demonstrates biologically meaningful directional regulatory effects in in silico perturbation experiments.

## Method Summary
SAGE-FM uses a 5-layer GCN with hidden sizes 1024-512-512-512-1024 trained via self-supervised masked gene prediction on 416 human Visium samples. The model constructs 15-spot subgraphs (central spot + 14 nearest neighbors at ~500 µm radius) and masks 30% of genes in the central spot, predicting them from both unmasked genes and neighboring spots. This forces the model to learn spatially conditioned gene-gene dependencies. Downstream applications include clustering (OSCC, GBM) and in silico perturbation experiments to validate directional regulatory relationships.

## Key Results
- Masked gene recovery achieves RMSE=0.305 (baseline without imputation=0.465) with 91% of masked genes showing significant correlations
- Outperforms MOFA and raw spatial transcriptomics in unsupervised clustering (ARI, DBI, Silhouette metrics)
- Achieves 81% accuracy in pathologist-defined spot annotation in OSCC
- Improves glioblastoma subtype prediction and demonstrates biologically meaningful directional regulatory effects in perturbation experiments

## Why This Works (Mechanism)

### Mechanism 1: Spatially-Conditioned Masked Gene Prediction
Masking 30% of genes in a central spot and predicting them from both unmasked genes and 14-neighbor context forces the model to learn spatially conditioned gene-gene dependencies. The GCN must infer masked gene values by aggregating information from the local spatial neighborhood (~500 µm radius), which implicitly encodes that gene expression at any location depends on both intrinsic cell state and extrinsic signaling from nearby cells. This reconstruction objective creates embeddings where co-occurring spatial patterns are preserved.

### Mechanism 2: Local Graph Convolution as Spatial Context Aggregation
A simple 5-layer GCN with 1024-512-512-512-1024 hidden units is sufficient to capture spatial gene dependencies without requiring transformer attention or hierarchical architectures. Each GCN layer aggregates features from immediate graph neighbors through weighted message passing, where edge weights encode Euclidean distances. Stacking 5 layers expands the effective receptive field while maintaining spatial locality. This approximates how molecular signals propagate through tissue neighborhoods.

### Mechanism 3: In Silico Perturbation Recovers Directional Regulatory Relationships
Artificially perturbing ligand expression in neighboring spots and predicting receptor response in central spots reveals that the model encodes directional regulatory effects consistent with known biology. By clamping ligand genes to min/max expression in neighbors while masking receptor genes in the central spot, the model's predictions reflect learned statistical dependencies that mirror causal regulatory relationships. High ligand → high receptor predictions indicate positive regulation.

## Foundational Learning

- **Graph Neural Networks (GCNs) for Spatial Data**: Why needed here: SAGE-FM represents tissue as a graph where nodes are spots and edges encode spatial adjacency. Understanding how GCNs propagate information through weighted message passing is essential for debugging embedding quality. Quick check question: Can you explain why increasing GCN depth expands the receptive field, and what tradeoff this introduces?

- **Self-Supervised Learning via Masked Prediction**: Why needed here: The model's pre-training uses no labels—only the reconstruction objective. Understanding how masking forces representation learning is critical for adapting the model to new tissues or platforms. Quick check question: Why does masking 30% of genes work better than 10% or 90% for this task?

- **Visium Spatial Transcriptomics Resolution**: Why needed here: SAGE-FM is trained specifically on 10x Visium data with ~55µm spot diameter and 500µm neighborhood radius. Platform-specific constraints determine generalization limits. Quick check question: What happens if you apply this model to subcellular-resolution data like Xenium without retraining?

## Architecture Onboarding

- **Component map**: Visium samples → 15-spot subgraphs → 5-layer GCN (1024→512→512→512→1024) → Linear reconstruction head → Masked MSE loss

- **Critical path**: Subgraph generation from Visium coordinates → Gene masking strategy (30%) → GCN forward pass through 5 layers → Masked prediction and loss computation

- **Design tradeoffs**: GCN vs. Graph Transformer (parameter-efficient vs. long-range dependencies), 15-spot vs. larger neighborhoods (computational cost vs. context), single-platform training (batch effect avoidance vs. generalization limits)

- **Failure signatures**: RMSE approaches baseline (0.465) indicates no spatial learning; clustering ARI below MOFA baseline indicates failed biological heterogeneity capture; uniform perturbation responses indicate memorized correlation without regulatory structure

- **First 3 experiments**: 1) Run pre-training on HEST1k training split with 30% masking; validate RMSE < 0.35 and 90%+ genes with significant correlations; 2) Extract embeddings and run k-means clustering; verify ARI exceeds MOFA baseline; 3) Perform in silico perturbation on 5 known ligand-receptor pairs; confirm predicted responses match expected directionality

## Open Questions the Paper Calls Out

- **Generalization to single-cell platforms**: Can SAGE-FM effectively generalize to single-cell or subcellular resolution spatial transcriptomics platforms such as Xenium or Stereo-seq? The model was pre-trained exclusively on 10x Visium data, which operates at spot-level resolution rather than single-cell resolution.

- **Integration of auxiliary data modalities**: To what extent does integrating auxiliary data modalities (e.g., proteomics, histopathology) improve the model's representation of spatial gene-protein interactions? The current model architecture relies solely on gene expression vectors and spatial coordinates, excluding other available tissue data layers.

- **Advanced architectures vs. GCNs**: Do advanced architectures, such as graph transformers, provide significant biological utility over simple GCNs when evaluated on the proposed interpretable benchmarks? The study demonstrates that simple GCNs are sufficient for current tasks, but it remains untested whether complex attention mechanisms capture distinct biological signals the GCN misses.

## Limitations

- Limited training details including optimizer choice, learning rate schedule, and batch size are unspecified
- Model performance on tissues/organs outside the 15 in HEST1k remains untested, raising questions about generalization to rare or novel tissue types
- Exact edge weight construction from Euclidean distances is unclear, and boundary spot handling is not described

## Confidence

- **High Confidence**: Masked gene prediction mechanism and GCN architecture effectiveness (RMSE=0.305 vs baseline 0.465, 91% gene correlation significance)
- **Medium Confidence**: Clustering performance claims and OSCC annotation accuracy (81%)
- **Medium Confidence**: GBM subtype prediction improvements and in silico perturbation biological interpretability

## Next Checks

1. Test SAGE-FM embeddings on 3-5 tissues not in HEST1k (e.g., kidney, pancreas, rare cancers) to assess cross-tissue generalization
2. Compare GCN performance against graph transformer baselines on the same spatial transcriptomics tasks to quantify architectural efficiency gains
3. Perform ablation study varying masking ratios (10%, 20%, 50%) and GCN depth (3-7 layers) to confirm optimal hyperparameters and identify failure modes