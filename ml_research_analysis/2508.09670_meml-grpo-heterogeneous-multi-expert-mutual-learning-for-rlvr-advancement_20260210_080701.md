---
ver: rpa2
title: 'MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement'
arxiv_id: '2508.09670'
source_url: https://arxiv.org/abs/2508.09670
tags:
- reasoning
- learning
- arxiv
- meml-grpo
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEML-GRPO addresses reward sparsity in RLVR by leveraging diverse
  expert prompts and inter-expert mutual learning to improve reasoning performance.
  It fine-tunes models on responses from heterogeneous LLMs under distinct system
  prompts, enabling exploration beyond a model's initial policy limitations.
---

# MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement

## Quick Facts
- **arXiv ID:** 2508.09670
- **Source URL:** https://arxiv.org/abs/2508.09670
- **Reference count:** 12
- **Primary result:** Achieves 4.89% (Qwen) and 11.33% (Llama) average performance gains on GSM8K, MathQA, and StrategyQA compared to state-of-the-art RLVR methods.

## Executive Summary
MEML-GRPO addresses reward sparsity in reinforcement learning with verifiable rewards (RLVR) by leveraging diverse expert prompts and inter-expert mutual learning. The framework fine-tunes models on responses from heterogeneous LLMs under distinct system prompts, enabling exploration beyond a model's initial policy limitations. By introducing Reinforced Inter-Expert Learning (RIEL) and Hard Example Accumulation via SFT Buffer, MEML-GRPO demonstrates significant improvements over state-of-the-art RLVR methods across reasoning benchmarks.

## Method Summary
MEML-GRPO employs a two-stage pipeline: Multi-Expert Fine-tuning (MEF) and Reinforced Inter-Expert Learning (RIEL). In MEF, a base LLM is fine-tuned on a dataset generated by multiple expert models (DeepSeek-r1, GPT4o, Doubao-1.5-thinking) under expert-specific system prompts. The RIEL stage then samples responses from each expert, computes rewards, and applies GRPO loss with inter-expert KL divergence regularization. When experts collectively fail (>K incorrect out of G samples), the question-answer pair is added to a hard example buffer, triggering supervised fine-tuning when the buffer fills.

## Key Results
- MEML-GRPO achieves average performance gains of 4.89% on Qwen and 11.33% on Llama across GSM8K, MathQA, and StrategyQA datasets
- Outperforms state-of-the-art RLVR methods including PPO, GRPO, and MoE-SFT baselines
- Demonstrates that diverse expert prompts increase correct solution generation probability in sparse reward settings
- Shows inter-expert KL divergence regularization enables weaker experts to internalize strengths of stronger experts

## Why This Works (Mechanism)

### Mechanism 1
Diverse expert prompts increase correct solution probability in sparse reward settings. Heterogeneous models exhibit low error overlap (3.06% on GSM8K), and distilling their reasoning paths into a single model enables exploration of broader solution space. This addresses RLVR's exploration bottleneck where standard methods focus on biasing toward rewarded behaviors rather than learning new knowledge.

### Mechanism 2
Inter-expert KL divergence regularization enables knowledge transfer between experts. For each question, the framework identifies best-performing (E+) and worst-performing (E-) experts, then penalizes distributional divergence between them on correct responses. This forces E- to align its output distribution toward successful reasoning paths, reducing inference-time ensemble costs.

### Mechanism 3
Hard example accumulation with periodic SFT ensures gradient flow when RL signals are uniformly zero. When experts produce >K incorrect answers out of G samples, the question-answer pair is added to a fixed-capacity buffer. SFT triggers when the buffer fills, providing supervised signal precisely when RL cannot, automatically adapting to problem difficulty.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed:** MEML-GRPO builds directly on GRPO's advantage estimation. Without understanding GRPO's group-relative normalization, the inter-expert extension is opaque.
  - **Quick check:** Given 8 sampled responses with rewards [0,0,0,1,0,0,0,0], what is the advantage of the correct response? (Answer: 1 - 1/8 = 0.875)

- **Concept: KL Divergence as Distributional Regularization**
  - **Why needed:** The inter-expert learning mechanism uses KL divergence to align weaker expert distributions with stronger ones. Understanding this as minimizing cross-entropy between distributions is essential.
  - **Quick check:** Why use log p(O+|Q,prompt_E-) - log p(O+|Q,prompt_E+) rather than directly maximizing p(O+|Q,prompt_E-)? (Answer: The difference form creates a relative penalty; directly maximizing would ignore the reference distribution's scaling)

- **Concept: Reward Sparsity in RLVR**
  - **Why needed:** The core problem MEML-GRPO addresses. When all sampled rollouts yield zero reward, policy gradient estimates have zero signal regardless of optimization algorithm.
  - **Quick check:** A model samples 16 responses to a challenging math problem; all are incorrect. What gradient signal does standard RLVR provide? (Answer: Zero—the policy cannot distinguish which incorrect paths were "closer" to correct)

## Architecture Onboarding

- **Component map:** Base LLM → Expert-prompted data generation → MEF (LR=1e-5, 1 epoch) → RIEL stage (LR=1e-6, 1 epoch, G=8 rollouts) → GRPO + KL loss + SFT buffer → Periodic SFT (B=64) → Inference (single expert selection)
- **Critical path:** MEF quality determines RIEL ceiling. If experts don't internalize distinct reasoning styles during SFT, subsequent RL has no diversity to exploit.
- **Design tradeoffs:** Buffer size (B=64) vs. SFT frequency; Number of experts (N=3) vs. compute (20-30% training overhead); Greedy inference vs. majority voting (MEML-GRPO single-expert outperforms MoE-SFT majority voting).
- **Failure signatures:** All experts converge to identical outputs → KL loss collapses; Buffer fills too slowly on easy datasets → HSFT never triggers; Buffer fills instantly on very hard datasets → training becomes pure SFT → overfitting risk.
- **First 3 experiments:**
  1. Reproduce Table 1 error overlap analysis on your expert models; if overlap > 15%, diversity assumption may not hold.
  2. MEF-only baseline: Train MoE-SFT without RIEL; verify experts produce distinct reasoning paths.
  3. Ablation checkpoint: Train full MEML-GRPO with logging for L_GRPO, L_KL, L_SFT; confirm L_KL decreases (knowledge transfer) and L_SFT triggers correlate with batch difficulty.

## Open Questions the Paper Calls Out
- How can the optimal system prompt (expert) be automatically selected at inference time without access to ground truth labels?
- To what extent does the framework's performance rely on the specific capabilities of the heterogeneous teacher models versus the mutual learning mechanism itself?
- Does the Inter-Expert Mutual Learning mechanism induce negative transfer when the designated "best-performing expert" generates a confident but incorrect reasoning path?

## Limitations
- The framework assumes ground truth availability for hard example SFT, limiting applicability to domains without explicit verification.
- Performance generalization to other model scales beyond 1.5B/1B models and different expert combinations remains untested.
- The inter-expert learning mechanism's effectiveness depends on maintaining meaningful expert diversity, which may degrade with model convergence.

## Confidence
- **High Confidence:** MEML-GRPO's improvement over baselines (4.89-11.33% gains) is well-supported by ablation studies showing RIEL and HSFT contributions. The expert diversity assumption (3.06% error overlap) is empirically validated in Table 1.
- **Medium Confidence:** The mechanism of inter-expert KL divergence transfer is theoretically sound but lacks strong direct evidence—the paper shows MEML-GRPO experts outperform majority voting but doesn't isolate the KL component's impact.
- **Low Confidence:** Generalization to other model scales (beyond 1.5B/1B models) and different expert combinations is untested. The SFT buffer's adaptive frequency mechanism hasn't been validated across diverse difficulty distributions.

## Next Checks
1. **Error Correlation Analysis:** Replicate Table 1's error overlap computation for your target expert models on your domain's validation set. If overlap exceeds 15%, diversity benefits may not materialize.
2. **KL Transfer Isolation:** Train MEML-GRPO with λ1=0 (no inter-expert learning) vs. λ1=1.0 on a held-out dataset. Compare response distributions—significant performance drop with λ1=0 would validate knowledge transfer.
3. **Buffer Adaptation Test:** On a dataset with known difficulty stratification, monitor SFT buffer fill rates across subsets. Verify the mechanism triggers more frequently on genuinely harder subsets rather than random sampling noise.