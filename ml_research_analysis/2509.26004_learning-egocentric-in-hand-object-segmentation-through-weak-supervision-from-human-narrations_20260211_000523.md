---
ver: rpa2
title: Learning Egocentric In-Hand Object Segmentation through Weak Supervision from
  Human Narrations
arxiv_id: '2509.26004'
source_url: https://arxiv.org/abs/2509.26004
tags:
- object
- hand
- objects
- narrations
- wish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel task called Narration-Supervised
  in-Hand Object Segmentation (NS-iHOS), where models learn to segment objects manipulated
  by a user from egocentric images using natural language narrations as the sole supervision
  during training, without requiring narrations at inference time. The proposed method,
  WISH (Weakly Supervised In-Hand Object Segmentation from Human Narrations), uses
  a two-stage architecture: first, it learns a shared vision-language alignment space
  between hand-specific noun phrases and detected objects; then, it distills this
  knowledge into a purely visual model to predict in-hand objects.'
---

# Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations

## Quick Facts
- arXiv ID: 2509.26004
- Source URL: https://arxiv.org/abs/2509.26004
- Reference count: 40
- Primary result: Recovers >50% of fully supervised performance using only natural language narrations for training

## Executive Summary
This paper introduces Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models learn to segment objects manipulated by a user in egocentric videos using only natural language narrations as weak supervision. The proposed WISH method employs a two-stage architecture: first aligning detected objects with hand-specific noun phrases from narrations through contrastive learning, then distilling this knowledge into a visual-only model that predicts in-hand objects at inference time without requiring narrations. Experiments on EPIC-Kitchens and Ego4D demonstrate that WISH achieves competitive performance compared to fully supervised baselines while operating under extreme annotation constraints.

## Method Summary
WISH employs a two-stage architecture to solve NS-iHOS. Stage 1 uses CLIP's vision-language embeddings to align detected objects with hand-specific augmented noun phrases from narrations via contrastive learning (InfoNCE loss). Hand-specific templates (e.g., "[noun] in contact with left hand") are generated for each detected object and compared against visual object embeddings. Stage 2 distills the alignment knowledge into a purely visual model with two heads: a contactness head predicting binary contact probability and a matching head selecting the most likely interacting object per hand. Pseudo-labels from Stage 1 train these heads, enabling inference without narrations.

## Key Results
- WISH achieves 27.66 E-mIoU on EPIC-Kitchens and 23.61 on Ego4D, outperforming all baselines
- Recovers more than 50% of fully supervised performance without pixel-wise annotations
- GSAM (taxonomy-aware) outperforms SAM (class-agnostic) across all metrics
- The two-head architecture (contactness + matching) improves performance from 21.85 to 27.66 E-mIoU

## Why This Works (Mechanism)

### Mechanism 1
Hand-specific text augmentation enables cross-modal alignment between narrations and visual objects without explicit grounding labels. Noun phrases from narrations are augmented with hand-specific templates, encoded via CLIP, and aligned with visual object embeddings through contrastive learning. The model learns which detected objects correspond to which narrated nouns, disambiguating hand assignment under the assumption that each hand interacts with at most one object at any moment.

### Mechanism 2
Two-head architecture enables the model to handle both contact detection and competitive assignment under visual-only inference. The contactness head predicts binary contact probability for each (object, hand) pair, while the matching head applies competitive softmax to select the single most likely interaction per hand. These heads are complementary: matching selects objects and contactness verifies physical contact, handling cases where matching alone cannot distinguish no-contact scenarios.

### Mechanism 3
Pseudo-labels distilled from Stage 1 alignment provide sufficient supervision for Stage 2 visual-only learning despite label noise. Stage 1's learned alignment space generates contact and matching pseudo-labels that Stage 2 trains on as targets. The model generalizes beyond noisy pseudo-labels, achieving performance above the pseudo-label quality itself (27.66 vs. 26.28 E-mIoU), demonstrating that the Stage 2 architecture can distill relevant signals from imperfect supervision.

## Foundational Learning

- **Contrastive Learning (InfoNCE)**: Why needed here: Stage 1 uses InfoNCE to align image-narration pairs without explicit object-phrase correspondence labels. Understanding how contrastive loss pushes apart mismatched pairs is essential for debugging alignment failures. Quick check question: Given a batch of 4 image-narration pairs, can you compute the symmetric InfoNCE loss for one positive pair and explain how it affects the similarity matrix?

- **CLIP Vision-Language Representations**: Why needed here: The entire architecture builds on CLIP's pretrained image and text encoders. Understanding CLIP's embedding space (cosine similarity, projection) is critical for adapting it with MLP adapters and mask-guided attention. Quick check question: Why does the architecture project CLIP embeddings through learnable MLP adapters rather than fine-tuning CLIP directly? What would break if adapters were removed?

- **Egocentric Vision Constraints**: Why needed here: Assumptions like "one object per hand" and simple heuristics for left/right hand assignment are task-specific. Understanding these priors helps assess failure modes on non-standard egocentric data. Quick check question: What would happen to the hand-side heuristic if the camera were mounted on the chest rather than the head? How would you detect and handle this failure?

## Architecture Onboarding

- **Component map**: Object/Hand Detector (SAM/GSAM + hand detector) -> Mask-guided CLIP image encoder with MLP adapters -> CLIP text encoder with MLP adapters -> Similarity matrix computation -> InfoNCE loss (Stage 1) -> Pseudo-label generation -> Contactness/Matching heads training (Stage 2) -> Visual-only inference

- **Critical path**: Detect objects/hands → extract CLIP embeddings → project via MLP_v → encode text with hand templates → compute similarity matrix → aggregate to global score → apply InfoNCE loss (Stage 1) → generate pseudo-labels → train Contactness/Matching heads (Stage 2) → inference with matching head selection and contactness validation

- **Design tradeoffs**: GSAM outperforms SAM but requires dataset-specific taxonomy limiting generalization. Freezing CLIP preserves general representations but may limit domain adaptation. One-object-per-hand prior simplifies pseudo-labeling but fails on complex bimanual interactions. Simple hand-side heuristic (85.5% mAP) may fail on different camera placements.

- **Failure signatures**: Low alignment quality if InfoNCE loss plateaus high; noisy pseudo-labels if Stage 2 loss diverges; contactness/matching conflict if inference outputs low contactness objects; hand-side misassignment if left/right performance diverges significantly; domain shift if performance drops between datasets.

- **First 3 experiments**: 1) Validate alignment quality by evaluating pseudo-label mIoU against ground truth; 2) Ablate heads to quantify individual contributions (contactness only, matching only, both); 3) Test generalization by training on one dataset and testing on another without retraining.

## Open Questions the Paper Calls Out

- Can the static image-based WISH framework be extended to effectively leverage temporal dynamics in egocentric videos?
- How does the "one-object-per-hand" assumption degrade performance in complex scenarios involving dexterous manipulation?
- To what extent does temporal misalignment between narrations and visual actions impact the quality of generated pseudo-labels?

## Limitations

- Hand detector dependency on Leonardi et al. [17] without provided implementation details or model weights
- Hand-side heuristic fragility with vertical split assumption that may not generalize to different camera placements
- One-object-per-hand prior that breaks down for complex bimanual interactions with multiple objects

## Confidence

- **High**: Core mechanism of aligning hand-specific noun phrases with detected objects via contrastive learning is sound and well-supported
- **Medium**: Two-head architecture provides statistically significant boost but exact head contributions not fully isolated
- **Low**: "Strong generalization across scenarios" claim based on single domain transfer with only 4-point performance drop

## Next Checks

1. **Hand Detector Stress Test**: Run hand detector on held-out frames with annotated positions to measure recall/precision and correlate failures with NS-iHOS performance drops
2. **Cross-Camera Generalization**: Train on EPIC-Kitchens and test on dataset with different camera placement to measure performance degradation and failure modes
3. **Bimanual Interaction Analysis**: Isolate frames where both hands interact with multiple objects to analyze whether "Both" class performance is due to hand-side confusion, object detection failures, or the one-object-per-hand prior