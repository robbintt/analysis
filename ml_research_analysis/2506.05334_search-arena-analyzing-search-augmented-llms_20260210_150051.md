---
ver: rpa2
title: 'Search Arena: Analyzing Search-Augmented LLMs'
arxiv_id: '2506.05334'
source_url: https://arxiv.org/abs/2506.05334
tags:
- search
- arena
- user
- figure
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Search Arena, the first large-scale human-preference
  dataset for search-augmented large language models (LLMs), containing over 24,000
  multi-turn conversations and 12,000 preference votes across 13 models and 70+ languages.
  The dataset captures diverse user intents beyond factual lookup, including analysis,
  recommendations, and creative generation.
---

# Search Arena: Analyzing Search-Augmented LLMs

## Quick Facts
- arXiv ID: 2506.05334
- Source URL: https://arxiv.org/abs/2506.05334
- Reference count: 40
- Key outcome: Search Arena introduces first large-scale human-preference dataset for search-augmented LLMs with 24,000+ conversations, revealing users prefer more citations regardless of relevance and that search augmentation has task-dependent effects

## Executive Summary
Search Arena presents the first large-scale human preference dataset for search-augmented large language models, containing over 24,000 multi-turn conversations and 12,000 preference votes across 13 models and 70+ languages. The dataset reveals that users show positive preference bias toward citation presence regardless of whether cited content supports the claim, with citation count being a stronger predictor of preference than actual attribution correctness. Analysis shows search augmentation improves performance for information-seeking queries but may degrade results for text processing tasks in non-search settings.

## Method Summary
The study collected paired comparison data through a crowd-sourced evaluation platform where users compared responses from two anonymous search-augmented models side-by-side. Models included Perplexity sonar variants, Gemini with Google Search grounding, and OpenAI search-preview models. User votes were analyzed using Bradley-Terry regression to quantify how features like citation count, response length, and source type influence preferences. Citation attribution analysis involved LLM-based claim-citation pair extraction, web scraping of source content, and attribution labeling as supporting, irrelevant, or contradicting.

## Key Results
- Users prefer responses with more citations regardless of whether citations support the claims (β_irrelevant ≈ β_support ≈ 0.27)
- Search augmentation improves user preference for Factual Lookup queries but degrades performance on Text Processing tasks
- Models with higher search context retrieve more sources and receive higher user preference (sonar-pro high context: 63.9% win rate vs medium: 57.6%)
- Wikipedia sources receive negative preference signals (β_wiki = -0.071) compared to community blogs and tech platforms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Users show positive preference bias toward citation presence regardless of whether cited content supports the claim
- Mechanism: Citation count acts as a credibility signal; users perceive more citations as more trustworthy even when attribution is incorrect or irrelevant. The presence of inline citations creates an illusion of groundedness that overrides actual verification
- Core assumption: Users do not systematically verify citations during evaluation
- Evidence anchors: abstract citation bias finding, Section 3.2 showing similar β coefficients for supporting vs irrelevant claims, related work on search-augmented verification

### Mechanism 2
- Claim: Larger search context windows improve user preference through increased source retrieval
- Mechanism: Models with higher search context retrieve more web sources, enabling broader coverage for synthesis and recommendation queries. More retrieved sources → more citations in final response → higher user preference (mediated by Mechanism 1)
- Core assumption: Retrieval quality does not degrade significantly with larger context; additional sources are relevant enough to be useful
- Evidence anchors: Section 3.1 sonar-pro context comparison, Section 3.2 correlation between search context and citation count, Over-Searching paper boundary condition

### Mechanism 3
- Claim: Search augmentation improves performance on information-seeking queries but degrades performance on text processing tasks in non-search settings
- Mechanism: Search models optimize for retrieval-grounded responses; for text processing (summarization, translation), retrieval introduces unnecessary context that can lead to verbose or poorly structured outputs. Non-search models produce cleaner, more structured responses for these task types
- Core assumption: Text processing tasks benefit from parametric knowledge and structured generation rather than external retrieval
- Evidence anchors: Section 3.3 task-dependent performance differences, non-search model structured response formatting, SealQA benchmark context

## Foundational Learning

- Concept: Bradley-Terry preference modeling
  - Why needed here: The paper uses Bradley-Terry regression to quantify how features (citation count, response length, source type) influence human preference. Understanding this lets you interpret the β coefficients correctly
  - Quick check question: If β_citations = 0.209, does increasing citations by 1 increase win probability, or does it increase the latent score that feeds into win probability?

- Concept: Retrieval-Augmented Generation (RAG) vs. Search-Augmented LLMs
  - Why needed here: The paper distinguishes search-augmented systems (live web retrieval during inference) from static RAG. This matters for understanding why freshness and citation patterns differ from traditional RAG benchmarks
  - Quick check question: In RAG, retrieval typically happens from a fixed corpus. What additional complexity does live web search introduce for citation attribution?

- Concept: Intent-aware evaluation
  - Why needed here: The paper shows that search helps Factual Lookup but hurts Text Processing. You need intent classification to route queries appropriately in production systems
  - Quick check question: Which intent category shows the largest citation count effect, and which shows no significant effect?

## Architecture Onboarding

- Component map: Search Arena Platform -> Crowd-sourced evaluation interface (side-by-side model comparison) -> Search APIs (Perplexity sonar, Gemini Google Search grounding, OpenAI search preview) -> Citation Attribution Pipeline (LLM-based claim-citation pair extraction -> Web scraping (Firecrawl) -> Attribution labeling (Support/Irrelevant/Contradict)) -> Bradley-Terry Regression (Feature coefficient estimation for preference analysis)

- Critical path: 1. User submits query -> 2. Two anonymous search-augmented models retrieve and generate responses -> 3. User votes -> 4. Conversation logged with metadata (retrieved URLs, reasoning traces) -> 5. Bradley-Terry model fitted to preference data

- Design tradeoffs: Citation style randomization (standardized format reduces de-anonymization risk but may obscure provider-specific citation quality differences); High vs. medium search context (higher context increases citations and preference but may introduce retrieval noise); Reasoning models (better at filtering irrelevant sources but more verbose)

- Failure signatures: Citation without attribution (models cite sources that do not support claims; users cannot distinguish); Wikipedia over-reliance for real-time queries (static encyclopedic sources inappropriate for time-sensitive questions); Search degradation on text processing (non-search models preferred for structured output tasks)

- First 3 experiments: 1. Replicate citation bias analysis on held-out subset; 2. A/B test search context size for Factual Lookup vs Text Processing queries; 3. Implement task-aware retrieval routing (skip search for Text Processing intents)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can search-augmented LLMs be designed to ensure users properly evaluate citation credibility, given that users prefer responses with more citations even when they are irrelevant to claims?
- Basis in paper: [explicit] The authors state: "We raise this as an open issue for the community: improving citation attribution is critical to ensure that citation-heavy responses are not misperceived as factual and trustworthy." They found β = 0.285 for supporting citations vs β = 0.273 for irrelevant citations
- Why unresolved: The study reveals users cannot distinguish between supporting and irrelevant citations based on preference signals alone, but does not propose interface or training interventions
- What evidence would resolve it: A/B studies comparing user accuracy in identifying unsupported claims across different citation presentation formats or model training objectives that penalize irrelevant citations

### Open Question 2
- Question: Why does search augmentation degrade performance on text processing prompts, and can this degradation be mitigated?
- Basis in paper: [inferred] Cross-arena analysis shows non-search models are preferred for Text Processing queries (p = 0.077), with non-search models providing more structured formatting. The mechanism for this degradation is not identified
- Why unresolved: The paper documents the phenomenon but does not determine whether it stems from attention dilution, citation overhead, retrieval noise, or other factors
- What evidence would resolve it: Controlled experiments isolating retrieval noise, citation formatting, and context window competition effects on text processing task performance

### Open Question 3
- Question: What patterns emerge when citation attribution analysis is scaled beyond the ~100 conversations per intent category studied?
- Basis in paper: [explicit] The citation attribution pipeline was limited due to "scraping challenges and the high cost of LLM calls." Appendix E.3 states: "Future work can further scale this pipeline or add onto analysis methodology"
- Why unresolved: The subsample may not capture variation across topics, languages, or model families, limiting generalizability of the finding that users conflate irrelevant and supporting citations
- What evidence would resolve it: Large-scale attribution analysis across all 24,000 conversations with automated verification of whether patterns hold across languages, intent categories, and citation source types

## Limitations

- Citation attribution pipeline using LLM-based extraction and web scraping introduces potential noise, particularly for domains with rate limiting or dynamic content
- Cross-arena experiments comparing search-augmented vs non-search models are limited by specific model families chosen and may not capture full spectrum of search-augmented architectures
- Core finding that users prefer more citations regardless of relevance relies on assumption that crowdworkers do not verify citations, which may not generalize to expert evaluators

## Confidence

- High confidence: Citation count effect on user preference (β = 0.209, p < 0.01), task-dependent search augmentation benefits (Factual Lookup vs Text Processing), Wikipedia source penalty (β = -0.071)
- Medium confidence: Source-type preferences (community blogs/tech platforms positive; news sites neutral; government/academic mixed), reasoning model citation filtering tradeoff
- Low confidence: Generalization across all search-augmented architectures

## Next Checks

1. Validate citation bias finding by replicating analysis on held-out conversation subset with independent attribution labeling
2. Test task-aware retrieval routing by implementing search skipping for Text Processing queries and measuring user preference changes
3. Scale citation attribution analysis to full dataset by automating web scraping with domain-specific rate limiting and fallback procedures