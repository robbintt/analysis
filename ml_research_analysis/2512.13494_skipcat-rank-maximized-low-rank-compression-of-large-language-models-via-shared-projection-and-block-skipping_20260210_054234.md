---
ver: rpa2
title: 'SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via
  Shared Projection and Block Skipping'
arxiv_id: '2512.13494'
source_url: https://arxiv.org/abs/2512.13494
tags:
- compression
- low-rank
- arxiv
- matrix
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large language
  models (LLMs) on resource-constrained edge devices by proposing a novel low-rank
  compression framework called SkipCat. The core idea is to maximize the number of
  effective ranks retained during compression while maintaining the same compression
  rate, thereby preserving model performance.
---

# SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping

## Quick Facts
- arXiv ID: 2512.13494
- Source URL: https://arxiv.org/abs/2512.13494
- Authors: Yu-Chen Lu; Sheng-Feng Yu; Hui-Hsien Weng; Pei-Shuo Wang; Yu-Fang Hu; Liang Hung-Chun; Hung-Yueh Chiang; Kai-Chiang Wu
- Reference count: 40
- Key outcome: SkipCat outperforms existing low-rank compression methods by up to 7% in zero-shot task accuracy and significantly reduces perplexity without additional fine-tuning.

## Executive Summary
This paper addresses the challenge of deploying large language models (LLMs) on resource-constrained edge devices by proposing a novel low-rank compression framework called SkipCat. The core idea is to maximize the number of effective ranks retained during compression while maintaining the same compression rate, thereby preserving model performance. SkipCat introduces two key techniques: (1) Cat, which enables multiple matrices within a layer to share a common low-rank projection, and (2) Skip, which allows block skipping within low-rank decompositions to further reduce computation. Experimental results demonstrate that SkipCat outperforms existing low-rank compression methods by up to 7% in zero-shot task accuracy and significantly reduces perplexity without additional fine-tuning. The method is effective across various model sizes and compression rates, making it a practical solution for efficient LLM deployment.

## Method Summary
SkipCat is a low-rank compression framework that maximizes retained ranks under fixed compression budgets through two mechanisms: Cat (shared projection) and Skip (block skipping). The method first applies whitening to model weights using calibration data, then concatenates matrices that share the same input (Q,K,V in attention; gate,up in MLP) and performs a single SVD decomposition to extract a shared projection matrix. For numerical stability, it applies column permutation via Strong Rank-Revealing QR before block skipping. The Skip technique reformulates the multiplication to skip explicit computation of certain sub-blocks, reducing both memory and computation. The framework achieves up to 7% better zero-shot accuracy and significantly lower perplexity compared to existing methods without requiring additional fine-tuning.

## Key Results
- SkipCat achieves 6.29 perplexity on LLaMA2-7B at 20% compression, outperforming Cat-only (7.84) and Skip-only (6.71)
- Zero-shot task accuracy improves by up to 7% compared to baseline low-rank compression methods
- Maintains stability under 8-bit quantization when combined with Hadamard transforms and channel scaling
- Outperforms ASVD and SVD-LLM by 7% on average across zero-shot tasks

## Why This Works (Mechanism)

### Mechanism 1: Intra-Layer Shared Projection (Cat)
- **Claim:** Sharing a single low-rank projection matrix across weight matrices that receive identical inputs enables higher retained ranks under the same compression budget.
- **Mechanism:** Rather than decomposing W_Q, W_K, W_V separately via SVD, the method concatenates them along the output dimension into W_QKV ∈ R^(3d_out × d_in), then performs a single low-rank decomposition: W_QKV ≈ B_QKV × A_QKV. The shared projection A_QKV (called W_S1) is reused across all three, while B_QKV is sliced to recover individual reconstruction matrices. The amortized parameters per matrix become r(d_in + C·d_out)/C where C is the concatenation count.
- **Core assumption:** Matrices sharing the same input (Q,K,V in attention; gate,up in MLP) exhibit redundant projection patterns that can be unified without significant information loss.
- **Evidence anchors:**
  - [abstract] "First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection."
  - [section 3.2] "We then slice B_QKV along the output dimension to recover the original shapes corresponding to Ŵ_Q, Ŵ_K, and Ŵ_V... Compared to the naïve approach, Cat allows more ranks to be enables the retention of more ranks under the same memory and computational budget."
  - [corpus] Related work "Basis Sharing" shares across layers but provides no inference benefit; Cat shares within-layer for both memory and compute gains.
- **Break condition:** If matrices have fundamentally different subspace requirements (e.g., Q vs K capture orthogonal patterns), forcing shared projection degrades accuracy despite compression gains.

### Mechanism 2: Block Skipping via Schur Complement Reformulation (Skip)
- **Claim:** Reformulating the low-rank multiplication as B·A₁·(x₁ + A₁⁻¹A₂x₂) eliminates explicit A₁x₁ computation, reducing FLOPs while preserving the effective rank capacity.
- **Mechanism:** The projection matrix A ∈ R^(r×d_in) is partitioned as A = [A₁ | A₂] where A₁ ∈ R^(r×r). Instead of computing B·(A₁x₁ + A₂x₂), the method computes B'·(x₁ + A'x₂) where B' = BA₁ and A' = A₁⁻¹A₂. This reduces FLOPs from 2r(d_in + d_out) to 2r(d_in + d_out - r) + r and parameters from r(d_in + d_out) to r(d_in + d_out - r).
- **Core assumption:** The leading r×r submatrix A₁ can be made well-conditioned so that A₁⁻¹ does not produce catastrophically large values.
- **Evidence anchors:**
  - [abstract] "Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition."
  - [section 3.3] "This reformulation eliminates the need to explicitly compute products involving A₁ by absorbing it into the redefined matrices B' and A'."
  - [corpus] Related SVD compression methods (ASVD, SVD-LLM) do not exploit this reformulation, making SkipCat's rank-maximization approach novel.
- **Break condition:** If A₁ remains ill-conditioned despite preprocessing, A' = A₁⁻¹A₂ explodes, causing FP16 overflow.

### Mechanism 3: Column Permutation for Numerical Stability
- **Claim:** Permuting the projection matrix columns to ensure the leading r×r block is well-conditioned prevents numerical overflow in half-precision inference.
- **Mechanism:** Apply Strong Rank-Revealing QR (Gu & Eisenstat, 1996) to identify a column permutation P such that Ã = AP = [Ã₁ | Ã₂] where Ã₁ ∈ R^(r×r) has provably bounded condition number. The reformulated computation uses Ã₁⁻¹Ã₂ instead of A₁⁻¹A₂, keeping activation magnitudes ~100× smaller.
- **Core assumption:** A well-conditioned submatrix leads to bounded values in Ã' = Ã₁⁻¹Ã₂ after inversion.
- **Evidence anchors:**
  - [section 3.3] "As shown in the right subfigure of Figure 3, our preprocessing step applies a column permutation to the weight matrix, which significantly stabilizes the activation values... nearly two orders of magnitude smaller."
  - [section 3.3] "The permutation enhances numerical stability by avoiding inversion of poorly conditioned matrices."
  - [corpus] Weak direct corpus evidence on permutation for SVD stability; this appears to be a SkipCat-specific contribution.
- **Break condition:** If activation outliers in x₂ are extreme, even stable Ã'·x₂ may exceed FP16 range; additional scaling (Hadamard transform) may be required.

## Foundational Learning

- **Concept: SVD-based Low-Rank Approximation**
  - Why needed here: SkipCat builds on the decomposition W ≈ BA where B = U_rΣ_r^(1/2) and A = Σ_r^(1/2)V_r^⊤. Understanding why truncation trades rank for compression is essential.
  - Quick check question: Derive why naïve low-rank compression requires r < d_in·d_out/(d_in + d_out) to yield computational gains. For a square matrix, what does this imply?

- **Concept: Schur Complement**
  - Why needed here: The Skip mechanism relies on the Schur complement identity (Equation 6) to algebraically justify skipping A₁x₁ while preserving mathematical equivalence.
  - Quick check question: Show how A_full = [A₁₁ A₁₂; A₂₁ A₂₂] decomposes via Schur complement into the block form enabling Equation (9).

- **Concept: Condition Number and Numerical Stability**
  - Why needed here: Understanding why A₁⁻¹ can produce large values and why permutation bounds this is critical for implementing Skip without FP16 overflow.
  - Quick check question: If cond(A₁) = 10⁶, what happens to ||A₁⁻¹|| when computing A' = A₁⁻¹A₂? Why does FP16 (max ~65,504) risk overflow?

## Architecture Onboarding

- **Component map:**
  Input x → [Whitening preprocessing (calibration data)]
          → [Cat: Concatenated SVD for shared projections]
             ├─ Attention: W_QKV → B_QKV + shared W_S1
             └─ MLP: W_GU → B_GU + shared W_S2
          → [Skip: Block skipping for each projection]
             ├─ Column permutation via Strong RRQR
             ├─ Reformulate: B'·(x₁ + A'x₂)
             └─ FP16 inference (stable due to permutation)
          → Optional: [Quantization (Hadamard + channel scaling)]

- **Critical path:**
  1. Calibration data (512 mixed WikiText-2 + C4 samples) → Weight whitening → Concatenated SVD → Shared projection matrices
  2. For each shared projection: Apply Strong RRQR → Extract permutation P → Compute Ã' = Ã₁⁻¹Ã₂ and B̃' = BÃ₁
  3. Deploy: At inference, compute B̃'·(x̃₁ + Ã'x̃₂) with FP16 (stable) or apply quantization preprocessing if needed

- **Design tradeoffs:**
  - Cat alone (7.84 PPL) < Skip alone (6.71 PPL) < SkipCat (6.29 PPL) on LLaMA2-7B at 20% compression (Table 3)
  - Higher compression (30%+) increases accuracy drop exponentially; SkipCat limits to 6.34% vs 13.54%+ for baselines
  - BF16 does not solve overflow (Table 5: 11,294 PPL at 20%); column permutation is essential

- **Failure signatures:**
  - FP16 overflow in Ã'x₂ computation → Column permutation missing or A₁ still ill-conditioned
  - Higher-than-expected perplexity → Shared projection forced on incompatible matrices, or whitening calibration insufficient
  - Quantization degradation (>0.1 PPL increase) → Missing Hadamard transform and channel scaling (Table 6)

- **First 3 experiments:**
  1. **Reproduce ablation (Table 3):** Apply Cat-only, Skip-only, SkipCat on LLaMA2-7B at 20% compression; verify perplexity 7.84 → 6.71 → 6.29 progression
  2. **Numerical stability validation:** Run Skip without column permutation on LLaMA2-7B, measure FP16 overflow rate; add permutation, verify activation distribution matches Figure 3 (100× reduction in outliers)
  3. **Compression scaling curve:** Evaluate SkipCat at 20%, 30%, 40%, 60% compression on zero-shot tasks; plot accuracy drop to characterize the tradeoff frontier and identify deployment-appropriate operating points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Intra-Layer Shared Low-Rank Projection (Cat) be effectively applied to Mixture-of-Experts (MoE) or State Space Models (SSMs), where the strict assumption of multiple weight matrices sharing the exact same input vector may not hold?
- Basis in paper: [inferred] The method relies on the architectural constant where "q, k, and v matrices share the same input" and "gate and up matrices... receive the same input" (Section 3.2).
- Why unresolved: The experiments are restricted to dense Transformer architectures (LLaMA, Qwen) and do not address the routing or structural uniqueness of MoE/SSM layers.
- What evidence would resolve it: Compression results on MoE models (e.g., Mixtral) showing whether shared projections can be forced across expert weights without degrading the router's effectiveness.

### Open Question 2
- Question: Does the Strong Rank-Revealing QR (SRRQR) factorization introduce prohibitive computational overhead during the offline compression phase for models with extremely large hidden dimensions (e.g., 70B+ parameters)?
- Basis in paper: [inferred] Section 3.3 identifies SRRQR as the method to find well-conditioned submatrices for the "Skip" technique, but does not analyze the time complexity or latency of this decomposition step on massive matrices.
- Why unresolved: The paper evaluates 7B and 14B models; the decomposition cost may scale non-linearly or become a bottleneck for larger deployments.
- What evidence would resolve it: Timing analysis of the SRRQR decomposition step specifically on the weight matrices of a 70B model during the compression workflow.

### Open Question 3
- Question: Does the requirement for Hadamard transforms and channel scaling to stabilize quantization (Supplementary D) negate the inference latency gains of Block Skipping on hardware with limited compute capabilities?
- Basis in paper: [inferred] While the paper claims efficiency gains via "Block Skipping" (Abstract), Supplementary Section D admits that column permutation alone is "insufficient for stable low-precision quantization" and requires additional transforms.
- Why unresolved: Throughput gains are reported for A100 GPUs (Supplementary G), but the added arithmetic overhead of stabilization transforms might outweigh FLOP savings on edge devices or CPUs.
- What evidence would resolve it: On-device latency benchmarks comparing the end-to-end inference speed of SkipCat with and without the 8-bit quantization stabilization steps.

## Limitations
- Limited empirical validation of numerical stability failure modes when column permutation is inadequate
- Compression-performance tradeoff characterization is limited to specific model sizes and compression rates (20-40%)
- Whitening calibration procedure is referenced but not fully specified, creating potential reproducibility gaps

## Confidence
- **High Confidence:** Shared projection mechanism (Cat) - well-grounded in linear algebra and clearly validated across ablation studies
- **Medium Confidence:** Block skipping (Skip) numerical stability - strong theoretical foundation but limited empirical failure mode analysis
- **Medium Confidence:** End-to-end perplexity and accuracy improvements - consistently demonstrated but with compression-rate dependent degradation patterns

## Next Checks
1. **Extreme compression validation:** Evaluate SkipCat at 60-80% compression on zero-shot tasks to characterize the accuracy-perplexity tradeoff frontier and identify practical deployment limits
2. **Numerical failure mode analysis:** Systematically test block skipping without column permutation across different rank values and input distributions to quantify FP16 overflow probability
3. **Cross-architecture generalization:** Apply SkipCat to transformer architectures beyond LLaMA2 (e.g., GPT-2, OPT) to assess the universality of shared projection benefits across architectural variants