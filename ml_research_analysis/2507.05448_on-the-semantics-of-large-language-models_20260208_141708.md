---
ver: rpa2
title: On the Semantics of Large Language Models
arxiv_id: '2507.05448'
source_url: https://arxiv.org/abs/2507.05448
tags:
- language
- llms
- sense
- meaning
- frege
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines whether large language models (LLMs) like\
  \ ChatGPT understand language, focusing on semantics at the word and sentence level.\
  \ By analyzing LLMs\u2019 internal representations and drawing on classical semantic\
  \ theories by Frege and Russell, it explores the potential for semantic understanding\
  \ in LLMs."
---

# On the Semantics of Large Language Models

## Quick Facts
- **arXiv ID:** 2507.05448
- **Source URL:** https://arxiv.org/abs/2507.05448
- **Reference count:** 8
- **Primary result:** LLMs instantiate a form of Fregean "sense" through distributed representations, but lack direct reference to the external world

## Executive Summary
This paper examines whether large language models (LLMs) like ChatGPT understand language by analyzing their semantic capabilities at the word and sentence level. Using classical semantic theories by Frege and Russell, the author explores whether LLMs possess semantic understanding through their internal representations. The analysis identifies four stable elements in LLMs: probabilistic language modeling, distributed representations, neural networks, and large-scale models. The paper concludes that while LLMs cannot directly reference the external world, their distributed representations share key properties with Frege's notion of "sense," suggesting a form of semantic understanding exists within the models' internal structure.

## Method Summary
The paper employs theoretical analysis and philosophical argumentation rather than empirical experimentation. It examines general LLM architectures (particularly Transformer-based models) and their use of distributed representations and latent space vectors. The methodology involves mapping classical semantic concepts—particularly Frege's distinction between sense (Sinn) and reference (Bedeutung)—onto the technical properties of LLMs. No specific model architectures, versions, or experimental protocols are specified, making this a conceptual framework rather than a reproducible experimental study.

## Key Results
- LLMs instantiate a form of Fregean "sense" through distributed vector representations
- LLMs lack direct reference to the external world, creating a fundamental limitation in their semantic capabilities
- The four stable elements of LLMs (probabilistic modeling, distributed representations, neural networks, and scale) collectively enable compositional semantic understanding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs instantiate Fregean "sense" through distributed vector representations
- **Mechanism:** Tokens map to high-dimensional vector space (embeddings) that function as "modes of presentation" (Sense) because they are objective, shared, and context-dependent
- **Core assumption:** "Sense" can exist mathematically as relational geometry in vector space without requiring conscious grasp of the object
- **Break condition:** If vector representations fail to differentiate synonyms with different senses (e.g., "Morning Star" vs. "Evening Star" vectors being identical)

### Mechanism 2
- **Claim:** Probabilistic modeling forces acquisition of semantic structure through context prediction
- **Mechanism:** The model learns joint probability function P(x₁, ..., xₜ) over sequences, internalizing "totality of usage" to minimize error and capture word "sense"
- **Core assumption:** Statistical distribution of text is sufficiently dense to derive objective semantic relationships
- **Break condition:** If curse of dimensionality or sparse data causes rote memorization rather than generalizable semantic relationships

### Mechanism 3
- **Claim:** Neural network scale creates emergent capabilities enabling compositional semantics
- **Mechanism:** Critical mass of neurons forms subroutines and submodules, allowing composition of senses required for logical sentences
- **Core assumption:** Compositional semantic ability naturally emerges from increasing connectionist system complexity
- **Break condition:** If logical reasoning tasks fail to improve with scale, suggesting compositional "sense" cannot be approximated by statistical scaling

## Foundational Learning

- **Concept:** **Frege's Sense (Sinn) vs. Reference (Bedeutung)**
  - **Why needed here:** Central to evaluating whether LLM embeddings have "meaning" but no "truth value"
  - **Quick check question:** Can you explain why "Morning Star" and "Evening Star" have same *Reference* but different *Senses*?

- **Concept:** **Distributed Representations (Embeddings)**
  - **Why needed here:** Technical implementation of "Sense" - meaning is geometric (distances between vectors) rather than symbolic
  - **Quick check question:** How does vector space capture relationship between "King" and "Queen" differently than dictionary definition?

- **Concept:** **Autoregressive Modeling**
  - **Why needed here:** Explains dynamic "Context" - meaning derived from sequence of previous tokens, aligning with philosophical view that part's meaning depends on whole sentence
  - **Quick check question:** Why does autoregressive model necessarily require context to define word "sense"?

## Architecture Onboarding

- **Component map:** Tokenizer (Text → IDs) → Embeddings (IDs → Vectors/Fregean Sense) → Transformer/Attention (Contextualization of Sense) → Probability Distribution (Prediction)

- **Critical path:** Transformation of static embeddings into contextualized representations - "Sense" is not static vector but its representation after attention processing in sentence context

- **Design tradeoffs:**
  - **Sense vs. Reference:** Optimizes for internal coherence (Sense) and fluency, explicitly trades off verifiable truth (Reference)
  - **Probabilistic vs. Logical:** Approximates logical truth via probability, leading to hallucinations (errors in reference) as inherent feature

- **Failure signatures:**
  - **Hallucination:** Generates text with valid *Sense* (grammatically coherent) but invalid *Reference* (factually false)
  - **Bias Amplification:** Outputs biased associations learned from dataset as objective relations

- **First 3 experiments:**
  1. **Synonym Probe:** Compare vector distances of synonyms (e.g., "big" vs "large") against antonyms to verify "Sense" geometry aligns with human intuition
  2. **Contextual Shift Analysis:** Input word with multiple meanings (e.g., "bank") in different sentences, measure distance between output vectors to confirm context-sensitivity
  3. **Truth Value Classification:** Test on declarative sentences to see if model distinguishes probable truths from falsehoods, investigating if "Reference" can be approximated indirectly

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Large Multimodal Models (LMMs) resolve lack of direct reference by grounding distributed representations in non-textual sensorimotor data?
- **Basis in paper:** [explicit] Abstract/conclusion state future multimodal models may address referential limitations; page 11 discusses grounding in images
- **Why unresolved:** LMMs face mediated access to world and truth determination challenges, just as humans do
- **What evidence would resolve it:** LMM accurately maps internal vector representations to physical objects/properties for verification against external reality

### Open Question 2
- **Question:** To what extent can LLM internal distributed representations be transposed into logical forms to satisfy Russellian semantic requirements?
- **Basis in paper:** [explicit] Page 10 states it's "not excluded" that representations can be transposed into logical form
- **Why unresolved:** LLMs don't inherently break language into logical structures required by Russell's theory of descriptions
- **What evidence would resolve it:** Successful extraction of consistent, formal logical structures from latent space that align with semantic content

### Open Question 3
- **Question:** Do alignment techniques (e.g., RLHF) fundamentally alter internal semantic representations (sense), or merely constrain output behavior?
- **Basis in paper:** [inferred] Page 5 asserts alignment doesn't significantly alter internal representation
- **Why unresolved:** Fine-tuning on human feedback could theoretically shift vector space geometry (the "sense") rather than just filtering outputs
- **What evidence would resolve it:** Comparative analysis of latent space representations in base versus aligned models

### Open Question 4
- **Question:** Can "indirect reference" (words referring to other words/representations) provide sufficient basis for meaning without infinite regress?
- **Basis in paper:** [inferred] Pages 10-11 discuss indirect reference/semiosis but note risks of infinite regress
- **Why unresolved:** Unclear if system of signs referring only to other signs can stabilize into grounded meaning without external anchor
- **What evidence would resolve it:** Demonstrating closed system of textual references achieves stable semantic consistency equivalent to externally grounded reference

## Limitations
- No empirical methodology specified to test whether LLM representations satisfy Frege's criteria for "sense"
- Claims about distributed representations instantiating "sense" remain philosophical assertions without experimental verification
- Paper acknowledges LLMs lack reference to external world but doesn't specify how this limitation could be quantitatively measured

## Confidence

**Claim Confidence Labels:**
- LLMs instantiate Fregean "sense" through embeddings: **Medium confidence** (philosophically coherent but empirically unverified)
- Probabilistic modeling captures semantic structure: **Medium confidence** (mechanism plausible but not tested)
- Scale enables emergent compositional semantics: **Low confidence** (scaling claims not directly supported with evidence)

## Next Checks

1. **Empirical sense verification:** Test whether LLM embeddings satisfy Frege's three criteria for sense (context-dependency, objectivity, compositionality) using controlled linguistic stimuli

2. **Reference approximation study:** Design experiments to measure whether multimodal LLMs can approximate reference through grounding, directly testing the paper's speculation about future capabilities

3. **Cross-linguistic validation:** Apply semantic analysis framework to non-English LLMs to test whether sense/reference distinction holds across languages and cultural contexts