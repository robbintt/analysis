---
ver: rpa2
title: 'Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning'
arxiv_id: '2601.21894'
source_url: https://arxiv.org/abs/2601.21894
tags:
- code
- complexity
- reasoning
- language
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether structural properties of code
  used during fine-tuning influence the reasoning abilities of large language models
  (LLMs). The authors construct two complexity-controlled datasets: one varying solution
  structure for identical problems, and another varying problem difficulty, using
  cyclomatic complexity and logical lines of code as metrics.'
---

# Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning

## Quick Facts
- arXiv ID: 2601.21894
- Source URL: https://arxiv.org/abs/2601.21894
- Reference count: 36
- Key outcome: Reasoning gains from code fine-tuning are non-uniform and non-monotonic, peaking at intermediate complexity levels and degrading at both low and high extremes.

## Executive Summary
This paper investigates whether structural properties of code used during fine-tuning influence the reasoning abilities of large language models (LLMs). The authors construct two complexity-controlled datasets: one varying solution structure for identical problems, and another varying problem difficulty, using cyclomatic complexity and logical lines of code as metrics. Across multiple model families and reasoning benchmarks, they find that reasoning gains from code fine-tuning are non-uniform and non-monotonic, peaking at intermediate complexity levels and degrading at both low and high extremes. Control datasets mixing all complexity levels are rarely optimal; in 83% of experiments, restricting fine-tuning to a specific complexity range yields better reasoning performance. These results challenge the assumption that greater code diversity is inherently beneficial and highlight the importance of carefully selecting structurally appropriate code for improving LLM reasoning.

## Method Summary
The study uses two datasets: CODENET (solution-driven) with 8,087 samples per split varying solution structure while holding problems constant, and INSTRUCT (problem-driven) with 8,087 samples per split varying problem difficulty. Both datasets are divided into 12 splits (5 complexity levels + 1 control × 2 metrics). Code complexity is measured using cyclomatic complexity (CC) and logical lines of code (LLOC). The authors fine-tune six model families (Qwen2.5, Llama-3.1/3.2, Mistral) using LoRA adapters (rank=16, α=16, lr=2e-5) for 2 epochs on each complexity split separately. Models are evaluated on six reasoning benchmarks (GSM8K, MATH401, MATH500, GPQA, BBEH-MINI, HLE) using greedy decoding with step-by-step reasoning prompts.

## Key Results
- Reasoning gains from code fine-tuning are non-monotonic, peaking at intermediate complexity levels (both CC and LLOC)
- Control datasets mixing all complexity levels are rarely optimal; in 83% of experiments, restricting fine-tuning to specific complexity ranges yields better performance
- Mistral-7B shows a distinct U-shaped curve where both very simple and very complex code improve reasoning, unlike other model families
- Optimal complexity ranges are architecture-dependent, with different model families showing different peak performance at different complexity levels

## Why This Works (Mechanism)

### Mechanism 1
Code structural complexity functions as an implicit chain-of-thought scaffold during fine-tuning. Code with branching logic and control flow (captured by cyclomatic complexity) exposes models to structured problem decomposition patterns. This mirrors explicit CoT prompting but internalizes the structure during training rather than inference. Control flow patterns in code transfer to multi-step reasoning in natural language tasks. If reasoning gains disappear when controlling for code length, the mechanism may be verbosity rather than structure.

### Mechanism 2
Reasoning gains from code complexity are non-monotonic, with optimal performance at intermediate levels. Very simple code lacks sufficient structure to teach decomposition. Very complex code introduces brittle control flow and optimization difficulty that obscures the reasoning signal. Intermediate complexity provides tractable scaffolding. Models have a "complexity capacity" beyond which additional structure degrades learning. If larger models show monotonic improvement with complexity, the effect may be capacity-dependent rather than inherent.

### Mechanism 3
Restricting training data to model-specific complexity ranges outperforms diverse code corpora. Different model architectures and pretraining histories create different optimal complexity windows. Mixed-complexity data introduces noise relative to each model's effective learning range. Models have architecture-dependent "sweet spots" for structural complexity absorption. If optimal complexity ranges shift dramatically with dataset scale, this may be a small-data artifact.

## Foundational Learning

- **Concept: Cyclomatic Complexity (CC)**
  - Why needed here: Primary metric for quantifying structural code complexity; measures number of independent execution paths through a program.
  - Quick check question: Can you explain why CC=1 represents linear code with no branching?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: All experiments use LoRA fine-tuning; understanding adapter mechanics is essential for reproduction.
  - Quick check question: What is the relationship between LoRA rank (r=16) and the number of trainable parameters?

- **Concept: Solution-driven vs. Problem-driven Complexity**
  - Why needed here: The paper disentangles complexity from code style vs. task difficulty through these two complementary settings.
  - Quick check question: Why does holding the problem constant (solution-driven) isolate code structure as a variable?

## Architecture Onboarding

- **Component map**: Dataset construction (CODENET/INSTRUCT) → Static analysis (CC/LLOC computation) → Fine-tuning (LoRA adapters) → Evaluation (reasoning benchmarks)
- **Critical path**: 1. Extract code from source datasets → compute CC/LLOC → bin into complexity splits; 2. Fine-tune each model on each split separately; 3. Evaluate on all 6 benchmarks; compare against NL baseline and CTRL (mixed complexity)
- **Design tradeoffs**: Small dataset size (8,087 samples) enables controlled comparison but may not scale; Using max CC across functions aggregates complexity; alternative (mean, sum) may capture different signal; Greedy decoding (temp=0) maximizes reproducibility but may underestimate model capability
- **Failure signatures**: U-shaped performance curve (Mistral-7B): model benefits from extremes but not intermediate complexity—suggests different complexity absorption pattern; Near-perfect negative correlations (Llama models on high-CC splits): high complexity actively degrades reasoning; NL baseline outperforming code splits: complexity mismatch or negative transfer
- **First 3 experiments**: 1. Reproduce the CC sweep on a single model (e.g., Qwen2.5-7B) on CODENET to verify intermediate-complexity peak exists; 2. Add a size-controlled LLOC experiment to disentangle structural complexity from code length; 3. Test whether the optimal CC range transfers across languages (train on Python, evaluate whether same CC window helps)

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Why does Mistral-7B exhibit a "U-shaped" performance curve where reasoning benefits from both very simple and very complex code, unlike the peak-at-intermediate pattern seen in other model families?
**Basis in paper**: Section 4.2 notes the distinct behavior and states, "We leave a deeper investigation of these effects to future work."
**Why unresolved**: The paper only observes the anomaly and speculates about architectural differences without providing a causal explanation.
**What evidence would resolve it**: Ablation studies isolating Mistral's architectural components or pre-training data to identify the source of this divergent complexity sensitivity.

### Open Question 2
**Question**: Can hybrid metrics (combining data flow, semantic patterns, and control flow) better capture the reasoning utility of code than static metrics like cyclomatic complexity?
**Basis in paper**: The Conclusion suggests future work should explore "more expressive measures of code structure" because static metrics "do not exhaust the space of code properties."
**Why unresolved**: The study is limited to cyclomatic complexity and logical lines of code, which may conflate verbosity with structural reasoning value.
**What evidence would resolve it**: Experiments using hybrid complexity metrics to curate datasets, testing if they yield higher or more consistent reasoning gains than the CC/LLOC baselines.

### Open Question 3
**Question**: Does the finding that "mixed-complexity control datasets are rarely optimal" hold true for datasets significantly larger than the 8,087 samples used in this study?
**Basis in paper**: The Limitations section notes the use of small datasets for controlled comparison and acknowledges "effects may differ at larger scales."
**Why unresolved**: It is unclear if the benefits of restricting complexity are an artifact of the small data regime or a fundamental property of model learning.
**What evidence would resolve it**: Repeating the complexity-restriction experiments on datasets ranging from 100k to 1M samples to see if the optimal strategy shifts toward mixing.

## Limitations

- Relatively small dataset size (8,087 samples per split) may not capture full complexity distribution and could lead to overfitting or variance in results
- Study uses max cyclomatic complexity across functions as aggregation method, which may miss structural patterns captured by mean or sum aggregations
- Optimal complexity ranges appear architecture-dependent, but the sample of 6 models may not be representative of broader model families
- Study focuses on Python, JavaScript, and Java, leaving open whether results generalize to other programming languages or non-coding domains

## Confidence

**High confidence**: The non-monotonic relationship between code complexity and reasoning performance (intermediate complexity peaks) is well-supported by multiple model families and benchmark combinations. The finding that control datasets are rarely optimal (83% of experiments) is robust across settings.

**Medium confidence**: The mechanism by which code structure transfers to reasoning ability is plausible but not definitively proven. While the paper shows correlation between structural complexity and reasoning gains, the causal mechanism (chain-of-thought scaffolding) remains theoretical. The optimal complexity ranges may shift with larger datasets or different model scales.

**Low confidence**: The claim that all models benefit from complexity restriction (vs. diverse training) may be specific to the 6 tested models and dataset scale. The U-shaped performance curve observed in Mistral-7B but not other models suggests individual model characteristics play a significant role that the paper doesn't fully explain.

## Next Checks

1. **Scale validation**: Replicate the study with 10× larger datasets (80,870 samples per split) to test whether the non-monotonic complexity-performance relationship persists at scale, or whether it's an artifact of limited data.

2. **Mechanism validation**: Conduct ablation studies controlling for code length—create complexity-matched pairs where one has high structural complexity and low LLOC, the other has low structural complexity and high LLOC. This would isolate whether reasoning gains come from structure or verbosity.

3. **Architecture validation**: Test the optimal complexity ranges across a broader model spectrum (including frontier models like GPT-4, Claude, and smaller models down to 1B parameters) to determine whether the architecture-dependent "sweet spots" are a general phenomenon or specific to the tested model families.