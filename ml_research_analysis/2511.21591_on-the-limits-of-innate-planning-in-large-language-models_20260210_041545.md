---
ver: rpa2
title: On the Limits of Innate Planning in Large Language Models
arxiv_id: '2511.21591'
source_url: https://arxiv.org/abs/2511.21591
tags:
- move
- moves
- state
- slide
- board
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the intrinsic planning and state-tracking
  capabilities of large language models using the 8-puzzle, a classic task that requires
  reasoning over evolving states and goal-directed planning. Four models are tested
  under three prompting strategies (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought)
  and with tiered feedback, followed by an external move validator condition that
  removes the need to determine valid moves.
---

# On the Limits of Innate Planning in Large Language Models

## Quick Facts
- **arXiv ID:** 2511.21591
- **Source URL:** https://arxiv.org/abs/2511.21591
- **Reference count:** 40
- **Primary result:** Current LLMs show substantial limitations in planning without external tools, as evidenced by poor performance on the 8-puzzle task even with feedback and simplified validation

## Executive Summary
This paper investigates the intrinsic planning and state-tracking capabilities of large language models using the 8-puzzle, a classic task requiring reasoning over evolving states and goal-directed planning. Four models are tested under three prompting strategies (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered feedback, followed by an external move validator condition that removes the need to determine valid moves. Across all settings, models exhibit two dominant deficits: brittle internal state representations leading to frequent invalid moves, and weak heuristic planning resulting in loops or moves that do not reduce the distance to the goal state. Despite corrective feedback and simplified move validation, no model achieves reliable performance; success rates remain low, and successful runs are long and computationally expensive.

## Method Summary
The study employs the 8-puzzle as a benchmark for evaluating LLM planning capabilities. Four different models are tested using three prompting strategies: Zero-Shot, Chain-of-Thought, and Algorithm-of-Thought. Models receive tiered feedback to help correct errors, and a separate condition uses an external move validator to eliminate the need for determining valid moves. The evaluation focuses on move quality metrics including loop detection and invalid move frequency, as well as overall task completion rates. The experimental design systematically varies the conditions to isolate whether limitations stem from state tracking, heuristic planning, or both.

## Key Results
- Across all models and prompting strategies, success rates remain low even with corrective feedback
- External move validation (removing the need to determine valid moves) does not substantially improve performance
- Models exhibit two primary deficits: brittle internal state representations and weak heuristic planning
- Successful runs are characterized by being both rare and computationally expensive

## Why This Works (Mechanism)
The study demonstrates that current LLMs struggle with maintaining accurate internal representations of evolving problem states and applying effective heuristics to guide search toward goals. When asked to plan moves in the 8-puzzle, models frequently lose track of the current state configuration, leading to invalid moves. Even when valid moves are provided externally, models still struggle to plan efficient sequences toward the goal, often getting stuck in loops or making moves that increase the distance to the solution. This suggests fundamental limitations in the models' ability to perform structured search and maintain coherent state representations across multiple reasoning steps.

## Foundational Learning

**State Tracking** - Why needed: Essential for maintaining accurate representations of evolving problem configurations across multiple reasoning steps
Quick check: Can the model correctly identify the current configuration after a series of moves?

**Heuristic Planning** - Why needed: Required to evaluate which moves bring the system closer to the goal state
Quick check: Does the model consistently choose moves that reduce distance to the goal?

**Structured Search** - Why needed: Necessary for systematic exploration of the solution space rather than random or looping behavior
Quick check: Can the model avoid revisiting previously explored states?

## Architecture Onboarding

**Component Map:** LLM models -> Prompting strategy -> State representation -> Move selection -> Feedback loop

**Critical Path:** Problem state representation → Move generation → State update → Goal evaluation → Next move decision

**Design Tradeoffs:** Pure LLM reasoning vs. external tooling; complexity of prompting strategies vs. model capability; feedback mechanisms vs. computational cost

**Failure Signatures:** Invalid moves (state tracking failure), looping behavior (heuristic planning failure), long solution paths (inefficient search)

**3 First Experiments:**
1. Test whether fine-tuning on planning-specific data improves state tracking accuracy
2. Evaluate whether explicit memory mechanisms help maintain coherent state representations
3. Compare performance on simpler planning tasks to establish baseline capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- The 8-puzzle is a highly structured domain that may not generalize to more complex, open-ended planning tasks
- Evaluation focuses on relative move quality rather than absolute task completion, which could conflate model limitations with task difficulty
- Does not investigate whether fine-tuning on planning tasks or incorporating memory mechanisms could improve performance

## Confidence
- Intrinsic LLM planning limitations: **High**
- Feedback and simplified validation do not substantially improve performance: **High**
- Two dominant deficits (state representation and heuristic planning): **Medium** (based on 8-puzzle evidence)
- Generalizability to other planning domains: **Low** (not directly tested)

## Next Checks
1. Test the same experimental protocol on a larger sliding puzzle (15-puzzle) to assess scalability of the observed limitations
2. Compare performance when models are augmented with explicit state tracking mechanisms versus pure LLM-based approaches
3. Evaluate whether fine-tuning on planning-specific data improves state tracking and heuristic planning capabilities in this task