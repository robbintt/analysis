---
ver: rpa2
title: 'GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data'
arxiv_id: '2505.17082'
source_url: https://arxiv.org/abs/2505.17082
tags:
- darija
- data
- language
- reasoning
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We show that a rigorously quality-over-quantity alignment strategy\
  \ can surface fluent Darija while safeguarding the backbone's cross-lingual reasoning\
  \ at a sliver of the usual compute. We translate three compact instruction suites\u2014\
  LIMA 1 K, DEITA 6 K and TULU 50 K\u2014into Darija, preserve 20 % of the English\
  \ originals, and add mathematics, coding and scientific prompts."
---

# GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data

## Quick Facts
- **arXiv ID:** 2505.17082
- **Source URL:** https://arxiv.org/abs/2505.17082
- **Reference count:** 40
- **Primary result:** LoRA-tuned Gemma 3-27B achieves 61.6% on DarijaMMLU, matching Atlas-Chat while retaining strong English reasoning, trained in 48 GPU-hours.

## Executive Summary
GemMaroc introduces an efficient fine-tuning approach that unlocks Moroccan Darija proficiency in large language models using minimal data. The method employs LoRA adapters to fine-tune Gemma 3 models on translated instruction datasets while preserving 20% of the original English content to prevent catastrophic forgetting. The approach demonstrates that quality and reasoning density in training data matter more than sheer volume, achieving strong Darija performance while maintaining the base model's mathematical and general reasoning capabilities. The entire training process is completed in just 48 GPU-hours, representing a Green AI pathway to inclusive language technology.

## Method Summary
The method centers on supervised fine-tuning (SFT) of Gemma 3 models using LoRA adapters. Three instruction datasets (LIMA 1K, DEITA 6K, and TULU 50K) are translated into Darija using Gemini 2.0 Flash API, with approximately 20% of the original English content preserved. The translated data is filtered for quality and mixed with English samples to form the training corpus. LoRA hyperparameters vary by model size (rank 32/16, alpha 64/32 for 4B/27B respectively), and training uses a Vicuna-style conversation template with bf16 precision and 2048 token context. The approach is validated through comprehensive benchmarking across Darija and English tasks.

## Key Results
- Gemma 3-4B trained on 5K mixed instructions improves DarijaMMLU from 32.8% to 42.7%, reaching 47.5% with reasoning-dense TULU data
- GemMaroc-27B matches Atlas-Chat on DarijaMMLU (61.6%) and achieves 60.5 on DarijaHellaSwag versus Atlas-Chat's 48.4
- The model retains strong GSM8K performance and English MMLU scores, demonstrating preserved reasoning capabilities
- Entire model training completed in just 48 GPU-hours, exemplifying Green AI efficiency

## Why This Works (Mechanism)
The approach succeeds by leveraging the "superficial alignment hypothesis," which posits that LLMs acquire most knowledge during pretraining, with SFT primarily teaching stylistic alignment. By using high-quality, reasoning-dense datasets and preserving a portion of the original English data, the model acquires Darija fluency without sacrificing its general reasoning abilities. LoRA's parameter-efficient fine-tuning enables this transformation on consumer hardware while maintaining the original model's weights frozen, preventing catastrophic forgetting of the base knowledge.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: This core technique enables efficient fine-tuning by injecting small, trainable rank-decomposition matrices into transformer layers. *Why needed:* Allows learning Darija on consumer-grade hardware (48 GPU-hours) while preserving original weights. *Quick check:* How does freezing main model weights and training only small adapter matrices prevent catastrophic forgetting?
- **Superficial Alignment Hypothesis**: This LIMA-derived theory posits that LLMs learn most knowledge during pretraining, with SFT teaching them desired style/format. *Why needed:* Provides theoretical foundation for minimal data approach, explaining why tiny high-quality datasets suffice. *Quick check:* Why does this suggest dataset quality and diversity matter more than volume for SFT?
- **Catastrophic Forgetting & Mixed-Language Training**: The key challenge addressed is maintaining original language proficiency while learning new languages. *Why needed:* Explains why mixing 20% English data is crucial for preserving English reasoning abilities. *Quick check:* Why is preserving English performance crucial for maintaining general reasoning abilities?

## Architecture Onboarding
- **Component map:** Gemma 3 transformer decoder -> LoRA adapter modules injected into attention layers -> Translated and mixed instruction datasets (70-75% Darija, 25-30% English) -> Supervised fine-tuning objective
- **Critical path:**
    1. **Data Curation:** Select high-quality source datasets (LIMA, DEITA, TULU) - most critical step
    2. **Translation & Filtering:** Use Gemini 2.0 Flash API with careful prompt to produce Darija text, filter non-English sources and long sequences, preserve ~20% English data
    3. **Efficient Fine-Tuning:** Apply LoRA to Gemma 3, train on mixed-language corpus using SFT objective
    4. **Evaluation:** Test on Darija-specific (DarijaMMLU, DarijaHellaSwag) and English (MMLU, GSM8K) benchmarks to ensure dialect acquisition without catastrophic forgetting
- **Design tradeoffs:**
    - **Quality vs. Quantity:** Smaller curated dataset reduces compute cost and carbon footprint but requires more upfront data selection effort
    - **Darija Proficiency vs. General Reasoning:** 20% English mix balances dialect specialization with reasoning preservation, sacrificing some Darija potential for stability
    - **Efficiency vs. Performance:** LoRA chosen for extreme efficiency, accepting potentially lower ceiling for drastically lower resource floor
- **Failure signatures:**
    - **Forgetting English:** GSM8K/MMLU score drops indicate 20% English ratio may be too low or learning rate/epochs too high
    - **Poor Darija Fluency:** English/MSA responses indicate dataset too small, low quality, or translation failed to capture dialect nuance
    - **Reasoning Degradation:** GSM8K drop suggests reasoning-dense TULU data was not effectively translated or integrated
- **First 3 experiments:**
    1. **Baseline & Minimal Viable Model:** Fine-tune Gemma 3-4B on LIMA-1K (translated, 20% English), evaluate on DarijaMMLU/MMLU
    2. **Scale Data for Conversational Competence:** Use DEITA-6K on same 4B model, test if increased data diversity/volume improves Darija without harming English
    3. **Add Reasoning and Scale Compute:** Move to Gemma 3-27B with full TULU-50K, test if reasoning-dense data unlocks higher dialect proficiency and cross-lingual reasoning

## Open Questions the Paper Calls Out
- **Open Question 1:** Does replacing SentencePiece tokenizer with a script-aware alternative significantly improve Darija fluency or efficiency? The paper notes the model retains the original SentencePiece tokenizer, which is not optimized for Darija's script and linguistic nuances, leaving tokenizer modification as future work.
- **Open Question 2:** Can the minimal-data alignment recipe close performance gaps in sentiment analysis and summarization without sacrificing reasoning capabilities? The study prioritized reasoning-dense TULU data, which boosted math and logic but may under-represent patterns needed for sentiment and summarization.
- **Open Question 3:** Does the GemMaroc quality-over-quantity alignment strategy transfer effectively to other low-resource North African dialects? The paper validated the method strictly on Moroccan Darija, and it's unconfirmed if the pipeline works for dialects with different linguistic features or less pretraining exposure.

## Limitations
- The method depends heavily on Gemini 2.0 Flash API for translation, making independent verification of translation fidelity difficult
- The "minimal data" claim rests on the debated "superficial alignment hypothesis," which may not generalize to all languages or architectures
- Evaluation is limited to specific benchmarks (DarijaMMLU, HellaSwag, GSM8K, MMLU) that don't cover the full spectrum of potential Darija use cases

## Confidence
- **High Confidence:** LoRA fine-tuning on small mixed-language dataset efficiently adds Darija proficiency while preserving English reasoning
- **Medium Confidence:** Dataset quality and reasoning density are more important than volume for SFT; 20% English mix prevents catastrophic forgetting
- **Low Confidence:** This represents the absolute minimal amount of data needed; full dialectal authenticity of translations is independently verifiable

## Next Checks
1. **Translation Fidelity Audit:** Conduct blind evaluation where native Darija speakers rate translated prompts/responses for dialectal accuracy, naturalness, and faithfulness to original meaning
2. **English Retention Ratio Sweep:** Re-run fine-tuning with different English data ratios (10%, 20%, 30%, 40%) to empirically determine optimal balance point for preventing forgetting while maximizing Darija proficiency
3. **Cross-Lingual Generalization Probe:** Design test suite of reasoning problems presented in Darija but requiring general knowledge application to test true cross-lingual reasoning beyond MMLU