---
ver: rpa2
title: Flexible Locomotion Learning with Diffusion Model Predictive Control
arxiv_id: '2510.04234'
source_url: https://arxiv.org/abs/2510.04234
tags:
- diffusion
- planning
- reward
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diffusion-MPC, a novel framework that leverages
  diffusion models as generative priors for model predictive control in legged locomotion.
  The method addresses the challenge of achieving flexible and adaptable behavior
  synthesis at test time, overcoming the limitations of fixed RL policies and classical
  MPC's reliance on accurate dynamics models.
---

# Flexible Locomotion Learning with Diffusion Model Predictive Control

## Quick Facts
- arXiv ID: 2510.04234
- Source URL: https://arxiv.org/abs/2510.04234
- Reference count: 40
- One-line primary result: Novel Diffusion-MPC framework achieves flexible, adaptive quadruped locomotion by combining diffusion models with model predictive control, validated on real Unitree Go2 hardware.

## Executive Summary
This paper introduces Diffusion-MPC, a novel framework that leverages diffusion models as generative priors for model predictive control in legged locomotion. The method addresses the challenge of achieving flexible and adaptable behavior synthesis at test time, overcoming the limitations of fixed RL policies and classical MPC's reliance on accurate dynamics models. Diffusion-MPC jointly predicts future states and actions, incorporating reward-based planning and constraint projection at each reverse step to synthesize trajectories that satisfy task objectives while remaining within physical limits. To enhance adaptability beyond imitation pretraining, the authors propose an interactive training algorithm that collects trajectories from the planner, filters and reweights them by realized returns, and updates the denoiser. The framework is validated on a real-world Unitree Go2 quadruped, demonstrating strong locomotion performance and flexible adaptation to new tasks such as base height variation, joint limit restriction, energy saving, joint acceleration/velocity regularization, and balancing under external disturbances. Real-world experiments show up to 20% energy savings and successful deployment on challenging terrains including soft uneven grass and grassy slopes.

## Method Summary
The Diffusion-MPC framework combines diffusion models with model predictive control to achieve flexible locomotion planning. The core idea is to use a diffusion model as a generative prior that can synthesize trajectories satisfying multiple constraints while optimizing for task rewards. The method performs reverse diffusion steps where each step jointly predicts future states and actions, incorporating reward-based planning and constraint projection. An interactive training algorithm is proposed to improve adaptability: trajectories generated by the planner are collected, filtered by realized returns, reweighted, and used to update the denoiser. This allows the system to learn from its own planning rather than relying solely on imitation learning. The framework is validated on a Unitree Go2 quadruped, demonstrating performance on various locomotion tasks and terrain types while achieving significant energy savings.

## Key Results
- Real-world validation on Unitree Go2 quadruped showing strong locomotion performance across multiple tasks
- Successful adaptation to new objectives including base height variation, joint limit restriction, and energy saving
- Up to 20% energy savings demonstrated in real-world experiments
- Robust performance on challenging terrains including soft uneven grass and grassy slopes

## Why This Works (Mechanism)
Diffusion-MPC works by leveraging the generative capabilities of diffusion models as priors for trajectory synthesis in model predictive control. The diffusion model acts as a powerful generative model that can produce physically plausible trajectories, while the MPC framework provides planning and optimization capabilities. By performing reverse diffusion steps with reward-based planning and constraint projection at each step, the system can synthesize trajectories that satisfy both physical constraints and task objectives. The interactive training algorithm further enhances adaptability by allowing the system to learn from its own generated trajectories, filtered by their realized performance. This combination enables flexible behavior synthesis at test time that goes beyond the limitations of fixed RL policies or classical MPC methods that require accurate dynamics models.

## Foundational Learning
- Diffusion models as generative priors - why needed: Traditional MPC requires accurate dynamics models that are difficult to obtain for complex systems like legged robots. Quick check: Can diffusion models generate physically plausible trajectories without explicit dynamics modeling?
- Model predictive control with learned priors - why needed: Pure RL policies are fixed at test time and struggle with new tasks. Quick check: Can MPC framework provide planning capabilities while using learned priors for flexible behavior synthesis?
- Interactive training with return-based filtering - why needed: Imitation learning alone cannot adapt to new tasks beyond the training distribution. Quick check: Does filtering and reweighting trajectories by realized returns improve the quality and adaptability of the learned policy?

## Architecture Onboarding

Component map: State-Action space -> Diffusion denoiser -> MPC planner -> Trajectory optimizer -> Robot execution

Critical path: Real-time control loop where MPC planner queries diffusion denoiser at each time step to generate optimal trajectories, which are then executed on the robot.

Design tradeoffs: The framework balances between using learned generative priors (diffusion model) and analytical planning (MPC), trading off computational complexity for flexibility and adaptability. The interactive training algorithm adds an additional learning phase but enables better performance on new tasks.

Failure signatures: Poor performance may arise from inadequate diffusion model training (generating physically implausible trajectories), suboptimal MPC planning (failing to optimize for task objectives), or ineffective interactive training (not properly filtering beneficial trajectories).

First experiments:
1. Validate diffusion model can generate physically plausible locomotion trajectories in simulation
2. Test MPC planner with fixed diffusion model priors on basic locomotion tasks
3. Evaluate interactive training algorithm by comparing performance before and after training with self-generated trajectories

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on quality of learned diffusion denoiser, requiring extensive training data
- Generalizability to other quadruped platforms and complex terrains remains unclear
- Computational requirements for real-time deployment and integration within MPC loops not thoroughly discussed

## Confidence

High confidence in the core technical contribution of combining diffusion models with MPC for flexible locomotion planning, supported by strong empirical results on real hardware.

Medium confidence in the effectiveness of the interactive training algorithm for improving adaptability, as the paper provides limited ablation studies on the impact of different trajectory filtering strategies.

Medium confidence in the energy efficiency claims, as the paper shows promising results but does not provide extensive comparisons with state-of-the-art energy-efficient locomotion methods.

## Next Checks

1. Conduct systematic ablation studies on the interactive training algorithm, comparing different trajectory filtering and reweighting strategies to quantify their impact on final performance and adaptability.

2. Perform extensive testing across multiple quadruped platforms and diverse terrain types to evaluate the framework's generalizability and robustness to hardware differences and environmental variations.

3. Analyze the computational requirements and latency of the diffusion-MPC framework in real-time deployment scenarios, comparing against traditional MPC and RL approaches to identify potential bottlenecks and optimization opportunities.