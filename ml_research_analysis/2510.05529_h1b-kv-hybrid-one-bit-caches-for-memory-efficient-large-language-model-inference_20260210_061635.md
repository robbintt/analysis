---
ver: rpa2
title: 'H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference'
arxiv_id: '2510.05529'
source_url: https://arxiv.org/abs/2510.05529
tags:
- cache
- memory
- h1b-kv
- quantization
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory bottleneck of caching key-value
  (KV) pairs during autoregressive decoding in large language models (LLMs), which
  grows linearly with sequence length and makes long-context inference memory-bound.
  The proposed Hybrid One-Bit KV Cache (H1B-KV) compresses the KV cache by representing
  key vectors as 1-bit binary sketches using random projections and compressing value
  vectors with 4-bit quantization.
---

# H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference

## Quick Facts
- arXiv ID: 2510.05529
- Source URL: https://arxiv.org/abs/2510.05529
- Authors: Harshil Vejendla
- Reference count: 2
- Primary result: 70x KV cache compression with <0.1% parameter finetuning while matching full-precision performance

## Executive Summary
H1B-KV addresses the memory bottleneck of caching key-value pairs during autoregressive decoding in large language models. The method compresses KV caches by representing key vectors as 1-bit binary sketches using random projections and compressing value vectors with 4-bit quantization. This hybrid approach enables a 70x reduction in cache size while maintaining perplexity and downstream task performance through lightweight finetuning of less than 0.1% of parameters.

## Method Summary
The proposed method uses binary sketches for key vectors (s_k = sign(Rk) where R is a Gaussian random matrix) and 4-bit affine quantization for value vectors. Attention is computed via normalized Hamming inner product between query and key sketches. The approach requires finetuning only the value projection layers and a global temperature parameter, achieving state-of-the-art quality-per-byte efficiency while providing significant speedups and energy savings on edge devices.

## Key Results
- Achieves 70x cache compression (e.g., 4.3GB → 58.7MB for 7B model with 8k context)
- Matches full-precision perplexity and downstream accuracy after <0.1% parameter finetuning
- Provides 5x speedup and 2x energy savings on edge devices
- State-of-the-art quality-per-byte (QpB) efficiency

## Why This Works (Mechanism)
The method leverages the Johnson-Lindenstrauss lemma to compress high-dimensional key vectors into binary sketches while preserving semantic similarity for attention computation. The random projection matrix R maps d-dimensional keys to b-dimensional binary sketches where b << d. Attention scores are computed via normalized Hamming inner product, which is efficient using XNOR and POPCOUNT operations. Value vectors are compressed using 4-bit affine quantization, and lightweight finetuning of projection layers and temperature parameter restores performance.

## Foundational Learning
- **Random Projection Sketching**: Projects high-dimensional vectors to lower dimensions while approximately preserving pairwise distances. Needed because it enables dramatic key compression without losing semantic relationships essential for attention. Quick check: Verify that inner products in the projected space correlate with original inner products for your dataset.
- **Binary Attention Computation**: Uses normalized Hamming distance (XNOR + POPCOUNT) instead of floating-point dot products. Needed because it enables efficient attention computation on binary sketches. Quick check: Confirm that (1/b) * s_q^T s_k produces reasonable attention scores when s vectors are binary.
- **Per-Tensor Affine Quantization**: Scales and shifts values to fit 4-bit representation using learnable scale λ and zero-point z. Needed because it provides a balance between compression ratio and value fidelity. Quick check: Ensure dequantized values are within acceptable error bounds of original values.
- **Light Parameter Finetuning**: Updates only value projection layers and temperature τ (<0.1% parameters). Needed because it restores performance without requiring full model retraining. Quick check: Monitor perplexity on validation set to ensure finetuning converges within 2 epochs.
- **Quality-per-Byte Metric**: QpB = 1/(PPL × MB) measures compression efficiency. Needed because it captures the trade-off between model quality and memory footprint. Quick check: Calculate QpB for different compression schemes to compare efficiency.

## Architecture Onboarding

**Component Map**: Input tokens -> Embedding layers -> H1B-KV sketch/quantization -> Attention computation -> Output logits

**Critical Path**: The key bottleneck is the random projection and binary conversion step for keys, followed by the quantized value computation and attention score calculation. The critical operations are the matrix multiplication for sketching and the XNOR-POPCOUNT for attention.

**Design Tradeoffs**: The method trades computational precision for memory efficiency. Binary sketches reduce key storage by 32x but require careful temperature scaling to maintain attention quality. 4-bit quantization reduces value storage by 8x but can introduce quantization noise that finetuning must compensate for.

**Failure Signatures**: High perplexity (>15) indicates temperature τ is not properly finetuned or gradients aren't flowing through the quantization path. Attention collapse with b < 64 suggests the sketch width is insufficient to capture semantic relationships. Value quantization artifacts manifest as degraded downstream task performance despite acceptable perplexity.

**3 First Experiments**:
1. Implement binary key sketching with fixed Gaussian projection matrix R (b=256) and verify attention scores via XNOR+POPCOUNT match expectations
2. Add 4-bit per-tensor affine quantization for values and test dequantization accuracy on validation set
3. Finetune V_proj layers and temperature τ on WikiText-2 with AdamW (LR=1e-4, 2 epochs) and evaluate perplexity recovery

## Open Questions the Paper Calls Out
- Can H1B-KV be successfully extended to multi-modal models with different data distributions?
- Does jointly optimizing the random projection matrix R yield further compression or quality gains?
- Can the sharp performance degradation at sketch widths b < 64 be mitigated through architectural changes?

## Limitations
- Application to multi-modal models with different data distributions remains unexplored
- Current process uses fixed random projection matrix R rather than jointly optimizing it
- Sharp performance degradation occurs at sketch widths b < 64, limiting compression potential

## Confidence

**High confidence**: Core algorithmic approach (binary sketching for keys, 4-bit quantization for values, attention via normalized Hamming inner product) is well-specified and theoretically sound. The 70x compression claim and perplexity matching on WikiText-2/Penn Treebank are supported.

**Medium confidence**: Downstream task performance claims are plausible given perplexity results but depend on exact finetuning protocol details not fully specified. The 5x speedup and 2x energy savings are reasonable extrapolations from memory reduction.

**Low confidence**: Exact trade-off curve between sketch width b and downstream performance, and sensitivity to quantization calibration method, cannot be verified without additional experiments.

## Next Checks
1. Implement and validate 4-bit value quantization with different calibration methods to determine which best preserves downstream task performance while maintaining 70x compression
2. Conduct ablation studies varying binary sketch width b (32, 64, 128, 256) to identify minimum effective width maintaining perplexity <8 and downstream accuracy within 2%
3. Measure actual inference latency and energy consumption on edge hardware (Raspberry Pi/Jetson Nano) to verify claimed 5x speedup and 2x energy savings