---
ver: rpa2
title: Learning Peer Influence Probabilities with Linear Contextual Bandits
arxiv_id: '2510.19119'
source_url: https://arxiv.org/abs/2510.19119
tags:
- influence
- regret
- probabilities
- learning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Learning Peer Influence Probabilities with Linear Contextual Bandits

## Quick Facts
- **arXiv ID**: 2510.19119
- **Source URL**: https://arxiv.org/abs/2510.19119
- **Reference count**: 40
- **Primary result**: None

## Executive Summary
This paper introduces *InfluenceCB*, a linear contextual bandit framework for learning peer influence probabilities in social networks. The approach addresses the challenge of balancing the exploration-exploitation trade-off while simultaneously learning accurate peer influence models. By modeling the reward as a linear function of peer influence and contextual features, the framework enables agents to make decisions that account for both individual and social factors.

## Method Summary
The *InfluenceCB* framework operates on a social network $G=(V, E)$ where agents must select actions from a pool $A_t$ at each time step. The core innovation lies in modeling the expected reward as $E[r] = X^\top \theta^*$, where $X$ represents the combined influence from peers and individual context. The algorithm achieves a regret bound of $O(\sqrt{T})$ while maintaining an RMSE of $O(1/\sqrt{T})$ for learning the influence probabilities. This dual objective is accomplished through a carefully designed exploration strategy that balances individual exploration with social learning from peer outcomes.

## Key Results
- Establishes a novel trade-off between cumulative regret and influence probability estimation accuracy
- Proves theoretical guarantees for the regret-RMSE relationship in linear influence models
- Demonstrates effectiveness through experiments on both synthetic and real-world datasets

## Why This Works (Mechanism)
The framework succeeds by explicitly modeling peer influence as a linear combination of contextual features, allowing for efficient estimation through standard bandit algorithms. The key mechanism is the separation of individual and social learning components, where agents can explore independently while also observing and learning from peer outcomes. This dual learning approach enables faster convergence to accurate influence probabilities compared to pure exploration strategies.

## Foundational Learning
- **Linear Contextual Bandits**: Required for modeling reward as a linear function of context; quick check: verify linear assumption holds for your influence data
- **Peer Influence Modeling**: Essential for capturing social effects on decision-making; quick check: ensure influence can be represented as feature interactions
- **Regret Analysis**: Needed to understand the exploration-exploitation trade-off; quick check: confirm regret bounds match your application requirements
- **Social Network Theory**: Important for understanding network structure assumptions; quick check: validate that the network is adequately observed and static
- **RMSE Metrics**: Critical for evaluating influence probability estimation accuracy; quick check: ensure appropriate baseline comparisons
- **Exploration Strategies**: Fundamental to balancing learning and performance; quick check: verify exploration rate is appropriate for your domain

## Architecture Onboarding

**Component Map**: Social Network -> Context Extraction -> Linear Model -> Action Selection -> Reward Observation -> Influence Update

**Critical Path**: Context Extraction → Linear Model → Action Selection → Reward Observation → Influence Update

**Design Tradeoffs**: Linear assumption vs. model expressiveness; exploration rate vs. convergence speed; individual vs. social learning balance

**Failure Signatures**: Poor influence estimation if network structure is incorrect; suboptimal regret if linear assumption violated; slow convergence if exploration rate too low

**First Experiments**:
1. Validate linear reward assumption on synthetic data with known influence structure
2. Test regret-RMSE trade-off on small-scale social network datasets
3. Compare performance against pure exploration and exploitation baselines

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the *InfluenceCB* framework be theoretically and empirically extended to capture non-linear influence dynamics?
- **Basis in paper**: [explicit] The conclusion states future work will focus on "extending this framework beyond the linear setting to capture non-linear influence dynamics."
- **Why unresolved**: The current theoretical guarantees (Theorem 5.1) and algorithm design rely on a linear reward model ($E[r] = X^\top \theta^*$).
- **What evidence would resolve it**: Theoretical analysis of the regret-RMSE trade-off using non-linear function approximators (e.g., neural networks) and experiments showing improved performance on non-synthetic data with complex feature interactions.

### Open Question 2
- **Question**: Can the approach be adapted to maintain the regret-RMSE trade-off in dynamic or partially observed networks?
- **Basis in paper**: [explicit] The conclusion lists "adapting the approach to dynamic or partially observed networks" as a specific direction for future work.
- **Why unresolved**: The current problem formulation assumes a static, fully observed network structure $G=(V, E)$, which is often unrealistic in evolving social systems.
- **What evidence would resolve it**: A modified algorithm that handles temporal changes in edges/nodes or missing data without losing the theoretical rate guarantees established for the static setting.

### Open Question 3
- **Question**: Can rigorous theoretical guarantees for the regret-RMSE trade-off be established for varying action pools ($A_t$)?
- **Basis in paper**: [inferred] The authors note that theoretical results are derived for a "fixed-action setting" because a uniform worst-case bound is impossible if nature restricts action sets, yet experiments use varying pools.
- **Why unresolved**: There is a gap between the general problem description (where $A_t$ varies) and the theoretical analysis (where $A_t$ is fixed), leaving the theoretical performance in the general case undefined.
- **What evidence would resolve it**: Derivation of regret and RMSE bounds under specific assumptions about the distribution or constraints of the available action set $A_t$ over time.

## Limitations
- Linear reward assumption may not capture complex influence dynamics
- Requires fully observed and static network structure
- Theoretical guarantees limited to fixed action sets despite experiments using varying pools

## Confidence
The proposed approach appears plausible but unverified from the title alone, leading to Low confidence without access to full methodology or experimental results.

## Next Checks
1. Examine the mathematical formulation of peer influence probability estimation within the linear contextual bandit framework
2. Review empirical evaluation results on real or synthetic social network datasets
3. Verify theoretical guarantees regarding regret bounds and convergence rates for the proposed algorithm