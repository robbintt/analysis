---
ver: rpa2
title: 'LLMs Don''t Know Their Own Decision Boundaries: The Unreliability of Self-Generated
  Counterfactual Explanations'
arxiv_id: '2509.09396'
source_url: https://arxiv.org/abs/2509.09396
tags:
- sces
- distance
- dataset
- minimal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-generated counterfactual explanations (SCEs) are a method
  for LLMs to explain their decisions by modifying inputs to change predictions. This
  study evaluates whether LLMs can produce SCEs that are both valid (change the model's
  prediction) and minimal (make the smallest necessary edit).
---

# LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations

## Quick Facts
- arXiv ID: 2509.09396
- Source URL: https://arxiv.org/abs/2509.09396
- Reference count: 40
- Key finding: LLMs fail to generate counterfactual explanations that are both valid (change prediction) and minimal (smallest necessary edit)

## Executive Summary
This study investigates whether large language models can generate self-generated counterfactual explanations (SCEs) - explanations that modify inputs to flip model predictions. Through extensive experiments across multiple LLMs, datasets, and settings, the authors find a fundamental trade-off: when prompted to generate counterfactuals, models produce valid but non-minimal explanations; when prompted for minimal counterfactuals, models make overly conservative edits that rarely change predictions. The study concludes that LLMs do not reliably engage in self-prediction, a necessary component for effective SCEs. These findings suggest SCEs are unreliable as explainability tools and could mislead decision-making in high-stakes applications.

## Method Summary
The authors evaluate self-generated counterfactual explanations through a structured experimental framework. They test multiple LLMs across several synthetic datasets with known decision boundaries. The methodology involves prompting models to generate counterfactual explanations under different conditions - sometimes asking for valid counterfactuals, other times asking for minimal ones. They measure two key properties: validity (whether the counterfactual actually changes the model's prediction) and minimality (whether the explanation makes the smallest necessary edit). The experiments include ablation studies testing whether explicit self-prediction instructions improve performance, and analysis of model reasoning traces to understand why models fail at this task.

## Key Results
- Models exhibit a clear trade-off: valid counterfactuals are non-minimal, while minimal counterfactuals rarely flip predictions
- No model consistently produces SCEs that are both valid and minimal across all tested conditions
- Analysis of reasoning traces shows models consistently fail to engage with self-prediction, instead referring to external model behavior they cannot assess
- Explicit self-prediction instructions provide only marginal improvement (validity 21.44% → 23.34%)

## Why This Works (Mechanism)
The study identifies that LLMs lack the capability for reliable self-prediction, which is essential for generating valid counterfactual explanations. When models attempt to create counterfactuals, they don't internally simulate how their own decision boundaries would respond to input modifications. Instead, they generate explanations based on surface-level reasoning about what "should" change the prediction, without actually verifying whether their own model would behave as suggested. This disconnect between explanation generation and actual model behavior explains why valid and minimal explanations cannot be jointly achieved.

## Foundational Learning
**Self-prediction capability** - Models need to simulate their own responses to modified inputs; without this, they cannot verify whether generated counterfactuals will actually work. Quick check: Can the model accurately predict its own output on modified versions of an input?
**Decision boundary awareness** - Understanding where the model's classification boundaries lie is crucial for identifying minimal edits that flip predictions. Quick check: Does the model have implicit knowledge of its own classification thresholds?
**Counterfactual validity measurement** - Determining whether a proposed counterfactual actually changes the model's prediction requires actual testing, not just reasoning. Quick check: Can the model distinguish between explanations that would work versus those that wouldn't?

## Architecture Onboarding
**Component map**: Input -> Model prediction -> Counterfactual generation -> Self-prediction -> Output explanation
**Critical path**: The self-prediction step is critical - without it, counterfactuals cannot be validated against actual model behavior
**Design tradeoffs**: The study uses synthetic datasets with known decision boundaries to enable precise measurement of minimality, but this limits generalizability to real-world scenarios
**Failure signatures**: Models generate explanations that reference external model behavior rather than self-prediction, produce non-minimal valid counterfactuals, or generate minimal changes that don't flip predictions
**3 first experiments**: 1) Test self-prediction capability directly by asking models to predict their own outputs on modified inputs, 2) Evaluate counterfactual generation on a simple binary classification task with clear decision boundary, 3) Compare performance across different prompting strategies for explicit self-prediction

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can LLMs be trained or fine-tuned to accurately self-predict their behavior in alternative context windows?
- Basis in paper: The authors state that "no optimisation pressure to develop a self-model" exists in current training, suggesting that "New learning objectives are likely required to incentivise accurate self-prediction."
- Why unresolved: The self-prediction ablation experiment showed only marginal improvement (validity 21.44% → 23.34%) when explicitly prompting self-prediction in reasoning traces, leaving open whether this is a fundamental architectural limitation or a training objective problem.
- What evidence would resolve it: Demonstration that models trained with explicit self-prediction objectives can generate SCEs that jointly satisfy validity and minimality criteria.

### Open Question 2
- Question: Do findings from low-dimensional tabular tasks generalize to complex natural language settings with unconstrained input spaces?
- Basis in paper: The methodology relies on synthetic datasets with 2-4 discrete features where minimal counterfactuals can be precisely identified. The authors note "models fail in these datasets" and "anticipate that they will also struggle in more complex, higher-dimensional datasets," but this remains untested.
- Why unresolved: Measuring minimality in natural language spaces requires identifying the minimal counterfactual, which is computationally intractable when input spaces are unconstrained.
- What evidence would resolve it: Extension of SCE evaluation methodology to natural language tasks with approximated minimality bounds, or alternative metrics that capture both validity and minimality for open-ended text.

### Open Question 3
- Question: What mechanisms could enable LLMs to acquire implicit knowledge of their own decision boundaries?
- Basis in paper: The authors conclude that "LLMs do not know their own decision boundaries" and analysis of reasoning traces shows models "consistently fail to engage with the self-explanation aspect of the task," referring instead to external model behavior they cannot assess.
- Why unresolved: The paper identifies lack of self-prediction as the limiting factor but does not investigate whether this capability could emerge through different architectures, training procedures, or inductive biases.
- What evidence would resolve it: Mechanistic interpretability studies identifying whether LLMs possess latent representations of their decision boundaries, or training experiments showing such representations can be learned.

### Open Question 4
- Question: How should minimality metrics incorporate both valid and invalid SCEs to provide a more complete picture of model behavior?
- Basis in paper: The authors acknowledge in Limitations that mean excess distance "does not account for the properties of invalid SCEs" and suggest "future work might design new metrics that incorporate both valid and invalid SCEs."
- Why unresolved: Current metrics either exclude invalid SCEs entirely or treat them uniformly, losing information about whether invalid SCEs lie just beyond the decision boundary or far from it.
- What evidence would resolve it: Development and validation of unified metrics that jointly penalize excess distance for valid SCEs and distance-to-boundary for invalid SCEs.

## Limitations
- Experimental design assumes models can engage in reliable self-prediction, but analysis suggests this assumption may be flawed
- Findings are based on binary classification tasks and may not generalize to regression, multi-class, or open-ended generation tasks
- Analysis relies heavily on qualitative interpretation of model behavior rather than direct measurement of internal mechanisms
- Does not explore potential improvements from fine-tuning approaches or more sophisticated prompting strategies

## Confidence
- Empirical observation of validity/minimality trade-off: Medium
- Claim about models not engaging in self-prediction: Low
- Generalizability across task types: Low
- Practical implications for high-stakes applications: Medium

## Next Checks
1. Test SCE generation across diverse task types (regression, multi-class, open-ended generation) to assess generalizability beyond binary classification
2. Implement and evaluate alternative prompting strategies that explicitly instruct models to perform self-prediction before generating counterfactuals
3. Conduct ablation studies varying the complexity of input modifications to determine whether the observed trade-off persists across different perturbation scales