---
ver: rpa2
title: 'Beyond Real Weights: Hypercomplex Representations for Stable Quantization'
arxiv_id: '2512.08524'
source_url: https://arxiv.org/abs/2512.08524
tags:
- dense
- multimodal
- while
- parameter
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a progressive reparameterization strategy
  that compresses multimodal language models by replacing dense feed-forward network
  blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual
  interpolation schedule, along with lightweight reconstruction and knowledge distillation
  losses, ensures that PHM modules inherit the functional behavior of their dense
  counterparts during training.
---

# Beyond Real Weights: Hypercomplex Representations for Stable Quantization

## Quick Facts
- arXiv ID: 2512.08524
- Source URL: https://arxiv.org/abs/2512.08524
- Reference count: 40
- Primary result: 20-30% parameter reduction with maintained performance on multimodal tasks

## Executive Summary
This work introduces a progressive reparameterization strategy that compresses multimodal language models by replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, along with lightweight reconstruction and knowledge distillation losses, ensures that PHM modules inherit the functional behavior of their dense counterparts during training. The approach maintains strong multimodal alignment while delivering substantial parameter and FLOP reductions, enabling faster inference without degrading output quality. Evaluated on multiple vision-language models, the method preserves performance comparable to base models while reducing parameters by 20–30% and improving latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning.

## Method Summary
The paper presents a two-stage training approach for compressing multimodal models. In Stage A, dense FFN layers are progressively replaced with PHM layers using residual interpolation (α schedule), while maintaining knowledge distillation and reconstruction losses. In Stage B, the dense components are removed and the model is fine-tuned with only PHM layers. The PHM factorization uses Kronecker products with a small number of learnable matrices, achieving compression ratios of approximately 4/B where B ∈ {2,3}. Capacity is selectively assigned to layers based on Fisher sensitivity, with higher B values allocated to output-proximal language layers.

## Key Results
- 20-30% parameter reduction while maintaining performance comparable to base models
- CIDEr scores of 85.2-85.5 on Flickr30k validation (vs 85.8 baseline)
- Up to 37% FLOP reduction in FFN layers with 3-6% latency improvement
- Effective across multiple vision-language benchmarks including NoCaps, ScienceQA, and FinMME

## Why This Works (Mechanism)

### Mechanism 1: Residual Interpolation for Stable Dense-to-Hypercomplex Transition
Gradually blending dense weights with PHM approximations via a scheduled interpolation coefficient prevents optimization instability during structural replacement. The residual formulation $W_{res}(\alpha) = (1-\alpha)W_{dense} + \alpha \cdot W_{PHM}$ starts at $\alpha=0$ (pure dense) and ramps to $\alpha=1$ (pure PHM) over $T_{fade}$ steps, allowing gradient flow through familiar dense pathways initially, with PHM components incrementally assuming computational responsibility.

### Mechanism 2: Reconstruction Loss for Operator-Level Alignment
Explicitly penalizing the output difference between dense and PHM operators stabilizes the factorization's functional approximation. The reconstruction term $L_{recon} = \frac{1}{|M|}\sum_{\ell} \mathbb{E}_x \|W_{PHM}^{(\ell)} x - W_{dense}^{(\ell)} x\|_2^2$ constrains PHM layers to approximate their dense counterparts' input-output mappings during the transition.

### Mechanism 3: Selective Capacity Assignment via Fisher Sensitivity Proxy
Allocating higher PHM capacity ($B=3$ vs $B=2$ bases) to output-proximal language layers preserves accuracy where logit sensitivity is highest. The paper assigns $B_\ell=3$ to top-$K$ language MLP layers based on the rationale that Fisher trace $\text{tr}(F_\ell)$ concentrates toward upper layers, making them more sensitive to approximation error.

## Foundational Learning

- **Kronecker Product Factorization**
  - Why needed here: PHM represents dense matrices as $\sum_b H_b \otimes A_b$, where $H_b$ are fixed $2\times2$ bases and $A_b$ are learnable. Understanding this decomposition is essential for grasping the compression ratio $\approx 4/B$.
  - Quick check question: Given $H_1 = I$ and $H_2 = J$ (rotation matrix), write out the block structure of $H_1 \otimes A_1 + H_2 \otimes A_2$.

- **Knowledge Distillation with Temperature Scaling**
  - Why needed here: The paper uses KL divergence between teacher and student logits at temperature $T=4$; understanding soft label sharpening is critical for tuning KD strength.
  - Quick check question: Why does higher temperature produce softer probability distributions, and what's the tradeoff?

- **Fisher Information as Sensitivity Proxy**
  - Why needed here: Selective capacity assignment relies on the intuition that layers with larger Fisher trace are more sensitive to perturbation; this guides where to allocate $B=3$ vs $B=2$.
  - Quick check question: If $\mathbb{E}[\Delta L_\ell] \approx \frac{1}{2}\langle \Delta W_\ell, F_\ell \Delta W_\ell \rangle$, what does a larger $F_\ell$ imply for acceptable $\Delta W_\ell$ magnitude?

## Architecture Onboarding

- **Component map**: Vision encoder (frozen) -> Projector (dense) -> Language transformer (PHM-FFN + LoRA Q/K/V) -> Output

- **Critical path**:
  1. Identify FFN layers exceeding size threshold $\tau$
  2. Partition dense weights into 4 blocks $(p,q,r,s)$ and initialize $A_1, A_2$ via projection
  3. Run Stage A with $\alpha(t)$ schedule; validate periodically
  4. Select best checkpoint, drop dense branches, run Stage B

- **Design tradeoffs**:
  - Higher $K$ (more layers with $B=3$) → better accuracy but larger model (Figure 3d shows plateau at $K \approx 12$)
  - Longer $T_{fade}$ → more stable transition but longer training
  - LoRA on all Q/K/V vs. Q-only → 85.0% vs. 77.5% accuracy (Figure 3c)

- **Failure signatures**:
  - CIDEr collapse to <40 during early training → $\alpha$ ramping too fast or reconstruction weight too low
  - NoCaps generalization lagging base model by >5 CIDEr → top-layer capacity under-allocated
  - Validation CE not decreasing as $\alpha \to 1$ → KD temperature or $\lambda_{max}$ needs adjustment

- **First 3 experiments**:
  1. **Baseline sanity check**: Reproduce Qwen2.5-VL-7B dense baseline on Flickr30k; confirm CIDEr ~85.8
  2. **Ablation on $K$**: Train with $K \in \{0, 4, 8, 12\}$ on a single dataset; plot CIDEr vs. parameter count to find knee point
  3. **Transition stability probe**: Monitor validation CE at $\alpha \in \{0.0, 0.25, 0.5, 0.75, 1.0\}$; if CE spikes at mid-range, increase $T_{fade}$

## Open Questions the Paper Calls Out

- **Extending to audio and video modalities**: Future directions include extending evaluation to audio and video modalities beyond static vision-language tasks. Current evaluation is limited to image-based captioning and VQA, but audio and video introduce temporal dependencies and different computational characteristics that may affect PHM factorization effectiveness.

- **Integrating with quantization-aware training**: The paper claims PHM "complements existing low-bit quantization techniques" and future work includes "integrating quantization-aware training for deployment in resource-constrained settings." The paper demonstrates PHM independently but provides no experimental validation of combined approaches with quantization methods.

- **Optimizing layer-wise capacity assignment**: Section 3.1 formulates capacity allocation as a discrete budgeted optimization problem but solves it with a simple threshold heuristic rather than formal optimization. The current rule assigns B=3 to top-K layers based on empirical Fisher sensitivity, but this may be suboptimal across different architectures or tasks.

## Limitations

- **Modality-specific validation**: All experiments focus on vision-language tasks, leaving uncertainty about applicability to pure text or audio modalities.
- **Fisher proxy validation**: The Fisher-trace proxy for layerwise capacity assignment lacks direct empirical validation against alternative selection strategies.
- **Optimization stability analysis**: The paper claims residual interpolation prevents optimization instability but provides only validation CE curves without gradient or weight divergence analysis.

## Confidence

**High Confidence (8-10/10)**:
- PHM can compress multimodal models by 20-30% with minimal accuracy loss
- Progressive residual interpolation enables stable transition from dense to PHM
- Selective capacity assignment to top layers improves performance

**Medium Confidence (5-7/10)**:
- Fisher-trace proxy reliably identifies sensitive layers for capacity assignment
- Reconstruction loss prevents catastrophic forgetting during transition
- Two-stage training (Stage A→Stage B) is optimal for final accuracy

**Low Confidence (1-4/10)**:
- PHM performance generalizes to non-vision modalities
- The specific values of $T_{fade}$, $\lambda_{max}$, and $\mu$ are universally optimal
- LoRA on all attention heads is superior to selective application

## Next Checks

1. **Fisher Proxy Ablation**: Train identical models with layer capacity assigned via Fisher-trace versus random assignment versus task-tuned selection. Compare CIDEr and parameter efficiency to determine if the proxy adds value beyond simply allocating more parameters to top layers.

2. **Gradient Stability Analysis**: During Stage A, monitor gradient norms for dense branches versus PHM branches across different $\alpha$ values. Plot weight divergence between original dense weights and their PHM approximations to verify the residual mechanism prevents optimization collapse.

3. **Cross-Modality Generalization**: Apply the PHM compression pipeline to a pure text model (e.g., LLaMA) or audio-language model. Compare compression ratios and accuracy retention against the vision-language results to assess modality dependence.