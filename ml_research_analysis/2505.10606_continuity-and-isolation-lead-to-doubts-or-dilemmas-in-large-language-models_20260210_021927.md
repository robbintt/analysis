---
ver: rpa2
title: Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models
arxiv_id: '2505.10606'
source_url: https://arxiv.org/abs/2505.10606
tags:
- sequence
- sequences
- python
- input
- decoder-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates two fundamental limitations of decoder-only
  Transformers with compact positional encoding: continuity and isolation. Continuity
  implies that small input perturbations lead to small output changes, while isolation
  means that any two sequences learnable by the same Transformer must be sufficiently
  far apart in relative Hamming distance.'
---

# Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models

## Quick Facts
- arXiv ID: 2505.10606
- Source URL: https://arxiv.org/abs/2505.10606
- Reference count: 40
- Primary result: Small input perturbations rarely affect next-token predictions in decoder-only Transformers with compact positional encoding unless they exceed model-specific thresholds, and any two learnable sequences must be sufficiently far apart in relative Hamming distance.

## Executive Summary
This paper investigates two fundamental limitations of decoder-only Transformers with compact positional encoding: continuity and isolation. Continuity implies that small input perturbations lead to small output changes, while isolation means that any two sequences learnable by the same Transformer must be sufficiently far apart in relative Hamming distance. The authors prove these limitations mathematically and validate them empirically across modern LLMs. For continuity, experiments show that small input changes rarely affect next-token predictions unless they exceed a model-specific threshold. For isolation, models struggle to learn periodic sequences beyond a critical period length, with performance degrading as pattern complexity increases. These findings reveal inherent representational constraints in Transformers, showing they must choose between learning certain simple patterns while being unable to learn others, even when both seem straightforward to humans.

## Method Summary
The paper combines theoretical proofs with empirical validation across multiple pretrained models. Theoretical work establishes continuity bounds for output distributions under input perturbations and proves isolation theorems requiring positive Hamming distance between learnable sequences. Empirically, three experiments test these limitations: (1) zero-sequence continuity test with controlled perturbations, (2) syntax verification comparing predictions for correct vs. erroneous code variants, and (3) periodic sequence generation to identify critical periods where learning fails. Models tested include GPT-2 variants, Gemma, Llama, and Phi across different sizes. The experiments measure next-token sensitivity (NTS), success rates, and certainty metrics to quantify the continuity and isolation effects.

## Key Results
- Small input perturbations (γ < 0.05) produce no change in next-token predictions for most models, demonstrating strong continuity
- Models fail to learn periodic sequences beyond a critical period length (p ≈ 20-30 for Llama-2-7B), validating isolation
- Syntax verification tasks show models often output identical predictions for syntactically correct vs. erroneous code differing by 1-2 tokens
- Larger models show modestly higher sensitivity but still exhibit strong continuity/isolation effects
- Log-length scaled attention (ssmax) increases sensitivity but doesn't eliminate the fundamental limitations

## Why This Works (Mechanism)

### Mechanism 1: Continuity of Output Distributions
- Claim: Decoder-only Transformers with compact positional encoding produce output distributions that change smoothly with input perturbations, bounded by relative Hamming distance.
- Mechanism: When input sequences α, β share the same last token and differ in at most δ-fraction of positions, their output distributions differ by at most ε, where δ depends only on ε and model parameters. This arises because attention weights remain bounded (in [c, C]) and value/activation functions are continuous, preventing unbounded propagation of small differences.
- Core assumption: Compact positional encoding (bounded vectors) and continuous value/activation functions.
- Evidence anchors:
  - [abstract]: "Continuity implies that small changes in input prompts lead to small changes in output distributions"
  - [Section 3, Theorem 1]: Formal statement with δ-ε bound for sequences with same last token
  - [Appendix A.1]: Proof showing ∥an - b̄n∥ bounded through attention layer composition
- Break condition: Non-compact positional encoding (e.g., absolute positional encoding with unbounded norms) or discontinuous layer norm (ε = 0).

### Mechanism 2: Isolation of Learnable Sequences
- Claim: Any two infinite sequences that a Transformer can "eventually learn" must be separated by positive relative Hamming distance.
- Mechanism: If sequence α is eventually learned, there exists δ > 0 such that no sequence β within Hamming ball of radius δ (differing at infinitely many positions) can also be learned. The continuity mechanism forces similar prefixes to produce identical top-token predictions, making it impossible for the model to correctly predict differing positions in β.
- Core assumption: The "eventual learnability" definition requires a fixed margin ε between top-token probability and alternatives, independent of sequence length.
- Evidence anchors:
  - [abstract]: "Isolation means that any learnable sequence must be isolated from another learnable sequence"
  - [Section 4, Theorem 2]: Formal isolation theorem with proof
  - [Corollary 2]: No CPE transformer can learn all periodic sequences
- Break condition: Allowing "doubts" (removing the margin requirement) or using non-compact positional encodings.

### Mechanism 3: Representational Collapse via Attractor Basins
- Claim: Learned sequences create basins where semantically distinct but Hamming-similar inputs collapse to identical predictions.
- Mechanism: As sequence length grows, early-token differences get "washed out" by softmax normalization over increasing attention weights. The model becomes "overwhelmed" by learned patterns, making it insensitive to modifications in middle/early positions (though end-position changes remain more detectable).
- Core assumption: Causal masking in decoder-only architecture, where position j only attends to positions ≤ j.
- Evidence anchors:
  - [Section 1.1]: "For any sequence β within the ball, the output distributions... will be so close that the top-probability token will be the same"
  - [Figure 2]: NTSγ = 0 for all models when γ = 0.01, showing collapse to all-zero prediction
  - [Appendix B.3]: Perturbations at end have greater impact than middle/beginning positions
  - [corpus]: Weak corpus evidence—related work focuses on attention mechanisms broadly, not specifically isolation phenomena
- Break condition: Log-length scaled attention (ssmax) amplifies sensitivity, partially breaking collapse (Appendix B.4).

## Foundational Learning

- Concept: **Relative Hamming Distance**
  - Why needed here: The paper's core theorems use d_H(α,β) = |positions where αᵢ ≠ βᵢ| / n as the metric for sequence similarity. Understanding this normalized measure (not absolute count) is essential for interpreting isolation results.
  - Quick check question: Two sequences of length 1000 differ at 5 positions. What is their relative Hamming distance?

- Concept: **Compact Positional Encoding (CPE)**
  - Why needed here: All theorems require positional encoding vectors p(i,j) to remain within a bounded set K ⊆ ℝᵈ. This includes sinusoidal and rotary encodings but excludes unbounded schemes. Understanding why compactness matters is crucial for knowing when results apply.
  - Quick check question: Does RoPE (rotary positional encoding) qualify as compact? What about absolute positional encoding p(i) = i?

- Concept: **Eventual Learnability Definition**
  - Why needed here: The paper defines a specific notion of "learning" requiring: (1) correct top-token prediction for all sufficiently long prefixes, and (2) a margin ε > 0 between top probability and alternatives. This is stronger than typical ML metrics and drives the isolation result.
  - Quick check question: Why does the margin requirement matter for the isolation theorem?

## Architecture Onboarding

- Component map:
  - Input embedding e(σ, i) → Attention layer L → Final hidden state yₙ → Projection P → Distribution
  - Attention layer: Weight function w → Value function val → Activation F → Residual connection

- Critical path: Input → Embedding → [Attention Layer × k] → Final hidden state yₙ → Projection P → Distribution. The continuity proof tracks how ∥yₙ - ȳₙ∥ remains bounded through each layer via Lemma 1.

- Design tradeoffs:
  - CPE vs. unbounded encoding: CPE (sinusoidal, RoPE) enables stability but causes isolation; unbounded encoding may escape limitations but lacks theoretical guarantees
  - Standard softmax vs. ssmax: ssmax increases sensitivity to input perturbations (Figure 8) but may harm training stability
  - Model scale: Larger models show higher sensitivity (Figure 3: Gemma-3-4B 7% vs. Gemma-3-12B 9%) but don't eliminate collapse

- Failure signatures:
  - Periodic pattern breakdown: Models fail to extrapolate beyond critical period p (Figure 4 shows failure around p=20-30 for Llama-2-7B)
  - Syntax insensitivity: In code verification, models output same prediction for syntactically correct/incorrect versions differing by 1-2 tokens (phi-4: 0% sensitivity in Figure 3)
  - Confidence dip before failure: Certainty metric shows characteristic dip near critical period (Figure 4, second row)

- First 3 experiments:
  1. Zero-sequence sensitivity test: Prompt model with 190 zeros plus instruction prefix, then perturb γ-fraction of positions. Measure NTSγ (count of divergent next-token predictions). Start with γ=0.01, increase to 0.5. Expect NTS=0 for γ<0.05 in most models.
  2. Periodic sequence generation: Prompt with r repetitions of pattern 0^(p-1)1, measure success rate and certainty across periods p=2 to 40. Identify critical period where success drops.
  3. Syntax verification pairs: Create α/β prompt pairs differing by single token that flips syntax correctness. Plot P(σ|α) vs P(σ|β). Points on diagonal indicate insensitivity; measure percentage off-diagonal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can chain-of-thought (CoT) reasoning with unbounded iterations overcome isolation and continuity limitations in Transformers with compact positional encoding?
- Basis in paper: [explicit] "leaving the theoretical power of CoT-equipped transformers in this regime open" — the authors note that while CoT makes Transformers Turing complete, this has not been proven for softmax attention with compact positional encoding.
- Why unresolved: The paper's theoretical results apply when CoT iterations are O(1), but the logic breaks when iterations scale with input length; whether this scaling actually escapes the limitations remains unproven.
- What evidence would resolve it: A formal proof showing whether unbounded CoT with CPE can or cannot overcome isolation, combined with empirical validation on tasks like periodic sequence completion with varying CoT depths.

### Open Question 2
- Question: What is the quantitative relationship between model scale (parameters, layers, embedding dimension) and the isolation radius δ for learnable sequences?
- Basis in paper: [inferred] The paper proves δ > 0 exists for any CPE Transformer but does not characterize how δ scales with model capacity; empirical results show smaller models sometimes exhibit larger sensitivity, suggesting non-monotonic relationships.
- Why unresolved: The theoretical proof is non-constructive regarding δ, and experiments across model families show inconsistent patterns that warrant systematic investigation.
- What evidence would resolve it: Systematic experiments measuring critical periods and sensitivity thresholds across controlled model scaling configurations, potentially yielding scaling laws for isolation/continuity bounds.

### Open Question 3
- Question: Do non-compact positional encoding schemes (absolute positional encoding, log-n scaling/scalable softmax) fundamentally avoid isolation and continuity limitations while maintaining competitive performance?
- Basis in paper: [explicit] The authors state their results require compact positional encoding and do not apply to absolute positional encoding or log-n scaling (scalable softmax); they show ssmax increases sensitivity empirically.
- Why unresolved: While non-compact encodings escape the theoretical framework, their practical efficacy and whether they introduce new trade-offs remain unexplored.
- What evidence would resolve it: Comparative experiments on the same periodic sequence and syntax verification tasks using identical model architectures with different positional encoding schemes, measuring both theoretical limitation metrics and standard downstream task performance.

## Limitations

- The theoretical results depend critically on the "compactness" assumption for positional encodings, which may not hold in all practical implementations
- Empirical validation uses relatively small datasets (100 syntax pairs, 5 seeds for perturbations) that may not capture full variability
- The NTS metric is binary in practice - models show either zero sensitivity or near-complete divergence, potentially oversimplifying continuity
- Results focus on decoder-only architecture and may not generalize to encoder-decoder or bidirectional models

## Confidence

- **High confidence**: Mathematical proofs for continuity and isolation theorems are sound; experimental methodology is clearly specified; zero sensitivity results for small perturbations across multiple models are robust
- **Medium confidence**: Practical implications for real-world tasks are less clear; periodic sequence task demonstrates limitations but such patterns may rarely occur in practice versus noisy sequences
- **Low confidence**: Claim that limitations are fundamental architectural constraints is plausible but not definitively proven; architectural modifications like log-length scaled attention may mitigate rather than eliminate issues

## Next Checks

1. **Extended perturbation analysis**: Test continuity with more diverse perturbation types beyond random bit flips - including semantic perturbations (synonym replacement), syntactic perturbations (changing operators), and structured perturbations (following Beta-Binomial distribution from Appendix B.3). This would validate whether isolation effects are truly about Hamming distance or specific perturbation patterns.

2. **Cross-model architectural comparison**: Evaluate the same continuity/isolation tasks across different attention mechanisms (standard softmax vs. ssmax, log-length scaled attention) and positional encodings (sinusoidal, rotary, absolute). This would test whether the limitations are truly fundamental to decoder-only Transformers or specific architectural choices.

3. **Fine-tuning impact study**: Train models on periodic sequences with increasing complexity and measure how training data quantity and diversity affect the critical period threshold. This would determine whether isolation is an inherent limitation or a training deficiency that could be overcome with sufficient data or curriculum learning.