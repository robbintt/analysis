---
ver: rpa2
title: 'MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment'
arxiv_id: '2505.08589'
source_url: https://arxiv.org/abs/2505.08589
tags:
- images
- segmentation
- semantic
- dataset
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MESSI, a Multi-Elevation Semantic Segmentation
  Image dataset comprising 2525 images captured by a drone flying over dense urban
  environments at various altitudes. MESSI is unique in containing images from multiple
  elevations and revisiting the same regions at different scales, capturing the visual
  richness of 3D drone flight.
---

# MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment

## Quick Facts
- arXiv ID: 2505.08589
- Source URL: https://arxiv.org/abs/2505.08589
- Reference count: 17
- Key result: MESSI dataset with 2525 drone images at multiple altitudes, enabling study of altitude effects on semantic segmentation performance

## Executive Summary
This paper introduces MESSI, a Multi-Elevation Semantic Segmentation Image dataset comprising 2525 images captured by a drone flying over dense urban environments at various altitudes. MESSI is unique in containing images from multiple elevations and revisiting the same regions at different scales, capturing the visual richness of 3D drone flight. The dataset includes detailed annotations with location, orientation, and camera parameters, enabling applications beyond semantic segmentation. The paper describes the dataset construction, annotation process, and presents statistics showing class distribution variations across altitudes. MESSI was used to train several semantic segmentation models, with SegFormer-B3 achieving the best performance (mIoU of 52.6% with Sqrt weighting). The dataset enables studying the effect of altitude on segmentation performance and provides a valuable benchmark for evaluating semantic segmentation algorithms using drone imagery in urban environments.

## Method Summary
The MESSI dataset contains 2525 high-resolution (5472×3648) drone images captured at multiple altitudes (10-120m) over a coastal urban environment in Israel. Images were captured using a DJI MAVIC 2 PRO drone flying pre-programmed paths, revisiting the same regions at different scales. The dataset includes 16 semantic classes with detailed annotations including location, orientation, and camera parameters. For training and evaluation, images were processed using a 3×3 overlapping tile approach (2048×1366 tiles with 1712×1141 overlap) to fit within GPU memory constraints. The paper tested five semantic segmentation models (BiSeNetV1, DeepLabV3+, SegFormer, Mask2Former) using MMsegmentation framework with Cityscapes pretrained weights. Training used 1024×1024 random crops, batch size 2, 320 epochs, and three class weighting schemes: Equal, Sqrt (inverse square root of class frequency), and Prop (inverse class frequency).

## Key Results
- SegFormer-B3 with Sqrt weighting achieved the best performance with 52.6% mIoU on the test set
- Multi-altitude training significantly improved performance across different test altitudes compared to single-altitude training
- Sqrt class weighting outperformed both Equal and Prop weighting schemes, particularly for rare classes
- Mask2Former-Swin-B underperformed expectations with 44.5% mIoU, likely due to implementation limitations with high-resolution images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training segmentation models on multi-elevation imagery can improve or maintain accuracy across altitude variations compared to single-altitude training.
- Mechanism: Exposure to objects at varying spatial resolutions during training creates scale-invariant feature representations. The paper demonstrates that models trained on multiple altitudes (30, 50, 70, 100m) maintain accuracy when tested on vertical descent sequences, while single-altitude training degrades as inference altitude diverges from training altitude.
- Core assumption: Feature hierarchies learned across scales transfer to intermediate, unseen altitudes.
- Evidence anchors:
  - [abstract] "MESSI is unique in containing images from multiple elevations and revisiting the same regions at different scales"
  - [section 4.1, p.8] "MESSI is unique in that it captures areas at different altitudes; thus, another experiment was performed to examine the effect of this feature on performance"
  - [section 4.2, Fig.7, p.11] Shows accuracy degradation when training at limited altitudes vs. multi-altitude training
  - [corpus] Related work (Turin3D) addresses domain adaptation but not specifically altitude-scale relationships
- Break condition: If target inference altitudes fall far outside training altitude distribution, multi-elevation benefit diminishes.

### Mechanism 2
- Claim: Square-root (Sqrt) class weighting outperforms uniform and inverse-proportional weighting for imbalanced aerial segmentation.
- Mechanism: Sqrt weighting moderately upweights rare classes (e.g., Person, Bicycle, Water) without overfitting to sparse samples. Inverse-proportional weighting over-amplifies noise in rare-class gradients; uniform weighting underrepresents rare classes. Sqrt provides a balance, improving mIoU from 50.3% (Equal) to 52.6% on SegFormer-B3.
- Core assumption: Rare classes contain meaningful signal rather than annotation noise.
- Evidence anchors:
  - [section 3.2, p.6] "There is a large imbalance between the highly populated and rare classes"
  - [section 4.2, Table 4, p.9] Sqrt weighting achieves highest mIoU across most models
  - [corpus] No direct corpus comparison on weighting strategies; this is dataset-specific
- Break condition: If rare-class labels are highly noisy or inconsistent, Sqrt weighting may amplify errors.

### Mechanism 3
- Claim: Tiled inference with overlap averaging enables high-resolution aerial image segmentation within GPU memory constraints.
- Mechanism: Full 5472×3648 images exceed GPU memory for most models. The paper divides images into 3×3 overlapping tiles (2048×1366 pixels), averages predictions in overlap regions, and reconstructs the full segmentation map. This preserves spatial detail while fitting within 24GB VRAM (RTX 3090).
- Core assumption: Overlap regions provide consistent predictions that average smoothly.
- Evidence anchors:
  - [section 4.1, p.8] "each image was divided into 3×3 overlapping tiles... prediction inside overlapped areas is averaged"
  - [corpus] SU-ESRGAN paper addresses super-resolution for drone imagery but uses different inference strategies
- Break condition: If tile boundaries introduce artifacts or objects span tiles without sufficient overlap, segmentation coherence degrades.

## Foundational Learning

- Concept: **Semantic Segmentation Evaluation Metrics (IoU/mIoU)**
  - Why needed here: The paper uses Intersection-over-Union (IoU) as the primary metric for evaluating pixel-wise classification accuracy. Understanding mIoU is essential to interpret Table 4 and per-class results.
  - Quick check question: Given a predicted mask with 80% overlap with ground truth, what is the IoU? (Answer: IoU = Overlap / Union ≈ 0.67 if false positives/negatives are equal)

- Concept: **Class Imbalance and Weighting Strategies**
  - Why needed here: MESSI has extreme imbalance (Transportation Terrain ≈ 40% vs. Bicycle < 1%). The paper compares Equal, Prop, and Sqrt weighting—understanding these is required to replicate or extend experiments.
  - Quick check question: Why might inverse-proportional weighting harm training on very rare classes? (Answer: Over-amplifies gradient noise, leading to instability)

- Concept: **Transfer Learning from Ground-Level to Aerial Domains**
  - Why needed here: Models were initialized with Cityscapes weights (ground-level urban scenes) despite MESSI being aerial. The paper assumes multi-scale object representations transfer across viewpoints.
  - Quick check question: What domain shifts exist between Cityscapes (ground-level) and MESSI (downward aerial)? (Answer: Camera perspective, object scale distribution, occlusion patterns)

## Architecture Onboarding

- Component map:
  - Raw 5472×3648 images -> 3×3 tiling (2048×1366) -> random crop (1024×1024) for training -> overlap averaging at inference
  - Models Tested: BiSeNetV1 (ResNet-18/50), DeepLabV3+ (ResNet-18/50), SegFormer (MiT-B0/B3), Mask2Former (Swin-B)
  - Training Infrastructure: MMsegmentation framework, RTX 3090 (24GB) for most models, A6000 (48GB) for Mask2Former
  - Data Splits: Training = Agamim Descend (295 images), Validation = Agamim Paths A-C (325 images), Test = Ir Yamim + Ha-Medinah Square (350 images)

- Critical path:
  1. Download MESSI dataset from https://isl.cs.technion.ac.il/research/messi-dataset/
  2. Set up MMsegmentation environment (Python, PyTorch, CUDA compatible with GPU)
  3. Configure dataset class names and colors per Table 8 taxonomy
  4. Select model (recommend SegFormer-B3 as baseline per paper results)
  5. Configure Sqrt weighting in loss function
  6. Train for 320 epochs with Cityscapes pretrained weights
  7. Run tiled inference on test set, submit to benchmark

- Design tradeoffs:
  - **Tile size vs. memory**: Larger tiles capture more context but require more VRAM. Paper uses 2048×1366; smaller tiles reduce memory but may lose spatial context.
  - **Model size vs. speed**: SegFormer-B3 achieves best mIoU (52.6%) but is ~37× slower than BiSeNetV1-18 per crop (316ms vs. 8.6ms). Real-time applications may require BiSeNetV1.
  - **Sqrt vs. Equal weighting**: Sqrt improves rare-class recall but may reduce precision on common classes. Choose based on application priority (e.g., safety-critical detection of persons vs. overall accuracy).

- Failure signatures:
  - **Mask2Former underperformance**: Paper reports Mask2Former-Swin-B achieves lower mIoU (44.5%) than SegFormer-B3 (52.6%) despite being larger. Cause: region-based attention struggles with small objects (bicycle IoU: 2.66% vs. 26.5% for SegFormer). Avoid for small-object-heavy aerial scenes.
  - **Building class confusion**: Buildings are misclassified as Soft Terrain (grass rooftops) or Shed (shaded roof areas). Paper excludes Building class to improve metrics—consider this if rooftop accuracy is non-critical.
  - **Rare-class collapse**: Water class IoU ranges 0–2.1% due to extreme scarcity and confusion with building rooftops (pools). Monitor per-class IoU, not just mIoU.

- First 3 experiments:
  1. **Baseline Replication**: Train SegFormer-B3 with Sqrt weighting on provided training split, evaluate on validation set. Target: mIoU ~52-53%. Verify data pipeline, tiling, and metric computation match paper.
  2. **Ablation on Altitude Coverage**: Train two models—one on single altitude (e.g., 50m only), one on multi-altitude (30, 50, 70, 100m). Test both on vertical descent sequences. Quantify accuracy degradation vs. altitude gap (per Fig. 7).
  3. **Weighting Strategy Comparison**: Run SegFormer-B3 with Equal, Sqrt, and Prop weighting. Log per-class IoU for rare classes (Person, Bicycle, Water) to understand tradeoffs between overall mIoU and rare-class sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can semantic segmentation models be optimized to maintain accuracy when inference altitudes differ significantly from training altitudes (e.g., training high, flying low)?
- Basis in paper: [explicit] The authors state that MESSI allows researchers to "model phenomena such as the effect of how training a network at single or multiple altitudes improves or degrades model performance."
- Why unresolved: Figure 7 shows a distinct trade-off where training at high altitudes degrades performance at low altitudes due to the loss of spatial resolution, and vice-versa.
- What evidence would resolve it: A training strategy or network architecture that yields a flatter accuracy curve across the vertical trajectory test set than the current SegFormer-B3 baseline.

### Open Question 2
- Question: Can Mask2Former achieve state-of-the-art performance on the MESSI dataset if implemented with overlapped tile inference?
- Basis in paper: [inferred] The paper notes Mask2Former underperformed (mIoU 44.5%) compared to SegFormer-B3 (mIoU 52.6%), specifically attributing this to the MMsegmentation implementation lacking support for "overlapped tiles," forcing inference on the full image.
- Why unresolved: The experiment was limited by memory constraints and software implementation, preventing a fair comparison of the architecture's true capability on high-resolution drone imagery.
- What evidence would resolve it: A modified Mask2Former implementation supporting tiled inference that surpasses the SegFormer-B3 baseline on the MESSI test set.

### Open Question 3
- Question: What specific techniques are required to resolve the confusion between semantically similar classes (e.g., Soft vs. Rough Terrain) and improve detection of rare classes?
- Basis in paper: [inferred] The confusion matrix (Fig. 6) and IoU tables show significant misclassification between similar terrains and near-zero or low scores for rare classes like 'Water' and 'Bicycle', which the authors note is a challenge.
- Why unresolved: The tested weighting schemes (Sqrt, Prop) failed to significantly boost performance for the rarest classes, and standard augmentation did not prevent confusion between visually similar "Terrain" categories.
- What evidence would resolve it: A model employing specialized loss functions or augmentation that increases the IoU for 'Bicycle' and 'Water' substantially above the reported 26.5% and 2.1% respectively.

## Limitations

- Geographic limitation: Dataset focuses exclusively on a single Israeli coastal urban area, limiting generalization to other geographic regions or climate zones
- Class imbalance challenges: Severe imbalance (Transportation Terrain at ~40% vs. Bicycle < 1%) creates challenges for rare-class detection, with Water achieving only 0-2.1% IoU despite Sqrt weighting
- Computational requirements: High-resolution images necessitate extensive tiling and overlapping inference, introducing potential boundary artifacts and requiring significant GPU resources

## Confidence

- **High Confidence**: The MESSI dataset construction methodology and class distribution statistics are well-documented and reproducible. The tiling/inference pipeline and Sqrt weighting benefits are empirically validated through controlled experiments.
- **Medium Confidence**: The superiority of SegFormer-B3 over other models is supported by results, but the specific cause of Mask2Former's underperformance requires further investigation. The transfer learning assumption from Cityscapes to aerial domains is reasonable but not rigorously tested.
- **Low Confidence**: The long-term generalization of multi-elevation training benefits across vastly different urban environments and the stability of Sqrt weighting for extremely rare classes (water, stairs) remain unproven.

## Next Checks

1. **Cross-Geographic Validation**: Test trained models from MESSI on drone imagery from different urban environments (e.g., European cities, Asian megacities) to quantify geographic generalization limits and identify failure modes.

2. **Extreme Altitude Testing**: Evaluate model performance on imagery captured at altitudes significantly outside the 30-100m training range (e.g., 5m, 150m) to determine the practical bounds of multi-elevation training benefits and identify altitude-specific failure patterns.

3. **Rare-Class Robustness Analysis**: Conduct ablation studies systematically removing rare-class samples from training to quantify the noise sensitivity of Sqrt weighting and determine the minimum sample threshold for stable rare-class segmentation.