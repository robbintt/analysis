---
ver: rpa2
title: 'ReaLitE: Enrichment of Relation Embeddings in Knowledge Graphs using Numeric
  Literals'
arxiv_id: '2504.00852'
source_url: https://arxiv.org/abs/2504.00852
tags:
- realite
- relation
- numeric
- literals
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReaLitE, a novel method for enhancing knowledge
  graph embeddings by incorporating numeric literals associated with entities. While
  existing approaches either integrate numeric values into entity embeddings or convert
  them into entities during preprocessing, ReaLitE uniquely focuses on enriching relation
  embeddings by dynamically aggregating and merging numeric attributes from connected
  entities.
---

# ReaLitE: Enrichment of Relation Embeddings in Knowledge Graphs using Numeric Literals

## Quick Facts
- **arXiv ID**: 2504.00852
- **Source URL**: https://arxiv.org/abs/2504.00852
- **Reference count**: 40
- **Primary result**: Relation-centric enrichment of KG embeddings with numeric literals outperforms entity-centric approaches, achieving up to 17% improvement in MRR on link prediction tasks.

## Executive Summary
This paper introduces ReaLitE, a novel method for enhancing knowledge graph embeddings by incorporating numeric literals associated with entities. Unlike existing approaches that integrate numeric values into entity embeddings or convert them into entities during preprocessing, ReaLitE uniquely focuses on enriching relation embeddings by dynamically aggregating and merging numeric attributes from connected entities. The method supports multiple aggregation strategies, including a learnable approach that combines different aggregation types. The authors comprehensively evaluate ReaLitE on both link prediction and node classification tasks using several benchmark datasets, demonstrating that relation-centric enrichment is more effective than entity-centric approaches.

## Method Summary
ReaLitE operates by first preprocessing numeric literals associated with entities connected by each relation. For a given relation, it aggregates the numeric values of all head entities and all tail entities into two vectors (l_h, l_t) using various aggregation strategies including mean, mode, variance, or a learnable combination. These aggregated vectors are then merged with the original relation embedding r using a fusion function g (either linear or gated). The enriched relation embedding r_lit is then used in standard KGE scoring functions. This approach dynamically encodes statistical correlations between numeric attributes of connected entities directly into the relation's representation, rather than treating literals as separate entities or modifying entity embeddings.

## Key Results
- Achieves up to 17% improvement in MRR over vanilla models on FB15k-237 for link prediction
- Shows particular strength on long-tail relations and relations with correlated numeric attributes
- Improves micro-F1 scores by up to 16% compared to baseline KGE models for node classification
- Outperforms state-of-the-art literal-aware KGE methods across multiple base models including TransE, DistMult, ComplEx, RotatE, and TuckER

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enriching relation embeddings (rather than entity embeddings) with aggregated numeric literals from head and tail entities improves link prediction accuracy, particularly for relations with correlated numeric attributes.
- **Mechanism:** ReaLitE aggregates the numeric values of all head entities and all tail entities connected by a specific relation into two vectors (l_h, l_t). These vectors are merged with the relation embedding r via a function g(r, l_h, l_t). This effectively encodes the statistical correlation between head and tail attributes (e.g., monthly income ≈ 3 * monthly rent) directly into the relation's representation.
- **Core assumption:** Numeric attributes of entities connected by a relation share a statistical distribution or correlation that is predictive of the link's existence, which entity-centric models miss.
- **Evidence anchors:**
  - [Abstract]: "...ReaLitE uniquely focuses on enriching relation embeddings by dynamically aggregating and merging numeric attributes from connected entities."
  - [Section 1]: Figure 1 illustrates the "potential benefit of the mathematical relationship between people's income and houses' rent."
  - [Section 6.1]: "ReaLitE can significantly boost the performance... especially TransE enjoys an increase of 47%... for triples with more correlated literals."
  - [Corpus]: NumCoKE (arXiv:2411.12950) supports the general premise that leveraging numerical attributes for reasoning improves over symbolic-only methods, though it does not validate the specific relation-centric approach.
- **Break condition:** If numeric attributes are randomly distributed or lack correlation for a given relation, the aggregation may add noise to the relation embedding, degrading performance.

### Mechanism 2
- **Claim:** A learnable combination of aggregation functions (mean, mode, variance, etc.) outperforms fixed heuristics because optimal aggregation depends on the data distribution of the specific attribute.
- **Mechanism:** Instead of fixing the preprocessing to a single function like mean, ReaLitE can learn a weighted combination y of 11 aggregation types (including central tendency and dispersion measures) using a sigmoid-gated linear layer. This allows the model to determine if the mode or the variance of an attribute is more informative for the downstream task.
- **Core assumption:** Different numeric attributes have different statistical properties (e.g., skewed vs. uniform) that require different summary statistics to be useful in a low-dimensional embedding.
- **Evidence anchors:**
  - [Section 4.1]: "We treat the aggregation method... as a tunable hyperparameter... Additionally, we provide the option to use a learnable linear combination y."
  - [Section 6.1]: Table 7 shows "mode" is best for TransE/FB15k, while "mean" is best for TuckER, validating that optimal aggregation varies.
  - [Corpus]: No direct evidence found regarding learnable aggregation of literals in the provided neighbors.
- **Break condition:** If the dataset is extremely sparse (few values per attribute), the learned weights may overfit to noise in the aggregation statistics.

### Mechanism 3
- **Claim:** A gated fusion mechanism allows the model to selectively bypass numeric information when it is irrelevant, preventing the degradation of base model performance.
- **Mechanism:** The fusion function g (specifically g_gated) uses a gating vector z to interpolate between the original relation embedding r and the numeric-informed transformation. If the literal information l_h, l_t proves unhelpful for a specific relation during training, the gate learns to output values close to the original r.
- **Core assumption:** Not all relations in a knowledge graph have meaningful or available numeric correlations; forcing numeric integration everywhere can be detrimental.
- **Evidence anchors:**
  - [Section 4.2]: "The latter [g_gated] employs a gated mechanism... because the authors claim this mechanism lets g use or ignore the numeric information as needed."
  - [Section 6.2]: "In this context, ReaLitE uses g_gated as it yielded better results than g_lin [for Node Classification]."
  - [Corpus]: No direct evidence found regarding gating mechanisms in the provided neighbors.
- **Break condition:** If the gate initialization is biased incorrectly, it might suppress useful signals (dead gate) or fail to filter noise.

## Foundational Learning

- **Concept:** **Knowledge Graph Embeddings (KGE)**
  - **Why needed here:** ReaLitE is not a standalone model but a "plug-in" for existing KGEs (TransE, ComplEx, etc.). Understanding that these models score triples based on vector proximity is required to grasp how ReaLitE modifies the scoring pipeline.
  - **Quick check question:** Can you explain how TransE calculates the score of a triple (h, r, t) using vector addition?

- **Concept:** **Numeric Literals vs. Relations**
  - **Why needed here:** The paper specifically targets the integration of "literals" (attribute values like numbers) which are distinct from "relations" (links between entities). Distinguishing these data types is the core problem ReaLitE solves.
  - **Quick check question:** In the triple (Person, hasAge, 30), which part is the literal and which is the relation?

- **Concept:** **Aggregation Statistics**
  - **Why needed here:** ReaLitE reduces a matrix of numeric values into a vector using statistics like mean, std, or mode. Understanding what these represent is necessary to interpret the "Learnable Aggregation" mechanism.
  - **Quick check question:** Why might the median be a better aggregation statistic than mean for the attribute "salary" in a company with one billionaire and 99 interns?

## Architecture Onboarding

- **Component map:** Input Layer (KG Triples, Literal Matrix) -> Preprocessing (Construct L_h, L_t) -> Aggregator (Reduce to l_h, l_t) -> Fusion (Function g combines r with l_h, l_t) -> Scoring (Standard KGE scorer uses r_lit)
- **Critical path:** The transformation L → l_h, l_t → r_lit is the critical path. Any bug in the aggregation (e.g., handling NaNs) or the fusion gate will propagate errors to the final triple score.
- **Design tradeoffs:**
  - **g_lin vs. g_gated:** Linear fusion (g_lin) is simpler but forces numeric integration; Gated fusion (g_gated) adds parameters but offers robustness against irrelevant literals. The paper suggests g_lin may be better for Link Prediction, while g_gated is better for Node Classification.
  - **Aggregation choice:** Fixed aggregation is fast; Learnable aggregation adds overhead but adapts to data distributions.
- **Failure signatures:**
  - **Performance Drop on Frequent Relations:** If ReaLitE underperforms on high-frequency relations compared to baselines, check if the aggregation is dominated by outliers (Table 5 suggests ReaLitE excels on "Long-tail" but can be mixed on "Frequent").
  - **NaN Gradients:** If literals contain missing values represented as NaNs without proper masking during aggregation, the fusion layer will crash.
- **First 3 experiments:**
  1. **Baseline Integration:** Implement ReaLitE with TransE on FB15k-237 using mean aggregation and g_lin. Verify that MRR improves over vanilla TransE to match Table 4 results (~0.315 to ~0.335).
  2. **Ablation on Aggregation:** Using the best TransE setup, swap mean for mode and learnable. Confirm if mode provides the boost seen in Table 7 for this specific dataset.
  3. **Node Classification Check:** Train DistMult+ReaLitE on dmg777k. Compare g_lin vs. g_gated to verify the paper's finding that the gated mechanism is superior for this downstream task (Table 8).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the aggregation of numeric literals be optimized adaptively per relation to account for value distribution skewness, given that the proposed learnable linear combination underperformed compared to fixed strategies?
- Basis in paper: [explicit] Page 6 notes that aggregation performance varies with "skewness of the value distribution," and Page 14 reports that the learnable function "y" was the least used optimal configuration (10%), with "median" never selected.
- Why unresolved: The authors treat aggregation as a tunable hyperparameter but do not propose a mechanism that dynamically selects or weights aggregation functions based on the specific statistical properties of a relation's literals.
- What evidence would resolve it: A modified version of ReaLitE that selects aggregation strategies based on attribute distribution metrics (e.g., kurtosis) outperforming the current static or globally learnable approaches.

### Open Question 2
- Question: Can the relation-centric enrichment strategy of ReaLitE be effectively generalized to non-numeric literals, such as text descriptions or images, without information loss?
- Basis in paper: [inferred] The method is strictly defined for numeric attributes (R^|A|), yet the "Related Work" (Page 2) identifies "multimodal KG completion" (including images/text) as a key research area where entity-conversion methods cause information loss.
- Why unresolved: The function g relies on vector arithmetic (e.g., W^T[l_h, r, l_t]), and it is unclear how unstructured modalities would be aggregated into the relation embedding without simply concatenating entity embeddings, which the paper argues against.
- What evidence would resolve it: An extension of ReaLitE applied to multimodal datasets (e.g., FB15k-237-IMG) demonstrating that fusing text/image features into relation embeddings outperforms entity-centric multimodal baselines.

### Open Question 3
- Question: What are the theoretical interactions between the literal-enriched relation embeddings and specific KGE scoring functions, particularly regarding the superior performance gains observed in ComplEx?
- Basis in paper: [explicit] Page 12 highlights that ReaLitE provides the largest gains for ComplEx (up to 17%) and suggests this is due to the separate enhancement of real and imaginary embedding parts, but it stops short of a full theoretical explanation.
- Why unresolved: The paper empirically observes the variance in performance gains across models (e.g., ComplEx vs. RotatE) but does not fully explain why enriching relation embeddings is theoretically more compatible with complex vector spaces than with rotational or tensor factorization approaches.
- What evidence would resolve it: An ablation study analyzing the gradient updates or geometric transformations in the complex plane for ReaLitE-enhanced ComplEx versus the geometric space of RotatE.

## Limitations

- **Hyperparameter sensitivity:** The paper omits critical details about learning rate, optimizer, batch size, and embedding dimensions, which could materially impact reproducibility and performance claims.
- **Scalability concerns:** Only one small KG (dmg777k) was tested for node classification, making it difficult to assess performance on larger-scale knowledge graphs.
- **Sparse data handling:** The method's effectiveness on relations with very few connected literals or highly sparse numeric attributes remains unverified.

## Confidence

**High Confidence**: The core conceptual contribution of relation-centric enrichment is well-supported by the evaluation results and ablation studies, particularly the performance gains on long-tail relations and the superiority of gated fusion for node classification.

**Medium Confidence**: The generalizability of the approach to other KGE models and datasets requires validation, as the ablation studies show aggregation strategy effectiveness varies significantly by base model and dataset.

**Low Confidence**: The scalability analysis is limited, with only one small KG (dmg777k) tested for node classification, making it difficult to assess performance on larger-scale knowledge graphs.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rate, batch size, and embedding dimension to establish performance bounds and identify optimal configurations for different base models.

2. **Cross-Dataset Robustness**: Implement ReaLitE on additional KG datasets (e.g., Wikidata, DBpedia) with varying literal distributions to validate the method's effectiveness across different domains.

3. **Ablation on Gating Mechanism**: Conduct controlled experiments disabling the gating mechanism to quantify its contribution to preventing performance degradation when literals are irrelevant or noisy.