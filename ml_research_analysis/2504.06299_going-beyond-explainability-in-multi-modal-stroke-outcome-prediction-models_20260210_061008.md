---
ver: rpa2
title: Going beyond explainability in multi-modal stroke outcome prediction models
arxiv_id: '2504.06299'
source_url: https://arxiv.org/abs/2504.06299
tags:
- outcome
- data
- prediction
- explanation
- maps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study adapts xAI methods Grad-CAM and Occlusion to deep transformation
  models (dTMs) for predicting functional outcomes after stroke using brain imaging
  and tabular patient data. The core idea is to combine interpretable statistical
  models for tabular features with deep learning models for imaging data, achieving
  both state-of-the-art prediction performance and interpretability.
---

# Going beyond explainability in multi-modal stroke outcome prediction models

## Quick Facts
- arXiv ID: 2504.06299
- Source URL: https://arxiv.org/abs/2504.06299
- Reference count: 15
- Primary result: dTMs achieve AUC values close to 0.8 for stroke outcome prediction while providing interpretable explanations of brain regions and tabular features

## Executive Summary
This study adapts XAI methods Grad-CAM and Occlusion to deep transformation models (dTMs) for predicting functional outcomes after stroke using brain imaging and tabular patient data. The core innovation is combining interpretable statistical models for tabular features with deep learning models for imaging data, achieving both state-of-the-art prediction performance and interpretability. The adapted methods highlight relevant brain regions and enable error analysis. The dTMs achieve AUC values close to 0.8, with the most important tabular predictors being functional independence before stroke and NIHSS on admission.

## Method Summary
The study implements Deep Transformation Models (dTMs) that combine a 3D CNN for processing DWI brain volumes with a linear shift term for tabular patient data. The transformation function h is constructed additively as CNN(Image) + Linear(Tabular), preserving the statistical interpretability of structured data while retaining deep learning's pattern-recognition power. The model uses a sigmoid function to map the transformation score to probability. Grad-CAM and Occlusion methods are adapted for 3D dTMs by computing gradients from the transformation function component rather than raw probability scores. Deep ensembling with 5 random initializations improves stability of both predictions and explanation maps.

## Key Results
- dTMs achieve AUC values close to 0.8 for binary classification of favorable vs unfavorable functional outcomes
- Most important tabular predictors identified as functional independence before stroke and NIHSS on admission
- Explanation maps reveal critical brain regions, particularly the frontal lobe, which is linked to age and unfavorable outcomes
- Dark explanation maps correlate with false unfavorable predictions in TIA patients
- The method successfully combines state-of-the-art prediction performance with interpretable explanations

## Why This Works (Mechanism)

### Mechanism 1
Separating tabular and imaging processing pathways via an additive transformation function preserves the statistical interpretability of structured data while retaining the pattern-recognition power of deep learning. The Deep Transformation Model (dTM) constructs the prediction function h as an additive sum: h = CNN(Image) + Linear(Tabular). Because the tabular term x^Tβ remains linearly connected to the output, the coefficients β can be interpreted as log-odds ratios, identical to standard logistic regression, independent of the complex CNN processing.

### Mechanism 2
Adapting gradient-based explainability (Grad-CAM) to the transformation function h rather than a raw probability score isolates the specific anatomical features driving the predicted outcome distribution. Standard Grad-CAM backpropagates from the class probability. Here, the gradient is calculated from the transformation function h (specifically ∂h/∂A) via the chain rule through the sigmoid function. This ensures the explanation map reflects how the image alters the latent transformation score, effectively highlighting regions like the frontal lobe that correlate with the outcome.

### Mechanism 3
Ensembling multiple transformation functions with learned weights improves the stability of explanation maps and reduces spurious correlations compared to single models. Instead of averaging probabilities, the method averages the transformation functions (h̄_wgt) from multiple initializations. This "deep ensembling" reduces the variance of the learned model parameters (β) and the activation maps (A), leading to more robust identification of critical regions (e.g., frontal lobe) and filtering out noise.

## Foundational Learning

**Transformation Models (TMs)**: Why needed here - The entire architecture relies on modeling a conditional probability distribution F_Y|input not by predicting Y directly, but by transforming a standard latent distribution (Logistic) via a function h. Understanding this is required to grasp why the model outputs a "transformation function" instead of a class score. Quick check: If F_Z is a standard logistic distribution, what does the output of the neural network h(y_0|x) represent in terms of log-odds?

**3D Convolutional Neural Networks (3D CNNs)**: Why needed here - The imaging input is 3D (brain volumes). The architecture uses 3D kernels to capture spatial volumetric context, which is critical for the Grad-CAM adaptation (generating 3D activation maps). Quick check: How does the receptive field of a 3D CNN differ from a 2D CNN when processing a stack of MRI slices?

**Occlusion Sensitivity**: Why needed here - This is one of the two primary XAI methods adapted. It serves as a robustness check (perturbation-based) against the gradient-based Grad-CAM. Understanding the trade-off (occlusion is slower but often more faithful to the model's local behavior) is key. Quick check: Why might occlusion maps be "noisier" or computationally more expensive to generate than gradient-based maps?

## Architecture Onboarding

**Component map**: Inputs: 3D DWI + Tabular Vector (Age, NIHSS, etc.) → Encoders: 3D CNN (Complex Intercept ϑ_0(B)) + Linear Layer (Linear Shift x^Tβ) → Fusion: Additive node combining CNN output and Linear output to form Transformation Function h → Head: Sigmoid function σ(h) mapping transformation score to probability → XAI Module: Gradient hooks on the last 3D CNN layer (Grad-CAM) and perturbation loop (Occlusion) projecting back to input space

**Critical path**: The gradient flow from the Sigmoid output back to the 3D CNN activations. If the transformation function h is not correctly differentiable, Grad-CAM will fail.

**Design tradeoffs**: SI-LSx vs. CI B-LSx: The paper shows the CIB-LSx model (Complex Intercept + Linear Shift) has high performance (AUC ~0.81) but requires XAI for the image part. The SI-LSx (Tabular only) is fully interpretable without XAI but has lower performance. Interpretability vs. Performance: Using the "Linear Shift" for tabular data ensures we get Odds Ratios, but assumes linearity. A "Complex Shift" (using a NN for tabular data) might improve performance but would destroy the direct interpretability of the tabular features.

**Failure signatures**: Dark Explanation Maps: Observed in TIA patients or cases with large blurred lesions, often correlating with false unfavorable predictions. Frontal Lobe Confounding: If the model highlights the frontal lobe heavily, it may be relying on "brain shrinkage" (atrophy) as a proxy for Age rather than the stroke pathology itself. Adding Age to the tabular features should suppress this.

**First 3 experiments**: 1. Baseline Linearity: Train the SI-LSx model on the tabular data alone. Verify that the calculated Odds Ratios for NIHSS and pre-stroke mRS align with clinical expectations before introducing the complexity of imaging. 2. Image-Only Sanity Check: Train the CIB model (Image only) and generate Grad-CAM maps. Check if the highlighted regions correspond to the visible stroke lesion or if they highlight unrelated artifacts (checking for "clever Hans" effects). 3. Confounding Analysis: Train CIB-LSx (Multi-modal) and compare the Grad-CAM maps against the CIB-only maps. Verify if adding the "Age" tabular feature reduces the activation intensity in the Frontal Lobe (confirming the confounding hypothesis described in the paper).

## Open Questions the Paper Calls Out
- Can pre-trained foundation imaging models improve the prediction performance of deep transformation models (dTMs) for stroke outcomes compared to training from scratch? The authors state that "the incorporation of more data or the use of a pretrained foundation imaging model may further improve prediction performance."
- Does the explanation map highlighting of the frontal lobe primarily represent age-related brain atrophy rather than stroke-specific pathology? The authors hypothesize that the highlighted frontal lobe is important due to its correlation with age, supported by the observation that highlighting disappears when age is added as a tabular feature.
- Can characteristic patterns in explanation maps, such as "dark" maps, be operationalized to automatically identify and correct false predictions? The paper notes that dark explanation maps are mostly generated in TIA patients often falsely predicted having an unfavorable outcome, yet no automated correction was implemented.

## Limitations
- Small sample size (407 patients) and single-center data limit external validity
- Additive architecture assumes no complex interactions between imaging and tabular features
- Explanation maps require clinical validation studies to confirm their practical utility
- Implementation details for 3D Grad-CAM and Occlusion are only partially specified

## Confidence
- Claim: dTMs achieve both state-of-the-art performance and interpretability (AUC ~0.8) - **High** confidence
- Claim: Adaptation of XAI methods to dTMs is novel and effective - **Medium** confidence
- Claim: Tabular parameters provide interpretable odds ratios - **High** confidence
- Claim: Clinical interpretation of frontal lobe highlighting is valid - **Medium** confidence

## Next Checks
1. **Confounding Analysis**: Add age as a tabular feature to verify if frontal lobe activations diminish, confirming the atrophy confounding hypothesis.
2. **Generalization Test**: Apply the trained model to an independent stroke cohort to assess performance stability and explanation map consistency.
3. **Ablation Study**: Train models with and without the imaging component to quantify the marginal benefit of multimodal learning versus pure tabular models.