---
ver: rpa2
title: 'RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large
  Language Models'
arxiv_id: '2508.01174'
source_url: https://arxiv.org/abs/2508.01174
tags:
- pass
- rspo
- responses
- policy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the mismatch between risk-neutral training
  objectives and risk-seeking evaluation metrics (Pass@k and Max@k) in large language
  model post-training. The authors propose Risk-Seeking Policy Optimization (RSPO),
  which directly optimizes these metrics by deriving closed-form expressions for the
  probability that a given response is the maximum among k samples.
---

# RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large Language Models

## Quick Facts
- arXiv ID: 2508.01174
- Source URL: https://arxiv.org/abs/2508.01174
- Reference count: 40
- Primary result: RSPO directly optimizes risk-seeking evaluation metrics (Pass@k, Max@k) by deriving closed-form probability expressions, resolving the "hitchhiking" problem where low-reward responses are reinforced alongside high-reward ones.

## Executive Summary
This paper addresses the mismatch between risk-neutral training objectives and risk-seeking evaluation metrics (Pass@k and Max@k) in large language model post-training. The authors propose Risk-Seeking Policy Optimization (RSPO), which directly optimizes these metrics by deriving closed-form expressions for the probability that a given response is the maximum among k samples. This approach resolves the "hitchhiking" problem where low-reward responses are inadvertently reinforced when co-occurring with high-reward responses. RSPO produces efficient, unbiased gradient estimators for both metrics through combinatorial techniques. Extensive experiments on math reasoning tasks demonstrate that RSPO consistently outperforms baseline algorithms across multiple datasets, achieving optimal performance when the training hyperparameter k matches the evaluation metric. The method shows improved scalability and robustness, effectively addressing the evaluation-training objective misalignment.

## Method Summary
RSPO introduces closed-form gradient estimators for Pass@k and Max@k metrics by deriving the probability that a response is the maximum among k samples. For Pass@k, the weight is $k \frac{\binom{n-c}{k-1}}{\binom{n-1}{k-1}} R(x,y)$, where c is the count of correct responses. For Max@k, the weight captures the marginal contribution of a response toward increasing the expected maximum reward. The method requires generating n responses per prompt (n â‰¥ 2k) and computing combinatorial weights using U-statistic estimators to avoid bias. Implementation uses the verl framework and optimizes Qwen2.5-Math models with standard hyperparameters (LR=3e-6, 10 epochs).

## Key Results
- RSPO consistently outperforms baseline algorithms across multiple math reasoning datasets (Math500, AIME2024, AMC, Minerva, OlympiadBench)
- Performance peaks when training k matches evaluation k, validating the alignment hypothesis
- RSPO effectively solves the "hitchhiking" problem where low-reward responses were inadvertently reinforced
- The method demonstrates improved scalability and robustness compared to existing approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If the probability of a response being the maximum in a set is computed explicitly, it decouples the reinforcement of high-reward samples from low-reward samples in the same batch.
- **Mechanism:** The paper derives a closed-form expression (Theorem 4.1) for the probability that a response $y$ is the maximum among $k$ samples. This transforms the objective from a joint distribution over $k$ responses (where hitchhiking occurs) into a single-response distribution. By weighting gradients by $P(y \text{ is max})$, low-reward responses that co-occur with high-reward ones no longer receive gradient credit, solving the "hitchhiking" problem.
- **Core assumption:** Assumes the reward function $R(x,y)$ is sufficiently reliable to establish a strict ordering of response quality.
- **Evidence anchors:** [abstract] "RSPO addresses this problem by leveraging the closed-form probability that a given response is the maximum among k samplings." [section 4.1] Theorem 4.1 derives the probability decomposition; Figure 2 illustrates the decoupling.

### Mechanism 2
- **Claim:** For Pass@k, assigning weights that decay as success probability increases may prevent entropy collapse and allocate capacity to unsolved problems.
- **Mechanism:** The derived Pass@k gradient weight is $k(1 - w_\theta)^{k-1}$. As the policy's probability of success ($w_\theta$) rises, this weight naturally decreases. This acts as an "opportunity cost" mechanism: the model stops wasting gradient updates on examples it already solves with high probability (Pass@k $\approx$ 1), preserving entropy and focusing capacity on harder prompts.
- **Core assumption:** Assumes that preserving probability mass for exploration is more valuable than further maximizing the probability of already-solved responses.
- **Evidence anchors:** [section 4.2] "The resulting gradient weight... naturally decreases as $w_\theta$ increases... saving opportunity cost." [figure 7] Shows RSPO maintains higher entropy over training steps compared to baselines.

### Mechanism 3
- **Claim:** Optimizing Max@k requires weighting responses by their marginal contribution relative to lower-reward alternatives.
- **Mechanism:** The Max@k gradient weight approximates $k \sum_{y' < y} (R(y) - R(y')) P(y' \text{ is max})$. This measures the "marginal gain" in maximum reward if response $y$ is selected. It penalizes the model for generating $y$ if it displaces a response with a similar or higher reward, ensuring gradients push strictly for new maxima.
- **Core assumption:** Assumes a continuous or granular reward signal where marginal differences $(R(y) - R(y'))$ are informative.
- **Evidence anchors:** [section 4.3] "The gradient weight... captures the marginal contribution of action y toward increasing the expected maximum reward." [equation 12] Shows the unbiased estimator subtracts the weighted sum of lower rewards.

## Foundational Learning

- **Concept: Risk-Neutral vs. Risk-Seeking Objectives**
  - **Why needed here:** The paper's premise is that standard RL (risk-neutral, optimizing mean reward) is mismatched with inference usage (risk-seeking, optimizing best-of-k).
  - **Quick check question:** If you sample 10 responses and one is perfect but 9 are wrong, does a risk-neutral optimizer punish the model heavily for the 9 failures?

- **Concept: Policy Gradient Theorem**
  - **Why needed here:** RSPO derives closed-form gradients $\nabla \log \pi$. Understanding how rewards scale log-probabilities is required to interpret the "weights" in Section 4.
  - **Quick check question:** How does multiplying the reward by $\nabla \log \pi(y|x)$ change the probability of generating $y$ in the future?

- **Concept: Combinatorial Estimators (U-statistics)**
  - **Why needed here:** The paper uses combinatorial terms like $\binom{n-c}{k-1}$ to construct unbiased estimators of probability tails $(1-w)^{k-1}$.
  - **Quick check question:** Why is a naive Monte Carlo estimate of $(1-w)^{k-1}$ biased compared to the combinatorial approach using subsets?

## Architecture Onboarding

- **Component map:** Sampler -> Rewarder -> Weight Estimator -> Optimizer
- **Critical path:** Calculating the combinatorial weights (specifically the subsets in Theorem 4.3/4.4) is the novel computational bottleneck. Implementation must efficiently handle $\binom{n}{k}$ logic without floating point overflow (see text: use logarithms or iterative multiplication).
- **Design tradeoffs:**
  - **Hyperparameter $k$:** Aligning training $k$ with evaluation $k$ yields best results (Section 5.2), but high $k$ requires higher $n$ (samples per prompt) to reduce variance.
  - **Sample Size $n$:** Paper recommends $n \ge 2k$. Lower $n$ reduces compute but forces gradient weights to be extreme ($k$ or 0), reducing flexibility.
- **Failure signatures:**
  - **Gradient Vanishing:** If the model learns too fast, $w_\theta \to 1$, and Pass@k weights $\to 0$.
  - **Hitchhiking (Baseline):** If you implement the "Baseline" (group-level max reward), performance degrades as $k$ increases (Figure 6 blue lines).
- **First 3 experiments:**
  1. **Sanity Check (k=1):** Run RSPO with $k=1$. It should mathematically collapse to standard policy gradient (weight = 1). Verify this matches the baseline PG performance.
  2. **Hitchhiking Reproduction:** Implement the baseline Max@k gradient (assign max reward to all $k$ samples). Plot accuracy vs. $k$. You should see performance drop as $k$ increases (confirming Figure 6).
  3. **Alignment Test:** Train with $k=4$ and evaluate on Pass@1, Pass@4, Pass@8. Confirm that performance peaks specifically at Pass@4 to validate the alignment hypothesis.

## Open Questions the Paper Calls Out
- The community needs to agree on the most appropriate choice of k for measuring a model's reasoning capacity to resolve debates on RL's efficacy.
- The method's performance on non-mathematical tasks with sparse rewards (such as software engineering or long-horizon tool use) remains unverified.

## Limitations
- The paper assumes reliable reward functions for math problems; in domains with noisy or subjective rewards, the ordering of responses becomes unstable, potentially breaking the marginal contribution mechanism in Max@k optimization.
- The computational overhead of sorting responses and calculating combinatorial weights may become prohibitive for frontier models (70B+ parameters).
- The actual impact of these weights in non-linear neural networks remains an empirical question beyond the mathematical derivation.

## Confidence
- **Derivation Validity** (Confidence: High) - The closed-form expressions for Pass@k and Max@k gradient weights are mathematically derived and verified in the appendix.
- **Scalability to Large Models** (Confidence: Medium) - The paper demonstrates effectiveness on 1.5B and 7B models, but the computational overhead may become prohibitive for frontier models.
- **Reward Function Sensitivity** (Confidence: Low) - The paper assumes reliable reward functions for math problems; in domains with noisy or subjective rewards, the ordering of responses becomes unstable.

## Next Checks
1. **Robustness to Reward Noise** - Add random noise to the reward function (e.g., flip 5-10% of binary rewards) and measure degradation in RSPO performance compared to baselines.
2. **Sample Efficiency Scaling** - Systematically vary the number of samples per prompt (n=8, 16, 32) while keeping k fixed, and measure how performance scales for both RSPO and baselines.
3. **Real-World Math Benchmark** - Evaluate on actual competition mathematics (e.g., IMO problems) rather than curated datasets to test whether the metric alignment advantage holds in authentic, challenging settings.