---
ver: rpa2
title: 'Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large
  Language Models'
arxiv_id: '2601.20546'
source_url: https://arxiv.org/abs/2601.20546
tags:
- creativity
- novelty
- appropriateness
- cdat
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study critically evaluates whether the widely-used Divergent\
  \ Association Task (DAT) validly measures creativity in large language models (LLMs).\
  \ While DAT captures novelty through semantic distance, it ignores appropriateness\u2014\
  a key component of human creativity\u2014leading to inflated scores from random\
  \ or task-agnostic outputs."
---

# Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models

## Quick Facts
- **arXiv ID:** 2601.20546
- **Source URL:** https://arxiv.org/abs/2601.20546
- **Reference count:** 32
- **Primary result:** CDAT framework reveals systematic novelty-appropriateness trade-off in LLMs, with smaller models showing higher novelty and larger models favoring appropriateness

## Executive Summary
This study critically evaluates the validity of the Divergent Association Task (DAT) for measuring creativity in large language models, demonstrating that DAT's focus on semantic novelty alone leads to inflated scores from contextually inappropriate responses. To address this limitation, the authors introduce the Conditional Divergent Association Task (CDAT), which gates novelty evaluation on contextual appropriateness using a cue word constraint. Through systematic testing across multiple model families, the research reveals a fundamental trade-off: smaller models achieve higher CDAT scores with less emphasis on appropriateness, while advanced models prioritize appropriateness at the expense of novelty, suggesting that scaling and alignment procedures systematically shift models toward more conservative, less creative outputs.

## Method Summary
The study introduces CDAT as an extension of DAT that conditions novelty on appropriateness through a cue word constraint. For each cue word, models generate candidate words, which are then evaluated for appropriateness using a minimal gate criterion (e.g., contextual relevance to the cue). Only words passing this gate are considered for semantic distance calculations, with CDAT scores representing the mean semantic distance under this constraint. The evaluation spans multiple model families with varying sizes and training approaches, comparing CDAT performance against traditional DAT scores. The approach validates that all models pass the appropriateness gate, establishing novelty as a legitimate creativity signal when appropriately contextualized.

## Key Results
- All tested models successfully passed the appropriateness gate, validating novelty as a creativity signal when contextualized
- Smaller model families achieved higher CDAT scores with less emphasis on appropriateness
- Advanced model families demonstrated the opposite pattern, prioritizing appropriateness at lower novelty levels
- Systematic trade-off observed suggests scaling and alignment shift models toward conservative outputs

## Why This Works (Mechanism)
The CDAT framework works by introducing a conditional gate that filters semantically distant responses based on contextual appropriateness. This mechanism prevents the DAT's fundamental flaw where models can score highly through random or semantically distant but contextually irrelevant responses. By requiring that novel responses also satisfy a minimal appropriateness criterion relative to a cue word, CDAT ensures that creativity measurements reflect genuine creative generation rather than mere novelty extraction from training data.

## Foundational Learning
- **Semantic distance measurement:** Why needed - to quantify novelty in word associations; Quick check - verify distance metrics align with human semantic intuitions
- **Contextual appropriateness gating:** Why needed - to filter out irrelevant but novel responses; Quick check - test gate with human-annotated appropriateness judgments
- **Model scaling effects:** Why needed - to understand how size impacts creativity-appropriateness balance; Quick check - compare models across multiple size tiers
- **Alignment and instruction tuning:** Why needed - to assess how post-training modifications affect creative outputs; Quick check - compare base vs. aligned versions of same architecture
- **Evaluation framework validity:** Why needed - to ensure creativity metrics reflect true creative capability; Quick check - correlate metric scores with human creativity assessments

## Architecture Onboarding
**Component map:** Cue word -> Model generation -> Appropriateness gate -> Semantic distance calculation -> CDAT score
**Critical path:** The gating mechanism is critical - inappropriate responses must be filtered before distance calculation to prevent inflated scores
**Design tradeoffs:** Single cue word vs. multiple cues (simplicity vs. comprehensiveness), minimal vs. strict appropriateness criteria (flexibility vs. stringency)
**Failure signatures:** DAT inflates scores through random outputs; CDAT failure occurs when appropriateness gate is too strict (blocks valid creativity) or too lenient (allows inappropriate responses)
**First experiments:** 1) Test appropriateness gate with human-annotated validation set, 2) Compare CDAT scores across cue word domains, 3) Analyze correlation between DAT and CDAT scores for individual models

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Single cue word may not comprehensively capture multifaceted human creativity
- Appropriateness gate definition may not account for domain expertise and cultural context
- Findings show correlation between scaling and creativity-appropriateness trade-off but don't establish causation
- Alternative explanations (training data differences, optimization objectives) not fully explored

## Confidence
- DAT inflates creativity scores through random outputs: Medium confidence (empirical demonstration but not exhaustive)
- Scaling and alignment shift models toward conservative outputs: Medium confidence (correlation shown, causation unclear)
- CDAT validity as creativity measure: Medium confidence (framework sound but validation limited to gate-passing criterion)

## Next Checks
1. Test CDAT across multiple domain-specific cue words to verify robustness and comprehensiveness of single-cue approach
2. Conduct human evaluation studies comparing DAT, CDAT, and holistic creativity assessments to validate CDAT alignment with human judgments
3. Analyze model internal representations during CDAT tasks to determine whether novelty-appropriateness trade-off reflects genuine creative reasoning or surface-level pattern matching