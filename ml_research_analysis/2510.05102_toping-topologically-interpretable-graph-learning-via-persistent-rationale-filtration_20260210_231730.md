---
ver: rpa2
title: 'TopInG: Topologically Interpretable Graph Learning via Persistent Rationale
  Filtration'
arxiv_id: '2510.05102'
source_url: https://arxiv.org/abs/2510.05102
tags:
- graph
- topological
- learning
- persistent
- rationale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of interpretability in Graph Neural
  Networks (GNNs) when rationale subgraphs are complex and varied. Existing intrinsic
  interpretable GNN methods assume nearly invariant subgraph rationales across instances,
  which is unrealistic in many real-world scenarios.
---

# TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration

## Quick Facts
- **arXiv ID:** 2510.05102
- **Source URL:** https://arxiv.org/abs/2510.05102
- **Reference count:** 40
- **Key outcome:** Proposes a novel topological framework using persistent homology to identify complex, varied rationale subgraphs in GNNs, achieving up to 100% AUC on interpretation tasks.

## Executive Summary
This paper addresses the challenge of interpretability in Graph Neural Networks when rationale subgraphs are complex and varied, rather than nearly invariant across instances. Existing methods assume nearly invariant subgraph rationales, which is unrealistic in many real-world scenarios. The authors propose TopInG, a novel framework that leverages persistent homology to identify persistent rationale subgraphs by modeling rationale identification as a graph generation process with a self-adjusted topological constraint. The approach theoretically guarantees unique optimization of the ground truth under specific conditions and demonstrates superior performance on both predictive accuracy and interpretation quality.

## Method Summary
TopInG models rationale subgraph identification as a graph generation process, using a self-adjusted topological constraint called topological discrepancy to enforce a persistent topological distinction between rationale subgraphs and irrelevant counterparts. The framework learns a filtration function that assigns importance scores to edges, samples subgraphs via Gumbel-Softmax, and computes persistent homology to create a topological loss. A bimodal Gaussian prior stabilizes the learning process. The combined loss (Cross-Entropy + Topological Discrepancy + Prior) theoretically recovers the ground truth rationale under specific conditions regarding minimal structure and topological distinction.

## Key Results
- Achieves up to 100% AUC in interpretation tasks on benchmark datasets
- Significantly outperforms existing methods on challenging datasets with spurious correlations
- Improves upon state-of-the-art methods on both predictive accuracy and interpretation quality
- Particularly effective in handling variform rationale subgraphs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating rationale subgraphs from noise by maximizing the statistical distance between their topological evolution.
- **Mechanism:** TopInG computes persistent homology for both candidate rationale and complement, then minimizes topological discrepancy (Wasserstein distance) between their topological features.
- **Core assumption:** Ground-truth rationale possesses topological signatures that evolve differently over filtration compared to irrelevant background structure.
- **Evidence anchors:** Abstract states persistent topological distinction is enforced; page 5 explains topological structures follow distinct evolutionary paths.
- **Break condition:** Fails if rationale and noise share identical topological signatures.

### Mechanism 2
- **Claim:** Theoretical recovery of ground truth rationale via a specifically constrained loss landscape.
- **Mechanism:** Proves combined loss function has unique global optimum at ground truth rationale, provided rationale is minimal and smaller than noise graph.
- **Core assumption:** Rationale is topologically minimal relative to prediction task.
- **Evidence anchors:** Theorem 3.4 proves L(Ï†) is uniquely optimized by ground truth; page 1 states theoretical guarantees are provided.
- **Break condition:** Fails if rationale is not minimal or larger than noise component.

### Mechanism 3
- **Claim:** Stabilizing the learning process using a fixed bimodal prior instead of heuristic decay.
- **Mechanism:** Uses 2-mixture Gaussian prior forcing edges to cluster around two distinct values (0.25 and 0.75), naturally separating rationale and noise without decay scheduling.
- **Core assumption:** Edge importance distribution is bimodal (edges are either "rationale" or "noise").
- **Evidence anchors:** Page 6 explains fundamental difference from GSAT; page 7 provides default parameter values.
- **Break condition:** Fails if data requires continuous spectrum of edge importance rather than binary partition.

## Foundational Learning

- **Concept: Persistent Homology & Barcodes**
  - **Why needed here:** Core mathematical tool TopInG uses to represent graphs; barcode represents birth and death of topological features as graph grows.
  - **Quick check question:** If you add an edge that closes a triangle in a graph, does a 1-dimensional barcode interval begin or end?

- **Concept: Filtration Functions**
  - **Why needed here:** TopInG learns filtration function assigning importance scores to edges, defining order in which graph is built.
  - **Quick check question:** How does changing weight of single edge shift "death time" of cycle in persistence diagram?

- **Concept: Wasserstein / Bottleneck Distance**
  - **Why needed here:** Loss function compares two sets of barcodes (rationale vs. noise) using this distance metric.
  - **Quick check question:** Why is bottleneck distance considered "stable" (small changes in input cause small changes in distance)?

## Architecture Onboarding

- **Component map:** Backbone GNN -> Filtration Head -> Sampler -> TDA Layer -> Vectorization -> Prediction Head
- **Critical path:** Differentiable path through TDA Layer requiring gradients to flow through topological sorting/disjoint-set operations of persistence calculation
- **Design tradeoffs:**
  - Accuracy vs. Speed: Computing persistent homology is computationally expensive (~10 mins/epoch on SPmotif)
  - Backbone Selection: CIN++ yields better results (100% AUC) but is more complex than standard GIN
- **Failure signatures:**
  - Trivial Homology Collapse: Struggles on datasets like MUTAG where rationales lack cycles
  - Computational Stall: TDA layer becomes bottleneck on very large graphs
  - Prior Collapse: Misconfigured Gaussian prior weights may cause model to ignore topological loss
- **First 3 experiments:**
  1. Sanity Check (BA-2Motifs): Train with GIN backbone, verify AUC approaches ~99-100%
  2. Ablation on Discrepancy: Run without d_topo term, confirm performance drops significantly
  3. Spurious Correlation Test: Evaluate on SP-Motif (b=0.9), verify TopInG maintains high accuracy while baselines drop

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on strict conditions (minimal rationale size, sufficient topological distinction) that may not hold in real-world datasets
- Computational cost of persistent homology calculations limits scalability to larger graphs
- Use of synthetic benchmark datasets with controlled rationales may not fully represent complexity and noise patterns in real-world applications

## Confidence

- Mechanism 1 (Topological Discrepancy): **High** - Core mathematical framework is sound and well-validated through experiments
- Mechanism 2 (Theoretical Recovery): **Medium** - Theorem is internally consistent but depends on strict assumptions that may not generalize
- Mechanism 3 (Bimodal Prior): **High** - Simple, well-established technique with clear empirical benefits

## Next Checks

1. **Robustness Testing**: Evaluate TopInG on real-world datasets with known structural patterns (e.g., social networks, molecular graphs) to verify framework's practical utility beyond synthetic benchmarks
2. **Scalability Analysis**: Measure performance degradation as graph size increases, particularly focusing on computational bottleneck of TDA layer
3. **Ablation on Prior Configuration**: Systematically test different prior distributions (beyond fixed bimodal) to assess sensitivity to design choice and potential overfitting to synthetic data distributions