---
ver: rpa2
title: 'RPP: A Certified Poisoned-Sample Detection Framework for Backdoor Attacks
  under Dataset Imbalance'
arxiv_id: '2602.00183'
source_url: https://arxiv.org/abs/2602.00183
tags:
- backdoor
- samples
- imbalance
- detection
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of backdoor attacks in deep learning
  models under dataset imbalance. It introduces Randomized Probability Perturbation
  (RPP), a certified poisoned-sample detection framework that operates in a black-box
  setting using only model output probabilities.
---

# RPP: A Certified Poisoned-Sample Detection Framework for Backdoor Attacks under Dataset Imbalance

## Quick Facts
- **arXiv ID:** 2602.00183
- **Source URL:** https://arxiv.org/abs/2602.00183
- **Reference count:** 40
- **Primary result:** Introduces RPP, a certified poisoned-sample detection framework using model output probabilities, achieving significantly higher detection accuracy than state-of-the-art defenses under dataset imbalance.

## Executive Summary
This paper addresses the critical challenge of detecting poisoned samples in backdoor attacks, particularly under conditions of dataset imbalance. The authors propose Randomized Probability Perturbation (RPP), a framework that quantifies the stability of model prediction probabilities under random noise to distinguish clean from poisoned samples. RPP employs a conformal prediction framework to calibrate detection thresholds, providing provable within-domain detectability guarantees and probabilistic upper bounds on false positive rates. Extensive experiments across five benchmark datasets and ten backdoor attacks demonstrate RPP's superior performance compared to twelve baseline defenses, especially in imbalanced settings.

## Method Summary
RPP is a certified detection framework that operates in a black-box setting using only model output probabilities. The method works by measuring the stability of prediction probability vectors under random Gaussian noise perturbations: poisoned samples with robust backdoor triggers show relatively stable probability vectors, while clean samples exhibit more variation. A small clean calibration set is used with split conformal prediction to set a detection threshold that guarantees an upper bound on the false positive rate. The framework provides theoretical certification conditions based on trigger size and noise scale, establishing a relationship between these parameters and guaranteed detectability.

## Key Results
- RPP achieves significantly higher detection accuracy than state-of-the-art defenses across five benchmarks (MNIST, SVHN, CIFAR-10, TinyImageNet, ImageNet10) and ten backdoor attacks.
- The method maintains robust performance under extreme dataset imbalance conditions, outperforming baselines particularly in imbalanced settings.
- RPP provides provable within-domain detectability guarantees and a probabilistic upper bound on the false positive rate through its conformal prediction framework.

## Why This Works (Mechanism)

### Mechanism 1: Sample-Level Stability Discrimination
RPP distinguishes clean from poisoned samples by quantifying the stability of model predicted probability vectors under random noise perturbations. Backdoor triggers are engineered to be robust, keeping the predicted probability vector relatively stable, while clean images are more sensitive to mild perturbations. This creates a stability gap that RPP measures per instance. The core assumption is that the probability of classifying a noise-perturbed poisoned sample into the target class is less than the probability of classifying the non-perturbed poisoned sample into the target class.

### Mechanism 2: Conformal Prediction for Threshold Calibration
RPP adapts conformal prediction techniques to set a principled, distribution-adaptive detection threshold using a small clean calibration set. By computing a quantile of RPP scores from known clean samples, this method guarantees that for a new clean test sample, the probability of its RPP score falling below the threshold (a false positive) is bounded by a function of the significance level. This approach is particularly well-suited for backdoor detection in imbalanced settings.

### Mechanism 3: Certified Detection via Randomized Smoothing
The method provides certified conditions that guarantee detection of poisoned samples based on trigger size and noise scale. Using principles of randomized smoothing, the analysis establishes a relationship between trigger magnitude, noise standard deviation, and provable detectability. If the trigger's norm exceeds a computed lower bound, the poisoned sample is guaranteed to be detected by the RPP criterion, depending on the probability of the target class prediction and stability under perturbation.

## Foundational Learning

- **Concept: Backdoor Attacks in DNNs**
  - **Why needed here:** RPP is a defense against data-poisoning backdoor attacks. Understanding that an attacker injects poisoned samples (input + trigger) with a target label into training data, forcing the model to misclassify any input containing the trigger at test time, is the fundamental threat model this work addresses.
  - **Quick check question:** Can you explain how a data-poisoning backdoor attack differs from adversarial examples in terms of when the attack occurs (training vs. inference) and what the attacker can manipulate?

- **Concept: Randomized Smoothing and Certification**
  - **Why needed here:** The core theoretical contribution relies on randomized smoothing to provide provable guarantees. This technique analyzes a model's behavior under random noise injection to certify robustness against bounded perturbations, adapted here for backdoor detection.
  - **Quick check question:** What is the basic idea behind randomized smoothing? How does it provide a certified guarantee, and what are its typical trade-offs (e.g., noise level vs. accuracy)?

- **Concept: Conformal Prediction**
  - **Why needed here:** Mechanism 2 uses conformal prediction for threshold calibration. This statistical framework provides distribution-free, finite-sample guarantees by using a held-out calibration set, critical for the paper's reliability claims under data imbalance.
  - **Quick check question:** What is the primary output of conformal prediction (e.g., a prediction set or interval) and what key assumption does it make about the calibration data to provide its guarantees?

## Architecture Onboarding

- **Component map:** Input sample -> RPP Scorer -> score comparison with Conformal Calibration Module's threshold -> detection decision
- **Critical path:** The path from input sample through RPP scoring to threshold comparison and detection decision
- **Design tradeoffs:**
  - **Noise scale ($\sigma$):** Larger $\sigma$ improves separation between clean and poisoned samples but may harm clean accuracy of preliminary model or calibration
  - **Significance level ($\alpha$):** Smaller $\alpha$ leads to more stringent threshold, lowering FPR but potentially reducing TPR
  - **Calibration set size ($n$):** Larger $n$ stabilizes threshold and tightens FPR bound but requires more clean data
- **Failure signatures:**
  - High FPR/low TPR with extreme imbalance due to overlapping RPP score distributions
  - Evasion by small-norm triggers that fall below certified detection threshold
  - Contaminated calibration set that shifts empirical distribution and inflates threshold
- **First 3 experiments:**
  1. Reproduce basic certification result by training on poisoned CIFAR-10 with BadNets trigger and measuring TPR/FPR
  2. Test impact of data imbalance by creating imbalanced versions with varying ratios and measuring performance changes
  3. Validate FPR bound by running detection pipeline multiple times with different calibration set seeds and comparing empirical FPR to theoretical upper bound

## Open Questions the Paper Calls Out
The paper explicitly identifies extending the analysis beyond i.i.d. Gaussian perturbations as a promising direction for future work to better understand robustness under realistic conditions. The current proofs fundamentally rely on Gaussian distribution properties, specifically the standard normal inverse CDF.

## Limitations
- Theoretical guarantees critically depend on availability of a clean calibration set that is i.i.d. with clean data distribution, which may not be practical when all training data is suspected to be poisoned
- Certification guarantees only apply to triggers above a certain norm threshold, leaving a gap for smaller, potentially stealthier triggers
- While method shows strong empirical performance, the practical strategy for obtaining clean calibration data under poisoned conditions is not discussed

## Confidence
- **High confidence:** Core experimental results showing RPP outperforming baseline defenses across multiple benchmarks and backdoor attacks; methodology for computing RPP scores and applying conformal prediction is clearly specified
- **Medium confidence:** Theoretical certification guarantees due to dependence on specific assumptions about trigger size and noise behavior that may not hold in all practical scenarios
- **Medium confidence:** Claim of resilience to data imbalance, as while experimental results support this, underlying mechanism's robustness across extreme imbalance scenarios would benefit from additional validation

## Next Checks
1. Design and test practical strategies for obtaining a clean calibration set when working with suspected poisoned data, such as using out-of-distribution samples or synthetic data generation approaches
2. Systematically evaluate RPP's performance against adaptive attackers using triggers specifically designed to minimize the RPP score (e.g., triggers that are themselves sensitive to noise) to identify potential failure modes
3. Conduct experiments with more severe class imbalance scenarios (e.g., imbalance ratios >200) to determine practical limits of RPP's performance degradation and identify when detection becomes unreliable