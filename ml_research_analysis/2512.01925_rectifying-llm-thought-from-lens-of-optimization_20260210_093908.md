---
ver: rpa2
title: Rectifying LLM Thought from Lens of Optimization
arxiv_id: '2512.01925'
source_url: https://arxiv.org/abs/2512.01925
tags:
- density
- same
- earth
- reasoning
- numbers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Rectifying LLM Thought from Lens of Optimization

## Quick Facts
- arXiv ID: 2512.01925
- Source URL: https://arxiv.org/abs/2512.01925
- Authors: Junnan Liu; Hongwei Liu; Songyang Zhang; Kai Chen
- Reference count: 40
- Key outcome: RePro (Rectifying Process) improves reasoning accuracy by up to 9.1% over GRPO baselines while reducing backtracking by ~50% on AIME benchmarks

## Executive Summary
This paper proposes RePro, a method that treats LLM reasoning as implicit gradient descent optimization, using process-level rewards (intensity and stability scores) to guide reasoning trajectories. By framing Chain-of-Thought (CoT) as an optimization process where each reasoning step constitutes an update toward problem resolution, the authors introduce a surrogate objective based on ground-truth perplexity and derive dual scores to reward both progress magnitude and trajectory smoothness. RePro integrates into RLVR pipelines and shows consistent improvements across multiple reasoning benchmarks while reducing computational overhead through entropy-based segment selection.

## Method Summary
RePro computes a surrogate objective J̃ measuring log-probability of ground truth along reasoning steps, then derives Magnitude Score (tanh-normalized improvement) and Stability Score (Kendall's Tau monotonicity). These are combined into process rewards, integrated with outcome advantages (Â_t = A + α·Ã_t), and normalized within groups/batches for PPO/GRPO/REINFORCE++ training. Entropy-based segment selection reduces computational cost while maintaining effectiveness.

## Key Results
- 9.1% accuracy improvement over GRPO on AIME24/25 benchmarks
- 50% reduction in backtracking patterns in reasoning trajectories
- Consistent improvements across DeepSeek-R1-Distill-Qwen-1.5B and Qwen3-8B models
- 1.5x token efficiency improvement while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1: CoT as Implicit Gradient Descent
The paper interprets reasoning trajectories as iterative optimization updates, with each step moving toward higher probability of the correct answer. This is formalized as θ_{t+1} ← θ_t + η̃·∇̃_θ J(π_θ, q, τ_{≤t}, a), treating reasoning steps as "implicit" parameter updates. The assumption is that perplexity on ground truth monotonically decreases along correct reasoning paths.

### Mechanism 2: Surrogate Objective via Perplexity Tracking
A surrogate objective J̃ based on log probability of ground-truth answer serves as an effective proxy for monitoring optimization progress. As reasoning accumulates, effective context increases, and if reasoning is productive, J̃ should increase (perplexity decrease). The paper provides empirical evidence showing -J̃ decreases along correct trajectories.

### Mechanism 3: Dual Scoring for Process-Level Reward
Combining intensity (net improvement) and stability (smoothness) scores into a composite reward effectively distinguishes productive from suboptimal reasoning. The dual scoring approach uses tanh-normalized magnitude improvements and Kendall's Tau correlation for monotonicity, reducing backtracking patterns by ~50% compared to vanilla GRPO.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed: RePro integrates into RLVR pipelines; understanding how outcome rewards propagate through policy gradients is essential for grasping where and how process-level rewards are injected
  - Quick check: Can you explain how PPO's clipped objective differs from GRPO's group-based normalization?

- **Concept: Perplexity as Confidence Metric**
  - Why needed: The surrogate objective J̃ is fundamentally a perplexity measure over ground truth; understanding why lower perplexity indicates higher model confidence is prerequisite to interpreting the optimization framing
  - Quick check: If a model assigns probability 0.01 to each token in a 10-token ground truth, what is the perplexity?

- **Concept: Kendall's Tau Correlation**
  - Why needed: The stability score relies on Kendall's Tau to measure monotonic increase in J̃ along reasoning steps; without this, the stability component is opaque
  - Quick check: For a sequence [1, 2, 4, 3, 5], would Kendall's Tau be positive, negative, or zero?

## Architecture Onboarding

- **Component map:**
  Question q → LLM generates trajectory τ = [τ_thinking; τ_conclusion] → Segment τ_thinking into {c_1, ..., c_N} by newlines → Select top-k segments by first-token entropy → For each selected segment, compute J̃, S_magn, S_stab → Combine into rectifying reward r̃_j → Normalize within group (GRPO) or batch (REINFORCE++) → Merge with outcome advantage: Â_t = A + α·Ã_t → Update policy via clipped surrogate objective

- **Critical path:** The entropy-based segment selection (Eq. 12) is the efficiency bottleneck. Selecting too few segments misses critical reasoning points; too many increases forward pass overhead. The paper uses k=10 as default.

- **Design tradeoffs:**
  - w (magnitude vs. stability balance): Paper finds w=0.5 works, but lower w (more magnitude emphasis) slightly better. Trade-off between rewarding progress vs. penalizing oscillation.
  - α (process vs. outcome reward balance): Set to 0.1. Higher α risks noise from imperfect process scores overwhelming clean outcome signal.
  - k (number of selected segments): Increasing k from 5→30 improves performance marginally but with computational cost.

- **Failure signatures:**
  - If J̃ oscillates wildly even on correct trajectories, stability scores will be near-zero, weakening signal
  - If ground truth is not available during training, the proxy objective cannot be computed
  - If model is already near-optimal (J̃ saturates early), magnitude scores provide little differentiation

- **First 3 experiments:**
  1. Validate proxy metric: Sample 50 correct and 50 incorrect trajectories from a reasoning model. Plot J̃ curves. Verify that correct trajectories show higher final J̃ and more monotonic increase than incorrect ones.
  2. Ablate dual scoring: Train with magnitude-only (w=0), stability-only (w=1), and combined (w=0.5) on a small dataset. Compare final accuracy and token efficiency.
  3. Integration sanity check: Apply RePro to GRPO on a 1.5B model with k=5, 10, 20 segments. Measure both accuracy improvement and training time overhead per step.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the RePro framework be extended to domains lacking verifiable ground-truth answers (e.g., open-ended generation or creative writing), given its reliance on computing perplexity against a ground-truth token sequence?
- Basis in paper: [inferred] The proxy objective function $\tilde{\mathcal{J}}$ is defined in Eq. (5) specifically as the log-likelihood of the ground-truth answer $a$, and all experiments (Sec 4) are confined to benchmarks with verifiable rewards (math, code, science).
- Why unresolved: The method is designed for RLVR (Reinforcement Learning with Verifiable Rewards); applying it to non-verifiable domains requires a new definition of the objective function that does not depend on a ground-truth answer.
- What evidence would resolve it: Successful application of RePro or a modified variant to benchmarks like MT-Bench or creative writing tasks where no single ground truth exists.

### Open Question 2
- Question: How robust is the assumption that high-entropy tokens reliably indicate critical decision points or suboptimal oscillations in reasoning, rather than simply reflecting model uncertainty or noise?
- Basis in paper: [inferred] Section 3.4 justifies the entropy-based selection strategy by stating "high-entropy tokens may cause oscillations near extrema," but provides empirical validation only on specific models (DeepSeek-R1-Distill-Qwen).
- Why unresolved: The correlation between high entropy and "suboptimal optimization" is presented as an assumption to justify the selection strategy. It is possible that high entropy in other contexts might indicate diverse but correct exploration rather than error.
- What evidence would resolve it: An analysis correlating entropy values with labeled error types or reasoning quality across a broader range of model architectures and tasks.

### Open Question 3
- Question: To what extent does the proxy objective $\tilde{\mathcal{J}}$ (perplexity of the ground truth) accurately approximate the theoretical "implicit gradient" of the LLM's internal optimization process?
- Basis in paper: [inferred] The paper states in Section 3.1 that the actual optimization process is "complex and nontrivial" and admits to using a proxy metric in Section 3.2, providing only empirical evidence (Figure 2) for a specific case rather than theoretical proof.
- Why unresolved: The theoretical connection relies on the assumption that maximizing the probability of the ground truth is equivalent to performing a valid optimization step in the latent reasoning space.
- What evidence would resolve it: Theoretical analysis or experiments showing that improvements in the proxy metric $\tilde{\mathcal{J}}$ correlate strongly with improvements in the model's internal representations of the problem structure.

### Open Question 4
- Question: Is the fixed $top-k$ segment selection strategy optimal for all reasoning complexities, or does it risk missing critical process rewards in extremely long or highly complex chains?
- Basis in paper: [inferred] Section 3.4 introduces the selection strategy to mitigate "prohibitive computational overhead," and Appendix C.4 shows marginal gains with higher $k$, suggesting a trade-off but not solving for an adaptive mechanism.
- Why unresolved: The fixed $k$ (e.g., 10) might be sufficient for the benchmark lengths tested but could fail to capture sparse critical steps in much longer reasoning trajectories (e.g., "o1-like" scaling).
- What evidence would resolve it: Experiments on tasks requiring significantly longer CoTs comparing fixed-$k$ versus an adaptive $k$ determined by the trajectory's specific entropy profile or length.

## Limitations
- The optimization framing relies heavily on ground-truth probability serving as a reliable proxy for reasoning quality, breaking down when multiple solution paths exist
- The method's reliance on outcome supervision limits applicability to tasks where correct answers are not available
- Evaluation is limited to math and code tasks where definitive ground truth exists, raising questions about applicability to open-ended reasoning domains

## Confidence
- **High:** The dual scoring mechanism (intensity + stability) effectively reduces backtracking patterns in reasoning trajectories. Empirical evidence shows clear improvement in token efficiency and accuracy across multiple benchmarks.
- **Medium:** The gradient descent interpretation of CoT provides useful intuition but lacks rigorous mathematical validation. The optimization metaphor helps explain observed behaviors but may oversimplify the actual reasoning dynamics.
- **Low:** The generalization of RePro to domains without clear ground truth remains unproven.

## Next Checks
1. **Cross-domain validation:** Apply RePro to reasoning tasks in scientific domains (e.g., physics problem solving) where multiple valid solution paths exist and ground truth format differs from math competitions.
2. **Ablation on scoring components:** Systematically vary w (0.0 to 1.0) across a broader range of datasets to precisely quantify the marginal benefit of stability scoring versus magnitude alone.
3. **Long-horizon reasoning analysis:** Evaluate RePro on multi-step reasoning tasks requiring 20+ steps to verify that process rewards remain effective when trajectory length exceeds typical training distributions.