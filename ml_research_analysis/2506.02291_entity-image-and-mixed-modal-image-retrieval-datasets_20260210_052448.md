---
ver: rpa2
title: Entity Image and Mixed-Modal Image Retrieval Datasets
arxiv_id: '2506.02291'
source_url: https://arxiv.org/abs/2506.02291
tags:
- image
- entity
- dataset
- mmir
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two new datasets to advance mixed-modal
  image retrieval: the Entity Image Dataset (EI) and the Mixed-Modal Image Retrieval
  Dataset (MMIR). EI provides canonical images for Wikipedia entities, while MMIR
  is derived from the WIT dataset and features challenging queries combining entity
  images with contextual text.'
---

# Entity Image and Mixed-Modal Image Retrieval Datasets

## Quick Facts
- arXiv ID: 2506.02291
- Source URL: https://arxiv.org/abs/2506.02291
- Reference count: 4
- Primary result: Introduced Entity Image Dataset (EI) with 1.79M entities and canonical images, plus Mixed-Modal Image Retrieval Dataset (MMIR) with 9.06M examples for challenging mixed-modal retrieval tasks.

## Executive Summary
This paper introduces two new datasets to advance mixed-modal image retrieval: the Entity Image Dataset (EI) and the Mixed-Modal Image Retrieval Dataset (MMIR). EI provides canonical images for Wikipedia entities, while MMIR is derived from the WIT dataset and features challenging queries combining entity images with contextual text. The datasets were constructed using a multi-stage process involving Wikipedia content crawling, image candidate identification, consolidation, and canonical image selection, with thresholds tuned per entity category to ensure quality. MMIR includes two novel query types: single entity-image queries and multi-entity-image queries, requiring models to understand complex cross-modal relationships. Human evaluation confirmed high quality, with over 97% of EI images rated as Good or Excellent and 81% of MMIR examples judged semantically coherent. Model-based evaluation showed that training on MMIR significantly improves retrieval performance compared to models trained only on general image-text datasets, while maintaining competitive performance on standard benchmarks like Flickr30k and MS COCO. These datasets offer a challenging benchmark for mixed-modal retrieval and a valuable resource for advancing multimodal learning research.

## Method Summary
The authors constructed the Entity Image Dataset by crawling Wikipedia content, identifying entity-image pairs using Google Cloud Vision API, consolidating candidates by URL, and selecting canonical images with category-specific confidence thresholds. The Mixed-Modal Image Retrieval Dataset was built by filtering the WIT dataset to reference descriptions only, annotating entities in both images and text using Cloud Vision and NLP APIs, and masking entity names with [MASK_*] tokens linked to EI canonical images. For evaluation, they fine-tuned a dual-encoder model (ViT-Large vision encoder + mT5-Base text encoder, initialized from WebLI-10B) using in-batch sampled softmax loss on cosine similarity embeddings, training for 14 epochs on MMIR (filtered to ≤5 entities) and optionally 7 epochs on CC3M.

## Key Results
- EI contains 1.79M entities with canonical images, more than double the coverage of single-threshold approaches
- MMIR test set achieves R@1=12.19, R@5=40.29 when fine-tuned on both MMIR and CC3M datasets
- Category-specific confidence thresholds significantly improved coverage over single-threshold approach
- Human evaluation confirmed 97.8% of EI images rated as Good or Excellent, and 81.4% of MMIR examples judged semantically coherent
- Training on MMIR improves mixed-modal retrieval performance while maintaining competitive results on standard benchmarks (Flickr30k R@1=63.60, MS COCO R@1=44.98)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity masking forces cross-modal grounding by removing textual entity identifiers and requiring visual context.
- Mechanism: The MMIR pipeline identifies entities present in both image and caption, then replaces entity names in text with [MASK_*] tokens that reference corresponding canonical entity images. Models must jointly encode the masked text with entity images to retrieve the correct target image.
- Core assumption: The masked caption plus entity images preserve sufficient semantic information to identify the target image.
- Evidence anchors:
  - [abstract]: "MMIR benchmark features two challenging query types requiring models to ground textual descriptions in the context of provided visual entities"
  - [section 2.2.2]: "For each remaining entity in Pi, we masked its name within the text description ti, thus generating a mixed-modal image retrieval example"
  - [corpus]: Related work on multimodal entity linking (DeepMEL) confirms entity grounding from joint visual-textual inputs is tractable but non-trivial.
- Break condition: If entity images are ambiguous or multiple entities share similar visual features, grounding may fail.

### Mechanism 2
- Claim: Category-specific confidence thresholds improve canonical image quality and coverage over single-threshold approaches.
- Mechanism: Rather than applying one global confidence threshold for all entity types, the EI pipeline determines optimal thresholds per category (Person, Animal, Locations, etc.). This preserves coverage for visually diverse categories like locations while filtering noise.
- Core assumption: Entity categories have systematically different annotation confidence distributions.
- Evidence anchors:
  - [section 2.1]: "This category-specific approach significantly improved the coverage, resulting in over 1.79M entities with canonical images, which is a more than twofold increase"
  - [section 2.1]: Single-threshold approach "reduced the number of entities with canonical images to approximately 862K and disproportionately filtering out key entity types"
  - [corpus]: No direct corpus comparison found; this appears to be a dataset-specific optimization.
- Break condition: If entity type distributions shift (e.g., new categories), thresholds may need recalibration.

### Mechanism 3
- Claim: Mixed-modal fine-tuning transfers to standard image-text retrieval without degradation.
- Mechanism: Training on MMIR's entity-image + masked text pairs teaches the model to integrate visual context with textual queries. When evaluated on Flickr30k/MS-COCO (pure text-to-image), this learned integration does not interfere with unimodal retrieval patterns.
- Core assumption: Mixed-modal representations are a superset of unimodal capabilities.
- Evidence anchors:
  - [section 4.2.2]: "The model fine-tuned on both MMIR and CC3M datasets performs at a level comparable to the model fine-tuned solely on CC3M"
  - [table 4]: Combined MMIR+CC3M achieves R@1 of 63.60/44.98 on Flickr30k vs. 65.10/51.10 for CC3M-only—comparable within variance.
  - [corpus]: Related work on mixed-modal RAG systems suggests joint training improves general retrieval, though evidence is preliminary.
- Break condition: Domain shift between MMIR (Wikipedia entities) and target benchmarks may cause performance gaps.

## Foundational Learning

- Concept: Dual-encoder retrieval architectures
  - Why needed here: The paper uses shared-parameter dual encoders where query (mixed-modal) and target (image) are encoded separately and matched via cosine similarity. Understanding this symmetry is essential for interpreting the I+T→I and I→I+T retrieval tasks.
  - Quick check question: Can you explain why dual-encoders enable efficient retrieval at scale compared to cross-encoders?

- Concept: Entity linking / grounding
  - Why needed here: The EI and MMIR construction relies on identifying entities in both images (via Cloud Vision) and text (via Cloud NLP), then aligning them. Understanding entity disambiguation helps debug data quality issues.
  - Quick check question: What happens if an entity appears in the caption but not in the image during MMIR construction?

- Concept: In-batch sampled softmax loss
  - Why needed here: The model is trained with in-batch negatives using cosine similarity, a standard contrastive approach. Understanding how negative sampling affects retrieval metrics is critical for training stability.
  - Quick check question: Why might increasing batch size improve retrieval performance with in-batch softmax?

## Architecture Onboarding

- Component map:
  EI Pipeline: Wikipedia crawl -> Candidate image identification (Vision API) -> Consolidation (dedup by URL) -> Canonical selection (category-specific thresholds)
  MMIR Pipeline: WIT filtering (reference descriptions only) -> Entity annotation (Vision + NLP APIs) -> Intersection -> Masking with [MASK_*] tokens
  Model: ViT-Large (frozen) + mT5-Base text encoder -> Shared encoder -> Mean pooling -> 768-dim projection -> Cosine similarity matching

- Critical path:
  1. Data preparation: EI construction must complete before MMIR masking (MMIR references EI canonical images).
  2. Training: Fine-tune WebLI-10B on MMIR (14 epochs) -> optional CC3M fine-tuning (7 epochs).
  3. Evaluation: Recall@1/5/10 on MMIR test set, plus Flickr30k/MS-COCO for generalization.

- Design tradeoffs:
  - Freezing vision encoder vs. end-to-end: Freezing reduces compute but may limit adaptation to entity-specific visual features.
  - Max 5 entities per example: Filtered for training efficiency; may exclude complex multi-entity scenes.
  - Category-specific thresholds: Improves coverage but adds annotation cost for threshold calibration.

- Failure signatures:
  - Low recall on multi-entity queries: Model may struggle with relational text describing entity interactions.
  - Performance drop on non-person entities: Table 6 shows lower "Excellent" rates for Historical Events (55%) vs. Person (92%); model may inherit this bias.
  - Masked entity not in EI: Example discarded during MMIR construction; coverage gaps possible for rare entities.

- First 3 experiments:
  1. Baseline retrieval: Run zero-shot WebLI-10B on MMIR test set to establish R@1/5 baseline before fine-tuning.
  2. Ablation on entity count: Evaluate performance stratified by number of entities (1, 2, 3, 3+) using Table 3 splits to identify where the model struggles most.
  3. Cross-dataset transfer check: After MMIR fine-tuning, evaluate on Flickr30k/MS-COCO to confirm no catastrophic forgetting (compare against CC3M-only baseline).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the temporal staleness of the Wikipedia snapshot affect the retrieval accuracy of entities with rapidly evolving visual representations?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "facts (captions) may become stale or images being changed or deleted," necessitating future regeneration of the datasets to maintain relevance.
- Why unresolved: The current datasets are static snapshots; the rate of degradation for retrieval performance over time is unknown.
- What evidence would resolve it: A longitudinal study comparing retrieval performance on the static dataset versus a concurrently updated dataset for time-sensitive entities.

### Open Question 2
- Question: Can architectures other than dual-encoders (e.g., generative or fusion-based models) better handle the "deep cross-modal contextual understanding" required for multi-entity queries?
- Basis in paper: [inferred] The evaluation exclusively uses a dual-encoder architecture (Section 4.1), which processes modalities separately; the complexity of multi-entity queries might benefit from more interactive cross-attention mechanisms.
- Why unresolved: The paper establishes a baseline but does not explore whether architectural inductive biases are a bottleneck for the challenging multi-entity task.
- What evidence would resolve it: Benchmarking the MMIR dataset using cross-attention or generative models and comparing Recall@K against the dual-encoder baseline.

### Open Question 3
- Question: Why does training on MMIR combined with CC3M fail to yield significant improvements over CC3M alone on standard benchmarks (Flickr30k, MS-COCO)?
- Basis in paper: [inferred] In Section 4.2.2 and Table 4, the MMIR+CC3M model shows negligible difference (and sometimes slight regression) compared to the CC3M-only model on standard tasks, suggesting a lack of positive transfer.
- Why unresolved: It is unclear if the "mixed-modal" complexity conflicts with standard text-to-image tasks or if the dataset scale is insufficient to bridge the domains.
- What evidence would resolve it: An ablation study analyzing the gradient updates and feature alignment between MMIR and CC3M during joint fine-tuning.

## Limitations
- Reliance on proprietary components (WebLI-10B pre-training weights, Google Cloud Vision/NLP services) that may affect exact reproduction
- Evaluation scope limited to MMIR, Flickr30k, and MS-COCO, constraining understanding of cross-domain generalization
- Category-specific confidence thresholds require manual calibration and may not generalize to new entity types

## Confidence
- High Confidence: The dataset construction methodology (entity masking, canonical image selection) is clearly specified and reproducible with open tools. The claim that MMIR improves mixed-modal retrieval performance is supported by direct comparisons showing R@1 improvement from 9.34 to 12.19.
- Medium Confidence: Claims about generalization to standard benchmarks are supported but based on a single model run. The assertion that mixed-modal training doesn't degrade unimodal performance is reasonable but requires more ablation studies across different base models.
- Low Confidence: The paper asserts category-specific thresholds "significantly improved coverage" but doesn't quantify the exact performance gain from this design choice compared to alternative approaches.

## Next Checks
1. Reconstruct EI from scratch: Use open entity linking tools (e.g., spaCy, WikiData) to identify entities in Wikipedia images and compare coverage to the published 1.79M entities. This validates the entity detection and consolidation pipeline.
2. Cross-model generalization test: Fine-tune a different base model (e.g., CLIP) on MMIR and evaluate on both MMIR and Flickr30k to determine if performance gains are architecture-dependent or dataset-driven.
3. Category bias analysis: Stratify MMIR test performance by entity category (Person, Location, Historical Event) to quantify whether the observed "Excellent" rate differences (Table 6) translate to systematic retrieval gaps for underrepresented categories.