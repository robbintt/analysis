---
ver: rpa2
title: Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning
  via Reinforcement Learning
arxiv_id: '2507.17512'
source_url: https://arxiv.org/abs/2507.17512
tags:
- performance
- training
- reasoning
- code
- puzzle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how training on multiple reasoning domains
  affects the performance of large language models under reinforcement learning with
  verifiable rewards. Using the GRPO algorithm and Qwen-2.5-7B models, the authors
  systematically evaluate single-domain training (math, code, and puzzle), dual and
  triple-domain combinations, curriculum learning, reward design variations, and the
  impact of training language.
---

# Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.17512
- Source URL: https://arxiv.org/abs/2507.17512
- Reference count: 40
- Key outcome: Puzzle and math domains exhibit mutual benefits under reinforcement learning, while code reasoning has mixed cross-domain effects.

## Executive Summary
This study investigates how training on multiple reasoning domains affects the performance of large language models under reinforcement learning with verifiable rewards. Using the GRPO algorithm and Qwen-2.5-7B models, the authors systematically evaluate single-domain training (math, code, and puzzle), dual and triple-domain combinations, curriculum learning, reward design variations, and the impact of training language. They find that puzzle and math data provide mutual benefits, code reasoning has mixed cross-domain effects, and multi-domain training improves overall performance and task balance. Template consistency is critical, and SFT before RL substantially boosts performance. Curriculum learning with periodic policy refresh accelerates convergence, and reward design should match task complexity. RLVR is sensitive to language, with Chinese training underperforming English. Overall, the study highlights the importance of domain combination, reward customization, and template alignment in developing robust multi-domain reasoning capabilities.

## Method Summary
The study employs Group Relative Policy Optimization (GRPO) to train Qwen-2.5-7B-Base and Instruct models on three reasoning domains: math (DeepScaleR, CountDown), code (CodeR1), and puzzle (Knights-and-Knaves, Logic Puzzle Baron). Training uses the R1-template format with binary or partial rewards. Experiments include single-domain training, dual/triple-domain combinations, curriculum learning, reward ablation studies, and language impact evaluation. Performance is measured on in-domain and out-of-domain benchmarks using OpenCompass with fixed temperature and top-p parameters.

## Key Results
- Puzzle and math training exhibit bidirectional positive transfer, while code shows asymmetric effects depending on pre-training
- SFT before RL substantially boosts performance and stabilizes policy optimization
- Template consistency between training and evaluation is critical for RLVR generalization
- Multi-domain training improves overall performance and task balance compared to single-domain approaches
- RLVR is sensitive to training language, with Chinese training underperforming English

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Puzzle and math domains exhibit bidirectional positive transfer under RLVR, while code shows asymmetric effects depending on pre-training.
- Mechanism: Logical reasoning (puzzles) and mathematical reasoning share underlying cognitive structures—multi-step deduction, constraint satisfaction, and symbolic manipulation. Training in one strengthens shared neural pathways that benefit the other. Code reasoning, however, requires rigid syntactic structure that can constrain output flexibility in base models lacking format awareness.
- Core assumption: The observed transfer reflects shared computational patterns rather than dataset artifacts.
- Evidence anchors:
  - [abstract] "puzzle and math data provide mutual benefits... code reasoning has mixed cross-domain effects"
  - [section 3.1] "Math training improves puzzle-solving performance... Base-DSR increases puzzle averages to 24.08 from 9.07"
  - [section 3.2] "Base-CodeR1 shows performance drops on most OOD tasks... the rigid structure of code training data can constrain the base model's output flexibility"
  - [corpus] Related work (arXiv:2510.02230) suggests RLVR may constrain reasoning boundaries, consistent with code's negative transfer on base models.

### Mechanism 2
- Claim: Supervised Fine-Tuning (SFT) before RL creates a format-aware initialization that stabilizes policy optimization and enables more effective credit assignment.
- Mechanism: SFT exposes the model to structured output formats, reducing format exploration burden during RL. This allows the RL signal to focus on reasoning quality rather than output structure discovery.
- Core assumption: The performance gap stems from format awareness rather than other SFT benefits.
- Evidence anchors:
  - [abstract] "SFT before RL substantially boosts performance"
  - [section 3.2] "the instruct model, enhanced by SFT, consistently achieves the highest performance... underscores the critical importance of SFT in unlocking the full potential of RL training"
  - [section 3.3] Instruct-LPB on CountDown: "sharp performance drop from 24.35 to 2.47... may stem from LPB's relatively fixed problem format"

### Mechanism 3
- Claim: Template consistency between training and evaluation is a binding constraint for RLVR generalization; mismatched templates cause catastrophic performance degradation.
- Mechanism: RLVR optimizes for reward signals conditional on the training template's token distribution. The learned policy becomes tightly coupled to template-specific attention patterns. At evaluation, template mismatch disrupts these patterns, causing the model to generate outputs outside the learned distribution.
- Core assumption: The degradation reflects template coupling rather than evaluation artifact.
- Evidence anchors:
  - [abstract] "Template consistency is critical"
  - [section 5] "mismatched base template lowers scores to 0, 3.00, and 31.29 on Countdown, MBPP, and KK... underscores the models' sensitivity to template mismatches"
  - [section 5] R1 template produces 47.84/54.56 avg for base/instruct vs. 17.54/21.45 with mismatched templates

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO is the core RL algorithm used throughout. Unlike PPO, it estimates advantages from within-group reward comparisons rather than a learned value function, making it sensitive to reward distribution and group composition.
  - Quick check question: Given a rollout group with rewards [0.2, 0.8, 0.5], what advantage does GRPO assign to the response with reward 0.8?

- Concept: **Verifiable Rewards (Binary vs. Partial)**
  - Why needed here: The paper demonstrates that reward design must match task complexity. Binary rewards fail on sparse-reward tasks (LPB), while partial rewards introduce noise on easier tasks (KK).
  - Quick check question: For a puzzle with 5 blanks where the model correctly fills 3, what reward would R1 (binary) vs. R2 (partial) assign?

- Concept: **Template-Token Coupling**
  - Why needed here: RLVR's apparent generalization is partially illusory—performance depends heavily on template alignment between train and test. Understanding this prevents misinterpreting benchmark results.
  - Quick check question: If a model trained with R1-template achieves 70% on a benchmark, what performance range would you expect if evaluated with an unseen template?

## Architecture Onboarding

- Component map:
  ```
  [Training Data] → [Template Application] → [GRPO Rollout] → [Reward Computation] → [Policy Update]
         ↓                  ↓                      ↓                  ↓
  [Domain: Math/Code/Puzzle] [R1-template]    [8 samples/prompt]  [Binary/Partial/etc.]
                                              ↓
                                    [Advantage = (R - mean) / std]
  ```

- Critical path:
  1. **Template selection** — Must be identical for train and eval; use R1-template as default.
  2. **Reward design** — Match to task: binary for achievable tasks, partial for sparse-reward tasks.
  3. **Domain combination** — Start with Math+Puzzle (positive transfer), add Code only for instruct models.
  4. **SFT initialization** — Always prefer instruct models over base for RLVR.

- Design tradeoffs:
  - Single-domain vs. multi-domain: Single-domain maximizes peak performance on that domain; multi-domain improves balance and overall robustness.
  - Binary vs. partial rewards: Binary provides cleaner signals but fails on hard tasks; partial provides dense signals but may reward incorrect intermediate steps.
  - Base vs. instruct: Base models have more "potential" gains but are unstable; instruct models are reliable but show smaller relative improvements.
  - Curriculum vs. mixed: Curriculum raises performance ceiling but requires careful difficulty stratification; mixed is simpler but suboptimal.

- Failure signatures:
  - Training collapse with binary rewards on hard tasks → reward sparsity too extreme; switch to partial.
  - Base model performance drops on OOD tasks after code training → format rigidity; use instruct model or add format diversity.
  - Catastrophic eval performance despite good training metrics → template mismatch; verify train/eval template consistency.
  - Countdown performance near 0% → model not following "use all numbers" constraint; model lacks instruction-following (use instruct model).

- First 3 experiments:
  1. **Template consistency validation**: Train on KK with R1-template, evaluate with R1/Qwen/Base templates. Confirm the paper's finding that mismatched templates cause >50% performance drop. This establishes your eval pipeline integrity.
  2. **Single-domain baseline sweep**: Train separate models on DSR, CodeR1, and KK. Measure in-domain improvement and cross-domain transfer. Verify that Math→Puzzle transfer is positive while Code→Math (on base model) is negative.
  3. **Reward design ablation on LPB**: Compare R1 (binary), R2 (partial), and R4 (rescaled) on LPB dataset. Confirm that R1 causes training collapse while R2/R4 remain stable, demonstrating reward-task matching importance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the cross-domain transfer dynamics (synergies and conflicts) identified in Qwen-2.5-7B generalize to other model families like Llama or DeepSeek?
- Basis in paper: [explicit] The authors state in the conclusion that due to hardware limitations, they focused on Qwen 2.5, and "Future research could expand on the impact of different datasets on models like Llama and DeepSeek."
- Why unresolved: The observed phenomenon where code training aids the instruct model but constrains the base model may be specific to Qwen's pre-training data mixture or architecture.
- What evidence would resolve it: Replicating the single-domain and triple-domain training experiments (Math, Code, Puzzle) using Llama-3 and DeepSeek base and instruct models to see if the "All Average" performance trends hold.

### Open Question 2
- Question: Can advanced post-training algorithms or reward modifications close the performance gap between Chinese and English reasoning in RLVR?
- Basis in paper: [inferred] Section 8 notes that models trained in Chinese consistently underperform English models, concluding that "RLVR is language-sensitive" and highlighting the need for "more advanced post-training algorithms."
- Why unresolved: The study demonstrates the existence of the language gap but does not propose or test methods to mitigate this cross-lingual inefficiency.
- What evidence would resolve it: Developing a cross-lingual RLVR method (e.g., mixed-language rollouts or alignment rewards) that results in statistically similar performance for Chinese and English training runs on the same reasoning benchmarks.

### Open Question 3
- Question: Does implementing fine-grained, cell-level reward schemes improve convergence and accuracy for complex puzzle tasks compared to response-level partial rewards?
- Basis in paper: [inferred] Section 7.2 identifies a "significant limitation" where current rewards "operate at the response level rather than the cell level," failing to accurately penalize specific erroneous cells.
- Why unresolved: The experiments were limited to binary and proportional rewards, which the authors suggest are suboptimal because they treat all cells equally within a single response.
- What evidence would resolve it: Designing a granular reward function for the Knights-and-Knaves or LPB datasets that assigns credit based on individual cell correctness and comparing the learning curve against the proportional reward baseline.

## Limitations

- The study is limited to Qwen-2.5-7B models, restricting generalizability across model families and architectures.
- Pseudo-labeling of puzzle data using DeepSeek-R1 may introduce noise into the training data, though performance gains suggest this is minimal.
- The evaluation framework relies on zero-shot or few-shot inference with fixed templates, which may not reflect real-world deployment variability.

## Confidence

- **High Confidence**: Puzzle-math bidirectional transfer, template consistency necessity, SFT benefits, and curriculum learning acceleration. These claims are directly supported by controlled ablations and consistent with prior work on format sensitivity.
- **Medium Confidence**: Code reasoning's asymmetric effects and language-dependent performance. While supported by experimental results, these findings depend on specific base model characteristics and may vary with model scale or pre-training.
- **Low Confidence**: Generalization of multi-domain benefits to other model families or task types. The study provides no cross-model validation, and the mechanisms proposed (shared reasoning primitives) are speculative.

## Next Checks

1. **Cross-Model Generalization Test**: Repeat the single-domain and multi-domain training experiments using a different base model (e.g., Llama-3-8B) to verify if puzzle-math transfer and code's asymmetric effects persist.

2. **Template Robustness Evaluation**: Train a model with R1-template and evaluate it on the same benchmarks using three unseen templates (e.g., Alpaca, Vicuna, and a custom format). Measure performance degradation to quantify the true extent of template coupling.

3. **Language Transfer Experiment**: Train the instruct model on a mixed-language dataset (50% English, 50% Chinese) and evaluate on both English and Chinese benchmarks. This would validate whether the language-dependent performance gap is due to tokenization, pre-training data, or reward computation artifacts.