---
ver: rpa2
title: 'Don''t Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting
  Strategy for Text Classification'
arxiv_id: '2502.07165'
source_url: https://arxiv.org/abs/2502.07165
tags:
- principles
- classification
- prompting
- principle
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent prompting strategy for text
  classification, where multiple LLM agents independently generate candidate principles
  from demonstrations, which are then consolidated into a final principle by a central
  agent. The consolidated principle is used to guide classification tasks.
---

# Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification

## Quick Facts
- arXiv ID: 2502.07165
- Source URL: https://arxiv.org/abs/2502.07165
- Reference count: 4
- Primary result: 1.55%–19.37% macro-F1 gains over zero-shot, matching few-shot ICL at lower cost

## Executive Summary
This paper introduces a multi-agent prompting strategy for text classification, where multiple LLM agents independently generate candidate principles from demonstrations, which are then consolidated into a final principle by a central agent. The consolidated principle is used to guide classification tasks. The approach outperforms strong baselines like CoT and stepback prompting, achieving 1.55%–19.37% gains in macro-F1 score over zero-shot prompting. It also matches or exceeds few-shot ICL performance while using shorter input tokens and lower inference costs. Principles generated by the method are more effective than human-crafted ones on two private datasets. Ablation studies confirm that label information and the multi-agent framework are critical to generating high-quality principles.

## Method Summary
The approach uses a multi-agent prompting framework where multiple LLM agents independently generate candidate principles from demonstrations, which are then consolidated into a final principle by a central agent. The consolidated principle is used to guide classification tasks. This method leverages the collective reasoning of multiple agents to generate more robust and effective principles compared to single-agent approaches. The system is designed to extract underlying principles from demonstrations rather than relying solely on pattern matching, making it more adaptable to new scenarios.

## Key Results
- Achieves 1.55%–19.37% macro-F1 gains over zero-shot prompting on text classification tasks
- Matches or exceeds few-shot ICL performance while using shorter input tokens and lower inference costs
- Generated principles outperform human-crafted principles on two private datasets
- Ablation studies confirm the importance of label information and the multi-agent framework for generating high-quality principles

## Why This Works (Mechanism)
The method works by leveraging multiple independent reasoning paths to extract underlying principles from demonstrations, rather than relying on pattern matching alone. The multi-agent framework allows for diverse perspectives on the task, which are then consolidated to form a more robust guiding principle. This approach is particularly effective for text classification because it captures the semantic essence of the task rather than memorizing specific examples. The consolidation step ensures that the final principle represents a consensus view that is more likely to generalize across different instances.

## Foundational Learning
- Multi-agent prompting: Why needed - Enables diverse reasoning approaches; Quick check - Compare single vs multi-agent performance
- Principle-based reasoning: Why needed - Captures task semantics vs pattern matching; Quick check - Test on out-of-distribution examples
- Prompt engineering: Why needed - Quality of prompts affects principle quality; Quick check - A/B test different prompt templates
- Label incorporation: Why needed - Provides task-specific guidance; Quick check - Compare with and without label information

## Architecture Onboarding

Component map: Demonstrations -> Multiple Agents -> Candidate Principles -> Central Agent -> Consolidated Principle -> Classification

Critical path: The most critical path is from the generation of candidate principles by individual agents through to the consolidation step, as the quality of the consolidated principle directly determines classification performance.

Design tradeoffs: The method trades increased complexity (multiple LLM calls) for potentially better generalization. While this increases engineering overhead, the reported lower inference costs compared to few-shot ICL suggest this tradeoff is worthwhile.

Failure signatures: Poor principle consolidation can lead to weak guidance for classification. If the consolidated principle is too vague or overly specific, classification accuracy will suffer. Additionally, if the individual agents generate highly divergent principles, the consolidation step may struggle to find a coherent middle ground.

3 first experiments:
1. Test single-agent vs multi-agent performance on a simple text classification task
2. Evaluate principle quality using human evaluation metrics (interpretability, correctness)
3. Compare consolidation strategies (voting, majority, weighted averaging) on principle effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused primarily on text classification, with no experiments on more complex NLP tasks like question answering or summarization
- Generated principles not evaluated for interpretability or correctness in a human-audited sense
- Method's sensitivity to noisy or ambiguous labels not explored
- Reliance on private datasets for some evaluations limits reproducibility and external validation

## Confidence
- Claims about macro-F1 improvement over zero-shot: High
- Claims about matching few-shot ICL with lower cost: Medium (depends on model and API costs)
- Claims about principles being more effective than human-crafted ones: Medium (private datasets limit independent verification)
- Claims about multi-agent framework being critical: Medium (ablation is supportive but not exhaustive)

## Next Checks
1. Evaluate the approach on non-classification tasks (e.g., QA, summarization) to test principle generalizability
2. Conduct human evaluation of generated principles for interpretability and correctness, not just downstream task performance
3. Test robustness to noisy or ambiguous labels to validate the method's practical utility in real-world settings