---
ver: rpa2
title: Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via
  Source-Shielded Updates
arxiv_id: '2512.04844'
source_url: https://arxiv.org/abs/2512.04844
tags:
- language
- source
- target
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in adapting instruct
  LLMs to new languages using only unlabeled target language data. The core method,
  Source-Shielded Updates (SSU), proactively identifies and freezes source-critical
  parameters before adaptation, using importance scores derived from a small set of
  source data.
---

# Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates

## Quick Facts
- arXiv ID: 2512.04844
- Source URL: https://arxiv.org/abs/2512.04844
- Reference count: 40
- Primary result: Source-Shielded Updates (SSU) method significantly reduces catastrophic forgetting while adapting LLMs to new languages, outperforming full fine-tuning

## Executive Summary
This paper addresses catastrophic forgetting in adapting instruction-tuned large language models to new languages using only unlabeled target language data. The authors propose Source-Shielded Updates (SSU), a method that identifies and freezes source-critical parameters before adaptation based on importance scores derived from a small set of source data. This approach maintains the model's foundational capabilities while enabling strong target-language performance gains. The method demonstrates significant improvements over full fine-tuning, reducing source language performance drops from over 20% to under 4% while achieving competitive target-language results.

## Method Summary
Source-Shielded Updates (SSU) works by first evaluating parameter importance scores using a small set of source language data, then freezing the most critical parameters before performing target language adaptation. The importance scores are computed by measuring each parameter's contribution to prediction accuracy on source tasks. During adaptation, parameters are frozen column-wise (across all layers) based on these scores, with a threshold determined by the desired trade-off between source preservation and target improvement. The method is designed to be efficient, requiring only a small amount of source data and adding minimal computational overhead during adaptation.

## Key Results
- SSU reduces source language performance drops to 3.4% (7B) and 2.8% (13B) compared to 20.3% and 22.3% for full fine-tuning
- Target language performance with SSU is highly competitive with full fine-tuning, achieving 98.6% and 97.9% of full FT's performance
- SSU achieves 99.7% and 99.3% of full FT's target performance while significantly better preserving source capabilities

## Why This Works (Mechanism)
The mechanism behind SSU's effectiveness lies in its proactive identification and protection of source-critical parameters. By computing importance scores based on how much each parameter contributes to source task performance, SSU can selectively freeze the most crucial parameters while allowing others to adapt to the target language. This column-wise freezing strategy preserves the model's core capabilities while still enabling adaptation to new language patterns. The small source data requirement makes this approach practical for real-world deployment scenarios where labeled source data may be limited.

## Foundational Learning
- **Parameter importance scoring**: Understanding how to quantify each parameter's contribution to task performance is crucial for selective freezing. Quick check: Verify importance scores correlate with actual impact on source task performance.
- **Catastrophic forgetting**: The phenomenon where models lose previously learned capabilities during new training. Quick check: Measure performance degradation on source tasks after adaptation.
- **Column-wise parameter freezing**: The strategy of freezing entire columns of parameters across all layers based on importance scores. Quick check: Compare results with row-wise or random freezing strategies.
- **Unlabeled target data adaptation**: Adapting models using only unlabeled data from the target domain. Quick check: Ensure adaptation quality with varying amounts of unlabeled target data.
- **Fine-tuning trade-offs**: Balancing between preserving source capabilities and gaining target capabilities. Quick check: Vary the number of frozen parameters and measure the performance trade-off curve.

## Architecture Onboarding

**Component map**: SSU -> Importance Scoring -> Parameter Freezing -> Target Adaptation

**Critical path**: The method's effectiveness depends critically on accurate importance scoring and appropriate threshold selection for parameter freezing.

**Design tradeoffs**: The main tradeoff is between source preservation and target adaptation capability. Higher freezing rates preserve source better but limit target adaptation.

**Failure signatures**: 
- Poor importance scoring leads to freezing wrong parameters, causing both source degradation and limited target improvement
- Inappropriate threshold selection results in either excessive source forgetting or insufficient target adaptation
- Too aggressive freezing can prevent the model from adapting to target language patterns

**3 first experiments**:
1. Compare SSU with different freezing rates (e.g., 10%, 30%, 50% of parameters) to find optimal balance
2. Test SSU with varying amounts of source data for importance scoring (e.g., 100, 300, 500 samples)
3. Evaluate SSU on a broader range of cross-lingual tasks beyond machine translation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Relies on a small set of source language data for parameter importance scoring, with unclear sensitivity to data quantity and quality
- Evaluation focuses primarily on machine translation tasks, limiting generalizability to other cross-lingual NLP tasks
- Does not extensively discuss scaling behavior across different model sizes

## Confidence
- High confidence in the overall effectiveness of SSU in reducing catastrophic forgetting compared to full fine-tuning
- Medium confidence in the competitive target language performance of SSU versus full fine-tuning, given the limited scope of tasks evaluated
- Medium confidence in the computational efficiency claims, as detailed runtime comparisons with baseline methods are not provided

## Next Checks
1. Conduct a sensitivity analysis on the quantity and quality of source language data used for importance scoring to determine the robustness of SSU across different source data regimes.
2. Extend the evaluation to a broader range of cross-lingual NLP tasks beyond machine translation to assess the method's generalizability.
3. Perform scaling experiments with models of varying sizes (both larger and smaller than 7B and 13B parameters) to understand how SSU's effectiveness changes with model scale.