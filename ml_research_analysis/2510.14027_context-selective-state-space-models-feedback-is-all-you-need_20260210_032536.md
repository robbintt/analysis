---
ver: rpa2
title: 'Context-Selective State Space Models: Feedback is All You Need'
arxiv_id: '2510.14027'
source_url: https://arxiv.org/abs/2510.14027
tags:
- state
- coffee
- sequence
- embedding
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COFFEE, a context-selective state-space model
  that uses state feedback to enable context-based selectivity in sequence modeling.
  Unlike S6, which relies on token-based selectivity, COFFEE modulates its dynamics
  using the internal state, allowing it to adapt behavior based on accumulated context.
---

# Context-Selective State Space Models: Feedback is All You Need

## Quick Facts
- arXiv ID: 2510.14027
- Source URL: https://arxiv.org/abs/2510.14027
- Reference count: 40
- Key result: COFFEE achieves near-perfect induction head accuracy with two orders of magnitude fewer parameters than S6

## Executive Summary
This paper introduces COFFEE, a context-selective state-space model that uses state feedback to enable context-based selectivity in sequence modeling. Unlike S6, which relies on token-based selectivity, COFFEE modulates its dynamics using the internal state, allowing it to adapt behavior based on accumulated context. The model also eliminates parameter redundancy through basis changes, reducing the number of learnable parameters. Experiments on the induction head and MNIST tasks show that COFFEE outperforms S6, achieving near-perfect accuracy on induction head with two orders of magnitude fewer parameters and training sequences, and reaching 97% accuracy on MNIST with only 3585 parameters.

## Method Summary
COFFEE implements context-selectivity through state feedback rather than token-level gating. The model uses the internal state to modulate its dynamics, enabling adaptation based on accumulated context. A key innovation is the use of basis changes to eliminate parameter redundancy, significantly reducing the number of learnable parameters. This approach contrasts with S6's token-based selectivity and allows COFFEE to maintain selectivity while being more parameter-efficient. The state feedback mechanism enables the model to capture long-range dependencies and context patterns more effectively than traditional approaches.

## Key Results
- COFFEE achieves near-perfect accuracy on induction head tasks with two orders of magnitude fewer parameters than S6
- The model requires significantly fewer training sequences to reach high performance on induction head
- COFFEE reaches 97% accuracy on MNIST classification using only 3585 parameters
- State feedback enables better context adaptation compared to token-based selectivity approaches

## Why This Works (Mechanism)
State feedback allows the model to use its internal state to modulate dynamics, creating context-aware behavior that adapts based on accumulated information. This is more powerful than token-level gating because it captures long-range dependencies and enables the model to make decisions based on the full context history. The basis change technique removes redundant parameters by finding a more efficient representation of the model's dynamics, allowing for better generalization with fewer parameters.

## Foundational Learning
1. **State-space models**: Why needed - fundamental framework for sequence modeling; Quick check - understand how state evolves over time
2. **State feedback control**: Why needed - enables context-aware behavior; Quick check - grasp how current state influences future dynamics
3. **Basis changes in linear systems**: Why needed - eliminates parameter redundancy; Quick check - understand how changing basis affects parameter efficiency
4. **Context-selectivity**: Why needed - crucial for adapting to different input patterns; Quick check - recognize difference between token-level and state-level selectivity
5. **Parameter efficiency**: Why needed - reduces model size without sacrificing performance; Quick check - quantify parameter reduction compared to baselines

## Architecture Onboarding
Component map: Input -> State Update (with feedback) -> Output
Critical path: The state feedback loop is central - current state directly influences next state computation
Design tradeoffs: State feedback provides better context awareness but may introduce stability concerns; basis changes improve efficiency but require careful implementation
Failure signatures: Poor context adaptation, instability in state evolution, failure to maintain selectivity across varying input patterns
First experiments:
1. Test state feedback mechanism on simple sequence classification task
2. Verify parameter reduction through basis changes on a linear system
3. Compare context adaptation between state feedback and token-based approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on synthetic induction head and MNIST tasks, limiting generalizability
- No computational efficiency metrics (FLOPs, latency, memory) provided for practical comparison
- Limited analysis of when state feedback might underperform or fail
- Claims of superiority lack support from diverse real-world benchmarks

## Confidence
High confidence: Mathematical formulation of state feedback is sound, parameter reduction through basis changes is verifiable
Medium confidence: Performance gains on induction head tasks are documented, but practical significance for real applications unclear
Low confidence: Assertion that state feedback supersedes existing methods for general sequence modeling lacks adequate empirical support

## Next Checks
1. Benchmark COFFEE on standard language modeling datasets (WikiText-103, LAMBADA) to assess practical utility beyond synthetic tasks
2. Conduct ablation studies comparing state feedback versus token-based selectivity across varying sequence lengths and noise levels
3. Measure wall-clock inference time and memory consumption on GPU/CPU to determine practical deployment viability relative to S6 and transformer baselines