---
ver: rpa2
title: 'Advancing Retrieval-Augmented Generation for Persian: Development of Language
  Models, Comprehensive Benchmarks, and Best Practices for Optimization'
arxiv_id: '2501.04858'
source_url: https://arxiv.org/abs/2501.04858
tags:
- retrieval
- persian
- language
- generation
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in developing Retrieval-Augmented
  Generation (RAG) systems for Persian, a low-resource language with complex morphology
  and syntax. It introduces Persian-specific models MatinaRoberta (masked language
  model) and MatinaSRoberta (fine-tuned Sentence-BERT), trained on a 73.11 billion
  token corpus.
---

# Advancing Retrieval-Augmented Generation for Persian: Development of Language Models, Comprehensive Benchmarks, and Best Practices for Optimization

## Quick Facts
- **arXiv ID:** 2501.04858
- **Source URL:** https://arxiv.org/abs/2501.04858
- **Reference count:** 23
- **Primary result:** Persian-specific MatinaSRoberta embeddings significantly outperform existing multilingual models in RAG retrieval accuracy across general knowledge, scientific, and formal document domains.

## Executive Summary
This paper addresses the challenges of building effective Retrieval-Augmented Generation systems for Persian, a morphologically complex low-resource language. The authors introduce MatinaRoberta (masked language model) and MatinaSRoberta (Sentence-BERT fine-tuned model) trained on a massive 73.11 billion token Persian corpus. Through comprehensive benchmarking across three distinct domains—general knowledge (PQuad), scientific-specialized texts, and formal organizational reports—the study demonstrates significant improvements in retrieval accuracy using these custom embeddings. The research establishes optimal configurations for RAG systems, including temperature tuning (0.25), chunk size optimization (512 tokens), and document summary indexing. Results show larger models like Llama-3.1 70B consistently achieve the highest generation accuracy, while smaller models struggle with domain-specific and formal contexts. The findings provide a roadmap for developing effective RAG systems in morphologically complex languages through customized embeddings and optimized retrieval-generation configurations.

## Method Summary
The authors developed MatinaRoberta by continually pretraining XLM-RoBERTa Large on 73.11 billion Persian tokens using MLM objective, max sequence length 512, learning rate 5e-5, batch size 30 per device with 8 GPUs and 2 gradient accumulation steps (effective batch 480), AdamW optimizer, 1 epoch, DeepSpeed Stage 0, FP16, taking one week on 8× A800 GPUs. MatinaSRoberta was then fine-tuned using Sentence-Transformers with Multiple Negatives Ranking Loss, Contrastive Loss, Triplet Loss, and Softmax Loss across Persian QA, entailment, and paraphrase datasets, batch size 30, 4 epochs, polynomial learning rate scheduler, warmup 0.4, weight decay 0.01, taking 7 days on 8× A800. The RAG evaluation used LlamaIndex with max tokens 2048, chunk sizes 512-2048, overlap 256, top-k 5, temperature 0-0.75, and RAGAS framework for measuring faithfulness, context precision, and answer relevancy across PQuad (80K questions), scientific textbook MCQs, and organizational policy documents.

## Key Results
- MatinaSRoberta embeddings significantly outperformed LaBSE, gte-large, and other baselines in retrieval accuracy across all three Persian datasets (PQuad, scientific, organizational)
- Temperature tuning revealed 0.25 as optimal for maximizing factual consistency in generation across all model sizes
- Chunk size of 512 tokens produced the best overall results for formal and technical documents, maintaining contextual relevance
- Larger models (Llama-3.1 70B) consistently achieved higher generation accuracy compared to smaller models, especially for domain-specific and formal contexts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Language-specific continual pretraining aligns embeddings with complex morphology better than multilingual general-purpose models.
- **Mechanism:** By continually pretraining XLM-RoBERTa on 73.11B Persian tokens (MatinaRoberta) and fine-tuning with Multiple Negatives Ranking Loss (MatinaSRoberta), the model learns dense vector representations that capture Persian's agglutination and flexible syntax, reducing the semantic distance between varied surface forms of the same concept.
- **Core assumption:** The performance gain stems from exposure to a massive, deduplicated Persian corpus rather than model architectural changes, assuming the underlying XLM-RoBERTa structure is sufficient if data-specific weights are optimized.
- **Evidence anchors:**
  - [abstract] "MatinaSRoberta outperformed existing embeddings... trained on a varied corpus of 73.11 billion Persian tokens."
  - [section 4] Mentions fine-tuning utilized "Multiple Negatives Ranking Loss" and "Triplet Loss" to align embeddings for semantic similarity.
  - [corpus] "Hakim: Farsi Text Embedding Model" appears in neighbor texts, suggesting a trend toward dedicated Persian embeddings, though direct citation linkage for this specific mechanism is low.
- **Break condition:** If the target domain uses highly informal or dialect-heavy Persian not represented in the Common Crawl/Scientific article mix, the semantic alignment may degrade.

### Mechanism 2
- **Claim:** Smaller chunk sizes (512 tokens) increase retrieval precision in formal and technical documents by limiting semantic dilution.
- **Mechanism:** In complex formal texts (like organizational reports), breaking documents into 512-token chunks forces the embedding model to represent specific, localized context rather than averaging across a broad, loosely related section. This increases the signal-to-noise ratio when matching against specific user queries.
- **Core assumption:** The query is specific enough to be answered within a 512-token window; if the answer requires synthesis across 1000+ tokens, retrieval accuracy will drop (recall vs. precision trade-off).
- **Evidence anchors:**
  - [section 6.2.2] "chunk size of 512 tokens produced the best overall results... maintaining a higher degree of contextual relevance."
  - [abstract] "chunk size modifications... were explored to enhance RAG setups."
- **Break condition:** If the document structure is highly interdependent (e.g., a legal clause defined in Section 1 and applied in Section 5), small chunks may miss the logical connection.

### Mechanism 3
- **Claim:** Document summary indexing improves efficiency and accuracy for broad queries by acting as a high-level semantic router.
- **Mechanism:** Instead of querying raw chunks immediately, the system generates and indexes document summaries. A query first hits these summaries (coarse retrieval) to identify relevant documents before drilling down into specific chunks (fine retrieval), reducing the search space and computational load.
- **Core assumption:** The summarization model (LLM) can capture the "gist" of a document without hallucinating key themes that exist in the source text.
- **Evidence anchors:**
  - [section 5.3.4] "This approach involved using an LLM during the indexing phase to generate concise summaries... enabling faster and more efficient retrieval."
  - [section 6.2.3] Table 9 shows "With Summary" outperforming "Without Summary" in PQuad and Organizational datasets.
- **Break condition:** If the summarization step introduces bias or omits specific details required for retrieval, the routing fails and the relevant document is never retrieved.

## Foundational Learning

- **Concept: Agglutinative Morphology**
  - **Why needed here:** Persian constructs words by stringing together morphemes (e.g., "going" might be a single word incorporating subject/tense). This creates a massive vocabulary where simple keyword matching fails.
  - **Quick check question:** Why would a standard tokenizer trained on English fail to properly chunk Persian verbs, and how does a Sentence-BERT model mitigate this?

- **Concept: Bi-Encoder (Dense Retrieval) vs. Cross-Encoder**
  - **Why needed here:** The paper relies on MatinaSRoberta (a bi-encoder) for retrieval. You must understand that this produces a fixed vector for a chunk, enabling fast semantic search, distinct from a cross-encoder which compares query and document simultaneously but is too slow for large corpora.
  - **Quick check question:** In the RAG pipeline described, where exactly does the MatinaSRoberta model fit—does it generate the final answer or the search index?

- **Concept: Temperature in Generation**
  - **Why needed here:** The study identifies 0.25 as optimal. You need to understand that temperature controls the probability distribution sharpness; low temp forces the model to pick the most likely token, reducing creativity but increasing factual consistency.
  - **Quick check question:** If a user wants a creative poem about a scientific concept found in the retrieved context, should you stick to the 0.25 temperature recommended for factual QA in this paper?

## Architecture Onboarding

- **Component map:**
  1.  **Indexer:** MatinaSRoberta (1024-dim vectors) + Document Summary Generator (LLM).
  2.  **Retriever:** Vector Store (likely FAISS/Chroma implied) using Cosine Similarity.
  3.  **Generator:** Llama-3.1 (8B or 70B) or Qwen2.
  4.  **Evaluator:** RAGAS Framework (measuring Faithfulness, Context Precision).

- **Critical path:**
  Raw Text -> Preprocessing (Dedup) -> [Split: 512 tokens] -> MatinaSRoberta Embedding -> Vector DB.
  Query -> MatinaSRoberta Embedding -> Vector Search (Top K=5) -> Context Injection -> Llama-3.1 (Temp=0.25) -> Answer.

- **Design tradeoffs:**
  - **Precision vs. Cost:** Using Llama-3.1 70B offers the highest accuracy (Abstract/Section 6.2.1) but requires significant GPU resources compared to 8B.
  - **Chunk Size:** 512 tokens maximize precision for formal text (Section 6.2.2), but may lose context for broad summaries; 1024 is a safer default if domain is unknown.
  - **Summary Indexing:** Adds latency and cost during the indexing phase (requires LLM pass) but speeds up query time for large documents.

- **Failure signatures:**
  - **Low Context Precision:** Retrieved chunks seem unrelated to the specific question. *Diagnosis:* Embedding model (e.g., non-Persian specific) failing to capture nuances; switch to MatinaSRoberta.
  - **Hallucination in Formal Context:** Model makes up laws or policies. *Diagnosis:* Temperature too high or model too small (e.g., Gemma 1.1); lower temperature to 0.25 or switch to 70B model.
  - **Retrieval Misses:** Answer exists but isn't found. *Diagnosis:* Chunk size too small, splitting the answer from its context clue.

- **First 3 experiments:**
  1.  **Baseline Retrieval:** Benchmark MatinaSRoberta against LaBSE or OpenAI embeddings on a sample of 100 Persian questions (PQuad) to verify the ~15-20% lift in retrieval accuracy claimed in Table 5.
  2.  **Temperature Sweep:** Run the Llama-3.1 8B model on the Scientific-Specialized dataset with Temp = [0, 0.25, 0.5, 0.75] to replicate the curve in Table 7 and confirm 0.25 is the sweet spot for factual consistency.
  3.  **Chunk Size A/B Test:** Process the Organizational Report using 512 vs. 2048 chunk sizes. Measure the "Context Precision" score using RAGAS to validate if smaller chunks actually improve precision for formal text as claimed in Table 8.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies primarily on synthetic datasets (PQuad, MCQs) rather than real-world user queries, which may not capture actual usage complexity
- 73.11 billion token corpus composition and preprocessing pipeline are not fully specified, making it difficult to assess potential biases or domain gaps
- Document summary indexing approach adds computational overhead during indexing phase that isn't quantified in terms of cost-benefit tradeoffs

## Confidence
- **High Confidence:** The retrieval accuracy improvements using MatinaSRoberta over baseline embeddings are well-supported by direct comparisons across three distinct datasets (Tables 5, 8, 9)
- **Medium Confidence:** The optimal temperature (0.25) and chunk size (512) findings are supported by systematic experimentation but may be domain-specific rather than universal
- **Medium Confidence:** The superiority of larger models (Llama-3.1 70B) for generation accuracy is demonstrated but could be influenced by the specific evaluation metrics and datasets used

## Next Checks
1. **Cross-Domain Generalization Test:** Apply the optimized RAG pipeline (MatinaSRoberta, temp=0.25, chunk=512) to a new domain of Persian legal documents or medical texts not represented in the original corpus, measuring retrieval accuracy and faithfulness to assess domain transfer

2. **Real-World Query Evaluation:** Replace synthetic datasets with a collection of actual user queries from Persian knowledge bases or forums, evaluating whether the performance gains observed in controlled experiments hold under realistic conditions

3. **Cost-Benefit Analysis of Summary Indexing:** Quantify the additional indexing time and computational resources required for document summary generation versus the retrieval speed improvements observed during query time, providing a complete efficiency picture