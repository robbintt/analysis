---
ver: rpa2
title: 'Understanding Critical Thinking in Generative Artificial Intelligence Use:
  Development, Validation, and Correlates of the Critical Thinking in AI Use Scale'
arxiv_id: '2512.12413'
source_url: https://arxiv.org/abs/2512.12413
tags:
- thinking
- critical
- scale
- https
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research developed and validated the Critical Thinking in
  AI Use Scale, a 13-item measure assessing individuals' tendencies to verify AI-generated
  content, understand AI systems, and reflect on broader implications. Across six
  studies (N = 1365), the scale demonstrated strong psychometric properties, including
  a stable three-factor structure (Verification, Motivation, Reflection), high reliability,
  convergent/discriminant validity, sex invariance, and good test-retest reliability.
---

# Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale

## Quick Facts
- arXiv ID: 2512.12413
- Source URL: https://arxiv.org/abs/2512.12413
- Reference count: 40
- The scale demonstrates strong psychometric properties and predicts more effective AI verification behaviors

## Executive Summary
This research introduces the Critical Thinking in AI Use Scale, a validated 13-item measure designed to assess individuals' tendencies to critically engage with AI-generated content. Across six studies with 1,365 participants, the scale demonstrates a stable three-factor structure (Verification, Motivation, Reflection), high reliability, and convergent/discriminant validity. Higher scores predict more frequent verification behaviors, greater accuracy in fact-checking tasks, and deeper reflection on responsible AI use. The scale provides a practical tool for measuring and promoting responsible AI engagement in educational and professional contexts.

## Method Summary
The scale was developed through iterative refinement: an initial 27-item pool was content-validated by 24 judges, then reduced to 13 items via exploratory factor analysis (N=270). Confirmatory factor analysis across independent samples (N=376, N=290) established a higher-order three-factor structure. Six studies tested reliability (internal consistency, test-retest), validity (convergent, discriminant, criterion), measurement invariance, and correlates with personality, affect, and task performance. Study 6 used a ChatGPT-4o-powered interface with 12 statements and 4 source-link conditions to assess criterion validity.

## Key Results
- The 13-item scale demonstrated a stable three-factor structure (Verification, Motivation, Reflection) across multiple samples
- Scale scores predicted more diverse verification methods and higher accuracy in a ChatGPT-powered fact-checking task
- Higher scores correlated with greater reflection on responsible AI use and showed measurement invariance across sex
- Internal consistency was high (Cronbach's alpha > .80) with good test-retest reliability (ICC ~0.70)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critical thinking in AI use acts as a motivational disposition that triggers override of heuristic cues and initiates systematic processing of AI outputs.
- Mechanism: The Heuristic-Systematic Model (HSM) is applied to AI interaction. AI outputs present heuristic cues (fluency, authority, interface design) that encourage passive acceptance. Critical thinking in AI use acts as a motivational disposition that triggers the override of these cues, initiating effortful evaluation behaviors like cross-referencing and source scrutiny.
- Core assumption: That the cognitive architecture described by dual-process theories (System 1/System 2) applies directly to the evaluation of AI-generated outputs.
- Evidence anchors: The paper conceptualizes critical thinking in AI use as involving "verification behaviours" and "motivational disposition to engage critically." Pages 7-9 explicitly map verification to overriding heuristic processing and initiating systematic processing under HSM and dual-process frameworks.

### Mechanism 2
- Claim: Epistemic motivation—the drive to understand how AI systems work—increases both the ability and willingness to engage in effective verification and reflection.
- Mechanism: According to HSM, systematic processing requires both motivation and ability. Epistemic motivation supplies the *will*, while knowledge of AI systems (training data, failure modes, biases) supplies the *skill*. This combination enables more targeted and efficient verification strategies and deeper ethical reflection.
- Core assumption: That declarative knowledge about AI systems causally improves the quality of evaluative judgments about specific AI outputs.
- Evidence anchors: Pages 9-10 detail how understanding AI systems shapes propensity for evaluative behaviors and supports forward-looking learning. Study 6 results show higher critical thinking scores predict greater diversity of verification methods and deeper reflection (pages 52-54).

### Mechanism 3
- Claim: Positive and negative affective states may differentially facilitate distinct components of critical thinking in AI use.
- Mechanism: Drawing on feelings-as-information theory, positive affect may signal benign conditions, promoting open-minded exploration (supporting Motivation and Verification facets), while negative affect may signal potential problems, triggering vigilant, analytic scrutiny (supporting Reflection facet).
- Core assumption: That mood functions as metacognitive information that tunes information-processing strategies in AI contexts as it does in other domains.
- Evidence anchors: Pages 11-12 and 61-62 discuss the theoretical basis for affect's influence and report differential associations at the subscale level. Study 4 found positive affect correlated with Verification and Motivation, while negative affect correlated with Reflection.

## Foundational Learning

- Concept: **Critical Thinking Disposition**
  - Why needed here: The construct extends general critical thinking disposition (a stable tendency toward reflective, effortful evaluation) to the domain of AI. Understanding this foundation is necessary to interpret the scale as a domain-specific adaptation, not an entirely new construct.
  - Quick check question: How does the paper's definition of critical thinking in AI use build upon Ennis's (1985, 2011) foundational definition?

- Concept: **Dual-Process Theories (HSM)**
  - Why needed here: The HSM provides the primary theoretical framework for explaining *why* and *how* users switch from passive to active engagement with AI. The scale's three factors are argued to map onto systematic processing and its prerequisites.
  - Quick check question: According to the paper's HSM application, what two conditions are required for systematic processing of AI outputs?

- Concept: **Psychometric Validation Principles**
  - Why needed here: To evaluate the rigor of the scale's development and the reliability of its claims. Key concepts include internal consistency (Cronbach's alpha, omega), test-retest reliability, factor structure (EFA, CFA), and validity (convergent, discriminant, criterion).
  - Quick check question: What does the paper's test-retest reliability result (ICC ~0.70) indicate about the stability of the critical thinking in AI use construct?

## Architecture Onboarding

- Component map:
  1. Item Pool (27 items) → Content Validation (24 judges) → EFA (N=270) → 13 items across 3 factors
  2. CFA (N=376, 290) → Higher-order model confirmation
  3. Reliability Studies → Internal consistency (α/.ω > .80), test-retest (1 & 2 week)
  4. Validity Studies → Convergent (e.g., with general critical thinking), discriminant (e.g., from AI dependency), criterion (prediction of fact-checking task performance)

- Critical path: The path from theory-driven item generation → content validation → factor structure discovery (EFA) → factor structure confirmation (CFA) → reliability testing → validity testing (convergent, discriminant, criterion) is the standard and correctly executed scale validation path. The criterion validity study (Study 6) is particularly critical for establishing practical relevance.

- Design tradeoffs:
  1. Breadth vs. depth: The 13-item scale is concise and practical but may sacrifice nuance within each dimension. The original 27-item pool had more depth.
  2. Self-report vs. behavioral: The scale relies on self-report, which may not always align with actual behavior. Study 6's behavioral tasks were a crucial but resource-intensive addition to mitigate this.
  3. Generality vs. specificity: Items are worded for general AI tools, not specific models (e.g., GPT-4 vs. Claude). This aids generalizability but may reduce precision for particular contexts.

- Failure signatures:
  1. Low test-retest reliability: Would suggest the construct is a transient state, not a stable disposition.
  2. Poor criterion validity: If scale scores did not predict performance on the fact-checking or reflection tasks, its practical utility would be undermined.
  3. Cross-loading or unstable factor structure: Would indicate the theoretical model (3 factors) is not empirically supported.

- First 3 experiments:
  1. Test invariance across AI familiarity: Administer the scale to groups with low, medium, and high AI usage. Test for measurement invariance to ensure the construct is measured equivalently across these groups. Hypothesis: The scale will show metric invariance.
  2. Predictive longitudinal study: Measure scale scores at Time 1. At Time 2 (e.g., 1 month later), track participants' real-world AI interactions (e.g., via self-report diary or app logging). Hypothesis: Higher Time 1 scores will predict more frequent verification behaviors and fewer instances of accepting hallucinated information at Time 2.
  3. Experimental manipulation of motivation: Recruit participants, pre-screen for scale scores. Experimentally manipulate epistemic motivation (e.g., by providing a short, engaging tutorial on LLM limitations to one group). Then administer the fact-checking task. Hypothesis: The motivation manipulation will improve fact-checking accuracy, particularly for those initially lower in the Motivation facet, demonstrating a causal link.

## Open Questions the Paper Calls Out

- Does the Critical Thinking in AI Use Scale demonstrate measurement invariance and construct validity across different cultures, languages, and organizational contexts? The validation studies relied exclusively on English-speaking samples from online platforms and universities in the U.S. and Singapore, limiting generalizability to non-Western or professional populations.

- Does critical thinking in AI use predict verification behavior and fact-checking accuracy incrementally over generative AI literacy? The authors note they "did not include direct measures of (generative) AI literacy," creating an inability to examine "overlap, discriminant validity, and incremental predictive value."

- How does domain-specific expertise interact with critical thinking in AI use to influence the detection of AI hallucinations? "Future studies should... measure and experimentally manipulate topic knowledge to clarify how critical thinking in AI use combines with domain-specific expertise in guiding verification behaviour."

## Limitations

- The correlational nature of evidence cannot establish whether increasing critical thinking dispositions causally improves AI use outcomes
- Scale generalizability across diverse AI applications and cultural contexts remains untested beyond US and Singapore samples
- The exploratory affect findings are particularly tentative, lacking theoretical integration with the primary validation framework

## Confidence

- **High Confidence**: The scale's psychometric properties (factor structure, reliability, measurement invariance) are well-established through multiple independent samples and rigorous statistical testing.
- **Medium Confidence**: The convergent and discriminant validity evidence is strong, though the theoretical framework connecting the scale to broader critical thinking literature could be more explicitly developed.
- **Low Confidence**: The proposed mechanisms (HSM application, affect's differential role) are primarily theoretical, with limited direct empirical support within this work.

## Next Checks

1. **Experimental Intervention**: Conduct a randomized controlled trial where participants receive critical thinking training specifically for AI use. Test whether this training causally increases scale scores and improves performance on AI fact-checking tasks.

2. **Ecological Momentary Assessment**: Use smartphone-based experience sampling to capture participants' real-time AI interactions and critical thinking behaviors over several weeks. Compare these behavioral data against scale scores to assess ecological validity.

3. **Cross-Cultural Validation**: Administer the scale to diverse cultural samples (e.g., East Asian, Middle Eastern, Latin American) and test for measurement invariance. This would establish whether the three-factor structure holds across different cultural contexts and educational systems.