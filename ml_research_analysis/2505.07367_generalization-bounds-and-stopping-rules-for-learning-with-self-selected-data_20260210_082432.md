---
ver: rpa2
title: Generalization Bounds and Stopping Rules for Learning with Self-Selected Data
arxiv_id: '2505.07367'
source_url: https://arxiv.org/abs/2505.07367
tags:
- learning
- sample
- reciprocal
- data
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes universal generalization bounds for reciprocal
  learning algorithms that self-select training data based on previously learned parameters.
  The key contribution is proving that these algorithms can generalize well despite
  altering their training samples, using covering numbers and Wasserstein ambiguity
  sets.
---

# Generalization Bounds and Stopping Rules for Learning with Self-Selected Data

## Quick Facts
- arXiv ID: 2505.07367
- Source URL: https://arxiv.org/abs/2505.07367
- Authors: Julian Rodemann; James Bailie
- Reference count: 40
- Primary result: Establishes universal generalization bounds for self-selecting learning algorithms using Wasserstein geometry

## Executive Summary
This paper addresses a fundamental challenge in machine learning: how algorithms that select their own training data can still generalize well. The authors develop a theoretical framework that proves self-selecting algorithms (like active learning and self-training) can maintain generalization guarantees by bounding the "drift" of their training distribution. The key insight is that sample adaptation can be treated as a geometric transformation measured by Wasserstein distance, with Lipschitz constraints ensuring the drift remains controlled.

## Method Summary
The approach centers on treating sample adaptation as a transformation in probability space measured by Wasserstein-p distance. By assuming the sample adaptation function is Lipschitz continuous, the adapted sample remains within a calculable "ambiguity set" around the initial distribution. The generalization bound consists of an "initial gap" (distance from true distribution to initial sample) and a "reciprocal gap" (distance from adapted sample to initial sample). For anytime-valid stopping, the cumulative drift grows predictably with iterations, allowing practitioners to solve for maximum iterations that keep the total bound below a threshold.

## Key Results
- Proves reciprocal learning algorithms can generalize by geometrically constraining sample drift via Wasserstein bounds
- Derives anytime-valid stopping rules that provide probabilistic guarantees at any iteration
- Demonstrates concrete stopping rule for semi-supervised learning: after adding 153 pseudo-labeled points, generalization error remains within specified bound with 95% confidence
- Establishes bounds requiring only verifiable conditions on algorithms themselves, not assumptions about self-selected data distribution

## Why This Works (Mechanism)

### Mechanism 1: Bounding Sample Drift via Wasserstein Geometry
The algorithm constrains how much the training distribution can drift from the initial i.i.d. distribution by treating sample adaptation as a transformation in Wasserstein space. The Lipschitz continuity of the sample adaptation function ensures the adapted sample stays within a calculable radius, preventing distortion into unrecognizable regions.

### Mechanism 2: Linking Distributional Shift to Excess Risk
The gap between training and generalization error is controlled by the Wasserstein distance the training sample has traveled. Via Kantorovich-Rubinstein duality, if the loss is Lipschitz, the risk difference is upper-bounded by Wasserstein distance times the loss Lipschitz constant.

### Mechanism 3: Anytime-Valid Stopping via Cumulative Drift Control
The generalization bound grows predictably with iterations based on cumulative distortion. Practitioners can solve for maximum iterations that keep the total bound below threshold, acting as a "fuel gauge" for data quality that works at any stopping point.

## Foundational Learning

**Wasserstein Distance (Earth Mover's Distance)**
- Why needed: Serves as the fundamental metric to quantify how much the self-selected training sample has "moved" from the original distribution
- Quick check: Can you explain why Euclidean distance on data points isn't sufficient to measure the distance between two *distributions*?

**Lipschitz Continuity**
- Why needed: Mathematically restricts the algorithm from changing the dataset too drastically in a single step, ensuring small changes in parameters lead to proportional changes in selected data
- Quick check: If a sample adaptation function has a very high Lipschitz constant, what does that imply about the stability of the learning process?

**Covering Numbers**
- Why needed: Measure the complexity of the hypothesis class required in bounds to account for model capacity to overfit
- Quick check: Why might Rademacher complexity be less suitable here than covering entropy integrals (hint: consider the dependence on data distribution)?

## Architecture Onboarding

**Component map:** Initial Anchor -> ERM Core -> Lipschitz Adapter -> Drift Monitor -> Stop Gate

**Critical path:** The regularization of the Sample Adaptation Function. This non-standard component bridges theoretical guarantees and engineering implementation, as typical active learning doesn't explicitly enforce Lipschitz constraints on data selection.

**Design tradeoffs:**
- Tight Bounds vs. Agility: Small Lipschitz constant guarantees tight bounds but may cripple ability to select informative outliers
- Convergence vs. Anytime: Anytime-valid bounds are looser than convergent bounds, requiring more conservative iterations

**Failure signatures:**
- Bound Explosion: If generalization bound rapidly exceeds 1.0 within few iterations, $L_s$ is likely >1 or initial sample size is too small
- Stagnation: If stopping rule fires immediately ($T=0$), desired error threshold is incompatible with initial sample quality or Lipschitz constant

**First 3 experiments:**
1. Implement self-training loop and empirically measure Wasserstein distance between consecutive samples relative to parameter change to estimate effective Lipschitz constant
2. Run semi-supervised example, calculate bound at $T=150$ and $T=200$, verify empirical generalization error exceeds theoretical bound only after suggested stopping point
3. Modify pseudo-labeling threshold to vary smoothness of sample selection, plot relationship between regularizer strength, empirical $L_s$, and bound tightness

## Open Questions the Paper Calls Out
None

## Limitations
- Lipschitz continuity assumption on sample adaptation function is theoretically sound but practically difficult to verify and enforce in real algorithms
- Bound tightness heavily depends on correctly estimating diameter and concentration constants, which may be challenging for complex data distributions
- Anytime-valid bounds are looser than convergent bounds, potentially limiting practical utility in high-precision scenarios

## Confidence
- **High Confidence**: Mathematical framework connecting Wasserstein distances to generalization bounds via Kantorovich-Rubinstein duality is rigorous and well-established
- **Medium Confidence**: Interpretation of Wasserstein ambiguity sets for self-selected data adaptation is novel and theoretically justified, but practical verification requires careful implementation
- **Low Confidence**: Concrete stopping rule example assumes specific parameter choices without demonstrating empirical validation on real data distributions

## Next Checks
1. Implement a self-training algorithm with explicit Lipschitz regularization on sample selection function and empirically measure whether actual Wasserstein drift matches theoretical bound
2. Test the stopping rule on multiple semi-supervised datasets to verify generalization gap remains bounded at predicted stopping point
3. Compare anytime-valid bounds with standard early stopping criteria on same tasks to quantify practical tradeoff between guaranteed validity and bound tightness