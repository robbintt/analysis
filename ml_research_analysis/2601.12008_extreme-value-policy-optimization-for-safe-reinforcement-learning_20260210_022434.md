---
ver: rpa2
title: Extreme Value Policy Optimization for Safe Reinforcement Learning
arxiv_id: '2601.12008'
source_url: https://arxiv.org/abs/2601.12008
tags:
- extreme
- constraint
- policy
- samples
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of safety in reinforcement learning
  by proposing the Extreme Value policy Optimization (EVO) algorithm, which leverages
  Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples.
  The key innovation is introducing an extreme quantile constraint based on Generalized
  Pareto Distributions (GPDs) to capture tail behavior, combined with an extreme prioritization
  mechanism that amplifies learning from rare but high-impact extreme samples during
  experience replay.
---

# Extreme Value Policy Optimization for Safe Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.12008
- Source URL: https://arxiv.org/abs/2601.12008
- Reference count: 40
- Primary result: EVO achieves lower constraint violation probability and reduced variance compared to expectation-based and quantile regression methods while maintaining competitive policy performance

## Executive Summary
This paper addresses the fundamental challenge of safety in reinforcement learning by introducing Extreme Value policy Optimization (EVO), a novel algorithm that leverages Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples. The key innovation lies in using Generalized Pareto Distributions (GPDs) to capture tail behavior and introducing an extreme quantile constraint that provides strict guarantees on constraint satisfaction. EVO combines theoretical rigor with practical effectiveness, demonstrating significant improvements in safety metrics across multiple benchmark environments.

The approach represents a paradigm shift from traditional expectation-based safe RL methods by focusing on the extremal behavior of rewards and costs rather than their average characteristics. By amplifying learning from rare but high-impact extreme samples during experience replay, EVO addresses the critical limitation of conventional methods that often fail to properly account for safety constraints in the tails of the distribution. The algorithm achieves this through an extreme prioritization mechanism that specifically targets the most relevant samples for constraint satisfaction.

## Method Summary
EVO introduces an extreme quantile constraint based on Generalized Pareto Distributions (GPDs) to model tail behavior of rewards and costs. The algorithm uses EVT to capture extreme events and implements an extreme prioritization mechanism that amplifies learning from rare but high-impact samples during experience replay. Theoretical analysis establishes upper bounds on expected constraint violations, proving strict constraint satisfaction at a zero-violation quantile level. The method combines GPD fitting for tail modeling with prioritized sampling to focus on the most critical experiences for safety constraints.

## Key Results
- EVO achieves lower constraint violation probability compared to expectation-based and quantile regression methods
- The algorithm demonstrates reduced variance in constraint satisfaction while maintaining competitive policy performance
- Extensive experiments show EVO significantly reduces constraint violations across Safety Gym and Safety MuJoCo tasks
- EVO remains effective even with small sample sizes for GPD fitting and shows robustness to different cost limits

## Why This Works (Mechanism)
EVO works by shifting the focus from average behavior to extreme tail events, which are critical for safety in real-world applications. The Generalized Pareto Distribution provides a theoretically sound framework for modeling these rare but impactful events, while the extreme prioritization mechanism ensures that the learning algorithm properly accounts for these critical samples. By targeting the zero-violation quantile level, EVO provides strict guarantees that are more aligned with practical safety requirements than traditional expectation-based approaches.

## Foundational Learning
- Extreme Value Theory (EVT): Why needed - provides mathematical framework for modeling rare, high-impact events; Quick check - verify GPD parameters converge with sufficient extreme samples
- Generalized Pareto Distribution (GPD): Why needed - models tail behavior beyond traditional distribution assumptions; Quick check - validate GPD fitting accuracy on known extreme distributions
- Quantile regression: Why needed - enables constraint satisfaction at specific probability levels rather than expectations; Quick check - compare quantile regression performance against EVO on various quantile levels
- Prioritized experience replay: Why needed - focuses learning on the most informative samples for constraint satisfaction; Quick check - measure sample efficiency improvement from extreme prioritization
- Safety-constrained RL: Why needed - ensures practical applicability in real-world scenarios with hard constraints; Quick check - validate constraint satisfaction in safety-critical test environments

## Architecture Onboarding
Component map: Environment -> Experience Replay Buffer -> GPD Fitter -> EVO Optimizer -> Policy Network -> Environment
Critical path: Experience collection → GPD fitting → Extreme prioritization → Policy update → Constraint satisfaction verification
Design tradeoffs: GPD fitting accuracy vs. computational overhead, extreme sample rarity vs. reliable estimation, strict constraint satisfaction vs. policy performance
Failure signatures: Poor GPD fitting leading to inaccurate tail modeling, insufficient extreme samples causing unreliable prioritization, over-conservative constraints limiting policy exploration
First experiments:
1. Test EVO on a simple environment with known extreme distribution to validate GPD fitting accuracy
2. Compare constraint violation rates between EVO and baseline methods on Safety Gym tasks
3. Evaluate EVO's performance under varying levels of extreme event rarity to determine minimum sample requirements

## Open Questions the Paper Calls Out
None

## Limitations
- GPD fitting sensitivity to parameter choices and potential failure to capture tail behavior accurately in all scenarios
- Performance degradation when extreme events are extremely rare or underlying distribution deviates from standard assumptions
- Dependence on sufficient extreme samples for reliable GPD estimation, though demonstrated robustness across various sample sizes

## Confidence
High - The experimental results across multiple environments provide strong empirical support
Medium-High - Theoretical analysis is rigorous but practical applicability depends on real-world conditions
Medium - Claims about robustness to different cost limits are supported but would benefit from broader testing

## Next Checks
1. Test EVO's performance in environments with highly skewed or multimodal cost distributions where standard EVT assumptions may not hold
2. Conduct ablation studies comparing EVO's extreme prioritization mechanism against alternative sampling strategies
3. Evaluate EVO's performance under different levels of extreme event rarity to determine minimum sample size requirements for reliable GPD estimation