---
ver: rpa2
title: Fairness Dynamics During Training
arxiv_id: '2506.01709'
source_url: https://arxiv.org/abs/2506.01709
tags:
- fairness
- male
- training
- jsd-p
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two new metrics\u2014Average Rank and Jensen-Shannon\
  \ Divergence by Parts\u2014to evaluate fairness dynamics during LLM training. These\
  \ metrics track how models' biases evolve over time, capturing both performance\
  \ and confidence in predictions."
---

# Fairness Dynamics During Training

## Quick Facts
- arXiv ID: 2506.01709
- Source URL: https://arxiv.org/abs/2506.01709
- Reference count: 29
- Primary result: New metrics track LLM bias evolution during training, revealing early-stopping can significantly reduce gender bias

## Executive Summary
This paper introduces two new metrics—Average Rank and Jensen-Shannon Divergence by Parts—to evaluate fairness dynamics during LLM training. These metrics track how models' biases evolve over time, capturing both performance and confidence in predictions. The authors apply these to Pythia models on the WinoBias gender prediction task, revealing that Pythia-6.9b becomes more biased toward male predictions during training. They show that early stopping can reduce this bias significantly (92.5% fairness gain) at the cost of only 1.7% accuracy. Additionally, larger models like Pythia-6.9b exhibit more gender assumptions in neutral contexts compared to smaller models. The findings demonstrate that fairness does not always align with standard performance metrics during training and highlight opportunities for bias mitigation through training interventions.

## Method Summary
The authors evaluate Pythia model checkpoints across training steps using WinoBias Type 2 samples. They construct prompts that ask models to predict gender (male/female/not specified) of occupations referenced by pronouns. From next-token probability distributions, they compute Average Rank (AR) - the mean rank of correct answer tokens among all vocabulary tokens - and Jensen-Shannon Divergence by Parts (JSD-P) - per-option divergence from ideal one-hot distributions. They statistically compare metric distributions across answer options using Mann-Whitney U Test (p<0.01) and identify early stopping points where bias is reduced while maintaining acceptable performance.

## Key Results
- Pythia-6.9b exhibits increasing male bias during training, with AR for "male" improving while "female" AR plateaus after ~80k steps
- Early stopping at ~80k steps achieves 92.5% fairness improvement with only 1.7% accuracy loss on LAMBADA
- Larger models (6.9B) make more gender assumptions in neutral contexts compared to smaller models (160M)
- Fairness dynamics during training do not always mirror conventional performance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tracking token probability rank across the full vocabulary reveals performance disparities that accuracy metrics miss during training.
- Mechanism: Average Rank (AR) computes the mean rank of the correct answer token's probability among all vocabulary tokens. Lower AR indicates the model is placing correct answers higher in its ranked output distribution. Unlike binary accuracy, AR captures the magnitude of ranking errors, enabling detection of performance differences when accuracy remains near zero.
- Core assumption: The rank position of correct tokens correlates meaningfully with model capability before threshold-based metrics show signal.
- Evidence anchors:
  - [abstract] "Average Rank...measures performance, fairness, and confidence"
  - [section C.1] "AR increases until ≈85k steps, declines between ≈85k and ≈100k steps, then increases again" while accuracy stays near 0%
  - [corpus] Related work on fairness metrics lacks comparable rank-based training dynamics; corpus provides limited direct support for this specific mechanism
- Break condition: If correct tokens systematically appear at similar ranks for biased vs. unbiased predictions, AR cannot distinguish fairness differences.

### Mechanism 2
- Claim: Decomposing Jensen-Shannon Divergence per answer option exposes differential confidence and bias that aggregated metrics hide.
- Mechanism: JSD-P computes divergence between model output probabilities and ideal one-hot distributions *separately* for each answer option (male, female, not specified). Smaller differences between groups' JSD-P values indicate fairness; lower absolute values indicate confidence. This reveals when models are confidently biased toward one option.
- Core assumption: Assumption: Per-token divergence differences reflect meaningful bias rather than statistical noise in probability estimates.
- Evidence anchors:
  - [abstract] "Jensen-Shannon Divergence by Parts...captures both performance and confidence in predictions"
  - [section B] Formula shows JSD-P computed individually across all potential answer tokens rather than summed
  - [corpus] Neighbor papers on fairness in ML (GNNs, ASR, recommendation systems) use aggregate fairness metrics; corpus shows no comparable per-option divergence decomposition
- Break condition: If output probabilities are uniformly uncertain across options, JSD-P differences collapse and cannot detect bias.

### Mechanism 3
- Claim: Fairness and performance follow divergent trajectories during training, creating exploitable early-stopping windows.
- Mechanism: Training dynamics show AR for "male" improves while "female" AR plateaus after ~80k steps. Simultaneously, JSD-P difference between male/female predictions grows. Early stopping captures a checkpoint where performance is acceptable but bias amplification has not yet occurred.
- Core assumption: Assumption: Bias emergence timing generalizes beyond Pythia-6.9B and WinoBias to other models/tasks.
- Evidence anchors:
  - [abstract] "Pythia-6.9b can exchange 1.7% accuracy on LAMBADA for a 92.5% increase in fairness"
  - [section 2] "fairness dynamics during training do not always mirror conventional performance metrics"
  - [corpus] Limited corpus support; neighbor papers focus on post-training fairness rather than training dynamics
- Break condition: If performance and fairness are tightly coupled for a given task, early stopping cannot trade one for the other.

## Foundational Learning

- Concept: Jensen-Shannon Divergence
  - Why needed here: JSD-P builds on JS divergence; understanding the base measure is prerequisite for interpreting per-part decomposition.
  - Quick check question: How does JSD differ from KL divergence in handling zero probabilities and symmetry?

- Concept: Token probability distributions in LLMs
  - Why needed here: Both AR and JSD-P operate on next-token probability distributions; misunderstanding softmax outputs invalidates metric interpretation.
  - Quick check question: Given vocabulary V and output logits L, what is the probability rank of token t?

- Concept: WinoBias benchmark structure
  - Why needed here: The prompting setup (Type 2, gender-ambiguous vs. disambiguated) defines what "correct" and "biased" mean in this framework.
  - Quick check question: In a Type 2 WinoBias sample, how does the pronoun determine which occupation's gender is queried?

## Architecture Onboarding

- Component map:
  Prompt generator -> Inference runner -> AR calculator -> JSD-P calculator -> Significance tester

- Critical path:
  1. Load Pythia checkpoint at training step S
  2. Generate prompts from WinoBias Type 2 samples
  3. Extract softmax probabilities for {male, female, not specified} tokens
  4. Compute AR for each correct answer type
  5. Compute JSD-P for each answer option
  6. Statistical test for significance (p < 0.01 threshold per paper)
  7. Repeat across training steps; identify divergence points

- Design tradeoffs:
  - Binary gender limitation: WinoBias only captures male/female; "not specified" is neutral but not representative of non-binary identities
  - Checkpoint granularity: More checkpoints increase temporal resolution but multiply compute cost
  - Random seed handling: Option ordering affects outputs; paper uses 5 seeds for robustness

- Failure signatures:
  - JSD-P values near 0.5 across all options: Model is uniformly uncertain; metric loses discriminative power
  - AR diverging but JSD-P flat: Performance gap without confidence difference; may indicate different bias mechanism
  - Statistical tests non-significant: Either insufficient samples or no real difference; do not claim bias

- First 3 experiments:
  1. Replicate Pythia-6.9B evaluation on WinoBias Type 2: Compute AR and JSD-P at steps 0, 40k, 80k, 120k, 143k. Verify early-stopping benefit at ~80k steps.
  2. Extend to smaller Pythia variant (e.g., 410M): Test whether bias scaling relationship holds or if larger models uniquely exhibit this pattern.
  3. Ablate prompt framing: Test whether explicit "out of the options..." phrasing vs. bare prompts changes JSD-P differences to isolate prompt-design effects.

## Open Questions the Paper Calls Out

- **Question:** Does applying early stopping to improve gender fairness inadvertently degrade model fairness on other social axes (e.g., race or religion)?
- **Basis:** [explicit] The authors warn that early stopping at the identified step "may have some unintended consequences on other axes of bias" since they only evaluated binary gender.
- **Why unresolved:** The study isolates a single bias axis (gender), leaving the multi-dimensional impact of training interventions unknown.
- **What evidence would resolve it:** A longitudinal study applying Average Rank and JSD-P to multiple bias dimensions simultaneously during the same training run.

- **Question:** Do the observed fairness dynamics generalize to Large Language Models (LLMs) trained specifically for production deployment?
- **Basis:** [explicit] The authors note the Pythia suite was trained for research, so results "may not completely reflect how models trained for deployment behave."
- **Why unresolved:** Production models often utilize different data mixtures, RLHF alignment, or safety filters that could alter bias trajectories.
- **What evidence would resolve it:** Replicating the proposed evaluation methodology on checkpoints from an industry-standard, deployment-ready model.

- **Question:** How do fairness metrics and bias trajectories differ when evaluating non-binary gender contexts?
- **Basis:** [explicit] The paper acknowledges the WinoBias dataset "only examines bias across binary gender, which is a simplification of the contemporary understanding of gender."
- **Why unresolved:** Model confidence (JSD-P) and rank behavior may evolve differently for less frequent or syntactically distinct non-binary pronouns.
- **What evidence would resolve it:** Adapting the prompting setup to include non-binary gender options and applying the metrics to a dataset like WinoQueer.

## Limitations

- Binary gender framework cannot capture non-binary or intersectional identity biases
- Findings narrowly tied to Pythia checkpoints and WinoBias Type 2; generalizability to other tasks or model families remains unproven
- Study focuses on prediction confidence and performance rankings but does not assess downstream harms or real-world deployment impacts

## Confidence

- **High confidence** in the mechanism that AR and JSD-P reveal training dynamics invisible to accuracy metrics—the mathematical formulation is clear and the empirical trajectories are well-documented
- **Medium confidence** in the early-stopping intervention—while the trade-off (1.7% accuracy for 92.5% fairness gain) is quantified, it is task-specific and may not generalize
- **Low confidence** in claims about model size effects—the comparison between Pythia-160M and Pythia-6.9B is suggestive but not rigorously controlled for architecture or training data differences

## Next Checks

1. **Replication on non-binary tasks:** Apply AR and JSD-P to a benchmark that includes non-binary gender options (e.g., modified WinoBias or Winogender) to test whether these metrics remain discriminative when gender is not binary.

2. **Cross-model dynamics:** Run the same metrics on a different LLM family (e.g., GPT-Neo checkpoints) to determine if bias amplification during training is a general phenomenon or specific to Pythia.

3. **Human-in-the-loop calibration:** Survey annotators on whether JSD-P-identified "biased" predictions align with human judgments of bias, especially in cases where accuracy is high but confidence distributions differ.