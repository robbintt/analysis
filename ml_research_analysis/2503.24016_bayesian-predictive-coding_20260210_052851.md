---
ver: rpa2
title: Bayesian Predictive Coding
arxiv_id: '2503.24016'
source_url: https://arxiv.org/abs/2503.24016
tags:
- parameters
- predictive
- learning
- bayesian
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bayesian Predictive Coding (BPC), a method
  that extends predictive coding to estimate posterior distributions over neural network
  parameters, enabling uncertainty quantification while maintaining biological plausibility
  through local Hebbian updates. The core idea is to replace maximum likelihood estimates
  of parameters with approximate posterior distributions using conjugate Matrix Normal
  Wishart priors.
---

# Bayesian Predictive Coding
## Quick Facts
- arXiv ID: 2503.24016
- Source URL: https://arxiv.org/abs/2503.24016
- Reference count: 40
- Extends predictive coding to estimate posterior distributions over neural network parameters, enabling uncertainty quantification while maintaining biological plausibility through local Hebbian updates

## Executive Summary
Bayesian Predictive Coding (BPC) introduces a framework that extends traditional predictive coding by estimating posterior distributions over neural network parameters rather than point estimates. This approach maintains biological plausibility through local Hebbian updates while enabling uncertainty quantification through conjugate Matrix Normal Wishart priors. The method achieves comparable accuracy to standard predictive coding and backpropagation while requiring fewer epochs in full-batch settings.

The key innovation lies in replacing maximum likelihood parameter estimates with approximate posterior distributions, using expectation-maximization with closed-form updates. Experiments demonstrate that BPC effectively quantifies both aleatoric and epistemic uncertainty on synthetic regression tasks and outperforms Bayes by Backprop on UCI regression datasets in terms of log predictive density and RMSE, while converging faster due to its closed-form update structure.

## Method Summary
BPC extends predictive coding by replacing point estimates of parameters with approximate posterior distributions using conjugate Matrix Normal Wishart priors. The method employs expectation-maximization, alternating between gradient descent on latent variables and exact posterior updates of parameters. The closed-form updates are Hebbian functions of pre- and post-synaptic activity, maintaining biological plausibility. This approach enables uncertainty quantification while achieving comparable accuracy to standard methods with faster convergence in full-batch settings.

## Key Results
- Achieves comparable accuracy to predictive coding and backpropagation while converging in fewer epochs in full-batch settings
- Effectively quantifies both aleatoric and epistemic uncertainty on synthetic regression tasks
- Outperforms Bayes by Backprop on UCI regression datasets with better log predictive density and lower RMSE while requiring fewer iterations

## Why This Works (Mechanism)
BPC works by maintaining distributions over parameters rather than point estimates, which allows for principled uncertainty quantification. The use of conjugate Matrix Normal Wishart priors enables closed-form posterior updates that are local Hebbian functions of neural activity, maintaining biological plausibility while achieving computational efficiency through exact updates rather than sampling-based approximations.

## Foundational Learning
- **Predictive Coding**: A biologically plausible learning algorithm that minimizes prediction errors through local updates
  - Why needed: Forms the foundation for BPC's biologically plausible framework
  - Quick check: Can you explain how prediction errors drive learning in this framework?

- **Conjugate Priors**: Prior distributions that, when combined with likelihood, yield posteriors in the same family
  - Why needed: Enables closed-form posterior updates without sampling
  - Quick check: What are the properties of Matrix Normal Wishart distributions?

- **Expectation-Maximization**: Iterative algorithm alternating between expectation and maximization steps
  - Why needed: Provides the algorithmic framework for updating latent variables and parameters
  - Quick check: How do the E and M steps differ in BPC compared to standard EM?

## Architecture Onboarding
**Component Map**: Input -> Predictive Coding Layer -> Parameter Posterior Updates -> Output
**Critical Path**: Data → Latent variable optimization → Posterior parameter update → Prediction
**Design Tradeoffs**: Closed-form updates vs. computational complexity for high-dimensional parameters; local Hebbian updates vs. flexibility in representing complex posterior distributions
**Failure Signatures**: Poor uncertainty quantification may indicate inappropriate prior specification; slow convergence may suggest need for learning rate adjustment
**First Experiments**: 1) Verify closed-form posterior updates on simple linear regression 2) Compare uncertainty estimates on synthetic data with known noise structure 3) Benchmark convergence speed against Bayes by Backprop on small UCI datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to fully connected networks with small-scale UCI datasets, leaving scalability to deeper architectures uncertain
- Computational complexity of closed-form updates may become prohibitive for high-dimensional parameter spaces
- Biological plausibility claim doesn't address limitations in representing complex posterior distributions with Matrix Normal Wishart priors

## Confidence
- Comparable accuracy to predictive coding and backpropagation: High confidence
- Better log predictive density and lower RMSE than Bayes by Backprop: Medium confidence (lacks statistical significance testing)
- Scalability to deeper architectures: Low confidence (not tested)

## Next Checks
1. Test BPC on convolutional and transformer architectures to assess scalability to modern deep learning architectures
2. Conduct ablation studies varying the prior distributions to understand their impact on uncertainty quantification performance
3. Perform statistical significance testing comparing BPC to Bayes by Backprop across all reported metrics, including confidence intervals and effect sizes