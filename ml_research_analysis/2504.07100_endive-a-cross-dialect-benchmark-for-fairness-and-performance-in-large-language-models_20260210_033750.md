---
ver: rpa2
title: 'EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language
  Models'
arxiv_id: '2504.07100'
source_url: https://arxiv.org/abs/2504.07100
tags:
- answer
- dialect
- english
- language
- dialects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EnDive, a cross-dialect benchmark that evaluates
  large language models (LLMs) on 12 reasoning and natural language understanding
  tasks translated into five underrepresented English dialects: African American Vernacular
  English, Indian English, Jamaican English, Chicano English, and Colloquial Singaporean
  English. Translations were generated using few-shot prompting informed by native
  speaker examples and filtered to ensure dialectal diversity.'
---

# EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models

## Quick Facts
- arXiv ID: 2504.07100
- Source URL: https://arxiv.org/abs/2504.07100
- Reference count: 40
- Models consistently underperform on dialectal inputs compared to Standard American English across 12 reasoning tasks

## Executive Summary
This paper introduces EnDive, a cross-dialect benchmark that evaluates large language models (LLMs) on 12 reasoning and natural language understanding tasks translated into five underrepresented English dialects: African American Vernacular English, Indian English, Jamaican English, Chicano English, and Colloquial Singaporean English. Translations were generated using few-shot prompting informed by native speaker examples and filtered to ensure dialectal diversity. Human evaluations confirmed high translation quality (6.02/7+ on fluency, faithfulness, and formality). Comprehensive testing of five LLMs revealed consistent performance disparities between SAE and dialectal inputs, with models underperforming on non-standard dialects across zero-shot and chain-of-thought settings. This highlights significant fairness gaps in current NLP systems and underscores the need for more inclusive, dialect-aware language technologies.

## Method Summary
EnDive translates 12 Standard American English datasets (BoolQ, GSM8K, HumanEval, etc.) into five underrepresented English dialects using few-shot prompting with GPT-4o and verified eWAVE exemplars. Translations are filtered by BLEU score (<0.7) to ensure linguistic distance from SAE. Five LLMs (GPT-4o, GPT-4o-mini, Claude-3.5-Sonnet, DeepSeek-v3, LLaMA-3-8B) are evaluated using zero-shot and chain-of-thought prompts. Human evaluators assess translation quality (fluency, faithfulness, formality), and automated metrics (ROUGE, BERTScore) measure diversity and quality. The benchmark reveals consistent performance disparities between SAE and dialectal inputs, highlighting linguistic bias in LLMs.

## Key Results
- Models consistently underperform on dialectal inputs compared to SAE across all 12 tasks
- BLEU filtering (threshold ≥0.7) ensures linguistic distance and creates a challenging benchmark
- Human evaluations confirm high translation quality (6.02/7+ on fluency, faithfulness, formality)
- Performance disparities persist across zero-shot and chain-of-thought settings

## Why This Works (Mechanism)

### Mechanism 1: Few-Shot Exemplars Enable Dialect Generalization
Few-shot prompting with native speaker examples allows generative models to capture sociolinguistic nuances that rule-based systems miss. By grounding generation in authentic eWAVE examples, the model generalizes syntactic and pragmatic patterns to new contexts rather than applying rigid word swaps. This mechanism assumes the underlying generative model possesses sufficient latent knowledge of the target dialect to replicate its structure when properly conditioned.

### Mechanism 2: BLEU Filtering Amplifies Linguistic Distance
Removing translations with high BLEU similarity (≥0.7) to SAE creates a challenging benchmark that prevents models from achieving high scores via superficial lexical overlap. This ensures the dataset requires genuine morphological and syntactic divergence processing rather than minor spelling variations. The mechanism assumes semantic equivalence is preserved even when lexical overlap is low.

### Mechanism 3: Performance Divergence Signals Training Bias
Consistent accuracy drops across diverse tasks when moving from SAE to dialect inputs suggest LLMs have internalized SAE as the "correct" prior, causing reasoning degradation on non-standard varieties. This occurs because LLMs optimize for training data probability distribution, and when input tokens deviate from SAE distribution, attention mechanisms may misalign, consuming cognitive capacity on parsing rather than reasoning.

## Foundational Learning

- **Concept: In-Context Learning (Few-Shot Prompting)**
  - Why needed here: This is the engine of the benchmark's creation. You must understand how LLMs use examples to conditionalize their output without weight updates to grasp how the paper generates authentic dialect data.
  - Quick check question: If you provide 3 examples of AAVE in a prompt, does the model "learn" AAVE permanently? (Answer: No, it only conditions the current context window)

- **Concept: BLEU Score (Bilingual Evaluation Understudy)**
  - Why needed here: This is the quality/diversity filter. You need to know that it measures n-gram overlap to understand why a low score implies "linguistic distance" in this context.
  - Quick check question: Why would a perfect dialect translation of "I am going to the store" to "I'm finna go to the store" result in a low BLEU score compared to the original?

- **Concept: Sociolinguistic Variation (Morphosyntax vs. Lexicon)**
  - Why needed here: The paper critiques rule-based systems for focusing on lexicon. You need to distinguish between swapping words (lexicon) and changing sentence structure/grammar (morphosyntax) to understand the "authenticity" claims.
  - Quick check question: Does changing "isn't" to "ain't" capture the full complexity of AAVE? (Hint: Consider the "habitual be" mentioned in the paper)

## Architecture Onboarding

- **Component map:** Source Datasets -> Translation Engine -> Filtering Layer -> Evaluation Harness -> Validation Suite
- **Critical path:** The integrity of the Translation Engine is paramount. If the few-shot examples do not result in high-quality semantic preservation, the entire evaluation fails (it measures noise, not dialect robustness)
- **Design tradeoffs:**
  - Synthetic vs. Natural: The paper generates synthetic dialect data via LLMs rather than collecting organic user data. This allows control over task difficulty but risks "textbook" dialect representations
  - Rule-based vs. Generative: They trade the reproducibility of rule-based systems for the fluency and syntactic integration of generative models
- **Failure signatures:**
  - Semantic Drift: The dialect version of a math problem becomes unsolvable because key numbers or conditions are altered during translation
  - Stereotyping: The LLM generates translations that are offensive caricatures rather than authentic linguistic representations
  - Metric Saturation: If models perform equally well on SAE and dialects, the benchmark may be too easy (BLEU filtering failed) or the models are already robust
- **First 3 experiments:**
  1. Sanity Check (SAE Baseline): Run all 5 models on the original SAE datasets to establish baseline competency
  2. Translation Validation: Randomly sample 20 translations per dialect to verify semantic preservation
  3. Bias Probe (The "Gap" Test): Run a lightweight model on a single task across all dialects to confirm performance drops relative to SAE

## Open Questions the Paper Calls Out

### Open Question 1
Can specific mitigation strategies, such as dialect-aware fine-tuning or prompt augmentation, close the performance gap between SAE and dialectal inputs without degrading SAE performance?
- Basis in paper: The authors state they "do not deeply investigate the underlying causes of these discrepancies or propose direct mitigation strategies"
- Why unresolved: The paper focuses exclusively on benchmark creation and evaluation to reveal biases, stopping short of implementing or testing methods to fix identified performance disparities
- What evidence would resolve it: A study using EnDive to fine-tune a model on dialectal data, followed by evaluation showing converged performance metrics across both SAE and dialectal test sets

### Open Question 2
Do performance disparities in EnDive correlate with performance on tasks requiring high pragmatic or cultural understanding, such as figurative language or conversational reasoning?
- Basis in paper: The Limitations section notes that the benchmark does not cover "Figurative Language Understanding, Commonsense Reasoning, and Conversational Reasoning," which may "reveal further biases"
- Why unresolved: The current benchmark is restricted to logic, math, and algorithmic reasoning; it does not test the sociocultural nuances where dialectal variation might have an even larger impact on model comprehension
- What evidence would resolve it: Extending EnDive to include the specified missing task types and comparing the magnitude of the performance gap against the reasoning-focused tasks

### Open Question 3
Can alternative data filtering metrics replace BLEU-score thresholds to preserve sufficient Chicano English (ChcE) data for statistically robust evaluation?
- Basis in paper: The authors report that for ChcE, "the number of remaining translations was extremely low because Multi-VALUE struggled... and many were further filtered out due to BLEU score thresholds"
- Why unresolved: The current methodology (BLEU > 0.7 filtering) inadvertently created a data scarcity issue for ChcE, leaving insufficient data points to evaluate the translation metrics reliably for that dialect
- What evidence would resolve it: Implementing semantic similarity filtering (e.g., embedding distance) or perplexity-based filtering to retain a larger corpus of ChcE translations, followed by successful re-evaluation

## Limitations

- Translation quality and dialect authenticity depend on the representativeness of eWAVE exemplars used for few-shot prompting
- BLEU-based filtering lacks direct empirical validation that low scores correlate with increased reasoning difficulty rather than semantic degradation
- Performance gaps may stem from cultural or contextual knowledge differences rather than purely linguistic bias

## Confidence

- **High Confidence:** The existence of performance disparities between SAE and dialectal inputs is well-supported by empirical results across multiple models and tasks
- **Medium Confidence:** The claim that disparities stem from training bias against non-standard dialects is plausible but not definitively proven
- **Low Confidence:** The assertion that few-shot prompting produces "authentic" dialect representations is difficult to verify without knowing exact exemplars used

## Next Checks

1. **Replication with Controlled Exemplars:** Re-run the translation pipeline using a fixed, publicly documented set of eWAVE exemplars to verify consistent production of semantically equivalent and dialectally authentic outputs
2. **Semantic Preservation Audit:** For a random sample of translated tasks (e.g., GSM8K math problems), have human evaluators verify that the dialectal version remains solvable and that no critical information is lost or distorted
3. **Alternative Bias Probes:** Test whether performance gaps persist when controlling for cultural context by creating SAE versions of tasks that incorporate dialect-specific cultural references, to isolate linguistic processing difficulty from knowledge-based effects