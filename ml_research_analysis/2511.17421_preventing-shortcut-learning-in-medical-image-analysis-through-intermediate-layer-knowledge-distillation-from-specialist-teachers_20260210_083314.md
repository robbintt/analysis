---
ver: rpa2
title: Preventing Shortcut Learning in Medical Image Analysis through Intermediate
  Layer Knowledge Distillation from Specialist Teachers
arxiv_id: '2511.17421'
source_url: https://arxiv.org/abs/2511.17421
tags:
- data
- shortcut
- training
- teacher
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel knowledge distillation approach to
  prevent shortcut learning in medical image analysis by leveraging intermediate-layer
  guidance from a teacher network fine-tuned on a small subset of unbiased, task-relevant
  data. The authors demonstrate that different types of shortcuts manifest distinctly
  across network layers, with diffuse shortcuts emerging in earlier layers and localized
  shortcuts appearing in later layers.
---

# Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers

## Quick Facts
- arXiv ID: 2511.17421
- Source URL: https://arxiv.org/abs/2511.17421
- Authors: Christopher Boland; Sotirios Tsaftaris; Sonia Dahdouh
- Reference count: 40
- Primary result: Intermediate-layer knowledge distillation from specialist teachers outperforms traditional bias-mitigation approaches on medical image datasets

## Executive Summary
This paper introduces a novel knowledge distillation approach that prevents shortcut learning in medical image analysis by leveraging intermediate-layer guidance from a teacher network fine-tuned on a small subset of unbiased, task-relevant data. The authors demonstrate that different types of shortcuts manifest distinctly across network layers, with diffuse shortcuts emerging in earlier layers and localized shortcuts appearing in later layers. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), they show that their intermediate-layer knowledge distillation method consistently outperforms traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches.

## Method Summary
The method involves training a teacher network on a small curated subset of unbiased data, then using this teacher to guide a student network through intermediate-layer knowledge distillation. Classification probes are attached to intermediate layers of both teacher and student networks, and KL divergence between probe outputs is minimized during student training. The approach requires only 20% of the training data to be curated and bias-free for the teacher network, while the student network is trained on the full dataset with combined cross-entropy and distillation losses.

## Key Results
- Intermediate-layer knowledge distillation outperforms traditional ERM, augmentation-based, and group-based bias-mitigation approaches
- Different shortcut types (diffuse noise vs. localized squares) manifest at distinct network depths
- Using only 5-9 randomly selected intermediate layers for distillation provides comparable performance to using all 17 layers
- The method achieves performance comparable to baseline models trained on bias-free data, even on out-of-distribution test data

## Why This Works (Mechanism)

### Mechanism 1
Different shortcut types manifest at distinct network depths—diffuse shortcuts cause early-layer overconfidence while localized shortcuts emerge in later layers. Diffuse features require minimal disambiguation, allowing networks to achieve high predictive confidence immediately. Localized shortcuts require spatial processing, so overconfidence appears only after several convolutional stages.

### Mechanism 2
A teacher network fine-tuned on a small curated subset provides task-specific guidance that redirects student learning away from spurious features. The teacher's intermediate-layer representations encode clinically relevant features without shortcut contamination. Minimizing KL divergence between teacher and student probes at each layer transfers this "clean" feature hierarchy, overriding the simpler decision boundary the student would otherwise learn from biased data.

### Mechanism 3
Intermediate-layer distillation is necessary; final-layer-only distillation fails to mitigate shortcuts effectively. Shortcut reliance is encoded throughout the network's feature hierarchy. Distilling only the final output leaves early-layer shortcut representations uncorrected, which propagate to predictions. Probes attached to batch normalization layers provide supervisory signals at multiple depths.

## Foundational Learning

- **Knowledge Distillation (KD)**
  - Why needed here: The entire method builds on transferring "soft" probability distributions from teacher to student. Understanding that KD traditionally targets only the final layer helps grasp why intermediate-layer extension is novel.
  - Quick check question: Can you explain why soft labels from a teacher might provide more information than hard ground-truth labels?

- **Shortcut Learning / Spurious Correlations**
  - Why needed here: Without understanding that models exploit easy features over robust ones, the motivation for intermediate-layer intervention is unclear. The paper assumes simplicity bias drives shortcut learning.
  - Quick check question: If a chest X-ray model achieves 95% accuracy by detecting chest drains rather than pneumothorax, what happens when deployed on pre-treatment patients?

- **KL Divergence for Distribution Matching**
  - Why needed here: The loss function uses KL divergence between teacher and student probe outputs at each layer. Understanding asymmetry (DKL(p||q) ≠ DKL(q||p)) matters for implementation.
  - Quick check question: Why might you choose KL divergence over MSE for matching probability distributions?

## Architecture Onboarding

- **Component map:**
  Teacher network (frozen) -> Student network (trainable) -> Classification probes (attached to intermediate layers) -> KL divergence loss

- **Critical path:**
  1. Curate small unbiased subset (class-balanced, manually verified shortcut-free)
  2. Train teacher on subset to convergence, freeze
  3. Train classification probes on frozen teacher
  4. Initialize student with ImageNet weights
  5. Each training epoch: (a) update student backbone on biased data with L_total = L_CE + L_KD, (b) freeze student backbone, (c) update student probes on task
  6. For partial-layer distillation: randomly sample n layers each epoch, pair sequentially by relative depth

- **Design tradeoffs:**
  - Teacher size: Smaller teachers (AlexNet) can guide larger students (ResNet-18, DenseNet-121), reducing overfitting risk on small curated data
  - Layer count: 5-9 layers provides comparable performance to full 17-layer distillation with less regularization overhead
  - In-distribution vs. OOD teacher data: OOD teachers (MIMIC for CheXpert experiments) require ~2-3x more training data for equivalent performance

- **Failure signatures:**
  - ΔTPR remains high (>0.3) despite distillation → Teacher training data likely contains residual shortcuts
  - Student AUC drops significantly on clean test set → Over-regularization from too many distillation layers; reduce to 5-9 layers
  - Large gap between biased and clean test performance → Student still relying on shortcuts; verify teacher probes are trained to convergence

- **First 3 experiments:**
  1. **Sanity check**: Train ERM baseline on clean data vs. biased data. Compute per-layer confidence using probes. Verify overconfidence pattern differs by shortcut type (replicate Figure 4)
  2. **Minimal replication**: Use 20% subset for teacher, ResNet-18 backbone, single shortcut type (constant square at 100% prevalence). Compare student with KD vs. ERM on biased test set. Target: ΔTPR reduction from ~1.0 to <0.2
  3. **Ablation**: Compare n=0 (final-layer-only), n=5, n=17 distillation on same setup. Verify final-layer-only fails (Table 4 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
Do the distinctive intermediate-layer confidence trajectories observed in CNNs (where diffuse shortcuts manifest early and localized shortcuts manifest late) also appear in transformer architectures for medical image analysis? The authors restrict their experiments to CNN-based architectures and note that many KD methods designed for CNNs are not directly applicable to transformers due to architectural differences.

### Open Question 2
How effectively does intermediate-layer knowledge distillation mitigate real-world demographic shortcuts (e.g., sex, age, race) compared to the synthetic shortcuts tested? The paper relies on synthetic bias features and morphological deformations; real demographic biases are more subtle, complex, and may interact in unexpected ways.

### Open Question 3
Can principled, layer-specific selection strategies outperform the random layer sampling approach for determining which intermediate layers should receive distillation? The current approach randomly samples layers each epoch, which may not optimally target layers where shortcut learning manifests; Table 4 suggests 5-9 layers can match full 17-layer performance, indicating potential for optimization.

### Open Question 4
Can self-supervised or unsupervised training techniques eliminate the requirement for a curated, unbiased teacher training dataset? The method's main practical limitation is the need for a small but carefully curated bias-free subset for teacher training, which may be difficult to obtain in clinical settings where bias sources are unknown.

## Limitations
- Data heterogeneity: The study focuses on controlled synthetic shortcuts; real-world medical shortcuts may exhibit different layer-wise patterns
- Teacher data sensitivity: The method requires a small subset of unbiased data to train the teacher network
- Computational overhead: Intermediate-layer distillation requires training additional probes each epoch, increasing training time by approximately 30-40%

## Confidence
- **High confidence**: The demonstration that different shortcut types manifest at distinct network depths is well-supported by empirical evidence across multiple datasets and architectures
- **Medium confidence**: The claim that intermediate-layer distillation outperforms final-layer-only distillation is robust but the optimal number of layers (5-9) is based on random sampling rather than principled selection
- **Medium confidence**: The teacher-guided redirection mechanism shows strong performance but assumes the teacher training subset is genuinely shortcut-free

## Next Checks
1. **Multi-source shortcut test**: Apply the method to a dataset with multiple overlapping spurious correlations to verify depth-aware mitigation works when shortcuts interact
2. **Teacher data scaling**: Systematically vary the size of the teacher training subset (1%, 5%, 20%, 50%) on a real medical dataset to establish the minimum unbiased data requirement for effective guidance
3. **Real-world shortcut transfer**: Test whether a teacher trained on one medical dataset can effectively guide a student on a related but distinct dataset when shortcuts differ between sources