---
ver: rpa2
title: 'Unlearning in LLMs: Methods, Evaluation, and Open Challenges'
arxiv_id: '2601.13264'
source_url: https://arxiv.org/abs/2601.13264
tags:
- unlearning
- knowledge
- data
- methods
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews unlearning methods for large language models,
  categorizing them into data-centric, parameter-centric, architecture-centric, hybrid,
  and other approaches. It evaluates the effectiveness of these methods using benchmarks
  like TOFU, focusing on the trade-off between forgetting targeted knowledge and preserving
  overall model utility.
---

# Unlearning in LLMs: Methods, Evaluation, and Open Challenges

## Quick Facts
- **arXiv ID**: 2601.13264
- **Source URL**: https://arxiv.org/abs/2601.13264
- **Reference count**: 7
- **Primary result**: Reviews unlearning methods for LLMs, evaluating them on benchmarks like TOFU while highlighting challenges in providing formal guarantees, achieving efficiency at scale, and extending to cross-language and multimodal settings.

## Executive Summary
This survey reviews unlearning methods for large language models, categorizing them into data-centric, parameter-centric, architecture-centric, hybrid, and other approaches. It evaluates the effectiveness of these methods using benchmarks like TOFU, focusing on the trade-off between forgetting targeted knowledge and preserving overall model utility. The paper identifies key challenges such as providing formal guarantees of forgetting, achieving efficiency at scale, and extending unlearning to cross-language and multimodal settings. It highlights the gap between current empirical methods and the stronger guarantees, efficiency, and robustness needed for trustworthy unlearning in real-world LLM deployments.

## Method Summary
The paper surveys machine unlearning approaches for LLMs, focusing on methods that selectively remove knowledge from trained models without full retraining. It particularly examines the TOFU benchmark, which uses synthetic author facts to test forgetting effectiveness (measured by p-value > 0.1 for indistinguishability from retrained models) and utility preservation (normalized performance ≈ 1.0). The survey categorizes methods into data-centric (gradient ascent/preference optimization), parameter-centric (task vectors/subspace projection), architecture-centric (adapters/contrastive decoding), and hybrid approaches, with special attention to UNLEARN's subspace projection technique that isolates and removes task-specific weight subspaces while preserving intersecting knowledge.

## Key Results
- No current method achieves both strong formal guarantees and practical efficiency for deployed LLMs
- Methods cluster along a Pareto frontier between forget quality and utility preservation
- Cross-lingual and multimodal unlearning remain open challenges with limited empirical evidence
- Current evaluations rely on statistical probes that may not capture adversarial extraction vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1: Gradient Ascent and Preference Optimization
Traditional training minimizes loss on data. Gradient ascent (GA) maximizes loss on the forget set: `L_UL = -Σ log(p(x_t|x_<t))`. Negative Preference Optimization (NPO) reframes this as a preference-alignment problem where the model assigns lower preference to forget-set responses. Core assumption: Knowledge is encoded in weight configurations that can be reversed through opposing gradient signals without destroying the underlying representational structure. Evidence shows GA is "notoriously unstable at scale" while NPO provides more stable optimization by reducing to gradient ascent only in high-temperature limits.

### Mechanism 2: Contrastive Decoding with Auxiliary Models
ULD trains an assistant LLM on the forget set and then derives the unlearned model by subtracting its logits from those of the target model. The optimization: `min_φ L(φ) = min_φ L_f(φ) - βL_r(φ)`. Core assumption: Knowledge representations are sufficiently reflected in logit distributions that algebraic subtraction can suppress specific knowledge while preserving general capabilities. These approaches substantially improve the trade-off between forgetting effectiveness and knowledge retention, though they leave base weights unchanged creating potential recovery vulnerabilities.

### Mechanism 3: Subspace Projection (UNLEARN)
UNLEARN isolates task-specific subspaces within weight space that can be selectively removed while preserving intersecting knowledge. The method sequentially freezes and trains layers to identify task-specific subspaces, then uses SVD with Graham-Schmidt orthogonalization to discriminate forget subspaces from retain subspaces. Core assumption: Task knowledge is approximately linearly separable in weight space; the intersection between forget and retain subspaces can be mathematically estimated and preserved. This approach achieves statistically significant forgetting across all evaluated forget-set sizes while preserving normalized model utility near 1.0.

## Foundational Learning

- **Gradient Descent/Ascent in Neural Networks**: All data-centric methods fundamentally rely on understanding how gradient signals update model weights toward or away from behavioral targets. Quick check: If loss represents "error" on training data, what happens computationally when you maximize loss instead of minimizing it?

- **KL Divergence and Preference Optimization**: NPO, distillation methods, and many stabilization techniques use divergence measures to align model behavior with target distributions. Quick check: Why might minimizing KL divergence to a retain-set teacher be more stable than directly maximizing loss on a forget set?

- **Singular Value Decomposition (SVD) and Low-Rank Structure**: Subspace projection methods (UNLEARN, Ethos) rely on SVD to decompose weight matrices and isolate task-specific directions. Quick check: What does a low-rank approximation of a weight matrix tell you about which directions in parameter space most influence a specific task?

## Architecture Onboarding

- **Component map**:
```
[Training Data] → [Data-Centric: GA/NPO/Distillation] → [Modified Gradients]
       ↓
[Pre-trained Model Weights] → [Parameter-Centric: Task Vectors/Subspace Projection] → [Modified Weights]
       ↓
[Model Architecture] → [Architecture-Centric: Adapters/Contrastive Decoding] → [Modified Inference]
       ↓
[Evaluation: TOFU/BLUR/MUSE] → [Forget Quality p-value] + [Model Utility Score]
```

- **Critical path**:
  1. Define forget set and retain set with explicit boundaries (overlapping domains require BLUR-style evaluation)
  2. Choose intervention locus: data-level (retraining), parameter-level (weight manipulation), or architecture-level (inference-time)
  3. Run unlearning with stability monitoring (watch for loss spikes, capability collapse)
  4. Evaluate on both forget quality (p-value > 0.1 = indistinguishable from retrained) AND model utility (normalized to base model ≈ 1.0)

- **Design tradeoffs**:
  - **Completeness vs. Efficiency**: SISA-style sharding provides stronger guarantees but is impractical for deployed LLMs; adapter methods are efficient but leave residual traces
  - **Forget Quality vs. Utility**: Methods cluster along a Pareto frontier—NPO achieves forgetting but sacrifices utility; FLAT preserves utility but falls below significance threshold
  - **Reversibility vs. Guarantees**: Contrastive decoding is reversible (base model intact) but provides no formal guarantee against adversarial extraction

- **Failure signatures**:
  - Catastrophic collapse: Loss diverges, model generates incoherent text (common with aggressive GA)
  - Over-forgetting: Performance degrades on retain set or adjacent domains (visible in BLUR overlapping evaluation)
  - Adversarial recovery: "Forgotten" knowledge extractable via jailbreaking, multimodal attacks, or fine-tuning
  - Cross-lingual leakage: Forgetting in English doesn't transfer to other languages

- **First 3 experiments**:
  1. **Baseline gradient ascent on TOFU**: Implement GA on synthetic author data (1% forget set), measure where model utility drops below 0.8 and forget quality p-value. This establishes the instability baseline the paper describes.
  2. **NPO vs. GA comparison at scale**: Run both methods at 5% and 10% forget-set sizes. Hypothesis: NPO will maintain stability longer but utility will still degrade at larger forget sets (confirming Figure 10 pattern).
  3. **Subspace overlap stress test**: Apply UNLEARN-style projection to two similar tasks (e.g., medical knowledge for benign vs. biosecurity applications). Test whether subspace discrimination successfully preserves retain knowledge or if the intersection causes degradation on both.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we provide formal, verifiable guarantees that targeted knowledge has been removed from billion-parameter LLMs without requiring prohibitive computational overhead?
- **Basis**: Section 5 states that "Formal guarantees remain elusive in large networks" and notes that differential privacy bounds degrade for over-parameterized models.
- **Why unresolved**: Current evaluations rely on empirical probes which cannot ensure data is unrecoverable, while mathematically rigorous methods like differential privacy fail to scale efficiently to LLM sizes.
- **What evidence would resolve it**: A theoretical framework extending certifiable forgetting or differential privacy to LLM scales that provides statistical bounds on information recovery without significantly degrading model utility.

### Open Question 2
- **Question**: How can unlearning methods ensure that knowledge removal generalizes effectively across languages and modalities?
- **Basis**: Section 5 identifies "Cross-Language and Cross-Modal Unlearning" as a key challenge, noting that unlearning in one language "generally does not lead to unlearning on other languages."
- **Why unresolved**: Multilingual and multimodal benchmarks are limited; current methods struggle with the complex representation sharing across languages, leaving models vulnerable to cross-lingual extraction.
- **What evidence would resolve it**: Methods evaluated on new multimodal/multilingual benchmarks (e.g., UnLOK-VQA, multilingual resources) demonstrating that forgetting specific knowledge in one language or modality effectively removes it across all represented languages and modalities.

### Open Question 3
- **Question**: How can we develop unlearning techniques that are robust against adversarial relearning and extraction attacks?
- **Basis**: Section 5 highlights "Adversarial Relearning," citing attacks like "Stealthy Unlearning Attack" that exploit residual representations to recover "forgotten" knowledge.
- **Why unresolved**: Many current methods suppress behavior rather than erasing representational traces, leaving models vulnerable to adversarial prompting or fine-tuning designed to retrieve the hidden information.
- **What evidence would resolve it**: Unlearning methods that pass rigorous adversarial evaluation suites, demonstrating that forgotten knowledge cannot be statistically distinguished or extracted even under optimized attack conditions.

## Limitations
- No current method achieves both strong formal guarantees and practical efficiency for deployed LLMs
- Cross-lingual and multimodal unlearning remain under-developed with limited empirical evidence
- Current evaluation metrics (p-values > 0.1, utility ≈ 1.0) may not capture adversarial extraction vulnerabilities
- Subspace projection methods face scalability challenges when applied to real-world model sizes

## Confidence

- **High Confidence**: The characterization of current methods as falling along a Pareto frontier between forget quality and utility preservation is well-supported by empirical results across multiple papers (Figure 10 pattern is consistently observed).
- **Medium Confidence**: The mechanism descriptions for gradient ascent versus preference optimization are theoretically sound, though real-world stability at scale remains questionable given the "notoriously unstable" characterization of pure GA methods.
- **Low Confidence**: Claims about cross-lingual unlearning capabilities are largely speculative, with the paper noting this as an "open challenge" without substantial empirical evidence or proposed solutions.

## Next Checks

1. **Adversarial Recovery Test**: Implement a jailbreaking protocol to attempt extraction of "forgotten" knowledge from contrastive decoding methods, validating whether base weights remaining intact creates vulnerabilities the paper acknowledges but doesn't quantify.

2. **Cross-Lingual Transfer Validation**: Apply UNLEARN-style subspace projection to bilingual training data (e.g., English medical knowledge and Spanish medical knowledge) to test whether forgetting transfers across languages as the paper suggests might be problematic.

3. **Efficiency Benchmarking**: Measure wall-clock time and memory overhead for subspace identification steps on progressively larger model sizes (7B → 70B parameters) to quantify the "impractical" scaling concerns raised for sharding approaches.