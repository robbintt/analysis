---
ver: rpa2
title: 'CORE: Contrastive Masked Feature Reconstruction on Graphs'
arxiv_id: '2512.13235'
source_url: https://arxiv.org/abs/2512.13235
tags:
- graph
- masked
- learning
- nodes
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges generative and contrastive approaches in self-supervised
  graph learning by showing their theoretical convergence under specific conditions.
  The authors propose Contrastive Masked Feature Reconstruction (CORE), which integrates
  contrastive learning into masked feature reconstruction by using masked nodes as
  both positive and negative samples.
---

# CORE: Contrastive Masked Feature Reconstruction on Graphs

## Quick Facts
- arXiv ID: 2512.13235
- Source URL: https://arxiv.org/abs/2512.13235
- Reference count: 40
- Outperforms GraphMAE and GraphMAE2 by up to 3.76% on graph classification and up to 3.72% on node classification

## Executive Summary
This paper bridges generative and contrastive approaches in self-supervised graph learning by demonstrating their theoretical convergence under specific conditions. The authors propose Contrastive Masked Feature Reconstruction (CORE), which integrates contrastive learning into masked feature reconstruction by using masked nodes as both positive and negative samples. This eliminates the need for complex graph augmentations while improving model discrimination. CORE achieves significant statistical improvements across multiple benchmarks while maintaining computational efficiency, particularly on large-scale graphs.

## Method Summary
CORE integrates contrastive learning into masked feature reconstruction by treating masked nodes as both positive and negative samples in the contrastive objective. This approach theoretically unifies generative and contrastive methods, showing their convergence under specific conditions (Proposition 2). The method eliminates complex graph augmentations by leveraging the masking strategy itself to generate contrastive pairs. CORE operates by first masking node features, then reconstructing them while simultaneously applying contrastive loss between masked nodes, where masked nodes serve as positive pairs and unmasked nodes as negatives.

## Key Results
- Achieves up to 3.76% improvement over GraphMAE on graph classification tasks
- Achieves up to 3.72% improvement over GraphMAE2 on node classification tasks
- Demonstrates superior performance across multiple benchmarks while maintaining computational efficiency

## Why This Works (Mechanism)
CORE works by unifying generative and contrastive learning objectives through the strategic use of masked nodes. The method treats masked nodes as positive samples and unmasked nodes as negative samples, creating a natural contrastive signal during feature reconstruction. This eliminates the need for complex augmentation strategies while maintaining the discriminative power of contrastive learning. The theoretical convergence between generative and contrastive approaches under specific conditions provides a principled foundation for this unified objective.

## Foundational Learning
- Masked feature reconstruction: Required for understanding the generative component; quick check: can the model reconstruct original features from masked inputs
- Contrastive learning on graphs: Needed to grasp the discriminative objective; quick check: can the model distinguish between similar and dissimilar node representations
- Graph neural networks: Fundamental for understanding the architecture; quick check: does the model properly aggregate neighborhood information
- Self-supervised learning principles: Essential for understanding the overall framework; quick check: can the model learn meaningful representations without labels
- Theoretical convergence analysis: Important for understanding Proposition 2; quick check: do the conditions hold across different graph types

## Architecture Onboarding

Component Map:
Input Graph -> Masking Layer -> GNN Encoder -> Reconstruction Head + Contrastive Head -> Combined Loss

Critical Path:
Graph input flows through masking, then through GNN encoder to produce node embeddings. These embeddings are simultaneously fed to reconstruction and contrastive heads, with their losses combined for training.

Design Tradeoffs:
- Masking ratio vs. reconstruction quality: Higher masking improves contrastive signal but may hurt reconstruction
- GNN depth vs. computational efficiency: Deeper networks capture more context but increase training time
- Contrastive temperature vs. discrimination: Temperature affects how sharply the model distinguishes positive from negative pairs

Failure Signatures:
- Poor reconstruction accuracy indicates masking ratio is too high or encoder capacity is insufficient
- Contrastive loss plateauing suggests temperature parameter needs adjustment or masking patterns lack diversity
- Training instability often results from improper weight balancing between reconstruction and contrastive objectives

First Experiments:
1. Test reconstruction accuracy with varying masking ratios (10%, 30%, 50%, 70%)
2. Evaluate contrastive loss stability with different temperature parameters (0.1, 0.5, 1.0, 2.0)
3. Compare training convergence with and without contrastive component

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence claims rely on specific conditions that may not hold universally across all graph datasets and architectures
- Method still depends on masking strategies that could introduce biases, particularly on graphs with heterogeneous structures
- Performance gains need validation across broader baselines including recent augmentation-heavy contrastive methods

## Confidence
- Theoretical foundation: Medium - relies on specific conditions in Proposition 2
- Performance claims: Medium - primarily compared against GraphMAE and GraphMAE2 variants
- Computational efficiency: Medium - lacks detailed complexity analysis and memory usage comparisons

## Next Checks
1. Test CORE's theoretical convergence claims across diverse graph types (heterogeneous, dynamic, weighted) and architectures beyond standard GNNs to verify the universality of Proposition 2 conditions
2. Conduct ablation studies systematically varying masking ratios and comparing against a broader set of baselines including recent augmentation-heavy contrastive methods to isolate CORE's specific advantages
3. Perform computational complexity analysis measuring memory usage, training time, and scalability on graphs exceeding 1 million nodes to validate efficiency claims under stress conditions