---
ver: rpa2
title: 'Grokking Explained: A Statistical Phenomenon'
arxiv_id: '2502.01774'
source_url: https://arxiv.org/abs/2502.01774
tags:
- grokking
- training
- data
- subclasses
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper formalizes and investigates grokking as a statistical\
  \ phenomenon, focusing on distribution shifts between training and test data as\
  \ a key factor. The authors introduce two synthetic datasets\u2014equidistant and\
  \ equivariant subclasses\u2014designed to systematically analyze grokking under\
  \ controlled conditions."
---

# Grokking Explained: A Statistical Phenomenon

## Quick Facts
- arXiv ID: 2502.01774
- Source URL: https://arxiv.org/abs/2502.01774
- Authors: Breno W. Carvalho; Artur S. d'Avila Garcez; Luís C. Lamb; Emílio Vital Brazil
- Reference count: 13
- Primary result: Distribution shifts between training and test data are a key factor in grokking, not data sparsity or regularization alone

## Executive Summary
This paper investigates grokking as a statistical phenomenon by formalizing the role of distribution shifts between training and test data. The authors introduce two synthetic datasets—equidistant and equivariant subclasses—to systematically analyze grokking under controlled conditions. Experiments demonstrate that while data sparsity is associated with grokking, it is not the cause; instead, distribution shifts induced by imbalanced subclass sampling enable the phenomenon. The equivariant dataset shows that grokking can occur even with dense data when classes are structured relationally. Results are validated across MLP and transformer architectures, with real-world MNIST experiments confirming findings beyond synthetic data. The study advances understanding of grokking and suggests new stopping criteria leveraging class hierarchy and relational knowledge.

## Method Summary
The authors introduce two synthetic datasets designed to systematically study grokking. The equidistant dataset generates points from multivariate normal distributions with equal variance and distance between class and subclass centroids. The equivariant dataset extends this by ensuring subclasses within a class are geometrically closer to each other than to subclasses of other classes. Both datasets use 4 classes with 4 subclasses each, sampling from subclasses with controlled imbalance (fraction f). Experiments train MLPs and transformers on these datasets, monitoring training and test accuracy over extended training periods to observe the grokking transition. The methodology emphasizes controlled distribution shifts and validates findings across architectures and real-world MNIST data.

## Key Results
- Distribution shifts between training and test sets are necessary for grokking to occur
- Data sparsity is associated with but not the cause of grokking
- Equivariant subclass structure enables generalization to unseen subclasses through relational transfer
- Weight decay facilitates but does not cause grokking
- Findings validated across synthetic datasets, MLPs, transformers, and MNIST experiments

## Why This Works (Mechanism)

### Mechanism 1: Distribution Shift Induces Delayed Generalization
- Claim: Grokking emerges primarily from distribution shifts between training and test sets, not from data sparsity or regularization alone.
- Mechanism: When subclasses are underrepresented in training but present in testing, the model first memorizes dominant patterns (training loss converges), then slowly extracts weaker signals from underrepresented subclasses that align with test distribution structure.
- Core assumption: The model can eventually leverage faint training signals if they share relational structure with well-sampled classes.
- Evidence anchors:
  - [abstract] "highlighting that a key factor in its emergence is a distribution shift between training and test data"
  - [section 5.2] "adding even a very small number of examples in that subclass (as defined by the fraction e.g. f = 0.01 of the missing subclasses) not only allowed the model to generalize well, but allowed the model to generalize late"
  - [corpus] Related work on grokking mechanisms (e.g., "A Theoretical Framework for Grokking") supports two-phase dynamics but does not directly test distribution shift as the primary cause—corpus evidence is weak for this specific claim.
- Break condition: If training and test distributions are identical (balanced sampling across all subclasses), grokking intensity diminishes substantially even with small datasets.

### Mechanism 2: Relational Class Structure Enables Transfer from Seen to Unseen Subclasses
- Claim: When subclasses share relational structure (equivariant maps), models can generalize to entirely unseen subclasses.
- Mechanism: Subclasses within a class are positioned closer in representation space than subclasses from different classes. This proximity allows the model to infer decision boundaries for missing subclasses from nearby observed subclasses.
- Core assumption: Geometric relationships in feature space encode semantic relationships that persist across the class-subclass hierarchy.
- Evidence anchors:
  - [section 5.3] "we were able to detect grokking even in the cases where samples were removed completely from specific subclasses...attributed to the fact that in the equivariant case, subclasses of a given class are closer to each other than to subclasses of other classes"
  - [section 4] "if C(Ci,j) is the centroid of subclass j of class i, then: |C(Ci,j) − C(Ci,l)| > |C(Ci,j) − C(Ck,l)|, i ≠ k"
  - [corpus] "The Geometry of Grokking" supports geometric structure in grokking but does not specifically validate equivariant subclass transfer—limited direct corpus support.
- Break condition: Equidistant subclasses (no relational structure) fail to generalize when any subclass is completely removed.

### Mechanism 3: Regularization Amplifies but Does Not Cause Grokking
- Claim: Weight decay and sparse representations facilitate grokking by slowly shifting from memorization to structured representations, but are not the root cause.
- Mechanism: After training loss convergence, weight decay gradually sparsifies weights, allowing weak subclass signals to reorganize decision boundaries toward simpler, generalizable solutions.
- Core assumption: Optimization continues in a "rich" regime after loss convergence, driven by regularization rather than loss gradients.
- Evidence anchors:
  - [abstract] "it can also occur with dense data and minimal hyper-parameter tuning"
  - [section 5.2] "their impact on the test set performance was seen only after training converged and weight decay led the network to a more sparse internal representation"
  - [corpus] "Grokking as the transition from lazy to rich training dynamics" (Kumar et al., 2024) supports the lazy-to-rich transition but disputes weight decay as the cause—partial support with nuance.
- Break condition: Extremely high regularization may prevent convergence altogether; zero regularization may delay or eliminate grokking in some cases.

## Foundational Learning

- Concept: **Distribution Shift**
  - Why needed here: The central claim of the paper hinges on understanding how training-test distribution mismatch creates conditions for late generalization.
  - Quick check question: Can you explain why a model trained on 99% class A samples and 1% class B samples might generalize poorly to a 50/50 test split, even with perfect training accuracy?

- Concept: **Class-Subclass Hierarchies**
  - Why needed here: The synthetic datasets use explicit hierarchies to control distribution shifts and test relational transfer.
  - Quick check question: If class "Animal" has subclasses "Dog" and "Cat," what happens to generalization if "Cat" is removed from training but present in testing?

- Concept: **Memorization vs. Generalization Dynamics**
  - Why needed here: Grokking is defined by the temporal separation between training convergence (memorization) and test improvement (generalization).
  - Quick check question: Why might test accuracy remain flat at 50% for 10,000 epochs and then jump to 95% within 100 epochs?

## Architecture Onboarding

- Component map: Input layer -> Hidden layers (MLP or 2-layer Transformer) -> Output layer (Softmax)
- Critical path:
  1. Generate synthetic data with controlled subclass sampling (fraction f controls imbalance)
  2. Train until training loss converges (monitor training accuracy → near 100%)
  3. Continue training past convergence while monitoring test accuracy
  4. Observe test accuracy inflection points α₀ (escape from low-performance region) and α₁ (entry to high-performance region)

- Design tradeoffs:
  - **Small f (e.g., 0.01)**: Strong grokking signal but longer delay before generalization; risk of never generalizing if signal is too weak
  - **Large f (e.g., 0.5)**: Faster generalization but weaker grokking effect; may not be distinguishable from normal learning
  - **Equivariant vs. equidistant subclasses**: Equivariant enables transfer to unseen subclasses; equidistant requires at least some samples from each subclass

- Failure signatures:
  - Test accuracy remains at chance level indefinitely → distribution shift too extreme or no relational structure
  - Test accuracy improves immediately with training → no grokking; distribution shift insufficient
  - Training loss never converges → learning rate too high or model capacity insufficient

- First 3 experiments:
  1. **Baseline equidistant dataset**: Train with f=0.2 (20% of samples from underrepresented subclass), plot training and test accuracy over 50,000 epochs. Verify training converges before test improves.
  2. **Ablation on subclass removal**: Compare f=0 (complete removal) vs. f=0.01 (1% retained) on equidistant dataset. Confirm complete removal prevents grokking while minimal retention enables it.
  3. **Equivariant transfer test**: Remove all samples from one subclass in equivariant dataset. Measure whether grokking still occurs, confirming relational transfer from nearby subclasses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do weight distributions change during the transition from memorization ($\alpha_0$) to late generalization ($\alpha_1$)?
- Basis in paper: [explicit] Section 6 states, "we plan to inspect the changes taking place in the weight distribution as the network transitions from $\alpha_0$ memorization to late generalization."
- Why unresolved: The current study focuses on statistical distribution shifts in the data rather than the internal mechanistic changes in network weights during the phase transition.
- What evidence would resolve it: A detailed analysis or visualization of weight matrices showing distinct structural changes occurring specifically during the transition interval defined in the paper.

### Open Question 2
- Question: Can entropy metrics for the internal structure of the latent space quantify the complexity and organization of representations during grokking?
- Basis in paper: [explicit] Section 6 outlines the aim to "devise entropy metrics for the internal structure of the latent space" to quantify complexity.
- Why unresolved: While the paper links grokking to distribution shifts, it does not provide a quantitative measure for the organization of the learned representations that accompany this shift.
- What evidence would resolve it: The formulation of a specific entropy-based metric that correlates strongly with the onset of the generalization phase in the proposed synthetic datasets.

### Open Question 3
- Question: How can early stopping criteria be effectively modified to incorporate the likelihood of late generalization?
- Basis in paper: [explicit] Section 1 notes that "late generalization challenges traditional stopping criteria" and Section 6 argues "new stopping criteria should be explored... incorporating measures of the likelihood of late generalization."
- Why unresolved: Standard criteria monitor test loss, which may appear stagnant for a long period (the "grokking" interval) before dropping, leading to premature stopping.
- What evidence would resolve it: A training algorithm or heuristic that utilizes relational class structures or distribution shift measurements to predict generalization potential before test loss decreases.

## Limitations
- Synthetic datasets may not fully capture real-world distribution shift complexity
- Experiments focus on small-scale problems; scalability to large vision/language tasks untested
- Limited exploration of alternative regularization techniques beyond weight decay
- Architecture-specific inductive biases in grokking dynamics not investigated

## Confidence
- **High confidence**: Distribution shift as a necessary condition for grokking (supported by multiple controlled experiments)
- **Medium confidence**: Distribution shift as the primary mechanism (correlation established but causal chain not fully proven)
- **Medium confidence**: Weight decay as a facilitator rather than cause (supported by minimal hyperparameter experiments but lacks ablation across regularization types)
- **Low confidence**: Universal applicability to real-world datasets beyond MNIST (limited empirical validation)

## Next Checks
1. **Scale Test**: Replicate experiments on CIFAR-10 with controlled class imbalance to verify grokking persists at realistic scale and resolution
2. **Architecture Ablation**: Compare grokking dynamics across diverse architectures (CNNs, transformers with different depths) to test universality of distribution shift mechanism
3. **Regularization Sweep**: Systematically vary regularization types (dropout, batch normalization, data augmentation) and strengths to map the full hyperparameter landscape of grokking emergence