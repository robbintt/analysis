---
ver: rpa2
title: 'BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language
  Models within Bengali Language and Culture'
arxiv_id: '2511.03180'
source_url: https://arxiv.org/abs/2511.03180
tags:
- moral
- ethical
- reasoning
- virtue
- ethics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'BengaliMoralBench is the first large-scale, culturally grounded
  ethics benchmark for Bengali language and South Asian contexts. It comprises 3,000
  human-annotated scenarios spanning five life domains and three ethical frameworks:
  Commonsense, Justice, and Virtue ethics.'
---

# BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture

## Quick Facts
- arXiv ID: 2511.03180
- Source URL: https://arxiv.org/abs/2511.03180
- Reference count: 40
- First large-scale, culturally grounded ethics benchmark for Bengali language and South Asian contexts

## Executive Summary
BengaliMoralBench is the first large-scale, culturally grounded ethics benchmark for Bengali language and South Asian contexts. It comprises 3,000 human-annotated scenarios spanning five life domains and three ethical frameworks: Commonsense, Justice, and Virtue ethics. We evaluated leading multilingual LLMs (Gemma, Llama, Qwen, DeepSeek) using a zero-shot protocol with tailored prompts. Performance varied widely, from 50% to 91% accuracy, with notable weaknesses in culturally nuanced reasoning and moral fairness. Qualitative analysis revealed consistent failures in contextual grounding, commonsense interpretation, and bias mitigation. BengaliMoralBench provides a reproducible, inclusive framework for evaluating and improving the ethical alignment of AI systems in non-Western, low-resource multilingual settings.

## Method Summary
BengaliMoralBench consists of 3,000 Bengali-language moral scenarios annotated across five domains and three ethical frameworks. Models were evaluated using a zero-shot protocol with binary classification prompts in both Bengali and English. Performance was measured using accuracy, F1 score, Matthews Correlation Coefficient, and Cohen's Kappa. The benchmark is available at HuggingFace (ciol-research/BengaliMoralBench) with code at GitHub (shahriyar-zaman/BengaliMoralBench).

## Key Results
- Model performance ranged from 50% to 91% accuracy across ethical frameworks
- Justice ethics showed the lowest performance, with MCC scores indicating unreliable reasoning
- Qualitative analysis identified consistent failures in cultural contextualization, commonsense interpretation, and bias mitigation

## Why This Works (Mechanism)

### Mechanism 1: Culturally Grounded Benchmark Construction
- **Claim:** Constructing ethical scenarios from within a specific cultural context, rather than translating them, creates a more valid measure of moral reasoning alignment.
- **Mechanism:** The benchmark relies on native annotators generating scenarios in Bengali, grounded in daily life (e.g., "rickshaw fare disputes," "zakat vs voluntary charity"). These scenarios are then categorized into three ethical frameworks (Virtue, Commonsense, Justice). This process bypasses translation error and captures culturally specific moral norms.
- **Core assumption:** A coherent set of moral norms exists within Bengali/South Asian culture that differs meaningfully from Western-centric norms.
- **Evidence anchors:**
  - [abstract/section 1] The benchmark "covers five moral domains... subdivided into 50 culturally relevant subtopics. Each scenario is annotated via native-speaker consensus...".
  - [section 3.2.3] Annotators were instructed that "Statements had to describe plausible, culturally grounded Bangladeshi scenarios, remain lexically neutral...".
  - [corpus] Related work like "Structured Moral Reasoning in Language Models" supports the value of structured moral evaluation but does not contradict the need for cultural grounding.
- **Break condition:** If the "native annotator consensus" process is flawed (e.g., annotators are too homogeneous, representing only a narrow slice of Bangladeshi opinion), the benchmark's validity would be compromised.

### Mechanism 2: Zero-Shot Evaluation with Framework-Specific Prompts
- **Claim:** Using zero-shot prompts tailored to specific ethical frameworks reveals differential model performance and alignment more effectively than a single general prompt.
- **Mechanism:** Models are presented with a scenario and a prompt like "You are a virtue ethics expert... Respond only with '1' (ethical) or '0' (unethical)." The model's binary choice is compared against the human-annotated ground truth. The lack of in-context examples (zero-shot) forces the model to rely on its internal representations of moral concepts.
- **Core assumption:** Models possess (or lack) internal representations of moral concepts that can be probed without fine-tuning.
- **Evidence anchors:**
  - [abstract] The authors "evaluated leading multilingual LLMs... using a zero-shot protocol with tailored prompts."
  - [Table 1 & Section 5.1] Results show significant variance in performance (50-91% accuracy) and lower MCC/Kappa scores on Justice, indicating the method successfully probes for nuanced reasoning.
  - [corpus] The paper "The Moral Consistency Pipeline" aligns with the idea of continuous ethical evaluation using structured prompts.
- **Break condition:** If the prompt phrasing in Bengali or English introduces unforeseen biases or is interpreted differently by the models than intended, the resulting scores would not accurately reflect moral reasoning capability.

### Mechanism 3: Multi-Metric and Error Taxonomy Analysis
- **Claim:** A combination of aggregate metrics (Accuracy, F1) and agreement metrics (MCC, Kappa), plus qualitative error analysis, provides a holistic diagnostic of model failure modes.
- **Mechanism:** High accuracy can mask poor reasoning (e.g., by guessing the majority class). MCC and Kappa account for chance agreement and class imbalance, revealing the coherence of the model's decisions. Qualitative error analysis (e.g., misclassifying "selfless efforts in family obligations") pinpoints specific cultural knowledge gaps.
- **Core assumption:** High scores on these metrics, particularly MCC/Kappa, correlate with robust and reliable moral reasoning.
- **Evidence anchors:**
  - [section 4, Prompting Strategy] The authors state "Model performance was measured using standard classification metrics: accuracy (%), F1 score... MCC, and Cohen's Kappa."
  - [Section 5.3.1 & 5.3.2] The analysis identifies "root causes" like "gaps in cultural and religious contextual knowledge" and "propagation of social biases."
  - [corpus] Weak/missing corpus evidence for this specific combination of metrics for moral evaluation; most related work focuses on accuracy or human preference.
- **Break condition:** If the qualitative error analysis is subjective or not systematic, the identified "root causes" may not generalize to all model failures.

## Foundational Learning

- **Concept: Ethical Frameworks (Virtue, Commonsense, Justice)**
  - **Why needed here:** The entire benchmark is built on this triadic framework. Understanding that Virtue focuses on character (e.g., honesty), Commonsense on social norms, and Justice on fairness/rights is essential to interpret results.
  - **Quick check question:** Would a scenario about "greeting elders" be primarily a Commonsense or Virtue ethics issue in this benchmark? (Answer: It can be both, but the paper links social etiquette to Commonsense/Virtue).

- **Concept: Cultural Grounding vs. Translation**
  - **Why needed here:** A core motivation of the paper. You need to understand why simply translating an English ethics benchmark into Bengali would be insufficient.
  - **Quick check question:** Name one type of error that could arise from translating an English ethical benchmark instead of creating one from scratch. (Answer: Loss of cultural nuance, introduction of translation errors).

- **Concept: Performance Metrics (Accuracy vs. MCC/Kappa)**
  - **Why needed here:** The paper highlights that accuracy can be misleading. MCC and Kappa are used to show that even with reasonable accuracy, reasoning can be incoherent.
  - **Quick check question:** If a model achieves 75% accuracy on a balanced binary task, is it necessarily doing well? What additional metric would give you more confidence? (Answer: Not necessarily. MCC or Cohen's Kappa would give a more robust measure of agreement beyond chance).

## Architecture Onboarding

- **Component map:**
  1.  **Benchmark Data (BengaliMoralBench):** 3,000 human-annotated Bengali sentences across 5 domains and 3 ethical frameworks.
  2.  **Evaluation Pipeline:** Loads data, formats it using framework-specific zero-shot prompts (Bengali/English), feeds it to the LLM.
  3.  **Metrics Module:** Calculates Accuracy, F1, MCC, and Cohen's Kappa from binary predictions vs. ground truth.
  4.  **Analysis Framework:** Qualitative error taxonomy to classify failures (e.g., literalism, cultural gap).

- **Critical path:**
  1.  Access benchmark data from Hugging Face (`ciol-research/BengaliMoralBench`).
  2.  Implement the unified zero-shot prompting protocol for your chosen model.
  3.  Pass all 3,000 examples through the model to get predictions.
  4.  Compute all four metrics per ethical framework.
  5.  Perform error analysis on misclassified examples, particularly those with low MCC.

- **Design tradeoffs:**
  - **Zero-shot vs. Few-shot:** Zero-shot tests "intrinsic" reasoning without providing examples, which is harder but more revealing of pre-training quality.
  - **Binary vs. Nuanced Labels:** Binary labels simplify evaluation but may oversimplify complex dilemmas, a tradeoff the authors acknowledge.

- **Failure signatures:**
  - **High Accuracy, Low MCC:** Model is getting answers right for wrong reasons (e.g., surface cues).
  - **Large Variance with Temperature:** Unstable moral reasoning. The paper notes Llama 3.1 (8B) showed fluctuations up to 1.7 F1 points.
  - **Domain-Specific Collapse:** Failure in "Virtue" and "Justice" while "Commonsense" remains stable signals a lack of deep cultural reasoning.

- **First 3 experiments:**
  1.  **Establish a Baseline:** Run Gemma 2 (9B) on the benchmark using English prompts. Compute all metrics to confirm your pipeline matches reported results (~91% Commonsense accuracy).
  2.  **Test Prompt Language:** Re-evaluate Gemma 2 (9B) using Bengali prompts. Compare results to the English baseline to measure cross-lingual alignment.
  3.  **Probe Cultural Generalization:** Identify subtopics with the most "country-specific" (CS) instances (e.g., "Queueing at Government Offices"). Manually inspect model failures to classify them using the paper's error taxonomy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does model performance on this benchmark generalize to the distinct socio-cultural context of West Bengal compared to Bangladesh?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the dataset focuses primarily on Bangladeshi contexts and that "generalization to other Bengali-speaking regions (e.g., West Bengal) may be limited."
- **Why unresolved:** While the language is shared, moral norms regarding family and religion may diverge across national borders, a nuance the current Bangladesh-centric dataset cannot capture.
- **What evidence would resolve it:** A comparative evaluation using a supplementary dataset of scenarios specific to West Bengal to measure the performance delta against the Bangladeshi subset.

### Open Question 2
- **Question:** Does adopting a continuous or multi-label annotation schema improve the evaluation of morally ambiguous scenarios compared to the current binary classification?
- **Basis in paper:** [explicit] The authors acknowledge in the Limitations section that the "binary label schema oversimplifies moral nuance," even though it was effective for core alignment.
- **Why unresolved:** Binary accuracy metrics cannot distinguish between a model that is confidently wrong and one that is correctly uncertain in complex gray areas.
- **What evidence would resolve it:** Re-annotating ambiguous instances using a Likert scale and analyzing model calibration scores to see if "uncertain" predictions correlate with lower confidence.

### Open Question 3
- **Question:** To what extent can adversarial data augmentation and user-in-the-loop fine-tuning mitigate the specific failure modes identified in "Justice" and "Virtue" reasoning?
- **Basis in paper:** [explicit] Appendix G.11 explicitly proposes "adversarial example generation" and "user-in-the-loop fine-tuning" as future trajectories to address root causes like "surface-level lexical reliance" and "social bias propagation."
- **Why unresolved:** It is currently unknown if these technical interventions can override the Western-centric pretraining biases inherent in models like Llama and Gemma for this specific cultural context.
- **What evidence would resolve it:** Pre- and post-fine-tuning performance comparisons specifically targeting the qualitative error categories identified in Figure 7 (e.g., misclassification of ritualized altruism).

## Limitations

- Cultural Representativeness: The benchmark relies on native annotator consensus to capture "Bengali moral norms," but the paper does not specify the demographic diversity of annotators (regional, religious, socioeconomic backgrounds).
- Generalizability Beyond Bengali: The specific methodology (native annotator generation, culturally grounded scenarios) may not be easily replicable for other low-resource languages lacking critical mass of AI-literate native speakers.
- Translation Quality Impact: The paper reports performance differences between English and Bengali prompts, but does not systematically analyze whether translation quality or prompt phrasing differences drive these variations.

## Confidence

- **High Confidence:** The benchmark construction methodology (native generation, framework-based categorization) is well-specified and reproducible; The zero-shot evaluation protocol with framework-specific prompts is clearly defined; The identified performance gap (particularly in Justice ethics) across models is empirically demonstrated
- **Medium Confidence:** The qualitative error analysis identifying specific failure modes (cultural gaps, literalism) is methodologically sound but relies on subjective interpretation; The claim that this benchmark uniquely captures "culturally grounded" moral reasoning assumes the annotator consensus process successfully captured authentic moral norms
- **Low Confidence:** The generalizability of findings to other non-Western contexts without direct validation; The extent to which binary classification adequately captures the complexity of moral reasoning

## Next Checks

1. **Annotator Diversity Validation:** Conduct a post-hoc analysis of annotator backgrounds (region, education, religious affiliation) to assess whether the benchmark reflects diverse Bengali perspectives or a narrow subset. Compare inter-annotator agreement rates across demographic groups.

2. **Translation Impact Study:** Systematically test whether observed performance differences between Bengali and English prompts stem from translation quality versus genuine cross-lingual alignment gaps. This could involve professional translation validation and back-translation analysis.

3. **Cross-Cultural Generalization Test:** Apply the same benchmark methodology (native generation, framework categorization) to create a parallel benchmark for another South Asian language/culture (e.g., Hindi, Tamil). Compare the resulting moral reasoning patterns to assess whether the methodology captures universal versus culturally specific patterns.