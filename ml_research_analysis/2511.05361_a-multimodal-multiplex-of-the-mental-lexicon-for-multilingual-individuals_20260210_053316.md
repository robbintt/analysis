---
ver: rpa2
title: A multimodal multiplex of the mental lexicon for multilingual individuals
arxiv_id: '2511.05361'
source_url: https://arxiv.org/abs/2511.05361
tags:
- language
- visual
- multimodal
- lexical
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a multimodal multiplex model of the mental
  lexicon to investigate how visual input influences translation performance in multilingual
  individuals. Building on network models of language, it introduces a visual layer
  connecting perceptual inputs to lexical representations across languages.
---

# A multimodal multiplex of the mental lexicon for multilingual individuals

## Quick Facts
- **arXiv ID**: 2511.05361
- **Source URL**: https://arxiv.org/abs/2511.05361
- **Reference count**: 4
- **Primary result**: Visual-text input enhances translation accuracy compared to text-only, with multilinguals outperforming bilinguals

## Executive Summary
This study proposes a multimodal multiplex model of the mental lexicon to investigate how visual input influences translation performance in multilingual individuals. Building on network models of language, it introduces a visual layer connecting perceptual inputs to lexical representations across languages. Participants (12-16 years old) were divided into bilingual and multilingual groups, each exposed to either text-only or visual-text stimuli during translation tasks. Translation accuracy was measured via cosine similarity between participant embeddings and reference translations. Results showed higher proficiency in the visual-text condition, with multilinguals outperforming bilinguals. These findings support the hypothesis that multimodal input enhances lexical retrieval and reduce cross-linguistic interference, demonstrating that the mental lexicon operates as an interconnected, multimodal network.

## Method Summary
The study employed a translation task where Dutch-speaking participants (12-16 years) translated Dutch sentences to English under two conditions: Visual-Textual (VT) with image + text, or Only-Text (OT). The experiment used 15 Dutch sentences representing everyday language use, with corresponding images for VT condition. Translation accuracy was measured via cosine similarity between participant sentence embeddings and reference embeddings created by proficient bilinguals. The method involved fixation cross (200ms) → image display 1s (VT only) → Dutch sentence 2s → red recording cue 4s. Spoken responses were recorded, transcribed using speech-to-text with manual review, and converted to embeddings using a pre-trained multilingual transformer.

## Key Results
- Translation accuracy was higher in the Visual-Textual condition compared to Only-Text
- Multilingual participants outperformed bilingual participants overall
- The findings support the hypothesis that multimodal input enhances lexical retrieval and reduces cross-linguistic interference

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Activation Spreading
Visual input strengthens lexical retrieval by providing additional activation pathways through a connected visual layer. When a visual stimulus activates perceptual representations, activation spreads to associated lexical nodes across language layers, creating more robust and redundant retrieval cues. This follows the multiplex network principle where edges connecting node-layer tuples enable navigation across the entire network. The mental lexicon operates as an interconnected multilayer network where activation propagates across modalities, not just within linguistic layers. Core assumption: Cross-modal edges have meaningful weights and form stable connections. Break condition: If visual representations do not form stable connections to lexical nodes.

### Mechanism 2: Interference Reduction via Perceptual Grounding
Visual cues reduce cross-linguistic interference by anchoring activation to a specific conceptual referent. The visual layer provides disambiguating input that biases competition toward the contextually appropriate lexical item, reducing parallel activation of conflicting candidates while preserving beneficial co-activation. The BIA+ model's language non-selective access operates across modalities; visual input modulates the competition process rather than eliminating parallel activation. Core assumption: Visual input can effectively disambiguate between competing lexical candidates. Break condition: If visual stimuli themselves introduce ambiguity or if interference patterns persist regardless of modality.

### Mechanism 3: Multilingual Network Density Amplification
Multilinguals benefit more from multimodal input because additional language layers create denser connectivity, amplifying spreading activation effects. Each additional language introduces new nodes and edges; the visual layer connects to all of them, so multilinguals have more pathways for activation to reach target lexical items. This aligns with the Largest Viable Cluster concept where highly interconnected subsets are more navigable. Core assumption: Network density correlates positively with retrieval efficiency. Break condition: If additional languages create competition that overwhelms facilitation.

## Foundational Learning

- **Concept: Multilayer Networks (Kivelä et al., 2014)**
  - **Why needed here:** The entire multiplex model is formalized as M = (V_M, E_M, V, L). Understanding node-layer tuples and cross-layer edges is essential to grasp how visual and linguistic representations connect.
  - **Quick check question:** In a multilayer network, what does a cross-layer edge represent, and why can't a single-layer graph capture the same structure?

- **Concept: BIA+ Model and Language Non-Selective Access**
  - **Why needed here:** The hypothesis assumes parallel activation extends to multimodal contexts. Without understanding how bilingual word recognition involves simultaneous co-activation of both languages, the interference-reduction claim is unmotivated.
  - **Quick check question:** What is the difference between the word identification system and the task/decision system in BIA+, and why does this distinction matter for predicting visual effects?

- **Concept: Embedding-Based Semantic Similarity**
  - **Why needed here:** The study measures translation accuracy via cosine similarity between participant embeddings and reference embeddings. Interpreting results requires understanding what this metric captures (and what it misses).
  - **Quick check question:** Why might cosine similarity be preferable to exact word matching for evaluating translation quality, and what types of errors would it fail to detect?

## Architecture Onboarding

- **Component map:** Fixation cross → Image (VT only) → Dutch sentence → Recording cue → Speech transcription → Sentence embeddings → Cosine similarity scoring
- **Critical path:** Stimulus encoding → Parallel activation across language layers → Cross-modal spreading activation (VT only) → Competition resolution and lexical selection → Verbal production → Automated transcription and embedding comparison
- **Design tradeoffs:** 15 sentences provides feasibility but limits statistical power; age range 12-16 balances literacy assurance with developmental homogeneity concerns; sequential image-then-text presentation avoids attention splitting but differs from natural multimodal perception
- **Failure signatures:** No modality main effect (VT ≈ OT across groups); no group × modality interaction (multilinguals gain no extra benefit from visual input); high within-group variance (individual proficiency differences swamp experimental manipulations); negative visual effect (images introduce distraction or cultural misinterpretation)
- **First 3 experiments:** 1) Baseline replication with expanded stimulus set (40-60 sentences) and standardized proficiency pre-test; 2) Timing manipulation (vary image exposure 500ms, 1000ms, 2000ms); 3) Typological variation