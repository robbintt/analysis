---
ver: rpa2
title: 'Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series
  Forecasting'
arxiv_id: '2509.20942'
source_url: https://arxiv.org/abs/2509.20942
tags:
- attention
- time
- series
- patch
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why attention mechanisms in time series
  forecasting transformers fail to provide clear advantages and often underperform
  simple linear baselines. The authors demonstrate that transformer blocks often degenerate
  into multilayer perceptrons (MLPs) and conduct a series of experiments to validate
  this phenomenon across multiple time series transformer models, including foundation
  models.
---

# Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting

## Quick Facts
- arXiv ID: 2509.20942
- Source URL: https://arxiv.org/abs/2509.20942
- Reference count: 40
- Primary result: Attention mechanisms in time series transformers often fail to provide advantages and may degenerate into simple MLPs

## Executive Summary
This paper investigates why attention mechanisms in time series forecasting transformers fail to deliver expected performance gains, often underperforming simple linear baselines. Through systematic experiments including attention replacement, perturbation, and patch length variation, the authors demonstrate that transformer blocks frequently degenerate into multilayer perceptrons. The core finding is that attention contributes minimally to model performance while the FFN module carries the predictive burden. Theoretical analysis reveals that current linear embedding methods fail to create well-structured latent spaces necessary for attention to function effectively, suggesting future improvements should focus on representation learning before refining attention mechanisms.

## Method Summary
The study employs attention replacement experiments where the attention matrix is replaced with zero, identity, average, or fixed trainable matrices, finding that model performance remains largely unaffected. Attention perturbation experiments and patch length variations further demonstrate minimal attention contribution. A toy dataset based on state machines reveals attention's failure to capture inter-patch dependencies. Frozen embedding experiments show no performance degradation, supporting the hypothesis that linear layers cannot properly embed time series data into appropriate latent spaces. The work spans multiple time series transformer models including foundation models like Lag-llama.

## Key Results
- Attention matrix replacement (zero, identity, mean, fixed trainable) has negligible impact on forecasting performance
- Frozen embeddings show no significant performance degradation, indicating embedding inadequacy
- Patch length variation does not affect performance, suggesting attention fails to capture long-range dependencies
- State machine toy dataset reveals attention weights remain uniform across event and non-event patches

## Why This Works (Mechanism)

### Mechanism 1: Attention Module Non-Contribution
- Claim: In time series transformers, the attention mechanism contributes minimally to model performance; the FFN module carries the predictive burden.
- Mechanism: The authors replaced the attention matrix with zero (complete ablation), identity, average, or fixed trainable matrices. Performance remained largely unchanged across all variants, indicating attention weights are not learning meaningful token relationships. The FFN operates token-wise, making the effective architecture an MLP.
- Core assumption: If attention were functioning as intended for inter-token dependency capture, replacing it with context-independent alternatives would degrade performance significantly.
- Evidence anchors:
  - [abstract] "transformer blocks often degenerate into simple MLPs in existing time-series transformers"
  - [section 3.1] "Remarkably, the model's performance remained unaffected even after the replacement of the context-aware attention mechanism"
  - [corpus] "Mechanistic evaluation of Transformers and state space models" notes attention deficiency in recalling context information
- Break condition: If downstream tasks require explicit inter-patch dependency modeling that MLPs cannot approximate, this degeneration would cause failure.

### Mechanism 2: Linear Embedding Structural Inadequacy
- Claim: Linear embedding layers fail to map time series data into well-structured latent spaces where attention can meaningfully operate.
- Mechanism: A full-rank linear layer is an isomorphic linear transformation. However, the latent space encoding semantic/dependency information cannot be isomorphic to the raw time series space. Unlike NLP where one-hot inputs map to learned dense embeddings, continuous time series values lack discrete semantic anchors.
- Core assumption: Meaningful attention requires tokens that encode higher-level properties beyond raw waveform values.
- Evidence anchors:
  - [abstract] "current embedding methods fail to allow transformers to function in a well-structured latent space"
  - [section 5.2] Frozen embedding experiments showed no performance degradation
  - [corpus] Weak direct evidence in corpus; "Understanding Transformers for Time Series: Rank Structure" analyzes rank but not embedding adequacy
- Break condition: If alternative embedding approaches (VQ-VAE, contrastive pretraining) produce semantically structured tokens, attention should become functional.

### Mechanism 3: Low Information Density per Token
- Claim: Time series patches contain insufficient intrinsic information for representation learning compared to image patches in ViT.
- Mechanism: In ViT, patches contain 768 values (3×16×16) projected to 768 dimensions—dimensionality-preserving. In PatchTST, 16 values project to 128 dimensions (8× expansion). The high expansion ratio with sparse semantic content prevents meaningful latent space structuring.
- Core assumption: Representation learning requires sufficient input information density to compress into meaningful features.
- Evidence anchors:
  - [section A, Table 5] Comparison of input sizes: BERT 30,722 → 768; ViT 768 → 768; PatchTST 16 → 128
  - [section 5.3] Adding more transformer blocks to PatchTST/iTransformer did not improve performance and caused overfitting
  - [corpus] No direct corpus support for this specific mechanism
- Break condition: Datasets with richer per-patch structure (multivariate with strong cross-channel dependencies) may avoid this issue.

## Foundational Learning

- Concept: **Self-Attention as Token Relationship Learning**
  - Why needed here: Understanding what attention should do (learn Q-K affinities to weight V contributions) is prerequisite to recognizing why it fails when tokens lack semantic structure.
  - Quick check question: Can you explain why permutation-invariant attention requires positional encoding for sequential data?

- Concept: **Latent Space Geometry**
  - Why needed here: The paper's core argument is that time series data occupies a different manifold than the latent space attention expects. Linear projections cannot bridge this gap.
  - Quick check question: Why might a linear layer fail to map time series waveforms to a space where "similarity" corresponds to "semantic relatedness"?

- Concept: **Tokenization Paradigms in Transformers**
  - Why needed here: The paper surveys timestamp-as-token, patch-as-token, and channel-as-token approaches. Understanding these is essential for interpreting the degeneration phenomenon.
  - Quick check question: What are the tradeoffs between treating timestamps vs. patches vs. channels as tokens?

## Architecture Onboarding

- Component map:
  Input → Patching → Linear Embedding (inadequate) → Positional Encoding (underutilized) → [Attention (degenerate) + FFN (functional)] × N blocks → Flatten → Linear Head

- Critical path:
  1. Embedding layer initialization (frozen vs. trained has minimal impact)
  2. FFN modules (primary locus of learning)
  3. Final aggregation layer (where token information combines)

- Design tradeoffs:
  - More blocks → overfitting risk without attention benefit
  - Larger patch length → fewer tokens but no performance loss
  - Different embedding strategies (linear, conv, MLP, residual) → all show attention degeneration

- Failure signatures:
  - Attention weights uniform across all token pairs regardless of content
  - Event patches in structured data receive no elevated attention
  - Zero attention variants match or exceed original performance
  - Positional encoding removal has no effect

- First 3 experiments:
  1. **Attention replacement test**: Replace attention matrix with mean/identity/zero on your dataset. If performance holds, attention is not contributing.
  2. **Frozen embedding test**: Fix embedding layer weights at initialization. Compare performance—no degradation confirms embedding inadequacy.
  3. **Perturbation sensitivity test**: Apply controlled noise to attention output vs. FFN output. Asymmetric sensitivity (FFN >> attention) confirms the degeneration pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the degeneration of transformers into MLPs extend to other time series domains beyond forecasting, such as classification and imputation?
- Basis in paper: [explicit] The authors explicitly state in the Future Work section that the degradation phenomenon "may also extend to other domains, such as classification and imputation, necessitating further investigation."
- Why unresolved: The study strictly focuses on forecasting tasks; therefore, the behavior of attention mechanisms in classification or imputation contexts remains unverified.
- What evidence would resolve it: Applying the paper's attention replacement and perturbation experiments to time series classification and imputation benchmarks to observe if performance remains similarly unaffected by attention removal.

### Open Question 2
- Question: How can researchers develop novel metrics or benchmarks to quantify the actual influence and effectiveness of attention mechanisms in time series models?
- Basis in paper: [explicit] The authors explicitly "advocate for the development of a novel metric or benchmark to assess the extent of attention's influence," noting that time series data is less interpretable than images or language.
- Why unresolved: Standard performance metrics like MSE do not reveal the internal failure of attention to capture dependencies, and interpreting raw attention weights is currently too intricate.
- What evidence would resolve it: The proposal of a standardized metric that correlates strongly with the functional utility of the attention map (e.g., measuring dependency capture) rather than just prediction error.

### Open Question 3
- Question: Do individual time series patches contain sufficient inherent information to justify or populate the high-dimensional semantic spaces required by transformers?
- Basis in paper: [explicit] In Appendix A, regarding the small input size of time series patches, the authors ask: "Whether such data inherently contains—or even requires—so many semantic dimensions remains an open question."
- Why unresolved: The paper demonstrates that current linear embeddings fail to create meaningful latent spaces, but it does not determine the fundamental information capacity of a time series patch.
- What evidence would resolve it: A theoretical or empirical analysis determining the optimal latent dimensionality relative to patch size, or showing that reducing the hidden size resolves the degeneration issue.

### Open Question 4
- Question: Can encoder-based architectures (e.g., VQ-VAE) effectively generate the structured discrete representations necessary to make attention mechanisms functional for time series?
- Basis in paper: [inferred] The authors suggest in Section D that VQ-VAE is a "promising approach" to align time series with the discrete token inputs transformers require, but admit a serious evaluation was beyond the scope of the paper.
- Why unresolved: While the paper identifies the lack of structure in linear embeddings as the cause of failure, it does not experimentally validate specific alternatives like VQ-VAE.
- What evidence would resolve it: Experiments demonstrating that a transformer operating on VQ-VAE tokens passes the paper's "attention replacement" tests (i.e., performance degrades when attention is removed).

## Limitations

- Empirical scope primarily focuses on univariate and low-dimensional multivariate time series, potentially limiting generalizability to high-dimensional multivariate streams
- Mechanism attribution provides less definitive evidence about which specific failure mode (embedding inadequacy vs. low information density vs. task simplicity) dominates in different contexts
- Alternative explanations such as insufficient training data, optimization difficulties, or positional encoding schemes remain unexplored

## Confidence

**High confidence** in the empirical observation that attention mechanisms contribute minimally to performance in tested time series transformers, and that replacing attention with context-independent alternatives yields similar results.

**Medium confidence** in the theoretical claim that linear embeddings cannot adequately structure latent spaces for time series data, as this relies more on conceptual arguments than comprehensive empirical validation.

**Medium confidence** in the information density hypothesis explaining attention degeneration, as the analysis is primarily qualitative and lacks direct measurement of token information content across different time series datasets.

## Next Checks

1. **Cross-dataset information density measurement**: Quantify the information content per patch across diverse time series datasets using established metrics (e.g., mutual information between patches and future values). Correlate these measurements with the degree of attention degeneration observed.

2. **Alternative embedding ablation study**: Replace linear embeddings with nonlinear alternatives (MLP, convolutional, VQ-VAE) while keeping all other components identical. Systematically compare attention functionality across embedding types to isolate embedding quality as the primary factor.

3. **Attention behavior on high-dimensional multivariate data**: Test the same attention replacement experiments on datasets with high cross-channel correlation (e.g., multivariate sensor streams with known interdependencies). If attention becomes functional in these cases, it would validate that the degeneration is dataset-structure dependent rather than an inherent architectural limitation.