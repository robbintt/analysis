---
ver: rpa2
title: 'TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing
  for AMS Circuits'
arxiv_id: '2509.14169'
source_url: https://arxiv.org/abs/2509.14169
tags:
- circuit
- design
- optimization
- understanding
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopoSizing is an end-to-end LLM-aided framework that performs topology-based
  understanding and sizing for AMS circuits. It addresses the challenge of efficient
  analog circuit design by combining graph-based hierarchical circuit representation
  with LLM agents for iterative circuit understanding, and integrates this with Bayesian
  optimization enhanced by LLM-guided sampling and trust-region updates.
---

# TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits

## Quick Facts
- arXiv ID: 2509.14169
- Source URL: https://arxiv.org/abs/2509.14169
- Authors: Ziming Wei; Zichen Kong; Yuan Wang; David Z. Pan; Xiyuan Tang
- Reference count: 31
- TopoSizing is an end-to-end LLM-aided framework that performs topology-based understanding and sizing for AMS circuits, achieving 100% correctness on four real-world circuits with 1.4×-4.8× higher sample efficiency than strong baselines.

## Executive Summary
TopoSizing is an end-to-end framework that addresses the challenge of efficient analog circuit design by combining graph-based hierarchical circuit representation with LLM agents for iterative circuit understanding, integrated with Bayesian optimization enhanced by LLM-guided sampling and trust-region updates. The framework transforms raw netlists into structured hierarchical graphs, uses LLM agents to understand circuit topology and assign parameters, and employs Bayesian optimization with stagnation-triggered LLM intervention for efficient sizing. It achieves 100% correctness in circuit understanding and parameter assignment on four real-world circuits while delivering significant improvements in sample efficiency and runtime compared to strong baselines.

## Method Summary
TopoSizing converts raw netlists into hierarchical graph structures (Component-Module-Stage) using subgraph isomorphism to identify sub-circuits and current-path analysis for stage grouping. GPT-4o LLM agents run an iterative hypothesis-verification loop with consistency checks to annotate functional roles and assign design parameters. The framework uses Trust Region Bayesian Optimization (TuRBO) as the optimizer backbone, with LLM intervention triggered only when optimization stagnates for K iterations. Initial sampling is guided by conservative LLM-based space pruning. The approach achieves 100% correctness on four circuits (Two-stage OTA, FCOTA, SACMP, LDO) implemented in SMIC 55nm process, with 1.4×-4.8× higher sample efficiency and 1.2×-3.5× faster runtime than strong baselines.

## Key Results
- 100% correctness in circuit understanding and parameter assignment on four real-world circuits
- 1.4×-4.8× higher sample efficiency compared to strong baselines
- 1.2×-3.5× faster runtime while requiring 2-4× fewer LLM calls than prior LLM-based frameworks

## Why This Works (Mechanism)

### Mechanism 1: Graph-Structured Semantic Enrichment
Converting raw netlists into hierarchical graph structures (Component-Module-Stage) enables reliable LLM-based circuit interpretation by reducing search space complexity and providing structural context that raw netlists lack. Algorithms identify current paths and sub-circuits via subgraph isomorphism, which is critical since LLMs struggle with raw text netlists.

### Mechanism 2: Confidence-Driven Iterative Refinement
An iterative hypothesis-verification loop with explicit consistency checks prevents hallucination propagation. LLM agents generate functional hypotheses but must validate them against a checklist and graph connectivity. Low confidence triggers queries for more information until certainty is achieved.

### Mechanism 3: Stagnation-Triggered Trust Region Guidance
LLMs are invoked only when optimization plateaus (stagnation) rather than continuously, balancing sample efficiency with resource constraints. The system runs standard Trust Region Bayesian Optimization and queries the LLM to re-center or resize trust regions when improvement stalls for K iterations.

## Foundational Learning

- **Concept: Graph Isomorphism for Template Matching**
  - **Why needed here:** The system relies on matching device connections to known library blocks to simplify the graph before the LLM sees it.
  - **Quick check question:** Can you explain how a subgraph matching algorithm detects a specific transistor configuration within a larger netlist?

- **Concept: Trust Region Bayesian Optimization (TuRBO)**
  - **Why needed here:** The framework uses TuRBO as the optimizer backbone; understanding "trust regions" is vital to knowing where and when the LLM intervenes.
  - **Quick check question:** What happens to the trust region radius in standard TuRBO when the objective function fails to improve?

- **Concept: Bipartite Graphs in Circuit Theory**
  - **Why needed here:** The method models circuits as bipartite graphs (device nodes vs. net nodes) to preserve lossless connectivity info for algorithms.
  - **Quick check question:** In a bipartite graph representation of a circuit, which two distinct sets of nodes represent the physical hardware?

## Architecture Onboarding

- **Component map:** Graph Engine -> Sub-circuit Matching -> Stage Grouping -> JSON Output -> LLM Agent Loop -> Verification -> Parameter Assignment -> LLM-guided Initial Sampling -> TuRBO Loop -> Stagnation Trigger -> LLM Update -> Simulator

- **Critical path:** The transformation of the raw netlist into the hierarchical JSON structure. If the graph extraction or module matching fails here, the LLM receives garbage input, leading to incorrect design space pruning.

- **Design tradeoffs:**
  - *Generality vs. Library:* The sub-circuit library allows for fast semantic injection but limits the framework to known topologies unless manually extended.
  - *Automation vs. Cost:* Using a generic LLM (GPT-4o) avoids training but introduces API latency and costs, mitigated here by stagnation triggers.

- **Failure signatures:**
  - *Understanding Failure:* The LLM assigns incorrect roles to devices, leading to symmetric parameters being tied incorrectly.
  - *Over-pruning:* Initial sampling becomes too conservative, excluding the optimal design point entirely.

- **First 3 experiments:**
  1. **Unit Test Graph Extraction:** Input a standard OTA netlist and verify the JSON output contains correct "Stage" and "Module" groupings manually.
  2. **Ablate the LLM:** Run the sizing loop with the graph extractor on but the LLM guidance off (pure TuRBO) to establish a baseline for sample count.
  3. **Stagnation Sensitivity:** Vary the `K` (stagnation threshold) parameter in Algorithm 2 to observe the trade-off between total LLM calls and optimization convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TopoSizing generalize to circuits with novel module topologies not covered in the predefined sub-circuit template library?
- Basis in paper: [explicit] "The initial library covers differential pairs, current mirrors, cascode stages... These modules are chosen because they constitute the fundamental building blocks... The library can also be readily extended to more complex structures."
- Why unresolved: The paper validates on four circuits that primarily use standard building blocks, but does not test circuits requiring custom or uncommon module structures that lack templates.

### Open Question 2
- Question: Can the framework maintain 100% circuit understanding correctness when scaling to circuits with significantly more than 27 design parameters and 33 components?
- Basis in paper: [explicit] The largest test case (LDO) has 27 parameters and approximately 10^69 sampling points. The paper does not test larger designs.
- Why unresolved: Graph complexity and hierarchical grouping may degrade as circuit size increases, potentially overwhelming the iterative verification loop or causing LLM context limitations.

### Open Question 3
- Question: How sensitive is TopoSizing to the choice of LLM backbone, and can smaller or open-source models achieve comparable performance?
- Basis in paper: [explicit] "The language model adopted in all LLM-related settings is GPT-4o... No fine-tuning or task-specific pretraining is applied."
- Why unresolved: The framework relies exclusively on GPT-4o; model dependency affects cost, accessibility, and reproducibility. The contribution of model reasoning capability vs. framework architecture remains unclear.

## Limitations

- Library coverage dependence: Framework performance critically depends on predefined sub-circuit library, potentially failing on novel topologies
- Black-box LLM evaluation: Ground truth annotations and evaluation methodology not provided for independent verification
- Process-specific claims: Performance metrics tied to SMIC 55nm technology may not generalize to other process nodes

## Confidence

**High Confidence Claims** (Level 4-5):
- The graph-based hierarchical representation methodology is well-specified and reproducible
- The stagnation-triggered trust region mechanism is clearly described in Algorithm 2
- The 100% correctness claim for circuit understanding on the four benchmark circuits

**Medium Confidence Claims** (Level 3):
- The 1.4×-4.8× sample efficiency improvement over baselines (requires independent replication)
- The 2-4× reduction in LLM calls compared to prior frameworks (depends on undisclosed prompt details)

**Low Confidence Claims** (Level 1-2):
- Generalization to arbitrary AMS circuits beyond the four benchmarks
- Cost-effectiveness given API usage (runtime savings may be offset by LLM costs)

## Next Checks

1. **Library Extension Test**: Apply the framework to a circuit containing a sub-circuit topology not in the original library (e.g., a folded cascode with Miller compensation). Measure whether the LLM still achieves 100% correctness or fails due to missing templates.

2. **Ablation Study on Stagnation Threshold**: Systematically vary the stagnation threshold K (Algorithm 2) across orders of magnitude (e.g., K=1, 5, 10, 20) and measure the trade-off between LLM call count and optimization convergence speed to verify the claimed efficiency gains.

3. **Cross-Process Verification**: Port one benchmark circuit (e.g., the Two-stage OTA) to a different process node (e.g., TSMC 65nm) and rerun the entire pipeline to verify that sample efficiency and runtime improvements persist or degrade predictably.