---
ver: rpa2
title: "Pok\xE9AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon\
  \ Red"
arxiv_id: '2506.23689'
source_url: https://arxiv.org/abs/2506.23689
tags:
- agent
- battle
- emon
- execution
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Pok\xE9AI is a text-based, multi-agent system for autonomously\
  \ playing Pok\xE9mon Red, comprising three specialized agents: Planning, Execution,\
  \ and Critique. The Planning Agent generates high-level tasks, the Execution Agent\
  \ performs actions within the game, and the Critique Agent verifies outcomes."
---

# PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red
## Quick Facts
- arXiv ID: 2506.23689
- Source URL: https://arxiv.org/abs/2506.23689
- Reference count: 15
- Primary result: 80.8% average win rate in wild Pokémon battles, 6% below human baseline

## Executive Summary
PokéAI is a text-based, multi-agent system designed to autonomously play Pokémon Red by combining Planning, Execution, and Critique agents. The system currently focuses on the battle module within the Execution Agent, which achieves strong performance in wild encounters. The architecture enables task delegation, execution, and verification through LLM-driven decision-making. While preliminary, results show promise for scaling to full-game autonomy.

## Method Summary
The system uses a Planning Agent to generate high-level tasks, an Execution Agent to perform actions via tool calls, and a Critique Agent to verify outcomes. The battle module is triggered by detecting memory address 0xD057 and queries an LLM for optimal actions based on battle state. Actions are parsed from JSON responses and executed through emulator controls. The current evaluation uses a Mt. Moon checkpoint with Level 15 Charmander and Pidgey against wild Pokémon, averaging 80.8% win rate over 50 encounters.

## Key Results
- Battle module achieves 80.8% win rate in 50 wild encounters, only 6% below experienced human player
- Ablation study shows disabling items has largest negative impact, while escape has minimal effect
- Different LLM backends exhibit distinct playstyles; higher Arena scores correlate with better performance
- Preliminary Letta integration shows improved decision-making via long-term memory recall

## Why This Works (Mechanism)
The system's effectiveness stems from its closed-loop agent control architecture, where Planning generates tasks, Execution performs actions via function calling, and Critique verifies results. Vector-based memory retrieval provides contextual information to agents, while the text-based interface ensures computational efficiency. The battle module's passive triggering during other tasks simplifies Planning but may interrupt complex goals.

## Foundational Learning
- **Concept:** Closed-Loop Agent Control
  - **Why needed here:** The Planning-Execution-Critique cycle forms the core feedback loop for autonomous gameplay
  - **Quick check question:** Can you trace the flow of a single task from generation in Planning Agent through execution to Critique verification and back?

- **Concept:** Function Calling / Tool Use in LLMs
  - **Why needed here:** Execution Agent uses function-calling to invoke tools and convert LLM outputs into game actions
  - **Quick check question:** How does the Execution Agent detect if a task cannot be completed with its available tools?

- **Concept:** Vector-Based Memory Retrieval
  - **Why needed here:** Both Planning and Execution Agents use vector memory banks for long-term context and tool storage
  - **Quick check question:** What type of information does the Planning Agent retrieve from its vector memory before generating tasks?

## Architecture Onboarding
- **Component map:** Planning Agent -> Task generation -> Execution Agent -> Tool invocation -> Battle module detection -> LLM query -> Action execution -> Critique Agent verification -> Task completion or retry
- **Critical path:** Game state capture → Prompt construction → LLM response parsing → Action execution → State update → Verification
- **Design tradeoffs:** Text-based interface chosen for efficiency over multi-modal visual input; passive battle module simplifies Planning but may interrupt complex goals
- **Failure signatures:** "Regenerate" loops when tools are unavailable; battle module stuck when invalid moves selected; Critique-retry cycles for ambiguous goals; creative rule-bending when constraints are weak
- **First 3 experiments:** 1) Battle module baseline reproduction in Mt. Moon with DeepSeek-V3, 2) Ablation validation disabling escape/switch/items separately, 3) LLM backend swap testing GPT-4o and Claude 3.5 Sonnet with action distribution analysis

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the complete closed-loop architecture successfully progress through the entire game of Pokémon Red, including navigation and trainer battles? [explicit] The authors state in Section IV that "immediate future work" involves implementing the Planning and Critique agents to complete the system, as current evaluation is limited to the battle module.
- **Open Question 2:** Does advanced reasoning capability improve game performance, or does it consistently lead to "overthinking" and suboptimal decisions? [explicit] Section III.C notes that preliminary tests of reasoning-oriented models (like DeepSeek-R1) suggest they "may even lead to worse decisions" in simple battles, but they were not fully evaluated.
- **Open Question 3:** Does the integration of persistent long-term memory significantly improve strategic decision-making compared to short-term context? [explicit] Section III.E presents a "pilot study" suggesting long-term memory helps avoid unfavorable battles, but notes the current system relies only on the history of the last three rounds.

## Limitations
- Evaluation limited to battle module only; Planning and Critique agents not yet integrated for full gameplay
- Win rate comparison based on small sample size (50 battles) with potential statistical uncertainty
- Long-term memory integration remains preliminary with only anecdotal qualitative benefits demonstrated
- Complete memory address mapping for battle state not specified, requiring reverse-engineering for reproduction

## Confidence
- **High Confidence:** Architectural design soundness and correlation between larger models/Arena scores with better performance
- **Medium Confidence:** 80.8% win rate claim relative to human baseline given small sample size
- **Low Confidence:** Playstyle differences per LLM backend without statistical testing; Letta memory benefits based on single example

## Next Checks
1. **Integrated Gameplay Test:** Run complete scenario combining navigation to Mt. Moon, wild battles, and return to Pewter City using all three agents in sequence
2. **Statistical Validation of Ablation:** Repeat ablation study with 100+ battles per condition to establish confidence intervals for win rate differences
3. **Long-Term Memory Evaluation:** Design systematic test comparing win rates with and without memory over 100 battles with increasingly difficult opponents