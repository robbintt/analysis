---
ver: rpa2
title: Towards Error Centric Intelligence I, Beyond Observational Learning
arxiv_id: '2510.15128'
source_url: https://arxiv.org/abs/2510.15128
tags:
- learning
- causal
- which
- knowledge
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing view that AGI can be achieved
  through scaling data and models alone, arguing instead that AGI is fundamentally
  limited by theoretical understanding. The authors propose a shift from observational
  learning to "error-centric intelligence," focusing on how agents discover, correct,
  and expand their capacity to address errors.
---

# Towards Error Centric Intelligence I, Beyond Observational Learning

## Quick Facts
- arXiv ID: 2510.15128
- Source URL: https://arxiv.org/abs/2510.15128
- Reference count: 32
- Primary result: Proposes a theoretical framework for AGI based on "error-centric intelligence" that shifts from observational learning to systems capable of discovering, correcting, and expanding capacity to address errors through conjecture and criticism.

## Executive Summary
This paper challenges the prevailing view that AGI can be achieved through scaling data and models alone, arguing instead that AGI is fundamentally limited by theoretical understanding. The authors propose a shift from observational learning to "error-centric intelligence," focusing on how agents discover, correct, and expand their capacity to address errors. The core method introduces Causal Mechanics, a framework with three structural principles: the Locality-Autonomy Principle (LAP) for modular interventions, a gauge-invariant Independent Causal Mechanisms (ICM) for separability, and the Compositional Autonomy Principle (CAP) for preserving analogical structure during learning. These principles are operationalized through diagnostics like gradient penalties and block-diagonal Fisher witnesses.

## Method Summary
The paper presents Causal Mechanics as a theoretical framework for understanding intelligence through error-centric principles. It formalizes three structural principles: LAP ensures local modularity through Lie derivative conditions (L_ξA M_i = 0), ICM provides gauge-invariant separability via block-diagonal Fisher witnesses and commuting flows, and CAP preserves analogical structure through locality, law stability, and analogy-consistency diagnostics. The framework operationalizes these principles through gradient penalties, Fisher information witnesses, and analogy consistency residuals. While the theoretical scaffold is detailed, practical implementation requires Part II (E-SCMs) which is not yet available.

## Key Results
- Demonstrates that observational adequacy alone cannot guarantee interventional competence due to observational equivalence of different causal structures
- Proposes LAP, ICM, and CAP as necessary structural principles for AGI systems
- Introduces formal diagnostics for measuring locality, separability, and compositional autonomy in learning systems
- Reframes AGI as a system's ability to convert unreachable errors into reachable ones through conjecture and criticism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Observational adequacy does not secure interventional competence—systems must explicitly represent and manipulate hypothesis spaces.
- Mechanism: The paper formalizes this gap via Propositions 1-3: purely observational data cannot identify interventional laws when causal structures are observationally equivalent, and Bayesian conditionalization "reweights" while do(·) "rewires"—no functional of observational data alone recovers interventional responses without additional surgery maps.
- Core assumption: Different causal data-generating processes can induce identical P_obs(X,Y) yet yield different P(Y|do(X=x)).
- Evidence anchors: [abstract] "observationally equivalent worlds can diverge under interventions, so observational adequacy alone cannot guarantee interventional competence"; [Section 2.1, Proposition 1] "Purely observational data do not, in general, identify interventional laws when causal structures are observationally equivalent"

### Mechanism 2
- Claim: Catastrophic forgetting is a structural consequence of fractured-entangled representations (FER), not merely an optimization failure.
- Mechanism: LAP formalizes locality (L_ξA M_i = 0 for non-descendants) and autonomy (L_ΞA M_i = 0 for parameter perturbations). When violated, gradients leak across mechanism blocks via non-use Jacobians, creating interference. Lemma 1 bounds cross-task gradient alignment by overlap terms plus (ε_loc + ε_aut) residuals.
- Core assumption: Mechanisms can be organized into approximately separable parameter blocks with block-diagonal Fisher structure.
- Evidence anchors: [Section 2.2] "catastrophic forgetting–the overwriting of earlier competence by later training–need not be viewed as an incidental stability–plasticity failure. We frame it instead as a structural consequence of fractured, entangled representations"; [Appendix B, Corollary 1] "Under exact LAP with disjoint mechanism support...the first-order change in A's risk during an update for B vanishes"

### Mechanism 3
- Claim: Analogical reasoning requires compositional structure preserved under learning—CAP provides locality, law stability, and analogy-consistency diagnostics.
- Mechanism: CAP requires: (1) non-use Jacobians vanish (∇_θσ[[T]] ≈ 0 when σ ∉ T), (2) algebraic laws remain satisfied under unrelated parameter updates, (3) Φ∘[[T]]_A ≈ [[F(T)]]_B∘Φ^×k (compose-then-map ≈ map-then-compose). Violations surface as measurable residuals.
- Core assumption: Domains admit finite signatures with interpretable primitives and composable terms.
- Evidence anchors: [Section 4.2.4] "CAP asserts three structural requirements: Locality...Law stability...Analogy consistency"; [Section 4.2.5] "Locality diagnostic: L_loc(σ,T) := E_X ||∇_θσ [[T]]_θ(X)||² ≤ ε_loc"

## Foundational Learning

- Concept: **Structural Causal Models (SCMs) and do-calculus**
  - Why needed here: The entire framework builds on SCM semantics—LAP and ICM presume understanding of mechanisms, interventions, and modularity.
  - Quick check question: Can you explain why P(Y|do(X=x)) ≠ P(Y|X=x) in general?

- Concept: **Lie derivatives and differential geometry basics**
  - Why needed here: LAP is formalized via Lie derivatives (L_ξ M_i = 0); Appendix C discusses adapted charts and commuting flows.
  - Quick check question: What does L_X Y = 0 mean geometrically?

- Concept: **Popperian epistemology (conjecture-refutation, not inductive justification)**
  - Why needed here: The paper's core argument—that explanatory knowledge grows via conjecture/criticism, not data accumulation—is explicitly Deutsch-Popper.
  - Quick check question: Why does the Popper-Miller decomposition challenge Bayesian inductive support?

## Architecture Onboarding

- Component map: SCM backbone (V, M_i: X_PA(i) × U_i → X_i) → LAP layer (Lie derivative diagnostics) → ICM witness (block-diagonal Fisher checks) → CAP module (term evaluator [[T]]_θ)

- Critical path:
  1. Define signature Σ and primitive mechanisms with parameterized implementations
  2. Instrument gradient flow to capture non-use Jacobians during training
  3. Design term grammar for CAP diagnostics; track residuals over training
  4. Implement hypothesis-space revision operators (mechanism addition/removal, invariance refactoring)

- Design tradeoffs:
  - Strict LAP vs. approximate: Exact LAP bounds forgetting but constrains architecture flexibility
  - CAP term coverage: Broader term sets improve diagnostic coverage but increase compute
  - Probabilistic vs. constraint-based: Paper explicitly makes probability "optional"—E-SCMs (Part II) use energy-based constraints instead

- Failure signatures:
  - Elevated non-use Jacobians → LAP violation → gradient interference risk
  - Growing CAP residuals H_T(Φ) despite stable in-domain loss → analogy erosion
  - Block-diagonal Fisher approximation fails → ICM structural coupling (missing mechanism)

- First 3 experiments:
  1. **LAP validation**: Train on multi-task sequence with disjoint mechanism support; measure ||⟨g_A, g_B⟩|| vs. baseline. Expect near-zero under exact LAP (Corollary 1).
  2. **CAP stress test**: Define two domains with known signature correspondence Φ; train while monitoring H_T(Φ) for covered terms. Verify residuals remain bounded during optimization.
  3. **Hypothesis-space expansion probe**: Introduce synthetic errors unreachable under current H; implement mechanism-addition operator; measure whether new mechanism reduces unreachable error set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can systems be designed to achieve endogenous hypothesis-space revision—the capacity to detect model inadequacy and propose structural alternatives without external intervention?
- Basis in paper: [explicit] Section 3 states "General intelligence requires endogenous hypothesis space revision: a capacity to detect the inadequacy of current models and propose structural alternatives. This process allows the agent to transform unreachable errors into reachable ones."
- Why unresolved: The paper provides structural principles (LAP, ICM, CAP) as a scaffold but does not implement or validate a system that actually performs hypothesis-space revision autonomously.
- What evidence would resolve it: A demonstrated system that, when encountering unreachable errors, autonomously expands its hypothesis class and corrects those errors without architectural or interface changes by human engineers.

### Open Question 2
- Question: Do better explanations systematically tend toward ICM compatibility—does augmenting models with latent mechanisms restore approximate separability in adapted parameter coordinates?
- Basis in paper: [explicit] Section 4.1 states the "meta-conjecture: Better explanations tend toward ICM compatibility" and notes this could be falsified by intrinsic couplings such as non-integrable constraints, critical phenomena, or quantum entanglement.
- Why unresolved: The meta-conjecture is proposed as a falsifiable claim but no systematic empirical or theoretical validation is provided.
- What evidence would resolve it: Empirical studies showing that across domains, model refinements that improve explanatory power systematically exhibit increasing block-diagonality of Fisher/metric tensors in adapted charts, or counterexamples where better explanations persistently violate separability.

### Open Question 3
- Question: Can Energy-Structured Causal Models (E–SCMs) successfully operationalize LAP, ICM, and CAP to support probability-optional abduction–intervention–prediction with cyclic graphs and latent-space interventions?
- Basis in paper: [explicit] Section 5 states "Part II develops Energy Structured Causal Models (E–SCMs) that operationalize aspects of this program" and lists these capabilities as goals.
- Why unresolved: E-SCMs are deferred to Part II and their ability to handle the claimed functionalities remains unvalidated.
- What evidence would resolve it: Published Part II demonstrating E-SCMs handling cyclic causal graphs, latent interventions, and comparing diagnostic performance against standard SCMs on benchmark tasks requiring counterfactual reasoning.

### Open Question 4
- Question: What practical surrogates can approximate the Constructor-Theoretic Task Description Length (CT-TDL) for causal directionality selection in real learning systems?
- Basis in paper: [explicit] Appendix D introduces CT-TDL as "speculative" and explicitly states "Practical surrogates would need to be developed."
- Why unresolved: CT-TDL is defined theoretically in terms of minimal physical resources for task realization, but no computable approximation is provided.
- What evidence would resolve it: Derivation of tractable bounds or estimators for CT-TDL, validated on causal discovery benchmarks where MDL-based methods currently fail or produce ambiguous results.

## Limitations

- Theoretical framework remains largely unvalidated experimentally with Part II implementations not yet available
- Mechanism partitioning scheme for neural networks is underspecified, leaving practical implementation questions open
- The assumption that domains admit finite signatures with interpretable primitives may not hold for highly abstract or emergent domains

## Confidence

- **High confidence**: The identification problem (Proposition 1-3) and the formal distinction between observational and interventional reasoning are well-established in causal inference literature
- **Medium confidence**: The framing of catastrophic forgetting as a structural consequence of FER rather than an optimization failure is plausible but requires empirical validation
- **Low confidence**: The claim that CAP-style compositional structure is necessary for analogical reasoning in AGI systems is highly speculative with no empirical evidence provided

## Next Checks

1. **Empirical LAP test**: Implement the gradient interference diagnostic on a standard continual learning benchmark (e.g., permuted MNIST or Split CIFAR). Compare interference levels between architectures with explicit mechanism separation (e.g., modular adapters) versus end-to-end fine-tuning. Measure whether the bound in Lemma 1 accurately predicts forgetting patterns.

2. **CAP analogy stress test**: Construct two synthetic domains with known signature correspondence (e.g., arithmetic in base-10 vs. base-2, or graph relations across isomorphic structures). Train models to perform tasks in both domains while monitoring CAP residuals H_T(Φ). Verify whether models preserving compositional structure show superior zero-shot transfer compared to black-box end-to-end learners.

3. **Hypothesis-space expansion probe**: Design a toy environment where certain errors are unreachable under a fixed hypothesis space (e.g., a causal graph missing necessary edges). Implement mechanism-addition operators as described in Section 4.2.3 and measure whether the system can discover and correct previously unreachable errors through conjecture and criticism, rather than pure data-driven pattern matching.