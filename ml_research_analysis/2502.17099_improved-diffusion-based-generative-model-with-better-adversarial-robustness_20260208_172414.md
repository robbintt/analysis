---
ver: rpa2
title: Improved Diffusion-based Generative Model with Better Adversarial Robustness
arxiv_id: '2502.17099'
source_url: https://arxiv.org/abs/2502.17099
tags:
- training
- conference
- adversarial
- diffusion
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the distribution mismatch problem in diffusion-based
  generative models, where the input data distribution differs between training and
  inference stages, potentially leading to inaccurate data generation. The authors
  propose using Adversarial Training (AT) to mitigate this issue by conducting efficient
  AT on both Diffusion Probabilistic Models (DPMs) and Consistency Models (CMs).
---

# Improved Diffusion-based Generative Model with Better Adversarial Robustness

## Quick Facts
- arXiv ID: 2502.17099
- Source URL: https://arxiv.org/abs/2502.17099
- Authors: Zekun Wang; Mingyang Yi; Shuchen Xue; Zhenguo Li; Ming Liu; Bing Qin; Zhi-Ming Ma
- Reference count: 40
- Primary result: Demonstrates that adversarial training (AT) mitigates distribution mismatch in diffusion models by framing it as a distributionally robust optimization (DRO) problem.

## Executive Summary
This paper addresses the distribution mismatch problem in diffusion-based generative models, where the input data distribution differs between training and inference stages, potentially leading to inaccurate data generation. The authors propose using Adversarial Training (AT) to mitigate this issue by conducting efficient AT on both Diffusion Probabilistic Models (DPMs) and Consistency Models (CMs). Theoretical analysis shows that AT is equivalent to Distributionally Robust Optimization (DRO) for these models, which improves their distributional robustness. Extensive experiments validate the effectiveness of AT, showing significant improvements in FID scores across various tasks, including image generation on CIFAR-10 and ImageNet, and text-to-image generation on MS-COCO. The proposed method consistently outperforms baseline approaches and is orthogonal to other techniques aimed at alleviating distribution mismatch.

## Method Summary
The paper proposes using Adversarial Training (AT) to mitigate distribution mismatch in diffusion-based generative models. The key insight is that the mismatch arises because training uses ground-truth data distributions while inference uses model-generated distributions. The authors theoretically demonstrate that AT is equivalent to Distributionally Robust Optimization (DRO) for these models. They implement an efficient AT algorithm that updates both the perturbation and model parameters simultaneously in each training iteration, rather than using the computationally expensive standard AT approach. The method is applied to both DPMs and Consistency Models, with theoretical analysis showing how AT addresses the mismatch in each case. The approach is evaluated on CIFAR-10, ImageNet, and MS-COCO datasets, showing consistent improvements in FID scores across various sampling steps.

## Key Results
- Significant FID improvements on CIFAR-10 and ImageNet when applying AT to diffusion models
- AT consistently outperforms baseline approaches across different sampling steps (NFEs)
- The method is effective for both image generation and text-to-image tasks on MS-COCO
- AT is orthogonal to other techniques for alleviating distribution mismatch, suggesting it can be combined with other improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distribution mismatch between training and inference in diffusion models causes error accumulation during sampling.
- **Mechanism:** During training, the model learns $p_\theta(x_t|x_{t+1})$ using ground-truth $x_{t+1} \sim q(x_{t+1})$. During inference, $x_{t+1}$ comes from $p_\theta(x_{t+1})$, which deviates from $q(x_{t+1})$. Per Proposition 2, this error propagates: $D_{KL}(q(x_t) \| p_\theta(x_t)) \leq \gamma_0 + (T-t)\gamma/T$.
- **Core assumption:** The conditional KL divergence per step ($\gamma/T$) remains bounded during standard training.
- **Evidence anchors:**
  - [abstract] "During the denoising process, the input data distributions differ between the training and inference stages"
  - [section 4.1] Proposition 3: "$L_t$ is well minimized, only if $q(x_{t+1})$ is Gaussian or $\|x_{t+1} - x_t\| \to 0$"
  - [corpus] Related work on exposure bias (Ning et al., 2023; Li et al., 2024) confirms mismatch is recognized but prior solutions require strong distributional assumptions.
- **Break condition:** If sampling steps $\to T$ (continuous limit) or score estimation is perfect, mismatch diminishes but computational cost increases.

### Mechanism 2
- **Claim:** Adversarial Training (AT) is mathematically equivalent to Distributionally Robust Optimization (DRO) for DPMs, improving distributional robustness.
- **Mechanism:** DRO objective $\min_\theta \sup_{\tilde{q}_t \in B_{D_{KL}}(q, \eta_0)} D_{KL}(\tilde{q}_t(x_t|x_{t+1}) \| p_\theta(x_t|x_{t+1}))$ is transformed via Theorem 1 into a perturbed noise prediction problem. Per Proposition 6, perturbation $\|\delta_t\|_1 \leq \eta$ with high probability, yielding the AT objective in Eq. (14).
- **Core assumption:** The perturbation $\delta_t$ stays within a small $\ell_p$-norm ball; the optimal $\tilde{q}_t(x_{t+1}) = p_\theta(x_{t+1})$ lies within the KL-ball around $q(x_{t+1})$.
- **Evidence anchors:**
  - [abstract] "theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT)"
  - [section 4.2] Theorem 1 establishes equivalence; Proposition 5 proves $L^{DRO}_t(\theta) \leq \eta_0$ guarantees $D_{KL}(q(x_0) \| p_\theta(x_0)) \leq \eta_0$
  - [corpus] Sinha et al. (2018) and Yi et al. (2021) link DRO to AT in supervised learning; this paper extends to generative models.
- **Break condition:** If perturbation radius $\eta$ is too large, adversarial samples may leave the data manifold, degrading generation quality.

### Mechanism 3
- **Claim:** Consistency Models (CM) suffer analogous mismatch due to ODE solver approximation, mitigated by AT.
- **Mechanism:** CM distills multi-step DPM into single-step via consistency distillation loss $\hat{L}_{CD}$ using numerical ODE solver $\hat{\Phi}_t$. The solver error $\|\delta_t(x_{t+1})\|$ (Theorem 3: $\mathbb{E}[\|\delta_t\|] \leq o(1)$) causes mismatch. AT objective (17) maximizes over perturbations within radius $\eta$.
- **Core assumption:** The ODE solver error is bounded; $f_\theta$ is Lipschitz-continuous with bounded second derivatives.
- **Evidence anchors:**
  - [abstract] "for the recently proposed Consistency Model (CM)...we prove that its training objective also encounters the mismatch issue. Fortunately, this issue can be mitigated by AT as well"
  - [section 5] Theorem 3: $\mathbb{E}_{x_{t+1}}[\|\delta_t(x_{t+1})\|] \leq o(1)$; Eq. (18) bounds Wasserstein distance
  - [corpus] No direct corpus papers address CM mismatch via AT; this appears novel.
- **Break condition:** If ODE discretization steps increase (finer solver), $\delta_t \to 0$ and AT benefit diminishes.

## Foundational Learning

- **Concept: Diffusion Probabilistic Models (Forward/Reverse Process)**
  - Why needed here: The entire method modifies the standard DPM training objective. Without understanding the forward noising $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon_t$ and reverse denoising $p_\theta(x_t|x_{t+1})$, the perturbation mechanism is opaque.
  - Quick check question: Can you derive why predicting noise $\epsilon_\theta$ is equivalent to learning $p_\theta(x_t|x_{t+1})$?

- **Concept: Distributionally Robust Optimization (DRO)**
  - Why needed here: The paper's core theoretical contribution is recasting mismatch as a DRO problem. Understanding the minimax formulation over distribution uncertainty sets is essential.
  - Quick check question: What does the KL-ball constraint $B_{D_{KL}}(q, \eta_0)$ represent geometrically?

- **Concept: Adversarial Training (PGD and Free-AT)**
  - Why needed here: Practical implementation uses efficient AT (Shafahi et al., 2019). Standard PGD requires multiple gradient steps per batch; Free-AT reuses computation. The paper achieves 2.6× speedup over PGD.
  - Quick check question: Why does Free-AT update both perturbation $\delta$ and model $\theta$ in the same iteration without extra backpropagation?

## Architecture Onboarding

- **Component map:**
  - Input: Dataset $D$, pretrained DPM or CM backbone, hyperparameters $(\alpha, K, \eta)$
  - AT Module: Perturbation generator (lines 6-8, Alg 1/2); maximizes loss w.r.t. $\delta$
  - Training Loop: Standard optimizer step + AT inner loop (shared backward pass)
  - Output: Robustified $\epsilon_\theta$ (DPM) or $f_\theta$ (CM)

- **Critical path:**
  1. Sample $(x_0, t, \epsilon)$ → construct $x_t$
  2. Initialize $\delta \leftarrow 0$
  3. For $i=1$ to $K$: compute $L = \|\epsilon_\theta(x_t + \delta) - \epsilon - \delta/\sqrt{1-\bar{\alpha}_t}\|^2$, update $\delta \leftarrow \delta + \alpha \nabla_\delta L / \|\nabla_\delta L\|$
  4. Update $\theta \leftarrow \theta - \kappa \nabla_\theta L$ (single backward)
  5. Repeat

- **Design tradeoffs:**
  - **Adversarial steps $K$:** Higher $K$ → stronger robustness, slower training. Paper uses $K \in \{2, 5\}$.
  - **Adversarial LR $\alpha$:** Controls perturbation magnitude. Larger images (ImageNet vs CIFAR) benefit from higher $\alpha$ (0.5 vs 0.1).
  - **Perturbation norm:** $\ell_2$ outperforms $\ell_1$ and $\ell_\infty$ in ablation (Table 17).

- **Failure signatures:**
  - FID degradation at high NFE (>50): May indicate over-robustification hurting fine details
  - CLIP score drop in T2I: AT not optimized for alignment; monitor separately
  - Training instability: Reduce $\alpha$ or $K$; ensure EMA rate is properly set

- **First 3 experiments:**
  1. **Sanity check:** Fine-tune pretrained ADM on CIFAR-10 with AT ($K=3, \alpha=0.1$) for 50K iterations. Compare FID at NFE=10 vs baseline. Expect ~15-20% FID reduction per Table 1.
  2. **Sampler robustness:** Evaluate same checkpoint across IDDPM, DDIM, DPM-Solver at NFEs $\in \{5, 10, 20\}$. Verify AT helps most at low NFE (larger mismatch).
  3. **Ablation $\alpha$:** Grid search $\alpha \in \{0.05, 0.1, 0.5\}$ on ImageNet-64. Confirm optimal $\alpha$ scales with image size per Table 16.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the precise theoretical relationship between input data dimensionality and the optimal adversarial perturbation magnitude ($\eta$) required to mitigate distribution mismatch?
- **Basis in paper:** [Explicit] Appendix G.3 speculates that larger optimal perturbation levels are needed for higher resolutions (ImageNet vs. CIFAR-10) due to the $\ell_2$-norm scaling with vector dimension, but offers no formal proof.
- **Why unresolved:** The authors empirically tune the learning rate $\alpha$ for different datasets but lack a derived scaling law that links pixel dimension to the radius of the uncertainty set in the DRO formulation.
- **What evidence would resolve it:** A theoretical derivation defining the perturbation radius relative to dimension, or consistent empirical scaling laws across varied resolutions (e.g., $128\times128$ to $1024\times1024$).

### Open Question 2
- **Question:** Can the initial training instability and convergence slowdown observed when training from scratch be mitigated by adaptive scheduling of the adversarial perturbation?
- **Basis in paper:** [Explicit] Appendix G.2 observes that the proposed AT method exhibits slower convergence than the baseline during the first 100K iterations when training from scratch (Figure 2).
- **Why unresolved:** The paper demonstrates the effectiveness of the final model but does not analyze the optimization dynamics that cause the initial performance degradation or how to prevent it without pre-training.
- **What evidence would resolve it:** Experiments utilizing a "warm-up" schedule for the perturbation strength $\eta$, or a convergence analysis showing how the gradient variance changes during the early training phase.

### Open Question 3
- **Question:** Does the theoretical equivalence between Adversarial Training and Distributionally Robust Optimization hold strictly under alternative perturbation norms ($\ell_1$ or $\ell_\infty$)?
- **Basis in paper:** [Inferred] While Appendix G.3 empirically compares $\ell_1$, $\ell_2$, and $\ell_\infty$ norms, the theoretical connection to DRO (specifically Proposition 6 and Theorem 1) relies on bounds and Talagrand's inequality typically associated with $\ell_2$ geometry.
- **Why unresolved:** It is unclear if the mathematical equivalence to the Wasserstein distance constraint in DRO is maintained when the perturbation constraint set is changed to a non-Euclidean norm.
- **What evidence would resolve it:** Theoretical proofs extending Theorem 1 to general $p$-norms, or empirical analysis of the distribution mismatch reduction using different norms on non-Gaussian data distributions.

## Limitations
- The paper does not specify the exact perturbation radius $\eta$ used in experiments, which is critical for reproducing the AT objective.
- Empirical results rely heavily on fine-tuning pretrained models rather than training from scratch, limiting generality.
- The extension to Consistency Models (CMs) is promising but less validated, as it builds on ODE solver error analysis without direct ablation studies.

## Confidence
- **High:** The theoretical equivalence between AT and DRO for DPMs (Theorem 1) is well-supported with rigorous proofs. The mechanism linking mismatch to exposure bias is sound.
- **Medium:** Empirical results show consistent FID improvements across tasks, but the paper relies heavily on fine-tuning pretrained models rather than training from scratch. The generality to other architectures (e.g., latent diffusion) is untested.
- **Low:** The extension to Consistency Models (CMs) is promising but less validated, as it builds on ODE solver error analysis without direct ablation studies.

## Next Checks
1. **Hyperparameter sensitivity:** Systematically vary $\eta$ and $K$ to map the robustness-FID tradeoff curve for a single model.
2. **Distributional shift test:** Evaluate model robustness under different inference samplers (DDIM vs DPM-Solver) and quantify mismatch reduction via KL divergence estimates.
3. **Cross-architecture transfer:** Apply the same AT objective to a latent diffusion model (e.g., Stable Diffusion) and compare convergence and FID gains.