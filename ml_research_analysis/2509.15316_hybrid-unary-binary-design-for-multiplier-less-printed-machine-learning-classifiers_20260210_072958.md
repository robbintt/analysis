---
ver: rpa2
title: Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers
arxiv_id: '2509.15316'
source_url: https://arxiv.org/abs/2509.15316
tags:
- unary
- printed
- hybrid
- power
- area
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Printed electronics enable flexible, low-cost ML circuits but face
  limitations in area and power due to large feature sizes and computational demands.
  This work introduces a hybrid unary-binary architecture for multiplier-less MLP
  classifiers that leverages PE's customization to remove costly encoders and simplify
  arithmetic operations.
---

# Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers

## Quick Facts
- arXiv ID: 2509.15316
- Source URL: https://arxiv.org/abs/2509.15316
- Reference count: 11
- Primary result: 46% area and 39% power reductions with <3% accuracy loss versus state-of-the-art

## Executive Summary
This work addresses the challenge of implementing efficient Machine Learning classifiers in printed electronics, where large feature sizes and computational demands create significant area and power constraints. The authors introduce a hybrid unary-binary architecture that leverages the customization potential of printed electronics to eliminate costly multipliers and encoders. By combining unary arithmetic for input processing with binary logic in later layers, the approach achieves substantial efficiency gains while maintaining classification accuracy.

The method employs a layer-aware training technique that optimizes weights to power-of-two values, enabling multiplier-less computation through simple wiring and bit-shifting. Evaluation across six datasets demonstrates 46% area and 39% power reductions compared to existing state-of-the-art, with self-powered operation below 30mW and less than 3% accuracy degradation. This represents the first exploration of unary arithmetic for printed ML classifiers.

## Method Summary
The approach introduces a hybrid unary-binary architecture for printed MLPs that eliminates multipliers through arithmetic-level co-design. The method uses unary encoding for input layers (leveraging thermometer code from flash ADCs without encoders) and power-of-two quantized weights in hidden layers. A layer-aware training procedure iteratively quantizes weights to nearest powers of two, retraining for 2 epochs after each step and reverting if accuracy drop exceeds threshold T. The architecture achieves multiplier-less computation by replacing AND-based multiplication with wiring for unary layers and bit-shifting for binary layers. Evaluation uses six UCI datasets with 70/30 train-test splits, targeting <30mW power for self-powered operation.

## Key Results
- 46% area reduction and 39% power reduction versus state-of-the-art printed ML classifiers
- Less than 3% accuracy loss across six datasets with self-powered operation below 30mW
- 33.6% of weights converted to power-of-two values, yielding 8.5% area and 7.9% power savings
- First application of unary arithmetic to printed ML classifiers

## Why This Works (Mechanism)

### Mechanism 1
Unary arithmetic enables multiplier-less computation by replacing AND-based multiplication with simple wiring. In unary encoding, the value is represented by the count or frequency of '1's in a bitstream. Multiplication between a temporal (thermometer) encoded input and a rate-encoded weight can be performed via bitwise-AND operations. When weights are constant (hardwired during fabrication), zeros in the weight vector eliminate corresponding connections entirely, and active inputs route directly to outputs without AND gates. The approximation error introduced by unary multiplication (theoretical maximum 1/N where N is bit-width) remains within acceptable bounds for classification tasks.

### Mechanism 2
Removing ADC encoders reduces area and power by leveraging the natural thermometer code output of flash ADCs. Flash ADCs inherently produce thermometer code (unary format) before encoding to binary. By eliminating the encoder stage, the ADC output directly feeds the unary input layer without conversion. This removes comparator-to-binary logic overhead while preserving the signal information needed for unary computation. Sensors and downstream logic can operate directly on thermometer-encoded values without requiring binary precision.

### Mechanism 3
Power-of-two weight quantization enables multiplier-less hidden layers via bit-shift equivalence to wiring. Training constrains weights to power-of-two values (e.g., 1, 2, 4, 8). In hardware, multiplication by powers of two becomes left-shifts, which are implemented as wire rerouting rather than logic gates. The training method iteratively quantizes weights and retrains, accepting changes only if accuracy drop remains below threshold T. A sufficient fraction of weights can be quantized to power-of-two values without exceeding accuracy constraints.

## Foundational Learning

- **Concept: Unary/Thermometer Encoding**
  - Why needed here: Understanding how unary representation works (value = count of '1's) is essential to grasp why multiplication simplifies to AND/wiring.
  - Quick check question: Given an 8-bit unary stream `00000111`, what decimal value does it represent in thermometer encoding?

- **Concept: Flash ADC Architecture**
  - Why needed here: The architecture exploits the internal structure of flash ADCs (comparator bank → thermometer → encoder); removing the encoder is the key optimization.
  - Quick check question: Why does a flash ADC naturally produce thermometer code before any encoding?

- **Concept: Quantization-Aware Training**
  - Why needed here: The layer-aware training constrains weights during optimization, not post-hoc; this preserves accuracy while enabling hardware-friendly representations.
  - Quick check question: What is the risk of quantizing weights after training versus during training?

## Architecture Onboarding

- **Component map:** Sensors → Pruned flash ADC (no encoder, thermometer output) → Unary input layer (wiring-only multiplication, parallel unary adders) → Binary hidden layers (power-of-two weights, shift-based multiply, standard adders) → ReLU activation → Comparator tree (argmax) → Output class

- **Critical path:** 1. Sensor analog signal enters pruned ADC 2. Thermometer-coded unary word feeds input layer 3. Unary-bitwise multiplication (wiring) + parallel accumulation 4. Binary-domain computation in hidden layers 5. Class selection via comparator tree

- **Design tradeoffs:**
  - Area/power vs. accuracy: Unary approximation and power-of-two quantization reduce hardware cost but introduce ~2-3% accuracy loss on average
  - Parallelism vs. area: Fully-parallel design enables single-cycle inference but requires more physical area than sequential alternatives
  - Unary bit-width N: Higher N reduces approximation error (1/N) but increases wiring and ADC comparator count

- **Failure signatures:**
  - Accuracy loss >5%: Likely bit-width insufficient or quantization threshold too aggressive
  - Power exceeding 30mW: Check for unoptimized binary layer weights (insufficient power-of-two conversion)
  - Input utilization <85%: May indicate redundant comparators; verify pruning logic

- **First 3 experiments:**
  1. **Baseline replication:** Implement exact binary MLP from [7] on a target dataset (e.g., Cardio) to measure area/power baseline; verify accuracy matches reported ~88%.
  2. **Unary input layer isolation:** Convert only the input layer to unary (no hidden layer quantization), measure area/power delta, and confirm accuracy loss <2%.
  3. **Full hybrid with quantization training:** Apply layer-aware power-of-two training to hidden layers, sweep threshold T (e.g., 1%, 2%, 3%), and plot accuracy vs. area/power tradeoff curve to identify optimal operating point.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Network architecture details (layer sizes, neuron counts) are not specified for the six datasets, preventing exact hardware replication.
- Training hyperparameters (optimizer, learning rate, quantization threshold T) are not fully detailed, introducing variability in accuracy vs. area/power tradeoffs.
- The ADC pruning and input-bit utilization calculation methods are referenced but not fully specified, limiting faithful hardware implementation.

## Confidence
- **High confidence**: Unary arithmetic enables multiplier-less input layers with <3% accuracy loss, based on demonstrated wiring simplifications and direct evidence from the paper.
- **Medium confidence**: Power-of-two quantization in hidden layers provides 7-9% additional area/power savings, as the exact quantization threshold and training stability are not fully detailed.
- **Medium confidence**: ADC encoder removal contributes to area/power savings, though the exact extent depends on ADC design specifics not provided.

## Next Checks
1. **Baseline replication:** Implement binary MLP on Cardio dataset to verify accuracy (~88%) and establish hardware baseline for comparison.
2. **Layer-aware quantization sensitivity:** Sweep power-of-two quantization threshold T and measure accuracy, area, and power to identify optimal operating point.
3. **Full hybrid evaluation:** Build complete unary-binary MLP with ADC encoder removal and compare against state-of-the-art in area, power, and accuracy on all six datasets.