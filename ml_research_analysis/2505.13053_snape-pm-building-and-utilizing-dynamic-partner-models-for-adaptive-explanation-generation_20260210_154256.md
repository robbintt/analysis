---
ver: rpa2
title: 'SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation
  Generation'
arxiv_id: '2505.13053'
source_url: https://arxiv.org/abs/2505.13053
tags:
- feedback
- explanation
- user
- information
- snape-pm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SNAPE-PM, a formal model for adaptive explanation
  generation in human-agent interaction. It combines a Dynamic Bayesian Network to
  track explainee features (expertise, cognitive load, attentiveness, cooperativeness)
  with a non-stationary Markov Decision Process to guide explanation strategies.
---

# SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation

## Quick Facts
- arXiv ID: 2505.13053
- Source URL: https://arxiv.org/abs/2505.13053
- Reference count: 11
- Primary result: Introduces a formal model combining DBN and non-stationary MDP for adaptive explanation generation, demonstrating persona-specific strategy adaptation in simulated interactions

## Executive Summary
SNAPE-PM presents a formal model for adaptive explanation generation in human-agent interaction. The system continuously infers four user features—expertise, cognitive load, attentiveness, and cooperativeness—using a Dynamic Bayesian Network that processes observable feedback patterns. These inferred beliefs drive a non-stationary Markov Decision Process that selects optimal explanation strategies, with Monte Carlo Tree Search providing real-time decision-making. Evaluation with five simulated personas shows the model successfully adapts explanation length and strategy selection to individual user characteristics, though human-subject validation remains necessary.

## Method Summary
SNAPE-PM integrates a Dynamic Bayesian Network to infer latent user features from observable feedback (backchannels, substantive contributions, typing behavior) and a non-stationary Markov Decision Process to select adaptive explanation strategies. The DBN continuously updates beliefs about user expertise, cognitive load, attentiveness, and cooperativeness based on feedback patterns. These beliefs condition the MDP's transition probabilities and rewards, creating a new decision problem at each time step. Monte Carlo Tree Search solves the MDP in real-time by searching within semantically related explanation blocks. The system uses a Neo4j knowledge graph storing domain information with per-triple Level of Understanding estimates, updating these values based on selected actions.

## Key Results
- Explanation length varied significantly by persona: mean iterations ranged from 134.76 (Hermione) to 481.83 (Neville)
- Strategy selection was strongly persona-dependent: "provide" strategy frequency ranged from 43.35% (Hermione) to 74.38% (Ron)
- Statistical significance: p-values < 1e-20 for differences in strategy distributions across personas
- Model successfully adapted to simulated behavior changes, though human validation is needed

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Bayesian Network for Partner Model Inference
The DBN enables continuous, probabilistic inference of latent user features from observable feedback patterns. User feedback (backchannels, substantive contributions, typing behavior) serves as evidence nodes; latent features (expertise, cognitive load, attentiveness, cooperativeness) are inferred through Bayesian filtering across time steps with feature dependencies modeled explicitly. Core assumption: Observable feedback patterns correlate reliably with latent user states, and these correlations are relatively stable across interactions.

### Mechanism 2: Non-stationary MDP with PM-Conditioned Transitions and Rewards
Modeling explanation as a non-stationary MDP enables adaptive decision policies that respond to evolving PM beliefs. The MDP's transition probabilities and reward functions are conditioned on current PM feature estimates; as the PM updates via DBN inference, the MDP is reformulated, creating a new decision problem suited to the current user state. Core assumption: The optimal explanation strategy genuinely varies by user features, and the reward function adequately captures explanation success.

### Mechanism 3: Semantic Block Decomposition with MCTS for Real-Time Solving
Decomposing explanations into semantically related blocks and solving via MCTS enables real-time decision-making while maintaining coherent structure. The MDP state space is restricted to conversationally valid moves within the current block; MCTS efficiently searches this reduced space to identify optimal action-move combinations, outputting top candidates that may be combined. Core assumption: Explanations decompose meaningfully into blocks, and locally optimal decisions within blocks yield globally coherent explanations.

## Foundational Learning

- **Dynamic Bayesian Networks (DBNs)**: Core inference engine for the partner model; understanding temporal belief updating is essential. Quick check: How does a DBN differ from a static BN, and what does "filtering" mean in this context?
- **Markov Decision Processes (MDPs)**: Formal framework for sequential decision-making; understanding states, actions, transitions, and rewards is foundational. Quick check: Given a state and action, how would you compute the expected cumulative reward for a fixed policy?
- **Non-stationary Decision Processes**: The key innovation enabling adaptivity; understanding why standard MDP solutions fail when transition/reward models change. Quick check: Why can't you pre-compute an optimal stationary policy for a non-stationary MDP?

## Architecture Onboarding

- **Component map**: DBN Partner Model -> Non-stationary MDP -> MCTS Solver -> Model Update -> NLG Component, with Neo4j Knowledge Graph storing domain information
- **Critical path**: 1) User provides feedback 2) Model Update extracts observables 3) DBN performs Bayesian update 4) New MDP constructed 5) MCTS solves MDP 6) Actions update Knowledge Base LoU; NLG generates utterance
- **Design tradeoffs**: PM feature granularity (4 features vs. richer models), MDP scope (block-level vs. full-dialog planning), cognitive load observables (typing behavior vs. linguistic features)
- **Failure signatures**: PM oscillation (wild swings in feature values), action stagnation (repeated ineffective moves), premature block transition (moves before grounding complete), unresponsive adaptation (no strategy adjustment despite clear feedback)
- **First 3 experiments**: 1) PM validation with synthetic users of known ground-truth features 2) Action distribution sanity check per persona matching Figure 7 patterns 3) Adaptation latency test with simulated behavior shifts

## Open Questions the Paper Calls Out

- **Question**: Does SNAPE-PM's adaptivity transfer effectively to real-world human-agent interactions? Basis: Authors note simulation feedback frequencies differ from human data, requiring further studies with real users. Evidence needed: User study results demonstrating successful adaptation with lower feedback frequencies.
- **Question**: How can the framework be extended to integrate full NLU and NLG components for complex input processing? Basis: Current implementation relies on binary feedback; full NLU needed to estimate cognitive load from linguistic input. Evidence needed: Functional pipeline integrating LLMs to extract features from open-ended user text.
- **Question**: How can the model utilize explicit user self-disclosures (e.g., "I play games often") to update partner beliefs? Basis: Current DBN infers features only from implicit feedback, not direct semantic assertions. Evidence needed: Extension mapping specific natural language claims directly to prior probabilities.

## Limitations

- DBN effectiveness depends on accuracy of feedback-to-feature mappings without empirical validation
- Non-stationary MDP solution via MCTS requires extensive hyperparameter tuning for real-time performance
- Knowledge graph structure and annotations significantly impact grounding, but details are sparse
- Human-subject validation remains necessary to confirm model effectiveness with real users

## Confidence

- **High confidence**: Overall architecture (DBN + non-stationary MDP + MCTS) is well-specified with clear evaluation demonstrating persona-based adaptation
- **Medium confidence**: DBN inference mechanism is formally defined but empirical validity of feedback-feature correlations untested
- **Low confidence**: Non-stationary MDP solution via MCTS is computationally feasible but optimality guarantees and hyperparameter sensitivity unclear

## Next Checks

1. **Empirical DBN validation**: Collect human-subject data with ground-truth expertise/engagement measures; compare inferred vs. actual PM features
2. **MDP hyperparameter sensitivity**: Systematically vary MCTS parameters (C, iterations, UCT bounds) and measure impact on explanation quality and runtime
3. **Knowledge graph annotation robustness**: Test system performance with synthetic noise in complexity/precondition annotations; assess degradation in grounding efficiency