---
ver: rpa2
title: 'Diffusion Active Learning: Towards Data-Driven Experimental Design in Computed
  Tomography'
arxiv_id: '2504.03491'
source_url: https://arxiv.org/abs/2504.03491
tags:
- diffusion
- learning
- active
- data
- reconstruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Diffusion Active Learning (DAL), a framework
  combining generative diffusion models with active learning for adaptive experimental
  design in computed tomography. DAL trains a diffusion model on domain-specific CT
  data to serve as a learned prior, then uses this model to approximate the posterior
  distribution conditioned on current measurements.
---

# Diffusion Active Learning: Towards Data-Driven Experimental Design in Computed Tomography

## Quick Facts
- **arXiv ID:** 2504.03491
- **Source URL:** https://arxiv.org/abs/2504.03491
- **Reference count:** 40
- **Primary result:** DAL achieves up to 4× reduction in measurements needed to reach PSNR of 30 dB, with up to 4.3× improvement in data efficiency compared to Laplace baseline on composite materials.

## Executive Summary
Diffusion Active Learning (DAL) combines generative diffusion models with active learning for adaptive experimental design in computed tomography. The method trains a diffusion model on domain-specific CT data to serve as a learned prior, then uses this model to approximate the posterior distribution conditioned on current measurements. By quantifying uncertainty in the current reconstruction estimate and selecting the most informative measurement angles, DAL aims to reduce data acquisition requirements and X-ray dose while improving image quality. The approach was evaluated on three real-world tomography datasets (integrated circuits, composite materials, and lung scans) and compared to uniform acquisition and active learning with dataset-agnostic generative models.

## Method Summary
DAL consists of two main phases: pre-training and active acquisition. First, an unconditional diffusion model is pre-trained on domain-specific CT reconstructions to learn a data-dependent prior that captures the structure of the underlying data distribution. During the active acquisition phase, the method starts with a few initial measurements and iteratively selects the next most informative measurement angle. At each step, the method samples k posterior images from the conditional posterior using the diffusion model with soft data consistency, computes the variance in measurement space across all candidate angles, and selects the angle that maximizes posterior total variance. This uncertainty sampling approach targets measurements that most reduce posterior uncertainty in the reconstruction.

## Key Results
- DAL achieves up to 4× reduction in measurements needed to reach a target PSNR of 30 dB compared to uniform acquisition
- For the chip dataset, DAL reached PSNR scores of 32.9 dB with only 50 measurements, outperforming other methods
- DAL requires less than two minutes per step at 512×512 resolution, significantly faster than competing methods
- Gains were particularly pronounced for highly structured datasets, while for isotropic datasets like lung scans, active learning showed no clear advantage over uniform acquisition

## Why This Works (Mechanism)

### Mechanism 1: Learned Prior Captures Domain-Specific Structure
A diffusion model pre-trained on domain-specific CT reconstructions provides a prior that enables more efficient experimental design than dataset-agnostic methods. The diffusion model learns the score function ∇xt log pt(xt) of the data distribution, capturing regularities (e.g., chip interconnects, fiber orientations). During reconstruction, this prior constrains the solution space more effectively than generic regularizers because it encodes which structures are plausible within the domain.

### Mechanism 2: Posterior Sampling via Soft Data Consistency
Early-stopped gradient descent for data consistency achieves comparable reconstruction quality to full optimization while reducing computational cost. At reverse diffusion step t, Tweedie's formula yields a denoised estimate x̂0(xt). Rather than fully solving the measurement-consistency optimization, the method performs a fixed number of SGD steps. Since optimization is initialized from x̂0(xt), the result retains learned prior features while enforcing measurement consistency.

### Mechanism 3: Uncertainty Sampling in Measurement Space
Selecting measurement angles that maximize posterior variance in projection space reduces measurements needed for target reconstruction quality. At step t, sample k images from the conditional posterior using the diffusion model. Apply the forward model to each sample, yielding predicted projections. Select the angle maximizing sample variance in projection space. This targets measurements that most reduce posterior uncertainty.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** DAL uses a pre-trained DDPM as the generative prior. You must understand the forward noising process, reverse denoising process, and score function training to implement the pre-training step and modify the reverse process for conditional sampling.
  - **Quick check question:** Given a noisy sample xt at time t, how would you compute a denoised estimate x̂0 using Tweedie's formula and a trained score network?

- **Concept: Bayesian Experimental Design / Active Learning**
  - **Why needed here:** DAL is an instance of uncertainty sampling. Understanding the Bayesian framework (prior, likelihood, posterior) and information-theoretic acquisition functions helps you understand why variance maximization is justified and when alternative acquisition functions might apply.
  - **Quick check question:** In a Bayesian setting with posterior p(x|Y), why does measuring where the posterior predictive variance is highest tend to be informative?

- **Concept: Radon Transform and CT Forward Model**
  - **Why needed here:** The forward model Aψ (parallel-beam Radon transform) maps images to projections. Computing uncertainty in measurement space requires applying Aψ to posterior samples efficiently.
  - **Quick check question:** For a 128×128 image and detector with 128 pixels, what is the dimensionality of the Radon transform operator for a single angle ψ?

## Architecture Onboarding

- **Component map:** Pre-train DDPM on domain CT slices → Active learning loop: sample k posteriors → compute variance for all candidate angles → select and acquire → update measurement set → repeat

- **Critical path:** Pre-train diffusion model on ~10k+ domain-specific CT slices → Initialize with 1-5 random measurements → For each acquisition step: sample k posteriors → compute variance across 180 candidate angles → select max-variance angle → acquire measurement → repeat

- **Design tradeoffs:** Pixel-space vs. Latent-space diffusion (paper uses pixel-space); Number of posterior samples k (more samples improve variance estimation but linearly increase compute); Pre-scan vs. cold start (low-resolution pre-scan improves initial estimates for SWAG/Ensemble but less for diffusion)

- **Failure signatures:** Isotropic/unstructured data (no active learning benefit); Out-of-distribution samples (reconstruction bias); Insufficient training data (diffusion prior quality depends on representative training data)

- **First 3 experiments:** (1) Reproduce on Chip dataset (128×128): train diffusion model, run DAL vs. uniform acquisition, verify ~2× measurement reduction for 30 dB PSNR target; (2) Ablate number of posterior samples: test k ∈ {5, 10, 20, 50} on validation set, plot PSNR vs. measurements; (3) Test on your domain data: apply to your own CT dataset, assess structure by computing empirical angular variance of features

## Open Questions the Paper Calls Out

- **Question:** How does Diffusion Active Learning perform on real-world CT acquisitions with measurement noise, sample misalignment, and other experimental artifacts?
  - **Basis:** Despite effort to make evaluation realistic, a sim-to-real gap remains unaddressed, e.g., by considering distribution shifts in the testing distribution.

- **Question:** Can the reconstruction bias introduced by the learned diffusion prior be quantified and mitigated when samples contain out-of-distribution features such as defects or anomalies?
  - **Basis:** The key advantage of a learned prior comes at the cost of introducing reconstruction bias when the measured sample is not contained in the training distribution, particularly relevant for detecting small deviations or defects.

- **Question:** Can pre-trained foundation models or transfer learning approaches reduce the requirement for domain-specific training data while maintaining DAL's performance gains?
  - **Basis:** A possible way forward is to pre-train large foundation models on a variety of tomographic images, or use pre-trained models such as Stable Diffusion for fine-tuning on much smaller datasets.

## Limitations

- Performance gains depend heavily on domain-specific structure; no advantage for isotropic data like lung scans
- Diffusion model architecture and training hyperparameters are underspecified, making exact reproduction challenging
- Greedy one-step acquisition strategy may be suboptimal compared to multi-step planning approaches

## Confidence

- **High Confidence:** Mechanism 1 (learned prior captures domain structure) - well-supported by ablation and cross-dataset comparison showing gains correlate with dataset structure
- **Medium Confidence:** Mechanism 2 (soft data consistency with early stopping) - comparable reconstruction quality demonstrated, but no ablation of optimization parameters or noise robustness testing
- **Medium Confidence:** Mechanism 3 (uncertainty sampling via variance maximization) - theoretical justification exists, but greedy strategy not compared to multi-step planning or alternative acquisition functions

## Next Checks

1. **Ablation on posterior sample count:** Systematically test k ∈ {5, 10, 20, 50} on validation set to identify compute-quality trade-off point for your specific domain

2. **Cross-domain generalization test:** Apply DAL to your CT dataset and compute empirical angular variance of features to predict whether active learning will help (high variance → expect gains; isotropic → expect minimal benefit)

3. **Baseline comparison completeness:** Implement and compare against not just SWAG/Laplace, but also multi-step lookahead or RL-based angle selection to verify greedy uncertainty sampling is sufficient for your application