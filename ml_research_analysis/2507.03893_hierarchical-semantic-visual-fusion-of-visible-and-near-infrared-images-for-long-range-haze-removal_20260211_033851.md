---
ver: rpa2
title: Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for
  Long-range Haze Removal
arxiv_id: '2507.03893'
source_url: https://arxiv.org/abs/2507.03893
tags:
- image
- semantic
- haze
- dehazing
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenging task of long-range haze removal,
  where severe signal degradation and detail loss make it impractical to recover distant
  scene details from degraded visible images alone. The authors propose a Hierarchical
  Semantic-Visual Fusion (HSVF) framework that leverages complementary cues from visible
  and near-infrared modalities.
---

# Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for Long-range Haze Removal

## Quick Facts
- arXiv ID: 2507.03893
- Source URL: https://arxiv.org/abs/2507.03893
- Reference count: 40
- Primary result: HSVF achieves FADE 0.7392 and NIQE 4.2162 on VNHD dataset, outperforming state-of-the-art long-range haze removal methods

## Executive Summary
This paper addresses the challenging task of long-range haze removal, where severe signal degradation and detail loss make it impractical to recover distant scene details from degraded visible images alone. The authors propose a Hierarchical Semantic-Visual Fusion (HSVF) framework that leverages complementary cues from visible and near-infrared modalities. The framework consists of two streams: a semantic stream that reconstructs haze-free scenes using modality-invariant semantics as priors, and a visual stream that recovers fine structural details through multimodal fusion. Extensive experiments on a novel real-world VNHD dataset demonstrate that HSVF outperforms state-of-the-art methods, achieving superior FADE and NIQE scores compared to existing dehazing and fusion-based approaches.

## Method Summary
The HSVF framework uses a two-stream architecture. The semantic stream employs intrinsic semantic alignment to disentangle features into modality-specific style spaces and a modality-invariant semantic space, producing haze-robust semantic predictions that serve as priors for clear scene reconstruction. The visual stream uses joint self- and cross-attention mechanisms to integrate complementary visual features from both modalities. The framework is trained in a staged manner: semantic alignment (200 epochs), reconstruction and fusion (400 epochs each), followed by joint fine-tuning (200 epochs). The approach leverages hardware-level pixel alignment between visible and NIR images captured simultaneously.

## Key Results
- HSVF achieves FADE score of 0.7392 and NIQE score of 4.2162 on VNHD dataset, outperforming state-of-the-art dehazing and fusion methods
- The intrinsic semantic alignment module improves segmentation mIoU from 0.7623 to 0.7865 compared to visible-only training
- Joint self- and cross-attention mechanisms outperform single-attention approaches on MI (3.2548), VIF (0.9523), and QAB/F (0.6419) metrics

## Why This Works (Mechanism)

### Mechanism 1
Aligning semantic representations across visible and NIR modalities produces haze-robust semantic priors that enable clear scene reconstruction in regions where visible-only information is irrecoverably degraded. The Intrinsic Semantic Alignment module disentangles features into modality-specific style spaces and a modality-invariant semantic space. Content encoders map both modalities into a shared latent space where feature consistency is enforced via L1 alignment loss, enabling a shared segmentation decoder to predict semantics that are resilient to haze degradation. Core assumption: Visible and NIR images share consistent intrinsic scene semantics (e.g., "sky" remains semantically identical across modalities) regardless of appearance differences.

### Mechanism 2
Region-level semantic discriminators provide more faithful cross-domain knowledge transfer than image-level or feature-level translation for long-range haze removal. The Cross-domain Semantic Reconstruction module uses semantic region-aware discriminators that decompose images into category-specific regions (building, vegetation, sky). Each dedicated discriminator learns category-specific discriminability, enabling fine-grained adversarial supervision that transfers clear-domain knowledge per semantic class. Core assumption: Semantic categories have distinct structural/appearance patterns that can be learned separately and reconstructed faithfully under severe degradation.

### Mechanism 3
Joint self- and cross-attention mechanisms enable adaptive disentanglement and fusion of modality-invariant and modality-specific visual features. The Cross-modal Visual Fusion module computes both intra-modal self-attention (preserving modality-specific structure) and inter-modal cross-attention (transferring complementary cues). Features from both attention branches are summed to produce fused representations that preserve comprehensive visual structures. Core assumption: NIR images preserve haze-resilient structural details that can complement visible information lost to long-range scattering.

## Foundational Learning

**Atmospheric Scattering Model** (I(x) = J(x)t(x) + A(1-t(x))): Long-range haze increases scattering coefficient β and depth d(x), making transmission t(x) approach zero and losing scene information irreversibly. Quick check: Explain why visible-only dehazing fails when t(x) ≈ 0 for distant regions.

**Modality-Invariant Representation Learning**: The framework requires separating semantics (category information) from style (modality-specific appearance) to leverage NIR for robust semantic prediction. Quick check: How does disentanglement differ from simple feature concatenation for cross-modal learning?

**Joint Self- and Cross-Attention**: Understanding how query/key/value projections enable both intra-modal structure preservation and inter-modal complementary transfer. Quick check: What information does self-attention capture that cross-attention cannot, and vice versa?

## Architecture Onboarding

**Component map**: Semantic Stream: VIS/NIR inputs → Content Encoders (HRNet) → Shared Semantic Space → Segmentation Decoder → Aligned Semantics → Reconstruction Generator (SPADE) → Semantic Region Discriminators → O_SR. Visual Stream: VIS/NIR inputs → Feature Extraction → Q/K/V Projections → Joint Attention (Eq. 8-9) → Visual Fusion Decoder → O_VF. Final Stage: O_SR + O_VF → Final Generator G_Final → O_Final.

**Critical path**: (1) Pretrain semantic alignment module (200 epochs with segmentation supervision) → (2) Pretrain cross-domain reconstruction and visual fusion (400 epochs each) → (3) Joint fine-tuning (200 epochs). Without staged pretraining, the paper notes optimization instability.

**Design tradeoffs**: Increasing reconstruction loss weight (α₁) improves FADE (haze removal) but degrades NIQE (texture quality); increasing fusion weight (β₁) preserves details but leaves residual haze. Empirically balanced at α₁=1, β₁=0.1.

**Failure signatures**: Semantic misalignment: Segmentation produces incorrect boundaries in distant regions → reconstruction with wrong textures (buildings with vegetation patterns). Visual-only mode: High contrast achieved but texture-poor outputs. Semantic-only mode: Residual haze persists with good local contrast but poor global coherence.

**First 3 experiments**: (1) Ablate intrinsic semantic alignment: Train segmentation with VIS-only vs. VIS+NIR with/without alignment loss to quantify mIoU improvement. (2) Attention mechanism comparison: Compare self-only, cross-only, and joint attention on visual fusion metrics (MI, VIF, QAB/F). (3) Loss weight sensitivity: Sweep α₁ ∈ {0, 0.01, 0.1, 1, 10} with fixed β₁=0.1 to observe FADE/NIQE tradeoff and identify optimal operating point.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the HSVF framework achieve real-time inference speeds suitable for practical video surveillance applications without significant quality degradation? The authors acknowledge the primary focus has been on improving restoration quality rather than optimizing inference speed, reporting 3.30 seconds per 1024×1024 image.

**Open Question 2**: Can the balance between semantic reconstruction and visual fusion losses be adaptively determined based on scene-specific haze density and depth characteristics rather than fixed hyperparameters? Table IV and Figure 11 show different weight ratios produce fundamentally different trade-offs, but the authors empirically set α₁=1, β₁=0.1 without providing an adaptive mechanism.

**Open Question 3**: How does HSVF performance degrade when visible-NIR image pairs exhibit temporal or spatial misalignment due to dynamic scene content or non-simultaneous capture? The paper emphasizes using specialized camera with hardware-level pixel alignment but real-world deployment may involve moving objects or asynchronous capture systems.

## Limitations
- VNHD dataset is not publicly available, preventing full reproduction of reported results
- Claims about framework's performance on general long-range haze scenarios beyond VNHD dataset are not independently verifiable
- Framework assumes consistent semantic representations across visible and NIR modalities, which may not hold when NIR reveals structural information invisible in RGB

## Confidence

**High Confidence**: The architectural framework and component relationships (two-stream design with semantic alignment and joint attention) are clearly specified. The staged training procedure and loss formulations are well-documented.

**Medium Confidence**: The ablation studies demonstrate effectiveness of individual components, though results are dataset-dependent. The claimed superiority over state-of-the-art methods is supported by metrics but limited by dataset accessibility.

**Low Confidence**: Claims about the framework's performance on general long-range haze scenarios beyond the VNHD dataset are not independently verifiable.

## Next Checks

1. **Dataset Accessibility Verification**: Contact authors to request access to VNHD dataset, or identify publicly available long-range haze datasets with visible-NIR pairs for independent validation.

2. **Semantic Alignment Robustness Test**: Evaluate semantic alignment performance on RGB-NIRScene/RANUS datasets by measuring segmentation mIoU and analyzing whether NIR-specific structural information causes semantic misalignment.

3. **Component Ablation Reproduction**: Reimplement the core components (intrinsic semantic alignment, joint attention fusion) and reproduce the ablation studies from Tables II-IV to verify the claimed improvements in segmentation accuracy and fusion quality.