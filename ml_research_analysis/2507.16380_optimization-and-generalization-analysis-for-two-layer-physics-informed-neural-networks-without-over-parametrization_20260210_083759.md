---
ver: rpa2
title: Optimization and generalization analysis for two-layer physics-informed neural
  networks without over-parametrization
arxiv_id: '2507.16380'
source_url: https://arxiv.org/abs/2507.16380
tags:
- training
- function
- loss
- neural
- wwwi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic gradient descent (SGD) training of
  two-layer physics-informed neural networks (PINNs) for solving Poisson's equation
  without requiring over-parameterization. The key innovation is to avoid the typical
  over-parameterization assumption by introducing a function class F for the target
  function and its discretization Fm.
---

# Optimization and generalization analysis for two-layer physics-informed neural networks without over-parametrization

## Quick Facts
- arXiv ID: 2507.16380
- Source URL: https://arxiv.org/abs/2507.16380
- Reference count: 3
- This paper studies stochastic gradient descent (SGD) training of two-layer physics-informed neural networks (PINNs) for solving Poisson's equation without requiring over-parameterization.

## Executive Summary
This paper presents theoretical analysis showing that two-layer PINNs can be trained effectively without over-parameterization by assuming the target PDE solution belongs to a specific Barron-like function class. The key innovation is decoupling network width from training sample size, requiring width to scale only with target accuracy rather than data size. The authors prove that SGD can decrease both training loss and expected risk below O(ε) when width exceeds a threshold dependent only on ε and the problem, not N. Numerical experiments on 3D Poisson equations validate these theoretical predictions.

## Method Summary
The method solves Poisson's equation on a unit ball using a two-layer PINN with architecture φ(x) = (||x||²-1)Σaᵢσ(wᵢᵀx+bᵢ), where σ is ReLU³ activation. Only the first-layer weights wᵢ are trained (aᵢ, bᵢ are fixed), satisfying Dirichlet boundary conditions automatically through the (||x||²-1) factor. The loss function minimizes the PDE residual |Δφ(x)-f(x)|² using SGD for T=10⁶ iterations. Width m ∈ {100, 1000, 10000} and sample sizes N ∈ {100, 1000, 10000} are tested. Theoretical analysis relies on function class approximation, pseudo-network tracking, and Rademacher complexity bounds.

## Key Results
- Training loss can be decreased below O(ε) when network width exceeds a threshold dependent only on ε and the problem, not on training sample size N
- Expected risk (generalization error) can be decreased below O(ε) when training data size is sufficiently large relative to width and accuracy requirements
- Numerical experiments show training loss reaching 10⁻³ to 10⁻⁴ levels, consistent with theoretical predictions
- Results suggest PINNs can achieve good performance without prohibitively wide networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If the target function belongs to a specific Barron-like function class F, the required network width depends on the target accuracy ε and the function norm ||f||_F, rather than the number of training samples N.
- **Mechanism:** The authors define an infinite-width function space F and its discretization Fₘ. By assuming f ∈ F, they prove random feature mapping can approximate f using width m that scales with 1/ε, decoupling model size from data size.
- **Core assumption:** The target function f belongs to the function class F and satisfies f(0) = 0 (or is shifted to do so).
- **Evidence anchors:** [abstract] ("network width exceeds a threshold that depends only on ε and the problem"), [section 2.4] (Definition of function class F and norm ||·||_F)
- **Break condition:** If the target PDE solution contains high-frequency components or discontinuities outside class F, width-accuracy coupling may break, requiring width to scale with N or failing to converge.

### Mechanism 2
- **Claim:** Stochastic Gradient Descent (SGD) succeeds by keeping trained network parameters close to a "pseudo-network" (linearized approximation) that already approximates target function well.
- **Mechanism:** Analysis tracks distance between actual learner network ψ and pseudo-network g (linear in W). Theorem 3.3 bounds distance and gradient differences between them. SGD behaves like optimizing convex pseudo-network, avoiding spurious local minima.
- **Core assumption:** Gradients remain bounded during training (Assumption 3.1), preventing exploding gradient phenomenon common in PINNs.
- **Evidence anchors:** [section 3.1] (Definition of pseudo-network g and approximation properties), [section 3.2] (Theorem 3.3 bounding distance between ψ and g)
- **Break condition:** If learning rate η is too large or iteration count T exceeds thresholds, trajectory diverges from pseudo-network, causing optimization instability.

### Mechanism 3
- **Claim:** Generalization is guaranteed by Rademacher complexity bounds which remain controlled even as width increases, provided sample size N is sufficient.
- **Mechanism:** Authors derive Rademacher complexity for PINN class (Theorem 4.1). Complexity scales as O(m⁻ᵅτ'/√N), increasing width does not degrade generalization indefinitely. Ensures expected risk tracks training loss.
- **Core assumption:** Loss function is Lipschitz continuous and training data is i.i.d.
- **Evidence anchors:** [section 4] (Theorem 4.2 establishing generalization bounds), [section 5] (Numerical validation showing loss convergence regardless of N)

## Foundational Learning

- **Concept:** Over-parameterization vs. Function Class Assumptions
  - **Why needed here:** Standard theory requires width to scale polynomially with data (m ∝ Nᵖ), making training expensive. This paper shifts paradigm by constraining target function (assuming it is in F) rather than model size, requiring understanding if PDE solution fits "Barron-style" smoothness criteria.
  - **Quick check question:** Does your PDE solution u have a representation as integral of random features (like polynomials vanishing at zero), or does it require high-frequency/non-smooth components?

- **Concept:** Neural Tangent Kernel (NTK) / Linearization
  - **Why needed here:** "Pseudo-network" is essentially network in NTK regime. Understanding optimization proof relies on network staying in linearized regime (weights staying close to initialization) is crucial for interpreting theoretical guarantees.
  - **Quick check question:** If weights move significantly from initialization (large τ'), does Theorem 3.3 guarantee still hold strongly?

- **Concept:** Hard Constraint Architecture
  - **Why needed here:** Paper uses specific architecture φ(x) = (||x||²-1)φ̃(x) to automatically satisfy boundary conditions on unit ball.
  - **Quick check question:** Can you map your problem domain to unit ball, or do you need different distance function to enforce boundary conditions hard?

## Architecture Onboarding

- **Component map:** Input x ∈ Γ (Unit ball) -> ReLU³ activation (Width m) -> Trainable Params W (First-layer weights) -> Hard constraint wrapper φ(x) = (||x||²-1)Σaᵢσ(wᵢᵀx+bᵢ) -> Loss: |Δφ(x)-f(x)|²

- **Critical path:**
  1. Initialize parameters (Uniform distribution, crucial scaling m⁻ᵅ, m⁻ᵝ)
  2. Freeze aᵢ, bᵢ
  3. Construct loss using Laplacian of network (requires 2nd order autodiff)
  4. Update W via SGD

- **Design tradeoffs:**
  - **ReLU³ vs ReLU:** ReLU³ provides smoothness required for Poisson equation (Δu). Standard ReLU would have zero second derivative almost everywhere.
  - **Training W only:** Reduces computational cost and stabilizes theoretical analysis, but may limit network's ultimate capacity to fit complex functions compared to full backprop.

- **Failure signatures:**
  - **Spectral Bias:** If solution requires high frequencies, "narrow" network might fail to converge, contradicting assumption f ∈ F.
  - **Gradient Explosion:** If initialization scales or learning rates violate theoretical assumptions, gradient of Laplacian loss may explode, violating boundedness assumption (Assumption 3.1).

- **First 3 experiments:**
  1. **Sanity Check (1D/2D Poisson):** Implement specific architecture on standard Poisson equation. Verify training loss drops below O(ε) with width m ≈ 100-1000 regardless of increasing N.
  2. **Width Scaling:** Fix N=1000 and vary m (e.g., 100, 500, 1000). Confirm threshold behavior where loss drops only after m exceeds problem-dependent size.
  3. **Ablation on Trainable Parameters:** Compare training only W vs training all parameters (W, a, b). Check if training all parameters offers faster convergence but higher instability, or if theoretical guarantee on width independence only holds for W-only case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimization and generalization analysis for PINNs be extended to deep neural networks (e.g., three layers) without over-parameterization?
- Basis in paper: [explicit] The Conclusion states that a limitation is the focus on shallow PINNs and suggests "future work could consider the behavior of gradient descent in training slightly deeper (e.g., three-layer) networks."
- Why unresolved: Training deep networks involves distinct gradient representations for outer versus inner layers, making dynamics fundamentally different from two-layer case analyzed in this paper.
- What evidence would resolve it: A theoretical extension of Theorems 3.4 and 4.2 providing convergence bounds for SGD in multi-layer PINNs.

### Open Question 2
- Question: Can this framework be adapted for PDEs defined on general, complex domains rather than the unit ball?
- Basis in paper: [explicit] The Conclusion identifies restriction to unit ball as limitation and notes that "future work could also be studying PDEs on general domains with types of boundary or initial conditions."
- Why unresolved: Current analysis relies on specific network architecture that automatically satisfies boundary conditions on unit ball, simplifying loss function to single term; general domains require handling coupled loss terms (e.g., residual + boundary), complicating analysis.
- What evidence would resolve it: Derivation of convergence and generalization bounds for PINNs solving PDEs on arbitrary geometries with composite loss functions.

### Open Question 3
- Question: Under what theoretical conditions is the assumption that SGD weights and network outputs remain bounded during training valid?
- Basis in paper: [explicit] Section 3 introduces an assumption that ||wᵢ⁽ᵗ⁾|| ≤ O(1) and explicitly states, "at present, we cannot provide a theoretical guarantee that the above assumption is valid."
- Why unresolved: Paper relies on this assumption to prove optimization results but currently depends on practical hyperparameter tuning to prevent exploding gradients rather than formal theoretical guarantee.
- What evidence would resolve it: A rigorous proof showing that SGD update rules inherently constrain parameter growth within specified iteration limits without manual intervention.

## Limitations
- The theoretical guarantees rely heavily on the assumption that the target PDE solution belongs to the Barron-like function class F, which may not hold for problems with high-frequency components or discontinuities.
- Learning rate specification is incomplete in experimental section, with only theoretical scaling Θ(ε/m) provided rather than concrete value.
- Generalization bounds depend on sample size N exceeding problem-dependent thresholds, but these thresholds are not explicitly characterized for practical settings.

## Confidence

- **High Confidence:** The optimization analysis (Mechanism 2) showing SGD's behavior near the pseudo-network is well-supported by bounded gradient assumptions and tracking arguments.
- **Medium Confidence:** The function class approximation results (Mechanism 1) are mathematically sound but depend on applicability of Barron-like representations to real PDE solutions.
- **Medium Confidence:** The generalization bounds (Mechanism 3) follow standard Rademacher complexity arguments but require careful verification that Lipschitz assumptions hold for specific loss landscape.

## Next Checks

1. Test the method on a PDE solution known to have high-frequency components (e.g., oscillatory solutions) to verify if width-independence breaks down as predicted.
2. Systematically vary the learning rate across several orders of magnitude to identify the stability threshold and confirm it aligns with theoretical predictions.
3. Compare training only the first-layer weights versus training all parameters to determine if the theoretical width-independence guarantee holds only for the constrained optimization case.