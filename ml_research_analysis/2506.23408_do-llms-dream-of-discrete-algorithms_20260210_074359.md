---
ver: rpa2
title: Do LLMs Dream of Discrete Algorithms?
arxiv_id: '2506.23408'
source_url: https://arxiv.org/abs/2506.23408
tags:
- reasoning
- data
- llms
- agent
- acquirer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of Large Language Models (LLMs)
  in domains requiring strict logical reasoning and discrete decision-making. The
  authors propose a neurosymbolic approach that integrates LLMs with Prolog-based
  logic reasoning modules, enabling structured problem decomposition and verifiable
  solutions.
---

# Do LLMs Dream of Discrete Algorithms?

## Quick Facts
- arXiv ID: 2506.23408
- Source URL: https://arxiv.org/abs/2506.23408
- Reference count: 0
- This paper proposes a neurosymbolic framework integrating LLMs with Prolog logic reasoning to improve multi-step reasoning on discrete domains.

## Executive Summary
This paper addresses the limitations of Large Language Models (LLMs) in domains requiring strict logical reasoning and discrete decision-making. The authors propose a neurosymbolic approach that integrates LLMs with Prolog-based logic reasoning modules, enabling structured problem decomposition and verifiable solutions. By composing predefined functions and leveraging first-order logic, the framework mitigates common LLM failures such as hallucination and incorrect step decomposition. Experiments on the DABStep benchmark demonstrate improved precision, coverage, and interpretability in multi-step reasoning tasks, highlighting the potential of combining probabilistic and symbolic reasoning for building reliable, trustworthy AI agents in complex domains.

## Method Summary
The method involves augmenting LLMs with Prolog predicates to enable verifiable step decomposition. The LLM translates natural language queries into Prolog predicates with facts/rules, which are then executed by a Prolog engine using first-order logic. The framework includes an evaluation-guided self-reflection scoring system that induces LLMs to self-correct toward valid Prolog compositions. The approach restricts LLMs to compose predefined, verified tools rather than generating arbitrary code, improving reliability and security.

## Key Results
- Improved precision and coverage in multi-step reasoning tasks compared to baseline LLM approaches
- Demonstrated 4% performance gain on the DABStep benchmark (though statistical significance not reported)
- Enhanced interpretability through structured Prolog-based problem decomposition
- Mitigation of common LLM failure modes such as hallucination and incorrect step decomposition

## Why This Works (Mechanism)

### Mechanism 1: Logic-Based Query Decomposition
Augmenting LLMs with Prolog predicates enables verifiable step decomposition that mitigates hallucination in multi-step reasoning tasks. The LLM translates natural language queries into Prolog predicates with facts/rules, the Prolog engine executes using first-order logic (including negation-as-failure), and verified results are returned to the user. This approach assumes LLMs can reliably generate syntactically valid Prolog even when they cannot perform the discrete reasoning themselves.

### Mechanism 2: Evaluation-Guided Self-Reflection Scoring
Embedding explicit evaluation criteria in the planner prompt induces LLMs to self-correct toward valid Prolog compositions. The prompt includes a rubric starting at 1.0 with specific penalties for various errors, and the LLM maximizes this score during generation. This assumes LLMs can accurately self-assess against explicit criteria and modify output accordingly.

### Mechanism 3: Modular Tool Composition Over Unconstrained Code Generation
Restricting LLMs to compose predefined, verified tools improves reliability and security compared to arbitrary code generation. The system provides a fixed function library that the LLM stitches together via Prolog predicates, with no arbitrary code execution. This assumes the predefined toolset sufficiently covers the task domain and that composition is cognitively simpler than generation.

## Foundational Learning

- **First-Order Logic (FOL) with Quantifiers and Negation**
  - Why needed here: The framework translates queries into FOL with ∀, ∃, and negation-as-failure; understanding these is essential for debugging Prolog output.
  - Quick check question: Given `acquirer_country(X, us)` and `acquirer_country(Y, gb)`, write a Prolog rule for "X and Y are acquirers in different countries."

- **Prolog Execution Model (Backtracking and Unification)**
  - Why needed here: Prolog's unification and backtracking differ from imperative execution; debugging requires tracing how predicates resolve.
  - Quick check question: What does `\+ acquirer_country(X, Z)` mean, and how does negation-as-failure behave if `X` is uninstantiated?

- **AI Agent Architecture (Core, Memory, Planner, Tools)**
  - Why needed here: The system implements this four-component structure; onboarding requires knowing where Prolog fits.
  - Quick check question: Which component would cache successful query-to-Prolog mappings for reuse across sessions?

## Architecture Onboarding

- **Component map:** User Query → Agent Core → Planner (generates Prolog) → Prolog Engine ← Facts/Rules DB → Foreign Functions → Response Aggregation → User

- **Critical path:**
  1. Define seed questions → run Algorithm 1 to expand and extract MVC components
  2. Implement foreign function interfaces matching extracted tool signatures
  3. Load domain facts into Prolog; define rules with explicit domains for negation
  4. Configure planner prompt with function list, schema, examples, and evaluation rubric
  5. Test on DABStep dev set; iterate on tool coverage

- **Design tradeoffs:**
  - Prolog vs. Z3/SAT: Prolog is more interpretable but FOL reasoning can be undecidable
  - In-memory facts vs. external DB: Prolog facts enable negation; large datasets may require hybrid approach
  - Closed-world assumption: Negation-as-failure requires complete domain enumeration; incomplete domains yield incorrect negatives

- **Failure signatures:**
  - Evaluation score drops to 0.0 → LLM called foreign function with uninstantiated input
  - Planner returns "gaps" field populated → required tool not in library
  - Prolog query hangs → unbounded search space (missing domain constraints)
  - Incorrect negation results → domain predicates not fully enumerated

- **First 3 experiments:**
  1. Replicate DABStep sample: Run "Which card scheme had the highest average fraud rate in 2023?" through full pipeline; verify output matches `SwiftCharge`.
  2. Ablation: Remove evaluation rubric from prompt; compare success rate on 20 multi-hop queries.
  3. Negation stress test: Query "merchants NOT in same country as their acquirer"; verify `not_in_same_country/2` returns correct set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs autonomously distinguish when to apply probabilistic semantic associations versus strict logical constraints without external enforcement?
- Basis in paper: The authors explicitly ask, "We humans, understand clearly when to switch between probabilistic reasoning and logical reasoning. Can LLMs do the same?" (Page 6).
- Why unresolved: While the paper enforces logic via Prolog, it does not demonstrate that the LLM intrinsically "understands" the boundary; it is guided by the neurosymbolic architecture.
- What evidence would resolve it: Experiments where LLMs must classify query types (probabilistic vs. discrete) before tool invocation, or success in dynamically switching modes without rigid prompting.

### Open Question 2
- Question: Is it feasible to express full domains for negation and quantifiers in a Prolog-augmented system when scaling to very large databases?
- Basis in paper: The paper notes, "expressing domains for very large databases may be infeasible," forcing a trade-off where the system might only accept instantiated variables as inputs (Page 11).
- Why unresolved: The proposed method relies on listing facts or domains for logical negation, which faces memory/computation limits on industrial-scale data.
- What evidence would resolve it: Successful implementation of the framework on datasets orders of magnitude larger than DABStep, specifically testing queries requiring negation over massive domains.

### Open Question 3
- Question: Does the LLM's prompt-based self-evaluation metric reliably predict the logical validity of the generated Prolog code?
- Basis in paper: The methodology relies on a prompt instructing the LLM to assign an "evaluation" score (0.0 to 1.0) based on logical constraints (Page 24).
- Why unresolved: It remains unclear if this self-reflection loop is robust or if the LLM can "game" the score without actually correcting logical errors like hallucinated predicates.
- What evidence would resolve it: A correlation analysis comparing the LLM's self-assigned evaluation scores against ground-truth execution success rates.

## Limitations

- The evaluation rubric is used only as an in-context guide; no experimental ablation shows whether it actually improves outcomes.
- The system relies entirely on LLM-generated Prolog; the paper never validates Prolog generation correctness separately from end-to-end task performance.
- No statistical significance testing is reported for performance improvements; the 4% DABStep gain is presented without variance or confidence intervals.

## Confidence

- **High confidence**: The modular Prolog-LLM integration is technically sound and reproducible; the data pipeline (CSV → Prolog facts) is straightforward.
- **Medium confidence**: The claimed 4% performance gain on DABStep; the exact LLM model and prompt details are missing, which materially affects reproducibility.
- **Low confidence**: That the evaluation rubric meaningfully improves LLM output; no ablation or control experiment is provided.

## Next Checks

1. **Ablation on evaluation rubric**: Run planner with and without the scoring rubric on 20+ multi-step queries; measure success rate and correctness.
2. **Prolog syntax validation**: Log LLM-generated Prolog before execution; check whether malformed syntax is a major failure mode.
3. **Negation domain completeness**: For queries with `\+`, manually verify that the domain predicates are fully enumerated; quantify false negatives due to incomplete enumeration.