---
ver: rpa2
title: 'Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic
  Reasoning'
arxiv_id: '2509.18316'
source_url: https://arxiv.org/abs/2509.18316
tags:
- reasoning
- path
- reward
- training
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates using reward modeling as a reasoning framework
  to enhance diagnostic reasoning over biomedical knowledge graphs. We propose fine-tuning
  large language models to judge the validity of knowledge graph paths for clinical
  diagnosis, inspired by computational theory that verification can be easier than
  generation.
---

# Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning

## Quick Facts
- **arXiv ID:** 2509.18316
- **Source URL:** https://arxiv.org/abs/2509.18316
- **Reference count:** 12
- **Key outcome:** Reward modeling improves path-judging performance but shows limited transferability to downstream diagnostic tasks.

## Executive Summary
This work investigates using reward modeling as a reasoning framework to enhance diagnostic reasoning over biomedical knowledge graphs. We propose fine-tuning large language models to judge the validity of knowledge graph paths for clinical diagnosis, inspired by computational theory that verification can be easier than generation. Through systematic evaluation of five task formulations and eight training paradigms across three open-source models, we find that direct preference optimization and reasoning distillation improve path-judging performance. However, the transferability to downstream diagnostic tasks such as diagnosis summarization and medical question answering remains limited, revealing both the promise and brittleness of this approach.

## Method Summary
The paper investigates reward modeling for diagnostic reasoning over biomedical knowledge graphs by fine-tuning LLMs to judge the validity of KG paths connecting patient findings to diagnoses. Using UMLS as the KG and ProbSum/MedQA as clinical datasets, the method constructs paths via 2-hop DFS from QuickUMLS-extracted entities. Five tasks (Path Selection, Next-Hop Prediction, etc.) are evaluated with eight training paradigms (SFT, DPO, GRPO, DSS, etc.) across three models (Qwen2.5, Qwen3, Gemma). The top performer uses Distilling Step-by-Step with GRPO, combining rationale generation with group-relative preference optimization.

## Key Results
- Reward modeling (particularly DSS+GRPO) significantly improves path-judging performance over standard fine-tuning.
- Models achieve high ROUGE-L scores on path selection tasks but show minimal improvement on downstream diagnostic tasks.
- The brittleness of transfer suggests verification skills don't necessarily confer diagnostic reasoning capabilities.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Verifying a diagnostic path is computationally more tractable than generating one from scratch.
- **Mechanism:** The system frames the LLM as a discriminator rather than a generator. Instead of navigating the entire UMLS graph to produce a diagnosis, the model evaluates a constrained set of candidate paths. This shifts the computational load from unguided search to binary or multi-class validity assessment.
- **Core assumption:** The validity of a clinical reasoning chain is easier to recognize than to synthesize, relying on the "verification vs. generation" complexity gap from computational theory.
- **Evidence anchors:**
  - [abstract]: "grounded in computational theory, which suggests that verifying a solution is often easier than generating one from scratch."
  - [Introduction]: Describes the task as judging whether a candidate path leads to a correct diagnosis rather than generating the path itself.
  - [corpus]: Neighbor papers like *KERAP* and *MedKGI* focus on generation or multi-agent retrieval; this paper distinctively applies a reward/verification lens.
- **Break condition:** Fails if the candidate paths provided to the model are of such low quality or high noise that valid reasoning is indistinguishable from spurious connections.

### Mechanism 2
- **Claim:** Reinforcing relative preferences across groups of candidates improves path-judgment accuracy.
- **Mechanism:** The paper utilizes Group Relative Policy Optimization (GRPO). Rather than optimizing a single response, this method samples a group of paths (e.g., 10 candidates) and reinforces the correct path relative to the group's baseline performance. This allows the model to learn ranking signals among competing diagnostic explanations.
- **Core assumption:** Diagnostic validity is a relative property best learned by comparing valid paths against multiple negative distractors simultaneously.
- **Evidence anchors:**
  - [Methods]: Describes GRPO as reinforcing "relative preference for correct paths across the set."
  - [Results]: Table 2 shows SFT+GRPO achieving significantly higher Rouge-L scores (84.96) compared to standard SFT (52.64) on the Qwen2.5 model.
  - [corpus]: Weak corpus support; standard RAG or fine-tuning approaches in related literature do not typically employ group-relative reward optimization for graph paths.
- **Break condition:** Fails if the group size is too small to provide a meaningful baseline or if the "valid" path is ambiguously labeled.

### Mechanism 3
- **Claim:** Generating explicit reasoning traces (rationales) before judging distills robust verification capabilities.
- **Mechanism:** The "Distilling Step by Step" (DSS) framework forces the model to generate a rationale explaining why a path is valid before outputting a judgment. This "thinking" step aligns the model's internal representations with the causal logic of the Knowledge Graph, rather than just pattern-matching on path syntax.
- **Core assumption:** LLMs can better verify graph paths if they explicitly verbalize the diagnostic logic connecting the patient state to the graph nodes.
- **Evidence anchors:**
  - [Methods]: "Distilling Step by Step... extends reasoning distillation by treating reasoning generation and path prediction as two distinct tasks."
  - [Results]: Table 2 indicates that DSS and DSS+GRPO consistently outperform standard SFT and DPO across all tested models.
- **Break condition:** Fails if the generated rationales are hallucinated or if the teacher model (e.g., GPT-o3-mini) generates flawed logic that the student model mimics.

## Foundational Learning

- **Concept: Knowledge Graphs (KG) & UMLS**
  - **Why needed here:** The system does not treat text as flat strings but as traversals over nodes (concepts) and edges (relations). Understanding that "Elevated k" must map to a specific CUI (Concept Unique Identifier) is essential for debugging prediction errors.
  - **Quick check question:** Can you explain the difference between a semantic type (e.g., "Finding") and a relation (e.g., "has_member") in a medical KG?

- **Concept: Reward Modeling vs. Standard Fine-Tuning**
  - **Why needed here:** The core innovation is using the LLM as a *Reward Model* (a judge/scorer) rather than a standard generator. This requires understanding how to train a model to output a preference or score rather than raw text.
  - **Quick check question:** In standard RLHF, how does the role of a "Reward Model" differ from the "Policy Model," and how does this paper blur that line?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** The paper benchmarks DPO against GRPO. Understanding DPO is necessary to see why a pairwise preference signal might be weaker than a group-relative signal for this specific task.
  - **Quick check question:** Why does DPO require paired data (a winner and a loser), and how does that constrain the training datasets used in this paper?

## Architecture Onboarding

- **Component map:**
  Input Processor -> QuickUMLS concept extraction -> Path Generator (DFS) -> LLM Reasoning Module -> Training Wrapper (GRPO/DSS) -> Path Judge Output

- **Critical path:**
  The **Data Construction Pipeline** (Section 3.2) is the most critical pre-compute step. The entire mechanism relies on the quality of "Positive" vs. "Negative" path labeling. If the DFS fails to find the gold-standard path or generates trivial negatives, the model cannot learn meaningful verification.

- **Design tradeoffs:**
  - **Task Formulation:** The paper tests Path Selection (Global judgment) vs. Next-Hop Prediction (Local structure). Selection is more robust but requires generating candidates; Next-Hop is harder to learn (Table 1 shows low scores) but requires no pre-sampled paths.
  - **Distillation Cost:** Using GPT-o3-mini for rationale generation improves performance (DSS) but adds dependency on a proprietary teacher model and increases training token count.

- **Failure signatures:**
  1. **Syntactic Hallucination:** Predicting relations that exist in language but not in the specific KG schema (e.g., predicting a synonym like "Hyperkalemia" instead of the node "K excess").
  2. **Transfer Brittleness:** A model may achieve high ROUGE-L on *Path Selection* (e.g., 84.9) but fail to improve on downstream *Diagnosis Prediction* (Table 3). This indicates the model learned to judge paths without internalizing the diagnostic logic.
  3. **Premature Termination:** In Path Completion tasks, the model stops generating early, missing the final diagnostic node.

- **First 3 experiments:**
  1. **Sanity Check (SFT Baseline):** Fine-tune the base LLM on the *Path Selection (P@10)* task using standard cross-entropy loss. Verify you can reproduce the "No SFT" vs. "SFT" gap in Table 1.
  2. **Ablation (GRPO):** Replace standard SFT with the GRPO loss on the same P@10 data. Confirm if the Rouge-L score increases significantly (as seen in Table 2 for Qwen2.5).
  3. **Transfer Probe:** Take the best checkpoint from Experiment 2 and evaluate it zero-shot on the *Diagnosis Prediction* task (ProbSum). Check if the "judging" capability actually helps the model summarize diagnoses better than a baseline (expecting "weak transfer" as per the paper's findings).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the disconnect between high performance on knowledge graph path-judging tasks and poor performance on downstream diagnostic tasks (like summarization and QA) be bridged?
- **Basis in paper:** [explicit] The authors state that while specific optimization leads to strong path-judging performance, "transferability to downstream tasks remain weak" (Abstract) and "proficiency in identifying valid KG paths does not necessarily translate to improved performance on downstream tasks" (Results).
- **Why unresolved:** The paper establishes that verification (judging) is computationally distinct from generation, but it remains unclear why fine-tuning on the former does not confer reasoning benefits to the latter.
- **What evidence would resolve it:** Demonstrating a training paradigm where improvements in path-judging metrics (e.g., ROUGE on P@10) yield statistically significant improvements in diagnostic accuracy (e.g., MedQA exact match) without task-specific fine-tuning.

### Open Question 2
- **Question:** Does supervised fine-tuning (SFT) on specific path reasoning tasks primarily teach superficial format compliance rather than deep structural reasoning?
- **Basis in paper:** [explicit] The Discussion notes that "SFT primarily teaches models to conform to a particular task format, rather than imparting deeper reasoning capabilities," citing that models often perform worse than the no-SFT baseline on related tasks.
- **Why unresolved:** Models trained on one task (e.g., Next-Hop Prediction) fail to generalize to others (e.g., Path Completion), suggesting they learn task-specific heuristics rather than the underlying clinical logic of the graph.
- **What evidence would resolve it:** An analysis showing that SFT models maintain high performance when task prompts are reformatted or when evaluated on out-of-distribution graph structures, indicating robustness beyond surface patterns.

### Open Question 3
- **Question:** How can evaluation metrics be adapted to account for semantic equivalence and synonymy in knowledge graph paths rather than relying on exact string matching?
- **Basis in paper:** [explicit] The authors identify a key limitation where models output clinically valid synonyms (e.g., "Hyperkalemia" for "K excess") that are penalized by current metrics based on exact string matching.
- **Why unresolved:** Current metrics (ROUGE) fail to capture "ontological equivalence," misclassifying correct reasoning chains as errors due to lexical divergence.
- **What evidence would resolve it:** The development and validation of a semantic evaluation framework that maps model outputs to UMLS concept unique identifiers (CUIs) and rewards semantic equivalence.

### Open Question 4
- **Question:** Can constrained decoding or curriculum learning effectively mitigate the issue of models hallucinating invalid relations or terminating paths prematurely?
- **Basis in paper:** [explicit] The Conclusion suggests that to move beyond brittle adaptations, "future work must focus on strategies... [such as] curriculum learning, constrained decoding over KG structures."
- **Why unresolved:** The paper observes that unconstrained generation leads to error modes like "hallucinate incorrect relations" and "terminate paths prematurely."
- **What evidence would resolve it:** Experiments implementing a constrained decoding layer that restricts the model's vocabulary to valid outgoing edges at the current node, resulting in higher path validity scores.

## Limitations
- **Transfer brittleness:** High path-judging performance doesn't translate to improved downstream diagnostic tasks.
- **Lexical mismatch:** Models penalized for valid synonyms due to exact string matching evaluation.
- **Data construction dependency:** Performance relies heavily on DFS-generated paths and QuickUMLS entity extraction quality.

## Confidence
- **High Confidence:** Reward modeling improves path-judging performance over standard fine-tuning.
- **Medium Confidence:** Verification is computationally easier than generation (theory-grounded but not empirically tested).
- **Medium Confidence:** Limited transferability to downstream tasks, though underlying cause remains speculative.

## Next Checks
1. **Lexical Mismatch Analysis:** Conduct detailed error analysis comparing low-scoring predictions to gold-standard paths, measuring semantic similarity rather than exact string matching.
2. **Cross-Domain Transfer Test:** Evaluate best path-judging models on different diagnostic reasoning tasks or KG structures to test fundamental brittleness.
3. **Intermediate Reasoning Evaluation:** Design experiment where models generate both path and diagnosis simultaneously to test genuine diagnostic reasoning.