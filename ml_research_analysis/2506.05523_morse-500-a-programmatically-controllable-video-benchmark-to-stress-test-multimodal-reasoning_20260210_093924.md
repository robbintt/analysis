---
ver: rpa2
title: 'MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test
  Multimodal Reasoning'
arxiv_id: '2506.05523'
source_url: https://arxiv.org/abs/2506.05523
tags:
- reasoning
- self
- video
- multimodal
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning

## Quick Facts
- arXiv ID: 2506.05523
- Source URL: https://arxiv.org/abs/2506.05523
- Reference count: 40
- Primary result: VLMs show uniformly low performance (30.0%-42.6%) across six reasoning categories, with planning and abstract reasoning near-random (0-5%)

## Executive Summary
MORSE-500 introduces a programmatically generated video benchmark with 500 clips designed to stress-test multimodal reasoning across six cognitive categories. Using Python scripts with Manim/Matplotlib/MoviePy, the benchmark enables systematic difficulty scaling through adjustable parameters like entity count, reasoning depth, and distractor density. Questions are embedded directly within videos to force genuine temporal-visual reasoning rather than text-based shortcuts. Current VLMs show fundamental architectural limitations, particularly in planning and abstract reasoning, with uniformly low performance across all categories suggesting these failures are not data-related but structural.

## Method Summary
The benchmark uses programmatic generation to create 500 videos (5.1-140s duration, 1.4GB total) across six reasoning categories. Scripts control visual complexity, distractor density, and temporal dynamics through parameters like entity count (2-15+), reasoning depth (1-5+ steps), and distractor density (0-8+). Questions are embedded as visual overlays during playback, with models receiving only "Answer the question in this video" prompt. For image-only models, 2 FPS sampling with max 32 frames is used. Evaluation uses LLM-extracted answers (Qwen2.5 72B AWQ) matched against ground-truth labels. Generation scripts and dataset are publicly available on GitHub and HuggingFace.

## Key Results
- Planning tasks show near-random performance (0-5%) across all models, indicating fundamental architectural limitations
- Video input degrades performance compared to static images (62.4% vs 57.3% on MathVista subset), confirming temporal reasoning adds difficulty
- Best model (Qwen2.5-VL-Max) achieves only 42.6% overall accuracy, with mathematical reasoning highest at 35.7% and planning lowest at 0-5%
- Increasing FPS beyond 2-4 frames/second does not improve planning performance, suggesting temporal integration issues

## Why This Works (Mechanism)

### Mechanism 1: Programmatic Generation Enables Systematic Difficulty Scaling
- Claim: Deterministic script-based generation allows controlled variation of complexity parameters, preventing benchmark saturation
- Mechanism: Python scripts generate videos with precisely adjustable entity count, reasoning depth, distractor density, and temporal dynamics; parameters can be tuned as models improve
- Core assumption: Difficulty parameters are orthogonal and can be independently scaled
- Evidence: Abstract states "fine-grained control over visual complexity, distractor density, and temporal dynamics—enabling difficulty to be scaled systematically"

### Mechanism 2: Embedded Questions Force Temporal-Visual Reasoning
- Claim: Placing questions within videos eliminates text-based shortcuts and requires genuine video understanding
- Mechanism: Questions appear as visual overlays; models receive only "Answer the question in this video" prompt with no external context
- Core assumption: Models cannot solve tasks via single-frame analysis or text-only reasoning
- Evidence: Image vs video performance gap (62.4% vs 57.3%) confirms temporal processing adds difficulty

### Mechanism 3: Multi-Category Taxonomy Reveals Architectural Gaps
- Claim: Spanning six reasoning categories exposes systematic weaknesses that single-category benchmarks miss
- Mechanism: Each category targets distinct cognitive capabilities; performance variance reveals whether failures are task-specific or architectural
- Core assumption: Different reasoning types require partially non-overlapping capabilities
- Evidence: Uniformly low performance across all categories (0-5% planning, 20-35% math) suggests fundamental architectural limitations

## Foundational Learning

- Concept: **Temporal vs. Static Multimodal Reasoning**
  - Why needed: Understanding why video-based evaluation differs from image-based approaches; models optimized for spatial reasoning may fail at temporal integration
  - Quick check: Can you explain why increasing FPS beyond 2-4 frames/second did not improve planning performance (staying at 1-4%)?

- Concept: **Cognitive Reasoning Taxonomy (CHC Framework)**
  - Why needed: Interpreting why planning and abstract reasoning show 0-5% accuracy while mathematical reasoning reaches 20-35%; these correspond to distinct cognitive abilities
  - Quick check: Based on Table 2, which reasoning category shows the largest human-model gap, and what cognitive ability does this implicate?

- Concept: **Programmatic Benchmark Design**
  - Why needed: Understanding how parameter-controlled generation enables forward-compatible evaluation; critical for extending MORSE-500 as models improve
  - Quick check: If Gemini 2.5 Pro achieves 36.9% on mathematical reasoning, what three difficulty parameters could you adjust to create harder instances?

## Architecture Onboarding

- Component map: Generation layer (Manim, Matplotlib, MoviePy, generative models) -> Parameter control (entity count, reasoning depth, distractor density, temporal complexity) -> Validation pipeline (automated technical checks, human evaluation) -> Evaluation harness (minimal prompting, frame sampling)
- Critical path: Task identification → Parameter specification → Script implementation → Automated validation → Human review → Difficulty calibration → Dataset release (2-3 cycles per category typical)
- Design tradeoffs: Synthetic generation enables precise control but lacks natural visual diversity; audio exclusion isolates visual reasoning but limits ecological validity; 2 FPS/32 frames balances context coverage against token limits
- Failure signatures: Planning shows near-random performance (0-5%) indicating multi-step reasoning failures; abstract reasoning shows hallucinated patterns suggesting rule induction failures; spatial tasks show gross undercounting revealing occlusion handling gaps
- First 3 experiments:
  1. Difficulty scaling validation: Generate task variants with increasing entity count and distractor density; plot model performance decay curves
  2. Temporal ablation: Convert 10 video tasks to static image-question pairs; measure performance delta (expect 5-15% degradation)
  3. Category cross-transfer: Fine-tune on high-performing categories, evaluate zero-shot on low-performing categories; near-random transfer confirms architectural limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications are required to improve VLM planning reasoning from near-random performance (0–5%) to human-level capabilities?
- Basis: Authors report planning tasks show "near-random performance across all models (0–5% accuracy)" indicating "fundamental architectural limitations rather than scaling issues"
- Why unresolved: Current models fail systematically at multi-step, goal-directed reasoning; paper does not propose specific architectural solutions
- What evidence would resolve it: Demonstrated improvements on MORSE-500 planning subtasks after implementing specific architectural changes

### Open Question 2
- Question: Why do VLMs consistently perform worse on video inputs compared to static image-text pairs, even when visual content is equivalent?
- Basis: Ablation study shows consistent performance decline from Image+Question Text (69.0) → Multi-Image Context (65.1) → Video Input (62.8)
- Why unresolved: Mechanism underlying this temporal brittleness is not identified
- What evidence would resolve it: Analysis identifying whether degradation stems from attention mechanisms, memory limitations, or temporal encoding failures

### Open Question 3
- Question: Would incorporating audio modalities meaningfully improve multimodal reasoning performance on video-based benchmarks?
- Basis: Authors state in Limitations: "absence of speech, ambient sounds, or audio-visual synchronization tasks represents a significant gap"
- Why unresolved: MORSE-500 currently excludes audio, so this hypothesis remains untested
- What evidence would resolve it: Modified benchmark with audio-visual tasks showing performance improvements for models with native audio processing

## Limitations
- Limited temporal reasoning analysis: The paper demonstrates video input improves difficulty but does not deeply investigate which specific temporal capabilities are most challenging
- Synthetic generation constraints: Reliance on programmatic generation may not capture full complexity of real-world visual reasoning scenarios
- Answer extraction pipeline opacity: Use of Qwen2.5 72B AWQ for answer parsing lacks specific implementation details, introducing potential variability

## Confidence

- **High confidence**: Systematic difficulty scaling mechanism is well-validated through parameter-controlled generation and clear performance degradation patterns
- **Medium confidence**: Embedded questions force temporal-visual reasoning is supported by image vs video performance gap
- **Low confidence**: Diagnostic value of six-category taxonomy for revealing architectural gaps lacks strong empirical validation

## Next Checks

1. **Temporal reasoning ablation study**: Convert 20 randomly selected video tasks from each category to equivalent static image-question pairs and measure performance degradation
2. **Architectural gap analysis**: Train a model on high-performing categories (mathematical, physical) and evaluate zero-shot on low-performing categories (planning, abstract)
3. **Parameter independence validation**: Generate multiple variants of the same task with independently varied entity count, reasoning depth, and distractor density; plot performance curves to confirm orthogonal scaling