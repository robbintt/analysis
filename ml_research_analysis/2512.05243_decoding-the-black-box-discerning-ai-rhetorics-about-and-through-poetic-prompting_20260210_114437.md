---
ver: rpa2
title: 'Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting'
arxiv_id: '2512.05243'
source_url: https://arxiv.org/abs/2512.05243
tags:
- poetry
- prompt
- language
- creative
- poems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces Poetry Prompt Patterns as a method to analyze\
  \ the rhetorical tendencies and biases of large language models (LLMs) through creative\
  \ text prompting. By examining how ChatGPT, Claude, and DeepSeek describe, analyze,\
  \ and adapt Maya Angelou\u2019s poetry, the research reveals that models tend to\
  \ use highly praising, panegyric language, often emphasizing universal themes while\
  \ downplaying the political and cultural specificity of the poet\u2019s work."
---

# Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting

## Quick Facts
- **arXiv ID:** 2512.05243
- **Source URL:** https://arxiv.org/abs/2512.05243
- **Reference count:** 0
- **Primary result:** Introduces Poetry Prompt Patterns to analyze LLM biases through creative text prompting, revealing models' tendencies toward panegyric language and paradoxical universalizing rhetoric.

## Executive Summary
This study introduces Poetry Prompt Patterns as a novel method to analyze the rhetorical tendencies and biases of large language models (LLMs) through creative text prompting. By examining how ChatGPT, Claude, and DeepSeek describe, analyze, and adapt Maya Angelou's poetry, the research reveals that models tend to use highly praising, panegyric language, often emphasizing universal themes while downplaying the political and cultural specificity of the poet's work. The study finds that models readily adapt Angelou's poems for broader audiences, but their explanations reveal a paradox: they describe the poems as universally relatable while also recommending specific adaptations for different groups. This suggests a tendency toward tokenization and stereotyping, even when attempting inclusivity. The Poetry Prompt Pattern offers a novel approach to probing LLM biases and rhetorics, highlighting the importance of nuanced, context-aware model evaluations. Researchers are encouraged to replicate and expand on this method, consulting subject matter experts to ensure accurate and culturally sensitive interpretations.

## Method Summary
The study employs a novel methodology called Poetry Prompt Patterns, which uses creative text prompting to analyze how large language models (LLMs) describe, analyze, and adapt Maya Angelou's poetry. The researchers crafted three types of prompts: general inquiry prompts asking models to discuss Angelou's work, analysis prompts requesting interpretations of specific poems, and composition prompts asking models to create new poems inspired by her style. By examining the outputs from ChatGPT, Claude, and DeepSeek across these prompt types, the authors identified recurring rhetorical patterns, including the use of panegyric language and the tendency to emphasize universal themes while downplaying political and cultural specificity. The method provides a creative lens for probing LLM biases and rhetorics, offering insights into how models handle culturally sensitive content.

## Key Results
- Models consistently use panegyric language when discussing Maya Angelou's poetry, emphasizing universal themes while downplaying political and cultural specificity.
- A paradox emerges where models describe Angelou's work as universally relatable yet recommend specific adaptations for different groups, suggesting tokenization and stereotyping tendencies.
- The Poetry Prompt Pattern method reveals how creative prompting can serve as a backdoor to content that would otherwise be refused in standard informational prompts.

## Why This Works (Mechanism)
The Poetry Prompt Pattern works by leveraging the creative and aesthetic framing of prompts to lower the perceived stakes for models, allowing researchers to access and analyze the underlying rhetorics and biases that might be masked in standard informational queries. By engaging models in poetic analysis and composition, the method bypasses some of the safety filters and refusals that might occur with direct questions about sensitive topics. This creative approach provides a unique window into how models process and present culturally significant content, revealing patterns of universalization and adaptation that might otherwise remain hidden.

## Foundational Learning
- **Creative Prompting Techniques:** Why needed - To access model behaviors that are otherwise restricted by safety filters; Quick check - Compare outputs from creative vs. standard prompts on sensitive topics.
- **Rhetorical Analysis in AI:** Why needed - To identify patterns in how models frame and present information; Quick check - Analyze model outputs for recurring linguistic patterns across different prompt types.
- **Cultural Sensitivity in AI Outputs:** Why needed - To ensure accurate and respectful handling of culturally significant content; Quick check - Consult subject matter experts to validate model interpretations of culturally specific material.

## Architecture Onboarding
The Poetry Prompt Pattern methodology involves a three-stage process:
1. **Prompt Design** (Creative Inquiry -> Analysis -> Composition) -> **Model Execution** (LLM processing of prompts) -> **Output Analysis** (Rhetorical and cultural evaluation)
- **Critical Path:** The prompt design stage is critical, as the creative framing determines the accessibility and nature of the model's responses.
- **Design Tradeoffs:** Balancing creative framing to bypass safety filters while maintaining analytical rigor for meaningful output evaluation.
- **Failure Signatures:** Outputs that are overly generic, refuse to engage with culturally specific content, or exhibit stereotypical adaptations.
- **First Experiments:**
  1. Replicate the study with a different poet to test pattern consistency across cultural contexts.
  2. Apply the method to open-source LLMs to compare with proprietary models.
  3. Combine Poetry Prompt Pattern with Persona Prompt Pattern to test tonal and expressive trends.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** What specific tonal and expressive trends emerge when combining the Poetry Prompt Pattern with the Persona Prompt Pattern?
- **Basis in paper:** [explicit] The conclusion explicitly suggests researchers "combine the Poetry Prompt Pattern with the Persona Prompt Pattern to test tonal and expressive trends."
- **Why unresolved:** The current study focused on general inquiry, analysis, and composition prompts without enforcing specific persona constraints on the model's voice.
- **What evidence would resolve it:** A comparative study analyzing outputs generated via the combined pattern against the baseline Poetry Prompt Pattern to identify shifts in bias or rhetoric.

### Open Question 2
- **Question:** To what extent do "plausible poetic hallucinations" in model outputs align with or diverge from expert scholarly interpretations?
- **Basis in paper:** [explicit] The authors warn of the "highly affective and plausible nature of poetic hallucinations" and explicitly advise future researchers to "consult experts on the subject matter."
- **Why unresolved:** The current analysis relied on rhetorical analysis by the authors rather than a validated comparison with subject matter experts (SMEs).
- **What evidence would resolve it:** A study comparing LLM-generated poetic analyses against validated SME annotations to quantify the accuracy of cultural and political contextualization.

### Open Question 3
- **Question:** Does the paradox of "universal relatability" versus "stereotyped adaptation" persist when models analyze poets from different cultural or political backgrounds?
- **Basis in paper:** [inferred] The study identifies a specific paradox where models describe Angelou's work as universal yet adapt it using stereotypes; however, the sample is limited to a single Black feminist poet.
- **Why unresolved:** It is undetermined if this rhetorical tendency is specific to the training data regarding Black feminist writers or a general feature of how LLMs handle "universal" themes.
- **What evidence would resolve it:** Replicating the methodology using a diverse set of poets (e.g., non-Black, male, or non-Western writers) to see if the "universalizing" rhetoric persists.

### Open Question 4
- **Question:** How does the "creative helpfulness" of models function as a backdoor to content that would otherwise be refused in standard informational prompts?
- **Basis in paper:** [explicit] The paper notes that appealing to "aesthetic motives" through the Poetry Prompt Pattern may "lower the stakes for a model" and provide a "back door" into restricted associations.
- **Why unresolved:** The study explores this qualitatively but does not quantitatively measure the success rate of poetic prompts in bypassing safety filters compared to standard prompts.
- **What evidence would resolve it:** A controlled experiment measuring refusal rates for sensitive topics when framed as standard inquiries versus poetic composition requests.

## Limitations
- The study's findings are based on a limited corpus of Maya Angelou's poetry and three specific LLMs, raising questions about generalizability to other poets or models.
- The subjective nature of rhetorical analysis introduces potential interpretive bias, particularly when evaluating culturally sensitive content.
- The method does not quantitatively measure the success rate of poetic prompts in bypassing safety filters compared to standard prompts.

## Confidence
- **High confidence:** Models consistently use panegyric language and emphasize universal themes while downplaying political and cultural specificity.
- **Medium-High confidence:** The paradox of universal relatability versus stereotyped adaptation is a clear internal contradiction in model outputs.
- **Medium confidence:** The interpretation of tokenization and stereotyping tendencies when attempting inclusivity may benefit from additional expert validation.

## Next Checks
1. Replicate the study using poetry from multiple cultural traditions and contemporary poets to test pattern consistency across different contexts.
2. Conduct blind expert evaluation where cultural and literary scholars assess model outputs for accuracy and sensitivity without knowledge of the study's hypotheses.
3. Test the method with additional LLM architectures (including open-source models) to determine if the identified patterns are consistent across different model families and training approaches.