---
ver: rpa2
title: 'Expected Attention: KV Cache Compression by Estimating Attention from Future
  Queries Distribution'
arxiv_id: '2510.00636'
source_url: https://arxiv.org/abs/2510.00636
tags:
- attention
- compression
- cache
- expected
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Key-Value (KV) cache compression
  in large language models (LLMs), where memory consumption grows linearly with sequence
  length, becoming a bottleneck for long-context inference. The core method, Expected
  Attention, is a training-free approach that estimates the importance of KV pairs
  by predicting how future queries will attend to them, leveraging the distributional
  properties of LLM activations to compute expected attention scores in closed form.
---

# Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution

## Quick Facts
- arXiv ID: 2510.00636
- Source URL: https://arxiv.org/abs/2510.00636
- Authors: Alessio Devoto; Maximilian Jeblick; Simon JÃ©gou
- Reference count: 40
- Primary result: Training-free KV cache compression method that outperforms state-of-the-art baselines while maintaining performance across prefilling and decoding phases

## Executive Summary
This paper addresses the challenge of Key-Value (KV) cache compression in large language models, where memory consumption grows linearly with sequence length, becoming a bottleneck for long-context inference. The authors propose Expected Attention, a training-free approach that estimates the importance of KV pairs by predicting how future queries will attend to them. By leveraging distributional properties of LLM activations, the method computes expected attention scores in closed form, enabling principled ranking and pruning of KV pairs with minimal impact on the residual stream. The approach achieves effective compression without performance degradation and operates seamlessly across both prefilling and decoding phases.

## Method Summary
Expected Attention is a training-free KV cache compression method that estimates the importance of KV pairs by predicting future query attention patterns. The core innovation lies in computing expected attention scores using closed-form solutions derived from the distributional properties of LLM activations. Rather than requiring additional training or fine-tuning, the method analyzes how future queries are likely to attend to existing KV pairs based on learned attention patterns from pre-training. This enables principled ranking and selective pruning of less important KV pairs while maintaining model performance. The approach works uniformly across both prefilling (batch processing) and decoding (autoregressive generation) phases, making it versatile for different inference scenarios.

## Key Results
- Achieves effective KV cache compression without performance degradation across multiple benchmarks
- Consistently outperforms state-of-the-art baselines on LongBench and Ruler benchmarks at all compression ratios
- Demonstrates robust performance on Needle in a Haystack tests and excels in high-compression scenarios for decoding tasks like AIME 2025 and MATH-500
- Provides a comprehensive KVPress library for benchmarking over 20 KV cache compression methods

## Why This Works (Mechanism)
The method works by exploiting the statistical regularity in how queries attend to KV pairs across sequences. Since LLMs learn attention patterns during pre-training that generalize across inputs, future queries tend to attend to KV pairs in predictable ways based on their positions and content. Expected Attention formalizes this intuition by modeling the distribution of attention weights and computing expected values in closed form, avoiding expensive sampling or training procedures. This principled approach captures the inherent structure in attention mechanisms while being computationally efficient during inference.

## Foundational Learning

**Attention Mechanisms**: Understanding self-attention and cross-attention is essential since KV cache compression directly targets the KV pairs used in these operations. Why needed: The entire compression approach depends on understanding how queries interact with key-value pairs. Quick check: Verify that expected attention scores correlate with actual attention weights during inference.

**Distributional Properties of Activations**: Knowledge of how LLM activations are distributed across layers and positions enables the closed-form computation of expected values. Why needed: The mathematical foundation relies on tractable probability distributions rather than empirical sampling. Quick check: Confirm that activation distributions remain stable across different input domains.

**Memory Management in LLMs**: Understanding how KV caches grow with sequence length and impact inference costs is crucial for appreciating the compression problem. Why needed: The motivation and evaluation metrics are framed around memory constraints and inference efficiency. Quick check: Measure memory savings versus performance trade-offs at different compression ratios.

**Closed-form Probability Calculations**: Familiarity with techniques for computing expected values without sampling is necessary to understand the computational efficiency claims. Why needed: The method's training-free nature depends on analytical solutions rather than iterative optimization. Quick check: Compare computation time against sampling-based alternatives.

## Architecture Onboarding

**Component Map**: Input Sequence -> Query Embeddings -> Expected Attention Scoring -> KV Pair Ranking -> Pruned KV Cache -> Attention Computation

**Critical Path**: The critical path involves computing expected attention scores for each KV pair, ranking them by importance, and selectively pruning the least important pairs. This occurs before the actual attention computation step, ensuring that the compressed cache still produces meaningful results.

**Design Tradeoffs**: The method trades some potential precision (by using distributional approximations rather than exact future attention) for computational efficiency and training-free operation. This makes it more practical for deployment but may miss some nuanced attention patterns that could be captured through more expensive methods.

**Failure Signatures**: The method may underperform when distributional assumptions about future queries break down, such as with highly irregular input patterns, adversarial examples, or domain-specific sequences that deviate significantly from pre-training distributions.

**First Experiments**:
1. Run Expected Attention on a small sequence with known attention patterns to verify that expected scores match actual attention weights
2. Test compression performance on a controlled benchmark with varying sequence lengths to measure memory savings
3. Evaluate performance degradation at different compression ratios on a standard language modeling task

## Open Questions the Paper Calls Out
The paper acknowledges that real-world deployment scenarios with diverse, non-standard inputs remain untested, and the computational overhead characterization across different hardware configurations is incomplete.

## Limitations
- Relies on distributional assumptions about future query patterns that may not hold uniformly across all domains and task types
- The claim of being "training-free" depends on pre-trained model behaviors that may not generalize to all scenarios
- Real-world deployment with diverse, non-standard inputs remains untested
- Computational overhead characterization is incomplete across different hardware configurations

## Confidence

**Major claim clusters and confidence labels:**
- KV cache compression effectiveness without performance degradation: **High**
- Superiority over state-of-the-art baselines: **Medium** (benchmark-dependent)
- Seamless operation across prefilling and decoding phases: **High**
- Training-free nature with closed-form solutions: **High**

## Next Checks

1. Test Expected Attention on real-world, diverse datasets beyond curated benchmarks to evaluate robustness across varied input distributions
2. Characterize the computational overhead and memory requirements during the expected attention calculation phase across different hardware configurations (GPU, CPU, and memory-constrained environments)
3. Evaluate performance degradation when distributional assumptions about future queries are violated through adversarial or out-of-distribution test cases