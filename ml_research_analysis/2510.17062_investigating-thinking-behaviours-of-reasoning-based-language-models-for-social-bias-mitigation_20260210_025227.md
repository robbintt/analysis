---
ver: rpa2
title: Investigating Thinking Behaviours of Reasoning-Based Language Models for Social
  Bias Mitigation
arxiv_id: '2510.17062'
source_url: https://arxiv.org/abs/2510.17062
tags:
- bias
- reasoning
- social
- contexts
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates why reasoning-based large language models
  exhibit social bias aggregation and proposes a lightweight prompt-based mitigation
  approach. The authors first demonstrate that while reasoning is necessary for model
  performance, it can also aggregate social bias through two specific failure patterns:
  stereotype repetition and irrelevant information injection.'
---

# Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation

## Quick Facts
- **arXiv ID**: 2510.17062
- **Source URL**: https://arxiv.org/abs/2510.17062
- **Reference count**: 15
- **Key outcome**: Proposes prompt-based mitigation approach that reduces social bias in reasoning-based LLMs while maintaining accuracy across three benchmarks

## Executive Summary
This paper investigates why reasoning-based large language models (LLMs) exhibit social bias aggregation and proposes a lightweight prompt-based mitigation approach. The authors demonstrate that while reasoning is necessary for model performance, it can also aggregate social bias through two specific failure patterns: stereotype repetition and irrelevant information injection. Through analysis of thinking-transition tokens and content-level patterns, they find that these failure modes consistently drive biased outputs. Their targeted prompting method guides models to review their own reasoning against these specific failure patterns, effectively reducing bias while maintaining or improving accuracy across multiple reasoning-based models.

## Method Summary
The authors analyze reasoning-based LLMs on three bias benchmarks (BBQ, StereoSet, BOLD) to identify failure patterns in how reasoning processes amplify social bias. They examine thinking-transition tokens and conduct content-level analysis to identify two primary failure modes: stereotype repetition (where reasoning reinforces existing stereotypes) and irrelevant information injection (where reasoning introduces irrelevant but biased content). Based on these insights, they develop a targeted prompting method that instructs models to review their reasoning against these specific failure patterns before generating final outputs. The approach is evaluated across multiple reasoning-based models including QwQ-32B, DeepSeek-Coder-V2, and DeepSeek-R1.

## Key Results
- Bias score reductions of up to 4 percentage points across three benchmarks (BBQ, StereoSet, BOLD)
- Accuracy improvements of up to 3.6 percentage points while reducing bias
- Effective across multiple reasoning-based models including QwQ-32B, DeepSeek-Coder-V2, and DeepSeek-R1
- Prompt-based approach maintains model performance while addressing bias aggregation

## Why This Works (Mechanism)
The paper demonstrates that reasoning-based LLMs amplify social bias through specific failure patterns in their reasoning processes. Stereotype repetition occurs when models reinforce existing stereotypes during their reasoning chains, while irrelevant information injection introduces biased content unrelated to the task. The mitigation approach works by making models explicitly aware of these failure patterns and prompting them to review their reasoning against these specific modes before generating final outputs. This self-review process effectively interrupts the bias amplification cycle while preserving the reasoning benefits needed for task performance.

## Foundational Learning

**Reasoning-based LLM architecture**
- Why needed: Understanding how reasoning processes work is essential for identifying where bias amplification occurs
- Quick check: Verify the model uses explicit reasoning chains (e.g., Chain-of-Thought, Tree-of-Thoughts)

**Social bias metrics and benchmarks**
- Why needed: Proper evaluation requires standardized measures of bias across different domains
- Quick check: Confirm benchmarks test multiple bias types (gender, race, religion, etc.)

**Prompt engineering techniques**
- Why needed: Effective mitigation requires precise control over model reasoning processes
- Quick check: Validate prompt format follows established best practices for reasoning models

**Token-level analysis methods**
- Why needed: Identifying specific failure patterns requires granular examination of reasoning chains
- Quick check: Ensure token analysis captures both semantic and syntactic patterns

## Architecture Onboarding

**Component map**: Input prompt → Reasoning generation → Bias amplification → Failure pattern identification → Self-review prompting → Output generation

**Critical path**: The key pathway is from reasoning generation through bias amplification to output, with the self-review prompting acting as an intervention point

**Design tradeoffs**: Prompt-based mitigation offers flexibility and model-agnostic application but may have limited long-term effectiveness compared to architectural or training-based approaches

**Failure signatures**: Stereotype repetition (reinforcing existing biases) and irrelevant information injection (adding biased content) are the two primary failure modes identified

**First experiments**:
1. Baseline evaluation of reasoning-based models on BBQ, StereoSet, and BOLD benchmarks to establish bias and accuracy metrics
2. Token-level analysis of reasoning chains to identify thinking-transition patterns and failure modes
3. A/B testing of mitigation prompts with and without self-review instructions to measure effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on reasoning-based LLMs limits generalizability to non-reasoning models
- Qualitative analysis of failure patterns lacks causal intervention studies
- Prompt-based approach may have limited long-term effectiveness as models scale

## Confidence

**High Confidence**: Empirical results showing bias reduction and accuracy maintenance across three benchmarks are robust, supported by statistical significance testing and multiple model evaluations.

**Medium Confidence**: Theoretical explanation of how reasoning amplifies bias through the two identified failure patterns is plausible but relies on post-hoc analysis rather than causal intervention studies.

**Medium Confidence**: Prompt-based mitigation approach shows effectiveness, but its generalizability to other bias types beyond social bias remains untested.

## Next Checks

1. Conduct ablation studies removing each component of the mitigation prompt to quantify individual contribution to bias reduction
2. Test the approach on non-reasoning LLMs to assess generalizability beyond reasoning-based models
3. Perform longitudinal evaluation to assess whether the prompt-based mitigation maintains effectiveness as models are updated or retrained