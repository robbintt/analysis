---
ver: rpa2
title: Factors affecting the in-context learning abilities of LLMs for dialogue state
  tracking
arxiv_id: '2506.08753'
source_url: https://arxiv.org/abs/2506.08753
tags:
- dialogue
- demonstrations
- user
- learning
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates in-context learning for dialogue state
  tracking (DST) using nearest-neighbor demonstration retrieval. The proposed method
  employs sentence embeddings to select relevant dialogue turns as demonstrations,
  which are structured with a modular prompt template containing conversation history,
  domain, and slot-value pairs.
---

# Factors affecting the in-context learning abilities of LLMs for dialogue state tracking

## Quick Facts
- arXiv ID: 2506.08753
- Source URL: https://arxiv.org/abs/2506.08753
- Reference count: 0
- Primary result: User-only utterance retrieval with LaBSE embeddings achieves 67-73% precision and 79-83% recall on MultiWOZ2.4

## Executive Summary
This paper investigates in-context learning for dialogue state tracking using nearest-neighbor demonstration retrieval. The proposed method employs sentence embeddings to select relevant dialogue turns as demonstrations, which are structured with a modular prompt template containing conversation history, domain, and slot-value pairs. The system was evaluated on the MultiWOZ2.4 dataset using OLMo-7B-Instruct, Mistral-7B-Instruct, and Llama3.2-3B-Instruct models. Key findings include that using only user utterances for embedding retrieval yields better performance than including agent utterances, that LaBSE embeddings perform comparably to dialogue-specific D2F embeddings, and that speaker tags have a minor but measurable impact on performance.

## Method Summary
The method uses in-context learning with k-nearest neighbor retrieval for dialogue state tracking. Pre-computed embeddings (LaBSE or D2F) are created for training dialogue turns, and at inference time, the most relevant demonstrations are retrieved via cosine similarity. These demonstrations, along with the test sample, are formatted using a modular prompt template that includes conversation history, domain list, and slot schema in JSON format. The system uses constrained decoding to predict slot values given domain and slot keys, achieving approximately 67-73% precision and 79-83% recall on MultiWOZ2.4.

## Key Results
- User-only utterance retrieval achieves 69.9% precision and 80.6% recall vs. 67.8%/78.3% for user-agent retrieval
- LaBSE embeddings provide better slot relevance and coverage than D2F, particularly with fewer demonstrations
- Constrained decoding (predicting slot values given slot keys) achieves 67.8% precision / 78.3% recall vs. 69.2% / 53.3% for full key-value generation
- Optimal performance achieved using 3-10 demonstrations
- Speaker tags provide minor but measurable performance improvements (~1-2%)

## Why This Works (Mechanism)

### Mechanism 1: User-Utterance-Only Retrieval Signal
Embedding retrieval using only user utterances yields better DST performance than user-agent turns because user utterances provide a cleaner semantic signal for demonstration matching. User utterances directly express intent and slot values, whereas agent responses introduce noise (clarifying questions, confirmations) that dilute embedding similarity. By isolating user turns, the retriever surfaces demonstrations where the user's information needs align with the test case.

### Mechanism 2: Slot-Key Coverage vs. Relevance Trade-off in Demonstration Selection
Better DST performance correlates with demonstrations that achieve both high slot-key relevance (precision) and coverage (recall) relative to the target's ground-truth slots. LaBSE embeddings, while not dialogue-specific, produce demonstrations with better slot relevance and coverage than D2F when fewer demonstrations are used. This is because LaBSE's language-agnostic representations capture semantic similarity that aligns with slot-bearing utterances.

### Mechanism 3: Constrained Decoding with Schema-Guided Slot Keys
Predicting slot values given the slot key (constrained decoding) achieves higher recall than generating the full key-value pair. By conditioning on the slot key, the LLM's output space is narrowed, reducing errors in key prediction and focusing capacity on value extraction. The modular prompt template explicitly guides the model to the correct slots before value prediction.

## Foundational Learning

- **In-Context Learning (ICL) with k-Nearest Neighbors Retrieval**: Why needed here - This paper's entire methodology is built on retrieving semantically similar demonstrations via embedding similarity and presenting them in-context. Quick check: Can you explain why demonstration order matters for ICL, and why the most relevant example is placed closest to the test sample?

- **Dialogue State Tracking (DST) Schema Design**: Why needed here - DST requires extracting (domain, slot-key, slot-value) tuples from multi-turn dialogue. Understanding slot schemas (e.g., restaurant-name, taxi-departure) is essential to interpret the modular prompt template and constrained decoding setup. Quick check: Given a user utterance "Book me a table for 4 at an Italian restaurant downtown tomorrow at 7pm," what slots would a restaurant-domain schema require?

- **Sentence Embeddings and Cosine Similarity for Retrieval**: Why needed here - The demonstration retriever uses pre-trained sentence embeddings (LaBSE, D2F) and cosine similarity. Understanding how these embeddings encode semantic information is critical for debugging retrieval quality. Quick check: Why might a multilingual embedding model (LaBSE) outperform a dialogue-specific model (D2F) in certain retrieval scenarios?

## Architecture Onboarding

- **Component map**: Preprocessing -> Embedding Store -> Demonstration Retriever -> Prompt Constructor -> LLM Inference -> Post-processor
- **Critical path**: Embedding quality → retrieval relevance/coverage → prompt formatting → constrained decoding. If retrieval surfaces demonstrations with poor slot-key overlap, downstream DST performance degrades regardless of LLM quality.
- **Design tradeoffs**: User-only vs. user-agent retrieval (user-only yields higher recall); fewer demonstrations (1-3) favor recall while more (10) favor precision but risk context dilution; LaBSE vs. D2F (LaBSE is general-purpose and language-agnostic while D2F is dialogue-act-tuned).
- **Failure signatures**: Low recall but high precision suggests insufficient slot-key coverage in retrieved demonstrations; low precision but high recall indicates demonstrations may contain irrelevant slots; JSON parsing failures suggest LLM output format drift; zero-shot performance is poor, requiring demonstrations.
- **First 3 experiments**:
  1. Baseline retrieval ablation: Compare user-only vs. user-agent embeddings with LaBSE, K=3, on a held-out validation set. Measure slot precision/recall.
  2. Embedding model comparison: LaBSE vs. D2F with K=1, 3, 10. Analyze slot-key relevance and coverage curves.
  3. Speaker tag impact: Run with and without "User:"/"Agent:" tags across OLMo-7B-Instruct and one other model. Measure precision/recall deltas.

## Open Questions the Paper Calls Out

### Open Question 1
Would user-only retrieval embeddings still outperform user-agent embeddings when tested on dialogue datasets with different interaction patterns (e.g., more agent-initiated exchanges, different domains)? The study focuses exclusively on MultiWOZ2.4 and in-domain examples; authors note this limitation. The finding may be dataset-specific, as MultiWOZ has particular user-agent dynamics. Experiments on diverse dialogue datasets (Taskmaster, SGD, cross-lingual datasets) comparing user-only vs. user-agent retrieval would resolve this.

### Open Question 2
Can the poor zero-shot DST performance be overcome through improved prompt engineering or hybrid approaches? Due to the poor performance of zero-shot prompting in their format, the paper limits investigation to demonstration-based DST. The paper excludes zero-shot entirely, leaving open whether this is a fundamental limitation of ICL for DST or a prompt format issue. Systematic comparison of alternative prompt templates, chain-of-thought prompting, or minimal-demonstration approaches would resolve this.

### Open Question 3
What mechanisms explain why general-purpose LaBSE embeddings achieve better slot relevance and coverage than dialogue-specific D2F embeddings? The paper observes this phenomenon but does not explain why LaBSE, trained on multilingual parallel data, outperforms D2F trained specifically for dialogue acts. Probing experiments analyzing what linguistic features each embedding model captures, or visualization of embedding spaces with respect to slot-key distributions, would resolve this.

### Open Question 4
Can slot key-value generation decoding achieve high recall while maintaining its precision advantage over constrained decoding? Table 6 shows slot key-value generation has higher precision (69.2 vs 67.8) but substantially lower recall (53.3 vs 78.3). The trade-off between these decoding strategies suggests a fundamental tension that may be addressable through improved prompting or post-processing. Experiments with constrained decoding during key generation, or iterative refinement approaches that combine both strategies, would resolve this.

## Limitations
- The constrained decoding setup assumes prior knowledge of slot schemas, limiting applicability to open-vocabulary DST
- The retrieval mechanism depends heavily on user utterance purity - if agent turns carry critical slot information, user-only retrieval may underperform
- The analysis of speaker tags shows only minor performance gains (~1-2%), suggesting this factor is secondary to embedding quality and demonstration relevance

## Confidence

- **High confidence**: User-only utterance retrieval outperforms user-agent retrieval; LaBSE embeddings achieve comparable or better slot relevance/coverage than D2F; constrained decoding improves recall over key-value generation
- **Medium confidence**: Optimal demonstration count is 3-10 (sensitive to dialogue length and schema complexity); speaker tags provide measurable but minor benefits
- **Low confidence**: Cross-model generalization of speaker tag benefits; scalability to larger schemas or multi-domain dialogues without performance degradation

## Next Checks
1. **Speaker Tag Ablation**: Run identical experiments across all three LLMs (OLMo-7B, Mistral-7B, Llama3.2-3B) with and without speaker tags to quantify model-specific impacts and verify the 1-2% performance delta.
2. **User-Agent Retrieval Stress Test**: Create synthetic dialogues where agents introduce critical slot values (e.g., system-initiative booking) and compare user-only vs. user-agent retrieval performance to identify failure modes.
3. **Schema Complexity Scaling**: Evaluate the retriever-prompt pipeline on a synthetic schema with 2x more slots and domains to test whether demonstration relevance/coverage trade-offs hold under increased complexity.