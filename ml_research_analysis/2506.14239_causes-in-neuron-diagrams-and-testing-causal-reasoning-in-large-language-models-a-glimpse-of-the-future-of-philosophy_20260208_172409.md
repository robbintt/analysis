---
ver: rpa2
title: Causes in neuron diagrams, and testing causal reasoning in Large Language Models.
  A glimpse of the future of philosophy?
arxiv_id: '2506.14239'
source_url: https://arxiv.org/abs/2506.14239
tags:
- occur
- would
- does
- occurring
- cause
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a method to test abstract causal reasoning in
  Large Language Models (LLMs) using neuron diagrams, a tool from philosophy of causation.
  The authors developed a precise definition (DEF-1) to identify causes in these diagrams
  and applied it to test ChatGPT, DeepSeek, and Gemini.
---

# Causes in neuron neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?

## Quick Facts
- arXiv ID: 2506.14239
- Source URL: https://arxiv.org/abs/2506.14239
- Authors: Louis Vervoort; Vitaly Nikolaev
- Reference count: 7
- Key outcome: Advanced LLMs can correctly identify causes in complex neuron diagrams, with Gemini 2.0 Flash achieving 14/25 correct answers on philosophical causal reasoning tests.

## Executive Summary
This paper presents a method to test abstract causal reasoning in Large Language Models using neuron diagrams from philosophy of causation. The authors developed a precise definition (DEF-1) to identify causes in these diagrams and applied it to test ChatGPT, DeepSeek, and Gemini. Results show that advanced LLMs can correctly identify causes in complex, debated cases, with Gemini 2.0 Flash performing best at 14/25 correct answers. The work highlights AI's emerging capacity for causal reasoning and suggests future interdisciplinary collaboration between AI and philosophy.

## Method Summary
The method transcribes visual neuron diagrams into textual conditional logic ("if-then-unless" statements) and queries LLMs to identify causes of specific outcomes. Each diagram specifies temporal rules where neurons fire or inhibit based on other neurons' states. The LLM acts as a logical inference engine over these natural language rules. Performance is evaluated against DEF-1, a formal definition requiring that X is a cause of Y if ¬X (ceteris paribus, off-path under max blocking) implies ¬Y. Tests were conducted using inference-only evaluation with default API settings across multiple models.

## Key Results
- ChatGPT-4 achieved 13/25 correct answers on cause identification tasks
- Gemini 2.0 Flash performed best with 14/25 fully correct answers
- DeepSeek-R1 achieved 10/25 correct answers
- Performance drops significantly on complex diagrams with crossed paths or interrupted chains

## Why This Works (Mechanism)

### Mechanism 1: Semantic Transduction of Abstract Causal Graphs
LLMs process abstract causal structures by parsing explicit textual "if-then-unless" logic chains derived from visual diagrams. The method relies on textual transcription, converting visual neuron diagrams into precise conditional statements. The LLM acts as a logical inference engine over these natural language rules, tracking variable states over time steps rather than probabilistic next-token guessing.

### Mechanism 2: Counterfactual "Off-Path" Blocking
High-performing LLMs approximate the specific philosophical definition of cause (DEF-1) by maintaining "off-path" blocking states during counterfactual simulation. To correctly identify causes in "early preemption" scenarios, the model must effectively implement the "off-path under max blocking" clause, simulating the removal of a cause while keeping blocking of backup paths active.

### Mechanism 3: Emergent Definition via Human-AI Interplay
Testing AI on philosophical problems can refine human theory; the LLM's "intuitive" verdicts exposed a need for a more precise definition of cause, leading to the formulation of DEF-1. The authors reverse-engineered a formal definition by observing where LLM outputs aligned with "intuitive" expert verdicts.

## Foundational Learning

- **Concept: Neuron Diagrams (Lewis-style)**
  - Why needed here: These are the test bed. They are directed acyclic graphs where nodes (neurons) fire or inhibit based on strict temporal rules.
  - Quick check question: In a diagram where A fires at t1 and inhibits B at t2, does B fire if A is removed? (Answer: Yes, assuming B's other conditions are met).

- **Concept: Early Preemption (Redundant Causation)**
  - Why needed here: This is the core "hard case." It occurs when a primary cause produces an effect while simultaneously blocking a backup cause.
  - Quick check question: Why does the "simple counterfactual" rule (if not C, then not E) fail in early preemption? (Answer: Because E would still have occurred via the backup cause A).

- **Concept: DEF-1 (The "Gold Standard")**
  - Why needed here: This is the metric for success. It modifies the counterfactual test by holding "off-path" blocks constant.
  - Quick check question: According to DEF-1, when simulating "if not C," do we maintain the block on the backup path? (Answer: Yes, "off-path under max blocking").

## Architecture Onboarding

- **Component map:** Visual Neuron Diagrams -> Transcription Engine (Textual conditional logic) -> LLM Subject (ChatGPT/DeepSeek/Gemini) -> DEF-1 Evaluator + Philosophical Intuition
- **Critical path:** The prompt design. You must transcribe visual arrows/dots into "If X occurs... unless Y occurs" syntax. The system's success depends entirely on the precision of this textual mapping.
- **Design tradeoffs:**
  - *Precision vs. Abstraction:* The paper uses explicit time steps (t1 < t2). Removing these abstractions reduces test reliability.
  - *Simplicity vs. Complexity:* Testing "classic" diagrams ensures DEF-1 applicability; testing complex "crossed" diagrams pushes models to failure.
- **Failure signatures:**
  - Transitivity Errors: Model asserts C→D and D→E, but denies C→E
  - Omission Blindness: Failing to identify "non-firing" neurons as causes
  - Hallucination: Asserting E fires when visual inspection proves it is blocked
- **First 3 experiments:**
  1. Reproduce "Early Preemption" (Diagram 1): Transcribe Fig 1 into text; verify if LLM identifies C (and not A) as the cause.
  2. Variable Abstraction Stress Test: Replace "Neuron A/B/C" with "Event Xness/Yness/Zness" to see if performance drops.
  3. Complexity Limit Search: Incrementally add time steps and crossings to identify where reasoning accuracy drops below 50%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a systematic study of LLM test scores as a function of neuron diagram complexity provide a quantitative measure of an AI's depth-of-reasoning?
- Basis in paper: [explicit] The authors suggest investigating "test score... as a function of diagram complexity" using numerical measures like neurons, time steps, or forks.
- Why unresolved: Preliminary tests suggest reasoning collapses with sufficient complexity, but a systematic study linking specific complexity metrics to performance has not been conducted.
- What evidence would resolve it: Large-scale benchmark results showing performance decay curves relative to quantified diagram complexity.

### Open Question 2
- Question: Are LLM causal reasoning capabilities robust under prompt variations, such as rephrasing or replacing abstract labels with concrete scenarios?
- Basis in paper: [explicit] The authors list testing "paraphrased prompts" and replacing diagrams with "concrete situations" as specific lines of further research.
- Why unresolved: Small-scale tests suggest stability, but performance consistency across diverse phrasings and abstract/concrete mappings remains unverified.
- What evidence would resolve it: Statistical comparison of accuracy rates across standardized, paraphrased, and concrete-scenario prompts for the same underlying diagrams.

### Open Question 3
- Question: To what extent does the counterfactual analysis via neuron diagrams overlap with other theories of causation?
- Basis in paper: [explicit] The text states that the overlap with functional, manipulability, and regularity models "remains an open problem."
- Why unresolved: Neuron diagrams do not necessarily capture all aspects of causation, potentially limiting the generalizability of the proposed DEF-1 definition.
- What evidence would resolve it: A formal analysis mapping DEF-1 outcomes to theorems or predictions in other established causal frameworks.

## Limitations
- Unknown LLM API versions and sampling parameters could significantly affect reproducibility
- Reliance on textual transcription of visual diagrams introduces potential ambiguity in interpretation
- The gold standard (DEF-1) was derived partly from LLM responses, creating potential circular validation
- Performance drops dramatically on complex diagrams (Diagrams 22-25), suggesting the method may not scale to more sophisticated causal structures

## Confidence
- **High confidence**: LLMs can perform causal reasoning on simple neuron diagrams; the transcription method works for basic cases.
- **Medium confidence**: DEF-1 provides a workable definition for classic neuron diagrams; LLMs show emergent counterfactual reasoning capabilities.
- **Low confidence**: The definition applies beyond classic diagrams; performance differences between models reflect fundamental reasoning capacity rather than prompt sensitivity.

## Next Checks
1. **Version Control Validation**: Reproduce results using specified exact API versions/timestamps to isolate model version effects from reasoning capability.
2. **Prompt Robustness Test**: Systematically vary temperature and prompt formatting to determine sensitivity to implementation details.
3. **Complexity Gradient Analysis**: Extend testing to diagrams with 5+ time steps and multiple crossed paths to precisely map the complexity threshold where LLM reasoning fails.