---
ver: rpa2
title: Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation
arxiv_id: '2505.13507'
source_url: https://arxiv.org/abs/2505.13507
tags:
- domain
- samples
- clip
- unknown
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a novel Open-Set Domain Adaptation (OSDA)
  method using vision-language models to address the challenge of aligning known-class
  distributions across domains while identifying unknown categories in the target
  domain. The key innovations are: (1) Prompt-driven cross-domain alignment: Learnable
  textual prompts conditioned on domain discrepancy metrics dynamically adapt CLIP''s
  text encoder, enabling semantic consistency between source and target domains without
  explicit unknown-class supervision; (2) Gradient-aware open-set separation: A gradient
  analysis module quantifies domain shift by comparing the L2-norm of gradients from
  the learned prompts, where known/unknown samples exhibit statistically distinct
  gradient behaviors.'
---

# Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation

## Quick Facts
- arXiv ID: 2505.13507
- Source URL: https://arxiv.org/abs/2505.13507
- Reference count: 28
- Authors: Haoyang Chen
- Key outcome: Proposed OSDA method achieves superior performance in classifying known classes and identifying unknown classes in target domain using gradient-aware separation with vision-language models

## Executive Summary
This paper proposes a novel Open-Set Domain Adaptation (OSDA) method using vision-language models to address the challenge of aligning known-class distributions across domains while identifying unknown categories in the target domain. The key innovations are prompt-driven cross-domain alignment and gradient-aware open-set separation, which enable semantic consistency between source and target domains without explicit unknown-class supervision. Experiments on the Office-Home dataset show the proposed method consistently outperforms CLIP and standard CoOp baselines across various adaptation scenarios.

## Method Summary
The method uses CLIP with learnable textual prompts initialized as "a photo of a" to perform open-set domain adaptation. The visual encoder remains frozen while text prompts are updated via SGD to minimize classification loss on source-domain labeled data and pseudo-labeled target-domain data. A gradient analysis module quantifies domain shift by comparing the L2-norm of gradients from the learned prompts, where known/unknown samples exhibit statistically distinct gradient behaviors. The approach uses bifurcated loss design: cross-entropy loss for identified known samples and KL divergence regularization toward uniform distribution for likely unknown samples. Training uses SGD with lr=1e-4, 5 epochs, and threshold selection based on retaining 90% of source samples as ID.

## Key Results
- Outperforms CLIP and standard CoOp baselines across 12 Office-Home transfer scenarios
- Achieves superior performance in classifying known classes and identifying unknown classes in target domain
- Ablation studies confirm critical role of gradient norm analysis in separating known and unknown samples
- Shows effectiveness of gradient-aware separation mechanism for open-set domain adaptation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient norms with respect to learnable prompts provide a discriminative signal for separating known-class samples from unknown-class samples in the target domain.
- **Mechanism:** The gradient computation for learnable prompts includes a Jacobian term (∂v/∂w) that amplifies differences in prediction distributions. Known samples typically produce highly peaked softmax outputs (e.g., one class at 0.99), resulting in sparse non-zero columns in the gradient matrix. Unknown samples produce flatter distributions (e.g., multiple classes at ~0.3), activating more gradient pathways and yielding larger L2 norms.
- **Core assumption:** The visual encoder remains frozen; gradient signals flow only through learnable textual prompts. Unknown-class samples in the target domain will produce less confident (flatter) softmax distributions than known-class samples.
- **Evidence anchors:**
  - [abstract] "gradient analysis module quantifies domain shift by comparing the L2-norm of gradients from the learned prompts, where known/unknown samples exhibit statistically distinct gradient behaviors"
  - [section 3.1] "In summary, under the settings of this paper, samples with larger gradient norms should be considered unknown, and vice versa."
  - [corpus] Corpus evidence is weak for this specific gradient-inversion phenomenon in prompt-learning contexts; related work on GradNorm assumes the opposite pattern.
- **Break condition:** If unknown samples produce similarly peaked softmax outputs (e.g., when they visually resemble known classes), gradient norm separation degrades.

### Mechanism 2
- **Claim:** Learnable textual prompts conditioned on domain discrepancy enable cross-domain semantic alignment without requiring unknown-class supervision.
- **Mechanism:** Prompts are initialized as "a photo of a" and updated via SGD to minimize classification loss on source-domain labeled data and pseudo-labeled target-domain data. The prompts adapt the text encoder's representations to bridge domain-specific visual variations while preserving class semantics.
- **Core assumption:** The pretrained CLIP joint embedding space contains sufficient semantic structure that prompt tuning can exploit for domain alignment.
- **Evidence anchors:**
  - [abstract] "Learnable textual prompts conditioned on domain discrepancy metrics dynamically adapt CLIP's text encoder, enabling semantic consistency between source and target domains without explicit unknown-class supervision"
  - [section 1] "These prompts dynamically adjust CLIP's text encoder, enabling semantic consistency across domains"
  - [corpus] Related paper "Towards Unsupervised Open-Set Graph Domain Adaptation via Dual Reprogramming" shows similar reprogramming-based alignment in graph domains.
- **Break condition:** If the domain shift is primarily lexical rather than visual (e.g., same objects with different linguistic descriptions), text-only prompt tuning may be insufficient.

### Mechanism 3
- **Claim:** Bifurcated loss design prevents error accumulation by treating identified known and unknown samples differently during training.
- **Mechanism:** For samples identified as shared-class, cross-entropy loss reinforces pseudo-label predictions. For samples likely from unknown classes, KL divergence regularizes predictions toward a uniform distribution, preventing overconfident misclassification and maintaining representation openness.
- **Core assumption:** The gradient-based identification correctly separates known/unknown samples with acceptable error rates.
- **Evidence anchors:**
  - [section 3.2] "For identified shared-class samples, we enforce prediction consistency with their pseudo-labels through cross-entropy minimization... For uncertain samples likely belonging to novel categories, we instead regularize their output distributions to approximate a uniform distribution via KL divergence minimization"
  - [table 5] Ablation shows "+CE" improves Acc10 from 66.82 to 68.15; combined method maintains performance above baseline.
  - [corpus] "Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering" uses related filtering strategies for open-set scenarios.
- **Break condition:** If initial unknown detection has high false positive rates, shared-class samples incorrectly regularized to uniform will degrade known-class accuracy.

## Foundational Learning

- **Concept: Contrastive Language-Image Pretraining (CLIP)**
  - Why needed here: The entire method builds on CLIP's joint image-text embedding space. Understanding that CLIP encodes images and text into a shared 512-dim space via cosine similarity is essential for interpreting the gradient flow analysis.
  - Quick check question: Given an image and K class text prompts, how does CLIP produce a probability distribution over classes?

- **Concept: Soft Prompt Learning (CoOp)**
  - Why needed here: The method extends CoOp's learnable prompt approach. Understanding that prompts are continuous vectors optimized via backpropagation (not discrete tokens) is prerequisite for understanding the gradient mechanism.
  - Quick check question: In CoOp, what parameters are updated during training, and what remains frozen?

- **Concept: Gradient-based OOD Detection (GradNorm)**
  - Why needed here: The paper explicitly contrasts its findings with GradNorm. Understanding that GradNorm traditionally finds ID samples have larger gradients (due to direct loss-to-logits gradient) clarifies why the prompt-learning case inverts this pattern.
  - Quick check question: Why does GradNorm use KL(u||p) rather than KL(p||u)?

## Architecture Onboarding

- **Component map:** Source/Target Images → CLIP Vision Encoder (frozen) → Image features z ∈ R^512 → Cosine similarity → Softmax → Predictions p. Learnable Prompts w → CLIP Text Encoder (frozen) → Text embeddings v ∈ R^(K×512). Gradient computation: ∂L/∂w via chain rule through prompt-to-embedding Jacobian. Threshold selection: 90th percentile of source-domain gradient norms.

- **Critical path:** The gradient norm calculation (Equation 7) is the core novelty. The Jacobian ∂v/∂w is computed via backprop through the frozen text encoder. The product N·M (where M depends on softmax distribution) determines gradient magnitude differences.

- **Design tradeoffs:**
  - Threshold percentile (90%) trades recall vs precision for unknown detection; higher thresholds reduce false positives but miss more unknowns.
  - Coefficients α=0.1, β=0.01 weight pseudo-label loss vs uniform regularization; the paper does not provide sensitivity analysis.
  - Prompt count (4 prompts) follows CoOp defaults; no ablation on prompt depth.

- **Failure signatures:**
  - If target-domain known-class samples are misclassified as unknown (high FPR), check whether domain shift causes softmax distributions to flatten even for shared classes.
  - If unknown detection AUROC drops significantly below source-domain behavior, the threshold may not transfer; consider recalibrating on a validation subset.
  - If CoOp baseline outperforms the proposed method on known-class accuracy, the KL regularization may be too aggressive (increase β).

- **First 3 experiments:**
  1. **Gradient norm distribution visualization:** Plot histogram of gradient norms for source-domain samples vs manually labeled unknown target samples. Verify the inversion pattern (unknown > known) holds.
  2. **Threshold sensitivity analysis:** Vary the percentile threshold from 80-95% and measure AUROC, FPR95, and CCR@FPR10 on a held-out target domain.
  3. **Ablation on prompt count:** Test with 1, 2, 4, 8 learnable prompts to verify that gradient separation is not an artifact of prompt depth.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed inversion of gradient behavior—where unknown samples yield larger gradient norms than known samples—generalize to other vision-language architectures or prompt tuning strategies beyond CoOp?
- Basis: [explicit] The authors note in Section 3.1 that their experimental finding is "exactly the opposite of the conclusion drawn by GradNorm," and they derive a specific mathematical explanation tied to the CoOp structure.
- Why unresolved: The derivation relies on the specific structure of learnable soft prompts within CLIP's text encoder. It is unclear if this gradient separation is a fundamental property of multimodal models or an artifact of the specific prompt tuning implementation used.
- What evidence would resolve it: Comparative experiments analyzing gradient norm distributions across different VLMs (e.g., BLIP, ALIGN) or different prompt tuning depths to verify if the separation signal persists.

### Open Question 2
- Question: Can the threshold for unknown detection be adapted dynamically based on target domain statistics rather than relying on source domain percentiles?
- Basis: [explicit] The authors state in Section 3.1 that "Relying on manual threshold selection would inevitably introduce information leakage," yet they proceed to select a threshold based on retaining 90% of source domain samples.
- Why unresolved: While the paper identifies the leakage risk of tuning on the test set, the proposed solution still depends on source-domain characteristics (the 90% source ID rate), which may not scale well if the source and target domains have vastly different purity levels or class balances.
- What evidence would resolve it: Ablation studies using adaptive thresholding methods (e.g., percentile-based selection on target pseudo-labels) that do not reference source statistics, showing whether performance is maintained.

### Open Question 3
- Question: How robust is the gradient-aware separation when the domain shift is severe enough to flatten the probability distributions of known target samples?
- Basis: [inferred] The theoretical justification in Section 3.1 assumes known target samples maintain a "dominant class" (e.g., 0.99 probability), while the Results section notes that standard CoOp "deteriorates substantially" under significant distribution gaps.
- Why unresolved: If the domain shift is extreme, known target samples might lose their high-confidence "dominant" probability, potentially making their gradient norms resemble the "flat" distribution of unknowns, causing the separation mechanism to fail.
- What evidence would resolve it: Evaluation on datasets with larger domain gaps than Office-Home (e.g., DomainNet or VisDA) where classifier confidence on known classes is lower, to test the limits of the gradient separation assumption.

## Limitations

- **Dataset specificity**: The method is evaluated only on the Office-Home dataset with 12 domain-transfer scenarios. Performance on more challenging datasets (VisDA, DomainNet) or real-world scenarios with severe domain shifts remains unknown.

- **Gradient inversion dependency**: The core mechanism relies on the inverted gradient norm pattern (unknown > known) compared to traditional GradNorm. This inversion depends critically on the Jacobian term structure in prompt-learning contexts, which may not generalize to other architectures or pretrained models.

- **Threshold calibration stability**: The 90th percentile threshold is set on source-domain gradient norms and assumed to transfer to target domain. No analysis of threshold robustness across varying domain shifts or class distributions is provided.

## Confidence

- **High confidence**: The gradient norm analysis and its relationship to prediction confidence is well-specified and supported by theoretical derivation. The inversion pattern relative to GradNorm is explicitly addressed.

- **Medium confidence**: Prompt-driven alignment is standard practice in vision-language models, but the specific conditioning on domain discrepancy metrics lacks detailed specification of the metric computation.

- **Medium confidence**: The bifurcated loss design is theoretically sound, but effectiveness depends heavily on the accuracy of the gradient-based unknown detection, which is not thoroughly validated.

## Next Checks

1. **Gradient norm distribution validation**: Generate and plot histograms of gradient norms for source-domain known classes vs. manually labeled unknown target samples across all 12 transfer scenarios. Verify that the inversion pattern (unknown > known) consistently holds and that threshold calibration is robust.

2. **Baseline ablation with CoOp and CLIP**: Implement and report performance of (a) zero-shot CLIP with temperature scaling, (b) standard CoOp prompt tuning without gradient analysis, and (c) GradNorm-style gradient-based OOD detection. This will isolate the contribution of the proposed gradient-aware separation mechanism.

3. **Threshold transfer analysis**: For each transfer scenario, compute the optimal threshold on a held-out validation subset and compare performance to the source-based 90th percentile threshold. Measure the gap in AUROC, FPR95, and CCR@FPR10 to quantify threshold calibration robustness.