---
ver: rpa2
title: 'Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization
  for Multi-Agent LLMs'
arxiv_id: '2511.06134'
source_url: https://arxiv.org/abs/2511.06134
tags:
- arxiv
- multi-agent
- preprint
- zhang
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAESTRO, a multi-agent LLM collaboration
  framework that balances divergent exploration with convergent synthesis. The core
  innovation is separating these cognitive modes into parallel Execution Agents for
  diverse solution generation and a Central Agent for discriminative selection, guided
  by a novel Conditional Listwise Policy Optimization (CLPO) that disentangles decision
  and rationale signals.
---

# Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs

## Quick Facts
- arXiv ID: 2511.06134
- Source URL: https://arxiv.org/abs/2511.06134
- Reference count: 30
- Primary result: MAESTRO with CLPO achieves 6-10% absolute accuracy gains over state-of-the-art multi-agent methods across reasoning benchmarks

## Executive Summary
MAESTRO introduces a multi-agent LLM collaboration framework that separates divergent exploration from convergent synthesis. The system employs parallel Execution Agents for diverse solution generation and a specialized Central Agent for discriminative selection, trained via a novel Conditional Listwise Policy Optimization (CLPO) that disentangles decision and rationale learning signals. Experimental results show consistent performance improvements across mathematical reasoning, general reasoning, and code synthesis benchmarks, with gains averaging 6% and reaching up to 10% on challenging tasks.

## Method Summary
MAESTRO operates through three parallel Execution Agents generating K candidate solutions each per round, with a Central Agent selecting the most promising candidate based on CLPO-trained policy. The framework uses LoRA fine-tuning with specific hyperparameters (rank=16, α=32, dropout=0.05) and temperature scaling (0.7 for execution, 0.0 for central). Training employs a composite loss function combining decision-focused policy gradient, listwise ranking over justifications, KL regularization, and entropy bonus. The system processes 3 rounds with N×K candidates per round, using nucleus sampling (p=0.95) and maximum 512 tokens per generation.

## Key Results
- CLPO consistently outperforms state-of-the-art multi-agent methods with average 6% absolute accuracy gains
- Maximum improvements reach 10% on challenging mathematical reasoning tasks (AIME, AMC)
- Framework demonstrates robustness across different LLM backbones and zero-shot settings
- Performance benefits scale with agent population up to 4 agents before diminishing returns

## Why This Works (Mechanism)

### Mechanism 1: Divergence as Collective Exploration
Parallel execution agents increase solution coverage probability through independent exploration. Multiple agents sample from the solution space, generating diverse reasoning paths that increase the likelihood of finding correct solutions. Success measured by coverage probability $p_t$ in fixed resource budget.

### Mechanism 2: Convergence as Listwise Bayesian Synthesis
Central agent trained to rank candidates approximates Bayesian decision over posterior probabilities. Evaluates all candidates and selects most promising solution based on learned discriminative patterns. Success measured by identification probability $q_t$.

### Mechanism 3: Conditional Listwise Policy Optimization (CLPO)
Separates learning signals for decisions and rationales to improve credit assignment. Combines decision-focused policy gradient ($L_{choice}$) with listwise ranking loss over justifications ($L_{reason\_rank}$), preventing reasoning style from interfering with discrete choice learning.

## Foundational Learning

- **Exploration vs. Exploitation Trade-off**
  - Why needed here: MAESTRO explicitly frames divergence as exploration and convergence as exploitation
  - Quick check question: Can you explain why increasing exploration (more agents/samples) eventually yields diminishing returns?

- **Credit Assignment in Multi-Agent RL**
  - Why needed here: CLPO's core contribution is disentangling credit for decisions vs. rationales
  - Quick check question: Why does global reward fail to identify which agent's reasoning was actually correct?

- **Listwise Ranking Loss**
  - Why needed here: $L_{reason\_rank}$ uses ListNet-style ranking over candidate justifications
  - Quick check question: How does listwise ranking differ from pairwise or pointwise approaches for learning to rank?

## Architecture Onboarding

- **Component map:** Execution Agents (N agents) -> Generate K candidates each -> Central Agent -> Select best candidate -> Broadcast selection to all agents
- **Critical path:** Initialize execution agents → Generate candidate pool (N × K samples) → Central agent evaluates and selects → Broadcast selection → Repeat for R rounds → Train central agent with CLPO
- **Design tradeoffs:** More agents → higher coverage but increased compute; More rounds → better evidence aggregation but risk of herding; Higher K → diminishing returns after K≈3 due to correlated outputs
- **Failure signatures:** Low identification $q_t$ despite high coverage $p_t$ (central selector not learning); Accuracy degrades with more rounds (herding); CLPO training unstable (check λ_rank balance)
- **First 3 experiments:** 1) Baseline comparison: MAESTRO vs. self-consistency vs. debate on GSM8K; 2) Ablation on central coordination: CENTRAL-SELECT vs. CENTRAL-GEN vs. CENTRAL-GEN+SC; 3) CLPO component ablation: Remove $L_{choice}$ and $L_{reason\_rank}$ separately

## Open Questions the Paper Calls Out

### Open Question 1
Can a unified policy objective be developed to jointly optimize both the execution agents (exploration) and the central agent (synthesis)? The paper currently freezes execution agents and applies CLPO solely to the central agent, treating exploration as a static input generation process rather than a learnable policy.

### Open Question 2
How can continuous learning paradigms be designed to enable multi-agent collectives to refine collaboration dynamics through self-improvement over time? The current framework is evaluated on static benchmarks and lacks mechanisms for online adaptation or long-term memory retention across distinct problem-solving sessions.

### Open Question 3
Can the identification probability be preserved to prevent performance saturation when scaling the system beyond four agents? The central agent struggles to filter out correlated or noisy outputs (distractors) as the candidate pool grows, suggesting the list-wise ranking mechanism may lose discriminative power at scale.

## Limitations

- Exact reward function formulation combining correctness and rationale quality is unspecified
- Optimal λ_rank coefficient value is tuned but not disclosed
- SFT data construction process from exploration trajectories lacks sufficient detail
- Herding/bias amplification risk across multiple rounds not fully characterized

## Confidence

- **High confidence** in the conceptual framework: Separation of divergent exploration and convergent synthesis into distinct agent roles is theoretically sound
- **Medium confidence** in the empirical results: Performance gains are consistently reported but ablation studies could be more thorough
- **Low confidence** in the CLPO training stability claims: Limited analysis of training dynamics and hyperparameter sensitivity

## Next Checks

1. **Ablation of reward components**: Systematically remove or isolate correctness and rationale quality components of reward function to determine relative contribution
2. **Round complexity analysis**: Conduct experiments varying R beyond 3 rounds to identify optimal trade-off between evidence aggregation and herding risk
3. **CLPO hyperparameter sensitivity**: Perform systematic sweep of λ_rank, λ_kl, and λ_ent values to establish robustness and identify performance degradation regions