---
ver: rpa2
title: The Condition Number as a Scale-Invariant Proxy for Information Encoding in
  Neural Units
arxiv_id: '2506.16289'
source_url: https://arxiv.org/abs/2506.16289
tags:
- information
- condition
- number
- forgetting
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using the condition number of neural network
  weight tensors as a scale-invariant proxy for information encoding. The condition
  number, defined as the ratio of largest to smallest singular values, indicates how
  much a unit has learned to selectively amplify or compress information.
---

# The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units

## Quick Facts
- arXiv ID: 2506.16289
- Source URL: https://arxiv.org/abs/2506.16289
- Reference count: 19
- One-line result: KappaTune selectively fine-tunes low-condition-number tensors to mitigate catastrophic forgetting, achieving 6x reduction on OLMoE and 2.8x on DeepSeek-V2-Lite vs LoRA baselines.

## Executive Summary
This paper proposes using the condition number of neural network weight tensors as a scale-invariant proxy for information encoding. The condition number, defined as the ratio of largest to smallest singular values, indicates how much a unit has learned to selectively amplify or compress information. Theoretical analysis shows that for a fixed weight norm, a high condition number corresponds to reduced information transfer, reflecting specialized encoding. The linear stage entropy bound provides an upper limit for post-activation information in contractive, element-wise nonlinearities. Empirically, the proposed KappaTune method selectively fine-tunes low-condition-number tensors in pre-trained models, achieving superior catastrophic forgetting mitigation compared to LoRA baselines.

## Method Summary
KappaTune computes the condition number κ(W) = σ_max/σ_min via SVD for each eligible weight tensor, ranks them by κ ascending, and unfreezes the K lowest-κ tensors while freezing all others. The method excludes biases and normalization parameters from selection. During fine-tuning, only the selected tensors receive gradient updates. The approach is tested on OLMoE (1B active/7B total) with K=75, DeepSeek-V2-Lite (16B total) with K=300, and Llama-3.2-8B-Instruct for multimodal ASR adaptation with K=100.

## Key Results
- On OLMoE sarcasm classification, KappaTune reduces forgetting by 6x while maintaining comparable accuracy
- For DeepSeek-V2-Lite on IMDB sentiment, KappaTune improves generalization and reduces forgetting by 2.8x compared to uniform LoRA placement
- In multimodal LLM adaptation, unfreezing low-κ tensors leads to monotonic WER reduction (5.61%), while unfreezing high-κ tensors causes degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High condition number indicates anisotropic transformations that selectively amplify task-relevant information while compressing irrelevant inputs.
- Mechanism: Under fixed Frobenius norm constraints, a concentrated singular value distribution (high κ) reduces the log-volume scaling factor, lowering differential entropy of outputs. This reflects specialization—discriminative features are preserved while irrelevant variations are attenuated.
- Core assumption: Inputs are approximately Gaussian-distributed (linear analysis); the model is well-trained (high κ reflects meaningful encoding, not numerical pathology).
- Evidence anchors:
  - [abstract] "high condition numbers indicate anisotropic transformations that selectively amplify task-relevant information while compressing irrelevant inputs"
  - [Section 3, Theorem 1] Proves that under fixed Frobenius norm, entropy is maximized when κ = 1 (uniform singular values); deviation implies compression.
  - [corpus] Related work on Lipschitz constants and condition numbers for robust networks (arXiv:2503.20454) supports κ as a structural signal, though does not directly validate the entropy link.

### Mechanism 2
- Claim: Selectively fine-tuning low-κ tensors mitigates catastrophic forgetting without requiring access to pre-training data.
- Mechanism: Low-κ tensors have more uniform singular value distributions, maximizing entropy and indicating less specialized encoding. Updating these preserves highly anisotropic (high-κ) tensors that encode critical pre-trained knowledge.
- Core assumption: Low-κ tensors are sufficiently generic to adapt to new tasks without holding task-critical pre-trained information; the budget K captures enough capacity for adaptation.
- Evidence anchors:
  - [abstract] "KappaTune leverages these insights to selectively fine-tune low-kappa tensors... significantly mitigating catastrophic forgetting"
  - [Section 4.2, Table 1] KappaTune shows ∆PPL = 0.2363 vs LoRA's 1.4783 on WikiText-2, with comparable accuracy.
  - [Section 4.4, Figure 1] Unfreezing lowest-κ tensors achieves monotonic WER improvement (5.61%), while highest-κ selection degrades after initial gains.
  - [corpus] No direct corpus validation for κ-guided selection; related work focuses on regularization rather than selective unfreezing.

### Mechanism 3
- Claim: Linear-stage entropy bounds propagate through contractive element-wise nonlinearities, extending κ's relevance beyond linear layers.
- Mechanism: For activations with |ϕ'(z)| ≤ 1 (e.g., ReLU, tanh, sigmoid), the Jacobian determinant satisfies log|det J_ϕ| ≤ 0, so post-activation entropy h(ϕ(Z)) ≤ h(Z). Thus, the linear entropy maximum (at κ = 1) remains an upper bound for practical networks.
- Core assumption: Nonlinearities are contractive almost everywhere; coupling between layers does not fundamentally alter the bound structure.
- Evidence anchors:
  - [Section 3, Remark 2] Derives h(ϕ(Z)) ≤ h(Z) for contractive activations, establishing the bound.
  - [abstract] "linear stage entropy bound provides an upper limit on post-activation information for contractive, element-wise nonlinearities"
  - [corpus] No corpus papers directly address entropy bounds through nonlinearities in this context.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and Condition Number
  - Why needed here: The method relies on computing κ(W) = σ_max / σ_min for each weight tensor to rank specialization.
  - Quick check question: Given a 64×64 weight matrix with σ_max = 8.0 and σ_min = 0.1, what is its condition number? (Answer: 80)

- Concept: Differential Entropy for Continuous Random Variables
  - Why needed here: Theoretical justification links κ to entropy changes under linear transformations of Gaussian inputs.
  - Quick check question: For a multivariate Gaussian output Y = WX with spherical input covariance, which term in h(Y) depends on the singular values of W? (Answer: The sum of log₂(σ_i) terms)

- Concept: Catastrophic Forgetting in Continual Learning
  - Why needed here: KappaTune targets CF mitigation; understanding why standard fine-tuning fails clarifies the method's value.
  - Quick check question: Why do methods like EWC require access to prior task data or statistics? (Answer: They compute parameter importance relative to previous tasks, needing gradients or Fisher information from old data.)

## Architecture Onboarding

- Component map:
  κ-computation module -> Tensor selector -> Training loop

- Critical path:
  1. Load pre-trained model
  2. Enumerate all eligible weight tensors (skip biases, layernorm params)
  3. Reshape conv kernels to (d_out, d_in), compute κ for each
  4. Sort by κ ascending, select K lowest
  5. Freeze all params, unfreeze selected tensors
  6. Train on new task with standard optimizer

- Design tradeoffs:
  - Smaller K → less forgetting but may underfit complex tasks
  - Larger K → more adaptation capacity but higher forgetting risk
  - MoE architectures offer finer granularity (expert-level selection) vs dense models
  - Assumption: One-time κ computation overhead is acceptable (paper reports ~370s for large models on CPU)

- Failure signatures:
  - WER or loss plateaus early → K may be too small; increase budget
  - Forgetting still high (∆PPL > baseline) → check if κ computation excluded critical tensors or if task requires modifying high-κ representations
  - Numerical instability during training → verify tensors are not near-singular (σ_min ≈ 0); consider adding small regularization

- First 3 experiments:
  1. Baseline validation: On a small pre-trained model (e.g., BERT-base), compute κ distribution across layers. Confirm that later task-specific layers have higher κ than early embedding layers.
  2. Ablation on budget K: Fine-tune on a classification task (e.g., GLUE) with K ∈ {10%, 25%, 50%} of tensors. Plot accuracy vs. ∆PPL to find the operating frontier.
  3. Inverse selection test: Compare lowest-κ vs. highest-κ selection on the same task. Expect highest-κ to show faster initial learning but greater forgetting (replicating Figure 1 pattern).

## Open Questions the Paper Calls Out

- Question: How does the relationship between condition number and information encoding change when the Gaussian input assumption is relaxed to realistic, non-Gaussian data distributions?
  - Basis in paper: [explicit] The conclusion states: "Future work should extend this analysis beyond Gaussian assumptions to realistic data distributions."
  - Why unresolved: The theoretical analysis (Theorem 1, entropy derivations) relies critically on multivariate Gaussian input assumptions; real neural activations often exhibit heavy-tailed, sparse, or multi-modal distributions.
  - What evidence would resolve it: Theoretical analysis extending Proposition 1 and Theorem 1 to non-Gaussian distributions, paired with empirical measurements comparing predicted vs. observed encoding behavior on real data.

- Question: What information encoding properties does the full singular value spectrum capture that the condition number alone (ratio of extreme singular values) does not?
  - Basis in paper: [explicit] The conclusion calls for "rigorously quantifying how the full singular value spectrum, not only the condition number, governs information encoding in deep networks."
  - Why unresolved: The condition number is a scalar summary that may obscure important structure in the spectrum; the paper does not analyze intermediate singular values.
  - What evidence would resolve it: Systematic ablation studies varying the full singular value distribution while controlling condition number, measuring downstream task performance and entropy metrics.

- Question: Is a high condition number a necessary condition for significant discriminative knowledge encoding in well-trained models, as conjectured?
  - Basis in paper: [explicit] Conjecture 1 states this hypothesis "warrants further empirical and theoretical investigation, particularly in the context of non-linear transformations and real-world data distributions."
  - Why unresolved: The paper provides theoretical motivation but treats necessity as conjecture; counterexamples may exist where low-kappa units encode important knowledge.
  - What evidence would resolve it: Large-scale analysis correlating condition numbers with removal-based importance scores across diverse trained networks, testing whether high-kappa units are consistently more critical.

## Limitations

- The method assumes low-κ tensors are sufficiently generic for adaptation, but doesn't explore scenarios where task-critical knowledge might be distributed across low-κ tensors, potentially leading to underfitting.
- The entropy bounds rely on contractive nonlinearities, which may not hold for all modern architectures using learned affine transformations with gain > 1.
- Numerical stability of SVD computations for very large tensors and sensitivity to small singular values (σ_min ≈ 0) are unaddressed.

## Confidence

- **High Confidence**: The empirical demonstration that selective fine-tuning of low-κ tensors outperforms uniform LoRA placement and reduces catastrophic forgetting. The ablation showing inverse selection (highest-κ) causes degradation provides strong validation of the core mechanism.
- **Medium Confidence**: The theoretical link between condition number and information encoding via entropy bounds. While the mathematical derivation is sound under stated assumptions (Gaussian inputs, contractive nonlinearities), the practical relevance depends on input distribution characteristics and layer coupling effects not fully explored.
- **Low Confidence**: The generalizability of fixed K values across different model scales and task complexities. The paper doesn't systematically investigate the K-tuning process or establish clear guidelines for selecting the adaptation budget.

## Next Checks

1. **Distribution Validation**: Analyze the κ distribution across all layers in a baseline model (e.g., BERT-base) to verify the paper's claim that later task-specific layers have higher κ than early embedding layers. Plot κ histograms by layer type.

2. **Budget Sensitivity Analysis**: Conduct a systematic ablation study varying K as a percentage of total tensors (e.g., 10%, 25%, 50%, 100%) on a standard benchmark (e.g., GLUE tasks). Plot accuracy vs. ΔPPL to identify the optimal operating frontier and test the paper's claim that larger K increases adaptation capacity but also forgetting risk.

3. **Task Complexity Stress Test**: Apply KappaTune to increasingly complex adaptation tasks (e.g., multi-domain sentiment analysis, few-shot learning) and measure whether the low-κ selection strategy remains effective or if task-critical knowledge distribution across tensors causes performance degradation.