---
ver: rpa2
title: Search-Time Data Contamination
arxiv_id: '2508.13180'
source_url: https://arxiv.org/abs/2508.13180
tags:
- agents
- huggingface
- contamination
- sonar
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Search-time contamination occurs when evaluation data is accessed
  during retrieval in search-based LLM agents, undermining benchmark integrity. The
  authors identify this novel leakage mode where agents retrieve sources containing
  both benchmark questions and ground truth answers, enabling copying rather than
  genuine reasoning.
---

# Search-Time Data Contamination

## Quick Facts
- arXiv ID: 2508.13180
- Source URL: https://arxiv.org/abs/2508.13180
- Reference count: 32
- Search-time contamination occurs when evaluation data is accessed during retrieval in search-based LLM agents, undermining benchmark integrity

## Executive Summary
Search-time contamination represents a novel challenge in evaluating search-based large language model (LLM) agents, where agents access and utilize evaluation data during the retrieval phase of their operation. This contamination undermines the integrity of benchmarks by enabling agents to copy answers directly from sources rather than demonstrating genuine reasoning capabilities. The authors demonstrate that approximately 3% of questions across multiple benchmarks (HLE, SimpleQA, GPQA) are contaminated through sources like HuggingFace-hosted datasets, with contaminated samples showing dramatically inflated accuracy rates compared to uncontaminated ones.

The study reveals that when agents retrieve sources containing both benchmark questions and ground truth answers, they can directly copy solutions rather than engage in reasoning. Using Perplexity's Sonar agents, the research shows contaminated samples achieving 100% accuracy versus 7% on uncontaminated samples for Sonar Pro and Sonar Reasoning Pro, with Sonar Deep Research showing 20% accuracy gains. Blocking HuggingFace domains reduces contaminated sample accuracy by approximately 15%, though ablation experiments indicate this accounts for only a small portion of overall contamination. The findings highlight the need for comprehensive source filtering and transparent contamination auditing to ensure trustworthy evaluation of search-based agents.

## Method Summary
The authors systematically identified search-time data contamination by analyzing how Perplexity's Sonar agents access evaluation data during retrieval across multiple benchmarks. They employed a multi-pronged approach: first, examining agent behavior on HLE, SimpleQA, and GPQA benchmarks to identify contamination patterns; second, implementing domain blocking experiments (particularly targeting HuggingFace) to measure the impact on accuracy; and third, conducting ablation studies to characterize the contribution of different contamination sources. The methodology involved tracking which sources agents accessed during retrieval and correlating this with accuracy differences between contaminated and uncontaminated samples, revealing dramatic performance differences that indicate direct copying rather than reasoning.

## Key Results
- Approximately 3% of benchmark questions are contaminated through sources like HuggingFace-hosted datasets
- Contaminated samples show 100% accuracy versus 7% on uncontaminated samples for Sonar Pro and Sonar Reasoning Pro
- Blocking HuggingFace domains reduces contaminated sample accuracy by approximately 15%, though HuggingFace accounts for only a small portion of total contamination

## Why This Works (Mechanism)
Search-time contamination works because search-based LLM agents retrieve external sources during their operation, and when these sources contain both the benchmark questions and their ground truth answers, the agents can directly access and copy the solutions. The mechanism exploits the fundamental design of search-based agents that rely on external information retrieval, creating a vulnerability where evaluation data becomes part of the retrieval corpus. This differs from traditional data contamination which occurs during training, as it happens at inference time during the retrieval phase. The effectiveness of this contamination depends on the agent's ability to identify and extract relevant information from retrieved sources, with more sophisticated agents showing higher susceptibility to copying when contamination is present.

## Foundational Learning
**Data Contamination**: Why needed - Understanding different types of data contamination is crucial for maintaining benchmark integrity. Quick check - Can distinguish between pre-training contamination and search-time contamination mechanisms.

**Retrieval-Augmented Generation (RAG)**: Why needed - Search-based agents use RAG architecture, making understanding this component essential for analyzing contamination pathways. Quick check - Can explain how retrieval integrates with generation in agent workflows.

**Benchmark Integrity**: Why needed - Contamination directly undermines the validity of performance measurements used to evaluate and compare AI systems. Quick check - Can identify how contamination skews accuracy metrics and what constitutes trustworthy evaluation.

## Architecture Onboarding

Component Map: Search Query -> Retrieval Engine -> Source Collection -> Answer Generation -> Output

Critical Path: Query formulation and retrieval represent the most vulnerable points where contamination can enter the system. The retrieval engine's source selection and the agent's processing of retrieved content determine whether contamination affects the final answer.

Design Tradeoffs: Search-based agents prioritize information access and relevance, which creates the vulnerability window for contamination. Balancing comprehensive retrieval against contamination risk requires careful source filtering and validation mechanisms.

Failure Signatures: Dramatic accuracy spikes on specific question subsets, unusually high precision in answers that reference specific sources, and perfect performance on questions that should require complex reasoning all indicate potential contamination.

First 3 Experiments:
1. Implement source watermarking to track when evaluation data is accessed during retrieval
2. Create controlled contamination scenarios with varying source types to measure impact severity
3. Develop automated detection systems that flag potentially contaminated samples based on answer-source correlation patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The ablation experiments show HuggingFace accounts for only a small portion of contamination, but the authors cannot fully characterize the remaining sources
- The study relies on examining only Perplexity's Sonar agents, limiting generalizability to other search-based systems
- The accuracy gains reported may be influenced by factors beyond direct copying, such as priming effects or repeated exposure to similar content during training

## Confidence
- Contamination identification and basic mechanism: High - The experimental design clearly demonstrates that agents access and utilize evaluation data during retrieval
- Quantitative accuracy impacts: Medium - While the contamination effects are substantial, the exact magnitude may vary across different benchmarks and agent configurations
- Proposed mitigation effectiveness: Low - Blocking HuggingFace domains shows partial effectiveness, but the incomplete understanding of contamination sources makes comprehensive solutions uncertain

## Next Checks
1. Test multiple search-based agents (beyond Perplexity's Sonar) across diverse benchmarks to establish generalizability of contamination patterns and impacts
2. Conduct controlled experiments systematically varying the accessibility of different source types (academic papers, forums, code repositories) to map the complete contamination landscape
3. Implement watermarking techniques on evaluation datasets to precisely track when and how contamination occurs during agent retrieval processes