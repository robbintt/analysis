---
ver: rpa2
title: 'AdaGC: Improving Training Stability for Large Language Model Pretraining'
arxiv_id: '2502.11034'
source_url: https://arxiv.org/abs/2502.11034
tags:
- adagc
- adamw
- training
- loss
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses loss spikes in large language model pretraining,
  which cause training instability and degraded performance. The authors propose AdaGC,
  an adaptive gradient clipping method that applies parameter-specific thresholds
  based on exponential moving averages of gradient norms.
---

# AdaGC: Improving Training Stability for Large Language Model Pretraining

## Quick Facts
- **arXiv ID:** 2502.11034
- **Source URL:** https://arxiv.org/abs/2502.11034
- **Reference count:** 40
- **Primary result:** AdaGC eliminates loss spikes in LLM pretraining, achieving 3.5% perplexity reduction on WikiText and 25% faster convergence on CLIP.

## Executive Summary
This paper addresses the critical problem of loss spikes in large language model pretraining, which cause training instability and degraded performance. The authors propose AdaGC, an adaptive gradient clipping method that applies parameter-specific thresholds based on exponential moving averages of gradient norms. Unlike global clipping, AdaGC dynamically adjusts per-parameter thresholds to handle both temporal gradient norm decay and spatial parameter heterogeneity. The method demonstrates complete loss spike elimination on Llama-2 7B/13B models while maintaining theoretical convergence guarantees.

## Method Summary
AdaGC implements per-parameter adaptive gradient clipping using thresholds derived from exponential moving averages (EMA) of historical gradient norms. The method replaces fixed global thresholds with dynamic local thresholds computed as λ_rel × EMA(gradient_norm), where λ_rel is a relative threshold factor (default 1.05). During the first T_start=100 steps, standard global clipping stabilizes initialization before switching to local clipping. The clipped gradients are then passed to standard optimizers like AdamW or Lion. The approach maintains O(1/√T) convergence rate comparable to standard optimizers while eliminating loss spikes.

## Key Results
- Eliminates loss spikes completely on Llama-2 7B/13B models
- Achieves 3.5% perplexity reduction on WikiText validation set
- Provides 25% faster convergence on CLIP ViT-Base compared to StableAdamW
- Maintains O(1/√T) convergence rate comparable to Adam
- Successfully generalizes to both AdamW and Lion optimizers

## Why This Works (Mechanism)

### Mechanism 1
Adaptive thresholds mitigate the mismatch between fixed clipping values and decaying gradient norms over time. AdaGC replaces the fixed global threshold with a per-parameter dynamic threshold derived from EMA of historical gradient norms, allowing the clipping limit to decay alongside actual gradients.

### Mechanism 2
Per-parameter clipping handles heterogeneous gradient behavior across layers better than global clipping. By computing norms and thresholds independently for each parameter tensor, AdaGC prevents layers with naturally small gradients from being ignored or large-gradient layers from dominating clipping decisions.

### Mechanism 3
Warm-up with Global Clipping stabilizes the EMA initialization phase. During the first T_start steps, standard Global Gradient Clipping prevents the EMA from being corrupted by massive, noisy initialization gradients that could set an erroneously high baseline.

## Foundational Learning

- **Gradient Clipping (Global vs. Local)**: Understanding the difference between global clipping (treating model as one vector) and local/tensor-wise clipping (treating each weight matrix independently) is crucial. Quick check: If layer A has norm 10 and layer B has norm 0.1 with global clip at 1.0, does layer B get updated proportionally? (No, scaling factor is shared).

- **Exponential Moving Average (EMA)**: AdaGC relies on EMA to track "normal" gradient behavior. Understanding how β hyperparameter balances historical stability vs. responsiveness is key. Quick check: If β = 0.98, roughly how much weight is given to current gradient vs. history? (2% current, 98% history).

- **Non-Convex Convergence (O(1/√T))**: The paper proves AdaGC maintains this convergence rate. Understanding this means the optimizer is theoretically guaranteed to find a stationary point at a speed comparable to standard Adam, not necessarily a global minimum.

## Architecture Onboarding

- **Component map:** Raw Gradients → Warmup Check → Global Clipping (if warmup) / Local Clipping (if post-warmup) → Clipped Gradients → Optimizer
- **Critical path:** Calculation of local norm ||g_t,i|| and subsequent element-wise operations inside training loop. Adds memory overhead for storing γ (one scalar per parameter tensor) and compute overhead for norm calculation per tensor.
- **Design tradeoffs:** λ_rel (Threshold): Lower values (e.g., 1.01) clip more aggressively (safer but potentially slower); higher values (e.g., 1.10) allow more variance (riskier, potentially faster). β (EMA Momentum): Higher β (0.99) makes threshold very stable but slow to react; lower β (0.95) is more reactive but noisier.
- **Failure signatures:** Early Divergence (if T_start too short), Stagnation (if λ_rel too low), Sharding Errors (in distributed training).
- **First 3 experiments:** 1) Reproduce GPT-2 345M experiment to verify GlobalGC spikes while AdaGC flattens loss curve. 2) Run λ_rel sweep on Llama-2 Tiny to observe trade-off between convergence speed and spike frequency. 3) Integrate AdaGC into Lion optimizer on CLIP ViT-Base to confirm generalization beyond AdamW.

## Open Questions the Paper Calls Out

- **Long-term Stability:** The paper acknowledges resource constraints prevented long-term training experiments on Llama-2 open-source model, leaving open whether AdaGC maintains benefits through full pre-training duration (>2T tokens).

- **Hyperparameter Transferability:** Optimal hyperparameters (λ_rel=1.05, β=0.98) were discovered on Llama-2 Tiny and applied to 7B/13B models without retuning, raising questions about transferability to significantly larger architectures (70B+ parameters).

- **Theoretical Learning Rate Bounds:** While AdaGC empirically tolerates 10× larger learning rates and 16× larger batch sizes, the paper doesn't theoretically quantify extension of the "stable region" for hyperparameters compared to GlobalGC.

## Limitations

- Lack of comparison against newer adaptive clipping methods like ZClip and SPAM published after initial submission
- Memory overhead of storing EMA norms for every parameter tensor not discussed (approximately 16MB per billion parameters)
- Limited testing to only two optimizers (AdamW and Lion) despite claiming generalization across optimizers

## Confidence

**High Confidence:** Theoretical convergence proof (O(1/√T) rate maintenance) is sound under stated assumptions. Mechanism explanation for why local clipping outperforms global clipping is well-supported.

**Medium Confidence:** Empirical claim of "complete loss spike elimination" is convincing for tested architectures but generalization to other model families remains untested. Ablation studies show clear trends but don't explore full parameter space.

**Low Confidence:** Assertion that AdaGC "generalizes across optimizers" is based on only two additional optimizers, with no theoretical justification for how it benefits optimizers with fundamentally different update rules.

## Next Checks

1. **Cross-Optimizer Validation:** Implement AdaGC on Adafactor and LAMB optimizers for 1B parameter models to verify claimed generalization, measuring both spike elimination rates and convergence speed.

2. **Edge Case Testing:** Design synthetic training scenarios with sustained high-frequency oscillations or sudden magnitude reversals to evaluate whether EMA-based threshold lags too far behind or clips legitimate updates.

3. **Distributed Training Stress Test:** Implement parameter-wise AdaGC across tensor-parallel configurations with 8+ GPUs, systematically comparing against shard-wise implementation to measure training stability and final model quality across Llama-2 variants.