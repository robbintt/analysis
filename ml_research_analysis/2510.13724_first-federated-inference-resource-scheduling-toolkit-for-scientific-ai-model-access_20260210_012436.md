---
ver: rpa2
title: 'FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model
  Access'
arxiv_id: '2510.13724'
source_url: https://arxiv.org/abs/2510.13724
tags:
- first
- inference
- globus
- requests
- request
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FIRST is a federated inference-as-a-service framework enabling
  secure, cloud-like access to AI models on HPC infrastructure. It leverages Globus
  Auth and Compute to provide an OpenAI-compliant API that can distribute requests
  across federated clusters while maintaining institutional security.
---

# FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access

## Quick Facts
- **arXiv ID:** 2510.13724
- **Source URL:** https://arxiv.org/abs/2510.13724
- **Reference count:** 39
- **Primary result:** 24-node HPC cluster processed 8.7M inference requests serving 76 users while generating 10B+ tokens in 10 months

## Executive Summary
FIRST is a federated inference-as-a-service framework that enables secure, cloud-like access to AI models on HPC infrastructure. It provides an OpenAI-compliant API that distributes requests across federated clusters while maintaining institutional security through Globus Auth and Compute integration. The system supports multiple inference backends, auto-scales resources, maintains "hot" nodes for low latency, and offers both interactive and batch processing modes. Deployed on a 24-node HPC cluster, it processed 8.7 million inference requests serving 76 users while generating over 10 billion tokens in 10 months.

## Method Summary
FIRST implements a federated inference architecture leveraging Globus Auth for authentication and Globus Compute for job scheduling across HPC clusters. The framework provides an OpenAI-compatible REST API endpoint that can distribute inference requests to multiple backend instances. It supports auto-scaling capabilities, maintains warm nodes for low-latency responses, and handles both interactive and batch inference workloads. The system was deployed on a 24-node HPC cluster and evaluated using Llama 3.3 70B models, achieving significant throughput improvements compared to direct vLLM access at high request rates.

## Key Results
- Processed 8.7 million inference requests serving 76 users while generating over 10 billion tokens in 10 months
- Achieved up to 23.9 requests/second throughput and 4131 tokens/second output with four scaled instances of Llama 3.3 70B
- Outperformed direct vLLM access at high request rates and demonstrated superior throughput compared to commercial cloud APIs

## Why This Works (Mechanism)
FIRST bridges traditional HPC infrastructure with growing AI inference demands by providing a federated, secure, and scalable inference service. The framework leverages existing HPC job scheduling and authentication infrastructure while presenting a familiar cloud-like API interface. By distributing requests across federated clusters and maintaining warm nodes, it achieves low latency and high throughput while preserving institutional data governance requirements.

## Foundational Learning
- **Federated Inference Architecture**: Distributed processing across multiple HPC clusters enables scalability and fault tolerance. Quick check: Verify request distribution patterns across federated nodes under varying loads.
- **HPC Resource Management**: Integration with existing job schedulers and resource managers allows efficient utilization of specialized HPC infrastructure. Quick check: Monitor resource utilization efficiency across different inference workloads.
- **Authentication and Authorization**: Globus Auth integration provides secure access control while maintaining institutional compliance. Quick check: Validate access control enforcement across federated deployments.

## Architecture Onboarding
- **Component Map**: Client API -> Load Balancer -> Request Router -> Federated Clusters -> Inference Backends
- **Critical Path**: Client request → Authentication → Request routing → Resource allocation → Model inference → Response delivery
- **Design Tradeoffs**: Federation enables scalability but introduces latency; warm nodes improve responsiveness but increase resource costs
- **Failure Signatures**: Authentication failures block all requests; cluster unavailability redirects to healthy nodes; backend failures trigger auto-scaling
- **First Experiments**: 1) Single-cluster latency measurement with warm/cold starts, 2) Multi-cluster request distribution testing, 3) Authentication failure handling validation

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation limited to single 24-node HPC cluster deployment, restricting generalizability
- Benchmarking focuses primarily on throughput without comprehensive error rate or long-term stability analysis
- Security model effectiveness against sophisticated attacks not demonstrated

## Confidence
- **High confidence**: Well-documented framework architecture with clear integration patterns for authentication and job scheduling
- **Medium confidence**: Performance benchmarking results internally consistent but lacking validation across diverse environments
- **Low confidence**: Long-term scalability claims and multi-cluster federation capabilities remain theoretical

## Next Checks
1. Conduct multi-cluster federation testing with heterogeneous hardware configurations to validate horizontal scaling capabilities
2. Perform comprehensive cost-performance analysis comparing FIRST against commercial cloud APIs across different model families
3. Implement fault injection testing to evaluate system resilience under network partitions and authentication service failures