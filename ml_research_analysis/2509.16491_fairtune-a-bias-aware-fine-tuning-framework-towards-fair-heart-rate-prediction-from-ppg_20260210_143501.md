---
ver: rpa2
title: 'FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction
  from PPG'
arxiv_id: '2509.16491'
source_url: https://arxiv.org/abs/2509.16491
tags:
- fairness
- across
- fine-tuning
- groupdro
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Fine-tuning foundation models for heart rate prediction improves\
  \ accuracy but can widen fairness gaps, especially in larger models. FairTune, a\
  \ bias-aware fine-tuning framework, benchmarks three strategies\u2014class weighting\
  \ (IF), GroupDRO, and adversarial debiasing (ADV)\u2014to reduce disparities across\
  \ gender subgroups without sacrificing performance."
---

# FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction from PPG

## Quick Facts
- arXiv ID: 2509.16491
- Source URL: https://arxiv.org/abs/2509.16491
- Reference count: 20
- Primary result: FairTune reduces fairness gaps in HR prediction without sacrificing accuracy, with IF and GroupDRO most effective across diverse deployment domains.

## Executive Summary
Fine-tuning foundation models for heart rate prediction improves accuracy but can widen fairness gaps, especially in larger models. FairTune, a bias-aware fine-tuning framework, benchmarks three strategies—class weighting (IF), GroupDRO, and adversarial debiasing (ADV)—to reduce disparities across gender subgroups without sacrificing performance. Results show that IF and GroupDRO effectively narrow fairness gaps and reshape internal representations to reduce demographic clustering, with method effectiveness varying by deployment domain. Larger models may amplify bias despite better accuracy. FairTune demonstrates that explicit fairness mitigation is essential for equitable deployment of physiological AI models in diverse healthcare settings.

## Method Summary
FairTune fine-tunes a pretrained PPG-GPT (19M–1B parameters) for heart rate prediction using full-layer adaptation with L1 + Logit Laplace loss. Three bias mitigation strategies are implemented: inverse frequency weighting (IF), GroupDRO (dynamic group reweighting), and adversarial debiasing (ADV). The framework evaluates cross-dataset performance on DaLiA (wrist-worn, 64Hz), BUT-PPG (smartphone, 30Hz), and MIMIC (ICU, 125Hz), measuring MAE, fairness gap (|MAE_male - MAE_female|), MMD, and Silhouette scores. Mitigation effectiveness varies by source-target pair and domain characteristics, with IF and GroupDRO generally outperforming ADV.

## Key Results
- IF and GroupDRO reduce fairness gaps by 15–25% across cross-dataset transfers without compromising accuracy.
- Larger models (345M–1B) sometimes worsen fairness gaps despite improved MAE, indicating susceptibility to demographic shortcuts.
- Mitigation strategies reshape internal embeddings, reducing demographic clustering (IF reduced MMD by 13% in DaLiA).
- Method effectiveness is deployment-domain-dependent: GroupDRO excels in clinical-to-consumer transfers, while IF is more stable overall.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverse-frequency (IF) weighting reduces fairness gaps by counterbalancing demographic imbalance during training.
- Mechanism: Samples receive weights inversely proportional to their group frequency (w_i = 1/(2·f_g(i))), then WeightedRandomSampler draws more heavily from underrepresented groups. This biases gradient updates toward minority-group patterns without changing the loss function.
- Core assumption: Demographic performance gaps stem partly from uneven group representation in training data.
- Evidence anchors:
  - [abstract] "class weighting based on inverse group frequency (IF)... significantly reduce fairness gaps without compromising accuracy"
  - [section 2.4] Mathematical formulation: "Each sample receives sampling weight: w_i = 1/(2·f_g(i))"
  - [corpus] Fairness-Aware Low-Rank Adaptation paper confirms reweighting as standard approach but notes limitations under demographic privacy constraints.
- Break condition: Fails when test-time distribution differs radically from training demographics, or when group labels are unavailable.

### Mechanism 2
- Claim: GroupDRO improves worst-group generalization by dynamically upweighting high-loss groups.
- Mechanism: Per-group losses are normalized, then group weights computed as w_g = 1 + η·L̂_g. Sampling probability shifts toward groups with highest current loss, creating adversarial-style pressure against subgroup neglect.
- Core assumption: Unbiased models should minimize worst-case group performance, not just average performance.
- Evidence anchors:
  - [section 2.4] "weights were updated dynamically so that optimization emphasized the group with the highest loss"
  - [section 4] "GroupDRO demonstrated strength in clinical settings, achieving near-parity in some cases"
  - [corpus] Weak direct evidence; GroupDRO cited in standard ML fairness literature but underexplored for physiological foundation models.
- Break condition: Fails when η is poorly tuned (overcorrects, harming accuracy) or when group definitions don't capture true bias axes (e.g., skin tone omitted).

### Mechanism 3
- Claim: Bias mitigation reshapes latent representations to reduce demographic clustering.
- Mechanism: Mitigation objectives (IF, GroupDRO) alter gradient flow during fine-tuning, pushing embeddings toward demographic-invariant configurations. Measured via Maximum Mean Discrepancy (MMD) and Silhouette scores.
- Core assumption: Demographic bias manifests as separable clusters in learned representations; reducing separability reduces downstream bias.
- Evidence anchors:
  - [abstract] "Representation analyses further reveal that mitigation techniques reshape internal embeddings to reduce demographic clustering"
  - [section 3.4] "IF achieved the most significant reduction of MMD between gender subgroups by 13% (from 0.004478 to 0.003896)"
  - [corpus] Fair Foundation Models for Medical Image Analysis notes representation-level debiasing as open challenge; limited direct corroboration.
- Break condition: Assumption: reducing MMD directly translates to fair predictions—unproven if downstream heads can re-learn demographic shortcuts.

## Foundational Learning

- **Foundation Model Fine-Tuning**
  - Why needed here: The paper adapts a pretrained PPG-GPT (trained on 200M ICU segments) to new domains via full-layer fine-tuning. Understanding transfer mechanics is essential.
  - Quick check question: Can you explain why fine-tuning improves accuracy but may worsen fairness, even with the same data?

- **Fairness Metrics for Regression**
  - Why needed here: Fairness is operationalized as MAE gap between gender subgroups (|Male - Female|), not classification parity. Requires understanding continuous-output fairness.
  - Quick check question: How would you interpret a fairness gap of 6.99 bpm in BUT-PPG→DaLiA transfer?

- **Distribution Shift in Physiological Signals**
  - Why needed here: Cross-dataset transfers (consumer↔clinical, wearable↔smartphone) introduce signal characteristic changes. Results show heterogeneous fairness patterns by domain.
  - Quick check question: Why might Clinical-to-Consumer transfer show different fairness dynamics than Consumer-to-Clinical?

## Architecture Onboarding

- **Component map:**
  - PPG-GPT backbone (19M–1B parameters) -> Patch embedding (40-sample patches) -> Multi-head attention with rotary positional embeddings -> Fine-tuning head (regression output) -> Bias mitigation modules (IF, GroupDRO, ADV) -> Evaluation layer (MAE, fairness gap, MMD, Silhouette scores)

- **Critical path:**
  1. Load pretrained PPG-GPT weights
  2. Configure mitigation strategy (IF/GroupDRO/ADV) with group labels
  3. Fine-tune on source dataset with composite loss (L1 + Logit-Laplace)
  4. Evaluate on target dataset—measure MAE per group, compute gap
  5. Extract penultimate-layer embeddings for MMD analysis

- **Design tradeoffs:**
  - IF vs. GroupDRO: IF is simpler, more stable; GroupDRO adapts dynamically but requires η tuning
  - ADV: Adds hyperparameter λ and training instability; inconsistent gains in this study
  - Model scaling: Larger models (345M–1B) sometimes worsen fairness despite accuracy gains

- **Failure signatures:**
  - High variance across random seeds (Figure 2 bottom): suggests unstable convergence
  - MAE improves but fairness gap widens: indicates demographic shortcut learning
  - GroupDRO achieves low fairness gap but high MAE: overcorrection

- **First 3 experiments:**
  1. Replicate 19M IF baseline on DaLiA→BUT-PPG (target: MAE ~13.44, gap ~1.30) to validate pipeline
  2. Ablate mitigation strategy: run Unbalanced vs. IF vs. GroupDRO on MIMIC→DaLiA (high-gap scenario, gap ~6.75–8.01) to compare sensitivity
  3. Visualize embeddings: plot t-SNE of penultimate layer colored by gender, pre/post-IF, to confirm clustering reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do bias mitigation strategies perform when applied to intersectional demographic subgroups (e.g., combinations of gender, age, race/ethnicity, and skin tone)?
- Basis in paper: [explicit] The authors state: "We focused primarily on binary gender disparities... while other key attributes such as age, race/ethnicity, and socioeconomic status were not explored. Moreover, such factors can interact with each other, complicating the isolation of any single bias sources."
- Why unresolved: Current analysis is limited to binary gender; intersectionality introduces multiclass or continuous variables and complex interaction effects that require distinct bias mechanisms (e.g., skin tone bias from sensor optics vs. gender bias from structural factors).
- What evidence would resolve it: Experiments on datasets with multi-attribute demographic labels, evaluating fairness gaps across intersectional subgroups and testing whether single-attribute mitigation strategies generalize.

### Open Question 2
- Question: Why does model scaling exhibit heterogeneous and sometimes contradictory effects on fairness across different transfer scenarios?
- Basis in paper: [explicit] The authors observe: "The effects of scaling were highly heterogeneous, varying by dataset combination" and "scaling consistently exacerbated demographic disparities, suggesting susceptibility to spurious correlations or demographic shortcuts."
- Why unresolved: The paper documents the phenomenon but does not identify mechanistic causes—whether overfitting, spurious correlation amplification, or domain-specific factors drive inconsistent patterns.
- What evidence would resolve it: Ablation studies analyzing learned representations at different scales to identify where spurious correlations emerge, and systematic testing across more source-target pairs with controlled distributional shifts.

### Open Question 3
- Question: Can lightweight, computationally efficient debiasing strategies achieve comparable fairness improvements to enable deployment in resource-constrained clinical settings?
- Basis in paper: [inferred] The authors note: "Some mitigation methods are computationally intensive—especially for larger models—which raises concerns about feasibility in resource-constrained settings" and aim to develop "scalable debiasing strategies for deployment in diverse clinical environments."
- Why unresolved: Computational costs of IF, GroupDRO, and ADV were not quantified, and no comparison to lightweight alternatives (e.g., parameter-efficient fine-tuning) was conducted.
- What evidence would resolve it: Benchmarking computational overhead of each strategy across model sizes and testing adapter-based or LoRA-style debiasing approaches against full fine-tuning.

### Open Question 4
- Question: How do fairness improvements in HR prediction propagate to downstream clinical decisions such as alarm threshold calibration and early warning systems?
- Basis in paper: [explicit] The discussion states: "Fairness gaps in physiological predictions may propagate into downstream clinical decisions... Disparities in accuracy could result in miscalibrated alerts or delayed responses, disproportionately affecting specific demographic groups."
- Why unresolved: The study evaluates prediction-level fairness but does not assess whether reduced gaps translate to equitable clinical outcomes.
- What evidence would resolve it: End-to-end evaluation connecting HR prediction errors to simulated alarm generation, measuring subgroup-specific false positive/negative rates in early warning systems.

## Limitations
- PPG-GPT pretrained weights and architecture details not yet publicly available, preventing exact reproduction.
- Unclear exact train/validation/test splits, batch size, and ADV hyperparameter λ, potentially affecting training dynamics.
- Limited ablation studies on group definitions (e.g., skin tone not addressed) and cross-group intersectional bias.
- GroupDRO evidence for physiological foundation models is sparse; most cited literature is from standard ML fairness.

## Confidence
- **High Confidence**: PPG-GPT fine-tuning improves accuracy but may widen fairness gaps; larger models can amplify bias.
- **Medium Confidence**: IF and GroupDRO effectively reduce fairness gaps and reshape embeddings; ADV is less consistent.
- **Low Confidence**: Effectiveness of mitigation methods is deployment-domain-dependent; mechanistic link between MMD reduction and fair predictions is not rigorously validated.

## Next Checks
1. Replicate 19M IF baseline on DaLiA→BUT-PPG to verify MAE ~13.44 and fairness gap ~1.30.
2. Ablate mitigation strategy (Unbalanced vs. IF vs. GroupDRO) on MIMIC→DaLiA to compare sensitivity and domain-specific effectiveness.
3. Visualize penultimate-layer embeddings (t-SNE by gender) pre- and post-IF to confirm clustering reduction and link to MMD metrics.