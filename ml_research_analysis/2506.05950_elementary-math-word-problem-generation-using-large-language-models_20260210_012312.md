---
ver: rpa2
title: Elementary Math Word Problem Generation using Large Language Models
arxiv_id: '2506.05950'
source_url: https://arxiv.org/abs/2506.05950
tags:
- generation
- mwps
- grade
- problem
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an MWP generation system based on Large Language
  Models (LLMs) that can generate MWPs belonging to a user-specified grade and question
  type. The system was developed through extensive experiments to identify the best
  open-source LLM and prompt template for MWP generation.
---

# Elementary Math Word Problem Generation using Large Language Models

## Quick Facts
- arXiv ID: 2506.05950
- Source URL: https://arxiv.org/abs/2506.05950
- Reference count: 34
- The system achieves 74.52% grade relevance and 78.76% section relevance even in the best model

## Executive Summary
This paper presents an MWP generation system based on Large Language Models (LLMs) that can generate math word problems belonging to user-specified grade levels and question types. The system was developed through extensive experimentation to identify the best open-source LLM and prompt template for MWP generation. The research team fine-tuned the most promising LLM with a manually created MWP dataset and incorporated human feedback to improve quality. The system includes a secondary LLM to guarantee solvability of generated problems and optimizes decoder hyper-parameters to ensure diversity of generated questions. Human and automated evaluations confirmed high overall quality of generated MWPs with minimal spelling and grammar issues.

## Method Summary
The system was developed through a multi-stage approach beginning with extensive experiments to identify the optimal open-source LLM and prompt template for MWP generation. The selected LLM was then fine-tuned using a manually created MWP dataset and further refined through incorporation of human feedback. A secondary LLM was incorporated to verify solvability of generated problems, while decoder hyper-parameters were optimized to ensure diversity in the generated questions. The approach combined automated evaluation metrics with human evaluation to assess the quality of generated problems, resulting in a dataset of 4K MWPs with error annotation that represents the only comprehensively error-annotated English MWP corpus available.

## Key Results
- The system achieves 74.52% grade relevance and 78.76% section relevance in the best model
- Generated MWPs demonstrate high overall quality with minimal spelling and grammar issues
- The research produced a 4K MWP dataset with error annotation, representing the only comprehensively error-annotated English MWP corpus available

## Why This Works (Mechanism)
The system leverages the pattern recognition and generation capabilities of large language models, combined with targeted fine-tuning on domain-specific data and iterative human feedback. The incorporation of a secondary LLM for solvability checking adds a layer of quality assurance, while optimized decoder hyper-parameters ensure diversity in the generated problems. The combination of automated and human evaluation provides comprehensive quality assessment.

## Foundational Learning
- **Large Language Models**: Why needed - Core generation capability; Quick check - Verify the model can generate coherent text
- **Prompt Engineering**: Why needed - Guide the LLM to generate appropriate MWP content; Quick check - Test different prompt templates for quality output
- **Fine-tuning**: Why needed - Adapt the LLM to the specific domain of math word problems; Quick check - Measure performance improvement after fine-tuning
- **Human Feedback Integration**: Why needed - Improve quality through expert input; Quick check - Compare outputs before and after feedback incorporation
- **Automated Evaluation Metrics**: Why needed - Provide scalable quality assessment; Quick check - Validate metrics against human judgments
- **Error Annotation**: Why needed - Create a valuable dataset for future research; Quick check - Verify annotation consistency across the dataset

## Architecture Onboarding

**Component Map**: Data Input -> Prompt Template Selection -> LLM Generation -> Secondary LLM Solvability Check -> Decoder Hyperparameter Optimization -> Human/AI Evaluation -> Dataset Creation

**Critical Path**: The critical path involves prompt template selection, LLM generation, and quality evaluation. The system must successfully generate problems and pass both solvability and quality checks to produce usable output.

**Design Tradeoffs**: The system trades computational efficiency for quality by incorporating a secondary LLM for solvability checking and extensive human evaluation. The use of open-source models rather than proprietary ones provides flexibility but may limit maximum performance potential.

**Failure Signatures**: Common failure modes include generation of problems that don't match specified grade levels (74.52% success rate) or question types (78.76% success rate), solvability issues that pass the secondary LLM check, and diversity limitations in generated problems despite hyperparameter optimization.

**First Experiments**:
1. Test basic LLM generation with simple prompt templates on a small validation set
2. Implement and validate the secondary LLM solvability checking mechanism
3. Run automated evaluation metrics against human judgments to establish baseline quality

## Open Questions the Paper Calls Out
None

## Limitations
- The system struggles significantly with adherence to specified grade levels and question types, achieving only 74.52% grade relevance and 78.76% section relevance
- The automated evaluation metrics and human evaluation processes lack detailed explanation, limiting confidence in quality assessments
- The incorporation of a secondary LLM for solvability checking introduces additional complexity without clear demonstration of its effectiveness

## Confidence
- Grade and Section Relevance Metrics: Medium
- Overall Quality Assessment: Medium
- Dataset Creation Value: High
- Solvability Checking Mechanism: Low

## Next Checks
1. Conduct an external validation study with independent educators to verify the accuracy of grade-level and question type classifications in the generated problems
2. Perform a comparative analysis of the solvability checking mechanism against alternative approaches to quantify its effectiveness and efficiency
3. Implement and evaluate additional prompt engineering techniques or fine-tuning strategies specifically targeting improvements in grade and section relevance metrics