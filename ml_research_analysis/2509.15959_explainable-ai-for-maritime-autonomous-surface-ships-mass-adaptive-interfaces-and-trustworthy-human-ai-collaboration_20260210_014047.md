---
ver: rpa2
title: 'Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces
  and Trustworthy Human-AI Collaboration'
arxiv_id: '2509.15959'
source_url: https://arxiv.org/abs/2509.15959
tags:
- autonomous
- maritime
- https
- human
- ships
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The review synthesizes 100 studies to clarify how transparency\
  \ and explainability can make Maritime Autonomous Surface Ships (MASS) safer and\
  \ more usable. It maps the Guidance-Navigation-Control stack to shore-based operations\
  \ and identifies where Human-UCAs concentrate\u2014especially in RSM-RCM handovers\
  \ and emergency loops."
---

# Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration

## Quick Facts
- arXiv ID: 2509.15959
- Source URL: https://arxiv.org/abs/2509.15959
- Reference count: 0
- Primary result: Reviews 100 studies mapping MASS GNC stack to shore operations, identifying where Human-UCAs concentrate and proposing adaptive transparency frameworks to improve safety.

## Executive Summary
This systematic literature review synthesizes 100 studies to clarify how transparency and explainability can make Maritime Autonomous Surface Ships (MASS) safer and more usable. The review maps the Guidance-Navigation-Control stack to shore-based operations and identifies concentration of Human-Unsafe Control Actions (UCAs) during RSM-RCM handovers and emergency loops. Evidence indicates that transparency features primarily improve operator understanding and trust calibration, while overall trust remains strongly moderated by reliability and predictability. The authors propose a dual-challenge framework: empirical validation of trust/SA using validated instruments in simulation/HIL/MIL testbeds, and communication of explanations through adaptive HMIs aligned with users' mental models.

## Method Summary
The study conducted a systematic literature review using the PICo framework across four academic databases (Web of Science, Scopus, IEEE Xplore, Dimensions), targeting 100 studies. The methodology included electronic search, deduplication via Zotero, and manual screening following inclusion/exclusion criteria. Data extraction focused on transparency features, HMI strategies, and UCA types, with bibliometric analysis using VOSviewer for co-authorship and keyword co-occurrence visualization. The review synthesized findings on automation transparency, XAI impacts, and their relationship to operator trust and Situation Awareness in maritime contexts.

## Key Results
- Transparency features (decision rationales, variable weights, uncertainty metrics) improve operator understanding and trust calibration
- RSM-RCM handovers and emergency loops are critical concentration points for Human-UCAs
- COLREGs formalization with standardized route-exchange can reduce ambiguity in mixed traffic scenarios

## Why This Works (Mechanism)

### Mechanism 1
Transparency features (e.g., decision rationales, variable weights, uncertainty metrics) improve operator understanding and trust calibration, though they do not solely determine overall trust. By exposing internal variables and confidence levels, operators can compare the machine's logic against their mental model, reducing the "black box" effect. This alignment helps operators distinguish between system competence and luck. The core assumption is that operators have sufficient cognitive capacity and domain knowledge to interpret exposed variables. Break condition: if underlying reliability is low, increased transparency may highlight incompetence, degrading trust.

### Mechanism 2
Adaptive interfaces that align with an operator's mental model reduce Human Unsafe Control Actions during mode transitions. The "clumsy automation" risk occurs when data volume spikes during emergency handover, exceeding human cognitive limits. Adaptive transparency filters this flow, presenting critical displays only when needed, preventing cognitive overload and "out-of-the-loop" syndrome. The core assumption is that the system can accurately estimate operator state via behavioral or physiological proxies. Break condition: if operator state estimation is inaccurate, the system might withhold critical information during distraction, causing an accident.

### Mechanism 3
Formalizing fuzzy regulations (COLREGs) into executable logic with visual compliance indicators reduces ambiguity in human-AI collaboration. By mapping linguistic rules to fuzzy logic models and displaying "rule compliance scores," the system creates a shared objective reference, reducing disputes between AI's planned path and operator intuition. The core assumption is that mathematical formalization accurately captures the intent of original maritime law. Break condition: in highly complex, multi-ship scenarios, rigid algorithmic interpretations might fail to account for edge cases not covered in training data.

## Foundational Learning

- **Concept: Guidance-Navigation-Control (GNC) Stack**
  - Why needed here: This is the architectural backbone of MASS. You cannot design transparency without knowing where in the stack the decision is being made.
  - Quick check question: Does the "Guidance" layer handle collision avoidance logic or actuator commands? (Answer: Guidance handles path planning; Control handles actuator commands)

- **Concept: Human Unsafe Control Actions (Human-UCAs)**
  - Why needed here: The paper moves beyond "human error" to a systems theory view. You must understand that an action is only "unsafe" in specific contexts.
  - Quick check question: Is "taking over control" always a safe action? (Answer: No, if done when not required or too late, it is a UCA)

- **Concept: Trust Calibration vs. Overall Trust**
  - Why needed here: This distinction is the core finding of the review. Increasing transparency does not necessarily make the user trust the system more; it makes their trust more accurate relative to performance.
  - Quick check question: If a system is unreliable, should transparency increase or decrease trust? (Answer: Properly calibrated trust should decrease)

## Architecture Onboarding

- **Component map:**
  Ship-side (Perception): Sensors (LiDAR, Camera, AIS) -> Sensor Fusion -> Situation Awareness (SA) Layer
  Ship-side (Action): Guidance (Path Planning/Collision Avoidance) -> Control (Actuators)
  Shore-side (ROC): Remote Operator (RO) <-> Adaptive HMI <-> Decision Support System (DSS)
  Link: VDES/Satellite Comms (connects Ship SA to ROC HMI)

- **Critical path:**
  1. Monitoring (RSM): System runs auto-pilot; RO monitors via QGILD (Quickly Getting into Loop Display)
  2. Alert: System encounters "unknown" or conflict; triggers alert
  3. Handover (RSM -> RCM): Adaptive HMI boosts info fidelity; RO diagnoses using XAI outputs
  4. Intervention: RO accepts system plan or overrides

- **Design tradeoffs:**
  - Transparency vs. Latency: Generating explanations takes compute cycles; real-time control cannot be delayed
  - Info vs. Overload: Detailed logic helps diagnosis but clutters screen during routine monitoring
  - Automation Level: Full autonomy (A4) removes human from loop entirely, reducing interaction risks but removing safety net

- **Failure signatures:**
  - "Automation Surprises": System acts without prior indication (transparency failure)
  - "Mode Confusion": RO thinks ship is in RCM, but it is in RSM (interface failure)
  - "Trust Fracture": RO ignores system advice after a single unexplained error (calibration failure)

- **First 3 experiments:**
  1. Baseline Calibration: Run operators in simulator with "Black Box" interface vs. "Glass Box" (full XAI). Measure takeover time and trust scores.
  2. Adaptive Load Test: Induce high traffic/dense fog. Compare static HMI vs. Adaptive HMI. Measure pupil dilation/EEG for cognitive load.
  3. COLREGs Conflict: Create head-on scenario where COLREGs are ambiguous. Test if "Rule Compliance Score" display reduces decision time compared to raw sensor data.

## Open Questions the Paper Calls Out

### Open Question 1
How can the causal link between specific transparency features and the reduction of Human Unsafe Control Actions (Human-UCAs) be empirically validated? The authors identify a "dual-challenge framework," noting that most guidelines rely on subjective experience and calling for "open audit trails linking explanations → operator responses → outcomes." This remains unresolved because current research relies heavily on subjective questionnaires rather than behavioral indicators or performance data in HIL/MIL testbeds. Multi-site trials using standardized scenario libraries that quantify effects on takeover timeliness and Human-UCA rates would resolve this.

### Open Question 2
How can adaptive HMIs effectively modulate explanation detail and timing to prevent cognitive overload while maintaining operator trust? Section 4.2.1 notes that "transparency layers do not uniformly improve SA" and may overwhelm operators, leading to the proposal of an "adaptive transparency framework." This remains unresolved because existing interfaces often provide static information dumps; the specific algorithms and thresholds for dynamically adjusting information flow based on real-time cognitive state are not yet established. Empirical data showing that adaptive interfaces result in better performance compared to static transparency interfaces during high-workload scenarios would resolve this.

### Open Question 3
Can linguistic ambiguities in COLREGs be resolved through formalization (e.g., fuzzy logic) to ensure consistent machine compliance in mixed traffic? Section 4.2.4 highlights that COLREGs contain "linguistically ambiguous terms" that lack standardized quantitative definitions, complicating algorithmic implementation. This remains unresolved because while fuzzy logic models have been proposed, their ability to handle multi-ship, multi-conflict scenarios in real-world mixed traffic alongside human-operated vessels remains unproven. Successful full-scale vessel trials or high-fidelity simulations demonstrating that formalized COLREGs algorithms produce unambiguous, safe solutions accepted by human navigators would resolve this.

## Limitations
- Limited empirical validation of transparency effects on actual MASS operator performance
- Lack of standardized measurement instruments for trust and SA specific to maritime autonomy contexts
- Assumption that COLREGs formalization will reduce ambiguity without field testing in mixed traffic

## Confidence
- **High confidence**: Systematic mapping of MASS GNC stack to shore operations and identification of RSM-RCM handover risks
- **Medium confidence**: Claims about transparency improving trust calibration and reducing UCAs (primarily based on laboratory studies with non-maritime automation)
- **Low confidence**: Specific recommendations for adaptive interface parameters and COLREGs formalization approaches (largely theoretical, lacking maritime field validation)

## Next Checks
1. **Behavioral validation study**: Conduct controlled simulation experiment comparing operator performance with and without XAI transparency displays during emergency handovers, measuring takeover time, trust ratings, and SA using validated maritime-specific instruments.
2. **Adaptive interface field trial**: Deploy adaptive transparency filters in HIL/MIL testbed with actual MASS operators, collecting physiological measures and task performance to validate workload estimation accuracy.
3. **COLREGs compliance experiment**: Test fuzzy logic rule formalization in multi-ship traffic simulation, measuring decision time, compliance consistency, and human-AI agreement compared to traditional manual COLREG interpretation.