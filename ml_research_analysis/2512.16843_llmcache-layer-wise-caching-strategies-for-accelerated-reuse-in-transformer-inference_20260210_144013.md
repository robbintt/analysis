---
ver: rpa2
title: 'LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer
  Inference'
arxiv_id: '2512.16843'
source_url: https://arxiv.org/abs/2512.16843
tags:
- caching
- llmcache
- inference
- cache
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLMCache, a layer-wise caching framework
  that accelerates transformer inference by reusing intermediate activations based
  on semantic similarity of input sequences. The core idea is to generate lightweight
  fingerprints from inputs and match them against cached hidden states at each transformer
  layer, allowing reuse of previously computed representations.
---

# LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference

## Quick Facts
- arXiv ID: 2512.16843
- Source URL: https://arxiv.org/abs/2512.16843
- Authors: Harsh Vardhan Bansal
- Reference count: 25
- Primary result: Up to 3.1× speedup with <0.5% accuracy degradation via layer-wise activation reuse

## Executive Summary
LLMCache introduces a layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. The system generates lightweight fingerprints from inputs and matches them against cached hidden states at each transformer layer, allowing reuse of previously computed representations. This approach is model-agnostic, supports both encoder and decoder architectures, and achieves significant speedups without requiring architectural changes or retraining.

The method demonstrates up to 3.1× inference speedup on BERT-base, DistilBERT, and GPT-2 with less than 0.5% accuracy degradation across SQuAD, WikiText-103, and OpenBookQA benchmarks. Cache hit rates are highest in early and middle layers, particularly for repetitive or structured input settings. The approach offers a practical, general-purpose solution for reducing transformer inference latency in real-world applications.

## Method Summary
LLMCache accelerates transformer inference through semantic fingerprinting and layer-wise activation reuse. The system generates lightweight fingerprints from input embeddings using techniques like SimHash or MinHash, then matches these against cached hidden states at each transformer layer. When a semantic match is found above a threshold τ, the cached intermediate representation is reused instead of recomputing the layer. The framework supports both encoder and decoder architectures without requiring architectural changes or retraining. Adaptive eviction strategies manage cache staleness through LRU, staleness-aware removal, and divergence monitoring, with memory overhead scaling logarithmically relative to hit rate gains.

## Key Results
- Up to 3.1× inference speedup on BERT-base, DistilBERT, and GPT-2 models
- Less than 0.5% accuracy degradation across benchmark tasks
- Highest cache hit rates (up to 92%) in early and middle transformer layers
- Logarithmic memory scaling with cache hit rate gains
- Optimal similarity threshold range of 0.82-0.88 for balancing speed and accuracy

## Why This Works (Mechanism)

### Mechanism 1: Semantic Fingerprinting for Sub-Linear Cache Lookup
Lightweight fingerprints derived from input embeddings can represent semantic content sufficiently for cache matching, avoiding full-sequence comparison costs. The system computes fixed-length fingerprint vectors using embedding aggregation (fX = Avg(E(X))), optionally augmented with prefix attention statistics, then applies SimHash or MinHash for dimensionality reduction. Comparison uses cosine similarity or LSH against cached keys. This works because semantically similar inputs produce fingerprints that cluster within a threshold τ, and this clustering correlates with reusable intermediate representations.

### Mechanism 2: Layer-wise Activation Reuse Based on Representational Stability
Intermediate transformer layer outputs can be safely reused across semantically similar inputs because early-to-middle layers exhibit stable representations. At each layer l, before computing fl(hl-1), the system checks cache Cl for a fingerprint match above threshold τ. On hit, returns cached hl directly, bypassing layer computation. This works because early and middle transformer layers encode semantic features that generalize across inputs with similar meaning, while upper layers are more task-specific and sensitive to variation.

### Mechanism 3: Adaptive Eviction to Bound Memory While Preserving Hit Rate
Cache freshness can be maintained with logarithmic memory scaling relative to hit rate gains using combination of recency, frequency, and divergence-aware policies. The system implements LRU eviction, staleness-aware removal based on decayed match rates, and divergence monitoring that tracks output differences for given fingerprints across calls. This works because past reuse patterns predict future reuse, and entries with declining match utility should be evicted before memory pressure becomes critical.

## Foundational Learning

- **Transformer Layer Computation Flow**: Understanding the standard hl = fl(hl-1) cascade is prerequisite to grasping where caching inserts. Quick check: Can you trace how an input token sequence X becomes hL through L sequential layer transformations?

- **Key-Value (KV) Caching in Autoregressive Models**: Understanding KV-cache's token-level, decoder-only limitation clarifies the novelty of layer-wise caching. Quick check: Why does KV-cache not help encoder-only models like BERT?

- **Similarity Metrics and Locality-Sensitive Hashing (LSH)**: Cache matching relies on sim(fX, f') ≥ τ using cosine similarity or Jaccard index; LSH enables sub-linear lookup. Quick check: Given two embedding vectors, how would you compute cosine similarity, and what does a value of 0.85 imply about their relationship?

## Architecture Onboarding

- **Component map**: Input X → Fingerprint Generator → For each layer l: Cache Lookup → (Hit: return cached hl) / (Miss: compute fl, store result) → Return hL
- **Critical path**: Input X → Fingerprint Generator → Layer-wise Cache Banks Cl → Cache Matching Engine → Layer Execution Manager → Refresh/Replacement Controller
- **Design tradeoffs**: 
  - Threshold τ: Higher values reduce semantic mismatch risk but lower hit rates; optimal range 0.82–0.88
  - Cache granularity: Caching all layers maximizes speedup but increases memory; early-layer-only caching offers compromise
  - Compression: PCA/autoencoder reduces memory but adds encode/decode overhead
- **Failure signatures**:
  - Hit rate < 30% across layers → input distribution too variable; caching inappropriate
  - Accuracy drop > 1% → τ too low, accepting semantically mismatched cache hits
  - Memory OOM → cache size unbounded; eviction policy misconfigured or disabled
  - Latency increases vs. NoCache → cache lookup overhead exceeds computation savings
- **First 3 experiments**:
  1. Baseline latency comparison: Run NoCache vs. LLMCache on BERT-base with WikiText-103; measure ms/inference and verify ~2.4× speedup
  2. Hit rate by layer depth: Log cache hits per layer on GPT-2 with WikiText; confirm early/middle layers >80%, upper layers <50%
  3. Threshold sweep: Vary τ from 0.75 to 0.95 on SQuAD; plot accuracy drop vs. hit rate to identify task-specific optimal τ

## Open Questions the Paper Calls Out
1. Can trainable, lightweight embedding encoders replace handcrafted hashing methods (SimHash/MinHash) to improve the precision of cache fingerprinting without adding significant latency?

2. Does implementing a dynamic, per-layer similarity threshold (τ) based on learned uncertainty scores outperform a static global threshold in balancing speed and accuracy?

3. To what extent can sharing cache banks or fingerprints across multi-node serving systems improve overall throughput without incurring prohibitive synchronization overhead?

4. How does LLMCache perform on out-of-distribution (OOD) inputs or domains where semantic similarity between user queries is naturally lower?

## Limitations
- Fingerprint fidelity may degrade for high-variance domains where semantic similarity decreases
- Performance depends heavily on input patterns; random or adversarial inputs could collapse hit rates
- Optimal threshold τ varies by task and requires exhaustive tuning for deployment
- Limited testing to standard benchmarks; real-world OOD performance unverified

## Confidence
- **High**: Layer-wise cache hit rate patterns, logarithmic memory scaling with hit rate, <0.5% accuracy degradation under optimal τ
- **Medium**: Semantic fingerprint reliability, eviction policy effectiveness, generalizability beyond tested architectures
- **Low**: Performance on out-of-distribution inputs, cross-modal (vision) applicability, real-time streaming scenarios

## Next Checks
1. Implement LLMCache on T5-base (encoder-decoder) and ViT-base; measure speedup and accuracy drop. If hit rates fall below 40% in middle layers, the layer-wise reuse hypothesis may not generalize.

2. Generate adversarial input sets with controlled semantic variance; measure false-positive cache hits and accuracy impact. If τ=0.88 still yields >1% accuracy drop, the fingerprint mechanism needs refinement.

3. Sweep τ from 0.75 to 0.95 on SQuAD; plot cache size vs. accuracy degradation to identify the operational envelope. If accuracy drop accelerates sharply below τ=0.80, the method's practical range is narrower than claimed.