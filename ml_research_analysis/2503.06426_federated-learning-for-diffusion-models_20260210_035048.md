---
ver: rpa2
title: Federated Learning for Diffusion Models
arxiv_id: '2503.06426'
source_url: https://arxiv.org/abs/2503.06426
tags:
- data
- learning
- ddpm
- training
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedDDPM, a federated learning algorithm for
  training diffusion models on non-IID data. The key idea is to generate an auxiliary
  dataset on the server using well-trained local diffusion models from each client,
  which better represents the global data distribution.
---

# Federated Learning for Diffusion Models

## Quick Facts
- **arXiv ID**: 2503.06426
- **Source URL**: https://arxiv.org/abs/2503.06426
- **Reference count**: 40
- **Primary result**: FedDDPM achieves FID scores of 1.91, 22.30, and 21.82 on MNIST, CIFAR10, and CIFAR100 respectively under non-IID shard distributions, outperforming baselines by 2-4×

## Executive Summary
This paper proposes FedDDPM, a federated learning algorithm for training diffusion models on non-IID data. The key innovation is generating an auxiliary dataset on the server using well-trained local diffusion models from each client, which better represents the global data distribution. The server then optimizes the global model using this auxiliary dataset after each aggregation round to reduce bias from heterogeneous data. Experiments on MNIST, CIFAR10, and CIFAR100 show FedDDPM and FedDDPM+ outperform state-of-the-art FL algorithms with significantly lower FID scores.

## Method Summary
FedDDPM addresses non-IID data heterogeneity in federated diffusion model training through server-side synthetic data generation. During warmup, each client trains a local diffusion model to convergence and uploads it to the server. The server then generates an auxiliary dataset proportional to each client's dataset size using these well-trained local models. After FedAvg aggregation, the server refines the global model using this auxiliary dataset for E epochs. FedDDPM+ adds a one-shot correction mechanism that detects training plateaus via FID monitoring and applies targeted corrections to reduce training overhead.

## Key Results
- FedDDPM achieves FID scores of 1.91 (MNIST), 22.30 (CIFAR10), and 21.82 (CIFAR100) on non-IID shard distributions
- Outperforms baselines including FedAvg (FID 4.89-39.33) and pFedMe (FID 2.95-24.93) by 2-4×
- FedDDPM+ achieves similar performance with ~50% fewer communication rounds through early stopping
- Convergence analysis provides O(1/√T) guarantee for non-convex loss functions

## Why This Works (Mechanism)

### Mechanism 1: Auxiliary Dataset Corrects Aggregation Bias
- Claim: Server-side training on synthetic data approximating the global distribution mitigates the directional bias introduced by non-IID client updates.
- Mechanism: Each client trains a local diffusion model to convergence during warmup. The server generates samples proportionally to each client's dataset size, creating an auxiliary dataset A with distribution λ ≈ global distribution. After FedAvg aggregation, server runs E steps of SGD on A to redirect the global model toward the global optimum.
- Core assumption: Local diffusion models trained to convergence can accurately capture local data distributions; proportional sampling yields a synthetic dataset whose distribution approximates the true global distribution.
- Evidence anchors: Abstract states server uses well-trained local models to generate auxiliary data representing global distribution; Section III-B describes server optimization using auxiliary dataset to alleviate heterogeneous data impact.

### Mechanism 2: Convergence Guarantee via Controlled Server-Side Gradient Updates
- Claim: FedDDPM achieves O(1/√T) convergence for non-convex loss functions under standard FL assumptions.
- Mechanism: Server-side gradient updates on auxiliary data introduce additional descent terms that accelerate early-stage loss reduction. With appropriately small learning rates, higher-order noise terms become negligible.
- Core assumption: L-Lipschitz gradient continuity, bounded local/global variance, and unbiased auxiliary gradients.
- Evidence anchors: Section IV provides full convergence bound with detailed proof in Appendix A; Lemma 2 shows Ef(wt+1) ≤ Ef(ŵt+1) - ηtE/2 E‖∇f(ŵt+1)‖² + noise terms.

### Mechanism 3: One-Shot Correction Reduces Overhead via Learning Progress Detection
- Claim: FedDDPM+ achieves competitive performance with ~50% fewer communication rounds by detecting training plateaus and applying targeted corrections.
- Mechanism: Every 10 rounds, QUICK TEST computes FID score on 500 generated samples. If |AvgScore - Score| ≤ Threshold (0.2), training halts and server performs E epochs of refinement on auxiliary data. Exponential moving average (γ = 0.4) smooths score fluctuations.
- Core assumption: Slow FID improvement correlates with convergence plateaus caused by non-IID bias accumulation.
- Evidence anchors: Section V describes QUICK TEST every 10 rounds with γ=0.4, threshold=0.2, test_size=500; Section VI-C shows FedDDPM+ terminates at rounds 280, 310, 320 vs. 600 baseline on CIFAR10.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPMs)**
  - Why needed here: Core generative model architecture; understanding forward/reverse processes is essential for implementing warmup training and sampling.
  - Quick check question: Can you explain how the forward process progressively adds Gaussian noise and how the reverse process learns to denoise?

- **Concept: Federated Averaging (FedAvg) and Non-IID Bias**
  - Why needed here: Baseline FL algorithm; non-IID data causes "client drift" where local objectives diverge from global objective, creating aggregation bias.
  - Quick check question: Why does local SGD on heterogeneous data cause the aggregated model to deviate from the global optimum?

- **Concept: Fréchet Inception Distance (FID)**
  - Why needed here: Primary evaluation metric; lower FID = better image quality/diversity. Used in QUICK TEST to detect training plateaus.
  - Quick check question: What does FID measure and why is it preferred over Inception Score for evaluating generative models?

## Architecture Onboarding

- **Component map**: [Client Warmup] → Local DDPM training (200-600 epochs) → [Model Upload] → Server collects wi_warmup from all clients → [Auxiliary Dataset Generation] → Proportional sampling |Ai| = 0.1 × mi per client → [FL Training Loop] → Client selection → Local SGD (K epochs) → FedAvg aggregation → [Server-Side Refinement] → E epochs SGD on auxiliary dataset (FedDDPM) OR One-shot correction when QUICK TEST triggers (FedDDPM+) → [Global Model Distribution] → Broadcast wt+1 to selected clients

- **Critical path**: Warmup quality determines auxiliary dataset fidelity—under-trained local models → poor synthetic data → weak bias correction; Auxiliary dataset size (0.1× local data) balances quality vs. server compute; Server-side learning rate ηt must be scaled to E and T per Theorem 1

- **Design tradeoffs**: FedDDPM: Better FID (1.910 vs 2.188 on MNIST shard) but ~2× server compute per round; FedDDPM+: ~50% fewer rounds but slightly worse FID; threshold tuning required; Privacy vs. quality: Server-side synthetic data never shared with clients (privacy-preserving) vs. methods distributing auxiliary data to clients (riskier, potentially more effective)

- **Failure signatures**: FID plateaus early with minimal improvement → auxiliary dataset may not match global distribution; QUICK TEST triggers too early → increase Threshold or TEST_SIZE; Training loss oscillates → reduce ηt or increase E gradually

- **First 3 experiments**: 1) Baseline reproduction: Run FedDDPM on MNIST with shard distribution, 100 clients, 15% participation. Target: FID < 2.0. Verify warmup epochs (200) and auxiliary dataset generation; 2) Ablation on auxiliary dataset size: Test |Ai| ∈ {0.05, 0.1, 0.2} × mi on CIFAR10 with Dirichlet α=0.1. Measure FID vs. server compute time; 3) FedDDPM+ threshold tuning: Vary QUICK TEST Threshold ∈ {0.1, 0.2, 0.5} on CIFAR100. Track termination round and final FID

## Open Questions the Paper Calls Out

- **Open Question 1**: How can differential privacy (DP) mechanisms be integrated into FedDDPM to mitigate potential data reconstruction risks while maintaining generation quality? The paper explicitly states future work can explore integrating differential privacy mechanisms to further enhance their potential for securely handling sensitive tasks.

- **Open Question 2**: Does the FedDDPM framework scale to high-resolution image synthesis or latent diffusion models where the computational cost of server-side generation is much higher? Experiments are limited to low-resolution datasets (MNIST, CIFAR10/100). The method requires the server to generate an auxiliary dataset using uploaded local models every round, which may be prohibitively slow for large U-Nets or high-resolution outputs.

- **Open Question 3**: How robust is FedDDPM when the local "Warmup" models are under-trained, causing the auxiliary dataset to deviate from the ideal global distribution? Assumption 4 assumes auxiliary gradients are globally unbiased, and Section VI specifies clients train for 200-600 epochs to "accurately fit" local distributions. The method's sensitivity to the quality of this initialization is not fully characterized.

## Limitations

- **Convergence analysis scope**: Theoretical proof assumes idealized conditions including perfectly representative auxiliary data and standard FL assumptions, but practical effectiveness depends heavily on auxiliary dataset quality
- **Reproducibility gaps**: Critical implementation details missing including exact optimizer hyperparameters, EMA settings, specific β schedule choices, and precise FID computation methodology
- **Scalability considerations**: Server-side refinement introduces significant computational overhead that scales with auxiliary dataset size and refinement epochs, not analyzed for larger-scale deployments

## Confidence

- **High confidence**: Core algorithmic framework and experimental methodology are well-specified and reproducible with standard diffusion model implementations
- **Medium confidence**: Convergence analysis provides theoretical backing but practical effectiveness depends on unverified auxiliary dataset quality
- **Medium confidence**: Performance improvements over baselines are demonstrated but exact replication requires addressing missing implementation details
- **Low confidence**: One-shot correction mechanism's robustness to different non-IID distributions and threshold settings

## Next Checks

1. **Warmup quality validation**: Generate and qualitatively evaluate samples from each client's warmup model before federated training begins. Document whether local models capture distinct data modes that auxiliary sampling can leverage.

2. **FID computation audit**: Implement FID using the exact same InceptionV3 feature extractor, sample generation procedure (5,000-50,000 samples), and evaluation protocol as the paper to ensure fair comparison with reported baselines.

3. **Threshold sensitivity analysis**: Systematically vary QUICK TEST parameters (threshold ∈ {0.1, 0.2, 0.5}, test_size ∈ {200, 500, 1000}) on CIFAR10 with α=0.1 to characterize termination behavior and identify overfitting vs. under-training regimes.