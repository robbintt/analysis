---
ver: rpa2
title: 'FloE: On-the-Fly MoE Inference on Memory-constrained GPU'
arxiv_id: '2505.05950'
source_url: https://arxiv.org/abs/2505.05950
tags:
- sparsity
- expert
- inference
- projection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FloE addresses the challenge of efficient on-the-fly inference
  for Mixture-of-Experts (MoE) models on memory-constrained GPUs by exploiting internal
  redundancy within sparsely activated experts. The core method introduces hybrid
  compression combining contextual sparsification (applied to gate and down projection
  matrices based on low-magnitude activations) and ultra-low-bit quantization (applied
  to the up projection matrix, which shows minimal sensitivity to quantization).
---

# FloE: On-the-Fly MoE Inference on Memory-constrained GPU

## Quick Facts
- arXiv ID: 2505.05950
- Source URL: https://arxiv.org/abs/2505.05950
- Reference count: 40
- Primary result: Achieves 9.3× parameter compression per expert in Mixtral-8x7B, enables deployment on 11GB VRAM GPU, and delivers 48.7× inference speedup vs DeepSpeed-MII on RTX 3090

## Executive Summary
FloE addresses the challenge of efficient on-the-fly inference for Mixture-of-Experts (MoE) models on memory-constrained GPUs by exploiting internal redundancy within sparsely activated experts. The core method introduces hybrid compression combining contextual sparsification and ultra-low-bit quantization, along with dual sparsity predictors that enable pipelining between transfer and computation. The system also implements an efficient sparse kernel and compact asynchronous transfer mechanism to maximize PCIe bandwidth utilization. Empirically, FloE achieves 9.3× parameter compression per expert in Mixtral-8x7B, enables deployment on a GPU with only 11GB VRAM (reducing memory footprint by up to 8.5×), and delivers a 48.7× inference speedup compared to DeepSpeed-MII on a single GeForce RTX 3090, with only 4.4%–7.6% average performance degradation.

## Method Summary
FloE introduces a hybrid compression approach that applies contextual sparsification to gate and down projection matrices based on low-magnitude activations, while using ultra-low-bit quantization (INT2) on the up projection matrix which shows minimal sensitivity to quantization. To enable pipelining between transfer and computation, FloE employs two sparsity predictors: an inter-expert learning-based predictor and an intra-expert reuse-based predictor, both leveraging hidden state similarity across layers. The system implements an efficient sparse kernel and compact asynchronous transfer mechanism to maximize PCIe bandwidth utilization. The approach targets Mixtral-8x7B specifically, achieving 9.3× parameter compression per expert while maintaining performance within 4.4–7.6% of the dense baseline.

## Key Results
- Achieves 9.3× parameter compression per expert in Mixtral-8x7B through hybrid compression
- Enables deployment on a GPU with only 11GB VRAM (reducing memory footprint by up to 8.5×)
- Delivers 48.7× inference speedup compared to DeepSpeed-MII on a single GeForce RTX 3090
- Maintains only 4.4%–7.6% average performance degradation compared to dense baseline

## Why This Works (Mechanism)

### Mechanism 1
Hybrid compression with differential treatment of expert projection matrices achieves 9.3× parameter reduction while limiting performance degradation to 4.4–7.6%. Contextual sparsification targets gate/down projections—exploiting activation concentration near zero—while INT2 quantization compresses the up projection, which exhibits lowest sensitivity to low-bit compression. The down projection functions as a "value memory" requiring highest precision; gate and up serve as "keys" with greater robustness.

### Mechanism 2
Dual sparsity predictors enable transfer-computation overlap by forecasting expert activations ~95% accurately without requiring router execution. Inter-expert predictor learns mapping from layer-i hidden state + historical trajectory → layer-(i+1) expert routing. Intra-expert predictor reuses layer-i hidden state with cached layer-(i+1) up projection weights to precompute activation magnitudes, deriving sparse masks. High cosine similarity (>0.95) between consecutive layer inputs enables this prediction.

### Mechanism 3
System-level optimizations translate algorithmic compression into 12.6× transfer acceleration and up to 2× compute speedup. Compact weight layout co-locates gate projection columns with corresponding down projection rows, doubling contiguous chunk size. Sparse GEMV kernel fuses SiLU activation into matrix operations, uses transposed W_down for column-major access, and selectively loads columns based on threshold mask. SIMD (AVX-512) + multithreaded async transfer saturates PCIe bandwidth.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) Sparse Activation**
  - Why needed here: FloE's entire premise depends on only k of n experts activating per token, enabling offloading. Without this, all 94GB must remain VRAM-resident.
  - Quick check question: Given Mixtral-8×7B with top-2 routing, what fraction of expert parameters are active per forward pass?

- **Concept: PCIe Bandwidth vs Compute Roofline**
  - Why needed here: The 32GB/s PCIe 4.0 bottleneck vs 300GB/s GPU-VRAM bandwidth explains why naive offloading shifts decoding from memory-bound to I/O-bound.
  - Quick check question: Why does transferring 300MB of expert weights take ~15ms while computing them takes only ~5ms on RTX 3090?

- **Concept: Activation Sparsity in SwiGLU-based LLMs**
  - Why needed here: FloE's contextual sparsification relies on magnitude-based pruning producing minimal perplexity impact. Understanding why down projection is most robust requires grasping the "value memory" role.
  - Quick check question: Which projection in SwiGLU-MLP exhibits highest perplexity increase under 70% activation pruning—gate, up, or down?

## Architecture Onboarding

- **Component map:**
  DRAM: Compressed Expert Store → compact async transfer (AVX-512, chunk=50) → VRAM: Expert Cache (12GB) → sparse GEMV kernel → GPU Compute: Layer-i Non-Experts + Experts → hidden state extraction → Predictors: Inter-expert MLP + Intra-expert reuse → prefetch trigger → Next-Layer Expert Loading

- **Critical path:** Hidden state from layer-i → predictors → prefetch layer-(i+1) compressed experts → sparse kernel execution. Latency is dominated by PCIe transfer (~15ms) when cache miss occurs; computation (~5ms) is overlapped.

- **Design tradeoffs:**
  - Sparsity ratio vs accuracy: 80% sparsity → 2.8% accuracy gain over baselines; 90% → 9.8% gain but higher perplexity
  - Predictor complexity vs memory: Learning-based predictors (2.19–9GB) vs parameter-free reuse predictor
  - Chunk size vs bandwidth: 50 columns optimal; smaller → API overhead; larger → DRAM packing bottleneck

- **Failure signatures:**
  - Cache thrashing: Repeated expert reloads indicate predictor accuracy degradation or insufficient cache size
  - Perplexity spike >15%: Calibration data distribution mismatch; re-estimate thresholds on target domain
  - Sub-50% PCIe utilization: Chunk size misconfigured; async transfer threads underutilized

- **First 3 experiments:**
  1. **Baseline transfer benchmark:** Transfer 20% of expert weights using naive PyTorch vs compact layout at chunk sizes [10, 50, 100, 500]; target 80%+ bandwidth utilization
  2. **Sparsity-perplexity sweep:** Run WikiText-2 perplexity evaluation at sparsity ratios [50, 60, 70, 80, 90]% for each projection independently; confirm up-projection sparsification yields lowest perplexity increase
  3. **End-to-end latency with cache size variation:** Measure TPS on ShareGPT with VRAM budgets [12, 14, 18, 21, 24]GB; verify 48.7× speedup vs DeepSpeed-MII at 12GB, identify cache size where FloE approaches Mixtral-GPU baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the sparse GEMV kernel be optimized to overcome kernel launch overheads on high-throughput GPUs (e.g., H100, A100)?
- Basis: Section 4.1 notes that while the kernel provides ~2x speedup on consumer GPUs (RTX 3090) at high sparsity, speedups on H100/A100 are limited ("1.64x") because non-computational factors and kernel launch overheads dominate due to higher computational throughput.
- Why unresolved: The current Triton-based kernel is optimized for memory-bound scenarios typical of consumer hardware, not the compute-bound or latency-sensitive dispatch scenarios of datacenter hardware.
- What evidence would resolve it: A modified kernel design or operator fusion strategy that demonstrates improved scaling on H100/A100 architectures at 90% sparsity.

### Open Question 2
- Question: Can FloE's hybrid compression and pipelining maintain efficiency under batched inference workloads?
- Basis: The paper explicitly restricts the evaluation scope to "single-batch latency-sensitive inference" and contrasts its approach with "high-throughput inference in offline scenarios" (Section 2).
- Why unresolved: Increasing batch sizes changes the computation-to-communication ratio and PCIe bandwidth utilization, which may break the assumption that transfer overhead can be hidden by single-batch computation.
- What evidence would resolve it: End-to-end latency and throughput metrics evaluating FloE with batch sizes greater than 1.

### Open Question 3
- Question: Does the intra-expert sparsity assumption hold for non-SwiGLU or alternative routing architectures?
- Basis: The methodology relies heavily on activation distributions specific to SwiGLU-based experts (Section 3.2.1) and the system is primarily validated on Mixtral-8x7B (Section 4).
- Why unresolved: MoE models using different activation functions (e.g., GeGLU, ReLU) or routing mechanisms (e.g., Expert Choice, Sinkhorn) may exhibit different sparsity patterns, affecting the accuracy of the reuse-based predictor.
- What evidence would resolve it: Evaluation of the contextual sparsification sensitivity and predictor accuracy on MoE models with diverse architectures.

## Limitations
- Performance claims are based on a single RTX 3090 configuration and may not generalize to different GPU architectures or PCIe generations
- Learning-based inter-expert predictor requires 2.19–9GB of memory, potentially consuming 50-75% of the target 12GB footprint
- Strong dependence on calibration data distribution matching inference inputs; out-of-distribution inputs may cause predictor accuracy degradation

## Confidence
- **High confidence** in compression mechanism (9.3× reduction with 4.4–7.6% perplexity impact) due to systematic ablation studies across multiple MoE models
- **Medium confidence** in dual predictor approach achieving 95% accuracy and enabling meaningful transfer-computation overlap, as reuse-based predictor is parameter-free but learning-based predictor's effectiveness depends on training quality
- **Medium confidence** in system optimizations delivering 12.6× transfer speedup and 48.7× end-to-end acceleration, as these claims are based on a single GPU configuration

## Next Checks
1. **Distribution shift robustness test**: Run perplexity evaluation on out-of-distribution datasets (e.g., technical documentation, code) to measure how predictor accuracy and sparsity mask quality degrade when activation distributions diverge from calibration corpus.

2. **Multi-GPU scaling validation**: Implement FloE on an H100 system with NVLink and measure whether the PCIe-bound optimizations remain beneficial or become bottlenecks compared to native GPU memory capacity and bandwidth.

3. **Cache size sensitivity analysis**: Systematically vary the expert cache size (12GB → 18GB → 24GB) and measure the crossover point where transfer overhead becomes negligible and FloE performance approaches native Mixtral-GPU baseline, identifying the minimum viable cache for different workload types.