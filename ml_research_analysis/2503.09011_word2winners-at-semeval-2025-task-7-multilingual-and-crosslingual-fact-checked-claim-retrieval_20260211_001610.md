---
ver: rpa2
title: 'Word2winners at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked
  Claim Retrieval'
arxiv_id: '2503.09011'
source_url: https://arxiv.org/abs/2503.09011
tags:
- multilingual
- english
- crosslingual
- languages
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a system for retrieving previously fact-checked
  claims from a multilingual dataset for the SemEval 2025 Task 7. The authors address
  the challenge of matching social media posts to relevant fact-checks across different
  languages.
---

# Word2winners at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval

## Quick Facts
- arXiv ID: 2503.09011
- Source URL: https://arxiv.org/abs/2503.09011
- Reference count: 5
- Primary result: Best model achieved 85% accuracy on crosslingual data and 92% on monolingual data

## Executive Summary
This paper presents a system for retrieving previously fact-checked claims from a multilingual dataset to match social media posts across different languages. The authors tackle the challenge of semantic similarity in crosslingual settings by combining preprocessing, summarization, state-of-the-art retrieval models, machine translation, and fine-tuning. Their approach demonstrates that translating non-English posts to English and using English-specific models significantly improves crosslingual accuracy, while majority voting ensembles provide robust performance across languages.

## Method Summary
The system preprocesses social media posts by removing noise (hashtags, URLs, emojis), concatenates OCR text, and optionally summarizes content. It evaluates bi-encoder models in zero-shot settings, then fine-tunes top performers using Multiple Negatives Ranking Loss with contrastive learning. For crosslingual retrieval, the approach translates non-English posts to English and uses English-specific models. The final system employs majority voting ensembles that weight model confidence and per-language accuracy scores. The method was evaluated on the MultiClaim dataset containing 31K+ post-claim pairs across 39 languages.

## Key Results
- Fine-tuned E5-Large-Instruct achieved 89% average accuracy across all languages
- Majority voting ensemble achieved 92% overall accuracy, outperforming best single model at 91%
- Crosslingual retrieval achieved 85% accuracy compared to 92% monolingual accuracy
- Fine-tuning improved performance more in crosslingual (0.09 gain) than monolingual (0.03 gain) settings

## Why This Works (Mechanism)

### Mechanism 1: Machine Translation + English-Specific Models for Crosslingual Retrieval
- Translating non-English posts to English and using English-specific retrieval models improves crosslingual accuracy by bypassing crosslingual alignment problems where multilingual models struggle to generate similar embeddings for semantically equivalent content across languages.

### Mechanism 2: Multiple Negatives Ranking Loss (MNRL) Fine-Tuning
- Fine-tuning with MNRL improves retrieval by learning to distinguish relevant post-claim pairs from unrelated claims, treating each claim in a batch as a negative sample for other posts and optimizing the similarity function.

### Mechanism 3: Majority Voting Ensemble with Confidence-Weighted Scoring
- Combining multiple models via weighted voting improves robustness and outperforms any single model by assigning scores based on model confidence and accuracy in the corresponding language, balancing the strengths of different models across languages.

## Foundational Learning

- Concept: **Bi-encoder / Dual-encoder architecture for retrieval**
  - Why needed here: The system encodes posts and claims independently into embeddings, then computes cosine similarity. Understanding this architecture is essential for debugging retrieval failures.
  - Quick check question: Can you explain why bi-encoders are faster but less precise than cross-encoders at inference time?

- Concept: **Multiple Negatives Ranking Loss (contrastive learning)**
  - Why needed here: MNRL is the core training objective. Without understanding how in-batch negatives work, you cannot debug fine-tuning instability or overfitting.
  - Quick check question: If you increase batch size from 2 to 16, what happens to the number of negative samples per query?

- Concept: **Zero-shot transfer vs. fine-tuning trade-offs**
  - Why needed here: The paper relies on zero-shot evaluation to select models before fine-tuning. Understanding what zero-shot means for embedding models informs model selection.
  - Quick check question: Why might a model with strong MTEB benchmark scores fail on this specific task in zero-shot mode?

## Architecture Onboarding

- Component map: Raw post → Preprocessing (remove hashtags/URLs/emojis, concatenate OCR) → Translation to English (for crosslingual/English models) → [Optional] Summarization via LLM → Embedding encoder (multiple models in parallel) → Cosine similarity search against claim embeddings → Majority voting ensemble → Top 10 retrieved claims

- Critical path: Embedding quality → similarity computation → voting. Fine-tuning is the highest-impact step; Table 5 shows E5-Large-Instruct improving from 0.81 to 0.89 average after fine-tuning.

- Design tradeoffs:
  - Summarization: Attempted to reduce noise but reduced accuracy (Table 2: SONAR dropped from 0.57 to 0.37). Summarization makes claims more similar to each other, hurting distinction.
  - Multilingual vs. English models: Multilingual models generalize better; English models achieve higher accuracy on translated data. Choose based on deployment context.
  - Fine-tuning stability: UAE-Large-V1 overfit; smaller learning rates or early stopping may be required.

- Failure signatures:
  - Low crosslingual accuracy + high monolingual: Model lacks crosslingual alignment; try translation or a multilingual model.
  - Fine-tuned model worse than zero-shot: Likely overfitting (see UAE-Large-V1); reduce epochs or increase warmup.
  - Disproportionately low accuracy on specific languages (e.g., Arabic at 0.70 vs. Thai at 0.96): Limited training data or poor tokenization for that language.

- First 3 experiments:
  1. **Zero-shot baseline**: Run GTE-Multilingual-Base and Bilingual-Embedding-Large on a held-out validation set (both original and English-translated versions) to establish baselines before fine-tuning.
  2. **Fine-tuning sweep**: Fine-tune the top 2 zero-shot models with batch sizes [4, 8, 16] and learning rates [1e-5, 2e-5, 3e-5] for 1-3 epochs. Monitor validation S@10 to detect overfitting early.
  3. **Ensemble ablation**: Build the majority voting ensemble with 3-5 models. Ablate by removing one model at a time to measure each model's marginal contribution to ensemble performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative summarization techniques be developed that preserve the discriminative features necessary for distinguishing highly similar fact-check claims?
- Basis in paper: The authors note that current summarization methods reduced accuracy by increasing similarity between claims, and they explicitly suggest exploring alternative summarization techniques in future work.

### Open Question 2
- Question: How can retrieval systems be adapted to effectively handle informal language and idiomatic expressions in low-resource languages without extensive parallel corpora?
- Basis in paper: The conclusion identifies handling informal language and ambiguous expressions in low-resource languages as a specific challenge and area for future focus.

### Open Question 3
- Question: Does the majority voting ensemble strategy mask fundamental deficiencies in crosslingual semantic alignment for low-resource languages?
- Basis in paper: While the ensemble improved overall scores, the results show significant performance discrepancies across languages (e.g., high English difficulty vs. high Thai accuracy), suggesting the voting mechanism averages errors rather than solving specific alignment gaps.

## Limitations
- Reliance on machine translation introduces potential semantic degradation, particularly for low-resource language pairs
- Preprocessing pipeline may remove context-specific information in certain domains
- Majority voting ensemble assumes model errors are uncorrelated, which may not hold for languages with similar characteristics

## Confidence

**High confidence**: Zero-shot evaluation methodology and fine-tuning procedure (MNRL loss implementation is well-specified)

**Medium confidence**: Exact majority voting scoring formula (confidence weighting scheme not fully detailed), crosslingual performance estimates (depends on unspecified machine translation quality)

**Low confidence**: External generalizability (results tied to specific dataset characteristics and preprocessing)

## Next Checks

1. **Translation quality validation**: Measure retrieval performance using both the machine translation approach and directly using multilingual models on the same crosslingual subset to quantify translation overhead.

2. **Ablation of preprocessing components**: Systematically remove individual preprocessing steps (hashtag/URL removal, OCR concatenation, summarization) to quantify their individual impact on retrieval accuracy across different languages.

3. **Ensemble stability testing**: Perform leave-one-out ablation studies on the majority voting ensemble to identify which models contribute most to performance gains and whether certain language pairs are particularly sensitive to ensemble composition.