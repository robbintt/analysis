---
ver: rpa2
title: 'Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential
  Quadratic Programming'
arxiv_id: '2512.08948'
source_url: https://arxiv.org/abs/2512.08948
tags:
- page
- have
- proof
- lemma
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online statistical inference for constrained
  stochastic optimization problems with both equality and inequality constraints.
  The authors propose a Stochastic Sequential Quadratic Programming (SSQP) method
  that addresses key challenges in such problems, including biased step directions
  and infeasibility of linearized constraints.
---

# Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming

## Quick Facts
- **arXiv ID**: 2512.08948
- **Source URL**: https://arxiv.org/abs/2512.08948
- **Reference count**: 40
- **Primary result**: First fully online method achieving primal-dual asymptotic minimax optimality for constrained stochastic optimization without projection operators

## Executive Summary
This paper develops a Stochastic Sequential Quadratic Programming (SSQP) method for online statistical inference in constrained stochastic optimization problems with both equality and inequality constraints. The method addresses key challenges including biased step directions and infeasibility of linearized constraints through constraint relaxation and momentum-style gradient moving-average techniques. The approach maintains both primal and dual variables while allowing for adaptive random stepsizes, and achieves strong theoretical guarantees including global almost-sure convergence and local asymptotic normality with optimal primal-dual limiting covariance matrix.

## Method Summary
The core method employs sequential quadratic approximations with constraint relaxation to address the fundamental challenges in online constrained optimization. By incorporating momentum-style gradient moving-average techniques, the approach debiases the step direction while maintaining feasibility of linearized constraints. The algorithm operates fully online without requiring projection operators, which are generally intractable for nonlinear problems. The method jointly updates primal and dual variables using stochastic gradients, with adaptive stepsizes that enable both convergence and efficient inference. The theoretical framework establishes that this approach achieves the minimax lower bound for both primal and dual estimation in the asymptotic regime.

## Key Results
- Global almost-sure convergence of KKT residual to zero from any initialization
- Local asymptotic normality with optimal primal-dual limiting covariance matrix matching theoretical lower bound
- Consistent plug-in covariance matrix estimator for practical inference
- First fully online approach achieving primal-dual asymptotic minimax optimality without projection operators

## Why This Works (Mechanism)
The method works by combining sequential quadratic programming with stochastic approximation techniques. The key innovation is the use of constraint relaxation in the quadratic subproblems, which prevents infeasibility issues that plague naive online approaches. The momentum-style gradient averaging debiases the step direction by reducing the variance of stochastic gradients over time. By maintaining both primal and dual variables simultaneously and using adaptive stepsizes, the algorithm can navigate the complex geometry of constrained optimization landscapes while ensuring convergence to KKT points. The sequential quadratic approximation structure allows the method to handle nonlinear constraints naturally while maintaining computational tractability.

## Foundational Learning
- **KKT conditions**: Necessary conditions for optimality in constrained optimization; essential because the method targets KKT residual convergence
- **Sequential Quadratic Programming (SQP)**: Classical method for constrained optimization using quadratic subproblems; provides the structural foundation for the stochastic variant
- **Asymptotic normality**: Statistical property indicating convergence to Gaussian distribution; needed to establish validity of inference procedures
- **Minimax optimality**: Framework for establishing best possible estimation accuracy; used to benchmark the primal-dual estimation performance
- **Constraint relaxation**: Technique for handling infeasibility in optimization; critical for making online updates practical
- **Primal-dual methods**: Joint optimization of primal variables and Lagrange multipliers; necessary for handling constraints without projection

## Architecture Onboarding

**Component Map**
SSQP method -> Sequential quadratic subproblems with relaxation -> Momentum-averaged stochastic gradients -> Adaptive stepsize updates -> Primal-dual variable updates

**Critical Path**
Stochastic gradient estimation → Constraint relaxation in QP subproblem → Momentum-averaged gradient computation → Primal-dual update with adaptive stepsize → KKT residual evaluation

**Design Tradeoffs**
- Accuracy vs. computational cost: More accurate QP subproblems increase per-iteration cost but may improve convergence
- Memory vs. momentum: Longer moving averages reduce variance but require more memory and may slow adaptation
- Adaptivity vs. stability: Larger stepsizes accelerate convergence but may compromise stability near optima

**Failure Signatures**
- Divergence when stepsizes are too large relative to gradient noise
- Persistent infeasibility when constraint relaxation is insufficient
- Slow convergence when momentum parameters are poorly tuned
- Bias accumulation when stochastic gradients have systematic errors

**3 First Experiments**
1. Verify global convergence on a simple convex constrained problem with known KKT point
2. Test primal-dual estimation accuracy on a quadratic programming instance with varying noise levels
3. Evaluate the finite-sample coverage of confidence intervals using the plug-in covariance estimator

## Open Questions the Paper Calls Out
None

## Limitations
- Computational efficiency and runtime comparisons with existing methods are not extensively provided
- Theoretical convergence relies on smoothness conditions and boundedness assumptions that may not hold in all applications
- Finite-sample behavior of the plug-in covariance matrix estimator requires more empirical validation
- Performance under non-smooth constraints or heavy-tailed noise distributions is not thoroughly explored

## Confidence
- Theoretical convergence guarantees (High): Almost-sure convergence and asymptotic normality results are well-supported by mathematical proofs
- Practical performance claims (Medium): Benchmark experiments show superior performance, but more extensive comparisons across diverse problem sets would strengthen claims
- Inference validity (Medium): Consistency of plug-in estimator is theoretically established, but finite-sample behavior needs more empirical validation

## Next Checks
1. Conduct extensive computational benchmarks comparing SSQP's runtime and memory usage against competing methods across various problem scales and dimensions
2. Perform Monte Carlo simulations to evaluate the finite-sample coverage probability of confidence intervals constructed using the proposed plug-in covariance estimator
3. Test the method's robustness to violations of technical assumptions, particularly on problems with non-smooth constraints or heavy-tailed noise distributions