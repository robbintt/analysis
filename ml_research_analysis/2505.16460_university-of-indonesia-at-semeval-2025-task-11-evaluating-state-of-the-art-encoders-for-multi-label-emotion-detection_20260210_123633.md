---
ver: rpa2
title: 'University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art
  Encoders for Multi-Label Emotion Detection'
arxiv_id: '2505.16460'
source_url: https://arxiv.org/abs/2505.16460
tags:
- wang
- zhang
- emotion
- chen
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses multilingual multi-label emotion classification
  across 28 languages in the SemEval 2025 Task 11. The authors explore two strategies:
  fully fine-tuning transformer models and classifier-only training using prompt-based
  encoders such as mE5 and BGE.'
---

# University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection

## Quick Facts
- arXiv ID: 2505.16460
- Source URL: https://arxiv.org/abs/2505.16460
- Reference count: 11
- Primary result: Embedding-based methods with tree-based classifiers outperform fine-tuning for multilingual emotion detection (F1-macro 56.58)

## Executive Summary
This paper addresses the SemEval 2025 Task 11 challenge of multilingual multi-label emotion classification across 28 languages using the BRIGHTER dataset. The authors evaluate two strategies: fully fine-tuning transformer models versus training classifiers on frozen embeddings from prompt-based encoders. Through extensive experimentation with various architectures, loss functions, and training modes, they demonstrate that embedding-based approaches using BGE and CatBoost classifiers with emotion-specific prompting significantly outperform end-to-end fine-tuning approaches. The best ensemble of multiple BGE models achieves an average F1-macro score of 56.58 across all languages, establishing a new state-of-the-art for this task.

## Method Summary
The authors explore two training strategies for multi-label emotion classification: (1) Classifier-only training where encoders (BGE, mE5, Jina, XLMR) are frozen and embeddings are extracted for training tree-based classifiers (CatBoost, XGBoost, Logistic Regression, SVC) with class imbalance mitigation through weighted loss functions; (2) End-to-end fine-tuning of transformer models using Focal Loss or Asymmetric Loss. The BRIGHTER dataset is split 80:20 using iterative stratification, with experiments conducted in both ALL (multilingual) and LANG (monolingual) modes. Emotion-specific prompting is applied to BGE models, and ensemble methods combine multiple model variants through weighted voting based on development set performance.

## Key Results
- Embedding-based methods (BGE + CatBoost) significantly outperform fully fine-tuned transformers (U = 456, p < 0.001)
- Multilingual training does not provide significant advantages over monolingual training (W = 48, p = 0.06)
- Best ensemble (Model V2) achieves 56.58 F1-macro across all 28 languages
- Class imbalance mitigation via weighted loss functions is essential for rare emotions
- Emotion-specific prompting consistently improves performance over general prompts

## Why This Works (Mechanism)
The success of embedding-based methods stems from leveraging high-quality pre-trained multilingual representations that capture cross-lingual semantic structures, combined with efficient tree-based classifiers that can effectively handle the multi-label nature of emotion detection. The frozen encoder approach preserves the integrity of pre-trained embeddings while allowing the classifier to learn the specific relationships between semantic features and emotion labels. Class imbalance mitigation through weighted loss functions addresses the inherent skew in emotion distribution, particularly for rare emotions like disgust. Emotion-specific prompting further refines the embeddings by directing the model's attention toward relevant emotional content.

## Foundational Learning
**Iterative Stratification**: A data splitting technique that preserves label distribution across folds, essential for multi-label classification to ensure rare emotions are represented in both train and validation sets. *Quick check: Verify class distribution in train/val splits matches original dataset.*

**Class Imbalance Mitigation**: Using weighted loss functions (w_i = N / (|C_i| × k)) to address skewed emotion distributions, particularly important for rare emotions like disgust. *Quick check: Compare per-emotion F1 scores with and without class weighting.*

**Focal Loss**: A modified cross-entropy loss that down-weights well-classified examples, designed to address class imbalance by focusing on hard examples. *Quick check: Compare Focal Loss vs. standard cross-entropy on rare emotion detection.*

**Asymmetric Loss**: A loss function that handles extreme class imbalance by applying different focusing parameters to positive and negative classes, with m controlling the trade-off between false positives and false negatives. *Quick check: Test different γ+ and γ- values to optimize for emotion detection.*

**Emotion-Specific Prompting**: Modifying input prompts to explicitly target specific emotions, improving embedding quality for emotion detection tasks. *Quick check: Compare general vs. emotion-specific prompt performance across different languages.*

**Weighted Ensemble Voting**: Combining multiple model predictions using weights based on individual model performance, improving overall robustness and accuracy. *Quick check: Validate ensemble weights by incrementally adding component models.*

## Architecture Onboarding

**Component Map**: Input Text -> Encoder (BGE/mE5) -> Embeddings -> Classifier (CatBoost/XGBoost) -> Multi-label Predictions OR Input Text -> Transformer -> Fine-tuning -> Multi-label Predictions

**Critical Path**: Text → BGE-mGemma2 (emotion-specific prompt) → CatBoost (weighted) → Final Predictions

**Design Tradeoffs**: 
- Frozen encoder preserves pre-trained quality vs. fine-tuning adapts to task but risks degradation on low-resource data
- Tree-based classifiers handle multi-label structure efficiently vs. transformers require more parameters and training
- Ensemble methods improve robustness vs. single model simplicity

**Failure Signatures**: 
- Low F1 on rare emotions indicates insufficient class imbalance mitigation
- Poor cross-lingual transfer suggests embeddings don't capture universal emotion semantics
- High variance across languages indicates model bias toward high-resource languages

**First Experiments**:
1. Test embedding quality by comparing BGE outputs with/without emotion-specific prompting
2. Validate class imbalance handling by analyzing per-emotion F1 scores
3. Verify iterative stratification preserves label distribution across splits

## Open Questions the Paper Calls Out
**Open Question 1**: What specific prompt engineering strategies beyond emotion-specific prompting could further improve performance for multilingual emotion detection? The authors note that modifying prompts from general to specific consistently improves performance, but this was only tested on two samples with CatBoost models.

**Open Question 2**: Why does multilingual training fail to provide significant advantages over monolingual training for emotion detection despite theoretical benefits of cross-lingual transfer? The paper shows no significant difference (W = 48, p = 0.06) but doesn't investigate whether this is due to dataset characteristics or model architecture limitations.

**Open Question 3**: What mechanisms explain why frozen prompt-based encoders with tree-based classifiers outperform fully fine-tuned transformers for multi-label emotion classification? The authors hypothesize that fine-tuning on low-resource data cannot match pre-trained embedding quality but don't examine whether fine-tuning degrades representations.

**Open Question 4**: How can qualitative analysis of emotion detection errors be conducted effectively across low-resource languages where annotator proficiency is limited? The paper acknowledges this limitation but offers no solution for conducting meaningful qualitative analysis across 28 predominantly low-resource languages.

## Limitations
- Lack of extensive qualitative analysis due to limited language proficiency across 28 languages
- CatBoost hyperparameters unspecified, potentially affecting reproducibility
- Incomplete ensemble composition details for final Model V2
- No investigation into why fine-tuning fails to match pre-trained embedding quality

## Confidence
**High**: Overall methodology and two-strategy framework - clearly specified
**Medium**: F1-macro score achievement (56.58) - depends on precise ensemble weighting
**Medium**: Embedding-based superiority claim - requires exact prompt and classifier configuration

## Next Checks
1. Verify class imbalance handling by comparing per-emotion F1 scores and adjusting class weights if negative class dominates
2. Test embedding quality by comparing BGE-mGemma2 outputs with and without emotion-specific prompting across multiple languages
3. Validate ensemble performance by incrementally adding component models and measuring F1-macro changes to confirm the stated 56.58 score