---
ver: rpa2
title: 'Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator
  for Group Chats'
arxiv_id: '2511.17315'
source_url: https://arxiv.org/abs/2511.17315
tags:
- human
- participants
- group
- agent
- huma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents HUMA, an AI facilitator for group chats that
  addresses the gap between LLMs'' conversational capabilities and their lack of natural,
  asynchronous group interaction patterns. HUMA implements an event-driven architecture
  with three components: Router (strategy selection), Action Agent (execution), and
  Reflection (context maintenance).'
---

# Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats

## Quick Facts
- arXiv ID: 2511.17315
- Source URL: https://arxiv.org/abs/2511.17315
- Reference count: 25
- Primary result: HUMA agents achieved 55.4% AI detection rate (near-chance) and matched human facilitator quality on subjective measures

## Executive Summary
This paper presents HUMA, an AI facilitator designed to participate naturally in group chats through human-like timing, strategy diversity, and interruption handling. The system implements an event-driven architecture with three components: Router (strategy selection), Action Agent (execution with delay simulation), and Reflection (context maintenance). In a controlled study with 97 participants, HUMA agents were classified as human at near-chance rates, with subjective experience measures showing only small differences from human community managers.

## Method Summary
HUMA uses an event-driven architecture processing chat messages, reactions, and typing indicators in real-time. The Router selects from 20 strategies using appropriateness scoring plus timeliness penalties to promote diversity. The Action Agent executes strategy via tools (send_message, send_reply, add_reaction) while simulating human typing delays (50-100 WPM). The Reflection component generates single-sentence synthesis for context maintenance. The system handles interrupts by preserving scratchpad state and restarting from Router.

## Key Results
- AI detection rates: 55.4% in AI condition vs 46.7% in human condition (near-chance)
- Subjective experience: Small effect sizes for all metrics (Cohen's d < 0.4)
- Strategy routing with timeliness regularization prevented repetitive behavior patterns
- Interrupt-aware architecture with preserved scratchpad enabled coherent re-engagement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulating human-like response timing reduces detectability by eliminating superhuman speed cues.
- Mechanism: Introduces artificial delays in tool execution to approximate 50-100 WPM human typing speed.
- Core assumption: Users rely on temporal cues as heuristics for humanness; removing these cues forces reliance on content alone.
- Evidence anchors: Section 3.4.1 describes timing simulation; abstract mentions "realistic response-time simulation."
- Break condition: If users develop new detection heuristics based on consistency or lack of typos.

### Mechanism 2
- Claim: Timeliness-regularized strategy selection prevents repetitive behavior patterns.
- Mechanism: Router scores strategies using A (appropriateness) + T (timeliness penalty based on recency).
- Core assumption: Behavioral diversity in strategy selection correlates with perceived humanness.
- Evidence anchors: Section 3.3 describes timeliness penalty; Section 5 discusses behavioral diversity contribution.
- Break condition: If certain strategies are systematically over-penalized in specific conversation types.

### Mechanism 3
- Claim: Interruption-aware architecture with preserved scratchpad enables coherent re-engagement.
- Mechanism: Incoming events during generation are queued; interrupts trigger Router restart with preserved scratchpad.
- Core assumption: Humans track interrupted intentions and can resume them contextually.
- Evidence anchors: Section 3.4.2 describes interrupt handling; Section 3.2 explains scratchpad preservation.
- Break condition: If interrupts cascade rapidly, agent may perpetually restart without completing actions.

## Foundational Learning

- **Event-driven architecture vs. turn-based dialogue**
  - Why needed here: HUMA processes discrete events rather than awaiting complete turns; misunderstanding this leads to blocking operations that stall the system.
  - Quick check question: Can you explain why a traditional request-response loop fails when three participants send overlapping messages?

- **Tool call constraints in LLM agents**
  - Why needed here: The system forbids parallel send_message calls but allows message+reaction parallelism; violating this produces error responses.
  - Quick check question: What happens if the Action Agent attempts two send_message calls in a single workflow iteration?

- **Scratchpad persistence across interrupts**
  - Why needed here: The Reflection component and interrupted intentions rely on state surviving workflow restarts.
  - Quick check question: Where is the scratchpad stored, and what happens if an interrupt arrives during Reflection generation?

## Architecture Onboarding

- **Component map:**
  - Event Queue -> Router -> Action Agent -> Reflection -> Context Store
  - Interrupts: Event Queue -> Router restart with preserved scratchpad

- **Critical path:**
  1. Event arrives (message/reaction/typing indicator)
  2. Router scores strategies, selects max(A+T)
  3. Action Agent begins execution with delay simulation
  4. If interrupt → queue, complete generation, restart from Router with preserved scratchpad
  5. If no interrupt → Reflection generates context summary, stores for next iteration

- **Design tradeoffs:**
  - Delay simulation improves humanness but increases latency and interrupt vulnerability
  - Timeliness penalty prevents repetition but may suppress optimal strategies in niche contexts
  - Single-message constraint prevents spam but requires multi-turn planning for longer responses

- **Failure signatures:**
  - Agent never responds → Router consistently selecting "Keep Silent"; check appropriateness scoring
  - Agent repeats same strategy → Timeliness calculation not persisting across sessions
  - Agent responds to outdated context → Interrupts not triggering Router restart; scratchpad not included
  - Missing reactions → Tool parallelism not configured; add_reaction blocked by message delay

- **First 3 experiments:**
  1. Baseline timing test: Disable delay simulation; measure detection rate change
  2. Strategy ablation: Remove timeliness penalty; measure behavioral diversity and ratings
  3. Interrupt stress test: Simulate high-velocity conversations; observe "Continue Pending Action" activation

## Open Questions the Paper Calls Out
- How do community social dynamics evolve over extended periods (weeks to years) of continuous AI facilitation, and do members form genuine attachment to HUMA agents?
- Can HUMA agents perform effectively in high-conflict discussions, workplace environments, or groups where human participants are a minority?
- What safeguards can prevent weaponization of human-like AI agents for manipulation, astroturfing, or social engineering at scale?

## Limitations
- Study relies on role-play scenarios rather than authentic group chat dynamics
- Specific LLM models, prompts, and strategy definitions remain unspecified
- Single-sentence Reflection component's sufficiency for extended interactions is not fully validated

## Confidence
- **High confidence:** Event-driven architecture with interruption handling effectively enables coherent multi-user participation
- **Medium confidence:** Combination of delay simulation, timeliness regularization, and interruption handling contributes to believable behavior
- **Low confidence:** Single-sentence Reflection component is sufficient for maintaining conversational coherence across extended interactions

## Next Checks
1. Conduct controlled experiment comparing HUMA with and without delay simulation to isolate timing mechanism's impact on detection rates
2. Implement system with and without timeliness regularization to validate contribution of behavioral diversity to perceived humanness
3. Deploy HUMA in real community management scenarios with experienced facilitators to establish external validity beyond controlled experimental conditions