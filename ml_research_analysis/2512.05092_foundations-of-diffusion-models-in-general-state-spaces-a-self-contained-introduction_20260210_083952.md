---
ver: rpa2
title: 'Foundations of Diffusion Models in General State Spaces: A Self-Contained
  Introduction'
arxiv_id: '2512.05092'
source_url: https://arxiv.org/abs/2512.05092
tags:
- diffusion
- process
- reverse
- discrete
- forward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified theoretical framework for diffusion
  models across continuous and discrete state spaces, bridging the gap between stochastic
  differential equations (SDEs) in continuous domains and continuous-time Markov chains
  (CTMCs) in discrete domains. The authors develop a self-contained introduction that
  unifies these settings through the infinitesimal generator formalism, showing how
  both SDEs and CTMCs emerge as special cases of general Markov processes.
---

# Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction

## Quick Facts
- arXiv ID: 2512.05092
- Source URL: https://arxiv.org/abs/2512.05092
- Reference count: 40
- Unifies diffusion models across continuous and discrete state spaces through infinitesimal generator formalism

## Executive Summary
This paper presents a unified theoretical framework for diffusion models that bridges the gap between continuous stochastic differential equations (SDEs) and discrete continuous-time Markov chains (CTMCs). The authors develop a self-contained introduction showing how both settings emerge as special cases of general Markov processes through the infinitesimal generator approach. They derive the evidence lower bound (ELBO) directly from this generator perspective, recovering both denoising score matching objectives for continuous spaces and denoising score entropy objectives for discrete spaces. The work provides explicit parameterisations for reverse processes in both settings, connecting noise prediction, score-based methods, and clean-data prediction through linear relationships.

## Method Summary
The authors develop a unified framework for diffusion models using infinitesimal generators to treat continuous and discrete state spaces simultaneously. They show that both SDEs and CTMCs can be understood as special cases of general Markov processes, with the generator providing a common mathematical language. The ELBO is derived directly from this generator perspective, naturally recovering both the denoising score matching objective for continuous spaces and the denoising score entropy objective for discrete spaces. The framework establishes explicit parameterisations for reverse processes, connecting noise prediction, score-based methods, and clean-data prediction through linear relationships. This unified treatment enables application of continuous diffusion techniques to discrete data through latent diffusion approaches.

## Key Results
- Unifies SDEs and CTMCs as special cases of general Markov processes through infinitesimal generators
- Derives ELBO directly from generator perspective, recovering both denoising score matching and denoising score entropy objectives
- Establishes linear relationships connecting noise prediction, score-based methods, and clean-data prediction across settings

## Why This Works (Mechanism)
The infinitesimal generator formalism provides a common mathematical language that naturally encompasses both continuous and discrete Markov processes. By focusing on the generator rather than the specific representation (SDE or CTMC), the authors can derive unified expressions for the ELBO and parameterisations that work across both settings. The key insight is that both forward and reverse processes can be characterized through their generators, allowing the ELBO to be expressed in terms of these operators. This generator-based approach reveals the underlying unity between score-based methods and noise prediction, showing they are connected through simple linear transformations.

## Foundational Learning
1. **Infinitesimal Generator**: Operator characterizing the local behavior of Markov processes
   - Why needed: Provides common mathematical framework for continuous and discrete processes
   - Quick check: Verify generator satisfies semigroup property for both SDE and CTMC examples

2. **Forward/Backward Kolmogorov Equations**: Evolution equations for probability distributions
   - Why needed: Connect generator to time evolution of processes
   - Quick check: Confirm solution satisfies both forward and backward forms

3. **Evidence Lower Bound (ELBO)**: Objective function for training diffusion models
   - Why needed: Guides learning of both forward and reverse processes
   - Quick check: Verify ELBO decomposition matches standard formulations

4. **Denoising Score Matching**: Learning objective for estimating score functions
   - Why needed: Alternative to ELBO-based training with different theoretical properties
   - Quick check: Confirm equivalence between score matching and ELBO gradients

5. **Latent Diffusion**: Framework for applying continuous diffusion to discrete data
   - Why needed: Enables use of continuous diffusion techniques on discrete domains
   - Quick check: Verify discrete-to-continuous mapping preserves probabilistic structure

## Architecture Onboarding

**Component Map**: Generator -> Forward Process -> Reverse Process -> ELBO -> Training Objective -> Model Parameters

**Critical Path**: Forward process (defined by generator) → ELBO computation → Score estimation → Reverse process sampling

**Design Tradeoffs**: Continuous vs discrete state spaces trade mathematical tractability for representational power; generator-based approaches offer unified theory but may require specialized numerical methods

**Failure Signatures**: Generator ill-conditioning leading to numerical instability; score estimation errors propagating through linear relationships; discretization artifacts in latent space mappings

**First Experiments**:
1. Implement generator-based ELBO computation for simple 1D diffusion processes
2. Compare score estimation accuracy between continuous and discrete generator formulations
3. Test latent diffusion mapping for binary data with varying noise levels

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, focusing instead on establishing the unified theoretical framework. However, the work naturally raises questions about practical implementation details, computational efficiency of generator-based approaches, and empirical validation across diverse domains and data types.

## Limitations
- Practical computational advantages of generator-based ELBO formulation remain unproven
- Empirical validation across diverse domains and high-dimensional settings is limited
- Numerical stability of linear relationships connecting noise prediction, score-based methods, and clean-data prediction needs thorough testing

## Confidence
- Unified treatment of continuous and discrete state spaces: Medium
- Mathematical correctness of generator-based ELBO derivation: High
- Practical effectiveness of linear relationships in real-world applications: Medium
- Connection to masked language modeling and discrete diffusion approaches: Medium

## Next Checks
1. Implement and benchmark the unified framework across multiple discrete state spaces (binary, categorical, ordinal) to verify the theoretical parameterisations perform as expected
2. Conduct empirical studies comparing the generator-based ELBO formulation with standard implementations in terms of convergence speed and sample quality
3. Test the latent diffusion approach for discrete data on real-world datasets to validate the claimed connections to masked language modeling and practical discrete diffusion methods