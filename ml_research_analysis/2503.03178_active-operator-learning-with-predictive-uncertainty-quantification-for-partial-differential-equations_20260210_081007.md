---
ver: rpa2
title: Active operator learning with predictive uncertainty quantification for partial
  differential equations
arxiv_id: '2503.03178'
source_url: https://arxiv.org/abs/2503.03178
tags:
- network
- uncertainty
- learning
- training
- deeponet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for predictive uncertainty quantification
  in deep operator networks (DeepONets) using a single network approach that introduces
  minimal computational overhead during training and inference. The authors developed
  a framework that calibrates uncertainty estimates to model errors observed during
  training, using a log-likelihood loss function to train the network to output parameters
  for a predictive probability distribution.
---

# Active operator learning with predictive uncertainty quantification for partial differential equations

## Quick Facts
- arXiv ID: 2503.03178
- Source URL: https://arxiv.org/abs/2503.03178
- Authors: Nick Winovich; Mitchell Daneker; Lu Lu; Guang Lin
- Reference count: 40
- Key outcome: Single-network method for calibrated predictive uncertainty in DeepONets, validated on linear and nonlinear PDEs

## Executive Summary
This paper introduces a computationally efficient method for predictive uncertainty quantification in deep operator networks using a single network architecture. The approach modifies DeepONet to output parameters for a predictive probability distribution (mean and log-variance) trained via negative log-likelihood loss, resulting in uncertainty estimates calibrated to observed model errors. The method is validated on linear and nonlinear partial differential equation problems, demonstrating accurate uncertainty quantification and significant speedups in inference through optimized trunk output precomputation.

## Method Summary
The method modifies DeepONet architecture by splitting the final layers of both branch and trunk networks into two paths that output mean (μ) and log-variance (log σ) parameters. These parameters are trained using a Gaussian negative log-likelihood loss function, which calibrates uncertainty estimates to observed model errors during training. An optimized inference strategy precomputes trunk outputs and uses a sparse placement matrix to reduce evaluation times by more than a factor of five. The framework is validated on linear (Poisson) and nonlinear (diffusion-reaction) PDEs, showing unbiased, non-skewed uncertainty estimates that accurately reflect model errors across different complexity levels.

## Key Results
- Predictive uncertainty estimates are unbiased and non-skewed, with high correlation (R² > 0.9) to observed model errors
- Inference speed improved by more than 5x through precomputed trunk outputs and sparse placement matrix optimization
- Active learning framework using uncertainty estimates yields improvements in accuracy and data-efficiency for outer-loop optimization procedures
- Method extended to Fourier Neural Operators with modest performance gains compared to ensemble approaches

## Why This Works (Mechanism)

### Mechanism 1
A single deterministic network can provide calibrated predictive uncertainty estimates by interpreting outputs as probability distribution parameters. The network outputs two sets of parameters (μ and log σ) trained using NLL loss, which forces variance to converge to observed model error. Core assumption: error distribution is Gaussian and training data is representative. Evidence: Section 2.2 shows NLL loss interpretation, and corpus neighbors confirm single-network efficiency gap.

### Mechanism 2
Inference latency is drastically reduced by decoupling coordinate processing from function processing and precomputing static graph elements. Trunk outputs for fixed domains are precomputed into a matrix, and a sparse placement matrix maps these outputs during inference. Core assumption: query domain is fixed or bounded. Evidence: Abstract mentions 5x speedup, Section 2.3.2 provides specific timing improvements.

### Mechanism 3
Predictive uncertainty acts as a reliable proxy for model error in active learning loops. Calibrated variance correlates with actual error, enabling efficient data acquisition by querying simulator at high-uncertainty points. Core assumption: uncertainty generalizes to out-of-distribution data. Evidence: Section 5.1 shows active learning improvements, corpus neighbors validate calibration necessity.

## Foundational Learning

- **DeepONet Architecture (Branch & Trunk)**: Needed to understand how the method modifies standard structure to output uncertainty. Quick check: Can you explain how the inner product of branch and trunk outputs forms the final prediction μ(f,g)(x)?

- **Negative Log-Likelihood (NLL) for Heteroscedastic Regression**: The specific loss function used to force the network to predict variance. Quick check: If a network predicts small variance (σ²) but large error (μ - u), does NLL loss increase or decrease relative to standard MSE loss?

- **Aleatoric vs. Epistemic Uncertainty**: Needed to interpret why the model succeeds or fails in extrapolation. Quick check: Does the method use Bayesian posteriors (epistemic) or deterministic output parameters (aleatoric) to represent uncertainty?

## Architecture Onboarding

- **Component map**: Input f (to Branch) and Coordinate x (to Trunk) → Shared Layers → Split Layers (Mean and Log-Sigma paths) → Inner Product → Output μ and log σ → NLL Loss → Backprop

- **Critical path**: Data Normalization → Branch/Trunk Forward Pass → Split Head Computation → Inner Product → NLL Calculation → Backprop

- **Design tradeoffs**: Integrated vs. External UQ (efficient vs. potentially more robust coverage), Speed vs. Coverage (single-network ~10x faster but may have lower safety floor), Memory vs. Resolution (precomputation memory-prohibitive for high-resolution grids)

- **Failure signatures**: Numerical overflow from unscaled inputs causing NLL explosion, Underestimation in OoD regions with lower uncertainty than observed error, Memory constraints exceeding GPU capacity for high-resolution grids

- **First 3 experiments**:
  1. Linear Sanity Check: Train on 1D Poisson with homogeneous BCs; verify relative error <1% and UQ doesn't degrade accuracy
  2. Statistical Calibration: Train on Nonlinear Diffusion; plot error vs. predicted σ histograms; check R² correlation exceeds 0.9
  3. Inference Speed Benchmark: Implement placement matrix optimization; measure 500-example inference time against naive loop to confirm >5x speedup

## Open Questions the Paper Calls Out

### Open Question 1
Does the predictive uncertainty framework maintain calibration and reliability when applied to PDEs containing discontinuities and shocks? The conclusion states the authors plan to investigate this, as validation focused on smooth solutions where errors follow normal distribution, but shocks typically induce highly localized, skewed error distributions that may violate Gaussian assumptions.

### Open Question 2
Can the framework be modified to explicitly disentangle epistemic uncertainty from aleatoric uncertainty in a single deterministic network? Section 2.2.1 notes there are no mathematically rigorous frameworks for separating these, and Section 5.2.2 highlights the lack of strong correlation between epistemic uncertainty in early active learning iterations as a future study topic.

### Open Question 3
Can the log-likelihood uncertainty method be effectively integrated into graph neural operators and transformer-based architectures? Section 2.2 mentions extension to FNOs but leaves other operator architectures like GNOs and transformers for future research, as feasibility depends on architecture's ability to split outputs without destabilizing learned operator mapping.

## Limitations

- Method assumes Gaussian-distributed residuals, which may not hold for highly nonlinear or multimodal PDE solutions
- Uncertainty estimates primarily capture aleatoric uncertainty and may under-cover in out-of-distribution regions where epistemic uncertainty dominates
- Precomputed trunk optimization only effective for fixed, bounded domains and becomes memory-prohibitive for very high-resolution grids

## Confidence

- **High Confidence**: Core claim that NLL loss with split network outputs enables calibrated uncertainty estimation for DeepONets. Empirical validation across linear and nonlinear PDEs supports this.
- **Medium Confidence**: Claim of >5x inference speedup via placement matrix optimization. The mechanism is sound, but performance depends heavily on domain size and hardware specifics.
- **Medium Confidence**: Active learning improvements from uncertainty-guided sampling. While statistically demonstrated, real-world applicability depends on quality of uncertainty generalization to truly novel data.

## Next Checks

1. **Gaussian Assumption Test**: For Nonlinear Diffusion problem, generate histograms of observed errors vs. predicted σ. Compute R² correlation and perform Kolmogorov-Smirnov test to quantify deviation from Gaussianity.

2. **OoD Uncertainty Coverage**: Train on l=0.2, 0.2333, 0.2667, 0.3 and test on l=0.1333, 0.1667. Report 1σ/2σ/3σ coverage percentages. If coverage is <68/95/99.7, investigate whether rescaling input length-scales during training helps.

3. **Memory-Constrained Trunk**: For 128x128x76 case, implement "discrete trunk" variant (trunk on coarse grid). Compare accuracy and uncertainty calibration against full-trunk model to quantify tradeoff.