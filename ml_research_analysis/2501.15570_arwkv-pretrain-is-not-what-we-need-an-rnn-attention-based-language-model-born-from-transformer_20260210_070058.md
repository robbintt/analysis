---
ver: rpa2
title: 'ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model
  Born from Transformer'
arxiv_id: '2501.15570'
source_url: https://arxiv.org/abs/2501.15570
tags:
- attention
- arxiv
- stage
- state
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents ARWKV, a novel RNN-attention-based language
  model that achieves competitive performance through knowledge distillation from
  Qwen 2.5. The approach replaces standard transformer self-attention with RWKV-7
  time mixing modules while preserving expressiveness through attention alignment
  during training.
---

# ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer

## Quick Facts
- arXiv ID: 2501.15570
- Source URL: https://arxiv.org/abs/2501.15570
- Authors: Lin Yueyu; Li Zhiyuan; Peter Yue; Liu Xiao
- Reference count: 7
- Primary result: RNN-attention model distilled from Qwen 2.5-7B-Instruct achieves MMLU 62.41, GSM8K 39.95 using 8 hours on 16 AMD MI300X GPUs

## Executive Summary
This work presents ARWKV, a novel RNN-attention-based language model that achieves competitive performance through knowledge distillation from Qwen 2.5. The approach replaces standard transformer self-attention with RWKV-7 time mixing modules while preserving expressiveness through attention alignment during training. The 7B parameter model is distilled from Qwen 2.5-7B-Instruct using a three-stage process: attention alignment, knowledge distillation, and fine-tuning. Notably, the entire knowledge processing pipeline can be completed in just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance characteristics.

## Method Summary
ARWKV is created through a three-stage distillation process from Qwen 2.5-7B-Instruct. Stage 1 aligns the student's RWKV-7 time mixer hidden states with the teacher's attention outputs using L2 distance minimization. Stage 2 applies word-level KL-Divergence distillation with balanced data distribution. Stage 3 performs supervised fine-tuning for context extension and direct preference optimization for alignment. The model uses BF16 training with FP16 inference, achieving numerical stability without requiring the layer scaling of the original RWKV implementation.

## Key Results
- ARWKV achieves MMLU score of 62.41 and GSM8K score of 39.95
- Complete distillation pipeline completed in 8 hours using 16 AMD MI300X GPUs
- Model maintains competitive performance while using fewer tokens than traditional distillation
- RNN architecture successfully captures transformer attention patterns through distillation approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing transformer self-attention with RWKV-7 time mixing modules preserves expressiveness when trained with attention alignment.
- **Mechanism:** The TimeMixer is trained to minimize the L2 distance between its hidden state output and the teacher's attention block output. The fixed state size in RWKV-7 acts as a compression process that maps attention patterns to matrix-valued recurrent states. The state update follows: `State_t = State_{t-1}(diag(w_t) - κ̂_t(a_t ⊙ κ̂_t)) + v_t · k̃_t`, where `a` serves as the in-context learning rate.
- **Core assumption:** The recurrent state matrix can sufficiently approximate the attention distribution learned by the transformer without requiring explicit attention initialization from the teacher.
- **Evidence anchors:**
  - [abstract]: "replaces standard transformer self-attention with RWKV-7 time mixing modules while preserving expressiveness through attention alignment during training"
  - [section 3.1]: "we align the hidden state output between the student and teacher attention block... initializing state attention from teacher's attention is not necessary"
  - [corpus]: Limited external validation; corpus neighbors focus on unrelated attention mechanisms in vision-language models.
- **Break condition:** If hidden state alignment loss plateaus without converging, the recurrent state capacity may be insufficient for the target attention complexity.

### Mechanism 2
- **Claim:** Word-level KL-Divergence distillation with balanced data distribution enables rapid knowledge transfer with minimal tokens.
- **Mechanism:** The distillation minimizes divergence between teacher and student probability distributions at the token level rather than sequence level. Data distribution balancing based on Stage 1 training enables convergence with only 20M tokens in Stage 1 and 40M tokens in Stage 2.
- **Core assumption:** Token-level distribution matching captures sufficient knowledge for downstream task performance without requiring full sequence-level optimization.
- **Evidence anchors:**
  - [abstract]: "enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens"
  - [section 3.2]: "We adopt word-level KL-Divergence... achieving fast convergence with only 20M tokens"
  - [corpus]: No direct corpus support for token efficiency claims in distillation.
- **Break condition:** If task performance degrades disproportionately on reasoning benchmarks (e.g., GSM8K dropped from 82.34 to 39.95), the token-level distillation may not capture multi-step reasoning patterns.

### Mechanism 3
- **Claim:** FP16 inference after BF16 training improves performance without the layer scaling required by original RWKV.
- **Mechanism:** Training in BF16 provides numerical stability during gradient computation, while FP16 inference reduces precision-related noise that may regularize predictions. The model inherits sufficient numerical robustness from the transformer distillation that careful layer scaling becomes unnecessary.
- **Core assumption:** The distilled attention patterns are sufficiently well-conditioned that reduced inference precision acts as beneficial regularization rather than causing overflow.
- **Evidence anchors:**
  - [section 4]: "performing inference with float16 (FP16) significantly improved the performance... differs from the original RWKV implementation which required careful tuning"
  - [corpus]: No external validation of precision effects on RNN-attention models.
- **Break condition:** If inference shows numerical instability or NaN values in long-context scenarios, the precision mismatch may be exposing state accumulation errors.

## Foundational Learning

- **Concept: RWKV Time Mixing vs. Standard Attention**
  - **Why needed here:** The paper's core contribution is replacing quadratic attention with subquadratic recurrent computation; understanding the state update equation is essential for debugging alignment loss.
  - **Quick check question:** Can you explain why RWKV-7's state matrix can theoretically track longer-range dependencies than RWKV-6?

- **Concept: Knowledge Distillation Objectives (KL-Divergence)**
  - **Why needed here:** Stage 2 relies on word-level KL-Divergence; understanding why sequence-level distillation was rejected informs implementation choices.
  - **Quick check question:** What information is lost when using token-level instead of sequence-level distillation for multi-step reasoning tasks?

- **Concept: Group Query Attention (GQA)**
  - **Why needed here:** The teacher model (Qwen 2.5) uses GQA; understanding the grouped query structure helps diagnose what the TimeMixer must approximate.
  - **Quick check question:** How does grouping queries in attention affect the compression difficulty for the recurrent state?

## Architecture Onboarding

- **Component map:**
  Input -> RMSNorm -> [AttentionWrapper: Self-Attention || TimeMixer] -> Residual Add -> SwiGLU MLP -> Output

- **Critical path:**
  1. Initialize from Qwen 2.5 weights (keep RMSNorm, SwiGLU)
  2. Stage 1: Freeze MLP, train TimeMixer with L2 alignment loss (4B tokens, 18 hours on 8×H800)
  3. Stage 2: Unfreeze MLP, apply KL-Divergence distillation (40M tokens)
  4. Stage 3: SFT for context extension + DPO for alignment (770M tokens)

- **Design tradeoffs:**
  - Gate mechanism: Disabling gate improves performance but removes learned gating flexibility
  - MLP freezing: Freezing MLP during Stage 2 distillation from 32B teacher yields suboptimal results (capacity mismatch)
  - Context length: Training at 2048 limits long-context capability; Stage 3 extends this

- **Failure signatures:**
  - GSM8K score dropping from 82→40 indicates reasoning capability loss
  - Knowledge distillation from larger teacher (32B→7B) with frozen MLP causes 2-3 point MMLU degradation
  - If Stage 1 loss doesn't converge within first 500M tokens, check learning rate scaling

- **First 3 experiments:**
  1. **Baseline alignment test:** Train Stage 1 with 500M tokens at context length 2048; verify loss converges to ~0.5 normalized L2. If loss plateaus above 1.0, reduce TimeMixer hidden dimension or increase training tokens.
  2. **Ablation on gate mechanism:** Compare ARWKV-M (no gate, active MLP) vs ARWKV-G-M (with gate) on MMLU and GSM8K using the paper's exact hyperparameters. Expect ~4 point MMLU difference per Table 1.
  3. **Teacher size sensitivity:** Distill from Qwen 2.5-7B vs Qwen 2.5-32B with identical Stage 2 settings; if 32B teacher underperforms, unfreeze MLP or reduce distillation learning rate by 10×.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the three-stage distillation process be successfully generalized to complex architectures such as Mixture-of-Experts (MoE), multimodal systems, and hybrid models?
- **Basis in paper:** [explicit] Section 6 (Future Work) explicitly proposes generalizing the methodology to these specific paradigms to validate robustness.
- **Why unresolved:** The current study only validates the approach on dense Transformer-to-RNN distillation (Qwen 2.5 to ARWKV).
- **What evidence would resolve it:** Successful application and performance benchmarking of this distillation pipeline on a MoE or multimodal model.

### Open Question 2
- **Question:** Can the ARWKV architecture replicate the advanced reasoning capabilities of models like DeepSeek-R1 through the proposed Stage 3 post-training?
- **Basis in paper:** [explicit] Section 6 outlines the plan to implement Stage 3 post-training specifically to reproduce DeepSeek-R1 reasoning behaviors.
- **Why unresolved:** While the architecture is defined, the specific training run to achieve R1-level reasoning has not yet been completed or evaluated.
- **What evidence would resolve it:** Benchmark results on reasoning-heavy datasets (e.g., math or coding challenges) following the Stage 3 training implementation.

### Open Question 3
- **Question:** What is the precise mechanism causing performance degradation when distilling from a 32B teacher to a 7B student with frozen MLPs?
- **Basis in paper:** [inferred] Section 4 observes that distilling from 32B to 7B with frozen MLPs yields suboptimal results, hypothesizing a capacity mismatch, but does not confirm the cause.
- **Why unresolved:** The paper notes the failure mode but leaves the "architectural mismatch" as a suggestion rather than a proven conclusion.
- **What evidence would resolve it:** An ablation study varying student MLP capacity or unfreezing layers during the 32B-to-7B transfer to isolate the limiting factor.

## Limitations

- Complete absence of training hyperparameters (learning rates, batch sizes, optimizer choices) prevents independent verification of claimed 8-hour training time
- GSM8K score drop from 82.34 to 39.95 indicates fundamental limitation in capturing multi-step reasoning through token-level distillation
- No ablation studies for critical architectural decisions like RWKV-7 state size or context length impact
- Limited external validation with corpus neighbors focusing on unrelated attention mechanisms

## Confidence

**High confidence** in the core mechanism: RWKV-7 can replace transformer attention when trained with L2 alignment loss, as evidenced by the Stage 1 convergence results and the competitive MMLU score (62.41) matching Qwen 2.5's performance range.

**Medium confidence** in the efficiency claims: The 8-hour training time and minimal token requirements are plausible given the described architecture and hardware, but cannot be verified without the missing hyperparameters. The FP16 inference improvement is supported by the paper's observations but lacks external validation.

**Low confidence** in the reasoning capability preservation: The dramatic GSM8K score drop (82.34 → 39.95) suggests the token-level distillation approach fundamentally fails to capture multi-step reasoning, contradicting the paper's claim that RNN architectures can effectively capture transformer attention patterns for all task types.

## Next Checks

1. **Verify Stage 1 convergence behavior**: Train the TimeMixer alignment stage for 500M tokens and monitor the L2 alignment loss. The paper claims convergence to normalized L2 loss around 0.5; if loss plateaus above 1.0, the RWKV-7 state capacity is insufficient for the teacher's attention complexity and requires either larger state dimensions or longer training.

2. **Test reasoning capability under sequence-level distillation**: Repeat Stage 2 distillation using sequence-level KL-Divergence instead of word-level, keeping all other hyperparameters constant. Compare GSM8K scores to determine if the token-level approach is responsible for the reasoning degradation, or if the RNN architecture itself cannot capture the sequential dependencies required for mathematical reasoning.

3. **Validate teacher size sensitivity with MLP unfrozen**: Reproduce the distillation from Qwen 2.5-32B-Instruct but unfreeze the MLP during Stage 2. Compare MMLU and GSM8K scores against the paper's ARWKV-from32B results (which froze MLP). If unfrozen MLP yields performance closer to ARWKV-from7B, the paper's observation about capacity mismatch is correct and the frozen MLP approach is suboptimal.