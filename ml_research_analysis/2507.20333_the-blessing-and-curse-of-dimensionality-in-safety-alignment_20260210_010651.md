---
ver: rpa2
title: The Blessing and Curse of Dimensionality in Safety Alignment
arxiv_id: '2507.20333'
source_url: https://arxiv.org/abs/2507.20333
tags:
- safety
- linear
- refusal
- fjlt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between model dimensionality
  and safety alignment in large language models (LLMs). It shows that as models scale
  up, their high-dimensional internal representations develop strong linear structures
  for concepts like safety, which can be exploited by jailbreak attacks that manipulate
  these representations.
---

# The Blessing and Curse of Dimensionality in Safety Alignment

## Quick Facts
- arXiv ID: 2507.20333
- Source URL: https://arxiv.org/abs/2507.20333
- Authors: Rachel S. Y. Teo; Laziz U. Abdullaev; Tan M. Nguyen
- Reference count: 40
- Key outcome: Dimensionality reduction in LLM activation spaces can significantly improve safety alignment by making jailbreak attacks harder to execute

## Executive Summary
This paper investigates the relationship between model dimensionality and safety alignment in large language models (LLMs). It shows that as models scale up, their high-dimensional internal representations develop strong linear structures for concepts like safety, which can be exploited by jailbreak attacks that manipulate these representations. The authors theoretically demonstrate that reducing the dimensionality of these activation spaces makes it harder for such attacks to succeed. To address this vulnerability, they propose two novel fine-tuning methods: projecting hidden representations onto lower-dimensional subspaces using either a Fast Johnson-Lindenstrauss Transform (FJLT) or a linear autoencoder bottleneck. Empirical results show that these approaches significantly reduce susceptibility to jailbreak attacks while maintaining safety alignment, with the Bottleneck method also preserving better utility on various benchmarks compared to FJLT.

## Method Summary
The authors propose two dimensionality reduction techniques for improving safety alignment in LLMs. The first method uses Fast Johnson-Lindenstrauss Transform (FJLT) to project hidden representations onto lower-dimensional subspaces. The second method employs a linear autoencoder bottleneck approach, where representations are compressed and reconstructed through a bottleneck layer. Both methods are applied during fine-tuning, with the goal of maintaining semantic information while reducing the exploitable linear structure that jailbreak attacks target. The techniques are evaluated on safety alignment benchmarks, jailbreak attack resistance, and utility preservation across multiple model scales.

## Key Results
- Dimensionality reduction techniques significantly improve resistance to jailbreak attacks
- The Bottleneck method outperforms FJLT in preserving utility on various benchmarks
- Both methods maintain safety alignment while reducing model vulnerability
- High-dimensional representations develop exploitable linear structures for safety concepts

## Why This Works (Mechanism)
The mechanism exploits the fact that high-dimensional representations in LLMs develop strong linear structures that can be manipulated by adversarial attacks. By reducing dimensionality, these exploitable structures are disrupted while preserving the essential semantic information needed for task performance. The linear autoencoder bottleneck particularly excels because it learns to preserve the most relevant features for the task while discarding dimensions that can be exploited by attacks.

## Foundational Learning
- **High-dimensional geometry in ML**: Why needed - Understanding how concepts manifest in high-dimensional spaces; Quick check - Verify that safety concepts show linear separability in original representations
- **Adversarial attacks on LLMs**: Why needed - Understanding attack vectors and vulnerabilities; Quick check - Test attack success rates on baseline models
- **Dimensionality reduction techniques**: Why needed - Core mechanism for proposed solutions; Quick check - Compare different reduction methods on representation quality
- **Safety alignment in LLMs**: Why needed - Understanding the safety concepts being preserved; Quick check - Evaluate safety alignment on multiple benchmarks
- **Fine-tuning methodologies**: Why needed - How to effectively apply dimensionality reduction; Quick check - Compare fine-tuning with and without dimensionality reduction

## Architecture Onboarding

Component Map: Input -> FJLT/Bottleneck Layer -> Hidden Representations -> Output

Critical Path: The FJLT or Bottleneck layer must be applied consistently across all layers where safety-relevant representations are computed, with the Bottleneck requiring additional reconstruction steps.

Design Tradeoffs: FJLT offers computational efficiency but less flexibility, while the Bottleneck method learns task-specific dimensionality reduction but requires more training time and parameters.

Failure Signatures: Models may fail to maintain semantic coherence if too much dimensionality is reduced, or may remain vulnerable if reduction is insufficient.

First Experiments:
1. Baseline evaluation of attack success rates on unmodified models
2. Comparison of FJLT vs Bottleneck reduction ratios (50%, 75%, 90%)
3. Ablation study on layer-wise application of dimensionality reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes linear separability, which may not capture complex representation geometries
- Results may not generalize across different model architectures beyond tested LLMs
- The evaluation focuses on specific benchmarks that may not capture all safety-relevant scenarios

## Confidence
- Effectiveness of dimensionality reduction: Medium
- Theoretical guarantees: Medium
- Generalization across model scales: Low
- Long-term safety implications: Low

## Next Checks
1. Test the dimensionality reduction techniques on models of varying scales (1B, 7B, 70B parameters) to assess scaling properties and identify any threshold effects
2. Evaluate the robustness of the approach against adaptive attacks that specifically target the dimensionality reduction mechanisms
3. Conduct human evaluation studies to assess whether the reduced-dimensionality models maintain coherent and contextually appropriate safety behaviors across diverse real-world scenarios