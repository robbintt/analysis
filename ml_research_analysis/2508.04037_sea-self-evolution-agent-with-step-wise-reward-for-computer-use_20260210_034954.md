---
ver: rpa2
title: 'SEA: Self-Evolution Agent with Step-wise Reward for Computer Use'
arxiv_id: '2508.04037'
source_url: https://arxiv.org/abs/2508.04037
tags:
- agent
- training
- tasks
- task
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing efficient computer
  use agents that can autonomously operate computers to complete user tasks. Existing
  agents struggle with sparse rewards in long-horizon tasks, high computational costs,
  and lack of alignment between reasoning and action.
---

# SEA: Self-Evolution Agent with Step-wise Reward for Computer Use

## Quick Facts
- arXiv ID: 2508.04037
- Source URL: https://arxiv.org/abs/2508.04037
- Reference count: 40
- Primary result: SEA achieves 30.1% success rate on OSWorld benchmark using only 7B parameters, outperforming larger models

## Executive Summary
This paper addresses the challenge of developing efficient computer use agents that can autonomously operate computers to complete user tasks. Existing agents struggle with sparse rewards in long-horizon tasks, high computational costs, and lack of alignment between reasoning and action. The authors propose SEA (Self-Evolution Agent), which integrates three key innovations: a closed-loop pipeline for generating verifiable task trajectories, Trajectory Reasoning by Step-wise Reinforcement Learning (TR-SRL) to enable efficient learning through step-wise rewards, and Grounding-Based Generalization Enhancement that merges grounding and planning capabilities into a single model using model merging and Temporal Compressed Sensing Mechanism (TCSM).

SEA, with only 7B parameters, achieves state-of-the-art performance on the OSWorld benchmark, surpassing larger models (e.g., 72B parameters) with a task success rate of 30.1%. It also demonstrates strong grounding capabilities, outperforming larger models on the ScreenSpot-Pro and ScreenSpot-V2 datasets. The proposed methods enable scalable, efficient, and generalizable computer use agents that address key limitations in current approaches through innovative reward structures and model architecture enhancements.

## Method Summary
The paper introduces SEA (Self-Evolution Agent) as a comprehensive solution to the challenges faced by computer use agents. The core innovation lies in TR-SRL (Trajectory Reasoning by Step-wise Reinforcement Learning), which addresses the sparse reward problem in long-horizon tasks by providing step-wise rewards during the learning process. This allows the agent to learn more efficiently than traditional reinforcement learning approaches that only provide rewards at task completion. Additionally, SEA incorporates Grounding-Based Generalization Enhancement, which merges grounding and planning capabilities into a single model through model merging techniques and a Temporal Compressed Sensing Mechanism (TCSM). The system operates within a closed-loop pipeline that generates verifiable task trajectories, enabling continuous improvement and validation of the agent's performance.

## Key Results
- SEA achieves 30.1% success rate on OSWorld benchmark, surpassing larger models with 72B parameters
- Demonstrates strong grounding capabilities, outperforming larger models on ScreenSpot-Pro and ScreenSpot-V2 datasets
- Achieves state-of-the-art performance while maintaining computational efficiency with only 7B parameters

## Why This Works (Mechanism)
The paper's success stems from addressing three fundamental challenges in computer use agents: sparse rewards, computational inefficiency, and misalignment between reasoning and action. The step-wise reward mechanism (TR-SRL) provides immediate feedback during task execution, enabling more efficient learning compared to sparse reward systems that only provide feedback at task completion. The grounding enhancement through model merging and TCSM allows the agent to better understand and interact with computer interfaces by combining visual grounding with planning capabilities in a unified model. The closed-loop pipeline ensures that the agent can generate, verify, and refine task trajectories autonomously, creating a self-improving system that adapts to various computer use scenarios.

## Foundational Learning

**Reinforcement Learning with Sparse Rewards**: Traditional RL approaches struggle when rewards are only given at task completion. Step-wise rewards provide immediate feedback, accelerating learning convergence and improving sample efficiency.

**Model Merging in Multi-Modal Systems**: Combining different capabilities (grounding and planning) through model merging allows for more efficient parameter utilization and can create synergies between different types of reasoning.

**Temporal Compressed Sensing Mechanism**: TCSM enables the model to compress temporal information while preserving critical features, which is essential for understanding sequences of computer interactions and screen changes.

**Closed-Loop Trajectory Generation**: The ability to generate, verify, and refine trajectories autonomously creates a self-improving system that can adapt to various scenarios without requiring constant human intervention.

## Architecture Onboarding

**Component Map**: Vision Module → Grounding Enhancement → Planning Module → Action Execution → Feedback Loop → Reinforcement Learning → Parameter Updates

**Critical Path**: The critical execution path involves visual perception, grounding comprehension, action planning, and execution, with continuous feedback from the environment informing the learning process.

**Design Tradeoffs**: The paper prioritizes parameter efficiency over raw model size, demonstrating that 7B parameters can outperform 72B parameter models through architectural innovations rather than scale. This tradeoff enables faster inference and lower computational costs.

**Failure Signatures**: The 30.1% success rate indicates that approximately 70% of tasks fail, suggesting limitations in handling complex, multi-step reasoning or edge cases in computer interface interactions. The paper lacks detailed analysis of specific failure modes.

**First Experiments**:
1. Validate TR-SRL's effectiveness by comparing learning curves with and without step-wise rewards on simplified tasks
2. Test the grounding enhancement by evaluating performance on visual-only tasks before and after model merging
3. Assess the closed-loop pipeline's self-improvement capability by measuring performance gains over multiple training iterations

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions or areas for future work.

## Limitations
- The 30.1% success rate, while state-of-the-art, still represents a relatively low success rate for practical deployment
- Lack of detailed analysis of failure cases and whether performance degrades on more complex tasks beyond the benchmark
- Comparison focuses primarily on parameter count efficiency rather than wall-clock inference time or real-world deployment costs

## Confidence
- High confidence: The step-wise reward mechanism (TR-SRL) and its theoretical justification for addressing sparse rewards
- Medium confidence: The overall performance claims on OSWorld and grounding datasets
- Medium confidence: The effectiveness of model merging and TCSM for grounding enhancement

## Next Checks
1. Conduct detailed ablation studies to quantify the individual contribution of TR-SRL, grounding enhancement, and model merging to overall performance gains
2. Test SEA's performance on more complex, real-world computer use tasks beyond the OSWorld benchmark to assess practical deployment readiness
3. Analyze failure modes and error patterns to understand the remaining limitations and identify areas for improvement beyond the current 30.1% success rate