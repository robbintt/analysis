---
ver: rpa2
title: 'FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference'
arxiv_id: '2601.13143'
source_url: https://arxiv.org/abs/2601.13143
tags:
- pruning
- attention
- tokens
- token
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FastAV, the first token pruning framework
  designed for audio-visual large language models (AV-LLMs). The key insight is that
  attention rollout analysis reveals a pattern where token influence concentrates
  on earlier positions in the sequence after middle layers, forming anchor-like structures.
---

# FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference

## Quick Facts
- arXiv ID: 2601.13143
- Source URL: https://arxiv.org/abs/2601.13143
- Reference count: 37
- Key outcome: FastAV achieves over 40% FLOP reduction on AV-LLMs while maintaining or improving performance across three datasets

## Executive Summary
This paper introduces FastAV, the first token pruning framework designed for audio-visual large language models (AV-LLMs). The key insight is that attention rollout analysis reveals a pattern where token influence concentrates on earlier positions in the sequence after middle layers, forming anchor-like structures. FastAV leverages this by applying a two-stage pruning strategy: (1) global pruning at the middle layer to remove less influential later tokens guided by attention rollout, and (2) fine-grained pruning in subsequent layers using last-query token attention weights to progressively remove the least important remaining tokens. The method achieves over 40% reduction in FLOPs on both VideoLLaMA2 and video-SALMONN2 models while maintaining or even improving performance across three datasets (AVQA, MUSIC-AVQA, and AVHBench).

## Method Summary
FastAV is a two-stage, training-free token pruning framework for AV-LLMs. The first stage applies global pruning at the middle layer using attention rollout to identify and remove less influential later-position tokens. The second stage applies fine-grained pruning in subsequent layers using last-query token attention weights to progressively remove the least important remaining tokens. This approach achieves over 40% FLOP reduction while maintaining or improving accuracy. The method is compatible with FlashAttention and demonstrates that audio tokens can be reduced by more than 99% without degrading performance, highlighting potential inefficiencies in current audio encoding strategies.

## Key Results
- FastAV achieves over 40% reduction in FLOPs on both VideoLLaMA2 and video-SALMONN2 models
- Maintains or improves performance across three datasets (AVQA, MUSIC-AVQA, and AVHBench)
- Reduces audio tokens by more than 99% (e.g., from 1,496 to 10 in VideoLLaMA2) without degrading performance
- Best performance achieved with global pruning at middle layer (L/2) and fine pruning ratio of 20%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention rollout accumulates token influence across layers and reveals that earlier tokens become "anchors" after middle layers, enabling informed global pruning.
- Mechanism: Attention rollout aggregates attention matrices from layer 1 to layer L using R^l = Ã^l · Ã^(l-1) · ... · Ã^1, where Ã^l = αA^l + (1-α)I incorporates residual connections. This cumulative view shows which tokens consistently influence downstream representations, unlike raw attention weights which show no clear pattern.
- Core assumption: Tokens with higher accumulated rollout values are more important for model predictions; the concentration on early positions reflects genuine information importance rather than architectural artifact.
- Evidence anchors:
  - [abstract] "attention rollout analysis reveals a pattern where token influence concentrates on earlier positions in the sequence after middle layers, forming anchor-like structures"
  - [section 2.2] "In the first row, showing the attention rollout, we observe that attention gradually concentrates on the earliest tokens starting from the middle layer... In contrast, the second row, showing raw attention weights, reveals no clear pattern"
  - [corpus] Related work (STAR, TransPrune) uses single-stage attention-based pruning for LVLMs; FastAV extends to multimodal AV-LLMs with two-stage approach leveraging rollout specifically.

### Mechanism 2
- Claim: Global pruning at the middle layer removes broadly less influential later-position tokens while preserving anchor-like early tokens, reducing token count by 50-67% without harming downstream reasoning.
- Mechanism: At layer L/2, attention rollout R^(L/2) is computed. Tokens beyond a position threshold (e.g., position 750) are identified as less influential based on accumulated rollout values. For VideoLLaMA2, only first 10 audio tokens kept; for video-SALMONN2, first 4 frames retained. This leverages the observation that later tokens contribute less to accumulated attention flow.
- Core assumption: The anchor-like concentration pattern generalizes across inputs and tasks; the position-based threshold determined from 100 non-test samples transfers to held-out data.
- Evidence anchors:
  - [abstract] "global pruning at the middle layer to remove less influential later tokens guided by attention rollout"
  - [section 2.2] "For both models, we analyze 100 non-test samples and apply an attention rollout threshold at the middle layer to remove less influential video and audio tokens, typically those occurring beyond position 750"
  - [corpus] PLPHP and TopV use attention-based pruning for visual tokens in LVLMs; FastAV applies rollout-guided global pruning specifically to multimodal audio-visual contexts.

### Mechanism 3
- Claim: Fine pruning using last-query token attention weights progressively removes the least important remaining tokens without requiring full attention matrices, maintaining compatibility with FlashAttention.
- Mechanism: After global pruning, at each subsequent layer l, the last query token Q^l_last attends to remaining key tokens K^l. Importance scores s^l = mean_h(softmax(Q^l_last(K^l)^T)) rank tokens by their contribution to next-token prediction. Bottom P% (e.g., 20%) are removed iteratively.
- Core assumption: The last query token's attention pattern reliably indicates token importance for final answer generation; this surrogate does not require full attention maps.
- Evidence anchors:
  - [abstract] "fine-grained pruning in subsequent layers using last-query token attention weights to progressively remove the least important remaining tokens"
  - [section 2.2] "As in prior works [33, 34], the last query token analysis efficiently identifies important tokens, as it directly influences the next token's prediction without computing the full attention matrix"
  - [corpus] SkipGPT and related token skipping methods use query-based importance; FastAV adapts last-query attention for multimodal pruning with FlashAttention compatibility.

## Foundational Learning

- Concept: **Attention Rollout**
  - Why needed here: FastAV relies on attention rollout to identify anchor tokens; understanding how rollout differs from raw attention is essential for implementing and debugging the global pruning stage.
  - Quick check question: Given a 3-layer transformer with attention matrices A^1, A^2, A^3 (each n×n), write the rollout R^3 with residual weight α=0.5.

- Concept: **Token Pruning in Transformers**
  - Why needed here: FastAV is a token pruning method; understanding how pruning affects KV cache, attention complexity, and autoregressive generation is prerequisite for evaluating tradeoffs.
  - Quick check question: If pruning reduces tokens from n to n/2 at layer k of an L-layer transformer, what is the approximate FLOP reduction for attention in remaining layers?

- Concept: **Multimodal Token Fusion in AV-LLMs**
  - Why needed here: FastAV handles video, audio, and text tokens with different concatenation orders (VideoLLaMA2: video→audio→text; video-SALMONN2: interleaved frames); understanding fusion patterns is required for correct pruning implementation.
  - Quick check question: For VideoLLaMA2 with M visual tokens, U audio tokens, and E text tokens, what is the global pruning output if only first 10 audio tokens are kept?

## Architecture Onboarding

- Component map:
  Input tokenization: Visual encoder → M visual tokens; Audio encoder → U audio tokens; Text tokenizer → E text tokens → Concatenated sequence X = [X_vis, X_aud, X_lang] with K = M+U+E tokens
  LLM Decoder: 28-layer transformer (for VideoLLaMA2)
  Global Pruning Module: At layer L/2 (layer 14), computes attention rollout R^(L/2), applies position threshold, outputs reduced sequence X' = [X'_vis, X'_aud, X'_lang]
  Fine Pruning Module: At layers L/2+1 to L, computes last-query importance scores s^l, removes bottom P% tokens per layer
  Output: Autoregressive next-token prediction

- Critical path:
  1. Full forward pass through layers 1 to L/2 (no pruning)
  2. Compute attention rollout R^(L/2) using accumulated attention matrices with residual connections
  3. Apply global pruning: keep early-position tokens, discard later tokens per modality rules
  4. For each layer l in L/2+1 to L:
     - Compute last-query attention: s^l = mean_h(softmax(Q^l_last(K^l)^T))
     - Remove bottom P% tokens by score
  5. Continue autoregressive generation with pruned KV cache

- Design tradeoffs:
  - Pruning layer selection: Earlier layers → more FLOP reduction but higher risk of removing useful information; middle layer (L/2) balances hallucination and matching performance (Fig. 4)
  - Pruning ratio P: Higher P → more FLOP reduction; P=20% achieves best accuracy-FLOP tradeoff (Table 4)
  - Rollout residual weight α: Balances inter-token dependencies vs. self-representation; larger α highlights dependencies, smaller stabilizes rollout
  - Modality-specific thresholds: VideoLLaMA2 keeps first 10 audio tokens; video-SALMONN2 keeps first 4 frames—tuned per architecture

- Failure signatures:
  - Performance drop on AV matching task: May indicate over-aggressive global pruning removing cross-modal alignment tokens; increase position threshold or reduce P
  - Performance drop on AV hallucination task: May indicate pruning too early (layer < L/2); delay global pruning to middle layer
  - No FLOP reduction observed: Check if pruning masks are correctly applied to KV cache; verify token count reduction propagates through layers
  - Incompatibility with FlashAttention: Ensure fine pruning uses only last-query attention (not full attention map); verify implementation uses Q^l_last only

- First 3 experiments:
  1. **Attention rollout visualization**: Compute and visualize R^l at layers 4, 14, 24 on 10-20 samples; confirm early-token concentration pattern emerges at middle layer; if not, reconsider α or pruning layer.
  2. **Global pruning ablation**: Apply global pruning alone (P=0%) on AVHBench; compare random, low-attentive, and low-informative (rollout-based) strategies; verify rollout-based method achieves best accuracy (target: Table 2 results).
  3. **End-to-end FLOP and accuracy measurement**: Run FastAV with P=20% on VideoLLaMA2 across AVQA, MUSIC-AVQA, AVHBench; measure theoretical FLOPs, latency, memory, and accuracy; target ~40% FLOP reduction with maintained or improved accuracy (Table 1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why can audio tokens be reduced by over 99% (e.g., from 1,496 to 10) without degrading performance, and does this indicate fundamental inefficiency in current audio encoding strategies for AV-LLMs?
- Basis in paper: [explicit] The paper states "applying FastAV to both AV-LLMs drastically reduces audio tokens (e.g., from 1,496 to 10 in VideoLLaMA2) without degrading performance, highlighting their essential yet compact role."
- Why unresolved: The paper demonstrates this phenomenon empirically but does not investigate whether the audio encoder architecture or tokenization process itself is fundamentally over-producing tokens, nor whether more efficient audio representations could be designed upstream.
- What evidence would resolve it: Systematic analysis comparing different audio encoder designs and tokenization strategies, measuring the minimum audio tokens required for various audio-heavy tasks before performance degrades.

### Open Question 2
- Question: How well does the attention rollout pattern (concentration on earlier tokens after middle layers) generalize to other AV-LLM architectures with different token ordering or attention mechanisms?
- Basis in paper: [inferred] The paper evaluates only two representative AV-LLMs (VideoLLaMA2 and video-SALMONN2) with specific token arrangements. VideoLLaMA2 places "all video tokens precede the audio tokens" while video-SALMONN2 has "video and audio tokens interleaved at the frame level."
- Why unresolved: The anchor-like attention patterns may be architecture-dependent. Different AV-LLMs with alternative multimodal fusion strategies, different LLM backbones, or alternative attention mechanisms may exhibit different rollout patterns that require modified pruning strategies.
- What evidence would resolve it: Evaluation of FastAV on a broader range of AV-LLM architectures with varying token orderings, encoder designs, and LLM backbones to assess pattern consistency.

### Open Question 3
- Question: Would adaptive pruning thresholds based on input characteristics (e.g., video length, audio complexity, question type) outperform the fixed thresholds determined from 100 non-test samples?
- Basis in paper: [inferred] The global pruning threshold was determined by analyzing "100 non-test samples and apply an attention rollout threshold at the middle layer to remove less influential video and audio tokens, typically those occurring beyond position 750." The fine pruning ratio P=20% was also empirically fixed.
- Why unresolved: Fixed thresholds may be suboptimal for diverse input distributions. Long videos with complex audio may require different pruning strategies than short clips with simple audio. Complex reasoning questions may need more tokens than simple factual queries.
- What evidence would resolve it: Experiments comparing fixed vs. adaptive thresholds across diverse input types, measuring performance variations and identifying scenarios where fixed thresholds fail or succeed.

### Open Question 4
- Question: Can incorporating light fine-tuning or training-aware pruning yield additional efficiency gains beyond the training-free approach?
- Basis in paper: [inferred] The paper emphasizes that FastAV is a training-free method that "enables efficient computation without compromising performance" without additional training. However, the models were not explicitly trained to handle pruned inputs.
- Why unresolved: Models trained with pruning-aware objectives or fine-tuned after pruning may learn to better distribute critical information across tokens, potentially enabling more aggressive pruning ratios or improved performance retention.
- What evidence would resolve it: Comparative experiments with (1) models fine-tuned after pruning, (2) models trained with differentiable pruning objectives, and (3) the current training-free approach, measuring both efficiency and performance.

## Limitations

- Architectural Details: The paper lacks specific implementation details for attention rollout computation, particularly the residual weight α in the rollout equation. The exact criteria for determining global pruning thresholds from the 100 analyzed samples is not specified.
- FlashAttention Compatibility: While claimed, no implementation details are provided for extracting last-query attention weights when using the fused attention kernel, representing a practical barrier to reproduction.
- Multimodal Generalization: The method's effectiveness across diverse multimodal tasks and model architectures remains uncertain, as the paper focuses on only two specific AV-LLM architectures and three datasets.

## Confidence

- **High Confidence**: The general two-stage pruning framework and its core mechanisms (attention rollout for global pruning, last-query attention for fine pruning) are well-supported by experimental results showing 40%+ FLOP reduction with maintained or improved accuracy.
- **Medium Confidence**: The specific pruning ratios (P=20%) and position thresholds (position 750 for global pruning, first 10 audio tokens for VideoLLaMA2, first 4 frames for video-SALMONN2) are likely effective but may require tuning for different models or tasks.
- **Low Confidence**: The claim that FastAV can reduce audio tokens by more than 99% without degrading performance requires additional validation across diverse audio-centric tasks, as the paper's datasets may not fully stress-test this capability.

## Next Checks

1. **Attention Rollout Pattern Verification**: Implement the attention rollout computation with varying α values (0.3, 0.5, 0.7) and visualize R^l at layers 4, 14, and 24 on 20 samples from AVHBench. Confirm that early-token concentration emerges consistently at the middle layer (L/2) across different α values, and identify which α produces the clearest anchor-like pattern.

2. **Modality-Specific Pruning Threshold Sensitivity**: Systematically vary the position thresholds for global pruning (positions 500, 750, 1000) and the fine pruning ratio P (10%, 20%, 30%) on VideoLLaMA2. Measure FLOPs reduction and accuracy on AVQA and AVHBench to determine optimal hyperparameters and assess the method's sensitivity to these choices.

3. **Audio Token Pruning Robustness**: Design a targeted experiment using an audio-centric subset of AVHBench that emphasizes temporal audio reasoning (e.g., questions about sound sequences or audio events). Apply FastAV with the extreme audio pruning (first 10 audio tokens only) and measure performance degradation compared to the baseline, validating whether the 99% audio reduction claim holds for audio-critical tasks.