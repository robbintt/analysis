---
ver: rpa2
title: 'WILTing Trees: Interpreting the Distance Between MPNN Embeddings'
arxiv_id: '2505.24642'
source_url: https://arxiv.org/abs/2505.24642
tags:
- dwilt
- dmpnn
- mpnn
- distance
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the distance function learned by message
  passing neural networks (MPNNs) and finds that MPNN distances align strongly with
  task-relevant functional distances rather than structural distances, and this alignment
  is critical for predictive performance. The authors propose distilling MPNN embeddings
  into a weighted Weisfeiler Leman Labeling Tree (WILT) distance, which is efficiently
  computable and generalizes two existing graph kernels.
---

# WILTing Trees: Interpreting the Distance Between MPNN Embeddings

## Quick Facts
- arXiv ID: 2505.24642
- Source URL: https://arxiv.org/abs/2505.24642
- Authors: Masahiro Negishi; Thomas GÃ¤rtner; Pascal Welke
- Reference count: 40
- Key outcome: WILT distances approximate MPNN distances much better than existing graph distances (RMSE reductions of ~90% in some cases), and only a small subset of WL colors determine the relative position of graphs in MPNN embedding space.

## Executive Summary
This paper investigates how message passing neural networks (MPNNs) learn distance functions between graphs and finds that these distances align strongly with task-relevant functional distances rather than purely structural distances. The authors propose WILT (Weisfeiler Leman Labeling Tree), a framework that distills MPNN embeddings into an efficiently computable distance function based on weighted Weisfeiler Leman colorings. WILT generalizes two existing graph kernels and identifies specific Weisfeiler Leman subgraphs that significantly influence MPNN embedding distances. Experimental results demonstrate that WILT distances approximate MPNN distances much better than existing graph distances, with RMSE reductions of approximately 90% in some cases.

## Method Summary
The method involves distilling pre-trained MPNN embeddings into a weighted Weisfeiler Leman Labeling Tree (WILT) distance function. WILT is constructed by assigning trainable edge weights to a tree structure where nodes represent Weisfeiler Leman (WL) colorings at different iteration depths. The distance between two graphs is computed by finding their closest common ancestor in this tree and summing the weights along the path. This framework generalizes both the Weisfeiler Leman Optimal Assignment (WLOA) kernel and the WL subtree kernel. The edge weights are learned to minimize the difference between WILT distances and MPNN embedding distances using a contrastive loss function. The method also identifies which WL colorings (subgraphs) are most influential in determining MPNN distances through weight magnitude analysis.

## Key Results
- WILT distances approximate MPNN distances much better than existing graph distances, with RMSE reductions of approximately 90% in some cases
- Only a small subset of WL colors (subgraphs) determine the relative position of graphs in MPNN embedding space
- Influential subgraphs identified by WILT correspond to functionally important patterns in molecular data, demonstrating that MPNNs learn to focus on task-relevant structural features

## Why This Works (Mechanism)
The method works because MPNNs learn to embed graphs based on task-relevant structural features rather than purely structural similarity. WILT captures this by learning edge weights that prioritize certain Weisfeiler Leman colorings (subgraphs) over others, effectively identifying which structural patterns are most important for the specific task. The tree structure allows for efficient computation while maintaining the ability to capture complex relationships between different subgraph patterns.

## Foundational Learning
1. **Message Passing Neural Networks (MPNNs)**: Neural networks that aggregate information from neighboring nodes through multiple message passing steps to create graph embeddings.
   - Why needed: MPNNs are the target model whose distance function we want to interpret and approximate.
   - Quick check: Can you explain how information flows through multiple layers of a GNN?

2. **Weisfeiler Leman (WL) Test**: A graph isomorphism test that iteratively refines node colorings based on neighbor information.
   - Why needed: WL colorings provide a systematic way to identify and categorize subgraph patterns.
   - Quick check: How does the WL test distinguish between different graph structures?

3. **Graph Kernels**: Methods for computing similarity between graphs based on common substructures.
   - Why needed: WILT generalizes existing graph kernels, providing a connection to established methods.
   - Quick check: What's the difference between WLOA and WL subtree kernels?

## Architecture Onboarding

**Component Map**: Input Graphs -> WL Colorings (iterations) -> WILT Tree (with trainable edges) -> Distance Computation -> Loss Function (vs MPNN distances) -> Optimized Edge Weights

**Critical Path**: The critical computational path involves computing WL colorings for all graphs, constructing the WILT tree, computing pairwise distances using the learned edge weights, and optimizing these weights to minimize the distance to MPNN embeddings.

**Design Tradeoffs**: The method trades computational complexity for interpretability - WILT is more efficient than computing full MPNN distances but requires storing and computing WL colorings. The tree structure balances expressiveness with computational tractability.

**Failure Signatures**: Poor approximation of MPNN distances (high RMSE), failure to identify meaningful influential subgraphs, or sensitivity to hyperparameter choices (number of WL iterations, attention thresholds).

**First Experiments**:
1. Verify that WILT distances correlate strongly with MPNN distances on a simple molecular dataset
2. Compare WILT approximation quality against baseline graph distances (WLOA, WL subtree)
3. Perform ablation studies to identify which WL colors are most influential for a specific task

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do different GNN architectures and hyperparameters affect the edge weights learned by WILT?
- Basis in paper: One limitation of our study is that we only distilled two GNN architectures (GCN and GIN) with fixed hyperparameters to WILT. Thus, it remains to be seen how different architectures and hyperparameters affect the edge weights of WILT.
- Why unresolved: The experimental scope was limited to two architectures with fixed settings, leaving unexplored whether WILT's interpretability findings generalize across the broader GNN design space.
- What evidence would resolve it: Systematic experiments varying architectures (e.g., GAT, GraphSAGE), layer counts, embedding dimensions, and pooling strategies, then comparing resulting WILT weight distributions and identified important subgraphs.

### Open Question 2
- Question: Is higher-order WILT practically tractable given the scaling of trainable weights with the number of WL colors?
- Basis in paper: However, as the number of trainable WILT weights scales with the number of colors, the practical relevance of higher-order WILTs remains an open question.
- Why unresolved: Extending WILT to higher-order WL variants (e.g., 2-WL, 3-WL) is theoretically straightforward but may face computational or overfitting challenges due to parameter explosion.
- What evidence would resolve it: Benchmark experiments applying WILT to higher-order GNNs, measuring training time, memory requirements, and whether interpretability insights remain meaningful with larger color sets.

### Open Question 3
- Question: Can WILT be trained from scratch as a standalone, interpretable graph learning method with competitive performance?
- Basis in paper: Using WILT for a purpose other than understanding GNNs is also interesting. For example, by training WILT's edge parameters from scratch, we might be able to build a high-performance, interpretable graph learning method.
- Why unresolved: Current work uses WILT only for post-hoc interpretation by distilling pre-trained MPNNs; direct training for prediction remains unexplored.
- What evidence would resolve it: Comparative evaluation training WILT directly on benchmark tasks against standard GNNs, reporting both predictive accuracy and interpretability metrics (e.g., alignment with known functional substructures).

## Limitations
- The analysis relies heavily on synthetic graph data with controlled properties, which may not fully capture the complexity of real-world molecular datasets
- The computational complexity analysis for very large graphs is not fully explored, potentially limiting practical applicability
- The method's sensitivity to hyperparameter choices (such as the number of WL iterations and attention thresholds) is not thoroughly examined

## Confidence
**High Confidence**: The core finding that MPNN distances align more with functional than structural distances is well-supported by experimental evidence. The demonstration that WILT distances approximate MPNN distances significantly better than existing methods is robust across multiple datasets.

**Medium Confidence**: The claim that only a small subset of WL colors determines relative positions in MPNN embedding space is supported by ablation studies, but the generalizability of specific subgraph importance across different molecular tasks needs further validation. The assertion that influential subgraphs correspond to functionally important patterns is suggestive but not definitively proven.

**Low Confidence**: The exact mechanisms by which MPNNs prioritize task-relevant structural features remain somewhat speculative. While the framework identifies influential subgraphs, the causal relationship between these patterns and predictive performance requires more rigorous investigation.

## Next Checks
1. **Cross-domain generalization**: Apply WILT to non-molecular graph datasets (e.g., social networks, citation networks) to test whether the distance alignment patterns hold across different graph types and tasks.

2. **Robustness analysis**: Systematically vary WL iteration counts and attention thresholds to determine the sensitivity of WILT distances to these hyperparameters and identify optimal settings for different graph classes.

3. **Causal inference experiment**: Design an intervention study where specific subgraphs identified as influential are artificially modified or removed, then measure the impact on MPNN predictive performance to establish causal relationships.