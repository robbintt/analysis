---
ver: rpa2
title: 'DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor
  Generation'
arxiv_id: '2507.11875'
source_url: https://arxiv.org/abs/2507.11875
tags:
- reward
- distractor
- learning
- distractors
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DualReward, a reinforcement learning framework
  for cloze test distractor generation that differentiates between human-created gold
  standard distractors and model-generated candidates through a dual reward structure
  with adaptive scaling. The framework dynamically adjusts reward signal intensity
  based on model performance and confidence.
---

# DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation

## Quick Facts
- arXiv ID: 2507.11875
- Source URL: https://arxiv.org/abs/2507.11875
- Authors: Tianyou Huang; Xinglu Chen; Jingshen Zhang; Xinying Qiu; Ruiying Niu
- Reference count: 40
- Primary result: DualReward achieves state-of-the-art P@1 scores of 28.87% on CLOTH-F and 37.84% on MCQ datasets through dual reward structure with adaptive scaling

## Executive Summary
This paper introduces DualReward, a reinforcement learning framework that improves cloze test distractor generation by differentiating between human-created gold standard distractors and model-generated candidates through a dual reward structure. The framework dynamically adjusts reward signal intensity based on model performance and confidence, treating gold distractors as ground truth while discounting generated candidates according to their confidence scores. Experimental results demonstrate substantial improvements over existing methods, particularly on the diverse, cross-domain MCQ dataset where DualReward achieves 3.48-3.86% higher P@1 scores compared to uniform reward baselines.

## Method Summary
DualReward employs a candidate generating-and-ranking (CGR) approach where BERT-based CSG generates 7 candidates per instance, combined with 3 gold distractors to form a 10-candidate pool. A T5-base policy model selects the best distractor using reinforcement learning with a dual reward structure: gold distractors receive reward_scale (0.1-0.2) while generated candidates receive 0.9×reward_scale×confidence_score. The reward_scale dynamically adjusts via a sigmoid function of training loss to prevent gradient saturation. For MCQ datasets, RAP pretraining on Sciq-all is required. The system optimizes a policy gradient objective to directly maximize ranking metrics like P@1 and NDCG@3.

## Key Results
- CLOTH-F dataset: P@1=28.87%, outperforming previous methods by 3.48%
- MCQ dataset: P@1=37.84% with RAP pretraining, showing 3.86% improvement over uniform reward baseline
- NDCG@3 improvements: 14.23% on MCQ dataset, demonstrating superior ranking capability
- Adaptive scaling effectiveness: Maintains stable gradients throughout training while preventing reward collapse

## Why This Works (Mechanism)

### Mechanism 1: Differential Reward Hierarchy
The system applies a dual reward structure where human-created distractors receive a fixed positive reward (reward_gold = 1 × scale) while model-generated candidates receive a discounted reward (reward_gen = 0.9 × scale × confidence_score). This hierarchy ensures the policy learns primarily from reliable pedagogical examples while using generated candidates for exploration, scaled by the model's own uncertainty. The 0.9 discount factor penalizes potential noise without discarding valid novel candidates.

### Mechanism 2: Loss-Adaptive Reward Scaling
The framework uses a sigmoid function to modulate reward_scale based on average training loss. When the model is struggling (high loss), the scale shifts toward max_scale (0.2) to provide stronger reinforcement signals. As loss decreases below threshold, scale transitions to base_scale (0.1) to prevent large, destabilizing policy updates. This creates an automated curriculum that adjusts gradient strength based on learning difficulty.

### Mechanism 3: Reinforced Candidate Re-ranking
Instead of purely predicting the "correct" distractor via cross-entropy, the model (Policy π_θ) optimizes the expected discounted return. It selects an action (choosing a distractor from the candidate pool) to maximize the objective L_RL = -E[reward(s, a) · log π_θ(a|s)]. This directly optimizes ranking metrics rather than classification accuracy, allowing the system to improve MRR and NDCG scores through policy optimization.

## Foundational Learning

- **Concept: Policy Gradient (RL)** - Understanding how log π_θ(a|s) updates model weights based on reward signals is required to debug the training loop. Quick check: How does the gradient update differ when the reward is 0.1 vs. 0.2?

- **Concept: Transformer Confidence Calibration** - The dual reward mechanism relies on the model's confidence_score (softmax probability) to discount generated candidates. Quick check: If the model is overconfident (99% probability on a wrong distractor), how does the DualReward formula handle it?

- **Concept: Candidate Generating-and-Ranking (CGR)** - DualReward operates on a pre-generated pool of candidates. You must understand that the "Action" is selecting from this pool, not generating tokens one-by-one. Quick check: Does the DualReward framework train the BERT generator, or the T5 selector?

## Architecture Onboarding

- **Component map:** Input (Cloze sentence + Answer) → Candidate Generator (BERT CSG, frozen) → Candidate Pool (7 gen + 3 gold) → Policy Model (T5-base) → Reward Engine (Dual Reward + Adaptive Scaling) → Optimizer (Policy Gradient)

- **Critical path:** The correct implementation of the reward_scale sigmoid function. If threshold (1.0) and alpha (5.0) are not tuned to the specific loss range, the adaptive scaling will flatten (no signal) or saturate (unstable training).

- **Design tradeoffs:** Candidate Pool Size tested 13 vs 10, chose 10 for balance between recall and computation. Pretraining requirement differs: MCQ needs RAP pretraining on Sciq-all, CLOTH-F does not.

- **Failure signatures:** Reward Collapse if avg_loss drops instantly, causing reward_scale to drop to base (0.1) with weak gradients. Domain Mismatch generating semantically too close distractors like "steel" for "metal" indicates semantic distance penalty is missing.

- **First 3 experiments:**
  1. Overfit Check: Run T5-base baseline on CLOTH-F without RL to verify supervised loss magnitude (~1.0) is compatible with adaptive threshold.
  2. Ablation Reproduction: Implement Uniform-Reward variant (remove 0.9 discount) to confirm 4% drop on MCQ datasets.
  3. Visualization: Log reward_scale vs step to ensure curve follows intended sigmoid shape rather than oscillating.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does DualReward improve actual student learning outcomes compared to supervised baselines? Current metrics measure similarity to human references, not psychological validity or difficulty level for real students. What's needed: User study measuring test-taker performance and discrimination indices.

- **Open Question 2:** Can the adaptive reward scaling mechanism transfer effectively to multilingual cloze test datasets? The scaling parameters were tuned on English datasets; cross-lingual transfer may require different loss thresholds due to tokenization variations. What's needed: Experimental results on non-English cloze benchmarks.

- **Open Question 3:** How can the framework be modified to prevent generation of plausible but factually incorrect distractors? Current rewards rely on confidence and similarity to references, lacking explicit factual consistency checking. What's needed: Integration of knowledge graph or factual consistency checker into reward function.

- **Open Question 4:** Is the dual reward structure applicable to open-ended educational assessment formats beyond multiple-choice cloze tests? The framework is designed for candidate ranking, while open-ended generation lacks discrete candidate sets. What's needed: Adaptation to free-text generation where confidence comes from generative model's probability distribution.

## Limitations
- The framework is evaluated only on English cloze tests, limiting generalizability to other languages or assessment formats.
- The dual reward mechanism does not explicitly prevent generation of semantically similar distractors that are factually incorrect.
- The paper does not provide ablation studies isolating the individual contributions of the dual reward and adaptive scaling components.

## Confidence
- **High Confidence**: Differential reward hierarchy mechanism and implementation details are clearly specified with supporting evidence from Table 4.
- **Medium Confidence**: Experimental results show clear improvements, but lack of ablation studies limits attribution of performance gains to specific mechanisms.
- **Low Confidence**: No specification for handling cases where all generated candidates are semantically incorrect or too close to correct answer.

## Next Checks
1. Implement ablation study with Uniform-Reward and Fixed-Scale variants to quantify individual mechanism contributions to performance improvements.
2. Systematically evaluate BERT confidence score calibration across multiple domains to verify correlation with actual distractor quality.
3. Test framework's performance on out-of-distribution domains beyond the cross-domain MCQ dataset to evaluate adaptive scaling robustness.