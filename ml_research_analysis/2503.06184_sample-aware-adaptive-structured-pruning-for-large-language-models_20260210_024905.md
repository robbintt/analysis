---
ver: rpa2
title: Sample-aware Adaptive Structured Pruning for Large Language Models
arxiv_id: '2503.06184'
source_url: https://arxiv.org/abs/2503.06184
tags:
- pruning
- adapruner
- data
- calibration
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of structured pruning for large
  language models (LLMs), focusing on the limitations of randomly selected calibration
  data and fixed importance estimation metrics. The proposed method, AdaPruner, introduces
  a sample-aware adaptive structured pruning framework that optimizes both calibration
  data and importance estimation metrics simultaneously.
---

# Sample-aware Adaptive Structured Pruning for Large Language Models

## Quick Facts
- **arXiv ID**: 2503.06184
- **Source URL**: https://arxiv.org/abs/2503.06184
- **Reference count**: 29
- **Primary result**: AdaPruner achieves 1.37% better average performance than LLM-Pruner while maintaining 97% of unpruned model performance at 20% pruning ratio.

## Executive Summary
This paper addresses the challenge of structured pruning for large language models by introducing AdaPruner, a sample-aware adaptive framework that optimizes both calibration data and importance estimation metrics simultaneously. Unlike existing methods that rely on randomly selected calibration data and fixed importance metrics, AdaPruner constructs a solution space containing both calibration data and metric subspaces, then uses Bayesian optimization to search for the optimal configuration. The method demonstrates superior performance across different pruning ratios and model families, particularly showing that the pruned model retains 97% of the original performance at 20% pruning ratio.

## Method Summary
AdaPruner addresses structured pruning limitations by jointly optimizing calibration data selection and importance estimation metrics through Bayesian optimization. The framework constructs a solution space containing calibration data from BookCorpus and importance metrics based on Taylor expansion (combining coarse-grained weight vector and fine-grained weight element information). TPE (Tree-Structured Parzen Estimator) is used to maximize expected improvement of pruning results. After optimization, the model is pruned and fine-tuned with LoRA on Stanford Alpaca data. The method specifically targets dependency-aware structured pruning for LLMs, removing groups of parameters while maintaining tensor dimension compatibility.

## Key Results
- AdaPruner outperforms existing structured pruning methods on a family of LLMs with varying pruning ratios
- Achieves 1.37% superior average performance over LLM-Pruner
- At 20% pruning ratio, maintains 97% of the performance of the unpruned model
- Demonstrates effectiveness across different model families including LLaMA and Vicuna

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Calibration Data Selection
Optimizing calibration data selection via Bayesian Optimization yields higher-quality gradient information than random selection, leading to more accurate importance scoring. The method searches a subspace of BookCorpus and evaluates candidate sample sets by calculating perplexity of the unfine-tuned pruned model on WikiText2/PTB. This adaptive selection improves gradient information quality, as evidenced by a 0.71% performance drop when using random data instead of adaptive data.

### Mechanism 2: Hybrid Importance Estimation
A weighted combination of coarse-grained (weight vector) and fine-grained (weight element) Taylor expansions captures structural importance more robustly than single-metric approaches. The framework defines a metric subspace balancing first-order gradients and Fisher-approximated second-order information, with BO tuning equilibrium coefficients to find optimal weighting for specific models and pruning ratios.

### Mechanism 3: Joint Solution Space Optimization
Jointly optimizing calibration data and importance metrics helps escape local optima that occur when optimizing these components independently. The method constructs a unified solution space containing both data and metric parameters, using TPE to maximize Expected Improvement. This allows finding compatible pairs of data and metrics where specific data might require specific metric sensitivity.

## Foundational Learning

- **Concept: Structured Pruning Dependencies**
  - **Why needed here**: AdaPruner prunes groups of parameters (structures). Understanding that removing one component forces removal of dependent components to maintain tensor dimensions is crucial.
  - **Quick check**: If you prune a specific attention head's weight matrix, must you also prune the associated LayerNorm parameters if they exist? (Check dependency grouping in Section "Structured Pruning for LLMs").

- **Concept: Taylor Expansion for Importance**
  - **Why needed here**: The paper relies on approximating the change in loss when weights are removed, requiring understanding of first-order (gradient) and second-order (Hessian/Fisher) approximations.
  - **Quick check**: Why does the paper approximate the Hessian diagonal using the Fisher Information Matrix rather than computing the full Hessian? (Check Eq. 4).

- **Concept: Tree-Structured Parzen Estimator (TPE)**
  - **Why needed here**: TPE is the optimization engine that models p(x|y) to prioritize promising hyper-parameter regions rather than using grid search.
  - **Quick check**: In TPE, how does the algorithm decide which candidate configuration to try next after observing previous losses? (Check Eq. 11 and the ratio l(x)/g(x)).

## Architecture Onboarding

- **Component map**: Solution Space Generator -> Evaluator (Inner Loop) -> BO-TPE Optimizer -> Finalizer
- **Critical path**: The Evaluator is the bottleneck, requiring a full pruning pass and forward propagation on the validation set for every BO iteration.
- **Design tradeoffs**: Search Cost vs. Quality (more iterations yield better metrics but increase search time); Proxy vs. Target (using WikiText2 PPL as fast proxy may not perfectly align with zero-shot reasoning accuracy).
- **Failure signatures**: Stagnation (BO converges early suggesting small search space or no gradient signal); Metric Explosion (alignment factors cause numerical overflow); Catastrophic Forgetting (pruned model outputs incoherent text).
- **First 3 experiments**: 1) Sanity Check comparing random vs. optimized configurations; 2) Ablation isolating metric subspace optimization; 3) Generalization test applying optimal configuration across different model architectures.

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical evaluation relies heavily on proxy metrics (WikiText2/PTB perplexity) rather than direct task performance, introducing potential misalignment between optimization objective and actual downstream effectiveness.
- The search space, while adaptive, is constrained to BookCorpus-derived calibration data and specific Taylor expansion formulations, which may not generalize to all LLM architectures or domains.
- The paper does not thoroughly address computational overhead during the BO search phase, which could limit practical applicability for very large models.

## Confidence
- **High Confidence**: The core mechanism of combining coarse and fine-grained Taylor importance metrics shows consistent performance gains across ablation studies (Table 2).
- **Medium Confidence**: The effectiveness of joint optimization over independent calibration and metric tuning is supported by comparative results but could benefit from deeper theoretical analysis.
- **Medium Confidence**: Transferability claims across different pruning ratios and model families are demonstrated empirically but lack systematic investigation of boundary conditions.

## Next Checks
1. **Cross-Architecture Transfer**: Apply the optimal (D, I) configuration from LLaMA-7B to an entirely different architecture (e.g., OPT or BLOOM) to test generalizability beyond model families.
2. **Long-Term Stability**: Evaluate model performance after extended fine-tuning periods (beyond 2 epochs) to determine if pruning-induced capacity reduction manifests as catastrophic forgetting over time.
3. **Search Space Sensitivity**: Systematically vary the size of the calibration data subspace and metric parameter ranges to identify the minimum viable search space that maintains performance, establishing computational efficiency bounds.