---
ver: rpa2
title: 'Filling in the Blanks: Applying Data Imputation in incomplete Water Metering
  Data'
arxiv_id: '2506.08882'
source_url: https://arxiv.org/abs/2506.08882
tags:
- data
- water
- missing
- imputation
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates data imputation methods to handle missing
  data from smart water meters in real-world IoT deployments. The authors preprocess
  time series data from 22 buildings, split it into training and validation sets,
  and compare multiple imputation approaches including kNN, MissForest, Transformers,
  TimesNet, USGAN, MRNN, and SAITS.
---

# Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data

## Quick Facts
- arXiv ID: 2506.08882
- Source URL: https://arxiv.org/abs/2506.08882
- Reference count: 16
- Primary result: Common Transformer model achieves lowest MAE (0.13) for imputing missing water consumption data

## Executive Summary
This paper evaluates data imputation methods to handle missing data from smart water meters in real-world IoT deployments. The authors preprocess time series data from 22 buildings, split it into training and validation sets, and compare multiple imputation approaches including kNN, MissForest, Transformers, TimesNet, USGAN, MRNN, and SAITS. They test both building-specific models and a common model trained on aggregated data. Results show that dedicated Transformer and kNN models achieve the lowest mean absolute error (MAE) per building, with MAEs ranging from 0.13 to 0.31. Common models improve overall accuracy, with the best-performing kNN achieving an MAE of 0.16. The common Transformer also performs well (MAE 0.13). The work demonstrates that imputation can effectively restore missing consumption data, supporting applications like leak detection and predictive maintenance in water management systems.

## Method Summary
The study uses hourly aggregated water consumption data from 22 university buildings, with missing data ranging from 21% to 79% per building. Data is split into 24-hour vectors to leverage diurnal patterns. The authors compare dedicated models (trained per building) with a common model (trained on aggregated data). Multiple imputation algorithms are evaluated including kNN (k=3), MissForest, SAITS, Transformer, TimesNet, USGAN, and MRNN. All neural networks use Adam optimizer with learning rate 0.001 for 100 epochs. The common model approach involves training on data from all buildings and applying the same model to each building's missing data. Evaluation uses artificial masking of 20% of validation data with one missing value per 24-hour vector, measured by MAE.

## Key Results
- Common kNN achieves MAE of 0.16, significantly better than dedicated kNN (MAE 0.31)
- Common Transformer achieves the best overall MAE of 0.13
- Dedicated Transformer achieves MAE of 0.30 per building
- MRNN consistently underperforms across all settings
- Building 8 with 79% missing data shows poor performance for all models (MAE >1.5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 24-hour vector encoding captures periodic consumption patterns, enabling more accurate imputation than treating data points independently.
- Mechanism: Data is segmented into 24-hour vectors before imputation. This leverages the "periodicity" of water consumption—higher during working hours, negligible at night—allowing models to learn temporal structure rather than treating each hour as isolated.
- Core assumption: Water consumption follows diurnal patterns that are consistent enough across days to inform missing value estimation.
- Evidence anchors:
  - [Section III-B]: "we split the data available in 24-hour-long vectors to benefit from the measurements' periodicity, as higher consumption is typically observed during working hours, while during nighttime the water consumed is negligible."
  - [Section IV-A]: "This 24-hour interval is used as a data entry in all further operations, and our imputation will try to fill in missing data in these vectors."
  - [corpus]: Weak direct evidence; neighbor papers focus on water quality rather than consumption patterns.
- Break condition: Buildings with highly irregular or event-driven consumption (e.g., hospital emergency usage) may violate diurnal assumptions, degrading imputation accuracy.

### Mechanism 2
- Claim: Common (aggregated) models outperform dedicated per-building models by exposing the learning algorithm to a broader distribution of consumption profiles.
- Mechanism: Training a single model on merged data from all 22 buildings allows the model to generalize across diverse consumption patterns. When a specific building exhibits "out-of-profile" irregular patterns, the common model can leverage learned patterns from other buildings to fill gaps.
- Core assumption: Buildings share some underlying consumption structure that transfers across sites, even if individual profiles differ.
- Evidence anchors:
  - [Section V-C, Table II]: Common kNN achieves MAE 0.16 vs. dedicated kNN MAE 0.31; common Transformer achieves MAE 0.13 vs. dedicated MAE 0.30.
  - [Section V-C]: "increasing the dataset helped to generate better estimates in total... out-of-profile consumption patterns that may exist in each building that are better covered in the common model using data from other buildings."
  - [corpus]: No direct comparative evidence on common vs. dedicated models in neighbor papers.
- Break condition: Buildings with highly idiosyncratic consumption (e.g., specialized research facilities) may not benefit from shared training; dedicated models could be superior in such edge cases.

### Mechanism 3
- Claim: Transformer-based architectures capture complex temporal dependencies via self-attention, enabling competitive imputation with minimal manual feature engineering.
- Mechanism: The Transformer model (6 layers, 256-dimension, 4 attention heads) applies self-attention across the 24-hour vector, learning which time steps are most relevant for imputing each missing value. This allows the model to weight context dynamically rather than relying on fixed temporal windows.
- Core assumption: The attention mechanism can identify meaningful cross-temporal relationships in consumption data without explicit domain encoding.
- Evidence anchors:
  - [Section IV-B]: "It deduces missing values by delving into the interplay and contextual nuances among features, harnessing the entire dataset's feature relationships through a mechanism that captures complex dependencies."
  - [Section V-C, Table II]: Transformer achieves best-in-class common model MAE of 0.13.
  - [corpus]: SAITS (self-attention imputation) is mentioned in related work [10] as showing "superior performance" on multivariate time series imputation.
- Break condition: With very high missingness rates (>70%), attention mechanisms may struggle due to insufficient context; Building 8 (79% missing) showed poor performance across all models.

## Foundational Learning

- Concept: Time Series Imputation vs. Interpolation
  - Why needed here: Interpolation assumes smooth continuity between adjacent points; imputation methods like kNN and Transformers can leverage non-adjacent patterns and cross-variable relationships. This distinction is critical for choosing the right approach for IoT sensor gaps.
  - Quick check question: If a sensor drops 6 consecutive hours of data during a typical demand spike, would linear interpolation over or underestimate the missing values?

- Concept: Train/Validation Splitting with Artificial Masking
  - Why needed here: The paper creates validation data by artificially masking 20% of known values, enabling quantitative evaluation against ground truth. This is standard practice when natural missingness patterns are uncontrolled.
  - Quick check question: Why is it necessary to mask already-complete data points rather than using naturally missing points for validation?

- Concept: Mean Absolute Error (MAE) for Imputation Quality
  - Why needed here: MAE provides an interpretable metric (average absolute deviation in consumption units) that is robust to outliers compared to MSE. Understanding this metric is essential for interpreting Table II results.
  - Quick check question: If Building A has MAE 0.13 and Building B has MAE 0.31, what factors beyond model quality might explain this difference?

## Architecture Onboarding

- Component map: Raw meter readings -> Hourly aggregation -> NaN marking -> 24-hour vector segmentation -> Train/validation split -> Model training -> Imputation inference

- Critical path:
  1. Verify data alignment across buildings (different meter reporting intervals require hourly normalization)
  2. Ensure train/validation split preserves temporal order (avoid leakage from future to past)
  3. Monitor for buildings with extreme missingness (>50%)—consider exclusion or separate handling

- Design tradeoffs:
  - **Dedicated vs. Common Models**: Dedicated models capture building-specific patterns but suffer with sparse data; common models generalize better but may miss site-specific nuances.
  - **kNN vs. Transformer**: kNN (k=3) is simple, interpretable, and fast; Transformer requires more compute and tuning but achieves slightly lower MAE in common model setting.
  - **24-hour vs. longer windows**: 24-hour captures diurnal patterns but may miss weekly seasonality; longer windows increase computational cost and data requirements.

- Failure signatures:
  - Building 8 (79% missing data) shows MAE >1.5 for all models—insufficient training data is the root cause.
  - MRNN consistently underperforms (MAE 0.62 dedicated, 0.30 common)—suggests hyperparameter mismatch or architectural unsuitability for this data.
  - Large gaps (multiple consecutive hours) are harder to impute than scattered single-point gaps.

- First 3 experiments:
  1. **Baseline replication**: Implement kNN (k=3) on a single building's data using 80/20 train/validation split with artificial masking. Verify MAE is in the 0.2–0.4 range.
  2. **Common model ablation**: Train a common Transformer on 5 buildings with diverse missingness rates. Compare per-building MAE against dedicated models to quantify transfer benefit.
  3. **Gap size sensitivity**: Artificially mask 1, 3, and 6 consecutive hours in validation data. Measure how MAE degrades with gap size for kNN vs. Transformer to characterize robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed imputation system perform in real-time deployment scenarios, and what computational overhead does it impose on edge-based metering infrastructure?
- Basis in paper: [explicit] "Next steps in our path include the use of the proposed system in the metering environment, as a real-time application that can predict and fill blanks, available to each customer, to collect feedback on the quality and accuracy of the predictions."
- Why unresolved: The current study evaluates offline batch imputation only; real-time latency, resource constraints on edge devices, and streaming data handling remain untested.
- What evidence would resolve it: Deployment on actual metering hardware with latency measurements, memory/CPU utilization metrics, and user feedback on prediction quality under operational conditions.

### Open Question 2
- Question: How does imputation accuracy degrade when multiple consecutive hours of data are missing, compared to the single-point gaps tested in this study?
- Basis in paper: [inferred] "For this evaluation, we focus on imputing a single blank value in each vector but the process can be extended to estimate more."
- Why unresolved: Real-world failures may cause extended outages (hours or days), yet the evaluation only tested single-value imputation within 24-hour vectors.
- What evidence would resolve it: Systematic experiments varying gap lengths (2, 4, 8, 24+ consecutive missing hours) across all models, measuring MAE degradation patterns.

### Open Question 3
- Question: What is the minimum data availability threshold required for each imputation method to maintain acceptable accuracy, and do simpler methods outperform neural approaches at extreme sparsity?
- Basis in paper: [inferred] The paper notes that "in building 8 all algorithms perform badly" due to 79% missing data, but does not systematically identify failure thresholds or method crossover points.
- Why unresolved: The study identifies poor performance at high missingness but lacks controlled experiments to determine when each method becomes unreliable.
- What evidence would resolve it: Experiments with synthetically varied missing rates (20-80%) across buildings, identifying per-method accuracy cliffs and potential regime switches between kNN and neural methods.

### Open Question 4
- Question: Does imputed data preserve sufficient fidelity for downstream tasks like leak detection and predictive maintenance, or do imputation artifacts distort anomaly patterns?
- Basis in paper: [inferred] The paper claims to "provide solutions in applications like leak detection and predictive maintenance scheduling" but evaluates only MAE of imputed values, not downstream task performance.
- Why unresolved: Low imputation MAE does not guarantee that temporal patterns critical for anomaly detection or maintenance prediction are preserved.
- What evidence would resolve it: End-to-end evaluation running leak detection and maintenance algorithms on complete vs. imputed datasets, comparing detection rates, false positives, and prediction accuracy.

## Limitations

- The paper lacks direct comparisons to interpolation baselines (linear, spline), leaving open the question of whether complex imputation models outperform simpler approaches for this data type.
- No cross-validation or robustness testing against different missingness patterns—results are specific to the 20% artificial masking strategy.
- MRNN consistently underperforms, suggesting potential architectural or hyperparameter mismatches, but the paper does not explore these failure modes in depth.

## Confidence

- **High Confidence**: The comparative results showing common models outperforming dedicated models (MAE 0.16 vs. 0.31 for kNN) are internally consistent and methodologically sound.
- **Medium Confidence**: Transformer achieving best MAE (0.13) is credible given attention mechanisms' known effectiveness for time series, but lacks direct comparison to recent transformer variants.
- **Low Confidence**: The claim that 24-hour vector encoding is superior to other temporal granularities is supported by design choice but not empirically validated against alternatives.

## Next Checks

1. Implement and compare linear interpolation baseline against kNN and Transformer to establish whether complex imputation adds value beyond simple methods.
2. Vary the artificial masking rate (10%, 20%, 30%, 50%) and measure how each model's MAE degrades to understand robustness to missingness levels.
3. Test the common model approach on a subset of buildings with high missingness (>50%) to determine if transfer learning can compensate for sparse local data.