---
ver: rpa2
title: 'Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces
  and Trustworthy Human-AI Collaboration'
arxiv_id: '2509.15959'
source_url: https://arxiv.org/abs/2509.15959
tags:
- autonomous
- maritime
- https
- human
- ships
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The review synthesizes 100 studies to clarify how transparency\
  \ and explainability can make Maritime Autonomous Surface Ships (MASS) safer and\
  \ more usable. It maps the Guidance-Navigation-Control stack to shore-based operations\
  \ and identifies where Human-UCAs concentrate\u2014especially in RSM-RCM handovers\
  \ and emergency loops."
---

# Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration

## Quick Facts
- arXiv ID: 2509.15959
- Source URL: https://arxiv.org/abs/2509.15959
- Authors: Zhuoyue Zhang; Haitong Xu
- Reference count: 0
- Primary result: Synthesizes 100 studies mapping MASS to Guidance-Navigation-Control stack, identifying where human-AI collaboration concentrates and proposing adaptive transparency interfaces to improve trust calibration and situation awareness.

## Executive Summary
This review synthesizes 100 studies to clarify how transparency and explainability can make Maritime Autonomous Surface Ships (MASS) safer and more usable. It maps the Guidance-Navigation-Control stack to shore-based operations and identifies where Human-UCAs concentrate—especially in RSM-RCM handovers and emergency loops. Evidence indicates that transparency features—exposed decision rationales, variables and weights, uncertainty/confidence, and ranked alternatives—primarily improve operator understanding and trust calibration, while overall trust remains strongly moderated by reliability and predictability. A dual-challenge framework is proposed: (1) empirical validation of trust/SA using validated instruments and behavioral indicators in simulation/HIL/MIL testbeds; (2) communication of explanations through adaptive HMIs aligned with users' mental models.

## Method Summary
The paper conducted a systematic literature review of 100 studies on explainable AI and human-AI collaboration in maritime contexts. It analyzed transparency features, trust mechanisms, and HMI design principles through thematic coding. The review mapped MASS operations onto the Guidance-Navigation-Control stack and identified critical handover points between remote supervisors and control modules. The analysis synthesized findings on how different types of transparency affect operator understanding, trust calibration, and decision-making effectiveness.

## Key Results
- Transparency features (decision rationales, confidence indicators, ranked alternatives) improve operator understanding but do not dominate trust formation compared to reliability and predictability
- Human-AI collaboration concentrates in RSM-RCM handovers and emergency control loops, requiring adaptive interfaces for effective handover
- COLREGs formalization remains challenging due to linguistic ambiguity, with fuzzy logic approaches showing promise but requiring validation in mixed traffic scenarios

## Why This Works (Mechanism)
The framework works by addressing two fundamental challenges: empirical validation of trust and situation awareness effects, and effective communication through adaptive HMIs. Transparency features provide operators with insight into AI decision-making, enabling better understanding and appropriate trust calibration. Adaptive interfaces modulate explanation detail based on operator state, preventing cognitive overload while ensuring sufficient information during critical handovers. The mechanism relies on aligning explanation content with operators' mental models of MASS operations.

## Foundational Learning
- **Guidance-Navigation-Control Stack**: MASS operations can be decomposed into guidance (route planning), navigation (position keeping), and control (maneuver execution) layers, each requiring different transparency features
  - *why needed*: Provides systematic framework for identifying where human-AI collaboration is required
  - *quick check*: Verify that identified collaboration points align with actual MASS operational workflows
- **Transparency Feature Taxonomy**: Decision rationales, variables and weights, uncertainty/confidence, and ranked alternatives represent distinct categories of explainability
  - *why needed*: Enables systematic design of explainable interfaces for different operational contexts
  - *quick check*: Test whether each feature type improves specific aspects of operator understanding
- **Trust Calibration vs. System Trust**: Transparency improves trust calibration (appropriate trust levels) rather than overall system trust, which depends more on reliability
  - *why needed*: Guides focus on improving explanation quality rather than assuming transparency automatically builds trust
  - *quick check*: Measure trust calibration accuracy with and without transparency features
- **Adaptive Transparency**: Modulating explanation detail based on operator state prevents both information overload and under-informativeness
  - *why needed*: Addresses cognitive limitations during high-stress handover scenarios
  - *quick check*: Compare performance with static vs. adaptive transparency interfaces
- **Mental Model Alignment**: Explanations must match operators' understanding of MASS control loops to be effective
  - *why needed*: Ensures explanations are comprehensible and actionable
  - *quick check*: Validate that proposed transparency variables align with actual operator mental models
- **COLREGs Formalization**: Linguistic ambiguity in collision avoidance rules requires fuzzy logic or risk-graph approaches for algorithmic implementation
  - *why needed*: Enables autonomous vessels to make consistent, defensible decisions in encounter scenarios
  - *quick check*: Test formalized COLREGs against human expert judgments in diverse scenarios

## Architecture Onboarding
- **Component Map**: Operator -> MASS Control System -> Environment, with RSM-RCM handovers and emergency loops as critical transition points
- **Critical Path**: Route planning → Navigation → Control execution → Environment feedback → Human oversight/emergency takeover
- **Design Tradeoffs**: Static vs. adaptive transparency (cognitive load vs. information sufficiency), explanation detail vs. response time, transparency completeness vs. system complexity
- **Failure Signatures**: Overtrust leading to reduced vigilance, undertrust causing unnecessary interventions, cognitive overload during handovers, misinterpretation of uncertainty indicators
- **First Experiments**:
  1. Within-subject comparison of trust calibration and SA with/without explainable displays using validated instruments
  2. Task-analysis study to verify operator mental models for MASS control loops
  3. Multi-ship simulation testing standardized route-exchange COLREGs procedures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive transparency—modulating explanation detail and timing based on operator state estimation—reliably prevent both cognitive overload and under-informative displays during RSM-RCM handovers?
- Basis in paper: [explicit] The authors conclude by arguing for "adaptive transparency—modulating explanation detail and timing using operator state estimated from behavior, performance and, where appropriate, physiological signals—to prevent both overload and under-informative displays."
- Why unresolved: Current implementations are largely conceptual or simulation-based; no empirical evidence demonstrates that real-time operator state estimation can successfully drive adaptive explanation delivery in actual MASS operations.
- What evidence would resolve it: Controlled experiments comparing static vs. adaptive transparency interfaces, measuring takeover timeliness, Human-UCA rates, and operator workload during emergency handover scenarios.

### Open Question 2
- Question: Do transparency features (decision rationales, confidence indicators, ranked alternatives) produce measurable improvements in trust calibration beyond the effects of system reliability and predictability?
- Basis in paper: [explicit] The paper notes that "transparency was not the dominant factor influencing trust; rather, reliability, predictability, and task support were more significant" (Lyons et al., 2019; Lynch et al., 2022), while also stating most guidelines rely on "subjective experience rather than empirical evidence, resulting in 'unvalidated guidance'" (Ranjan et al., 2025).
- Why unresolved: Studies show transparency improves understanding, but its independent contribution to properly calibrated trust—versus overtrust or distrust—remains unclear when controlling for system performance.
- What evidence would resolve it: Factorial experiments isolating transparency manipulations from reliability/predictability variables, using validated trust instruments and behavioral trust indicators.

### Open Question 3
- Question: How can COLREGs be formally quantified for algorithmic implementation while preserving the contextual judgment currently exercised by human mariners?
- Basis in paper: [explicit] The paper states that "COLREGs were originally designed under the assumption of human involvement, and many provisions are expressed in linguistically ambiguous terms. Key elements—such as what constitutes a 'safe distance' or when a vessel 'shall give way'—lack standardized quantitative definitions, complicating formalization and algorithmic implementation."
- Why unresolved: Fuzzy logic approaches (Bakdi & Vanem, 2022, 2024) show promise but remain unvalidated in mixed autonomous-manned traffic with real-time intent sharing.
- What evidence would resolve it: Multi-vessel simulation trials testing fuzzy/risk-graph COLREGs pipelines against human expert judgments across diverse encounter scenarios.

### Open Question 4
- Question: What shared benchmarks and scenario libraries are needed to enable comparable, cumulative research on transparency-driven human-AI collaboration in MASS?
- Basis in paper: [explicit] The conclusion explicitly calls for "shared scenario libraries and benchmarks, open audit trails linking explanations → operator responses → outcomes, and multi-site trials that quantify effects on takeover timeliness and Human-UCA rates" as priorities for future work.
- Why unresolved: No comprehensive database or standardized platform currently exists for navigation research focused on human–machine cooperation; simulator interface standards differ across platforms, limiting generalizability.
- What evidence would resolve it: Consensus on standard test scenarios, common metrics for trust/SA/workload, and cross-platform validation studies demonstrating reproducibility of findings.

## Limitations
- Trust and situation awareness claims remain largely inferred from secondary literature without quantitative behavioral outcome data
- The causal pathway from transparency to calibrated trust to safer operation is theorized rather than empirically measured
- Mental model alignment for adaptive HMI guidance is described but not tested in practice
- No comprehensive scenario database or standardized platform exists for navigation research focused on human-AI collaboration

## Confidence
- **High**: Structural mapping of MASS to Guidance-Navigation-Control stack; documented transparency feature taxonomies
- **Medium**: Trust-benefit links; adaptive HMI guidance mental model alignment
- **Low**: Empirical validation of trust calibration improvements; behavioral outcome measurements

## Next Checks
1. Run a within-subject experiment in ship-bridge or simulation testbed to compare trust calibration and SA with and without explainable displays, using validated instruments and behavioral markers
2. Conduct a task-analysis study to verify which mental models shore-based operators actually hold for MASS control loops and whether proposed transparency variables match them
3. Pilot a standardized route-exchange COLREGs procedure in a multi-ship simulation and measure time-to-comprehension and conflict reduction compared to verbal orders