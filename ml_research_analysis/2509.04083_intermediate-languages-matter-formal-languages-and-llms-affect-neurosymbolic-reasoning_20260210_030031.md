---
ver: rpa2
title: 'Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic
  Reasoning'
arxiv_id: '2509.04083'
source_url: https://arxiv.org/abs/2509.04083
tags:
- language
- formal
- reasoning
- llms
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the impact of formal language choice on\
  \ neurosymbolic LLM reasoning. The authors conduct an extensive empirical study\
  \ comparing four formal languages (Pyke, ASP, NLTK, FOL) across three datasets (ProntoQA,\
  \ ProofWriter, FOLIO) and seven LLMs (8B\u2013671B parameters)."
---

# Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning

## Quick Facts
- **arXiv ID**: 2509.04083
- **Source URL**: https://arxiv.org/abs/2509.04083
- **Reference count**: 34
- **Primary result**: FOL outperforms other formal languages, achieving up to 85.45% accuracy on DeepSeek-R1 with 20480 max-output-tokens

## Executive Summary
This paper investigates how the choice of formal language impacts neurosymbolic reasoning performance when using large language models (LLMs). The authors conduct an extensive empirical study comparing four formal languages (Pyke, ASP, NLTK, FOL) across three datasets and seven LLMs ranging from 8B to 671B parameters. The core methodology involves translating natural language reasoning problems into formal languages using LLMs, then solving them with symbolic solvers. The study reveals that formal language syntax significantly affects reasoning performance, with FOL consistently outperforming other languages and showing particular advantages for smaller language models.

## Method Summary
The authors systematically evaluate how different formal languages affect neurosymbolic reasoning performance. They translate natural language reasoning problems into four formal languages (Pyke, ASP, NLTK, FOL) using LLMs, then execute these formal representations with symbolic solvers. The study covers three datasets (ProntoQA, ProofWriter, FOLIO) and seven LLMs with parameters ranging from 8B to 671B. Translation accuracy and overall reasoning accuracy are measured for each language-model-dataset combination, with a particular focus on how max-output-tokens affects performance. The methodology treats LLMs as black boxes while systematically varying the formal language interface.

## Key Results
- FOL achieves highest accuracy (85.45%) on DeepSeek-R1 with 20480 max-output-tokens
- Smaller language models show the largest improvements from optimal formal language selection
- Translation errors vary significantly across formal languages, affecting overall reasoning performance
- No single formal language dominates across all models and datasets

## Why This Works (Mechanism)
The performance differences stem from how well LLMs can generate syntactically correct and semantically meaningful representations in different formal languages. Each formal language has distinct syntactic constraints and semantic structures that interact differently with LLM token generation patterns. The study reveals that formal languages with simpler, more constrained syntaxes tend to produce more reliable translations, particularly for smaller models with limited reasoning capacity.

## Foundational Learning
- **Formal Languages**: Symbolic systems with precise syntax and semantics for representing logical relationships
  - Why needed: Provide unambiguous representations for automated reasoning
  - Quick check: Can the language express the problem without ambiguity?
- **Neurosymbolic Reasoning**: Integration of neural networks with symbolic AI for enhanced reasoning capabilities
  - Why needed: Combines pattern recognition with logical inference
  - Quick check: Does the system use both neural and symbolic components?
- **Translation Accuracy**: Measure of how well natural language problems are converted to formal representations
  - Why needed: Poor translation quality directly impacts final reasoning accuracy
  - Quick check: Can a human verify the formal representation matches the original problem?
- **Solver Execution**: Process of evaluating formal representations to produce answers
  - Why needed: Translates symbolic representations into concrete solutions
  - Quick check: Does the solver return valid results for known inputs?
- **Max-output-tokens**: Parameter controlling maximum tokens an LLM can generate
  - Why needed: Affects the completeness and quality of formal language generation
  - Quick check: Does increasing this parameter improve translation quality?
- **Intermediate Language Challenge**: Problem of selecting optimal formal language for a given reasoning task
  - Why needed: Different languages perform better for different models and problems
  - Quick check: Can the choice of language be predicted based on model characteristics?

## Architecture Onboarding
**Component Map**: Natural Language Problem -> LLM Translator -> Formal Language Representation -> Symbolic Solver -> Answer

**Critical Path**: Translation (LLM) → Formal Representation → Solver Execution → Answer Generation

**Design Tradeoffs**: 
- Language Expressiveness vs. Translation Reliability: More expressive languages may be harder to translate correctly
- Model Size vs. Language Complexity: Smaller models benefit more from simpler formal languages
- Translation Quality vs. Solver Power: Even perfect solvers cannot overcome poor translations

**Failure Signatures**: 
- Low translation accuracy indicates LLM struggles with formal syntax
- High translation but low overall accuracy suggests solver limitations or semantic mismatches
- Performance gaps between models suggest model-specific language compatibility issues

**First Experiments**:
1. Test translation accuracy of each formal language on a simple subset of problems
2. Compare solver execution time across formal languages for identical problems
3. Measure impact of max-output-tokens on translation quality for each formal language

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can crafting custom intermediate languages and fine-tuning SLMs on them outperform standard formal languages like FOL?
- Basis in paper: [explicit] The authors state they plan to explore "crafting custom intermediate languages and fine-tuning the SLMs on these custom languages" in a future study.
- Why unresolved: The current work only evaluates existing, standard formal languages (Pyke, ASP, NLTK, FOL) on immutable models, without testing optimized or synthetic syntaxes.
- What evidence would resolve it: Empirical results showing SLMs fine-tuned on a synthetic, optimized syntax achieving higher translation accuracy and overall reasoning scores than baseline FOL implementations.

### Open Question 2
- Question: Does the prevalence of specific formal languages in LLM pre-training data directly correlate with the neurosymbolic reasoning performance observed for those languages?
- Basis in paper: [explicit] The authors "hypothesize that the performance differences can be explained with a lack or abundance of the formal languages in the training data."
- Why unresolved: The study treats LLMs as black boxes and does not analyze the specific composition of the models' training corpora.
- What evidence would resolve it: A controlled study measuring the frequency of syntax constructs (e.g., Prolog vs. FOL) in training sets and correlating it with execution-accuracy on translation tasks.

### Open Question 3
- Question: Can an automated meta-selector dynamically choose the optimal intermediate language for a specific problem instance to maximize accuracy?
- Basis in paper: [inferred] The paper defines the "intermediate language challenge" as the task of *choosing* a language, and results show the "best" language varies by model (e.g., ASP wins on DeepSeek-R1, FOL on GPT-4o-mini).
- Why unresolved: The paper provides a manual comparison but does not propose a mechanism to automate this choice for a given input and model pair.
- What evidence would resolve it: A classifier that predicts the likelihood of successful translation for available languages and routes the input to the one with the highest expected overall-accuracy.

## Limitations
- Study focuses exclusively on text-based reasoning tasks, limiting generalizability to multimodal domains
- Translation quality heavily depends on initial LLM-generated formal representations, creating potential compounding errors
- Only four formal languages were tested, despite existence of numerous specialized reasoning formalisms

## Confidence
- **High Confidence**: FOL superiority in accuracy metrics, impact of formal language syntax on reasoning performance, SLM improvement patterns
- **Medium Confidence**: Translation error analysis across formal languages, max-output-token effect on reasoning quality
- **Low Confidence**: Generalization to other formal language families, applicability to non-text reasoning domains

## Next Checks
1. Conduct ablation studies isolating translation quality from solver performance by comparing human-generated vs. LLM-generated formal representations
2. Test additional formal languages beyond the four studied, particularly those optimized for specific reasoning domains (temporal, probabilistic, or spatial reasoning)
3. Extend evaluation to multimodal datasets to assess whether formal language advantages persist when visual information is incorporated into reasoning tasks