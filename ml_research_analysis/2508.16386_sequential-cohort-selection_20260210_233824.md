---
ver: rpa2
title: Sequential Cohort Selection
arxiv_id: '2508.16386'
source_url: https://arxiv.org/abs/2508.16386
tags:
- policy
- fairness
- utility
- selection
- admission
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses fair cohort selection under uncertainty, focusing
  on university admissions. The authors propose a policy gradient framework that jointly
  learns and optimizes selection policies using a population model trained on historical
  admission data.
---

# Sequential Cohort Selection

## Quick Facts
- arXiv ID: 2508.16386
- Source URL: https://arxiv.org/abs/2508.16386
- Reference count: 8
- Primary result: Neural network policies outperform logistic regression in fair cohort selection, achieving higher utility and better fairness outcomes under uncertainty.

## Executive Summary
This work addresses fair cohort selection under uncertainty, focusing on university admissions. The authors propose a policy gradient framework that jointly learns and optimizes selection policies using a population model trained on historical admission data. They incorporate fairness constraints such as demographic parity and equality of opportunity based on expected marginal contribution (EMC). Experiments in both one-shot and sequential settings show that neural network policies outperform logistic regression, achieving higher utility and better fairness outcomes. In the sequential setting, adaptive policies consistently improve over static baselines, especially when admission costs are high.

## Method Summary
The framework optimizes admission policies using a population model trained on historical data, with two policy architectures: logistic regression and neural networks. It employs Monte Carlo policy gradient estimation to handle unknown populations, using CTGAN to simulate applicants and Bayesian multivariate regression for outcome prediction. The sequential setting allows policy updates across stages as new data becomes available, using Thompson sampling to manage selection bias from censored outcomes. Fairness is enforced through demographic parity and equality of opportunity metrics based on expected marginal contribution (EMC).

## Key Results
- Neural network policies achieve higher utility and better fairness outcomes than logistic regression in both one-shot and sequential settings
- Adaptive policies consistently outperform static baselines in sequential admission, particularly at high admission costs
- The sequential setting demonstrates significant improvements in policy performance through continuous updates as new applicant data becomes available

## Why This Works (Mechanism)

### Mechanism 1: Monte Carlo Policy Gradient for Unknown Populations
The framework optimizes admission policies for unknown populations by estimating gradients via Monte Carlo sampling from a learned population simulator. The policy $\pi(a|x)$ is updated by approximating the gradient of the expected utility $\nabla U(\pi, P)$ through synthetic population sampling $x^{(k)} \sim P(x)$ and action sampling $a^{(m)} \sim \pi(a|x)$. This breaks if the simulated population distribution drifts significantly from reality, causing the policy to optimize for a phantom population.

### Mechanism 2: Meritocratic Fairness via Expected Marginal Contribution (EMC)
The system operationalizes "equality of opportunity" by quantifying merit as the Expected Marginal Contribution (EMC)—the expected increase in cohort utility if a specific candidate is added. Instead of enforcing demographic parity regardless of qualification, the model penalizes disparities in acceptance rates only among candidates with similar EMC. This breaks if the utility function does not align with the institution's definition of "success."

### Mechanism 3: Bayesian Handling of Selection Bias (Censored Outcomes)
The sequential setting maintains robustness despite only observing outcomes for admitted students by using Bayesian posterior updates. Since outcomes $y$ are unobserved for rejected students, the outcome model uses Bayesian multivariate regression with posterior predictive sampling (Thompson Sampling) to manage uncertainty and explore the feature space. This breaks if rejected populations are fundamentally different from admitted populations in ways not captured by available features.

## Foundational Learning

- **Policy Gradient (REINFORCE):** Needed because standard supervised learning cannot optimize "set" utility directly—the utility of admitting one student depends on who else is admitted. Policy gradient allows optimizing parameters to maximize expected cohort utility. Quick check: How does the gradient estimate change if the batch size (applicant pool size) is reduced to 1?

- **Counterfactual Fairness / Censoring:** Needed because in sequential settings you suffer from "selective labels"—you only see outcomes for admitted students. Understanding why you need an exploration strategy (like Thompson Sampling) to occasionally admit risky candidates is crucial for long-term learning. Quick check: Why would a purely greedy policy (only admitting top predicted GPAs) fail to improve the model over time?

- **Generative Adversarial Networks (CTGAN):** Needed because the method relies on a "Population Model" $P(x)$ to simulate applicants. The quality of the policy is bounded by the realism of the simulator used to generate training data. Quick check: If CTGAN fails to capture correlations between "science points" and "language points," how might the resulting admission policy underperform?

## Architecture Onboarding

- **Component map:** Simulator ($P^*$) -> Outcome Model ($P(y|x,a)$) -> Policy Network ($\pi_\theta$) -> Optimizer (Gradient Ascent via REINFORCE)
- **Critical path:** Offline Phase: Train CTGAN on history; Train Outcome Model on admitted history. Policy Learning Loop: Sample $x$ from CTGAN → Sample action $a$ from Policy → Sample outcome $y$ from Outcome Model → Compute Utility $U$ → Compute Gradient $\nabla \theta$ → Update Policy. Sequential Deployment: Apply $\pi$ to real applicants → Observe real $y$ → Update Outcome Model Posterior → Retrain Policy.
- **Design tradeoffs:** Logistic vs. Neural Policy: Logistic is transparent/stable but lower utility. Neural is high-performance but opaque/variable. Cost Parameter ($c$): Low $c$ encourages large cohorts (high utility, potential fairness drops). High $c$ forces selectivity (lower utility, forces prioritization of highest merit). Delayed Updates: Neural policies tolerate delayed updates well; Logistic policies degrade rapidly if not updated every stage.
- **Failure signatures:** Selection Bias Collapse: If policy becomes too selective too fast, Outcome Model stops receiving diverse data, leading to overfitting. Variance Explosion: If sample size is too low, gradient estimate becomes noise, causing policy to oscillate or diverge.
- **First 3 experiments:** 1) Verify Gradient Estimator: Implement Monte Carlo gradient estimation on dummy linear utility function. Check if policy maximizes known optimal solution. 2) Ablate the Outcome Model: Run sequential loop with fixed vs. Bayesian-updating Outcome Model. Plot utility gap over 10 stages to quantify adaptivity value. 3) Stress Test Fairness: Fix utility cost $c$ and vary fairness penalty weights ($\lambda_{dem}, \lambda_{eq}$). Plot Pareto frontier to verify trade-off mechanism.

## Open Questions the Paper Calls Out

- Can the policy be updated incrementally after each individual applicant evaluation rather than after complete admission cycles, and how would this affect utility and fairness outcomes? [explicit] "Future research could expand this work to online learning as sequential decision-making where we update the policy incrementally as each applicant is evaluated, rather than waiting for a full annual cycle."

- How can the framework be extended to handle two-sided matching problems where both institutions and applicants have preferences and selection constraints? [explicit] "Another future work could be the matching problem."

- How should decision-makers systematically select the cost parameter c and fairness penalty weights (λdem, λeq) to balance utility and fairness under different institutional priorities? [inferred] The paper demonstrates the tradeoff empirically but provides no principled method for parameter selection.

## Limitations

- The validity of CTGAN-generated synthetic populations as proxies for real applicant distributions is unverified, potentially limiting policy generalization
- Fairness claims based on Expected Marginal Contribution (EMC) lack external validation against established fairness frameworks
- The exclusive use of Norwegian university admission data limits generalizability to institutions with different applicant pools and outcome definitions

## Confidence

- Policy gradient optimization effectiveness: Medium confidence - Theoretically sound but lacks specific corpus validation for this application
- Fairness via EMC: Low confidence - Novel metric without external validation or comparison to established frameworks
- Sequential adaptation benefits: Medium confidence - Demonstrates improvement over static policies, but benefits may depend heavily on specific data distribution and cost parameters

## Next Checks

1. **Distribution fidelity validation:** Compare statistical properties (means, variances, correlations) of synthetic CTGAN-generated applicants against real applicant data using hypothesis tests to quantify simulation quality.

2. **EMC fairness robustness test:** Vary the utility function definition and measure how EMC-based fairness outcomes change to assess sensitivity to the merit definition.

3. **Cross-institutional transfer test:** Train the policy on Norwegian data, then evaluate performance and fairness on admissions data from a different institution with distinct applicant demographics and outcome distributions.