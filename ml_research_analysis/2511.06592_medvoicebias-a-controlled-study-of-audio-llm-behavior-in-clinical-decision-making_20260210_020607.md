---
ver: rpa2
title: 'MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making'
arxiv_id: '2511.06592'
source_url: https://arxiv.org/abs/2511.06592
tags:
- audio
- bias
- clinical
- gender
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic evaluation of demographic
  bias in audio LLMs for clinical decision-making, focusing on surgical recommendations.
  The authors constructed MedVoiceBias, a dataset of 170 clinical cases synthesized
  into speech using 36 distinct voice profiles varying in age, gender, and emotion.
---

# MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making

## Quick Facts
- arXiv ID: 2511.06592
- Source URL: https://arxiv.org/abs/2511.06592
- Reference count: 25
- Primary result: Audio LLMs show up to 34.9% modality-dependent bias in surgical recommendations based on voice demographics

## Executive Summary
This paper presents the first systematic evaluation of demographic bias in audio LLMs for clinical decision-making. The authors constructed MedVoiceBias, a dataset of 170 clinical cases synthesized into speech using 36 distinct voice profiles varying in age, gender, and emotion. Across six state-of-the-art audio LLMs, they found severe modality-dependent bias, with surgery recommendation rates varying by up to 34.9 percentage points between text and audio inputs. Age disparities of up to 12% persisted in most models even with chain-of-thought prompting, while gender bias was eliminated through explicit reasoning. Emotion effects were not detected due to poor recognition performance.

## Method Summary
The study constructed MedVoiceBias using 170 clinical cases from DDXPlus, synthesized into speech using 36 voice profiles (12 speakers from Common Voice balanced across age/gender; 4 Expresso speakers × 6 emotions). Voice cloning via Sesame-1B, with quality validation via Whisper-v3 ASR (6.4% avg WER) and MOSA-Net+. Six audio LLMs were evaluated (DeSTA2.5-Audio 8B, Qwen2.5-Omni 3B/7B, Gemini Flash 2.0/2.5, GPT-4o-mini-audio) using two prompting strategies (Direct Answer and diagnose-then-decide Chain-of-Thought) across three input conditions (Text, Text+Profile, Audio). Surgery recommendation rates were computed per condition/demographic with statistical significance via Fisher's exact test (p<0.05).

## Key Results
- Surgery recommendation rates varied by up to 34.9 percentage points between text and audio inputs
- Age disparities of up to 12% persisted in most models even with chain-of-thought prompting
- Gender bias was eliminated through explicit chain-of-thought reasoning
- Emotion effects were not detected due to poor recognition performance

## Why This Works (Mechanism)
None

## Foundational Learning
- **Fisher's exact test**: Statistical method for determining significance in categorical data with small sample sizes; needed for validating bias differences across demographic groups; quick check: verify p-values below 0.05 threshold
- **Chain-of-thought prompting**: Explicit reasoning strategy that guides models through step-by-step problem solving; needed to assess whether structured reasoning reduces demographic bias; quick check: compare output formats with and without CoT templates
- **Whisper ASR WER**: Word Error Rate metric for speech recognition accuracy; needed to validate audio synthesis quality before model input; quick check: calculate average WER across all synthesized samples
- **MOSA-Net+**: Model for assessing speech quality metrics including PESQ and intelligibility; needed to ensure synthesized voices are perceptually adequate; quick check: verify all samples exceed target quality thresholds
- **Clinical significance threshold**: Minimum absolute difference (2%) considered practically meaningful in medical contexts; needed to distinguish statistically significant from clinically relevant bias; quick check: apply threshold when reporting bias magnitudes

## Architecture Onboarding
**Component Map**: Clinical Cases -> Voice Synthesis -> Model Input -> Decision Output -> Statistical Analysis
**Critical Path**: Voice synthesis quality validation -> Model inference consistency -> Demographic bias detection
**Design Tradeoffs**: Synthetic voices enable controlled experimentation but may not capture real-world acoustic variation; comprehensive demographic coverage versus manageable sample size
**Failure Signatures**: Inconsistent binary response parsing, poor emotion recognition masking true bias effects, synthetic voice profiles not reliably conveying intended demographics
**First Experiments**: 1) Validate demographic classification accuracy on synthesized audio, 2) Test binary response parsing on held-out samples, 3) Verify Fisher's exact test implementation with simulated bias data

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Why does explicit chain-of-thought (CoT) reasoning eliminate gender bias but fail to mitigate—and sometimes amplify—age-related disparities in surgical recommendations?
- Basis in paper: The authors found that while CoT eliminated gender bias, it increased the prevalence of age-related differences (Section 4.3), suggesting models process these cues via fundamentally different mechanisms.
- Why unresolved: The study quantified the behavioral output but did not perform mechanistic analysis to explain why reasoning prompts activate "problematic clinical heuristics" for age but not gender.
- What evidence would resolve it: Mechanistic interpretability studies (e.g., attention head analysis) comparing how age versus gender audio features are weighted during the reasoning process.

### Open Question 2
- Question: Does emotional expression in voice significantly influence clinical decision-making when models possess high emotional recognition accuracy?
- Basis in paper: Section 4.4 states the impact of emotion was "not detected due to poor recognition performance," noting that most models' inability to perceive cues made robustness "impossible" to assess.
- Why unresolved: The low emotion detection accuracy (mostly <17%) effectively nullified the emotional variable, preventing differentiation between model robustness and model deafness.
- What evidence would resolve it: Re-evaluating the MedVoiceBias dataset using audio models specifically fine-tuned for paralinguistic emotion recognition to ensure the input cues are successfully perceived.

### Open Question 3
- Question: What specific architectural modifications are required to disentangle medical semantics from paralinguistic voice characteristics in end-to-end audio LLMs?
- Basis in paper: The Conclusion states that "bias-aware architectures are essential" to address the "fundamental architectural challenge" of separating patient information from voice features.
- Why unresolved: Current architectures lack the necessary inductive biases to ignore demographic proxies (voice) while attending to clinical evidence (content), leading to the observed modality-dependent behavior.
- What evidence would resolve it: The development of audio encoders that project inputs into a "voice-anonymized" semantic space or adversarial training techniques that minimize demographic leakage.

## Limitations
- Synthetic voice dataset may not fully capture real-world acoustic variation
- Fixed case set of 170 examples limits statistical power for detecting smaller bias effects
- Emotion bias detection was inconclusive due to poor recognition performance

## Confidence
- **High confidence**: Modality-dependent bias findings (text vs audio differences up to 34.9 percentage points)
- **Medium confidence**: Age-related bias findings (up to 12% disparity)
- **Low confidence**: Emotion bias conclusions due to technical limitations in emotion recognition

## Next Checks
1. Replicate with real patient audio to validate findings beyond synthetic voices
2. Expand case diversity across multiple clinical decision types to test generalizability
3. Implement bias-aware prompting with explicit demographic context to evaluate systematic mitigation strategies