---
ver: rpa2
title: 'This Is Your Doge, If It Please You: Exploring Deception and Robustness in
  Mixture of LLMs'
arxiv_id: '2503.05856'
source_url: https://arxiv.org/abs/2503.05856
tags:
- deceptive
- agents
- passage
- answer
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the robustness of Mixture of Agents (MoA) architectures\
  \ against deceptive agents. The authors introduce deceptive agents into MoA systems\
  \ and demonstrate that even a single deceptive agent can severely harm performance\u2014\
  on AlpacaEval 2.0, accuracy drops from 49.2% to 37.9%, erasing all MoA gains, and\
  \ on QuALITY, accuracy falls by 48.5%."
---

# This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs

## Quick Facts
- arXiv ID: 2503.05856
- Source URL: https://arxiv.org/abs/2503.05856
- Reference count: 40
- Key outcome: Single deceptive agent in Mixture of Agents systems can drop accuracy from 49.2% to 37.9% on AlpacaEval 2.0, with unsupervised defenses recovering most performance

## Executive Summary
This paper investigates the vulnerability of Mixture of Agents (MoA) architectures to deceptive agents, demonstrating that even a single malicious agent can severely compromise system performance. The authors show that deception effectiveness increases when the deceptive agent is positioned closer to the aggregator and when it's instructed to ignore truthful references from previous layers. They propose four unsupervised defense mechanisms inspired by the Venetian Doge election process - Dropout & Vote, Dropout & Cluster, Cluster & Filter, and Cluster & Prompt - which successfully recover most of the lost performance without requiring labeled training data.

## Method Summary
The authors evaluate a 3-3-1 MoA architecture on QuALITY (500 hard questions) and AlpacaEval 2.0 (805 questions) with truthful agents including WizardLM-2-8x22B, Llama-3.1-70B-Instruct, and Mixtral-8x22B-Instruct. Deceptive agents are created by providing wrong answer labels and specific prompts to either promote (advocate for wrong answer) or oppose (advocate for wrong answer while arguing against correct one). The system tests various defense mechanisms including embedding-based clustering using OpenAI text-embedding-3-small and dropout-based aggregation with majority voting. Performance metrics include accuracy, Deception Success Rate (DSR), Recovery Rate (RR), and Deception Corruption Rate (DCR).

## Key Results
- Single deceptive agent drops MoA accuracy from 49.2% to 37.9% on AlpacaEval 2.0, erasing all MoA gains
- On QuALITY, accuracy falls by 48.5% when deceptive agents are introduced
- Larger aggregators (70B vs 8B) show increased robustness but don't eliminate vulnerability
- Cluster & Filter defense recovers most lost performance by filtering minority cluster responses
- Deception effectiveness is highest when deceptive agents are closest to the aggregator

## Why This Works (Mechanism)

### Mechanism 1: Clustering-Based Outlier Detection
Embedding-space clustering identifies deceptive responses by their semantic divergence from truthful ones. Deceptive agents argue for incorrect positions, creating responses that cluster differently in embedding space. K-means with k=2 separates minority deceptive clusters from majority truthful clusters, allowing filtering of suspicious responses.

### Mechanism 2: Dropout-Based Aggregation Robustness
Randomly excluding agents during aggregation creates multiple subsets, reducing single-point-of-failure vulnerability. Even if one subset includes a deceptive agent, others won't, and majority voting across subsets can correct individual errors. This improves robustness when p_deception < 1 for individual subsets.

### Mechanism 3: Layer Proximity and Information Propagation
Deceptive agents closer to the aggregator cause more damage, but truthful references passed through layers can constrain deceptive aggregating proposers. When deceptive agents ignore references, proximity directly correlates with attack success. Truthful references from earlier layers make it harder for deceptive aggregators to synthesize convincing false arguments.

## Foundational Learning

- **Mixture of Agents (MoA) Architecture**: Understanding proposers, aggregating proposers, and aggregators is essential before analyzing where deception enters and how it propagates.
  - Quick check question: In a 3-3-1 MoA, which layer contains the final aggregator and how many inputs does it receive?

- **Embedding Space Similarity**: Defense mechanisms rely on clustering responses in embedding space to separate truthful from deceptive outputs.
  - Quick check question: Why might k-means with k=2 fail if deceptive agents are skilled at mimicking truthful response patterns?

- **Majority Voting Robustness**: Dropout & Vote defense assumes voting can correct individual errors; understanding voting failure modes is critical.
  - Quick check question: What percentage of deceptive agents would cause majority voting to completely fail?

## Architecture Onboarding

- **Component map**: Proposers (Layer 1) → Aggregating Proposers (Layer 2) → Aggregator (Layer 3). Each agent receives user query + previous layer outputs.

- **Critical path**: Deceptive agent placement in Layer 2 (closest to aggregator) with instruction to ignore references = maximum damage. Defense deployment at aggregator input stage.

- **Design tradeoffs**:
  - Cluster & Filter: Low compute, assumes good clustering quality
  - Dropout & Vote: Higher compute (multiple aggregator calls), requires categorical outputs
  - Dropout & Cluster: Medium compute, handles non-categorical outputs
  - Cluster & Prompt: Lowest defense overhead, relies on aggregator's ability to use cluster labels

- **Failure signatures**:
  - Sudden performance drop to baseline levels (suggests single deceptive agent)
  - Deception Success Rate (DSR) approaching 1.0 for specific answer choices
  - DCR (Deception Corruption Rate) > 50% indicates previously correct answers being flipped

- **First 3 experiments**:
  1. **Baseline vulnerability test**: Insert single deceptive agent in Layer 2 position A2,3 with "ignore references" instruction; measure DSR and accuracy drop on QuALITY subset.
  2. **Defense comparison**: Apply all four defense mechanisms to the compromised MoA; compare accuracy recovery and DSR reduction.
  3. **Scaling test**: Vary aggregator size (8B, 70B, 405B) with fixed deceptive agent; test whether larger aggregators maintain consistent vulnerability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: If scaling aggregator model size fails to robustly defend against deception in question-answering tasks, what specific architectural or training modifications are required to create inherently resilient aggregator models?
- Basis in paper: [explicit] Section 5.2.3 asks "Are larger aggregators really more robust?" and concludes that "merely scaling up model size may not be sufficient to overcome MoA's vulnerabilities."
- Why unresolved: The authors demonstrate that larger aggregators do not consistently resist deceptive inputs (specifically in QA), identifying a capability gap without proposing a solution beyond the unsupervised defense mechanisms.

### Open Question 2
- Question: Do the "unsupervised defense mechanisms" (e.g., Cluster & Filter) remain effective against sophisticated, non-prompted deception strategies, such as emergent reward-hacking or "sleeper agent" behaviors?
- Basis in paper: [inferred] The paper focuses on "carefully-instructed" deceptive agents using specific prompts (Section 4), but acknowledges literature on emergent deception ("sleeper agents") in the Introduction and Related Work.
- Why unresolved: The defenses proposed (clustering based on semantic similarity or voting) are tested against agents explicitly prompted to lie; it is unclear if these methods can detect agents that are subtly misaligned or hallucinating rather than overtly "opposing" the truth.

### Open Question 3
- Question: How can standardized safety evaluations be designed to specifically measure the propagation and amplification of deception in multi-layer MoA systems compared to single models?
- Basis in paper: [explicit] The conclusion states that "future work must focus on developing... standardized safety evaluations to ensure reliable real-world deployment."
- Why unresolved: Current benchmarks (AlpacaEval, QuALITY) measure task performance (accuracy/win rate) but lack metrics specifically designed to quantify the "infection rate" or "corruption rate" of deception through hierarchical agent layers.

## Limitations

- The paper relies on synthetic deception scenarios that may not generalize to real-world deployment conditions
- Embedding-based clustering assumes deceptive responses create distinct semantic clusters, which may fail with sophisticated deceptive agents
- The effectiveness of dropout-based defenses depends on p_deception being significantly below 1, but this threshold isn't thoroughly explored

## Confidence

- **High Confidence**: Baseline vulnerability results showing performance degradation from 49.2% to 37.9% accuracy are well-supported by controlled experiments
- **Medium Confidence**: Clustering defense mechanisms show promise but rely on assumptions about embedding space separation that weren't rigorously validated across different embedding models
- **Low Confidence**: Layer proximity analysis assumes aggregators synthesize rather than ignore references, but this behavioral assumption wasn't empirically tested with diverse aggregator models

## Next Checks

1. Test clustering defenses with alternative embedding models (e.g., OpenAI embeddings vs. sentence-transformers) to verify that the 2-cluster separation isn't an artifact of the specific embedding choice.
2. Systematically vary the deception success probability p_deception for individual agents to determine the threshold at which dropout-based defenses fail completely.
3. Conduct ablation studies removing the "ignore references" instruction from deceptive agents to measure the actual protective effect of truthful reference propagation through layers.