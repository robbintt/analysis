---
ver: rpa2
title: Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning
arxiv_id: '2505.13628'
source_url: https://arxiv.org/abs/2505.13628
tags:
- languages
- alignment
- language
- multilingual
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of aligning multilingual sentence
  representations without requiring parallel text data, which is expensive and time-consuming
  to obtain for low-resource languages. The authors propose using visual information
  as a shared modality to align text representations across languages by fine-tuning
  multilingual encoders using image-caption pairs in multiple languages.
---

# Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning

## Quick Facts
- arXiv ID: 2505.13628
- Source URL: https://arxiv.org/abs/2505.13628
- Authors: Nathaniel Krasner; Nicholas Lanuzo; Antonios Anastasopoulos
- Reference count: 14
- Primary result: Multilingual text-image alignment achieves 55.7% bitext retrieval accuracy, outperforming English-only (18.3%) and approaching text-text alignment (62.2%)

## Executive Summary
This work addresses the problem of aligning multilingual sentence representations without requiring parallel text data, which is expensive and time-consuming to obtain for low-resource languages. The authors propose using visual information as a shared modality to align text representations across languages by fine-tuning multilingual encoders using image-caption pairs in multiple languages. The core method involves training a multilingual text encoder jointly with an image encoder using contrastive learning on image-caption pairs from English, Spanish, Japanese, Hindi, and Quechua.

## Method Summary
The authors propose a method for aligning multilingual sentence representations by leveraging visual information as a shared modality. The approach uses contrastive learning to train a multilingual text encoder jointly with an image encoder on image-caption pairs from multiple languages. By fine-tuning pre-trained multilingual encoders with this image-caption alignment objective, the method implicitly aligns text representations between languages without requiring parallel text data. The model is trained on captions in English, Spanish, Japanese, Hindi, and Quechua, and evaluated on bitext retrieval tasks and downstream natural language inference tasks.

## Key Results
- Multilingual text-image alignment achieves 55.7% accuracy on bitext retrieval, significantly outperforming English-only baseline (18.3%) and approaching text-text alignment (62.2%)
- Adding Quechua captions improves Quechua bitext retrieval from 18% to 29.2% while maintaining performance on other languages
- Downstream NLI task performance is preserved and even improved with this alignment approach
- The method enables effective cross-lingual transfer for low-resource languages without requiring parallel text data

## Why This Works (Mechanism)
The method works by leveraging visual grounding as a shared semantic space across languages. When captions describing the same image are aligned with that image through contrastive learning, the text representations from different languages are implicitly pulled closer together in semantic space. This occurs because the model learns to map semantically equivalent captions (describing the same visual content) to similar representations, regardless of their language. The visual modality provides a bridge that bypasses the need for direct translation pairs, allowing the model to learn cross-lingual alignments through shared real-world concepts.

## Foundational Learning
- **Contrastive learning**: Why needed - to pull together semantically similar representations while pushing apart dissimilar ones; Quick check - verify loss function correctly implements InfoNCE or similar contrastive objective
- **Multimodal alignment**: Why needed - to use visual information as a bridge for cross-lingual semantic alignment; Quick check - ensure image and text encoders project to compatible embedding spaces
- **Multilingual pretraining**: Why needed - provides initial cross-lingual representations that can be fine-tuned; Quick check - confirm pretraining quality by testing zero-shot cross-lingual transfer before alignment
- **Cross-modal consistency**: Why needed - captions in different languages describing the same image should have similar semantic content; Quick check - measure cosine similarity between aligned representations from different languages

## Architecture Onboarding

Component map: Image Encoder -> Contrastive Loss -> Text Encoder -> Multimodal Dataset

Critical path: Image encoder produces embeddings → Contrastive loss compares with text embeddings → Text encoder is updated to align semantically equivalent captions across languages

Design tradeoffs: The approach trades computational efficiency (fine-tuning large multilingual models) for the ability to work without parallel text data. It also assumes visual grounding provides sufficient semantic overlap, which may not hold for abstract concepts.

Failure signatures: Poor performance on languages with limited visual grounding, failure to align abstract concepts, degradation when training data includes captions with varying levels of visual grounding.

First experiments:
1. Train on a single language pair with image captions to verify basic alignment capability
2. Test zero-shot cross-lingual retrieval performance on languages not seen during fine-tuning
3. Evaluate performance degradation when removing individual language pairs from training

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies on multilingual image-caption datasets, which may not cover all linguistic phenomena and could introduce bias toward visually grounded concepts
- Performance gains on low-resource languages like Quechua remain modest (29.2% vs 18% baseline), suggesting limitations in capturing deeper semantic alignment
- The method assumes that visual grounding provides sufficient semantic overlap across languages, which may not hold for abstract or culturally specific concepts

## Confidence
- **Technical approach**: High - contrastive learning framework is well-established and results are reproducible
- **Cross-lingual transfer magnitude**: Medium - performance improvements vary significantly across languages and tasks
- **Generalizability to truly low-resource languages**: Low - limited evidence beyond the Quechua example with available multilingual image-caption datasets

## Next Checks
1. Evaluate the method on additional low-resource languages with varying linguistic families and script systems to assess generalizability beyond Indo-European languages and Quechua

2. Test performance degradation when training data includes captions with varying levels of visual grounding (e.g., abstract vs concrete descriptions) to understand the limits of visual alignment

3. Conduct ablation studies removing individual language pairs from training to quantify the contribution of each language to cross-lingual alignment performance, particularly for zero-shot transfer to unseen languages