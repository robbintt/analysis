---
ver: rpa2
title: On the Reliability of Sampling Strategies in Offline Recommender Evaluation
arxiv_id: '2508.05398'
source_url: https://arxiv.org/abs/2508.05398
tags:
- evaluation
- sampling
- exposure
- bias
- recommender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the reliability of sampling strategies for
  offline recommender system evaluation. The authors propose a framework to assess
  sampling reliability across four dimensions: resolution (model separability), fidelity
  (agreement with full evaluation), robustness (stability under exposure bias), and
  predictive power (alignment with ground truth).'
---

# On the Reliability of Sampling Strategies in Offline Recommender Evaluation

## Quick Facts
- **arXiv ID:** 2508.05398
- **Source URL:** https://arxiv.org/abs/2508.05398
- **Reference count:** 40
- **Primary result:** Sampling strategy choice significantly impacts evaluation reliability, with bias-aware methods (WTD, Skew) showing superior performance, especially under high sparsity.

## Executive Summary
This paper evaluates the reliability of sampling strategies for offline recommender system evaluation across four dimensions: resolution (model separability), fidelity (agreement with full evaluation), robustness (stability under exposure bias), and predictive power (alignment with ground truth). Using the fully observed KuaiRec dataset as ground truth, the authors simulate various exposure biases and compare different sampling strategies under different sparsity levels. Their key finding is that no single sampling strategy excels across all reliability dimensions, with bias-aware methods like WTD and Skew showing superior performance, especially under high sparsity. The study demonstrates that larger sample sizes don't guarantee better evaluations if item selection is biased, and highlights the importance of considering multiple reliability metrics when evaluating sampling strategies.

## Method Summary
The authors propose a framework to assess sampling reliability using the KuaiRec dataset, which provides a fully observed ground truth matrix. They binarize interactions using a watch ratio threshold, simulate logging biases at various sparsity levels (0%-95%), and evaluate seven recommendation models (ALS, BPR, LightFM variants, SAR-Cosine, SAR-Jaccard, Popularity, Random) using negative sampling strategies. The framework compares model rankings from sampled evaluations against ground truth rankings using nDCG@100 as the primary metric, along with tie rates for resolution and Kendall's τ for fidelity, robustness, and predictive power. The study systematically varies sample sizes and logging biases to understand how different sampling strategies perform across reliability dimensions.

## Key Results
- Bias-aware sampling strategies (WTD, Skew) outperform uniform and popularity-based methods, particularly under high sparsity conditions
- Increasing sample size does not monotonically improve evaluation reliability; resolution degrades at both very small and very large sample sizes
- No single sampling strategy excels across all reliability dimensions, necessitating trade-offs between fidelity and predictive power

## Why This Works (Mechanism)

### Mechanism 1: Distribution Alignment via Bias-Aware Sampling
Bias-aware sampling strategies align the negative sample distribution with the underlying exposure or popularity distributions, reducing contamination with "false negatives" and stabilizing metric estimates. This works when the logging policy's bias is recoverable and correlates with ground truth behavior.

### Mechanism 2: Resolution Optimization via Sample Size Calibration
Intermediate sample sizes (R ≈ 100-200) optimize the signal-to-noise ratio for model separability. Very small samples are noisy, while very large samples compress metric scores through uninformative negatives, increasing tie rates between models.

### Mechanism 3: Reliability Decomposition
By decomposing reliability into four dimensions, the framework exposes trade-offs between methods that score high on fidelity (matching biased logs) versus predictive power (matching ground truth). This is crucial when logs themselves are biased.

## Foundational Learning

- **Concept:** **Missing Not At Random (MNAR) & Exposure Bias**
  - **Why needed here:** The paper relies on simulating MNAR data from fully observed data; understanding that users only rate what they see is essential for distinguishing logged interactions from ground truth preferences.
  - **Quick check question:** If a user never saw an item, is the lack of interaction a negative signal or a missing data point?

- **Concept:** **Tie Rate vs. Statistical Power**
  - **Why needed here:** The paper uses "tie rate" as a proxy for resolution; you must distinguish between detecting an effect and practically separating model scores.
  - **Quick check question:** Does a low tie rate always mean a better sampler, or could it imply high variance/noise?

- **Concept:** **Negative Sampling Strategies (Uniform vs. Popularity vs. WTD)**
  - **Why needed here:** The core experimental variable is how negatives are selected; understanding the difference between random and "hard" negatives is essential for interpreting results.
  - **Quick check question:** Why might sampling popular items as negatives be a "harder" test for a recommender than sampling random items?

## Architecture Onboarding

- **Component map:** Ground Truth (O) -> Logger -> Logged Data (P) -> Sampler -> Evaluation Sets (P_N) -> Evaluator -> Meta-Evaluator
- **Critical path:** The generation of P_N from P, where the choice of sampler and sample size directly determines the predictive power of offline evaluation.
- **Design tradeoffs:**
  - **Resolution vs. Fidelity:** High resolution (low ties) often comes from moderate sampling, while high fidelity requires massive sampling.
  - **Exposed vs. Random:** Evaluating only on "Exposed" items is robust to false negatives but sensitive to logger bias; evaluating on "Random" items is unbiased but introduces noise.
  - **WTD/Skew vs. Popularity:** WTD/Skew are computationally slightly more complex but offer better predictive power than naive Popularity sampling.
- **Failure signatures:**
  - **High Tie Rate:** Models appear indistinguishable; likely caused by sample size being too small (<10) or too large (Full).
  - **Low Kendall's τ:** Offline rankings don't match ground truth; likely caused by using "Popularity" sampling in sparse, popularity-biased datasets.
  - **High Fidelity, Low Predictive Power:** You're perfectly measuring the wrong thing (the biased logs).
- **First 3 experiments:**
  1. **Baseline Establishment:** Run the framework on your own data or a public dataset; compare "Full" vs "Random@100" to establish the resolution/fidelity gap.
  2. **Sparsity Sensitivity:** Subsample your training data to simulate 50% and 90% sparsity; observe if "Skew" or "WTD" maintains predictive power better than "Random" as sparsity increases.
  3. **Logger Robustness:** Swap the logger simulation (e.g., from Uniform to Popularity-biased); verify if the chosen sampler maintains stable rankings or drifts significantly.

## Open Questions the Paper Calls Out

- **Can learned sampling strategies adaptively select evaluation candidates based on exposure patterns or model uncertainty to outperform static heuristic methods?**
  - The authors identify this as a promising direction, noting that current research relies on static strategies while a dynamic, learning-based approach has not been operationalized.

- **Do the reliability rankings of sampling strategies generalize to domains beyond short-video recommendation and datasets with different interaction densities?**
  - The authors note the limited availability of fully observed datasets and suggest applying the framework to other domains to validate generality.

- **How does the choice of sampling strategy distort evaluation outcomes for non-accuracy objectives, such as fairness and user trust?**
  - The paper explicitly lists fairness and user trust as objectives for future work to extend analysis beyond ranking accuracy.

## Limitations

- The framework assumes the fully observed matrix is a valid ground truth for user preferences, but this assumes all relevant items were exposed during logging.
- Hyperparameter optimization details are not fully specified, limiting exact reproduction.
- The study focuses on a single dataset (KuaiRec), raising questions about generalizability to other domains or data distributions.

## Confidence

- **High confidence** in the mechanism that bias-aware sampling (WTD, Skew) improves predictive power under high sparsity, consistently supported across experimental conditions.
- **Medium confidence** in the optimal sample size range (R ≈ 100-200) for maximizing resolution, as this appears dataset-dependent and may vary with different model families.
- **Low confidence** in the universality of trade-offs between reliability dimensions, as patterns may be influenced by the particular logging simulation method used.

## Next Checks

1. Test the framework on a different fully observed dataset (e.g., Yahoo! R3) to verify generalizability of sampling strategy rankings.
2. Implement alternative logging simulations (e.g., position-based exposure) to assess robustness of conclusions to different bias types.
3. Conduct ablation studies varying model complexity (e.g., matrix factorization vs. deep learning) to determine if sampling strategy effectiveness depends on model class.