---
ver: rpa2
title: 'SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs'
arxiv_id: '2509.25390'
source_url: https://arxiv.org/abs/2509.25390
tags:
- spatial
- object
- reasoning
- rotation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SpinBench is a cognitively grounded diagnostic benchmark for evaluating\
  \ spatial reasoning in vision-language models. The benchmark targets perspective\
  \ taking\u2014the ability to reason about scene changes under viewpoint transformations\u2014\
  by decomposing it into fine-grained diagnostic tasks including identity matching,\
  \ object-relation grounding, dynamic translation/rotation, canonical view selection,\
  \ mental rotation, and perspective taking."
---

# SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs

## Quick Facts
- arXiv ID: 2509.25390
- Source URL: https://arxiv.org/abs/2509.25390
- Reference count: 40
- Primary result: SpinBench benchmark reveals systematic spatial reasoning weaknesses in VLMs, with top models achieving only 69.8% accuracy versus human performance of 91.2%

## Executive Summary
SpinBench introduces a cognitively grounded benchmark for evaluating spatial reasoning in vision-language models through perspective taking and rotation tasks. The benchmark decomposes spatial reasoning into six diagnostic tasks including identity matching, object-relation grounding, dynamic translation/rotation, canonical view selection, mental rotation, and perspective taking. Tested across 37 state-of-the-art VLMs using real-world and synthetic datasets of cars, faces, and household objects, SpinBench reveals fundamental gaps in VLM spatial reasoning capabilities.

Evaluation shows VLMs exhibit strong egocentric bias in rotation tasks, poor performance on mental rotation and perspective taking, and inconsistencies under symmetrical and syntactic variations. While top models achieve 69.8% overall accuracy with 95.7% consistency, human subjects significantly outperform them at 91.2% accuracy. Response time correlates strongly with task difficulty for both humans (r=-0.54) and VLMs. Chain-of-thought prompting helps with complex transformations but not all tasks. These findings highlight critical limitations in current VLM spatial reasoning and provide actionable insights for developing more capable embodied AI systems.

## Method Summary
SpinBench is a comprehensive diagnostic benchmark that evaluates spatial reasoning in vision-language models through perspective taking tasks. The benchmark decomposes spatial reasoning into six fine-grained tasks: identity matching (determining if two views show the same object), object-relation grounding (assessing relationships under viewpoint changes), dynamic translation/rotation (tracking object transformations over time), canonical view selection (choosing optimal viewing angles), mental rotation (imagining object rotations), and perspective taking (reasoning about scene changes from different viewpoints).

The evaluation uses three object categories (cars, faces, household objects) with both real-world and synthetic datasets. Each task includes controlled variations such as reference frame changes, syntactic augmentations, and symmetrical transformations to systematically probe model capabilities. The benchmark employs a strict matching-based evaluation protocol and tests 37 state-of-the-art VLMs. Human performance is established using 12 participants for baseline comparison. The framework is designed to identify systematic weaknesses in spatial reasoning rather than just overall accuracy, providing granular insights into specific failure modes.

## Key Results
- Top VLMs achieve only 69.8% accuracy on SpinBench versus human performance of 91.2%
- VLMs show strong egocentric bias in rotation tasks and poor performance on mental rotation and perspective taking
- Response time correlates with task difficulty (r=-0.54 for humans) and scaling analysis reveals emergent capabilities for identity matching and dynamic translation
- Chain-of-thought prompting helps with complex transformations but not all tasks, highlighting fundamental gaps in VLM spatial reasoning

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its cognitively grounded approach to spatial reasoning, which mirrors human cognitive development stages. By decomposing complex spatial reasoning into fine-grained diagnostic tasks, SpinBench isolates specific capabilities that VLMs struggle with, such as mental rotation and perspective taking. The systematic variation of reference frames, syntactic structures, and symmetrical transformations creates controlled conditions that reveal fundamental limitations in how models process spatial information. The correlation between response time and task difficulty provides an additional diagnostic dimension, showing that both humans and models struggle more with certain transformations, validating the benchmark's design.

## Foundational Learning
- **Perspective taking**: The ability to reason about scene changes under different viewpoints - needed because it's fundamental to spatial reasoning in embodied AI; quick check: test model's ability to track objects across viewpoint transformations
- **Mental rotation**: Imagining object rotations without physical movement - critical for navigation and object manipulation; quick check: compare performance on rotated vs. canonical views
- **Reference frame transformations**: Converting between egocentric and allocentric coordinate systems - essential for understanding spatial relationships; quick check: vary frame of reference in identical spatial configurations
- **Syntactic reasoning**: Understanding spatial relationships through linguistic structure - bridges vision and language modalities; quick check: test performance with varied sentence structures describing same spatial arrangement
- **Canonical view selection**: Identifying optimal viewing angles for object recognition - important for scene understanding; quick check: measure accuracy when optimal vs. non-optimal views are presented
- **Dynamic transformations**: Tracking objects through continuous rotation/translation - crucial for real-time spatial reasoning; quick check: test temporal reasoning across sequential viewpoint changes

## Architecture Onboarding

**Component map**: VLM architecture -> Vision encoder -> Text encoder -> Cross-modal fusion -> Spatial reasoning head -> Output classification

**Critical path**: Input images and text queries flow through vision and text encoders, merge at cross-modal fusion layer, pass through spatial reasoning module, and produce classification output. The spatial reasoning head is critical as it determines how well the model handles perspective transformations.

**Design tradeoffs**: Single-stream vs. dual-stream architectures affect how spatial information is processed; transformer-based models offer better cross-modal fusion but may struggle with spatial transformations compared to specialized spatial modules; trade-off between general reasoning capabilities and task-specific spatial modules.

**Failure signatures**: Egocentric bias appears as consistent errors when reference frames shift; mental rotation failures show as performance drops on non-canonical views; perspective taking errors manifest as inability to track objects across viewpoint changes; syntactic variation failures indicate weak language-vision grounding.

**3 first experiments**:
1. Test identity matching task with canonical vs. non-canonical views to establish baseline spatial reasoning capability
2. Evaluate dynamic rotation task with varying degrees of rotation to map performance degradation curves
3. Compare egocentric vs. allocentric reference frame performance to quantify reference frame transformation capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark scope is limited to perspective taking and rotation-based transformations, potentially missing other important spatial reasoning aspects like path planning and occlusion handling
- Human baseline comparison based on small sample (12 participants) may not represent population-level performance across demographics
- Generalizability of findings depends on specific tasks and models evaluated, with confidence varying across different aspects of the analysis

## Confidence
- High confidence in core empirical results about model performance gaps and rigorous experimental methodology
- Medium confidence in interpretations of egocentric bias and scaling analysis, which depend on specific task configurations
- Low confidence in generalizability of response time correlations across different model architectures and training regimes

## Next Checks
1. Evaluate SpinBench across broader range of model architectures, including those specifically trained for spatial reasoning, to determine if weaknesses are universal or architecture-dependent
2. Conduct cross-cultural human studies to verify whether reported human baseline performance generalizes beyond current participant pool
3. Test model performance after targeted finetuning on spatial reasoning tasks to distinguish fundamental limitations from artifacts of standard pretraining approaches