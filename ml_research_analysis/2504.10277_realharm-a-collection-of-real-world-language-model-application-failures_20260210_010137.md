---
ver: rpa2
title: 'RealHarm: A Collection of Real-World Language Model Application Failures'
arxiv_id: '2504.10277'
source_url: https://arxiv.org/abs/2504.10277
tags:
- systems
- content
- language
- user
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RealHarm is a dataset of 136 annotated real-world incidents involving
  language model application failures, systematically collected from public sources.
  The dataset categorizes failures by type, impact, and cause, revealing that misinformation
  constitutes the most common hazard while reputational damage dominates organizational
  harms.
---

# RealHarm: A A Collection of Real-World Language Model Application Failures

## Quick Facts
- arXiv ID: 2504.10277
- Source URL: https://arxiv.org/abs/2504.10277
- Authors: Pierre Le Jeune; Jiaen Liu; Luca Rossi; Matteo Dora
- Reference count: 15
- Primary result: Dataset of 136 real-world LLM application failures with systematic annotation and evaluation of moderation systems

## Executive Summary
RealHarm is a dataset of 136 annotated real-world incidents involving language model application failures, systematically collected from public sources. The dataset categorizes failures by type, impact, and cause, revealing that misinformation constitutes the most common hazard while reputational damage dominates organizational harms. Evaluation of ten moderation systems shows they detect only 10-50% of unsafe content with varying false positive rates, highlighting significant gaps in current safeguards. State-of-the-art LLMs with taxonomy-based prompts outperform traditional moderation systems. The dataset provides an evidence-based framework for assessing AI deployment risks, complementing theoretical approaches by grounding safety research in documented incidents rather than speculative scenarios.

## Method Summary
The RealHarm dataset was constructed through systematic collection of LLM application failures from public sources between 2022-2025. Each incident was annotated with detailed categorization of harm type, impact severity, affected parties, and root causes. The annotation process involved domain experts who classified incidents using a standardized taxonomy covering misinformation, bias, privacy violations, and other failure modes. The dataset was then used to evaluate ten different moderation systems, including both traditional content filters and LLM-based approaches with taxonomy-based prompting. Performance metrics included detection rates, false positive rates, and comparison across different harm categories.

## Key Results
- Misinformation represents the most common hazard type in real-world LLM failures
- Reputational damage constitutes the dominant organizational harm impact
- Current moderation systems detect only 10-50% of unsafe content across tested approaches
- LLMs with taxonomy-based prompts outperform traditional moderation systems in detection accuracy

## Why This Works (Mechanism)
The effectiveness of RealHarm stems from its grounding in actual documented incidents rather than hypothetical scenarios, providing empirical evidence for AI safety research. The systematic annotation framework enables consistent categorization and comparison across diverse failure modes. The evaluation methodology directly tests moderation systems against real failure patterns, revealing practical gaps in current safeguards. The taxonomy-based prompting approach leverages LLMs' understanding of nuanced contexts that traditional rule-based systems miss.

## Foundational Learning
1. **Real-world incident documentation** - Understanding actual failure patterns in deployed systems
   - Why needed: Theoretical scenarios often miss practical deployment challenges and edge cases
   - Quick check: Compare documented incidents against known deployment contexts

2. **Systematic harm categorization** - Standardized framework for classifying AI failures
   - Why needed: Enables consistent analysis and comparison across diverse incidents
   - Quick check: Verify inter-annotator agreement on harm classifications

3. **Multi-stakeholder impact assessment** - Evaluating effects on users, organizations, and society
   - Why needed: AI failures have cascading consequences across different parties
   - Quick check: Map each incident's impact across stakeholder dimensions

4. **Moderation system benchmarking** - Empirical evaluation of safety guardrails
   - Why needed: Theoretical safety claims require validation against real failures
   - Quick check: Test systems on held-out incidents from different domains

5. **LLM-based safety evaluation** - Using language models to assess content safety
   - Why needed: Traditional filters struggle with nuanced context and ambiguity
   - Quick check: Compare LLM detection rates against rule-based baselines

6. **Taxonomy-guided prompting** - Structured prompting for consistent safety assessment
   - Why needed: Unstructured prompts lead to inconsistent safety judgments
   - Quick check: Evaluate consistency across different annotators using same taxonomy

## Architecture Onboarding

**Component Map:** Public incident sources -> Incident collection -> Expert annotation -> Harm categorization -> Moderation system testing -> Performance evaluation

**Critical Path:** Incident collection → Expert annotation → System evaluation → Performance analysis

**Design Tradeoffs:** Breadth vs depth in incident coverage (136 incidents provide good diversity but may miss domain-specific patterns), expert annotation quality vs scalability (high quality but resource-intensive), traditional vs LLM-based moderation (simplicity vs context understanding)

**Failure Signatures:** Incomplete documentation of incidents, subjective bias in harm classification, overfitting of moderation systems to specific incident patterns, underestimation of false positive costs

**3 First Experiments:**
1. Test taxonomy-based LLM prompts on a separate validation set of incidents not used in initial evaluation
2. Conduct ablation study removing different annotation categories to assess their impact on system performance
3. Evaluate moderation systems specifically on incidents from underrepresented domains in the current dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset covers only 136 incidents, potentially missing diverse failure modes across all domains
- Incidents collected from public sources between 2022-2025, introducing selection bias toward visible cases
- Categorization framework involves subjective judgment in determining incident severity and impact classification

## Confidence

**Dataset construction methodology and incident annotation:** High
**Characterization of harm types and their frequencies:** Medium (due to potential selection bias)
**Moderation system evaluation results:** Medium (limited system sample size)
**LLM with taxonomy-based prompts outperforming traditional moderation:** Medium (requires further validation)

## Next Checks

1. Expand incident collection to include non-public and earlier failures, particularly from enterprise deployments and healthcare applications, to assess whether the current harm distribution holds across broader contexts

2. Conduct longitudinal analysis to determine if reported failure rates and types change over time as systems improve and deployment patterns evolve

3. Test the taxonomy-based LLM approach against a more diverse set of incidents, including those outside the original evaluation set, to assess generalizability and identify edge cases where it may fail