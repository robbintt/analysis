---
ver: rpa2
title: Optimizing Blood Transfusions and Predicting Shortages in Resource-Constrained
  Areas
arxiv_id: '2506.12647'
source_url: https://arxiv.org/abs/2506.12647
tags:
- blood
- data
- bank
- type
- simulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This research addresses blood supply management challenges in
  resource-constrained regions through heuristic optimization and machine learning.
  The study developed a Cassandra NoSQL database-backed system featuring three progressively
  advanced simulations: randomized allocation (79% acceptance ratio), proximity-based
  optimization (85%), and a multi-heuristic approach incorporating rarity scores (89%).'
---

# Optimizing Blood Transfusions and Predicting Shortages in Resource-Constrained Areas

## Quick Facts
- arXiv ID: 2506.12647
- Source URL: https://arxiv.org/abs/2506.12647
- Authors: El Arbi Belfarsi; Sophie Brubaker; Maria Valero
- Reference count: 7
- 3 simulations improved acceptance ratio from 79% to 89%, with 47.6% marginal improvement

## Executive Summary
This research addresses blood supply management challenges in resource-constrained regions through heuristic optimization and machine learning. The study developed a Cassandra NoSQL database-backed system featuring three progressively advanced simulations: randomized allocation (79% acceptance ratio), proximity-based optimization (85%), and a multi-heuristic approach incorporating rarity scores (89%). The system integrates proximity-based selection, blood type compatibility, expiration prioritization, and rarity scoring to optimize blood allocation while maintaining high availability through the CAP theorem-compliant database architecture.

## Method Summary
The study developed a blood supply management system using Cassandra NoSQL database with four tables (blood banks, users, blood inventory, blood transactions). Three simulations were run over 30-day periods with 40-50 transactions per day: Simulation 1 used random allocation, Simulation 2 added proximity-based selection (closest 15% of banks), expiration prioritization, and blood type compatibility, while Simulation 3 incorporated rarity scores. Three predictive models (LSTM, Linear Regression, ARIMA) were trained on 170 days of acceptance ratio data to forecast day 180 acceptance. The system applies heuristics in stages - first proximity selection, then expiration prioritization, then rarity weighting - to progressively filter candidate blood units.

## Key Results
- Multi-level heuristic matching achieved 47.6% marginal improvement in blood request acceptance (89% vs baseline 79%)
- Total distance traveled reduced by 37.8% compared to baseline
- Linear Regression outperformed LSTM and ARIMA for shortage prediction with 1.40% average absolute percentage difference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level heuristic matching improves blood request acceptance rates by progressively incorporating domain constraints.
- Mechanism: The system applies heuristics in stages—first proximity-based selection (reducing distance), then expiration prioritization (minimizing waste), then rarity scores (conserving scarce blood types). Each additional constraint filters the candidate pool more intelligently than random allocation.
- Core assumption: Blood type distribution in the synthetic dataset (O+ 38%, A+ 34%, AB- 1%) reflects real-world scarcity patterns, and optimizing for rarity preserves resources that are statistically harder to replace.
- Evidence anchors: [abstract] "Moving from blind matching to a heuristic-based approach yielded a 28.6% marginal improvement... multi-level heuristic matching resulted in a 47.6% improvement." [section 6.2.1] Acceptance ratio improved from 79% → 85% → 89% across three simulations with z-scores indicating statistical significance (p < 0.05).

### Mechanism 2
- Claim: Linear Regression provides competitive short-term forecasting accuracy for blood bank acceptance ratios with lower computational overhead than LSTM or ARIMA.
- Mechanism: All three models were trained on 170 days of historical acceptance ratio data to predict day 180. Linear Regression captured linear trends effectively despite its simplicity, slightly outperforming more complex models on mean percentage difference.
- Core assumption: Acceptance ratio trends are approximately linear over 10-day horizons, and historical patterns generalize to near-future predictions.
- Evidence anchors: [abstract] "Linear Regression slightly outperformed others with a 1.40% average absolute percentage difference in predictions." [section 6.3] Table 6 shows Linear Regression (1.40%) < LSTM (1.48%) < ARIMA (1.83%). All models struggled with outliers like Bank 15 (5-6% error).

### Mechanism 3
- Claim: Cassandra's AP (Availability + Partition Tolerance) architecture ensures system continuity during network disruptions critical for emergency blood supply coordination.
- Mechanism: Cassandra's multi-master replication and eventual consistency model allow reads/writes to continue even when nodes are partitioned. The schema uses four tables (blood banks, users, blood inventory, blood transactions) optimized for query patterns via CQL.
- Core assumption: Slight staleness in inventory data is acceptable in exchange for guaranteed availability during emergencies.
- Evidence anchors: [abstract] "Our solution leverages a Cassandra NoSQL database... CAP theorem-compliant database architecture." [section 5.2.2] "Cassandra's multi-master architecture ensures continued functionality even if some nodes fail, unlike traditional SQL databases."

## Foundational Learning

- Concept: Blood Type Compatibility Matrix
  - Why needed here: The heuristic matching algorithm must know which blood types can safely transfuse to which recipients (O- is universal donor, AB+ is universal recipient).
  - Quick check question: Can A+ blood be given to an O+ patient? (Answer: No—O+ can only receive O+ or O-.)

- Concept: CAP Theorem
  - Why needed here: Understanding why Cassandra was chosen over SQL databases requires knowing the tradeoff between consistency, availability, and partition tolerance.
  - Quick check question: Which two guarantees does Cassandra prioritize, and what does it sacrifice? (Answer: Availability and Partition Tolerance; sacrifices strong consistency for eventual consistency.)

- Concept: Marginal Performance (MP) Metric
  - Why needed here: The paper uses a specific formula to quantify relative improvement over baseline; understanding this enables proper interpretation of the 47.6% claim.
  - Quick check question: If baseline acceptance is 79% and final acceptance is 89%, what does MP = 47.6% mean in practical terms? (Answer: Nearly half of the "room for improvement" above baseline was captured.)

## Architecture Onboarding

- Component map:
  - Cassandra DB -> Simulator Module -> Heuristic Engine -> Prediction Module
  - 4 tables (blood_banks, users, blood_inventory, blood_transactions) -> Python-based simulator -> Proximity filter → Compatibility check → Expiration sort → Rarity weighting -> LSTM, Linear Regression, ARIMA models

- Critical path: Blood request received → proximity filter selects closest 15% of banks → compatibility matrix filters valid donors → expiration sort prioritizes soon-to-expire units → rarity score adjusts final selection → transaction logged to Cassandra.

- Design tradeoffs:
  - AP vs. CP: Chose availability over strong consistency for emergency resilience
  - Complexity vs. interpretability: Linear Regression selected over LSTM despite similar accuracy, for lower computational cost
  - Synthetic vs. real data: Used synthetic data to bypass HIPAA constraints; may not capture all real-world edge cases

- Failure signatures:
  - High prediction error on specific banks (e.g., Bank 15 with 5-6% error) indicates stochastic acceptance patterns the models cannot capture
  - Distance reduction plateaued at Simulation 2 (only 1.37% additional reduction in Simulation 3), suggesting diminishing returns from current heuristics

- First 3 experiments:
  1. Reproduce acceptance ratio improvement: Run all three simulations on the provided synthetic dataset and verify 79% → 85% → 89% progression matches reported values.
  2. Shortage prediction baseline comparison: Train LSTM, Linear Regression, and ARIMA on the 170-day training split; confirm Linear Regression achieves ≤1.50% mean percentage difference.
  3. Database query latency test: Execute the sample CQL queries (Figures 8-9) against a local Cassandra instance with the schema loaded; measure response times under simulated network partition to verify AP behavior.

## Open Questions the Paper Calls Out

- Would an integrated model that combines shortage prediction with heuristic matching improve acceptance rates compared to the current independent approach? The paper states: "we aim to create an integrated model that combines shortage prediction with heuristic matching. This combined approach will be thoroughly assessed to determine its impact on acceptance rates."

- How would the optimization algorithms and prediction models perform when validated on real-world blood bank data rather than synthetic datasets? The abstract states: "Future developments will incorporate real-world data and additional variables to improve prediction accuracy and optimization performance."

## Limitations

- The synthetic dataset may not capture real-world complexity including last-minute donation variability, geographic barriers, and unexpected demand spikes.
- Performance metrics are based on simulated rather than operational deployment, leaving unknown the impact of real-world constraints like donor availability windows and transportation logistics.
- The 170-day training window may be insufficient to capture annual seasonality patterns in blood donation and demand.

## Confidence

- Multi-level heuristic matching improvement (89% acceptance): High confidence - supported by statistically significant progression across three simulations with clear z-scores
- Linear Regression prediction accuracy (1.40% error): Medium confidence - based on synthetic data; real-world accuracy may differ with actual blood bank patterns
- CAP theorem implementation: Medium confidence - architectural claims are sound but operational validation during actual network partitions is absent

## Next Checks

1. Deploy system in a pilot blood bank network with 3-5 facilities to measure real-world acceptance ratio improvement versus simulated performance
2. Conduct stress testing with variable donation patterns (seasonal spikes, unexpected shortages) to validate heuristic robustness
3. Implement A/B testing comparing the full heuristic system against current manual allocation practices to quantify operational impact on blood wastage and delivery times