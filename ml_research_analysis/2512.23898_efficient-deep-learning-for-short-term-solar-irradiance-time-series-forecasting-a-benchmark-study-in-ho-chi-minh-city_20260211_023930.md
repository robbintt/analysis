---
ver: rpa2
title: 'Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting:
  A Benchmark Study in Ho Chi Minh City'
arxiv_id: '2512.23898'
source_url: https://arxiv.org/abs/2512.23898
tags:
- solar
- forecasting
- page
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study benchmarks ten deep learning architectures for short-term\
  \ Global Horizontal Irradiance (GHI) forecasting in Ho Chi Minh City using NSRDB\
  \ satellite data. The Transformer architecture achieved the highest accuracy with\
  \ an R\xB2 of 0.9696, outperforming classical models like LSTM and TCN."
---

# Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City

## Quick Facts
- **arXiv ID:** 2512.23898
- **Source URL:** https://arxiv.org/abs/2512.23898
- **Reference count:** 12
- **Primary result:** Transformer achieved R² of 0.9696 for 1-hour GHI forecasting in Ho Chi Minh City, outperforming other deep learning architectures.

## Executive Summary
This study benchmarks ten deep learning architectures for short-term Global Horizontal Irradiance (GHI) forecasting in Ho Chi Minh City using NSRDB satellite data. The Transformer architecture achieved the highest accuracy with an R² of 0.9696, outperforming classical models like LSTM and TCN. SHAP analysis revealed that Transformers exhibit recency bias, focusing on immediate atmospheric conditions, while Mamba leverages 24-hour periodic dependencies. Knowledge Distillation compressed the Transformer by 23.5% while reducing MAE to 23.78 W/m², enabling efficient deployment on resource-constrained edge devices.

## Method Summary
The study compares ten deep learning architectures for 1-hour ahead GHI forecasting using a 24-hour lookback window on NSRDB Himawari-7 satellite data (2011-2020) for Ho Chi Minh City. Key preprocessing includes Min-Max scaling, Clearsky GHI calculation, cyclical time encoding, and nighttime masking. The Transformer (2 layers, 4 heads, d_model=128) achieved the best performance (R² 0.9696). Knowledge Distillation was used to compress the model by 23.5% while improving accuracy (MAE 23.78). Evaluation focused on daytime samples to avoid trivial zero-bias from nighttime data.

## Key Results
- Transformer achieved R² of 0.9696, outperforming LSTM (R² 0.9681) and TCN (R² 0.9691)
- Knowledge Distillation reduced model size by 23.5% while improving MAE from 24.26 to 23.78 W/m²
- Mamba model generated negative R² values at night, indicating hallucination of non-zero irradiance
- Structured pruning caused severe accuracy collapse, increasing MAE to 176 W/m²

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Transformer achieves superior accuracy for 1-hour forecasting by exploiting a strong "recency bias," relying almost exclusively on immediate atmospheric conditions rather than long-term history.
- **Mechanism:** The self-attention mechanism assigns the highest weights to the most recent time step ($t-0$) while effectively ignoring earlier steps in the 24-hour lookback window. This suggests that for short-term horizons, current cloud type and solar geometry are strictly more informative than historical trends.
- **Core assumption:** The state of the atmosphere changes slowly enough that $t-0$ is a sufficient proxy for the state at $t+1$.
- **Evidence anchors:**
  - [abstract] SHAP analysis revealed that Transformers exhibit recency bias, focusing on immediate atmospheric conditions.
  - [section 5.7.2] Figure 12(b) shows Transformer feature importance is concentrated at $t-0$, with negligible values for $t-23$ to $t-1$.
  - [corpus] Neighbor paper *19232 supports the importance of immediate "Clearsky" synchronization for short-term forecasting.
- **Break condition:** This mechanism may degrade if the forecast horizon extends significantly (e.g., to 24-hours), where historical periodicity becomes more critical than immediate state.

### Mechanism 2
- **Claim:** Providing a theoretical "Clearsky GHI" baseline allows the model to separate predictable geometric variance from stochastic meteorological variance.
- **Mechanism:** Instead of learning the raw GHI value (which fluctuates wildly), the model learns the deviation from the calculated physical maximum ($GHI_{clearsky}$). This reduces the complexity of the loss landscape, allowing the optimizer to converge on the residual noise (clouds) rather than the primary signal (sun position).
- **Core assumption:** The Clearsky model is accurate enough to serve as a stable baseline; errors in the baseline would propagate into the model's residual predictions.
- **Evidence anchors:**
  - [section 3.3.3] Clearsky GHI helps the model distinguish between irradiance drops caused by geometry vs. cloud cover.
  - [section 5.7.1] SHAP analysis identifies clearsky_ghi (specifically at $t-23$ for Mamba) as a dominant feature.
  - [corpus] Paper *19232 ("On the Importance of Clearsky Model...") confirms that synchronization with Clearsky models is critical for handling short-term variability.
- **Break condition:** Fails if aerosol or pollution levels are extremely high and not accounted for in the simplified Clearsky equation ($I_0 \cdot \cos(z) \cdot 0.7$), leading to an incorrect baseline.

### Mechanism 3
- **Claim:** Knowledge Distillation (KD) acts as a powerful regularizer, allowing a smaller "Student" model to outperform both the "Teacher" and a student trained from scratch.
- **Mechanism:** The Student model minimizes the divergence (KL-divergence) from the Teacher's soft probability distributions rather than just the ground truth error. These "soft targets" contain implicit information about the similarity between classes (generalization) that hard labels lack, acting as a regularizer that prevents overfitting.
- **Core assumption:** The Teacher model has successfully captured generalizable patterns that are not merely overfitting to noise in the training data.
- **Evidence anchors:**
  - [abstract] Knowledge Distillation compressed the Transformer by 23.5% while reducing MAE to 23.78 W/m².
  - [section 6.3] The Distilled Student achieved lower error (23.78) than the Teacher (24.26) and the student trained from scratch (25.35).
  - [corpus] No direct corpus support for this specific "better-than-teacher" result in solar forecasting; this is a unique finding of this paper.
- **Break condition:** If the Teacher model is poorly calibrated or highly overfitted, distillation will propagate these errors to the Student.

## Foundational Learning

- **Concept: Cyclical Time Encoding (Sin/Cos)**
  - **Why needed here:** Standard integer time encoding (0-23) creates a discontinuity between "23:00" and "00:00". Since solar irradiance is continuous, the model needs to understand that these times are adjacent.
  - **Quick check question:** If you input Hour as a raw integer [0, 1, ..., 23], what numerical error does the model see between 23:59 and 00:01?

- **Concept: Nighttime Masking**
  - **Why needed here:** GHI is trivially zero at night. Training on 12 hours of zeros biases the model to predict zero, degrading daytime accuracy.
  - **Quick check question:** Why would including nighttime data artificially inflate your $R^2$ score while ruining your actual energy generation forecasts?

- **Concept: State Space Models (SSMs/Mamba) vs. Attention**
  - **Why needed here:** Transformers scale quadratically ($O(L^2)$), which is fine for short windows but expensive for long contexts. Mamba scales linearly ($O(L)$).
  - **Quick check question:** Why did the quadratic Transformer still run faster than the linear Mamba in this specific study? (Hint: Check the sequence length $L=24$ vs. kernel overhead).

## Architecture Onboarding

- **Component map:** NSRDB Satellite Data (2011-2020) -> Min-Max Scaling -> Cyclical Encoding -> Sliding Window ($L=24$) -> Core Model (Transformer/Mamba) -> SHAP Analysis -> Knowledge Distillation
- **Critical path:** The **Clearsky GHI calculation** and **Nighttime Masking** are the most critical preprocessing steps. If you skip masking, the model optimizes for the trivial zero-case. If you skip Clearsky GHI, the model struggles to differentiate cloud cover from sunset.
- **Design tradeoffs:**
  - **Transformer vs. TCN:** TCN offers high throughput (644k samples/sec) and competitive accuracy ($R^2$ 0.9691). Transformer offers peak accuracy ($R^2$ 0.9696) but lower throughput (239k samples/sec).
  - **Compression:** Avoid Structured Pruning (causes severe accuracy collapse). Use Knowledge Distillation (improves accuracy while shrinking size).
- **Failure signatures:**
  - **Mamba at Night:** The Mamba model generated negative $R^2$ values at night (hallucinating non-zero irradiance), whereas Transformer/TCN remained stable.
  - **Pruning:** Structured Pruning at 50% sparsity spiked MAE from ~24 to ~176 W/m².
- **First 3 experiments:**
  1. **Baseline Verification:** Train LSTM vs. Transformer on the 2011-2018 split using only raw GHI. Verify that the Transformer overfits without the Clearsky baseline feature.
  2. **Mechanism Validation (SHAP):** Replicate the SHAP analysis on a trained Transformer. Confirm if the attention weights are strictly concentrated at $t-0$ (recency bias) or if they utilize the full 24-hour context.
  3. **Compression Test:** Apply Knowledge Distillation from the Transformer (Teacher) to a smaller MLP (Student). Measure if the Student's MAE drops below 24.0 W/m² as claimed in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do Mamba's linear scaling properties provide superior efficiency and accuracy compared to Transformers when extending the forecast horizon to 24 or 48 hours?
- Basis in paper: [explicit] The authors state an aim to extend forecasting horizons and hypothesize that Mamba's $O(L)$ complexity may outperform the quadratic complexity of Transformers on the longer sequences required for day-ahead predictions.
- Why unresolved: The study found Mamba underperformed Transformers in the short-term ($L=24$) regime due to GPU overheads; it is unknown if this trend reverses for long-sequence tasks ($L > 1000$).
- What evidence would resolve it: Comparative benchmarks of MAE and inference latency for 24–48 hour horizons using the same NSRDB dataset.

### Open Question 2
- Question: How do the benchmarked architectures generalize across diverse climatic zones (e.g., arid vs. monsoonal) to support a national-scale foundation model?
- Basis in paper: [explicit] The authors propose evaluating model transferability across Vietnam's diverse latitudes and terrains to develop a universal foundation model.
- Why unresolved: The current study is geographically constrained to the specific tropical microclimate of Ho Chi Minh City.
- What evidence would resolve it: Performance metrics (R², MASE) of the Ho Chi Minh City-trained models when tested on out-of-sample data from different climatic regions without retraining.

### Open Question 3
- Question: Does integrating high-dimensional dynamic data, such as real-time Aerosol Optical Depth (AOD) and raw satellite imagery, significantly refine predictive precision over tabular features?
- Basis in paper: [explicit] The authors suggest future iterations should enrich the input feature space with AOD for urban pollution and raw imagery for cloud texture, rather than relying solely on tabular derivatives.
- Why unresolved: It is unclear if the added computational cost of processing high-dimensional image data yields a statistically significant reduction in forecasting error compared to the current tabular approach.
- What evidence would resolve it: Ablation studies comparing the current Transformer performance against models trained with additional AOD and satellite image channels.

## Limitations
- Mamba model generated negative R² values at night, indicating it hallucinates non-zero irradiance during darkness
- Structured pruning caused severe accuracy collapse, increasing MAE from ~24 to ~176 W/m²
- The study is geographically constrained to Ho Chi Minh City, limiting generalizability to other climatic zones

## Confidence
- **High Confidence:** Transformer's superior performance (R² 0.9696) and its recency bias as identified by SHAP analysis
- **Medium Confidence:** Effectiveness of Knowledge Distillation in improving upon the Teacher model's performance
- **Low Confidence:** Exact conditions under which Mamba generated negative R² values at night

## Next Checks
1. **Baseline Verification:** Train LSTM vs. Transformer on the 2011-2018 split using only raw GHI. Verify that the Transformer overfits without the Clearsky baseline feature.
2. **Mechanism Validation (SHAP):** Replicate the SHAP analysis on a trained Transformer. Confirm if the attention weights are strictly concentrated at t-0 (recency bias) or if they utilize the full 24-hour context.
3. **Compression Test:** Apply Knowledge Distillation from the Transformer (Teacher) to a smaller MLP (Student). Measure if the Student's MAE drops below 24.0 W/m² as claimed in the paper.