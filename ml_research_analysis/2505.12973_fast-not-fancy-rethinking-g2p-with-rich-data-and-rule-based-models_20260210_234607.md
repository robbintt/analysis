---
ver: rpa2
title: 'Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models'
arxiv_id: '2505.12973'
source_url: https://arxiv.org/abs/2505.12973
tags:
- dataset
- homograph
- data
- persian
- disambiguation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the dual challenges of homograph disambiguation
  in grapheme-to-phoneme (G2P) conversion for low-resource languages: the scarcity
  of balanced homograph datasets and the need for fast, real-time solutions. The authors
  propose a semi-automated pipeline for constructing homograph-focused datasets, resulting
  in the HomoRich dataset for Persian, which contains 528,891 sentences covering 285
  homograph words.'
---

# Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models

## Quick Facts
- **arXiv ID**: 2505.12973
- **Source URL**: https://arxiv.org/abs/2505.12973
- **Reference count**: 40
- **Primary result**: Combines large homograph datasets with lightweight statistical disambiguation to achieve neural-level accuracy with rule-based speed

## Executive Summary
This paper addresses the dual challenges of homograph disambiguation in grapheme-to-phoneme (G2P) conversion for low-resource languages: the scarcity of balanced homograph datasets and the need for fast, real-time solutions. The authors propose a semi-automated pipeline for constructing homograph-focused datasets, resulting in the HomoRich dataset for Persian, which contains 528,891 sentences covering 285 homograph words. They demonstrate that this rich dataset can significantly improve both neural and rule-based G2P models. Specifically, fine-tuning a state-of-the-art neural model (GE2PE) on HomoRich yields a 29.72% improvement in homograph disambiguation accuracy. Additionally, they develop a lightweight statistical method that enhances the widely used eSpeak rule-based engine, resulting in HomoFast eSpeak, which achieves a 30.66% improvement in homograph accuracy while maintaining real-time performance. The work shows that high-quality offline datasets can effectively bridge the gap between accuracy and speed in G2P systems.

## Method Summary
The authors propose a hybrid approach combining large-scale dataset construction with two distinct model improvements. First, they create the HomoRich dataset through a semi-automated pipeline using human annotators and LLM-generated sentences (primarily GPT-4o with Finglish prompts). Second, they develop two homograph disambiguation methods: a three-phase curriculum fine-tuning approach for neural models (GE2PE) and a lightweight statistical context-word overlap method for rule-based engines (eSpeak). The statistical method builds a pronunciation-to-context-word mapping from the dataset and uses normalized overlap scoring at inference, achieving sub-millisecond latency while maintaining neural-level accuracy.

## Key Results
- HomoRich dataset contains 528,891 sentences covering 285 Persian homograph words
- Fine-tuning GE2PE on HomoRich improves homograph accuracy by 29.72% (to 76.89%)
- HomoFast eSpeak achieves 74.53% homograph accuracy with 0.0084s inference time (30.66% improvement)
- Statistical method maintains real-time performance (53x faster than neural inference) with near-neural accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A hybrid human-LLM pipeline can produce balanced, large-scale homograph datasets at practical cost.
- Mechanism: LLMs generate sentences conditioned on target homograph pronunciations via few-shot prompting (using human-written examples); human annotators provide diversity and quality control. Finglish (an accessible phonemic representation) is used instead of IPA to improve LLM labeling accuracy.
- Core assumption: LLMs possess sufficient phonetic knowledge to generate contextually appropriate sentences and phonemize them with acceptable accuracy (paper reports 6.43% PER, 64% homograph accuracy for GPT-4o).
- Evidence anchors:
  - [abstract] "We propose a semi-automated pipeline for constructing homograph-focused datasets, resulting in the HomoRich dataset for Persian, which contains 528,891 sentences covering 285 homograph words."
  - [Section 3.1] "We adopted a hybrid approach, combining manual and LLM-generated sentences... we used some of these human-written examples as few-shot prompts to guide LLM-based sentence generation."
  - [corpus] Limited direct corpus validation; related work (arXiv:2505.06599) addresses Persian G2P challenges but uses a different intermediate-language approach.
- Break condition: If LLM phonetic knowledge is insufficient for the target language (e.g., truly low-resource languages with minimal pretraining data), labeling quality may degrade below usable thresholds.

### Mechanism 2
- Claim: A lightweight statistical context-word overlap method can provide homograph disambiguation without neural inference overhead.
- Mechanism: From the dataset, build a mapping from each homograph pronunciation to its frequent co-occurring context words (after stopword removal). At inference, compute weighted overlap between input sentence's context words and each pronunciation's context list, normalizing by list length to reduce bias. Select pronunciation with highest normalized score.
- Core assumption: Homograph pronunciations have sufficiently distinct contextual word distributions that simple co-occurrence statistics can discriminate them.
- Evidence anchors:
  - [abstract] "a lightweight statistical method that enhances the widely used eSpeak rule-based engine, resulting in HomoFast eSpeak, which achieves a 30.66% improvement in homograph accuracy while maintaining real-time performance."
  - [Section 3.2.2] "This strategy is purely statistical and does not rely on neural models or even embeddings... For a new sentence, we compute a weighted overlap between its context words and each pronunciation's context list to derive a similarity score."
  - [corpus] No direct corpus evidence for this specific statistical method; related approaches (Yarowsky 1997 decision lists, Gorman et al. 2018 hybrid systems) use contextual features but with different architectures.
- Break condition: If homograph meanings share highly similar context words (e.g., domain-specific synonyms), overlap-based disambiguation will fail.

### Mechanism 3
- Claim: Three-phase curriculum-style fine-tuning improves neural homograph disambiguation over single-phase training.
- Mechanism: Progressive fine-tuning starts with general G2P data, then LLM-generated homograph sentences, finally human-authored high-quality homograph sentences. This curriculum allows the model to first learn general phonotactics before specializing in disambiguation.
- Core assumption: Curriculum ordering matters; exposing the model to progressively higher-quality homograph data prevents overfitting to noisy LLM-generated samples.
- Evidence anchors:
  - [Section 3.2.1] "We further fine-tuned GE2PE on our dataset using a three-phase process: 1. Initial fine-tuning on the regular G2P subset 2. Second-phase fine-tuning on LLM-generated homograph sentences 3. Final fine-tuning on high-quality, human-authored homograph sentences."
  - [Figure 7] Shows learning curves across all three phases with decreasing validation loss.
  - [corpus] No corpus validation for this specific curriculum approach.
- Break condition: If the initial general G2P phase is too short or the final human-curated phase is too small, the model may underfit or overfit respectively.

## Foundational Learning

- Concept: **Homographs and heterophones**
  - Why needed here: The entire paper addresses words with identical spelling but different pronunciations (e.g., English "read" as /rEd/ vs /ri:d/). Without this concept, the problem formulation is unintelligible.
  - Quick check question: In the sentence "The wind was too strong to wind the clock," how many homograph instances require disambiguation?

- Concept: **Grapheme-to-Phoneme (G2P) conversion**
  - Why needed here: The core task is converting written text to phoneme sequences (IPA or similar). Understanding the input-output specification is essential.
  - Quick check question: What is the IPA transcription for "I will read it" according to the paper's example?

- Concept: **Rule-based vs neural inference latency tradeoff**
  - Why needed here: The paper's central contribution is achieving neural-level accuracy with rule-based speed. Table 3 shows HomoFast eSpeak at 0.0084s vs Homo-GE2PE at 0.4473s—a ~53x speed difference.
  - Quick check question: Why are screen readers particularly sensitive to G2P latency compared to offline TTS applications?

## Architecture Onboarding

- Component map:
  Homograph selection module → Sentence generation pipeline → LLM phonemizer → [Branch A: Neural trainer] OR [Branch B: Statistical disambiguator → eSpeak integration]

- Critical path: Homograph selection → Sentence generation → Phonemization → [Branch A: Neural fine-tuning] OR [Branch B: Statistical database construction → eSpeak integration]

- Design tradeoffs:
  - **Accuracy vs speed**: Homo-GE2PE achieves 76.89% homograph accuracy (0.4473s); HomoFast eSpeak achieves 74.53% (0.0084s). The ~2% accuracy gap buys ~53x speedup.
  - **Data quality vs cost**: Human-only annotation is expensive; LLM-only risks systematic errors. The hybrid approach trades some quality (6.43% PER) for scale (528K sentences).
  - **Augmentation diversity vs coherence**: Aggressive augmentation (10x scaling) may introduce unnatural sentences; conservative augmentation preserves quality.

- Failure signatures:
  - If statistical disambiguator defaults to most frequent pronunciation → check context list balance (Figure 5 should show near-equal distribution)
  - If neural model has low homograph accuracy despite fine-tuning → verify phase ordering and epoch counts; early phases may need more epochs
  - If Ezafe constructions are corrupted during augmentation → POS tagger (Hazm, 99.249% accuracy) may be failing; add manual validation

- First 3 experiments:
  1. **Baseline replication**: Run original eSpeak and GE2PE on SentenceBench (400 sentences) to confirm reported baselines (43.87% and 47.17% homograph accuracy).
  2. **Ablation on statistical method**: Remove normalization by context-list length; measure if bias toward frequent pronunciations increases.
  3. **Data efficiency test**: Train Homo-GE2PE with only 10% of HomoRich data; compare homograph accuracy degradation to assess data-quality vs data-quantity contribution.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the proposed lightweight statistical method be adapted to resolve the Persian Ezafe linking phoneme without compromising real-time latency?
- Basis in paper: [explicit] Section 6 (Limitations) explicitly identifies Ezafe phonemization as a major context-sensitive weakness in current rule-based systems that requires further research.
- Why unresolved: The current statistical method relies on word overlap for homograph context; Ezafe requires detecting grammatical relations between adjacent words, which may need a different linguistic feature set.
- What evidence would resolve it: An extension of the HomoFast eSpeak system that successfully detects Ezafe constructions with high accuracy while maintaining inference speeds comparable to the baseline eSpeak.

### Open Question 2
- Question: Does the semi-automated dataset generation pipeline effectively transfer to low-resource languages with significantly different morphological structures?
- Basis in paper: [inferred] The abstract and introduction generalize the solution to "low-resource languages," but the methodology and evaluation are strictly limited to Persian.
- Why unresolved: The pipeline relies on specific properties like the existence of extensive dictionaries (KaamelDict) and the ability of GPT-4o to handle the specific language's phonetics, which may not hold for all low-resource languages.
- What evidence would resolve it: Replicating the HomoRich dataset creation pipeline for a non-Persian, low-resource language (e.g., a tonal language or one with limited LLM training data) and benchmarking the resulting G2P improvements.

### Open Question 3
- Question: Can lightweight static word embeddings be integrated into the statistical disambiguation method to improve semantic understanding while preserving sub-millisecond latency?
- Basis in paper: [inferred] Section 3.2.2 notes the statistical method is "devoid of any neural components or learned embeddings," yet Contextual Word Embeddings (CWEs) are cited as the standard for neural approaches (Section 2.1).
- Why unresolved: It is unclear if the binary choice between "fast/rule-based" and "slow/neural" could be bridged by fast, non-contextual embeddings that capture more semantic nuance than simple word counts.
- What evidence would resolve it: An ablation study replacing the normalized word overlap score with a cosine similarity score based on pre-computed fastText embeddings, measured against the current latency constraints.

## Limitations
- Dataset quality uncertainty: The reported 6.43% PER for LLM phonemization represents significant error propagation risk, with quality assessment relying on expert annotators rather than gold-standard transcriptions.
- Generalizability constraint: Methodology is demonstrated only for Persian, a language with relatively standardized orthography and moderate resource availability; effectiveness for truly low-resource languages remains unverified.
- Statistical method fragility: The context-word overlap approach assumes homograph pronunciations have sufficiently distinct contextual distributions, which may fail in domain-specific text or when homographs appear in similar contexts.

## Confidence
- **High confidence** in the core finding that rich, homograph-focused datasets improve both neural and rule-based G2P models. The empirical results show consistent improvements across multiple models and metrics.
- **Medium confidence** in the specific mechanisms: while the three-phase fine-tuning curriculum shows improved performance, the contribution of each phase is not individually quantified through ablation studies.
- **Low confidence** in the approach's scalability and generalizability beyond Persian. The LLM-generated data quality (6.43% PER) and reliance on sufficient pretraining data for the target language suggest potential failure modes in truly low-resource scenarios.

## Next Checks
1. **Ablation study on training phases**: Train Homo-GE2PE models skipping each phase individually (no initial G2P phase, no LLM-generated phase, no human-curated phase) to quantify each phase's contribution to the final 76.89% homograph accuracy.

2. **Cross-linguistic transferability test**: Apply the exact pipeline to another language with different orthographic properties (e.g., Turkish with vowel harmony, or a genuinely low-resource language like Kurdish) and measure degradation in dataset quality and model performance.

3. **Statistical method robustness evaluation**: Test HomoFast eSpeak on domain-shifted text (scientific literature, social media, poetry) to measure performance degradation and identify failure patterns when homographs appear in atypical contexts.