---
ver: rpa2
title: 'Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier
  Vision-Language Models'
arxiv_id: '2501.14818'
source_url: https://arxiv.org/abs/2501.14818
tags:
- data
- arxiv
- dataset
- zhang
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present Eagle 2, a vision-language model (VLM) that
  achieves state-of-the-art performance by focusing on data strategy, training recipe,
  and model architecture. They build a diverse and high-quality dataset with 180+
  sources, filter low-quality samples, and apply advanced data selection and augmentation
  techniques.
---

# Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models

## Quick Facts
- arXiv ID: 2501.14818
- Source URL: https://arxiv.org/abs/2501.14818
- Reference count: 40
- Achieves 73.5 average score across 13 benchmarks, matching or surpassing models with up to 70B parameters

## Executive Summary
Eagle 2 presents a state-of-the-art vision-language model (VLM) that achieves top performance through innovative data strategy, training methodology, and architecture. The work emphasizes transparency in VLM development, sharing detailed methodologies to benefit the open-source community. By building a diverse dataset from 180+ sources and applying advanced filtering and selection techniques, Eagle 2-9B achieves an average score of 73.5 across 13 benchmarks, demonstrating that thoughtful data strategy can match or exceed the performance of much larger models.

## Method Summary
Eagle 2 employs a three-stage training strategy that separates modality alignment, large-scale pre-training, and high-quality instruction refinement. The model uses a tiled mixture of vision encoders (MoVE) combining SigLIP and ConvNeXt through channel concatenation with dynamic tiling support. Data strategy follows a "diversity first, then quality" principle, collecting 180+ sources across 9 categories before applying sophisticated filtering and balanced subset selection via K-means clustering. The training recipe includes balanced greedy data packing to prevent length distribution imbalances, and careful normalization of data formats to prevent spurious template learning.

## Key Results
- Eagle 2-9B achieves 73.5 average score across 13 benchmarks
- Matches or surpasses models with up to 70B parameters
- MoVE architecture improves OCRBench from 842 to 868 and ChartQA from 85.9 to 86.4
- Balanced data packing and three-stage training contribute significantly to final performance

## Why This Works (Mechanism)

### Mechanism 1: Three-Stage Training Strategy
- **Claim:** Three-stage training with Stage-1.5 (intermediate large-scale pre-training) improves model foundation and accelerates data iteration
- **Mechanism:** Stage-1 trains only the MLP connector for modality alignment. Stage-1.5 exposes full model to 21.6M diverse samples, building robust visual-language representations. Stage-2 refines with 4.6M high-quality instruction data. This separation allows Stage-2 insights to inform Stage-1.5 updates.
- **Core assumption:** The model benefits from seeing all data categories during pre-training, not just captioning/knowledge data
- **Evidence anchors:** Table 6 shows Stage-1.5 alone achieves 69.7 average score; adding Stage-2 yields 70.9
- **Break condition:** If your data pool is small (<2M samples), Stage-1.5 may not provide sufficient coverage

### Mechanism 2: Diversity-First Data Strategy
- **Claim:** Prioritizing data diversity first, then applying quality filtering and subset selection, yields better VLMs than starting with a small high-quality set
- **Mechanism:** Collect 180+ sources across 9 categories. Filter low-quality samples (mismatched QA, irrelevant pairs, repeated text, numeric formatting issues). Use K-means clustering on image embeddings for balanced subset selection rather than random sampling.
- **Core assumption:** Diverse data sources with redundant/low-quality samples are preferable to smaller curated sets
- **Evidence anchors:** Figure 2 shows progressive gains from 58.8 to 73.5; Table 6 shows data formatting/filtering improves OCRBench from 798 to 843
- **Break condition:** If your evaluation benchmarks are narrow (e.g., only natural images), extensive chart/OCR diversity provides diminishing returns

### Mechanism 3: Tiled Mixture of Vision Encoders (MoVE)
- **Claim:** Combining SigLIP with ConvNeXt via channel concatenation plus dynamic tiling improves document, chart, and OCR understanding
- **Mechanism:** Each image tile is encoded by both SigLIP (448×448) and ConvNeXt-XXL (512×512). SigLIP features are 2× downsampled via PixelShuffle to match ConvNeXt's 16×16 output. Features are concatenated and passed through MLP connector. Dynamic tiling supports resolutions up to {i×j | i,j ∈ Z+, i×j ≤ 12}.
- **Core assumption:** Different vision encoders capture complementary visual features; ConvNeXt contributes robustness beyond SigLIP's strengths
- **Evidence anchors:** Table 6 shows MoVE improves OCRBench from 842 to 868, ChartQA from 85.9 to 86.4
- **Break condition:** If inference latency is critical, the dual-encoder overhead may be prohibitive

## Foundational Learning

- **Vision-Language Alignment via Connector Training**
  - *Why needed here:* Stage-1 trains only the MLP connector to bridge pre-trained vision encoder outputs to the LLM embedding space before full-model training
  - *Quick check question:* Can you explain why freezing the LLM during connector training prevents catastrophic forgetting of language capabilities?

- **Data Packing / Knapsack Problem**
  - *Why needed here:* Packing multiple short samples into fixed-length sequences reduces padding waste. The paper shows naive greedy packing creates imbalanced length distributions that harm training
  - *Quick check question:* Why might grouping all long samples together and all short samples together create training instability?

- **Mixture of Experts/Encoders**
  - *Why needed here:* MoVE combines complementary vision backbones. Understanding feature fusion (concatenation vs. attention-based fusion) is essential for extending this design
  - *Quick check question:* What is the computational cost difference between channel concatenation and cross-attention for fusing encoder outputs?

## Architecture Onboarding

- **Component map:** Image → Dynamic Tiling → [SigLIP encoder + ConvNeXt encoder] → PixelShuffle (2× downsample SigLIP) → Channel Concatenation → MLP Connector → Qwen2.5 LLM backbone

- **Critical path:**
  1. Stage-1 (1.2M samples, ~2.5 hours on 128×H100): Train MLP connector only
  2. Stage-1.5 (21.6M samples, ~28 hours on 256×H100): Full model training with diverse data
  3. Stage-2 (4.6M samples, ~6 hours on 256×H100): Full model training with curated high-quality data

- **Design tradeoffs:**
  - MoVE (SigLIP + ConvNeXt) vs. single encoder: +performance on OCR/charts, +inference cost
  - Balanced packing vs. naive packing: +training stability, slightly lower packing efficiency
  - 3-stage vs. 2-stage: +final performance, +complexity and compute

- **Failure signatures:**
  - Model outputs fixed LaTeX equation templates → check data formatting for unnecessary decorations
  - OCRBench scores anomalously low → verify low-quality data filtering (Figure 5/6 examples)
  - Performance drops after adding new data source → compute similarity score to detect duplicates

- **First 3 experiments:**
  1. Reproduce Stage-1 baseline with ALLaVA (1.2M) training connector only; verify loss converges in ~2.5 hours
  2. Ablate balanced vs. naive packing on a 500K subset; measure training stability via loss variance and final benchmark delta
  3. Test MoVE vs. SigLIP-only on OCRBench subset; isolate encoder contribution by keeping data/recipe constant

## Open Questions the Paper Calls Out

### Open Question 1: Optimal Data Composition Ratios
- *Question:* What are the optimal data composition ratios for the Stage-1.5 pre-training phase across different VLM capability domains?
- *Basis in paper:* [explicit] The authors state "Other works tend to use more knowledge-related data, such as captioning data, at this stage. In this work, we add all data sources intended for visual instruction to Stage-1.5" but do not systematically ablate different compositions
- *Why unresolved:* Adding all data sources works empirically, but the relative importance of each category (Captioning, Science, Math, OCR QA, etc.) in Stage-1.5 remains unknown
- *What evidence would resolve it:* Controlled ablations varying the proportion of each data category in Stage-1.5 while measuring downstream task performance

### Open Question 2: Improving Similarity Score Metric
- *Question:* Can the Similarity Score metric be improved to effectively measure data diversity for natural scene images?
- *Basis in paper:* [explicit] The authors note that "K-means using SSCD image embeddings performs poorly on natural scene images, it excels with mathematical, medical, and document-based data"
- *Why unresolved:* SSCD embeddings were designed for copy detection, not semantic diversity, limiting their effectiveness for general-purpose VLM training data selection
- *What evidence would resolve it:* Development of new embedding methods or metrics that correlate with training utility across all image types, validated through ablation studies on natural scene datasets

### Open Question 3: Scaling Laws for VLM Post-Training Data
- *Question:* What are the scaling laws governing VLM post-training data, and where do diminishing returns or instabilities emerge?
- *Basis in paper:* [explicit] The authors observed that "beyond 10M samples... costs have risen sharply, and the efficiency of data iteration has decreased" with "considerable performance fluctuations across specific benchmarks at this scale, especially in challenging benchmarks like MMMU, MathVista, and MMVet"
- *Why unresolved:* The relationship between data scale, data quality, and model capability plateau points is not systematically characterized
- *What evidence would resolve it:* Large-scale controlled experiments varying dataset size while tracking per-benchmark performance curves and variance

### Open Question 4: Balanced Packing vs. Alternative Methods
- *Question:* How does the balanced data packing strategy compare to other sophisticated packing methods beyond naive greedy approaches?
- *Basis in paper:* [inferred] The authors compare only to "naive greedy knapsack algorithm" and mention SPFHP but state "our method prioritizes balanced length distribution over packing efficiency" without direct comparison
- *Why unresolved:* Trade-offs between packing efficiency, training speed, and model quality under different packing strategies remain unexplored
- *What evidence would resolve it:* Comparative experiments with SPFHP and other packing algorithms measuring both training throughput and final benchmark scores

## Limitations

- **Data pipeline opacity:** The paper lacks specific implementation details for filtering mismatched QA pairs and irrelevant image-question pairs at scale
- **Dataset availability:** Key internal datasets "Arxiv2Markdown" and "Textbooks-QA" are not publicly released
- **Compute requirements:** The three-stage training approach requires significant computational resources (28+ hours on 256×H100 for Stage-1.5 alone)

## Confidence

- **High Confidence:** The three-stage training methodology (connector-only → diverse pre-training → curated refinement) is well-documented and theoretically sound. The balanced data packing algorithm is explicitly described with pseudocode.
- **Medium Confidence:** The MoVE architecture combining SigLIP and ConvNeXt with dynamic tiling is clearly specified, though the exact contribution of each component to final performance remains partially speculative.
- **Low Confidence:** The data filtering and subset selection procedures lack critical implementation details. The paper describes what to filter but not exactly how to detect these issues at scale.

## Next Checks

1. **Data Filtering Validation:** Implement Eagle 2's filtering pipeline (mismatched QA detection, irrelevant pair filtering, repeated text removal, numeric formatting cleanup) and measure its impact on OCRBench performance compared to unfiltered data. This isolates the contribution of data quality improvements.

2. **Packing Algorithm Comparison:** Train Eagle 2-9B using both naive greedy packing and the balanced greedy algorithm on identical data subsets. Measure training stability (loss variance) and final benchmark performance to quantify the impact of balanced packing.

3. **MoVE Component Ablation:** Systematically compare SigLIP-only, ConvNeXt-only, and full MoVE configurations on OCRBench and ChartQA tasks while keeping all other variables constant. This validates whether the dual-encoder approach provides complementary benefits as claimed.