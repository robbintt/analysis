---
ver: rpa2
title: 'Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and
  Positional Fidelity'
arxiv_id: '2511.04686'
source_url: https://arxiv.org/abs/2511.04686
tags:
- cache
- tokens
- positional
- eviction
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of KV cache management in long-context,
  stateful LLM inference, specifically how cache eviction strategies impact generation
  quality when approaching or exceeding the model's pre-trained context window. The
  core insight is that preserving positional encoding integrity is as critical as
  retaining salient content; eviction strategies that compact non-contiguous token
  states disrupt RoPE positional signals, leading to severe degradation even with
  high retention (e.g., 99% AttentionTop).
---

# Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity

## Quick Facts
- arXiv ID: 2511.04686
- Source URL: https://arxiv.org/abs/2511.04686
- Authors: Pratik Poudel
- Reference count: 23
- One-line primary result: Simple eviction strategies preserving contiguous context blocks can outperform complex high-retention methods when operating near architectural context limits due to positional encoding integrity preservation.

## Executive Summary
This paper addresses the critical challenge of KV cache management in long-context, stateful LLM inference, specifically how cache eviction strategies impact generation quality when approaching or exceeding the model's pre-trained context window. The core insight is that preserving positional encoding integrity is as critical as retaining salient content; eviction strategies that compact non-contiguous token states disrupt RoPE positional signals, leading to severe degradation even with high retention (e.g., 99% AttentionTop). The study benchmarks Meta-Llama-3-8b-instruct on extended multi-turn dialogues and shows that exceeding the 8192-token context window causes model failure (e.g., prompt repetition, gibberish) regardless of GPU memory. It finds that simpler strategies preserving contiguous context blocks (e.g., retaining only the initial "gist" of 2000 tokens) can yield more coherent outputs than complex, high-retention strategies applied to stressed contexts.

## Method Summary
The study uses Meta-Llama-3-8b-instruct with ShareGPT dataset subset containing 30+ turn conversations adapted to exceed Llama 3's 8192-token context window. Stateful inference maintains past_key_values across turns with eviction triggered when cache exceeds 600MB (~5600 tokens). Three eviction strategies are tested: AttentionTop (99% retention based on attention scores), SlidingWindowGist (first 2000 tokens only), and EvictOldest (FIFO). Generation quality is evaluated via GPT-4o judge (1-10 scale) measuring coherence/relevance/helpfulness, alongside KV cache size, TTFT, and throughput metrics.

## Key Results
- LLM generation quality degrades catastrophically when accumulated KV cache exceeds the model's trained context window (e.g., 8192 tokens for Llama 3), independent of GPU memory availability.
- Even high-retention eviction strategies (99% via AttentionTop) can worsen performance if they disrupt positional coherence through non-contiguous token removal.
- Simple strategies preserving contiguous context blocks (e.g., keeping an initial "gist") can yield more coherent generations than complex or positionally disruptive ones when operating near architectural limits.

## Why This Works (Mechanism)

### Mechanism 1: Context Window Exceedance Induces Positional Encoding Extrapolation Failure
- Claim: Generation quality degrades catastrophically when the accumulated KV cache exceeds the model's trained context window, independent of GPU memory availability.
- Mechanism: Positional encoding schemes like RoPE are optimized for specific sequence lengths during pre-training. When the effective cache length S exceeds this limit, rotational angles extrapolate beyond their trained range, corrupting the model's ability to interpret token order and relative distances.
- Core assumption: The degradation is primarily driven by positional encoding breakdown rather than attention capacity limits.
- Evidence anchors: [abstract] "LLM generation quality degrades sharply when the accumulated KV cache approaches or exceeds the model's trained context window (e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory exhaustion." [Section 5.1] Baseline at Turn 9/10 with >8800 tokens exhibited failure modes like repeating user prompts verbatim; by Turn 20 (>15000 tokens), responses degenerated into incoherent repetitive sequences.

### Mechanism 2: Non-Contiguous Eviction Scrambles Positional Coherence
- Claim: Eviction strategies that remove non-contiguous tokens from a long cache can harm generation quality even when retaining 99% of tokens, because the remaining cached states carry original positional encodings that become misleading when re-compacted.
- Mechanism: KV cache entries retain their original positional indices. When non-contiguous tokens are removed and the cache is compacted, adjacent entries in the modified cache may have vastly different original positions, creating conflicting positional signals for attention heads.
- Core assumption: The paper assumes the primary cause is positional disruption rather than loss of specific semantic content from the evicted 1%.
- Evidence anchors: [abstract] "Common eviction strategies, even high-retention ones (e.g., 99% via AttentionTop), can worsen performance if they disrupt positional coherence." [Section 5.3] AttentionTop with 99% retention applied to 8000+ token cache resulted in degenerated, repetitive output on subsequent turns. [Section 6] "Adjacent states in this modified cache may possess vastly different original positional indices, creating conflicting and misleading signals for the attention heads."

### Mechanism 3: Contiguous Block Preservation Maintains Positional Integrity
- Claim: Simple eviction strategies that preserve contiguous blocks of context (e.g., initial "gist" tokens) can outperform complex high-retention strategies when operating near architectural limits.
- Mechanism: By retaining a contiguous block, the original positional relationships among cached tokens remain intact, allowing the attention mechanism to process sequence information coherently even with reduced total context.
- Core assumption: The initial tokens contain sufficient foundational context for the task (may not hold for tasks requiring recent conversational history).
- Evidence anchors: [abstract] "Simple strategies preserving contiguous context blocks (e.g., keeping an initial 'gist') can yield more coherent generations than complex or positionally disruptive ones." [Section 5.4] SlidingWindowGist retaining only first 2000 tokens produced coherent Shark Tank pitch when baseline (>8000 tokens) failed completely.

## Foundational Learning

- Concept: **KV Cache in Autoregressive Generation**
  - Why needed here: Understanding that KV cache stores precomputed key/value projections from previous tokens to avoid O(t²) recomputation is essential for grasping why cache management matters.
  - Quick check question: Why does the KV cache grow linearly with sequence length rather than quadratically?

- Concept: **Rotary Positional Embeddings (RoPE)**
  - Why needed here: RoPE encodes position by rotating query/key embeddings based on absolute position, with attention scores depending on relative distances. This is the root cause of positional scrambling when non-contiguous eviction occurs.
  - Quick check question: If two cached tokens have original positions 100 and 5000, what happens to their positional relationship when tokens between them are evicted and the cache is compacted?

- Concept: **Prefill vs. Decoding Phases**
  - Why needed here: The prefill phase processes all user input tokens at once, causing sudden cache inflation before generation begins. This explains why eviction thresholds are often exceeded despite prior eviction.
  - Quick check question: At what point during a conversational turn does the KV cache experience its largest sudden size increase?

## Architecture Onboarding

- Component map: DynamicCache -> past_key_values storage -> Eviction Policies (AttentionTop, SlidingWindowGist, EvictOldest) -> Threshold Monitor (kv_threshold_mb) -> Phase Handler (Prefill → Generation)

- Critical path:
  1. Start of turn: Check if cache from previous turn exceeds threshold
  2. If exceeded, apply eviction policy
  3. Prefill phase: Process user input → cache size surges
  4. Generation phase: Produce tokens → cache grows incrementally
  5. End of turn: Record final cache state for next turn's evaluation

- Design tradeoffs:
  - Retention ratio vs. positional integrity: Higher retention doesn't guarantee better quality if contiguity is broken
  - Gist size vs. recency needs: Initial tokens provide foundation; recent tokens provide immediate context—both may be needed
  - Eviction frequency vs. latency: More aggressive eviction reduces memory but adds computational overhead

- Failure signatures:
  - Context exceedance: Prompt repetition, vacuous responses, character-level gibberish (Turn 20+ behavior)
  - Positional scrambling: Coherent initial output trailing into repetition after eviction applied to near-limit cache
  - Insufficient gist: Task-irrelevant responses when initial context lacks required information

- First 3 experiments:
  1. Establish baseline failure point: Run multi-turn conversation without eviction, record turn number and cache size when output quality collapses (expect ~8800+ tokens for Llama 3).
  2. Test positional disruption hypothesis: Compare AttentionTop (99% retention) vs. SlidingWindowGist (2000-token gist) on identical long conversations near context limit; measure coherence scores.
  3. Quantify prefill surge impact: Log cache size before prefill, after prefill, and after generation for each turn to identify where threshold-crossing occurs and whether prefill or generation is the primary driver.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can eviction strategies be developed that effectively balance initial "gist" retention, recency, and salience from the middle context while explicitly maintaining positional continuity?
- Basis in paper: The authors explicitly propose exploring "strategies that explicitly balance gist, recency, and (positionally-aware) salience from the middle context."
- Why unresolved: Current methods either prioritize salience (AttentionTop) at the cost of continuity or use simple rules (SlidingWindowGist) that discard the entire middle context.
- What evidence would resolve it: An eviction algorithm that successfully retains mid-conversation information in contiguous blocks without inducing the "positional scrambling" observed in attention-based methods.

### Open Question 2
- Question: Can LLMs be fine-tuned on post-eviction cache states to improve their robustness to context gaps and positional irregularities?
- Basis in paper: The authors list investigating "the impact of fine-tuning models on post-eviction cache states to improve their robustness to context gaps" as a direction for future work.
- Why unresolved: Current models rely on continuous positional encodings (RoPE) learned during pre-training; they fail when eviction disrupts this structure, suggesting a need for specialized adaptation.
- What evidence would resolve it: A model fine-tuned on fragmented cache sequences demonstrating higher coherence scores than the baseline when subjected to non-contiguous eviction strategies.

### Open Question 3
- Question: How can "positional disruption" be quantitatively measured to create eviction algorithms that minimize structural integrity loss?
- Basis in paper: The paper calls for "methods to quantify and minimize positional disruption during eviction."
- Why unresolved: The study qualitatively attributes performance degradation to scrambled positional signals but lacks a formal metric to measure this disruption during the eviction process.
- What evidence would resolve it: A defined metric for positional coherence that correlates strongly with generation quality, enabling algorithms to optimize for structural preservation alongside token salience.

## Limitations

- The study attributes generation failure to RoPE positional encoding breakdown but does not experimentally disentangle whether this is specifically due to positional extrapolation failure versus attention capacity saturation or other architectural limits.
- The evaluation focuses on three eviction strategies but does not explore hybrid approaches, re-indexing strategies, or models with different positional encoding schemes (e.g., ALiBi, local attention).
- Generation quality is assessed through GPT-4o judging with a 1-10 rubric, but the specific prompt template and scoring criteria are not provided, introducing potential variability.

## Confidence

**High confidence**: The observation that LLM generation quality degrades catastrophically when cache exceeds the trained context window is well-supported by empirical results showing prompt repetition and gibberish at Turn 20+ with >15000 tokens.

**Medium confidence**: The comparative performance of eviction strategies (SlidingWindowGist outperforming AttentionTop with 99% retention) is empirically demonstrated but may not generalize across different conversation types, task domains, or model architectures.

**Low confidence**: The assertion that positional encoding extrapolation is the primary failure mechanism (versus attention limits or other architectural constraints) remains speculative without direct experimental validation through controlled ablation studies or alternative positional encoding comparisons.

## Next Checks

1. **Disentangle positional versus attention capacity limits**: Run identical experiments with a model using ALiBi positional encoding (which extrapolates better) versus RoPE to determine if the failure mode changes. Compare against a variant with expanded attention capacity but same positional encoding to isolate which limit is binding.

2. **Validate positional scrambling hypothesis**: Implement a modified eviction strategy that re-indexes positional encodings post-eviction to maintain contiguity, then compare generation quality against the original AttentionTop strategy. If quality improves, this would confirm that positional scrambling (not semantic content loss) drives the degradation.

3. **Test recency versus gist importance across domains**: Repeat the SlidingWindowGist evaluation (2000-token initial block) across different conversation types—technical support, creative writing, factual Q&A—to determine if the "initial gist sufficiency" finding generalizes or is domain-specific. This would validate whether simple gist preservation is universally superior or context-dependent.