---
ver: rpa2
title: Can Past Experience Accelerate LLM Reasoning?
arxiv_id: '2505.20643'
source_url: https://arxiv.org/abs/2505.20643
tags:
- reasoning
- arxiv
- memory
- methods
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether LLMs can achieve reasoning speedup\
  \ through repeated exposure to similar tasks, analogous to human learning. The authors\
  \ formalize this problem across task similarity and compute budget dimensions, and\
  \ propose SpeedupLLM\u2014a framework combining adaptive compute allocation with\
  \ memory mechanisms."
---

# Can Past Experience Accelerate LLM Reasoning?

## Quick Facts
- arXiv ID: 2505.20643
- Source URL: https://arxiv.org/abs/2505.20643
- Authors: Bo Pan; Liang Zhao
- Reference count: 34
- Primary result: LLMs can reduce reasoning compute costs by up to 56% when equipped with appropriate memory and scaling strategies

## Executive Summary
This paper investigates whether Large Language Models can achieve reasoning speedup through repeated exposure to similar tasks, analogous to human learning. The authors formalize this problem across task similarity and compute budget dimensions, and propose SpeedupLLM—a framework combining adaptive compute allocation with memory mechanisms. Through extensive experiments across four test-time scaling methods, five memory methods, and four similarity levels, they demonstrate that LLMs can reduce reasoning compute costs by up to 56% when equipped with appropriate memory and scaling strategies.

## Method Summary
SpeedupLLM is a framework that combines adaptive compute allocation with memory mechanisms to accelerate LLM reasoning. The approach extends test-time scaling methods (Best-of-N, Tree-of-Thoughts, Self-Refine, Long CoT) with early stopping based on quality thresholds. Memory modules (No Memory, SFT, In-Context, Reflect variants) construct a memory from prior QA pairs that influences both the quality threshold satisfaction probability and candidate generation. The framework operates on a question stream with annotated similarity levels, aiming to minimize compute while maintaining answer quality.

## Key Results
- LLMs can reduce reasoning compute costs by up to 56% when equipped with appropriate memory and scaling strategies
- Episodic memory methods (In-Context: 27.4%, SFT: 10.8%) reduce compute budgets more effectively than semantic reflection-based methods (3.6%, 5.5%, 8.8%)
- Speedup effect is strongest for similar questions, with reductions most significant in S1 (16.0%) and S2 (15.4%) similarity groups

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Compute Budget Allocation via Early Stopping
The framework extends methods like Best-of-N and Tree-of-Thoughts with a quality threshold τ. Generation stops once `max(score(r)) ≥ τ`, minimizing the candidate set `|ℛ|` while preserving answer quality. Formally: `min cost(ℛ) s.t. max(score(r)) ≥ τ`. This works when the evaluation function `score(·)` reliably estimates true answer correctness.

### Mechanism 2: Memory-Augmented Reasoning Quality Improvement
Accumulating relevant past question-answer pairs increases the probability that early candidate answers exceed quality thresholds. Memory `M(t)` from prior QA pairs via function `g_m` is used in generation as `f_s'(q(t); M(t))`. If additional correct memory does not degrade performance, then `P(max score(r) ≥ τ)` is non-decreasing with time.

### Mechanism 3: Question Similarity Modulates Transfer Efficiency
Speedup magnitude scales with question similarity; near-duplicate questions yield largest compute reductions. Similarity levels S1–S4 define transfer distance. Episodic memory provides instance-based retrieval most effective at S1–S2; semantic reflection generalizes better at S3–S4 but with weaker magnitude.

## Foundational Learning

### Concept: Test-Time Scaling
- **Why needed here:** SpeedupLLM builds on existing scaling methods (Best-of-N, Tree-of-Thoughts, Self-Refine, Long CoT). Understanding their compute units (answers, nodes, tokens) is prerequisite to adaptive allocation.
- **Quick check question:** Can you explain why Best-of-N uses "number of sampled answers" as its cost metric while Tree-of-Thoughts uses "number of expanded nodes"?

### Concept: Episodic vs. Semantic Memory in LLMs
- **Why needed here:** The paper distinguishes episodic (In-Context, SFT) from semantic (reflection-based) memory. Episodic memory stores concrete QA pairs; semantic memory stores abstracted rules. Effectiveness differs by similarity level.
- **Quick check question:** Given a sequence of math problems with different numbers but identical structure (S3), would you expect In-Context or Reflect-Update to yield greater compute reduction, and why?

### Concept: Stochastic Dominance in Quality Distributions
- **Why needed here:** The theoretical analysis (Theorem 2 extension) relies on stochastic ordering of score distributions under memory augmentation. This formalizes "memory improves quality."
- **Quick check question:** If `P(score(r^{(t+1)}_j) ≥ τ) ≥ P(score(r^{(t)}_j) ≥ τ)` for all thresholds τ, what does this imply about expected compute cost over time?

## Architecture Onboarding

### Component map:
Question Stream -> Memory Module -> Test-Time Scaling Engine -> Scorer -> Compute Tracker

### Critical path:
1. Receive question `q(t)` with similarity annotation
2. Retrieve memory `M(t)` from Memory Module
3. Generate candidates via Scaling Engine with early stopping
4. If answer quality ≥ τ, terminate; log compute cost
5. Update memory with `(q(t), ℛ(t))` if answer is correct and high-quality

### Design tradeoffs:
- **In-Context vs. SFT:** In-Context provides stronger speedup (27.4% vs. 10.8%) but plateaus when context window fills (index≈3). SFT continues improving but requires gradient updates.
- **Reflection granularity:** Reflect-Update (generalizable rules) outperforms Reflect/Multi-Case (specific calculations) by avoiding overfitting to question-specific numbers.
- **Scaling method selection:** No single best method; DFS pairs well with episodic memory, Self-Refine/Long CoT pair better with reflection-based memory.

### Failure signatures:
- **Compute increases at low similarity:** Memory contains irrelevant examples; model overfits to dissimilar patterns. Mitigation: filter memory by similarity threshold or use Reflect-Update for generalization.
- **Accuracy drops despite speedup:** Score function `score(·)` misaligned with ground truth; early stopping truncates before correct answer. Mitigation: recalibrate τ or improve PRM quality.
- **Text-based memory plateaus:** Context window saturation. Mitigation: switch to parametric memory (SFT) or implement memory compression/summarization.

### First 3 experiments:
1. **Baseline calibration:** Run Best-of-N with No Memory on S1–S4 questions. Measure compute cost and accuracy per question index. This establishes the baseline non-decreasing compute trend.
2. **Memory ablation:** Compare In-Context, SFT, and Reflect-Update on S2 questions using Best-of-N. Log compute reduction percentage and verify early-stopping triggers. Expected: In-Context > SFT > Reflect-Update in magnitude.
3. **Scaling method interaction:** Test DFS vs. Self-Refine with In-Context memory on S3 questions. Compare compute reduction and accuracy. Expected: DFS yields larger compute reduction; Self-Refine may have higher accuracy ceiling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can memory mechanisms be specifically optimized to significantly reduce compute budgets for Long Chain-of-Thought (Long-CoT) reasoning models?
- Basis in paper: [explicit] The authors state in Section 6 (Limitations) that they "did not identify memory methods that significantly reduce the compute budget" for Long-CoT reasoning and leave the exploration of effective approaches to future work.
- Why unresolved: While speedup was observed in Best-of-N, Tree-of-Thought, and Self-Refine, the intrinsic complexity and implicit search behavior of Long-CoT models like DeepSeek-R1 did not yield efficiency gains with the tested memory strategies.
- What evidence would resolve it: A memory integration method that demonstrates a statistically significant reduction in token generation or compute cost for Long-CoT models without degrading accuracy on complex reasoning benchmarks.

### Open Question 2
- Question: How can in-context episodic memory be leveraged to sustain reasoning speedup without being constrained by the context window length?
- Basis in paper: [explicit] The authors list "exploring how to leverage in-context episodic memory while overcoming the limitations of context window length" as a specific direction for future work in Section 6.
- Why unresolved: The experiments (Finding 7) showed that text-based memory methods, such as In-Context learning, plateaus in efficiency gains once the context window becomes full (around index=3), whereas parametric memory (SFT) continues to improve.
- What evidence would resolve it: A technique (e.g., compression, retrieval-augmentation) that allows episodic memory to maintain the high early-stage speedup benefits over longer question sequences (index > 10) without exhausting the context budget.

### Open Question 3
- Question: How can reflection-based memory methods be designed to optimally balance generalizability with specific informativeness?
- Basis in paper: [explicit] In Finding 8, the authors note that "Reflect" and "Multi-Case Reflect" often contain specific calculations that lack generalizability, concluding that the "tradeoff between generalizability and informativeness needs to be considered in future work."
- Why unresolved: Reflections that are too specific risk misleading the model on new variations, while generic reflections may fail to provide the necessary guidance to reduce compute, a trade-off currently unresolved in the tested semantic memory methods.
- What evidence would resolve it: A reflection generation protocol that consistently produces abstract principles applicable to different numerical variations (S3/S4) while retaining the utility needed to lower the compute budget.

## Limitations

- The strongest empirical finding relies heavily on similarity annotations and the quality of the PRM scorer, with generalization to truly open-domain reasoning tasks remaining uncertain.
- The theoretical analysis assumes monotonic improvement in quality distributions with memory accumulation, but empirical results show this doesn't always hold at low similarity (S4).
- Memory can increase compute costs and degrade accuracy in low similarity cases (S4), suggesting the theoretical model has limitations in capturing negative transfer effects.

## Confidence

**High confidence (8-10/10):** The empirical observation that memory-augmented LLMs can reduce reasoning compute costs by up to 56% when question similarity is high. This is supported by extensive experiments across four test-time scaling methods, five memory methods, and four similarity levels with statistically significant results.

**Medium confidence (5-7/10):** The mechanism that episodic memory outperforms semantic reflection-based methods for compute reduction. While empirically demonstrated, the underlying reasons for this difference could involve factors beyond the paper's analysis, such as implementation details or specific task characteristics.

**Low confidence (1-4/10):** The theoretical extension of Theorem 2 to multiple memory construction functions. The proof assumes no degradation from additional memory, but the empirical results show this assumption breaks down at low similarity levels, suggesting the theoretical framework may be too optimistic.

## Next Checks

1. **Cross-domain transfer validation:** Test SpeedupLLM on reasoning tasks from completely different domains (e.g., mathematical reasoning applied to legal reasoning) to assess whether similarity annotations and memory mechanisms generalize beyond the curated datasets.

2. **Scorer alignment verification:** Conduct ablation studies where the PRM scorer is replaced with human judgment for a subset of questions to quantify the impact of scorer misalignment on early stopping decisions and resulting compute-accuracy tradeoffs.

3. **Memory saturation analysis:** Systematically test the interaction between context window limits and In-Context memory performance by varying the number of examples stored and measuring the point at which compute reduction plateaus or reverses, particularly for different similarity levels.