---
ver: rpa2
title: A Definition of AGI
arxiv_id: '2510.18212'
source_url: https://arxiv.org/abs/2510.18212
tags:
- ability
- memory
- illustrative
- examples
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive, quantifiable framework for
  defining and measuring Artificial General Intelligence (AGI) by operationalizing
  the concept as matching the cognitive versatility and proficiency of a well-educated
  adult. The methodology grounds its approach in Cattell-Horn-Carroll (CHC) theory,
  the most empirically validated model of human cognition, and systematically decomposes
  general intelligence into ten core cognitive domains, including reasoning, memory,
  and perception.
---

# A Definition of AGI

## Quick Facts
- arXiv ID: 2510.18212
- Source URL: https://arxiv.org/abs/2510.18212
- Reference count: 40
- Primary result: Operationalizes AGI as matching the cognitive versatility of a well-educated adult using CHC theory, revealing current models have critical deficits in long-term memory while scoring 27% (GPT-4) and 57% (GPT-5)

## Executive Summary
This paper presents a comprehensive framework for defining and measuring Artificial General Intelligence (AGI) by operationalizing the concept as matching the cognitive versatility and proficiency of a well-educated adult. The methodology grounds its approach in Cattell-Horn-Carroll (CHC) theory, the most empirically validated model of human cognition, and systematically decomposes general intelligence into ten core cognitive domains. Application of this framework to current AI systems reveals a highly "jagged" cognitive profile, with models proficient in knowledge-intensive domains but exhibiting critical deficits in foundational cognitive machinery, particularly long-term memory storage.

## Method Summary
The framework operationalizes AGI by adapting established human psychometric batteries to evaluate AI systems across ten cognitive domains based on CHC theory. It systematically tests capabilities including knowledge, reading/writing, math, reasoning, working memory, long-term storage, long-term retrieval, visual processing, auditory processing, and speed. The methodology uses a tiered scoring system with equal weighting across domains, requiring models to demonstrate proficiency across all areas rather than excelling in only a few. Specific constraints include disabling tools for reasoning/math tasks, enabling search only for current affairs, and implementing strict protocols for long-term memory tests through session resets.

## Key Results
- GPT-4 achieves an AGI score of 27%, while GPT-5 reaches 57%, quantifying the gap to human-level cognitive versatility
- Current models exhibit a "jagged" cognitive profile with strengths in knowledge domains but near-zero capability in long-term memory storage
- Long-term memory storage is identified as the most critical bottleneck, with models scoring 0% due to architectural contortions using context windows as compensation

## Why This Works (Mechanism)

### Mechanism 1: Psychometric Grounding via CHC Theory
- **Claim:** Established human psychometric frameworks provide a more rigorous evaluation baseline than task-specific benchmarks for defining AGI
- **Mechanism:** The framework maps the Cattell-Horn-Carroll (CHC) hierarchy of human cognitive abilities directly to AI evaluation domains, adapting clinical psychometric batteries to isolate specific cognitive components
- **Core assumption:** Human cognitive architecture (specifically the structure of "broad" and "narrow" abilities in CHC) is the appropriate structural blueprint for mapping artificial general intelligence
- **Evidence anchors:** The framework is explicitly grounded in CHC theory as "the most empirically validated model of human cognition" and neighbors like *Thinking Beyond Tokens* support the shift from performance metrics to cognitive foundations
- **Break condition:** If AI develops "alien" cognitive architectures that solve tasks via methods totally orthogonal to human cognition, the CHC mapping may become a mismatched proxy

### Mechanism 2: Diagnostic Disaggregation ("Jaggedness" Detection)
- **Claim:** Aggregate performance scores obscure critical deficits; disaggregating scores into specific domains reveals the "jagged" profile necessary for accurate diagnosis
- **Mechanism:** By assigning equal weights (10%) to ten distinct domains, the framework forces multi-dimensional evaluation that prevents high scores in "Knowledge" from masking near-zero scores in "Long-Term Memory Storage"
- **Core assumption:** General intelligence requires a baseline proficiency across all core domains; a deficit in one acts as a bottleneck for the system's overall "horsepower"
- **Evidence anchors:** The framework reveals GPT-4 scoring 8% in Knowledge but 0% in Memory Storage, illustrating the specific discrepancy the mechanism is designed to highlight
- **Break condition:** If specific cognitive deficits can be perfectly compensated for by other modalities without loss of generality, the strict separation of domains might overstate the "broken" nature of the system

### Mechanism 3: Identification of Capability Contortions
- **Claim:** Current high performance often relies on "contortions"—expensive compensatory strategies—rather than genuine cognitive capability
- **Mechanism:** The framework identifies specific architectural flaws by observing where models struggle with simple "human" tasks, such as using massive context windows to compensate for lack of long-term memory storage
- **Core assumption:** Efficiency and mechanism matter; solving a task via "retrieving from a massive prompt" is structurally distinct from "retrieving from stored experience"
- **Evidence anchors:** The paper identifies "a prominent contortion is the reliance on massive context windows (Working Memory) to compensate for the lack of Long-Term Memory Storage" and neighbors like *Beyond Turing: Memory-Amortized Inference* align with this reasoning
- **Break condition:** If context windows scale indefinitely and costlessly, the distinction between "working memory" and "long-term storage" becomes irrelevant for capability assessment

## Foundational Learning

- **Concept: Cattell-Horn-Carroll (CHC) Theory**
  - **Why needed here:** This is the theoretical substrate of the entire paper. Without understanding CHC (the hierarchy of *g*, broad abilities, and narrow abilities), the choice of the 10 specific domains appears arbitrary
  - **Quick check question:** Can you distinguish between "Fluid Reasoning" (Gf - solving novel problems) and "Crystallized Intelligence" (Gc - acquired knowledge) in the context of the paper's "On-the-Spot Reasoning" vs. "General Knowledge"?

- **Concept: The "Engine" Analogy / Bottlenecking**
  - **Why needed here:** The paper argues that general intelligence is constrained by the *weakest* component. You must understand why a system with 90% knowledge but 0% memory is considered "impaired" rather than just "specialized"
  - **Quick check question:** If an AI has a high AGI score (e.g., 90%) but 0% in Long-Term Memory Storage (MS), why does the paper classify this as "functionally impaired"?

- **Concept: Contamination & Distribution Shift**
  - **Why needed here:** The paper notes that AI corporations may "juice" numbers by training on test data. Understanding how psychometrics handles "novelty" is key to understanding why the paper tests "On-the-Spot Reasoning"
  - **Quick check question:** Why does the framework recommend testing model performance under "minor distribution shifts" or "rephrasing"?

## Architecture Onboarding

- **Component map:** Knowledge (K) -> Reading/Writing (RW) -> Math (M) -> Reasoning (R) -> Working Memory (WM) -> Memory Storage (MS) -> Memory Retrieval (MR) -> Visual (V) -> Auditory (A) -> Speed (S)
- **Critical path:** Focus evaluation efforts on Long-Term Memory Storage (MS) and On-the-Spot Reasoning (R), as MS is identified as the most critical bottleneck and R as a key differentiator of "general" vs. "specialized" intelligence
- **Design tradeoffs:**
  - Static vs. Dynamic Weighting: The paper assigns equal weight (10%) to prioritize breadth, sacrificing nuance for robustness against over-optimization on a single axis
  - Task Specification vs. Datasets: The framework defines capabilities by *task descriptions* rather than static datasets to prevent "solving the dataset" without solving the cognitive task
- **Failure signatures:**
  - The "Amnesiac" High-Scorer: A model with >80% total AGI score but <5% in Memory Storage indicates a "brittle" system that cannot learn over time
  - The Context-Window Crutch: A model that performs well on "Memory" tasks only when the prompt contains all necessary information (High WM, Zero MS)
  - Hallucination Cascades: High Retrieval Fluency (MR) but Zero Retrieval Precision, resulting in confident but false outputs
- **First 3 experiments:**
  1. **The "MS" Stress Test:** Implement the "Long-Term Memory Storage" battery. Test if a model can recall a novel 3000-word story after a "48-hour equivalent" delay (simulated by context flushing or new session)
  2. **Visual vs. Textual Induction:** Run the "On-the-Spot Reasoning" induction tests (Raven's Progressive Matrices) using both visual inputs and textual descriptions of patterns. Compare scores to see if reasoning is modality-dependent
  3. **Quantify the Contortion:** Measure inference cost of solving a logic puzzle via "in-context learning" (WM) versus "weight-updating" to quantify the "inefficiency" of the contortion

## Open Questions the Paper Calls Out
- How should the framework be adapted to be culturally and linguistically agnostic, given its current reliance on US-centric metrics may conflate cultural literacy with general cognitive proficiency?
- How sensitive is the "AGI Score" to alternative weighting schemes of the ten broad abilities, considering the current equal weighting may not adequately reflect which capabilities are most critical for functional generality?
- Does the factor structure of intelligence in AI systems mirror the inter-correlated hierarchy found in human CHC theory, or do AI systems develop non-human cognitive structures that perform equally or better without conforming to this model?

## Limitations
- The framework's reliance on CHC theory as the "correct" cognitive architecture for AGI remains unproven, as AI systems may develop non-human cognitive structures
- Private Raven's Progressive Matrices sets used for reasoning evaluation cannot be independently verified, limiting reproducibility of critical reasoning scores
- Human grader variability in subjective assessments (writing ability, fluency) introduces measurement uncertainty not quantified in reported scores

## Confidence
- **High Confidence**: Identification of Long-Term Memory Storage as critical bottleneck is strongly supported by empirical 0% scores across current models
- **Medium Confidence**: Equal-weighting approach provides robustness against optimization gaming, but assumption of equal contribution to general intelligence remains debatable
- **Low Confidence**: Specific numerical AGI scores depend heavily on private RPM sets and subjective grader consistency, making precise replication challenging

## Next Checks
1. **MS Isolation Test**: Implement controlled experiment where model must recall information after strict context reset to definitively separate Working Memory from Long-Term Memory Storage capabilities
2. **Cross-Modality Reasoning Validation**: Test same reasoning problems across multiple modalities to verify whether current "reasoning" scores reflect modality-dependent pattern matching rather than general reasoning ability
3. **Contortion Cost Quantification**: Measure computational overhead of context-window-based "memory" versus weight-based memory to empirically validate efficiency claims about architectural contortions