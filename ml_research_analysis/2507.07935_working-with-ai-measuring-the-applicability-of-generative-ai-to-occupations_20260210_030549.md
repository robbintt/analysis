---
ver: rpa2
title: 'Working with AI: Measuring the Applicability of Generative AI to Occupations'
arxiv_id: '2507.07935'
source_url: https://arxiv.org/abs/2507.07935
tags:
- user
- work
- occupations
- applicability
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study measures the applicability of generative AI to occupations
  using real-world usage data from Microsoft Bing Copilot. The researchers analyzed
  200,000 anonymized conversations to identify work activities assisted or performed
  by AI, mapping them to ONET's hierarchical occupation-task framework.
---

# Working with AI: Measuring the Applicability of Generative AI to Occupations

## Quick Facts
- arXiv ID: 2507.07935
- Source URL: https://arxiv.org/abs/2507.07935
- Reference count: 40
- Primary result: Analysis of 200,000 Bing Copilot conversations reveals AI applicability is highest for information work activities like communication and teaching, with most occupations showing some AI impact due to information components

## Executive Summary
This study develops an empirical framework for measuring generative AI's applicability to occupations by analyzing real-world usage data from Microsoft Bing Copilot. The researchers examined 200,000 anonymized conversations to identify work activities assisted or performed by AI, mapping them to O*NET's hierarchical occupation-task framework. The analysis reveals that AI is most commonly used for information work activities—creating, processing, and communicating information—with learning, communicating, teaching, and writing being the most prevalent user goals. The study introduces an AI applicability score that distinguishes between occupations where AI assists existing workflows versus those where tasks might be delegated to AI.

## Method Summary
The researchers collected and analyzed 200,000 anonymized conversations from Microsoft Bing Copilot to identify work activities assisted or performed by generative AI. They mapped these activities to O*NET's hierarchical occupation-task framework, categorizing user goals and analyzing which occupational tasks showed the highest AI applicability. The study focused on information work activities and developed metrics to distinguish between AI assistance (where humans remain in the loop) and AI delegation (where tasks are handed off to AI). The analysis examined patterns across different occupations to understand where AI provides the most value and where it faces limitations.

## Key Results
- AI is most commonly used for information work activities: creating, processing, and communicating information
- Communication and teaching tasks show highest AI performance, while image generation and data analysis present challenges
- Most occupations demonstrate some AI applicability due to information work components, though impact varies significantly
- The AI applicability score framework distinguishes between assistance (human-in-the-loop) and delegation (task transfer to AI) patterns

## Why This Works (Mechanism)
The study works by leveraging real-world usage data rather than hypothetical scenarios or expert assessments. By analyzing actual conversations between workers and AI systems, the researchers capture authentic patterns of AI adoption and identify genuine pain points and opportunities. The mapping to O*NET's established occupational framework provides a standardized way to compare AI applicability across different types of work, while the distinction between assistance and delegation captures the nuanced ways AI integrates into existing workflows.

## Foundational Learning

**O*NET Occupation-Task Framework**: A standardized taxonomy of occupations and their associated tasks, used to categorize and compare work activities across different jobs. Needed to provide consistent classification of occupational activities and enable cross-occupation comparisons. Quick check: Verify the O*NET codes used align with official occupational categories.

**AI Applicability Score**: A metric quantifying how well generative AI can assist with or perform occupational tasks. Needed to move beyond binary assessments of AI suitability to capture degrees of applicability and different interaction patterns. Quick check: Test the scoring algorithm on occupations with known AI integration patterns.

**Information Work Activities**: Tasks involving the creation, processing, and communication of information. Needed as the primary domain where generative AI shows highest applicability, distinguishing it from physical or manual work. Quick check: Confirm that identified information work activities align with established knowledge work classifications.

**Assistance vs. Delegation Distinction**: Classification of AI interactions as either supporting human workers (assistance) or taking over tasks entirely (delegation). Needed to capture the different ways AI integrates into workflows and predict potential impacts on employment. Quick check: Validate that observed usage patterns match the intended classification criteria.

## Architecture Onboarding

**Component Map**: Data Collection (Bing Copilot conversations) -> Task Classification (O*NET mapping) -> Applicability Scoring (assistance/delegation metrics) -> Occupational Analysis (impact assessment)

**Critical Path**: The analysis pipeline follows a linear progression from raw conversation data through classification to final applicability scores, with each stage building on the previous one's outputs.

**Design Tradeoffs**: The study prioritizes empirical real-world data over controlled experiments, accepting potential noise and bias in exchange for authentic usage patterns. The focus on information work trades completeness for depth in the most relevant domain for generative AI.

**Failure Signatures**: If the O*NET mapping is inaccurate, applicability scores will be systematically biased. If the conversation sample is not representative, results will overgeneralize from limited patterns. If the assistance/delegation distinction is unclear, the framework loses its predictive value.

**First Experiments**:
1. Replicate the analysis using a different AI platform's usage data to test generalizability
2. Apply the framework to a subset of occupations with known AI integration to validate scoring accuracy
3. Test the classification system on conversations from multiple industries to assess domain specificity

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Analysis relies exclusively on Microsoft Bing Copilot data, potentially missing patterns from other AI tools
- 200,000 conversations may not fully represent the diversity of workplace contexts or less common occupations
- Focus on information work activities may underrepresent physical, manual, or field-based occupations
- Classification of AI applicability is based on observed usage patterns rather than validated productivity or quality outcomes

## Confidence
- High confidence: Information work as primary domain for AI applicability; communication and teaching tasks as well-suited to AI assistance
- Medium confidence: AI applicability score framework's ability to distinguish assistance vs. delegation patterns
- Low confidence: Long-term impact predictions on occupations; assumption that current usage patterns will persist

## Next Checks
1. Replicate the analysis using usage data from multiple AI platforms to assess consistency across tools
2. Conduct longitudinal studies to track how AI applicability scores change as both AI capabilities and workplace practices evolve
3. Validate the framework through field studies measuring actual productivity impacts and quality outcomes in occupations identified as having high AI applicability