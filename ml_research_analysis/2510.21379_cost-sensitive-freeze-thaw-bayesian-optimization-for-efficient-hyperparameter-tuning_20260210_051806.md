---
ver: rpa2
title: Cost-Sensitive Freeze-thaw Bayesian Optimization for Efficient Hyperparameter
  Tuning
arxiv_id: '2510.21379'
source_url: https://arxiv.org/abs/2510.21379
tags:
- learning
- utility
- performance
- normalized
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces cost-sensitive freeze-thaw Bayesian optimization
  (CFBO) to address hyperparameter optimization (HPO) where users want to balance
  computational cost against performance improvement. The method introduces a utility
  function representing user preferences for this trade-off, which can be estimated
  from preference data.
---

# Cost-Sensitive Freeze-thaw Bayesian Optimization for Efficient Hyperparameter Tuning

## Quick Facts
- arXiv ID: 2510.21379
- Source URL: https://arxiv.org/abs/2510.21379
- Authors: Dong Bok Lee; Aoxuan Silvia Zhang; Byungjoo Kim; Junhyeon Park; Steven Adriaensen; Juho Lee; Sung Ju Hwang; Hae Beom Lee
- Reference count: 40
- Introduces CFBO framework that achieves 0.2-0.9 normalized regret improvements over baselines while balancing cost-performance trade-offs

## Executive Summary
This paper presents Cost-Sensitive Freeze-thaw Bayesian Optimization (CFBO), a framework for hyperparameter optimization that explicitly balances computational cost against performance improvement. The method introduces a utility function that represents user preferences for this trade-off, which can be estimated from preference data. CFBO employs a novel acquisition function based on expected utility improvement and an adaptive stopping criterion that terminates optimization when improvement becomes unlikely relative to cost. The approach also leverages transfer learning with LC mixup on Prior-Fitted Networks for learning curve extrapolation to enhance sample efficiency.

## Method Summary
CFBO introduces a cost-sensitive utility function that captures user preferences for trading off computational cost against performance improvement in hyperparameter optimization. The framework uses an acquisition function based on expected utility improvement to guide the search process, combined with an adaptive stopping criterion that terminates optimization when further improvements are unlikely to justify the computational cost. To improve sample efficiency, CFBO employs transfer learning through LC mixup on Prior-Fitted Networks for learning curve extrapolation, allowing the method to leverage prior optimization experiences. The approach is validated on three multi-fidelity HPO benchmarks, demonstrating significant improvements in achieving better cost-performance trade-offs compared to relevant baselines.

## Key Results
- CFBO achieves normalized regret improvements of 0.2-0.9 across different utility functions compared to baseline methods
- The method significantly outperforms relevant baselines in achieving better cost-performance trade-offs on three multi-fidelity HPO benchmarks
- CFBO maintains comparable performance to conventional HPO methods in traditional settings without explicit cost consideration
- The adaptive stopping criterion effectively terminates optimization when improvement becomes unlikely relative to cost

## Why This Works (Mechanism)
The framework works by explicitly modeling user preferences for cost-performance trade-offs through a utility function, rather than treating all evaluations equally regardless of computational cost. By incorporating expected utility improvement into the acquisition function, CFBO can intelligently balance exploration-exploitation with cost considerations. The adaptive stopping criterion prevents wasteful evaluations when marginal gains don't justify computational expense. Transfer learning through LC mixup on Prior-Fitted Networks enables more accurate learning curve predictions, reducing the number of full evaluations needed. This combination allows CFBO to make more informed decisions about which configurations to evaluate and when to stop, leading to better overall efficiency.

## Foundational Learning

**Bayesian Optimization**: A sequential model-based optimization technique for expensive black-box functions. Why needed: Forms the foundation for CFBO's optimization strategy. Quick check: Verify understanding of acquisition functions and surrogate models.

**Multi-fidelity HPO**: Optimization that leverages evaluations at different computational budgets (e.g., training for fewer epochs). Why needed: CFBO is validated on multi-fidelity benchmarks, requiring understanding of fidelity levels. Quick check: Confirm grasp of how partial evaluations inform full model predictions.

**Utility Theory**: Framework for quantifying preferences between different outcomes. Why needed: CFBO uses utility functions to encode user preferences for cost-performance trade-offs. Quick check: Ensure understanding of how utilities combine cost and performance metrics.

**Transfer Learning in HPO**: Using knowledge from previous optimization tasks to improve new ones. Why needed: CFBO employs transfer learning through Prior-Fitted Networks and LC mixup. Quick check: Verify understanding of how prior optimization data can accelerate new tasks.

**Learning Curve Extrapolation**: Predicting final performance from early training curves. Why needed: Critical for CFBO's sample efficiency through LC mixup. Quick check: Confirm understanding of how partial curves inform full performance predictions.

## Architecture Onboarding

**Component Map**: User Preferences -> Utility Function -> Expected Utility Improvement -> Acquisition Function -> Bayesian Optimization Loop -> Adaptive Stopping Criterion -> Transfer Learning (LC Mixup + Prior-Fitted Networks)

**Critical Path**: The core optimization loop flows from user preference estimation through utility function formulation, acquisition function computation, Bayesian optimization iterations, and adaptive stopping. Transfer learning components support early-stage predictions to improve sample efficiency.

**Design Tradeoffs**: The framework trades implementation complexity for better cost-performance trade-offs. The utility function formulation requires user preference data, which may not always be available. Transfer learning components add overhead but improve sample efficiency. The adaptive stopping criterion balances premature termination against computational waste.

**Failure Signatures**: Poor performance may result from incorrectly estimated utility functions, insufficient preference data, ineffective transfer learning when prior data is mismatched, or overly aggressive stopping criteria that terminate too early. The framework may also struggle when the cost-performance relationship is highly nonlinear or when preference data is noisy.

**First Experiments**: 1) Validate utility function estimation on synthetic preference data with known ground truth. 2) Test LC mixup learning curve extrapolation accuracy on benchmark datasets. 3) Evaluate CFBO's performance with varying amounts of transfer learning data to determine minimum effective dataset size.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The utility function formulation assumes users can provide preference data to estimate trade-off preferences, but practical scalability of this assumption is unexamined
- Transfer learning effectiveness depends on availability of relevant prior data, with unclear performance when such data is insufficient or mismatched
- Benchmark generalizability is limited, with validation only on three benchmarks raising concerns about performance across diverse HPO domains
- The comparison with existing cost-sensitive HPO methods could be more comprehensive to establish relative advantages

## Confidence
- Utility function formulation: Medium
- Performance improvements: Medium
- Transfer learning effectiveness: Medium
- Benchmark generalizability: Low

## Next Checks
1. Test CFBO on additional diverse HPO benchmarks beyond the three presented to verify robustness across different problem domains
2. Conduct ablation studies to isolate the contribution of individual components (LC mixup, adaptive stopping criterion) to overall performance
3. Evaluate CFBO's performance when preference data is noisy or incomplete to assess real-world applicability