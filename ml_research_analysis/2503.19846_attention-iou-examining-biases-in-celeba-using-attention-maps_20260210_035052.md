---
ver: rpa2
title: 'Attention IoU: Examining Biases in CelebA using Attention Maps'
arxiv_id: '2503.19846'
source_url: https://arxiv.org/abs/2503.19846
tags:
- bias
- hair
- male
- dataset
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Attention-IoU, a metric using attention maps
  to measure model bias by quantifying spurious correlations between target attributes
  and confounding attributes. The method compares Grad-CAM attention maps for target
  attributes with those of protected attributes or ground-truth feature masks.
---

# Attention IoU: Examining Biases in CelebA using Attention Maps

## Quick Facts
- arXiv ID: 2503.19846
- Source URL: https://arxiv.org/abs/2503.19846
- Reference count: 40
- Key outcome: Attention-IoU metric quantifies model bias by measuring spatial overlap between Grad-CAM attention maps for target attributes and either ground-truth masks or protected attribute attention maps, revealing spurious correlations beyond simple label analysis

## Executive Summary
This paper introduces Attention-IoU, a metric that uses attention maps to quantify spurious correlations between target and confounding attributes in computer vision models. The method computes the intersection-over-union between attention maps generated via GradCAM for target attributes and either ground-truth feature masks or attention maps of protected attributes. Validation on the synthetic Waterbirds dataset shows the metric accurately reflects known bias levels, with mask scores decreasing proportionally as bias increases. Analysis of the CelebA dataset reveals that attributes like Wearing Lipstick exhibit strong bias correlations with Male beyond simple label correlations, while Blond Hair shows potential hidden confounding variables. The metric successfully identifies biases at a fine-grained level by revealing which specific image regions models attend to when making predictions, providing insights beyond traditional accuracy-based bias metrics.

## Method Summary
The Attention-IoU metric quantifies model bias by computing the intersection-over-union (IoU) between attention maps for target and confounding attributes. The method generates attention maps using GradCAM on final convolutional layers, with a modification for binary cross-entropy models using gradients of absolute logits. For each target attribute, two scores are computed: mask score (IoU with ground-truth feature masks) and heatmap score (IoU with attention maps of protected/confounding attributes). The BA-IoU formula normalizes continuous attention maps to be size-invariant and scale-invariant. Validation uses Waterbirds with known bias levels and CelebA with CelebAMask-HQ ground-truth masks. Models are trained using ResNet architectures with ImageNet pretraining, and experiments include systematic subsampling to vary attribute correlations and identify hidden confounders.

## Key Results
- Waterbirds validation shows mask scores decrease proportionally from 0.72±0.02 to 0.42±0.03 as bias increases, matching worst-group accuracy changes from 0.81 to 0.21
- CelebA analysis reveals Wearing Lipstick has high heatmap correlation with Male (0.48±0.04) despite low label correlation (MCC -0.16±0.04), indicating strong bias beyond labels
- Blond Hair shows heatmap scores invariant to training distribution changes (Kendall τ ≈ 0.007), suggesting hidden confounding variables distinct from dataset labels
- Mustache and Wavy Hair show opposite heatmap score trends with MCC (τ ≈ 0.78 and 0.78 respectively), validating the metric's sensitivity to label correlation effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention maps (via GradCAM) reveal which image regions models rely on for predictions, exposing spurious correlations when models attend to confounding attribute regions instead of target regions.
- Mechanism: When a model learns spurious correlations between target and confounding attributes, it shifts attention from target-relevant features to confounding-relevant features. GradCAM visualizes this shift by computing gradient-weighted activation maps for target attributes, highlighting attended regions.
- Core assumption: Models relying on spurious correlations will exhibit detectable attention patterns diverging from ground-truth target features.
- Evidence anchors:
  - [abstract] "Attention-IoU...uses attention maps to measure model bias by quantifying spurious correlations between target attributes and confounding attributes."
  - [section 3] "if a model learns a spurious correlation between a target attribute and a confounding attribute in the dataset, it will learn to use features helpful for the confounding attribute instead of the target attribute."
  - [corpus] Weak direct evidence; related papers focus on bias mitigation/CLIP mechanisms rather than attention-based bias quantification.
- Break condition: When attention maps fail to capture relevant features (e.g., due to model architecture limitations) or when spurious correlations manifest through non-spatial features (color, texture) that attention maps cannot represent.

### Mechanism 2
- Claim: The Attention-IoU metric quantifies bias by measuring overlap between attention maps and either ground-truth masks (mask score) or attention maps of protected attributes (heatmap score), with lower mask scores indicating higher bias.
- Mechanism: BA-IoU computes a scale-invariant, size-invariant overlap between two maps via Frobenius inner product normalized by union area. Mask scores decrease as attention shifts away from target features toward confounding features; heatmap scores increase as target and protected attribute attention patterns align.
- Core assumption: Attention map similarity/dissimilarity meaningfully reflects model reliance on confounding vs. target features.
- Evidence anchors:
  - [section 3.2] Full metric definition with invariance proofs in Appendix B.
  - [section 4, Figure 4] "The decrease from 0.72 ± 0.02 to 0.42 ± 0.03 in mask score almost exactly mirrors the proportional decrease in WGA, validating that the metric accurately measures model bias."
  - [corpus] No direct corpus support; related works use attention maps for debiasing but not for this specific metric formulation.
- Break condition: When attributes are co-localized (e.g., eyeglasses and eyes) but models attend to different features within the same region, BA-IoU may incorrectly indicate high correlation (limitation noted in section 3.2).

### Mechanism 3
- Claim: Subsampling training distributions to vary attribute correlations reveals hidden confounding variables when heatmap scores remain unchanged despite changing label correlations.
- Mechanism: By systematically varying the Matthews correlation coefficient (MCC) between target and protected attributes in training data, if heatmap scores remain invariant (as with Blond Hair), this suggests unlabeled confounders drive model bias beyond dataset label proportions.
- Core assumption: If bias stems solely from label correlations, changing those correlations should change attention patterns proportionally.
- Evidence anchors:
  - [section 5.2, Figure 9] "For Blond Hair we find that there is no statistically significant change in heatmap score...indicating that there might be an unlabeled confounder present."
  - [abstract] "Attention-IoU reveals potential confounding variables not present in dataset labels."
  - [corpus] No direct corpus evidence; confounder discovery via attention is novel in this formulation.
- Break condition: When confounders themselves are spatially overlapping with target features, or when model capacity limits prevent learning complex correlations.

## Foundational Learning

- Concept: Spurious correlations in computer vision
  - Why needed here: Core problem the paper addresses; models exploit dataset-specific correlations (e.g., waterbirds→water background) that fail on out-of-distribution data.
  - Quick check question: In a dataset where 90% of images with "blond hair" are labeled "not male," what spurious correlation might a model learn, and how would it fail at test time?

- Concept: GradCAM (Gradient-weighted Class Activation Mapping)
  - Why needed here: Primary interpretability tool used to generate attention maps; understanding its operation on binary vs. categorical cross-entropy is critical.
  - Quick check question: Why does the paper take gradients of |logits| for binary cross-entropy models rather than raw logits?

- Concept: Intersection-over-Union (IoU) metrics
  - Why needed here: Attention-IoU generalizes IoU to continuous maps; understanding standard IoU provides baseline for grasping the extension.
  - Quick check question: Standard IoU requires binary masks. What modifications does BA-IoU make to handle continuous-valued attention maps while preserving invariance properties?

## Architecture Onboarding

- Component map:
  Input images → Trained classifier → GradCAM attention maps → BA-IoU comparison (vs. ground-truth masks or protected attribute attention maps) → Bias scores → Cross-validation with known bias levels (Waterbirds) or discovery analysis (CelebA)

- Critical path:
  Input images → Trained classifier → GradCAM attention maps → BA-IoU comparison (vs. ground-truth masks or protected attribute attention maps) → Bias scores → Cross-validation with known bias levels (Waterbirds) or discovery analysis (CelebA)

- Design tradeoffs:
  - Spatial-only attention: Cannot capture color/texture biases; may misclassify co-localized but distinct features
  - GradCAM dependency: Sensitive to model architecture and layer selection (final conv layer used)
  - Binary attribute labels: Forces continuous gender presentation into discrete categories; label noise in attributes like Wavy Hair may affect reliability
  - Training set subsampling: Requires sufficient samples per subgroup to avoid small-group artifacts

- Failure signatures:
  - High heatmap scores between spatially co-located attributes with different features (e.g., eyeglasses/eyes) → false positive bias indication
  - Low mask scores but high worst-group accuracy → suggests bias not spatially localized
  - Heatmap scores insensitive to training distribution changes → indicates hidden confounders (feature, not diagnosis)

- First 3 experiments:
  1. Replicate Waterbirds validation at 70%, 90%, 95%, 100% bias levels; verify mask score decreases proportionally with worst-group accuracy (should show ~0.72→0.42 score decrease matching 0.81→0.21 WGA decrease)
  2. Train CelebA attribute classifiers and compute heatmap scores vs. Male for all 40 attributes; identify outliers to MCC trend (Mustache above, Wavy Hair below) and analyze mask score distributions per facial region
  3. Subsample training set for Blond Hair and Wavy Hair to vary MCC from -0.5 to -0.1; verify Wavy Hair heatmap score decreases with MCC while Blond Hair remains constant (Kendall τ ~0.78 vs 0.007)

## Open Questions the Paper Calls Out

- Question: What are the specific visual features (the "hidden confounders") driving the bias in the Blond Hair attribute in CelebA, distinct from simple label correlations?
  - Basis in paper: [explicit] The paper concludes that for Blond Hair, "there might be an unlabeled confounder present... there is an innate quality to the features distinct from dataset labels that create bias within the model" (Page 8).
  - Why unresolved: While the subsampling experiment proved the bias persists regardless of dataset label distribution, the paper did not identify the specific visual characteristic (e.g., specific texture, styling, or background) causing this effect.
  - What evidence would resolve it: A follow-up study isolating potential visual features of blond hair (e.g., using controlled synthetic data) to see which feature manipulation causes the Attention-IoU score to drop.

- Question: Can the Attention-IoU metric be refined to distinguish between actual spurious correlations and mere spatial co-localization of features?
  - Basis in paper: [explicit] The authors note a limitation: "if a target and confounding attribute are co-located... our metric will still indicate high correlation" (Page 3). They show this empirically with Eyeglasses, where the high score is due to shared location (eyes) rather than shared logic (Page 7).
  - Why unresolved: The current formulation relies purely on spatial overlap (IoU). It flags false positives when two attributes physically overlap (like eyes and glasses) even if the model processes them independently.
  - What evidence would resolve it: Modifying the metric to incorporate feature-level comparisons (e.g., texture or channel activations) or testing if masking the co-localized region eliminates the correlation.

- Question: How effectively does Attention-IoU transfer to general object recognition datasets where ground-truth feature masks are unavailable?
  - Basis in paper: [explicit] The authors state: "Future investigations of the proposed methods on other datasets and tasks can provide further insights into the nature of biases within computer vision models" (Page 8).
  - Why unresolved: The study was limited to Waterbirds (synthetic, clear masks) and CelebA (facial features). It is unclear if the metric is robust for complex scenes where defining the "target region" is ambiguous without manual segmentation.
  - What evidence would resolve it: Applying the metric to large-scale datasets like COCO or ImageNet and comparing the identified biases against known dataset skews or unsupervised clustering of error modes.

## Limitations
- Spatial-only detection cannot capture color, texture, or other non-spatial spurious correlations that may drive model bias
- False positive bias indications when target and confounding attributes are spatially co-localized but rely on different features within the same region
- Binary attribute labeling forces continuous gender presentation into discrete categories, potentially oversimplifying complex demographic attributes

## Confidence
- **High Confidence**: The core mechanism that attention maps reveal spurious correlations through spatial overlap patterns is well-supported by Waterbirds validation showing proportional mask score decreases matching known bias levels (0.72±0.02 to 0.42±0.03 mirroring WGA changes from 0.81 to 0.21)
- **Medium Confidence**: The confounder discovery mechanism (heatmap scores invariant to label correlation changes) shows promising signals for Blond Hair (Kendall τ ≈ 0.007) but requires more rigorous validation across multiple attributes and datasets
- **Low Confidence**: The practical utility of the metric for real-world bias mitigation is limited by its spatial-only detection capability and the binary attribute labeling that forces continuous gender presentation into discrete categories

## Next Checks
1. Replicate Waterbirds validation at multiple bias levels (70%, 90%, 95%, 100%) to verify that mask scores decrease proportionally with worst-group accuracy, confirming the metric accurately reflects known bias levels

2. Systematic confounder validation by subsampling CelebA training data for multiple target attributes (both high-MCC outliers like Mustache and invariant heatmap score cases like Blond Hair) to verify the correlation between MCC changes and heatmap score sensitivity

3. Cross-attribute heatmap analysis by computing heatmap scores between all 40 CelebA attributes and Male, then validating that high-scoring pairs show consistent spatial attention patterns through visual inspection of attention map overlays on ground-truth masks