---
ver: rpa2
title: A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class
  Classification
arxiv_id: '2508.10926'
source_url: https://arxiv.org/abs/2508.10926
tags:
- value
- ensemble
- performance
- game
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in ensemble learning methods,
  particularly the inability of existing weighting schemes to incorporate multiple
  evaluation criteria when assigning importance to classifiers. To overcome this,
  the author proposes a multi-criteria weighted ensemble approach based on cooperative
  game theory and the VIKOR method.
---

# A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification

## Quick Facts
- **arXiv ID:** 2508.10926
- **Source URL:** https://arxiv.org/abs/2508.10926
- **Reference count:** 0
- **Primary result:** Multi-criteria weighted ensemble using VIKOR + cooperative game theory improves accuracy by 2-3% over single-metric approaches

## Executive Summary
This paper addresses limitations in ensemble learning methods, particularly the inability of existing weighting schemes to incorporate multiple evaluation criteria when assigning importance to classifiers. To overcome this, the author proposes a multi-criteria weighted ensemble approach based on cooperative game theory and the VIKOR method. The proposed method uses performance metrics from classifiers (e.g., accuracy, PPV, NPV, TPR, TNR) across all classes, then applies the VIKOR method to evaluate and rank classifiers based on these multiple criteria. The VIKOR results are used in a bankruptcy problem formulation to distribute weights to classifiers using various cooperative game values such as Shapley, Banzhaf, and ENIC values. Experiments on OpenML-CC18 datasets show the proposed method outperforms existing weighted ensemble methods, with accuracy improvements of up to 2-3%.

## Method Summary
The method combines multi-criteria decision making with cooperative game theory for ensemble weighting. Base classifiers are trained and evaluated using multiple metrics (accuracy, PPV, NPV, TPR, TNR) computed per-class. VIKOR aggregates these metrics to rank classifiers, treating the rankings as demands in a bankruptcy problem. A characteristic function is defined based on the remaining resources after satisfying non-members' demands, and various cooperative game values (Shapley, Banzhaf, etc.) are computed to distribute weights. The method specifically addresses class imbalance by inversely weighting evaluation criteria by class frequency. Experiments compare 8 different cooperative game values across 6 datasets from the OpenML-CC18 benchmark.

## Key Results
- Proposed method outperforms existing weighted ensemble methods with accuracy improvements of up to 2-3%
- Shapley and Banzhaf values, which emphasize individual contributions, performed particularly well
- ENIC, Banzhaf, and Shapley values demonstrated superior performance in sequence, indicating that in the distribution of weights in an ensemble, individual contribution and the fairness of the entire coalition play significant roles
- Simple average voting (SAV) and single weighted voting (SWV) using only accuracy as the criterion were outperformed by the multi-criteria approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-criteria evaluation of classifiers captures complementary performance dimensions that single metrics miss.
- **Mechanism:** VIKOR aggregates class-wise performance across multiple metrics (Accuracy, PPV, NPV, TPR, TNR) by computing: (1) S_n, the weighted sum of normalized distances from ideal performance (comprehensive opportunity loss), and (2) R_n, the maximum individual criterion gap (worst-case risk). These combine into Q_n, ranking classifiers by proximity to an ideal point.
- **Core assumption:** Different evaluation metrics reveal non-redundant information about classifier behavior; their trade-offs can be meaningfully compromised.
- **Evidence anchors:**
  - [abstract]: "existing methods that reflect the pre-information of classifiers in weights consider only one evaluation criterion, which limits the reflection of various information"
  - [section]: "In multi-class classification, the considerations increase compared to binary classification, as the evaluation methods are viewed differently for each class, leading to varying measurements even for the same evaluation metric"
  - [corpus]: Multi-criteria rank-based aggregation work (FMR 0.65) suggests broader validity of multi-criteria approaches, though not ensemble-specific
- **Break condition:** If selected metrics are highly correlated or if one metric dominates decision quality, multi-criteria complexity yields diminishing returns.

### Mechanism 2
- **Claim:** Cooperative game values translate classifier evaluations into principled weight distributions by modeling contribution to coalition success.
- **Mechanism:** VIKOR scores become "demands" in a bankruptcy problem. The characteristic function v(O) = max{0, W - Σz_i for i∉O} defines coalition values. Game-theoretic values (Shapley, Banzhaf, etc.) compute each classifier's marginal contribution across all coalition permutations, then normalize to weights.
- **Core assumption:** Classifier contribution to ensemble accuracy follows patterns analogous to player contribution in cooperative games; marginal contributions predict ensemble value.
- **Evidence anchors:**
  - [abstract]: "Shapley and Banzhaf values, which emphasize individual contributions, performed particularly well"
  - [section]: "ENIC, Banzhaf, and Shapley values demonstrated superior performance in sequence, indicating that in the distribution of weights in an ensemble, individual contribution and the fairness of the entire coalition play significant roles"
  - [corpus]: Limited direct corpus evidence for game-theoretic ensembles; related work on collaborative ensemble training exists but uses different mechanisms
- **Break condition:** If classifiers make highly correlated errors, marginal contribution-based values may misallocate; Solidarity/Consensus values underperformed, suggesting partial cooperation is less relevant.

### Mechanism 3
- **Claim:** Class-weighted evaluation handles imbalance by giving minority classes proportional influence in multi-criteria scoring.
- **Mechanism:** Criterion weights p_j are computed inversely proportional to class frequency using p_j = m·exp(-m·w'_j), where w'_j is the normalized instance count. This upweights minority class performance in VIKOR aggregation.
- **Core assumption:** Equalizing class influence in evaluation produces weights that improve minority class handling without explicit resampling.
- **Evidence anchors:**
  - [section]: "the weights p_j for each criterion considered in the game are given inversely proportional to the number of classes to account for imbalanced classes"
  - [section]: "In the proposal, the weights are simply given inversely proportional to the number of classes, but these weights can vary depending on the information to be considered"
  - [corpus]: Corpus lacks direct evidence on class-weighted ensemble evaluation; imbalance handling in related work uses different approaches
- **Break condition:** If class imbalance is severe and performance metrics saturate (e.g., TPR=1.0 for minority), weighting provides no signal.

## Foundational Learning

- **Concept: VIKOR Multi-Criteria Optimization**
  - **Why needed here:** Core algorithm for aggregating conflicting evaluation criteria; produces the z_i evaluation vectors fed to game-theoretic weight calculation.
  - **Quick check question:** Given three classifiers with Q-values [0.1, 0.5, 0.9] after VIKOR, which gets the highest weight in subsequent steps? (Answer: The one with Q=0.1, but the method inverts to z_i = [max(Q)+min(Q)]-Q_i, so z=[0.5, 0.5, 0.1], giving classifiers 1 and 2 equal highest demand.)

- **Concept: Shapley Value in Cooperative Games**
  - **Why needed here:** Best-performing weight distribution method; requires understanding marginal contribution averaging over coalition permutations.
  - **Quick check question:** In a 3-classifier ensemble, how many coalition orderings does Shapley average over? (Answer: 3! = 6 permutations.)

- **Concept: Bankruptcy Problem Formulation**
  - **Why needed here:** Converts classifier evaluations to game characteristic function; defines what each coalition "deserves" based on remaining resources after non-members' demands.
  - **Quick check question:** If total resource W=0.8 and two classifiers have demands z=[0.6, 0.5], what is v({classifier_1})? (Answer: v({1}) = max{0, 0.8 - 0.5} = 0.3.)

## Architecture Onboarding

- **Component map:**
  1. Base Classifiers: Train heterogeneous classifiers (KNN, DT, SVM, NB, ANN, QDA, LR in paper) on training split
  2. Evaluation Stage: Compute per-class metrics (Accuracy, PPV, NPV, TPR, TNR) on validation split → confusion matrices
  3. VIKOR Layer: Two-stage aggregation—first per-metric across classes, then across metrics → produces z_i evaluation vector
  4. Game Theory Layer: Bankruptcy characteristic function → game value calculation (Shapley/Banzhaf/etc.) → normalized weights r_i
  5. Ensemble Inference: Soft voting with weights: E = argmax_j Σ(r_i × SoftOutput_i^j)

- **Critical path:**
  1. Split data into train/validation/test (stratified)
  2. Train base classifiers on train, evaluate on validation
  3. Extract class-wise metrics from confusion matrices
  4. Run VIKOR with class-frequency weights → get z_i
  5. Apply bankruptcy formulation with W=0.8×Σz_i
  6. Compute game values (try Shapley first based on results)
  7. Normalize to weights, apply soft voting on test set

- **Design tradeoffs:**
  - **Shapley vs. Banzhaf:** Shapley weights coalition permutations by size; Banzhaf weights equally. Paper shows similar performance; Shapley has stronger theoretical foundations, Banzhaf is O(2^n) vs O(n!)—use Banzhaf for larger ensembles.
  - **VIKOR parameter v:** Controls balance between group utility (S) and individual regret (R). Paper uses v=0.5; v<0.5 shows "unstable performance changes."
  - **Resource level W=0.8×Σz_i:** Controls how much "resource" is available; 0.8 is a heuristic. Lower values increase competition for weight.

- **Failure signatures:**
  - Negative weights from ENIC values: Apply max(0, V_i) before normalization (Eq. 40)
  - All weights equal: Check if z_i values are identical (classifiers have similar multi-metric profiles)
  - Performance worse than simple averaging: VIKOR/game theory not adding value; check if base classifiers are too similar or one dominates all metrics

- **First 3 experiments:**
  1. **Baseline comparison:** Implement SWV and SAV on same data; verify proposed method with Shapley value exceeds these by expected margin (2-3% on benchmark datasets)
  2. **Ablation on game values:** Compare all 8 values (Shapley, Banzhaf, Solidarity, CIS, ENSC, ENPAC, ENBC, Consensus) on 3 datasets; confirm Shapley/Banzhaf superiority pattern
  3. **Single vs. multi-criteria:** Run VIKOR with only accuracy vs. all 5 metrics; quantify multi-criteria contribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating classifier diversity metrics or computational cost as additional criteria in the VIKOR stage further improve ensemble performance?
- **Basis in paper:** [Explicit] The conclusion states that "future research could explore additional indicators and methods" and specifically mentions "diversity" and "correlation analysis" as potential alternative numerical methods.
- **Why unresolved:** The current study limits the multi-criteria input to five performance metrics (Accuracy, PPV, NPV, TPR, TNR) and does not quantify the diversity or efficiency of the base classifiers.
- **What evidence would resolve it:** Experimental results comparing the current model against variants where diversity indices (e.g., Q-statistic) or execution time are added as dimensions in the MCDM matrix.

### Open Question 2
- **Question:** Do game-theoretic negotiation solutions (e.g., Nash bargaining, Kalai-Smorodinsky) outperform the current cooperative game values for weight distribution?
- **Basis in paper:** [Explicit] The author suggests "considering other game-theoretic solutions like negotiation solutions in multi-criteria situations" as a direction for future work.
- **Why unresolved:** The study only evaluates solutions derived from cooperative game theory values (Shapley, Banzhaf, CIS, etc.) based on a bankruptcy problem formulation.
- **What evidence would resolve it:** Comparative analysis implementing negotiation-based algorithms to determine weights and measuring resulting ensemble accuracy against the cooperative bankruptcy-based method.

### Open Question 3
- **Question:** How does the dynamic adjustment of the VIKOR parameter $v$ (strategy weight) affect the stability and accuracy of the ensemble?
- **Basis in paper:** [Inferred] The method section fixes $v=0.5$ but notes that $(v < 0.5)$ shows unstable performance changes, implying the optimal balance between group utility and individual regret is not fully explored.
- **Why unresolved:** The paper does not perform a sensitivity analysis on the $v$ parameter, which dictates the trade-off between the majority rules (group utility) and individual vetoes (maximum regret).
- **What evidence would resolve it:** An ablation study varying $v$ across a range (e.g., 0.1 to 0.9) on the OpenML-CC18 datasets and reporting the impact on accuracy and variance.

## Limitations

- Limited ablation on game-theoretic value selection (8 values tested, but Shapley/Banzhaf clearly superior)
- No sensitivity analysis on VIKOR parameters (v, W) or class weighting scheme
- Unclear whether multi-criteria complexity is necessary when accuracy alone might suffice

## Confidence

- **High Confidence:** Multi-criteria evaluation improves ensemble weighting over single-metric approaches
- **Medium Confidence:** Shapley and Banzhaf values are optimal for this weighting problem (based on limited comparisons)
- **Low Confidence:** The specific bankruptcy formulation and VIKOR parameter choices are optimal; results may not generalize to all ensemble types

## Next Checks

1. **Game Value Sensitivity:** Test Shapley vs. Banzhaf on 3 datasets with varying ensemble sizes to confirm consistent superiority pattern
2. **VIKOR Parameter Sweep:** Vary v parameter (0.3-0.7) and resource level W (0.6-1.0) to assess robustness of weighting
3. **Multi-criteria Necessity:** Compare single-criterion (accuracy only) vs. multi-criteria VIKOR weighting to quantify performance contribution from metric aggregation