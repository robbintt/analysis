---
ver: rpa2
title: 'AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory'
arxiv_id: '2510.15261'
source_url: https://arxiv.org/abs/2510.15261
tags:
- memory
- user
- information
- search
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AUGUSTUS is a multimodal agent system designed to retain and utilize
  user context across conversations. The system encodes multimodal inputs, stores
  information in hierarchical long-term memory (recall and contextual), retrieves
  relevant data via concept-driven search, and acts using multimodal generation tools.
---

# AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory

## Quick Facts
- **arXiv ID:** 2510.15261
- **Source URL:** https://arxiv.org/abs/2510.15261
- **Reference count:** 40
- **Primary result:** AUGUSTUS outperforms traditional multimodal RAG approaches while being 3.5x faster for ImageNet classification, and it surpasses MemGPT on the MSC benchmark for conversational consistency.

## Executive Summary
AUGUSTUS is a multimodal agent system designed to retain and utilize user context across conversations. The system encodes multimodal inputs, stores information in hierarchical long-term memory, retrieves relevant data via concept-driven search, and acts using multimodal generation tools. It conceptualizes information into semantic tags and associates them with context nodes in a graph-structured multimodal memory. AUGUSTUS outperforms traditional multimodal RAG approaches while being 3.5x faster for ImageNet classification, and it surpasses MemGPT on the MSC benchmark for conversational consistency.

## Method Summary
AUGUSTUS implements a four-stage loop: encode, store, retrieve, and act. User inputs are encoded into text captions using multimodal encoders (Video-LLaVA, WhisperX, etc.), then stored in a graph-structured contextual memory with semantic tags generated by an LLM. Retrieval uses a concept-driven CoPe search that first finds relevant tags via hierarchical clustering (UMAP + HDBSCAN) then fetches associated context nodes. An LLM planner (dolphin-2.6-mixtral-8x7b) orchestrates all operations through function calls. The system maintains three memory stores: in-context (prompt buffer), recall (SQLite), and contextual (ArangoDB graph). During evaluation, tag embeddings are computed using ImageBind, and the system uses 8x A100 GPUs.

## Key Results
- AUGUSTUS achieves 75.0 Top-1 accuracy at 99.1ms retrieval time vs. multimodal RAG at 78.2 Top-1 accuracy at 403.8ms for ImageNet classification (1.2M nodes)
- System is 3.5x faster than traditional multimodal RAG while maintaining competitive accuracy
- AUGUSTUS surpasses MemGPT on the MSC benchmark for conversational consistency

## Why This Works (Mechanism)

### Mechanism 1: Concept-Driven Hierarchical Retrieval (CoPe Search)
Searching semantic concepts before context nodes reduces retrieval time while maintaining accuracy. The system first retrieves relevant tag nodes from a clustered contextual tree, then fetches associated context nodes. Tag clustering via UMAP + HDBSCAN creates a hierarchical structure where each node represents averaged context embeddings. The personalized knowledge graph provides additional tags through learned concept associations. Core assumption: Concept abstractions sufficiently represent user context for first-stage filtering; embedding similarity correlates with semantic relevance. Evidence: CoPe search with clustering achieves 75.0 Top-1 accuracy at 99.1ms vs. multimodal RAG at 78.2 Top-1 at 403.8ms. Break condition: When concepts are too granular or sparse, clustering fails to meaningfully reduce search space; accuracy drops from 78.0 to 75.0.

### Mechanism 2: Graph-Structured Memory with Tag-Context Association
Storing information as semantic tags connected to context nodes enables efficient concept-driven retrieval and association learning. Each user interaction is conceptualized into semantic tags via LLM ICL. Tags become nodes in a graph; context nodes store multimodal data (text, media URLs, timestamps, modality). Edges form between tags that share context nodes, creating an implicit association graph that supports personalized retrieval. Core assumption: LLM-generated tags capture salient concepts; shared context implies semantic relatedness between tags. Evidence: System conceptualizes information into semantic tags and associates them with context to store in graph-structured multimodal contextual memory. Break condition: If tags are too generic or inconsistent across sessions, graph becomes noisy; retrieval precision degrades.

### Mechanism 3: Multimodal-to-Text Encoding with Dual Storage
Encoding non-text modalities to text captions while retaining original media preserves LLM reasoning capability and multimodal fidelity. Video-LLaVA, WhisperX, and other encoders convert images/video/audio to text captions following the language-of-thought hypothesis. Both caption and original media URL are stored in context nodes. During retrieval, text captions enable similarity matching while URLs allow regenerating or referencing original media. Core assumption: Text captions sufficiently represent multimodal content for retrieval; original media remains accessible via stored URLs. Evidence: System converts all non-text inputs into text captions and stores both the text caption and corresponding non-text input media to prevent any loss. Break condition: Encoder failures or caption hallucinations propagate misinformation; media URL expiration breaks retrieval of original content.

## Foundational Learning

- **Concept:** Hierarchical clustering for approximate nearest neighbor search
  - Why needed here: CoPe search builds a contextual tree using UMAP dimensionality reduction and HDBSCAN clustering to enable sub-linear retrieval time over tag nodes.
  - Quick check question: Can you explain why clustering tags before searching reduces computational complexity compared to linear scan?

- **Concept:** LLM function calling / tool use
  - Why needed here: AUGUSTUS operates through an LLM (dolphin-2.6-mixtral-8x7b) generating structured function calls (encode_image, contextual_memory_insert, cope_search, etc.) rather than free-form text.
  - Quick check question: What is the difference between chain-of-thought prompting and function-calling orchestration in agent systems?

- **Concept:** Embedding-based retrieval and cosine similarity
  - Why needed here: Both tag nodes and context nodes are embedded (using ImageBind or LanguageBind); retrieval ranks candidates by cosine similarity to query embeddings.
  - Quick check question: Why might cosine similarity fail for certain semantic relationships that Euclidean distance captures?

## Architecture Onboarding

- **Component map:** User input → encode (if non-text) → LLM decides storage → contextual_memory_insert (conceptualize tags, store context node, update graph edges) → on future query, cope_search (traverse tree → get tags → retrieve context nodes) → generate response

- **Critical path:** User input → encode (if non-text) → LLM decides storage → contextual_memory_insert (conceptualize tags, store context node, update graph edges) → on future query, cope_search (traverse tree → get tags → retrieve context nodes) → generate response

- **Design tradeoffs:**
  - Accuracy vs. speed: Clustering reduces accuracy from 78.0 to 75.0 Top-1 but cuts retrieval time from 126.1ms to 99.1ms
  - Modality coverage vs. encoder quality: Open-source encoders chosen for reproducibility; may underperform closed-source alternatives
  - Storage overhead: Storing both captions and media URLs increases memory footprint vs. text-only systems

- **Failure signatures:**
  - Thought-action inconsistency: LLM monologue suggests one action but executes another
  - Tag proliferation: Without deduplication or canonicalization, semantically similar tags fragment retrieval
  - Context overflow: When in-context memory exceeds 12k tokens, 50% eviction with lossy summary may discard critical recent context

- **First 3 experiments:**
  1. Reproduce ImageNet retrieval baseline: Load 997 ImageNet class names as tag nodes with training images as context nodes; query with validation images; verify Top-1 accuracy matches reported 75.0-78.0 range
  2. Ablate clustering: Run CoPe search with and without UMAP+HDBSCAN clustering on same ImageNet setup; confirm speed-accuracy tradeoff
  3. MSC conversation test: Populate contextual memory with 5 multi-session conversation samples; query with persona-relevant questions; measure ROUGE-L against baseline MemGPT results

## Open Questions the Paper Calls Out

- **Open Question 1:** Can AUGUSTUS be equipped to autonomously learn new tools and generate novel function calls?
  - Basis: Authors explicitly state in Limitations section that equipping AUGUSTUS with ability to learn new tools and generate novel function calls is a promising future direction.
  - Why unresolved: Current system relies on fixed collection of predefined tools for multimodal operations.
  - What evidence would resolve it: Demonstration of agent successfully integrating and utilizing a tool not defined in initial system prompt without manual updates.

- **Open Question 2:** What benchmark datasets are required to effectively evaluate multimodal conversational agents on context retention?
  - Basis: Conclusion notes it is crucial to develop benchmark datasets to evaluate multimodal conversational agents on their context-retaining ability effectively in the future.
  - Why unresolved: Existing benchmarks like MSC focus primarily on text; no standard dataset for evaluating memory consistency across image, audio, and video modalities.
  - What evidence would resolve it: Proposal and adoption of standardized dataset that tests retrieval of specific multimodal details over long conversation horizons.

- **Open Question 3:** How can inconsistencies between agent's internal thoughts and executed actions be resolved?
  - Basis: Limitations section highlights authors sometimes observed inconsistencies between agent's thoughts and actions, citing chain-of-thought reliability issues.
  - Why unresolved: While rare, these discrepancies hinder development of dependable, cognition-aligned systems.
  - What evidence would resolve it: Refined planning module or prompting strategy ensuring function calls strictly adhere to preceding internal reasoning traces.

- **Open Question 4:** Can the tactile (touch) modality be integrated into encoding and memory stages?
  - Basis: Appendix A.1 states system does not currently support touch, but supporting touch modality is straightforward and left for future work.
  - Why unresolved: Current implementation limited to visual, acoustic, and semantic (text) signals.
  - What evidence would resolve it: Extension including tactile encoder that successfully maps touch data to semantic tags within contextual memory.

## Limitations
- Tag generation reliability depends on LLM ICL quality without quantitative ablation on tag consistency
- Memory storage overhead increases due to storing both text captions and original media URLs
- Thought-action inconsistency in chain-of-thought prompting occurs despite being noted as limitation

## Confidence
- **High confidence:** 3.5x speed improvement over multimodal RAG for ImageNet retrieval with specific experimental setup and hardware
- **Medium confidence:** Architectural claims around concept-driven retrieval and graph-structured memory (implementation details underspecified)
- **Low confidence:** Generalizability claims beyond ImageNet and MSC benchmarks (no validation across diverse multimodal domains)

## Next Checks
1. **Tag quality ablation:** Systematically evaluate retrieval performance when LLM-generated tags are replaced with ground-truth semantic labels on ImageNet and MSC datasets. Measure degradation in accuracy and retrieval time.
2. **Clustering hyperparameter sensitivity:** Run CoPe search with varying UMAP (n_neighbors: 5, 15, 30) and HDBSCAN (min_cluster_size: 5, 15, 30) parameters. Report accuracy-speed tradeoff curves to identify robust operating points.
3. **Memory scaling benchmark:** Populate contextual memory with increasing numbers of nodes (10K, 100K, 1M) and measure retrieval latency and accuracy degradation. Compare against linear scan baseline to verify claimed sub-linear complexity benefits.