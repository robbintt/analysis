---
ver: rpa2
title: Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge
arxiv_id: '2505.19266'
source_url: https://arxiv.org/abs/2505.19266
tags:
- scoring
- task
- rater
- teachers
- raters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates construct-irrelevant variance (CIV) in
  large language model (LLM) scoring of teachers' pedagogical content knowledge (PCK)
  compared to human raters and traditional machine learning (ML) models. Using generalized
  linear mixed models, the authors analyzed variance components across two PCK tasks
  (analyzing student thinking and evaluating teacher responsiveness) with 187 teachers
  and three scoring sources.
---

# Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge

## Quick Facts
- arXiv ID: 2505.19266
- Source URL: https://arxiv.org/abs/2505.19266
- Reference count: 21
- Key outcome: LLM scoring introduces construct-irrelevant variance (CIV) similar to human raters, with ML most severe, LLM most lenient, and rater-related factors dominating variance

## Executive Summary
This study examines construct-irrelevant variance (CIV) in large language model (LLM) scoring of teachers' pedagogical content knowledge (PCK) compared to human raters and traditional machine learning (ML) models. Using generalized linear mixed models (GLMMs), the authors analyzed variance components across two PCK tasks with 187 teachers and three scoring sources. Results showed scenario-level variance was minimal while rater-related factors contributed substantially to CIV, particularly in the more interpretive Task II. The ML model was the most severe and least sensitive rater, while the LLM was the most lenient. The LLM showed slightly increased context sensitivity in Task II but remained more stable than human raters.

## Method Summary
The study used binary scoring (0/1) of teachers' written responses to video-based constructed-response tasks assessing two PCK sub-constructs: analyzing student thinking (Task I) and evaluating teacher responsiveness (Task II). 187 in-service science teachers viewed 3 video scenarios and responded to 2 prompts per scenario (6 responses total). Three trained human raters scored responses using consensus-based rubrics. GPT-4 with few-shot prompting (3 examples per task) scored responses using rubric restatement, reasoning steps, and binary label + justification outputs. GLMMs fitted via `glmer()` in R's `lme4` package with logit link analyzed variance components, with random intercepts for teacher ID, scenario, rater, and raterÃ—scenario interaction.

## Key Results
- Scenario-level variance was minimal while rater-related factors dominated construct-irrelevant variance
- ML model was the most severe and least sensitive rater, LLM was most lenient
- LLM showed slightly increased context sensitivity in Task II but remained more stable than human raters

## Why This Works (Mechanism)
The GLMM approach effectively partitions variance sources to identify construct-irrelevant variance components. By comparing rater severity and sensitivity across scoring sources, the study reveals how different AI and human raters handle interpretive tasks differently. The few-shot prompting approach with GPT-4 allows systematic scoring while maintaining rubric adherence.

## Foundational Learning
- GLMM with binomial link function: Needed to model binary scores while accounting for hierarchical structure; Quick check: verify variance components sum appropriately
- Variance component analysis: Needed to quantify CIV sources; Quick check: ensure random effects structure captures all relevant variance sources
- Best linear unbiased predictors (BLUPs): Needed to compare rater severity across sources; Quick check: verify BLUPs follow expected patterns
- Few-shot prompting: Needed for consistent LLM scoring; Quick check: ensure prompts contain sufficient examples for each task
- Construct-irrelevant variance: Needed to understand sources of scoring variation beyond the target construct; Quick check: confirm variance components align with theoretical expectations

## Architecture Onboarding
- Component map: Teacher responses -> Scoring (Human/LLM/ML) -> GLMM analysis -> Variance components -> Rater BLUPs
- Critical path: Data preparation -> GLMM fitting -> Variance extraction -> Rater comparison
- Design tradeoffs: Few-shot examples balance prompt length vs. coverage; GLMM complexity vs. interpretability
- Failure signatures: Singular fits indicate unstable variance estimates; Non-convergence suggests model misspecification
- First experiments: 1) Fit null model to establish baseline variance; 2) Test full model with all random effects; 3) Compare rater BLUPs across sources

## Open Questions the Paper Calls Out
None

## Limitations
- Exact prompt text and few-shot examples unspecified
- Raw data (teacher responses, human/ML/LLM scores) not publicly available
- Supervised ML model architecture and features undefined

## Confidence
- High confidence in GLMM findings about minimal task-level variance and dominant rater-related factors
- Medium confidence in relative severity/sensitivity rankings across raters
- Low confidence in claim about LLM's "slightly increased context sensitivity" without exact prompt structure

## Next Checks
1. Request and test the exact few-shot prompts and rubric definitions with publicly available PCK response samples
2. Replicate the GLMM variance component extraction with simulated data matching the study's structure
3. Conduct a sensitivity analysis varying the random effects structure to assess stability of rater BLUPs