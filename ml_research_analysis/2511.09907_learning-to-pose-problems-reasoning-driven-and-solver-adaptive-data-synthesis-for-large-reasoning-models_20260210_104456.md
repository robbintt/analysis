---
ver: rpa2
title: 'Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis
  for Large Reasoning Models'
arxiv_id: '2511.09907'
source_url: https://arxiv.org/abs/2511.09907
tags:
- problem
- reasoning
- solver
- problems
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a problem generator that reasons explicitly
  to plan problem directions before synthesis and adapts difficulty to the solver's
  ability. The generator first learns problem-design reasoning from related problem
  pairs augmented with intermediate chain-of-thought traces.
---

# Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models

## Quick Facts
- **arXiv ID:** 2511.09907
- **Source URL:** https://arxiv.org/abs/2511.09907
- **Reference count:** 28
- **Primary result:** 3.4% average accuracy improvement across 10 reasoning benchmarks using solver-adaptive data synthesis

## Executive Summary
This paper introduces a novel approach to training reasoning models by generating problems that are specifically calibrated to the solver's competence boundary. The method combines explicit problem-design reasoning (via chain-of-thought traces extracted from multi-part math questions) with reinforcement learning that uses the solver's own consistency as a verifiable reward signal. The approach demonstrates significant performance gains across mathematical and general reasoning benchmarks while being adaptable to different model architectures.

## Method Summary
The method trains a problem generator through a two-phase approach: first, supervised fine-tuning on problem-design CoT pairs extracted from multi-part math questions using a reverse-engineering prompt; then reinforcement learning via GRPO using solver feedback as reward. The solver is trained on the synthetic data using GRPO with verifiable rewards based on boxed answers. A co-evolutionary training loop alternates between generator and solver updates, with the solver providing consistency-based accuracy estimates to guide problem difficulty calibration. The approach requires curating multi-part math problems, implementing GRPO with solver feedback rewards, and training both generator and solver iteratively.

## Key Results
- **3.4% average improvement** across 10 mathematical and general reasoning benchmarks
- **0.7% additional gain** from co-evolutionary training of generator and solver
- **Generalizes across model architectures** including language and vision-language models
- **Outperforms baselines** using preference-based reward models

## Why This Works (Mechanism)

### Mechanism 1: Problem-Design Chain-of-Thought Cold-Start
Bootstrapping the generator with explicit reasoning traces about problem design improves synthesis quality over direct prompting. Multi-part questions naturally encode pedagogical reasoning, and a reverse-engineering prompt extracts latent design logic between related subquestions, producing `Problem1 → (CoT + Problem2)` pairs for supervised fine-tuning. This teaches the generator to analyze → strategize → instantiate rather than just paraphrasing.

### Mechanism 2: Solver Consistency as Verifiable Reward Proxy
Using solver accuracy (estimated via self-consistency) as reward calibrates problem difficulty near the solver's competence boundary without requiring ground-truth labels or preference models. The solver samples *m* responses per problem; majority vote yields a pseudo-label, and consistency (fraction matching) proxies accuracy. The reward function encourages difficulty inversion while targeting the 0.5 decision boundary.

### Mechanism 3: Generator-Solver Co-Evolution
Iterative alternating training of generator and solver yields compounding improvements beyond single-pass synthesis. An improved solver provides more informative difficulty feedback, which drives generator updates to produce harder problems, which further advances the solver—forming a positive feedback loop.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** The generator is trained via RL without a learned value function. GRPO uses group rollouts to compute advantages, reducing memory overhead.
  - **Quick check question:** Can you explain how GRPO replaces the critic with a group baseline?

- **Concept: Self-Consistency / Majority Voting**
  - **Why needed here:** Ground-truth labels for synthetic problems are unavailable during rollouts. Self-consistency provides a proxy for solver accuracy.
  - **Quick check question:** If a solver answers 7/10 times with "42" and 3/10 with "17", what is the pseudo-label and estimated accuracy?

- **Concept: Verifiable Rewards vs. Preference Rewards**
  - **Why needed here:** The paper explicitly contrasts RLVR (rule-based, extractable answers) with RLHF-style preference models. Understanding this distinction clarifies why solver feedback is preferable here.
  - **Quick check question:** Why might a preference model (e.g., Skywork) underperform a verifiable solver-accuracy reward for problem generation?

## Architecture Onboarding

- **Component map:** Problem Generator (SFT → RL) -> Problem Solver (GRPO) -> Reward Oracle (self-consistency) -> Training Loop
- **Critical path:** 1. Curate multi-part math problems → extract (Problem1, Problem2) pairs 2. Generate problem-design CoT via reverse-engineering prompt → SFT generator 3. For each seed: generator proposes problem → solver samples *m* responses → compute consistency → compute reward → GRPO update 4. Synthesize full dataset → label with reasoning model → train solver with GRPO + verifiable rewards
- **Design tradeoffs:** *m* (rollouts for accuracy estimation): Higher *m* improves reliability but increases compute. Paper uses *m* = 10. Cold-start vs. direct RL: Cold-start stabilizes training but requires curated multi-part data. Inversion-only vs. boundary-only reward: Full reward (both terms) outperforms either alone.
- **Failure signatures:** Generator outputs missing `<question>` tags → invalid samples (R_gen = -1). Reward plateaus early → check if solver is overfitting or generator is stuck in local mode. Synthetic problems unsolvable → labeling pipeline filters, but persistent issues indicate generator drift.
- **First 3 experiments:** 1. Sanity check: Train generator with SFT-only (no RL) on 34k CoT pairs; verify it outputs structured rationale + question. 2. Reward ablation: Compare full reward vs. boundary-only (R-Zero) vs. inversion-only on a held-out benchmark subset. 3. Co-evolution loop: Run 3 iterations of generator-solver alternating training; plot per-iteration accuracy on MATH and GSM8K to validate compounding gains.

## Open Questions the Paper Calls Out

### Open Question 1
Can the problem-design CoT paradigm effectively generalize to non-mathematical domains (e.g., coding or formal logic) where structured multi-part question hierarchies are scarce? The methodology relies on the specific pedagogical structure of math curricula to reverse-engineer design logic, a structure that may not exist in other reasoning domains. Experiments applying the generator to code generation tasks or logical reasoning benchmarks without relying on multi-part seed data would resolve this.

### Open Question 2
Does problem synthesis adapted for one specific solver model (e.g., Qwen3-4B) effectively transfer to improve the reasoning of a significantly different solver architecture? Problems calibrated for a small 4B model might be too trivial or structurally specific for a larger model, limiting the reusability of the synthetic data. Cross-model evaluation where data synthesized by a Qwen3-4B-adapted generator is used to train a Llama or Gemma model would resolve this.

### Open Question 3
What are the convergence limits and potential failure modes of the co-evolutionary training loop beyond the three iterations tested? It is unclear if performance plateaus or if the generator and solver eventually collapse into over-fitting specific synthetic artifacts over extended training. A convergence analysis tracking generator/solver performance over >10 iterations would resolve this.

## Limitations

- **Multi-part data dependency:** The cold-start mechanism relies on curated multi-part math problems, limiting applicability to domains without clear hierarchical question structures.
- **Computational overhead:** The method requires *m*=10 solver rollouts per generated problem for accuracy estimation, creating significant computational cost that isn't fully addressed in ablation studies.
- **Internal validation:** Key assumptions (solver-consistency correlation of 0.89 Pearson) are based on internal analysis without external validation, and long-term co-evolution stability hasn't been thoroughly stress-tested.

## Confidence

- **High Confidence:** GRPO implementation details, verifiable reward formulation, and reported benchmark improvements (3.4% average gain) are well-documented and reproducible.
- **Medium Confidence:** Problem-design CoT cold-start mechanism works as described for math problems with clear multi-part structure, but generalization to other domains is uncertain.
- **Low Confidence:** Solver-consistency correlation claim (0.89 Pearson) is based on internal analysis without external validation, and long-term stability of co-evolutionary training hasn't been thoroughly stress-tested.

## Next Checks

1. **Correlation Validation:** Independently verify the solver-consistency accuracy correlation using held-out labeled problems across different difficulty levels.
2. **Generalization Test:** Apply the method to non-math domains (e.g., logical reasoning or commonsense QA) without multi-part problem pairs to assess cold-start mechanism limitations.
3. **Efficiency Analysis:** Compare performance vs. computational cost across different *m* values (m=1, 5, 10, 20) to establish the marginal benefit of expensive rollouts.