---
ver: rpa2
title: 'Data as a Lever: A Neighbouring Datasets Perspective on Predictive Multiplicity'
arxiv_id: '2510.21303'
source_url: https://arxiv.org/abs/2510.21303
tags:
- data
- multiplicity
- datasets
- train
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for understanding how data processing
  choices influence predictive multiplicity, the phenomenon where multiple models
  achieve similar training performance but make conflicting predictions. By reframing
  data processing as choosing between neighbouring datasets, the authors provide theoretical
  and empirical insights into how overlap in class distributions affects downstream
  multiplicity.
---

# Data as a Lever: A Neighbouring Datasets Perspective on Predictive Multiplicity

## Quick Facts
- arXiv ID: 2510.21303
- Source URL: https://arxiv.org/abs/2510.21303
- Reference count: 40
- Primary result: The paper proves that under a shared Rashomon parameter, higher inter-class distribution overlap leads to lower multiplicity, reversing trends observed in prior work.

## Executive Summary
This paper introduces a framework for understanding how data processing choices influence predictive multiplicity—the phenomenon where multiple models achieve similar training performance but make conflicting predictions. By reframing data processing as choosing between neighbouring datasets, the authors provide theoretical and empirical insights into how overlap in class distributions affects downstream multiplicity. They prove that under a shared Rashomon parameter, greater inter-class distribution overlap leads to lower multiplicity, reversing trends observed in prior work. The framework is applied to active learning and data imputation, where the authors demonstrate that multiplicity-aware algorithms can effectively steer multiplicity up or down without sacrificing accuracy. Empirical results show that higher missing data ratios amplify the steerability of data imputation, making the framework more impactful in practice.

## Method Summary
The paper proposes a framework where data processing is viewed as selecting among k-neighboring datasets—datasets differing by at most k data points. Under a shared Rashomon parameter ε, the authors prove that higher inter-class distribution overlap produces smaller Rashomon sets and thus lower multiplicity. For active learning, they introduce MultLow/MultHigh algorithms that use committee disagreement to select points that systematically increase or decrease class overlap. For data imputation, they extend these algorithms to select imputation strategies that steer multiplicity. The framework uses histogram binning (5 bins per feature) for density estimation, trains 100 models per dataset, and filters them by L(θ, D_train) ≤ ε to construct Rashomon sets. Ambiguity on held-out test sets measures multiplicity.

## Key Results
- Under shared Rashomon parameter ε, greater inter-class distribution overlap leads to lower multiplicity (Theorem 4.2)
- MultLow/MultHigh active learning algorithms achieve lowest/highest multiplicity while preserving accuracy (Spearman ρ ≈ -0.8)
- Higher missing data ratios amplify steerability of multiplicity-aware imputation algorithms
- Shared ε assumption reverses multiplicity trends compared to task-dependent adaptive thresholds

## Why This Works (Mechanism)

### Mechanism 1: Shared Rashomon Parameter Subset Relationship
- Claim: Under fixed ε shared across neighboring datasets, higher inter-class distribution overlap produces smaller Rashomon sets and thus lower multiplicity.
- Mechanism: Higher overlap → higher Bayes optimal 0-1 loss (L* = ½·OVL) → models satisfying L(θ) ≤ ε on lower-overlap data may exceed threshold → subset relationship Θ(D₁,ε) ⊆ Θ(D₂,ε) forms → fewer models can disagree.
- Core assumption: The "good model" threshold ε should be anchored to the task itself and not vary due to data processing choices.
- Evidence anchors: Proves Θ(D₁_train,ε) ⊆ Θ(D₂_train,ε) when per-point loss dominance holds and OVL₁ ≥ OVL₂; related work shows opposite trend for distinct tasks confirming shared-ε framing is novel.
- Break condition: When comparing entirely different tasks rather than k-neighboring datasets, or when using task-dependent adaptive thresholds.

### Mechanism 2: Confidence-Based Steering via Committee Disagreement
- Claim: A committee of models can identify data points that systematically increase or decrease class overlap when added/labeled.
- Mechanism: MultLow selects points where all committee members have low confidence (ambiguous regions) → adding these points expands ambiguous region → overlap increases → Rashomon set shrinks. MultHigh does the opposite.
- Core assumption: Low-confidence points for all committee members lie near decision boundaries in regions that increase class overlap when incorporated.
- Evidence anchors: MultLow uses c_max(x) = max_i c_i(x), selecting bottom-Q; MultHigh uses c_min(x), selecting top-Q; active learning multiplicity is largely unexplored in literature.
- Break condition: When initial labeled set is too small (n small) or t is large (many acquisition steps), correlation weakens; committee becomes uninformative.

### Mechanism 3: Missing Data Ratio Amplifies Steerability
- Claim: As missing data ratio increases, the control gap between multiplicity-minimizing and maximizing imputation widens.
- Mechanism: More missing values → more degrees of freedom in imputation → neighboring datasets differ more substantially → downstream OVL varies more → multiplicity control range expands.
- Core assumption: Different imputation methods produce meaningfully different downstream class-conditional distributions.
- Evidence anchors: Separation between MultLow and MultHigh becomes visually pronounced at r ≥ 0.10; no direct corpus evidence on this relationship.
- Break condition: When missing patterns are systematic (MAR/MNAR with structure) rather than MCAR, or when features are highly correlated making imputations converge.

## Foundational Learning

- Concept: **Rashomon Set**
  - Why needed here: This is the foundational object—the set of models within ε of optimal loss, from which conflicting predictions arise.
  - Quick check question: If dataset A has higher Bayes optimal loss than dataset B, and both use the same ε, which has the larger Rashomon set?

- Concept: **Overlapping Coefficient (OVL)**
  - Why needed here: The paper uses OVL as the proxy for "difficulty" and the mechanistic driver of multiplicity changes via its relationship to Bayes loss.
  - Quick check question: For two class-conditional distributions with OVL = 0.4, what is the minimum possible 0-1 loss any classifier can achieve?

- Concept: **k-Neighboring Datasets**
  - Why needed here: Frames all data processing as selecting among similar datasets; enables the shared-ε assumption and subset relationship proofs.
  - Quick check question: In active learning with n=1000, T=5 steps, and q=100 per step, what is the maximum k between two resulting datasets?

## Architecture Onboarding

- **Component map:** Raw data → (missingness injection if imputation) → OVL estimation → steering decision → model ensemble training → Rashomon filtering → ambiguity measurement
- **Critical path:** Raw data → (missingness injection if imputation) → OVL estimation → steering decision → model ensemble training → Rashomon filtering → ambiguity measurement
- **Design tradeoffs:**
  - Shared vs. adaptive ε: Shared enables cross-dataset comparison but may exclude reasonable models; adaptive preserves model count but breaks subset proofs
  - Committee size K: Larger K improves confidence estimates but O(K×training cost)
  - Histogram bin count: More bins = finer OVL estimation but requires more data per bin
- **Failure signatures:**
  - Spearman correlation (OVL vs. ambiguity) near zero or positive when k ≈ n (not "neighboring")
  - MultLow/MultHigh indistinguishable from random when n small (e.g., n=500, t=5)
  - Empty Rashomon set (ε too tight) or near-complete retention (ε too loose)
- **First 3 experiments:**
  1. **Correlation sanity check**: Run 10 random active learning trajectories on your dataset; compute OVL after each step and ambiguity on test set; verify negative Spearman correlation (expect ρ ≈ -0.3 to -0.8 per Figure 4).
  2. **Shared-ε ablation**: For the same trajectory, compute ambiguity using (a) shared ε across all datasets, (b) adaptive ε ensuring 50-model minimum; confirm trend reversal occurs.
  3. **Steerability gradient test**: Run MultLow/MultHigh imputation at r ∈ {0.01,0.05,0.15,0.25}; plot ambiguity gap; confirm widening separation at higher r.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the neighbouring datasets framework be formally extended beyond binary classification to multi-class settings?
- Basis in paper: The conclusion states the extension to problems beyond binary classification is non-trivial as the overlapping coefficient does not generalize straightforwardly to multi-class problems.
- Why unresolved: The overlapping coefficient used to measure inter-class distribution overlap does not generalize straightforwardly to multi-class problems, and the theoretical proofs rely on binary class structure.
- What evidence would resolve it: A formal definition of multi-class "difficulty" measures and theoretical results showing whether the higher-overlap-lower-multiplicity trend persists in multi-class settings.

### Open Question 2
- Question: What is the formal theoretical connection between the neighbouring datasets framework and differential privacy?
- Basis in paper: The conclusion identifies establishing a formal connection between the neighbouring dataset framework and differential privacy as future research.
- Why unresolved: While the framework draws inspiration from differential privacy's notion of neighbouring datasets, the mathematical relationship between multiplicity and privacy guarantees remains unexplored.
- What evidence would resolve it: Formal theorems connecting privacy parameters to multiplicity bounds, or empirical studies showing how privacy-preserving mechanisms affect downstream multiplicity.

### Open Question 3
- Question: Can alternative definitions of neighbouring datasets based on L1/L2 distances better capture relationships to robustness and distribution shift?
- Basis in paper: The conclusion proposes revisiting the definition of neighbouring datasets, as alternatives based on L1/L2 distances may offer a closer alignment with robustness literature.
- Why unresolved: The current k-neighbouring definition counts discrete point differences, but continuous distance metrics may better capture perturbation-based notions from adversarial robustness.
- What evidence would resolve it: Comparative analysis showing L1/L2-based neighbouring definitions yield different multiplicity predictions or better alignment with robustness phenomena.

### Open Question 4
- Question: Under what conditions can Conjecture 4.1 be formally proven for general k-neighbouring datasets?
- Basis in paper: The paper presents Conjecture 4.1 as an extension of Theorem 4.2, noting it "remains provable under strong assumptions... However, as k increases, such an assumption becomes increasingly unrealistic."
- Why unresolved: The proof technique for 1-neighbouring datasets requires assumptions about loss dominance at each differing point, which become impractical as k grows.
- What evidence would resolve it: Either a proof for general k-neighbouring datasets under weaker assumptions, or identification of the precise boundary conditions where the conjecture holds.

## Limitations

- The shared Rashomon parameter assumption is critical to theoretical claims but not directly validated across diverse datasets
- Active learning multiplicity results rely on weak prior literature with limited empirical grounding
- Empirical steerability in imputation lacks independent replication and broader validation

## Confidence

- Shared ε subset relationship: High (theoretically proven with clear assumptions)
- OVL as driver of multiplicity: Medium (strong theoretical link, moderate empirical support)
- Steerability via MultLow/MultHigh: Medium (empirically demonstrated but sensitive to hyperparameters)
- Imputation amplification effect: Low-Medium (novel finding with limited corpus evidence)

## Next Checks

1. **Cross-dataset robustness**: Apply the framework to datasets with varying class balance and feature distributions to test the generality of the shared ε assumption.
2. **Hyperparameter sensitivity**: Systematically vary committee size K, histogram bin count, and Rashomon set threshold ε to quantify their impact on multiplicity steering.
3. **Real-world missingness patterns**: Evaluate the imputation framework on datasets with structured missingness (MAR/MNAR) rather than purely random, to test break conditions.