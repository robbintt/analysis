---
ver: rpa2
title: Can AI be Accountable?
arxiv_id: '2510.26057'
source_url: https://arxiv.org/abs/2510.26057
tags:
- accountability
- accountable
- forum
- agent
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores accountability in AI, emphasizing the need for
  users to be protected from the powerful, yet often inscrutable, nature of AI systems.
  It defines accountability as the ability of a forum to request information, discuss
  actions, and sanction an AI agent if necessary.
---

# Can AI be Accountable?

## Quick Facts
- arXiv ID: 2510.26057
- Source URL: https://arxiv.org/abs/2510.26057
- Authors: Andrew L. Kun
- Reference count: 20
- Primary result: Defines accountability for AI as the ability of a forum to request information, debate actions, and sanction an AI agent, and models it as an Accountable AI Markov chain.

## Executive Summary
This paper argues that AI systems must be held accountable to protect users from their inscrutable and powerful nature. Accountability is defined as the ability of a forum to request information, discuss actions, and impose sanctions on an AI agent when necessary. The paper introduces the Accountable AI Markov chain, a conceptual model that maps the workflow of AI accountability from task delegation to sanctions and updates. Through examples like the pymints audit, it demonstrates how accountability can be achieved, while also highlighting challenges such as information asymmetry, power imbalances, and conflicts with privacy or safety requirements.

## Method Summary
The paper constructs a conceptual Accountable AI Markov chain to model the accountability process as a discrete-state system. It defines seven states—Delegate task, Request to explain, Response, Debate, Judgment, Sanction, and Updates—and the directed transitions between them. The model is informed by real-world audit workflows (e.g., pymints) but does not specify transition probabilities or termination conditions, leaving the dynamics abstract. The goal is to provide a framework for understanding how accountability flows from action to sanction and eventual updates, rather than to simulate actual processes quantitatively.

## Key Results
- Accountability requires the ability to request information, debate, and impose sanctions, not just transparency.
- The Accountable AI Markov chain maps accountability workflows and highlights the need for cycles (e.g., Updates → Delegate) to enable continuous improvement.
- Current AI accountability is hindered by power imbalances, lack of effective sanctions, and conflicts with privacy or safety requirements.

## Why This Works (Mechanism)
The Accountable AI Markov chain works by formalizing the accountability process as a finite-state system, where each state represents a step in the audit or oversight process. The directed transitions model the logical flow from task delegation to potential sanctions and updates, capturing both linear and cyclic paths. This structure allows for the representation of real-world audit workflows (e.g., pymints) and highlights where accountability can be short-circuited or stalled. By making the process explicit, the model reveals gaps (like missing debate phases) and provides a basis for designing mechanisms to enforce accountability.

## Foundational Learning
- **Accountability as sanction**: Accountability isn't just transparency—it's the ability to impose consequences. *Why needed*: Without sanctions, powerful actors (AI developers) can evade oversight. *Quick check*: Does the audit process include a clear path to judgment and sanction?
- **Information asymmetry**: Users often lack the technical knowledge to understand AI decisions. *Why needed*: This gap enables exploitation and hinders effective oversight. *Quick check*: Are audit reports written in accessible language?
- **Cyclic accountability**: Updates to AI systems must feed back into the accountability loop. *Why needed*: Without revisiting after updates, accountability is incomplete. *Quick check*: Does the process loop back to "Delegate" after "Updates"?

## Architecture Onboarding
- **Component map**: Delegate task -> Request to explain -> Response -> Debate -> Judgment -> Sanction -> Updates -> (back to Delegate)
- **Critical path**: The main accountability flow is Request -> Response -> Debate -> Judgment -> Sanction. Deviations (e.g., skipping Debate) can short-circuit accountability.
- **Design tradeoffs**: The model trades quantitative precision for conceptual clarity—no transition probabilities means it can't predict outcomes, but it can guide audit design.
- **Failure signatures**: Infinite loops (e.g., Judgment -> Request -> Response -> Judgment) indicate stalled accountability. Lack of debate leads to superficial oversight.
- **First experiments**:
  1. Map a real audit (e.g., pymints) onto the seven states and trace the actual transitions.
  2. Simulate the chain with uniform probabilities to check for connectivity and termination.
  3. Identify where real audits skip the "Debate" state and assess impact on outcomes.

## Open Questions the Paper Calls Out
- **How can the "debate and discussion" phase of the Accountable AI Markov chain be quantitatively evaluated, particularly for non-professional users?** The paper notes there are very few studies providing quantitative evaluations of discussions in the accountability process, and this is critical for future AI interaction research.
- **How can accountability requirements be reconciled with conflicts in other responsible AI principles, such as safety, privacy, and intellectual property?** The paper highlights that information disclosure for accountability could expose system information that conflicts with safety, legal protections, or privacy requirements.
- **What effective socio-technical mechanisms can enforce sanctions against AI developers when market pressures and legal requirements are insufficient?** The paper argues that neither market pressures nor legal requirements are powerful enough to reliably make today’s AI products accountable, and without the ability to sanction, there is no true accountability.

## Limitations
- The model is conceptual, with no transition probabilities or termination conditions, so it cannot simulate actual accountability workflows.
- It does not address how to resolve conflicts between accountability and other AI principles (e.g., privacy, safety).
- The paper does not propose concrete mechanisms for enforcing sanctions when current market and legal frameworks are insufficient.

## Confidence
- **High**: The conceptual framework and state definitions are clearly specified and reproducible.
- **Medium**: The model's dynamics (transition probabilities, termination) are unspecified, limiting its practical utility and predictive power.
- **Low**: The paper does not provide empirical validation or quantitative measures for the accountability process.

## Next Checks
1. Assign plausible probabilities to transitions based on qualitative descriptions and test if the resulting chain converges to a sanction/update state.
2. Check for cycles and termination by simulating the chain and identifying conditions under which accountability processes terminate or loop indefinitely.
3. Validate the mapping from case studies (e.g., pymints) to the model by tracing the actual audit steps through the defined states and transitions.