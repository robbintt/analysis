---
ver: rpa2
title: 'PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels'
arxiv_id: '2506.07606'
source_url: https://arxiv.org/abs/2506.07606
tags:
- stance
- user
- dataset
- posts
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PolitiSky24 addresses the lack of user-level stance detection datasets
  for emerging platforms like Bluesky by providing the first stance detection dataset
  for the 2024 U.S. presidential election, focusing on Kamala Harris and Donald Trump.
---

# PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels

## Quick Facts
- arXiv ID: 2506.07606
- Source URL: https://arxiv.org/abs/2506.07606
- Reference count: 20
- 16,044 user-target stance pairs for 2024 U.S. election (Harris/Trump) with 81% labeling accuracy

## Executive Summary
PolitiSky24 introduces the first user-level stance detection dataset for the Bluesky social platform, focusing on the 2024 U.S. presidential election between Kamala Harris and Donald Trump. The dataset contains 16,044 user-target stance pairs enriched with engagement metadata, interaction graphs, and user posting histories. A novel pipeline combining information retrieval and large language models generates stance labels with supporting rationales and text spans, achieving 81% accuracy. This resource addresses critical gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective.

## Method Summary
The dataset was created using a two-step pipeline: first, an embedding model (KaLM-mini-v1.5) retrieves the top stance-relevant posts from each user's history, then a large language model (DeepSeek-Chat-v3) classifies the user's stance toward each target candidate. The system outputs stance labels (Favor/Against/Neither) along with reasoning explanations and supporting text spans. Validation was performed on a 5% hashtag-sampled subset (446 users) to select optimal models before full dataset labeling.

## Key Results
- 16,044 user-target stance pairs covering 8,467 users
- 81% accuracy in stance classification
- Error breakdown: 21.0% context errors, 32.9% reasoning errors, 19.2% subject misclassification
- Most errors occur when users express neither favor nor opposition to candidates

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Stance Labeling
- Selectively retrieving stance-relevant posts from a user's history before LLM classification improves accuracy over using raw or randomly sampled history.
- An embedding model (KaLM-mini-v1.5) retrieves the top-k posts most relevant to each stance query (e.g., "I am in favor of Donald Trump"), creating a condensed but signal-rich context window for the LLM.
- Core assumption: Stance information is sparse in user histories; relevant posts can be identified via semantic similarity before classification.
- Evidence: [abstract] "A pipeline combining advanced information retrieval and large language models... achieves 81% accuracy"
- Break condition: If users' stance-relevant posts are semantically dissimilar to query phrasing (e.g., heavy sarcasm, coded language), retrieval fails and LLM receives uninformative context.

### Mechanism 2: Validation-Guided Component Selection
- Evaluating embedding models and LLMs on a human-annotated validation set before full deployment enables systematic pipeline optimization.
- A 5% hashtag-sampled validation set (446 users) is used to benchmark embedding models for retrieval and LLMs for classification; best performers are selected for full dataset labeling.
- Core assumption: Hashtag-based sampling yields validation users representative of broader population stance distributions.
- Evidence: [section 4.2.1] "we used random sampling to select 446 users—approximately 5% of all users—who had used the identified hashtags"
- Break condition: If hashtag-using users systematically differ from non-hashtag users (e.g., more polarized), validation performance may not generalize.

### Mechanism 3: Explainable Outputs via Rationale Generation
- Requiring LLMs to output reasoning and text spans alongside labels enables error diagnosis and transparency.
- The LLM prompt explicitly requests stance label, reasoning explanation, and three supporting text spans; this structure supports downstream error categorization.
- Core assumption: LLM-generated rationales faithfully reflect actual decision factors rather than post-hoc rationalization.
- Evidence: [abstract] "generates stance labels with supporting rationales and text spans for transparency"
- Break condition: If LLM rationales are hallucinated or inconsistent with actual decision process, error analysis becomes misleading.

## Foundational Learning

- **Stance Detection (User-Level vs. Post-Level)**
  - Why needed here: PolitiSky24 aggregates across a user's posting history rather than classifying individual posts; understanding this distinction is prerequisite to interpreting the task design.
  - Quick check question: Given a user who posts both pro-candidate and anti-candidate content, what user-level stance label would capture this?

- **Information Retrieval (Embedding Models, ANN Search)**
  - Why needed here: The pipeline depends on semantic similarity search to surface stance-relevant posts; without this, LLM context windows would be noisy or truncated.
  - Quick check question: Why might a keyword-based retrieval (e.g., BM25 alone) underperform for stance-relevant post retrieval?

- **LLM Prompting for Structured Classification**
  - Why needed here: The system uses a specific prompt template to enforce consistent output format (label, rationale, text spans); understanding prompt design is essential for replication.
  - Quick check question: What prompt-level constraints would help enforce valid stance labels (Favor/Against/Neither) and reduce output format errors?

## Architecture Onboarding

- **Component map:**
  1. Data Collection: Bluesky feeds → user post histories → engagement networks (likes, reposts, follows)
  2. Validation Set: Hashtag-based sampling → human annotation (stance relevancy + user stance)
  3. Retrieval Module: Embedding model (KaLM-mini-v1.5) + ANN search → top-k stance-relevant posts per user-target pair
  4. Classification Module: LLM (DeepSeek-Chat-v3) → stance label + rationale + text spans
  5. Output: 16,044 user-target stance pairs with metadata

- **Critical path:**
  Validation set creation → Embedding model evaluation → LLM evaluation → Full dataset labeling. Errors propagate: poor retrieval starves the LLM; poor LLM reasoning mislabels users.

- **Design tradeoffs:**
  - Retrieval budget (top-5 favor + top-5 against vs. larger context): Paper uses 10 posts; increasing may help ambiguous cases but costs more tokens.
  - LLM choice (accuracy vs. cost): DeepSeek-v3 achieves 81.2% accuracy; smaller models (e.g., Phi-4 at 74.4%) trade accuracy for efficiency.
  - Validation set size (5% vs. larger): Smaller validation is efficient but may underrepresent rare stance combinations.

- **Failure signatures:**
  - "Neither" misclassified as "Favor" (19.2% error rate): Neutral reporting with positive language triggers false positives.
  - Context errors (21.0%): Retrieved posts lack stance signal; user may not express explicit opinions.
  - Reasoning errors (32.9%): Sarcasm, irony, or complex conditionals confuse the LLM.

- **First 3 experiments:**
  1. Retrieval ablation: Compare KaLM-mini-v1.5 vs. BM25 vs. hybrid retrieval on validation stance relevancy dataset; measure Precision@10 and impact on downstream stance accuracy.
  2. Context window sweep: Test top-3, top-5, top-10, top-20 retrieved posts per stance direction; track accuracy, confidence, and token cost.
  3. Error stratification by user activity: Bin users by post count; evaluate whether low-activity users have higher error rates due to sparse stance signal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating user graph networks (likes, reposts, follows) improve stance detection accuracy beyond content-based LLM labeling?
- Basis in paper: [explicit] "For future work, we aim to leverage users' graph networks to assist with data labeling."
- Why unresolved: The current pipeline uses only retrieved post content; the collected interaction graphs remain unexploited for stance inference.
- What evidence would resolve it: Comparative experiments showing accuracy gains when graph-based features are combined with the existing LLM pipeline.

### Open Question 2
- Question: How do user stances toward political candidates evolve across different time windows during and after election periods?
- Basis in paper: [explicit] "We plan to apply our pipeline across different time windows to examine how users' stances evolve over time."
- Why unresolved: The dataset provides a cross-sectional snapshot; temporal dynamics of stance change are not analyzed.
- What evidence would resolve it: Longitudinal tracking of individual users' stance labels across pre-election, election, and post-election periods.

### Open Question 3
- Question: Does hashtag-based validation set selection systematically bias model evaluation toward users with more polarized stances?
- Basis in paper: [inferred] From Limitations: "Users who explicitly utilize hashtags could represent a particular subset...potentially exhibiting more extreme or clearly articulated stances."
- Why unresolved: The validation set relies on hashtag-based sampling, which may overrepresent explicit stance expressers compared to the general population.
- What evidence would resolve it: Comparison of label distributions and model performance between hashtag-sampled and randomly-sampled validation sets.

## Limitations
- Retrieval may fail for users expressing opinions through indirect language, sarcasm, or infrequent posting
- Validation set drawn from hashtag-using users may not represent the full user population
- LLM-generated rationales may not faithfully reflect the true decision process

## Confidence
- **High confidence**: Data collection methodology, retrieval-augmented labeling pipeline, and error categorization framework
- **Medium confidence**: Validation set representativeness and generalizability of the 81% accuracy figure
- **Low confidence**: Reliability of LLM rationales as faithful explanations for stance decisions

## Next Checks
1. **Retrieval relevance audit**: Sample 100 user-target pairs and manually assess whether the top-5 retrieved posts per stance direction are truly stance-relevant; compute Precision@5 to quantify retrieval quality.
2. **Validation set generalization**: Construct an independent test set from non-hashtag users and compare stance label accuracy to the original validation set to detect sampling bias.
3. **Rationale faithfulness evaluation**: For a subset of 50 labeled users, conduct a blind review where human annotators judge whether the LLM's rationales accurately describe the evidence in the retrieved posts.