---
ver: rpa2
title: 'Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence
  Through Multimodal Story Generation'
arxiv_id: '2508.16762'
source_url: https://arxiv.org/abs/2508.16762
tags:
- cultural
- uni00000003
- uni00000013
- uni00000048
- uni0000004f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive evaluation of cultural
  competence in Vision-Language Models (VLMs) through multimodal story generation.
  The authors develop a novel framework that perturbs cultural identity cues while
  maintaining constant visual inputs across five contemporary VLMs and 42 countries,
  systematically assessing how cultural identity affects model outputs.
---

# Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation

## Quick Facts
- **arXiv ID:** 2508.16762
- **Source URL:** https://arxiv.org/abs/2508.16762
- **Reference count:** 40
- **Primary result:** First comprehensive evaluation of cultural competence in VLMs through multimodal story generation across 5 models and 42 countries.

## Executive Summary
This paper presents the first systematic evaluation of cultural competence in Vision-Language Models through multimodal story generation. The authors develop a framework that perturbs cultural identity cues while maintaining constant visual inputs across five contemporary VLMs and 42 countries. The study reveals that VLMs demonstrate significant cultural adaptation capabilities with systematic lexical variation, but prioritize cultural safety over authentic representation, and exhibit architectural bias in automated evaluation metrics that contradict human assessments.

## Method Summary
The study uses a novel framework that perturbs cultural identity cues (nationality) while maintaining constant visual inputs to evaluate cultural competence in VLMs. The method involves scraping culturally-relevant images from Google Images using custom queries, generating stories across 5 VLMs (Gemma3 4B/12B, Qwen 2.5 VL 7B, InternVL3 8B, SmolVLM2 2.2B) with 73,500 total stories, and evaluating outputs through automated metrics (CLIPScore, lexical analysis), cross-modal retrieval, and human evaluation on 10 cultural competence dimensions. Cultural alignment is assessed using Hofstede's Cultural Dimensions and World Values Survey frameworks.

## Key Results
- Significant cultural adaptation capabilities across all models with F-statistics ranging from 1540 to 8707 (p < 1e-48) demonstrating systematic lexical variation
- Rich culturally-specific vocabulary spanning names, familial terms, and geographic markers present in model outputs
- Cross-modal evaluation reveals culturally distinct outputs are detectable through visual-semantic similarity (28.7% within-nationality vs. 0.2% cross-nationality recall)
- Human evaluation reveals critical limitations: automated metrics show architectural bias, models exhibit inverse cultural alignment, and larger models achieve higher cultural authenticity scores

## Why This Works (Mechanism)

### Mechanism 1: Conditional Cultural Activation via Prompting
- **Claim:** VLMs appear to store cultural knowledge as conditional probability distributions that are activated by explicit nationality cues in text prompts, triggering systematic lexical shifts.
- **Mechanism:** When the prompt specifies a nationality (e.g., "for an Indian kid"), the model conditions its generation on embeddings associated with that label. This restricts the sampling space to culturally relevant tokens (names like "Priya," food like "pierogi") rather than generic outputs.
- **Core assumption:** The model's pre-training data contains distinct, statistically separable clusters of lexical co-occurrences for different nationalities.
- **Evidence anchors:**
  - [Section 4.1]: Reports significant F-statistics (1540–8707, p < 1e-48) demonstrating that nationality perturbation causes systematic lexical variation, confirming non-random adaptation.
  - [Section 4.2]: TF-IDF analysis shows nationality-specific adjectives and names (e.g., "Priya," "Omar") appearing consistently across different model architectures.
  - [Corpus]: Neighbor paper *CultureVLM* confirms VLMs often misinterpret cultural symbols due to Western-centric training data, implying cultural knowledge is present but unevenly distributed.
- **Break condition:** If the prompt cue is removed or ambiguous, lexical variance should drop to random baseline levels, and culturally specific terms should disappear.

### Mechanism 2: Safety-Superiority Alignment Constraint
- **Claim:** VLMs prioritize "stereotype avoidance" over "cultural nuance" due to safety alignment objectives, resulting in outputs that are culturally safe but superficially authentic.
- **Mechanism:** Reinforcement Learning from Human Feedback (RLHF) or similar alignment techniques likely penalize outputs that risk stereotyping. This forces the model to settle for surface-level cultural markers (names, foods) rather than deep narrative structures reflecting an "insider perspective," as this minimizes the risk of violating safety guidelines.
- **Core assumption:** The alignment process creates a trade-off where maximizing safety scores inherently suppresses specific, nuanced cultural expression.
- **Evidence anchors:**
  - [Section 4.5]: Human evaluation shows models score significantly higher on "Stereotype Avoidance" (e.g., Gemma3 12B: 7.86/10) than on "Cultural Nuance" (5.10/10) or "Insider Perspective" (4.34/10).
  - [Abstract]: Notes that models "prioritize cultural safety over authentic representation."
  - [Corpus]: *SocialFusion* mentions VLMs struggle to unify social perception tasks, potentially relevant to balancing safety vs. nuance.
- **Break condition:** If a model is trained without safety alignment (e.g., a base pre-trained model), we would expect higher nuance scores but potentially lower safety/steretype avoidance scores.

### Mechanism 3: Evaluation Metric Decoupling (The CLIPScore Bias)
- **Claim:** Automated visual-semantic metrics like CLIPScore exhibit architectural bias that creates an "inverse scaling" relationship with human-perceived cultural competence.
- **Mechanism:** SmolVLM2 (2.2B) uses SigLIP for image understanding, which may align more closely with CLIPScore's embedding space than the architectures of larger models. This creates artificial metric inflation for smaller, less culturally competent models, decoupling the automated score from the actual quality of the generated story.
- **Core assumption:** CLIPScore captures semantic similarity to the image but fails to capture the *cultural coherence* of the text relative to the image, or it shares specific biases with the vision encoder of the smaller model.
- **Evidence anchors:**
  - [Section 4.4]: SmolVLM2 shows strong positive CLIP correlations, while human evaluation rates it lowest (4.05/10 vs. 6.81/10 for Gemma3 12B).
  - [Section 4.5]: Explicitly states "CLIPScore exhibits architectural bias... contradicting human assessments."
  - [Corpus]: Weak or missing direct evidence in corpus for this specific inverse relationship; rely on [Section 4.5].
- **Break condition:** If a different visual-semantic metric (e.g., BLIP-2 score or a human-in-the-loop metric) is used, the correlation between model size/quality and metric score should normalize (positive scaling).

## Foundational Learning

- **Concept: Hofstede’s Cultural Dimensions (HCD) & World Values Survey (WVS)**
  - **Why needed here:** These provide the ground-truth coordinate system for "cultural distance." The paper uses them to test if a model generates similar stories for culturally similar countries (e.g., Australia/New Zealand). Understanding these is necessary to interpret the correlation results (Kendall’s tau).
  - **Quick check question:** If Model X generates stories where the text-similarity distance between Japan and the US matches their HCD distance, does this imply positive or negative cultural alignment?

- **Concept: Analysis of Variance (ANOVA) for Lexical Evaluation**
  - **Why needed here:** The paper relies on ANOVA to prove that the model’s generation shifts are *systematic* (signal) rather than random (noise). Understanding F-statistics and p-values is required to verify the validity of the "cultural adaptation" claim.
  - **Quick check question:** In the paper's ANOVA setup, what does a high F-value (>1500) specifically tell us about the relationship between "within-nationality" variance and "across-nationality" variance?

- **Concept: Cross-Modal Retrieval (Recall@K)**
  - **Why needed here:** This is the mechanism used to test if the generated story "belongs" to the culture it was generated for. It measures if the story embedding is closer to the correct cultural anchor than to others.
  - **Quick check question:** Why does the paper report both "Within-Nationality Recall" and "Cross-Nationality Recall," and what does the gap between them (e.g., 28.7% vs 0.2%) signify?

## Architecture Onboarding

- **Component map:** Input Layer (Text Prompt + Image) -> Processing Layer (VLM Backbone) -> Evaluation Layer (Automated Metrics, Alignment Analysis, Human Evaluation)
- **Critical path:**
  1. **Data Curation:** Scraping images via custom query ("concept for [nationality] kid") and filtering via human review (Section 3.1).
  2. **Generation:** Running 5 VLMs with varying temperatures (0.3, 0.7) to produce 73,500 stories.
  3. **Validation:** Computing F-statistics to confirm adaptation, then running Human Eval to validate automated metrics.
- **Design tradeoffs:**
  - **Prompting vs. Fine-tuning:** The study uses prompting (perturbation) which is cheaper and tests *intrinsic* model knowledge, but may elicit shallower cultural depth than fine-tuning on cultural corpora.
  - **Metric Selection:** CLIPScore is automated and scalable but proved biased. Human eval is accurate but not scalable. A hybrid or new metric is needed.
  - **Scope:** English-only evaluation limits applicability to multilingual cultural nuances.
- **Failure signatures:**
  - **Inverse Cultural Alignment:** Negative correlation (Kendall’s tau) between story similarity and cultural distance (e.g., Gemma3 models giving similar stories to culturally distant countries).
  - **Ethnic Majoritarianism:** Models default to majority group names (e.g., Hindi names for India, Yoruba/Igbo for Nigeria), ignoring internal diversity.
  - **Visual Hallucination:** SmolVLM2 confusing Berlin Cathedral with the TV Tower ("Fernsehturm"), indicating a failure in visual grounding.
- **First 3 experiments:**
  1. **Baseline Check:** Run the perturbation experiment on a new VLM (not in the study). Calculate the ANOVA F-statistic to see if it exceeds the ~1500 threshold for "systematic adaptation."
  2. **Metric Stress Test:** Take the SmolVLM2 outputs and re-evaluate them using a different visual-semantic similarity metric (e.g., BLIP or a VQA-based approach) to see if the architectural bias persists.
  3. **Ablation Study:** Remove the image input and run the text-only prompt. Compare the lexical variance (F-stat) and cultural alignment to the multimodal case to isolate the contribution of the visual signal.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** When cultural adaptation in generated text doesn't align with cultural elements present in input images, how do users perceive authenticity?
- **Basis in paper:** [explicit] The authors explicitly ask: "Are culturally-adapted stories genuinely authentic to their visual contexts, or do generic images combined with culturally-specific narratives feel forced or inauthentic?"
- **Why unresolved:** The study evaluates cultural adaptation in text outputs but does not measure user perception when text-image cultural alignment mismatches.
- **What evidence would resolve it:** User studies presenting participants with stories where cultural cues in text either match or mismatch visual cultural elements, measuring perceived authenticity ratings.

### Open Question 2
- **Question:** Do established cultural psychology frameworks (Hofstede's Cultural Dimensions, World Values Survey) adequately capture narrative-based cultural competence in VLM outputs?
- **Basis in paper:** [explicit] The authors state: "The fundamental question remains whether established cultural psychology frameworks adequately capture narrative-based cultural competence."
- **Why unresolved:** Only 31% of countries showed consistent correlation signs between HCD and WVS frameworks, suggesting they capture orthogonal aspects of cultural competence.
- **What evidence would resolve it:** Development and validation of narrative-specific cultural evaluation frameworks that better predict human judgments of cultural authenticity.

### Open Question 3
- **Question:** What alternatives to CLIPScore can accurately measure cross-modal cultural competence without architectural bias?
- **Basis in paper:** [inferred] The paper shows CLIPScore exhibits architectural bias—SmolVLM2 scores well automatically but poorly with humans (4.05/10 vs. 6.81/10)—and states: "We hope future work will introduce better alternatives to accurately measure cross-modal cultural competence."
- **Why unresolved:** Current automated metrics contradict human assessments, undermining validity for cross-cultural VLM evaluation.
- **What evidence would resolve it:** New evaluation metrics that show consistent rankings with human cultural competence judgments across diverse VLM architectures.

### Open Question 4
- **Question:** Why do some VLMs exhibit inverse cultural alignment, generating more similar stories for culturally distant countries?
- **Basis in paper:** [inferred] Gemma models show "consistently negative medians (-0.08 to -0.10)" in HCD correlations, suggesting inverse relationships where culturally distant countries receive more similar stories.
- **Why unresolved:** The paper documents this phenomenon but does not identify its causes or training data properties that produce it.
- **What evidence would resolve it:** Systematic analysis of training data composition and model architecture features that correlate with positive vs. inverse cultural alignment patterns.

## Limitations
- **English-only evaluation:** The study only evaluates outputs in English, potentially missing multilingual cultural nuances.
- **Ethnic majoritarianism:** Models default to majority group names within countries, ignoring internal cultural diversity.
- **Measurement problems:** Automated metrics like CLIPScore exhibit architectural bias that contradicts human assessment of cultural competence.

## Confidence
- **Confidence: Medium** - The paper's core claims about cultural adaptation (F-statistics 1540-8707) are statistically robust, but the cultural competence assessment relies heavily on English-only evaluation, potentially missing multilingual nuances.
- **Confidence: Low** - The inverse relationship between automated metrics (CLIPScore) and human evaluation reveals a critical measurement problem with incomplete understanding of underlying mechanisms.
- **Confidence: High** - The finding that models prioritize cultural safety over authentic representation is well-supported through human evaluation showing consistently higher scores for stereotype avoidance.

## Next Checks
1. **Metric Validation Study**: Conduct head-to-head comparison of CLIPScore against alternative visual-semantic metrics (BLIP, SigLIP) on the same output set to quantify architectural bias across different model families and establish which metric best correlates with human cultural competence judgments.

2. **Cultural Diversity Stress Test**: Re-run the evaluation with prompts specifically targeting internal cultural diversity within countries (e.g., multiple ethnic groups for India, regional variations for China) to assess whether models exhibit majoritarianism or can capture intra-national cultural variation.

3. **Cross-Lingual Extension**: Translate prompts and evaluate model outputs in languages native to target cultures to determine if cultural competence varies significantly with language, potentially revealing gaps in the English-centric assessment approach.