---
ver: rpa2
title: 'BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for
  Offline Reinforcement Learning'
arxiv_id: '2506.05762'
source_url: https://arxiv.org/abs/2506.05762
tags:
- bitrajdiff
- offline
- trajectories
- trajectory
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data augmentation for offline
  reinforcement learning (RL), where static datasets often exhibit distribution bias,
  limiting generalizability. The authors propose Bidirectional Trajectory Diffusion
  (BiTrajDiff), a novel framework that generates both forward-future and backward-history
  trajectories from intermediate states using diffusion models.
---

# BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.05762
- Source URL: https://arxiv.org/abs/2506.05762
- Reference count: 22
- Outperforms state-of-the-art data augmentation baselines on D4RL benchmarks

## Executive Summary
BiTrajDiff addresses the challenge of data augmentation for offline reinforcement learning, where static datasets often exhibit distribution bias limiting generalizability. The authors propose a novel bidirectional trajectory generation framework that generates both forward-future and backward-history trajectories from intermediate states using diffusion models. This approach enables the synthesis of novel trajectories connecting previously unreachable states, expanding behavioral diversity beyond what forward-only methods can achieve. Extensive experiments on D4RL benchmark suites demonstrate that BiTrajDiff consistently outperforms state-of-the-art data augmentation baselines across various offline RL algorithms, achieving significant performance improvements in locomotion, navigation, and manipulation tasks.

## Method Summary
BiTrajDiff employs two separate diffusion models (forward and backward) with 1D DiT backbones to generate state trajectories conditioned on an anchor state and target return. The forward model generates future states while the backward model generates history states, which are then stitched at the anchor point. Generated state-only trajectories are completed with actions and rewards using separately trained inverse dynamics and reward models. A two-stage filtering process (OOD filter via Isolation Forest followed by greedy reward-based selection) ensures high-quality augmented data. The final augmented dataset, mixed at 30% ratio with the original data, is used to train standard offline RL algorithms.

## Key Results
- Achieves consistent performance improvements across D4RL locomotion, maze, and kitchen tasks
- Outperforms state-of-the-art data augmentation baselines (DAWM, DyFA, DAT) across multiple offline RL algorithms (IQL, TD3BC, CQL, DT)
- 30% augmentation ratio provides optimal balance between diversity and stability
- Bidirectional generation provides significant advantages over unidirectional approaches

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Trajectory Stitching for Global State Connectivity
The framework generates forward-future and backward-history trajectories from intermediate anchor states, enabling global connectivity between previously unreachable states. Two separate diffusion models create sequences that are stitched at the anchor state to form longer, more diverse trajectories than possible with forward-only generation.

### Mechanism 2: Trajectory Completion with Inverse Dynamics and Reward Models
Generated state-only trajectories are made compatible with standard offline RL algorithms through action and reward inference using separately trained inverse dynamics and reward models, allowing the use of any standard offline RL algorithm on the augmented data.

### Mechanism 3: Quality Control via OOD and Greedy Filtering
A two-stage filtering process removes low-quality, out-of-distribution, or suboptimal generated trajectories. An Isolation Forest detects OOD sequences, followed by greedy filtering that retains only trajectories with highest predicted rewards, stabilizing downstream RL training.

## Foundational Learning

- **Diffusion Probabilistic Models (Denoising Process)**: BiTrajDiff uses diffusion models to generate trajectory sequences; understanding forward noising and reverse denoising is essential for comprehending how coherent state sequences are generated.
  - *Quick check*: Can you explain how a diffusion model generates a new data sample from random noise using a learned denoising network?

- **Classifier-Free Guidance (CFG)**: The paper uses CFG to condition trajectory generation on starting state and target return; this mechanism controls the diffusion model's output.
  - *Quick check*: How does Classifier-Free Guidance allow a single diffusion model to generate samples conditioned on a specific attribute (like 'high return') without retraining?

- **The Out-of-Distribution (OOD) Problem in Offline RL**: The conservative approach (filtering) prevents the offline RL policy from querying values for state-action pairs far from the original dataset, which causes value overestimation.
  - *Quick check*: Why does function approximation in offline RL often lead to overestimation of Q-values for actions not present in the static dataset?

## Architecture Onboarding

- **Component map**: Anchor State -> Forward Diffusion Model + Backward Diffusion Model -> Stitched State Trajectory -> Inverse Dynamics Model + Reward Model -> Completed Trajectory -> OOD Filter -> Greedy Filter -> Augmented Dataset -> Offline RL Algorithm

- **Critical path**: 
  1. Train forward and backward trajectory diffusion models on the offline dataset
  2. Train IDM and RM on the same dataset
  3. Generate: Sample anchor state, generate forward/backward sequences, stitch, complete with IDM/RM
  4. Filter: Apply OOD filter, then greedy filter to completed trajectories
  5. Train: Mix augmented dataset with original and train downstream offline RL policy

- **Design tradeoffs**: 
  - Bidirectional vs. Forward-only: Bidirectional is more computationally expensive but aims for higher diversity and global connectivity
  - Augmented Data Ratio (Ïƒ): 30% provides optimal balance; too little provides insufficient new patterns, too much introduces excessive noise
  - Separate vs. Joint Generation: Generating states first, then actions/rewards simplifies diffusion modeling but introduces IDM/RM error

- **Failure signatures**:
  - Training instability: Generated trajectories are of poor quality, check OOD filter efficacy
  - Low performance improvement: Generated trajectories lack diversity or are misvalued by RM, greedy filter may over-select based on erroneous rewards
  - Physically implausible trajectories: Backward diffusion model fails to learn valid reverse dynamics or stitching at anchor point is poor

- **First 3 experiments**:
  1. Bidirectional vs. Unidirectional Ablation: Compare performance of only forward, only backward, and full bidirectional methods on D4RL locomotion tasks
  2. Filter Ablation: Train agents with no filter, only OOD filter, only greedy filter, and both filters; plot learning curves
  3. Sensitivity to Augmentation Ratio: Run full pipeline with varying ratios (10%, 30%, 50%, 100%) and report final performance

## Open Questions the Paper Calls Out

- **Multi-task Settings**: Future work will explore bidirectional generation for offline RL under multi-task settings with broader behavioral patterns, as all experiments were conducted on single-task D4RL benchmarks
- **Anchor State Selection**: The paper does not investigate whether prioritizing certain states (e.g., high-reward, high-uncertainty, or bottleneck states) would yield better augmentation than random sampling
- **Error Accumulation**: For n-step TD estimators, accumulated mismatching errors from the inverse dynamics model compound as horizon increases, though the current H=5 limits this
- **Joint Training**: The paper trains forward and backward diffusion models separately, without analysis of whether joint training would improve trajectory coherence at the stitching point

## Limitations

- Limited direct empirical validation that generated trajectories actually exhibit the proposed global connectivity property
- Reliance on separately trained IDM and RM models introduces potential compounding error sources not thoroughly quantified
- OOD filtering mechanism may inadvertently remove novel but valid high-reward trajectories, limiting exploration potential

## Confidence

- **High Confidence**: Outperforms state-of-the-art data augmentation baselines on D4RL benchmarks is supported by extensive experimental results
- **Medium Confidence**: Bidirectional trajectory generation mechanism is well-defined but claims about "global connectivity" are inferred from performance gains rather than directly measured
- **Medium Confidence**: Filtering pipeline is essential for stability based on ablation studies, but specific hyperparameters and potential for over-aggressive filtering are not fully explored

## Next Checks

1. **Direct Connectivity Analysis**: Use graph-based methods to compare state reachability in original vs. augmented datasets, quantifying number of previously unreachable state pairs now connected

2. **Error Propagation Study**: Compare BiTrajDiff performance with and without filtering, or synthetically inject noise into IDM/RM predictions to measure downstream effect on RL training stability

3. **OOD Filter Ablation with Reward Analysis**: Perform detailed ablation on OOD filter, analyzing reward distribution of filtered vs. non-filtered generated data to determine if filter is overly conservative