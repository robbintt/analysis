---
ver: rpa2
title: Success and Cost Elicit Convention Formation for Efficient Communication
arxiv_id: '2510.24023'
source_url: https://arxiv.org/abs/2510.24023
tags:
- games
- speaker
- listener
- success
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to train large multimodal models to
  form linguistic conventions that enable efficient communication with people. The
  approach uses simulated reference games between models and preference optimization
  based on both communicative success and cost, without requiring additional human-produced
  data.
---

# Success and Cost Elicit Convention Formation for Efficient Communication

## Quick Facts
- arXiv ID: 2510.24023
- Source URL: https://arxiv.org/abs/2510.24023
- Reference count: 40
- Primary result: Models trained on both communicative success and cost form efficient linguistic conventions, reducing message length by up to 41% while increasing success by 15%

## Executive Summary
This paper presents a method to train large multimodal models to form linguistic conventions that enable efficient communication with people. The approach uses simulated reference games between models and preference optimization based on both communicative success and cost, without requiring additional human-produced data. Experiments with human listeners in repeated reference games over COCO photographs and tangram images show that the trained model reduces message length by up to 41% while increasing communicative success by 15% over the interaction. Human listeners also respond 35-52% faster to utterances from the convention-forming model compared to baselines. Crucially, the paper demonstrates that training based on success or cost alone is insufficient - both components are necessary to elicit efficient convention formation.

## Method Summary
The method trains multimodal models to play repeated reference games by simulating speaker-listener interactions without human data. A speaker model generates multiple utterances for each target image within a context, while a listener model attempts to identify the correct referent. Preference pairs are constructed by selecting the shortest successful utterance as the winner and comparing it against all longer or unsuccessful alternatives. The speaker is then trained using Identity Preference Optimization (IPO) on these pairs, with LoRA adapters applied to all linear layers. The approach requires no additional human-produced data, relying instead on simulated games between models to generate the training signal. The method produces models that adapt by reusing words from previous utterances while dropping unnecessary details, achieving low and decreasing word novelty rates.

## Key Results
- SUCCESS+COST training reduces message length by 41% while increasing communicative success by 15% over repeated interactions
- Human listeners respond 35-52% faster to utterances from the convention-forming model compared to baselines
- Training based on success or cost alone is insufficient - both are necessary to elicit convention formation
- Models form conventions by reusing words from prior utterances while dropping unnecessary details, achieving low and decreasing word novelty rates

## Why This Works (Mechanism)

### Mechanism 1: Dual-Component Preference Optimization
Combining communicative success and cost in preference pairs is necessary; either alone is insufficient. Preference pairs are constructed by selecting the shortest successful utterance as the winner, and comparing it against all longer or unsuccessful alternatives. This creates a training signal that simultaneously rewards success and penalizes verbosity. Core assumption: Listeners in the simulation are reasonable proxies for human interpretation behavior. Evidence: "We also show that training based on success or cost alone is insufficient - both are necessary to elicit convention formation."

### Mechanism 2: Self-Play Simulation for Zero-Human-Data Training
Simulated reference games between models generate sufficient training signal without human-produced interaction data. A speaker model samples n utterances per trial; a listener model guesses the referent for each. Trials are accumulated into game history T, which conditions subsequent generations. Preference pairs are extracted from these samples. Core assumption: The base speaker and listener have enough visual-language grounding from pretraining to produce and interpret reasonable utterances. Evidence: "Our approach uses simulated reference games between models, and requires no additional human-produced data."

### Mechanism 3: In-Context Convention Formation via Word Reuse
Trained models form conventions by reusing words from prior utterances while dropping unnecessary details. The preference signal rewards utterances that succeed with fewer words. Over repeated references to the same image, the model learns that certain words are sufficient (given context) and others can be dropped without harming success. This manifests as decreasing word novelty rate (WNR). Core assumption: The game history T fits within the model's effective context window and is attended to appropriately. Evidence: "A low and decreasing WNR suggests that the SUCCESS+COST speaker forms conventions by reusing words from previous utterances."

## Foundational Learning

- **Concept: Direct/Identity Preference Optimization (DPO/IPO)**
  - Why needed: The entire training pipeline is IPO-based. You must understand preference pairs, KL regularization, and how the loss shapes the policy without an explicit reward model.
  - Quick check: Given two utterances for the same image—one successful+short, one unsuccessful—how would IPO use this pair?

- **Concept: Reference Games (Lewis Signaling Games)**
  - Why needed: This is the experimental paradigm. A speaker describes a target from a context set; a listener guesses. Repeated games measure adaptation.
  - Quick check: In a 4-image context, if the speaker says "the dog" and two images contain dogs, what should happen to the success signal?

- **Concept: Word Novelty Rate (WNR)**
  - Why needed: WNR is the primary metric for convention formation. You must understand how it's computed (word error rate without deletion penalties, normalized by previous length).
  - Quick check: If an utterance goes from "a man eating a banana" to "eating banana," what is the WNR?

## Architecture Onboarding

- **Component map:** Base speaker (Gemma-3-12B/Pixtral-12B) -> Simulator (Algorithm 1) -> Preference pair builder -> IPO trainer -> Fine-tuned listener (Gemma-3-12B) -> Human evaluation

- **Critical path:** 1) Prepare image contexts (CLIP-based similarity sampling with temperature) -> 2) Run simulation to collect candidate trials V -> 3) Build preference pairs from V -> 4) Train speaker with IPO -> 5) Evaluate with human or model listeners on held-out images

- **Design tradeoffs:**
  - SUCCESS+COST vs. SUCCESS-only: The former yields efficiency + success; the latter yields success but increased message length
  - Greedy vs. nucleus decoding: Greedy maximizes accuracy for COCO; nucleus sampling prevents degenerate repetition for tangrams
  - LoRA rank: Higher r (32) captures more adaptation but risks overfitting to simulation artifacts

- **Failure signatures:**
  - Training with COST-only: Listener accuracy drops to near chance; messages become uninformative
  - Greedy decoding on tangrams: All models (including base) degenerate to identical descriptions for all repetitions
  - Weak listener model: Preference pairs become noisy; IPO signal degrades

- **First 3 experiments:**
  1. Reproduce SUCCESS+COST vs. SUCCESS-only ablation on COCO: Verify that SUCCESS-only increases message length while SUCCESS+COST decreases it
  2. Test generalization to held-out image contexts: Train on temperature 0.01-0.05 contexts; evaluate on unseen contexts at each temperature
  3. Pilot human listener study with 10 participants: Compare response times for SUCCESS+COST vs. baseline; confirm 35-52% speedup trend before scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this method scale to complex, collaborative tasks beyond repeated reference games without relying on specific game structures?
- Basis in paper: The Discussion states, "In future work, these rewards might be instantiated in other settings... when carrying out complex tasks." The Limitations section notes the method is only demonstrated in repeated reference games.
- Why unresolved: The current experimental setup is constrained by the mechanics of image reference games (fixed context blocks), leaving the efficacy of this training paradigm in open-ended or non-game domains unknown.
- What evidence would resolve it: Successful application of the success-and-cost preference optimization to multi-step embodied AI tasks or open-ended dialogue scenarios.

### Open Question 2
- Question: How can models maintain stable conventions when interaction histories exceed the model's effective context window?
- Basis in paper: The Limitations section highlights that extending to complex domains involves managing histories that "may not fit in the (effective) context window of the models."
- Why unresolved: The current architecture relies on the entire interaction history being available in the context window to establish shared grounding, a constraint that breaks in long-term interactions.
- What evidence would resolve it: Experiments integrating external memory mechanisms or retrieval-augmented generation that demonstrate consistent convention maintenance over thousands of tokens.

### Open Question 3
- Question: Does optimizing for communicative efficiency via cost minimization inadvertently encourage the generation of deceptive or manipulative language?
- Basis in paper: The "Potential risks" section warns that a model communicating efficiently "may do so by withholding important information... hence acting deceptively."
- Why unresolved: The study optimizes for brevity (cost) and accuracy (success), but does not measure if the model omits critical nuance or ethical constraints to achieve lower word counts.
- What evidence would resolve it: Evaluation of model outputs in domains requiring nuance (e.g., medical advice) to check if cost minimization leads to dangerous oversimplification.

## Limitations
- Exact prompt templates for speaker and listener models are not fully specified, creating potential for deviation in training dynamics
- The "GPT-5-mini" listener used in some experiments is not publicly available, preventing exact replication of those results
- Simulation-based training depends heavily on the quality of the base listener model - weak or overly strong listeners could distort the preference optimization signal

## Confidence

- **High confidence:** The core finding that combining success and cost signals is necessary for convention formation is well-supported by the ablation experiments (SUCCESS+COST vs. SUCCESS-only shows 41% length reduction while maintaining success). The mechanism of word reuse leading to decreasing WNR is directly observable in the results.

- **Medium confidence:** The generalization claims to held-out image contexts at varying temperatures are supported but could benefit from more systematic analysis across different domain shifts. The human listener results (35-52% faster response times) are promising but based on relatively small-scale studies that would benefit from larger samples.

- **Low confidence:** The exact training dynamics of the IPO optimization with the specific preference pair construction method cannot be fully verified without the prompt templates. The claim about training without human-produced data is accurate but the simulation quality depends on base model capabilities that may not transfer to all domains.

## Next Checks

1. **Prompt template validation:** Implement the speaker and listener prompt templates based on the paper description and code, then compare simulation outputs with reported values to ensure the training setup matches.

2. **Ablation replication:** Reproduce the SUCCESS-only vs. SUCCESS+COST comparison on COCO to verify that the 41% message length reduction occurs when both signals are combined.

3. **Human listener scaling:** Conduct a larger-scale human listener study (minimum 50 participants) to confirm the 35-52% response time improvement and test whether the effect holds across different participant demographics.