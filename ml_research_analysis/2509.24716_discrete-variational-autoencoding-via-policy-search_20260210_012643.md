---
ver: rpa2
title: Discrete Variational Autoencoding via Policy Search
arxiv_id: '2509.24716'
source_url: https://arxiv.org/abs/2509.24716
tags:
- daps
- policy
- latent
- discrete
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DAPS addresses the challenge of training discrete variational autoencoders
  (VAEs) by proposing a policy-search-based approach that avoids reparameterization
  techniques. It leverages insights from reinforcement learning to update the parametric
  encoder using weighted maximum likelihood, enabling stable training on high-dimensional
  datasets.
---

# Discrete Variational Autoencoding via Policy Search

## Quick Facts
- arXiv ID: 2509.24716
- Source URL: https://arxiv.org/abs/2509.24716
- Authors: Michael Drolet; Firas Al-Hafez; Aditya Bhatt; Jan Peters; Oleg Arenz
- Reference count: 40
- Primary result: DAPS achieves superior performance in reconstructing high-dimensional data from compact latent spaces

## Executive Summary
DAPS addresses the challenge of training discrete variational autoencoders (VAEs) by proposing a policy-search-based approach that avoids reparameterization techniques. It leverages insights from reinforcement learning to update the parametric encoder using weighted maximum likelihood, enabling stable training on high-dimensional datasets. DAPS incorporates automatic step size adaptation and achieves superior performance in reconstructing high-dimensional data from compact latent spaces.

## Method Summary
DAPS introduces a policy-search-based training method for discrete VAEs that circumvents the need for reparameterization gradients. The approach uses weighted maximum likelihood to update the encoder parameters, drawing from reinforcement learning principles to handle discrete latent spaces. Automatic step size adaptation is incorporated to improve training stability. The method demonstrates effectiveness in reconstructing high-dimensional data while maintaining compact latent representations, with particular success on benchmark image datasets.

## Key Results
- Achieves superior performance in reconstructing high-dimensional data from compact latent spaces
- On ImageNet, outperforms existing methods in terms of reconstruction quality, PSNR, and FID scores
- Demonstrates effectiveness in handling complex data through stable training on high-dimensional datasets

## Why This Works (Mechanism)
DAPS works by reformulating the discrete VAE training problem as a policy search task. Instead of using reparameterization gradients which are incompatible with discrete latent variables, it employs a weighted maximum likelihood objective derived from reinforcement learning. This allows the encoder to be updated directly without requiring continuous relaxations of discrete variables. The policy search framework naturally handles the stochasticity of discrete sampling while providing stable gradient estimates. Automatic step size adaptation further enhances training stability by adjusting the learning rate based on the optimization landscape.

## Foundational Learning
- Variational Autoencoders (VAEs): Generative models that learn latent representations through an encoder-decoder architecture with a probabilistic framework. Needed to understand the baseline method being improved. Quick check: Can you explain the evidence lower bound (ELBO) in VAEs?
- Discrete Latent Variables: Categorical or quantized representations that require different training approaches than continuous latents. Needed to understand the specific challenge DAPS addresses. Quick check: What makes training with discrete variables more challenging than continuous ones?
- Policy Search Methods: Reinforcement learning techniques that directly optimize policies through gradient estimation. Needed to understand the core mechanism of DAPS. Quick check: How does policy gradient estimation differ from standard supervised learning gradients?
- Reparameterization Trick: A technique for backpropagating through stochastic nodes by reparameterizing the randomness. Needed to understand what DAPS avoids. Quick check: Why does the reparameterization trick fail for discrete variables?

## Architecture Onboarding

Component Map:
Encoder -> Discrete Latent Space -> Decoder -> Reconstruction

Critical Path:
Input image → Encoder network → Discrete latent sampling → Decoder network → Reconstructed image

Design Tradeoffs:
- Discrete vs continuous latent space: Discrete provides interpretability and compression but loses gradient information
- Policy search vs reparameterization: Policy search handles discreteness naturally but may require more samples
- Step size adaptation: Improves stability but adds complexity to hyperparameter tuning

Failure Signatures:
- Mode collapse: The model generates limited diversity in reconstructions
- Posterior collapse: The latent variables become ignored by the decoder
- Instability during training: Learning rates that are too aggressive or too conservative

Three First Experiments:
1. Train DAPS on a simple dataset like MNIST to verify basic functionality
2. Compare reconstruction quality against standard VAEs on CIFAR-10
3. Test different latent space dimensionalities to find the optimal compression ratio

## Open Questions the Paper Calls Out
The paper acknowledges that major uncertainties remain regarding the scalability and robustness of DAPS to datasets beyond controlled image benchmarks. While the paper demonstrates strong performance on CIFAR-10 and ImageNet, the method's behavior on highly heterogeneous or multimodal datasets is unclear. The reliance on policy search may introduce hyperparameter sensitivity, though this is not extensively explored.

## Limitations
- Scalability to datasets beyond controlled image benchmarks remains uncertain
- Performance on highly heterogeneous or multimodal datasets is unclear
- Reliance on policy search may introduce hyperparameter sensitivity

## Confidence
- High confidence: Claims about avoiding reparameterization and using weighted maximum likelihood for encoder updates
- Medium confidence: Claims about automatic step size adaptation improving stability
- Medium confidence: Performance improvements on CIFAR-10 and ImageNet reconstruction quality

## Next Checks
1. Evaluate DAPS on a diverse set of high-dimensional datasets including audio, text, and multimodal data to assess generalizability
2. Conduct systematic ablation studies to quantify the contribution of policy search versus other architectural choices
3. Test DAPS under varying latent space dimensionalities and quantization levels to establish robustness boundaries