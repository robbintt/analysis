---
ver: rpa2
title: Acquiescence Bias in Large Language Models
arxiv_id: '2509.08480'
source_url: https://arxiv.org/abs/2509.08480
tags:
- prompt
- agree
- neutral
- disagree
- yesno
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) exhibit
  acquiescence bias, the human tendency to agree with survey statements regardless
  of true beliefs. Using five LLMs across nine tasks in three languages (English,
  German, Polish), the research tests responses under five different prompt phrasings
  designed to vary levels of suggested agreement.
---

# Acquiescence Bias in Large Language Models

## Quick Facts
- arXiv ID: 2509.08480
- Source URL: https://arxiv.org/abs/2509.08480
- Reference count: 18
- Primary result: LLMs show a bias toward "No" responses, not human-like acquiescence bias, with strongest effect in English.

## Executive Summary
This study investigates whether large language models exhibit acquiescence bias—the human tendency to agree with survey statements. Testing five LLMs across nine tasks in three languages using five prompt variations, the research finds that LLMs instead display a strong bias toward answering "no," regardless of whether "no" implies agreement or disagreement. The effect was particularly pronounced in English, with "no" responses increasing by 31%-203% across conditions. These findings suggest LLMs do not replicate human response biases and raise concerns about their reasoning capabilities, as they often contradict themselves across similar questions.

## Method Summary
The study tested five LLMs (Llama-3.1-8B-Instruct, Mistral-Small-24B-Instruct-2501, Gemma-2-27B-IT, Llama-3.3-70B-Instruct, GPT-4o) across nine binary classification tasks using seven English, one German, and one Polish dataset. For each question, five prompt variations were generated: neutral, yes/no, agreement, negated agreement, and disagreement. Models were run at temperature 1.0, and responses were parsed for single-word answers. Statistical significance was assessed using McNemar's test to compare response patterns across conditions.

## Key Results
- LLMs show a systematic bias toward "No" responses across all five prompt conditions
- English-language tasks showed 31%-203% increase in "No" responses compared to neutral condition
- No consistent bias pattern emerged for German or Polish languages
- Models often contradicted themselves, answering "No" to both agreement and disagreement prompts

## Why This Works (Mechanism)
The study suggests that LLM outputs are driven by surface-level statistical patterns rather than deep logical reasoning. The consistent "No" bias across multiple prompt phrasings indicates that models may be responding to token-level preferences rather than understanding the semantic content of questions. This surface-form dominance hypothesis explains why models can contradict themselves—they're selecting tokens based on learned probabilities rather than engaging in genuine reasoning about agreement or disagreement.

## Foundational Learning

- **Concept: Acquiescence Bias in Humans**
  - **Why needed here:** This is the baseline phenomenon against which LLM behavior is being compared. Without understanding this, the paper's core finding—that LLMs do *not* replicate this bias—is meaningless.
  - **Quick check question:** A human is asked, "Is the weather good?" and "Is the weather bad?" According to the concept of acquiescence bias, which question is more likely to get a "Yes" answer, and why?

- **Concept: Negative Yes/No Questions**
  - **Why needed here:** The paper explicitly critiques prior work for using ambiguous questions like "Wouldn't you agree?" Understanding the inherent ambiguity of such questions is crucial to appreciating the experimental design and its attempt to isolate the bias.
  - **Quick check question:** Consider the question "Don't you think the project was a success?" What is the inherent ambiguity in a "Yes" or "No" answer to this question?

- **Concept: Statistical vs. Reasoning-Based Model Outputs**
  - **Why needed here:** The paper's findings suggest that LLM outputs are driven by surface-level statistical patterns (a "No" token bias) rather than deep logical reasoning, especially when prompted with yes/no questions. Distinguishing between these two drivers is key to interpreting the results.
  - **Quick check question:** An LLM answers "No" to the question "Do you agree this contract is valid?" and also answers "No" to "Do you disagree that this contract is valid?" What does this pattern suggest about the primary driver of its output?

## Architecture Onboarding

- **Component Map:**
  - Datasets (Legalbench, AGB-DE, LEPISZCZE) -> Prompting Engine -> Model Inference -> Evaluation Module

- **Critical Path:**
  1. Data preprocessing and prompt generation
  2. Large-scale batch inference across all models, prompts, and languages
  3. Response parsing and counting of "Yes"/"No" and "A"/"B" answers
  4. Statistical analysis to identify systematic biases (the "No" bias) across conditions

- **Design Tradeoffs:**
  - **Control vs. Realism:** The study uses legal texts, which are challenging and realistic, but limits generalizability. The tradeoff is between depth of analysis on a specific domain and breadth of claims about general model behavior.
  - **Conciseness vs. Nuance:** The prompts were chosen to be as concise as possible to isolate the effect of the phrasing. The tradeoff is that more nuanced or context-rich prompts might yield different results.
  - **Temperature Setting:** A temperature of 1.0 was chosen for practical relevance. The tradeoff is that lower temperatures (e.g., 0.0) might reduce noise and could potentially alter the strength of the observed bias.

- **Failure Signatures:**
  - **Self-Contradiction:** A model answers "No" to both "Do you agree X is true?" and "Do you disagree X is true?". This is a key failure signature indicating that reasoning has been bypassed by a surface-form bias.
  - **Language Contingency:** A discovered bias that is only reproducible in one language (e.g., English) and not in others. This signals a failure to identify a universal mechanism, pointing instead to language-specific artifacts.

- **First 3 Experiments:**
  1. **Replicate "No" Bias on a New Domain:** Take a standard NLI dataset (e.g., SNLI) and create the same five prompt variations. Run them on a model like Llama-3.1-8B and measure if the "No" bias persists. This tests the domain-dependence of the finding.
  2. **Token-Level Probability Analysis:** For a set of English prompts that trigger the "No" bias, log the output token probabilities for the entire vocabulary. Confirm if the probability for the "No" token is disproportionately high compared to "Yes" and other tokens. This provides mechanistic evidence for the surface-form dominance hypothesis.
  3. **Mitigation with Constrained Decoding:** Design a new prompt that forces the model to generate a short rationale before answering "Yes" or "No" (e.g., "Analyze the clause, then answer Yes or No:"). Measure if this Chain-of-Thought style prompt reduces the "No" bias and improves consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed "no" bias persist across non-legal domains?
- Basis in paper: [explicit] The authors note that all utilized datasets stem from the legal domain, which provides unique challenges, and state that "further investigation is necessary to find out whether the findings are also applicable to documents from other domains."
- Why unresolved: It is currently unclear if the bias is a general feature of the model's reasoning or a specific reaction to the formal, negative-connotation structures often found in legal text.
- What evidence would resolve it: Replicating the five-condition prompt experiment on diverse datasets, such as creative writing, medical diagnostics, or casual conversation corpora.

### Open Question 2
- Question: Why is the "no" bias consistent in English but absent or inconsistent in German and Polish?
- Basis in paper: [inferred] The abstract and results section highlight that while English showed a strong bias toward "no," in German and Polish "no consistent pattern emerged."
- Why unresolved: The paper establishes the existence of this cross-lingual discrepancy but does not determine if the cause is differing training data distributions, specific linguistic structures, or model safety alignments for specific languages.
- What evidence would resolve it: A comparative analysis of the pre-training corpora and safety instruction data for English versus German and Polish within the tested models.

### Open Question 3
- Question: How robust is the observed bias to variations in prompt phrasing beyond the specific conditions tested?
- Basis in paper: [explicit] The authors list as a limitation that "only one prompt was tested per condition," acknowledging that "other prompts could have been found that fit the described conditions."
- Why unresolved: The specific syntactic structure of the chosen prompts (e.g., "Do you agree...") may have inadvertently triggered the negative response, rather than the semantic condition of "agreement" itself.
- What evidence would resolve it: Testing a wider variety of syntactic structures for each condition to isolate semantic bias from syntactic artifacts.

## Limitations
- The study's findings are primarily driven by English-language data, with inconsistent results in German and Polish
- Legal-domain focus limits generalizability to other types of reasoning tasks
- Temperature of 1.0 introduces stochastic variability that prevents deterministic reproduction

## Confidence
- **High Confidence**: The observation that LLMs show increased "No" responses across multiple conditions in English, particularly in Mistral and GPT-4o, is well-supported by statistical analysis (McNemar's test, p < 0.05).
- **Medium Confidence**: The conclusion that this "No" bias represents a departure from human acquiescence bias is reasonable but depends on the assumption that prompt variations effectively isolate agreement tendencies.
- **Low Confidence**: The interpretation that the bias is driven by a preference for the "No" token rather than genuine disagreement is speculative and would require token-level probability analysis for confirmation.

## Next Checks
1. **Domain Transfer Test**: Replicate the experiment on a non-legal binary classification dataset (e.g., SNLI) to determine if the "No" bias persists outside the legal domain.
2. **Token Probability Audit**: For a subset of English prompts that trigger the "No" bias, log the full output token distribution from the model to compare "No" versus "Yes" probabilities.
3. **Chain-of-Thought Mitigation**: Modify prompts to require a short rationale before the yes/no answer to measure if this reduces the "No" bias and improves consistency.