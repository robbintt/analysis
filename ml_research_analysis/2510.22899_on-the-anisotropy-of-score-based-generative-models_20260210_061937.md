---
ver: rpa2
title: On the Anisotropy of Score-Based Generative Models
arxiv_id: '2510.22899'
source_url: https://arxiv.org/abs/2510.22899
tags:
- data
- geometry
- diffusion
- biases
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the architecture of score-based generative
  models affects their inductive biases. The authors introduce Score Anisotropy Directions
  (SADs), which are orthonormal directions in the output space ranked by how well
  the model can generate data along them.
---

# On the Anisotropy of Score-Based Generative Models

## Quick Facts
- arXiv ID: 2510.22899
- Source URL: https://arxiv.org/abs/2510.22899
- Reference count: 18
- Primary result: Introduces Score Anisotropy Directions (SADs) to predict and analyze architectural biases in score-based generative models using initialization geometry.

## Executive Summary
This paper investigates how the architecture of score-based generative models affects their inductive biases. The authors introduce Score Anisotropy Directions (SADs), which are orthonormal directions in the output space ranked by how well the model can generate data along them. SADs are computed from the eigendecomposition of the model's average geometry matrix, estimated from forward passes at initialization. The core idea is that different architectures induce different geometries in their output spaces, and data aligned with small-eigenvalue SADs is easier to model than data aligned with large-eigenvalue SADs. This theory is supported by experiments on rank-one datasets and image benchmarks, where alignment with SADs correlates with performance measured by Wasserstein metrics.

## Method Summary
The method introduces Score Anisotropy Directions (SADs) as a framework to analyze architectural biases in score-based generative models. The core approach involves computing the Average Geometry matrix $G_F = \mathbb{E}[F F^\top]$ from forward passes of an untrained network, then performing eigendecomposition to obtain SADs. The theory proposes that small-eigenvalue eigenvectors represent directions where the network can generate data more easily. The framework includes alignment metrics to quantify how data distributions relate to SADs, and orthogonal transformations can be applied to optimize this alignment. The method is validated through synthetic rank-one experiments and image benchmark tests.

## Key Results
- Convolutional U-Nets perform better on high-frequency data, aligning with harmonic representations.
- Transformer-based models show weaker directional biases with higher eigenvalue multiplicity.
- Aligning training data with low-eigenvalue SADs improves generation quality and reduces artifacts like mode collapse.

## Why This Works (Mechanism)

### Mechanism 1: Output-Space Geometry as a Prior
- **Claim:** If a network architecture exhibits anisotropic activation patterns at initialization, it implicitly defines a set of preferred basis vectors (SADs) that constrain generative modeling independent of training data.
- **Mechanism:** By probing the untrained network $F_\theta$ with random inputs, one computes the Average Geometry $G_F = \mathbb{E}[F F^\top]$. The eigendecomposition of this covariance matrix reveals the "stiff" (large eigenvalue) and "compliant" (small eigenvalue) directions. The paper hypothesizes that the network prefers generating data along the "compliant" directions (small eigenvalues).
- **Core assumption:** The network's directional bias at initialization persists sufficiently to influence the optimization trajectory and final solution.
- **Evidence anchors:** [Definition 2]: Formalizes the Average Geometry $G_F$. [Conjecture 1]: Explicitly links small eigenvalue eigenvectors to SADs. [corpus]: Weak direct support; neighboring papers focus on forward noise processes or Riemannian metrics rather than initialization geometry.
- **Break condition:** If the network capacity is effectively infinite or training dynamics fundamentally reshape the loss landscape (e.g., "geometric invariance" fails), the initialization geometry may cease to predict final performance.

### Mechanism 2: SGD Noise Amplification in Anisotropic Subspaces
- **Claim:** For linear denoisers trained via SGD, optimization noise is amplified in directions corresponding to large eigenvalues of the geometry, hindering convergence compared to small-eigenvalue directions.
- **Mechanism:** Theoretical analysis (Theorem 1) shows that near optimality, the covariance of SGD steps scales with the eigenvalue $\lambda_i$. Thus, large $\lambda_i$ directions experience higher variance, whereas small $\lambda_i$ directions converge faster and more stably.
- **Core assumption:** The linear network approximation holds locally for deep non-linear networks, and the "noise" aspect of SGD is the dominant differentiator for these directions.
- **Evidence anchors:** [Section 3.1]: "Anisotropic Conditioning of the Optimization Landscape." [Theorem 1]: Derives the $\propto \lambda_i$ scaling of gradient covariance. [corpus]: Not supported by the provided corpus summaries.
- **Break condition:** If full-batch Gradient Descent is used (where anisotropy effects are limited) or learning rates are aggressively decayed to suppress noise.

### Mechanism 3: Alignment Control via Orthogonal Transformations
- **Claim:** If data is orthogonally transformed to align with the small-eigenvalue SADs, generative performance (measured by Wasserstein distance) improves; alignment with large eigenvalues degrades it.
- **Mechanism:** The data distribution is rotated using $W_{min}$ (derived in Theorem 2) to minimize the alignment metric $\alpha = \mathbb{E}[z^\top G_F z]$. This forces the "manifold" of the data into the network's preferred low-energy subspaces.
- **Core assumption:** The orthogonal transform does not destroy the semantic structure required for the specific generative task (or acts as a valid latent space).
- **Evidence anchors:** [Section 3.3]: "Score Anisotropy Directions in the Wild." [Figure 9]: Shows improved SW2 scores for minimized alignment. [corpus]: N/A (No comparable alignment techniques in corpus).
- **Break condition:** If the data manifold is significantly more complex than the rank-one synthetic data used to verify the theory, or if the "natural" alignment of the data is already optimal.

## Foundational Learning

- **Concept:** **Denoising Score Matching (DSM)**
  - **Why needed here:** The paper analyzes how architectures learn the "score" ($\nabla \log p$). Understanding that diffusion models are trained to reverse a noise process (approximated by $F_\theta$) is required to grasp what the "Average Geometry" represents.
  - **Quick check question:** How does the network $F_\theta$ approximate the score function $\nabla_x \log p(x)$?

- **Concept:** **Eigendecomposition of Covariance**
  - **Why needed here:** The core method relies on decomposing the matrix $G_F$ to find "stiff" vs. "compliant" directions. Without this, the notions of "alignment" and "spectral bias" are opaque.
  - **Quick check question:** What does a small eigenvalue in the Average Geometry $G_F$ imply about the variance of network outputs in that direction?

- **Concept:** **Inductive Bias & Neural Anisotropy Directions (NADs)**
  - **Why needed here:** This work extends "NADs" (from discriminative models) to generative models. Knowing that architectures have directional preferences (e.g., CNNs preferring textures/low freq) helps contextualize why the authors look for "Score Anisotropy."
  - **Quick check question:** How do SADs differ from NADs regarding which eigenvalues correlate with better performance?

## Architecture Onboarding

- **Component map:** Input $\to$ **Probing Distribution ($P$)** (e.g., Noise/Zero) $\to$ **Untrained Network ($F_\theta$)** $\to$ **Covariance ($G_F$)** $\to$ **SADs (Eigenvectors)**.
  *Note:* The training pipeline is standard; the novelty is the *pre-training diagnostic*.

- **Critical path:** Accurate estimation of $G_F$. This requires sampling many initializations $\theta$ and inputs to form a robust covariance matrix of the output space.

- **Design tradeoffs:**
  - **U-Nets:** High anisotropy, distinct harmonic eigenvectors (GAHBs). Good for images but strong structural bias.
  - **Transformers (DiT):** High eigen-multiplicity (weak anisotropy). More flexible but less structurally predictable; larger patch sizes increase multiplicity.

- **Failure signatures:**
  - **Mode Collapse:** Observed when data is aligned with *large* eigenvalues ($W_{max}$), causing the model to generate a limited subset of modes (e.g., digit "1" only in MNIST).
  - **Artifact Generation:** Generating samples that look like "noise" or padding artifacts if data lies in "stiff" subspaces.

- **First 3 experiments:**
  1. **Geometry Extraction:** Implement the estimation of $G_F$ for a standard iDDPM. Visualize the top eigenvectors (do they look like low-frequency DCT basis functions?).
  2. **Synthetic Alignment:** Create a rank-one dataset aligned with $u_{last}$ (smallest $\lambda$) vs. $u_{first}$ (largest $\lambda$). Train and plot convergence speed to verify Theorem 1.
  3. **Transformation Ablation:** Take MNIST, compute $W_{min}$ using the estimated $G_F$, train a model on the rotated data, and measure MSW-2 distance against a baseline trained on untransformed data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a fundamental trade-off between discriminative and generative modeling regarding alignment with geometry eigenvalues?
- Basis in paper: [explicit] The authors note that discriminative models perform better when aligned with *largest* eigenvalues, while their generative analysis shows preference for *smallest* eigenvalues, stating "such an investigation is out of the scope of this paper and we leave it for future research."
- Why unresolved: The contrasting findings between the NAD framework for classifiers and SADs for generative models suggest an unexplored theoretical relationship.
- What evidence would resolve it: A unified theoretical framework explaining both phenomena, or empirical studies directly comparing discriminative and generative models on identical architectures and data.

### Open Question 2
- Question: Do SADs and the average geometry framework scale to state-of-the-art foundation models?
- Basis in paper: [explicit] The authors acknowledge "our experiments, while thorough, focus on relatively small scale settings, which may not reflect the current state-of-the-art" and that "rigorous validation of our claims at large scales is computationally prohibitive."
- Why unresolved: Computational costs prevent validation on large-scale models where architectural geometry may interact differently with massive datasets.
- What evidence would resolve it: Experiments on large-scale diffusion models (e.g., Stable Diffusion) demonstrating that SADs computed at initialization predict generalization on realistic data distributions.

### Open Question 3
- Question: Can Conjecture 1 be formally proven for general non-linear architectures?
- Basis in paper: [inferred] The conjecture that "eigenvectors of G_F, in ascending eigenvalue order, are the SADs" is supported empirically but only proven for linear networks (Theorem 1). The non-linear case relies on heuristic arguments from Markov's inequality.
- Why unresolved: The Markov bound argument provides intuition but not a rigorous guarantee for arbitrary neural network families.
- What evidence would resolve it: A formal proof for broad classes of non-linear networks, or counterexamples showing where the conjecture fails.

### Open Question 4
- Question: How do SADs interact with other implicit priors such as double descent or explicit regularization?
- Basis in paper: [explicit] The authors state "the overall dynamics of diffusion models may also be influenced or dominated by different factors such as explicit regularization or other implicit priors" and that they "do not claim to have a complete theory of inductive biases."
- Why unresolved: The paper isolates architectural geometry but does not study its relative importance compared to other known phenomena.
- What evidence would resolve it: Ablation studies varying regularization strength, model width, and dataset size while tracking SAD alignment and final performance.

## Limitations
- The theory's extension from rank-one synthetic data to complex natural images needs more rigorous validation.
- Computational costs prevent validation on large-scale state-of-the-art models where architectural geometry may interact differently with massive datasets.
- The assumption that initialization geometry reliably predicts trained model behavior across diverse architectures and datasets remains to be fully established.

## Confidence
- **High Confidence:** The mathematical framework for computing SADs and the rank-one synthetic experiments (Theorem 1 and 2, Figure 9) are well-supported and internally consistent.
- **Medium Confidence:** The empirical correlation between SAD alignment and performance on image benchmarks (Section 4.1-4.3) is demonstrated but could benefit from more diverse datasets and ablation studies on different noise schedules.
- **Low Confidence:** The claim that Transformer architectures exhibit fundamentally weaker directional biases (higher eigen-multiplicity) is based on limited architectural comparisons and needs broader validation across different attention mechanisms and scales.

## Next Checks
1. **Cross-Architecture Generalization:** Test SAD predictions on architectures not covered in the paper (e.g., Vision Transformers, MLPs, state-space models) to verify if the geometric framework generalizes beyond CNNs and standard DiTs.
2. **Temporal Dynamics:** Track how SADs evolve during training (not just at initialization) to determine if the initialization geometry remains predictive or if training reshapes the directional biases over time.
3. **Real-World Application:** Apply SAD analysis to a practical generative task (e.g., molecule generation, time series forecasting) where the orthogonal transformation approach could have tangible benefits, validating the framework's utility beyond synthetic and image domains.