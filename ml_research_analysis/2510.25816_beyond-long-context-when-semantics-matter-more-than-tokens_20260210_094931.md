---
ver: rpa2
title: 'Beyond Long Context: When Semantics Matter More than Tokens'
arxiv_id: '2510.25816'
source_url: https://arxiv.org/abs/2510.25816
tags:
- clinical
- clear
- retrieval
- notes
- efficiency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of accurate semantic question
  answering from large, base64-encoded clinical notes in EHR systems, where traditional
  chunk-based vector retrieval often misses critical clinical relationships. It introduces
  an enhanced Clinical Entity Augmented Retrieval (CLEAR) method that uses entity-aware
  retrieval with medical domain knowledge, section-aware processing, and targeted
  context selection around identified medical entities.
---

# Beyond Long Context: When Semantics Matter More than Tokens

## Quick Facts
- arXiv ID: 2510.25816
- Source URL: https://arxiv.org/abs/2510.25816
- Authors: Tarun Kumar Chowdhury; Jon D. Duke
- Reference count: 7
- Primary result: Entity-aware retrieval with section weighting achieves 58.3% win rate and 0.878 semantic similarity using 78% fewer tokens than wide-context processing

## Executive Summary
This study addresses semantic question answering from large, base64-encoded clinical notes in EHR systems where traditional chunk-based vector retrieval often misses critical clinical relationships. The proposed Clinical Entity Augmented Retrieval (CLEAR) method combines entity-aware retrieval with medical domain knowledge, section-aware processing, and targeted context selection around identified medical entities. Evaluation on 12 synthetic clinical notes (10K–65K tokens) demonstrated CLEAR's superior performance, achieving a 58.3% win rate and 0.878 average semantic similarity while using 78% fewer tokens than wide-context processing.

## Method Summary
The CLEAR method extracts medical entities from clinical documents, identifies their surrounding context (±150 words), and retrieves the most relevant sections based on question-entity semantic alignment and section priority weights. The system recognizes six entity types (medications, symptoms, diseases, procedures, labs, anatomical references) and assigns priority weights to clinical document sections, with ASSESSMENT and PLAN sections receiving the highest weight. This approach contrasts with traditional RAG's chunk-based vector retrieval and wide-context processing that simply passes entire documents to LLMs.

## Key Results
- CLEAR achieved a 58.3% win rate against baseline methods with 0.878 average semantic similarity
- Performance gains were strongest on larger documents (75% win rate for 65K+ tokens)
- CLEAR used 78% fewer tokens than wide-context processing while maintaining superior accuracy
- Consistent token usage of approximately 8,500 tokens across all document sizes demonstrates scalability

## Why This Works (Mechanism)

### Mechanism 1: Entity-Centered Context Retrieval
- Claim: Retrieving fixed-size windows around identified medical entities yields higher semantic accuracy than chunk-based vector retrieval while using fewer tokens.
- Mechanism: Extract medical entities (medications, symptoms, diseases, procedures, labs, anatomical references), then retrieve ±150 words around each entity rather than arbitrary token-based chunks. Question-entity semantic alignment prioritizes which entity contexts to include.
- Core assumption: Clinical reasoning depends on local relationships between entities and their surrounding context, not global statistical similarity of text embeddings.
- Evidence anchors:
  - [abstract] "CLEAR achieved a 58.3% win rate and 0.878 average semantic similarity, using 78% fewer tokens than wide-context processing"
  - [section 2.2.1] "The system recognizes six primary entity categories: medications, symptoms, diseases, procedures, laboratory values, and anatomical references"
  - [corpus] "Attention Reveals More Than Tokens" supports attention-guided retrieval over naive long-context processing
- Break condition: When entity recognition misses key clinical concepts (keyword-based NER limitations), or when answers require synthesizing information from distant entities with non-overlapping windows.

### Mechanism 2: Section-Aware Priority Weighting
- Claim: Weighting clinical document sections by diagnostic importance improves retrieval relevance for clinical QA.
- Mechanism: Parse standardized section headers (ASSESSMENT, PLAN, HISTORY OF PRESENT ILLNESS) and assign priority weights—ASSESSMENT/PLAN = 1.0, HISTORY = 0.9, others proportional. Entity matches in high-priority sections receive boosted scores.
- Core assumption: Not all document sections contribute equally to diagnostic reasoning; clinician-synthesized sections contain higher-value semantic content.
- Evidence anchors:
  - [abstract] "section-aware processing, and targeted context selection around identified medical entities"
  - [section 2.2.2] "ASSESSMENT and PLAN sections receiving highest priority (weight = 1.0), followed by HISTORY OF PRESENT ILLNESS (weight = 0.9)"
  - [corpus] Weak direct evidence; related papers focus on retrieval methods, not section weighting specifically
- Break condition: When critical information resides in non-prioritized sections (e.g., social history containing key risk factors, or family history relevant to genetic conditions).

### Mechanism 3: Bounded Token Budget with Scalable Cost
- Claim: Fixed-size entity windows produce consistent computational cost regardless of source document length.
- Mechanism: By capping retrieval to ±150 words per entity and deduplicating overlapping spans, total context stays at ~8,500 tokens across 10K–65K source documents.
- Core assumption: Local entity context is sufficient for most clinical QA; long-range dependencies are captured through multiple entity windows rather than full-document processing.
- Evidence anchors:
  - [abstract] "using 78% fewer tokens than wide-context processing"
  - [section 3.4] "consistent token usage of approximately 8,500 tokens across all document sizes demonstrates the scalability advantage"
  - [corpus] "Beyond RAG vs. Long-Context" supports distraction-aware retrieval over full-context approaches
- Break condition: When answers require temporal synthesis across multiple visits, or when critical narrative context falls outside entity windows.

## Foundational Learning

- **Concept: "Lost in the Middle" Phenomenon**
  - Why needed here: Explains why wide-context processing underperforms on long documents; LLMs struggle to attend to relevant information positioned in the middle of long inputs.
  - Quick check question: On a 50K-token document, where would you expect Wide Context to fail most—information at the start, middle, or end?

- **Concept: FHIR DocumentReference Structure**
  - Why needed here: EHR systems store clinical notes as base64-encoded attachments; production deployment requires decoding and parsing this structure before retrieval.
  - Quick check question: What FHIR resource type stores clinical documentation, and what encoding must be reversed to access raw text?

- **Concept: Semantic Similarity Metrics (Cosine, METEOR)**
  - Why needed here: The paper evaluates using cosine semantic similarity and METEOR; understanding what these measure is essential for interpreting win rates and accuracy claims.
  - Quick check question: Why might cosine similarity on embeddings be preferred over exact token match for evaluating clinical QA answers?

## Architecture Onboarding

- **Component map:**
  Entity Extractor → Section Parser → Context Selector → Token Assembler → LLM Inference

- **Critical path:**
  Raw clinical note → Section identification → Entity extraction → Question-entity semantic alignment → Context window assembly → LLM prompt → Answer evaluation

- **Design tradeoffs:**
  - RAG: 98.6% token savings but lowest accuracy (0.835 similarity, 16.7% win rate)
  - Wide Context: No retrieval complexity but 39,173 avg tokens, vulnerable to lost-in-middle (25% win rate)
  - CLEAR: Balanced—78.4% savings, 0.878 similarity, 58.3% win rate, strongest on 65K+ documents (75% win rate)

- **Failure signatures:**
  - Semantic similarity drops below RAG on short documents: Entity extraction overhead may not justify itself for <15K token notes
  - Inconsistent results across LLMs: Prompt sensitivity to entity-context formatting
  - Token budget exceeded: Overlapping entity windows causing redundant retrieval

- **First 3 experiments:**
  1. Reproduce win rates: Run all three strategies (Wide, RAG, CLEAR) on 3 notes each at 10K/42K/65K with the paper's two evaluation questions; verify ~58% CLEAR win rate.
  2. Entity ablation: Disable one entity category at a time and measure semantic similarity delta; identify which entity types drive accuracy.
  3. Window size sweep: Test ±100, ±150, ±200 words on the 65K notes; plot token usage vs. similarity to find optimal balance point.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on synthetic clinical notes rather than real-world EHR data, limiting generalizability to actual clinical documentation patterns and complexity.
- Entity recognition uses keyword/pattern matching without validation of clinical accuracy or handling of negated entities, potentially leading to false positives in retrieval.
- The evaluation framework tests only two questions per document, providing limited coverage of clinical reasoning scenarios.

## Confidence
- **High Confidence**: The core performance claims comparing CLEAR against RAG and Wide Context on the same synthetic dataset.
- **Medium Confidence**: The scalability claims regarding consistent token usage across document sizes.
- **Medium Confidence**: The clinical relevance of improvements, as semantic similarity gains need validation for clinical meaningfulness.

## Next Checks
1. **Real-World Document Testing**: Apply CLEAR to 50+ real clinical notes from multiple specialties to validate performance on authentic documentation with varied writing styles and clinical complexity.

2. **Entity Recognition Validation**: Compare keyword-based entity extraction against a medical NLP system (e.g., cTAKES or MetaMap) to quantify false positives/negatives and assess impact on retrieval accuracy.

3. **Long-Document Boundary Testing**: Evaluate CLEAR on documents exceeding 100K tokens to identify performance degradation points and determine whether the 8,500 token budget remains optimal.