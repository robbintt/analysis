---
ver: rpa2
title: Does the Prompt-based Large Language Model Recognize Students' Demographics
  and Introduce Bias in Essay Scoring?
arxiv_id: '2504.21330'
source_url: https://arxiv.org/abs/2504.21330
tags:
- scoring
- essay
- llms
- students
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates bias in Large Language Models (LLMs) when
  scoring student essays, focusing on how well LLMs infer demographic attributes (gender
  and first-language background) and whether this ability influences scoring fairness.
  Using a dataset of over 25,000 argumentative essays, the authors prompted GPT-4o
  to predict demographics and assign scores, then analyzed bias using multiple fairness
  metrics and multivariate regression.
---

# Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?

## Quick Facts
- arXiv ID: 2504.21330
- Source URL: https://arxiv.org/abs/2504.21330
- Reference count: 40
- Key outcome: Prompt-based LLMs can accurately infer first-language background and introduce scoring bias, especially for non-native English speakers when correctly identified.

## Executive Summary
This study investigates whether Large Language Models (LLMs) can infer student demographic attributes such as gender and first-language background from essays, and whether this ability introduces bias in automated essay scoring. Using a dataset of over 25,000 argumentative essays, the authors employed GPT-4o to predict demographics and assign scores, then analyzed the results using multiple fairness metrics and multivariate regression. The findings reveal that LLMs can accurately infer first-language background with high precision but show less accuracy for gender prediction with low coverage rates. Importantly, scoring bias was significantly more pronounced when LLMs correctly identified students' first-language background, particularly affecting non-native English speakers. These results highlight the potential for prompt-based LLMs to perpetuate demographic bias in educational assessments, emphasizing the need for targeted debiasing strategies.

## Method Summary
The researchers collected a dataset of over 25,000 argumentative essays from the Automated Student Assessment Prize (ASAP) competition. They used GPT-4o to predict student demographics (gender and first-language background) and assign essay scores based on specific prompts. To assess bias, they employed multiple fairness metrics including Equal Opportunity Difference, Disparate Impact, and Average Odds Difference, alongside multivariate regression analysis to control for essay quality and other factors. The study compared LLM scoring accuracy and bias across different demographic groups, with particular attention to cases where the LLM correctly or incorrectly inferred student backgrounds.

## Key Results
- LLMs accurately inferred first-language background (accuracy 0.75-0.87) but were less accurate for gender (accuracy 0.86-0.96 with low coverage)
- Scoring bias increased significantly when LLMs correctly identified first-language background, disadvantaging non-native English speakers
- No significant gender-based scoring bias was observed in the analysis

## Why This Works (Mechanism)
The mechanism behind demographic inference in LLMs appears to rely on linguistic patterns and stylistic markers that correlate with a writer's background. First-language background can be inferred through vocabulary choices, syntactic structures, and idiomatic expressions that reflect a writer's native language influence. Gender inference may depend on more subtle linguistic markers, though the study found this less reliable. The increased bias when demographics are correctly identified suggests that LLMs may apply different scoring criteria or weight certain linguistic features differently based on inferred background, potentially reflecting or amplifying existing societal biases present in training data.

## Foundational Learning

### Prompt Engineering
**Why needed:** LLMs rely heavily on how instructions are formulated, affecting both inference accuracy and scoring behavior.
**Quick check:** Test different prompt formulations to observe changes in demographic inference accuracy and scoring consistency.

### Fairness Metrics in ML
**Why needed:** Standard metrics (Equal Opportunity Difference, Disparate Impact, Average Odds Difference) quantify bias across demographic groups.
**Quick check:** Calculate these metrics on validation data to establish baseline fairness levels.

### Multivariate Regression Analysis
**Why needed:** Controls for confounding variables like essay quality when isolating demographic bias effects.
**Quick check:** Run regression with and without demographic variables to measure their independent impact on scoring.

## Architecture Onboarding

### Component Map
Dataset -> Prompt Engineering -> LLM Inference -> Fairness Metrics -> Bias Analysis

### Critical Path
Essay text → Demographic inference prompts → Score prediction prompts → Bias metric calculation → Regression analysis

### Design Tradeoffs
The study chose prompt-based inference over fine-tuning to maintain LLM generalizability, but this approach may limit accuracy compared to specialized models. Using multiple fairness metrics provides comprehensive bias assessment but increases computational complexity.

### Failure Signatures
Low coverage rates in gender prediction suggest prompts may not capture subtle linguistic gender markers. High accuracy in first-language inference but increased bias when correct indicates the LLM may apply different scoring criteria based on inferred background.

### First Experiments
1. Test demographic inference accuracy on essays from different subject areas
2. Vary prompt formulations to assess impact on inference accuracy and bias
3. Compare LLM bias levels against human scoring baseline

## Open Questions the Paper Calls Out

## Limitations
- Findings may not generalize to essay types beyond argumentative writing
- The study focuses on one large dataset, potentially limiting diversity representation
- Does not explore how different prompt formulations might affect inference capability or bias

## Confidence
- High confidence in LLM's ability to accurately infer first-language background (consistent accuracy metrics across validation runs)
- Medium confidence that scoring bias increases when LLMs correctly identify first-language background (correlation could be influenced by unmeasured essay factors)
- Medium confidence in absence of significant gender bias findings (low coverage rates may have limited statistical power)

## Next Checks
1. Test the same methodology on essays from different subject areas and educational levels to assess generalizability
2. Systematically vary prompt formulations to determine how different instructions affect demographic inference and scoring bias
3. Conduct human scoring comparisons to establish baseline bias levels and validate whether LLM bias exceeds human scoring bias in similar contexts