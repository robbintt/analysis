---
ver: rpa2
title: 'CAMEO: Collection of Multilingual Emotional Speech Corpora'
arxiv_id: '2505.11051'
source_url: https://arxiv.org/abs/2505.11051
tags:
- speech
- emotional
- datasets
- dataset
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CAMEO, a publicly available collection of
  13 multilingual emotional speech datasets spanning 8 languages and 17 emotions.
  The collection provides standardized metadata, audio normalization, and open evaluation
  tools to facilitate reproducible benchmarking in speech emotion recognition (SER).
---

# CAMEO: Collection of Multilingual Emotional Speech Corpora

## Quick Facts
- arXiv ID: 2505.11051
- Source URL: https://arxiv.org/abs/2505.11051
- Reference count: 0
- Primary result: CAMEO collection of 13 multilingual emotional speech datasets with standardized metadata and evaluation tools

## Executive Summary
This work introduces CAMEO, a publicly available collection of 13 multilingual emotional speech datasets spanning 8 languages and 17 emotions. The collection provides standardized metadata, audio normalization, and open evaluation tools to facilitate reproducible benchmarking in speech emotion recognition (SER). Four models were evaluated across different temperature settings, with the best model (Qwen2-Audio) achieving a macro F1 score of 0.20, indicating the inherent difficulty of multilingual zero-shot SER. Sadness was most reliably recognized, while neutral speech posed the greatest challenge. Performance disparities across languages and datasets suggest possible training data overlap, highlighting the need for careful dataset curation and evaluation protocols. CAMEO is released with a public leaderboard on Hugging Face to support ongoing model comparison and future expansion.

## Method Summary
The CAMEO collection unifies 13 emotional speech datasets by standardizing audio to FLAC (16-bit, 16 kHz) and metadata to JSON Lines format with consistent fields. Evaluation uses zero-shot AudioLLM inference with text instructions and audio inputs, followed by Levenshtein-ratio based post-processing to extract emotion labels from model outputs. Four models (Qwen2-Audio, Ichigo-llama3.1-s, SeaLLMs-Audio, ultravox-v0_5-llama-3_1-8b) were tested at temperatures 0.0, 0.3, and 0.7. The approach deliberately omits train/test splits due to potential pretraining contamination, treating the collection as a zero-shot benchmark rather than a held-out test set.

## Key Results
- Qwen2-Audio achieved best macro F1 of 0.20 at temperature 0.0
- Sadness recognition averaged F1 0.27, while neutral speech averaged F1 0.03
- CREMA-D and RAVDESS showed unusually high performance (F1 0.80 and 0.73) suggesting pretraining contamination
- Weighted F1 (0.37) substantially exceeded macro F1 (0.20), indicating performance skew toward frequent classes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standardized audio formatting and unified metadata enable cross-lingual SER benchmarking that would otherwise be impractical due to dataset heterogeneity.
- **Mechanism:** Converting all audio to FLAC (16-bit, 16 kHz) and serializing metadata to JSON Lines with consistent fields (file_id, emotion, transcription, speaker_id, gender, language, etc.) removes format-level variability, allowing models to be evaluated on emotional content rather than artifact differences.
- **Core assumption:** Acoustic emotion cues are preserved across sampling rate conversions and that transcription alignment via Whisper Large v2 maintains utterance boundaries accurately.
- **Evidence anchors:**
  - [section] "Audio samples were converted to FLAC (16-bit, 16 kHz). Metadata and transcriptions were unified across datasets."
  - [section] Table 2 shows standardized metadata fields available across all samples.
  - [corpus] Neighbor papers (EM2LDL, EmoTale) similarly emphasize corpus standardization for multilingual SER but focus on single-language collection; CAMEO's cross-corpus unification is distinct.
- **Break condition:** If original datasets contain emotion annotations based on fundamentally incompatible taxonomies (beyond Plutchik alignment), standardization may introduce label noise rather than reduce it.

### Mechanism 2
- **Claim:** Levenshtein-ratio based post-processing recovers valid emotion labels from generative model outputs that deviate from instructed single-word noun format.
- **Mechanism:** When models output phrases ("The answer is sadness") or adjectives ("sad") instead of nouns ("sadness"), the system splits responses, filters by similarity threshold (0.57), and aggregates scores to select the best-matching label. The threshold was calibrated on noun-adjective Levenshtein ratios for the 7 primary emotions.
- **Core assumption:** The Levenshtein ratio threshold of 0.57 correctly separates intentional emotion words from incidental similar strings, and aggregated scoring doesn't systematically favor certain labels.
- **Evidence anchors:**
  - [section] "The value of threshold was determined based on the Levenshtein ratio between the noun and adjective forms of the seven primary emotional states" (Table 3).
  - [section] Figure 1 illustrates the full post-processing pipeline with example outputs.
  - [corpus] No direct corpus evidence on alternative post-processing strategies; this appears novel to CAMEO's evaluation protocol.
- **Break condition:** If models generate semantically related but lexically distant descriptors (e.g., "melancholy" for sadness), the Levenshtein approach will fail; semantic similarity (e.g., embedding-based) would be required.

### Mechanism 3
- **Claim:** Sadness recognition outperforms other emotions due to distinctive low-arousal acoustic features; neutral speech underperforms due to acoustic overlap with multiple low-arousal states.
- **Mechanism:** Sadness exhibits stable acoustic cues (reduced pitch variation, slower tempo) that generalize across languages. Neutral speech lacks salient prosodic markers and overlaps acoustically with low-arousal emotions, causing frequent misclassification.
- **Core assumption:** The acoustic features distinguishing sadness are language-independent, while neutral speech variability is similarly cross-linguistic.
- **Evidence anchors:**
  - [section] "Sadness was the most reliably recognized emotion across all systems, while neutral speech was the most challenging" (Table 5: sadness avg F1=0.27, neutral avg F1=0.03).
  - [section] "This may be due to the fact that sadness often exhibits more stable and distinctive acoustic cues, such as reduced pitch variation and slower tempo."
  - [corpus] Neighbor papers don't provide comparative cross-lingual emotion difficulty rankings; this finding is CAMEO-specific.
- **Break condition:** If training data contamination (e.g., CREMA-D, RAVDESS in Qwen2-Audio pretraining) systematically biases certain emotion-label mappings, observed patterns may reflect data exposure rather than true acoustic distinctiveness.

## Foundational Learning

- **Concept:** Zero-shot evaluation with AudioLLMs
  - **Why needed here:** All CAMEO evaluations use instruction-following audio models without task-specific fine-tuning; understanding what zero-shot means (no gradient updates on target task) is essential to interpret why macro F1 scores are low (0.12–0.20).
  - **Quick check question:** If you fine-tuned Qwen2-Audio on CAMEO training data, would the resulting evaluation still be "zero-shot"? (Answer: No—fine-tuning breaks the zero-shot condition.)

- **Concept:** Macro vs. Weighted F1 for imbalanced multiclass problems
  - **Why needed here:** CAMEO reports both; macro F1 (0.20 best) treats all classes equally, while weighted F1 (0.37) accounts for class frequency. English and primary emotions dominate, so weighted metrics can mask per-class failures.
  - **Quick check question:** If a model achieves 0.40 weighted F1 but 0.10 macro F1 on a 10-class problem, what does this indicate? (Answer: The model performs well on frequent classes but fails on rare classes.)

- **Concept:** Data contamination in benchmarking
  - **Why needed here:** CAMEO explicitly flags that Qwen2-Audio's high performance on CREMA-D (0.80) and RAVDESS (0.73) likely reflects pretraining exposure. Understanding contamination is critical for interpreting leaderboard results.
  - **Quick check question:** Why doesn't CAMEO define train/test splits? (Answer: Public datasets may already be in model training data; splitting doesn't prevent contamination, so CAMEO treats the collection as a zero-shot probe rather than a held-out test.)

## Architecture Onboarding

- **Component map:** 13 source datasets → standardized FLAC audio + JSON Lines metadata → Hugging Face dataset (`amu-cai/CAMEO`) → Model inference (text instruction + audio) → Levenshtein post-processing → Label extraction → F1/accuracy computation → Hugging Face leaderboard (`amu-cai/cameo-leaderboard`)

- **Critical path:**
  1. Load CAMEO dataset from Hugging Face
  2. For each sample: construct instruction prompt + provide audio
  3. Generate model output at specified temperature
  4. Apply Levenshtein post-processing if output doesn't match labels exactly
  5. Compute macro F1, weighted F1, accuracy across all samples

- **Design tradeoffs:**
  - No train/test splits → enables use as benchmark but doesn't prevent contamination; assumption is that zero-shot evaluation probes generalization, not held-out performance
  - Levenshtein threshold 0.57 → balances noun-adjective matching vs. false positive matches; may miss semantically related synonyms
  - Single-word output expectation → simplifies evaluation but conflicts with generative model tendencies toward full sentences

- **Failure signatures:**
  - Model outputs full sentence despite instruction → post-processing should recover label, but if multiple emotion words appear, aggregated scoring may select incorrectly
  - Very low neutral F1 (~0.03) → expected; if neutral F1 is high, suspect data contamination or label imbalance issues
  - Disproportionately high performance on CREMA-D/RAVDESS → likely training data overlap; not a model capability signal

- **First 3 experiments:**
  1. **Baseline reproduction:** Run Qwen2-Audio at temperature 0.0 on full CAMEO; verify macro F1 ≈ 0.20 and per-dataset CREMA-D/RAVDESS elevation.
  2. **Post-processing ablation:** Compare exact-match-only vs. Levenshtein post-processing; quantify how many valid outputs are recovered vs. incorrectly matched.
  3. **Language-specific analysis:** For a single model, compute per-language macro F1 and per-emotion breakdown; test whether sadness advantage and neutral deficit hold across all 8 languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can contamination between pretraining data and benchmark evaluation sets be reliably detected and quantified in speech emotion recognition?
- Basis in paper: [explicit] The authors observe that Qwen2-Audio performed substantially better on widely-used datasets (CREMA-D: 0.80, RAVDESS: 0.73, eNTERFACE: 0.54) and explicitly state "these results suggest the possibility of data contamination."
- Why unresolved: The paper identifies the problem but provides no methodology to distinguish genuine model generalization from memorization of training examples.
- What evidence would resolve it: A systematic comparison of model performance on held-out emotional speech versus known-public benchmarks, or analysis of training data logs from model developers.

### Open Question 2
- Question: What acoustic or representational features could enable more reliable recognition of neutral speech across languages?
- Basis in paper: [explicit] The authors report neutral speech as the most challenging emotion (average F1: 0.03) and hypothesize it "is inherently heterogeneous and often acoustically overlaps with low-arousal emotional states."
- Why unresolved: The paper documents the problem but offers no intervention or architectural modification to address it.
- What evidence would resolve it: Targeted experiments with models trained on enhanced neutral speech representations or multi-task learning with arousal detection.

### Open Question 3
- Question: What evaluation protocols can balance the need for standardized benchmarks against the risk of dataset memorization in an era of web-scale audio pretraining?
- Basis in paper: [inferred] The authors deliberately omit train/test splits, stating "further splitting would reduce diversity and not solve the problem of contamination," yet this leaves no held-out evaluation mechanism.
- Why unresolved: The paper acknowledges the trade-off but offers no solution beyond releasing public leaderboards that may themselves become training data.
- What evidence would resolve it: Development of privacy-preserving benchmark construction (e.g., synthetic emotional speech) or temporally-held-out evaluation sets created after model training cutoffs.

## Limitations

- Performance metrics are low (macro F1 0.12–0.20) due to zero-shot evaluation on multilingual, imbalanced data
- No train/test splits prevent traditional generalization assessment and may reflect contamination rather than capability
- Levenshtein-based post-processing may fail on semantically related but lexically distant emotion descriptors

## Confidence

- **High** in empirical observations: CAMEO collection availability, evaluation methodology clarity, and reported performance metrics are reproducible
- **Medium** in performance interpretation: Sadness recognition advantage and neutral speech difficulty are supported, but acoustic distinctiveness claims lack feature-level validation
- **Low** in protocol generalizability: Levenshtein threshold calibration is limited, and absence of held-out evaluation prevents true generalization assessment

## Next Checks

1. **Acoustic Feature Validation:** Conduct controlled acoustic analysis (pitch range, tempo, spectral centroid) on sadness vs. neutral utterances across languages to confirm recognition gap is grounded in measurable acoustic differences

2. **Post-Processing Robustness Test:** Systematically vary Levenshtein ratio threshold (0.4–0.8) to evaluate trade-off between valid emotion word recovery and incorrect matches; supplement with manual annotation set for semantic matching failures

3. **Contamination Impact Quantification:** For each high-performing model-dataset pair, estimate training exposure probability using dataset release dates and model training timelines; compare performance on potentially contaminated vs. uncontaminated datasets to isolate contamination effect