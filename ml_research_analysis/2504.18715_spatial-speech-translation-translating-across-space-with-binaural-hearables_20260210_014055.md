---
ver: rpa2
title: 'Spatial Speech Translation: Translating Across Space With Binaural Hearables'
arxiv_id: '2504.18715'
source_url: https://arxiv.org/abs/2504.18715
tags:
- translation
- speech
- binaural
- expressive
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces spatial speech translation, the first real-time\
  \ binaural hearable system that translates multiple speakers in the wearer\u2019\
  s environment while preserving spatial cues and voice characteristics. The system\
  \ combines blind source separation, localization, simultaneous expressive translation,\
  \ and binaural rendering to handle noisy, reverberant, real-world conditions."
---

# Spatial Speech Translation: Translating Across Space With Binaural Hearables

## Quick Facts
- arXiv ID: 2504.18715
- Source URL: https://arxiv.org/abs/2504.18715
- Reference count: 40
- Real-time binaural hearable system translating multiple speakers while preserving spatial cues and voice characteristics

## Executive Summary
This paper introduces spatial speech translation, the first real-time binaural hearable system that translates multiple speakers in the wearer's environment while preserving spatial cues and voice characteristics. The system combines blind source separation, localization, simultaneous expressive translation, and binaural rendering to handle noisy, reverberant, real-world conditions. A joint localization and separation module divides the auditory space into angular regions and extracts clean binaural speech from each speaker. A streaming translation model translates speech with minimal latency while preserving prosody and speaker identity. Binaural rendering transfers spatial cues from the input to the translated output.

## Method Summary
The system synthesizes binaural training data by convolving speech from CoVoST2 with BRIRs from four datasets and mixing 2-3 speakers plus noise. A search-based joint localization and separation module divides the auditory space into 36 angular regions, using a TF-GridNet model to extract separated binaural speech. The translation pipeline uses a Conformer encoder with chunk-based streaming and simultaneous S2T with READ/WRITE policy. Fine-tuning on separation outputs improves robustness to artifacts. Expressive T2S preserves speaker characteristics through text-to-units conversion with frozen expressive encoder/vocoder. Binaural rendering applies generic HRTF with ILD compensation from separated sources.

## Key Results
- Achieved BLEU scores up to 22.01 on real-world data
- Improved speaker similarity with expressive translation (VSim 0.25 vs 0.013)
- Preserved spatial cues with low angular and ITD/ILD errors
- Maintained real-time performance with RTF 0.725

## Why This Works (Mechanism)

### Mechanism 1: Search-Based Joint Localization and Separation
The system divides acoustic space into angular regions and searches each with a neural separator to extract individual speakers without prior knowledge of speaker count. Binaural input is shifted by TDoA corresponding to each target angle, aligning channels when a source exists at that angle. A TF-GridNet model outputs separated binaural speech or silence, with clustering eliminating multipath duplicates.

### Mechanism 2: Fine-Tuning Translation on Separation Artifacts
Fine-tuning the translation model on imperfect separation outputs improves robustness to residual distortions from interfering speakers. The model learns to handle distortions by mixing 50% original CoVoST2 data with 50% separation model outputs during training.

### Mechanism 3: ILD Compensation for Binaural Rendering
Transferring interaural level differences from separated source to translated output preserves spatial perception better than generic HRTF convolution alone. Generic HRTF provides accurate ITD but poor ILD estimates. The system computes ILD from separated binaural signal and scales HRTF-convolved translated speech to match.

## Foundational Learning

- **Head-Related Transfer Functions (HRTF)**: Captures how sound filters through head, ears, and torso before reaching eardrums, encoding spatial cues essential for binaural rendering. Quick check: Can you explain why a generic HRTF provides reasonable ITD but poor ILD estimates?

- **Blind Source Separation (BSS)**: Separates mixed speech signals from multiple speakers without knowing source signals or mixing coefficients. The search-based approach addresses unknown speaker count. Quick check: How does the clustering step eliminate false positives from multipath reflections?

- **Simultaneous Translation with READ/WRITE Policy**: Real-time translation requires deciding when enough context has accumulated to output target tokens, balancing latency against translation quality. Quick check: What triggers a WRITE action in the simultaneous S2T translation module?

## Architecture Onboarding

- Component map: Binaural input -> Angular search (36 parallel TF-GridNet instances) -> Clustering -> Separated binaural speech -> Mel features -> Speech encoder -> S2T translation -> T2S generation -> HRTF convolution + ILD compensation -> Binaural output

- Critical path: Binaural input → Angular search (36 parallel TF-GridNet instances) → Clustering → Separated binaural speech → Mel features → Speech encoder → S2T translation → T2S generation → HRTF convolution + ILD compensation → Binaural output

- Design tradeoffs: Larger chunks (960 ms) improve translation context but increase latency (~3s average lag). Smaller chunks reduce latency but degrade BLEU. 175M params fit on-device but limit BLEU vs. 2B param cloud models. Expressive preserves speaker characteristics but slightly reduces ASR-BLEU.

- Failure signatures: Translation produces garbled output when multiple speakers overlap → Separation model failed to isolate sources. Perceived direction mismatches speaker location → ILD compensation not applied or AoA estimation error >20°. Robotic translated speech → Expressive encoder not integrated. RTF exceeds 1.0 → Too many angular regions active.

- First 3 experiments: 1) Validate separation on synthetic mixtures with known AoAs. 2) Ablate fine-tuning to compare ASR-BLEU improvement. 3) Verify spatial cue preservation by computing ΔITD and ΔILD.

## Open Questions the Paper Calls Out

### Open Question 1
Can knowledge distillation or fine-tuning techniques effectively decouple speaker expressiveness from residual noise and distortion in the separated source signal? The current frozen expressive encoder preserves input noise/distortion, and the authors suggest applying fine-tuning and knowledge distillation to resolve this.

### Open Question 2
Can the system maintain the necessary sub-millisecond binaural synchronization when deployed on a wireless form factor? The prototype relies on a wired connection to avoid synchronization and latency issues inherent to wireless transmission standards.

### Open Question 3
To what degree does replacing generic HRTFs with personalized HRTFs improve localization accuracy and user immersion? The current system uses the generic CIPIC dataset, which may not account for individual ear geometry variations.

### Open Question 4
How does scaling the translation model to 2 billion parameters impact the trade-off between translation latency and accuracy on mobile hardware? The current 175M parameter model limits BLEU scores, and the authors suggest exploring larger models as hardware accelerators advance.

## Limitations

- Physical and perceptual constraints from far-field assumptions and fixed microphone geometry may degrade spatial accuracy with near-field sources or head movements
- Generalization boundaries due to reliance on synthetic mixtures from specific BRIR datasets may not cover all real-world acoustic environments
- Technical trade-offs between chunk size, translation quality, and latency require balancing ASR-BLEU against Average Lagging

## Confidence

**High Confidence**: Search-based joint localization and separation mechanism, ILD compensation for binaural rendering, and overall system architecture achieving real-time performance.

**Medium Confidence**: Fine-tuning strategy improving translation robustness and expressive T2S preserving speaker characteristics.

**Low Confidence**: Claims about handling arbitrary numbers of speakers in real-world conditions without specific speaker count limits.

## Next Checks

1. Deploy the system in environments with varying reverberation times (T60 from 100ms to 1000ms) and evaluate localization accuracy, separation SI-SDRi, and translation BLEU.

2. Implement head-tracking mechanism and test spatial cue preservation when the wearer rotates their head ±45°, measuring angular error and ITD/ILD drift.

3. Test the system with sources at 0.5-1m distance versus the assumed far-field (>1m) to quantify performance degradation near the far-field boundary.