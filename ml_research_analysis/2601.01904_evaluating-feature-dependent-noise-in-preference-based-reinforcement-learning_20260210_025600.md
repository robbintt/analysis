---
ver: rpa2
title: Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning
arxiv_id: '2601.01904'
source_url: https://arxiv.org/abs/2601.01904
tags:
- noise
- learning
- reward
- uniform
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces feature-dependent noise (FDN) models within
  preference-based reinforcement learning (PbRL), where noise in preference labels
  is systematically linked to trajectory features. The authors formalize FDN and propose
  variants such as trajectory similarity noise, trajectory feature magnitude noise,
  uncertainty-aware noise, and language model (LM) noise.
---

# Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.01904
- Source URL: https://arxiv.org/abs/2601.01904
- Reference count: 40
- Primary result: Feature-dependent noise significantly degrades preference-based RL performance and is harder to detect than uniform noise.

## Executive Summary
This paper introduces feature-dependent noise (FDN) models in preference-based reinforcement learning (PbRL), where noise in preference labels is systematically linked to trajectory features. The authors formalize FDN and propose variants such as trajectory similarity noise, trajectory feature magnitude noise, uncertainty-aware noise, and language model (LM) noise. They evaluate these noise models across DMControl and Meta-world domains using a state-of-the-art denoising algorithm, RIME. The experiments show that FDN is harder to detect than uniform noise, particularly hybrid noise, which combines behavioral and model-uncertainty factors. FDN significantly degrades learning performance, and existing denoising methods struggle to handle it. Additionally, LM-based feedback exhibits characteristics similar to FDN, further complicating preference learning. The study highlights the need for robust denoising techniques tailored to feature-dependent noise.

## Method Summary
The paper introduces a formalism for feature-dependent noise in PbRL by defining a noisy teacher $T_n$ that corrupts preference labels based on a noise function $N(\tau_1, \tau_2)$ which depends on trajectory feature subsets. The authors propose several variants: trajectory similarity noise (flips labels more often for similar trajectories), trajectory feature magnitude noise (flips based on feature magnitude), uncertainty-aware noise (flips when the model is uncertain), and hybrid noise (combining behavioral and uncertainty factors). They evaluate these using RIME, a denoising algorithm that filters samples based on KL divergence between predicted and observed preferences. Experiments are conducted on DMControl and Meta-world domains with both synthetic teachers and VLMs.

## Key Results
- Feature-dependent noise (FDN) is harder to detect than uniform noise, especially hybrid noise combining behavioral and model-uncertainty factors.
- Existing denoising methods like RIME significantly degrade performance under FDN, with hybrid noise causing learning collapse in several cases.
- Language model (LM) based feedback exhibits FDN-like characteristics, particularly when comparing similar observations or requiring advanced 3D reasoning.
- In some cases, PbRL methods without explicit denoising (like PEBBLE or SURF) outperform RIME when facing FDN, suggesting current denoising strategies may be counterproductive.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature-Dependent Noise (FDN) degrades PbRL performance more than uniform noise because its correlation with trajectory features allows it to mimic legitimate learning uncertainty, evading standard denoising heuristics.
- Mechanism: The noise function $N(\tau_1, \tau_2)$ computes the probability of a label flip based on feature subsets of the trajectory pair, rather than a fixed constant. This creates structured errors. For instance, **Trajectory Similarity Noise** flips labels more often for similar trajectories ($N \propto 1/D(\phi(\tau_1), \phi(\tau_2))$), which are inherently ambiguous. Denoising methods like RIME, which use KL divergence between predicted and observed preferences, can mistake these structured errors for high-uncertainty but valid data points, failing to filter them.
- Core assumption: The noise function is symmetric and denoising algorithms are primarily calibrated for uniform, feature-independent noise.
- Evidence anchors:
  - [abstract] "...noise is correlated with certain features...state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated..."
  - [section 3] Formal definition of the noisy teacher $T_n$ and the feature-dependent noise function $N(\tau_1, \tau_2) = P(Y \neq Y^* | Y^*, \phi(\tau_1), \phi(\tau_2))$.
  - [corpus] Related work acknowledges robustness issues but often focuses on diverse feedback sources or data augmentation, not explicitly on the correlation between noise and specific features as a primary attack vector.
- Break condition: If denoising algorithms are adapted to model the feature-noise correlation directly (e.g., by learning $N$ or using feature-aware filtering), or if noise is uniform, this mechanism's effect diminishes.

### Mechanism 2
- Claim: Hybrid noise, combining behavioral features and model uncertainty, is particularly challenging because it targets instances where the teacher is likely to err *and* the model is unsure, making noise statistically indistinguishable from legitimate hard samples.
- Mechanism: Hybrid noise computes a combined score $score(x) = \alpha \cdot score_f(x) + (1 - \alpha) \cdot score_u(x)$. It flips labels for pairs that are behaviorally ambiguous (e.g., similar or unstable) *and* for which the current reward model has high uncertainty (small predicted return difference). This dual-targeting makes noisy samples appear similar to genuinely difficult samples, bypassing both uncertainty-based and feature-based filters.
- Core assumption: Teacher errors and model uncertainty are positively correlated; behavioral ambiguity leads to both human error and model uncertainty.
- Evidence anchors:
  - [abstract] "...hybrid noise, which combines behavioral and model-uncertainty factors... significant degrades learning performance..."
  - [section 3.1] Definition of Hybrid Noise and the intuition that targeting uncertain regions with behavioral noise makes preference ambiguity and annotation error hard to distinguish.
  - [corpus] Weak/missing. The corpus does not explicitly discuss hybrid feature-uncertainty noise models.
- Break condition: If behavioral ambiguity and model uncertainty are decorrelated, or if the denoising method can effectively disentangle noise from true uncertainty, the hybrid effect weakens.

### Mechanism 3
- Claim: Language Model (LM) based feedback exhibits characteristics of FDN because VLMs struggle with subtle visual differences, leading to errors concentrated on similar-looking observation pairs or those requiring advanced 3D reasoning.
- Mechanism: VLMs are prompted to compare two image observations. Errors occur when (1) the observations are visually similar (similar angle in CartPole) or (2) the visual task requires understanding beyond the model's capability (e.g., occluded soccer ball). This concentrates noise on specific feature-dependent subsets, mimicking the **Trajectory Similarity Noise** formalized for humans.
- Core assumption: VLM visual encoders have limited resolution or semantic understanding, causing systematic failures on specific feature subsets rather than randomly.
- Evidence anchors:
  - [abstract] "...LM-based feedback exhibits characteristics similar to FDN..."
  - [section 4.2] "...VLM gives noisy preferences mostly due to these two reasons: (1) similar observations; (2) it requires image understanding ability beyond the VLM."
  - [corpus] Related work like "RL-VLM-F" uses LLMs for preference but may not explicitly frame their noise as feature-dependent.
- Break condition: If VLMs achieve human-level visual understanding with no systematic biases, or if prompts are engineered to force the model to reason about subtle features, the FDN-like characteristic may be reduced.

## Foundational Learning

- Concept: **Preference-based Reinforcement Learning (PbRL) and the Bradley-Terry Model**
  - Why needed here: This is the core framework. You cannot understand FDN's impact without understanding how preferences are converted to a reward model via the Bradley-Terry probability formulation $P_\theta(\tau_1 \succ \tau_2)$.
  - Quick check question: Given two trajectories $\tau_1$ and $\tau_2$, how does the Bradley-Terry model compute the probability of preferring $\tau_1$ using a learned reward function $\hat{R}_\theta$?

- Concept: **Noise Functions and Label Corruption**
  - Why needed here: The paper's central contribution is formalizing noise functions $N(\tau_1, \tau_2)$. You must grasp the difference between a constant noise probability (uniform) and one that varies with feature subsets $\phi(\tau_1), \phi(\tau_2)$.
  - Quick check question: In the non-expert teacher model $T_n$, how does the noise function $N$ affect the probability of observing a flipped preference label?

- Concept: **Denoising in PbRL (e.g., RIME's KL Divergence)**
  - Why needed here: The paper evaluates FDN's impact using RIME, a state-of-the-art denoising method. You need to understand that RIME filters samples based on KL divergence between predicted and observed preferences, which assumes certain noise patterns.
  - Quick check question: Under what condition does RIME consider a preference sample potentially noisy, and why might this criterion fail against feature-dependent noise?

## Architecture Onboarding

- Component map:
  - Environment -> Teacher Oracle ($T_o$) -> Noise Function ($N$) -> Reward Model ($\hat{R}_\theta$) -> Denoising Filter (RIME) -> RL Policy ($\pi$)

- Critical path:
  1. Collect trajectory pairs $(\tau_1, \tau_2)$.
  2. Query teacher $T_o$ for ground truth based on episodic return.
  3. Apply noise function $N(\tau_1, \tau_2)$ to flip labels and produce noisy label $y$.
  4. Feed $(y, \tau_1, \tau_2)$ to the Reward Model and Denoising Filter.
  5. Filter processes labels and provides a cleaner dataset for Reward Model updates.
  6. Policy learns from updated Reward Model.

- Design tradeoffs:
  - **Noise Complexity vs. Realism**: Uniform noise is simple but unrealistic. Hybrid FDN is realistic (mimics human/VLM error) but complex to model and harder to defend against.
  - **Denoising Aggressiveness**: A strict filter (low KL threshold) may remove correct labels in uncertain regions; a lenient filter may miss FDN.

- Failure signatures:
  - **Learning Collapse**: Under high (30-40%) hybrid or uncertainty-aware FDN, the policy's episodic return plateaus near zero (e.g., HalfCheetah in Figure 3).
  - **Denoising Inversion**: RIME performs worse than no denoising (as seen in 94% of Cheetah run cases under FDN), suggesting the filter is systematically rejecting clean but uncertain labels or accepting cleverly placed noise.
  - **VLM Paradox**: A smaller, noisier VLM sometimes enables faster learning than a larger, more accurate one (section 4.2, CartPole), because its noise, while higher, may be less adversarially correlated.

- First 3 experiments:
  1. **Baseline Noise Comparison**: Train PEBBLE and RIME on HalfCheetah with 20% Uniform noise vs. 20% Trajectory Similarity Noise. Compare final episodic returns to confirm FDN is harder.
  2. **Hybrid Noise Analysis**: Train RIME on Walker with 10% and 30% Hybrid Noise (Magnitude + Uncertainty, $\alpha=0.5$). Plot learning curves to observe if the hybrid combination causes a steeper performance drop than either noise type alone.
  3. **VLM Noise Characterization**: Use a VLM (e.g., Qwen 2.5 VL 7B) to label preferences for CartPole. Measure the noise rate and then inject that rate of Trajectory Similarity Noise via the scripted teacher. Compare policy learning performance to test if the VLM's noise is functionally similar to the formalized FDN model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can denoising algorithms be specifically designed to identify and correct feature-dependent noise (FDN) rather than uniform noise?
- **Basis in paper:** [explicit] The conclusion states that future work must "explore denoising algorithms tailored to such noise."
- **Why unresolved:** Current state-of-the-art methods like RIME rely on assumptions of uniform noise distribution or metrics like KL divergence, which fail to detect the structured correlations characteristic of FDN.
- **What evidence would resolve it:** The development of an algorithm that maintains performance stability on the introduced "Hybrid Noise" benchmarks (which combine behavioral and model-uncertainty factors) would resolve this.

### Open Question 2
- **Question:** To what extent do real-world human annotators systematically induce the proposed feature-dependent noise models?
- **Basis in paper:** [explicit] The authors explicitly call for "user studies to understand how often non-experts induce these biases" in the conclusion.
- **Why unresolved:** The current study relies on scripted synthetic teachers and theoretical noise models (simulated irrationality) rather than empirical data from human subjects.
- **What evidence would resolve it:** User studies that map human error rates against trajectory features (e.g., similarity or magnitude) to validate if the formalized FDN models accurately reflect actual human feedback behavior.

### Open Question 3
- **Question:** Why do PbRL methods without explicit denoising mechanisms sometimes outperform state-of-the-art denoising algorithms like RIME when facing feature-dependent noise?
- **Basis in paper:** [inferred] The abstract and results note that methods without explicit denoising "can surprisingly outperform noise-robust PbRL in majority settings," raising questions about the efficiency of current denoising strategies.
- **Why unresolved:** It is unclear if current denoising mechanisms inadvertently filter informative preference data or if standard regularization in non-denoising methods offers implicit robustness to structured noise.
- **What evidence would resolve it:** An ablation study analyzing the specific data samples filtered by RIME versus those retained by PEBBLE/SURF under FDN conditions to identify the cause of performance degradation.

## Limitations
- The paper relies on synthetic noise models rather than empirical human feedback data, limiting real-world applicability.
- The assumption that denoising methods can be adapted to handle FDN without significant architectural changes remains speculative.
- The study does not explore the full spectrum of potential noise sources in human-in-the-loop systems, such as cognitive biases or contextual factors.

## Confidence
- **High**: Core claims about FDN's impact on PbRL performance are supported by systematic experiments across multiple domains and noise types.
- **Medium**: Analysis of hybrid noise and its distinction from uniform noise is compelling but relies on specific noise function formulations that may not capture all real-world scenarios.
- **Medium**: VLM noise characterization demonstrates FDN-like behavior but does not fully validate the proposed noise function's applicability to all VLM models.

## Next Checks
1. Test FDN robustness in a real human feedback loop with tasks requiring fine-grained visual or behavioral distinctions.
2. Evaluate alternative denoising algorithms (e.g., uncertainty-aware or feature-conditional filters) against FDN to identify potential improvements.
3. Analyze the impact of FDN on policy generalization by testing trained policies on unseen tasks or environments.