---
ver: rpa2
title: Sensitivity Analysis for Diffusion Models
arxiv_id: '2509.23092'
source_url: https://arxiv.org/abs/2509.23092
tags:
- samples
- diffusion
- score
- training
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a tractable framework for computing how diffusion
  model scores and samples change in response to small perturbations in the training
  data distribution. The key insight is that diffusion models define a map from their
  training distribution to optimal score functions, and the authors derive a closed-form
  expression for the directional derivative of this map.
---

# Sensitivity Analysis for Diffusion Models

## Quick Facts
- arXiv ID: 2509.23092
- Source URL: https://arxiv.org/abs/2509.23092
- Authors: Christopher Scarvelis; Justin Solomon
- Reference count: 34
- Primary result: Framework for computing how diffusion model scores and samples change under small perturbations to training data

## Executive Summary
This paper develops a tractable framework for computing how diffusion model scores and samples change in response to small perturbations in the training data distribution. The key insight is that diffusion models define a map from their training distribution to optimal score functions, and the authors derive a closed-form expression for the directional derivative of this map. This allows them to compute sensitivity of both score functions and model samples to additive perturbations in the training distribution.

The method requires only black-box access to a pre-trained score model and its derivatives, making it broadly applicable. The authors demonstrate their approach on synthetic mixtures of Gaussians and show it achieves the expected O(η) convergence rate for approximating perturbed samples, even with numerical errors from density estimation and neural score approximations.

## Method Summary
The authors exploit the fact that diffusion models define a map from their training distribution to optimal score functions. By deriving a closed-form expression for the directional derivative of this map, they can compute how the score function and generated samples change under small perturbations to the training distribution. The approach requires only black-box access to a pre-trained score model and its derivatives, making it broadly applicable without needing to retrain the model.

## Key Results
- The framework achieves O(η) convergence rate for approximating perturbed samples on synthetic data
- Sample sensitivity scores correlate with actual changes after retraining (median correlation 0.46 on CelebA) and fine-tuning (median correlation 0.66 on CelebA)
- The method substantially outperforms optimal transport baselines on image datasets
- The approach requires only black-box access to pre-trained models, making it broadly applicable

## Why This Works (Mechanism)
The framework works by leveraging the mathematical structure of diffusion models as maps from training distributions to score functions. The directional derivative of this map can be computed in closed form, allowing for efficient sensitivity analysis without retraining. This is possible because the optimal score function for a perturbed distribution can be approximated using first-order Taylor expansion, with the derivative computed through the chain rule applied to the diffusion model's training objective.

## Foundational Learning

**Diffusion Models**
- Why needed: Understanding the core framework that maps training distributions to score functions
- Quick check: Can explain how score-based models learn the gradient of log probability

**Directional Derivatives**
- Why needed: Key mathematical tool for computing how outputs change with small input perturbations
- Quick check: Can compute directional derivatives for simple functions

**Score Matching**
- Why needed: The training objective that connects data distribution to learned score function
- Quick check: Understand the relationship between score matching and maximum likelihood

## Architecture Onboarding

Component Map: Training Distribution -> Score Model -> Directional Derivative -> Perturbed Samples

Critical Path: The computation of the directional derivative requires access to the score model's gradients with respect to its inputs and parameters, making the quality of these gradients crucial for accurate sensitivity analysis.

Design Tradeoffs: The framework trades off accuracy for computational efficiency by using first-order approximations rather than retraining, but this introduces potential errors from numerical density estimation and neural network approximations.

Failure Signatures: Poor performance may manifest as large discrepancies between predicted and actual changes after retraining, particularly for non-Gaussian distributions or large perturbations.

First Experiments:
1. Test on simple Gaussian mixtures with known analytical solutions
2. Compare predicted vs actual changes on MNIST with small perturbations
3. Evaluate correlation between sensitivity scores and retraining outcomes on CelebA

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes training data distribution can be well-approximated by Gaussian mixtures
- Performance on complex real-world datasets beyond standard image benchmarks is uncertain
- Numerical errors from density estimation and neural approximations may affect accuracy

## Confidence

**High**: The theoretical derivation of the directional derivative for the map from training distribution to score function is mathematically rigorous and the closed-form expression is correctly derived

**Medium**: The empirical validation showing correlation between predicted and actual changes after retraining/fine-tuning is promising but based on limited datasets and perturbation types

**Medium**: The O(η) convergence rate claim is supported by synthetic experiments but may be affected by numerical errors from density estimation and neural network approximations in practice

## Next Checks

1. Test the sensitivity framework on more complex, non-Gaussian distributions and real-world datasets with significant mode mixing to evaluate robustness beyond synthetic mixtures

2. Systematically quantify the impact of numerical errors from density estimation and neural score approximation on the accuracy of sensitivity predictions across different perturbation magnitudes

3. Compare against additional baseline methods for dataset sensitivity analysis, including gradient-based attribution techniques and leave-one-out retraining approaches, on a broader range of diffusion model architectures