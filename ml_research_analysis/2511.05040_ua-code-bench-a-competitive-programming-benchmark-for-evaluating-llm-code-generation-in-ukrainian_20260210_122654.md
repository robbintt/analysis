---
ver: rpa2
title: 'UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code
  Generation in Ukrainian'
arxiv_id: '2511.05040'
source_url: https://arxiv.org/abs/2511.05040
tags:
- code
- language
- generation
- ukrainian
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces UA-Code-Bench, the first Ukrainian-language
  competitive programming benchmark for evaluating large language models' (LLMs) code
  generation abilities. It includes 500 problems from Eolymp, evenly distributed across
  five difficulty levels.
---

# UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian

## Quick Facts
- **arXiv ID**: 2511.05040
- **Source URL**: https://arxiv.org/abs/2511.05040
- **Reference count**: 18
- **Primary result**: Introduces UA-Code-Bench, the first Ukrainian-language competitive programming benchmark, showing even top LLMs solve only ~50% of problems.

## Executive Summary
This study introduces UA-Code-Bench, the first Ukrainian-language competitive programming benchmark for evaluating large language models' (LLMs) code generation abilities. It includes 500 problems from Eolymp, evenly distributed across five difficulty levels. Thirteen leading LLMs, both proprietary and open-source, were evaluated using a one-shot prompt approach with Python solutions. Results show that even top models like OpenAI o3 and GPT-5 solved only about half the problems, with performance sharply declining as task complexity increased. The benchmark provides insights into multilingual code generation and reasoning capabilities, emphasizing the need for more inclusive and effective AI models across languages. The dataset and evaluation tools are publicly available.

## Method Summary
The benchmark consists of 500 competitive programming problems sourced from Eolymp, distributed evenly across five difficulty levels. Thirteen leading LLMs (both proprietary and open-source) were evaluated using a one-shot prompting approach, with Python as the target programming language. Performance was measured by the percentage of problems solved, with results analyzed across difficulty levels to assess model capabilities in Ukrainian-language code generation tasks.

## Key Results
- Top models (OpenAI o3, GPT-5) solved only about 50% of problems on average
- Performance declined sharply with increasing task complexity
- Demonstrates significant gaps in multilingual code generation capabilities

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its use of authentic competitive programming problems in Ukrainian, which requires both language understanding and algorithmic reasoning. The one-shot prompting approach provides a standardized evaluation method, while the difficulty distribution reveals performance degradation patterns as problem complexity increases.

## Foundational Learning
- **Competitive Programming Problems**: Algorithmic challenges requiring specific solutions; needed for authentic evaluation of code generation.
- **One-shot Prompting**: Single example prompting strategy; needed for standardized evaluation across models.
- **Difficulty Calibration**: Assignment of problems to difficulty levels; needed to analyze performance across complexity gradients.

## Architecture Onboarding
- **Component Map**: Problem Dataset -> LLM Evaluation -> Performance Analysis
- **Critical Path**: Problem selection → Prompt generation → Model execution → Solution verification
- **Design Tradeoffs**: Python-only solutions vs. multi-language support; one-shot vs. multi-shot prompting
- **Failure Signatures**: Sharp performance decline at higher difficulty levels; language-specific comprehension issues
- **First Experiments**:
  1. Evaluate same models on English competitive programming problems
  2. Test multi-shot prompting strategies for performance improvement
  3. Analyze error types across difficulty levels

## Open Questions the Paper Calls Out
None

## Limitations
- One-shot prompting may underestimate LLM potential with more context
- Python-only solutions limit generalizability to other programming languages
- Unclear distribution and representativeness of problem types

## Confidence
- **High**: Benchmark novelty and demonstration of performance gaps in multilingual code generation
- **Medium**: Broader claims about multilingual AI model inclusivity (limited to Ukrainian and Python)
- **Medium**: Claims about reasoning capabilities (primarily measures code generation)

## Next Checks
1. Expand benchmark to include multiple programming languages to assess cross-language performance
2. Test multi-shot and interactive prompting strategies to improve performance beyond one-shot baseline
3. Conduct systematic analysis of problem type distribution and difficulty calibration for competitive programming challenges