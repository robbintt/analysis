---
ver: rpa2
title: 'The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed
  Text'
arxiv_id: '2506.05209'
source_url: https://arxiv.org/abs/2506.05209
tags:
- license
- data
- text
- arxiv
- apache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the Common Pile v0.1, an 8TB dataset of openly
  licensed text curated from 30 diverse sources including research papers, code, books,
  government documents, and more. The authors demonstrate the dataset's utility by
  training two 7B parameter models, Comma v0.1-1T and Comma v0.1-2T, on 1 and 2 trillion
  tokens respectively.
---

# The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text

## Quick Facts
- **arXiv ID:** 2506.05209
- **Source URL:** https://arxiv.org/abs/2506.05209
- **Reference count:** 40
- **Primary result:** An 8TB openly licensed text corpus that enables competitive LLM training, outperforming prior openly licensed datasets on 9/11 benchmarks.

## Executive Summary
The Common Pile v0.1 is an 8TB dataset of openly licensed text compiled from 30 diverse sources including research papers, code, government documents, and Wikipedia. The authors demonstrate its utility by training two 7B parameter models (Comma v0.1-1T and v0.1-2T) on 1T and 2T tokens respectively, achieving competitive performance compared to models trained on unlicensed data. The dataset is notable for its rigorous filtering, deduplication, and heuristic data mixing strategies that prioritize high-quality sources while maintaining diversity across domains.

## Method Summary
The Common Pile was constructed by collecting 30 openly licensed text sources, converting them to JSONL format, and applying extensive preprocessing including language filtering, quality classification, OCR error removal, toxicity filtering, PII redaction, source-specific regex cleaning, and global fuzzy deduplication. A heuristic data mixing approach was used where small per-source models evaluated each source's quality, then weights were assigned to maximize high-quality data exposure (up to 6 repetitions) across 1T tokens. The final dataset was tokenized with a custom 64k BPE tokenizer and used to train Llama-7B models in a two-stage process (main phase + cool-down).

## Key Results
- Both Comma models achieve competitive performance compared to Llama 1/2 7B on knowledge-based and coding benchmarks
- Comma v0.1-1T outperforms prior openly licensed datasets (OLC, Common Corpus, KL3M) on all 11 evaluated benchmarks
- The dataset demonstrates that diverse, openly licensed sources can match unlicensed corpora in model performance
- Code-heavy sources (Stack V2, USPTO) contribute significantly to performance on coding tasks

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Diverse, openly licensed sources can produce pretraining data that matches unlicensed corpora in model performance.
**Mechanism:** The Common Pile aggregates 30 heterogeneous sources (code, government text, academic papers, wikis, etc.), compensating for the smaller total volume (8TB) with domain breadth and high per-domain data quality.
**Core assumption:** Performance gains from domain diversity and curation can offset the scale advantage of web-scraped unlicensed data.
**Evidence anchors:**
- [abstract] "The Common Pile comprises content from 30 sources that span diverse domains... Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B."
- [section 3] Detailed source breakdown showing large volumes in code (4775 GB), government (1172 GB), and wikis (528 GB).
- [corpus] Related work "The German Commons" shows similar approach for German text; no direct corpus evidence validating this mechanism for the Common Pile specifically.

### Mechanism 2
**Claim:** Rigorous filtering and deduplication transform raw openly licensed text into effective pretraining data.
**Mechanism:** Multi-stage pipeline (language filtering, quality classification, OCR error removal, toxicity filtering, PII redaction, source-specific regex cleaning, global fuzzy deduplication) removes noise and harmful content while preserving signal.
**Core assumption:** Quality heuristics developed for web corpora transfer to the characteristics of openly licensed text.
**Evidence anchors:**
- [section 4.1] "We independently preprocessed each of the Common Pile's non-code datasets using pipelines implemented with the Dolma data processing toolkit... we perform global document-level fuzzy deduplication across all sources."
- [section 4.3] Controlled experiments show Comma dataset outperforms prior openly licensed datasets (OLC, Common Corpus, KL3M) on all benchmarks.
- [corpus] No corpus papers directly validate this specific filtering pipeline for openly licensed data.

### Mechanism 3
**Claim:** Heuristic data mixing based on per-source quality estimates improves training efficiency and final performance.
**Mechanism:** Train small per-source models (1.7B params, 28B tokens) to estimate source quality, then upweight high-performing sources and downweight low-performing ones, targeting max 6 repetitions over 1T tokens. Cool-down phase focuses on high-quality subset.
**Core assumption:** Small-scale model performance predicts large-scale training value; controlled repetition of high-quality data is preferable to single-pass over lower-quality data.
**Evidence anchors:**
- [section 4.2] "Based on the performance of these per-source models, we heuristically set mixing weights to up- and down-weight high- and low-performance sources respectively while targeting a maximum of six repetitions."
- [section 4.4] Comma v0.1-1T outperforms budget-matched baselines on 6/11 benchmarks, particularly excelling on knowledge-based (MMLU, ARC-C) and coding tasks.
- [corpus] No corpus evidence validates this specific mixing heuristic.

## Foundational Learning

- **Concept: Open Knowledge Definition and license types**
  - Why needed here: The entire paper hinges on distinguishing "openly licensed" from unlicensed or restrictive licenses (e.g., CC BY vs. CC ND). Misunderstanding leads to legal risk.
  - Quick check question: Which Creative Commons license types meet the Open Definition 2.1, and which do not?

- **Concept: Language model data processing pipelines**
  - Why needed here: Section 4.1 details a production-grade pipeline; understanding each filter's purpose is essential for replication or adaptation.
  - Quick check question: Why might OCR likelihood filtering harm a corpus of historical texts with archaic spelling?

- **Concept: Data mixing and curriculum learning**
  - Why needed here: The paper's mixing strategy is central to achieving competitive performance; understanding the tradeoffs enables informed adaptation.
  - Quick check question: What are the risks of repeating a small high-quality source 16 times versus using it once?

## Architecture Onboarding

- **Component map:** Data sourcing -> License validation -> Preprocessing/filtering -> Mixing/rewriting -> Tokenizer training -> Model training -> Evaluation
- **Critical path:** License-compliant sourcing → robust filtering → intelligent mixing → tokenizer → staged training → benchmark evaluation. Breaks at any step degrade final performance.
- **Design tradeoffs:**
  - License strictness vs. data volume (excluding OpenAlex, YouTube Commons reduces size but ensures compliance)
  - Automatic mixing (MixMin) vs. heuristic mixing (authors found heuristics better)
  - Repetition tolerance (up to 16× for small high-quality sources) vs. memorization risk
  - Cool-down phase complexity vs. single-stage training
- **Failure signatures:**
  - Poor performance on commonsense benchmarks (HellaSwag, PIQA) → likely missing everyday language domains
  - Unexpected legal challenges → license laundering in sourced data
  - Training instability or memorization → excessive repetition without deduplication
  - Tokenizer inefficiency → high OOV rate on domain-specific terms
- **First 3 experiments:**
  1. **Small-scale data ablation:** Train 1.7B models on 28B tokens from Comma dataset vs. Pile vs. OSCAR to validate data quality before scaling (replicates Section 4.3).
  2. **Mixing strategy comparison:** Compare heuristic mixing vs. proportional mixing vs. MixMin on a 100B token run to identify optimal approach for your compute budget.
  3. **Domain gap analysis:** Evaluate trained models on a broad benchmark suite; analyze per-domain performance to identify missing sources for targeted collection.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can the Common Pile support training runs beyond 2 trillion tokens without performance degradation if an optimal data mixture and curriculum are utilized?
**Basis in paper:** [Explicit] The authors note that the Comma v0.1-2T result is "likely not a best-case" due to "excessive repetition" and that "better performance could likely be attained through a 2T-specific mixture and curriculum."
**Why unresolved:** The paper's 2T experiment simply repeated the 1T mixture, causing some sources to be seen up to 16 times, which is known to cause diminishing returns.
**What evidence would resolve it:** Training a model to 2T tokens using a distinct data curriculum designed to minimize repetition per source while maintaining high-quality data exposure.

### Open Question 2
**Question:** Does the exclusion of "low-quality" or informal web text (e.g., personal blogs, tutorials) inevitably cap the performance of openly licensed models on commonsense reasoning benchmarks?
**Basis in paper:** [Explicit] The authors observe significantly worse performance on HellaSwag, PIQA, and CommonSenseQA, citing work suggesting these tasks rely on domains like personal blogs and sports that are "poorly represented in the Common Pile."
**Why unresolved:** The paper identifies the performance gap but does not isolate whether the gap is due to the lack of *openly licensed* content in these domains or the absence of that specific content type regardless of license.
**What evidence would resolve it:** A controlled ablation study comparing model performance when specific informal domains are included versus excluded from the training set.

### Open Question 3
**Question:** Is manual curation strictly necessary to prevent "license laundering" in large-scale datasets, or can automated detection be made sufficiently robust?
**Basis in paper:** [Inferred] The authors describe license laundering as "notoriously hard to identify exhaustively in practice" and relied on manual curation for sources like YouTube channels and top web domains to ensure license accuracy.
**Why unresolved:** The paper relies on manual verification and conservative exclusion (e.g., removing OpenAlex) to handle ambiguity, implying automated methods were insufficient for their standards.
**What evidence would resolve it:** The development of an automated tool capable of detecting inconsistencies between a site's terms of service, robots.txt, and declared Creative Commons licenses with high precision.

## Limitations
- **Domain gaps in informal content:** Poor performance on commonsense reasoning benchmarks (HellaSwag, PIQA) due to underrepresentation of everyday language domains like personal blogs
- **License compliance trade-offs:** Exclusion of major sources (OpenAlex, YouTube Commons) to ensure license compliance reduces dataset size and diversity
- **Repetition risks at scale:** The 2T token experiment shows potential for excessive repetition (up to 16×) when using the same mixing strategy, suggesting curriculum learning needs for longer runs

## Confidence
- **Data quality and filtering pipeline:** High - Extensive preprocessing with multiple validation steps and controlled experiments showing performance gains
- **Model training methodology:** High - Standard Llama-7B architecture with well-documented two-stage training and established evaluation benchmarks
- **License compliance claims:** Medium - Manual curation provides confidence but cannot guarantee exhaustive detection of license laundering
- **Generalization to other domains:** Low - Poor performance on commonsense benchmarks suggests significant domain gaps that limit broader applicability

## Next Checks
1. **Validate license compliance:** Manually verify a random sample of 50 documents across 5 major sources to confirm license accuracy and check for potential license laundering
2. **Benchmark domain analysis:** Run Comma v0.1-1T on additional commonsense benchmarks (Winogrande, SocialIQA) to quantify the extent of informal language domain gaps
3. **Mixing strategy ablation:** Train a small-scale model (500M params) using the exact per-source quality estimates to verify whether the heuristic mixing weights translate to meaningful performance differences