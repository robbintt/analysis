---
ver: rpa2
title: Testing Question Answering Software with Context-Driven Question Generation
arxiv_id: '2511.07924'
source_url: https://arxiv.org/abs/2511.07924
tags:
- questions
- question
- test
- generated
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents CQ\xB2A, a context-driven approach for testing\
  \ QA software using LLMs. It addresses limitations in existing metamorphic testing\
  \ methods by extracting entities and relationships from the context as ground truth\
  \ answers, then generating natural questions using LLMs with consistency verification\
  \ and constraint checking to improve reliability."
---

# Testing Question Answering Software with Context-Driven Question Generation

## Quick Facts
- **arXiv ID**: 2511.07924
- **Source URL**: https://arxiv.org/abs/2511.07924
- **Reference count**: 40
- **Key outcome**: CQ²A approach detects 45.17% more true positives than QAQA and 22.84% more than QAAskeR-plus in QA software testing

## Executive Summary
This paper introduces CQ²A, a context-driven approach for testing QA software using LLMs that addresses limitations in existing metamorphic testing methods. The approach extracts entities and relationships from context as ground truth answers, then generates natural questions using LLMs with consistency verification and constraint checking to improve reliability. The method employs a two-stage similarity checking process combining embedding models and LLM-assisted judgment for accurate answer comparison.

Experiments on BoolQ, SQuAD2, and NarrativeQA datasets demonstrate that CQ²A generates more natural questions (average scores 4.59-4.66 vs 2.24-2.60) and covers 24.7-43.7% more context than baseline methods. The test cases generated by CQ²A also improve QA model performance, reducing error rates by an average of 30.2% when used for fine-tuning.

## Method Summary
CQ²A uses a context-driven approach for testing QA software by extracting entities and relationships from the context as ground truth answers, then generating natural questions using LLMs. The method incorporates consistency verification and constraint checking to improve reliability. A two-stage similarity checking process combines embedding models and LLM-assisted judgment for accurate answer comparison. The approach is evaluated on BoolQ, SQuAD2, and NarrativeQA datasets, showing superior performance compared to existing metamorphic testing methods like QAQA and QAAskeR-plus.

## Key Results
- CQ²A detects 45.17% more true positives than QAQA and 22.84% more than QAAskeR-plus
- Generated questions are more natural with average scores of 4.59-4.66 compared to 2.24-2.60 for baselines
- Test cases cover 24.7-43.7% more context than competing methods
- Using CQ²A-generated test cases for fine-tuning reduces QA model error rates by 30.2% on average

## Why This Works (Mechanism)
CQ²A works by grounding question generation in the actual context rather than arbitrary transformations. By extracting entities and relationships as ground truth answers first, the method ensures that generated questions have verifiable answers within the provided context. The two-stage similarity checking mechanism provides robust answer comparison by combining semantic similarity from embeddings with nuanced judgment from LLMs. The consistency verification and constraint checking prevent generation of invalid or misleading questions, while the natural language generation capabilities of LLMs create questions that better match human query patterns.

## Foundational Learning
- **Entity extraction from context**: Essential for creating ground truth answers that are verifiably correct within the given text. Quick check: Verify extracted entities are actually present and relevant in the source context.
- **LLM-based question generation with constraints**: Needed to transform ground truth answers into natural, contextually appropriate questions while maintaining quality and validity. Quick check: Ensure generated questions can be answered by the provided ground truth without requiring external knowledge.
- **Two-stage similarity checking**: Required for robust answer comparison that balances computational efficiency with accuracy. Quick check: Test edge cases where semantic similarity exists but exact match does not, and vice versa.
- **Metamorphic testing for QA systems**: Important for identifying subtle bugs that traditional testing might miss by leveraging input-output relationships. Quick check: Verify that generated test cases maintain the metamorphic relationships when applied to the target QA system.

## Architecture Onboarding

**Component Map**: Context -> Entity Extraction -> Ground Truth Creation -> LLM Question Generation -> Constraint Checking -> Similarity Verification -> Test Case Output

**Critical Path**: The entity extraction and ground truth creation stage is critical as it establishes the verifiable answers that anchor the entire testing process. Errors here propagate through all subsequent stages.

**Design Tradeoffs**: The approach trades computational overhead from LLM usage and two-stage verification against improved test case quality and reliability. Using context-derived ground truths increases confidence but may miss questions requiring reasoning beyond explicit context.

**Failure Signatures**: Common failures include LLM generating questions that cannot be answered by extracted entities, similarity checking producing false positives/negatives in edge cases, and context extraction missing key entities that would enable better test coverage.

**First 3 Experiments**: 1) Compare true positive detection rates against QAQA and QAAskeR-plus baselines. 2) Evaluate naturalness of generated questions using human judgment. 3) Measure context coverage percentage relative to competing methods.

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on LLM-generated content introduces potential circularity risks where the same models being tested influence test case quality
- Evaluation focuses primarily on two metamorphic testing methods without benchmarking against more recent QA testing approaches
- Similarity checking mechanism lacks formal validation of accuracy rates and may introduce false positives/negatives in edge cases
- Study doesn't address potential biases introduced by the entity extraction step or examine generalization across specialized knowledge domains

## Confidence
- **High**: Comparative results showing CQ²A outperforms QAQA and QAAskeR-plus in true positive detection rates
- **Medium**: Claim about more natural question generation based on average scores
- **Medium**: Reported improvement in QA model performance when using generated test cases for fine-tuning

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component (entity extraction, LLM generation, similarity checking) to overall performance
2. Implement cross-validation with multiple LLM providers to assess dependency on specific model architectures
3. Test the approach on domain-specific QA datasets (e.g., medical, legal) to evaluate generalization across specialized knowledge domains