---
ver: rpa2
title: 'One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational
  Moral Reasoning'
arxiv_id: '2509.21443'
source_url: https://arxiv.org/abs/2509.21443
tags:
- moral
- reasoning
- languages
- across
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically examines how language and culture influence
  moral reasoning in LLMs by translating two moral reasoning benchmarks into five
  diverse languages and conducting zero-shot evaluations. The analysis reveals significant
  cross-linguistic inconsistencies in moral judgments, with English consistently outperforming
  other languages.
---

# One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning

## Quick Facts
- arXiv ID: 2509.21443
- Source URL: https://arxiv.org/abs/2509.21443
- Authors: Sualeha Farid; Jayden Lin; Zean Chen; Shivani Kumar; David Jurgens
- Reference count: 40
- One-line primary result: LLMs show significant cross-linguistic inconsistencies in moral judgments, with English consistently outperforming other languages, revealing cultural misalignments in moral reasoning.

## Executive Summary
This study systematically examines how language and culture influence moral reasoning in large language models by translating two moral reasoning benchmarks into five diverse languages and conducting zero-shot evaluations. The analysis reveals significant cross-linguistic inconsistencies in moral judgments, with English consistently outperforming other languages. The research uncovers that models' reasoning patterns vary across cultures, with South Asian languages emphasizing duty in early stages while Western languages focus on outcomes later. A key finding shows that pretraining data sources, particularly psychology and education domains, significantly influence models' moral reasoning patterns.

The study introduces a structured typology of moral reasoning errors (FAULT) and demonstrates that models primarily abstract rather than replicate training content when making moral judgments. These findings highlight the need for culturally balanced training data and evaluation protocols to ensure equitable AI deployment in global contexts. The research provides empirical evidence that current LLMs exhibit cultural biases in moral reasoning that could have significant implications for their deployment in diverse linguistic communities.

## Method Summary
The study conducted a comprehensive cross-linguistic evaluation of moral reasoning in LLMs by translating two established benchmarks (Social-Chem-101 and ETHICS) into five languages: English, Chinese, Hindi, Arabic, and Spanish. The researchers performed zero-shot evaluations across 15 different LLMs spanning various model families, collecting responses for 2,814 scenarios across the five languages. They developed a novel FAULT typology (Framework misfits, Asymmetric judgments, Uneven reasoning, Loss in low-resource languages, Tilted values) to categorize cross-linguistic moral reasoning errors. The analysis examined correlations between pretraining data sources and moral reasoning patterns, with special attention to the OLMo-2-32B model due to its transparent training data. A case study on 75 English scenarios provided deeper insight into whether models abstract or replicate training content during moral reasoning.

## Key Results
- English consistently outperformed other languages in moral reasoning tasks, with Chinese, Hindi, Arabic, and Spanish showing statistically significant performance gaps
- Models exhibited different reasoning patterns across cultures, with South Asian languages emphasizing duty in early reasoning stages while Western languages focused on outcomes later
- Pretraining data sources, particularly psychology and education domains, showed significant correlation with models' moral reasoning patterns
- The FAULT typology effectively categorized observed cross-linguistic moral reasoning errors across multiple dimensions
- Models primarily abstracted rather than replicated training content when making moral judgments, as demonstrated in the OLMo-2-32B case study

## Why This Works (Mechanism)
The cross-linguistic evaluation reveals systematic patterns in how models process moral reasoning across different languages and cultures. The mechanism appears to stem from the interplay between training data composition and language-specific reasoning patterns. When models are trained primarily on English-language data from specific domains like psychology and education, they develop reasoning patterns that align with those cultural contexts. During zero-shot evaluation on translated benchmarks, these pre-existing patterns manifest differently across languages, creating performance disparities and reasoning inconsistencies.

The FAULT framework captures how these mechanisms break down across linguistic boundaries - from framework mismatches where models struggle to map reasoning patterns to culturally appropriate moral frameworks, to asymmetric judgments where language-specific nuances lead to inconsistent moral conclusions. The abstraction-over-replication pattern suggests models are developing internal representations of moral principles rather than memorizing training examples, but these representations are culturally biased toward the dominant language and domain of their training data.

## Foundational Learning
- **Cross-linguistic evaluation methodology**: Needed to systematically compare model performance across languages; Quick check: consistent translation quality and scenario equivalence across languages
- **Moral reasoning benchmarks**: ETHICS and Social-Chem-101 provide standardized scenarios; Quick check: benchmark scenarios cover diverse moral situations and are culturally neutral
- **FAULT typology framework**: Categorizes cross-linguistic moral reasoning errors systematically; Quick check: framework captures distinct error types across multiple dimensions
- **Zero-shot evaluation protocols**: Tests model capabilities without task-specific fine-tuning; Quick check: consistent prompt formatting and evaluation criteria across languages
- **Pretraining data analysis**: Links training corpus composition to reasoning patterns; Quick check: comprehensive coverage of training domains and sources
- **Statistical significance testing**: Validates observed performance differences; Quick check: appropriate statistical methods for cross-linguistic comparisons

## Architecture Onboarding

**Component Map:**
Translation Pipeline -> Benchmark Application -> Model Evaluation -> Error Classification -> Data Analysis

**Critical Path:**
Benchmark Translation -> Zero-shot Evaluation -> FAULT Categorization -> Performance Analysis -> Pretraining Data Correlation

**Design Tradeoffs:**
- Zero-shot evaluation preserves general capability assessment but may underestimate true performance potential
- Limited language selection balances diversity with practical evaluation constraints
- FAULT framework provides structure but may miss nuanced cultural differences
- Correlation analysis reveals patterns but cannot establish causation without controlled experiments

**Failure Signatures:**
- Performance gaps between languages exceeding statistical significance thresholds
- Systematic errors in specific FAULT categories across multiple models
- Strong correlations between specific training domains and reasoning patterns
- Inconsistent reasoning patterns across similar scenarios in different languages

**First Experiments:**
1. Evaluate additional 10+ languages to test generalizability of cross-linguistic patterns
2. Conduct controlled pretraining data experiments varying domain composition
3. Implement fine-tuning with culturally balanced data to test error reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can human-generated ground truth for moral reasoning benchmarks be established across diverse languages to properly evaluate cross-linguistic moral competence?
- Basis in paper: [explicit] "Without human-generated ground truth in these languages, it becomes hard to evaluate the 'correctness' of these systems, raising concerns about the equitable deployment of LLMs in global, linguistically diverse applications."
- Why unresolved: The study used translated English benchmarks without collecting native human judgments, making it impossible to determine whether model responses align with authentic cultural moral values versus translation artifacts.
- What evidence would resolve it: Native-speaker moral judgment collections across cultures for parallel scenarios, enabling comparison between model outputs and authentic community values.

### Open Question 2
- Question: Does the abstraction-over-replication pattern in moral reasoning generalize across diverse model architectures and training datasets beyond OLMo-2-32B?
- Basis in paper: [inferred] The RQ4 case study was limited to a single model (OLMo-2-32B) and only 75 English scenarios due to computational overhead and OLMo's unique training data transparency.
- Why unresolved: Other proprietary models lack traceable training data access, so whether models uniformly abstract moral principles or sometimes directly replicate training content remains unknown.
- What evidence would resolve it: Analysis of additional open models with accessible training data, or development of indirect methods to infer training data influence without full transparency.

### Open Question 3
- Question: What specific data curation and augmentation methods effectively reduce cross-linguistic FAULT errors while preserving cultural authenticity?
- Basis in paper: [explicit] "Overcoming these errors requires culturally balanced moral reasoning corpora and value-aligned data augmentation during pretraining and fine-tuning...embedding culturally grounded ethical theories into training objectives so that models do not default to a single dominant paradigm."
- Why unresolved: The paper identifies the need and proposes directions but does not implement or test specific intervention strategies.
- What evidence would resolve it: Intervention studies demonstrating that targeted curation or augmentation reduces the five FAULT error categories (Framework misfits, Asymmetric judgments, Uneven reasoning, Loss in low-resource languages, Tilted values) while maintaining cultural validity.

## Limitations
- Evaluation limited to five languages, representing a small sample of global linguistic diversity
- Zero-shot evaluation prevents assessment of performance improvements with fine-tuning or in-context learning
- Correlation analysis cannot establish causal relationships due to training data opacity
- FAULT typology represents a novel classification system requiring external validation

## Confidence

**High Confidence Claims:**
- Cross-linguistic performance disparities exist and are statistically significant
- Models show different reasoning patterns across cultural contexts
- Pretraining data domain influences correlate with moral reasoning patterns

**Medium Confidence Claims:**
- Cultural differences in moral reasoning emphasis (duty vs. outcomes)
- FAULT typology effectively categorizes moral reasoning errors
- Models abstract rather than replicate training content

**Low Confidence Claims:**
- Specific causal mechanisms behind cross-linguistic gaps
- Generalizability to other languages and moral reasoning tasks
- Real-world applicability of findings

## Next Checks

1. Cross-linguistic Expansion: Replicate the evaluation across 20+ additional languages spanning different language families and cultural regions to determine whether observed patterns generalize beyond the initial five languages tested.

2. Causal Analysis of Training Data: Conduct controlled experiments varying pretraining data composition across different moral reasoning domains to establish causal relationships between training data sources and model reasoning patterns.

3. Real-world Deployment Testing: Evaluate model performance on authentic multilingual moral reasoning tasks in real-world applications (e.g., content moderation, ethical decision support) to assess practical implications beyond benchmark performance.