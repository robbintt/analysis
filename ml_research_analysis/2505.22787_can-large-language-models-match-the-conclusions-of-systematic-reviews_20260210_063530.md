---
ver: rpa2
title: Can Large Language Models Match the Conclusions of Systematic Reviews?
arxiv_id: '2505.22787'
source_url: https://arxiv.org/abs/2505.22787
tags:
- llama
- higher
- lower
- effect
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are increasingly explored for automating
  systematic reviews (SRs) in medicine, yet their ability to match expert conclusions
  when given the same studies remains unclear. To address this, researchers created
  MedEvidence, a benchmark of 284 questions derived from 100 open-access SRs across
  10 medical specialties.
---

# Can Large Language Models Match the Conclusions of Systematic Reviews?

## Quick Facts
- **arXiv ID**: 2505.22787
- **Source URL**: https://arxiv.org/abs/2505.22787
- **Reference count**: 40
- **Primary result**: Even top LLMs achieve only 60-62% accuracy on matching systematic review conclusions, far below expert performance.

## Executive Summary
Large language models are increasingly explored for automating systematic reviews in medicine, but their ability to match expert conclusions remains unclear. Researchers created MedEvidence, a benchmark of 284 questions derived from 100 open-access systematic reviews across 10 medical specialties. When 24 LLMs were evaluated on this task, even top models like DeepSeek V3 and GPT-4.1 achieved only 60-62% accuracy, struggling particularly with uncertain evidence and failing to apply scientific skepticism toward low-quality studies. Performance declines with longer inputs and medical fine-tuning often degrades results.

## Method Summary
The study created MedEvidence, a benchmark with 284 QA pairs from 100 Cochrane systematic reviews across 10 medical specialties. Each question asks whether a treatment outcome is higher, lower, no difference, uncertain effect, or insufficient data based on provided source studies. Twenty-four LLMs ranging from 7B to 700B parameters were evaluated using zero-shot prompting with two variants: basic prompt and expert-guided prompt. Primary metric was accuracy (exact string match), with 95% CIs via bootstrap. Models processed either abstracts only or full-text sources when available, with chunking and refinement for context limits.

## Key Results
- Top models (DeepSeek V3, GPT-4.1) achieve only 60-62% accuracy, far from expert level
- Performance declines significantly as input token count increases
- Medical fine-tuned models often perform worse than base models
- All models struggle with uncertain evidence and fail to apply scientific skepticism to low-quality findings

## Why This Works (Mechanism)

### Mechanism 1: Evidence Quality Blindness
Models cannot reliably assess evidence quality or apply scientific skepticism to low-quality findings. They process text statistically rather than evaluating methodological rigor, accepting numerical results at face value without weighting by evidence certainty. This reflects a fundamental limitation in how LLMs reason about epistemic uncertainty.

### Mechanism 2: Context Length Performance Degradation
Model accuracy decreases as total token count of input sources increases. Long-context attention mechanisms dilute signal from key evidence, and models may rely on heuristics rather than thorough synthesis when context exceeds training distribution.

### Mechanism 3: Uncertainty Aversion
Models systematically avoid predicting "uncertain effect," preferring definitive answers even when inappropriate. RLHF training may penalize hedging, creating bias toward confident outputs, and models lack explicit epistemic uncertainty calibration for scientific inference.

## Foundational Learning

- **Concept: GRADE Evidence Quality Framework**
  - Why needed here: Systematic reviews use GRADE to classify evidence certainty. Understanding this is essential to interpret why models fail at skepticism.
  - Quick check question: Can you explain why a statistically significant result from a small trial with wide confidence intervals might receive "low certainty" evidence grading?

- **Concept: Meta-Analysis Weighting**
  - Why needed here: SR conclusions weight studies by sample size and quality. Models appear to treat all text equally rather than applying principled aggregation.
  - Quick check question: If Study A (n=50) finds an effect and Study B (n=5000) finds no effect, how should a meta-analysis weight them?

- **Concept: Intention-to-Treat vs. Per-Protocol Analysis**
  - Why needed here: SRs prioritize ITT analysis; models may incorrectly weight sub-analyses.
  - Quick check question: Why do systematic reviewers prefer intention-to-treat over per-protocol analyses?

## Architecture Onboarding

- **Component map**: MedEvidence benchmark (284 QA pairs from 100 Cochrane SRs) -> Source data (abstracts + full-text from 329 articles) -> Evaluation (zero-shot prompting, exact match accuracy) -> Metadata (evidence certainty, source concordance, specialty labels)

- **Critical path**: Retrieve source studies (PMID → abstract/full-text) → Format prompt with study content + question → Extract model answer (rationale + classification) → Compare to ground truth (expert SR conclusion)

- **Design tradeoffs**: Closed-form QA enables scalable evaluation but removes nuance; zero-shot testing reveals "natural" behavior but may underestimate guided performance; using abstracts only vs. full-text: simpler but loses detail

- **Failure signatures**: Predicting definitive answer when correct label is "uncertain effect" or "insufficient data"; ignoring evidence certainty metadata; performance drop >15% when token count exceeds 10K; medical fine-tuned models underperforming base models

- **First 3 experiments**:
  1. Baseline: Run DeepSeek V3 or GPT-4.1 on a 20-question subset across all 5 answer classes
  2. Ablation: Test same questions with abstracts-only vs. full-text to isolate context-length effects
  3. Prompt sensitivity: Compare basic prompt vs. expert-guided prompt on uncertainty-class questions

## Open Questions the Paper Calls Out

1. **How can LLMs be engineered to apply scientific skepticism to low-quality evidence, given that current prompting strategies fail to induce this behavior?**
   - Basis: All models show lack of scientific skepticism even when prompted to consider study design
   - Why unresolved: Expert-guided prompts failed, suggesting architectural or training objective limitations
   - What evidence would resolve it: Training interventions showing improved performance on low-certainty subsets

2. **Why does medical knowledge fine-tuning degrade performance on evidence synthesis tasks compared to generalist base models?**
   - Basis: Medical fine-tuning consistently degrades accuracy, contradicting intuition that domain specialization helps
   - Why unresolved: Paper doesn't isolate whether degradation stems from catastrophic forgetting, overfitting, or reduced reasoning flexibility
   - What evidence would resolve it: Ablation study analyzing internal attention mechanisms of medically fine-tuned vs. base models

3. **Can specific calibration techniques resolve the "verbal overconfidence" of LLMs in systematic reviews, particularly for ambiguous outcomes?**
   - Basis: Models are reluctant to express uncertainty and perform worst on uncertain-effect questions
   - Why unresolved: Paper identifies overconfidence but doesn't test uncertainty quantification methods
   - What evidence would resolve it: Experiments showing calibrated models increase recall for uncertain classes without precision loss

## Limitations
- Evaluation uses zero-shot prompting without optimization, potentially underestimating LLM capabilities
- Closed QA format removes nuanced reasoning that characterizes expert systematic reviews
- Only 114 of 329 source articles include full-text, meaning most questions rely on abstracts alone

## Confidence
**High Confidence**: Core finding that LLMs struggle with uncertain evidence and show performance degradation with increasing context length is well-supported
**Medium Confidence**: Conclusion that medical fine-tuning consistently harms performance requires more systematic testing
**Medium Confidence**: Claim that LLMs cannot apply scientific skepticism to low-quality findings is supported but could potentially be addressed through targeted prompting

## Next Checks
1. Systematically test whether expert-guided prompts incorporating GRADE evidence hierarchies can improve performance on uncertain-effect questions by >15 percentage points
2. Conduct controlled experiments comparing model accuracy on questions where both abstract-only and full-text sources are available
3. Implement and test chunked processing with intermediate summarization to determine if context-length performance degradation stems from attention limitations or missing information