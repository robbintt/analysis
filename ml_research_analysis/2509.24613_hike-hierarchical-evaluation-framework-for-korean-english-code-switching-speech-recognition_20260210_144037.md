---
ver: rpa2
title: 'HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching
  Speech Recognition'
arxiv_id: '2509.24613'
source_url: https://arxiv.org/abs/2509.24613
tags:
- data
- speech
- cs-asr
- code-switching
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HiKE, the first publicly available non-synthetic
  Korean-English code-switching (CS) speech recognition benchmark, addressing the
  scarcity of CS datasets for typologically distant language pairs. HiKE contains
  1,121 high-quality utterances across eight topics, with hierarchical CS-level annotations
  (word, phrase, sentence) and loanword labels for precise evaluation.
---

# HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition

## Quick Facts
- arXiv ID: 2509.24613
- Source URL: https://arxiv.org/abs/2509.24613
- Authors: Gio Paik; Yongbeom Kim; Soungmin Lee; Sangmin Ahn; Chanwoo Kim
- Reference count: 21
- Primary result: First publicly available Korean-English code-switching speech recognition benchmark, showing significant performance drops in CS scenarios.

## Executive Summary
This paper introduces HiKE, the first publicly available non-synthetic Korean-English code-switching (CS) speech recognition benchmark, addressing the scarcity of CS datasets for typologically distant language pairs. HiKE contains 1,121 high-quality utterances across eight topics, with hierarchical CS-level annotations (word, phrase, sentence) and loanword labels for precise evaluation. The authors evaluate ten multilingual ASR models, showing significant performance drops in CS scenarios—up to 13x higher error rates compared to monolingual tasks—with word- and phrase-level CS proving especially challenging. Fine-tuning experiments demonstrate that synthetic sentence-level CS data (generated by concatenating monolingual utterances) can effectively improve CS-ASR performance, though natural intra-sentential CS data yields greater gains. These results highlight both the difficulty of CS-ASR and the potential of synthetic data as a scalable training resource.

## Method Summary
The HiKE benchmark consists of 1,121 human-verified Korean-English CS utterances across 8 topics, with hierarchical CS-level annotations (word, phrase, sentence) and loanword labels. The evaluation uses Mixed Error Rate (MER; character-level for Korean, word-level for English) and Point of Interest Error Rate (PIER; evaluates at language transition points). For fine-tuning experiments, the authors use Whisper-Medium with synthetic sentence-level CS data created by concatenating monolingual utterances from FLEURS and Common Voice, as well as natural intra-sentential CS data from AI-Hub. The training procedure involves fine-tuning for approximately 1 epoch with batch size 16, cosine annealing scheduler with 10% warmup, and peak learning rate of 1e-5 on a single A100 GPU.

## Key Results
- Ten multilingual ASR models show up to 13x higher error rates in CS scenarios compared to monolingual tasks
- Word-level and phrase-level CS are significantly more challenging than sentence-level CS for all models tested
- Fine-tuning on synthetic sentence-level CS data reduces overall MER from 31.3% to 22.1% and PIER from 41.3% to 34.5%
- Natural intra-sentential CS data yields greater gains than synthetic data, reducing MER to 9.0% and PIER to 19.5%

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical CS-level labeling (word/phrase/sentence) enables targeted diagnosis of where multilingual ASR models fail. By separating CS types by linguistic complexity—word-level (single lexical substitution), phrase-level (multi-word units with potential word-order conflicts), and sentence-level (clean boundary switches)—evaluators can isolate whether a model fails on vocabulary access, grammatical integration, or basic language detection. Typologically distant pairs like Korean-English amplify phrase-level difficulty due to divergent word orders (SOV vs SVO).

### Mechanism 2
Fine-tuning on synthetic sentence-level CS data (monolingual utterance concatenation) improves CS-ASR, though natural intra-sentential CS data yields larger gains for word/phrase-level CS. Concatenating Korean and English monolingual utterances exposes the model to language switches at utterance boundaries, teaching basic language-transition detection. However, this does not model the dense, intra-sentential switch points and code-mixed grammatical structures present in natural word/phrase-level CS, limiting transfer.

### Mechanism 3
Model scale alone is insufficient for practical CS-ASR; even the largest models show >5x error-rate increases over monolingual baselines. Pretraining corpora are dominated by monolingual speech; CS examples are extremely rare. Scaling increases capacity but does not compensate for distributional mismatch. The model does not learn reliable language-switch triggers or code-mixed grammatical patterns without targeted exposure.

## Foundational Learning

- **Concept: Mixed Error Rate (MER) vs Point of Interest Error Rate (PIER)**
  - **Why needed here:** MER evaluates overall transcription quality across mixed-script text; PIER isolates errors at language-transition points. Both are required to distinguish "good transcription but wrong language at switch" from "globally poor transcription."
  - **Quick check question:** If a model transcribes all English words in Korean script (phonetic transcription), will MER or PIER capture this failure more sharply?

- **Concept: Code-Switching Typology (Inter-sentential vs Intra-sentential; Word/Phrase/Sentence levels)**
  - **Why needed here:** Different CS types impose different cognitive and computational loads. Sentence-level CS requires only boundary detection; phrase-level may require on-the-fly syntactic reanalysis.
  - **Quick check question:** Why would phrase-level CS between Korean and English be harder than phrase-level CS between Spanish and Catalan?

- **Concept: Loanword Ambiguity in Evaluation**
  - **Why needed here:** Loanwords like '버스'/'bus' have near-identical pronunciations and interchangeable usage. Without special handling, a correct transcription in the "wrong" script is unfairly penalized.
  - **Quick check question:** How does the paper's loanword labeling change the interpretation of model performance?

## Architecture Onboarding

- **Component map:** HiKE Benchmark Dataset -> Evaluation Metrics (MER/PIER) -> Baseline Models (10 multilingual ASR systems) -> Fine-tuning Pipeline (Natural vs Synthetic CS data)
- **Critical path:** Load HiKE benchmark and preprocess with loanword handling -> Run inference with target ASR model on HiKE utterances -> Compute MER and PIER stratified by CS-level -> (Optional) Fine-tune on synthetic or natural CS data; re-evaluate
- **Design tradeoffs:**
  - Synthetic vs Natural CS Data: Synthetic (concatenated) is scalable but limited to sentence-level switches and may have acoustic discontinuities; natural captures intra-sentential patterns but is scarce and domain-limited
  - Model Size vs Efficiency: Larger models (Whisper-Large, LLM-based) perform better but may still fail catastrophically on CS without fine-tuning; smaller models (SenseVoice, Whisper-Small) are faster but weaker
  - Metric Choice: MER gives overall quality; PIER isolates switch-point handling. Use both—relying on either alone misses failure modes
- **Failure signatures:**
  - Phonetic Transcription: Model transcribes English words using Hangul (or vice versa) rather than switching scripts; common across all models
  - Instruction Following Failure: Multi-task models (e.g., Audio-Flamingo-3) misinterpret transcription requests in CS contexts, performing translation or QA instead
  - Hallucination: Repetitive or excessive output not present in the audio; increases sharply in CS scenarios vs monolingual
- **First 3 experiments:**
  1. **Baseline Profiling:** Run Whisper-Medium on HiKE; report MER/PIER stratified by CS-level to establish failure distribution (expect word-level worst, sentence-level best)
  2. **Synthetic Data Ablation:** Fine-tune Whisper-Medium on synthetic sentence-level CS only; measure MER/PIER delta vs baseline. Confirm ~6-9 point MER reduction as reported
  3. **Natural vs Synthetic Comparison:** Fine-tune separate Whisper-Medium instances on (a) natural intra-sentential CS and (b) synthetic inter-sentential CS; compare word-level and phrase-level PIER to quantify the natural-data advantage

## Open Questions the Paper Calls Out

- **Open Question 1:** Can scaling up synthetic inter-sentential data close the performance gap with natural intra-sentential fine-tuning for word- and phrase-level code-switching?
  - **Basis in paper:** The authors state that data scarcity "precluded a thorough investigation into methods for eliciting robust CS capabilities, such as by scaling up synthetic data."
  - **Why unresolved:** The fine-tuning experiments were restricted to a small scale due to the lack of high-quality code-switching data, leaving the impact of data volume unknown.
  - **What evidence would resolve it:** Experiments comparing performance curves of models fine-tuned on varying magnitudes of synthetic versus natural data.

- **Open Question 2:** Do sophisticated data synthesis techniques enable robust CS-ASR performance using exclusively synthetic training data?
  - **Basis in paper:** The authors speculate that "with the development of more sophisticated data synthesis... it may become possible to train robust CS-ASR models using only synthetic data."
  - **Why unresolved:** Current experiments used simple concatenation, which lacks the acoustic variability and natural transition patterns of intra-sentential CS found in natural data.
  - **What evidence would resolve it:** A study evaluating models trained solely on advanced synthetic CS data (e.g., utilizing generative audio models) on the HiKE benchmark.

- **Open Question 3:** Is the superior word-level performance observed in GPT-4o a consistent trait of LLM-based ASR architectures?
  - **Basis in paper:** While GPT-4o showed superior word-level performance (unlike non-LLMs), the authors noted the scarcity of other capable LLMs prevented a "reliable analysis of their common characteristics."
  - **Why unresolved:** Only one LLM-based model yielded meaningful results, making it impossible to distinguish architectural trends from model-specific training artifacts.
  - **What evidence would resolve it:** Evaluating a wider range of LLM-based ASR models on the hierarchical CS levels to see if the inverse correlation between CS-level difficulty and performance holds.

## Limitations

- **Natural CS data access restriction:** The primary intra-sentential CS dataset (AI-Hub) is restricted to Korean nationals, making global replication of the full fine-tuning experiment difficult.
- **Concatenation procedure ambiguity:** Exact details of synthetic CS data creation (silence insertion, transcript pairing logic) are not specified, potentially affecting reproducibility of the synthetic-data results.
- **Loanword handling dependency:** Evaluation results depend on a loanword list that is not publicly provided, though the paper reports a measurable impact on MER/PIER scores.

## Confidence

- **High Confidence:** The claim that model scaling alone does not solve CS-ASR is strongly supported by the 5-13x error rate increases observed across models, consistent with related literature.
- **Medium Confidence:** The claim that synthetic sentence-level CS data improves performance is supported, but exact gains depend on the unknown concatenation protocol and the restricted access to natural CS data for comparison.
- **Medium Confidence:** The diagnostic value of the three-level CS hierarchy is theoretically sound and the reported results show it works for Korean-English, but cross-dataset validation is not yet available.

## Next Checks

1. **Loanword Handling Impact:** Re-run HiKE evaluation with and without the loanword tolerance to measure the actual change in MER/PIER, validating the reported 4.9% and 7.9% improvements.
2. **Synthetic Data Ablation:** Replicate the fine-tuning on synthetic sentence-level CS using the published HiKE code, but with two different concatenation strategies (with and without inserted silence) to isolate the effect of acoustic discontinuities.
3. **Cross-LingPair Hierarchy Validation:** Apply HiKE's three-level CS labeling scheme to an existing Mandarin-English CS dataset (e.g., CS-Dialogue) to test whether phrase-level difficulty is consistently greater than word-level across typologically distinct pairs.