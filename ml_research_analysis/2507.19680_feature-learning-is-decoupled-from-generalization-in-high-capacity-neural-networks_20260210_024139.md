---
ver: rpa2
title: Feature learning is decoupled from generalization in high capacity neural networks
arxiv_id: '2507.19680'
source_url: https://arxiv.org/abs/2507.19680
tags:
- arxiv
- learning
- feature
- neural
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that current theories of feature learning in
  neural networks are inadequate for explaining generalization. While existing metrics
  measure how much neural representations change during training (feature learning
  strength), they fail to capture the quality of these learned features and their
  impact on generalization.
---

# Feature learning is decoupled from generalization in high capacity neural networks

## Quick Facts
- arXiv ID: 2507.19680
- Source URL: https://arxiv.org/abs/2507.19680
- Authors: Niclas Alexander GÃ¶ring; Charles London; Abdurrahman Hadi Erturk; Chris Mingard; Yoonsoo Nam; Ard A. Louis
- Reference count: 40
- Key outcome: Current feature learning theories fail to capture generalization quality in high-capacity networks

## Executive Summary
This paper challenges the prevailing understanding of feature learning in neural networks by demonstrating that existing metrics measuring representation changes during training do not reliably predict generalization performance. The authors show that neural tangent kernel (NTK) and conjugate kernel (CK) changes, as well as feature dimensionality, cannot distinguish between networks trained on true versus shuffled labels, despite significant differences in generalization. Through experiments with CNNs on CIFAR-10 and MLPs on staircase functions, they introduce the concept of "feature quality" measured by the FL gap, which quantifies the performance difference between a network and its NTK counterpart.

## Method Summary
The authors conducted controlled experiments comparing neural networks trained on true labels versus shuffled labels across different architectures and tasks. They measured feature learning strength using NTK/CK distance changes and feature dimensionality, then introduced the FL gap metric to quantify feature quality. The experiments included CNNs on CIFAR-10 and MLPs on synthetic staircase functions, systematically varying training duration, model capacity, and dataset characteristics. The methodology focused on isolating the relationship between representation changes during training and actual generalization performance.

## Key Results
- NTK/CK changes and feature dimensionality do not distinguish between networks generalizing well versus those failing to generalize
- The FL gap metric successfully identifies the quality difference between learned features and NTK predictions
- Networks trained on true labels consistently show larger FL gaps than those trained on shuffled labels, despite similar feature learning strength metrics

## Why This Works (Mechanism)
Assumption: The mechanism likely relates to the fact that feature learning strength metrics (NTK/CK changes, dimensionality) measure only the magnitude of representation changes during training, not the quality or relevance of those changes to the underlying data distribution. The FL gap metric may capture whether learned features align with task-relevant information by comparing against the NTK baseline.

## Foundational Learning
- Neural Tangent Kernel (NTK): Why needed - provides a theoretical framework for understanding neural network training dynamics; Quick check - verify NTK convergence during training
- Conjugate Kernel (CK): Why needed - related kernel that captures feature space geometry; Quick check - compare CK changes with NTK changes
- Feature Learning Metrics: Why needed - quantify how much representations change during training; Quick check - test metric sensitivity to label noise

## Architecture Onboarding
- Component map: Input -> Feature Extraction -> NTK Computation -> Generalization Evaluation
- Critical path: Training loop -> Feature representation evolution -> Metric computation -> Generalization assessment
- Design tradeoffs: Balance between representation complexity and generalization vs computational tractability of NTK/CK metrics
- Failure signatures: High feature learning strength but poor generalization indicates metric inadequacy
- First experiments: 1) Train CNN on CIFAR-10 with true vs shuffled labels, 2) Measure NTK/CK changes, 3) Compute FL gap and compare generalization

## Open Questions the Paper Calls Out
Unknown: The source material does not explicitly identify open questions, but potential areas for investigation include: How do other kernel-based metrics perform in capturing feature quality? Does the FL gap correlate with other measures of feature relevance? How does this phenomenon manifest in different training regimes or optimization algorithms?

## Limitations
- Narrow experimental scope focused mainly on CIFAR-10 and synthetic staircase functions
- Analysis limited to NTK/CK-style metrics, potentially missing other relevant measures
- Results may not generalize to other architectures like transformers or different training regimes

## Confidence
- Claim: Current feature learning theories are fundamentally inadequate - Medium confidence
- Claim: Experimental results showing NTK/CK metrics fail to distinguish generalizing networks - High confidence

## Next Checks
1. Test whether FL gap measurements predict generalization across diverse architectures (CNNs, transformers, MLPs) and tasks
2. Investigate whether other feature learning metrics beyond NTK/CK can better capture feature quality
3. Examine whether the relationship between feature learning strength and generalization holds in continual learning settings with catastrophic forgetting