---
ver: rpa2
title: A Large Vision-Language Model based Environment Perception System for Visually
  Impaired People
arxiv_id: '2504.18027'
source_url: https://arxiv.org/abs/2504.18027
tags:
- people
- visually
- impaired
- image
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a wearable system for visually impaired people
  to better perceive their surroundings. It uses a large vision-language model (LVLM)
  combined with a semantic segmentation model to provide global and local descriptions
  of the environment.
---

# A Large Vision-Language Model based Environment Perception System for Visually Impaired People

## Quick Facts
- arXiv ID: 2504.18027
- Source URL: https://arxiv.org/abs/2504.18027
- Authors: Zezhou Chen; Zhaoxiang Liu; Kai Wang; Kohou Wang; Shiguo Lian
- Reference count: 40
- Key outcome: Wearable system using LVLM + segmentation provides global/local scene descriptions for VIPs; segmentation-grounded prompting reduces hallucinations; technical experiments show improved accuracy over Qwen-VL-Chat

## Executive Summary
This paper presents a wearable vision-language system for visually impaired people that combines a Large Vision-Language Model with semantic segmentation to provide environmental descriptions. The key innovation is using segmentation results as external knowledge to ground LVLM prompts, reducing hallucinations. Users interact through simple gestures to capture images and receive audio feedback. The system achieves state-of-the-art performance on multiple vision-language benchmarks while providing exploratory validation with visually impaired users.

## Method Summary
The system uses a Qwen-VL-Chat LVLM and a ViT-based semantic segmentation model (SETR) sharing the same visual encoder. For each captured image, the segmentation model identifies objects and their boundaries, which are compiled into a sentence and prepended to the LVLM prompt to reduce hallucinations. The shared encoder enables feature reuse, reducing inference latency. Users interact via long-press (global description), tap/swipe (local object info with distance-based audio feedback), and double-tap (detailed cropped object description). The system runs on a terminal-cloud architecture with RGB-D sensing and HTTP communication.

## Key Results
- POPE adversarial accuracy improves from 0.842 to 0.862 with segmentation grounding
- POPE precision increases from 0.897 to 0.923 with segmentation grounding
- 5-point Likert scale user study shows strong effectiveness for VIPs perceiving surroundings
- Response times: 4400ms for global description, 2800ms for detailed object description

## Why This Works (Mechanism)

### Mechanism 1: Segmentation-Grounded Prompting for Hallucination Reduction
The system injects segmentation results as external knowledge into the LVLM input prompt to reduce object hallucination. A ViT-based segmentation model first identifies all objects in the scene, compiles detected categories into a sentence, and prepends this to the LVLM prompt. This constrains output to objects that actually exist. POPE results show adversarial accuracy improving from 0.842 to 0.862 and precision from 0.897 to 0.923. Break condition: if segmentation produces false positives or misses objects, the LVLM will inherit or amplify these errors.

### Mechanism 2: Shared Visual Encoder for Latency Reduction
Sharing a ViT encoder between segmentation and LVLM reduces inference time by reusing visual features. A single ViT encoder processes the RGB image once, with extracted visual features passed to both the segmentation decoder and (via an adaptor) the LLM. This eliminates redundant encoding passes. SETR decoder is finetuned on VizWiz while encoder remains frozen. Break condition: if segmentation and LVLM require fundamentally different feature representations, shared encoder may degrade one or both task performances.

### Mechanism 3: Depth-Modulated Audio Feedback for Spatial Perception
Volume scaling based on object distance (from depth map + segmentation mask) improves spatial awareness. When users tap/swipe, the system retrieves the object's segmentation mask, averages depth values within that region, and maps distance to playback volume—closer objects are louder. Break condition: noisy depth sensors or irregular object shapes may produce misleading distance estimates, confusing users.

## Foundational Learning

- **Semantic Segmentation (Pixel-wise Classification)**
  - Why needed: System relies on segmentation masks to identify object boundaries, categories, and spatial extent for both prompt grounding and tap-based retrieval
  - Quick check: Given an image with a chair partially occluded by a table, would a segmentation model assign pixels at the occlusion boundary to "chair," "table," or "background"?

- **LVLM Hallucination Modes**
  - Why needed: Understanding what hallucinates (non-existent objects, wrong attributes, incorrect counts) clarifies why external grounding helps
  - Quick check: If an LVLM describes "a red wine glass on the table" but the image shows a clear glass, which hallucination type is this?

- **Vision-Language Alignment via Adaptors**
  - Why needed: Visual feature x_v must be projected into the LLM's embedding space before concatenation with text prompts
  - Quick check: What happens if the adaptor is poorly trained—would the LLM receive meaningful visual information or noise?

## Architecture Onboarding

- **Component map:** Terminal: RGB-D sensor → USB → Smartphone (touch UI, speaker, HTTP client) → Cloud: HTTP server → ViT encoder (shared) → [Segmentation decoder (SETR) AND Adaptor → LLM (Qwen-VL-Chat)]

- **Critical path:** 1. Long-press triggers image capture and upload 2. Cloud runs segmentation; constructs grounded prompt; runs LVLM 3. Global description returned as audio (~4400ms latency) 4. Tap/swipe queries seg mask locally for category; depth map for volume 5. Double-tap crops object region; runs LVLM for detailed description (~2800ms)

- **Design tradeoffs:** Cloud enables heavy LVLM but introduces network latency (4400ms for long-press); edge would limit model size. Speaker preserves ambient sound awareness (safety) but reduces privacy and clarity in noisy environments. Frozen encoder reduces compute/risk of catastrophic forgetting but may limit task-specific optimization.

- **Failure signatures:** Hallucination persists: Segmentation model fails to detect key objects → LVLM lacks grounding constraint. Wrong object on tap: Segmentation mask misaligned with RGB or low-confidence region → user confused. Silent/degraded audio: Depth sensor returns NaN/zero values in bright sunlight or reflective surfaces. Timeout on long-press: Network congestion or cloud inference queue backlog.

- **First 3 experiments:** 1. Ablation on prompt grounding: Run identical scenes with and without segmentation-derived prompts; measure POPE/MME hallucination rates to isolate contribution of external knowledge. 2. Latency profiling: Instrument each stage (image upload, ViT encode, seg decode, LLM inference, audio synthesis) to identify bottleneck—hypothesis: LLM inference dominates the 4400ms. 3. User task success rate: Recruit 5 visually impaired participants; measure success on structured tasks (seat finding, obstacle avoidance) in controlled vs. naturalistic environments; correlate with segmentation quality and depth accuracy.

## Open Questions the Paper Calls Out
- How to efficiently deploy the model on local terminals to reduce latency and protect data privacy without relying on cloud connectivity
- To what extent errors in the semantic segmentation model propagate to the LVLM, potentially causing new forms of hallucination or description errors
- How effective the static snapshot-based interaction paradigm is compared to continuous, real-time video stream analysis for dynamic navigation scenarios

## Limitations
- The exact prompt construction format for incorporating segmentation results is unspecified
- Technical implementation details of encoder sharing between SETR and Qwen-VL-Chat are unclear
- User study involved only 5 participants with sighted experimenters assisting in navigation
- All neighbor papers are from 2025 with zero citations, limiting external validation

## Confidence
- **High**: Shared visual encoder architecture for latency reduction is technically sound and well-established
- **Medium**: Hallucination reduction claim supported by POPE improvements but lacks full prompt details and generalization analysis
- **Low**: Depth-modulated audio feedback effectiveness for spatial perception is asserted but not empirically validated

## Next Checks
1. Conduct prompt ablation study with baseline, segmentation-only, and full grounding conditions to isolate contribution of external knowledge on hallucination rates
2. Profile latency at each cloud inference stage to identify whether 4400ms is dominated by network transfer, segmentation, or LVLM inference
3. Validate system effectiveness with independent user testing on structured navigation tasks in both controlled and real-world environments without sighted assistance