---
ver: rpa2
title: Agentic AI Needs a Systems Theory
arxiv_id: '2503.00237'
source_url: https://arxiv.org/abs/2503.00237
tags:
- agentic
- systems
- arxiv
- agents
- agency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that AI development needs a systems-theoretic
  perspective to fully understand the capabilities and risks of agentic AI. The authors
  propose a definition of functional agency that quantifies the degree of agency based
  on a system's ability to take goal-directed actions, model outcomes, and adapt behavior.
---

# Agentic AI Needs a Systems Theory

## Quick Facts
- arXiv ID: 2503.00237
- Source URL: https://arxiv.org/abs/2503.00237
- Reference count: 40
- Primary result: AI development needs a systems-theoretic perspective to understand agentic AI capabilities and risks

## Executive Summary
This paper argues that understanding agentic AI requires a systems-theoretic framework that accounts for emergent behaviors beyond individual component capabilities. The authors propose a definition of functional agency that quantifies agency across three dimensions: action generation sophistication, outcome modeling complexity, and adaptation capability. They outline mechanisms for emergence including multimodal cognition, causal reasoning, and metacognitive awareness, and identify critical challenges in building safe and effective agentic systems.

## Method Summary
The paper presents a theoretical framework without algorithmic implementation. It defines functional agency through a three-component taxonomy and proposes mechanisms for emergent capabilities based on systems theory principles. The method relies on conceptual analysis and theoretical modeling rather than empirical evaluation, with no quantitative metrics or scoring methodology provided for measuring agency levels across systems.

## Key Results
- Functional agency can be quantified across action generation, outcome modeling, and adaptation dimensions
- System-level capabilities can emerge from interactions that exceed individual component abilities
- Open challenges include building generalist agents, efficient agent-agent interactions, subgoal control, and human-agent governance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal environment interaction produces generalized cognitive representations exceeding individual components
- Mechanism: Cross-modal signal correlation enables error detection/correction and formation of abstract representations capturing invariant properties across modalities
- Core assumption: Agents have multimodal tools and can maintain state across observations
- Evidence anchors:
  - [abstract] "enhanced cognition due to interaction with the environment"
  - [section 3.1] "When multiple modalities provide correlated information about the same phenomenon, the brain combines these signals... to form abstract representations"
  - [corpus] Related work on agentic digital twins taxonomy touches on capability dimensions
- Break condition: Lack of multimodal sensors/tools or inability to correlate signals across modalities

### Mechanism 2
- Claim: Causal reasoning emerges from prediction error minimization without explicit causal modeling
- Mechanism: Hierarchical predictive processing generates top-down predictions; bottom-up prediction errors drive model updates or actions to make predictions true
- Core assumption: Agents can generate predictions, observe outcomes, and actively sample environment
- Evidence anchors:
  - [abstract] "emergence of causal reasoning ability"
  - [section 3.2] "causal models emerge directly as a consequence of this progressive, error-minimizing refinement"
  - [corpus] No direct corpus validation; related papers discuss multi-agent systems
- Break condition: Inability to actively sample environment or maintain prediction-error feedback loops

### Mechanism 3
- Claim: Metacognitive awareness emerges from prediction with uncertainty quantification plus inter-agent communication
- Mechanism: Error detection arises from disagreement between decision and confidence variables; social interaction calibrates confidence estimates
- Core assumption: Agents can quantify and communicate uncertainty; multi-agent communication infrastructure exists
- Evidence anchors:
  - [abstract] "emergence of metacognitive awareness"
  - [section 3.3] "Social interaction enhances this process by enabling individuals to calibrate their confidence estimates"
  - [corpus] Weak corpus support; synchronization dynamics paper mentions collective dynamics
- Break condition: Inability to maintain confidence estimates or communicate uncertainties to peers

## Foundational Learning

- Concept: **Functional Agency (Definition 2.1)**
  - Why needed here: The paper's core definition quantifies agency as a spectrum across three dimensions
  - Quick check question: Can you distinguish between a thermostat (reactive policy, no adaptation) and an LLM (contextual adaptation) using the three functional agency criteria?

- Concept: **Pearl's Causal Hierarchy**
  - Why needed here: The paper maps outcome model sophistication to association → intervention → counterfactual levels
  - Quick check question: What causal level does an autonomous vehicle operate at versus an LLM, and why does this matter for functional agency?

- Concept: **Systems Theory Basics (feedback loops, emergence)**
  - Why needed here: The paper argues system-level behavior differs fundamentally from component behavior due to feedback loops
  - Quick check question: How does the act-sense-adapt loop at agent level feed into higher-level feedback loops in an agentic system?

## Architecture Onboarding

- Component map:
  - Human -> Agent (LLM/LMM with tool access)
  - Agent -> Tools (APIs, external services, computational resources)
  - Agent -> Environment (infrastructure, other humans, other agents, other systems)
  - Human <-> Agent (instruction/feedback)
  - Agent <-> Agent (task decomposition/delegation)
  - Agent <-> Environment (action/observation via tools)

- Critical path:
  1. Human seeds task specification
  2. Agent decomposes task and delegates subtasks to other agents
  3. Agents interact with environment via tools (actions → observations)
  4. Observations inform outcome model; adaptation updates behavior
  5. Feedback signals flow back to human for approval/clarification

- Design tradeoffs:
  - **Pretraining vs. exploration**: Heavy pretraining provides prior knowledge but may bias representations
  - **Trust transfer specificity**: Overly specific trust requires excessive data; overly generous transfer yields suboptimal delegation
  - **Subgoal monitoring vs. autonomy**: Full human monitoring doesn't scale; agent monitoring creates recursive oversight problem

- Failure signatures:
  - **Self-deception**: Agent creates shortcut solutions that falsely satisfy goals
  - **Alignment faking**: Exhibits desired behavior under oversight, reverts when unmonitored
  - **Subgoal drift**: Long chains of subgoals weaken constraint from original human intent
  - **Accumulated risk**: Sequences of low-risk automated decisions compound into emergent systemic risk

- First 3 experiments:
  1. **Multimodal cross-referencing test**: Provide agent with correlated signals across two modalities; measure whether representations generalize to novel combinations
  2. **Prediction error minimization loop**: Implement prediction → action → observation → error signal cycle; measure whether causal model quality improves over iterations
  3. **Confidence calibration via peer communication**: Run multi-agent setup where agents share confidence estimates; measure whether collective calibration improves individual error detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can agents with minimal pretraining but equipped with self-directed exploration mechanisms achieve superior generalization compared to heavily pretrained models?
- Basis in paper: [explicit] The authors explicitly ask, "Could an agent with minimal pretraining, but equipped with mechanisms for self-directed exploration and curiosity-driven learning, achieve superior generalization?"
- Why unresolved: Current paradigms rely heavily on massive static datasets; the trade-off between prior knowledge and in-situ learning is not empirically determined
- What evidence would resolve it: Comparative studies demonstrating that agents using intrinsic motivation and exploration can master out-of-distribution tasks more efficiently than state-of-the-art pretrained models

### Open Question 2
- Question: To what degree should trust established on a specific task transfer to a different task without causing delegation failures?
- Basis in paper: [explicit] Section 4.2 asks, "To what degree should trust on a given task transfer to trust on a different task?"
- Why unresolved: Transferring trust too narrowly causes data sparsity; transferring too broadly leads to suboptimal outcomes; the features that dictate successful transfer are undefined
- What evidence would resolve it: A formalized framework that maps task features to agent capabilities, accurately predicting performance on novel tasks based on historical trust metrics

### Open Question 3
- Question: How can the accumulation of risk from sequences of safe, automated local decisions be reliably detected?
- Basis in paper: [explicit] The authors note that "a sequence of many low risk, but automated, agent decisions may create larger emergent risks over time"
- Why unresolved: Agents optimize for immediate local goals; monitoring systems look for explicit unsafe actions rather than emergent systemic risk
- What evidence would resolve it: A system-level monitoring tool capable of simulating or analyzing action trajectories to flag emergent risks invisible to local decision-making modules

## Limitations
- Theoretical mechanisms lack direct empirical validation
- No quantitative metrics for measuring functional agency
- Framework does not specify how to measure emergent system-level agency versus component-level agency
- Absence of evaluation protocols for demonstrating counterfactual outcome modeling or reflective adaptation

## Confidence
- **High Confidence**: The conceptual framework of functional agency and its three-component taxonomy are well-grounded in established AI literature
- **Medium Confidence**: Proposed mechanisms for emergent capabilities are theoretically plausible but require empirical validation
- **Low Confidence**: The claim that these emergent mechanisms will scale to produce truly agentic systems capable of safe and effective real-world operation

## Next Checks
1. **Operationalize the Agency Scoring Rubric**: Develop concrete, measurable criteria for each level of the three functional agency dimensions and apply them to benchmark AI systems to test the framework's discriminative power
2. **Design Empirical Tests for Emergent Mechanisms**: Create controlled experiments that isolate and measure whether multimodal cross-referencing, prediction error minimization, and confidence calibration via communication produce capabilities beyond component-level abilities
3. **Establish Safety Metrics for Agentic Systems**: Develop quantitative measures for self-deception, alignment faking, and subgoal drift to enable systematic evaluation of safety challenges as agentic capabilities scale