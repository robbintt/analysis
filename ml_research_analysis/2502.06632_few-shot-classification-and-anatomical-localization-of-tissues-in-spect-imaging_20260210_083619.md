---
ver: rpa2
title: Few-Shot Classification and Anatomical Localization of Tissues in SPECT Imaging
arxiv_id: '2502.06632'
source_url: https://arxiv.org/abs/2502.06632
tags:
- classification
- learning
- prototypical
- prnet
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a few-shot learning approach for classifying
  and localizing tissues in SPECT imaging. The authors adapted Prototypical Networks
  with a pre-trained ResNet-18 backbone to classify ventricles, myocardium, and liver
  tissues, achieving 96.67% training and 93.33% validation accuracy.
---

# Few-Shot Classification and Anatomical Localization of Tissues in SPECT Imaging

## Quick Facts
- arXiv ID: 2502.06632
- Source URL: https://arxiv.org/abs/2502.06632
- Reference count: 8
- Classification accuracy: 96.67% training, 93.33% validation for tissue types

## Executive Summary
This paper presents a few-shot learning approach for classifying and localizing tissues in SPECT imaging. The authors adapted Prototypical Networks with a pre-trained ResNet-18 backbone to classify ventricles, myocardium, and liver tissues, achieving 96.67% training and 93.33% validation accuracy. They also modified PRNet for 2D SPECT imaging, using an encoder-decoder architecture with skip connections, achieving a training loss of 1.395 while accurately reconstructing patches and capturing spatial relationships. These results demonstrate the effectiveness of Prototypical Networks for tissue classification with limited labeled data and PRNet for anatomical landmark localization, providing a foundation for improved deep learning frameworks in medical imaging.

## Method Summary
The study combines Prototypical Networks for tissue classification and PRNet for anatomical localization in 2D SPECT imaging. For classification, a pre-trained ResNet-18 backbone maps images to an embedding space where class prototypes are computed as mean support embeddings, and query images are classified via softmax over Euclidean distances to prototypes. For localization, PRNet uses a self-supervised approach with random double cropping to predict anatomical coordinates and reconstruct patches, trained with a combined loss of distance and reconstruction terms. The model operates on a limited dataset of 12 SPECT images, using episodic training for few-shot classification.

## Key Results
- Prototypical Network achieved 96.67% training accuracy and 93.33% validation accuracy for classifying ventricles, myocardium, and liver tissues
- PRNet achieved a training loss of 1.395, accurately reconstructing patches and capturing spatial relationships in 2D SPECT imaging
- Both approaches demonstrated effectiveness with severely limited labeled data (12 total SPECT images)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prototypical Networks enable tissue classification with minimal labeled samples by embedding images into a metric space where class membership is determined by proximity to computed prototypes.
- **Mechanism:** The ResNet-18 backbone maps input images to an embedding space. For each class k, a prototype ck is computed as the mean of support set embeddings. Query images are classified via softmax over negative Euclidean distances to each prototype (Eq. 1). Training minimizes negative log-probability (Eq. 2), pulling same-class embeddings closer to their prototype.
- **Core assumption:** Tissue types form distinct clusters in the learned embedding space, and the pre-trained ResNet-18 features transfer meaningfully to SPECT imaging despite domain shift.
- **Evidence anchors:**
  - [abstract]: "Prototypical Network, with a pre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver tissues with 96.67% training and 93.33% validation accuracy."
  - [Section II.A]: Describes embedding function fϕ, prototype computation as "mean feature vectors of the support examples," and distance-based classification.
  - [corpus]: Neighbor paper on "Prototypical Contrastive Learning For Improved Few-Shot Audio Classification" confirms prototypical methods generalize across domains, though this is cross-domain evidence only.
- **Break condition:** If tissue classes exhibit high intra-class variance exceeding inter-class differences, prototype representations may collapse or overlap, degrading accuracy.

### Mechanism 2
- **Claim:** PRNet learns anatomical landmark localization through self-supervised spatial relationship prediction without requiring explicit landmark annotations.
- **Mechanism:** Random double cropping selects two points ci and cj within an image. Patches around these points are encoded, and the network predicts their anatomical coordinates a(ci) and a(cj). The predicted relative offset d′ji is compared to ground truth offset dji via distance loss Ldis. A reconstruction loss Lrec ensures the decoder can reconstruct original patches from latent representations. The combined loss Lssl = Ldis + Lrec trains both objectives jointly.
- **Core assumption:** Anatomical structures share spatial regularity across individuals, enabling the network to learn generalizable coordinate representations from a single image's patch relationships.
- **Evidence anchors:**
  - [abstract]: "PRNet, adapted for 2D imaging with an encoder-decoder architecture and skip connections, achieved a training loss of 1.395, accurately reconstructing patches and capturing spatial relationships."
  - [Section II.B]: Details self-supervised loss formulation and patch-based training procedure.
  - [corpus]: Weak direct evidence—neighbor papers on landmark localization (e.g., "Reliable uncertainty quantification for 2D/3D anatomical landmark localization") address different methods; no direct PRNet validation in corpus.
- **Break condition:** If anatomical variability across patients is high relative to training data diversity, learned spatial relationships may not generalize beyond the training image.

### Mechanism 3
- **Claim:** Episodic training with support/query splits creates a meta-learning objective that directly optimizes for few-shot generalization.
- **Mechanism:** Rather than standard batch training, each training episode samples N support images and M query images per class. The model computes prototypes from support images, then predicts query labels. Gradients flow through the entire episode, teaching the network to learn from limited examples at inference time.
- **Core assumption:** The episode distribution during training matches the test-time few-shot evaluation scenario.
- **Evidence anchors:**
  - [Section III]: "Each episode included a fixed number of classes, support images, and query images... 3 support images and 6 query images per class, training the model for 10 episodes."
  - [corpus]: No direct corpus validation of episodic training effectiveness for medical imaging; this remains an assumption based on the original Prototypical Networks paper.
- **Break condition:** If test episodes use different numbers of support examples or class distributions than training episodes, meta-learning transfer may degrade.

## Foundational Learning

- **Concept: Few-Shot Learning / Meta-Learning**
  - **Why needed here:** Standard deep learning requires thousands of labeled examples. This study operates with only 12 SPECT images. Understanding meta-learning explains why the model can generalize from episode-based training rather than traditional batch training.
  - **Quick check question:** Can you explain why training on episodes (support + query sets) is fundamentally different from training on fixed batches, and why this matters for generalization?

- **Concept: Metric Learning and Embedding Spaces**
  - **Why needed here:** Prototypical Networks rely on Euclidean distance in an embedding space. Without understanding how neural networks learn distance metrics, the classification mechanism appears arbitrary.
  - **Quick check question:** Given two embedded vectors, what does a smaller Euclidean distance imply about the model's learned representation, and how does contrastive training shape this?

- **Concept: Self-Supervised Learning from Proxy Tasks**
  - **Why needed here:** PRNet uses spatial offset prediction as a pretext task. Understanding self-supervision explains how the model learns meaningful representations without explicit landmark labels.
  - **Quick check question:** Why would predicting the relative position between two random patches force a network to learn anatomically meaningful features?

## Architecture Onboarding

- **Component map:**
  ```
  Prototypical Network Pipeline:
  Input Image → ResNet-18 Backbone (pretrained, fine-tuned)
              → Embedding Vector fϕ(x)
              → Euclidean Distance to Prototypes {ck}
              → Softmax Classification

  PRNet Pipeline:
  Image → Random Double Crop → Patch Pair [x(ci), x(cj)]
        → Encoder (2D conv) → Latent Coordinates [a(ci), a(cj)]
        → Offset Prediction d′ji vs Ground Truth dji
        → Decoder (skip connections) → Reconstructed Patches
        → Combined Loss: Ldis + Lrec
  ```

- **Critical path:** For Prototypical Networks, the embedding quality from ResNet-18 determines prototype separability. For PRNet, the encoder must learn spatially meaningful latents; skip connections preserve local detail for reconstruction.

- **Design tradeoffs:**
  - 3 support images per class balances data efficiency against prototype reliability (fewer samples = noisier prototypes)
  - ResNet-18 (vs. deeper variants) trades capacity for overfitting risk with limited data
  - PRNet's 2D adaptation from 3D sacrifices volumetric context for computational tractability and data availability
  - 10 training episodes for Prototypical Networks is minimal—may underfit on complex distributions

- **Failure signatures:**
  - High training accuracy but low validation accuracy → overfitting to support set artifacts, insufficient episode diversity
  - PRNet reconstruction loss plateaus high → encoder failing to capture spatial structure, check skip connection integrity
  - Prototype collapse (similar distances across classes) → embedding space not discriminative, may need backbone fine-tuning or different learning rate

- **First 3 experiments:**
  1. **Baseline validation:** Reproduce results with 5-way 3-shot setup using the same 3 tissue classes. Log training/validation accuracy per episode to confirm convergence behavior matches reported ~93% validation.
  2. **Ablation on support set size:** Test with 1-shot, 3-shot, and 5-shot configurations. Expect accuracy to scale with support examples; if not, embedding space may be poorly conditioned.
  3. **PRNet reconstruction sanity check:** Visualize reconstructed patches (as in Fig. 1) across training epochs. Confirm loss decrease correlates with visible improvement; if reconstruction is good but localization fails, the distance loss term may need weighting adjustment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating PRNet-based spatial features directly into a semantic segmentation framework improve the delineation of myocardial and liver tissues in SPECT images?
- Basis in paper: [explicit] The conclusion states that the results pave the way for "integrating and improving segmentation accuracy in deep learning frameworks in the future."
- Why unresolved: The current study evaluates PRNet for localization and Prototypical Networks for classification as separate tasks, but does not implement or test a unified segmentation pipeline.
- What evidence would resolve it: A comparative study showing improved Dice scores or IoU for tissue segmentation when using PRNet spatial priors versus a baseline U-Net or similar architecture.

### Open Question 2
- Question: How does the 2D-adapted PRNet perform in terms of quantitative localization error (e.g., Mean Radial Error) relative to the original 3D implementation?
- Basis in paper: [inferred] The results section reports a training loss (1.395) and visual reconstruction accuracy, but omits standard localization metrics (such as pixel-distance error) required to benchmark against other landmark detection methods.
- Why unresolved: Without quantitative distance metrics, it is unclear if the "accurate" localization mentioned in the text meets clinical precision requirements.
- What evidence would resolve it: Reporting the Mean Radial Error (MRE) or similar metrics in millimeters or pixels between predicted and ground-truth anatomical landmarks.

### Open Question 3
- Question: Do the classification and localization models maintain performance when applied to full 3D SPECT volumes rather than 2D-sliced image patches?
- Basis in paper: [explicit] The abstract and introduction specify that the study used "2D-sliced image cropped around heart" as a "proof of concept," acknowledging that standard SPECT analysis often involves 3D data.
- Why unresolved: The efficacy of the adapted 2D architectures (ResNet-18 backbone, 2D PRNet) on volumetric data remains untested, and 2D slices may lose critical spatial context found in 3D imaging.
- What evidence would resolve it: Validation results from the proposed models when applied to volumetric SPECT datasets, comparing 2D slice-wise predictions to full 3D analysis.

### Open Question 4
- Question: Is the high validation accuracy (93.33%) robust across different patient demographics and scanner protocols, given the model was trained on only 12 images?
- Basis in paper: [inferred] The introduction highlights the "severely limited dataset of only 12 SPECT images" as a challenge, raising concerns about overfitting and generalization to unseen clinical data distributions.
- Why unresolved: High accuracy on a validation set derived from such a small sample size may not reflect real-world variability in anatomy or imaging noise.
- What evidence would resolve it: Cross-validation results using external SPECT datasets from different clinical sites or leave-one-out cross-validation to demonstrate stability.

## Limitations
- Limited sample size (12 images) constrains generalization claims; results may not transfer to broader patient populations or different SPECT scanners
- Domain adaptation from natural image pretraining (ResNet-18) to medical SPECT imaging introduces unknown transfer gaps
- PRNet modification from 3D to 2D architecture lacks validation against original performance benchmarks

## Confidence
- **High confidence**: Prototypical Networks' theoretical framework and training methodology are well-established in literature
- **Medium confidence**: Reported classification accuracies are plausible given few-shot setup, but limited data size prevents robust generalization assessment
- **Low confidence**: PRNet localization claims rely heavily on self-supervised reconstruction metrics without independent landmark verification

## Next Checks
1. **Cross-validation robustness**: Implement 5-fold cross-validation across the 12 SPECT images to assess classification stability and prevent split-specific overfitting
2. **Domain-specific pretraining**: Compare ResNet-18 with SPECT-domain pretraining (if available) versus ImageNet weights to quantify transfer learning impact
3. **PRNet ablation study**: Train PRNet with and without reconstruction loss (L_rec only) to isolate the contribution of spatial relationship prediction versus pure reconstruction capability