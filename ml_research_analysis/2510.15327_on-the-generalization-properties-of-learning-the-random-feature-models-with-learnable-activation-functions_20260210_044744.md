---
ver: rpa2
title: On the Generalization Properties of Learning the Random Feature Models with
  Learnable Activation Functions
arxiv_id: '2510.15327'
source_url: https://arxiv.org/abs/2510.15327
tags:
- features
- number
- have
- learning
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the generalization properties of Random Feature
  models with Learnable Activation Functions (RFLAF). The key idea is to apply data-dependent
  sampling schemes to generate features more efficiently.
---

# On the Generalization Properties of Learning the Random Feature Models with Learnable Activation Functions

## Quick Facts
- **arXiv ID:** 2510.15327
- **Source URL:** https://arxiv.org/abs/2510.15327
- **Reference count:** 40
- **Primary result:** RFLAF with leverage weighted sampling achieves same performance as plain sampling with significantly fewer random features.

## Executive Summary
This paper analyzes the generalization properties of Random Feature models with Learnable Activation Functions (RFLAF), introducing leverage weighted sampling based on ridge leverage scores. The key contribution is showing that by sampling features according to their importance (leverage scores) rather than uniformly, the number of required random features can be dramatically reduced. The theoretical analysis provides sharp bounds on feature requirements for both regression and classification tasks, with empirical validation demonstrating significant efficiency gains on standard datasets.

## Method Summary
The method combines two key ideas: learnable activation functions and data-dependent feature sampling. First, activation functions are parameterized as linear combinations of basis functions (RBF or B-splines), creating a bilinear model. Second, features are sampled using leverage weighted sampling, where the sampling probability for each feature direction is proportional to its ridge leverage score. The algorithm first learns an approximate kernel using a large feature pool, computes leverage scores, resamples a smaller set of important features, and retrains the final model on this reduced set.

## Key Results
- For MSE loss, the bound on required random features improves from Ω(1/ϵ²) to Ω((1/ϵ)^(1/t)) for general t ≥ 1, and to Ω(1) when the Gram matrix has finite rank.
- For Lipschitz loss, the bound improves from Ω(1/ϵ²) to Ω((1/ϵ²)^(1/t)).
- Empirical results show weighted RFLAF achieves same performance as plain sampling with significantly fewer features (e.g., 100 vs 1000 features).
- The error decomposition shows disentangled control of feature complexity (s) and sample complexity (n).

## Why This Works (Mechanism)

### Mechanism 1: Data-Dependent Feature Selection via Leverage Scores
Sampling random features based on ridge leverage scores reduces required features by focusing on directions most relevant to the data geometry. The algorithm calculates feature importance using l_λ(w) ∝ p(w)σ̃(Xw)ᵀ(K̃ + nλI)⁻¹σ̃(Xw), sampling proportional to this score. This works when the spectral measure and data geometry have rapid eigenvalue decay, making the effective dimension d_λ much smaller than total features. Break condition: if the Gram matrix spectrum is flat (slow decay), the advantage disappears.

### Mechanism 2: Adaptive Kernel Shaping via Learnable Activations (RFLAF)
Replacing fixed activation functions with learnable approximations σ̃(z) = ΣaᵢBᵢ(z) allows the model to fit the target function more accurately. The activation is parameterized as linear combinations of basis functions, turning the model into f(x) = ΣaᵢBᵢ(xᵀW)Qv. This works when the basis grid is sufficiently dense (large N) and wide (appropriate h) to approximate the optimal activation within error ϵ. Break condition: if grid number N is too small or width h is poorly chosen, the activation approximation error dominates.

### Mechanism 3: Disentangled Error Complexity
RFLAF separates control of feature complexity (number of random features s) from sample complexity (sample size n). The error decomposition splits excess risk into regularization term λ (controlling feature sampling error) and approximation/Rademacher complexity term O(1/h√n) (controlling sample size). This allows aggressive sampling strategies where s can be drastically reduced without massive increase in n, provided spectral conditions hold. Break condition: if the problem is mis-specified with insufficient samples n for chosen model complexity 1/h, the O(1/h√n) term dominates.

## Foundational Learning

**Concept: Ridge Leverage Score**
- **Why needed here:** This is the statistical engine of the paper. Understanding that it measures the "influence" of a specific feature direction on the kernel matrix is crucial to understanding why weighting the sampling by this score reduces the required feature count.
- **Quick check question:** If a feature direction w has a leverage score of 0.5, is it more or less likely to be sampled than a direction with a score of 0.001?

**Concept: Rademacher Complexity**
- **Why needed here:** The paper uses this to bound the generalization gap. It explains the cost of the "learnable" part of RFLAF—while you get better feature efficiency, the hypothesis class complexity increases, impacting the required sample size n.
- **Quick check question:** Does increasing the grid number N of basis functions increase or decrease the Rademacher complexity of the hypothesis class?

**Concept: Spectral Decay (Eigenvalues of Gram Matrix)**
- **Why needed here:** The theoretical gains are explicitly conditional on the eigenvalue decay rate of the kernel matrix (finite rank vs. polynomial decay).
- **Quick check question:** According to Table 1, does finite rank or polynomial decay (λᵢ ∝ i⁻ᵗ) yield a tighter bound on the number of required features s?

## Architecture Onboarding

**Component map:**
Input X -> Feature Generator W (random weights from p(w)) -> Basis Layer Bᵢ(xᵀW) (RBF/B-splines with parameters N,h) -> Activation Parameters a (coefficients) -> Sampling Weights Q (diagonal from leverage scores) -> Output Layer v (final linear weights)

**Critical path:**
1. **Initialization:** Sample large pool of features W from p(w)
2. **Kernel Approximation:** Solve preliminary optimization to find ã (shaping approximate kernel)
3. **Scoring:** Calculate approximate leverage scores pᵢ using ã
4. **Resampling:** Draw final smaller set of features W using probabilities from pᵢ
5. **Training:** Solve final regularized problem on reduced feature set

**Design tradeoffs:**
- **Grid Size (N) vs. Speed:** Higher N better approximates activation (lower error) but increases computational cost
- **Pool Size (s) vs. Final Size (S):** Need large initial pool s to accurately estimate leverage scores, but final inference model size S can be significantly smaller (e.g., 100 vs 1000 in experiments)

**Failure signatures:**
- **High Test Error:** If S is reduced too aggressively relative to spectral complexity (effective dimension d_λ), approximation error will spike
- **No Convergence in Step 2:** If initial pool s is too small, leverage score approximation fails, leading to biased resampling distribution

**First 3 experiments:**
1. **Sanity Check (Plain vs. Weighted):** Implement RFLAF with plain sampling vs. leverage weighted sampling on adult or CIFAR-10. Plot Test Loss vs. Number of Features to verify performance compression.
2. **Grid Sensitivity:** Run weighted algorithm with varying grid numbers N ∈ {16, 32, 64}. Observe if performance saturates, validating small N is often sufficient.
3. **Spectral Analysis:** Compute eigenvalues of Gram matrix K̃ on small data subset. Verify if eigenvalues decay rapidly; if not, confirm theoretical speedup is not observed empirically.

## Open Questions the Paper Calls Out

**Open Question 1:** Can a refined analysis utilizing local Rademacher complexity be developed for RFLAF to derive a tighter bound on sample size n than current e^Ω(1/ϵ³)?
- **Basis in paper:** Page 6 states the refined analysis that might deduce a sharper bound on n remains to be explored. The major difficulty consists in the estimate of local Rademacher complexity, which requires that the hypothesis class F_V be convex.
- **Why unresolved:** The hypothesis class of RFLAF is bilinear and non-convex with respect to joint parameters (a,v), blocking standard techniques used for plain RF models.
- **Evidence would resolve it:** A novel theoretical framework for estimating local Rademacher complexity for non-convex/bilinear function classes, resulting in a bound tighter than e^Ω(1/ϵ³).

**Open Question 2:** Is it possible to improve the bound on the number of random features s for RFLAF under plain sampling scheme when using Lipschitz continuous losses?
- **Basis in paper:** Page 6 notes that except from the case of Lipschitz continuous losses with plain sampling scheme, the estimated order of 1/ϵ in all other cases decreases.
- **Why unresolved:** The paper successfully improved bounds for MSE (plain/weighted) and Lipschitz (weighted), but the Lipschitz plain sampling bound remained at Ω(1/ϵ²) (matching previous results).
- **Evidence would resolve it:** A proof reducing the plain Lipschitz bound below Ω(1/ϵ²) or a lower bound proof demonstrating that Ω(1/ϵ²) is tight for this specific loss/sampling combination.

**Open Question 3:** How does the non-convexity of the activation learning step affect theoretical guarantees if the algorithm converges to a local minimum rather than global minimum?
- **Basis in paper:** Algorithm 1 minimizes a non-convex objective (Eq. 8) to find a. Theorems 4.1 and 4.2 assume the optimal a is found, while Appendix B only guarantees the existence of a minimizer.
- **Why unresolved:** The theoretical excess risk bounds depend on the "optimal parameters" of the approximate kernel. The error propagation from approximating these parameters via non-convex optimization is not analyzed.
- **Evidence would resolve it:** Generalization bounds that explicitly include the optimization error |E[l_â] - E[l_ã]| or empirical proof that local minima suffice for the required kernel approximation quality.

## Limitations
- The theoretical benefits explicitly depend on the eigenvalue decay of the Gram matrix, which may not hold for all datasets
- Missing specific hyperparameter values (regularization constants λ and RBF bandwidth h) make reproduction difficult
- All theoretical bounds and empirical results are demonstrated on only four specific datasets (Adult, CIFAR-10, Protein, Workloads)

## Confidence
- **High:** The theoretical decomposition of error bounds and core mechanism of leverage-weighted sampling are mathematically sound and well-explained
- **Medium:** The empirical validation showing performance compression is convincing for tested datasets, but lack of hyperparameter details limits reproducibility confidence
- **Low:** The claim that RFLAF separates feature complexity (s) from sample complexity (n) is theoretically supported but not empirically validated across diverse spectral conditions

## Next Checks
1. **Spectral Condition Verification:** Compute the eigenvalue decay of the kernel matrix on a new dataset. Verify if the rapid decay assumed in theory (leading to s=O(1) for finite rank) holds empirically.
2. **Hyperparameter Sensitivity:** Systematically vary the regularization parameter λ and RBF bandwidth h. Quantify how these choices affect the approximation error and required number of features.
3. **Generalization Test:** Apply the RFLAF framework to a dataset with known slowly decaying spectrum (e.g., synthetic data with polynomial eigenvalue decay). Verify if theoretical speedup over plain sampling still materializes.