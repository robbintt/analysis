---
ver: rpa2
title: 'RRO: LLM Agent Optimization Through Rising Reward Trajectories'
arxiv_id: '2505.20737'
source_url: https://arxiv.org/abs/2505.20737
tags:
- reward
- process
- action
- latexit
- rising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RRO addresses the scalability challenge of process supervision
  in LLM agent training by proposing a dynamic exploration strategy that focuses on
  identifying steps with rising rewards rather than uniformly sampling all action
  candidates. The method incrementally augments process supervision until finding
  steps exhibiting positive reward differentials relative to preceding iterations,
  dynamically expanding the search space for next action candidates while efficiently
  capturing high-quality training data.
---

# RRO: LLM Agent Optimization Through Rising Reward Trajectories

## Quick Facts
- arXiv ID: 2505.20737
- Source URL: https://arxiv.org/abs/2505.20737
- Reference count: 7
- Key outcome: RRO achieves 62.91 reward on WebShop (vs. 61.39 for IPR) and 55.08 on InterCode-SQL (vs. 52.39), requiring only 1.86 and 1.64 sampled trajectories respectively.

## Executive Summary
RRO addresses the scalability challenge of process supervision in LLM agent training by proposing a dynamic exploration strategy that focuses on identifying steps with rising rewards rather than uniformly sampling all action candidates. The method incrementally augments process supervision until finding steps exhibiting positive reward differentials relative to preceding iterations, dynamically expanding the search space for next action candidates while efficiently capturing high-quality training data. Experimental results demonstrate that RRO achieves superior performance on WebShop and InterCode-SQL benchmarks compared to baselines, requiring significantly fewer sampled trajectories than existing process supervision approaches while maintaining or improving task completion metrics.

## Method Summary
RRO is a three-stage pipeline for training LLM agents on multi-step decision tasks. First, a base model undergoes supervised fine-tuning on expert trajectories to learn basic planning capabilities. Second, at each trajectory step, the method iteratively samples action candidates and computes process rewards via Monte Carlo rollouts until finding an action whose reward exceeds or matches the previous step's reward (rising reward condition). Third, preference pairs consisting of the highest-reward and lowest-reward actions are used to train the agent via Direct Preference Optimization (DPO), improving planning without requiring separate reward model training.

## Key Results
- RRO achieves 62.91 reward on WebShop benchmark using only 1.86 sampled trajectories per step
- On InterCode-SQL, RRO obtains 55.08 reward with just 1.64 sampled trajectories per step
- Compared to IPR baseline, RRO uses 63% fewer samples on WebShop (1.86 vs 5) and 45% fewer on InterCode-SQL (1.64 vs 3)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamically expanding exploration until finding an action with rising reward (compared to its predecessor) efficiently identifies high-quality preference pairs while reducing unnecessary sampling.
- Mechanism: At each trajectory step t, iteratively sample action candidates a_t from πSFT. Compute process reward r_t for each via Monte Carlo rollouts. Stop sampling when r_t ≥ r_{t-1} (rising reward condition). Select the highest-reward candidate as "chosen" and lowest-reward as "rejected" for DPO training.
- Core assumption: At least one sampled action exists with r_t ≥ r_{t-1}, which the paper proves via law of total probability: since rPRM(s_t, a_t) = Σ_{a_{t+1}} rPRM(s_{t+1}, a_{t+1}) · P(a_{t+1}|e_{1:t}), at least one successor must meet or exceed the current expected reward.
- Evidence anchors:
  - [abstract] "we incrementally augment the process supervision until identifying a step exhibiting positive reward differentials, i.e. rising rewards, relative to its preceding iteration"
  - [section 3.2] "Once we sample an action a^(τ)_t whose process reward r^(τ)_t is greater than or equal to the preceding action's reward r_{t-1}, we terminate the sampling process"
  - [corpus] Related work on process verification (MAS-ProVe, arXiv:2602.03053) confirms intermediate step evaluation improves multi-agent reasoning, but doesn't specifically validate the rising-reward stopping criterion.
- Break condition: If process reward estimates are noisy or m (rollout count) is too small, the rising reward condition may trigger prematurely or never trigger, degrading preference pair quality.

### Mechanism 2
- Claim: Monte Carlo estimation of process rewards via averaged outcome rewards provides a tractable proxy for expected future success at each intermediate step.
- Mechanism: For state-action pair (s_t, a_t), sample m future trajectory rollouts from the current policy, compute the outcome reward rORM for each full trajectory, and average: rPRM(s_t, a_t) = (1/m) Σ_j rORM(u, e_{1:t} ⊕ ê^{(j)}_{t+1:n}).
- Core assumption: Outcome rewards are dense enough (range [0,1]) that averaging over rollouts yields meaningful step-level signals; this holds in WebShop and InterCode-SQL where partial credit exists.
- Evidence anchors:
  - [section 2] "The process reward model is represented as r(s_t, a_t) ∈ [0, 1]... we also adopt the MCTS-based process reward model"
  - [section 3.3] "rPRM(s_t, a_t) = P(1|u, a_1, o_1, ..., a_t, o_t) = P(1|e_{1:t})"
  - [corpus] AgentPRM (arXiv:2502.10325) uses similar Monte Carlo rollouts for agent process rewards, supporting this approach.
- Break condition: If outcome rewards are sparse (binary success/failure only), Monte Carlo estimates will have high variance, requiring more rollouts or alternative estimation methods.

### Mechanism 3
- Claim: Training the agent with DPO on preference pairs constructed from highest vs. lowest process reward actions improves planning while avoiding the complexity of separate reward model training.
- Mechanism: After RRO sampling produces (a^+_t, a^-_t) pairs, apply DPO loss: L_DPO = -E[log σ(β log(π_θ(a^+)/π_ref(a^+)) - β log(π_θ(a^-)/π_ref(a^-)))]. This directly optimizes the policy to rank preferred actions higher without an explicit reward model.
- Core assumption: The preference pairs reflect meaningful quality differences (i.e., process rewards correlate with true action quality); DPO's implicit reward model captures these preferences.
- Evidence anchors:
  - [section 3.4] "We follow the classic Direct Preference Optimization (DPO) training paradigm and continuously train the supervised fine-tuned model πSFT through DPO"
  - [table 1] RRO achieves 62.91 reward on WebShop (vs. 61.39 for IPR) and 55.08 on InterCode-SQL (vs. 52.39), with fewer samples (1.86 vs. 5; 1.64 vs. 3)
  - [corpus] Verified Critical Step Optimization (arXiv:2602.03412) similarly addresses step-level credit attribution for long-horizon tasks, supporting the general direction but not DPO specifically.
- Break condition: If preference pairs are noisy (e.g., process rewards don't reflect true quality), DPO may reinforce spurious patterns. Hyperparameter β requires tuning to prevent overfitting or underfitting to preferences.

## Foundational Learning

- **Concept: Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**
  - Why needed here: RRO builds on PRM-style supervision; understanding that PRMs evaluate intermediate steps while ORMs evaluate only final outcomes is essential for grasping why rising rewards matter.
  - Quick check question: Given a 5-step agent trajectory where steps 1-4 are optimal but step 5 fails, would an ORM assign zero credit to steps 1-4? Would a PRM?

- **Concept: Monte Carlo Estimation for Value Functions**
  - Why needed here: RRO estimates process rewards by averaging outcome rewards over sampled rollouts; this is a Monte Carlo value estimation technique.
  - Quick check question: If you sample only m=1 rollout per action, what happens to the variance of your process reward estimate? What's the tradeoff in choosing m?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: RRO uses DPO (not PPO or RLHF with a separate reward model) to train from preference pairs. Understanding the DPO objective is necessary to implement or debug the agent optimization stage.
  - Quick check question: In DPO, what does the β hyperparameter control? If β is too large, what behavior might you observe?

## Architecture Onboarding

- **Component map:** SFT Module -> Monte Carlo Process Reward Estimator -> RRO Sampling Loop -> DPO Training Module

- **Critical path:** SFT quality → Process reward accuracy (depends on m rollouts, outcome reward quality) → Rising reward stopping condition reliability → Preference pair quality → DPO optimization effectiveness. The weakest link is typically process reward estimation; if m is too low or outcome rewards are sparse, the entire pipeline degrades.

- **Design tradeoffs:**
  - **m (rollouts per process reward)**: Higher m improves estimate accuracy but increases compute. Paper doesn't specify exact m; ablation needed.
  - **Exploration budget**: RRO dynamically determines sample count; fixed-size baselines use K=3-5. RRO averages 1.86-1.64 samples, suggesting the rising reward condition often triggers early.
  - **β (DPO temperature)**: Controls strength of preference enforcement. Not specified in paper; likely task-dependent.
  - **Assumption:** The theoretical guarantee (at least one action with r_t ≥ r_{t-1}) assumes infinite sampling; in practice, a maximum sample cap may be needed as a safety valve.

- **Failure signatures:**
  - **Premature stopping**: If r_t ≥ r_{t-1} triggers on a low-quality action due to noisy estimates, preference pairs will be weak. Symptom: DPO training shows low loss but agent reward doesn't improve.
  - **Never triggering**: If process rewards are monotonically decreasing (e.g., environment gets harder as trajectory progresses), the condition may never fire. Symptom: Infinite loop or hitting max sample cap.
  - **Sparse outcome rewards**: If rORM is binary (0 or 1), Monte Carlo estimates have high variance. Symptom: Large swings in process reward across similar actions.

- **First 3 experiments:**
  1. **Validate Monte Carlo process reward quality**: For a held-out set of trajectory prefixes, compute process rewards with varying m (1, 3, 5, 10). Correlate with ground-truth future outcomes to determine adequate m.
  2. **Ablate rising reward condition vs. fixed-size sampling**: Compare RRO's dynamic stopping against fixed K=2, 3, 5 sampling on WebShop. Measure both final reward and average samples used.
  3. **Stress test on sparse-reward variant**: Create a binary-success-only version of InterCode-SQL (no partial credit). Compare RRO against baselines to identify if the mechanism degrades with sparse rewards.

## Open Questions the Paper Calls Out
None

## Limitations
- The rising reward stopping condition assumes process rewards provide stable, monotonic signals, but Monte Carlo estimates with sparse outcome rewards can violate this, causing premature stopping or infinite loops.
- The effectiveness of DPO training on preference pairs depends heavily on reward quality—if process rewards don't correlate with true action quality, the optimization may reinforce spurious patterns.
- The theoretical guarantee of at least one rising reward action assumes infinite sampling; in practice, a maximum sample cap may be needed, which the paper doesn't specify.

## Confidence
- **High**: The core mechanism of dynamic sampling to reduce trajectory evaluation costs is well-supported by the theoretical proof and empirical results showing fewer samples (1.86-1.64) than baselines.
- **Medium**: The Monte Carlo process reward estimation is plausible given AgentPRM precedent, but m (rollout count) is unspecified, and sparse reward variants weren't tested.
- **Low**: The rising reward stopping condition's robustness to noisy estimates and monotonic reward environments is assumed but not empirically validated.

## Next Checks
1. **Ablation study on process reward estimation**: Compare RRO with varying Monte Carlo rollout counts (m=1, 3, 5, 10) to quantify variance impact on preference pair quality and final agent performance.
2. **Robustness to reward sparsity**: Evaluate RRO on a binary-success-only variant of WebShop/InterCode-SQL to test mechanism degradation when outcome rewards lack partial credit.
3. **Maximum sampling cap analysis**: Implement and compare different maximum iteration limits (K=5, 10, 20) for the rising reward stopping condition to identify practical safety valves.