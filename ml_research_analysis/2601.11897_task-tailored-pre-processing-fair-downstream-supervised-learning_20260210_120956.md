---
ver: rpa2
title: 'Task-tailored Pre-processing: Fair Downstream Supervised Learning'
arxiv_id: '2601.11897'
source_url: https://arxiv.org/abs/2601.11897
tags:
- fairness
- data
- downstream
- learning
- supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of ensuring fairness in supervised\
  \ learning models while maintaining high predictive accuracy, especially when the\
  \ downstream models are not pre-specified. The authors propose a novel pre-processing\
  \ framework that tailors the data transformation to supervised learning tasks by\
  \ leveraging Hirschfeld\u2013Gebelein\u2013R\xE9nyi (HGR) correlation to control\
  \ fairness and utility trade-offs."
---

# Task-tailored Pre-processing: Fair Downstream Supervised Learning

## Quick Facts
- **arXiv ID:** 2601.11897
- **Source URL:** https://arxiv.org/abs/2601.11897
- **Reference count:** 40
- **Primary result:** Novel pre-processing framework achieves better fairness-accuracy trade-offs than existing methods on both tabular and image datasets.

## Executive Summary
This paper addresses the challenge of ensuring fairness in supervised learning models while maintaining high predictive accuracy, especially when downstream models are not pre-specified. The authors propose a novel pre-processing framework that tailors the data transformation to supervised learning tasks by leveraging Hirschfeld–Gebelein–Rényi (HGR) correlation to control fairness and utility trade-offs. The method optimizes a pre-processing map to transform original data into a fair intermediate distribution while preserving task-specific utility, using a constrained min-max bilevel optimization approach solved via neural networks.

The method outperforms existing data fairness and task-tailored approaches on multiple benchmarks, including tabular datasets (Adult, ACSEmployment, ACSPublicCoverage) and image datasets (CelebA). Empirical results show consistent fairness improvements across diverse downstream models while maintaining competitive accuracy, with particular effectiveness in altering semantic features in image data to balance subgroup probabilities.

## Method Summary
The framework implements a constrained min-max bilevel optimization where the outer optimization finds a pre-processing map G* that minimizes loss plus fairness penalty on transformed data, while the inner optimization finds the optimal upstream classifier h̄* on that transformed data. HGR correlation is approximated via χ²-divergence dual representation with a neural network V, turning the problem into min-max optimization. The approach uses constraint budgets (δ_X, δ_Y) to control utility preservation while ensuring fairness improvement through HGR regularization. The method is implemented with separate networks for covariate transformation (G_X), label transformation (G_Y), upstream classification (h), and HGR approximation (V).

## Key Results
- Achieves better fairness-utility trade-offs than FairRR, FairTabGAN, and MultiFair on tabular datasets, with hypervolume improvements of 6.2% and 3.4% on Adult and ACSEmployment respectively
- Demonstrates effectiveness on CelebA image dataset by semantically altering features (e.g., smiling intensity, hair color) to balance subgroup probabilities while maintaining image quality
- Shows consistent fairness improvements across diverse downstream models (RandomForest, SVM, MLP, Gradient Boosting) with high consistency scores (σ_AUC > 0.8)
- Theoretical guarantees show fairness improvement for arbitrary downstream models when upstream model class is sufficiently large

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data fairness approaches impose overly strong regularization compared to task-tailored methods, potentially sacrificing utility unnecessarily.
- Mechanism: Proposition 1 shows that for any Borel measurable function u, ρ(u(X̄), A) ≤ ρ(X̄, A). By penalizing ρ(h̄*(X̄), A) instead of ρ(X̄, A), the method targets fairness specifically at the model output level rather than the data distribution level.
- Core assumption: The upstream model class H is sufficiently expressive to capture the downstream task relationship.
- Evidence anchors: [abstract] "authors argue that the data fairness approaches impose overly strong regularization from the perspective of the HGR correlation"; [Section II-A] Proposition 1 and toy example in Figure 2 demonstrate trade-off improvement.
- Break condition: If H is too small (e.g., linear models on highly non-linear data), the gap between data-fair and task-tailored regularization may not materialize.

### Mechanism 2
- Claim: A constrained min-max bilevel optimization can jointly optimize pre-processing and supervised learning with controllable fairness-utility trade-offs.
- Mechanism: The outer optimization finds G* that minimizes loss + fairness penalty on transformed data, while the inner optimization finds h̄* on that transformed data. HGR correlation is approximated via χ²-divergence dual representation with neural network V, turning the problem into min-max optimization.
- Core assumption: The constraints Δ_X(X, X̄) ≤ δ_X and Δ_Y(Y, Ȳ) ≤ δ_Y prevent degenerate solutions while controlling utility loss.
- Evidence anchors: [abstract] "using a constrained min-max bilevel optimization approach solved via neural networks"; [Section III-B] Equation (3) and Algorithm 1 detail full optimization procedure.
- Break condition: If δ_X or δ_Y are set too large, transformed data may become semantically meaningless; if too small, fairness improvement is limited.

### Mechanism 3
- Claim: Downstream model fairness guarantees are achievable when H_k ⊂ H and models satisfy Lipschitz conditions.
- Mechanism: Theorem 2 shows downstream fairness improvement Δ̃ᵏ_F is bounded by upstream improvement Δ̃_F minus terms related to model accuracy gaps and distance metrics. Lemma 1 bounds downstream risk relative to upstream risk through Lipschitz continuity.
- Core assumption: Assumption 1 requires H_k ⊂ H; models must be M-Lipschitz; loss functions must be M_l-Lipschitz.
- Evidence anchors: [Section II-C1] Theorem 2 provides formal bounds with sufficient condition in equation (2); [Section II-C2] Proposition 2 bounds variance of downstream risk across end users.
- Break condition: Lipschitz assumption fails for tree-based models (e.g., gradient boosting), leading to higher consistency variance as shown in Figure S17.

## Foundational Learning

- Concept: **Hirschfeld-Gebelein-Rényi (HGR) Correlation**
  - Why needed here: Core theoretical tool for measuring nonlinear dependence between model predictions and sensitive attributes; justifies why task-tailored regularization is more efficient.
  - Quick check question: Can you explain why ρ(X, A) = 0 implies independence but ρ(h(X), A) = 0 only implies fairness for that specific h?

- Concept: **Bilevel Optimization**
  - Why needed here: The pre-processing map G depends on the solution h̄* of the inner supervised learning problem, requiring gradient computation through an optimization process.
  - Quick check question: How does iterative differentiation enable backpropagation through the inner optimization loop?

- Concept: **Independence vs. Separation (Group Fairness)**
  - Why needed here: Two distinct fairness definitions with different conditional independence structures; method handles both via HGR and conditional HGR.
  - Quick check question: For a loan approval task, which fairness notion better captures "qualified applicants should have equal approval rates regardless of protected attributes"?

## Architecture Onboarding

- Component map: X × A -> G_X -> X̃ -> h -> ŷ̃; h(X̃), A -> V -> HGR approximation; X̃, A, Y -> G_Y -> Ỹ (optional)
- Critical path: Initialize all networks → for each iteration: (1) transform data via G, (2) update h via T' inner steps, (3) update V via gradient ascent, (4) update Lagrangian multipliers λ_X, λ_Y, (5) update G_X, G_Y via gradient descent
- Design tradeoffs:
  - δ_Y = 0 preserves original labels but limits fairness-utility flexibility; δ_Y > 0 allows auxiliary labels that improve consistency for difficult tasks (Remark 2)
  - Higher λ_F enforces stronger fairness but may sacrifice utility; trade-off should be explored via multiple budget settings
  - Larger upstream H improves downstream coverage but increases computational cost
- Failure signatures:
  - Low consistency scores across downstream models → check if non-Lipschitz models (gradient boosting) are included
  - Fairness not improving → verify V is being updated (gradient ascent) and h̄* is actually minimizing loss on transformed data
  - Image distortion at high δ_X → add variational loss (f-divergence) per Algorithm 2 in supplementary material
- First 3 experiments:
  1. Replicate toy example (Figure 2) with synthetic data to verify task-tailored advantage over Wasserstein barycenter methods; validate trade-off curves visually
  2. Run tabular experiments on Adult dataset with δ_X ∈ {0.01, 0.10, 0.30}, δ_Y = 0, λ_F = 10; compare AUC-SP trade-off against FairRR and FairTabGAN using hypervolume indicator
  3. Test downstream consistency: train 5 different downstream models on same transformed data; compute consistency scores σ_AUC and σ_SP; verify they decrease when gradient boosting is excluded (confirming Lipschitz importance)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be efficiently adapted for text classification tasks using Large Language Models (LLMs) while preserving semantic meaning?
- Basis in paper: [explicit] Section V states that extending the framework to text domains like LLMs is set aside for future research because Algorithm 1 is "computationally too expensive" for immediate training.
- Why unresolved: Current computational requirements are prohibitive for LLMs, and defining Δ_X or Δ_Y to ensure grammatical correctness and semantic meaning in text is a non-trivial challenge.
- What evidence would resolve it: A modified, lightweight optimization scheme applied to an LLM that maintains text fluency while achieving fairness objectives.

### Open Question 2
- Question: How can the need to apply the pre-processing map G* to test data be mitigated to simplify data distribution policies?
- Basis in paper: [explicit] Section V discusses that the framework requires converting test data via G*_X to align domains, noting that "this adaptation step... may complicate a policy or procedure of distributing data."
- Why unresolved: Real-world application often limits end-users' access to the transformation map G*_X, making the covariate shift handling difficult for downstream users.
- What evidence would resolve it: The integration of domain adaptation techniques that allow downstream models to generalize to untransformed test data or a theoretical relaxation of the G*_X requirement during inference.

### Open Question 3
- Question: Can the theoretical fairness and utility guarantees be extended to non-Lipschitz downstream models, such as tree-based algorithms?
- Basis in paper: [inferred] Section II-C acknowledges a limitation where "some popular supervised models... may not satisfy the Lipschitz condition," and empirical analysis (Section IV-A2) showed the non-Lipschitz boosting model negatively impacted consistency scores.
- Why unresolved: The theoretical proofs (Lemma 1, Theorem 2) explicitly rely on Lipschitz continuity to bound the risk and fairness deviation of downstream models.
- What evidence would resolve it: Theoretical bounds derived for non-smooth function classes or a modified distance metric that holds for tree-based ensembles.

## Limitations

- The method's theoretical guarantees rely heavily on Lipschitz continuity assumptions for downstream models, which are violated by common tree-based methods like gradient boosting
- While the method shows superior performance on image datasets, the semantic transformations (e.g., altering facial features) raise questions about interpretability and potential unintended biases
- The choice of constraint budgets (δ_X, δ_Y) significantly impacts results but lacks clear guidance for practitioners on optimal tuning

## Confidence

- **High Confidence:** The task-tailored approach's theoretical advantage over data-fair methods (Proposition 1) and the empirical superiority on tabular datasets are well-supported with clear quantitative evidence
- **Medium Confidence:** The image dataset results are compelling but rely on subjective visual assessments of semantic feature changes; quantitative measures of semantic preservation would strengthen claims
- **Medium Confidence:** Downstream consistency bounds are mathematically rigorous but assume model class relationships that may not hold in practice; real-world validation across diverse model architectures is limited

## Next Checks

1. **Non-Lipschitz Model Robustness:** Test downstream consistency when including gradient boosting models, comparing observed consistency scores against theoretical bounds to quantify the impact of Lipschitz violations

2. **Semantic Preservation Quantification:** For image datasets, implement automated metrics (e.g., SSIM, LPIPS) to measure perceptual quality and semantic attribute preservation alongside fairness improvements

3. **Constraint Budget Sensitivity Analysis:** Systematically vary δ_X and δ_Y across multiple orders of magnitude, plotting fairness-utility Pareto frontiers to identify regimes where task-tailored advantages are most pronounced