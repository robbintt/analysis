---
ver: rpa2
title: How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought
  Reasoning with Sparse Autoencoding
arxiv_id: '2507.22928'
source_url: https://arxiv.org/abs/2507.22928
tags:
- features
- patching
- nocot
- feature
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether chain-of-thought (CoT) prompting
  leads to more faithful internal reasoning in large language models. Using sparse
  autoencoders (SAEs) to extract interpretable features and activation patching to
  perform causal interventions, the authors analyze the GSM8K math dataset with Pythia-70M
  and Pythia-2.8B models.
---

# How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding

## Quick Facts
- arXiv ID: 2507.22928
- Source URL: https://arxiv.org/abs/2507.22928
- Authors: Xi Chen; Aske Plaat; Niki van Stein
- Reference count: 18
- Key outcome: CoT prompting induces more interpretable and causally effective internal structures in high-capacity LLMs, validating its role in structured reasoning.

## Executive Summary
This paper investigates whether chain-of-thought (CoT) prompting leads to more faithful internal reasoning in large language models. Using sparse autoencoders (SAEs) to extract interpretable features and activation patching to perform causal interventions, the authors analyze the GSM8K math dataset with Pythia-70M and Pythia-2.8B models. CoT features were swapped into noCoT reasoning runs to measure causal impact on answer log-probabilities. In the 2.8B model, CoT features consistently improved output confidence, while in the 70M model, no reliable gains were observed. CoT also induced significantly higher activation sparsity and more interpretable features in the larger model, indicating more modular internal computation.

## Method Summary
The authors combine SAEs with activation patching to evaluate whether CoT reasoning produces faithful internal representations. They use GSM8K math problems formatted in CoT (3 few-shot examples with step-by-step solutions) and NoCoT (problem only) conditions. Activations are extracted from layer 2 residual stream at final token position in Pythia-70M and 2.8B models. Separate SAEs are trained on CoT and NoCoT activations with dictionary ratios 4 and 8. The researchers patch Top-K or Random-K features from CoT runs into NoCoT runs and measure changes in log-probability of correct answers. Feature interpretability is scored using GPT-3.5-turbo, and activation sparsity is quantified via L1 norms.

## Key Results
- CoT features causally improve answer log-probabilities in 2.8B model (from 1.2 to 4.3) but not in 70M model
- CoT induces significantly higher activation sparsity in 2.8B model, signaling more modular internal computation
- Random-K feature selection often outperforms Top-K in 2.8B model, suggesting useful information is distributed rather than concentrated

## Why This Works (Mechanism)

### Mechanism 1: Scale-Dependent Structured Sparsity
CoT prompting induces sparse, modular internal representations contingent on sufficient model scale. In larger models (2.8B), CoT prompts suppress noise neurons while strongly activating specific features, creating structured sparsity that disentangles representations and makes features more monosemantic.

### Mechanism 2: Distributed Feature Causality
Useful reasoning information activated by CoT is widely distributed across many features rather than concentrated in a few top activations. Randomly selected features often outperform Top-K selections, suggesting that Top-K overfits to local activation peaks while missing supportive features that are moderately activated but causally necessary.

### Mechanism 3: Feature-Level Faithfulness via Activation Patching
CoT-derived features have direct causal influence on output probability of correct answers. By extracting feature vectors via SAE and swapping them from CoT to NoCoT runs, significant increases in log-probability of correct answers confirm these features are functional components of the reasoning circuit.

## Foundational Learning

- **Sparse Autoencoders (SAEs)**: Core tool for decomposing dense activations into interpretable features through dictionary learning. The L1 penalty enforces monosemanticity by encouraging sparsity in the feature representation.
- **Activation Patching**: Causal validation method distinguishing correlation from causation by replacing features between model runs and measuring output effects.
- **Superposition & Polysemanticity**: Challenge where single neurons represent multiple concepts, necessitating sparse dictionaries to isolate reasoning steps.

## Architecture Onboarding

- **Component map**: GSM8K prompt formatter -> Pythia model -> Layer 2 residual extractor -> SAE encoder/decoder -> Feature swapper -> Model decoder -> Log-probability evaluator
- **Critical path**: Extracting Layer 2 Residuals → Training SAEs → Identifying Top/Random K features → Patching h_NoCoT with h_CoT features → Decoding back to residual space → Measuring output log-prob shift
- **Design tradeoffs**: Top-K vs Random-K selection strategy; patching layer choice (layer 2); dictionary ratio (4 vs 8) affecting sparsity vs reconstruction tradeoff
- **Failure signatures**: Small model instability (symmetric log-prob distribution in 70M); performance degradation when Top-K patching causes monotonic decline; sparsity mismatch when CoT doesn't shift activation histogram
- **First 3 experiments**: 1) Run Top-20 patching on both 70M and 2.8B to verify scale threshold; 2) Compare Top-K vs Random-K patching curves in 2.8B; 3) Generate histogram plots of residual activations for CoT vs NoCoT in 2.8B

## Open Questions the Paper Calls Out
1. Do the observed causal benefits of CoT prompting and random-feature patching generalize to models significantly larger than 2.8B parameters?
2. How do CoT-induced features causally interact and evolve across the full sequence of the reasoning process?
3. Can SAE feature interpretations be grounded in specific model components (neurons or heads) rather than relying on external LLM-based scoring?

## Limitations
- Hyperparameter sensitivity affects SAE performance and feature quality, with critical parameters not fully specified
- Exact CoT few-shot examples and prompt structure are not provided, introducing variability
- Patching at layer 2, final token position may not capture the full causal reasoning chain across the sequence

## Confidence
- **High**: CoT features in 2.8B model causally improve output confidence (∆ logP from 1.2 to 4.3) and induce higher activation sparsity
- **Medium**: Claim that useful information is widely distributed (random-K > top-K) is supported but could be sensitive to selection method
- **Low**: Interpretation that sparsity and interpretability directly indicate "faithful" internal reasoning lacks strong external validation

## Next Checks
1. Perform hyperparameter sweep of SAE training with varied L1 weights and dictionary ratios to assess robustness
2. Conduct layer ablation study by repeating patching at multiple residual stream layers to determine necessity of early-layer features
3. Apply the same SAE + patching pipeline to non-math reasoning tasks to test generalization of CoT-induced sparsity and distributed causality