---
ver: rpa2
title: Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches
arxiv_id: '2509.11241'
source_url: https://arxiv.org/abs/2509.11241
tags:
- beat
- music
- tracking
- meter
- carnatic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated the generalization of state-of-the-art deep\
  \ learning models for meter tracking in Carnatic music, a rhythmically complex non-Western\
  \ tradition. Two models\u2014Temporal Convolutional Network (TCN) and Beat This!"
---

# Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches

## Quick Facts
- arXiv ID: 2509.11241
- Source URL: https://arxiv.org/abs/2509.11241
- Reference count: 0
- Primary result: Fine-tuning deep learning models trained on Western music substantially improves meter tracking in Carnatic music, with Beat This! achieving 90.3% beat and 66.8% downbeat F-measure

## Executive Summary
This study evaluates the generalization of state-of-the-art deep learning models for meter tracking in Carnatic music, a rhythmically complex non-Western tradition. Two models—Temporal Convolutional Network (TCN) and Beat This! (transformer-based)—were tested on the Carnatic Music Rhythm dataset with various training strategies. The results demonstrate that off-the-shelf models trained on Western music show poor performance, especially in downbeat detection, while fine-tuning significantly improves results. The study reveals important trade-offs between accuracy and temporal continuity, with transformer-based models excelling in accuracy and TCNs better suited for continuity and real-time applications.

## Method Summary
The study evaluated two deep learning architectures for meter tracking on the Carnatic Music Rhythm dataset (CMRf) using two-fold cross-validation with 88/88 splits. The TCN model employed dilated convolutions with exponentially increasing dilation rates, while Beat This! used self-attention mechanisms across time. Both models were tested in three configurations: training from scratch on Carnatic data, using pre-trained Western music models off-the-shelf, and fine-tuning pre-trained models on Carnatic data. Evaluation metrics included beat and downbeat F-measure with 70ms tolerance and continuity metrics (CMLt, AMLt). The TCN required DBN post-processing with tāla-specific constraints, while Beat This! used lightweight peak picking without post-processing.

## Key Results
- Off-the-shelf models trained on Western music showed poor performance, with Beat This! achieving only 27.6% downbeat F-measure
- Fine-tuning significantly improved performance, with Beat This! fine-tuned achieving the highest beat and downbeat F-measures (90.3% and 66.8%)
- TCN fine-tuned showed the best continuity scores, with CMLt of 62.9% for beat and 52.1% for downbeat tracking
- Performance varied across tālas, with rhythmically complex ones like Ādi and Rūpaka posing greater challenges

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning pre-trained Western music models on domain-specific Carnatic data substantially improves meter tracking performance compared to off-the-shelf deployment. Transfer learning leverages learned rhythmic representations from Western corpora, then adapts model weights to capture tāla-specific accent patterns, non-isochronous beat structures, and percussion timbres unique to Carnatic music. The pre-trained features provide better initialization than random weights, enabling effective adaptation with limited annotated data (~70 training tracks per fold).

### Mechanism 2
Transformer-based architectures (Beat This!) achieve higher raw prediction accuracy without post-processing, while TCN with DBN post-processing yields superior temporal continuity, particularly for downbeat tracking. Transformers use self-attention to access global context at every layer, capturing long-range rhythmic dependencies and expressive timing variations without fixed dilation patterns. TCNs with dilated convolutions model local-to-global patterns progressively, producing noisier activations that require DBN post-processing to enforce metrical constraints.

### Mechanism 3
Music-informed post-processing parameters (tāla-specific beats per bar, tempo ranges) can improve tracking but create system dependencies that may mask intrinsic model capabilities and fail on edge cases. The DBN post-processor uses Viterbi decoding to find the most likely beat sequence given learned transition and observation models. Setting beats_per_bar=[3,5,7,8] constrains the search space to valid tāla structures, and tempo ranges (55-230 BPM) prevent octave errors.

## Foundational Learning

- **Meter tracking (beat and downbeat detection)**: The fundamental task being addressed; understanding the hierarchical structure (tatum → beat → downbeat) is essential for interpreting model outputs and evaluation metrics. Quick check: Given an 8-beat Ādi tāla cycle, can you identify which beat positions correspond to downbeats vs. regular beats?

- **Transfer learning and fine-tuning strategies**: The paper's core contribution is demonstrating that Western-pretrained models can be adapted to Carnatic music via fine-tuning, but not via direct deployment. Quick check: What is the difference between training from scratch, using an off-the-shelf model, and fine-tuning—and which strategy worked best for each architecture?

- **Continuity metrics (CMLt, AMLt) vs. F-measure**: The paper reveals that high F-measure does not guarantee usable tracking; continuity metrics capture whether predictions maintain correct tempo and phase over time, which is critical for real-time applications. Quick check: If a model achieves 85% beat F-measure but only 30% CMLt, what does this indicate about its tracking behavior?

## Architecture Onboarding

- **Component map**:
  - **TCN Pipeline**: Audio → Log/Mel spectrogram → Convolutional block (2D conv + frequency pooling) → TCN block (dilated 1D convolutions) → Task heads (beat/downbeat activations) → DBN post-processor (Viterbi decoding)
  - **Beat This! Pipeline**: Audio → Mel spectrogram → Stem block + convolution-attention blocks → Transformer encoder (self-attention) → Sum head (beat + downbeat combined) → Lightweight peak picking

- **Critical path**:
  1. Replicate two-fold cross-validation splits from baseline (CMRf dataset, 88 train/88 test per fold)
  2. For TCN: Start with final0 Beat This! checkpoint or TCN trained on Western datasets (GTZAN, Ballroom, Beatles, RWC)
  3. Fine-tune with learning rate 0.001 (TCN) or default Beat This! settings, using validation loss for early stopping
  4. Configure DBN post-processor with beats_per_bar=[3,5,7,8] and min_tempo=55, max_tempo=230
  5. Evaluate using mir_eval for F-measure, CMLt, AMLt on both beat and downbeat predictions

- **Design tradeoffs**:
  - Accuracy vs. Continuity: Beat This! FT achieves best F-measure (90.3% beat, 66.8% downbeat); TCN-FS achieves best continuity (CMLt 62.9% beat, 52.1% downbeat)
  - Resource requirements: TCN (116k parameters) is lightweight and CPU-trainable; Beat This! (20M parameters) requires GPU and ~4 hours on A100
  - Post-processing dependency: TCN requires music-informed DBN configuration; Beat This! is end-to-end but cannot enforce metrical constraints explicitly
  - Data efficiency: TCN-FS works well with 70 training examples; Beat This! requires large pre-training corpus plus fine-tuning

- **Failure signatures**:
  - Half-beat/half-cycle offset: Predictions systematically shifted by 0.5 beats or 0.5 cycles—evidenced by low F-measure but high AMLt
  - Zero downbeat F-measure with perfect AMLt: Downbeats detected at wrong phase—post-processor locked into incorrect metrical interpretation
  - Rapid tempo/meter modulation failure: Polymeter or metric modulation causes post-processor to lose tracking
  - Off-the-shelf model collapse: BeatThis-BL achieves only 27.6% downbeat F-measure, indicating Western training does not transfer

- **First 3 experiments**:
  1. Baseline replication: Run TCN-BL and BeatThis-BL on CMRf test fold using off-the-shelf Western-trained checkpoints to verify evaluation pipeline correctness
  2. Fine-tuning comparison: Fine-tune both architectures on CMRf training fold; compare beat/downbeat F-measure and continuity metrics to identify which architecture suits your accuracy vs. continuity requirements
  3. Tāla-stratified error analysis: Evaluate fine-tuned models per-tāla; identify which tālas underperform and inspect outlier tracks for phase offset, polymeter, or tempo modulation failures

## Open Questions the Paper Calls Out

- **Can hybrid modeling strategies, such as integrating shift-tolerant loss into Temporal Convolutional Networks (TCN), balance sharp prediction accuracy with temporal coherence?**
  The authors propose exploring "hybrid strategies that integrate the strengths of both TCN and Beat This!" specifically by "utilising shift-tolerant loss and sum heads in TCN." This remains unresolved as the study only evaluated the architectures in their standard configurations.

- **Do ground truth annotations for difficult tālas like Ādi and Rūpaka reflect actual performance tempo, or are they systematically annotated at half-tempo?**
  The authors explicitly ask: "Do the ground truth annotations reflect the actual tempo, or are the Ādi and Rūpaka tracks annotated at half tempo, potentially contributing to their underperformance?" This remains unresolved as the analysis showed these tālas cluster in a narrow, slow BPM range but did not verify if this matches perceptual pulse.

- **Can the adaptation strategies (specifically fine-tuning) validated for Carnatic music generalize effectively to North Indian Hindustani music?**
  Future work suggests "Extending research to North Indian Hindustani music... could validate the generalizability of adaptation strategies and enrich ethnomusicological MIR research." This remains unresolved as the study was restricted to the Carnatic tradition.

## Limitations

- Limited data for certain tālas (Khaṇḍa chāpu with only 5 tracks) may cause overfitting and unreliable performance estimates
- The study does not provide explicit fold indices for reproducibility, making exact replication difficult
- Performance drop on complex tālas like Rūpaka may indicate fundamental limitations in handling polymeter and tempo modulation that aren't fully addressed

## Confidence

- **High confidence**: The superiority of fine-tuning over off-the-shelf deployment for domain adaptation (supported by clear quantitative evidence across multiple metrics)
- **Medium confidence**: The architectural differences between TCN and Beat This! in terms of continuity vs. accuracy trade-offs (though the mechanisms are well-explained, the evidence is primarily from this single study)
- **Low confidence**: The generalizability of these findings to other non-Western musical traditions with different rhythmic structures

## Next Checks

1. Conduct ablation studies on the DBN post-processing parameters (beats_per_bar, tempo ranges) to quantify their impact on tracking performance across different tālas
2. Test the fine-tuned models on unseen tālas or creative performances with edupu variations to assess robustness to real-world performance practices
3. Compare the learned rhythmic representations from fine-tuned models to human annotations to identify systematic biases or phase errors in meter tracking