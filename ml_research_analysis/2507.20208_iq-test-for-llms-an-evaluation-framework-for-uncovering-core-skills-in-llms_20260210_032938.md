---
ver: rpa2
title: 'IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs'
arxiv_id: '2507.20208'
source_url: https://arxiv.org/abs/2507.20208
tags:
- tasks
- factor
- task
- skills
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a psychometric framework for evaluating large
  language models (LLMs) by applying exploratory factor analysis (FA) to model performance
  data. Instead of aggregating scores across diverse benchmarks, the method identifies
  latent skills underlying performance, such as general NLU, long-document comprehension,
  and domain-specific reasoning.
---

# IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs

## Quick Facts
- arXiv ID: 2507.20208
- Source URL: https://arxiv.org/abs/2507.20208
- Reference count: 40
- This paper introduces a psychometric framework for evaluating large language models (LLMs) by applying exploratory factor analysis (FA) to model performance data. Instead of aggregating scores across diverse benchmarks, the method identifies latent skills underlying performance, such as general NLU, long-document comprehension, and domain-specific reasoning.

## Executive Summary
This paper introduces a novel psychometric framework for evaluating large language models (LLMs) by applying exploratory factor analysis (FA) to model performance data. Rather than aggregating scores across diverse benchmarks, the method identifies latent skills underlying performance, such as general NLU, long-document comprehension, and domain-specific reasoning. The framework is applied to a comprehensive leaderboard of 60 LLMs across 44 tasks, uncovering eight interpretable skill dimensions. The analysis reveals that most high performance is driven by specialization rather than model size, with skills like factual precision and domain QA benefiting from targeted training rather than scaling. The proposed approach includes practical tools for assessing task novelty, profiling new models from limited data, and selecting optimal models for unseen tasks. The latent skill space is shown to be robust across perturbations and aligns partially with human preference rankings. This work advocates for a shift from single-score evaluations to skill-based assessment, offering more nuanced and actionable insights for model comparison and development. All code and data are publicly released.

## Method Summary
The framework applies exploratory factor analysis (FA) to LLM performance data across multiple benchmarks, identifying latent skills that underlie task performance. Instead of aggregating scores into a single metric, the method decomposes performance into interpretable skill dimensions. The analysis covers 60 LLMs and 44 tasks, using FA to uncover eight core skill dimensions. The approach also includes tools for assessing task novelty, profiling new models from limited data, and selecting optimal models for unseen tasks. The latent skill space is validated for robustness across perturbations and shown to partially align with human preference rankings.

## Key Results
- FA identified eight interpretable skill dimensions underlying LLM performance, including general NLU, long-document comprehension, and domain-specific reasoning.
- Most high performance is driven by specialization rather than model size, with skills like factual precision and domain QA benefiting from targeted training.
- The skill-based model rankings align partially with human preference rankings (Spearman correlation of 0.82), validating the framework's practical utility.

## Why This Works (Mechanism)
The framework works by decomposing LLM performance across diverse benchmarks into latent skills using exploratory factor analysis. This approach uncovers underlying abilities that drive task performance, rather than relying on single aggregated scores. By identifying interpretable skill dimensions, the method enables nuanced comparison of models and reveals whether performance gains come from scaling or specialization. The robustness of the skill space across perturbations and its partial alignment with human preferences demonstrate that these latent skills capture meaningful aspects of LLM capabilities.

## Foundational Learning
- **Exploratory Factor Analysis (FA)**: Statistical method to identify latent variables underlying observed data correlations. Needed to uncover hidden skills driving LLM performance. Quick check: Verify factor loadings and interpretability of extracted dimensions.
- **Psychometric evaluation**: Application of psychological measurement principles to AI model assessment. Needed to move beyond single-score benchmarks to skill-based profiling. Quick check: Confirm that identified skills are meaningful and actionable.
- **Task novelty assessment**: Framework for determining whether a new task requires previously unseen skills. Needed to guide model selection and development. Quick check: Test on truly novel tasks outside the original benchmark set.
- **Model profiling from limited data**: Method to characterize new models using only a subset of benchmarks. Needed for practical deployment and comparison. Quick check: Validate profiling accuracy with reduced task sets.

## Architecture Onboarding
**Component map:** Benchmarks/tasks -> Performance data -> Factor analysis -> Latent skills -> Model profiles -> Skill-based rankings
**Critical path:** Performance data collection → FA model fitting → Skill interpretation → Model profiling → Validation
**Design tradeoffs:** Single-score vs. multi-skill evaluation (favors nuance over simplicity); linear vs. non-linear skill contributions (assumes additivity); comprehensive vs. targeted benchmarking (favors breadth over efficiency)
**Failure signatures:** Poor factor interpretability; unstable skill dimensions across perturbations; weak alignment with human preferences; overfitting to benchmark biases
**3 first experiments:**
1. Apply FA to a new, independent benchmark set to test generalizability of skill dimensions
2. Profile a new model using only a subset of tasks and compare to full-profile results
3. Test model selection accuracy for novel tasks using the skill-based framework

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's ability to uncover generalizable latent skills is constrained by the quality and coverage of input benchmarks; biases or gaps directly affect identified skill dimensions.
- The factor analysis assumes linear relationships and additive skill contributions, which may not fully capture complex LLM behaviors.
- The claim that specialization outweighs scaling is inferred rather than experimentally verified; causal links between training and skill gains are not established.

## Confidence
- Identification of interpretable latent skills: Medium
- Specialization outweighs scaling for most skills: Medium
- Practical utility for model selection and profiling: High
- Alignment with human preferences: Medium

## Next Checks
1. Apply the factor analysis to an independent set of benchmarks not used in the original study to verify the stability and generalizability of the identified skill dimensions.
2. Conduct controlled experiments varying model size and training data composition within the same architecture family to directly test the relative contributions of scaling versus specialization to each skill.
3. Compare the skill-based model rankings with multiple independent human preference datasets to assess the consistency and comprehensiveness of the alignment.