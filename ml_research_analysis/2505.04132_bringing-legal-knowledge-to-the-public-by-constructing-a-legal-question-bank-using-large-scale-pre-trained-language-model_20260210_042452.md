---
ver: rpa2
title: Bringing legal knowledge to the public by constructing a legal question bank
  using large-scale pre-trained language model
arxiv_id: '2505.04132'
source_url: https://arxiv.org/abs/2505.04132
tags:
- questions
- legal
- what
- question
- landlord
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the legal knowledge gap by improving navigability
  and comprehensibility of legal information for the public. The authors develop a
  three-step approach: translating legal concepts into plain language CLIC-pages,
  constructing a large-scale Legal Question Bank (LQB), and creating an AI-powered
  CLIC Recommender (CRec) to match user queries with relevant legal knowledge.'
---

# Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model

## Quick Facts
- arXiv ID: 2505.04132
- Source URL: https://arxiv.org/abs/2505.04132
- Reference count: 10
- Constructs a Legal Question Bank using GPT-3 with 93% coverage and 68% precision

## Executive Summary
This study addresses the legal knowledge gap by improving navigability and comprehensibility of legal information for the public. The authors develop a three-step approach: translating legal concepts into plain language CLIC-pages, constructing a large-scale Legal Question Bank (LQB), and creating an AI-powered CLIC Recommender (CRec) to match user queries with relevant legal knowledge. Machine-generated questions (MGQs) are created using GPT-3 with three different partitioning strategies (section-based, paragraph-based, and hybrid). The hybrid approach yields the best results: 93% coverage, 68% precision, and generates 3,362 MGQs compared to 2,686 human-composed questions. MGQs are more scalable, cost-effective, and diversified than human questions, while also identifying content gaps in CLIC-pages through "augmenting questions." The CRec prototype successfully recommends relevant legal questions based on user descriptions, demonstrating the approach's effectiveness in bridging the legal knowledge gap.

## Method Summary
The authors develop a three-step approach to bridge the legal knowledge gap. First, they translate legal concepts into plain language CLIC-pages using text-to-text machine translation. Second, they construct a large-scale Legal Question Bank (LQB) by generating questions using GPT-3 with three different context-partitioning strategies: section-based, paragraph-based, and hybrid. The hybrid approach combines broad section-level context with targeted paragraph-level attention, yielding the best performance. Third, they create an AI-powered CLIC Recommender (CRec) that uses semantic retrieval to match user queries with relevant legal questions. The system encodes both user scenarios and legal content into vector spaces, then uses cosine similarity to retrieve the most relevant questions. The approach is evaluated against human-composed questions across metrics including coverage, precision, and diversity.

## Key Results
- Hybrid partitioning strategy achieves 93% coverage and 68% precision, outperforming section-based (50% precision) and paragraph-based (41% precision) approaches
- Machine-generated questions produce 3,362 questions compared to 2,686 human-composed questions, with 33% diversity versus 21% for human questions
- CRec prototype successfully retrieves relevant legal questions based on user descriptions, demonstrating effective semantic matching
- Augmenting questions identify content gaps in 37 CLIC-pages, suggesting 40% of pages can be enriched

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Context-Attention Partitioning
Providing a Large Language Model (LLM) with broad section-level context while targeting specific paragraph-level attention yields higher question precision and coverage than treating the document as a monolith or isolated fragments. The "Hybrid" strategy inputs the entire section text (broad context) but appends a specific instruction targeting a single paragraph (e.g., "10 FAQs for Paragraph 1"). This grounds the generation in the specific legal concept of the paragraph while maintaining the semantic coherence provided by the surrounding section. Core assumption: The LLM requires sufficient context window tokens to maintain semantic coherence, but requires explicit prompting constraints to focus generation on specific granular facts. Evidence anchors: [Section 4.1, Results] Table 2 and text state: "Hybrid partitioning gives a much higher precision... [and] a very high coverage of 93%." [Section 3.3, Method] Describes the construction: "We partition d by sections... prepend the paragraphs... with a paragraph label." [Corpus] Related work on legal retrieval (e.g., "Intelligent Legal Assistant" arXiv:2502.07904) suggests laypersons struggle to formulate precise queries; hybrid partitioning creates a diverse bank that bridges this gap by capturing both specific facts and general concepts. Break condition: If the source text segments are semantically incoherent or lack clear boundaries, the broad context may confuse the specific attention mechanism, degrading precision to levels seen in paragraph-based partitioning.

### Mechanism 2: Semantic Retrieval via Answer Vectors
Matching user scenarios to the semantic content of the answer (answer vector) rather than just the phrasing of the question improves the navigability of legal knowledge for laypersons. The CLIC Recommender (CRec) encodes the text of the CLIC-page paragraphs (the answer scope) into a vector ($v_a(q)$). It compares the user's scenario vector ($v(t_u)$) against these answer vectors. This allows the system to retrieve relevant legal concepts even if the user's description uses completely different terminology than the model questions. Core assumption: The embedding model maps non-technical layperson descriptions to the same vector space as technical legal explanations with sufficient fidelity. Evidence anchors: [Section 5, CRec] "We compute the cosine similarity between v(tu) [user input] against all the questions' answer vectors va(q)'s." [Abstract] The approach aims to "shortlists questions... most likely relevant to the given legal situation." [Corpus] Weak corpus evidence; neighbor papers focus on question generation or government data. The specific mechanism of prioritizing answer-scope vectors over question-string vectors for legal navigation is a specific contribution of this paper's architecture. Break condition: If the user's description is too ambiguous or the legal concept is too abstract, the cosine similarity between the user vector and the specific answer vector may fall below the retrieval threshold, failing to surface relevant content.

### Mechanism 3: Automated Gap Detection (Augmenting Questions)
LLM-generated "hallucinations" or out-of-scope questions serve as a valuable signal for identifying content gaps in the source knowledge base. When the LLM generates a relevant legal question that cannot be answered by the current CLIC-page (labeled "incorrect" in precision metrics), it highlights a user-need or perspective not covered by the existing documentation. These "augmenting questions" function as automated content audit tools. Core assumption: The LLM's training data contains common layperson legal queries that exceed the scope of the provided context, reflecting real-world information needs. Evidence anchors: [Section 4.2, Further Discussion] Defines "augmenting questions" as those "relevant to the legal issues... but are not answered directly by the pages." [Section 4.2] Notes that 69 augmenting questions were found, covering 37 pages, suggesting "how nearly 40% of the CLIC-pages can be enriched." Break condition: If the LLM generates irrelevant or nonsensical questions (true hallucinations) rather than relevant but unanswerable ones, the signal-to-noise ratio for content gap analysis becomes too low to be useful.

## Foundational Learning

- **Concept:** **Vector Space Models & Cosine Similarity**
  - **Why needed here:** The CRec system relies entirely on encoding text into vectors (using `all-mpnet-base-v2`) and ranking them by cosine similarity. Understanding that "semantic closeness" equates to "angle closeness" in high-dimensional space is required to debug retrieval failures.
  - **Quick check question:** If a user inputs "landlord won't fix AC" and the CLIC page uses "breach of implied warranty," how does the embedding model bridge this lexical gap compared to keyword search?

- **Concept:** **Prompt Engineering (Context vs. Instruction)**
  - **Why needed here:** The paper's core contribution is a specific prompting strategy (Hybrid). Distinguishing between the *context* (the section text) and the *instruction* (generate FAQs for paragraph X) is critical for reproducing the 68% precision result.
  - **Quick check question:** Why does appending "10 FAQs" to a single paragraph (Paragraph-based partitioning) result in lower precision than prepending the section header and text first (Hybrid)?

- **Concept:** **Single-Link Clustering**
  - **Why needed here:** The system generates thousands of questions, many redundant. Understanding single-link clustering (merging clusters if any pair exceeds the threshold) is necessary to manage the LQB size (Section 3.4).
  - **Quick check question:** Why is a high similarity threshold (0.95) chosen for deduplication rather than a lower one? What is the risk of over-merging distinct legal questions?

## Architecture Onboarding

- **Component map:** Source Knowledge (CLIC-pages) -> Generator (GPT-3) -> Processor (DistilBERT + Clustering) -> Indexer (Vector Encoding) -> Retriever (CRec) -> Results
- **Critical path:** The Hybrid Prompting Strategy. If the prompts are not constructed correctly (i.e., omitting the section context for paragraph targets), the system falls back to paragraph-based performance (41% precision vs 68%). This is the single most sensitive control point.
- **Design tradeoffs:** MGQ vs. HCQ: The paper explicitly trades *precision* (higher in Human-Composed Questions) for *coverage, diversity, and cost* (superior in Machine-Generated Questions). Answer vs. Question Matching: The system prioritizes matching the *answer content* over the *question string* to handle the vocabulary mismatch between lay users and legal text.
- **Failure signatures:** Low Precision: Observed in Paragraph-based partitioning; manifests as questions that are vaguely related but not answered by the specific paragraph. Augmenting Questions: Questions marked "incorrect" by human evaluators but labeled "relevant." These are system "failures" to retrieve an answer, but "successes" in identifying content gaps. High Recall, Low Diversity: If the redundancy removal (Step 3.4) fails, the user receives 10 variations of the same question.
- **First 3 experiments:**
  1. Prompt Ablation: Run the generator on 5 CLIC-pages using Section-based vs. Hybrid partitioning. Verify if Hybrid increases precision by ~18 percentage points as claimed (50% -> 68%).
  2. Retrieval Validation: Input a vague scenario (e.g., "noisy neighbors") into CRec. Measure the rank of the correct "nuisance" CLIC-page. Check if matching $v_a$ (answer vector) outperforms matching $v_s$ (question string).
  3. Gap Analysis: Extract "incorrect" MGQs from a batch run. Manually check if they represent valid legal queries missing from the source text (Augmenting Questions) or pure hallucinations.

## Open Questions the Paper Calls Out
None

## Limitations
The paper's evaluation relies heavily on human annotations for precision, recall, and coverage metrics. While the reported numbers (93% coverage, 68% precision for hybrid approach) appear robust, the specific annotation criteria and inter-annotator agreement are not detailed, introducing potential subjectivity. The "augmenting questions" concept is promising but the paper does not quantify the actual legal relevance or feasibility of addressing these identified gaps. The CRec prototype is demonstrated but lacks quantitative user studies showing actual improvements in legal knowledge accessibility for laypersons.

## Confidence
- **High confidence:** Hybrid partitioning strategy yields better precision (68%) than section-based (50%) or paragraph-based (41%) approaches, as this is directly measured in controlled experiments.
- **Medium confidence:** Semantic retrieval via answer vectors improves navigation compared to keyword search, though this is inferred from architecture design rather than head-to-head comparison experiments.
- **Low confidence:** The practical impact of "augmenting questions" for content gap identification, as the paper presents the concept but lacks validation of its real-world utility.

## Next Checks
1. Annotation Reliability: Re-run human evaluation with explicit inter-annotator agreement metrics (Cohen's kappa) to quantify subjectivity in "relevant" vs "not relevant" classifications.
2. A/B Retrieval Test: Compare CRec's answer-vector matching against a baseline keyword search system using the same user scenarios to measure actual improvement in relevant result retrieval.
3. Content Gap Validation: Take 10 "augmenting questions" and have legal experts assess whether addressing these gaps would meaningfully improve the CLIC-page content, establishing the practical value of this detection mechanism.