---
ver: rpa2
title: 'Beyond Output Faithfulness: Learning Attributions that Preserve Computational
  Pathways'
arxiv_id: '2509.04588'
source_url: https://arxiv.org/abs/2509.04588
tags:
- internal
- activation
- faithfulness
- layers
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses a critical flaw in existing feature attribution
  methods: high scores on external faithfulness metrics like insertion and deletion
  can be achieved by explanations that exploit alternative computational pathways
  inconsistent with the model''s actual reasoning. To solve this, the authors propose
  Faithfulness-guided Ensemble Interpretation (FEI), which optimizes both external
  faithfulness through Ensemble Quantile Optimization (a differentiable formulation
  of insertion/deletion curves) and internal faithfulness through selective gradient
  clipping (which preserves activation patterns).'
---

# Beyond Output Faithfulness: Learning Attributions that Preserve Computational Pathways

## Quick Facts
- arXiv ID: 2509.04588
- Source URL: https://arxiv.org/abs/2509.04588
- Reference count: 40
- Key outcome: FEI achieves state-of-the-art insertion/deletion scores while maintaining significantly lower activation deviation compared to unconstrained optimization

## Executive Summary
This paper addresses a critical flaw in existing feature attribution methods: high scores on external faithfulness metrics like insertion and deletion can be achieved by explanations that exploit alternative computational pathways inconsistent with the model's actual reasoning. To solve this, the authors propose Faithfulness-guided Ensemble Interpretation (FEI), which optimizes both external faithfulness through Ensemble Quantile Optimization (a differentiable formulation of insertion/deletion curves) and internal faithfulness through selective gradient clipping (which preserves activation patterns). Across VGG and ResNet on ImageNet and CUB-200-2011, FEI achieves state-of-the-art insertion/deletion scores while maintaining significantly lower activation deviation compared to unconstrained optimization.

## Method Summary
FEI optimizes attributions by simultaneously maximizing external faithfulness (insertion/deletion metrics) and preserving internal computational pathways (activation patterns). The method has two key components: Ensemble Quantile Optimization (EQO) provides differentiable gradient flow over the complete insertion/deletion curve by parameterizing continuous retention maps for each quantile level with monotonic constraints, while selective gradient clipping (ClipGrad) prevents updates that would disrupt the original activation pattern. The framework is trained using Adam for 100 iterations per quantile on 1,000 images from ImageNet and CUB-200-2011 datasets, using random monochromatic reference images.

## Key Results
- FEI achieves state-of-the-art insertion/deletion scores while maintaining significantly lower activation deviation compared to unconstrained optimization
- FEI_NONE attains the highest external scores but exhibits 6× higher activation deviation and produces spurious attributions on blank images
- Constrained FEI variants achieve strong external performance with robust internal preservation, with 0–0.4% spurious attribution on blank images vs. 92–98% for FEI_NONE

## Why This Works (Mechanism)

### Mechanism 1
External faithfulness metrics can be maximized through alternative computational pathways that do not reflect the model's actual reasoning. Perturbations can reroute computation through different feature detectors while preserving output behavior. Unconstrained optimization exploits these shortcuts, producing high metric scores with spurious attributions (e.g., noisy artifacts, explanations on blank images). Activation deviation correlates with pathway alteration; mechanisms that preserve layer-wise activations maintain the model's causal computation.

### Mechanism 2
Ensemble Quantile Optimization provides differentiable, exact gradient flow over the complete insertion/deletion curve, eliminating convergence ambiguity. EQO parameterizes continuous retention maps α_q for each quantile level with monotonic constraints. The expected perturbed image is computed via soft blending, and constraints enforce exact quantile coverage. Gradients flow through all quantile levels simultaneously.

### Mechanism 3
Selective gradient clipping prevents activation-disrupting updates while preserving optimization flexibility, yielding both external and internal faithfulness. ClipGrad blocks gradients that would push perturbed activations away from original activations. Variants differ in constraint strictness: VM (bidirectional), IVM (prevent amplification), AVM (prevent suppression), IBM (prevent inactive neuron activation). This implicit constraint avoids hyperparameter tuning required by explicit L2 penalties.

## Foundational Learning

- **Concept**: Insertion/Deletion Metrics
  - **Why needed here**: Core evaluation framework; insertion measures output recovery as important pixels are added, deletion measures output drop as important pixels are removed. EQO directly optimizes these.
  - **Quick check question**: Given an attribution map M, does high insertion score guarantee that M reflects the model's reasoning pathway?

- **Concept**: Computational Pathway
  - **Why needed here**: The paper's central thesis is that explanations must preserve the network's internal computation, not just match input-output behavior. Activations serve as the tractable proxy.
  - **Quick check question**: If two different perturbations produce identical output changes but different activation patterns, which one is more "faithful"?

- **Concept**: Gradient Clipping for Constraints
  - **Why needed here**: ClipGrad implements implicit constraints without explicit loss terms. Understanding when to block gradients (vs. penalize) is essential for selecting variants.
  - **Quick check question**: Why might blocking a gradient (setting it to zero) be preferable to adding an L2 penalty on activation deviation?

## Architecture Onboarding

- **Component map**: Forward Pass Cache -> EQO Module -> ClipGrad Module -> Attribution Aggregator
- **Critical path**:
  1. Forward pass to cache original activations {h_ℓ}
  2. For each quantile q_i ∈ Q: (a) blend perturbed input x̃ = α_qi ⊙ x + (1 − α_qi) ⊙ R; (b) compute loss L_ins = −ϕ(x̃) + L_con; (c) compute gradients; (d) apply ClipGrad to all conv layers; (e) update α_qi
  3. Aggregate M from final α maps

- **Design tradeoffs**:
  - VM vs. IBM: VM provides strongest pathway preservation (lowest MSE) but may reduce external scores slightly. IBM offers best external performance with acceptable internal faithfulness
  - Early vs. all-layer clipping: Early-layer-only (1–16) offers slightly better external scores; all-layer clipping provides most robust pathway preservation
  - Quantile granularity: 3, 5, or 9 levels perform similarly; 5 is the default

- **Failure signatures**:
  - FEI_NONE pattern: High insertion scores but 6× activation deviation, spurious attributions on blank images (92–98%), noisy visual artifacts
  - AVM on sequential networks (AlexNet, VGG): Catastrophic blank-image failure (93–95%) due to activation accumulation; stable on ResNet/GoogLeNet (0.1%)
  - Late-layer-only clipping: Insertion collapse (0.430) with high MSE (0.717); confirms early-layer cascade effects

- **First 3 experiments**:
  1. Reproduce blank-image diagnostic: Run all FEI variants on all-black inputs. Confirm FEI_NONE produces spurious attributions while constrained variants do not
  2. Layer-wise ablation: Compare clipping all layers vs. early-only (1–16) vs. late-only (16–30) on VGG16/ImageNet. Measure both insertion scores and MSE/cosine similarity
  3. Variant sweep on sequential vs. multi-branch: Run VM, IVM, AVM, IBM on VGG16 (sequential) and ResNet50 (multi-branch). Confirm AVM fails on VGG but succeeds on ResNet

## Open Questions the Paper Calls Out

- **Open Question 1**: Can FEI be effectively generalized to Vision Transformers (ViTs) using attention pattern alignment as a proxy for internal faithfulness? The authors explicitly state extending FEI to architectures like ViTs is a "key step" and hypothesize that "attention pattern alignment may serve as a natural analogue to pathway preservation." This remains unresolved because the current implementation is restricted to CNNs with ReLU-like activations, and the paper does not test FEI on transformer-based architectures where the computational pathway differs significantly.

- **Open Question 2**: Do alternative internal metrics, such as semantic detector activation, provide a more precise or robust proxy for computational pathway consistency than the layer-wise activation preservation used in this study? The Conclusion notes that the work relies on activation preservation as a proxy and suggests "future work should explore alternative internal metrics (e.g., semantic detector activation)." This is unresolved because while the paper establishes activation preservation as a sufficient proxy to eliminate spurious blank-image attributions, it does not compare this against other potential internal consistency metrics that might capture semantic meaning rather than just numerical activation similarity.

- **Open Question 3**: What are the formal theoretical trade-offs between maximizing external faithfulness (insertion/deletion) and maintaining internal faithfulness (activation preservation)? The authors explicitly request that future work "formally analyze their trade-offs," noting that while they showed the metrics are not aligned, a rigorous theoretical analysis of the optimization landscape remains absent. This is unresolved because the paper empirically demonstrates the conflict but does not provide a theoretical bound or geometric explanation for why these objectives conflict or how the selective gradient clipping navigates this trade-off mathematically.

## Limitations

- The architecture-specific behavior of ClipGrad variants is not fully explained, particularly why AVM fails catastrophically on sequential networks (VGG, AlexNet) but succeeds on residual architectures (ResNet, GoogLeNet)
- The soft blending approximation in EQO may introduce optimization artifacts not present in discrete masking, though empirical validation shows EQO outperforms L1 area regularization
- The mechanism by which residual connections mitigate activation accumulation failures in AVM remains unclear

## Confidence

- **High Confidence**: The core claim that external faithfulness metrics can be gamed through alternative pathways is well-supported by the blank-image diagnostic (92–98% spurious attributions for FEI_NONE) and the 6× activation deviation comparison
- **Medium Confidence**: EQO's differentiability and exact gradient flow are supported by ablation showing L1 area regularization fails catastrophically, but the soft blending approximation's fidelity remains untested against discrete masking in controlled settings
- **Medium Confidence**: ClipGrad variants improve both external and internal faithfulness, but the failure mode of AVM on sequential architectures and the unclear mechanism of residual connections' protective effect reduce confidence in universal applicability

## Next Checks

1. **Architecture transfer**: Apply all ClipGrad variants to VGG16, ResNet50, and AlexNet. Document which variants fail catastrophically on sequential networks and whether residual connections systematically prevent these failures
2. **Soft blending fidelity**: Implement a discrete masking baseline for EQO and compare against soft blending on insertion/deletion scores and convergence stability. Measure gradient flow quality through both methods
3. **Activation deviation causality**: Use targeted adversarial perturbations to force small activation changes that produce large output changes. Measure whether ClipGrad's blocking behavior correlates with actual pathway shifts or merely activation magnitude changes