---
ver: rpa2
title: 'DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based
  Sentiment Analysis'
arxiv_id: '2601.23022'
source_url: https://arxiv.org/abs/2601.23022
tags:
- aspect
- sentiment
- arousal
- absa
- valence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DimABSA introduces the first multilingual and multidomain dataset\
  \ for dimensional aspect-based sentiment analysis, annotating 76,958 aspect instances\
  \ across six languages and four domains with continuous valence-arousal scores alongside\
  \ traditional ABSA elements. It defines three novel subtasks\u2014DimASR (valence-arousal\
  \ regression), DimASTE (joint aspect-opinion extraction with sentiment scoring),\
  \ and DimASQP (full quadruple extraction with category classification and sentiment\
  \ scoring)\u2014and proposes the continuous F1 (cF1) metric to evaluate both categorical\
  \ and continuous outputs."
---

# DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based Sentiment Analysis

## Quick Facts
- arXiv ID: 2601.23022
- Source URL: https://arxiv.org/abs/2601.23022
- Reference count: 36
- Introduces first multilingual and multidomain dataset for dimensional aspect-based sentiment analysis with 76,958 aspect instances across six languages and four domains

## Executive Summary
DimABSA introduces a novel dataset and benchmark for multilingual and multidomain dimensional aspect-based sentiment analysis (DimABSA). The dataset extends traditional ABSA by replacing categorical sentiment polarity with continuous valence-arousal (VA) scores, capturing both polarity and intensity of sentiment. It defines three new subtasks—DimASR (valence-arousal regression), DimASTE (joint aspect-opinion extraction with sentiment scoring), and DimASQP (full quadruple extraction with category classification and sentiment scoring)—and introduces the continuous F1 (cF1) metric to evaluate hybrid extraction-regression tasks. Experiments with large language models show that while zero/few-shot prompting provides strong baselines, fine-tuned models (especially 70B and 120B scales) achieve better performance, though all subtasks remain challenging, particularly for low-resource languages.

## Method Summary
DimABSA was created by collecting reviews across six languages (English, German, Chinese, Russian, Tatar, Ukrainian) and four domains (restaurant, hotel, laptop, phone). Annotators labeled aspect terms, aspect categories, opinion terms, and continuous valence-arousal scores on 1-9 scales. The dataset was split into train/dev/test sets for each language-domain combination. Three subtasks were defined: DimASR (text + aspect → VA regression), DimASTE (text → aspect, opinion, VA extraction), and DimASQP (text → aspect, category, opinion, VA quadruple extraction). The continuous F1 (cF1) metric was introduced to evaluate hybrid tasks by incorporating normalized VA distance into precision and recall calculations. Experiments used zero/few-shot prompting and supervised fine-tuning with QLoRA on models ranging from 14B to 120B parameters.

## Key Results
- Fine-tuned 120B model achieves best DimASR RMSE (1.19 avg) vs. zero-shot GPT-5 mini (2.76 avg)
- 70B model achieves best DimASTE cF1 (0.464) vs. zero-shot (0.353 avg for closed-source models)
- Tatar achieves lowest DimASTE cF1 (0.358 fine-tuned 120B) and DimASQP cF1 (0.309) among all languages
- Performance drops 50%+ from DimASTE to DimASQP, indicating aspect category classification as the bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous VA scores capture sentiment nuances that categorical labels miss
- Mechanism: The VA space encodes both polarity (valence: negative–positive) and intensity (arousal: calm–excited) on independent 1–9 scales, enabling distinction between affective states that share the same categorical polarity but differ in emotional intensity (e.g., "super happy" vs. "recovery"; "very rude" vs. "a bit far")
- Core assumption: Human emotional responses to aspect-level opinions can be meaningfully represented as continuous 2D coordinates, and annotators can reliably assign these scores
- Evidence anchors: [abstract] "This categorical approach fails to capture subtle affective differences..."; [section: Page 2, Figure 1] Visualizes VA space showing U-shaped distribution; [corpus] Weak/missing: Related work exists but limited direct evidence for VA's superiority in ABSA specifically
- Break condition: If downstream applications only require coarse positive/negative/neutral decisions, or if VA annotation proves unreliable for specific domains (arousal RMSE > 2.0 suggests annotation instability)

### Mechanism 2
- Claim: Fine-tuning at larger model scales (70B–120B parameters) is necessary for competitive performance on hybrid extraction-regression tasks
- Mechanism: Hybrid tasks (DimASTE, DimASQP) require simultaneous learning of (1) structured sequence labeling for aspect/opinion span extraction, (2) categorical classification for aspect categories, and (3) numerical regression for VA prediction. Larger models provide greater capacity to handle these heterogeneous objectives without catastrophic interference
- Core assumption: Model parameter count correlates with multi-task learning capacity for mixed discrete-continuous outputs
- Evidence anchors: [abstract] "fine-tuned models (especially 70B and 120B scales) achieve better performance"; [section: Table 3] Fine-tuned 120B achieves best DimASR RMSE (1.19 avg); 70B achieves best DimASTE cF1 (0.464)
- Break condition: If task-specific smaller architectures (e.g., dual-encoder + regression head) match 70B performance, or if inference cost constraints make large models impractical

### Mechanism 3
- Claim: Low-resource languages face persistent performance gaps due to insufficient structural pattern acquisition during pretraining
- Mechanism: ABSA requires learning language-specific syntactic patterns for aspect-opinion span identification. Models pretrained predominantly on English (or high-resource languages) lack adequate exposure to low-resource language grammar, morphology, and discourse patterns, limiting fine-tuning effectiveness even with task-specific supervision
- Core assumption: Pretraining language exposure correlates with downstream ABSA performance; translation-based data creation (as used for Tatar/Ukrainian) provides insufficient structural signal
- Evidence anchors: [section: Page 7] "Tatar also benefits from fine-tuning, it remains the lowest-performing language"; [section: Table 3] Tatar achieves lowest DimASTE cF1 (0.358 fine-tuned 120B); [corpus] Cross-lingual ABSA survey discusses challenges for low-resource languages
- Break condition: If multilingual pretraining (e.g., mT5, XLM-R) or few-shot cross-lingual transfer eliminates the gap, or if translation quality is the confound (45.5% of Tatar instances required manual correction)

## Foundational Learning

- Concept: **Aspect-Based Sentiment Analysis (ABSA) Structure**
  - Why needed here: DimABSA extends traditional ABSA (which extracts aspect terms, categories, opinion terms, and polarity) by replacing categorical polarity with continuous VA scores. Understanding the base task is prerequisite
  - Quick check question: Given "The screen is bright but expensive," can you identify: (A) aspect terms, (O) opinion terms, (C) categories, and how they form triplets/quadruplets?

- Concept: **Valence-Arousal (VA) Dimensional Emotion Model**
  - Why needed here: The entire DimABSA contribution rests on representing sentiment as continuous VA coordinates. Understanding this 2D affective space (from Russell's circumplex model) is essential for interpreting results and potential failure modes
  - Quick check question: On a 1–9 scale, where would "frustrated" (high arousal, negative valence) vs. "melancholic" (low arousal, negative valence) plot? Why does arousal annotation show higher RMSE than valence?

- Concept: **Continuous F1 (cF1) Metric Formulation**
  - Why needed here: DimASTE/DimASQP require evaluating both exact categorical matches AND continuous VA error. Standard F1 cannot penalize imperfect VA predictions; cF1 extends F1 by weighting true positives by (1 – normalized VA distance)
  - Quick check question: If a predicted triplet has correct (A, O) spans but VA prediction error of distance 0.3, what is its cTP contribution? How does this differ from standard binary TP?

## Architecture Onboarding

- Component map: Data Layer (10 language-domain datasets) -> Task Layer (DimASR, DimASTE, DimASQP) -> Evaluation Layer (RMSE, cF1) -> Model Layer (Zero/few-shot prompting vs. fine-tuning)

- Critical path: 1. Select target subtask (DimASR is simplest; start here) 2. Choose language-domain pair (eng-rest has highest annotation agreement) 3. Load DimABSA dataset, verify annotation quality 4. Establish zero-shot baseline using prompts in Appendix C 5. Fine-tune with QLoRA (4-bit, learning rate 2e-5, batch size 4, 5 epochs) 6. Evaluate using appropriate metric (RMSE for DimASR, cF1 for others) 7. Analyze error patterns: extraction failures vs. VA prediction errors vs. category confusion

- Design tradeoffs:
  - VA resolution vs. annotation cost: 1–9 scale provides granularity but increases RMSE; 1–5 or 1–7 scales may improve annotator agreement at cost of expressiveness
  - Model scale vs. deployment feasibility: 120B models achieve best DimASR performance but require substantial GPU memory; 14B models may suffice for DimASR but underperform on extraction tasks
  - Language coverage vs. data quality: Machine translation (Tatar, Ukrainian) enables coverage but introduces noise; native speaker correction mitigates but doesn't eliminate structural transfer issues
  - Aspect category granularity vs. classification accuracy: Laptop domain has 148 categories (long-tailed) vs. restaurant's 18; finer categories increase DimASQP difficulty

- Failure signatures:
  - DimASR: Grid-like VA output distribution (zero-shot tokenization artifact); systematic arousal underestimation in finance domain; RMSE > 2.0 suggests scale calibration failure
  - DimASTE: High recall but low precision → over-extraction of aspect/opinion spans; cF1 << standard F1 → VA prediction dominates error
  - DimASQP: Category confusion for long-tailed labels; performance drops 50%+ from DimASTE → aspect category classification is the bottleneck
  - Low-resource languages: Fine-tuning improves high-resource languages but not Tatar → structural pattern transfer failure

- First 3 experiments:
  1. Baseline calibration on eng-rest DimASR: Run zero-shot GPT-5 mini with 1/8/32 shots; compare VA distribution against gold. Confirm one-shot improves scale calibration (per Figure 5). Target RMSE < 2.5
  2. Fine-tuning scale comparison on DimASTE: Train Qwen-3 14B vs. Llama-3.3 70B on eng-rest + zho-rest. Measure cF1 decomposition (cPrecision vs. cRecall) to diagnose extraction vs. VA prediction bottlenecks. Expect 70B to show ~0.1 cF1 improvement
  3. Low-resource language diagnostic: Fine-tune on rus-rest → evaluate on tat-rest and ukr-rest. Compare (a) directly fine-tuned on translated data vs. (b) cross-lingual transfer from Russian. Hypothesis: Direct fine-tuning underperforms due to translation artifacts; transfer may help if structural patterns align

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does measurement invariance hold across the multilingual DimABSA datasets given potential cultural differences in valence-arousal interpretation?
- Basis in paper: [explicit] The "Limitations" section explicitly calls for expanding language coverage and "testing measurement invariance" as important directions for future work
- Why unresolved: While native speakers annotated data locally, the paper acknowledges that interpretations of valence and arousal can vary across cultures, potentially affecting cross-lingual comparability
- Evidence to resolve: A statistical analysis (e.g., Multi-Group Confirmatory Factor Analysis) comparing the psychometric properties of the VA scales across the six languages

### Open Question 2
- Question: Can architectural adaptations overcome the "constraints of token-based VA prediction" identified in the DimASR task?
- Basis in paper: [inferred] The conclusion lists "constraints of token-based VA prediction" as a key limitation, noting that zero-shot prompting produces grid-like distributions rather than continuous values
- Why unresolved: Current LLM baselines rely on discrete token generation, which struggles to model the smooth, continuous nature of the valence-arousal space
- Evidence to resolve: Benchmarking models equipped with continuous regression heads against the provided token-based LLM baselines on the DimASR subtask

### Open Question 3
- Question: What specific cross-lingual transfer strategies are most effective for hybrid extraction-regression tasks in extremely low-resource languages like Tatar?
- Basis in paper: [inferred] The results show that despite fine-tuning, performance on Tatar remains significantly lower than other languages, indicating persistent difficulties with low-resource structural patterns
- Why unresolved: The paper demonstrates that fine-tuning large models helps, but it does not close the performance gap for languages likely underrepresented in pre-training data
- Evidence to resolve: Experiments applying advanced cross-lingual transfer techniques (e.g., translate-train, adversarial adaptation) specifically to the Tatar DimASTE and DimASQP datasets

## Limitations

- Annotation quality and stability concerns due to high arousal RMSE values (>2.0) and need for extensive manual correction in translated datasets
- Persistent performance gaps for low-resource languages like Tatar suggest structural pattern transfer limitations
- Large model requirements (70B-120B parameters) create practical deployment challenges and may indicate task formulation issues

## Confidence

**High Confidence**: The core claim that continuous VA scores capture sentiment nuances missed by categorical labels is well-supported by the dataset's examples and theoretical framework of dimensional emotion models.

**Medium Confidence**: The claim about low-resource language performance gaps is supported by experimental results but confounded by translation quality issues. The superiority of fine-tuned models over zero-shot approaches is demonstrated but may be sensitive to prompt engineering quality.

**Low Confidence**: The claim that VA annotation can be reliably performed by humans across all domains and languages is undermined by high arousal RMSE values and extensive manual correction needs. The dataset's coverage of long-tailed aspect categories may create evaluation artifacts.

## Next Checks

1. **Cross-Lingual Transfer Experiment**: Fine-tune a model on Russian restaurant data (high-resource) and evaluate on both Tatar and Ukrainian restaurant data. Compare this against direct fine-tuning on translated Tatar/Ukrainian data to isolate whether structural pattern transfer or translation quality drives the performance gap.

2. **Annotation Reliability Study**: Conduct a second round of annotation on 100 randomly selected instances from each language-domain pair, focusing on instances with high arousal RMSE. Calculate inter-annotator agreement and identify whether specific domains or languages show systematic disagreement patterns that could invalidate VA scores.

3. **Alternative Architecture Evaluation**: Implement a specialized dual-encoder architecture with separate heads for (1) aspect/opinion span extraction and (2) VA regression. Compare its performance against the 70B fine-tuned model on DimASTE to determine whether the large model requirement is intrinsic to the task or an artifact of the general-purpose architecture used.