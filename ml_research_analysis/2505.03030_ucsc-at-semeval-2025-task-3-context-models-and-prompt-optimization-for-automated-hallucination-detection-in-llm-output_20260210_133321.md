---
ver: rpa2
title: 'UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated
  Hallucination Detection in LLM Output'
arxiv_id: '2505.03030'
source_url: https://arxiv.org/abs/2505.03030
tags:
- context
- answer
- system
- text
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The UCSC system achieves the highest overall performance in the
  SemEval-2025 Task 3, ranking 1 in average position across 14 languages for multilingual
  hallucination detection. The approach uses a three-stage pipeline: context retrieval
  from external sources, detection of false content in LLM outputs, and mapping errors
  back to text spans.'
---

# UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output

## Quick Facts
- arXiv ID: 2505.03030
- Source URL: https://arxiv.org/abs/2505.03030
- Reference count: 14
- UCSC system achieves highest overall performance in SemEval-2025 Task 3, ranking #1 in average position across 14 languages for multilingual hallucination detection

## Executive Summary
UCSC introduces a three-stage pipeline for detecting hallucinations in LLM outputs that combines context retrieval, detection of false content, and span mapping. The system achieves state-of-the-art performance by leveraging retrieval-augmented generation, automated prompt optimization via MiPROv2, and ensemble methods across multiple detection systems. Key findings include that with good external context, simple prompting-based methods can outperform human annotators, and that prompt optimization provides consistent improvements across different detection approaches.

## Method Summary
The approach uses a three-stage pipeline: (1) context retrieval from external sources using search APIs, (2) detection of false content in LLM outputs using detection models like GPT-4o or o1, and (3) mapping errors back to text spans using methods like substring matching or edit distance. The system employs MiPROv2 for automatic prompt optimization, which uses Bayesian search to iteratively refine prompts on validation data. A multi-system combination aggregates predictions from multiple detection pipelines, treating each as an independent annotator to produce robust soft labels.

## Key Results
- Ranks #1 in average position across 14 languages for multilingual hallucination detection
- Achieves top-two positions for 11 of 14 languages on Intersection-over-Union (IoU) and 10 of 14 on Spearman correlation (Corr)
- Shows significant performance jump from IoU 0.44 (no context) to 0.56 (with context)
- System combination improves Corr by 5% but degrades IoU by 5%
- Best single-system performance: DeepSeek-R1 without optimization (0.59 IoU, 0.60 Corr avg)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Retrieving external, grounded context enables a verification model to outperform human annotators
- **Mechanism**: The pipeline retrieves relevant documents from external sources based on the question, then uses a separate LLM instance to identify segments of the original answer not supported by the provided context
- **Core assumption**: The external knowledge source is authoritative, up-to-date, and contains necessary verification information
- **Evidence anchors**: IoU improves from 0.44 to 0.56 with context; external validation through "REFIND" corpus
- **Break condition**: Fails if external search returns irrelevant, outdated, or hallucinated content

### Mechanism 2
- **Claim**: Systematic optimization of detection prompts improves reliability of span extraction
- **Mechanism**: MiPROv2 uses Bayesian search to iteratively propose, evaluate, and refine prompt candidates on validation data
- **Core assumption**: The space of possible prompts contains significantly better configurations than initial design
- **Evidence anchors**: Test IoU improves from 0.55 (unoptimized) to 0.60-0.61 (optimized); Section 3.4 describes the optimization process
- **Break condition**: May fail if optimizer overfits to validation set or underlying model cannot follow complex instructions

### Mechanism 3
- **Claim**: Aggregating predictions from diverse detection systems produces superior soft labels by approximating human annotation variance
- **Mechanism**: Treats each detection model as an independent annotator, calculating the proportion of systems agreeing a span is hallucinated
- **Core assumption**: Errors and biases of individual systems are sufficiently uncorrelated
- **Evidence anchors**: Multi-System Combination achieves highest Corr (0.65) and IoU (0.61); "MSA at SemEval-2025 Task 3" mentions "LLM Ensemble Verification"
- **Break condition**: Won't improve results if component systems are too similar or share systematic bias

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here**: Required to understand how the system decouples answer generation from knowledge used for verification
  - **Quick check question**: Does the LLM generate the answer and context, or receive one to judge the other?

- **Concept: Intersection-over-Union (IoU)**
  - **Why needed here**: Primary metric that penalizes incorrect span location even with correct detection
  - **Quick check question**: If a system predicts "2008 Summer Olympics" but ground-truth is "the 2008 Summer Olympics in Beijing," will IoU be 1.0? Why or why not?

- **Concept: Bayesian Optimization**
  - **Why needed here**: Engine behind MiPROv2 prompt optimizer, explains efficient search of vast prompt space
  - **Quick check question**: How does Bayesian optimization determine next prompt to test, and how does this differ from grid search?

## Architecture Onboarding

- **Component map**: Context Retrieval (Perplexity Sonar Pro API) -> Hallucination Detection (GPT-4o/o1) -> Span Mapping (substring match/edit distance) -> Prompt Optimizer (MiPROv2) -> System Combiner

- **Critical path**: Context Retrieval from Questions -> Direct Text Extraction (with GPT-4o) -> Substring Match provides best baseline

- **Design tradeoffs**:
  - Simplicity vs. Complexity: Direct Text Extraction > Knowledge Graph Verification
  - Cost vs. Performance: o1 (expensive, competitive) vs GPT-4o (efficient, effective)
  - Language Strategy: Original language retrieval slightly better than translation to English

- **Failure signatures**:
  - Low IoU on Chinese (0.46) indicates model familiarity issues
  - Knowledge Graph pipeline fails due to unreliable fact-to-span mapping
  - System accuracy fundamentally limited by relevance of retrieved context

- **First 3 experiments**:
  1. Ablate context: Run Direct Text Extraction with and without search API context, measure IoU drop
  2. Compare detection methods: Implement Direct Text Extraction vs Minimal Cost Revision, evaluate IoU and Corr
  3. Test prompt optimization: Apply MiPROv2 to Direct Text Extraction prompt on validation set, compare pre/post IoU

## Open Questions the Paper Calls Out

- **Open Question 1**: Can detection systems achieve comparable performance with lower-cost models and reduced context retrieval requirements? (Cost-effectiveness of premium models)
- **Open Question 2**: What annotation framework could reduce observed variance in human span-level hallucination labels? (Annotation protocol improvements)
- **Open Question 3**: Why does fact-to-span mapping degrade knowledge graph verification, and can structured verification be made reliable? (Mapping error diagnosis)
- **Open Question 4**: Why does system combination improve correlation while degrading IoU, and can both metrics be optimized simultaneously? (Trade-off explanation)

## Limitations

- Dataset dependency on Mu-SHROOM quality and annotation process without inter-annotator agreement analysis
- Knowledge base limitations constrain retrieval-augmented approach's effectiveness for recent/specialized topics
- Optimization generalizability uncertain for different domains, question types, or updated models

## Confidence

**High Confidence**: Three-stage pipeline architecture well-specified and reproducible; clear performance rankings between detection methods
**Medium Confidence**: Better-than-human accuracy claim depends on specific dataset and evaluation metrics; may not hold under domain shifts
**Low Confidence**: Prompt optimization improvements based on limited validation experiments; potential overfitting concerns

## Next Checks

1. **Ablation Study on Knowledge Base Quality**: Systematically vary retrieved context quality to quantify causal contribution to detection performance
2. **Cross-Domain Generalization Test**: Apply optimized pipeline to different domains (technical, medical, current events) to assess transferability
3. **Human Baseline Comparison in Realistic Settings**: Compare human annotators with/without same retrieval API access, measuring accuracy and time efficiency under realistic conditions