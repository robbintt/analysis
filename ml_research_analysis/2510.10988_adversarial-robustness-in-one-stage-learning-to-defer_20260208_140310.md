---
ver: rpa2
title: Adversarial Robustness in One-Stage Learning-to-Defer
arxiv_id: '2510.10988'
source_url: https://arxiv.org/abs/2510.10988
tags:
- adversarial
- loss
- surrogate
- deferral
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses adversarial robustness in one-stage learning-to-defer
  (L2D) systems, where a predictor and allocation policy are trained jointly. Unlike
  prior two-stage analyses, the authors formalize untargeted and targeted attacks
  that can manipulate both prediction and deferral decisions.
---

# Adversarial Robustness in One-Stage Learning-to-Defer

## Quick Facts
- arXiv ID: 2510.10988
- Source URL: https://arxiv.org/abs/2510.10988
- Reference count: 40
- One-stage L2D system jointly learns predictor and allocation policy with cost-sensitive adversarial surrogate losses

## Executive Summary
This paper addresses adversarial robustness in one-stage learning-to-defer (L2D) systems where a predictor and allocation policy are trained jointly. Unlike prior two-stage analyses, the authors formalize untargeted and targeted attacks that can manipulate both prediction and deferral decisions. They propose cost-sensitive adversarial surrogate losses with tractable relaxations and establish theoretical guarantees including H-, (R,F)-, and Bayes consistency. Empirically, their method (RERM-C for classification, RERM-R for regression) improves robustness on CIFAR-10 and DermaMNIST, achieving 24-49% higher accuracy under attacks while preserving clean performance. On regression tasks like Communities and Crime, it reduces RMSE by 37% under targeted attacks and maintains lower deferral loss, indicating more reliable and efficient expert consultation.

## Method Summary
The method employs cost-sensitive adversarial surrogate losses with smooth convex relaxations for tractable optimization. For classification, RERM-C uses a margin-based surrogate with outcome-specific adversarial perturbations weighted by shifted costs (μ_j = α_j·1{error} + β_j). The smooth relaxation replaces non-convex margin transformations with scaled cross-entropy plus margin-difference regularization. For regression, RERM-R jointly optimizes predictor f and rejector r networks with analogous cost-sensitive adversarial surrogates. Both methods use PGD for inner maximization and AdamW for optimization, with consistency guarantees ensuring asymptotic convergence to Bayes-optimal robust deferral policies.

## Key Results
- RERM-C achieves 24-49% higher accuracy under targeted attacks on CIFAR-10 while maintaining 82.67% clean accuracy
- On Communities and Crime regression, RERM-R reduces RMSE by 37% under targeted attacks compared to non-robust baselines
- DermaMNIST shows 13.76% accuracy improvement under targeted attack-9 while maintaining 97.45% clean accuracy
- Lower deferral loss indicates more efficient expert consultation without sacrificing robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cost-sensitive adversarial surrogate losses enable joint robustness of prediction and deferral decisions.
- Mechanism: The surrogate loss re-weights outcome-specific adversarial perturbations by shifted costs μ_j that encode both misclassification penalties (α_j) and consultation costs (β_j). This forces the model to learn decision boundaries that remain stable under worst-case perturbations for each possible action (predict class k or defer to expert j).
- Core assumption: The hypothesis class H is symmetric and locally ρ-consistent (closed under action permutations and admits margin-stable functions within perturbation balls).
- Evidence anchors:
  - [abstract] "propose cost-sensitive adversarial surrogate losses with tractable relaxations"
  - [Section 5.1.1, Definition 6] Formal definition of eΦc,u_def with outcome-specific adversarial examples x'_j and cost weights μ_i
  - [corpus] Related work on two-stage L2D robustness (Montreuil et al., 2025a) addresses only fixed experts; one-stage joint optimization is novel.
- Break condition: If costs μ_j are misspecified (e.g., β_j values poorly calibrated to actual consultation overhead), the surrogate may over-defer or under-defer regardless of robustness gains.

### Mechanism 2
- Claim: Smooth convex relaxation of non-convex margin losses preserves consistency guarantees while enabling practical optimization.
- Mechanism: Replace the ρ-margin transformation Ψ_ρ(v) = min{max(0, 1−v/ρ), 1} with a smooth upper bound combining scaled cross-entropy Φ^u_cls(h(x)/ρ, j) and a regularization term κ·sup||Δh(x'_j,j)−Δh(x,j)||² that penalizes margin instability under perturbation. This yields convex objectives amenable to gradient-based ERM.
- Core assumption: Assumption: Smoothness parameter κ ≥ √(|A|−1)/ρ suffices to upper-bound the original non-convex surrogate (proven in Lemma 8).
- Evidence anchors:
  - [Section 5.1.2, Lemma 8] Derivation of eΦ^u_cls,s with margin-difference regularization
  - [Corollary 10] Guarantees that smooth surrogate inherits H-consistency from original
  - [corpus] Prior adversarial training (Awasthi et al., 2023; Mao et al., 2023b) uses similar smooth relaxations for standard classification, but deferral-specific cost weighting is new.
- Break condition: If regularization weight κ is too low, optimization may converge to poor local minima; if too high, clean accuracy degrades from over-conservative margins.

### Mechanism 3
- Claim: Consistency bounds ensure surrogate minimization recovers Bayes-optimal robust deferral policies.
- Mechanism: The proofs establish calibration gap inequalities showing that excess surrogate risk controls excess true adversarial deferral risk up to multiplicative constants Ψ_u(1) (classification) or Γ_u(1) (regression). Minimizability gaps vanish under realizability, guaranteeing asymptotic Bayes-optimality.
- Core assumption: The hypothesis class admits functions achieving the Bayes-optimal action within each perturbation ball (local ρ-consistency).
- Evidence anchors:
  - [Theorem 7] H-consistency bound: E[eℓ^c_def(h)] − E^B[eℓ^c_def(H)] ≤ Ψ_u(1)·(surrogate excess risk + gap)
  - [Theorem 15] (R,F)-consistency for regression with joint predictor-rejector optimization
  - [corpus] No prior work establishes consistency guarantees for adversarial one-stage L2D; two-stage analysis (Montreuil et al., 2025a) is simpler since allocation policy alone is learned.
- Break condition: If hypothesis class is too restrictive (e.g., linear models on complex data), minimizability gap remains positive and guarantees weaken to non-asymptotic bounds only.

## Foundational Learning

- Concept: **Score-based Learning-to-Defer formulation**
  - Why needed here: One-stage L2D unifies prediction and deferral via a shared score function h: X → R^(K+J) where first K outputs are class scores and last J are expert scores. Decision is argmax over augmented action space.
  - Quick check question: Given h(x) = [0.3, 0.5, 0.2, 0.8] for K=3 classes and J=1 expert, what action does the system take? (Answer: Defer to expert, since 0.8 > max(0.3, 0.5, 0.2))

- Concept: **Adversarial perturbation balls and worst-case surrogates**
  - Why needed here: Robustness requires optimizing over B_p(x,γ) = {x' : ||x'−x||_p ≤ γ}. The sup over x'_j ∈ B_p(x,γ) in surrogate losses captures worst-case outcomes.
  - Quick check question: Why must each outcome j have its own adversarial example x'_j rather than a single perturbation for all outcomes? (Answer: Different outcomes may be achievable by different perturbation directions; maximizing loss for one outcome differs from maximizing for another.)

- Concept: **Consistency and Bayes-optimality in surrogate loss theory**
  - Why needed here: Surrogate losses must be H-consistent or (R,F)-consistent so that empirical minimization converges to the Bayes-optimal policy as data increases. Without this, minimizing surrogate may not reduce true deferral loss.
  - Quick check question: What does the minimizability gap U_Φ(H) represent, and when does it vanish? (Answer: Gap between best achievable excess risk in H and pointwise minimum; vanishes when H = H_all or under realizability.)

## Architecture Onboarding

- Component map:
  - **Predictor h (classification)**: Neural network outputting K+J scores; jointly optimized for classification accuracy and deferral cost
  - **Predictor f + Rejector r (regression)**: Separate networks; f outputs real-valued predictions, r outputs J+1 allocation scores
  - **Expert models m_1,...,m_J**: Fixed offline models providing predictions on training data; their accuracies determine deferral costs
  - **Adversarial perturbation module**: PGD attacker generating x'_j for each outcome during training

- Critical path:
  1. Collect training samples with expert predictions: S_n = {(x_i, z_i, m_i)} where m_i = (m_{i,1},...,m_{i,J})
  2. Compute cost weights: μ_j = α_j·1{error} + β_j for each outcome
  3. For each mini-batch, generate |A| adversarial examples via T-step PGD per outcome
  4. Evaluate smooth surrogate loss eΦ^u_cls,s on all perturbed inputs
  5. Backpropagate combined loss with regularization η·Ω(h)

- Design tradeoffs:
  - **Clean vs. robust accuracy**: Higher robustness (larger γ, κ) reduces clean performance; Tables 1-2 show 5-7% clean accuracy drop
  - **Expert cost calibration**: β_j too low causes over-deferral; too high underutilizes experts
  - **PGD steps T**: More steps improve attack strength but increase training cost by factor |A|·T (Proposition 22: n(1+|A|T) forward/backward passes per epoch)

- Failure signatures:
  - Deferral rate collapses to 0% or 100%: cost weights or κ misconfigured
  - High clean error but low robust error: regularization dominates, model underfits
  - Training diverges: learning rate too high relative to κ; margin regularization creates sharp gradients

- First 3 experiments:
  1. **Sanity check on synthetic data**: Create 2D classification with 2 classes + 1 expert; verify clean accuracy >90% and robust accuracy improves vs. non-robust baseline under PGD attack with γ=0.1
  2. **Expert cost sensitivity**: Vary β_j ∈ {0.01, 0.05, 0.1, 0.2} on CIFAR-10 subset; plot deferral rate vs. robust accuracy to identify calibration sweet spot
  3. **Ablation on smoothness κ**: Compare κ ∈ {0, √(K+J-1)/ρ, 2·√(K+J-1)/ρ} to quantify regularization impact on optimization convergence speed and final robust accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the finite-sample complexity bounds for the proposed adversarial surrogate losses, and how do they scale with the number of experts J?
- Basis in paper: [inferred] The paper establishes asymptotic consistency guarantees (H-consistency, (R,F)-consistency, Bayes consistency) but does not derive finite-sample bounds that quantify how many training samples are needed to achieve ε-approximation to the Bayes-optimal robust policy.
- Why unresolved: The minimizability gap U(H) and consistency bounds are distribution-dependent and asymptotic; no explicit sample complexity analysis is provided.
- What evidence would resolve it: Derivation of sample complexity bounds showing dependence on n, J, K, and the Rademacher complexity of H, plus empirical verification of convergence rates.

### Open Question 2
- Question: How does the robustness-accuracy trade-off scale when deploying the proposed method on larger model architectures (e.g., ResNet-50, transformers)?
- Basis in paper: [inferred] Experiments use ResNet-4 and ResNet-16 on CIFAR-10/DermaMNIST, and MLP/linear models on tabular regression. The authors do not evaluate scalability to larger-scale models or datasets.
- Why unresolved: Computational complexity analysis (Proposition 22) shows O(n|A|T) network traversals, which may become prohibitive for large models with many experts.
- What evidence would resolve it: Empirical evaluation on larger architectures (e.g., ResNet-50, Vision Transformers) and datasets (ImageNet), reporting both robustness metrics and training/inference costs.

### Open Question 3
- Question: Can the adversarial robustness framework be extended to settings where experts are also trainable (co-learning) rather than fixed?
- Basis in paper: [inferred] The paper assumes "J fixed experts" (Section 3) whose predictions are conditioned on (x, z). The one-stage formulation jointly learns predictor and allocation, but experts remain static.
- Why unresolved: The surrogate losses and consistency proofs rely on fixed expert costs; extending to trainable experts would require analyzing a joint optimization problem with coupled consistency guarantees.
- What evidence would resolve it: Theoretical analysis of consistency when expert parameters are updated, and empirical comparison of fixed vs. co-trained expert scenarios.

## Limitations
- Expert Model Specification: The paper assumes pre-trained expert models are available but does not specify training procedures for the ResNet-16 experts used in CIFAR-10 (3 experts on random class subsets) or the MLPs for regression tasks.
- Empirical Scope: Experiments cover 2-4 datasets per task. Results on DermaMNIST (few classes, high baseline accuracy) may not generalize to more complex deferral scenarios.
- Hyperparameter Sensitivity: Robustness gains come at the cost of clean accuracy (5-7% drop on CIFAR-10). The regularization weight κ, cost parameters α_j and β_j, and PGD step count T are not extensively analyzed for sensitivity.

## Confidence
- **High Confidence**: Theoretical consistency guarantees (H-, (R,F)-, Bayes consistency) are well-supported by formal proofs and established surrogate loss theory.
- **Medium Confidence**: Empirical robustness gains (24-49% accuracy improvement under attacks) are demonstrated across multiple datasets, but depend on expert model quality and cost calibration.
- **Low Confidence**: Generalizability claims to safety-critical applications are asserted but not validated. The paper does not test on domains with asymmetric deferral costs or non-i.i.d. expert predictions.

## Next Checks
1. **Expert Model Fidelity**: Train the same ResNet-16 experts on CIFAR-10 class subsets (e.g., 3 classes per expert) and verify their accuracy matches the assumed 85% baseline before testing RERM methods.
2. **Cost Parameter Sensitivity**: Systematically vary β_j from 0.01 to 0.2 on CIFAR-10 and plot deferral rate vs. robust accuracy to identify optimal cost calibration that balances consultation costs with robustness gains.
3. **PGD Attack Verification**: Implement PGD attacks with T=10 steps and step size α=0.003 (based on ϵ values) to confirm that the untargeted and targeted attack scenarios reproduce the stated accuracy drops for baseline and RERM models.