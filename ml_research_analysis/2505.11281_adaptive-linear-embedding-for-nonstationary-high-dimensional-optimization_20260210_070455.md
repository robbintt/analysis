---
ver: rpa2
title: Adaptive Linear Embedding for Nonstationary High-Dimensional Optimization
arxiv_id: '2505.11281'
source_url: https://arxiv.org/abs/2505.11281
tags:
- embedding
- rembo
- optimization
- random
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Adaptive Cross Embedding REMBO (SA-cREMBO),
  a novel framework that extends REMBO to support multiple random Gaussian embeddings,
  each capturing a different local subspace structure of the high-dimensional objective.
  The method jointly models an embedding index variable with the latent optimization
  variable via a product kernel in a Gaussian Process surrogate, enabling adaptive
  embedding selection conditioned on location.
---

# Adaptive Linear Embedding for Nonstationary High-Dimensional Optimization

## Quick Facts
- arXiv ID: 2505.11281
- Source URL: https://arxiv.org/abs/2505.11281
- Reference count: 16
- Primary result: Introduces SA-cREMBO, a novel framework that extends REMBO to support multiple random Gaussian embeddings, each capturing a different local subspace structure of the high-dimensional objective, enabling adaptive embedding selection conditioned on location.

## Executive Summary
This paper introduces Self-Adaptive Cross Embedding REMBO (SA-cREMBO), a novel framework that extends REMBO to support multiple random Gaussian embeddings, each capturing a different local subspace structure of the high-dimensional objective. The method jointly models an embedding index variable with the latent optimization variable via a product kernel in a Gaussian Process surrogate, enabling adaptive embedding selection conditioned on location. This approach effectively captures locally varying effective dimensionality, nonstationarity, and heteroscedasticity in the objective landscape. Theoretical analysis of the index-conditioned product kernel demonstrates expressiveness and stability, while experiments on synthetic and real-world high-dimensional benchmarks show significant advantages over traditional REMBO and other low-rank BO methods.

## Method Summary
SA-cREMBO extends REMBO by using multiple random Gaussian embeddings plus orthogonal variants, jointly modeled via a product kernel in a Gaussian Process surrogate. The method introduces an index variable governing embedding choice, modeled jointly with the latent optimization variable to enable adaptive selection among subspaces. This setup captures locally varying effective dimensionality and nonstationarity while maintaining computational efficiency through shared information across embeddings.

## Key Results
- SA-cREMBO significantly outperforms vanilla REMBO on synthetic nonstationary benchmarks (Styblinski-Tang, Hartmann6) in terms of regret reduction
- The product kernel formulation enables effective information sharing across multiple embeddings compared to independent GP models
- Orthogonal embeddings provide complementary subspace coverage, mitigating sampling bias from single random projections
- The method demonstrates practical advantages over traditional low-rank BO methods on high-dimensional design problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint modeling of multiple embeddings via a product kernel improves sample efficiency over independent GP models.
- Mechanism: The method augments the GP input space to (x, z) ∈ ℝ^d × Z where z is a discrete embedding index. A product kernel K((xi, zi), (xj, zj)) = Kx(xi, xj) · Kz(zi, zj) allows information sharing across embeddings—each evaluation contributes to a single unified surrogate rather than K fragmented models.
- Core assumption: Embeddings share exploitable structure; correlations between embeddings are learnable via Kz.
- Evidence anchors:
  - [abstract] "An index variable governs the embedding choice and is jointly modeled with the latent optimization variable via a product kernel in a Gaussian Process surrogate."
  - [section 4.1] "This setup enables the model to learn correlations across embeddings. It can emphasize embeddings that better represent the function structure while down-weighting others."
  - [corpus] Neighbor paper "Mixed-variable Bayesian optimization" (Daxberger et al.) uses discrete identifiers with GP—conceptually aligned but not identical; no direct citation comparison available.

### Mechanism 2
- Claim: Orthogonalized embeddings (via Gram-Schmidt) reduce sampling bias introduced by a single random projection.
- Mechanism: Given random matrix A, compute orthogonal(A) via QR decomposition. The orthogonal embedding explores complementary subspaces, mitigating cases where the random projection aligns poorly with the effective subspace or concentrates samples along suboptimal directions.
- Core assumption: The objective function's effective subspace is not perfectly aligned with any single random projection; orthogonal directions provide coverage value.
- Evidence anchors:
  - [section 4, Algorithm 1] "Find orthogonal of random matrix A: orthogonal_A" and "Measurement of objective function: yt+2 ← orthogonal(A) · y"
  - [section 4] "Figure 1 and Figure 2 (b), (c) illustrates how 2D and 3D examples of cREMBO operate... the added orthogonal embedding enables complementary and more efficient exploration by expanding the search into multiple subspaces."
  - [corpus] Weak direct evidence; neighbor papers focus on multi-embedding ensembles but not orthogonalization specifically.

### Mechanism 3
- Claim: The index kernel Kz enables adaptive, location-conditioned embedding selection.
- Mechanism: Kz(zi, zj) = exp(−λ²(zi − zj)²) treats embedding indices as smooth latent variables. The GP learns which embeddings yield lower predictive uncertainty or higher likelihood in different input regions, effectively performing soft selection among subspaces during acquisition optimization.
- Core assumption: Embedding relevance varies across the input domain; the smooth kernel is appropriate for discrete indices.
- Evidence anchors:
  - [section 4.1] "This formulation enables adaptive selection among multiple subspaces and mitigates the limitations of using a single projection in standard REMBO."
  - [section 4.1] Lists kernel options: Delta (no sharing), Learned Embedding Kernel, Tanimoto/Categorical—the choice affects adaptivity.
  - [corpus] No direct validation in neighbor papers; multi-task GP literature (Swersky et al.) supports structured kernels for task correlation but does not validate this specific construction.

## Foundational Learning

- Concept: **Random projection preserves subspace structure with high probability (REMBO theory)**
  - Why needed here: The entire framework rests on Theorems 1–2 (Section 2), which guarantee that a random Gaussian matrix A almost surely preserves rank and that the optimum in the effective subspace T has a preimage in the projected space.
  - Quick check question: Given effective dimension de = 4 and projection dimension d = 6, can you explain why Pr(rank(Φ^T A) = de) = 1 under Gaussian A?

- Concept: **Gaussian Process product kernels for mixed-variable inputs**
  - Why needed here: SA-cREMBO's core innovation is K(x, z) = Kx(x) · Kz(z). Understanding how hyperparameters are learned jointly and how predictions propagate through the product is essential for debugging kernel design.
  - Quick check question: If Kx has lengthscale ℓ = [2.0, 0.1, 5.0] for a 3D input, what does this imply about anisotropy? How does Kz affect the joint posterior variance?

- Concept: **Acquisition optimization in mixed continuous-discrete domains**
  - Why needed here: Section 4.2 notes the acquisition function α(x, z) must be optimized over both continuous x and discrete z. Methods (enumeration, MIES, Gumbel-softmax) have different tradeoffs.
  - Quick check question: For K = 10 embeddings and d = 8, is exhaustive enumeration over z tractable? What is the cost if each continuous optimization requires 500 evaluations?

## Architecture Onboarding

- Component map:
  Embedding Layer (K random matrices {A1,...,AK} ∈ ℝ^(D×d) + orthogonal variants) -> Unified GP Surrogate (Input (x, z) with product kernel Kx · Kz) -> Acquisition Optimizer (Mixed-variable optimizer over (x, z)) -> Objective Evaluator (Expensive black-box f: ℝ^D → ℝ)

- Critical path:
  1. Initialize K embedding matrices (random + orthogonal).
  2. Generate initial design in low-d space, project via each embedding, evaluate f.
  3. Fit unified GP on augmented dataset {(x_i, z_i, y_i)}.
  4. Optimize acquisition over (x, z) jointly.
  5. Project selected x* via embedding z* to high-d, evaluate, update dataset.
  6. Repeat until budget exhausted.

- Design tradeoffs:
  - **K (number of embeddings):** Larger K improves coverage but increases acquisition optimization cost and risks conflicting data. Paper uses K=2 (random + orthogonal).
  - **Kernel choice for Kz:** Smooth exponential enables sharing but may over-smooth; delta kernel disables sharing entirely.
  - **Projection dimension d:** Must satisfy d ≥ de (effective dimension, unknown). Conservative choice (larger d) increases BO cost; aggressive choice risks missing optimum.

- Failure signatures:
  - **Boundary over-exploration:** REMBO projects many low-d points outside feasible high-d region → clamped to boundary → wasted samples. Monitor fraction of projections requiring clamping.
  - **Embedding distortion:** Distance preservation degrades for points far from origin in low-d space. Check ||Ay|| vs ||y|| ratios across samples.
  - **Conflicting embeddings:** If Kz learns near-zero correlations and posterior uncertainty spikes near embedding boundaries, embeddings may be incompatible.

- First 3 experiments:
  1. **Synthetic validation (Styblinski-Tang, D=21, d=8):** Replicate Figure 3a. Compare SA-cREMBO vs vanilla REMBO vs random search over 100 iterations. Log best observed value per iteration.
  2. **Ablation on Kz kernel:** Run SA-cREMBO with (a) smooth exponential Kz, (b) delta kernel, (c) learned embedding kernel. Compare regret curves to quantify value of information sharing.
  3. **Embedding count sensitivity:** Test K ∈ {2, 4, 8} on a nonstationary synthetic function (e.g., Hartmann6 embedded in D=21). Measure both final regret and per-iteration wall-clock time to assess scalability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific choice of kernel function over the discrete embedding index ($K_z$) impact the surrogate model's ability to generalize and transfer information across different embeddings?
- Basis in paper: [explicit] Section 4.4 identifies "Kernel Design for z" as a limitation, stating that modeling the discrete index is "nontrivial and sensitive to kernel choice."
- Why unresolved: While the paper proposes a smooth exponential kernel, it lists alternatives (Delta, Learned, Tanimoto) but does not provide an empirical comparison to demonstrate which best captures embedding correlations.
- What evidence would resolve it: An ablation study comparing optimization performance and predictive log-likelihood using different kernels for $K_z$ (e.g., Delta vs. Smooth Exponential) on the same nonstationary benchmarks.

### Open Question 2
- Question: Which acquisition optimization strategy (e.g., exhaustive enumeration vs. Mixed-Integer Evolution Strategies) provides the optimal trade-off between computational overhead and solution quality for the mixed-variable space?
- Basis in paper: [explicit] Section 4.2 and Section 4.4 highlight "Optimization Complexity," noting the requirement for "specialized mixed-variable optimization techniques" without converging on a single best practice.
- Why unresolved: The text lists several valid approaches for optimizing $\alpha(x, z)$ but leaves the specific implementation and relative efficiency of these methods as an open design choice.
- What evidence would resolve it: A comparative analysis of convergence speed and wall-clock time for acquisition optimization using the different strategies listed (enumeration, MIES, relaxation).

### Open Question 3
- Question: To what extent does the joint modeling of highly dissimilar random embeddings lead to "conflicting data" that degrades the Gaussian Process surrogate's performance?
- Basis in paper: [explicit] Section 4.4 lists "Embedding Interaction Complexity" as a limitation, warning that the model may suffer if embeddings are "too dissimilar."
- Why unresolved: It is unclear if the product kernel structure can sufficiently regularize the influence of low-quality embeddings that fail to intersect with the effective subspace, or if they actively harm the optimization.
- What evidence would resolve it: Experiments analyzing model degradation when a specific percentage of the random embeddings are intentionally designed to be irrelevant to the true objective function subspace.

## Limitations
- The method requires careful tuning of kernel parameters and embedding count, with no clear guidelines for problem-specific selection
- Computational overhead grows combinatorially with the number of embeddings due to mixed-variable acquisition optimization
- Performance may degrade when embeddings are highly dissimilar or when the effective subspace varies drastically across the input domain

## Confidence
**High confidence** in: The core mechanism of joint modeling via product kernel improves information sharing over independent GPs; orthogonal embeddings provide complementary coverage; the method outperforms vanilla REMBO on synthetic benchmarks.

**Medium confidence** in: The adaptive selection capability is practically effective (depends on Kz kernel choice and problem structure); scalability to very high dimensions (>100) with multiple embeddings.

**Low confidence** in: Performance on real-world high-dimensional problems beyond synthetic benchmarks; robustness to highly nonstationary objectives where effective dimensionality varies drastically across the domain.

## Next Checks
1. **Kernel sensitivity study**: Run SA-cREMBO with delta, smooth exponential, and learned embedding kernels on a synthetic nonstationary function; compare regret curves to quantify information-sharing value.

2. **Embedding count scalability**: Test K ∈ {2, 4, 8} on a D=50 problem; measure both final regret and per-iteration wall-clock time to assess computational scaling.

3. **Boundary effect quantification**: For a constrained D=30 problem, measure the fraction of projected points requiring boundary clamping across REMBO vs SA-cREMBO; compare resulting sample efficiency.