---
ver: rpa2
title: 'LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics'
arxiv_id: '2512.04957'
source_url: https://arxiv.org/abs/2512.04957
tags:
- novel
- poetry
- drama
- linguistic
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models can capture\
  \ deeper linguistic properties\u2014such as syntax, metaphor, and phonetic patterns\u2014\
  by introducing a multilingual genre classification dataset from Project Gutenberg\
  \ in six languages. The authors augment sentences with three explicit linguistic\
  \ features: syntactic tree depth, metaphor counts, and metre patterns, then fine-tune\
  \ BERT-based models to classify pairs of literary genres (poetry vs."
---

# LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics

## Quick Facts
- arXiv ID: 2512.04957
- Source URL: https://arxiv.org/abs/2512.04957
- Authors: Weiye Shi; Zhaowei Zhang; Shaoheng Yan; Yaodong Yang
- Reference count: 40
- Primary result: Adding explicit linguistic features (syntax, metaphor, metre) to LLMs improves multilingual literary genre classification, with metre patterns yielding the most consistent gains.

## Executive Summary
This paper investigates whether large language models can capture deeper linguistic properties—such as syntax, metaphor, and phonetic patterns—by introducing a multilingual genre classification dataset from Project Gutenberg in six languages. The authors augment sentences with three explicit linguistic features: syntactic tree depth, metaphor counts, and metre patterns, then fine-tune BERT-based models to classify pairs of literary genres (poetry vs. novel, drama vs. poetry, drama vs. novel). Results show that models improve when linguistic features are added, with metre patterns yielding the most consistent gains across tasks and models, and syntax/metaphor features providing smaller, task-specific benefits. The findings demonstrate that LLMs can effectively learn and leverage latent linguistic structures for genre classification, underscoring the value of incorporating explicit linguistic cues during model training.

## Method Summary
The authors created a multilingual dataset from Project Gutenberg with sentences from six languages (English, French, German, Spanish, Italian, Portuguese) across three literary genres. They extracted three types of linguistic features: syntactic tree depth using spaCy-Benepar, metaphor counts using Metaphor-RoBERTa, and metre patterns using PoetryTools. These features were concatenated with sentence embeddings from BERT-based models (BERT, RoBERTa, DistilBERT) and fine-tuned for binary genre classification tasks. The evaluation used F1 score on an 80/20 train-test split.

## Key Results
- Adding explicit linguistic features improved genre classification performance across all six languages and three model architectures
- Metre patterns provided the most consistent and substantial performance gains across all tasks and models
- Syntax features improved classification only when genres differed significantly in structural complexity (e.g., poetry vs. novel)
- Metaphor features showed task-specific benefits but were limited by the actual distribution of figurative language in the genres

## Why This Works (Mechanism)

### Mechanism 1: Prosodic Signal Injection for Genre Discrimination
Injecting explicit phonetic features (metre patterns) improves classification accuracy for rhythmic genres like poetry, suggesting raw text embeddings may under-represent prosodic cues. The authors concatenate a binary stress vector (stressed/unstressed syllables) to the input, providing the model with an explicit "rhythm signature" that helps separate poetry (high regularity) from prose, even when lexical content is ambiguous.

### Mechanism 2: Syntactic Structure Enrichment
Augmenting inputs with syntactic tree depth improves classification when genres differ significantly in structural complexity, but helps less when structures overlap. The authors append syntax tree depth and a depth-to-length ratio to the input. In English, where poetry and novels show linear separability in syntax space, this guides the model. In French, where distributions overlap, the signal is weaker.

### Mechanism 3: Stylistic Density Control (Metaphor)
Metaphor counts provide task-specific benefits, but their utility is limited by the actual distribution of figurative language across the genres being compared. The model is fed a scalar count of metaphorical tokens. While expected to boost poetry detection, the paper finds novels actually contain more metaphors on average in their dataset, turning this feature into a novel-detection signal rather than a poetry one.

## Foundational Learning

- **Input Augmentation / Feature Concatenation**
  - Why needed here: The paper does not use a complex fusion network; it simply appends linguistic vectors (F) to the sentence (S) via I = S ⊕ F. Understanding how to align embedding dimensions is critical for implementation.
  - Quick check question: If the BERT output is a 768-dim vector and your Metre Pattern is a 20-dim vector, how do you concatenate them for the classification head?

- **Encoder-only Architectures (BERT-family)**
  - Why needed here: The study relies on BERT, RoBERTa, and DistilBERT. These models look at tokens bidirectionally but generate a fixed representation (usually the [CLS] token) for classification.
  - Quick check question: Why might an encoder-only model miss "sequential rhythm" that a decoder or RNN might capture, necessitating explicit metre features?

- **Linear Separability in Feature Space**
  - Why needed here: The analysis relies on visualizing whether syntax features separate genres (PCA plots). If features aren't separable, the model can't use them effectively.
  - Quick check question: Why does the paper argue that Novel vs. Drama is a harder task than Poetry vs. Novel based on the "Linear Separability" of features?

## Architecture Onboarding

- **Component map**: Sentence (S) -> Feature Extractors (spaCy-Benepar, Metaphor-RoBERTa, PoetryTools) -> Concatenation Layer (S ⊕ F) -> BERT/RoBERTa Backbone -> Linear Classification Head

- **Critical path**: The Metre Pattern extraction is the highest-ROI component. The paper identifies this as the most robust feature across languages. Ensuring the PoetryTools library correctly parses syllable stress for the target languages is the primary implementation risk.

- **Design tradeoffs**: 
  - Off-the-shelf vs. End-to-end: The authors use separate tools to extract features before fine-tuning. This is modular and interpretable but suffers from error propagation (if the parser fails, the LLM sees noise).
  - BERT vs. DistilBERT: DistilBERT performed surprisingly well, offering a cheaper compute footprint with minimal performance loss.

- **Failure signatures**:
  - Novel vs. Drama: The "Failure Mode" of this architecture is distinguishing genres with high structural/lexical overlap. If features are distributed similarly, the model defaults to baseline performance.
  - Feature Redundancy: For models already pre-trained on stylistic data, adding metaphor features occasionally hurt performance, suggesting over-parameterization or confusion.

- **First 3 experiments**:
  1. Sanity Check (Baseline): Fine-tune BERT on English Poetry vs. Novel without features to replicate the high F1 score (~0.97) and establish a floor.
  2. Ablation (Metre): Add only the Metre Pattern feature to the English Poetry vs. Novel task to verify the "most consistent gain" claim (target ~1-2% improvement).
  3. Stress Test (Hard Negatives): Run Novel vs. Drama with all features enabled to confirm the paper's finding that performance does not improve, validating the "feature overlap" hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can explicit linguistic cues improve model performance on complex generation tasks (e.g., style transfer) or just classification?
- **Basis in paper:** The conclusion states a future direction is "applying the method to broader and more complex tasks."
- **Why unresolved:** The current study is restricted to binary classification tasks (Poetry vs. Novel, etc.).
- **What evidence would resolve it:** Benchmarks showing performance improvements when syntactic/metre features are integrated into generative or summarization pipelines.

### Open Question 2
- **Question:** Does refining feature extraction with neural or hybrid methods improve the utility of metaphor and syntax features?
- **Basis in paper:** The Limitations section notes that current heuristic/proxy extraction "may miss nuanced or culturally specific expressions" and suggests refining extraction methods.
- **Why unresolved:** The modest gains observed for syntax/metaphor may be artifacts of noisy automatic annotation tools (spaCy, Metaphor RoBERTa) rather than the features' irrelevance.
- **What evidence would resolve it:** Experiments utilizing higher-fidelity (e.g., human-annotated or advanced neural) feature extraction showing significant performance lifts.

### Open Question 3
- **Question:** Do these findings generalize to contemporary, non-Western, or marginalized literary traditions?
- **Basis in paper:** Authors state the dataset "overrepresent[s] canonical literature... and underrepresent contemporary, non-Western, or marginalized voices," limiting generalizability.
- **Why unresolved:** Project Gutenberg sources are historically biased toward Western public domain texts.
- **What evidence would resolve it:** Evaluation of models trained on linguistically enriched datasets comprising modern or non-Western literary corpora.

## Limitations
- The baseline F1 scores (e.g., 0.97 for English Poetry vs. Novel) appear unrealistically high for a binary classification task, suggesting potential overfitting or dataset leakage.
- The multilingual feature extraction pipeline may perform inconsistently across the six languages, particularly for morphologically rich languages.
- The metaphor detection pipeline relies on a pre-trained model that may not generalize well to archaic or literary language from Project Gutenberg.

## Confidence

- **High Confidence**: The observation that metre patterns yield consistent gains across tasks and models is well-supported by quantitative results and visual analysis.
- **Medium Confidence**: The claim that syntactic features improve classification only when genres differ in structural complexity is supported by PCA visualizations, but the analysis lacks statistical significance testing.
- **Low Confidence**: The baseline F1 scores are suspiciously high, and the lack of hyperparameter details or cross-validation undermines confidence in the absolute performance improvements reported.

## Next Checks
1. Verify that sentences were sampled uniformly across genres and languages, checking for potential class imbalance or leakage between train/test splits.
2. Test the accuracy of spaCy-Benepar and Poetry Tools on a held-out sample of non-English sentences to ensure feature extraction quality does not degrade.
3. Re-run the fine-tuning experiments with a grid search over learning rates, batch sizes, and random seeds, reporting mean and standard deviation of F1 scores across multiple runs.