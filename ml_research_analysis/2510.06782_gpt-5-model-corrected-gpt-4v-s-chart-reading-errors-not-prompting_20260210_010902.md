---
ver: rpa2
title: GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting
arxiv_id: '2510.06782'
source_url: https://arxiv.org/abs/2510.06782
tags:
- gpt-5
- average
- what
- holf
- holf-multi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluated how model architecture versus prompt engineering
  affects chart reading accuracy. Using 107 difficult questions from the CHART-6 benchmark
  where GPT-4V failed, it compared three LLMs (GPT-4V, GPT-4o, GPT-5) under three
  prompting conditions: full instructions, question-only, and GPT-5-generated chart
  descriptions.'
---

# GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting

## Quick Facts
- arXiv ID: 2510.06782
- Source URL: https://arxiv.org/abs/2510.06782
- Authors: Kaichun Yang; Jian Chen
- Reference count: 40
- Primary result: Model architecture dominates chart reading accuracy improvements, with GPT-5 showing 20-40 percentage point gains over GPT-4o, while prompt variations showed minimal effects.

## Executive Summary
This study evaluated whether model architecture or prompt engineering drives improvements in chart reading accuracy. Using 107 difficult questions from the CHART-6 benchmark where GPT-4V failed, it compared three LLMs (GPT-4V, GPT-4o, GPT-5) under three prompting conditions. Results showed GPT-5 substantially outperformed both GPT-4o and GPT-4V, with accuracy improvements of 20-40 percentage points. Prompting variations had minimal effects, with chart descriptions sometimes reducing performance. Statistical analysis confirmed model type as the dominant factor in correct responses, while prompt conditions showed no significant impact. The findings indicate that advances in model architecture, particularly agentic reasoning capabilities, drive performance improvements in visualization understanding more than prompt engineering.

## Method Summary
The study selected 107 difficult questions from the CHART-6 benchmark where GPT-4V had previously failed, spanning five sub-datasets. Three models were tested (GPT-4V, GPT-4o with specific temperature settings, and GPT-5 with default reasoning settings) under three prompt conditions: full instructions, question-only, and GPT-5-generated chart descriptions. Five repeated queries were submitted per condition, totaling 4,825 responses. Responses were cleaned and evaluated for binary correctness or numeric accuracy. Statistical analysis used Generalized Estimating Equations (GEE) for binary outcomes and Linear Mixed-Effects Models (LMM) for continuous errors, with bootstrap confidence intervals for accuracy estimates.

## Key Results
- GPT-5 consistently outperformed GPT-4o across all datasets with 20-40 percentage point performance gaps
- Prompting conditions showed minimal effects, with non-significant differences between question-only and full instruction prompts
- Chart descriptions generated by GPT-5 sometimes reduced performance, with significant negative interaction effects
- Statistical analysis confirmed model architecture as the dominant factor (β=1.4689 for GPT-5 vs. GPT-4o) while prompt conditions showed no significant impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-5's improved chart reading accuracy appears driven by architectural differences rather than prompt design.
- Mechanism: The paper describes GPT-5 as an "agentic reasoning model" with a unified system architecture that includes a "deeper reasoning model for harder problems" and a "real-time router that quickly decides which model to use." This suggests GPT-5 may allocate more computational steps to difficult visual reasoning tasks compared to GPT-4V's fixed multimodal processing pipeline.
- Core assumption: The performance difference stems from architectural capacity for extended reasoning chains, not from training data differences alone.
- Evidence anchors:
  - [abstract] "Our results show that model architecture dominates the inference accuracy: GPT-5 largely improved accuracy, while prompt variants yielded only small effects."
  - [section 4.1] "GPT-5 consistently outperformed GPT-4o across all the datasets... The average performance gap between GPT-5 and GPT-4o was approximately from 20 to 40 percentage points."
- Break condition: If GPT-5's advantage disappears on non-visual reasoning tasks, or if controlled experiments show the improvement stems primarily from training set differences, this mechanism would be weakened.

### Mechanism 2
- Claim: Prompt engineering interventions show diminishing returns as model architecture advances.
- Mechanism: Statistical analysis found prompting conditions produced small and inconsistent effects: Question Only vs. CHART-6 (β=0.0208, p=0.928) and GPT-5 Description vs. CHART-6 (β=−0.1964, p=0.358) were non-significant. This suggests advanced reasoning models may internally generate sufficient task framing without external scaffolding.
- Core assumption: The null prompting effects generalize beyond the specific CHART-6 benchmark subset tested.
- Evidence anchors:
  - [abstract] "GPT-5 largely improved accuracy, while prompt variants yielded only small effects."
  - [section 4.1] "differences between prompting conditions were relatively small, and prompts with GPT-5 self-descriptions did not improve compared to baseline or question-only prompts."
- Break condition: If prompting shows significant effects on broader chart understanding benchmarks or different task types, the claim of diminishing returns would need revision.

### Mechanism 3
- Claim: Adding descriptive context to chart images can impair rather than improve model performance.
- Mechanism: Chart descriptions generated by GPT-5 (describing visual structure without data values) showed negative effects in some conditions. A significant positive interaction was found between GPT-5 and the GPT-5 Description condition on error rates (β=0.219, p<0.001), indicating GPT-5's error increased when combined with descriptions. This may reflect interference between textual descriptions and visual processing, or description inaccuracies in complex charts.
- Core assumption: The descriptions themselves were not systematically misleading, and the negative effect stems from information interaction rather than description quality.
- Evidence anchors:
  - [section 4.1] "prompts with chart descriptions did not provide benefits over baseline or question-only prompts, and in some cases the question-only condition performed slightly better than baseline."
  - [section 4.1] "we observed a significant positive interaction between GPT-5 with GPT-5 Description condition (β=0.219, p<0.001), indicating that GPT-5's error increased when combined with GPT-5 descriptions."
- Break condition: If ablation studies show specific description components (axes-only, legend-only) improve performance, the interference mechanism would be qualified or rejected.

## Foundational Learning

- Concept: **Zero-shot evaluation methodology**
  - Why needed here: The study evaluates models without task-specific fine-tuning, measuring inherent capabilities. Understanding zero-shot evaluation is essential to interpret the 20-40 percentage point improvements as architectural rather than training-data effects.
  - Quick check question: If GPT-5 were fine-tuned on CHART-6, would the results still isolate architectural differences?

- Concept: **Statistical inference with repeated measures (GEE, LMM)**
  - Why needed here: The paper uses Generalized Estimating Equations (GEE) for binary outcomes and Linear Mixed-Effects Models (LMM) for continuous errors, clustering by item to account for repeated queries. Interpreting log-odds coefficients (β=1.4689 for GPT-5 vs. GPT-4o) requires understanding these methods.
  - Quick check question: Why cluster standard errors by item rather than treating each of the 535 responses (107 questions × 5 trials) as independent?

- Concept: **Bootstrap confidence intervals for accuracy**
  - Why needed here: The paper reports item-level bootstrap CIs (B=1000 replicates) to quantify uncertainty in accuracy estimates. This addresses the hierarchical structure where 5 trials per question are not independent observations.
  - Quick check question: If bootstrap CIs for GPT-5 and GPT-4o overlap on some datasets, does that mean the difference is not statistically significant?

## Architecture Onboarding

- Component map: Input layer (Chart image + question text + optional description) -> Model variants (GPT-4V, GPT-4o, GPT-5) -> Output processing (Clean auxiliary text -> extract float/string -> normalize to answer format -> NaN for invalid multiple-choice) -> Evaluation metrics (Binary correctness, relaxed accuracy, LRAE)

- Critical path:
  1. Dataset curation: Filter CHART-6 for GPT-4V failures + low accuracy across 7 other models → 107 difficult questions across 5 sub-datasets
  2. Prompt generation: Create 3 conditions per question (instruction, question-only, chart description)
  3. Inference: Submit 5 repeated queries per condition × 3 models × 107 questions = 4,825 responses
  4. Analysis: Fit GEE for binary outcomes, LMM for continuous errors, bootstrap for CIs

- Design tradeoffs:
  - **Strength vs. generalizability**: Selecting only GPT-4V failures creates a hard benchmark but limits conclusions about average-case performance
  - **Control vs. ecological validity**: Fixed temperature/routing settings enable fair comparison but may not reflect real-world usage where users optimize prompts per model
  - **Description constraints**: Excluding data values from descriptions prevents information leakage but may create descriptions that are uninformative or misleading about chart complexity

- Failure signatures:
  - **GPT-5 precision failure**: On one GGR question asking "approximately" about percentages, GPT-5 answered 24 instead of 25, matching extended reference-line values more closely than ground truth—suggesting reasoning chains may optimize for internal consistency over annotation accuracy
  - **Description degradation**: On complex HOLF datasets, chart descriptions consistently reduced accuracy, particularly for GPT-5—indicating descriptions may introduce noise in multi-step reasoning tasks
  - **Model-prompt interaction**: The significant interaction term for GPT-5 × Description suggests architectural advances don't uniformly benefit from all prompting strategies

- First 3 experiments:
  1. **Description ablation**: Decompose chart descriptions into components (title-only, axes-only, legend-only, style-only) to identify which elements, if any, improve performance. This tests whether the negative description effect stems from specific information types.
  2. **Cross-model description transfer**: Generate descriptions with GPT-4o and test on GPT-5 (and vice versa) to determine if the negative interaction is model-specific or description-specific.
  3. **Difficulty stratification analysis**: Partition the 107 questions by complexity (value retrieval vs. comparison vs. aggregation vs. prediction) to test whether GPT-5's advantage scales with reasoning depth, and whether prompt sensitivity varies by task type.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do GPT-5's accuracy advantages persist across diverse chart reasoning tasks beyond the specific failure cases identified in the CHART-6 benchmark?
  - Basis in paper: The authors state that "Further testing on other datasets and reasoning tasks is necessary to generalize its performance," explicitly listing value retrieval and data interpretation tasks as examples.
  - Why unresolved: The study strictly evaluated a subset of 107 "difficult" questions where GPT-4V had previously failed, potentially biasing the results toward error correction rather than general visual literacy.
  - What evidence would resolve it: Replication of the experimental design using a randomized sample of chart questions across varying difficulty levels (easy, medium, hard) and alternative benchmarks like VLAT or HOLF.

- **Open Question 2**: Which specific structural elements of chart descriptions (e.g., axes, legend, styling) drive the observed performance degradation in GPT-5?
  - Basis in paper: The authors propose a "Mechanistic evaluation" involving an "Ablation of description components: title-only / axes-only / legend-only / style-only, to map which elements (if any) shift answers."
  - Why unresolved: The study only tested "full" descriptions against no descriptions; it did not isolate which parts of the text description caused the unexpected negative impact on GPT-5's performance.
  - What evidence would resolve it: A factorial experiment where models are tested with descriptions containing only isolated visual elements to identify which specific textual cues interfere with visual reasoning.

- **Open Question 3**: How does GPT-5's inference stability compare to GPT-4o when subjected to incremental prompt lengthening or semantic perturbations?
  - Basis in paper: The authors suggest generating "perturbation curves: for each chart, estimate each model's consistency... and tolerance to incremental description length."
  - Why unresolved: The current study relied on static comparisons between three distinct prompt types without measuring sensitivity to gradual noise or verbosity.
  - What evidence would resolve it: Analysis of performance variance when adding incremental, non-essential tokens to the prompt to determine if the model's reasoning degrades gracefully.

## Limitations

- The study's conclusions depend on access to the GPT-5 model (`gpt-5-2025-08-07`), which may not be publicly available for independent verification.
- The benchmark construction—selecting only GPT-4V failures—creates a deliberately difficult subset that may not represent general chart understanding performance.
- Chart descriptions exclude data values, potentially making them incomplete or misleading for complex visualizations.

## Confidence

- **High confidence**: Model architecture dominates prompting effects (supported by robust statistical analysis showing significant β=1.4689 for GPT-5 vs. GPT-4o across multiple datasets)
- **Medium confidence**: Prompting has minimal effects (supported by non-significant coefficients for prompt conditions, but limited to one specific benchmark subset)
- **Medium confidence**: Chart descriptions can impair performance (statistically significant interaction found, but mechanism remains unclear without decomposition analysis)

## Next Checks

1. **Description component ablation**: Test whether specific description elements (axes, legend, title, style) individually improve or degrade performance, identifying which information types cause interference.
2. **Cross-model description transfer**: Generate descriptions with GPT-4o and test on GPT-5 to determine if the negative interaction is model-specific or description-specific.
3. **Difficulty-stratified analysis**: Partition questions by reasoning complexity to test whether GPT-5's advantage scales with task difficulty and whether prompt sensitivity varies by task type.