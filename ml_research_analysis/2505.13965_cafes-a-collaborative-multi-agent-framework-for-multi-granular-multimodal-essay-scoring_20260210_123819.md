---
ver: rpa2
title: 'CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal
  Essay Scoring'
arxiv_id: '2505.13965'
source_url: https://arxiv.org/abs/2505.13965
tags:
- essay
- scoring
- arxiv
- cafes
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CAFES, a multi-agent framework for automated
  essay scoring (AES) using multimodal large language models (MLLMs). Traditional
  AES methods struggle with generalization and multimodal input, while MLLM-based
  methods can produce hallucinated justifications and misaligned scores.
---

# CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring

## Quick Facts
- arXiv ID: 2505.13965
- Source URL: https://arxiv.org/abs/2505.13965
- Authors: Jiamin Su; Yibo Yan; Zhuoran Gao; Han Zhang; Xiang Liu; Xuming Hu
- Reference count: 40
- One-line primary result: 21% relative improvement in Quadratic Weighted Kappa (QWK) vs ground truth on multimodal essay scoring

## Executive Summary
CAFES introduces a multi-agent framework for automated essay scoring that addresses the limitations of both traditional AES methods and recent multimodal large language model (MLLM) approaches. Traditional methods struggle with generalization and multimodal input, while MLLMs often produce hallucinated justifications and scores misaligned with human judgment. CAFES overcomes these issues by orchestrating three specialized agents: an Initial Scorer for trait-specific evaluations, a Feedback Pool Manager for generating evidence-grounded strengths, and a Reflective Scorer for iterative refinement. The framework demonstrates robust performance across multiple MLLMs, achieving significant QWK improvements particularly for lower-performing traits and models.

## Method Summary
CAFES employs a three-stage, two-model multi-agent architecture where a weaker "student" MLLM produces initial trait scores, and a stronger "teacher" MLLM (GPT-4o by default) provides feedback and reflection. The process begins with the Initial Scorer assigning 0-5 scores across 10 fine-grained traits (lexical, sentence, and discourse levels). The Feedback Pool Manager then generates positive, trait-specific feedback based on the essay content. Finally, the Reflective Scorer revises the initial scores using the feedback, producing JSON-formatted outputs with explicit revision decisions and justifications. The framework was tested across multiple MLLMs including InternVL2.5, Qwen2.5-VL, LLaMA-3.2-Vision, Claude-3.5-Sonnet, Gemini-2.5-Flash, and GPT-4o-mini.

## Key Results
- Achieves 21% relative improvement in Quadratic Weighted Kappa (QWK) against ground truth
- Particularly effective for grammatical and lexical diversity traits
- Ablation studies confirm the critical importance of the feedback pool and teacher-student collaboration
- Robust performance across different MLLM sizes, with larger models showing higher baseline QWK but still benefiting from CAFES

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured positive feedback extraction counteracts MLLMs' systematic under-scoring tendency
- Mechanism: The Feedback Pool Manager generates only positive, trait-specific feedback, which the Reflective Scorer uses to revise initial scores upward when warranted
- Core assumption: MLLMs default to overly critical scoring aligned with strict rubric interpretation rather than holistic human judgment
- Evidence anchors: Abstract mentions MLLMs can produce hallucinated justifications; Section 3.2 notes MLLMs tend to adhere to rubrics more strictly than human raters; Section 4.5 shows removing Feedback Pool Manager results in worse reflected scores

### Mechanism 2
- Claim: Teacher-student role separation with capability asymmetry enables effective score refinement
- Mechanism: A weaker "student" MLLM produces initial scores; a stronger "teacher" MLLM (GPT-4o default) provides feedback and reflection
- Core assumption: The teacher model has demonstrably better alignment with human judgment and reasoning capacity
- Evidence anchors: Section 4.5 shows QWK decreases notably when both roles use the same model; Section 4.4 shows larger student MLLMs benefit from CAFES despite higher baseline QWK

### Mechanism 3
- Claim: Iterative reflection with structured output format reduces hallucination by grounding justification in evidence
- Mechanism: The Reflective Scorer outputs JSON with explicit revision decisions (True/False) and trait-specific reasons
- Core assumption: Structured output formats constrain model behavior toward more consistent, less hallucinated responses
- Evidence anchors: Figure 3 shows explicit JSON format with revision decisions; Section 4.6 case study shows score revision from 1→3 on Argument Clarity after feedback highlights overlooked strengths

## Foundational Learning

- Concept: Quadratic Weighted Kappa (QWK)
  - Why needed here: Primary evaluation metric; measures agreement between model scores and human ground truth while penalizing larger disagreements more heavily
  - Quick check question: If a model scores essays as 1,2,3 and human scores are 3,2,1 respectively, would QWK be negative?

- Concept: Multi-trait AES evaluation
  - Why needed here: CAFES evaluates 10 fine-grained traits (lexical, sentence, discourse levels) rather than holistic scores; understanding trait granularity is essential for debugging per-trait performance
  - Quick check question: Which trait category would "organizational structure" belong to—lexical, sentence, or discourse level?

- Concept: MLLM visual-textual grounding
  - Why needed here: Essays reference charts/tables; models must interpret visual data to assess "justifying persuasiveness" and related traits
  - Quick check question: If an essay misreads a bar chart's trend, which trait(s) would likely receive low scores?

## Architecture Onboarding

- Component map: Essay text (E) + Topic image (I) + Topic text (T) + Rubrics (R) → Initial Scorer → Feedback Pool Manager → Reflective Scorer → Final scores with justifications
- Critical path: Initial Scorer → Feedback Pool Manager → Reflective Scorer. The feedback pool is the integration point; without it, reflection degrades scores (ablation confirmed)
- Design tradeoffs:
  - Positive-only feedback vs. balanced feedback: Paper chooses positive-only to counter under-scoring; risks over-correction on weak essays
  - Single teacher (GPT-4o) vs. configurable: Standardization enables fair comparison; limits flexibility for cost/latency optimization
  - 10-trait granularity vs. holistic: Finer detail increases computational cost but provides actionable feedback
- Failure signatures:
  - QWK drops after reflection → Check if Feedback Pool contains negative feedback (mechanism break)
  - Large variance across runs → Check JSON parsing reliability, prompt consistency
  - Strong models show minimal improvement → Expected behavior; diminishing returns near ceiling
  - Multi-image essays score lower → Conservative scoring under complexity; larger improvement margin expected
- First 3 experiments:
  1. Reproduce Table 2 baseline with single student MLLM (e.g., InternVL2.5-8B) on 50 essays; verify initial QWK before implementing full pipeline
  2. Ablate Feedback Pool Manager; confirm QWK drop matches paper's finding (~negative improvement)
  3. Test teacher-student swap (use weaker model as teacher, stronger as student); quantify performance degradation to validate asymmetry assumption

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CAFES framework generalize to scoring essays based on dynamic or complex visual inputs, such as film frames, rather than static charts?
- **Basis in paper:** The authors note that the EssayJudge dataset "focuses mainly on chart-based topics and does not cover more complex visual inputs such as film frames," and list evaluating "a broader range of multimodal essay types" as future work
- **Why unresolved:** The current evaluation is restricted to static images (charts, maps, tables), leaving the framework's ability to handle temporal or high-complexity visual data untested
- **What evidence would resolve it:** Testing CAFES on a multimodal essay dataset containing video clips or complex infographics and comparing performance against the static-image baseline

### Open Question 2
- **Question:** How can the reflection mechanism be enhanced to strictly ground evidence and eliminate residual hallucination-induced scoring errors?
- **Basis in paper:** The limitations section states that while reflection suppresses some hallucinations, "hallucination-induced scoring errors still occur," and the authors aim to "further strengthen evidence grounding"
- **Why unresolved:** The current Reflective Scorer relies on reasoning capabilities that are still susceptible to fabrication, meaning scores can be based on non-existent essay features
- **What evidence would resolve it:** A qualitative and quantitative analysis of scoring errors before and after integrating a retrieval-augmented generation (RAG) or explicit citation mechanism into the Reflective Scorer

### Open Question 3
- **Question:** Is the performance gain dependent on the use of a high-capacity proprietary model (GPT-4o) as the teacher, or can open-source models serve as effective teachers?
- **Basis in paper:** The authors assign GPT-4o as the default teacher for all experiments and show that removing the teacher-student mechanism hurts performance. However, they do not test if a less capable, open-source model can successfully fulfill the teacher role
- **Why unresolved:** It is unclear if the 21% improvement requires the specific reasoning power of GPT-4o or if the collaborative architecture itself is the primary driver of performance
- **What evidence would resolve it:** An ablation study swapping GPT-4o with a strong open-source model (e.g., LLaMA-3.2-Vision-90B) as the teacher for various student models

### Open Question 4
- **Question:** Does the exclusive use of positive feedback in the Feedback Pool Manager risk over-correcting and inflating scores for very low-quality essays?
- **Basis in paper:** The Feedback Pool Manager is designed to generate only "positive feedback" to counter MLLMs' tendency to assign lower scores. This assumes there are always valid strengths to find, which may bias the revision process upwards even for poor essays
- **Why unresolved:** The paper evaluates alignment with ground truth (QWK) but does not isolate the effect of the "positive-only" constraint on the scoring distribution of failing essays
- **What evidence would resolve it:** Comparing the score distributions of low-scoring essays (ground truth < 2) processed by the standard CAFES framework against a version allowed to generate negative feedback

## Limitations
- Data access dependency: Reproduction requires the EssayJudge dataset, which is not publicly available, limiting independent validation
- Teacher model rigidity: GPT-4o is used as the fixed teacher, making performance highly dependent on its availability and alignment quality
- Potential over-correction bias: Positive-only feedback may inflate scores for weak essays, trading precision for recall in lower-performing cases

## Confidence
- **High confidence**: QWK improvement mechanism via teacher-student asymmetry and feedback-driven reflection, supported by ablation studies and case examples
- **Medium confidence**: Generalizability of CAFES across diverse MLLMs, limited by dataset size and single-teacher design
- **Low confidence**: Long-term scalability of the approach without further human oversight, especially for negative feedback scenarios

## Next Checks
1. Ablate the Feedback Pool Manager and verify QWK drops significantly to confirm its contribution
2. Test teacher-student capability swap using a stronger model as student and weaker as teacher to validate the asymmetry assumption
3. Test CAFES on a different AES dataset (e.g., ASAP) to assess robustness beyond EssayJudge