---
ver: rpa2
title: Learning Guarantee of Reward Modeling Using Deep Neural Networks
arxiv_id: '2505.06601'
source_url: https://arxiv.org/abs/2505.06601
tags:
- reward
- comparison
- function
- learning
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides non-asymptotic regret bounds for deep reward
  modeling in reinforcement learning from human feedback (RLHF). The authors establish
  a novel margin-type condition that quantifies clear human preferences in pairwise
  comparison data, enabling sharper regret bounds that depend on both the neural network
  architecture and the quality of preference data.
---

# Learning Guarantee of Reward Modeling Using Deep Neural Networks

## Quick Facts
- arXiv ID: 2505.06601
- Source URL: https://arxiv.org/abs/2505.06601
- Authors: Yuanhang Luo; Yeheng Ge; Ruijian Han; Guohao Shen
- Reference count: 40
- This paper provides non-asymptotic regret bounds for deep reward modeling in reinforcement learning from human feedback (RLHF).

## Executive Summary
This paper establishes theoretical foundations for reward modeling in reinforcement learning from human feedback (RLHF) using deep neural networks. The authors introduce a margin-type condition that quantifies clear human preferences in pairwise comparison data, enabling sharper regret bounds that depend on both the neural network architecture and the quality of preference data. The analysis shows that properly configured deep neural networks can achieve regret bounds of O(N^−β/[(d+2β)(3−2α)]), where N is the sample size, β is the smoothness parameter of the reward function, d is the input dimension, and α quantifies the clarity of human preferences.

## Method Summary
The method uses pairwise comparison data to learn reward functions via maximum likelihood estimation (MLE) with deep neural networks. The Bradley-Terry model connects human preferences to reward differences, while the theoretical analysis establishes regret bounds based on network architecture and preference data quality. The approach requires specifying network depth D and width W based on sample size N and input dimension d, with optimal scaling D=O(√N) and W=O(d^(⌊β⌋+1)). The analysis depends on three key assumptions: a margin condition for preference clarity, smoothness of the true reward function, and sufficient coverage of the comparison graph.

## Key Results
- Clear human preferences, quantified by a margin condition, accelerate regret convergence from O(∥r−r*∥²)^1/3 to O(∥r−r*∥²)^1/(3−2α)
- Optimal network depth scales as D=O(√N) and width as W=O(d^(⌊β⌋+1)), balancing approximation error against stochastic error
- Comparison graph spectral connectivity (λ₂(Λ)>0) is necessary for reward identifiability and convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clear human preferences, quantified by a margin condition, accelerate regret convergence from O(∥r−r*∥²)^1/3 to O(∥r−r*∥²)^1/(3−2α).
- Mechanism: When the conditional winning probability of optimal actions is bounded away from 1/2, the reward gap between optimal and suboptimal actions becomes distinguishable, reducing ambiguity in policy extraction. The exponent α ∈ (0,1) captures this clarity; α→1 (noiseless case) achieves linear regret scaling.
- Core assumption: Assumption 2.1 (Margin Condition): For action pairs (πr*(s), a′), P(P(y>0|s,πr*(s),a′)−1/2≤t) ≤ c_gt^{α/(1−α)}.
- Evidence anchors:
  - [abstract]: "we introduce a margin-type condition that assumes the conditional winning probability of the optimal action in pairwise comparisons is significantly distanced from 1/2. This condition enables a sharper regret bound"
  - [section]: Theorem 2.5 vs Corollary 2.6 explicitly compares bounds with/without margin condition; Lemma 2.4 connects margin to reward structure
  - [corpus]: Weak evidence; related papers focus on preference learning methods but don't analyze margin conditions theoretically
- Break condition: α→0 (highly ambiguous preferences) degrades bound to O(∥r−r*∥²)^1/3; margin condition provides no benefit when winning probabilities cluster near 1/2.

### Mechanism 2
- Claim: Optimal network depth scales as D=O(√N) and width as W=O(d^(⌊β⌋+1)), balancing approximation error against stochastic error.
- Mechanism: Approximation error decreases as M₁M₂ increases (Proposition 3.6), while stochastic error increases with network complexity via covering number (Proposition 3.5). Theorem 3.9 optimizes this tradeoff, achieving regret O(N^{-β/[(d+2β)(3-2α)]}).
- Core assumption: Assumption 3.3 (smoothness): r*(s,a) ∈ H^β([0,1]^d, c_H) (Hölder class with smoothness β>0).
- Evidence anchors:
  - [abstract]: "regret bounds of O(N^−β/[(d+2β)(3−2α)])...demonstrating how network architecture and comparison data quality affect estimation performance"
  - [section]: Theorem 3.9 specifies exact width/depth formulas; Figure 1 shows empirical regret surface with optimal region
  - [corpus]: "A Unified Pairwise Framework for RLHF" discusses reward modeling but lacks theoretical architecture guidance
- Break condition: Over-parameterization (D≫√N) makes stochastic error dominate; under-parameterization makes approximation error dominate. Regret surface (Figure 1) shows degradation beyond optimal architecture.

### Mechanism 3
- Claim: Comparison graph spectral connectivity (λ₂(Λ)>0) is necessary for reward identifiability and convergence.
- Mechanism: The Laplacian matrix Λ captures comparison graph structure. When λ₂(Λ) is bounded away from zero, Theorem 3.8's estimation error converges. Sparse graphs (e.g., path graph with λ₂=Θ(1/|A|³)) cause error divergence even with large N.
- Core assumption: Assumption 3.7 (Data Coverage): λ₂(Λ)>κ_Λ for positive constant κ_Λ.
- Evidence anchors:
  - [section]: "If some actions are almost not queried among the data, the spectral gap 1/λ₂(Λ) diverges, and thus the regret of the MLE estimator could not converge"
  - [section]: Example A.3 demonstrates path graph with sparse connections has λ₂(Λ)=Θ(1/n), causing MLE degeneracy
  - [corpus]: No corpus papers discuss spectral properties of comparison graphs; this is a unique contribution
- Break condition: When action pairs have few comparisons (λ₂(Λ)→0), estimation error ∥r̂−r*∥² diverges regardless of sample size. Complete graphs achieve λ₂=Θ(1/|A|); path graphs fail.

## Foundational Learning

- Concept: Bradley-Terry Preference Model
  - Why needed here: Core probabilistic link between human preferences and reward differences; P(y>0|s,a₁,a₀)=exp(Δr)/(1+exp(Δr)) where Δr=r*(s,a₁)-r*(s,a₀).
  - Quick check question: Why does BT model require normalization Σ_a r(s,a)=0 for identifiability?

- Concept: Regret vs. Estimation Error
  - Why needed here: Paper optimizes regret E(r)=∫[r*(s,πr*)-r*(s,πr)]dρ_s, not L² error; low estimation error doesn't guarantee low regret without margin condition.
  - Quick check question: If ∥r̂−r*∥=0.1 everywhere, can regret still be high? Under what conditions?

- Concept: Covering Number and Network Complexity
  - Why needed here: Proposition 3.5 bounds stochastic error via covering number N(F_DNN,τ,∥·∥); understanding how D,W affect complexity is essential for architecture selection.
  - Quick check question: Why does deep network (large D) have smaller covering number growth than wide network (large W) for same parameter count?

## Architecture Onboarding

- Component map:
  Input: (s,a)∈[0,1]^d → Network: D-layer ReLU with width W → Output: r(s,a;θ)

- Critical path:
  1. Estimate smoothness β from pilot data or domain knowledge
  2. Set width W≈114(⌊β⌋+1)²d^(⌊β⌋+1) (polynomial in dimension)
  3. Set depth D≈21(⌊β⌋+1)²N^(d/(2d+4β))≈O(√N) for d≫β
  4. Verify comparison graph has λ₂(Λ)=Θ(1/|A|) (complete or star graph)
  5. Train with MLE; extract policy π̂(s)=argmax_a r̂(s,a)

- Design tradeoffs:
  - Depth vs. width: Deep networks (D~√N, W~poly(d)) use fewer parameters than wide networks for same approximation power
  - Margin vs. data: High-α (clear preferences) allows smaller N; low-α requires more samples
  - Graph structure: Complete graph (λ₂=Θ(1/|A|)) needs O(|A|²) comparisons; star graph achieves same λ₂ with O(|A|)

- Failure signatures:
  - Regret plateaus with increasing N → Check λ₂(Λ) via Laplacian eigendecomposition
  - Performance degrades with larger network → Over-parameterization; reduce D to O(√N)
  - High variance across runs → Noisy labels (low α); filter samples with P(y>0)∈[0.4,0.6]

- First 3 experiments:
  1. Architecture sweep: Fix N=2¹⁴, vary W∈[2²,2¹²] and D∈[3,13]; plot regret surface to find flat optimal region (Figure 1 replication)
  2. Noise robustness: Inject noise by resampling m% of labels with P(y>0)∼Uniform[0.4,0.6]; measure regret vs. noise level m (Figure 2 replication)
  3. Graph connectivity: Compare complete graph vs. path graph vs. star graph; verify regret correlates with λ₂(Λ)

## Open Questions the Paper Calls Out

- Question: Can the derived regret bounds be extended to trajectory-based comparisons within Markov decision processes?
  - Basis in paper: [explicit] The authors state, "Looking ahead, it would be valuable to extend our results to trajectory-based comparisons within Markov decision process frameworks."
  - Why unresolved: The current theoretical framework focuses on action-based pairwise comparisons. Trajectory-based methods require handling temporal dependencies and credit assignment across state transitions, which complicates the identification of specific action rewards.
  - What evidence would resolve it: Deriving non-asymptotic regret bounds that explicitly incorporate transition dynamics and trajectory-level preference data.

- Question: Do the theoretical guarantees for fully connected ReLU networks transfer to Transformer architectures?
  - Basis in paper: [inferred] The analysis explicitly restricts the function class $\mathcal{F}_{DNN}$ to fully connected feed-forward ReLU networks, despite RLHF being predominantly used for Transformers.
  - Why unresolved: The approximation error bounds (Proposition 3.6) rely on the specific compositional structure of ReLU networks and Hölder smoothness. Transformer attention mechanisms have different approximation properties and inductive biases.
  - What evidence would resolve it: Extending the approximation error analysis to the Transformer function class or demonstrating that the smoothness assumptions hold for attention-based models.

- Question: How do optimization dynamics and non-convex training affect the realization of these regret bounds?
  - Basis in paper: [explicit] The authors note, "The optimization aspects are beyond the scope of this study," and assume the estimator $\hat{r}$ is obtained by maximizing the empirical likelihood.
  - Why unresolved: The bounds are statistical (assuming the global optimum is found), but training deep networks involves non-convex optimization landscapes where convergence to the global maximum is not guaranteed.
  - What evidence would resolve it: Proving that gradient-based optimization algorithms can find solutions that achieve the derived statistical rates within the specified network configurations.

## Limitations

- The Hölder smoothness parameter β is not specified for the synthetic experiments, making it impossible to verify the claimed optimal network architecture without assumptions
- Training hyperparameters are unspecified, which significantly affects reproducibility
- The margin condition α quantifying preference clarity is theoretical; practical methods for estimating it from real data are not discussed
- The theoretical analysis assumes infinite data coverage and specific comparison graph structures that may not hold in practical RLHF scenarios

## Confidence

- High confidence: The theoretical framework connecting margin conditions to regret bounds is mathematically sound and well-supported by proofs
- Medium confidence: The architectural guidance (D~√N, W~poly(d)) is derived rigorously but depends on smoothness β which is unknown
- Low confidence: Practical applicability to real RLHF systems is limited by idealized assumptions about comparison data quality and graph connectivity

## Next Checks

1. Implement spectral analysis of comparison graphs to verify λ₂(Λ) bounds in practice, using real preference data to test the assumption that complete/star graphs are achievable
2. Conduct sensitivity analysis by varying the margin condition parameter α in synthetic experiments to quantify its impact on regret scaling
3. Test the architectural scaling laws with different smoothness values β by fitting synthetic reward functions with known smoothness to validate the D~√N and W~poly(d) relationships