---
ver: rpa2
title: Adapting Multilingual Models to Code-Mixed Tasks via Model Merging
arxiv_id: '2510.19782'
source_url: https://arxiv.org/abs/2510.19782
tags:
- code-mixed
- merging
- data
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates model merging as an alternative to conventional
  fine-tuning strategies for code-mixed natural language processing tasks. Starting
  from a multilingual base model, the authors perform continued pre-training on unlabeled
  code-mixed text, merge the adapted checkpoint with the base model, and then fine-tune
  on downstream task data.
---

# Adapting Multilingual Models to Code-Mixed Tasks via Model Merging

## Quick Facts
- arXiv ID: 2510.19782
- Source URL: https://arxiv.org/abs/2510.19782
- Reference count: 40
- Key outcome: Model merging consistently outperforms full fine-tuning and CPT→FT on code-mixed tasks, achieving 2-5 F1 point gains over full fine-tuning and 1-2 points over CPT→FT

## Executive Summary
This study demonstrates that model merging is an effective alternative to conventional fine-tuning for adapting multilingual models to code-mixed natural language processing tasks. The authors develop a two-stage pipeline: continued pre-training on unlabeled code-mixed text followed by merging the adapted checkpoint with the base model, then fine-tuning on downstream task data. Experiments on sentiment analysis and hate speech detection tasks in English-Hindi and English-Spanish using XLM-R and Llama-3.2-1B models show that merged models consistently outperform full fine-tuning and CPT→FT approaches. The results indicate that model merging more effectively leverages unlabeled code-mixed data and provides better transfer to low-resource language pairs compared to monolingual English baselines.

## Method Summary
The method involves three stages: (1) Continued pre-training (CPT) on unlabeled code-mixed text using MLM for encoder models (XLM-R) or causal LM for decoder models (Llama-3.2-1B), (2) Computing task vectors from CPT checkpoints and merging with base models using Task Arithmetic or TIES methods, and (3) Fine-tuning the merged models on labeled downstream task data. The merging approach preserves monolingual representations while incorporating code-mixed competence, and allows controlled integration through scaling factors. The authors compare against baselines including full fine-tuning, CPT→FT, sequential fine-tuning, and joint fine-tuning.

## Key Results
- Merged models consistently outperform full fine-tuning and CPT→FT approaches by 2-5 F1 points
- Cross-pair transfer experiments show merged checkpoints transfer more strongly to low-resource language pairs (0.65-0.68 F1 vs 0.61-0.63 for full fine-tuning)
- Zero/few-shot prompting with larger LLMs underperforms fine-tuned and merged checkpoints
- Marginal difference (±1 F1) between real and synthetic code-mixed corpora for CPT

## Why This Works (Mechanism)

### Mechanism 1: Preservation of Monolingual Representations
Model merging preserves monolingual representations while incorporating code-mixed competence. CPT alone can erode general language understanding by shifting parameters away from base capabilities. Merging (via Task Vectors or TIES) interpolates between base and adapted weights, retaining original monolingual strengths while adding code-mixed patterns. The paper states code-mixed text "contains monolingual spans from its constituent languages (L1, L2) and coexists with purely monolingual utterances, maintaining these capabilities is crucial for downstream performance."

### Mechanism 2: Task Vector Extraction
Task vectors extracted from CPT capture compact, transferable code-mixed linguistic signals. Task vectors (τ = θ_CPT - θ_base) isolate parameter changes specific to code-mixed exposure. Scaling (λ) allows controlled integration. TIES further reduces interference by trimming low-magnitude parameters and resolving sign conflicts. This yields more efficient knowledge extraction than CPT→FT which applies all parameter changes indiscriminately.

### Mechanism 3: Cross-Pair Transfer
Code-mixed checkpoints transfer more effectively to low-resource language pairs than monolingual English baselines. Code-mixed training exposes models to intra-sentential language alternation patterns, shared vocabulary, and code-mixing grammar. These structural properties partially transfer across language pairs, whereas monolingual English provides no mixing-specific signal. En-Hi trained merged models achieved 0.65-0.68 F1 on En-Ta/En-Ml vs. 0.61-0.63 for full FT from English.

## Foundational Learning

- **Task Arithmetic / Model Merging**: Core technique expressing fine-tuned weights as vectors and combining them mathematically. Quick check: Given a base model θ and fine-tuned model θ_ft, write the expression for the task vector τ.
- **Continued Pre-training (CPT)**: First stage of pipeline using MLM or causal LM objectives on unlabeled code-mixed data. Quick check: What training objective would you use for CPT on an encoder-only model vs. a decoder-only model?
- **Code-mixing in NLP**: Domain-specific challenge with unique properties like intra-sentential switching. Quick check: Why might a model trained only on monolingual English fail on English-Hindi code-mixed inputs even if Hindi is in its pre-training data?

## Architecture Onboarding

- **Component map**: Base model (XLM-R / Llama-3.2-1B) → CPT on unlabeled code-mixed corpus → θ_CPT checkpoint → Compute task vector: τ_CPT = θ_CPT - θ_base → Merge: θ_merged = θ_base + λ × τ_CPT (Task Vector) OR TIES merge → Fine-tune θ_merged on labeled downstream task data → Optional: Add τ_en from English task data for multi-source merging
- **Critical path**: CPT quality directly determines task vector signal strength; λ scaling factor (tuned on validation set) controls merge interpolation; final fine-tuning adapts merged model to specific task labels
- **Design tradeoffs**: Task Vector vs. TIES (TV is simpler; TIES handles interference better); CPT corpus choice (marginal difference between real and synthetic); merge before vs. after fine-tuning (paper merges CPT checkpoint, then fine-tunes)
- **Failure signatures**: Merged model underperforms Full FT (λ too large or CPT overfits); high variance across runs (check CPT stability, task vector magnitude, or TIES sign resolution conflicts); transfer fails to new language pair (source and target pairs may have incompatible mixing patterns)
- **First 3 experiments**: 1) Replicate TV merge with XLM-R on En-Hi sentiment (GLUECoS): Run CPT → compute τ_CPT → grid search λ ∈ {0.1, 0.3, 0.5, 0.7, 1.0} → fine-tune → compare to Full FT baseline; 2) Ablate CPT corpus: Train identical pipeline with synthetic vs. real code-mixed corpus; measure F1 variance; 3) Test cross-pair transfer: Take best En-Hi merged checkpoint, evaluate zero-shot on En-Ta test set; compare against Full FT(En-Hi) baseline

## Open Questions the Paper Calls Out

### Open Question 1
Does the effectiveness of model merging for code-mixed tasks generalize beyond sentence classification to token-level or generation tasks? The authors state experiments were restricted to sentence classification and "Future research should evaluate these methods on a broader range of tasks."

### Open Question 2
How does the model merging approach scale to Large Language Models significantly larger than 1B parameters? The authors note they did not scale to models >1B parameters due to training costs, identifying this as "an interesting avenue for future work."

### Open Question 3
Can model merging be effectively adapted for scenarios where unlabeled code-mixed data is available but no labeled task data exists? The authors identify this missing data regime, noting "We do not consider the case where unlabeled data is available, but no task-specific labeled data is present."

## Limitations

- The paper does not specify exact λ values for TV/TIES merges, requiring grid search that may not replicate reported performance
- CPT duration, learning rate schedule, and checkpoint selection criteria are unspecified
- LoRA configuration details (rank, alpha, target modules) are missing beyond learning rate
- Random seed usage and number of runs per configuration are not documented

## Confidence

- **High confidence**: Claims about model merging outperforming full fine-tuning and CPT→FT on same language pairs (XLM-R En-Hi results)
- **Medium confidence**: Cross-pair transfer claims (limited to two transfer directions with single run per configuration)
- **Low confidence**: Claims about relative performance of different merge variants (TIES vs TV) due to insufficient ablation studies

## Next Checks

1. Replicate the core XLM-R En-Hi sentiment analysis pipeline with systematic λ grid search (0.1-1.0) and compare against Full FT baseline
2. Conduct ablation study varying CPT duration (1-10 epochs) to establish sensitivity to continued pre-training length
3. Test cross-pair transfer from En-Hi to En-Ta with multiple random seeds to establish statistical significance of reported gains