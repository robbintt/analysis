---
ver: rpa2
title: 'The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive
  Language-Image Pretraining'
arxiv_id: '2512.17121'
source_url: https://arxiv.org/abs/2512.17121
tags:
- clip
- negation
- text
- image
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates and improves the ability of CLIP-based models
  to handle negation in medical imaging prompts. The Stanford AIMI CheXagent model
  was fine-tuned using contrastive learning strategies to enhance its understanding
  of negated clinical language, particularly for chest X-ray image retrieval.
---

# The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining

## Quick Facts
- arXiv ID: 2512.17121
- Source URL: https://arxiv.org/abs/2512.17121
- Reference count: 30
- Primary result: CON2 CLIP improves negated prompt retrieval by ~15% over base model, with better internal representation consistency and token attribution

## Executive Summary
This study evaluates and improves the ability of CLIP-based models to handle negation in medical imaging prompts. The Stanford AIMI CheXagent model was fine-tuned using contrastive learning strategies to enhance its understanding of negated clinical language, particularly for chest X-ray image retrieval. Two fine-tuning approaches were tested: a standard InfoNCE contrastive loss (CON1 CLIP) and a CoN-CLIP-based method incorporating explicit semantic opposition with distractor images (CON2 CLIP). Results show that CON2 CLIP achieved the highest accuracy on negated prompts, improving retrieval by nearly 15% over the base model, with more consistent internal representations across both structured and natural language prompts. Token attribution analysis confirmed increased sensitivity to negation, while t-SNE projections revealed clearer separation of positive and negative clusters. These findings demonstrate that contrastive fine-tuning, especially with explicit negation pairs, significantly improves CLIP's reliability in medical AI applications involving negation.

## Method Summary
The study fine-tuned the Stanford AIMI CheXagent (ViT-B/16 vision encoder) on chest X-ray images from the Open-i repository, focusing on pleural effusion vs. no pleural effusion. Two approaches were tested: CON1 CLIP (standard InfoNCE contrastive loss, fine-tuning text encoder only) and CON2 CLIP (CoN-CLIP loss with explicit semantic opposition using distractor images, also fine-tuning text encoder only). Training used 230 balanced images over 5 epochs with AdamW optimizer. Evaluation involved 99 prompts (structured/natural, positive/negative) on a 70-image test set, measuring Top-10 retrieval accuracy, token attribution, and t-SNE cluster visualization.

## Key Results
- CON2 CLIP achieved ~15% improvement in retrieval accuracy for negated prompts compared to the base model
- t-SNE analysis showed clearer separation between positive and negative prompt clusters after fine-tuning
- Token attribution scores for negation tokens increased significantly (from 0.101 to 0.127) in CON2 CLIP
- CON2 CLIP generalized better to natural language prompts than CON1 CLIP

## Why This Works (Mechanism)

### Mechanism 1: Semantic Opposition via Explicit Distractors
The CON2 CLIP loss function extends standard contrastive learning by introducing a distractor image (e.g., an X-ray with effusion) when training on a negated prompt ("no effusion"). This explicitly penalizes the model if it aligns the negated text with the visual features of the condition, thereby breaking the semantic confusion present in the base model.

### Mechanism 2: Unimodal Text Encoder Fine-Tuning
By freezing the vision encoder while fine-tuning only the text encoder, the model preserves robust visual feature extraction while shifting the burden of logical alignment entirely to the language model. The optimization process focuses on moving text embeddings in the shared latent spaceâ€”pushing "no effusion" away from the fixed visual cluster for "effusion."

### Mechanism 3: Attention Redistribution to Logical Tokens
The contrastive fine-tuning regime creates a loss signal that can only be minimized by attending to the negation token, thereby "teaching" the attention heads to weight logical modifiers more heavily. This prevents high-frequency clinical nouns from dominating the attention weights.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: The study relies on contrastive loss to organize the embedding space. Understanding that InfoNCE pulls positive pairs together and pushes negative pairs apart is required to interpret why CON1 improves separation but CON2 is superior.
  - Quick check question: In a batch of 32, how does the loss calculation change if "hard negatives" (distractors) are introduced versus relying on random in-batch negatives?

- **Concept: CLIP Architecture (Dual Encoders)**
  - Why needed here: The paper modifies the standard CLIP setup. One must understand that CLIP consists of separate image and text towers projecting into a shared space to grasp why freezing one tower is a valid strategy.
  - Quick check question: If the image encoder is frozen, which modality's representations must move to satisfy the loss function?

- **Concept: Token Attribution & Saliency**
  - Why needed here: The paper uses attribution to prove the model is "reading" the negation. Understanding how gradients map back to input tokens helps distinguish between a model "guessing" and a model "reasoning."
  - Quick check question: If the attribution score for the word "no" increases from 0.10 to 0.13, what does that imply about the text encoder's dependency on that specific token?

## Architecture Onboarding

- **Component map:** Stanford AIMI CheXagent (ViT-B/16 for vision, Transformer for text) -> CON1 CLIP (Text Encoder unfrozen; Loss: InfoNCE) OR CON2 CLIP (Data pipeline includes (Image, Prompt, Distractor Image, Distractor Prompt); Text Encoder unfrozen; Loss: Triplet-style contrastive)

- **Critical path:**
  1. Data Curation: Construct "opposition pairs" (Image A with "Effusion" + Image B with "No Effusion")
  2. Forward Pass: Pass all four inputs (True/Negated Image/Text) through encoders (Vision frozen)
  3. Similarity Compute: Calculate Cosine Similarity matrix for the batch
  4. Loss Calculation: Apply the modified CoN-CLIP loss ($L_{conclip}$) which explicitly penalizes alignment between negated text and positive images
  5. Backprop: Update only Text Encoder weights

- **Design tradeoffs:**
  - Negation vs. Positive Accuracy: The paper notes a slight decrease in accuracy for positive prompts in exchange for robust negation handling. Systems must be tuned to balance false positives (missing negation) vs. false negatives (missing positive findings)
  - Structured vs. Natural Language: CON1 overfits to structured prompts; CON2 generalizes better but requires more complex data preparation (distractors)

- **Failure signatures:**
  - Semantic Collapse: High similarity scores between "Condition X" and "No Condition X" in the embedding space
  - Token Blindness: Low gradient attribution scores on negation tokens (e.g., "no", "without") during saliency analysis
  - Visual Hallucination: Retrieval of pathology images when explicitly prompted with "clear" or "normal" findings

- **First 3 experiments:**
  1. Baseline t-SNE: Visualize the base CheXagent embeddings for "Effusion" vs. "No Effusion" prompts to confirm the cluster overlap (the "Before" state)
  2. Ablation on Freezing: Run CON2 with the image encoder unfrozen vs. frozen to quantify the degradation of visual features vs. the improvement in text alignment
  3. Hard Negative Scaling: Test retrieval accuracy while varying the ratio of hard negatives (distractors) to random negatives in the batch to find the optimal point between the CON1 and CON2 extremes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does contrastive fine-tuning for negation understanding generalize to thoracic findings beyond pleural effusion and across multi-institutional datasets?
- Basis in paper: The authors state: "Future work should extend these methods to additional thoracic findings and evaluate generalization across multiple datasets. ...our dataset is relatively small compared to the scale at which foundation models are usually adapted, and the distribution is limited to a single condition drawn from a single institution (OpenI). This raises concerns about generalization."
- Why unresolved: The study only evaluated pleural effusion using 300 images from a single source (Open-i repository), limiting claims about broader applicability.
- What evidence would resolve it: Testing CON2 CLIP on additional findings (e.g., cardiomegaly, pneumothorax, consolidation) using external datasets like MIMIC-CXR or CheXpert to measure whether negation improvements transfer.

### Open Question 2
- Question: Does improved negation comprehension in CON2 CLIP translate to practical clinical benefits when evaluated with clinician-guided testing?
- Basis in paper: The Discussion notes: "Quantifying uncertainty and clinician-guided testing will be necessary in evaluating whether improved negation comprehension results in practical benefits in clinical settings."
- Why unresolved: The study used only automated retrieval accuracy metrics without clinical validation or assessment of whether the improvements meaningfully affect diagnostic workflows.
- What evidence would resolve it: User studies with radiologists comparing retrieval usefulness across model versions, measuring time-to-diagnosis and error rates in simulated clinical scenarios.

### Open Question 3
- Question: Can the trade-off between improved negation handling and decreased positive prompt accuracy be mitigated through alternative training objectives?
- Basis in paper: The authors acknowledge: "Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation" and "improving one clinically important capability may slightly degrade performance on others, and these trade-offs must be explicitly evaluated before clinical adoption."
- Why unresolved: Neither fine-tuning strategy achieved simultaneous improvement on both negated and positive prompts, suggesting a fundamental tension in the contrastive objective.
- What evidence would resolve it: Ablation studies varying loss function weights, or multi-task training that jointly optimizes negation sensitivity and positive prompt accuracy, measuring both metrics on held-out test sets.

## Limitations
- The study relies on a small curated dataset (230 training images, 70 test images), which may limit generalizability to other medical imaging tasks or broader pathology sets
- The approach freezes the vision encoder, assuming it is pathology-agnostic, which may not hold if pretrained visual features contain biases or miss subtle pathological cues
- The reported improvement is specific to "pleural effusion" negation, with unclear transfer to other pathologies or imaging modalities without retraining

## Confidence
- High confidence: Retrieval accuracy improvements for negated prompts (15% gain), t-SNE cluster separation, and the core hypothesis that contrastive fine-tuning improves CLIP's negation handling
- Medium confidence: The claim that freezing the image encoder is optimal, as this depends on the quality of the pretrained visual features and may not hold for all pathologies
- Medium confidence: The interpretation of token attribution as evidence of logical processing, since alternative explanations (e.g., position-based attention) are not ruled out
- Low confidence: Extrapolation of these results to clinical deployment without further validation on larger, more diverse datasets

## Next Checks
1. **Scale validation:** Replicate the experiment on a dataset with 10x more images and 5+ different pathologies to test whether the CON2 gains persist and to measure overfitting risk
2. **Encoder ablation:** Run CON2 with the image encoder unfrozen to quantify the trade-off between preserving visual features and improving text alignment, and to test the hypothesis that freezing is optimal
3. **Generalization probe:** Evaluate the fine-tuned model on a held-out set of natural language prompts that describe negation in structurally different ways (e.g., "cannot rule out," "unlikely to show," "no evidence of") to test robustness beyond simple "no X" patterns