---
ver: rpa2
title: 'Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms'
arxiv_id: '2505.21792'
source_url: https://arxiv.org/abs/2505.21792
tags:
- learning
- multimodal
- data
- federated
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a taxonomy of multimodal federated learning
  (MFL) through the lens of three FL paradigms: horizontal FL (HFL), vertical FL (VFL),
  and hybrid FL. The key insight is that multimodal data introduces distinct challenges
  in each paradigm - HFL faces modality heterogeneity across clients, VFL encounters
  increased privacy risks from cross-party embedding transmission, and hybrid FL struggles
  with computational and communication efficiency due to complex data partitioning.'
---

# Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms

## Quick Facts
- **arXiv ID**: 2505.21792
- **Source URL**: https://arxiv.org/abs/2505.21792
- **Reference count**: 40
- **Primary result**: A paradigm-aware taxonomy of MFL challenges across HFL (modality heterogeneity), VFL (privacy risks), and hybrid FL (efficiency bottlenecks)

## Executive Summary
This paper presents a comprehensive survey of multimodal federated learning (MFL) through the lens of three federated learning paradigms: horizontal FL (HFL), vertical FL (VFL), and hybrid FL. The authors identify that multimodal data introduces distinct challenges in each paradigm - HFL faces modality heterogeneity across clients, VFL encounters increased privacy risks from cross-party embedding transmission, and hybrid FL struggles with computational and communication efficiency due to complex data partitioning. The survey systematically reviews representative works in each category, identifies key challenges, and proposes future research directions including lightweight model design, privacy-preserving mechanisms, unsupervised learning, and personalized MFL.

## Method Summary
The paper systematically reviews MFL research through a taxonomy based on data partitioning paradigms. It synthesizes findings from 40+ references to identify challenges and solutions across HFL, VFL, and hybrid FL settings. The survey analyzes federated learning mechanisms including FedAvg aggregation, modality-specific encoders, fusion strategies (early/late/co-attention), and security considerations. For reproduction, the minimum viable plan involves implementing modality-specific encoders with a fusion head, partitioning a multimodal dataset across simulated clients, and evaluating using FedAvg aggregation against centralized and unimodal baselines.

## Key Results
- HFL faces modality heterogeneity challenges where clients with incomplete modality sets cause aggregation failures and biased representations
- VFL encounters increased privacy risks as multimodal embeddings carry richer semantic correlations that can be exploited for sensitive attribute reconstruction
- Hybrid FL struggles with computational and communication efficiency due to compounded bottlenecks from bi-orthogonal data partitioning

## Why This Works (Mechanism)

### Mechanism 1: Partition-Driven Heterogeneity
- Claim: In HFL, modality heterogeneity causes aggregation failures and biased representations.
- Mechanism: Clients hold different samples but share feature space. With multimodal data, clients possess only subsets of modalities due to hardware limits. Aggregating models trained on non-overlapping feature spaces introduces conflicting gradients and noise.
- Core assumption: Clients have varying sensing capabilities resulting in incomplete local modality sets.
- Evidence anchors: [abstract] "HFL faces modality heterogeneity across clients"; [section 3.4] "Clients with non-overlapping modalities may learn incompatible representations... degrades global model performance."
- Break condition: If all clients possess identical, complete modality sets, this challenge theoretically vanishes.

### Mechanism 2: Cross-Party Semantic Leakage
- Claim: In VFL, transmitting multimodal embeddings increases risk of sensitive attribute reconstruction.
- Mechanism: VLF requires parties to exchange intermediate embeddings to compute joint gradients. Multimodal embeddings contain richer semantic correlations that adversaries can exploit to reveal sensitive attributes.
- Core assumption: Adversary has access to intermediate embeddings or gradient updates during training.
- Evidence anchors: [abstract] "VFL encounters increased privacy risks from cross-party embedding transmission"; [section 4.3] "Multimodal embeddings often carry rich semantic information... may unintentionally reveal sensitive attributes."
- Break condition: If secure multi-party computation or homomorphic encryption fully obfuscates embeddings without performance loss, this risk is mitigated.

### Mechanism 3: Bi-Orthogonal Coordination Overhead
- Claim: In hybrid FL, system efficiency degrades due to compounded communication and computational bottlenecks.
- Mechanism: Hybrid FL requires simultaneous coordination: intra-silo vertical embedding exchange and inter-silo horizontal model aggregation. This "bi-orthogonal" structure creates synchronization barriers where straggling devices bottleneck the network.
- Core assumption: System operates on resource-constrained devices with varying connectivity.
- Evidence anchors: [abstract] "Hybrid FL struggles with computational and communication efficiency"; [section 5.3] "Hybrid FL systems often operate in heterogeneous environments... communication overhead can quickly become a bottleneck."
- Break condition: If communication is asynchronous and aggregation is decentralized, synchronization bottleneck is reduced.

## Foundational Learning

- **Concept: Horizontal vs. Vertical Partitioning**
  - Why needed here: The taxonomy hinges on distinguishing whether clients share same features (HFL) or same samples (VFL). Without this, multimodal challenges are indistinguishable.
  - Quick check question: Do Client A and Client B hold different users (Horizontal) or different data types about same user (Vertical)?

- **Concept: Multimodal Fusion Strategies**
  - Why needed here: The paper reviews algorithms based on how they fuse data (early vs. late fusion). Understanding this distinction is critical for VFL/HFL analysis.
  - Quick check question: Does the model combine raw pixels and audio waves immediately (early), or combine output of image classifier and audio classifier (late)?

- **Concept: Gradient Inversion & Inference Attacks**
  - Why needed here: Section 4.3 relies on understanding how shared gradients or embeddings can be mathematically reversed to reconstruct input data.
  - Quick check question: If I share gradient update for batch of size 1, can I mathematically derive original input image from that gradient?

## Architecture Onboarding

- **Component map:**
  - Input: Distributed Multimodal Data (Image, Text, Audio, Sensors)
  - HFL Node: Local Modality Encoders -> Global Aggregator (FedAvg variants)
  - VFL Node: Split Encoders (Bottom models) -> Server Head (Top model) -> Embedding Exchange
  - Hybrid Node: Intra-silo VFL hubs -> Inter-silo Global Server
  - Output: Global Multimodal Model or Predictive Head

- **Critical path:**
  1. Identify Partition: Determine if data silos split by sample ID (VFL), user ID (HFL), or both (Hybrid)
  2. Diagnose Challenge: Map partition to taxonomy (HFL -> Heterogeneity; VFL -> Privacy; Hybrid -> Efficiency)
  3. Select Aggregator: Choose aggregation logic (FedAvg for HFL, split backward propagation for VFL)

- **Design tradeoffs:**
  - Completeness vs. Complexity: Handling missing modalities requires complex imputation or distinct encoders, increasing model size
  - Utility vs. Privacy: Sharing richer embeddings in VFL improves fusion but drastically increases attack surface
  - Synchronization vs. Speed: Hybrid FL requires strict synchronization for bi-orthogonal updates, trading speed for consistency

- **Failure signatures:**
  - HFL Failure: Global model diverges; performance on minority modalities drops to random chance
  - VFL Failure: Adversary successfully reconstructs training images or infers sensitive attributes from gradients
  - Hybrid Failure: Training stalls or times out due to "stragglers" in intra-silo communication

- **First 3 experiments:**
  1. Modality Ablation (HFL): Simulate HFL on dataset where 50% of clients randomly drop specific modalities to test robustness to heterogeneity
  2. Embedding Inversion (VFL): Implement VFL setup and apply gradient inversion attack to quantify data leakage from multimodal embeddings
  3. Latency Profiling (Hybrid): Construct 2-silo hybrid setup with intra-silo VFL and inter-silo HFL to measure communication overhead relative to unimodal baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can scalable, general-purpose unsupervised MFL frameworks be designed to handle prevalence of unlabeled data in distributed sensor networks?
- Basis in paper: [explicit] Authors state "Future work should design scalable, general-purpose unsupervised MFL frameworks inspired by centralized self-supervised strategies," noting most frameworks rely on supervised learning
- Why unresolved: Real-world scenarios often involve partially or entirely unlabeled multimodal data, yet current MFL research focuses predominantly on supervised paradigms
- What evidence would resolve it: Development of MFL framework that effectively utilizes contrastive learning or pseudo-labeling to achieve performance comparable to supervised methods on standard unlabeled multimodal benchmarks

### Open Question 2
- Question: How can prompt-based learning be adapted for MFL to achieve communication-efficient personalization across clients with heterogeneous modality availability?
- Basis in paper: [explicit] Survey identifies prompt tuning as underexplored direction, asking future work to explore "federated prompt aggregation, visual-prompt alignment, and adapter sharing"
- Why unresolved: While LLMs use prompts effectively, integrating prompt-based mechanisms into federated settings for multimodal data requires solving challenges in aligning heterogeneous feature spaces efficiently
- What evidence would resolve it: Algorithm demonstrating that federated prompt aggregation reduces communication overhead while maintaining high accuracy on devices with varying modality subsets

### Open Question 3
- Question: What methods can provide cross-modal interpretability in MFL without compromising privacy of client data or model updates?
- Basis in paper: [explicit] Authors argue "Future research should focus on cross-modal attribution methods... in federated setups," as standard techniques like Grad-CAM are insufficient for complex modality combinations
- Why unresolved: Federated architecture limits visibility into client data and model updates, making standard interpretability techniques difficult to apply, especially for cross-modal attribution
- What evidence would resolve it: Technique that generates per-modality influence scores or cross-modal attribution maps for global model without requiring access to raw local data

## Limitations
- The taxonomy assumes clear separation between HFL, VFL, and hybrid paradigms, though real-world data often exhibits mixed characteristics
- Privacy risk analysis in VFL relies heavily on theoretical concerns rather than empirical attack demonstrations specific to multimodal embeddings
- Many proposed future directions remain conceptual without implementation details or validation

## Confidence
- **High confidence**: Partition-driven heterogeneity mechanism in HFL and bi-orthogonal coordination overhead in hybrid FL are well-supported by corpus evidence
- **Medium confidence**: Cross-party semantic leakage claim in VFL is theoretically sound but lacks specific empirical validation from corpus
- **Low confidence**: Proposed future research directions are reasonable but lack concrete validation or implementation guidance

## Next Checks
1. Implement a controlled gradient inversion attack on VFL multimodal embeddings to empirically measure privacy leakage versus unimodal baselines
2. Conduct comprehensive ablation study testing HFL performance across varying degrees of modality incompleteness and client heterogeneity
3. Profile end-to-end communication and computation costs in hybrid FL across different modality combinations and device capabilities