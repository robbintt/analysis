---
ver: rpa2
title: Towards Robust Expert Finding in Community Question Answering Platforms
arxiv_id: '2503.02674'
source_url: https://arxiv.org/abs/2503.02674
tags:
- tuef
- expert
- question
- experts
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TUEF, a topic-oriented user-interaction model
  for fair Expert Finding in Community Question Answering (CQA) platforms. TUEF improves
  the robustness and credibility of the CQA platform through a more precise Expert
  Finding component by exploiting diverse types of information, specifically content
  and social information, to identify more precisely experts.
---

# Towards Robust Expert Finding in Community Question Answering Platforms

## Quick Facts
- arXiv ID: 2503.02674
- Source URL: https://arxiv.org/abs/2503.02674
- Reference count: 40
- Primary result: TUEF outperforms state-of-the-art methods on StackOverflow, achieving 65.06% MRR and 112% P@1 improvement.

## Executive Summary
This paper introduces TUEF, a topic-oriented user-interaction model designed to improve Expert Finding in Community Question Answering (CQA) platforms. By leveraging both content and social information, TUEF constructs a Multi-Layer Graph to model users' relationships under community topics. The method applies Information Retrieval and Social Network analysis techniques to identify candidate experts and uses a Learning-to-Rank model to precisely order them. Experiments on a large-scale StackOverflow dataset demonstrate that TUEF significantly outperforms existing methods while promoting transparent expert identification.

## Method Summary
TUEF models users' relationships in CQA platforms using a Multi-Layer Graph (MLG) where nodes represent users and edges represent similarities within each layer. The model identifies candidate experts by combining Information Retrieval techniques (BM25 indexes for text and tags) with Social Network analysis (Random Walks on the MLG). A Learning-to-Rank model (LambdaMART) is then used to rank candidates based on features including reputation, graph centrality measures, and BM25 scores. The approach is evaluated on a StackOverflow dataset using metrics like Mean Reciprocal Rank (MRR) and Precision@1 (P@1).

## Key Results
- TUEF achieves 65.06% improvement in Mean Reciprocal Rank (MRR) over baseline methods.
- TUEF achieves 112% improvement in Precision@1 (P@1) over baseline methods.
- The method demonstrates superior performance in identifying experts while promoting transparent and fair expert identification.

## Why This Works (Mechanism)
TUEF improves Expert Finding by combining content-based retrieval with social network analysis. The Multi-Layer Graph captures user expertise across different topics, while Random Walks explore the network to identify relevant candidates. The Learning-to-Rank model integrates multiple features (reputation, centrality, BM25 scores) to produce more accurate rankings. This hybrid approach addresses the limitations of purely content-based or purely social methods.

## Foundational Learning
- **Multi-Layer Graph Construction**: Why needed - To capture user relationships across different topics. Quick check - Verify that the graph has reasonable connectivity and that layers represent meaningful topic clusters.
- **BM25 Indexing**: Why needed - To retrieve relevant questions and experts based on textual content. Quick check - Ensure that BM25 scores correlate with relevance judgments.
- **Random Walks**: Why needed - To explore the social network and discover experts who may not be directly connected to the query. Quick check - Monitor the diversity of nodes visited during Random Walks.
- **Learning-to-Rank (LambdaMART)**: Why needed - To combine multiple features and produce optimal rankings. Quick check - Validate that the model improves ranking quality compared to simple heuristics.
- **Probabilistic Stopping Criterion**: Why needed - To efficiently limit candidate collection without missing relevant experts. Quick check - Test sensitivity to the α threshold.
- **Feature Engineering**: Why needed - To capture both static user attributes and query-dependent signals. Quick check - Analyze feature importance scores from the trained model.

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Graph Construction -> BM25 Index Creation -> Candidate Selection -> Ranking Model Training -> Evaluation

**Critical Path**: The core pipeline involves preprocessing the StackOverflow data, constructing the Multi-Layer Graph and BM25 indexes, selecting candidate experts using a combination of IR and Random Walks, and training a LambdaMART model to rank candidates. The most critical components are the graph construction (which determines the social signal quality) and the ranking model (which integrates all features).

**Design Tradeoffs**: The model balances content-based retrieval with social network analysis, trading computational complexity for improved accuracy. The use of a Multi-Layer Graph allows for topic-specific expertise modeling but increases graph construction complexity. The probabilistic stopping criterion improves efficiency but may miss experts in low-activity tags.

**Failure Signatures**: Disconnected graph components (low average node degree), low recall in candidate selection (small candidate sets), and poor ranking performance (low MRR/P@1) are key failure indicators. These can result from overly strict thresholds, poor text preprocessing, or inadequate feature engineering.

**3 First Experiments**:
1. **Graph Connectivity Check**: Measure the average node degree and component size in the Multi-Layer Graph to ensure the graph is not overly fragmented.
2. **Candidate Set Size Analysis**: Monitor the size of the candidate set D before and after Random Walks to assess the effectiveness of the probabilistic stopping criterion.
3. **Feature Importance Validation**: Train the LambdaMART model and analyze feature importance scores to verify that the engineered features contribute meaningfully to ranking quality.

## Open Questions the Paper Calls Out
None

## Limitations
- The reported improvements are specific to the StackOverflow dataset and may not generalize to other CQA platforms or time periods.
- The exact hyperparameter configuration for the LambdaMART model is not provided, affecting reproducibility.
- The probabilistic stopping criterion (α=0.001) is tuned for this dataset and may not transfer to domains with different answer distribution patterns.

## Confidence
- **High confidence** in the core methodology (graph-based expert finding with multi-layer structure and random walk exploration).
- **Medium confidence** in the relative performance improvements, due to dependency on dataset specifics and unspecified hyperparameters.
- **Low confidence** in absolute generalizability across platforms and time periods.

## Next Checks
1. **Reproduce the MLG construction and measure node connectivity** to confirm that the edge pruning threshold δ=0.5 does not produce excessive fragmentation.
2. **Re-run the candidate selection pipeline with varying α values** (e.g., 0.01, 0.001, 0.0001) to assess sensitivity to the stopping criterion.
3. **Implement the exact BM25 merging logic** (round-robin alternation) and verify retrieval performance matches reported results.