---
ver: rpa2
title: 'GOFAI meets Generative AI: Development of Expert Systems by means of Large
  Language Models'
arxiv_id: '2507.13550'
source_url: https://arxiv.org/abs/2507.13550
tags:
- knowledge
- system
- expert
- systems
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid approach that combines the recall
  capacity of large language models (LLMs) with the precision of symbolic systems
  to develop expert systems. The authors extract structured knowledge from LLMs using
  well-designed prompts, encode it into Prolog, and validate it with human experts
  to ensure accuracy and interpretability.
---

# GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models

## Quick Facts
- **arXiv ID**: 2507.13550
- **Source URL**: https://arxiv.org/abs/2507.13550
- **Reference count**: 32
- **Primary result**: Hybrid approach combining LLMs with symbolic systems achieves >99% factual accuracy in expert system development

## Executive Summary
This paper presents a novel hybrid methodology for developing expert systems that leverages the broad knowledge recall capabilities of large language models while maintaining the precision and interpretability of symbolic reasoning systems. The approach addresses a fundamental challenge in AI: how to harness the power of generative models while mitigating their tendency toward hallucinations and maintaining transparency. By extracting structured knowledge from LLMs through carefully designed prompts, encoding this knowledge into Prolog, and validating with human experts, the authors create a framework that produces reliable, interpretable expert systems suitable for critical domains such as medicine, law, and education.

## Method Summary
The methodology involves a three-stage process: knowledge extraction from LLMs using well-crafted prompts, structured encoding into Prolog logic programming format, and expert validation to ensure accuracy and interpretability. The extracted knowledge is transformed from unstructured LLM responses into deterministic logical rules that can be queried and verified. This hybrid approach combines the pattern recognition and broad knowledge base of LLMs with the formal reasoning capabilities of symbolic systems, creating expert systems that are both knowledgeable and transparent in their decision-making processes.

## Key Results
- Generated knowledge bases achieve over 99% factual accuracy when validated by human experts
- Statistical significance demonstrated with p < 0.05 against 80% accuracy threshold
- The approach effectively mitigates LLM hallucinations through structured knowledge encoding and expert validation

## Why This Works (Mechanism)
The approach succeeds by creating a synergistic relationship between two AI paradigms. LLMs provide broad knowledge recall across diverse domains, while symbolic systems offer precision, interpretability, and formal verification capabilities. By using prompts to extract structured knowledge from LLMs, the system captures the breadth of information available in the language model while converting it into a format that supports deterministic reasoning. The expert validation step ensures that the knowledge encoded into Prolog is accurate and domain-appropriate, addressing the reliability concerns associated with generative AI outputs.

## Foundational Learning

**Prolog Logic Programming** - A declarative programming language based on formal logic, used here to encode structured knowledge. *Why needed*: Provides the formal reasoning framework that enables deterministic inference and verification. *Quick check*: Can the encoded rules be queried to produce consistent, verifiable results?

**Prompt Engineering for Structured Extraction** - The art of designing prompts that elicit specific, structured responses from LLMs. *Why needed*: Enables systematic extraction of knowledge in formats suitable for symbolic encoding. *Quick check*: Do prompts consistently produce the desired structured output format?

**Expert System Validation Methodology** - Processes for verifying that encoded knowledge is accurate and domain-appropriate. *Why needed*: Ensures the final expert system produces reliable, trustworthy results. *Quick check*: Can multiple independent experts consistently verify the accuracy of encoded knowledge?

## Architecture Onboarding

**Component Map**: LLM Knowledge Base -> Prompt Processing -> Prolog Encoding -> Expert Validation -> Expert System

**Critical Path**: The most critical path is the validation loop where extracted knowledge must be verified by human experts before being encoded into Prolog. Any errors or hallucinations that slip through the prompt processing stage must be caught at this validation point to ensure system reliability.

**Design Tradeoffs**: The system trades the generative flexibility of pure LLM approaches for the precision and interpretability of symbolic systems. While this reduces the system's ability to handle novel or ambiguous scenarios creatively, it dramatically improves reliability and explainability - crucial factors for expert systems in high-stakes domains.

**Failure Signatures**: Primary failure modes include: (1) Prompt engineering failures leading to unstructured or inaccurate knowledge extraction, (2) Expert validation bottlenecks due to limited expert availability, (3) Knowledge base scalability issues as domain complexity increases, and (4) LLM version dependencies affecting knowledge extraction consistency.

**First Experiments**:
1. Test knowledge extraction accuracy across 10 different LLM architectures using identical prompts to assess consistency
2. Measure validation throughput by having multiple experts independently verify the same knowledge base to establish inter-rater reliability
3. Benchmark system performance with knowledge bases of increasing size and complexity to identify scalability limits

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation based on only 100 test questions, limiting generalizability
- Heavy dependence on human expert validation creates potential bottlenecks and scalability challenges
- Assumes consistent accuracy across diverse, complex knowledge domains without extensive empirical validation

## Confidence
- **High confidence** in the conceptual framework and hybrid approach design - the methodology is sound and addresses real limitations in both GOFAI and generative AI
- **Medium confidence** in experimental results and accuracy claims - while impressive, the limited test scope and sampling methodology are not fully detailed
- **Low confidence** in scalability and performance across diverse, real-world domains - the paper does not address how the approach performs with larger, more complex knowledge bases or across multiple specialized fields

## Next Checks
1. Conduct a larger-scale validation study with 1000+ diverse test cases across multiple specialized domains to assess accuracy consistency and scalability limits
2. Implement cross-validation with multiple independent expert panels to establish inter-rater reliability and reduce potential bias in the verification process
3. Benchmark computational performance and latency metrics when deploying the hybrid system with knowledge bases of varying sizes and complexity levels