---
ver: rpa2
title: An Enhancement of Jiang, Z., et al.s Compression-Based Classification Algorithm
  Applied to News Article Categorization
arxiv_id: '2502.14444'
source_url: https://arxiv.org/abs/2502.14444
tags:
- classification
- text
- compression
- algorithm
- gzip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study improves Jiang et al.\u2019s compression-based classification\
  \ algorithm for news article categorization by addressing its limitations in detecting\
  \ semantic similarities between text documents. The proposed method uses unigram\
  \ extraction and optimized concatenation to enhance compression efficiency and similarity\
  \ detection."
---

# An Enhancement of Jiang, Z., et al.s Compression-Based Classification Algorithm Applied to News Article Categorization

## Quick Facts
- arXiv ID: 2502.14444
- Source URL: https://arxiv.org/abs/2502.14444
- Authors: Sean Lester C. Benavides; Cid Antonio F. Masapol; Jonathan C. Morano; Dan Michael A. Cortez
- Reference count: 15
- Primary result: 5.73% average accuracy improvement over Jiang et al.'s method across six datasets

## Executive Summary
This study improves Jiang et al.'s compression-based classification algorithm for news article categorization by addressing its limitations in detecting semantic similarities between text documents. The proposed method uses unigram extraction and optimized concatenation to enhance compression efficiency and similarity detection. Instead of compressing entire documents, it compresses extracted unigrams, reducing redundancy and improving the accuracy of Normalized Compression Distance (NCD) calculations. Experimental results across diverse datasets show an average accuracy improvement of 5.73%, with gains up to 11% on datasets containing longer documents.

## Method Summary
The enhanced algorithm extracts unique unigrams from text documents, compresses these unigram sets using gzip, and calculates NCD using the union of unigram sets rather than direct concatenation. For classification, it employs k-NN with NCD as the distance metric. The method processes documents by tokenizing text, filtering for alphanumeric tokens, converting to unique sets, and comparing pairs via union-based compression. While the paper mentions a "unigram compression threshold" in Figure 2, the specific threshold value is not provided in the methodology.

## Key Results
- Average accuracy improvement of 5.73% across six datasets compared to Jiang et al.'s method
- Maximum improvement of 11% on Ohsumed dataset containing longer medical documents
- Particularly effective for datasets with high-label diversity and complex text structures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preprocessing documents into unigram sets mitigates the fixed sliding window constraint of gzip, improving similarity detection in longer texts.
- **Mechanism:** Gzip operates using a 32KB sliding window (LZ77), meaning it cannot identify repeated patterns that are spaced further apart than 32KB. By reducing a document to a set of unique unigrams, the effective text length shrinks significantly. This increases the probability that semantically relevant words shared between two documents fall within the compressor's window range during the union calculation.
- **Core assumption:** The semantic "signature" required for classification is preserved in the set of unique words, and word frequency/order contributes less to category identity than vocabulary overlap.
- **Evidence anchors:**
  - [abstract] "...mitigating the sliding window constraints of gzip... focus on compressing extracted unigrams rather than entire documents."
  - [page 3] "Gzip's performance in compressing large files can be suboptimal... as the fixed window size is unable to capture these distant patterns efficiently."
  - [corpus] Weak direct validation. While neighbors like "Context Steering" discuss compression embeddings, they do not specifically validate the sliding window mitigation via unigrams.
- **Break condition:** Fails if the classification task relies heavily on syntax, phrasing, or sequential context (e.g., sentiment analysis where "not good" != "good").

### Mechanism 2
- **Claim:** Using the union of unigrams (set operation) instead of direct concatenation improves the signal-to-noise ratio of the Normalized Compression Distance (NCD).
- **Mechanism:** Standard NCD concatenates $x$ and $y$ ($xy$). If $x$ and $y$ share high-frequency filler words (e.g., "the", "and"), the compressor finds matches regardless of topic. By taking the union of unique unigrams ($x \cup y$), the algorithm strips frequency data. If the documents share vocabulary, the union set is roughly the size of a single document (high compressibility relative to size). If they are dissimilar, the union set is nearly double the size, increasing the compressed length $C(xy)$.
- **Core assumption:** Category similarity is a function of vocabulary overlap (Jaccard-like similarity) rather than shared high-frequency stop words.
- **Evidence anchors:**
  - [abstract] "...optimized concatenation strategy replaces direct concatenation with the union of unigrams, reducing redundancy..."
  - [page 4] "Complete similarity: The union is half the size of the concatenation."
  - [corpus] Indirect support from "A Compression Based Classification Framework..." which links symbolic sequence overlap to classification, though specific union logic is unique to this paper.
- **Break condition:** Degrades in datasets where distinct categories share significant technical jargon or proper nouns (e.g., distinguishing "Medical News" from "Medical Journals").

### Mechanism 3
- **Claim:** The enhanced method indirectly implements a variable "attention" mechanism by filtering input tokens before compression.
- **Mechanism:** The unigram extraction step (checking if tokens are alphanumeric and applying a frequency threshold if used, though the paper emphasizes unique set extraction) acts as a hard filter. It removes punctuation and structural noise before the compressor sees the data. This forces the compressor to allocate bits only to semantic content (words), effectively "weighing" word existence higher than structural formatting.
- **Core assumption:** Non-alphanumeric tokens and repeated words add noise to the distance metric rather than signal.
- **Evidence anchors:**
  - [page 4] "Check if the token is alphanumeric... If condition is true, add the token to the unigrams list."
  - [results] Higher accuracy gains in "content-rich" datasets like Ohsumed (+11.1%) suggests filtering noise aids complex categorization.
  - [corpus] Not explicitly discussed in neighbor papers.
- **Break condition:** If structural markers (e.g., code indentation, XML tags) are required for classification.

## Foundational Learning

- **Concept: Normalized Compression Distance (NCD)**
  - **Why needed here:** This is the core distance metric replacing vector embeddings. Understanding that $NCD(x,y) \approx 0$ means $x$ and $y$ share compressible patterns is vital.
  - **Quick check question:** If document A is a subset of document B, will NCD approach 0 or 1?

- **Concept: The LZ77 Sliding Window**
  - **Why needed here:** The entire premise of the enhancement is fixing the 32KB window limitation. Without understanding this bottleneck, the benefit of "unigram extraction" is opaque.
  - **Quick check question:** Why would a 50KB document compress poorly compared to two 25KB documents with identical content using gzip?

- **Concept: k-Nearest Neighbors (kNN) with Non-Euclidean Distance**
  - **Why needed here:** The pipeline uses NCD (which is a metric, but not geometric like Euclidean) to find nearest neighbors.
  - **Quick check question:** Does kNN require a fixed-dimensional vector space, or can it operate on a pairwise distance matrix?

## Architecture Onboarding

- **Component map:**
  1. **Tokenizer:** Splits raw text, filters for alphanumeric tokens
  2. **Set Generator:** Converts list of tokens to unique unigrams (`set()`)
  3. **Union Operator:** Computes $x \cup y$ for pair-wise comparison
  4. **Compressor (gzip):** Compresses $x$, $y$, and $x \cup y$
  5. **NCD Calculator:** Computes distance from compressed byte lengths
  6. **kNN Classifier:** Selects class based on lowest NCD to training samples

- **Critical path:** The **Union Operator** is the critical novelty. Standard implementations concatenate strings here ($x + y$). Changing this to set union ($x \cup y$) is the specific switch that yields the accuracy gains.

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Computing NCD for kNN is $O(N^2)$ or $O(N \times M)$ (query vs. training set). This is slow for large datasets compared to ANN (Approximate Nearest Neighbor) search in vector databases.
  - **Information Loss:** Converting to unigrams destroys sequence information. This architecture trades nuance (syntax/sentiment) for robustness (topic detection via keyword overlap).

- **Failure signatures:**
  - **Short Text Failure:** Minimal improvement on short texts (e.g., AGNews) because the sliding window limit is not reached; preprocessing adds overhead without benefit.
  - **Memory Overflow:** Despite being "lightweight," loading full distance matrices for large $N$ can crash RAM.
  - **Synonym Blindness:** Since gzip works on byte/character matching, "car" and "automobile" appear completely dissimilar (NCD $\approx 1$), unlike embedding-based models.

- **First 3 experiments:**
  1. **Sanity Check (Baseline):** Run standard gzip-NCD on 20Newsgroups vs. the Enhanced Unigram-NCD. Verify the +9.6% delta mentioned in the paper.
  2. **Union vs. Intersection Stress Test:** Create a synthetic dataset where Document A and B are semantically identical but share 0 vocabulary (synonym substitution). Hypothesis: Accuracy will drop to near zero.
  3. **Threshold Sweep:** The paper mentions unigram extraction; test if removing stop-words (high frequency, low semantic value) *before* the set operation further improves accuracy or if the union operation alone suffices.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the enhanced algorithm perform when tested with alternative compression algorithms (e.g., bzip2, lzma, zstd) that have larger window sizes or different encoding strategies?
- **Basis in paper:** [explicit] The limitations section states the study "does not extend to comparing the enhanced algorithm with alternative compressors or classification approaches, which could provide a broader understanding of its relative performance."
- **Why unresolved:** Only gzip was evaluated, leaving unclear whether the unigram-based preprocessing benefits are compressor-agnostic or specific to gzip's 32KB sliding window constraint.
- **What evidence would resolve it:** Comparative accuracy and runtime results across multiple compressors on the same six datasets.

### Open Question 2
- **Question:** What is the actual performance and resource consumption of the enhanced algorithm on resource-constrained devices such as mobile phones or IoT hardware?
- **Basis in paper:** [explicit] The paper claims the approach is "ideal for resource-constrained environments" and "suitable for devices with limited computational power," but also acknowledges "the performance of the enhanced algorithm is potentially influenced by the specific hardware and computational resources available during testing."
- **Why unresolved:** All experiments were conducted in controlled testing environments without empirical validation on actual constrained hardware.
- **What evidence would resolve it:** Benchmarks measuring accuracy, memory usage, and latency on low-power devices or simulated resource-constrained conditions.

### Open Question 3
- **Question:** Does discarding word frequency information through set-based unigram extraction negatively affect classification accuracy on tasks where term frequency is semantically meaningful?
- **Basis in paper:** [inferred] The methodology explicitly uses set() to eliminate duplicates, stating the NCD "reflects the overlap of unique words while ignoring word frequency or text structure." This design choice may lose information relevant to certain classification tasks.
- **Why unresolved:** No ablation study compares set-based versus frequency-preserving unigram representations.
- **What evidence would resolve it:** Comparative experiments retaining frequency-weighted unigrams versus unique-only unigrams across datasets with varying semantic density.

## Limitations
- The paper does not specify the k value for k-NN classification, which is a critical hyperparameter for reproducing results
- No train/test split ratios or sampling methodology described, making fair comparison difficult
- The "unigram compression threshold" mentioned in Figure 2 is never quantified or explained in the methodology

## Confidence
- **High:** The core claim that unigram extraction + union operation improves NCD accuracy for longer documents is well-supported by the 11% gain on Ohsumed
- **Medium:** The claim that this method is "lightweight" is context-dependent; while individual NCD calculations are cheap, the O(NÂ²) complexity makes large-scale deployment challenging
- **Low:** The assertion that the method is "particularly effective for datasets with high-label diversity" lacks direct experimental validation beyond the Ohsumed result

## Next Checks
1. **Sanity check baseline:** Implement the original Jiang et al. gzip-NCD on 20Newsgroups and verify the claimed +9.6% accuracy improvement with the enhanced method
2. **Union vs. concatenation ablation:** Create a controlled synthetic dataset where documents share semantic content but no vocabulary (synonym substitution) to test if the union operation actually improves classification
3. **Sliding window boundary test:** Systematically vary document lengths to empirically demonstrate when the 32KB window constraint becomes limiting, validating the core mechanism claim