---
ver: rpa2
title: 'Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual
  Language Models'
arxiv_id: '2507.20704'
source_url: https://arxiv.org/abs/2507.20704
tags:
- text2vlm
- image
- prompt
- multimodal
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Text2VLM, an automated pipeline that converts
  text-only datasets into multimodal formats to evaluate Visual Language Model (VLM)
  safety. The pipeline extracts harmful concepts from text, replaces them with numbered
  placeholders, and renders the concepts as a typographic image.
---

# Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models

## Quick Facts
- arXiv ID: 2507.20704
- Source URL: https://arxiv.org/abs/2507.20704
- Reference count: 15
- Key outcome: Text2VLM pipeline exposes multimodal safety weaknesses in open-source VLMs, showing reduced refusal rates for typographic harmful prompts compared to text-only prompts.

## Executive Summary
Text2VLM is an automated pipeline that converts text-only safety datasets into multimodal prompts (text + typographic images) to evaluate Visual Language Model (VLM) safety. The pipeline extracts harmful concepts, replaces them with numbered placeholders, and renders them as images. Experiments on four malicious datasets (MITRE, Interpreter, MedSafetyBench, ToxiGen) and one benign control (Vicuna-Bench) show that typographic multimodal inputs reduce both model understanding and safety refusal rates in open-source VLMs, revealing vulnerabilities in current multimodal alignment training.

## Method Summary
The Text2VLM pipeline transforms text-only safety prompts into multimodal formats by: (1) summarizing long prompts >200 characters using Dolphin-2.9-Llama3-8B to fit OCR limits, (2) extracting salient harmful concepts with GPT-4o-mini, (3) generating numbered typographic images via Matplotlib, and (4) replacing concepts with `<insert item N from attached image>` placeholders. The resulting multimodal prompts are evaluated on LLaVA and VILA models, measuring understanding rates and unsafe response rates (relevant responses without refusal). Human evaluation validates the transformation quality, with >90% of prompt summarizations and concept extractions rated as "Great" or "Good."

## Key Results
- Typographic multimodal inputs reduced understanding scores across all open-source VLMs tested, with VILA-8B showing the most pronounced degradation.
- Safety refusal rates decreased significantly for multimodal harmful prompts compared to text-only versions, especially in MedSafetyBench dataset.
- Over 90% of human evaluations rated prompt summarizations and concept extractions as "Great" or "Good," with 93% correct classification for refusal detection.
- The pipeline successfully exposed weaknesses in open-source VLM alignment training, showing they struggle more with typographic multimodal inputs than text-only prompts.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting harmful content across text and image modalities reduces safety refusal rates in open-source VLMs.
- Mechanism: The pipeline extracts salient harmful concepts from text, renders them as a numbered typographic image, and replaces original terms with placeholder tags. The VLM must perform OCR, extract numbered concepts, and semantically reconstruct the full prompt. This reconstruction burden appears to weaken the activation of safety alignment mechanisms trained primarily on text-only harmful patterns.
- Core assumption: Safety alignment training in current VLMs is predominantly conditioned on text-only patterns and does not generalize equivalently when harmful semantics are distributed across modalities.
- Evidence anchors:
  - [abstract]: "Evaluation across four malicious datasets shows that typographic inputs reduce model understanding and decrease safety refusal rates, exposing weaknesses in open-source VLMs' alignment training."
  - [section 4.3.3]: "Text2VLM transformation of malicious text-only prompts reduced the frequency of refusals across all models... When the input was understood by the model, the rate of refusals decreased when using typographic inputs, indicating a higher likelihood of unsafe behavior."
- Break condition: If VLMs were fine-tuned with safety data where harmful content is explicitly distributed across text and image modalities, refusal rates would likely not decrease under Text2VLM inputs.

### Mechanism 2
- Claim: Imperfect alignment between text and image embedding spaces in modular VLM architectures increases vulnerability to multimodal attacks.
- Mechanism: Open-source VLMs typically combine a separately pre-trained vision encoder (e.g., CLIP) with a pre-trained language model, without fully harmonizing their semantic representations. When harmful content is delivered simultaneously through both modalities, the model may inconsistently weight or fuse these inputs, causing the safety classifier to fail to recognize the composite harmful intent.
- Core assumption: The cross-modal attention and fusion layers in modular VLMs do not enforce semantic consistency between text and image embeddings at the level required for robust safety detection.
- Evidence anchors:
  - [section 5]: "A likely explanation, especially in open-source models, is the imperfect alignment between the text and image embedding spaces. This issue arises from the use of modular architectures that combine separately pre-trained components... As a result, models may inconsistently interpret or weight multimodal inputs, increasing the risk of alignment failures."
- Break condition: If VLM architectures used joint pre-training or cross-modal contrastive alignment on safety-labeled multimodal data, the embedding spaces would be more harmonized and this attack surface would shrink.

### Mechanism 3
- Claim: Limited OCR capabilities in open-source VLMs cause both reduced task understanding and inconsistent safety trigger activation for typographic inputs.
- Mechanism: General-purpose VLMs are trained for broad visual understanding, not precise OCR. When typographic images contain multiple numbered harmful concepts, models may fail to extract all terms correctly, leading to partial semantic reconstruction. This partial understanding can fail to trigger refusal responses (because the full harmful intent is not recognized) while still producing relevant outputs to the incomplete prompt.
- Core assumption: Safety classifiers in VLMs rely on recognizing complete harmful patterns; partial or degraded representations are less likely to trigger refusals.
- Evidence anchors:
  - [section 3.1]: "Summarization is essential because general-purpose VLMs have limited Optical Character Recognition (OCR) capabilities. Long prompts containing many salient concepts may exceed these models' OCR limits, reducing their ability to comprehend the task."
- Break condition: If VLMs incorporated specialized OCR modules or were fine-tuned on typographic safety data, understanding rates would improve and safety triggers would activate more reliably on multimodal harmful prompts.

## Foundational Learning

- Concept: **Multimodal Fusion Architectures**
  - Why needed here: Text2VLM exploits how VLMs fuse text and image representations. Understanding encoder-projector-LLM pipelines clarifies why cross-modal semantic consistency is fragile.
  - Quick check question: Can you sketch how a vision encoder's output is projected into the language model's embedding space in a typical VLA architecture?

- Concept: **Safety Alignment Training in LLMs/VLMs**
  - Why needed here: The paper measures whether safety fine-tuning (RLHF, refusal data) generalizes to multimodal attacks. This requires understanding what alignment training optimizes and its distributional assumptions.
  - Quick check question: What type of data predominates in current safety alignment corpora, and what modality gap does this create for VLMs?

- Concept: **Typographic and Cross-Modal Prompt Injection**
  - Why needed here: Text2VLM is a typographic attack variant. Understanding prior work (FigStep, HADES, MM-SafetyBench) positions this method in the attack taxonomy and clarifies its trade-offs.
  - Quick check question: How does a typographic attack differ from an adversarial noise attack in terms of computational cost and transferability?

## Architecture Onboarding

- Component map:
  - Raw prompt -> Summarization (Dolphin-2.9-Llama3-8B) -> Salient concept extraction (GPT-4o-mini) -> Placeholder replacement -> Typographic image generation (Matplotlib) -> Multimodal prompt output

- Critical path:
  1. Input: Raw text-only safety dataset prompt.
  2. If prompt >200 chars → Summarizer → Shortened prompt.
  3. Concept Extractor → List of salient harmful concepts.
  4. Placeholder Replacer → Text with tags.
  5. Image Generator → Typographic image file.
  6. Output: Multimodal prompt (text + image) ready for VLM evaluation.

- Design tradeoffs:
  - **Summarization vs. fidelity**: Long prompts must be summarized to stay within OCR limits, but summarization may alter nuance. Human eval shows 91.25% "Great/Good" ratings but 25% of cases miss ≥1 salient concept.
  - **Typographic vs. generative images**: Typographic ensures generalizability (even abstract concepts like "SQL injection" can be rendered) but may be easier to detect/defend than photorealistic attacks.
  - **LLM-based vs. string-based evaluation**: Relevance uses semantic LLM judge (more nuanced), refusal uses string patterns (faster but brittle). Consider unifying or calibrating.

- Failure signatures:
  - **Image preprocessing distortion**: Few concepts with long text → narrow, tall image → VLM preprocessing stretches/crops → illegible OCR.
  - **Concept extraction drift**: GPT-4o-mini misses implicit harmful concepts → incomplete typographic list → residual harmful text remains in prompt → safety may still trigger.
  - **Classifier miscalibration**: Refusal string list doesn't cover all refusal phrasings → false negatives in refusal detection → overestimates unsafe response rate.

- First 3 experiments:
  1. **Baseline validation**: Run Text2VLM on Vicuna-Bench (benign control). Expect near-identical understanding scores between text-only and multimodal formats. Confirms pipeline doesn't artificially degrade benign task performance.
  2. **Ablation on summarization**: For a subset of long prompts, skip summarization and compare understanding/safety rates. Quantifies the fidelity loss vs. OCR capacity trade-off.
  3. **Cross-model transfer test**: Generate Text2VLM prompts using GPT-4o-mini extraction, then evaluate on LLaVA-7B, LLaVA-34B, VILA-8B, VILA-40B. Compare refusal degradation magnitude. Larger models may show less degradation, suggesting scaling improves cross-modal safety robustness.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do closed-source frontier models compare to open-source VLMs in terms of vulnerability to typographic prompt injection attacks generated by Text2VLM?
  - Basis in paper: [explicit] The Conclusion states, "Future work will also focus on applying Text2VLM to closed-source frontier models, including Claude, Gemini, and GPT, subject to access permissions."
  - Why unresolved: The study was restricted to open-source models (LLaVA, VILA) due to access permissions, leaving the robustness of proprietary models against this specific pipeline unquantified.
  - What evidence would resolve it: Benchmark results from running the Text2VLM pipeline against APIs of major frontier models, reporting understanding and refusal rates.

- **Open Question 2**: Can the salient concept extraction process be refined to eliminate the 25% failure rate where key malicious concepts are missed?
  - Basis in paper: [inferred] The Discussion notes that "in 25% of cases one or more salient concepts were missed by the extraction component," indicating a need for "further refinement" to ensure comprehensive evaluation.
  - Why unresolved: The current extraction component (GPT-4o-mini) occasionally fails to identify all harmful text, resulting in incomplete multimodal prompts that may underestimate the model's true vulnerability.
  - What evidence would resolve it: An updated extraction methodology that achieves near-perfect concept recall in human validation trials.

- **Open Question 3**: How does the mandatory summarization of long prompts affect the efficacy of Text2VLM in complex, multi-turn conversational contexts?
  - Basis in paper: [inferred] The Discussion identifies summarization as a "notable constraint" for "multi-turn conversations," noting that such contexts are "known to increase the likelihood of safety failures" but are currently truncated by the pipeline.
  - Why unresolved: The pipeline summarizes inputs >200 characters to bypass OCR limits, potentially stripping away the nuanced context required to induce failures in many-shot or multi-turn attacks.
  - What evidence would resolve it: A comparative study of attack success rates on full-length multi-turn prompts (using advanced OCR models) versus summarized versions.

## Limitations
- The pipeline's concept extraction occasionally misses harmful concepts (25% failure rate), potentially underestimating true safety vulnerabilities.
- Results are limited to open-source VLMs and may not generalize to proprietary models with different alignment training approaches.
- The typographic attack surface, while generalizable, may be more detectable than photorealistic or adversarial attacks in real-world deployment scenarios.

## Confidence
- **High**: Core finding that multimodal inputs reduce safety refusal rates, supported by consistent experimental results across four malicious datasets.
- **Medium**: Mechanism explanation (embedding-space misalignment and OCR limitations) as contributing factors, not definitively isolated through ablation studies.
- **High**: Human evaluation reliability (93% correct classification, 90%+ "Great/Good" ratings) lending credibility to the measurement framework.

## Next Checks
1. **Ablation on concept extraction quality**: Systematically compare refusal rates when using GPT-4o-mini vs human-verified concept extraction on the same prompts to quantify the impact of extraction errors on safety measurement.

2. **Cross-model scaling analysis**: Evaluate Text2VLM on a broader range of VLMs (including proprietary models) to determine if the multimodal safety degradation persists at scale or is specific to current open-source architectures.

3. **Real-world attack surface validation**: Test Text2VLM-generated prompts against VLM safety filters in production environments (e.g., OpenAI API, Anthropic Claude) to assess practical vulnerability beyond research settings.