---
ver: rpa2
title: Maximizing Uncertainty for Federated learning via Bayesian Optimisation-based
  Model Poisoning
arxiv_id: '2501.08002'
source_url: https://arxiv.org/abs/2501.08002
tags:
- attack
- learning
- data
- uncertainty
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Delphi, a novel model poisoning attack method
  for federated learning that maximizes uncertainty in the global model output. The
  attack exploits the relationship between uncertainty and model parameters in the
  first hidden layer of local models.
---

# Maximizing Uncertainty for Federated learning via Bayesian Optimisation-based Model Poisoning

## Quick Facts
- arXiv ID: 2501.08002
- Source URL: https://arxiv.org/abs/2501.08002
- Reference count: 40
- Primary result: Delphi-BO attack maximizes global model uncertainty by poisoning first-layer weights, reducing mean predictive confidence by half

## Executive Summary
This paper introduces Delphi, a model poisoning attack that maximizes uncertainty in federated learning systems by exploiting the relationship between first-layer parameters and model confidence. The attack employs Bayesian Optimization (Delphi-BO) to find optimal poisoned weights that minimize KL divergence from a crafted uncertain target distribution. The approach is compared against a trust-region method (Delphi-LSTR) and validated against both standard FedAvg and robust Krum aggregation, demonstrating significant uncertainty induction even under defense.

## Method Summary
Delphi targets the first hidden layer of local models in federated learning, selecting neurons via L2 norm gradient sensitivity. Two optimization techniques are employed: Bayesian Optimization with Gaussian Process surrogates and Expected Improvement acquisition (Delphi-BO), and Least Squares Trust Region optimization (Delphi-LSTR). The attack quantifies uncertainty using KL divergence between predicted and crafted uncertain distributions, where the target assigns 0.25 probability to the true class and uniform distribution to others. Experiments use AlexNet on CIFAR10/100 with 6 clients under IID and imbalanced data distributions.

## Key Results
- Delphi-BO outperforms Delphi-LSTR in inducing uncertainty, reducing mean predictive confidence by half
- Dynamic neuron selection is more effective than fixed neuron selection in IID settings
- Attack remains effective against Krum aggregation, though with reduced impact
- Mathematical analysis establishes attack effectiveness bounds for FedAvg

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeting the first hidden layer propagates maximal uncertainty through the network
- Mechanism: Modifies weights in the first hidden layer selected via L2 norm gradient sensitivity, with poisoned parameters cascading through subsequent computations
- Core assumption: Early layers exhibit higher input-output sensitivity than deeper layers
- Evidence anchors: [abstract] relationship between uncertainty and first hidden layer parameters; [section III-B] L2 norm gradient sensitivity justification

### Mechanism 2
- Claim: Bayesian Optimization finds more effective poisoned parameters than gradient-based trust-region methods
- Mechanism: GP surrogate with Matern kernel models black-box weight-uncertainty relationship; Expected Improvement acquisition balances exploration-exploitation
- Core assumption: Mapping from first-layer weights to uncertainty is non-convex and poorly approximated by local quadratic models
- Evidence anchors: [section IV-B] BO surrogate and acquisition function description; [section VI-A] BO outperforms LSTR in both datasets

### Mechanism 3
- Claim: Dynamic neuron selection outperforms fixed neuron selection in non-IID data regimes
- Mechanism: Recomputes gradient-based sensitivity scores each training round, adapting to shifting feature importance as global model evolves
- Core assumption: Neuron importance changes across rounds and is sensitive to data heterogeneity
- Evidence anchors: [abstract] dynamically searched neurons more effective than fixed selection; [section VI-C] DS neuron selection leads to lower predictive confidence in IID settings

## Foundational Learning

- **Federated Averaging (FedAvg)**: Core aggregation mechanism where local updates are averaged to update global model. Needed to understand attack effectiveness bounds. Quick check: If 3 of 6 clients are malicious sending perturbation δ, what is the aggregate expected perturbation under FedAvg?

- **KL Divergence as Uncertainty Metric**: Delphi minimizes KL divergence between predictive distribution and crafted "uncertain" target distribution Z. Needed to understand attack objective. Quick check: Why does minimizing KL(y_hat || Z) push y_hat toward Z, and what is Z for 10-class problem?

- **Gaussian Process Surrogate Models**: Delphi-BO uses GP regression to approximate expensive black-box function mapping weights to uncertainty. Needed to understand Bayesian Optimization approach. Quick check: What is computational bottleneck in GP-based BO as observations n grows?

## Architecture Onboarding

- **Component map**: Global model broadcast -> Compute gradient sensitivity -> Select top-k neurons -> Run optimization -> Inject poisoned weights -> Submit to aggregator -> Evaluate uncertainty

- **Critical path**: 1) Global model broadcast → 2) Compute gradient sensitivity → 3) Select top-k neurons (fixed or dynamic) → 4) Run optimization (T iterations for BO, convergence for LSTR) → 5) Inject poisoned weights → 6) Submit to aggregator → 7) Evaluate uncertainty via KL divergence

- **Design tradeoffs**:
  - Fixed vs. dynamic neurons: Fixed is computationally cheaper; dynamic adapts to model drift but incurs gradient computation each round
  - BO vs. LSTR: BO achieves higher uncertainty but has O(n³) complexity per iteration; LSTR is O(d³) and may scale better for high-dimensional layers with few observations
  - Attack budget δw: Larger perturbations increase effectiveness but risk detection by robust aggregators like Krum

- **Failure signatures**:
  - Mean predictive confidence remains above 0.5 despite attack → likely insufficient neuron coverage or observation budget
  - Krum aggregator excludes attacker → weight vector too distant from benign cluster; reduce perturbation magnitude
  - BO surrogate variance explodes → insufficient initial samples; increase warm-start observations before acquisition

- **First 3 experiments**:
  1. Baseline calibration: Run Delphi-BO on CIFAR10 with FedAvg, 2 attackers, 5 dynamic neurons, 50 rounds. Log accuracy, mean predictive confidence, and entropy
  2. Optimizer comparison: Compare Delphi-BO vs. Delphi-LSTR across 1, 2, and 3 attackers under fixed neurons with imbalanced data
  3. Defense probing: Deploy Delphi-BO against Krum aggregation with IID and imbalanced data. Measure degradation and identify perturbation threshold where Krum excludes malicious clients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical upper bound of attack effectiveness be mathematically derived for Byzantine-resilient aggregation rules (e.g., Krum, Trimmed Mean) as it was for FedAvg?
- Basis: [explicit] Future work will expand attack effectiveness and derive mathematical proofs for different aggregation functions and FL schemes
- Why unresolved: Current mathematical analysis specifically derives bound based on FedAvg mechanics, not distance-based selection logic of robust aggregators
- What evidence would resolve it: Formal derivation of expected perturbation bounds and attack effectiveness specifically for Krum or similar robust aggregation functions

### Open Question 2
- Question: How does the Delphi-BO attack strategy perform when applied to modern deep learning architectures such as Transformers?
- Basis: [explicit] Future work will provide comprehensive analysis for variety of DL models, such as transformers
- Why unresolved: Empirical evaluation restricted to AlexNet (CNN), leaving attack's efficacy on attention mechanisms and non-convolutional layers untested
- What evidence would resolve it: Experimental results showing KL divergence and mean predictive confidence when attacking Vision Transformer or BERT model in federated setting

### Open Question 3
- Question: Does expanding the attack to manipulate multiple hidden layers significantly improve penetration against robust aggregators without increasing detectability?
- Basis: [inferred] Krum resisted single-layer attack; expansion to rest of layers suggested for improvement
- Why unresolved: Current methodology restricts manipulation to first hidden layer to exploit specific sensitivity properties; multi-layer trade-off unknown
- What evidence would resolve it: Ablation studies comparing detection rates and success rates of single-layer versus multi-layer poisoning attacks against Krum

### Open Question