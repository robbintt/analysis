---
ver: rpa2
title: 'Performance of LLMs on Stochastic Modeling Operations Research Problems: From
  Theory to Practice'
arxiv_id: '2506.23924'
source_url: https://arxiv.org/abs/2506.23924
tags:
- problems
- problem
- llms
- time
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models were tested on two core types of stochastic
  modeling problems: theoretical homework problems and simulation-optimization problems
  from the SimOpt benchmark. On the theoretical side, state-of-the-art models (o1,
  o3-mini, Claude 3.5 Sonnet) scored between 74% and 96% on graduate-level problems
  and over 84% on PhD qualification-exam problems, demonstrating performance comparable
  to human experts.'
---

# Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice

## Quick Facts
- **arXiv ID**: 2506.23924
- **Source URL**: https://arxiv.org/abs/2506.23924
- **Reference count**: 40
- **Key outcome**: LLMs achieved 74%-96% accuracy on graduate-level stochastic modeling problems and near-optimal solutions on simulation-optimization tasks

## Executive Summary
This study evaluates large language models on stochastic modeling operations research problems, testing both theoretical homework problems and simulation-optimization tasks from the SimOpt benchmark. State-of-the-art models like o1, o3-mini, and Claude 3.5 Sonnet demonstrated strong performance on theoretical problems, scoring between 74% and 96% on graduate-level questions and over 84% on PhD qualification exams. On simulation-optimization tasks, Claude 3.5 Sonnet achieved near-optimal solutions for most problems, while other models showed varying degrees of success depending on the specific problem type and implementation strategy.

## Method Summary
The evaluation employed two distinct testing frameworks: theoretical problems consisting of 20 graduate-level homework problems and 20 PhD-level qualification exam questions, and simulation-optimization problems using 10 benchmarks from the SimOpt suite. The theoretical problems were designed to test understanding of stochastic modeling concepts, while the simulation-optimization problems required practical implementation of optimization algorithms. Multiple state-of-the-art LLMs were tested across both domains, with performance measured through accuracy scores for theoretical problems and solution quality metrics for optimization tasks.

## Key Results
- State-of-the-art LLMs (o1, o3-mini, Claude 3.5 Sonnet) achieved 74%-96% accuracy on graduate-level theoretical problems
- All tested models scored over 84% accuracy on PhD qualification exam problems
- Claude 3.5 Sonnet achieved near-optimal solutions on most simulation-optimization benchmark problems
- Model performance varied significantly across different problem types, with some struggling on specific implementation challenges

## Why This Works (Mechanism)
The strong performance of LLMs on stochastic modeling problems appears to stem from their extensive training on mathematical and operations research literature, which provides them with conceptual understanding of stochastic processes and optimization techniques. The ability to chain reasoning across multiple steps and leverage knowledge from diverse domains enables these models to tackle complex theoretical problems. For simulation-optimization tasks, the models can generate and adapt optimization algorithms based on their understanding of problem structure, though their success depends heavily on the specific implementation approach and problem characteristics.

## Foundational Learning
The paper demonstrates that LLMs have acquired substantial foundational knowledge in stochastic modeling through pretraining on academic literature and technical documents. This knowledge transfer enables them to understand and solve complex theoretical problems that require deep conceptual understanding of probability theory, queueing systems, and stochastic processes. The models appear to have internalized core principles of operations research, allowing them to apply appropriate methodologies to different problem types. However, their performance on practical implementation tasks suggests that while they possess theoretical knowledge, translating this into effective code and optimization strategies remains challenging for some models.

## Architecture Onboarding
The study implicitly highlights how different LLM architectures handle stochastic modeling tasks, though specific architectural details are not provided. The varying performance across models suggests that architecture plays a significant role in their ability to solve these problems. Models with stronger reasoning capabilities and better code generation abilities tend to perform better on simulation-optimization tasks. The success of models like Claude 3.5 Sonnet on practical problems indicates that certain architectural designs may be better suited for translating theoretical understanding into effective implementation strategies.

## Open Questions the Paper Calls Out
The paper raises several important open questions regarding the capabilities and limitations of LLMs in stochastic modeling. How do different training approaches and data compositions affect performance on theoretical versus practical problems? What are the fundamental limitations of current LLM architectures in handling high-dimensional stochastic optimization problems? How can we better evaluate and benchmark LLM performance on complex operations research tasks? The study also questions whether the observed performance improvements are due to genuine understanding or pattern matching from training data, particularly for theoretical problems that require deep conceptual reasoning.

## Limitations
- Evaluation relied on a relatively small set of theoretical problems (20 homework and 20 PhD-level questions)
- Simulation-optimization evaluation used only 10 SimOpt benchmark problems
- Performance varied significantly across different LLMs and problem types
- The study did not investigate the impact of different prompting strategies on performance
- Limited analysis of why certain models struggled with specific problem types
- No examination of computational efficiency or resource requirements for different approaches

## Confidence
- **Theoretical problems**: High confidence - consistent achievement above 84% accuracy across multiple state-of-the-art models
- **Simulation-optimization**: Medium confidence - significant variation in performance across models and problem types

## Next Checks
1. Expand evaluation to include a broader range of stochastic modeling problems, particularly those with higher dimensionality and more complex constraints
2. Test additional LLM models with varying architectures and training approaches to identify which approaches work best for different problem types
3. Conduct longitudinal studies to assess how performance changes as models are updated and whether improvements transfer across both theoretical and practical problem domains
4. Investigate the impact of different prompting strategies and few-shot examples on performance
5. Analyze the computational efficiency and resource requirements of different approaches to identify practical implementation considerations
6. Explore hybrid approaches that combine LLM reasoning with traditional optimization algorithms for improved performance on complex problems