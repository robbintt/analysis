---
ver: rpa2
title: 'Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily
  Identifiable Unrelated Padding)'
arxiv_id: '2502.17169'
source_url: https://arxiv.org/abs/2502.17169
tags:
- evidence
- retrieval
- reasoning
- language
- premise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new methodology for evaluating long-context
  logical reasoning in large language models. Instead of using easily identifiable
  unrelated padding, it generates extensive simplified English text paired with first-order
  logic representations spanning up to 2048 clauses (around 25k tokens).
---

# Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding)

## Quick Facts
- arXiv ID: 2502.17169
- Source URL: https://arxiv.org/abs/2502.17169
- Reference count: 8
- Primary result: Models' effective context windows are much smaller than claimed, with performance degrading significantly at just 128 clauses

## Executive Summary
This paper introduces a novel methodology for evaluating long-context logical reasoning in large language models. Unlike previous approaches that used easily identifiable unrelated padding, this method generates extensive simplified English text paired with first-order logic representations spanning up to 2048 clauses. The evaluation focuses on evidence retrieval for contradiction detection, requiring models to identify which premise clauses contradict a given hypothesis. The dataset is constructed using grammars to ensure logical consistency while creating challenging distractors.

The results reveal that current models have much smaller effective context windows than their claimed capabilities suggest. Performance degrades significantly even at relatively modest clause counts, indicating that previous evaluations may have substantially overestimated language models' long-context processing abilities. The methodology addresses a critical gap in long-context evaluation by preventing models from exploiting easily identifiable padding to simplify the task.

## Method Summary
The methodology generates synthetic datasets using grammars to create logically consistent text paired with first-order logic representations. Each instance contains a hypothesis and multiple premise clauses (up to 2048), where models must identify which premises contradict the hypothesis. The synthetic generation ensures controlled conditions while preventing models from using padding identification strategies. The approach uses simplified English and creates challenging distractors that are semantically related to the hypothesis, making the retrieval task genuinely difficult. The evaluation measures precision and recall at different clause thresholds to assess effective context window sizes.

## Key Results
- Model performance degrades significantly at just 128 clauses when using realistic distractors
- Effective context windows are much smaller than claimed model capabilities
- Previous evaluations that used easily identifiable padding substantially overestimated long-context reasoning abilities
- The methodology reveals genuine limitations in long-context logical reasoning that simpler padding-based evaluations missed

## Why This Works (Mechanism)
The methodology works by eliminating the shortcut that models could use in previous evaluations: identifying and ignoring unrelated padding. By generating semantically related distractors that are logically consistent with the hypothesis but don't contradict it, the task requires genuine reasoning rather than pattern matching or padding detection. The synthetic generation ensures that all clauses are in the model's vocabulary and that logical relationships are properly represented, while the controlled grammar-based approach allows systematic variation of context length to measure degradation patterns.

## Foundational Learning
**First-order logic representation**: The study uses logical representations to encode relationships between statements, allowing precise definition of contradiction and consistency. Why needed: Enables unambiguous specification of logical relationships for evaluation. Quick check: Verify that the FOL representation correctly captures the intended logical relationships in sample sentences.

**Evidence retrieval task**: Models must identify relevant premises that contradict a hypothesis from a large set of candidates. Why needed: Provides a quantifiable measure of reasoning capability that scales with context length. Quick check: Test that retrieval performance degrades predictably as noise increases.

**Synthetic data generation with grammars**: Uses formal grammars to generate logically consistent text and corresponding logical representations. Why needed: Ensures controlled conditions while preventing models from exploiting dataset patterns. Quick check: Verify logical consistency of generated instances through automated checking.

## Architecture Onboarding
**Component map**: Grammar generator -> FOL encoder -> English text generator -> Premise set assembler -> Model evaluation pipeline

**Critical path**: The generation pipeline (grammar -> FOL -> English) feeds into the evaluation pipeline where models process premise sets and output contradiction judgments. The bottleneck is the model's attention mechanism when processing long sequences with semantically related distractors.

**Design tradeoffs**: Synthetic data provides control but may miss real-world complexity; simplified English ensures coverage but may be too artificial; grammar-based generation ensures consistency but could create exploitable patterns.

**Failure signatures**: Sharp performance drops at specific clause thresholds indicate attention limitations; consistent errors on certain logical patterns reveal reasoning gaps; successful identification of unrelated padding in previous studies but failure with related distractors shows the importance of evaluation methodology.

**First experiments**:
1. Test model performance on varying numbers of clauses (16, 32, 64, 128, 256) to establish degradation curves
2. Compare performance with semantically related versus unrelated distractors to isolate the impact of distractor relevance
3. Evaluate different prompt strategies (chain-of-thought vs direct) to assess whether prompting affects long-context reasoning

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic nature of the dataset may not fully capture real-world logical reasoning complexity
- Grammar-based generation might create exploitable patterns that don't reflect genuine reasoning capabilities
- The focus on contradiction detection may not generalize to other types of logical reasoning tasks
- The distinction between attention mechanism limitations and actual reasoning degradation remains unclear

## Confidence
- High confidence in the methodology's validity and its advantages over previous approaches
- Medium confidence in the interpretation of performance degradation as evidence of limited effective context windows
- Medium confidence in the generalizability of findings to real-world long-context reasoning tasks

## Next Checks
1. Test the evaluation methodology on naturally occurring long documents (e.g., legal texts, academic papers) to verify that performance patterns hold outside synthetic datasets

2. Conduct ablation studies varying the complexity of logical relationships (beyond simple contradiction detection) to understand whether the observed limitations extend to more sophisticated reasoning tasks

3. Implement controlled experiments comparing model performance when distractors are semantically related versus unrelated to the hypothesis, to isolate the impact of distractor relevance on reasoning capabilities