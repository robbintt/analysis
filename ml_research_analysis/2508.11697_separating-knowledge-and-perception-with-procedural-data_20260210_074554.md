---
ver: rpa2
title: Separating Knowledge and Perception with Procedural Data
arxiv_id: '2508.11697'
source_url: https://arxiv.org/abs/2508.11697
tags:
- procedural
- data
- mixup
- real
- shaders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to visual representation learning
  using procedural data and visual memory. Instead of training models on real-world
  images, the authors generate non-realistic images via code and train self-supervised
  vision transformers on them.
---

# Separating Knowledge and Perception with Procedural Data

## Quick Facts
- arXiv ID: 2508.11697
- Source URL: https://arxiv.org/abs/2508.11697
- Reference count: 40
- Key outcome: Procedural data + visual memory achieves strong classification/segmentation without real data training, enabling perfect compartmentalization

## Executive Summary
This paper presents a novel approach to visual representation learning using procedural data and visual memory. Instead of training models on real-world images, the authors generate non-realistic images via code and train self-supervised vision transformers on them. They then apply k-nearest neighbors (KNN) classification and segmentation using a database of reference image embeddings (visual memory) without further training. The approach achieves strong performance on fine-grained classification tasks while enabling perfect data compartmentalization, efficient unlearning, and privacy guarantees.

## Method Summary
The method involves generating procedural training data through OpenGL shaders, applying Shaders KML Mixup augmentation (K-Means masking + Interpolation), training a ViT-Small with DINO self-supervised learning on this data, and building a visual memory of real image embeddings. For inference, query images are embedded and classified via KNN search over the visual memory. The key novelty is using procedural data to train a "pure perception" feature extractor while storing all semantic knowledge externally in the visual memory database.

## Key Results
- Procedural models trained on Shaders KML Mixup outperform Places-trained models on fine-grained classification tasks (CUB200, Flowers102) by 8-15%
- Achieve ImageNet-1K classification within 10% of Places-trained models
- Demonstrate strong zero-shot segmentation performance within 10% of real-data models on COCO
- Achieve perfect compartmentalization with respect to real-world data while retaining strong performance

## Why This Works (Mechanism)

### Mechanism 1: Perception-Knowledge Decoupling via Procedural Priors
Training on procedural data forces the model to learn "pure perception" skills (texture, edge detection) without encoding semantic knowledge of real-world entities, creating a domain-agnostic feature space. Standard SSL on natural images causes representations to cluster by semantic objects, while procedural data optimizes only for geometric and textural similarity.

### Mechanism 2: Non-Parametric Knowledge Injection (Visual Memory)
Replacing a parametric classification head with KNN search over an explicit database allows knowledge to be added or removed without retraining. The procedural model acts as a fixed feature extractor while knowledge is stored explicitly as embeddings in a database.

### Mechanism 3: Data-Driven Masking for Diversity (Shaders KML)
Generating mixing masks via K-Means clustering on procedural images increases data diversity and reduces shortcut learning better than standard constant-masking Mixup. KML extracts irregular shapes from image content to define mixing boundaries, forcing the model to handle more complex occlusion boundaries.

## Foundational Learning

- **Concept**: Self-Supervised Learning (DINO)
  - Why needed: The paper relies on DINO to train the feature extractor
  - Quick check: How does the DINO loss function encourage local-to-global consistency in the absence of labels?

- **Concept**: k-Nearest Neighbors (KNN) Classification
  - Why needed: This is the inference engine
  - Quick check: What happens to the inference latency if the size of the Visual Memory doubles, and how does this compare to retraining a linear classifier?

- **Concept**: Gestalt Principles (Closure, Continuity, Proximity)
  - Why needed: The paper explicitly evaluates the absence of these principles in procedural models
  - Quick check: Why would a model that perceives "similarity" fail to group the spokes and hub of a wheel into a single object if it lacks "Closure"?

## Architecture Onboarding

- **Component map**: Procedural Generator -> Shaders KML Mixup -> ViT-S (DINO) -> Visual Memory -> Inference Client
- **Critical path**: The Shaders KML Mixup generation is the specific novelty driving performance
- **Design tradeoffs**: Storage vs. Compute (shifting burden from GPU compute to Storage/IO), Privacy vs. Gestalt (maximizing privacy breaks object-level perception)
- **Failure signatures**: The "Spoke" Problem (distinct parts of same object appear as different colors), Incorrect Retrieval (retrieving images with similar textures but wrong semantics)
- **First 3 experiments**:
  1. Ablate the Mask: Train two models, one with standard Mixup and one with KML Mixup, and compare Top-1 accuracy on Flowers102
  2. Unlearning Speed Test: Remove a specific class from Visual Memory and immediately run inference to verify performance drops to zero
  3. Gestalt Stress Test: Run PCA visualizations on images with strong Gestalt cues to confirm the model fails to connect implied contours

## Open Questions the Paper Calls Out

### Open Question 1
How can procedural training be adapted to create coherent object-level representations rather than just local features? The authors note they "leave finding ways of addressing these limitations" - specifically that models fail to identify objects as single entities.

### Open Question 2
Can self-supervised models be trained to capture Gestalt principles like continuity and closure, which are currently absent in both real and procedural models? The analysis finds current models "do not seem to have gestalt perception," failing on illusions humans easily parse.

### Open Question 3
Does increasing model capacity and procedural data scale fully close the performance gap with models trained on real-world data? The study only scales up to ViT-Small; it remains untested if larger models overcome the locality bias inherent in procedural data.

## Limitations

- The exact nature of what constitutes "pure perception" versus semantic knowledge remains underspecified
- KML Mixup lacks ablation studies showing its specific contribution versus other data augmentation strategies
- The evaluation focuses primarily on classification accuracy metrics without deeper analysis of embedding space structure

## Confidence

- **High confidence**: Claims about KNN-based inference enabling efficient data unlearning and privacy preservation
- **Medium confidence**: Claims about procedural data creating domain-agnostic features
- **Medium confidence**: Claims about Shaders KML Mixup improving generalization

## Next Checks

1. Ablation on mask generation: Compare performance using Shaders KML Mixup against standard Mixup, random masks, and no Mixup
2. Embedding space analysis: Conduct t-SNE or UMAP analyses on the embedding space to quantify how procedural models cluster semantic concepts versus real-data models
3. Cross-dataset generalization test: Evaluate the procedural model's KNN performance on a dataset from a completely different domain (e.g., medical imaging) to test the universality of the "pure perception" features