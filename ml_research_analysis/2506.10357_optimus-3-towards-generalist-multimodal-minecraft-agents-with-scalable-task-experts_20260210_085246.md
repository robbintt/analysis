---
ver: rpa2
title: 'Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task
  Experts'
arxiv_id: '2506.10357'
source_url: https://arxiv.org/abs/2506.10357
tags:
- craft
- task
- minecraft
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Optimus-3, a generalist multimodal agent for
  Minecraft that integrates perception, planning, action, grounding, and reflection
  capabilities. The key challenges addressed are insufficient domain-specific data,
  task interference in heterogeneous learning, and visual diversity in open-world
  environments.
---

# Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts

## Quick Facts
- arXiv ID: 2506.10357
- Source URL: https://arxiv.org/abs/2506.10357
- Authors: Zaijing Li; Yuquan Xie; Rui Shao; Gongwei Chen; Weili Guan; Dongmei Jiang; Liqiang Nie
- Reference count: 40
- Primary result: Achieves SOTA performance across all evaluated tasks, surpassing existing agents by 20% on planning, 66% on captioning, 76% on embodied QA, 3.4x on grounding, and 18% on reflection tasks.

## Executive Summary
This paper presents Optimus-3, a generalist multimodal agent for Minecraft that integrates perception, planning, action, grounding, and reflection capabilities. The key challenges addressed are insufficient domain-specific data, task interference in heterogeneous learning, and visual diversity in open-world environments. The proposed approach introduces a knowledge-enhanced automated data generation pipeline, a task-level routing Mixture-of-Experts architecture to prevent task interference, and a Multimodal Reasoning-Augmented Reinforcement Learning method to improve visual reasoning. Experimental results show that Optimus-3 achieves state-of-the-art performance across all evaluated tasks.

## Method Summary
Optimus-3 uses a three-phase training approach on 8x NVIDIA L40 GPUs: (1) SFT with 230k samples, lr=5e-5, 2 epochs; (2) Multimodal reasoning tuning with 58k samples, lr=3e-5, 2 epochs; (3) GRPO RL with 5k samples, lr=1e-6, 20 epochs. The architecture employs a ViT encoder with a task-level routing MoE LLM containing 1 shared expert and 5 task-specific experts (planning, perception, action, grounding, reflection), integrated with a VPT action head for low-level control. The knowledge-enhanced data generation pipeline uses expert models (DeepSeek-VL2, Grounding DINO) with environment feedback to reduce hallucinations, while the Multimodal Reasoning-Augmented RL approach uses Chain-of-Thought reasoning followed by GRPO with IoU-Density rewards.

## Key Results
- 20% improvement over existing agents on planning tasks
- 66% improvement on captioning tasks
- 76% improvement on embodied QA tasks
- 3.4x improvement on grounding tasks
- 18% improvement on reflection tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-level routing in MoE architectures reduces interference among heterogeneous tasks compared to token-level routing.
- Mechanism: A task router classifies each instruction to a task type before model processing. Each query activates only one task-specific expert plus a shared knowledge expert. The shared expert captures cross-task generalization while task experts specialize.
- Core assumption: Heterogeneous tasks (action generation vs. captioning vs. grounding) have conflicting optimization gradients that degrade dense-architecture models.
- Evidence anchors:
  - [abstract]: "To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing."
  - [section 3.2]: "Each task is learned by updating only the parameters of its corresponding expert, thereby avoiding interference among heterogeneous tasks."
  - [corpus]: Weak direct corpus support; related work on MoE routing exists but does not specifically validate task-level vs. token-level in game agents.
- Break condition: If tasks share substantial underlying representations or if the task router misclassifies frequently, task-level routing may underperform token-level.

### Mechanism 2
- Claim: Knowledge-enhanced data generation with environment feedback reduces hallucinations and improves task-specific data quality.
- Mechanism: Expert models (e.g., DeepSeek-VL2, Grounding DINO) annotate sampled frames using environment feedback (agent state, inventory, surrounding objects) as ground truth. This grounds captions and QA pairs in verifiable world state.
- Core assumption: Providing models with environment state during annotation reduces factual errors compared to unconstrained generation.
- Evidence anchors:
  - [abstract]: "We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data."
  - [section 3.1]: "By providing the caption model with information about the objects present in the scene, the incidence of hallucinations in the generated captions is significantly reduced."
  - [section 4.3]: "When we remove the expert models and environmental feedback from the data generation pipeline, the performance drops by 81% on Planning."
  - [corpus]: No direct corpus comparison for this specific pipeline; evidence is internal to the paper.
- Break condition: If environment feedback is noisy or delayed, or if expert annotators are biased, data quality may not improve.

### Mechanism 3
- Claim: Explicit multimodal reasoning chains followed by reinforcement learning improve vision-grounded task performance.
- Mechanism: Model is fine-tuned to generate visual descriptions before answering (Chain-of-Thought). GRPO reinforcement learning with an IoU-Density Reward further refines grounding by iteratively sampling outputs and optimizing against reward.
- Core assumption: Forcing the model to describe visual content before answering directs attention to observations and reduces ungrounded responses.
- Evidence anchors:
  - [abstract]: "We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity."
  - [section 3.3]: "Removing the multimodal reasoning phase and the reinforcement learning phase results in a performance drop of 26% and 16% in Grounding, respectively."
  - [corpus]: Corpus supports RL for reasoning in agents (e.g., DeepSeek-R1) but not specifically the IoU-Density reward in Minecraft.
- Break condition: If reasoning templates are mechanically applied without genuine grounding, or if reward shaping is misaligned, RL may amplify surface patterns rather than true reasoning.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) routing
  - Why needed here: Optimus-3 uses MoE with task-level routing to avoid interference among heterogeneous tasks.
  - Quick check question: Can you explain the difference between token-level and task-level routing in MoE models?

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: GRPO is the RL algorithm used to refine reasoning without a separate value function.
  - Quick check question: How does GRPO differ from PPO in terms of baseline estimation?

- Concept: Chain-of-Thought (CoT) reasoning in multimodal models
  - Why needed here: Multimodal reasoning fine-tuning uses CoT templates to force visual description before answering.
  - Quick check question: Why might CoT improve grounding in visual question answering?

## Architecture Onboarding

- Component map: ViT encoder (visual input) → Text Tokenizer → Task Router (Sentence-BERT) → MoE LLM (shared expert + 5 task experts: planning, perception, action, grounding, reflection) → Action Head (VPT) for low-level control. Outputs include text responses and action sequences.
- Critical path: Instruction and image → Task router classifies task type → Corresponding task expert + shared expert activated → Output generated (text or action). For long-horizon tasks, planning generates sub-goals executed sequentially.
- Design tradeoffs: Task-level routing reduces interference but requires accurate task classification; token-level routing is more flexible but risks expert overlap. Shared expert enables cross-task transfer but may dilute specialization if over-weighted.
- Failure signatures: (1) Task router misclassification leads to wrong expert activation and poor performance; (2) Data without environment feedback causes hallucinations in captioning/QA; (3) Skipping reasoning phase drops grounding accuracy by 26% (per ablation).
- First 3 experiments:
  1. Replicate the task-level vs. token-level routing comparison on a subset of tasks (Planning, Captioning, Grounding) to verify reduced interference.
  2. Ablate environment feedback in the data pipeline for captioning and measure hallucination rate using the LLM-as-Judge protocol.
  3. Train with and without the multimodal reasoning fine-tuning phase on grounding tasks and compare IoU@0.5 scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Optimus-3 be extended with a memory module to enable lifelong learning and self-evolution in Minecraft without catastrophic forgetting of previously learned tasks?
- Basis in paper: [explicit] "due to the absence of a memory module, Optimus-3 lacks the ability for life-long learning and self-evolution. Equipping Optimus-3 with the capability to autonomously explore and learn new tasks remains a promising direction for future research."
- Why unresolved: The current architecture has no mechanism to store and retrieve past experiences across sessions, limiting agents to fixed capabilities post-training.
- What evidence would resolve it: A modified Optimus-3 architecture with memory that can learn new Minecraft tasks (e.g., novel redstone circuits) while maintaining performance on original tasks after extended interaction.

## Limitations

- Reliance on internally generated datasets without independent validation of data quality or task distribution balance
- Performance improvements depend heavily on the proposed data generation pipeline with limited external benchmark validation
- Unspecified hyperparameters in the IoU-Density Reward formulation (α, β, η) and incomplete task router training details

## Confidence

- **High Confidence**: The fundamental MoE architecture with task-level routing is well-established in the literature, and the reported improvements over dense architectures align with expected benefits of reducing task interference.
- **Medium Confidence**: The multimodal reasoning chain and GRPO reinforcement learning approach is supported by related work on Chain-of-Thought reasoning and GRPO in agent training, though the specific IoU-Density reward formulation for Minecraft grounding is novel.
- **Low Confidence**: The knowledge-enhanced data generation pipeline's effectiveness claims rely entirely on internal ablation studies without independent verification of hallucination reduction or data quality improvements.

## Next Checks

1. **Task Router Robustness**: Test the Sentence-BERT task classifier on a held-out instruction set with ambiguous or multi-task instructions to measure misclassification rates and their impact on expert activation accuracy.

2. **Cross-Environment Generalization**: Evaluate Optimus-3's performance on a different open-world game (e.g., NetHack Learning Environment) using the same MoE architecture and data generation pipeline to assess architectural generalizability beyond Minecraft.

3. **Data Quality Verification**: Implement an independent hallucination detection protocol using multiple LLM judges to verify the claimed reduction in factual errors when environment feedback is included in the data generation pipeline.