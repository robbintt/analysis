---
ver: rpa2
title: Geometry of Decision Making in Language Models
arxiv_id: '2511.20315'
source_url: https://arxiv.org/abs/2511.20315
tags:
- llama2
- gpt-2
- gpt-neo
- llama3
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Large Language Models (LLMs) make decisions
  by analyzing the intrinsic dimension (ID) of hidden representations across layers.
  Using 28 open-weight transformer models, the authors estimate ID at each layer with
  multiple estimators (MLE, TwoNN, GRIDE) while tracking performance on multiple-choice
  question answering (MCQA) tasks.
---

# Geometry of Decision Making in Language Models

## Quick Facts
- arXiv ID: 2511.20315
- Source URL: https://arxiv.org/abs/2511.20315
- Authors: Abhinav Joshi; Divyanshu Bhatt; Ashutosh Modi
- Reference count: 40
- Primary result: Intrinsic dimension peaks in middle layers and correlates with decision confidence in LLMs

## Executive Summary
This study investigates how Large Language Models make decisions by analyzing the intrinsic dimension (ID) of hidden representations across layers. Using 28 open-weight transformer models, the authors estimate ID at each layer with multiple estimators while tracking performance on multiple-choice question answering tasks. They find a consistent "hunchback" pattern where ID rises in early layers, peaks in middle layers, and declines in later layers. This peak coincides with the onset of confident predictions, suggesting ID as a geometric marker of decision-making.

## Method Summary
The authors estimate intrinsic dimension (ID) of hidden representations in transformer models using MLE, TwoNN, and GRIDE estimators across all layers. They analyze 28 open-weight models on multiple-choice question answering (MCQA) tasks, tracking ID trajectories alongside accuracy metrics. The study employs few-shot prompting to examine how demonstration samples affect ID dynamics, and compares residual post-activations with MLP outputs to isolate information compression sources. Computational constraints limited experiments to 7B parameter models.

## Key Results
- Intrinsic dimension shows a "hunchback" pattern: rising in early layers, peaking in middle layers, declining in later layers
- ID peaks coincide with onset of confident predictions, suggesting geometric markers of decision-making
- MLP outputs show sharper ID transitions than residual post-activations, indicating their role in task-specific refinement
- Few-shot prompting accelerates compression, reducing final-layer ID and improving accuracy

## Why This Works (Mechanism)
The humpback ID pattern reflects the information processing pipeline in transformers: early layers perform general linguistic processing with expanding representational capacity, middle layers extract task-specific features with maximal dimensionality, and later layers compress information into decision-ready representations. MLP layers drive sharper ID transitions by injecting specialized transformations that refine representations for prediction. Few-shot prompts accelerate this compression by providing task-relevant exemplars that guide the model toward more efficient representations. The coupling between ID peaks and decision confidence suggests that LLMs make decisions when their internal representations achieve optimal task-specific dimensionality.

## Foundational Learning

**Intrinsic Dimension (ID)**: The minimum number of parameters needed to describe a dataset's manifold. Needed to quantify representational efficiency; check by comparing ID estimates across different estimators.

**MLE, TwoNN, GRIDE estimators**: Statistical methods for estimating ID from distance distributions in embedding space. Needed for robust ID measurement; check by verifying consistency across estimators.

**MLP layers vs residual post-activations**: Different sources of representational updates in transformers. Needed to isolate information compression sources; check by comparing ID dynamics across these outputs.

**Few-shot prompting**: Providing demonstrations to guide model behavior. Needed to study prompting effects on representation geometry; check by measuring ID changes with varying shot counts.

## Architecture Onboarding

**Component map**: Input tokens -> Embedding layer -> Multiple transformer blocks (Attention + MLP) -> Final classifier -> Output predictions

**Critical path**: Embedding -> Transformer blocks (attn+mlp) -> MLP output -> Decision boundary crossing

**Design tradeoffs**: ID estimation accuracy vs computational cost; model scale vs probing feasibility; task complexity vs interpretability

**Failure signatures**: ID peaks not aligning with accuracy gains; inconsistent patterns across estimators; MLP contributions masked by residual dynamics

**First experiments**:
1. Compare ID trajectories across different MCQA datasets to test pattern robustness
2. Isolate MLP layer contributions by ablating residual connections
3. Vary few-shot prompt structure to map prompt geometry to ID compression

## Open Questions the Paper Calls Out
None

## Limitations
- Humpback pattern may reflect architectural biases rather than universal principles
- Estimator-specific artifacts cannot be fully disambiguated
- Analysis limited to MCQA tasks, limiting generalizability to generative scenarios
- Computational constraints necessitated using smaller models (7B parameters)

## Confidence
- High confidence: Empirical observation of humpback ID patterns and correlation with decision boundaries
- Medium confidence: Causal claims about MLP layers driving ID transitions and few-shot prompting effects
- Medium confidence: Interpretations linking ID peaks to "decision-making" versus simpler explanations

## Next Checks
1. Test humpback patterns in generative tasks (story completion, code generation) with less discrete decision boundaries
2. Systematically ablate MLP layers to isolate their specific contribution to ID transitions
3. Compare ID trajectories in alternative architectures (RNNs, CNNs, state-space models) to assess transformer-specificity