---
ver: rpa2
title: 'Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer
  Attention'
arxiv_id: '2508.19414'
source_url: https://arxiv.org/abs/2508.19414
tags:
- heads
- layer
- format
- features
- even
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates a format-dependent reasoning failure in
  Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger than
  "9.8" in chat or Q&A formats but answers correctly in simple format. Through systematic
  intervention experiments, the authors discover that transformers organize computation
  by attention head index parity: even-indexed heads handle numerical comparison while
  odd-indexed heads serve incompatible functions.'
---

# Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention

## Quick Facts
- **arXiv ID:** 2508.19414
- **Source URL:** https://arxiv.org/abs/2508.19414
- **Reference count:** 40
- **Primary result:** Format-dependent reasoning failure in Llama-3.1-8B-Instruct fixed by transplanting attention patterns from even heads at Layer 10

## Executive Summary
This paper investigates a format-dependent reasoning failure in Llama-3.1-8B-Instruct where the model incorrectly judges "9.11" as larger than "9.8" in chat or Q&A formats but answers correctly in simple format. Through systematic intervention experiments, the authors discover that transformers organize computation by attention head index parity: even-indexed heads handle numerical comparison while odd-indexed heads serve incompatible functions. Perfect repair requires exactly 8 even heads at Layer 10, with sharp phase transitions at both the 8-head minimum and 60% pattern replacement threshold. SAE analysis reveals the mechanism: formats separate at earlier layers (10% feature overlap at Layer 7) then re-entangle at Layer 10 with different weightings (80% feature overlap). The authors achieve complete bug repair through attention pattern transplantation at Layer 10, demonstrating that the "9.8 vs 9.11" error can be perfectly fixed by targeting the precise computational component.

## Method Summary
The authors use attention pattern transplantation at Layer 10 to repair a format-dependent reasoning failure in Llama-3.1-8B-Instruct. They identify that even-indexed attention heads handle numerical comparison while odd-indexed heads serve incompatible functions. The repair requires capturing attention outputs from exactly 8 even heads (indices 0, 2, 4, ..., 30) during a forward pass with the Simple Format (which produces correct answers), then transplanting these patterns to the Q&A Format (which produces incorrect answers). The intervention uses nnsight v0.2.1 with temperature=0.0 and greedy decoding, patching only attention sub-layer outputs at Layer 10 rather than full layer activations. SAE analysis tracks feature overlap between formats across layers to understand the computational separation and re-entanglement.

## Key Results
- Perfect repair achieved: 100% success rate (n=1000, 95% CI: [99.7%, 100%])
- Sharp threshold at exactly 8 even heads required (7→8 switches 0→100% success)
- 60% pattern replacement threshold for successful intervention
- SAE reveals format separation at Layer 7 (10% feature overlap) and re-entanglement at Layer 10 (80% feature overlap)

## Why This Works (Mechanism)
The mechanism works because transformers organize computation by attention head index parity. Even-indexed heads at Layer 10 handle numerical comparison tasks, while odd-indexed heads serve incompatible functions that interfere with correct reasoning in certain formats. The format-dependent failure occurs because different input templates activate different computational pathways that converge at Layer 10 with incompatible weightings. Attention pattern transplantation from the correct-format computation (Simple Format) to the buggy-format computation (Q&A Format) replaces the incompatible pathway with the correct one, achieving perfect repair with minimal collateral damage.

## Foundational Learning
- **Attention pattern transplantation**: Transferring attention outputs from one model state to another to modify computation. Needed to isolate and replace specific computational components. Quick check: Verify attention outputs are properly captured and injected at correct layer/head indices.
- **SAE (Sparse Autoencoder) analysis**: Decomposing activations into interpretable features to track format separation. Needed to understand how different formats activate different features across layers. Quick check: Confirm feature overlap percentages match expected values at Layers 7 and 10.
- **Head parity organization**: Computational partitioning where even vs odd heads serve different functions. Needed to identify which heads to target for intervention. Quick check: Test single even head vs single odd head to verify differential effects.
- **Sharp phase transitions**: Binary switching behavior where small changes produce large effects. Needed to understand intervention thresholds. Quick check: Systematically vary number of heads from 7 to 8 to observe threshold behavior.
- **Format-dependent reasoning**: Same task producing different answers based on input template. Needed to identify the bug class. Quick check: Test all three formats to confirm error pattern.
- **Greedy decoding with temperature=0**: Deterministic decoding for consistent evaluation. Needed to eliminate stochastic effects during testing. Quick check: Verify temperature=0 produces consistent outputs across runs.

## Architecture Onboarding
**Component Map**: Input → Embedding → 32 Layers (4096 dim) → Attention Sub-layer (16 heads each) → Feed-forward → Output
**Critical Path**: Layer 10 even heads → Attention outputs → Numerical comparison computation
**Design Tradeoffs**: Precise head targeting vs full-layer patching (minimal collateral damage vs implementation complexity)
**Failure Signatures**: Gibberish output with full-layer activation patching; 0% success with insufficient even heads or wrong head parity
**Three First Experiments**:
1. Verify bug exists: Test three formats with temperature=0.0 to confirm Q&A format produces 100% "9.11 is bigger" errors
2. Implement basic transplantation: Use all 16 even heads to establish proof-of-concept repair
3. Boundary testing: Systematically test 7, 8, and 9 even heads to confirm sharp threshold behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Intervention targets single, well-characterized bug in one specific model
- Requirement for exactly 8 even heads reveals brittle dependency that may not generalize
- Biological plausibility of computational organization by head parity remains unclear
- Success may not extend to more complex multi-step reasoning failures

## Confidence
**High Confidence**: Core experimental observations regarding format-dependent failure pattern and 8-even-head requirement at Layer 10
**Medium Confidence**: SAE analysis linking feature overlap changes to attention mechanism (correlation vs causation)
**Low Confidence**: Generalizability to other numerical reasoning tasks, model architectures, or complex failure modes

## Next Checks
1. **Cross-Model Generalization**: Test whether 8-even-head requirement holds for other Llama variants (7B, 70B) and competing architectures when subjected to similar format-dependent numerical reasoning tasks
2. **Task Complexity Scaling**: Evaluate whether attention-based intervention principles apply to more complex numerical reasoning chains and whether head parity organization persists under increased computational demands
3. **Mechanism Isolation**: Perform causal tracing experiments to definitively establish whether even heads directly compute numerical comparisons versus serving as routing mechanisms