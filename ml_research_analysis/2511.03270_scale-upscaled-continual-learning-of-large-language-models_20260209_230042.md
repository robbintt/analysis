---
ver: rpa2
title: 'SCALE: Upscaled Continual Learning of Large Language Models'
arxiv_id: '2511.03270'
source_url: https://arxiv.org/abs/2511.03270
tags:
- learning
- forgetting
- task
- arxiv
- upscaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SCALE, a width upscaling approach for continual\
  \ pre-training of large language models that mitigates catastrophic forgetting while\
  \ enabling adaptation to new domains. Unlike depth expansion methods like LLaMA\
  \ Pro, SCALE inserts lightweight expansion blocks within linear modules while freezing\
  \ all pre-trained parameters, preserving the base model\u2019s functionality."
---

# SCALE: Upscaled Continual Learning of Large Language Models

## Quick Facts
- arXiv ID: 2511.03270
- Source URL: https://arxiv.org/abs/2511.03270
- Reference count: 24
- Primary result: SCALE reduces catastrophic forgetting in continual pre-training via width upscaling, achieving lower English forgetting perplexity than depth expansion methods while maintaining adaptation capacity

## Executive Summary
SCALE introduces a width upscaling approach for continual pre-training of large language models that mitigates catastrophic forgetting while enabling adaptation to new domains. Unlike depth expansion methods like LLaMA Pro, SCALE inserts lightweight expansion blocks within linear modules while freezing all pre-trained parameters, preserving the base model's functionality. The method is instantiated in three variants - SCALE-Preserve, SCALE-Adapt, and SCALE-Route - and demonstrates superior stability-plasticity trade-offs compared to baseline continual learning methods across synthetic and real-world benchmarks.

## Method Summary
SCALE applies width upscaling by partitioning weight matrices in linear modules into four blocks [W, W12; W21, W22], where the original weights W are frozen, W12 is zero-initialized and selectively trained, and W21/W22 are initialized via SVD. This creates "preservation layers" that maintain base model behavior and "collaborative layers" that enable adaptation. Three variants implement different strategies: SCALE-Preserve freezes W12 in all layers for maximum preservation, SCALE-Adapt trains W12 for maximum adaptation, and SCALE-Route uses token-level routing between preservation and adaptation paths based on cosine similarity of logits. The approach contrasts with depth expansion methods by expanding width rather than depth, enabling more parameter-efficient continual learning.

## Key Results
- SCALE significantly reduces forgetting compared to depth expansion (LLaMA Pro) while maintaining learning capacity on synthetic biography benchmark
- In Korean corpus continual pre-training, SCALE variants achieve lower English forgetting perplexity than FFT, LoRA, Freeze, and LLaMA Pro baselines
- SCALE-Route offers the best stability-plasticity balance with competitive Korean learning perplexity and minimal English forgetting
- Theoretical analysis shows when preservation holds and why routing-based continual learning converges better than standard approaches

## Why This Works (Mechanism)
SCALE works by decoupling preservation and adaptation through width expansion rather than depth expansion. By freezing the original pre-trained weights and only training lightweight expansion blocks, the method ensures the base model's functionality remains intact while new capabilities are added through the collaborative layers. The routing mechanism in SCALE-Route further optimizes this trade-off by directing tokens to either the preservation or adaptation path based on semantic similarity, preventing interference between old and new knowledge.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to rapidly lose previously learned information when trained on new tasks - critical for understanding why continual learning is challenging
- **Width vs depth expansion**: Width expansion adds parameters laterally within existing layers while depth expansion adds new layers - width expansion is more parameter-efficient for continual learning
- **Parameter-efficient fine-tuning**: Methods like LoRA and FFT that add small trainable components to frozen base models - relevant context for comparing SCALE against existing approaches
- **Routing mechanisms**: Techniques that direct inputs to different processing paths based on learned criteria - essential for understanding SCALE-Route's token-level decision making
- **SVD initialization**: Using singular value decomposition to initialize expansion blocks with meaningful parameter values - important for understanding how SCALE bootstraps new capabilities

## Architecture Onboarding

**Component Map**
Input -> Linear Module (partitioned into [W, W12; W21, W22]) -> Preservation Path (frozen W + W12) -> Routing (cosine similarity) -> Adaptation Path (W21, W22) -> Output

**Critical Path**
Token embedding → Preservation path computation (Z_preserve) → Adaptation path computation (Z_adapt) → Routing decision (cosine similarity threshold) → Selected path logits → Loss computation

**Design Tradeoffs**
- Width expansion vs depth expansion: SCALE trades parameter efficiency for better preservation
- Preservation vs adaptation: Three variants offer different points on the stability-plasticity spectrum
- Static vs dynamic routing: SCALE-Route uses static threshold routing, limiting adaptability to varying data distributions

**Failure Signatures**
- Severe forgetting if W12 is accidentally trainable in lower layers (check cosine similarity of output hidden states)
- Poor adaptation if too many preservation layers or insufficient collaboration (monitor learning PPL curve)
- Routing inefficiency if threshold τ is poorly chosen (analyze token distribution across paths)

**3 First Experiments**
1. Implement width upscaling module with proper partitioning and freezing
2. Run biography task continual learning with 160M Pythia base model
3. Compare English forgetting perplexity across SCALE variants vs baseline methods

## Open Questions the Paper Calls Out
- How does SCALE perform when applied to significantly larger model backbones (>7B parameters) and over longer training horizons?
- Can learned or dynamic routing mechanisms outperform the static cosine-similarity threshold used in SCALE-Route?
- Is there a principled, automated method for selecting the optimal number and location of collaborative layers?

## Limitations
- Evaluation limited to modest model scales (1B parameters), leaving efficiency unproven for frontier-scale models
- Static routing threshold may fail to generalize across diverse domains or data distributions
- Manual configuration required for selecting collaborative layers rather than automated selection

## Confidence
- **High confidence**: Theoretical framework for Persistent Preservation and Collaborative Adaptation principles is sound
- **Medium confidence**: Experimental results showing improved forgetting metrics are promising but limited to one base model and specific domain pairs
- **Low confidence**: Claim that SCALE-Route offers the "best stability-plasticity balance" is based on single dataset pair

## Next Checks
1. Implement routing mechanism with sensitivity analysis on threshold τ parameter across different domain pairs
2. Conduct ablation studies varying L_fp and collaboration scope on multiple base models to validate theoretical preservation claims
3. Test approach on non-synthetic continual learning benchmarks to assess generalization beyond Korean/English pair used in experiments