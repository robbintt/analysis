---
ver: rpa2
title: 'The Yokai Learning Environment: Tracking Beliefs Over Space and Time'
arxiv_id: '2508.12480'
source_url: https://arxiv.org/abs/2508.12480
tags:
- agents
- cards
- card
- game
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Yokai Learning Environment (YLE), a novel
  multi-agent reinforcement learning benchmark for testing Theory of Mind reasoning
  in collaborative settings. YLE is based on the cooperative card game Yokai, where
  agents must cluster cards by color without direct communication, requiring belief
  tracking, memory, and implicit communication via hints.
---

# The Yokai Learning Environment: Tracking Beliefs Over Space and Time

## Quick Facts
- arXiv ID: 2508.12480
- Source URL: https://arxiv.org/abs/2508.12480
- Authors: Constantin Ruhdorfer; Matteo Bortoletto; Andreas Bulling
- Reference count: 40
- Agents struggle to coordinate effectively even with perfect memory

## Executive Summary
This paper introduces the Yokai Learning Environment (YLE), a novel multi-agent reinforcement learning benchmark for testing Theory of Mind reasoning in collaborative settings. YLE is based on the cooperative card game Yokai, where agents must cluster cards by color without direct communication, requiring belief tracking, memory, and implicit communication via hints. The authors evaluate various RL agents and find that even with perfect memory, agents struggle to coordinate effectively. Belief modeling helps but agents fail to generalize to new partners or maintain accurate beliefs over longer games, relying instead on brittle conventions. The paper highlights the difficulty of scaling belief reasoning, especially in four-player settings, and positions YLE as a challenging testbed for advancing collaborative AI with Theory of Mind capabilities.

## Method Summary
The Yokai Learning Environment is a multi-agent RL benchmark where agents play a cooperative card game requiring implicit communication and belief tracking. Agents observe their own cards and can hint at specific cards in other players' hands, but cannot communicate directly. The environment tracks each agent's belief about the true card states and game state, providing this as auxiliary information. The authors implement several agent architectures including standard RL, belief-aware agents, and agents with perfect memory, evaluating their performance on coordination tasks. The key innovation is the structured belief representation that agents can use to reason about other players' knowledge states, making it possible to test Theory of Mind capabilities in a controlled setting.

## Key Results
- Even with perfect memory, agents struggle to coordinate effectively in the YLE environment
- Belief modeling helps but is insufficient for robust coordination
- Agents fail to generalize to new partners or maintain accurate beliefs over longer games
- Agents rely on brittle conventions rather than robust Theory of Mind reasoning
- Four-player settings present significant scalability challenges for belief reasoning

## Why This Works (Mechanism)
The YLE environment creates a controlled setting where agents must reason about other agents' beliefs without direct communication. The card game structure forces agents to use hints as a form of implicit communication, while the belief tracking system provides a ground truth for evaluating Theory of Mind capabilities. The environment's design makes belief reasoning necessary for success, as agents cannot simply optimize for their own immediate rewards but must coordinate through understanding what other agents know and don't know.

## Foundational Learning
- **Multi-agent reinforcement learning**: Understanding how multiple agents interact and learn in shared environments; needed because YLE is fundamentally a multi-agent problem
- **Theory of Mind reasoning**: The ability to model and reason about other agents' beliefs and knowledge states; needed because the game requires understanding what other players know
- **Belief tracking**: Maintaining and updating beliefs about hidden information over time; needed because agents must reason about card distributions they cannot directly observe
- **Implicit communication**: Using actions to convey information without explicit messaging; needed because agents can only hint at cards, not communicate directly
- **Cooperative game theory**: Understanding how agents can work together toward shared goals; needed because the game is cooperative rather than competitive
- **Memory architectures for RL**: How agents store and retrieve information over long time horizons; needed because successful play requires remembering past hints and observations

## Architecture Onboarding
**Component Map**: Environment -> Agent Policies -> Belief Tracker -> Reward Signal
Agents observe their cards and hints, take actions (hint or cluster), and receive rewards based on successful coordination. The belief tracker maintains state about card distributions and agent knowledge.

**Critical Path**: Observation -> Belief Update -> Action Selection -> Environment Step -> Reward Calculation
The agent observes its hand and any hints received, updates its beliefs about the game state, selects an action (hint or cluster), the environment processes the action and updates the game state, then rewards are calculated based on coordination success.

**Design Tradeoffs**: Memory vs. Computation (more belief tracking improves reasoning but increases computational cost), Communication Bandwidth (hints are limited in expressiveness but force implicit communication), Generalization vs. Specialization (agents can learn brittle conventions that work but don't generalize).

**Failure Signatures**: Agents get stuck in local conventions that work for specific partners but fail with new ones, belief tracking becomes inaccurate over longer games leading to coordination failures, memory limitations cause agents to forget important information needed for coordination.

**First 3 Experiments**: 1) Train a baseline agent without belief modeling to establish performance floor, 2) Train belief-aware agents and compare to baseline, 3) Test generalization by training on one partner type and evaluating on another.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond noting that the YLE environment itself raises new questions about how to build agents with robust Theory of Mind capabilities.

## Limitations
- Experimental evaluation limited to specific agent architectures and learning algorithms
- Scalability analysis is preliminary with only basic four-player experiments
- Limited exploration of alternative belief modeling or communication strategies
- Does not fully characterize the brittle conventions agents develop
- Generalization results across partner behaviors are limited in scope

## Confidence
- High confidence in core experimental results showing RL agents struggle with collaborative ToM reasoning
- Medium confidence in interpretation that belief modeling alone is insufficient
- Medium confidence in scalability challenges for four-player settings

## Next Checks
1. Test additional agent architectures including transformer-based policies and hierarchical RL approaches to see if different inductive biases help with belief tracking.

2. Conduct ablation studies isolating the effects of memory capacity, observation history length, and belief update frequency on coordination performance.

3. Implement and evaluate explicit communication protocols alongside the current hint-based system to determine if direct communication alleviates some ToM challenges.