---
ver: rpa2
title: Unrewarded Exploration in Large Language Models Reveals Latent Learning from
  Psychology
arxiv_id: '2601.22474'
source_url: https://arxiv.org/abs/2601.22474
tags:
- learning
- unrewarded
- rewards
- llms
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work establishes that large language models exhibit latent\
  \ learning dynamics analogous to Tolman\u2019s psychological theory. By introducing\
  \ an initial phase of unrewarded exploration\u2014where models generate responses\
  \ without reward signals and learn directly from their own outputs\u2014LLMs demonstrate\
  \ measurable performance gains even in the absence of reinforcement."
---

# Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology

## Quick Facts
- arXiv ID: 2601.22474
- Source URL: https://arxiv.org/abs/2601.22474
- Reference count: 19
- Key finding: Large language models demonstrate latent learning dynamics analogous to Tolman's psychological theory through unrewarded exploration phases

## Executive Summary
This work investigates a novel learning paradigm in large language models where an initial phase of unrewarded exploration—generating responses without reward signals and learning directly from self-generated outputs—leads to measurable performance gains. The approach draws inspiration from Tolman's latent learning theory in psychology, demonstrating that LLMs can acquire knowledge even without explicit reinforcement. When rewards are subsequently introduced after unrewarded exploration, models outperform those trained with rewards throughout. Theoretical analysis confirms that ratio-clipped Group Relative Policy Optimization with KL regularization guarantees monotone improvement in expected utility during unrewarded exploration phases.

## Method Summary
The method introduces an initial unrewarded exploration phase where LLMs generate responses without receiving reward signals, instead learning directly from their own outputs. This is followed by a reward phase where traditional reinforcement learning signals are introduced. The theoretical framework uses ratio-clipped Group Relative Policy Optimization with KL regularization to provide guarantees for monotone improvement in expected utility. The approach was validated across multiple model families and scales, testing mathematical reasoning and GUI agent tasks to demonstrate generalizability.

## Key Results
- Unrewarded exploration alone leads to measurable performance improvements in LLMs
- Combining unrewarded exploration with subsequent reward-based training outperforms models trained with rewards throughout
- Theoretical guarantees confirm monotone improvement in expected utility during unrewarded exploration using ratio-clipped GRPO with KL regularization
- Benefits observed across multiple model families, scales, and diverse task types (mathematical reasoning and GUI agents)

## Why This Works (Mechanism)
The mechanism leverages the intrinsic learning capability of LLMs to extract patterns and knowledge from self-generated outputs during the unrewarded exploration phase. By removing the constraint of external reward signals, models can freely explore the solution space and develop internal representations that capture latent structure in the task. This aligns with Tolman's theory that organisms can learn about their environment without immediate reinforcement. The theoretical framework ensures that policy updates during this phase maintain or improve expected utility through careful gradient clipping and regularization.

## Foundational Learning
- Reinforcement Learning: Why needed - Core framework for understanding reward-based optimization; Quick check - Verify understanding of policy gradients and value functions
- Group Relative Policy Optimization: Why needed - Algorithm used for monotone improvement guarantees; Quick check - Confirm knowledge of GRPO's relative advantage estimation
- KL Regularization: Why needed - Prevents policy collapse during exploration; Quick check - Understand how KL penalties constrain policy updates
- Latent Learning Theory: Why needed - Psychological foundation for the approach; Quick check - Review Tolman's experiments and their implications
- Policy Gradient Methods: Why needed - Fundamental technique for optimizing language model behaviors; Quick check - Distinguish between on-policy and off-policy methods

## Architecture Onboarding
Component Map: Input Text -> Unrewarded Generation -> Self-learning Module -> (Optional) Reward Phase -> Output Policy
Critical Path: The unrewarded generation phase is critical as it establishes the initial policy that subsequent reward learning builds upon
Design Tradeoffs: Unrewarded exploration trades immediate reward optimization for potentially better long-term representations; requires careful duration tuning
Failure Signatures: Over-exploration leading to policy divergence; insufficient exploration yielding no latent learning benefits
First Experiments: 1) Test minimal unrewarded exploration duration needed for benefits, 2) Compare different reward signal types post-exploration, 3) Evaluate scaling effects across model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis primarily focused on ratio-clipped GRPO with KL regularization, limiting generalizability to other RL algorithms
- Experimental validation limited to mathematical reasoning and GUI agent tasks, leaving domain transferability unclear
- Computational costs and efficiency trade-offs of implementing unrewarded exploration phases not addressed

## Confidence
High Confidence: Experimental demonstration of performance gains from unrewarded exploration is well-supported across multiple model families and scales
Medium Confidence: Theoretical guarantees for monotone improvement are mathematically sound but practical significance requires further investigation
Low Confidence: Claims about bridging cognitive psychology and AI learning paradigms remain largely metaphorical without concrete mechanistic connections

## Next Checks
1. Test whether unrewarded exploration benefits extend to diverse task domains beyond mathematical reasoning and GUI agents
2. Conduct ablation studies to determine optimal duration and intensity of unrewarded exploration phases relative to rewarded training
3. Evaluate computational overhead and training efficiency trade-offs compared to baseline reward-only training approaches