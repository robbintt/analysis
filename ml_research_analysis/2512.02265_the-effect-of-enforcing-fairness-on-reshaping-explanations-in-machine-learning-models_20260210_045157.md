---
ver: rpa2
title: The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning
  Models
arxiv_id: '2512.02265'
source_url: https://arxiv.org/abs/2512.02265
tags:
- fairness
- across
- mitigation
- feature
- rankings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how fairness constraints in machine learning
  models affect the stability of feature importance rankings in clinical settings.
  The authors analyze three healthcare datasets (pediatric UTI, AFib bleeding risk,
  and COMPAS recidivism) using four model types (logistic regression, random forest,
  XGBoost, and neural networks).
---

# The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning Models

## Quick Facts
- arXiv ID: 2512.02265
- Source URL: https://arxiv.org/abs/2512.02265
- Authors: Joshua Wolff Anderson; Shyam Visweswaran
- Reference count: 28
- Enforcing fairness constraints through exponentiated gradient reduction significantly alters SHAP-based feature importance rankings across healthcare datasets

## Executive Summary
This study investigates how enforcing equalized odds fairness constraints affects feature importance rankings in clinical machine learning models. Using three healthcare datasets and four model types, the authors apply exponentiated gradient reduction to enforce fairness and measure changes in SHAP-based explanations. The research reveals that fairness mitigation reduces equalized odds disparities across all datasets and models, but comes with varying performance trade-offs. Most significantly, feature importance rankings become substantially less stable after mitigation, particularly in complex models and non-Black subgroups. These findings highlight the fundamental tension between achieving fairness and maintaining stable, interpretable explanations in healthcare ML deployments.

## Method Summary
The authors analyze three healthcare datasets (pediatric UTI, AFib bleeding risk, and COMPAS recidivism) using logistic regression, random forest, XGBoost, and neural networks. They apply Fairlearn's exponentiated gradient reduction with equalized odds constraints to enforce fairness across racial subgroups (Black vs. non-Black). SHAP values are computed pre- and post-mitigation using appropriate estimators (KernelSHAP for logistic regression, TreeSHAP for tree-based models, DeepSHAP for neural networks). Spearman's correlation coefficient measures the stability of feature importance rankings, while standard ML metrics (accuracy, AUROC, F1) and equalized odds differences quantify performance and fairness.

## Key Results
- Equalized odds disparities are reduced across all datasets and models after fairness mitigation
- Performance trade-offs vary by dataset: AFib models experience minimal degradation while UTI models show substantial drops in recall and F1 scores
- Feature importance rankings become significantly less stable after mitigation, with complex models showing greater instability than simpler ones
- Ranking stability differs across racial subgroups, with non-Black patients showing more dramatic changes than Black patients in some models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing equalized odds constraints through in-processing optimization reshapes feature importance rankings by fundamentally altering how the model weights input features to satisfy fairness criteria.
- **Mechanism:** Exponentiated gradient reduction iteratively adjusts model parameters to minimize prediction error while constraining true positive rate and false positive rate parity across protected groups. This constrained optimization forces the model to redistribute attention across features—potentially downweighting features correlated with protected attributes and upweighting others to achieve group-level parity. The redistribution is not uniform; features highly correlated with the protected attribute (e.g., race) may see larger SHAP value changes.
- **Core assumption:** The optimization landscape permits solutions where fairness constraints can be satisfied without completely sacrificing predictive signal, and that SHAP faithfully captures the resulting decision logic.
- **Evidence anchors:**
  - [abstract] "increasing model fairness across racial subgroups can significantly alter feature importance rankings"
  - [Methods] "exponentiated gradient reduction, an in-processing method that runs constrained optimization using fairness metrics (e.g., EOD) as constraints"
  - [corpus] Related work "Enforcing Fairness Where it Matters" similarly uses constrained optimization but focuses on partial fairness regions; does not directly validate SHAP-ranking mechanisms.
- **Break condition:** If the dataset has no features predictive of the outcome that are uncorrelated with the protected attribute, the optimizer may produce degenerate solutions with near-random predictions, making SHAP rankings meaningless.

### Mechanism 2
- **Claim:** Model complexity moderates the stability of feature importance rankings under fairness constraints—simpler models (logistic regression) preserve rankings more than complex models (neural networks, XGBoost).
- **Mechanism:** Complex models have higher-capacity function approximators with more degrees of freedom, allowing the optimizer to find diverse parameter configurations that satisfy the same fairness constraint. SHAP values approximate conditional expectations across this larger hypothesis space, leading to greater variance in feature attributions post-mitigation. Linear models have fewer paths to satisfy constraints, resulting in more stable rank orderings.
- **Core assumption:** Spearman's ρ adequately captures meaningful ranking changes; higher capacity implies more diverse optima under constraints.
- **Evidence anchors:**
  - [Results] "Complex models generally exhibit greater variability in feature importance rankings, whereas simpler models, such as logistic regression, show high stability"
  - [Table 2] Neural Network for UTI: ρ=0.61 overall vs. Logistic Regression: ρ=0.96 overall
  - [corpus] Weak direct evidence; related papers on explainability (FLEX) discuss counterfactual-based importance but do not test complexity-stability interactions under fairness constraints.
- **Break condition:** If complex models are heavily regularized or the dataset is small with limited feature interactions, the complexity advantage collapses and ranking stability may converge to linear model behavior.

### Mechanism 3
- **Claim:** Fairness mitigation produces divergent feature importance ranking changes across racial subgroups—the ranking for Black patients may shift differently than for non-Black patients.
- **Mechanism:** Equalized odds enforces parity in error rates across groups but does not enforce parity in feature attribution. The optimizer may satisfy EOD by adjusting decision boundaries differently in different regions of feature space where each subgroup is concentrated. SHAP values are computed locally per instance; aggregating by subgroup reveals heterogeneous adjustments. A feature may become more important for one group's predictions while less important for another's.
- **Core assumption:** Local SHAP aggregations by subgroup are statistically reliable; subgroups have sufficient sample sizes for meaningful comparison.
- **Evidence anchors:**
  - [abstract] "sometimes in different ways across groups"
  - [Table 2] Random Forest UTI: ρ=0.96 for Black vs. ρ=0.86 for non-Black; Neural Network UTI: ρ=0.57 for Black vs. ρ=0.75 for non-Black
  - [corpus] "An Explainable and Fair AI Tool for PCOS" integrates SHAP with demographic audits but does not report subgroup-specific ranking divergence.
- **Break condition:** If subgroup sample sizes are too small or the protected attribute is nearly uncorrelated with all features, subgroup-level SHAP differences become noise rather than signal.

## Foundational Learning

- **Concept: SHAP (Shapley Additive Explanations)**
  - Why needed here: This is the core explainability method. You must understand that SHAP allocates credit to features based on their marginal contribution across all possible feature coalitions, enabling both local (per-prediction) and global (mean |SHAP|) importance rankings.
  - Quick check question: Given a model with features A, B, C where A and B are highly correlated, would you expect SHAP values to stably distinguish their individual contributions? Why or why not?

- **Concept: Equalized Odds (EOD)**
  - Why needed here: This is the fairness constraint used throughout. EOD requires equal true positive rates AND equal false positive rates across protected groups—a stricter condition than demographic parity or predictive parity alone.
  - Quick check question: If a model has 80% TPR for Group A and 80% TPR for Group B, but 10% FPR for Group A and 30% FPR for Group B, does it satisfy equalized odds? Explain.

- **Concept: Exponentiated Gradient Reduction**
  - Why needed here: This is the in-processing mitigation technique. It reframes fair classification as a constrained optimization problem, iteratively updating a weight distribution over training examples to reduce violations of the fairness constraint while maintaining accuracy.
  - Quick check question: Why might an in-processing method (like exponentiated gradient) change feature importance rankings more than a post-processing threshold adjustment would?

## Architecture Onboarding

- **Component map:** Data Layer (UTI, COMPAS, AFib datasets) -> Model Layer (Logistic Regression, Random Forest, XGBoost, Neural Network) -> Fairness Layer (Fairlearn exponentiated gradient reduction with EOD) -> Explanation Layer (SHAP library computing local then global feature importance) -> Evaluation Layer (Spearman's ρ for ranking stability, accuracy/AUROC/F1 for performance, EOD difference for fairness)

- **Critical path:** Train baseline model → compute SHAP → establish baseline rankings; Apply exponentiated gradient reduction with EOD constraint → produce mitigated model; Compute SHAP on mitigated model → compare rankings via Spearman's ρ (overall and by subgroup); Report performance, fairness, and ranking stability metrics

- **Design tradeoffs:**
  - Equalized odds vs. other fairness metrics: EOD is stricter but may cause larger performance drops and ranking changes than predictive parity
  - Model complexity vs. explanation stability: Neural networks may achieve better fairness-performance tradeoffs but exhibit unstable rankings
  - Dataset size vs. mitigation severity: Small datasets (UTI n=387) show larger performance drops; large datasets (AFib n=15,959) absorb constraints with minimal cost

- **Failure signatures:**
  - Recall collapsing to near-zero (Neural Network UTI: recall 0.52→0.01): optimizer found only degenerate fair solutions
  - ρ approaching 0: complete reordering of feature importance—explanations become unrecognizable to clinicians
  - Subgroup ρ diverging >0.2 points: explanations may be misleading if presented as uniform across populations

- **First 3 experiments:**
  1. **Replicate on a single dataset:** Train baseline and mitigated logistic regression on UTI; compute SHAP; verify ρ≈0.96 and EOD reduction 0.29→0.03. This validates your pipeline.
  2. **Test complexity gradient:** Run all four model types on COMPAS; confirm that neural network ρ (0.67) < random forest ρ (0.85) < logistic regression ρ (0.95). This tests the complexity-stability hypothesis.
  3. **Subgroup analysis drilldown:** For UTI Random Forest, extract SHAP values separately for Black vs. non-Black instances; visualize the top-5 features per subgroup pre/post mitigation. Document where rankings diverge.

## Open Questions the Paper Calls Out
None

## Limitations
- SHAP's additive structure may not capture non-additive interactions introduced by the optimizer's constrained adjustments
- Small sample sizes in some datasets (UTI n=387) raise concerns about statistical power for subgroup analyses
- The exclusive focus on equalized odds may limit generalizability to other fairness definitions healthcare practitioners might prefer

## Confidence

- **High Confidence:** Performance-accuracy trade-offs (AFib showing minimal degradation while UTI shows substantial drops) - directly measured and consistently observed across models
- **Medium Confidence:** Ranking stability differences by model complexity - supported by Spearman correlations but could benefit from permutation testing to establish significance
- **Low Confidence:** Subgroup-specific ranking divergence mechanisms - observed differences exist but the causal pathways remain speculative without intervention studies

## Next Checks

1. **Permutation significance testing:** Apply permutation tests to Spearman correlations to establish whether observed ranking changes are statistically significant rather than sampling artifacts
2. **Cross-validation stability:** Implement k-fold cross-validation to assess whether ranking changes persist across different data splits, particularly for smaller datasets
3. **Alternative fairness metrics:** Replicate the analysis using demographic parity and predictive parity constraints to determine if ranking instability is unique to equalized odds or a general property of fairness mitigation