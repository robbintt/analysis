---
ver: rpa2
title: Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy
  Labels
arxiv_id: '2506.21151'
source_url: https://arxiv.org/abs/2506.21151
tags:
- scar
- segmentation
- myocardial
- cardiovascular
- magnetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a deep learning pipeline for fully automated
  myocardial scar segmentation from cardiac MRI, addressing challenges of label noise,
  data heterogeneity, and class imbalance. The method combines YOLO for bounding box
  detection with fine-tuned SAM for segmentation, using KL divergence loss and extensive
  data augmentation to improve robustness.
---

# Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels

## Quick Facts
- arXiv ID: 2506.21151
- Source URL: https://arxiv.org/abs/2506.21151
- Reference count: 27
- Dice similarity coefficient of 0.601 achieved on multi-cohort LGE-MRI dataset

## Executive Summary
This paper introduces a fully automated deep learning pipeline for myocardial scar segmentation from cardiac MRI that addresses challenges of label noise, data heterogeneity, and class imbalance. The method combines YOLO for bounding box detection with fine-tuned SAM for segmentation, using KL divergence loss and extensive data augmentation to improve robustness. Evaluated on a multi-cohort dataset, the approach achieves clinically reliable scar mass estimates and generalizes to acute MI cases, outperforming baseline segmentation methods.

## Method Summary
The pipeline employs a two-stage cascade approach: YOLO detects candidate scar regions via bounding boxes, which serve as prompts for SAM's segmentation decoder. Training uses a combined loss function (KL divergence 0.6, Dice 0.2, cross-entropy 0.2) with Gaussian-smoothed soft labels to handle semi-automatic annotation uncertainty. Extensive MRI-specific augmentations including Rician noise injection and adaptive histogram equalization improve generalization across heterogeneous imaging conditions. The model processes 256×256 pixel slices independently after patient-level stratification and preprocessing.

## Key Results
- Achieved Dice similarity coefficient of 0.601 and perimeter similarity of 0.797 on multi-cohort test set
- Scar mass predictions closely match semi-automatic ground truth (11.337±8.201 g vs 12.510±8.784 g, p=0.42)
- Outperformed baseline nnU-Net with Box-SAM achieving 0.653 DSC using ground-truth boxes
- Successfully generalized to acute MI cases despite training only on chronic scars

## Why This Works (Mechanism)

### Mechanism 1: KL Divergence Loss for Noisy Label Regularization
The KL divergence loss improves robustness to imperfect semi-automatic annotations by penalizing overconfident boundary predictions. Gaussian smoothing (σ=2) converts hard binary masks into soft labels, representing boundary uncertainty. The model's sigmoid-activated predictions are transformed via log-softmax and compared to soft targets via KL divergence, discouraging confident predictions in ambiguous transition zones. This loss is combined with Dice (0.2) and cross-entropy (0.2) using a higher weight (0.6) for KL.

### Mechanism 2: Detection-Guided Segmentation Cascade (YOLO → SAM)
Using YOLO-generated bounding boxes as prompts for SAM focuses segmentation attention on scar regions, improving accuracy over direct segmentation. YOLO first localizes candidate scar regions via bounding box detection. These boxes serve as automatic prompts to SAM's prompt encoder, constraining the mask decoder to produce segmentations within relevant anatomical regions. This cascade reduces false positives from background pixels and addresses class imbalance (scars are small relative to image area).

### Mechanism 3: MRI-Specific Data Augmentation for Heterogeneity
Extensive augmentation simulating MRI-specific variability improves generalization across scanners, sequences, and protocols. Standard spatial augmentations (flip, rotation, scale, deformation) are combined with MRI-specific intensity augmentations: gamma correction, adaptive histogram equalization, brightness adjustment, and Rician noise injection (mimicking MRI noise characteristics). Bounding box jittering simulates annotation variability.

## Foundational Learning

- **KL Divergence for Distribution Matching**
  - Why needed here: Understanding how KL loss differs from pixel-wise losses (Dice, CE) and why it helps with uncertain labels.
  - Quick check question: Can you explain why softening labels with Gaussian smoothing before KL comparison reduces overconfidence at boundaries?

- **Prompt-Based Segmentation (SAM Architecture)**
  - Why needed here: SAM's prompt encoder and mask decoder operate differently from standard encoder-decoder segmentation networks.
  - Quick check question: What types of prompts can SAM accept, and how does a bounding box prompt constrain the segmentation output?

- **Evaluation Metrics for Small Lesions**
  - Why needed here: Dice scores can be misleading for small, spatially shifted predictions; perimeter similarity provides complementary assessment.
  - Quick check question: Why does a slight spatial shift cause a large Dice drop but minimal perimeter similarity change for small scars?

## Architecture Onboarding

- **Component map:**
  - Input: 256×256 LGE-MRI slice → Augmentation pipeline
  - Stage 1: Fine-tuned YOLO → Bounding box detection
  - Stage 2: SAM Image Encoder → Image embeddings (frozen or fine-tuned)
  - Stage 3: SAM Prompt Encoder ← Bounding box from YOLO
  - Stage 4: SAM Mask Decoder → Binary scar mask
  - Loss: KL (0.6) + Dice (0.2) + CE (0.2) computed against soft labels

- **Critical path:**
  1. Patient-level data split (70/15/15) with scar-pixel stratification
  2. Slice-level preprocessing and augmentation
  3. YOLO fine-tuning for bounding box detection
  4. SAM fine-tuning with combined loss and soft labels
  5. Inference: YOLO box → SAM prompt → Mask output

- **Design tradeoffs:**
  - Fully automated (YOLO-SAM) vs. semi-automated (Box-SAM with manual prompts): Automation introduces ~5% DSC drop per Table 2 vs. Table 3
  - Training only on chronic scars: Better-defined labels but requires generalization testing on acute cases
  - Soft labels via Gaussian σ=2: Hyperparameter choice affecting boundary uncertainty representation

- **Failure signatures:**
  - Very low DSC with high perimeter similarity → Spatial shift in prediction (model captures extent but not precise location)
  - Fragmented predictions → Insufficient smoothness regularization or detection failure
  - Poor acute MI generalization → Morphological feature distribution shift beyond augmentation coverage

- **First 3 experiments:**
  1. **Ablate KL loss weight**: Compare KL weights {0.0, 0.3, 0.6, 0.9} while holding Dice/CE constant to validate the 0.6 choice.
  2. **Detection failure analysis**: Manually inject YOLO box errors (shifted, missed, spurious) and measure downstream SAM degradation to quantify cascade fragility.
  3. **Out-of-distribution holdout**: Train on cohorts 1-4, test on held-out cohorts 5-8 to verify generalization claims without acute MI confounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the morphological features extracted from automated segmentations effectively predict patient prognosis and treatment outcomes?
- Basis in paper: The authors state that "Using shape-based analysis of segmented scars offers the potential to develop diagnostic classifiers and predict patient risk and treatment outcomes," but only demonstrate differentiation between acute and chronic cases.
- Why unresolved: The current study validates the morphological distinction between scar types but does not link these features to longitudinal clinical endpoints like mortality or arrhythmia risk.
- What evidence would resolve it: A longitudinal study correlating extracted scar features (solidity, circularity) with patient recovery rates or adverse cardiac events.

### Open Question 2
- Question: Does incorporating volumetric (3D) context improve segmentation accuracy compared to the current 2D slice-level approach?
- Basis in paper: The methods section notes, "Each LGE-MRI scan was then processed at the slice level, with individual slices serving as network input," ignoring inter-slice spatial continuity.
- Why unresolved: Processing slices independently may result in discontinuous volumetric reconstructions, failing to leverage the 3D anatomical structure of the myocardium and scar.
- What evidence would resolve it: Comparison of the current 2D pipeline against a 3D implementation using volumetric metrics such as 3D Dice or surface distance.

### Open Question 3
- Question: Is training exclusively on chronic scars optimal for generalization, or can the model be adapted to handle acute scar heterogeneity directly?
- Basis in paper: The paper notes the "advantage of training the model exclusively in chronic cases... minimizing bias from the fragmented and less distinct appearance often seen in acute scars."
- Why unresolved: While excluding acute data reduces noise, it leaves uncertainty regarding whether the model could learn robust acute-specific features if trained on a verified acute dataset.
- What evidence would resolve it: Experiments training the model on a curated dataset of acute MI cases and evaluating performance on acute-specific geometric complexities.

## Limitations
- Model trained exclusively on chronic MI cases, requiring validation for acute MI generalization
- Semi-automatic FWHM ground truth introduces systematic label noise that cannot be fully eliminated
- Dataset composition details (chronic vs. acute proportions, scanner types) not fully reported, limiting external validity assessment

## Confidence
- **High**: Scar mass correlation with ground truth (11.337±8.201 g vs 12.510±8.784 g, p=0.42), indicating clinical reliability for quantitative assessment
- **Medium**: DSC and perimeter similarity improvements over baselines, though absolute values (0.601 DSC, 0.797 PS) suggest substantial room for refinement
- **Medium**: Generalization to acute MI cases, as the model was not explicitly trained on this pathology

## Next Checks
1. **Cross-cohort holdout validation**: Train on 6 cohorts, test on 2 unseen cohorts to verify multi-cohort robustness claims independent of acute MI confounds
2. **Label noise sensitivity analysis**: Systematically vary Gaussian smoothing σ (e.g., 1.0, 2.0, 3.0) and KL weight to quantify impact on DSC/PS trade-offs
3. **Failure mode characterization**: Analyze false positives/negatives on acute MI cases to identify whether degradation stems from morphological differences or insufficient training representation