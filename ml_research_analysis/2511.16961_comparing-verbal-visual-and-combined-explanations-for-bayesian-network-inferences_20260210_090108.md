---
ver: rpa2
title: Comparing verbal, visual and combined explanations for Bayesian Network inferences
arxiv_id: '2511.16961'
source_url: https://arxiv.org/abs/2511.16961
tags:
- verbal
- visual
- figure
- participants
- explanatory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares three explanation modalities (verbal, visual,
  combined) for Bayesian Networks (BNs) against a baseline. Users performed better
  with all explanation types than baseline for questions about observation impact,
  path contributions, and interaction effects (p<0.05).
---

# Comparing verbal, visual and combined explanations for Bayesian Network inferences

## Quick Facts
- arXiv ID: 2511.16961
- Source URL: https://arxiv.org/abs/2511.16961
- Reference count: 40
- Primary result: Combined verbal-visual explanations improve user comprehension of BN inferences over single-modality explanations for specific question types

## Executive Summary
This study investigates how different explanation modalities—verbal, visual, and combined—affect user comprehension of Bayesian Network (BN) inferences. Using a between-subjects design with 124 participants, the research tested six question types (control, finding impact, path contributions, and interaction effects) across two small BNs. The results show that all explanation conditions significantly outperformed a baseline UI for questions about observation impact, path contributions, and interaction effects, with combined explanations providing the most benefit for certain question types. System usability scores were highest for explanation conditions, though no significant differences emerged for control questions requiring only BN knowledge.

## Method Summary
The study employed a Qualtrics survey with 124 participants recruited via Prolific Academic (UK). Participants were randomly assigned to four conditions: Verbal, Visual, Both, or Neither (baseline). The experiment involved a demographics/skill assessment, BN tutorial (with 50% threshold), explanatory tools tutorial, and main study using two small BNs (Rats, Podunk) with six question types. The verbal explanations used templates for contribution scales and counterfactuals, while visual explanations included animations highlighting belief propagation paths. Combined explanations synchronized both modalities. The study used mixed effects logistic regression with participant and question IDs as random effects to analyze accuracy and SUS scores.

## Key Results
- Users performed better with all explanation types than baseline for questions about observation impact, path contributions, and interaction effects (p<0.05)
- Combined explanations outperformed visual ones for impact questions and verbal ones for path questions
- No significant differences emerged for control questions requiring only BN knowledge
- System usability scores were highest for explanation conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combined verbal-visual explanations improve user comprehension of BN inferences over single-modality explanations for specific question types.
- Mechanism: Dual-coding allows users to cross-reference information between modalities—verbal explanations provide explicit contribution magnitudes and counterfactuals, while visual animations reveal belief propagation paths and interactions dynamically. This reduces cognitive load by externalizing mental simulations.
- Core assumption: Users can integrate information across modalities without excessive split-attention cost.
- Evidence anchors: [abstract] "using verbal and visual modalities together is better than using either modality alone for some of these question types"; [section] H4 results: "Both explanations larger than Visual for Finding questions, larger than Verbal for Path questions (p<0.05)"

### Mechanism 2
- Claim: Visual explanations with animations highlighting belief propagation paths improve understanding of causal pathways in BNs.
- Mechanism: Animations progressively reveal arc-by-arc probability changes along paths, making implicit d-separation and influence propagation explicit. Color-coding of contribution magnitude (dark green for strong positive, red for negative) provides rapid perceptual encoding of directional impact.
- Core assumption: Users can track sequential visual changes and map them to BN structure.
- Evidence anchors: [abstract] "Visual explanations included animations highlighting belief propagation"; [section] "Each animation matches an arc-by-arc path description from the text box, and progressively highlights a sequence of arcs while revealing probability changes on intermediate variables"

### Mechanism 3
- Claim: Verbal explanations using contribution scales and counterfactuals improve understanding of finding impacts and interaction effects.
- Mechanism: Template-based verbal explanations convert numerical probability differences into qualitative scales (e.g., "greatly increases," "moderately reduces") aligned with a color-coded contribution scale. Counterfactual explanations decompose interaction effects by describing (1) finding contribution alone, (2) how other findings change the situation, and (3) combined contribution.
- Core assumption: Users interpret qualitative verbal descriptions more reliably than raw probability differences.
- Evidence anchors: [abstract] "verbal explanations used templates for contribution scales and counterfactuals"; [section] "These contributions are expressed as absolute differences (e.g., +25%, -17%), conveyed verbally using a similar scale to that in [27], and visually through matching color coding"

## Foundational Learning

- Concept: **D-separation and belief propagation in BNs**
  - Why needed here: The explanations target questions about observation impact, paths, and interaction effects—all fundamentally require understanding how evidence propagates through conditional dependencies blocked or enabled by d-separation.
  - Quick check question: Given a V-structure A → C ← B, does observing C create dependence between A and B? Does observing A create dependence between C and B?

- Concept: **Common-effect (V-structure) interactions**
  - Why needed here: The study explicitly tests CommonEffect questions (7 questions) where two causes converge on one effect. Understanding explaining-away requires grasping how one cause's observation changes another's contribution.
  - Quick check question: If you observe both symptoms (Peeling and Bruising) that can result from a disease (Dermascare), how does learning about an alternative cause (Mutation) change your belief about the disease?

- Concept: **Contribution vs. absolute probability**
  - Why needed here: The explanation system conveys contributions (marginal impact of a finding given all other evidence), not just posterior probabilities. Users must distinguish "what is the probability?" from "how much did this finding change the probability?"
  - Quick check question: If P(Disease|Evidence A, B) = 70% and P(Disease|Evidence A) = 50%, what is the contribution of Evidence B? How would you express this on a qualitative scale?

## Architecture Onboarding

- Component map:
  - Verbal generator -> Contribution calculator -> Template-based NLG
  - Visual renderer -> BN visualization with color coding and animations
  - Animation sequencer -> Maps verbal segments to visual stages
  - Contribution calculator -> Computes absolute probability differences and ranks

- Critical path:
  1. Instantiate BN with evidence findings
  2. Calculate posterior probabilities for target variable
  3. Compute contribution of each finding (with/without other findings for interactions)
  4. Identify active paths and rank by contribution magnitude
  5. Generate verbal explanation from templates
  6. Synchronize visual highlights and animations with verbal segments

- Design tradeoffs:
  - Template-based vs. LLM generation: Templates ensure fidelity to BN inferences but scale poorly; LLMs offer flexibility but risk misrepresenting probabilistic reasoning
  - Small BNs vs. scalability: Current approach explains every arc and interaction—feasible for 5-10 nodes but not large networks
  - Sequential animations vs. static visuals: Animations reveal path order but require user time; static visuals allow comparison but may confuse overlapping paths

- Failure signatures:
  - Users perform no better than baseline on ControlThink questions (require BN knowledge, not explanations)
  - PathThink questions show smaller effect sizes than Path questions—indicates insufficient training for multi-step inference
  - SUS scores correlate with marks, not directly with condition—signals usability perception driven by success, not interface novelty

- First 3 experiments:
  1. Ablation study: Test combined explanations with visual-only and verbal-only components on separate participant groups using the same Rats/Podunk BNs to isolate modality contributions for each question type
  2. Scalability probe: Apply current template-based approach to a medium-sized BN (15-20 nodes) with 5+ findings; measure (a) explanation generation time, (b) user comprehension on Path questions, (c) subjective cognitive load
  3. Cohort comparison: Recruit participants with prior BN training (e.g., students completing a BN course) vs. crowd-workers; test whether effect sizes differ across Verbal, Visual, and Both conditions for Finding and Path questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed verbal and visual explanation methods be adapted to scale efficiently for larger Bayesian Networks without overwhelming the user?
- Basis in paper: [explicit] The authors state their system is "well suited only to small BNs" because explaining every arc and counterfactual scenario "scales poorly," but note it is "worth investigating how to adapt our methods."
- Why unresolved: The current implementation relies on manual or template-based approaches designed for small, binary networks. It is unclear how to summarize complex path contributions in large networks without losing interpretability.
- What evidence would resolve it: A user study measuring comprehension and cognitive load using an automated version of the interface on networks with significantly more variables and complex topologies.

### Open Question 2
- Question: Do the effectiveness and preference for verbal versus visual explanations differ between novice users and domain experts?
- Basis in paper: [explicit] The authors identify a limitation in recruiting "mainly participants with no prior BN experience" and explicitly propose evaluating the system with "different cohorts, e.g., students who are learning BNs."
- Why unresolved: The study focused on crowd-workers who may lack the spatial reasoning or domain knowledge of actual BN users; it is unknown if experts would find the combined explanations redundant or if they would benefit from different modalities.
- What evidence would resolve it: A comparative study replicating the experimental design with computer science students or decision analysts to measure performance differences across expertise levels.

### Open Question 3
- Question: Can Large Language Models (LLMs) generate accurate verbal explanations for Bayesian Network inferences without hallucinating probabilistic logic?
- Basis in paper: [explicit] The authors note that LLMs are a "natural next step" for realizing verbal explanations, but warn that "high-quality training and response validation are essential to avoid misrepresenting inferences."
- Why unresolved: The study used manually crafted templates. LLMs offer flexibility but introduce the risk of generating text that sounds plausible but contradicts the mathematical inferences of the network.
- What evidence would resolve it: An evaluation framework comparing the factual accuracy and logical consistency of LLM-generated explanations against ground-truth network inferences.

## Limitations
- The explanation system is well suited only to small BNs, as explaining every arc and counterfactual scenario scales poorly
- The study used manually crafted templates rather than automated explanation generation, limiting scalability and maintainability
- The participant pool consisted mainly of UK crowd-workers with no prior BN experience, constraining external validity

## Confidence

- **High confidence**: The finding that combined explanations outperform baseline for Finding, Path, and CommonEffect questions is well-supported by p<0.05 significance and clear effect sizes in odds ratios
- **Medium confidence**: The specific advantage of combined over visual-only explanations for Finding questions and over verbal-only for Path questions depends on the particular BN structures and question sets used
- **Low confidence**: The claim that "verbal-only explanations are nearly as effective" lacks direct statistical comparison between Verbal and Combined conditions across all question types

## Next Checks

1. **Ablation study replication**: Test combined explanations with separate participant groups using visual-only and verbal-only components on the same BNs to isolate modality contributions for each question type, addressing whether "nearly as effective" holds across all conditions

2. **Scalability probe**: Apply the current template-based approach to a medium-sized BN (15-20 nodes) with 5+ findings to measure explanation generation time, user comprehension on Path questions, and subjective cognitive load, validating the scalability concerns

3. **Cohort comparison**: Recruit participants with prior BN training (e.g., students completing a BN course) versus crowd-workers to test whether effect sizes differ across Verbal, Visual, and Both conditions for Finding and Path questions, addressing the knowledge base dependency of explanation effectiveness