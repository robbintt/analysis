---
ver: rpa2
title: 'Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks'
arxiv_id: '2508.14536'
source_url: https://arxiv.org/abs/2508.14536
tags:
- ch-dqn
- function
- chebyshev
- learning
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Chebyshev-DQN (Ch-DQN), a novel architecture
  that integrates a Chebyshev polynomial basis into the DQN framework to improve function
  approximation for deep reinforcement learning. By projecting input states into a
  feature space defined by Chebyshev polynomials, Ch-DQN provides a more powerful
  and efficient representation of complex value functions compared to standard MLPs.
---

# Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks

## Quick Facts
- arXiv ID: 2508.14536
- Source URL: https://arxiv.org/abs/2508.14536
- Authors: Saman Yazdannik; Morteza Tayefi; Shamim Sanisales
- Reference count: 17
- Key outcome: Ch-DQN integrates Chebyshev polynomials into DQN, achieving up to 39% better asymptotic performance and 3x sample efficiency on sparse-reward tasks compared to standard DQN.

## Executive Summary
This paper introduces the Chebyshev-DQN (Ch-DQN), a novel architecture that integrates a Chebyshev polynomial basis into the DQN framework to improve function approximation for deep reinforcement learning. By projecting input states into a feature space defined by Chebyshev polynomials, Ch-DQN provides a more powerful and efficient representation of complex value functions compared to standard MLPs. The approach leverages the orthogonality and minimax properties of Chebyshev polynomials to reduce inherent approximation error and improve training stability. Experiments on CartPole-v1, MountainCar-v0, and Acrobot-v1 show that Ch-DQN significantly outperforms standard DQN, achieving up to a 39% improvement in asymptotic performance and nearly 3x better sample efficiency on sparse-reward tasks. The results highlight the trade-off in polynomial degree selection, with moderate degrees (N=4) excelling on simpler tasks and higher degrees (N=8) needed for more complex value landscapes. This work validates the use of orthogonal polynomial bases in deep reinforcement learning.

## Method Summary
Ch-DQN replaces the standard ReLU-based MLP architecture in DQN with a Chebyshev polynomial basis expansion. The input state is first normalized to the [-1,1] range and then projected into a higher-dimensional feature space using Chebyshev polynomials of the first kind. This transformed representation is then processed through a standard MLP network. The Chebyshev polynomial expansion provides an orthogonal basis that better captures the underlying structure of the value function, reducing approximation error and improving training stability. The polynomial degree N is a key hyperparameter that controls the complexity of the feature space, with higher degrees providing more expressive power at the cost of increased computational overhead.

## Key Results
- Ch-DQN achieves up to 39% improvement in asymptotic performance compared to standard DQN across tested environments
- Sample efficiency improved by nearly 3x on sparse-reward tasks like MountainCar-v0
- Moderate polynomial degree (N=4) excels on simpler tasks like CartPole-v1, while higher degree (N=8) needed for complex value landscapes in Acrobot-v1
- Training stability improved through reduced approximation error from orthogonal polynomial basis

## Why This Works (Mechanism)
Chebyshev polynomials form an orthogonal basis on the interval [-1,1] with respect to the weight function 1/√(1-x²). This orthogonality property ensures that the polynomial coefficients are uncorrelated, which reduces multicollinearity in the feature space and improves the conditioning of the optimization problem. The minimax property of Chebyshev polynomials means they minimize the maximum approximation error, providing better worst-case guarantees compared to monomial bases. When used in reinforcement learning, this translates to more stable value function approximation and reduced overestimation bias, leading to improved policy learning and sample efficiency.

## Foundational Learning
**Chebyshev Polynomials** - Orthogonal polynomials on [-1,1] with weight function 1/√(1-x²). Why needed: Provides basis with superior approximation properties compared to monomials. Quick check: Verify orthogonality condition ∫T_m(x)T_n(x)/√(1-x²)dx = 0 for m≠n.

**Orthogonal Basis Functions** - Set of functions where inner products between different basis elements are zero. Why needed: Reduces multicollinearity and improves numerical stability in function approximation. Quick check: Confirm basis satisfies ⟨φ_i, φ_j⟩ = 0 for i≠j in the feature space.

**Minimax Approximation** - Approximation that minimizes the maximum error across the domain. Why needed: Provides better worst-case performance guarantees than least-squares approximation. Quick check: Compare maximum error of Chebyshev vs monomial approximation on test functions.

**Value Function Approximation** - Process of estimating the value function using parameterized function approximators. Why needed: Core component of deep reinforcement learning that directly affects policy quality. Quick check: Monitor value estimation error during training to detect instability.

## Architecture Onboarding

**Component Map**: State Normalization -> Chebyshev Expansion -> MLP -> Q-Value Output

**Critical Path**: Input state undergoes normalization to [-1,1], Chebyshev polynomial transformation creates orthogonal feature representation, MLP processes features to predict Q-values. The Chebyshev expansion layer is the critical innovation that replaces standard linear input layer.

**Design Tradeoffs**: Higher polynomial degree (N) increases representational power but also computational cost and risk of overfitting. The orthogonal basis reduces approximation error but requires careful normalization of inputs. Standard ReLU networks are simpler but may struggle with complex value function landscapes.

**Failure Signatures**: Poor performance with high polynomial degrees on simple tasks (overfitting), instability when inputs not properly normalized to [-1,1], and computational bottleneck during training if degree too high for available hardware.

**First Experiments**:
1. Compare Ch-DQN with N=4 vs standard DQN on CartPole-v1 to verify improvement on simple control tasks
2. Test polynomial degree sensitivity by evaluating N=2, N=4, and N=8 on Acrobot-v1
3. Measure training stability by monitoring Q-value estimation error and TD error variance during learning

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to three simple OpenAI Gym environments, lacking evaluation on complex continuous control tasks
- Computational overhead from Chebyshev polynomial transformations not quantified, potentially impacting real-time applications
- Polynomial degree analysis limited to only N=4 and N=8, leaving uncertainty about optimal configurations for intermediate cases

## Confidence
- "More powerful and efficient representation" of complex value functions: **High**
- "Up to 39% improvement in asymptotic performance": **Medium**
- "Nearly 3x better sample efficiency on sparse-reward tasks": **Low**

## Next Checks
1. Evaluate Ch-DQN on more challenging continuous control tasks (e.g., MuJoCo environments) to assess scalability to higher-dimensional state spaces and more complex value functions.

2. Conduct ablation studies comparing Ch-DQN against other orthogonal basis function approaches (Legendre, Fourier) to isolate the specific benefits of Chebyshev polynomials versus the general orthogonal basis framework.

3. Measure computational overhead during training and inference to quantify the practical trade-offs between improved performance and increased computational requirements.