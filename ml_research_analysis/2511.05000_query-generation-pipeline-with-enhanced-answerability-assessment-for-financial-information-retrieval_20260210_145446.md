---
ver: rpa2
title: Query Generation Pipeline with Enhanced Answerability Assessment for Financial
  Information Retrieval
arxiv_id: '2511.05000'
source_url: https://arxiv.org/abs/2511.05000
tags:
- retrieval
- query
- queries
- arxiv
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a query generation pipeline for constructing
  domain-specific IR benchmarks in financial applications, where real customer data
  is restricted. The methodology combines single-document and multi-document query
  generation with a reasoning-augmented answerability assessment method.
---

# Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval

## Quick Facts
- arXiv ID: 2511.05000
- Source URL: https://arxiv.org/abs/2511.05000
- Reference count: 40
- Authors: Hyunkyu Kim; Yeeun Yoo; Youngjun Kwak
- Primary result: Introduces a query generation pipeline with reasoning-augmented answerability assessment for financial IR benchmarks, achieving Pearson's ρ=0.60 human alignment.

## Executive Summary
This paper addresses the challenge of building domain-specific IR benchmarks for financial applications where real customer data is restricted. The authors propose a comprehensive query generation pipeline that combines single-document and multi-document query generation with a novel reasoning-augmented answerability assessment method. The approach generates 815 queries from 204 Korean banking documents, demonstrating that retrieval models struggle particularly with complex comparing and contrasting queries, highlighting fundamental limitations in current IR systems.

## Method Summary
The methodology processes 204 Korean banking documents (product disclosures, terms, policies) into 1,794 passages based on hierarchical structure. Single-document queries are generated using GPT-4o with structured prompts, then filtered through reasoning-augmented answerability assessment using DeepSeek-R1-Distill-Qwen-14B. Multi-document queries are synthesized through three strategies: topic-based merging (merging queries from same product), context deepening (merging query-passage pairs from same document), and comparing and contrasting (across products in same category). A dependency validation filter ensures queries genuinely require cross-document reasoning before human review.

## Key Results
- Reasoning-augmented evaluation achieves Pearson's ρ=0.60 alignment with human judgments versus ρ=0.356 for existing methods
- Comparing and contrasting queries show lowest retrieval performance (NDCG@5=0.414) compared to topic-based (0.709) and context deepening (0.696) types
- Retrieval performance degrades sharply with increasing document count (NDCG@5=0.302 for 4+ document queries)
- Best retrieval results come from hybrid sparse+dense models, with BM25+ance showing strong performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reasoning-augmented evaluation substantially improves alignment with human judgments for query answerability assessment.
- **Mechanism:** Explicit chain-of-thought "Think" steps guide the evaluator through structured reasoning before scoring, reducing superficial pattern matching and enforcing logical verification.
- **Core assumption:** Smaller reasoning-trained models (14B parameters) can outperform larger non-reasoning models when given structured reasoning prompts.
- **Evidence anchors:** DeepSeek-14B achieves ρ=0.598 vs G-EVAL-gpt4's ρ=0.356; only 1.33% false-positive rate for filtering.

### Mechanism 2
- **Claim:** Multi-document dependency filtering ensures generated queries genuinely require cross-document reasoning.
- **Mechanism:** A query must score higher on answerability when evaluated with ALL supporting passages combined than when evaluated with ANY individual passage alone (Equation 8).
- **Core assumption:** The answerability scoring function F provides monotonic scores where combined context yields higher scores iff additional information is genuinely needed.
- **Evidence anchors:** By applying this criterion, we eliminated a substantial number of queries, ensuring the final dataset included only those that require information across multiple contexts.

### Mechanism 3
- **Claim:** Comparing and contrasting queries expose fundamental limitations in current retrieval models that other query types do not.
- **Mechanism:** These queries require identifying fine-grained differences across documents, demanding both precise cross-document alignment and comparative reasoning.
- **Core assumption:** The performance gap reflects genuine model limitations rather than dataset artifacts.
- **Evidence anchors:** Comparing and contrasting queries achieve NDCG@5=0.414 vs 0.709 (topic-based merging) and 0.696 (context deepening).

## Foundational Learning

- **Concept: Multi-hop vs. Multi-document Retrieval**
  - Why needed here: The paper distinguishes multi-hop reasoning (inference chains within/between documents) from multi-document retrieval (information spread across sources).
  - Quick check question: Given queries A: "What is the interest rate for Product X?" and B: "How do interest rates differ between Products X and Y?", which requires multi-document retrieval and which might be single-document?

- **Concept: Hybrid Retrieval Paradigms**
  - Why needed here: Best results come from combining sparse (lexical) and dense (semantic) retrieval.
  - Quick check question: Why might BM25 (sparse) fail on "early termination fee" when the document says "prepayment penalty"—and would dense retrieval help?

- **Concept: Benchmark Contamination and Domain Specificity**
  - Why needed here: The paper argues existing benchmarks lack financial domain specificity.
  - Quick check question: If a retrieval model was trained on Wikipedia-based QA datasets, what failure modes might you expect on Korean banking product disclosures?

## Architecture Onboarding

- **Component map:** Raw Documents (204 banking PDFs) → Chunking → Passages (1,794 chunks) → Single-doc generation (GPT-4o + domain prompts) → Single-doc Queries → Reasoning-augmented answerability filter (DeepSeek-14B) → Filtered Single-doc Queries → Multi-doc synthesis (3 strategies) → Multi-doc Candidates → Dependency validation (Equation 8) → Human review (banking professionals) → KoBankIR Benchmark (815 queries)

- **Critical path:** The answerability evaluator is the gating component. If it produces noisy scores, the entire pipeline degrades.

- **Design tradeoffs:**
  - GPT-4o for generation vs. smaller model: Higher quality queries but higher cost
  - 14B vs. 7B evaluator: 14B achieves ρ=0.598 vs 7B's ρ=0.299—substantial gap justifies larger model
  - Human review threshold: Only queries with score ≥4.0 reviewed; lowering threshold increases coverage but raises review cost

- **Failure signatures:**
  - High false-positive rate in filtering: If >10% of accepted queries are unanswerable, evaluator needs recalibration
  - Multi-doc queries answerable from single doc: Dependency validation (Eq. 8) is failing
  - Comparative queries near-random performance: May indicate generation produces invalid comparisons

- **First 3 experiments:**
  1. **Evaluator ablation:** Compare DeepSeek-14B with reasoning steps vs. without, measuring correlation with human judgments on a held-out set of 50 queries
  2. **Generation model substitution:** Replace GPT-4o with a smaller model (e.g., Qwen-72B) and measure query quality degradation via answerability scores
  3. **Cross-domain transfer test:** Apply the same pipeline to a different domain (e.g., legal documents) and measure whether evaluator alignment holds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training modifications would enable retrieval models to achieve parity between comparing-and-contrasting queries and simpler query types?
- Basis in paper: Table 7 shows comparing-and-contrasting queries achieve only 0.414 NDCG@5 versus 0.749 for single-document queries
- Why unresolved: The paper identifies the gap but does not investigate whether the issue stems from embedding representations, training data distribution, or fundamental model architectures
- What evidence would resolve it: Ablation studies comparing fine-tuning on comparative queries, architectural changes (e.g., cross-attention mechanisms), or contrastive learning objectives designed for comparative reasoning

### Open Question 2
- Question: Does the query generation pipeline transfer effectively to other languages and financial regulatory frameworks beyond Korean banking?
- Basis in paper: The methodology is demonstrated only on Korean banking documents
- Why unresolved: No cross-lingual or cross-jurisdictional validation was conducted; legal/regulatory terminology differences may affect both query generation quality and answerability assessment
- What evidence would resolve it: Application of the pipeline to English, Chinese, or other language financial documents with corresponding human evaluation

### Open Question 3
- Question: Can the reasoning-augmented answerability assessment method maintain its superior human alignment (Pearson's ρ=0.60) when scaled to significantly larger query datasets?
- Basis in paper: Human validation used only 100 stratified samples; the 14B parameter evaluator may introduce computational bottlenecks not tested at scale
- Why unresolved: The paper demonstrates effectiveness on a filtered subset but does not analyze whether the correlation holds consistently across query complexity levels
- What evidence would resolve it: Large-scale annotation studies across the full 815-query dataset with inter-annotator agreement metrics and correlation analysis segmented by query type

### Open Question 4
- Question: What hybrid retrieval strategies beyond sparse+dense combinations could address the persistent performance gap for 4+ document queries (NDCG@5=0.302)?
- Basis in paper: Table 6 shows sharp degradation from 2-document (0.590) to 4-document (0.302) queries
- Why unresolved: The paper tests existing hybrid approaches but does not explore query decomposition, iterative retrieval, or graph-based document relationship modeling
- What evidence would resolve it: Experiments with multi-hop retrieval architectures, query decomposition methods, or attention mechanisms designed for aggregating evidence across many documents

## Limitations
- The pipeline depends critically on reasoning-augmented evaluator quality and exact prompt formulations, which are not fully specified
- Document chunking strategy and product category taxonomy for comparative queries are unspecified, potentially affecting query generation quality
- Human validation was limited to 100 samples, raising questions about generalizability to the full dataset

## Confidence
- **High confidence:** Core pipeline architecture and retrieval model performance gaps are well-demonstrated
- **Medium confidence:** Reasoning-augmented evaluation methodology shows strong initial results but requires broader validation
- **Low confidence:** Exact prompt templates and sampling parameters are unspecified, making exact reproduction challenging

## Next Checks
1. **Evaluator generalization test:** Apply the reasoning-augmented evaluation pipeline to a different domain (e.g., legal or medical documents) and measure whether ρ ≥ 0.50 alignment with human judgments is maintained
2. **Generation model ablation:** Replace GPT-4o with a smaller, fine-tuned model for query generation and measure the degradation in answerability scores and retrieval performance
3. **Multi-document dependency validation audit:** Manually verify a random sample of 50 multi-document queries to confirm that each genuinely requires cross-document reasoning rather than being answerable from a single source