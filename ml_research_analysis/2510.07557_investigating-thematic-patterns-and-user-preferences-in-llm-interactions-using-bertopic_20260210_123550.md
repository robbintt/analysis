---
ver: rpa2
title: Investigating Thematic Patterns and User Preferences in LLM Interactions using
  BERTopic
arxiv_id: '2510.07557'
source_url: https://arxiv.org/abs/2510.07557
tags:
- topic
- topics
- user
- thematic
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies BERTopic, a transformer-based topic modeling
  technique, to the lmsys-chat-1m dataset to uncover thematic patterns in LLM conversations
  and analyze their relation to user preferences. BERTopic identified 29 coherent
  topics including AI, programming, ethics, and cloud infrastructure.
---

# Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic

## Quick Facts
- arXiv ID: 2510.07557
- Source URL: https://arxiv.org/abs/2510.07557
- Authors: Abhay Bhandarkar; Gaurav Mishra; Khushi Juchani; Harsh Singhal
- Reference count: 40
- Primary result: BERTopic analysis of 1M LLM conversations revealed 29 topics and domain-specific model performance patterns

## Executive Summary
This study applies BERTopic, a transformer-based topic modeling technique, to the lmsys-chat-1m dataset to uncover thematic patterns in LLM conversations and analyze their relation to user preferences. The analysis identified 29 coherent topics including AI, programming, ethics, and cloud infrastructure, revealing that certain LLMs consistently outperform others within specific thematic domains. The findings demonstrate that domain-specific LLM performance is crucial for specialized applications, informing targeted fine-tuning strategies to improve real-world LLM performance and user satisfaction.

## Method Summary
The researchers employed BERTopic, a transformer-based topic modeling technique, to analyze conversations from the lmsys-chat-1m dataset. BERTopic uses pre-trained sentence transformers to generate document embeddings, applies density-based clustering (HDBSCAN) to group similar conversations, and then reduces the number of topics using hierarchical density-based clustering. This approach enables the identification of semantically coherent topics while filtering out noise and outliers in the conversation data.

## Key Results
- BERTopic identified 29 coherent topics including AI, programming, ethics, and cloud infrastructure
- Top 10 topics account for over 78% of user interactions, indicating concentrated user engagement
- Model performance varied significantly by topic, with gpt-4-1106-preview excelling in gaming and gpt-4-0314 in social issues
- No single model demonstrates uniform proficiency across all topics

## Why This Works (Mechanism)
The study leverages BERTopic's transformer-based approach to capture semantic relationships in LLM conversations, enabling the discovery of meaningful thematic patterns that correlate with user preferences and model performance variations across different domains.

## Foundational Learning

### Topic Modeling
- Why needed: To automatically discover and categorize thematic patterns in large text corpora without manual labeling
- Quick check: Visualize topic distributions and validate coherence through sample conversation analysis

### BERT-based Embeddings
- Why needed: To capture semantic meaning and context in conversations beyond simple keyword matching
- Quick check: Compare embedding similarity scores for semantically related vs. unrelated conversations

### HDBSCAN Clustering
- Why needed: To group semantically similar conversations while handling noise and varying cluster densities
- Quick check: Evaluate cluster separation using silhouette scores or similar metrics

### User Preference Analysis
- Why needed: To understand which model characteristics users value across different conversation topics
- Quick check: Correlate topic distributions with preference scores to identify performance patterns

## Architecture Onboarding

### Component Map
BERTopic pipeline: Conversation corpus -> Sentence transformer embeddings -> HDBSCAN clustering -> Topic reduction -> Topic-label mapping

### Critical Path
The most critical path involves the sentence transformer embeddings, as they directly determine the semantic quality of clustering and subsequent topic identification. Poor embeddings lead to incoherent topics regardless of clustering parameters.

### Design Tradeoffs
- Embedding model choice vs. computational efficiency
- Cluster density parameters vs. topic granularity
- Topic reduction depth vs. semantic distinctiveness

### Failure Signatures
- Topics with mixed, unrelated content suggest poor embedding quality
- Too many small topics indicate overly sensitive clustering parameters
- Missing expected topics may indicate insufficient corpus representation

### First Experiments
1. Visualize embedding space using dimensionality reduction to verify semantic clustering
2. Test topic coherence scores with different embedding models
3. Analyze topic stability across different random seeds

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Reliance on single large-scale dataset may introduce sampling bias toward certain user demographics or conversation types
- Topic quality validation limited to human inspection without quantitative metrics
- Performance comparisons based on user preference data may conflate model capabilities with user familiarity
- No analysis of temporal variations in model performance or user preferences

## Confidence

- BERTopic identified 29 coherent topics: Medium
- Certain LLMs consistently outperform others within specific domains: Medium
- No single model demonstrates uniform proficiency across all topics: High
- Top 10 topics account for 78% of interactions: High
- Domain-specific performance is crucial for specialized applications: Medium

## Next Checks

1. Conduct quantitative evaluation of topic coherence using established metrics (e.g., UCI coherence, NPMI) to validate the quality and distinctiveness of the 29 identified topics beyond qualitative assessment.

2. Perform statistical significance testing on LLM performance differences across topics using appropriate metrics (e.g., pairwise comparisons, confidence intervals) to confirm that observed variations are not due to random chance.

3. Replicate the analysis using multiple independent LLM conversation datasets to assess the robustness and generalizability of the observed thematic patterns and model performance rankings across different user populations and conversation contexts.