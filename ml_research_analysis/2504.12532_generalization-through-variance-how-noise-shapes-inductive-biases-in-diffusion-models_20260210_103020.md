---
ver: rpa2
title: 'Generalization through variance: how noise shapes inductive biases in diffusion
  models'
arxiv_id: '2504.12532'
source_url: https://arxiv.org/abs/2504.12532
tags:
- score
- training
- distribution
- diffusion
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how diffusion models generalize beyond
  their training data. The key insight is that diffusion models do not learn the true
  score function of the training distribution, but instead learn a "noisy" version
  called the proxy score.
---

# Generalization through variance: how noise shapes inductive biases in diffusion models

## Quick Facts
- **arXiv ID**: 2504.12532
- **Source URL**: https://arxiv.org/abs/2504.12532
- **Reference count**: 40
- **Key outcome**: Diffusion models learn a "proxy score" with high variance in boundary regions, creating an inductive bias that fills gaps between training examples rather than memorizing them

## Executive Summary
This paper investigates how diffusion models generalize beyond their training data through a novel mechanism involving score function variance. The key insight is that diffusion models learn not the true score function of the training distribution, but a "noisy" version called the proxy score. This proxy score exhibits high variance in boundary regions between training examples, which introduces randomness into the learning process and enables generalization by "filling in gaps" rather than simply memorizing training data.

The author develops a theoretical framework using path integrals to characterize the typical distribution learned by diffusion models. This analysis reveals that the learned distribution depends on both the average score estimator and a noise term proportional to the variance of score estimators across different training sample realizations. The framework demonstrates that even naive score estimators that directly memorize training data still generalize due to proxy score variance, and that the extent of generalization depends on model capacity and time cutoff parameters.

## Method Summary
The paper develops a theoretical framework using path integrals to analyze diffusion model generalization. The approach characterizes the typical distribution learned by diffusion models as a combination of the average score estimator and a variance-dependent noise term (V-kernel). The analysis examines both naive estimators that memorize training data and learned estimators in the NTK regime, showing how proxy score variance creates an inductive bias for boundary region smoothing. The framework derives explicit formulas for the typical distribution and demonstrates how generalization depends on model capacity (ratio of features to training samples) and time cutoff parameters.

## Key Results
- Diffusion models learn a proxy score function with high variance in boundary regions between training examples, enabling generalization rather than pure memorization
- The typical distribution learned depends on both the average score estimator and a V-kernel term proportional to score estimator variance across training samples
- Even naive score estimators that directly memorize training data still generalize due to proxy score variance
- Linear models and NTK neural networks exhibit similar generalization patterns modulated by their feature-related inductive biases
- Generalization extent depends on model capacity (feature-to-sample ratio) and time cutoff parameters

## Why This Works (Mechanism)
The mechanism relies on the inherent noise structure of diffusion training. As the model learns to denoise corrupted inputs, the proxy score function it learns has high variance in regions between training examples. This variance acts as a source of randomness that "smears" probability mass across boundary regions rather than concentrating it on training points. The path integral framework shows this variance-induced noise term scales with the feature-to-sample ratio, explaining why more expressive models generalize more effectively through this mechanism.

## Foundational Learning

**Score matching and score functions** - Understanding how diffusion models learn gradients of log-density rather than the density itself is crucial for grasping why proxy scores emerge. Quick check: Verify that the score function of a Gaussian is proportional to the negative of the input.

**Proxy scores and variance** - The distinction between true and proxy scores, and how their variance affects learning, forms the core theoretical insight. Quick check: Confirm that proxy score variance is largest in regions between training examples.

**Path integral formulation** - The mathematical framework that allows characterization of the typical distribution learned by diffusion models. Quick check: Verify the path integral converges under the assumptions stated in the paper.

**Neural Tangent Kernel regime** - Understanding how infinitely wide neural networks behave as kernel methods is key to analyzing learned estimators. Quick check: Confirm that the NTK limit captures the relevant inductive biases for finite-width networks.

**Boundary regions and interpolation** - The geometric intuition about how probability mass flows in high-dimensional spaces between training examples. Quick check: Visualize how variance-based interpolation differs from nearest-neighbor interpolation.

## Architecture Onboarding

**Component map**: Training data -> Score estimator (linear/NTK models) -> Proxy score function -> Denoising process -> Generated samples

**Critical path**: The proxy score function learned during training directly determines the denoising trajectory, which shapes the final generated distribution through the variance-induced boundary smoothing mechanism.

**Design tradeoffs**: Higher model capacity increases the feature-to-sample ratio, enhancing generalization through variance but potentially requiring more training data to maintain stability. Time cutoff parameters must balance between capturing long-range dependencies and computational feasibility.

**Failure signatures**: When proxy score variance is too low (underfitting) or unbounded (instability), the model either memorizes training data or fails to converge. Excessive boundary smoothing can lead to mode collapse or unrealistic interpolations.

**First experiments**:
1. Train a linear model on synthetic data with clear boundaries and measure the extent of interpolation between training points
2. Vary the feature-to-sample ratio in NTK regime and quantify the change in boundary region smoothing
3. Compare diffusion model generalization on datasets with different boundary structures (sharp vs. gradual transitions)

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework relies on idealized assumptions about Gaussian noise and linear models that may not fully capture high-capacity neural networks
- Several technical conditions (path integral convergence, bounded variance) may not hold in practical implementations
- The paper does not fully characterize when unbounded variance becomes problematic for practical diffusion models
- Quantitative predictions about exact generalization extent in practical settings remain to be validated

## Confidence
- **High**: The core mathematical framework connecting proxy score variance to generalization behavior
- **Medium**: The specific role of boundary regions in inducing generalization across different model classes
- **Low**: The quantitative predictions about the exact extent of generalization in practical neural network settings

## Next Checks
1. **Empirical validation of boundary effects**: Test whether increasing boundary region size (through larger time cutoffs) systematically improves interpolation quality between training examples in real diffusion models.

2. **Model capacity experiments**: Verify the predicted relationship between feature-to-sample ratio and generalization extent by training diffusion models with varying network widths on controlled datasets with clear boundaries.

3. **Alternative noise schedules**: Examine whether non-Gaussian noise schedules or different variance patterns produce different generalization behaviors, testing the robustness of the variance-based inductive bias mechanism.