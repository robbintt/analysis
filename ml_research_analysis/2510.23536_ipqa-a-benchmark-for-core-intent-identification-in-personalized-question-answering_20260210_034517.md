---
ver: rpa2
title: 'IPQA: A Benchmark for Core Intent Identification in Personalized Question
  Answering'
arxiv_id: '2510.23536'
source_url: https://arxiv.org/abs/2510.23536
tags:
- intents
- intent
- core
- user
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces core intents as motivations users prioritize
  when selecting answers in personalized question answering. It constructs IPQA, a
  benchmark dataset derived from community question answering platforms, where core
  intents are identified from observable answer selection behavior rather than explicit
  user statements.
---

# IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering

## Quick Facts
- arXiv ID: 2510.23536
- Source URL: https://arxiv.org/abs/2510.23536
- Reference count: 39
- Core intent identification remains challenging with F1 scores of 0.30-0.54

## Executive Summary
This paper introduces core intents as motivations users prioritize when selecting answers in personalized question answering. It constructs IPQA, a benchmark dataset derived from community question answering platforms, where core intents are identified from observable answer selection behavior rather than explicit user statements. Using LLM-based annotation and human validation, the dataset covers 47 domains with verified quality metrics showing 85.67% human agreement on annotation criteria. Experimental results across state-of-the-art models demonstrate core intent identification remains challenging, with F1 scores ranging from 0.30-0.54 and performance degrading as question complexity increases. Models struggle to extract intent patterns from raw user histories, though preprocessing historical intents substantially improves performance.

## Method Summary
IPQA is built from SE-PQA and LaMP-QA datasets through intersection filtering to identify personalization-requiring instances. Core intents are generated using GPT-5-Mini with structured output (name, description, reference), then filtered through a two-stage LLM-based quality assessment applying four binary criteria. The second stage extracts atomic information pieces from selected answers and maps them to intents—intents with at least one mapped information piece become core intents. Evaluation uses IPQA-Eval framework with LLM-based matching to compute Precision, Recall, and F1 scores. Human validation confirms annotation quality with 85.67% agreement on criteria.

## Key Results
- Core intent identification F1 scores range from 0.30-0.54 across state-of-the-art models
- Performance degrades consistently as question complexity increases (5+ intents)
- Raw historical context (User Profile Raw) frequently underperforms No Personalization baseline
- Preprocessing historical intents (User Profile Intents) substantially improves performance
- Extended history helps all complexity levels but doesn't close the gap for high-complexity questions

## Why This Works (Mechanism)

### Mechanism 1: Core Intent Derivation from Observable Behavior
Core intents can be inferred from user answer selection patterns rather than explicit articulation. Grounded in satisficing theory, when users select an answer, they demonstrate which intents met their minimum acceptance threshold. Intents that align with information in the selected answer are classified as "core intents"—those the user prioritized for satisfaction. This provides verifiable ground truth without requiring subjective importance judgments.

### Mechanism 2: Two-Stage LLM-Based Filtering with Verification
Automated intent annotation quality can be ensured through sequential LLM-based quality assessment followed by core intent selection. Stage 1 evaluates each generated intent against four binary criteria (Completeness, Faithfulness, Motivational Fidelity, Answer Justification). Stage 2 extracts atomic information pieces from selected answers and maps them to intents—intents with at least one mapped information piece become core intents.

### Mechanism 3: Historical Intent Patterns Enable Personalization
Providing preprocessed historical intents improves core intent identification more than raw user histories. Models struggle to extract intent-level patterns from unstructured post histories. When historical items are pre-annotated with core intents, models receive explicit intent-question mappings that reveal user priorities, enabling pattern recognition.

## Foundational Learning

- **Concept: Satisficing Theory (Bounded Rationality)**
  - Why needed here: The entire core intent framework depends on understanding that users select "good enough" answers rather than optimal ones, creating observable thresholds.
  - Quick check question: If a user selects an answer addressing 2 of 5 expressed intents, what does satisficing theory predict about the other 3 intents?

- **Concept: Multi-Intent Detection**
  - Why needed here: Questions in PQA contexts express multiple coexisting intents with varying importance—core intent identification requires distinguishing all intents before filtering.
  - Quick check question: How does core intent identification differ from standard multi-intent detection tasks like MixATIS?

- **Concept: Atomic Information Decomposition**
  - Why needed here: Core intent selection depends on mapping intents to information pieces extracted from answers—understanding atomic decomposition is essential for the alignment mechanism.
  - Quick check question: What makes an information piece "atomic" versus a compound assertion?

## Architecture Onboarding

- **Component map**: Data Collection -> Intent Generation -> Quality Control -> Core Intent Filtering -> Evaluation
- **Critical path**: Intent generation quality → alignment accuracy → evaluation reliability. Errors propagate: poor initial intents cannot be recovered by downstream filtering.
- **Design tradeoffs**: LLM annotation vs. human annotation (scale vs. biases); information piece granularity (precision vs. computational cost); history size (pattern richness vs. noise).
- **Failure signatures**: Random User Profile outperforms No Personalization → dataset contamination; high-complexity questions show >20% recall drop → fundamental multi-intent limitation; precision drops with small histories → insufficient context.
- **First 3 experiments**: 1) Baseline establishment: Run No Personalization vs. Random User Profile to verify personalization signal. 2) History size sweep: Test k ∈ {5, 10, 20, 30} to identify optimal context window. 3) Intent-to-answer correlation: Validate predicted intents improve answer generation.

## Open Questions the Paper Calls Out

### Open Question 1
How can models be improved to effectively extract intent-level patterns directly from raw, unstructured user histories without requiring preprocessed intent annotations? The paper states models fail to extract intent-level patterns from unstructured post histories, as User Profile (Raw) frequently underperforms No Personalization despite providing complete user histories.

### Open Question 2
What techniques can mitigate the performance degradation observed for high-complexity questions (5+ core intents), even when extensive user history is available? The paper reports performance degrades consistently as complexity increases, with high-complexity questions maintaining substantially lower absolute recall even with extended history.

### Open Question 3
To what extent does the satisficing-based core intent definition—grounded in answer selection behavior—capture users' true prioritized intents versus alternative annotation approaches? While not all intents aligned with selected answers may represent true priorities, the paper acknowledges this approach provides a more practical framework grounded in observable behavior.

### Open Question 4
How well does IPQA generalize to domains beyond the three tested categories (Entertainment, Living, Social) and to non-community Q&A platforms? The dataset covers 47 domains but groups them into only three categories, relying exclusively on community Q&A data for both questions and user histories.

## Limitations
- Satisficing theory assumptions about user behavior may not hold across all populations or question types
- LLM-based annotation pipeline depends on model-specific behaviors that may not generalize
- Atomic information piece extraction and intent mapping introduce potential subjectivity
- Performance numbers are highly sensitive to implementation details like history size tuning

## Confidence

**High Confidence**: Satisficing theory foundation for deriving core intents; two-stage LLM filtering methodology with human validation; finding that raw historical data is less useful than preprocessed intent patterns.

**Medium Confidence**: Core intent identification as a fundamental challenge requiring specialized benchmarks; performance degradation with question complexity as a universal phenomenon.

**Low Confidence**: Generalizability of specific performance numbers (F1 scores of 0.30-0.54) across different model architectures and scales.

## Next Checks

1. **Cross-Platform Validation**: Test the core intent identification methodology on non-community QA platforms (e.g., customer service interactions, search query logs) to verify satisficing theory assumptions hold beyond the current dataset.

2. **Model Architecture Robustness**: Evaluate whether observed performance patterns (raw history underperformance, preprocessing benefits) persist across a broader range of model families, including open-weight models not used in original experiments.

3. **Intent Granularity Sensitivity**: Systematically vary the atomicity of information pieces and intent definitions to determine how sensitive core intent identification is to these annotation choices, potentially revealing whether current definitions capture meaningful user priorities or reflect annotation artifacts.