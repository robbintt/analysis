---
ver: rpa2
title: 'Initialization Schemes for Kolmogorov-Arnold Networks: An Empirical Study'
arxiv_id: '2509.03417'
source_url: https://arxiv.org/abs/2509.03417
tags:
- initialization
- power-law
- training
- function
- glorot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the under-explored problem of initialization
  schemes for Kolmogorov-Arnold Networks (KANs), proposing three methods: theory-driven
  LeCun- and Glorot-inspired schemes, and an empirical power-law family with tunable
  exponents. Evaluations on function fitting, forward PDE problems, and a subset of
  the Feynman dataset reveal that while LeCun-based initializations underperform,
  Glorot-inspired initialization significantly outperforms baseline in parameter-rich
  models, and power-law initialization achieves the strongest overall performance
  across tasks and architectures.'
---

# Initialization Schemes for Kolmogorov-Arnold Networks: An Empirical Study

## Quick Facts
- arXiv ID: 2509.03417
- Source URL: https://arxiv.org/abs/2509.03417
- Authors: Spyros Rigas; Dhruv Verma; Georgios Alexandridis; Yixuan Wang
- Reference count: 40
- This study addresses the under-explored problem of initialization schemes for Kolmogorov-Arnold Networks (KANs), proposing three methods: theory-driven LeCun- and Glorot-inspired schemes, and an empirical power-law family with tunable exponents. Evaluations on function fitting, forward PDE problems, and a subset of the Feynman dataset reveal that while LeCun-based initializations underperform, Glorot-inspired initialization significantly outperforms baseline in parameter-rich models, and power-law initialization achieves the strongest overall performance across tasks and architectures. Training dynamics analysis via loss curves and Neural Tangent Kernel (NTK) spectra confirms these findings, with power-law initialization yielding stable, well-conditioned spectra and faster convergence. Power-law exponents α∈{0.25,0.5} and β≥1.0 are consistently favorable for function fitting, while lower β (0.75≤β≤1.25) performs best for PDEs. Overall, initialization emerges as a critical factor for KAN training, with power-law initialization offering a robust, domain-adaptable solution.

## Executive Summary
This empirical study systematically evaluates initialization strategies for KANs across diverse tasks. The authors propose three schemes: LeCun-inspired (forward variance only), Glorot-inspired (forward and backward variance), and a power-law family (empirical scaling). Evaluations on function fitting, forward PDE problems, and Feynman dataset problems show that LeCun-based methods underperform, Glorot is robust for large models, and power-law achieves the strongest overall performance. NTK spectral analysis reveals that power-law initialization maintains stable eigenvalue spectra while baselines collapse, explaining faster convergence and better final accuracy. The study identifies specific exponent ranges (α∈{0.25,0.5}, β≥1.0 for functions; β∈[0.75,1.25] for PDEs) as optimal, demonstrating initialization's critical role in KAN training.

## Method Summary
The study proposes three initialization schemes for KANs: LeCun-inspired (preserving forward activation variance only), Glorot-inspired (balancing forward and backward variance via harmonic mean of fan-in/fan-out), and power-law (empirical scaling σ ∝ (fan-in × (G+k+1))^-α or β). Evaluations use jaxKAN framework with Adam optimizer (lr=10^-3), spline order k=3, and architectures up to [2,32,32,32,1] with G=20. Tasks include function fitting (4,000 samples, 5 synthetic functions), PDEs (Allen-Cahn, Burgers, 2D Helmholtz with 2^12 collocation points), and Feynman dataset subset (20 equations). Metrics include MSE loss, relative L^2 error, and NTK eigenvalue spectrum analysis to diagnose training stability.

## Key Results
- LeCun-based initializations underperform across all tasks, particularly failing completely on PDE problems due to derivative sensitivity
- Glorot-inspired initialization significantly outperforms baseline in parameter-rich models by balancing forward/backward variance
- Power-law initialization achieves the strongest overall performance, with α∈{0.25,0.5} and β≥1.0 optimal for function fitting, and β∈[0.75,1.25] best for PDEs
- NTK spectral analysis confirms power-law initialization maintains stable, well-conditioned spectra while baseline/LeCun schemes cause eigenvalue collapse

## Why This Works (Mechanism)

### Mechanism 1: Variance Balancing for Forward and Backward Passes
Balancing the variance of both activations and gradients across layers appears to stabilize training in parameter-rich KANs. The Glorot-inspired scheme sets weight standard deviations based on the harmonic mean of fan-in and fan-out dimensions, preventing activation explosion in deep layers. This ensures the "signal" neither vanishes nor explodes as it passes through the residual (R) and basis (B) components of the KAN layer. Assumes statistical independence between residual and basis terms and that variance equipartition is a valid proxy for optimal training dynamics.

### Mechanism 2: Spectral Conditioning via Power-Law Scaling
Empirical power-law scaling of weights stabilizes the Neural Tangent Kernel (NTK) spectrum, accelerating convergence. By tuning exponents α and β to control initialization scale relative to layer width and grid size (G), the method produces an NTK spectrum with gentle power-law decay. This "well-conditioned" spectrum prevents dominance of few large eigenvalues, allowing gradient descent to optimize multiple modes simultaneously. Assumes stable, non-collapsing NTK spectrum at initialization correlates with faster training and lower final error.

### Mechanism 3: Derivative Sensitivity in Physics-Informed Contexts
Normalizing spline basis functions to enforce variance preservation harms performance in PDE solving. Normalization pushes standard deviation constants into the denominator of basis function definition. When PDE loss differentiates these functions, these constants alter stiffness and scale of gradients, destabilizing optimization of residual terms. Assumes "automatic" variance normalization is beneficial generally, which fails when loss function explicitly depends on derivatives of basis functions.

## Foundational Learning

- **B-Spline Basis Functions & Grids**
  - Why needed here: KANs use learnable spline functions defined on a grid, unlike MLPs with fixed activations. Understanding that G (grid size) controls resolution and basis weights b_jim control shape is essential for implementing initialization schemes.
  - Quick check question: Does increasing grid size G require re-scaling initialization weights to preserve variance?

- **Neural Tangent Kernel (NTK)**
  - Why needed here: The paper uses NTK evolution to prove why one initialization is better than another. You must grasp that NTK describes how network output changes during training (gradient descent).
  - Quick check question: If NTK eigenvalue spectrum collapses (rapidly decays to zero), does network stop learning?

- **Variance Preservation (Forward vs. Backward)**
  - Why needed here: Core theoretical proposal adapts LeCun (forward variance) and Glorot (forward + backward variance) to KANs.
  - Quick check question: Why might preserving only forward activation variance (LeCun) fail to prevent vanishing gradients during backpropagation?

## Architecture Onboarding

- **Component map:** Input (x) → KAN Layer: Residual Function R(x) (SiLU) + Spline Basis B_m(x) on Grid G → Output (y): Σ r·R(x) + c·Σ b·B(x)

- **Critical path:** Initialization of r_ji and b_jim is the intervention point. Power-Law scheme sets σ_r ∝ (n_in(G+k+1))^-α and σ_b ∝ (n_in(G+k+1))^-β.

- **Design tradeoffs:**
  - Glorot vs. Power-Law: Glorot is theory-driven and robust for large models but requires computing expectations of basis functions. Power-Law is empirical/tunable, offering peak performance but requiring grid search for α, β.
  - Grid Size (G): Larger G increases parameter count and changes optimal initialization scale (denominator in Eq. 7).

- **Failure signatures:**
  - LeCun-Normalized in PDEs: Training loss stagnates immediately (0% success rate in paper)
  - Large Model Baseline: Slow convergence with oscillating loss
  - NTK Collapse: If eigenvalues decay too fast at initialization, model effectively loses capacity to learn high-frequency components

- **First 3 experiments:**
  1. Replicate Function Fitting: Initialize [2, 8, 8, 1] KAN using Power-Law (α=0.25, β=1.0) on target function f_1(x,y)=xy. Compare convergence speed against baseline (σ=0.1).
  2. NTK Spectral Check: For large architecture (G=20, width 32), plot NTK eigenvalues at initialization for Baseline vs. Power-Law. Verify Baseline has steeper decay (poor conditioning).
  3. PDE Sensitivity Test: Attempt to solve Allen-Cahn equation using "LeCun-Normalized" initialization. Confirm failure mode (stagnation) described in Section 4.1 to understand why derivative-aware initialization is necessary.

## Open Questions the Paper Calls Out

- What is the theoretical justification for superior performance of specific power-law initialization exponent ranges (e.g., α ∈ {0.25, 0.5}) compared to variance-preserving schemes? The authors state their power-law scheme "currently lacks a rigorous theoretical foundation" and ask "why specific exponent ranges perform well." The study relies on empirical grid searches to identify optimal exponents, but standard variance preservation theories failed to predict these values.

- Do proposed initialization strategies transfer effectively to KAN variants with different architectural bases, such as Chebyshev-based or residual-free formulations? The authors list "investigating transferability across KAN variants (e.g., Chebyshev-based or residual-free forms)" as specific direction for future work. Current study restricted focus to standard spline-based KANs, leaving behavior in other architectures unknown.

- Are proposed initialization schemes effective in non-supervised domains such as reinforcement learning or generative modeling? The authors note that "further exploration in other domains such as reinforcement learning or generative modeling could provide additional insights." Paper's scope was limited to supervised function fitting and physics-informed PDEs; performance on tasks with different loss landscapes remains untested.

## Limitations

- Evaluation confined to relatively small architectures (max 4 layers) and specific task domains, raising questions about scalability to deeper networks or other scientific ML applications
- Theoretical derivation of Glorot-inspired initialization relies on simplifying assumptions about statistical independence between residual and basis components that may not hold in practice
- Power-law scheme, while empirically successful, lacks rigorous theoretical justification for optimal exponent ranges identified

## Confidence

- High confidence in Glorot-inspired initialization outperforming LeCun-based methods for parameter-rich KANs (supported by multiple task types and consistent NTK spectral analysis)
- Medium confidence in power-law initialization as best overall performer (empirical results strong but exponent tuning requirements suggest sensitivity to hyperparameters)
- Medium confidence in NTK spectral interpretation as primary mechanism (correlation demonstrated but causation not rigorously established)

## Next Checks

1. Test power-law initialization across broader range of architectures (5+ layers) to evaluate scalability claims and identify potential breakdown points
2. Conduct ablation studies on statistical independence assumption in Glorot derivation by measuring actual activation and gradient variance correlations
3. Apply initialization schemes to different scientific ML domain (e.g., molecular dynamics or climate modeling) to assess domain transferability beyond evaluated PDEs and Feynman equations