---
ver: rpa2
title: Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots
arxiv_id: '2512.06193'
source_url: https://arxiv.org/abs/2512.06193
tags:
- risk
- safety
- gauge
- conversational
- harm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GAUGE is a real-time framework that detects hidden conversational
  escalation in AI chatbots by analyzing the LLM's internal probability shifts toward
  affective risk, without relying on external classifiers. It outperforms existing
  safety mechanisms on implicit harm benchmarks, achieving higher AUROC, AUPRC, and
  F1 scores than external classifiers like HateBERT and Llama-Guard-3-8B.
---

# Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots
## Quick Facts
- arXiv ID: 2512.06193
- Source URL: https://arxiv.org/abs/2512.06193
- Authors: Jihyung Park; Saleh Afroogh; David Atkinson; Junfeng Jiao
- Reference count: 22
- Primary result: GAUGE reduces attack success rates from 97.3% to 6% on MinorBench

## Executive Summary
GAUGE is a real-time framework that detects hidden conversational escalation in AI chatbots by analyzing the LLM's internal probability shifts toward affective risk, without relying on external classifiers. It outperforms existing safety mechanisms on implicit harm benchmarks, achieving higher AUROC, AUPRC, and F1 scores than external classifiers like HateBERT and Llama-Guard-3-8B. On the MinorBench adversarial test, GAUGE reduces attack success rates from 97% to 6%, demonstrating superior robustness to bypass attempts. The method offers interpretability by projecting logits onto an affective space, enabling monitoring of conversational drift. Limitations include difficulty distinguishing therapeutic validation from harmful reinforcement and reliance on static emotion lexicons.

## Method Summary
GAUGE calibrates a risk direction vector λ from labeled safe/harmful dialogues by tracking the LLM's internal probability trajectory over an emotion lexicon. During calibration, it extracts log-probabilities for emotion-related tokens at each generation step, aggregates them into mean feature vectors, and updates λ via exponential moving average weighted by harm labels. At inference, GAUGE computes two metrics—Negative Risk Shift (NRS) for directional alignment and Absolute Risk Potential (ARP) for magnitude-based risk—to quantify conversational escalation in real-time without external classifiers.

## Key Results
- GAUGE achieves 0.6698 AUROC, 0.6451 AUPRC, and 0.6424 F1 on DiaSafety benchmark, outperforming external classifiers
- On MinorBench adversarial test, GAUGE reduces attack success rates from 97.3% to 6%
- Real-time inference with logit-based trajectory analysis detects implicit harms invisible to traditional toxicity filters

## Why This Works (Mechanism)
### Mechanism 1: Logit-Based Affective Trajectory Probing
- Claim: The internal probability distribution over emotion-related tokens during generation encodes conversational risk signals that surface-level text analysis misses.
- Mechanism: At each generation step k, GAUGE extracts log-probabilities for tokens in a curated emotion lexicon (derived from NRC Emotion Lexicon), aggregating subtoken probabilities into word-level risk vectors r_k. These vectors form a trajectory representing how the model's output shifts affective state across the response.
- Core assumption: The model's next-token probability distribution over affective vocabulary reflects its implicit "belief state" about appropriate emotional direction, even when surface text appears neutral.

### Mechanism 2: Calibrated Risk Direction Vector (λ) via Contrastive EMA
- Claim: A single normalized weight vector λ, learned via exponential moving average over labeled safe/harmful dialogues, captures the directional signature of conversational escalation.
- Mechanism: During Stage 1 calibration, GAUGE processes each labeled dialogue, extracting mean feature vectors z. Harmful dialogues (S=+1) pull λ toward their direction; safe dialogues (S=-1) push λ away. The EMA update (λ ← (1-β)λ + α·S·ẑ) gradually shapes λ into a reference direction pointing toward harmful affective drift.

### Mechanism 3: Dual-Metric Risk Quantification (NRS + ARP)
- Claim: Combining directional alignment (NRS) and magnitude intensity (ARP) provides complementary signals that detect both gradual drift and acute risk states.
- Mechanism: NRS = cos(λ, z) measures how closely the current response trajectory aligns with the harmful direction. ARP uses Z-score normalization to detect when the conversation statically dwells in high-risk regions even without directional shift.
- Core assumption: Harmful escalation can manifest either as movement toward negative affect or as sustained presence in high-intensity negative states.

## Foundational Learning
- **Autoregressive logit extraction**
  - Why needed here: GAUGE requires access to raw token logits during generation, not just output text. Understanding how to hook into the forward pass and extract probability distributions is prerequisite.
  - Quick check question: Can you explain why logits must be gathered at each generation step rather than only at the final token?

- **Emotion lexicon mapping and tokenization handling**
  - Why needed here: The NRC Emotion Lexicon contains words that may span multiple subtokens. Aggregating log-probabilities across subtokens correctly is essential for accurate risk vector computation.
  - Quick check question: How would you compute the log-probability of a multi-subtoken word like "hopeless" given a model's vocabulary?

- **Cosine similarity for high-dimensional direction comparison**
  - Why needed here: NRS relies on cosine similarity between λ and z. Understanding why cosine (not Euclidean distance) is appropriate for directional alignment is critical.
  - Quick check question: If two vectors have identical direction but different magnitudes, what is their cosine similarity?

## Architecture Onboarding
- Component map:
  - **Stage 1 (Offline Calibration)**: DiaSafety training set → LLM response generation → Logit extraction over lexicon → Mean feature vector z → EMA update of λ → Normalized λ
  - **Stage 2 (Real-time Inference)**: Live conversation → LLM generates response → Logit extraction → Mean trajectory vector z → Compute NRS (cosine similarity) and ARP (Z-score weighted sum) → Risk score output
  - **Shared Components**: NRC Emotion Lexicon, subtoken aggregation logic, normalization procedures

- Critical path:
  1. Lexicon preparation: Pre-tokenize all emotion words and store subtoken indices for fast lookup
  2. During generation: At each token step, perform indexed gather on logits for lexicon tokens (O(|W|) not O(V))
  3. Accumulate trajectory: Sum log-probabilities across generation steps
  4. Compute metrics: Single dot product for NRS, Z-score lookup for ARP
  5. Threshold comparison: Binary classification if needed (τ=0 used in MinorBench)

- Design tradeoffs:
  - **Lexicon scope vs. coverage**: Larger lexicon captures more affective nuance but increases computation; static lexicon misses slang/emojis
  - **Aggregation strategy**: Mean (GAUGE-mean) performed best, but min/top-k/percentile offer different sensitivity profiles
  - **Therapeutic validation false positives**: Current system cannot distinguish harmful reinforcement from legitimate empathy—future work requires pragmatic markers

- Failure signatures:
  - **High NRS on supportive responses**: "I understand why you feel hopeless" scores high due to negative affect words—indicates empathy/harm ambiguity
  - **Low sensitivity to novel harm vectors**: Slang, emojis, or coded language not in NRC lexicon will produce near-zero risk vectors
  - **Calibration dataset bias**: λ is only as good as DiaSafety labels; if implicit harm taxonomy is incomplete, λ will miss those directions

- First 3 experiments:
  1. **Baseline validation on DiaSafety test set**: Implement GAUGE-mean, compare AUROC/AUPRC/F1 against HateBERT, ToxicBERT, and Llama-Guard-3-8B. Verify ~0.67 AUROC result.
  2. **Ablation on aggregation strategies**: Test min, top-k, and percentile aggregations to confirm that performance gains come from trajectory signal, not aggregation choice.
  3. **Lexicon coverage stress test**: Construct adversarial prompts using slang/emojis absent from NRC lexicon; measure detection failure rate to quantify coverage gap.

## Open Questions the Paper Calls Out
- **Open Question 1**: How can the framework effectively distinguish between maladaptive reinforcement (implicit harm) and therapeutic validation without incorporating pragmatic intent markers?
  - Basis in paper: Section 5 states that a "critical challenge lies in distinguishing maladaptive reinforcement from therapeutic validation," noting that supportive responses can trigger high risk scores.
  - Why unresolved: GAUGE relies on the direction of affect; therefore, a valid therapeutic response containing negative affect words (e.g., "I understand your hopelessness") appears mathematically similar to harmful reinforcement.
  - What evidence would resolve it: A modified framework that successfully integrates pragmatic features to maintain high recall while reducing false positives on verified therapeutic dialogue datasets.

- **Open Question 2**: To what extent does the reliance on the static NRC Emotion Lexicon reduce detection sensitivity for harms encoded in out-of-vocabulary terms, internet slang, or emojis?
  - Basis in paper: Section 5 identifies "Lexical Coverage Constraints," warning that the method "may exhibit reduced sensitivity to harm conveyed through out-of-vocabulary terms, internet slang, or emojis."
  - Why unresolved: The lexicon is fixed, whereas adolescent communication is dynamic and frequently utilizes novel linguistic constructs that the static word list cannot capture.
  - What evidence would resolve it: An ablation study evaluating GAUGE's performance on a specialized dataset of adversarial prompts utilizing contemporary slang and emoji-based manipulation.

- **Open Question 3**: Does the calibrated risk weight vector (λ) transfer effectively across different LLM architectures, or is it tightly coupled to the specific probability distributions of the training model?
  - Basis in paper: The methodology describes calibrating λ using the trajectory of a specific model (Llama-3.1-8B), but the evaluation does not test this calibration on other architectures.
  - Why unresolved: It is unclear if the "harmful affective direction" learned for one model's logits maps universally to others or if the vector is merely learning model-specific artifacts.
  - What evidence would resolve it: Cross-model evaluation results where a λ vector calibrated on one architecture (e.g., Llama) is applied to detect risks in another (e.g., Mistral or GPT) without re-training.

## Limitations
- **Therapeutic validation false positives**: GAUGE cannot distinguish harmful reinforcement from legitimate therapeutic empathy, leading to false positives on supportive responses containing negative affect words
- **Static lexicon coverage gaps**: Reliance on NRC Emotion Lexicon means GAUGE misses harms encoded in slang, emojis, and out-of-vocabulary terms common in adolescent communication
- **Architecture-specific calibration**: The risk weight vector λ is calibrated for specific LLM probability distributions and may not transfer effectively across different architectures

## Confidence
- **Method reproducibility**: Medium - Core methodology specified but key hyperparameters (α, β) and lexicon composition details missing
- **Result validity**: High - Clear benchmark comparisons with external classifiers and adversarial testing on MinorBench
- **Practical applicability**: Medium - Strong theoretical framework but significant limitations in real-world deployment due to false positives and coverage gaps

## Next Checks
1. Implement baseline GAUGE-mean on DiaSafety test set and verify AUROC/AUPRC/F1 scores against reported benchmarks
2. Conduct lexicon coverage stress test by constructing adversarial prompts with slang/emojis absent from NRC lexicon
3. Test cross-model transfer by applying λ calibrated on Llama to detect risks in another architecture without re-training