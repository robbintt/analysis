---
ver: rpa2
title: A Study of the Plausibility of Attention between RNN Encoders in Natural Language
  Inference
arxiv_id: '2501.13735'
source_url: https://arxiv.org/abs/2501.13735
tags:
- attention
- heuristic
- words
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the plausibility of attention-based explanations
  in sentence comparison, specifically natural language inference (NLI). The authors
  propose a heuristic attention map based on word similarity between premise and hypothesis
  sentences, focusing on verbs, nouns, and adjectives.
---

# A Study of the Plausibility of Attention between RNN Encoders in Natural Language Inference

## Quick Facts
- arXiv ID: 2501.13735
- Source URL: https://arxiv.org/abs/2501.13735
- Authors: Duc Hau Nguyen; Duc Hau Nguyen; Pascale Sébillot
- Reference count: 31
- Key outcome: This paper evaluates the plausibility of attention-based explanations in sentence comparison, specifically natural language inference (NLI). The authors propose a heuristic attention map based on word similarity between premise and hypothesis sentences, focusing on verbs, nouns, and adjectives. This heuristic reasonably correlates with human-annotated explanations in the eSNLI corpus and serves as a baseline for plausibility evaluation. Experiments with RNN encoders and attention layers show that model-based attention maps poorly align with either human or heuristic annotations. Attention weights are scattered across unimportant tokens like stop words, failing to highlight the most informative syntactic elements. The heuristic method is shown to be a better proxy for plausible explanations than the learned attention mechanism.

## Executive Summary
This paper investigates whether attention mechanisms in RNN-based NLI models produce explanations that humans find plausible. The authors introduce a heuristic attention method based on word similarity between premise and hypothesis sentences, excluding stop words. This heuristic serves as a baseline for evaluating the plausibility of learned attention maps. Experiments show that the learned attention weights poorly align with human-annotated explanations and the heuristic method, scattering across unimportant tokens like stop words rather than highlighting content words. The heuristic method demonstrates better plausibility, suggesting current attention mechanisms may not naturally produce human-understandable explanations.

## Method Summary
The authors propose a heuristic attention map that computes cosine similarity between word embeddings in premise and hypothesis sentences, excluding stop words. This yields attention weights concentrated on semantically related content words. For model-based attention, they use RNN encoders (BiLSTM) with cross-attention where each word attends to the final hidden state of the opposite sentence. The model is trained on the eSNLI corpus for NLI classification (entailment/contradiction/neutral). Plausibility is evaluated by comparing attention maps against human-annotated explanations in eSNLI using ROC curves and AUC metrics.

## Key Results
- The heuristic attention method based on word similarity correlates reasonably with human-annotated explanations in eSNLI
- Model-based attention weights scatter across unimportant tokens like stop words rather than highlighting informative content words
- For 85.24% of samples, human annotations match the heuristic better than model attention
- The heuristic method serves as a better baseline for plausibility evaluation than learned attention mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A similarity-based heuristic can serve as a plausible proxy for human explanation in sentence comparison tasks.
- Mechanism: For each word token, compute cosine similarity against all tokens in the opposite sentence (excluding stop words), sum these similarities, and normalize to [0,1] range. This yields attention weights concentrated on semantically related content words—primarily verbs, nouns, and adjectives.
- Core assumption: In entailment relationships, words with close meanings between premise and hypothesis are most relevant for human explanation.
- Evidence anchors:
  - [abstract] "The authors propose a heuristic attention map based on word similarity between premise and hypothesis sentences... This heuristic reasonably correlates with human-annotated explanations in the eSNLI corpus."
  - [Section IV] Formal definition: h(wi) = σ(Σ similarity(wi, vj)) where S is the set of stop words.
  - [corpus] Related paper "Regularization, Semi-supervision, and Supervision for a Plausible Attention-Based Explanation" explores similar plausibility optimization, but direct corpus evidence for this specific heuristic is limited.
- Break condition: Heuristic fails for neutral class relationships where semantic similarity doesn't indicate entailment; also assumes content words are universally more explanatory than function words.

### Mechanism 2
- Claim: Cross-attention between RNN encoders produces sentence embeddings enhanced by attending to semantically related tokens in the opposite sentence.
- Mechanism: BiLSTM contextualizes word vectors into hidden representations. Each word's hidden state hi is compared via dot product with the final hidden state (sentence embedding) hn of the opposite sentence. Softmax normalization yields attention weights α, which weight the hidden states to produce an enhanced context vector c.
- Core assumption: The final hidden state hn captures sentence meaning well enough to guide meaningful attention.
- Evidence anchors:
  - [Section V] "The intuition is that α vector should give how much relevant a word is comparing to the sentence embedding of the other side hm."
  - [Section V] Equation 4 defines attention weight: αi = exp(h⊺i hn) / Σ exp(h⊺k hn)
  - [corpus] Weak direct corpus support for this specific cross-attention variant; related architectures use similar approaches but differ in implementation.
- Break condition: If sentence embeddings fail to capture semantic gist (e.g., for negation, long sentences), attention weights become uninformative.

### Mechanism 3
- Claim: Learned attention weights in RNN-based NLI models exhibit low plausibility—they scatter across unimportant tokens rather than human-relevant content words.
- Mechanism: The attention layer optimizes for classification accuracy, not interpretability. Without explicit plausibility constraints, gradients distribute attention broadly, including to stop words, determinants, and punctuation.
- Core assumption: Model optimization for task performance does not naturally yield human-aligned attention patterns.
- Evidence anchors:
  - [abstract] "Attention weights are scattered across unimportant tokens like stop words, failing to highlight the most informative syntactic elements."
  - [Section VI.B] "For 85.24% of ϵheuristic values, the human annotation matches the heuristic map better than the model attention."
  - [Section VI.B] Qualitative analysis confirms model gives scattered attention weights, mostly on stop words.
  - [corpus] Paper "Regularization, Semi-supervision, and Supervision for a Plausible Attention-Based Explanation" addresses this by proposing regularization to improve plausibility.
- Break condition: If explicit plausibility supervision or regularization is added, learned attention may better align with human explanations.

## Foundational Learning

- Concept: Faithfulness vs. Plausibility distinction
  - Why needed here: The paper explicitly distinguishes whether attention reflects model reasoning (faithfulness) versus whether it helps humans understand decisions (plausibility). These are orthogonal—a model can be faithful but incomprehensible, or plausible but not reflect actual computation.
  - Quick check question: If a model's attention highlights words humans find explanatory but the model actually uses different features for its decision, is this a failure of faithfulness or plausibility?

- Concept: Softmax attention normalization
  - Why needed here: Understanding how raw similarity scores become probability distributions is essential for interpreting attention maps and why they may distribute weight across all tokens rather than concentrating on key words.
  - Quick check question: Why might softmax cause attention to spread across many tokens even when only one word is semantically relevant?

- Concept: Word embedding similarity (cosine)
  - Why needed here: Both the heuristic and model attention rely on embedding similarity. Understanding what cosine similarity captures—and its limitations—is critical for interpreting why the heuristic outperforms learned attention.
  - Quick check question: What semantic relationships might cosine similarity miss that could be important for NLI (e.g., antonyms, negation)?

## Architecture Onboarding

- Component map:
  Embedding Layer (GloVe 300d) -> Contextualization Layer (BiLSTM 300d) -> Cross-Attention Layer (dot product with opposite sentence final hidden state) -> Context vectors concatenated with final hidden states -> MLP Classifier (1 hidden layer, ReLU) -> 3-class prediction (entailment/contradiction/neutral)

- Critical path:
  Input tokens → Embedding lookup → BiLSTM encoding → Cross-attention computation (premise attends to hypothesis final state, vice versa) → Context vectors concatenated with final hidden states → MLP classifier → 3-class prediction

- Design tradeoffs:
  - Shared encoder weights: Reduces parameters, assumes premise/hypothesis have similar distributions, but may miss asymmetric patterns
  - Single-layer BiLSTM: Simpler but limited capacity for complex dependencies
  - Dot product attention: Computationally efficient but may miss nuanced semantic relationships compared to additive attention
  - No plausibility regularization: Optimizes accuracy at expense of interpretable attention

- Failure signatures:
  - Attention concentrated on stop words (determinants, punctuation) rather than content words
  - Low correlation between attention weights and human-highlighted words (AUC comparisons show model < heuristic)
  - Jensen-Shannon divergence between model attention and human annotation distributions indicates significant mismatch
  - High variance in per-instance correlation with human annotations

- First 3 experiments:
  1. Reproduce the AUC comparison: Train model on eSNLI train set, compute ROC curves comparing model attention vs. heuristic attention against human annotations on entailment test samples. Verify heuristic AUC > model AUC.
  2. Ablate the stop-word exclusion in the heuristic: Test whether including stop words degrades correlation with human annotations, confirming the design choice.
  3. Add plausibility regularization: Use the heuristic map as a supervision signal (or regularization term) for the attention layer and measure whether plausibility improves without significant accuracy loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed word-similarity heuristic be incorporated as a regularization term during training to improve attention plausibility without sacrificing model faithfulness or task performance?
- Basis in paper: [explicit] The authors state: "As an extension, the heuristic could serve as a complementary to a regularization proposed in [27] to enhance the plausibility without sacrificing the faithfulness of the attention mechanism."
- Why unresolved: The paper only uses the heuristic for post-hoc evaluation against human annotations and model attention, but does not attempt to integrate it into the training objective.
- What evidence would resolve it: Experiments comparing models trained with and without heuristic-based regularization, measuring both plausibility (alignment with human annotations) and faithfulness (prediction consistency when attention is manipulated).

### Open Question 2
- Question: Do the findings regarding poor plausibility of cross-attention generalize to Transformer-based architectures and self-attention mechanisms in NLI tasks?
- Basis in paper: [inferred] The study is limited to RNN encoders with cross-attention; the authors note self-attention models [4–6] exist but do not evaluate them. The related work mentions attention interpretability debates primarily concern specific architectures.
- Why unresolved: Transformers dominate modern NLI, yet this study only examines BiLSTM-based cross-attention. Self-attention distributions may differ substantially in their alignment with human explanations.
- What evidence would resolve it: Replication of the same evaluation methodology (comparison with eSNLI annotations and the heuristic) on Transformer-based NLI models such as BERT or RoBERTa.

### Open Question 3
- Question: Would extending the heuristic and evaluation to the contradiction and neutral classes yield similar results, or is the word-similarity approach only suitable for entailment?
- Basis in paper: [inferred] The authors state they "focus mostly on the former [entailment] in this study" and note "the same heuristic might be justified for the contradiction class, however not for the neutral one." The evaluation is explicitly restricted to entailment samples.
- Why unresolved: The relationship between lexical similarity and plausibility may differ for contradiction (where opposing meanings matter) and neutral (where neither alignment nor opposition holds).
- What evidence would resolve it: Analysis of attention plausibility on contradiction and neutral classes in eSNLI, potentially with modified heuristics that capture semantic opposition for contradiction.

## Limitations

- The evaluation focuses only on the entailment class, potentially missing class-specific attention patterns in contradiction and neutral examples
- The RNN architecture used is relatively simple compared to modern transformer-based approaches, limiting generalizability of findings
- The absence of ablation studies on the BiLSTM architecture prevents determining whether attention plausibility issues are inherent to attention mechanisms or specific to this implementation

## Confidence

- High confidence: The core finding that learned attention weights poorly align with human explanations (supported by quantitative metrics and qualitative analysis)
- Medium confidence: The heuristic method's superiority as a plausibility proxy (based on AUC comparisons but limited to entailment class)
- Medium confidence: The cross-attention mechanism's theoretical framework (based on established attention principles but limited empirical validation of effectiveness)

## Next Checks

1. Conduct inter-annotator agreement analysis on human explanations in eSNLI to establish baseline reliability for plausibility metrics
2. Test the heuristic method's performance on contradiction and neutral classes to evaluate class-specific attention patterns
3. Implement and evaluate plausibility regularization techniques (as referenced in related work) to determine if learned attention can be made more human-aligned without sacrificing accuracy