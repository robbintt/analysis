---
ver: rpa2
title: 'RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward'
arxiv_id: '2505.10218'
source_url: https://arxiv.org/abs/2505.10218
tags:
- training
- data
- metrics
- arxiv
- role-playing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of maintaining role consistency
  in role-playing conversational agents (RPCAs) by introducing RAIDEN-R1, a reinforcement
  learning framework that integrates Verifiable Role-Awareness Reward (VRAR). The
  method generates quantifiable rewards by assessing role-specific keys through singular
  and multi-term mining strategies.
---

# RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward

## Quick Facts
- arXiv ID: 2505.10218
- Source URL: https://arxiv.org/abs/2505.10218
- Authors: Zongsheng Wang; Kaili Sun; Bowen Wu; Qun Yu; Ying Li; Baoxun Wang
- Reference count: 40
- Key outcome: 14B-GRPO model achieves 88.04% SBK and 88.65% CM accuracy on RAIDEN benchmark, outperforming baselines while maintaining first-person Chain-of-Thought reasoning

## Executive Summary
This paper addresses the challenge of maintaining role consistency in role-playing conversational agents by introducing RAIDEN-R1, a reinforcement learning framework that integrates Verifiable Role-Awareness Reward (VRAR). The method generates quantifiable rewards by assessing role-specific keys through singular and multi-term mining strategies. Additionally, the authors construct a high-quality, role-aware Chain-of-Thought dataset through multi-LLM collaboration to enhance reasoning coherence. Experiments on the RAIDEN benchmark demonstrate that the 14B-GRPO model achieves 88.04% and 88.65% accuracy on Script-Based Knowledge and Conversation Memory metrics, respectively, outperforming baseline models while maintaining robustness. Case analyses reveal enhanced ability to resolve conflicting contextual cues and sustain first-person narrative consistency.

## Method Summary
The RAIDEN-R1 framework employs GRPO with dual rewards: format rewards enforcing structured CoT output within `<think/>` tags, and accuracy rewards using VRAR's keyword-based verification. VRAR implements Single-Term Validation (binary rewards for role-specific keywords) and Multi-Term Dynamic Parsing (Python function verification for semantic equivalence). The training pipeline uses Qwen2.5-14B-Instruct initialized from Open-R1 library, with 8x H800 GPUs, lr=3e-6 cosine scheduler, and generation quantity=7. A high-quality CoT corpus is generated through DeepSeek-R1-671B with multi-LLM refinement for cold-start SFT experiments.

## Key Results
- 14B-GRPO achieves 88.04% Script-Based Knowledge (SBK) accuracy versus 86.59% baseline
- 14B-GRPO achieves 88.65% Conversation Memory (CM) accuracy versus 80.25% baseline
- Cold-start SFT with CoT data degrades performance (71.74% SBK, 53.78% CM), suggesting reinforcement learning discovers reasoning patterns that supervised imitation cannot replicate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Verifiable Role-Awareness Reward (VRAR) enables quantifiable optimization of role-consistency in RPCAs by converting abstract role fidelity into executable keyword-verification tasks.
- Mechanism: VRAR implements two strategies—Single-Term Validation (STV) assigns binary rewards based on presence of role-specific keywords extracted via multi-model consensus; Multi-Term Dynamic Parsing (MTDP) uses LLM-generated Python functions to verify semantic equivalence across keyword variants. This converts the non-quantifiable role-playing objective into discrete, executable reward signals.
- Core assumption: Role-consistency can be adequately captured through keyword presence and semantic equivalence, and correct responses contain identifiable role-specific terms that models can learn to generate.
- Evidence anchors:
  - [abstract]: "The method introduces both singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys."
  - [section 2.1]: "We propose the VRAR framework which implements a keyword-driven reward mechanism designed to systematically assess role consistency and contextual alignment."
  - [corpus]: Related work Character-R1 similarly uses RLVR for role-aware reasoning, suggesting convergent validation of verifiable-reward approaches for role-playing domains.
- Break condition: If role-consistency requires pragmatic appropriateness, emotional resonance, or narrative coherence beyond factual keyword coverage, VRAR's rule-based rewards will produce reward-hacking without genuine role-fidelity.

### Mechanism 2
- Claim: GRPO with format+accuracy dual rewards induces first-person Chain-of-Thought reasoning that improves contextual memory retrieval and conflict resolution.
- Mechanism: The format reward enforces structured CoT output within `<think/>` tags, while accuracy rewards provide learning signal for keyword generation. GRPO's group-relative advantage estimation enables stable policy updates despite sparse binary rewards. The emergent first-person CoT (avg 30.1 tokens) reflects internal role-identification rather than task-framing metacognition.
- Core assumption: Models can learn to generate useful intermediate reasoning through reinforcement signals alone, and this reasoning will generalize to unseen role-playing contexts.
- Evidence anchors:
  - [section 2.1.2]: "We employ two reward mechanisms in the GRPO-based reward model: format reward and accuracy reward."
  - [section 3.4]: "Statistical analysis of test set data indicates that 14B-GRPO generates relatively short CoT outputs, with an average length of 30.1 tokens."
  - [section 3.4]: "VRAR framework demonstrates natural generation of first-person CoT content, model tend to intrinsically identifies it with its assigned role."
  - [corpus]: GRPO-λ and Clinical-R1 demonstrate GRPO's applicability to domain-specific reasoning, but corpus lacks direct evidence for role-playing generalization.
- Break condition: If CoT length remains constrained (~30 tokens), complex multi-turn reasoning or deep character motivation tracking may exceed reasoning capacity, limiting scalability to richer personas.

### Mechanism 3
- Claim: Cold-start SFT with multi-LLM-curated CoT corpus fails to improve role-playing metrics, suggesting supervised imitation of reasoning traces is insufficient without reinforcement-driven discovery.
- Mechanism: DeepSeek-R1-671B generates raw CoT, refined via content compression (removing meta-instructions) and style adaptation (first-person internal monologue). Claude-3.5 generates final responses. Despite higher-quality training data, 14B-SFT(CoT cold-start) degrades on SBK (71.74%) and CM (53.78%) versus baseline (86.59%, 80.25%).
- Core assumption: Assumption: Reasoning capability transfer requires the model to discover reasoning patterns through reward-guided exploration rather than imitation of expert demonstrations.
- Evidence anchors:
  - [section 2.2]: "We employ DeepSeek-R1-671B for role-specific response generation, with focused optimization of its Chain-of-Thought outputs."
  - [table 1]: "14B-SFT(CoT cold start) only outperformed 14B-Instruct on topic related metrics (TA and TS), while exhibiting degradation across all other dimensions."
  - [corpus]: No corpus evidence directly addresses SFT vs. RL for reasoning acquisition in role-playing; related GRPO papers focus on RL-only approaches.
- Break condition: If alternative CoT curation strategies (longer reasoning, different teacher models, curriculum sampling) could produce effective cold-start data, the failure may be data-quality-specific rather than methodology-inherent.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO replaces critic networks with group-relative advantage estimation, computing advantages by comparing each output against group mean rewards. This simplifies training for sparse-reward role-playing tasks where value-function learning is unstable.
  - Quick check question: Can you explain why GRPO samples multiple outputs per prompt and computes relative advantages rather than using an explicit value function?

- Concept: **Verifiable Reward Design**
  - Why needed here: Unlike math/coding with ground-truth verification, role-playing responses lack unique correct answers. VRAR's keyword-based rewards require understanding how to convert soft alignment criteria into executable boolean functions.
  - Quick check question: For a role-playing query "What's your hometown?", can you sketch how STV vs. MTDP would differ in reward computation?

- Concept: **Chain-of-Thought Reasoning in RL**
  - Why needed here: The paper extends CoT beyond prompting to reinforcement-learned reasoning. Understanding the interaction between format enforcement, reasoning depth, and reward sparsity is essential for diagnosing training dynamics.
  - Quick check question: Why might enforcing `<think/>` tags improve reasoning quality even when the format reward alone provides no correctness signal?

## Architecture Onboarding

- Component map:
  ```
  Input: (profile, dialog_history, query, references)
      ↓
  VRAR Data Pipeline:
    ├─ STV: keyword extraction → multi-model validation → cardinality=1 constraint
    └─ MTDP: keyword expansion (QwQ-32B) → legitimacy check (Qwen-72B) → Python code gen → validation
      ↓
  GRPO Training:
    ├─ Format Reward: pattern check + Chinese ratio + special token filter
    └─ Accuracy Reward: STV (keyword ∈ response) OR MTDP (python_exec(response))
      ↓
  Output: <think/>CoT reasoning (first-person, ~30 tokens)<response/>role-consistent reply
  ```

- Critical path: The MTDP validation pipeline (keyword expansion → code generation → 70% consistency threshold between QwQ judgment and code execution) is the highest-risk component. Failures here propagate noisy rewards through GRPO, destabilizing training.

- Design tradeoffs:
  - STV precision vs. MTDP coverage: STV provides high-confidence rewards but excludes diverse valid responses; MTDP handles variability but introduces LLM-judge noise.
  - CoT length vs. efficiency: 30-token CoT enables fast inference but may limit reasoning depth for complex personas.
  - Cold-start SFT vs. RL-only: Cold-start provides reasoning templates but risks imitation-without-understanding; pure RL discovers reasoning but requires more exploration.

- Failure signatures:
  - Reward hacking: Model generates target keywords in unnatural contexts (e.g., embedding "福州" in irrelevant responses).
  - Format collapse: Model generates empty `<think/>` tags or copies keywords into reasoning section without genuine inference.
  - Role drift: CoT reverts to third-person task-framing ("I need to analyze the character...") instead of first-person internal monologue.
  - SFT degradation: Significant metric drops on SBK/CM after cold-start training indicate overfitting to synthetic CoT patterns.

- First 3 experiments:
  1. **VRAR ablation**: Train with format-only reward (no accuracy reward) to isolate whether CoT emergence requires task-specific learning signal or arises from structural constraint alone.
  2. **Keyword coverage audit**: On held-out test set, compute precision/recall of VRAR-extracted keywords against human-judged correct responses to quantify reward signal quality.
  3. **Cold-start data scaling**: Train SFT models with varying CoT corpus sizes (1K, 5K, 10K) followed by GRPO to determine if SFT degradation is data-quantity or data-quality driven.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the VRAR framework maintain its effectiveness and efficiency when scaled to models significantly larger than 14B parameters?
- Basis in paper: [explicit] The authors state in Section 4 that "existing experiments have focused on models below 14B parameters, future investigations will be implemented on larger-scale models."
- Why unresolved: It is unclear if the improvements observed in the 14B model (via GRPO and keyword-based rewards) will translate to larger architectures, where reasoning capabilities and data efficiency may differ significantly.
- What evidence would resolve it: Evaluation results of the identical VRAR pipeline applied to 70B+ parameter models, demonstrating comparable or superior accuracy gains over baselines.

### Open Question 2
- Question: Can higher-quality Chain-of-Thought (CoT) data resolve the failure of cold-start SFT to enhance role-playing performance?
- Basis in paper: [explicit] Section 4 notes that "acquiring CoT capabilities through SFT fails to enhance role-playing performance" and requires "further validation" to determine if "improving the quality of CoT training data" can fix this.
- Why unresolved: The current cold-start approach (using distilled DeepSeek-R1 data) led to performance degradation, leaving the causal factors (data quality vs. methodological fit) unidentified.
- What evidence would resolve it: Ablation studies using rigorously filtered or human-verified CoT datasets that result in positive transfer learning effects during the SFT phase.

### Open Question 3
- Question: Is it feasible to extend the Verifiable Role-Awareness Reward (VRAR) mechanism to handle subjective metrics like emotional resonance or stylistic alignment?
- Basis in paper: [explicit] The authors note their current rewards are "relatively limited to memory and profile consistency" and list exploring "additional role-playing evaluation metrics" as a key next step.
- Why unresolved: The current method relies on keyword matching (STV) and executable code (MTDP), which are suited for factual verification but difficult to apply to the nuances of personality or emotional tone.
- What evidence would resolve it: A successful expansion of the reward system that includes verifiable criteria for subjective attributes, showing measurable improvement in "character" or "style" scores on standard benchmarks.

## Limitations

- The VRAR framework's keyword-based verification may inadequately capture complex role-consistency requirements, potentially rewarding superficial keyword inclusion without genuine role-fidelity.
- The MTDP pipeline introduces LLM-judge noise through multiple model dependencies (QwQ-32B expansion, Qwen-72B legitimacy checking, Python generation), creating potential reward signal corruption.
- The CoT reasoning remains relatively shallow (~30 tokens), suggesting limited capacity for deep character motivation tracking or multi-turn reasoning.

## Confidence

- **High confidence**: GRPO training methodology, VRAR keyword extraction mechanisms, experimental protocol on RAIDEN benchmark, and baseline comparisons are clearly specified and reproducible.
- **Medium confidence**: The effectiveness of VRAR rewards in capturing true role-consistency versus keyword matching, and the generalizability of first-person CoT emergence across diverse personas.
- **Low confidence**: Whether the observed SFT degradation reflects inherent limitations of supervised reasoning imitation versus data quality issues, and whether the 30-token CoT constraint will scale to more complex role-playing scenarios.

## Next Checks

1. **Reward Signal Quality Audit**: Compute precision/recall of VRAR-extracted keywords against human-annotated correct responses on held-out test data to quantify how well keyword verification captures genuine role-consistency versus rewarding keyword stuffing.

2. **CoT Depth Scaling Experiment**: Train variants with enforced minimum CoT lengths (50, 100 tokens) to determine whether reasoning depth correlates with performance improvements on complex persona attributes or multi-turn consistency tasks.

3. **Cross-Domain Transfer Validation**: Apply the trained 14B-GRPO model to open-domain role-playing datasets (character.ai, AI Dungeon) to assess whether RAIDEN-specific training generalizes to naturalistic conversational contexts and diverse character archetypes.